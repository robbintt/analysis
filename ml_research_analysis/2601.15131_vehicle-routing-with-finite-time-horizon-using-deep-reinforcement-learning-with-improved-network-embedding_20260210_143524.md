---
ver: rpa2
title: Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning
  with Improved Network Embedding
arxiv_id: '2601.15131'
source_url: https://arxiv.org/abs/2601.15131
tags:
- routing
- embedding
- time
- vehicle
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep reinforcement learning-based routing
  method with a graph attention network (GAT)-Edge-based network representation module
  to solve the vehicle routing problem with a finite time horizon (VRP-FTH). The proposed
  routing network embedding module generates local node encoding vectors and a context-aware
  global graph embedding vector using GAT-Edge and Cross-Attention mechanism.
---

# Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding

## Quick Facts
- arXiv ID: 2601.15131
- Source URL: https://arxiv.org/abs/2601.15131
- Reference count: 6
- Key result: Achieves 57.7% higher customer service rate on real-world EMA highway networks and 52.6% improvement on Euclidean instances compared to existing methods

## Executive Summary
This paper introduces a deep reinforcement learning framework for vehicle routing with a finite time horizon (VRP-FTH) that uses graph attention networks with edge features for improved network representation. The method employs a GAT-Edge encoder for local node embeddings combined with cross-attention for context-aware global graph embeddings, incorporating the remaining time horizon into the routing decision process. Experimental results show significant improvements over existing methods, achieving 86.6% customer service rate on EMA-50 instances while being substantially faster than genetic algorithms and other DRL approaches.

## Method Summary
The approach uses a graph attention network with edge features (GAT-Edge) to encode local node information, followed by a cross-attention mechanism to generate context-aware global graph embeddings conditioned on the vehicle state and remaining time horizon. The policy network outputs action probabilities for routing decisions, trained using REINFORCE policy gradient with failure penalties. The method incorporates edge features (normalized travel time inverse) into the attention mechanism and uses sparse KNN adjacency matrices for computational efficiency.

## Key Results
- Achieves 86.6% customer service rate on EMA-50 instances (vs. 50.0% for DRL-Transformer)
- Solves EMA-100 instances in 0.55 seconds (vs. 19.44 seconds for genetic algorithm)
- Provides 52.6% improvement on Euclidean instances compared to DRL-Transformer method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating edge features into graph attention improves routing performance on real-world networks.
- Mechanism: GAT-Edge computes attention scores using node features, then multiplies by normalized edge features (travel time inverse) before softmax aggregation. This weights neighboring node contributions by edge connectivity strength, capturing that real roads are not straight lines.
- Core assumption: Travel time encoded in edges provides signal beyond Euclidean distances.
- Evidence anchors:
  - [abstract] "The experimental results demonstrate the importance of incorporating network structure and edge features in the embedding module"
  - [section] Ablation study (Table 5): PG-GAT without Edge Features achieves 83.2% vs. 86.6% with Edge Features—a 4.1% improvement
  - [corpus] EFormer paper confirms edge-based Transformers improve VRP over coordinate-only methods
- Break condition: If your routing network is purely Euclidean with no distinct edge costs, edge feature benefits diminish.

### Mechanism 2
- Claim: Context-aware global graph embedding enables better routing decisions under time constraints.
- Mechanism: Cross-Attention uses the vehicle state (current location, remaining time horizon) as a query against node embeddings (keys/values). The attention-weighted aggregation produces a global embedding conditioned on what's currently relevant—nodes reachable within remaining time get higher implicit weight.
- Core assumption: Global context, not just local node features, is necessary for finite-horizon optimization where trade-offs matter.
- Evidence anchors:
  - [abstract] "context-aware global graph embedding vector using GAT-Edge and Cross-Attention mechanism"
  - [section] Ablation study: Without global graph embedding, customer service drops 11.8% (76.4% vs. 86.6%)
  - [corpus] SED2AM and DRL-Transformer papers lack explicit global context embeddings, which may explain performance gaps
- Break condition: If time horizon is effectively unlimited (all customers reachable), global context conditioning provides marginal benefit.

### Mechanism 3
- Claim: Embedding the remaining time horizon directly into the graph representation improves service rate.
- Mechanism: Remaining time τ is concatenated into the context vector and used in cross-attention query generation. This allows the policy to modulate decisions based on urgency—e.g., favoring nearby customers when time is scarce.
- Core assumption: The optimal routing strategy shifts as remaining time decreases.
- Evidence anchors:
  - [abstract] "We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context"
  - [section] Ablation study: Without finite horizon in embedding, performance drops 10.7% (78.2% vs. 86.6%)
  - [corpus] Weak direct evidence; neighboring papers do not explicitly condition embeddings on remaining time
- Break condition: If time horizon is static across all instances or very long, time-conditioning provides less signal.

## Foundational Learning

- Concept: Graph Attention Networks (GAT)
  - Why needed here: Core encoder for node embeddings; must understand how attention coefficients are computed and aggregated.
  - Quick check question: Can you explain how GAT differs from GCN in aggregating neighbor information?

- Concept: Cross-Attention / Query-Key-Value attention
  - Why needed here: Used to generate global graph embedding conditioned on vehicle state.
  - Quick check question: Given query Q from vehicle state and keys K from node embeddings, how is the attention-weighted output computed?

- Concept: REINFORCE Policy Gradient
  - Why needed here: Training algorithm for the routing policy; loss uses log-probability weighted by returns.
  - Quick check question: What is the gradient of the policy objective J(θ) = E[G_t · log π(a_t|s_t; θ)]?

## Architecture Onboarding

- Component map:
  Input → GAT-Edge Encoder → Cross-Attention Global Embedding → Context Vector → Compatibility + Masking → Action Probabilities

- Critical path:
  1. State encoding (GAT-Edge) → node embeddings h
  2. Global context (Cross-Attention) → h_G
  3. Context concatenation → H_C
  4. Compatibility scoring → u_i
  5. Masking (visited nodes + infeasible returns) → policy π

- Design tradeoffs:
  - KNN adjacency (sparse) vs. full adjacency: Sparse reduces computation but may miss long-range dependencies
  - Sigmoid activation in GAT-Edge vs. ReLU/LeakyReLU: Paper uses sigmoid; may affect gradient flow
  - Failure penalty M = |C|: Heuristic; too low may encourage timeout violations

- Failure signatures:
  - Low customer service rate + high timeout violations → remaining time not properly integrated; check context vector construction
  - Performance degrades on larger instances → check KNN sparsity; may need more neighbors or deeper GAT
  - Training instability → check reward scaling; M may need tuning per instance size

- First 3 experiments:
  1. Replicate ablation: Train PG-GAT without edge features on EMA-50; verify ~3-4% drop vs. full model
  2. Vary KNN neighbors: Test k=5, 10, 20 on EMA-100; observe impact on solution time and service rate
  3. Horizon sensitivity: Sweep U values on Vienna-160; confirm PG-GAT-Edge gap over DRL-Transformer widens with larger U (as shown in Figure 3)

## Open Questions the Paper Calls Out

- Question: Can the proposed deep RL framework be effectively adapted for multi-vehicle fleet routing?
  - Basis in paper: [explicit] The Conclusion states, "In future work, research efforts could be made to extend the proposed deep RL-based framework for vehicle fleet routing."
  - Why unresolved: The current MDP and experimental validation are restricted to a single-vehicle scenario.
  - What evidence would resolve it: An extension of the MDP to handle multiple agents and inter-vehicle constraints, tested on multi-vehicle benchmarks.

- Question: How does the method perform on time-minimization or distance-minimization VRP variants?
  - Basis in paper: [explicit] The authors note in the "Effects of Different Finite Horizons" section that the method "can be extended to other variants... such as time-minimization VRP, distance-minimization VRP."
  - Why unresolved: The current objective function is strictly limited to maximizing the number of served customers.
  - What evidence would resolve it: A modification of the reward function to minimize time/distance, with comparative results against baselines for those variants.

## Limitations

- Training hyperparameters (learning rate, optimizer, batch size, number of episodes) are unspecified, making direct reproduction difficult
- KNN parameter K and edge feature normalization statistics are not provided
- Real-world network generation process for EMA and Vienna instances lacks detail
- Compatibility layer constant C and masking constant Z values are unspecified

## Confidence

- Mechanism 1 (edge features): High confidence - Clear ablation evidence showing 4.1% improvement; corroborated by EFormer paper
- Mechanism 2 (global context): Medium confidence - Strong ablation evidence (11.8% drop) but relies on interpretation of Cross-Attention's role
- Mechanism 3 (time horizon embedding): Medium confidence - Ablation shows 10.7% drop, but neighboring papers don't explicitly condition on remaining time

## Next Checks

1. Replicate core ablation: Train PG-GAT without edge features on EMA-50; verify ~3-4% customer service rate drop versus full model
2. KNN sensitivity test: Vary k=5, 10, 20 neighbors on EMA-100; measure impact on solution time and service rate to assess sparsity trade-offs
3. Horizon scaling experiment: Sweep U values on Vienna-160; confirm PG-GAT-Edge gap over DRL-Transformer widens with larger U as shown in Figure 3