---
ver: rpa2
title: Enabling Heterogeneous Adversarial Transferability via Feature Permutation
  Attacks
arxiv_id: '2503.20310'
source_url: https://arxiv.org/abs/2503.20310
tags:
- adversarial
- feature
- transferability
- attacks
- permutation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring adversarial
  attacks across heterogeneous neural architectures (CNNs, Vision Transformers, and
  MLPs), where transferability significantly degrades due to fundamental architectural
  differences. The authors propose Feature Permutation Attack (FPA), a zero-FLOP,
  parameter-free method that enhances transferability by strategically rearranging
  pixel values in selected feature maps within CNNs.
---

# Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks

## Quick Facts
- arXiv ID: 2503.20310
- Source URL: https://arxiv.org/abs/2503.20310
- Reference count: 40
- Authors: Tao Wu; Tie Luo
- One-line primary result: Feature Permutation Attack (FPA) achieves maximum absolute gains in attack success rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming existing black-box attacks.

## Executive Summary
This paper tackles the challenge of transferring adversarial attacks across heterogeneous neural architectures (CNNs, Vision Transformers, and MLPs), where traditional transferability methods fail due to fundamental architectural differences. The authors introduce Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method that enhances transferability by strategically rearranging pixel values in selected feature maps within CNNs. This operation simulates long-range dependencies, making CNNs behave more like ViTs and MLPs, thereby improving feature diversity and transferability. Extensive evaluations on 14 state-of-the-art architectures demonstrate that FPA outperforms existing black-box attacks and can be seamlessly integrated with other transfer-based attacks.

## Method Summary
The paper proposes Feature Permutation Attack (FPA), a novel method designed to enhance adversarial transferability across heterogeneous neural architectures. FPA operates by strategically rearranging pixel values in selected feature maps within CNNs, simulating long-range dependencies typically found in Vision Transformers and MLPs. This permutation operation is parameter-free and requires no additional computational overhead (zero-FLOP). The method aims to bridge the architectural gap between CNNs and other architectures, improving feature diversity and transferability. FPA can be integrated with existing transfer-based attacks to further boost their performance, and extensive evaluations show significant gains in attack success rates across 14 different architectures.

## Key Results
- FPA achieves maximum absolute gains in attack success rates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs.
- Outperforms existing black-box attacks on 14 state-of-the-art architectures.
- FPA is highly generalizable and can seamlessly integrate with other transfer-based attacks to further boost their performance.

## Why This Works (Mechanism)
FPA enhances transferability by rearranging pixel values in selected feature maps within CNNs, simulating long-range dependencies. This makes CNNs behave more like ViTs and MLPs, improving feature diversity and transferability. The permutation operation bridges the architectural gap between CNNs and other architectures, allowing adversarial examples generated on one architecture to transfer more effectively to others.

## Foundational Learning
- **Heterogeneous Neural Architectures**: Understanding the fundamental differences between CNNs, Vision Transformers, and MLPs is crucial for addressing transferability challenges. Quick check: Compare the receptive field and feature extraction mechanisms across architectures.
- **Adversarial Transferability**: The ability to transfer adversarial examples from one model to another is essential for black-box attacks. Quick check: Evaluate transferability rates between different architectures using standard attacks.
- **Feature Maps and Permutation**: Feature maps are the intermediate representations in neural networks, and permutation operations rearrange pixel values to simulate long-range dependencies. Quick check: Visualize feature maps before and after permutation to assess changes in spatial relationships.
- **Zero-FLOP Operations**: Operations that require no additional computational overhead are desirable for efficiency. Quick check: Measure the computational cost of FPA compared to gradient-based attacks.

## Architecture Onboarding
- **Component Map**: Input -> Feature Extraction -> Feature Permutation -> Adversarial Example Generation -> Transfer to Target Architecture
- **Critical Path**: The critical path involves feature extraction, permutation, and adversarial example generation, as these directly impact transferability.
- **Design Tradeoffs**: FPA trades off computational simplicity for improved transferability, making it efficient but potentially limited against adaptive defenses.
- **Failure Signatures**: Reduced effectiveness on architectures with unique inductive biases or when facing defenses that detect permutation operations.
- **First Experiments**: 1) Test FPA on a simple CNN-to-ViT transfer scenario, 2) Evaluate integration with existing transfer-based attacks, 3) Measure computational overhead compared to gradient-based methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Major uncertainties remain around the long-term robustness of FPA when facing adaptive defenses specifically designed to detect or mitigate feature permutation.
- The claim that FPA is "zero-FLOP" requires careful scrutiny, as even trivial pixel rearrangement operations involve computational overhead when applied to large feature maps.
- The generalization across diverse architectures appears strong in controlled settings, but real-world deployment may reveal edge cases where architectural idiosyncrasies reduce effectiveness.

## Confidence
- High confidence in empirical results showing improved transferability on tested architectures, given systematic evaluation across 14 models and clear quantitative gains.
- Medium confidence in the theoretical explanation of why feature permutation enhances transferability, as the mechanism relies on intuitive but not rigorously proven analogies between CNNs and ViTs.
- Low confidence in claims about FPA's robustness against future adaptive defenses, since these were not directly tested.

## Next Checks
1. Test FPA against state-of-the-art adaptive defenses that explicitly detect or counteract feature permutation.
2. Evaluate the method's performance on out-of-distribution data and in real-world adversarial scenarios beyond controlled benchmark datasets.
3. Quantify the actual computational overhead of FPA in large-scale applications to verify the "zero-FLOP" claim under realistic constraints.