---
ver: rpa2
title: 'CardiCat: a Variational Autoencoder for High-Cardinality Tabular Data'
arxiv_id: '2501.17324'
source_url: https://arxiv.org/abs/2501.17324
tags:
- data
- features
- categorical
- tabular
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CardiCat is a VAE-based framework for generating high-quality synthetic
  data from heterogeneous tabular datasets with high-cardinality categorical features.
  It introduces regularized dual encoder-decoder embedding layers that replace one-hot
  encoding, allowing embeddings to be jointly learned with the full network and conditioned
  on all covariates.
---

# CardiCat: a Variational Autoencoder for High-Cardinality Tabular Data

## Quick Facts
- arXiv ID: 2501.17324
- Source URL: https://arxiv.org/abs/2501.17324
- Reference count: 21
- Primary result: Outperforms VAE and tVAE on marginal and bi-variate reconstruction of high-cardinality categorical features while using fewer parameters

## Executive Summary
CardiCat is a VAE-based framework designed to generate high-quality synthetic data from heterogeneous tabular datasets with high-cardinality categorical features. It introduces regularized dual encoder-decoder embedding layers that replace one-hot encoding, allowing embeddings to be jointly learned with the full network and conditioned on all covariates. This reduces the number of trainable parameters and avoids the pitfalls of one-hot encoding high-cardinality features. CardiCat was evaluated on seven real and one simulated dataset against VAE, tVAE, and conditional GAN models, showing superior performance on categorical reconstruction while maintaining competitive performance on numerical features.

## Method Summary
CardiCat replaces one-hot encoding for high-cardinality categorical features with learned embeddings. It uses a dual encoder-decoder architecture where the same embedding layer is shared between the encoder (mapping categories to dense vectors) and the decoder (reconstructing dense vectors). The reconstruction loss for categorical features is computed in the embedding space via L2 distance rather than in the one-hot space. A variance-based regularization term prevents embedding collapse by penalizing deviation of embedding coordinate-wise variance from initialization. The model maintains competitive performance on numerical features while dramatically reducing parameter count for high-cardinality categorical features.

## Key Results
- Achieved higher total variation distance scores on categorical marginal distributions (e.g., up to 0.93 vs. 0.78 for VAE on Medical dataset)
- Required fewer trainable parameters compared to VAE with one-hot encoding
- Outperformed competing VAEs in marginal and bi-variate reconstruction of categorical features
- Conditional variant performed comparably to tGAN without needing training-by-sampling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing reconstruction loss in the embedding space rather than one-hot space improves categorical feature recovery.
- Mechanism: The decoder outputs a k_j-dimensional numerical vector (embedding) that is directly compared to the true feature's embedding vector via L2 loss, bypassing the need to predict among c_j categories simultaneously and allowing gradient signals to be more informative per parameter.
- Core assumption: The learned embedding geometry meaningfully encodes relationships among categorical values that aid reconstruction; the paper does not independently validate embedding quality beyond downstream metrics.
- Evidence anchors: [abstract] "enables us to use embeddings that depend also on the other covariates, leading to a compact and homogenized parameterization"; [section 3.3] "The reconstruction loss of the categorical features is computed in the embedding space... our loss deviates from the traditional ELBO formulation"
- Break condition: If embeddings collapse to near-identical vectors (losing discriminative geometry), the L2 loss in embedding space will not enforce accurate category recovery. The regularization term is intended to mitigate this.

### Mechanism 2
- Claim: Sharing embedding weights between encoder and decoder (dual embeddings) enables efficient joint learning with fewer parameters.
- Mechanism: A single embedding matrix e_j is used both to map input categories to dense vectors in the encoder and as the target space for decoder reconstruction; gradients from reconstruction backpropagate through both paths, aligning the embedding geometry to the joint distribution.
- Core assumption: Encoder-side embeddings provide useful representations when the decoder is trained to reconstruct them; the paper does not ablate against separate encoder/decoder embeddings.
- Evidence anchors: [abstract] "regularized dual encoder-decoder embedding layers, which are jointly learned"; [section 3.2] "the embeddings appear both as trainable layers in the encoder, and in the output of the decoder where they actively participate in calculating the loss"
- Break condition: If the decoder cannot reliably reconstruct embeddings from the latent space (e.g., latent bottleneck too small), the shared embedding will not stabilize and may degrade or collapse.

### Mechanism 3
- Claim: Variance-based regularization prevents embedding collapse and maintains representational capacity.
- Mechanism: An explicit regularization term penalizes deviation of per-coordinate embedding variance from its initialization value, preventing all category embeddings from converging to a small region while still allowing the overall geometry to adapt.
- Core assumption: Preserving coordinate-wise variance is a sufficient proxy for preventing harmful collapse without over-constraining useful adaptation; this is not formally proven.
- Evidence anchors: [section 3.3] "embedding regularization penalizes changes to the total coordinate-wise variance of the embedding, compared to the initialization"; [section 3.1] Embedding dimension k_j is typically chosen such that k_j << c_j, making collapse a concrete risk without regularization
- Break condition: If the regularization weight λ_2 is too high, embeddings may stay near random initialization and fail to learn meaningful structure; if too low, collapse may still occur.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) fundamentals (ELBO, encoder/decoder roles, latent space, KL divergence)
  - Why needed here: CardiCat modifies the standard VAE loss and architecture; understanding the baseline is prerequisite to recognizing what changes and why.
  - Quick check question: Can you explain why the ELBO contains both a reconstruction term and a KL divergence term, and what role each plays in training?

- Concept: Embeddings for categorical variables
  - Why needed here: The core innovation is replacing one-hot with learned embeddings in a VAE context; you must understand how embeddings map discrete categories to dense vectors.
  - Quick check question: For a categorical feature with 1,000 unique values, what is the parameter count difference between one-hot encoding into a 1,000-dim vector versus a 16-dim learned embedding?

- Concept: High-cardinality and imbalanced categorical features in tabular data
  - Why needed here: The paper explicitly targets datasets where one-hot fails (parameter explosion, minority category neglect); you need to recognize when these conditions apply.
  - Quick check question: On a dataset with a categorical feature having 5,000 categories where the top 10 account for 90% of occurrences, what specific failure modes would you expect from a standard VAE using one-hot encoding?

## Architecture Onboarding

- Component map: Numerical features (standardized) + Binary features (one-hot) + Categorical features (via shared embedding layer E) -> Encoder FC layers -> Latent space (μ, σ) -> Sample z -> Decoder FC layers -> Numerical features (tanh) + Binary features (softmax) + Categorical features (embedding vector in embedding space)

- Critical path:
  1. Identify high-cardinality categorical features (c_j ≥ 3) in your dataset.
  2. Initialize embedding matrices e_j with appropriate dimensions (k_j << c_j).
  3. Forward pass: encode categories to embeddings, concatenate with other features, pass through encoder to latent space.
  4. Sample from latent distribution (reparameterization trick), decode to reconstruct embeddings and other features.
  5. Compute loss with embedding-space reconstruction for categoricals and variance regularization.
  6. Backpropagate to update network weights and shared embeddings jointly.

- Design tradeoffs:
  - Embedding dimension k_j: Larger values increase representational capacity but reduce parameter savings; the paper does not provide a systematic guideline.
  - Regularization weight λ_2: Must be tuned to prevent collapse without over-constraining; no ablation values are reported in the paper.
  - Latent dimension a: Smaller values may bottleneck information, especially for many categorical features; 15 was used across all experiments.
  - Network depth/width: The paper uses a simple 3-layer 128-128-128 architecture; gains on more complex architectures are not verified.

- Failure signatures:
  - Embedding collapse: All or most category embeddings converge to similar vectors; check by computing pairwise distances in e_j.
  - Dominant category bias: Generated samples over-represent frequent categories; monitor marginal TVD per category.
  - Numerical feature degradation: Over-regularization or latent bottleneck harms numerical reconstruction; compare KS statistics.
  - Training instability: Loss oscillates or diverges; check λ_1 and λ_2 balance and learning rate.

- First 3 experiments:
  1. Reproduce the Bank dataset benchmark with CardiCat, vanilla VAE (one-hot), and tVAE from the paper; compare categorical marginal TVD and numerical KS to verify your implementation matches reported scores (CardiCat categorical marginal ~0.86 vs VAE ~0.71).
  2. Ablate the embedding variance regularization by setting λ_2 = 0; train on a high-cardinality dataset (e.g., Medical with 1,146 total cardinality) and measure whether embedding collapse occurs (average pairwise cosine similarity of embeddings) and its impact on categorical TVD.
  3. Vary embedding dimension k_j for a single high-cardinality feature (e.g., try k_j ∈ {4, 8, 16, 32}) on the Credit dataset; plot parameter count vs categorical TVD to quantify the tradeoff between parameter efficiency and reconstruction quality.

## Open Questions the Paper Calls Out

- Question: What is the efficacy of CardiCat-generated synthetic data for downstream supervised learning tasks compared to real data?
  - Basis in paper: [explicit] The authors state in the Discussion section that they "leave downstream effects on supervised learning... for later work."
  - Why unresolved: The paper evaluates statistical fidelity (marginal and bivariate distribution reconstruction) but does not measure the utility of the synthetic data for training predictive models (e.g., the "Train on Synthetic, Test on Real" paradigm).
  - What evidence would resolve it: Benchmarks showing the predictive accuracy of classifiers trained on CardiCat synthetic data versus those trained on real data.

- Question: Can the reconstruction quality of numerical features be improved by combining CardiCat's embedding layers with mode-specific normalization techniques?
  - Basis in paper: [explicit] The Discussion suggests that "combining a refined model such as the mixture modeling of tVAE should further improve the recovery of numerical features."
  - Why unresolved: CardiCat currently employs a basic Gaussian likelihood for numerical features, whereas the authors hypothesize that integrating a variational Gaussian mixture model (VGM) could better handle multi-modal numerical distributions.
  - What evidence would resolve it: Ablation studies comparing numerical Kolmogorov-Smirnov (KS) scores between the standard CardiCat and a version integrated with tVAE's mixture modeling.

- Question: Do the parameter efficiency and reconstruction advantages of CardiCat persist when applied to more sophisticated or deeper network architectures?
  - Basis in paper: [explicit] The authors note that the "network architecture we use is basic" and admit there is a "chance that gains observed here would not carry over to much more sophisticated architectures."
  - Why unresolved: The experiments deliberately utilized a simple three-layer fully connected network to isolate the benefits of the embedding mechanism, leaving the interaction with complex architectures (e.g., attention layers or deep residual networks) untested.
  - What evidence would resolve it: Comparative benchmarks showing Total Variation Distance (TVD) and parameter counts for CardiCat embeddings implemented within state-of-the-art deep generative architectures.

## Limitations
- Exact embedding dimension k_j and regularization weight λ_2 are underspecified, making exact replication difficult
- Lack of ablation studies to confirm shared encoder-decoder embeddings are strictly necessary for performance gains
- Reliance on aggregate distributional metrics without per-category fidelity analysis

## Confidence
- Parameterization efficiency claim: High
- Reconstruction improvement claims: Medium

## Next Checks
1. Perform a per-category TVD breakdown on the Medical dataset to verify rare category recovery, not just marginal improvement.
2. Ablate λ₂ across orders of magnitude to determine the stability/robustness of embedding geometry.
3. Test whether separate encoder and decoder embeddings (non-dual) achieve comparable performance, isolating the benefit of shared embeddings.