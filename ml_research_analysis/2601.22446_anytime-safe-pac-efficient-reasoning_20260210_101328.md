---
ver: rpa2
title: Anytime Safe PAC Efficient Reasoning
arxiv_id: '2601.22446'
source_url: https://arxiv.org/abs/2601.22446
tags:
- reasoning
- b-pac
- efficient
- efficiency
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: B-PAC reasoning addresses the problem of safe and efficient online
  reasoning with Large Reasoning Models (LRMs) under partial feedback and non-stationary
  data. The core idea is to formulate threshold selection as a betting game, using
  inverse propensity scoring estimators and supermartingales to dynamically adjust
  routing thresholds between thinking and non-thinking models.
---

# Anytime Safe PAC Efficient Reasoning

## Quick Facts
- arXiv ID: 2601.22446
- Source URL: https://arxiv.org/abs/2601.22446
- Reference count: 40
- Primary result: Reduces thinking model usage by up to 81.01% while maintaining performance loss below user-specified thresholds

## Executive Summary
B-PAC reasoning addresses the challenge of safe and efficient online reasoning with Large Reasoning Models (LRMs) under partial feedback and non-stationary data. The method formulates threshold selection as a betting game, using inverse propensity scoring estimators and supermartingales to dynamically adjust routing thresholds between thinking and non-thinking models. Theoretical guarantees include anytime-valid performance loss control and logarithmic regret for the adaptive betting strategy. Empirically, B-PAC achieves significant efficiency gains while maintaining safety constraints across diverse reasoning benchmarks.

## Method Summary
B-PAC implements a binary routing system between a fast, non-thinking model and a slower, thinking model. The method uses an uncertainty scorer to produce confidence estimates, which are compared against a dynamically updated threshold. When uncertainty exceeds the threshold, the system routes to the thinking model; otherwise, it uses the non-thinking model. The key innovation is the use of inverse propensity scoring to handle partial feedback and supermartingales to provide anytime-valid safety guarantees. The routing decision is governed by a two-stage exploration strategy with adaptive betting fractions that optimize log-wealth while maintaining safety.

## Key Results
- Reduces thinking model usage by up to 81.01% compared to baseline
- Maintains empirical risk below user-specified thresholds with high probability
- Achieves logarithmic regret in threshold selection through adaptive betting strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IPS estimation corrects selection bias from partial feedback, enabling unbiased risk estimation when losses are only observed for thinking-model calls.
- Mechanism: The estimator $Z_t(u) = (1-\rho_{min}) \cdot l_t \cdot \xi_t/\pi_t \cdot \mathbb{I}\{U_t < u\}$ weights observed losses by inverse selection probability, correcting for the fact that $l_t$ is only visible when $\xi_t=1$.
- Core assumption: The propensity $\pi_t$ remains bounded away from zero (enforced by minimum exploration probability $\rho_t$).
- Evidence anchors: [abstract] "utilize inverse propensity scoring estimators to construct test supermartingales"; [Section 3.2, Eq. 3] Formal IPS estimator definition with correction factor; [corpus] Related work on PAC reasoning (arxiv:2510.09133) addresses offline settings but lacks partial feedback handling.
- Break condition: If $\rho_t \to 0$ or exploration ceases, IPS variance explodes and threshold updates stagnate.

### Mechanism 2
- Claim: Non-negative supermartingales provide anytime-valid safety guarantees without requiring fixed sample sizes or i.i.d. calibration data.
- Mechanism: Under null hypothesis $H_{0,u}: R(u) > \epsilon$, the wealth process $K_t(u) = K_{t-1}(u)(1 + \lambda_t(u)D_t(u))$ is a supermartingale. Ville's inequality then bounds the probability that wealth ever exceeds $1/\alpha$ under the null.
- Core assumption: Bounded loss function $l(\cdot,\cdot) \in [0,1]$ ensuring $D_t(u)$ has bounded range.
- Evidence anchors: [abstract] "theoretical guarantees include anytime-valid performance loss control"; [Section 4.1, Lemma 4.1] Supermartingale property proof under $H_{0,u}$; [corpus] Standard martingale theory (not explicitly in corpus neighbors).
- Break condition: If loss is unbounded or $\lambda_t$ violates $1 + \lambda_t D_t(u) \geq 0$, wealth can become negative, breaking the guarantee.

### Mechanism 3
- Claim: The adaptive betting strategy achieves logarithmic regret, enabling rapid convergence to optimal safe thresholds.
- Mechanism: The strategy optimizes a quadratic proxy for log-wealth using Follow-The-Regularized-Leader, yielding $\lambda_t(u) = \text{proj}_{\mathcal{K}_t}\left(\frac{\sum D_i(u)}{\sum D_i^2(u) + \beta}\right)$ with regret $O(\log T)$.
- Core assumption: Threshold candidates $u \in \mathcal{U}$ are fixed and finite, enabling fixed-sequence testing.
- Evidence anchors: [Section 3.3, Eq. 8] Closed-form adaptive $\lambda_t$ formula; [Section 4.2, Theorem 4.3] Logarithmic regret bound with explicit constants; [corpus] Online convex optimization foundations (Hazan 2016 cited but not in neighbor corpus).
- Break condition: If threshold space $\mathcal{U}$ is too coarse, the maximal safe threshold may be far below the true optimum, reducing efficiency.

## Foundational Learning

- Concept: **Martingales and Ville's Inequality**
  - Why needed here: The entire safety guarantee rests on treating threshold validation as a betting game where accumulated wealth is a nonnegative supermartingale under unsafe thresholds.
  - Quick check question: If $K_t$ is a nonnegative supermartingale with $K_0=1$, what does $\mathbb{P}(\exists t: K_t \geq 1/\alpha) \leq \alpha$ imply for sequential testing?

- Concept: **Inverse Propensity Scoring (IPS)**
  - Why needed here: Without IPS, naive risk estimators under partial feedback are biased toward zero (unobserved losses treated as zero), causing unsafe threshold selection.
  - Quick check question: Why does dividing by $\pi_t$ rather than ignoring unobserved samples yield an unbiased risk estimator?

- Concept: **Fixed-Sequence Testing**
  - Why needed here: Enables threshold search over ordered candidates $\mathcal{U}$ without multiple-testing penalty, exploiting monotonicity of risk with threshold.
  - Quick check question: How does the monotonicity $R(u) \leq R(u')$ for $u \leq u'$ enable more efficient testing than union bounds?

## Architecture Onboarding

- Component map: Query arrival -> Uncertainty scorer -> Current threshold comparison -> Propensity computation -> Model selection -> (if thinking called) Loss observation -> IPS update -> Wealth update -> Threshold update
- Critical path: Query arrival → Uncertainty score → Current threshold comparison → Propensity computation → Model selection → (if thinking called) Loss observation → IPS update → Wealth update → Threshold update. Latency of threshold update is $O(|\mathcal{U}|)$ basic arithmetic, negligible vs. model inference.
- Design tradeoffs:
  - Higher $\rho_{warm}$ → faster convergence but higher exploration cost
  - Larger $\mathcal{U}$ → finer threshold granularity but more wealth processes to maintain
  - Smaller $\epsilon$ → stronger safety guarantee but more conservative routing
  - Mixture martingales (Section 5) handle non-stationarity but are less efficient than fixed-sequence testing
- Failure signatures:
  - **Stuck at $\hat{u}_t = 0$**: Wealth never exceeds $1/\alpha$ for any threshold; check if uncertainty scores correlate with disagreement
  - **Oscillating thresholds**: May indicate non-stationary data without mixture martingale adaptation
  - **ECP plateaus at ~$\rho_{deploy}$**: Expected behavior; efficiency bounded by mandatory exploration
  - **Empirical risk exceeds $\epsilon$**: Catastrophic; indicates implementation error or unbounded loss
- First 3 experiments:
  1. **Sanity check on synthetic data**: Generate queries with known uncertainty-error correlation; verify $\hat{u}_t$ converges to oracle-safe threshold and ER stays below $\epsilon$.
  2. **Ablation on exploration strategy**: Compare two-stage $\rho_t$ (Eq. 9) vs. fixed $\rho_t \in \{0.05, 0.2, 0.7\}$; measure convergence speed (time to stable $\hat{u}_t$) and final ECP.
  3. **Non-stationarity stress test**: Concatenate datasets of varying difficulty (e.g., BBH → MMLU-Pro); verify B-PAC with mixture martingales (Eq. 12) maintains ER < $\epsilon$ while offline PAC baseline fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the B-PAC framework be extended to route queries among multiple models rather than just a thinking/non-thinking pair?
- Basis in paper: [explicit] The authors state in the Limitations section that "extending it to multiple models remains unsolved."
- Why unresolved: The current theoretical construction relies on a binary switching mechanism and a single threshold determination process.
- What evidence would resolve it: A generalization of the wealth process and betting strategy to a multi-armed or cascaded model setting.

### Open Question 2
- Question: How can the system adaptively select the optimal uncertainty score when multiple scoring methods are available?
- Basis in paper: [explicit] The authors note that "when multiple scores are available, how to adaptively select the most reliable score remains unknown."
- Why unresolved: While safety is guaranteed regardless of score quality, efficiency is highly dependent on it, and current implementations use a fixed score type.
- What evidence would resolve it: A meta-algorithm capable of dynamically weighting or selecting uncertainty scores online while preserving the supermartingale property.

### Open Question 3
- Question: Can the threshold selection be conditioned on specific input features to further enhance reasoning efficiency?
- Basis in paper: [explicit] The authors suggest that "designing the conditional case... of B-PAC reasoning may further enhance the efficiency."
- Why unresolved: Current thresholds are global or time-dependent but do not utilize the specific covariates of the input query.
- What evidence would resolve it: A conditional betting scheme that maintains anytime-valid safety guarantees while allowing thresholds to vary by input type.

### Open Question 4
- Question: Is there a theoretically optimal, adaptive strategy for the exploration probability $\rho_t$ that outperforms the fixed two-stage schedule?
- Basis in paper: [inferred] The paper acknowledges that $\rho_t$ impacts efficiency and proposes a heuristic "two-stage exploration strategy," implying the optimal theoretical solution is not yet derived.
- Why unresolved: A fixed or simple staged $\rho_t$ balances warm-up and deployment but may not minimize regret for all data distributions.
- What evidence would resolve it: An adaptive $\rho_t$ policy with theoretical guarantees bounding the regret relative to the optimal fixed exploration rate.

## Limitations

- Reliance on IPS estimation introduces potential variance explosion if exploration rates drop too low, particularly in non-stationary settings where the optimal threshold shifts.
- The fixed-sequence testing approach assumes monotonicity of risk with threshold, which may not hold if the uncertainty scorer's calibration drifts.
- Empirical validation is limited to four datasets and two model sizes (4B parameters), leaving generalization to larger models and more diverse reasoning tasks uncertain.

## Confidence

- **High confidence**: The martingale-based safety guarantees under the stated assumptions, the IPS estimator's unbiasedness under partial feedback, and the logarithmic regret bound for the adaptive betting strategy.
- **Medium confidence**: The practical efficiency gains (ECP and TP reductions) reported in experiments, as these depend on specific implementation details of uncertainty scoring and the correlation structure of the datasets used.
- **Low confidence**: The robustness to severe non-stationarity and the performance when scaling to much larger models (e.g., 70B parameters) without recalibration of exploration parameters.

## Next Checks

1. **Non-stationary stress test**: Implement B-PAC with mixture martingales (Section 5) on a dataset with abrupt difficulty shifts (e.g., EasyReasoning → MATH). Verify that ER remains below ε while offline PAC fails, and measure the cost of mixture martingale adaptation (increased ECP vs. fixed-sequence).

2. **Model scaling sensitivity**: Run B-PAC on the same datasets using 70B-parameter models. Track whether the optimal threshold (û_t at convergence) scales proportionally or requires new exploration parameters, and whether efficiency gains hold.

3. **Uncertainty scorer ablation**: Replace the verbalized uncertainty scorer with a purely logits-based scorer (Appendix B.2) on the same datasets. Compare ECP, TP, and ER to isolate the contribution of verbalization to uncertainty estimation quality.