---
ver: rpa2
title: 'DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed
  Data Center Workloads'
arxiv_id: '2511.00117'
source_url: https://arxiv.org/abs/2511.00117
tags:
- data
- dataset
- each
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DCcluster-Opt is an open-source benchmark for sustainable, geo-distributed
  data center scheduling that integrates real-world datasets (AI workloads, carbon
  intensity, electricity prices, weather, transmission costs/delays) with physics-informed
  DC energy models. It provides a high-fidelity simulation environment enabling multi-objective
  optimization of cost, carbon emissions, water usage, and SLA compliance.
---

# DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads

## Quick Facts
- arXiv ID: 2511.00117
- Source URL: https://arxiv.org/abs/2511.00117
- Reference count: 40
- DCcluster-Opt is an open-source benchmark for sustainable, geo-distributed data center scheduling that integrates real-world datasets (AI workloads, carbon intensity, electricity prices, weather, transmission costs/delays) with physics-informed DC energy models. It provides a high-fidelity simulation environment enabling multi-objective optimization of cost, carbon emissions, water usage, and SLA compliance.

## Executive Summary
DCcluster-Opt addresses the critical need for benchmarking sustainable scheduling in geo-distributed data centers by providing an open-source, high-fidelity simulation environment. The benchmark integrates real-world datasets with physics-informed energy models to enable rigorous multi-objective optimization research. It supports various control strategies including rule-based, reinforcement learning, and novel agentic AI approaches, with demonstrated performance improvements over traditional methods.

## Method Summary
The benchmark provides a Gymnasium API for sustainable geo-distributed data center scheduling, integrating physics-informed energy models with real-world time-varying data. It simulates AI workloads using Alibaba Cluster Trace, environmental factors including carbon intensity and electricity prices, and detailed thermal dynamics. The system supports configurable reward functions for multi-objective optimization and includes baseline controllers (rule-based and RL) along with an interpretable agentic AI controller using LLM-based agents.

## Key Results
- RL agents outperform rule-based controllers in balancing cost, carbon emissions, water usage, and SLA compliance
- Advanced local control (RL-controlled HVAC and HRU) provides additional performance benefits
- Agentic AI controller using LLM-based agents enables transparent decision-making and zero-shot generalization to unseen cluster topologies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Physics-informed simulation fidelity allows reinforcement learning agents to approximate real-world sustainability trade-offs.
- Mechanism: By coupling real-world time-varying data (carbon intensity, weather, prices) with detailed thermal and power models (IT load, HVAC, cooling towers), the environment generates state transitions that capture the physical inertia and non-linear energy costs of data center operations.
- Core assumption: The thermal coefficients and external data distributions provide a sufficiently accurate proxy for production data center dynamics.
- Evidence anchors: Abstract mentions "physics-informed DC energy models"; Section 3.3 details HVAC system modeling; related LC-Opt work validates this approach.
- Break condition: If simulation's thermal inertia or cooling efficiency curves diverge significantly from real hardware physics, agents may suffer from "sim-to-real" gap.

### Mechanism 2
- Claim: Modular reward decomposition enables navigation of multi-objective Pareto frontiers that single-objective heuristics cannot capture.
- Mechanism: The system employs a configurable composite reward function combining cost, carbon, water, and SLA penalties, allowing gradient-based agents to learn policies that balance trade-offs.
- Core assumption: Reward weights accurately reflect operator priorities and agents can learn correlations between high-dimensional observations and these rewards.
- Evidence anchors: Abstract mentions "modular reward system"; Section 6.3 shows RL agent finding superior balance vs. "Lowest Carbon" RBC; related work discusses limitations of simple heuristics.
- Break condition: If reward components are mis-scaled or one objective is too weak, agents may engage in "reward hacking."

### Mechanism 3
- Claim: Policy distillation into Large Language Models facilitates zero-shot generalization to unseen cluster topologies via semantic reasoning.
- Mechanism: An expert RL policy generates state-action trajectories, which are textualized into structured prompts. A base LLM is fine-tuned on this semantic mapping, allowing application to new configurations without retraining.
- Core assumption: Semantic state representation captures essential features and LLM possesses sufficient reasoning capacity.
- Evidence anchors: Section 7.2 describes methodology; Section 7.3 reports generalization to larger clusters; corpus signals suggest trend toward interpretable agents.
- Break condition: If environment state depends on non-semantic nuances not captured in text summary, LLM may make suboptimal decisions.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) & Partial Observability**
  - Why needed here: The environment is formalized as an MDP, but the observation space varies in length. Understanding how RL agents handle variable-length inputs is crucial for debugging policy failures.
  - Quick check question: If the number of pending tasks exceeds the `max_tasks` hyperparameter in the SAC configuration, how does the agent's observation space change, and what is the potential impact on the policy?

- **Concept: Thermal Inertia & Time-Constant Systems**
  - Why needed here: The benchmark models thermal dynamics. Decisions to shift load do not instantly change temperature; there is a lag. Understanding this delay is key to tuning the HVAC control agent.
  - Quick check question: Why does the paper choose a 15-minute timestep, and how does this relate to the "thermal inertia" of the simulated CRAC units and chillers?

- **Concept: Multi-Objective Optimization (Scalarization)**
  - Why needed here: The reward function is a weighted sum of conflicting objectives. Understanding scalarization is necessary to interpret why an agent might choose a high-cost action and how to tune weights.
  - Quick check question: In the provided `reward_config.yaml`, if the weight for `sla_penalty` is increased from 0.1 to 0.5, how should the agent's deferral behavior theoretically change?

## Architecture Onboarding

- **Component map**: TaskSchedulingEnv -> DatacenterClusterManager -> SustainDC (per-DC physics engine) -> Data Managers (ElectricityPrice_Manager, CI_Manager, Weather_Manager) -> Agents (SAC, RBC, LLM)

- **Critical path**:
  1. Init: `env.reset()` loads workload and environmental data for time t=0
  2. Observe: Agent receives s_t (list of task features + per-DC states)
  3. Decide: Agent outputs a_t (list of DC indices or 0 for defer)
  4. Simulate: Tasks assigned to remote DCs enter "in-transit" state; local DCs update queues, calculate IT power, update thermal states, and compute HVAC power
  5. Learn/Log: Reward r_t is computed; if training, gradients update the policy

- **Design tradeoffs**:
  - Fidelity vs. Speed: Detailed HVAC model improves realism but increases compute cost per step compared to fixed PUE model
  - RL vs. Agentic: RL offers potentially higher optimality via continuous optimization but requires heavy training; Agentic AI offers interpretability and zero-shot generalization but relies on quality of distilled "teacher" policy
  - Observation Aggregation: RLlib agents use "aggregated global state" for fixed-size inputs, losing per-task granularity compared to custom SAC implementation

- **Failure signatures**:
  - Starvation Loop: Agent sets SLA penalty too low, learning to defer tasks indefinitely, causing "Tasks Deferred" counter to explode
  - Thermal Runaway: HVAC control agent fails to lower setpoints during load spikes, causing simulated inlet temperatures to exceed safety limits
  - Observation Collapse: Variable-length observation handling fails, causing critic to assign value to "padded" ghost tasks

- **First 3 experiments**:
  1. Baseline Reproduction: Run `evaluate_DCcluster-Opt_agent.ipynb` to reproduce Table 3, comparing "Lowest Carbon" RBC vs. SAC (Geo+Time) to verify trade-off between CO2 emissions and SLA violations
  2. Reward Ablation: Modify `reward_config.yaml` to zero out `carbon_emissions` weight; retrain SAC agent and observe shift toward pure cost minimization
  3. Topology Generalization: Train LLM controller on 3-DC setup and evaluate it directly on 5-DC configuration without retraining; compare performance against Random or Round-Robin baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does integration of dynamic Locational Marginal Pricing (LMP) models alter economic optimization strategies and peak-shaving behaviors of scheduling agents?
- Basis in paper: [explicit] Conclusion states future work can extend to capture LMP, noting modular architecture is designed to support this
- Why unresolved: Current benchmark relies on historical LMP or aggregated ISO data rather than reactive LMP model that responds to data center's own demand
- What evidence would resolve it: Experiments comparing agent performance and behavior in fixed-price environment vs. reactive LMP environment where large loads influence spot price

### Open Question 2
- Question: To what extent do scheduling policies trained on Alibaba GPU traces transfer effectively to emerging LLM workloads with different GPU memory intensities and inference latency constraints?
- Basis in paper: [explicit] Discussion highlights trace-agnostic design to support modern LLM traces as they become available
- Why unresolved: Current workload is derived from 2020 AI jobs; feasibility of applying optimized controllers to LLM serving/training without retraining is unknown
- What evidence would resolve it: Evaluation of pre-trained baseline agents on new synthetic or real-world LLM traces to measure performance degradation and resource utilization efficiency

### Open Question 3
- Question: What are specific cost and carbon trade-offs introduced by integrating auxiliary battery storage systems for local energy arbitrage within geo-distributed scheduling framework?
- Basis in paper: [explicit] Appendix J poses research question about coordinated battery control impact on cost, carbon, and grid stability
- Why unresolved: Battery model code exists in repository but experimental results don't include active battery control, leaving interaction between battery discharge strategies and global task scheduling unquantified
- What evidence would resolve it: Comparative study of system-wide emissions and operational costs in scenarios with and without active RL-based battery management

## Limitations
- Physics-informed simulation lacks explicit validation against production data center operations, introducing uncertainty about sim-to-real gap
- Multi-objective optimization depends heavily on reward weight specification, with risk of reward hacking if components are mis-scaled
- LLM-based controller's semantic representation may miss critical environmental nuances that influence optimal scheduling decisions

## Confidence

- **High Confidence**: Benchmark's implementation quality and integration of real-world datasets (carbon intensity, electricity prices, weather) is well-documented and reproducible
- **Medium Confidence**: RL agents outperform rule-based controllers in balancing multiple objectives, but results depend on specific hyperparameter choices and reward weight configurations
- **Medium Confidence**: LLM-based agentic controller shows competitive performance on out-of-distribution clusters, but generalizability is demonstrated on limited set of configurations

## Next Checks

1. **Sim-to-Real Gap Validation**: Deploy RL policies trained in DCcluster-Opt on a real or high-fidelity test data center environment to measure performance degradation and identify specific simulation inaccuracies that need correction.

2. **Reward Sensitivity Analysis**: Systematically vary the reward weights across multiple orders of magnitude and retrain agents to map the full Pareto frontier, ensuring the claimed multi-objective optimization capability is robust to operator preference changes.

3. **Stress Test LLM Generalization**: Evaluate the agentic AI controller on extreme cases including heterogeneous data center configurations, sudden environmental anomalies, and workload distributions that differ significantly from training data to test limits of semantic generalization.