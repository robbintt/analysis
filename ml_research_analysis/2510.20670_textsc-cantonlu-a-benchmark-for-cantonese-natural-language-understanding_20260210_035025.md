---
ver: rpa2
title: '\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding'
arxiv_id: '2510.20670'
source_url: https://arxiv.org/abs/2510.20670
tags:
- cantonese
- language
- mandarin
- linguistics
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CantoNLU introduces a Cantonese natural language understanding
  benchmark with seven tasks including WSD, LAJ, LD, NLI, SA, POS, and DEPS. The benchmark
  features novel datasets for WSD and LAJ, along with adapted datasets for other tasks.
---

# \textsc{CantoNLU}: A benchmark for Cantonese natural language understanding

## Quick Facts
- arXiv ID: 2510.20670
- Source URL: https://arxiv.org/abs/2510.20670
- Reference count: 0
- Primary result: Cantonese-adapted models outperform Mandarin-only and monolingual Cantonese models across most NLU tasks

## Executive Summary
CantoNLU introduces a comprehensive benchmark for Cantonese natural language understanding, evaluating models across seven tasks including Word Sense Disambiguation, Linguistic Acceptability Judgment, Language Detection, Natural Language Inference, Sentiment Analysis, Part-of-Speech tagging, and Dependency Parsing. The benchmark features novel datasets for WSD and LAJ, along with adapted versions of existing datasets. Three model types were evaluated: Mandarin-only, Cantonese-adapted (continual pre-training), and monolingual Cantonese. Results show Cantonese-adapted models perform best overall, while monolingual models excel in syntactic tasks. The work highlights the need for more high-quality Cantonese corpora to improve language representation.

## Method Summary
The benchmark evaluates three model types on seven Cantonese NLU tasks. Pre-training data combines Cantonese Wikipedia (40M characters) with the cantonese-sentences corpus (660M characters), totaling 700M characters. The monolingual Cantonese model uses a custom SentencePiece BPE tokenizer (32k vocab) trained from scratch, while the adapted model uses bert-base-chinese with continued MLM pre-training on Cantonese text. The Mandarin baseline uses bert-base-chinese fine-tuned directly. Five tasks use task-specific fine-tuning with provided hyperparameters, while WSD and LAJ use zero-shot surprisal/cosine similarity methods.

## Key Results
- Cantonese-adapted models achieve highest average score (69.4) across all tasks
- Monolingual Cantonese models excel in syntactic tasks (POS: 78.2 F1, DEPS: 78.2 UAS/LAS)
- Mandarin models remain competitive on semantic tasks (NLI: 93.2 accuracy, LAJ: 91.7 accuracy)
- All models struggle with dependency parsing due to small dataset size (DEPS UAS/LAS < 32.4%)

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Transfer via Continued Pre-training
Continual pre-training a Mandarin model on Cantonese text yields the best overall performance by leveraging shared Sinitic orthographic patterns while refining lexical and semantic representations. Mandarin pre-training provides strong initialization, and Cantonese exposure preserves syntactic priors while improving language-specific understanding.

### Mechanism 2: Task-type Determines Optimal Model Selection
Syntactic tasks (POS, DEPS) favor monolingual Cantonese models due to precise language-specific grammatical knowledge, while semantic tasks benefit from richer representations inherited from large-scale Mandarin pre-training. The monolingual tokenizer better captures Cantonese-specific particles and word order.

### Mechanism 3: Zero-shot Mandarin Transfer via Shared Orthography
Mandarin models achieve competitive performance on NLI and LAJ tasks through shared Traditional Chinese characters and orthographic conventions, demonstrating effective written Cantonese understanding can emerge from Mandarin training without explicit Cantonese exposure.

## Foundational Learning

- **Cross-lingual Transfer Learning (Model Transfer)**: Essential for understanding transfer strategies from Mandarin to Cantonese; check: Can you explain why transferring from Mandarin to Cantonese differs from transferring from English to Cantonese?

- **Tokenization and Subword Segmentation**: Critical for interpreting monolingual model performance attributed to custom tokenizer; check: How does character-level vs. subword-level tokenization affect representation learning for under-resourced languages?

- **Syntactic vs. Semantic NLU Tasks**: Important for understanding why different models excel on different task types; check: Why might a model excel at sentiment analysis but struggle with dependency parsing?

## Architecture Onboarding

- **Component map**: Cantonese Wikipedia + cantonese-sentences corpus → Monolingual: BERT-base + custom SentencePiece tokenizer → Adapted: bert-base-chinese + continued pre-training → Mandarin: bert-base-chinese fine-tuning

- **Critical path**: 1) Pre-training data preparation and cleaning, 2) Tokenizer training (monolingual only), 3) Continued pre-training (adapted only), 4) Fine-tuning with task-specific hyperparameters, 5) Evaluation using task-specific metrics

- **Design tradeoffs**: Adapted vs. Monolingual (pre-training corpus vs. tokenizer design), Tokenizer choice (BPE vs. character-level), Corpus quality vs. quantity (700M chars vs. typical PLM pre-training)

- **Failure signatures**: DEPS UAS/LAS < 30% (insufficient syntactic grounding), WSD accuracy < 80% (lexical coverage issues), LAJ near-chance (semantic knowledge limitations)

- **First 3 experiments**: 1) Tokenizer ablation: train adapted model with monolingual tokenizer, 2) Corpus scaling: incrementally increase Cantonese pre-training data, 3) Cross-task correlation analysis: validate syntax/semantic clustering hypothesis

## Open Questions the Paper Calls Out

1. How does model performance change when evaluating on informal, code-switched Cantonese rather than written standard? Models were evaluated strictly on written text; robustness to informal registers remains untested.

2. What data volume is required for a monolingual Cantonese model to outperform Cantonese-adapted models on semantic tasks? The current 700M character corpus may be insufficient; it's unclear if architecture or data scarcity limits performance.

3. Does Mandarin transfer success stem from deep linguistic similarity or surface-level character overlap? Results show competitive performance, but the specific mechanism (semantics vs. character heuristics) is not isolated.

## Limitations

- Corpus size (700M characters) is orders of magnitude smaller than typical PLM pre-training, limiting semantic task performance
- Several datasets show quality issues, particularly WSD (70.6% F1) and DEPS (all models < 32.4% UAS/LAS)
- Benchmark does not address spoken/colloquial Cantonese aspects like code-switching or cultural knowledge

## Confidence

- **High Confidence**: Cantonese-adapted models outperform Mandarin-only models; Monolingual models excel at syntactic tasks; Mandarin models competitive on semantic tasks
- **Medium Confidence**: Adapted models' superiority due to pre-training vs. tokenization; Orthographic similarity enables zero-shot transfer; Syntax/semantic task clustering
- **Low Confidence**: Tokenizer effects without ablation studies; Generalizability to other low-resource Sinitic languages; Long-term stability of transfer benefits

## Next Checks

1. **Tokenizer Ablation Study**: Train the Cantonese-adapted model using the monolingual BPE tokenizer to isolate tokenizer effects from pre-training corpus effects on syntactic task performance.

2. **Corpus Scaling Experiment**: Incrementally increase the Cantonese pre-training corpus by adding proprietary or web-scraped Cantonese text, measuring the impact on semantic task performance.

3. **Cross-Task Correlation Analysis**: Compute pairwise performance correlations across all seven tasks to statistically validate the observed syntax/semantic clustering and identify shared representation bottlenecks.