---
ver: rpa2
title: Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic
  Question Generation in Educational Domains
arxiv_id: '2501.17397'
source_url: https://arxiv.org/abs/2501.17397
tags:
- generation
- question
- u1d458
- questions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of generating contextually relevant
  and pedagogically sound questions in educational domains. The authors propose a
  hybrid approach combining In-Context Learning (ICL) with Retrieval-Augmented Generation
  (RAG) to overcome limitations of existing methods.
---

# Leveraging In-Context Learning and Retrieval-Augmented Generation for Automatic Question Generation in Educational Domains

## Quick Facts
- arXiv ID: 2501.17397
- Source URL: https://arxiv.org/abs/2501.17397
- Reference count: 40
- Primary result: Hybrid ICL-RAG model outperformed baseline models and standalone approaches in human evaluations across grammaticality, appropriateness, relevance, and complexity metrics

## Executive Summary
This study addresses the challenge of generating high-quality educational questions by proposing a hybrid approach that combines In-Context Learning (ICL) with Retrieval-Augmented Generation (RAG). The method retrieves relevant educational documents and uses GPT-4 with few-shot examples to generate questions that are both contextually relevant and pedagogically sound. Evaluated on the EduProbe dataset, the hybrid model achieved superior performance in human evaluations across multiple quality dimensions, demonstrating that combining retrieval capabilities with few-shot learning produces more effective educational question generation than either approach alone.

## Method Summary
The hybrid approach operates in two stages: first, it retrieves relevant documents from an external corpus using BM25, then it employs GPT-4 with few-shot examples to generate questions based on the retrieved context. The system was evaluated against baseline models including T5-large and BART-large, as well as standalone ICL and RAG approaches. The ICL component used varying numbers of examples (1-10) to optimize performance, while the RAG component focused on retrieving the most relevant educational content. The hybrid model integrates both retrieval and few-shot learning capabilities to generate questions that are contextually accurate and pedagogically valuable.

## Key Results
- Hybrid ICL-RAG model achieved highest human evaluation scores: grammaticality (4.84), appropriateness (4.74), relevance (4.25), and complexity (4.02)
- ICL with 7 examples performed best in automated metrics (ROUGE-L, METEOR, BERTScore)
- Hybrid model outperformed both baseline models and standalone ICL/RAG approaches
- Human evaluations consistently favored the hybrid approach across all quality dimensions

## Why This Works (Mechanism)
The hybrid approach succeeds by addressing the fundamental limitations of both standalone methods. Retrieval-Augmented Generation provides contextual accuracy by incorporating relevant external information, while In-Context Learning enables the model to adapt to specific question generation tasks through few-shot examples. The combination allows the system to generate questions that are not only grammatically correct and contextually relevant but also pedagogically appropriate for educational use. The retrieval component ensures questions are grounded in accurate educational content, while the ICL component fine-tunes the generation process for the specific task of educational question creation.

## Foundational Learning
- **In-Context Learning**: Why needed - enables task adaptation without fine-tuning; Quick check - verify model can generate appropriate questions with minimal examples
- **Retrieval-Augmented Generation**: Why needed - provides contextual grounding from external sources; Quick check - ensure retrieved documents are relevant and accurate
- **Educational Question Quality Metrics**: Why needed - measures pedagogical effectiveness; Quick check - validate human evaluation criteria align with educational goals
- **Few-shot Learning Optimization**: Why needed - balances performance with computational efficiency; Quick check - test different example counts to find optimal trade-off
- **BM25 Retrieval**: Why needed - retrieves relevant documents from corpus; Quick check - verify retrieval precision and recall on educational content
- **Human Evaluation Protocols**: Why needed - assesses real-world usability; Quick check - ensure rater expertise and inter-rater reliability

## Architecture Onboarding
**Component Map**: External Corpus -> BM25 Retriever -> GPT-4 with Few-shot Examples -> Question Generator
**Critical Path**: Document Retrieval → Context Preparation → Few-shot Prompting → Question Generation → Quality Evaluation
**Design Tradeoffs**: The system balances retrieval quality against computational cost, with BM25 providing fast but potentially less semantically accurate retrieval compared to neural approaches. The ICL component trades off example count against API costs and latency.
**Failure Signatures**: Poor retrieval quality leads to contextually irrelevant questions; insufficient few-shot examples result in generic or off-topic questions; over-reliance on examples can cause overfitting to specific formats.
**First Experiments**:
1. Test retrieval quality by measuring precision/recall on known educational queries
2. Evaluate ICL performance across different example counts (1, 3, 5, 7, 10)
3. Compare hybrid model against standalone ICL and RAG on benchmark datasets

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies entirely on human judgments without specified rater expertise or inter-rater reliability metrics
- EduProbe dataset focus on STEM topics limits generalizability to other educational domains
- Fixed BM25 retrieval strategy may be suboptimal compared to advanced semantic retrieval methods
- No analysis of computational efficiency or cost implications for practical deployment

## Confidence
- High confidence in the hybrid model's superior performance over standalone baselines
- Medium confidence in the relative ranking of ICL vs RAG standalone models
- Low confidence in the scalability and practical deployment implications

## Next Checks
1. Conduct cross-domain evaluation using datasets from humanities and social sciences educational domains
2. Perform ablation studies testing different retrieval strategies (semantic, dense, hybrid) and few-shot example counts
3. Implement multi-rater evaluation with documented inter-rater reliability and include domain-expert educational practitioners as raters