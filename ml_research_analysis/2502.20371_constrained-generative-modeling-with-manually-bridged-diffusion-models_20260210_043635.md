---
ver: rpa2
title: Constrained Generative Modeling with Manually Bridged Diffusion Models
arxiv_id: '2502.20371'
source_url: https://arxiv.org/abs/2502.20371
tags:
- diffusion
- bridges
- bridge
- manual
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces manually bridged models (MBM), a novel framework
  for diffusion-based generative modeling under constraints. The core idea is to use
  manually designed distance functions and scaling functions to guide diffusion models
  toward satisfying complex constraints (e.g., collision avoidance, staying on roads)
  while training to match the data distribution.
---

# Constrained Generative Modeling with Manually Bridged Diffusion Models

## Quick Facts
- arXiv ID: 2502.20371
- Source URL: https://arxiv.org/abs/2502.20371
- Authors: Saeid Naderiparizi; Xiaoxuan Liang; Berend Zwartsenberg; Frank Wood
- Reference count: 30
- Primary result: MBM achieves 0.10% infraction rate with r-ELBO of -0.95 on traffic scene generation

## Executive Summary
This paper introduces manually bridged models (MBM), a novel framework for diffusion-based generative modeling under constraints. The core idea is to use manually designed distance functions and scaling functions to guide diffusion models toward satisfying complex constraints (e.g., collision avoidance, staying on roads) while training to match the data distribution. The authors propose three architectural variants for incorporating these "manual bridges" into the model, with their MBM-arch variant combining both conditioning on and offsetting by the bridge function. Theoretical analysis shows these models converge to constraint-satisfying distributions at t=0. Experiments on synthetic 2D data, traffic scene generation, and image watermarking demonstrate that MBM achieves near-zero constraint violations while maintaining high sample quality. In traffic scene generation, MBM achieved 0.10% infraction rate with an r-ELBO of -0.95, outperforming baseline methods. The approach is more flexible than diffusion bridges while achieving comparable constraint satisfaction.

## Method Summary
The MBM framework modifies diffusion models by adding a "manual bridge" term to the score function. This bridge is defined as b_Ω = -γ(t)∇ℓ_Ω(x;t), where ℓ_Ω is a manually designed distance function to the constraint boundary and γ(t) is a scaling function that approaches infinity as t→0. The framework proposes three architectural variants: DB-offset (adds bridge to score), DB-arch (adds bridge to score and conditions on it), and MBM-arch (combines both approaches). The method is trained using standard denoising score matching loss, with the bridge function computed during training based on the current noisy state. For traffic scene generation, the framework uses a transformer backbone with cross-attention for vehicle-map interactions, and distance functions for collision avoidance and offroad constraints.

## Key Results
- MBM-arch achieves 0.10% infraction rate (0.05% collision + 0.05% offroad) on traffic scene generation
- MBM-arch achieves r-ELBO of -0.95, outperforming baselines (Guided Diffusion: -1.53, DB-arch: -1.48)
- MBM converges 10x faster than DB-arch (20k-30k iterations vs 250k iterations)
- On synthetic 2D data, MBM successfully generates samples on constrained manifolds with near-zero violations

## Why This Works (Mechanism)

### Mechanism 1: Annealed Constraint Annealing via Score Offsetting
- **Claim:** Adding a scaled gradient of a distance function to the score estimate forces the reverse diffusion process to place zero mass outside the constraint set at generation time (t=0).
- **Mechanism:** The framework defines a "manual bridge" b_Ω = -γ(t)∇ℓ_Ω(x;t) where ℓ_Ω is a distance to the constraint boundary. This term is added to the learned score. As t→0, the scaling function γ(t)→∞. This effectively multiplies the distribution by exp(-γℓ_Ω), which tends toward infinity outside Ω and 1 inside, forcing the terminal distribution p(x;0) to be constrained.
- **Core assumption:** The distance function ℓ_Ω is differentiable almost everywhere, and the scaling γ(t) is chosen such that it dominates the drift term as noise approaches zero.
- **Evidence anchors:** [Section 4.1, Proposition 1]: "Manually bridged models correspond to score functions of distributions of the form p_Ω(x;t) ∝ p(x;t)exp(-γ(t)ℓ_Ω(x;t))." [Abstract]: "Theoretical analysis shows these models converge to constraint-satisfying distributions at t=0."

### Mechanism 2: Dual-Path Gradient Stabilization (MBM-arch)
- **Claim:** Injecting the bridge function both as an explicit additive offset and as a conditioning input stabilizes training compared to offset-only approaches.
- **Mechanism:** Standard diffusion bridges (DB-arch) add the bridge term directly (s_θ + b), which enforces constraints but increases training loss variance. The proposed MBM-arch also conditions the network on the bridge gradient (s_θ(x, ∇ℓ)). This allows the neural network to "anticipate" the bridge push, reducing the residual error the network must learn and stabilizing convergence.
- **Core assumption:** The score network has sufficient capacity to learn the correlation between the bridge gradient input and the required denoising direction.
- **Evidence anchors:** [Section 4.2]: "We have found that additionally providing (a re-weighted version of) the bridge to the score network stabilizes training..." [Figure 4]: Shows DB-arch converging slower (250k iter) vs MBM-arch (20k-30k iter).

### Mechanism 3: Compositional Constraint Satisfaction
- **Claim:** Multiple independent constraints (e.g., collision avoidance AND road boundaries) can be satisfied simultaneously by summing their respective manual bridges.
- **Mechanism:** If b_1 guides to set Ω_1 and b_2 to Ω_2, the combined bridge b_total = b_1 + b_2 mathematically corresponds to a gradient of a potential defining the intersection Ω_1 ∩ Ω_2, provided the intersection is non-empty.
- **Core assumption:** The constraints are compatible (the intersection is not empty).
- **Evidence anchors:** [Section 4.1, Proposition 2]: "The space of manual bridges is closed under addition." [Section 5.2]: Demonstrates combining collision (Ω_c) and offroad (Ω_o) bridges.

## Foundational Learning

- **Concept: Score-Based Generative Models (SDEs)**
  - **Why needed here:** MBMs modify the reverse-time Stochastic Differential Equation (SDE). Understanding that the "score" ∇ log p(x) acts as a drift term pushing samples toward high-density regions is essential to understanding how adding -γ∇ℓ shifts that region.
  - **Quick check question:** How does the drift term in the reverse SDE change the probability flow of a sample?

- **Concept: Constrained Optimization via Penalty Methods**
  - **Why needed here:** The manual bridge acts as a dynamic penalty term (γℓ_Ω). Understanding how penalty coefficients enforce constraints helps interpret why γ(t)→∞ is necessary for hard constraints at t=0.
  - **Quick check question:** What happens to the gradient of a loss function if the penalty coefficient for a constraint violation approaches infinity?

- **Concept: Differentiable Geometry/Distance Functions**
  - **Why needed here:** The method requires a manually defined ℓ_Ω (e.g., Euclidean distance to road, intersection area for collision). You must know how to compute gradients for these geometric primitives.
  - **Quick check question:** If the constraint is "no collision," how would you formulate a differentiable loss that is zero when objects don't touch and positive when they do?

## Architecture Onboarding

- **Component map:**
  - Inputs: Noisy state x_t, Time t, Context (e.g., map image)
  - Constraint Module: Computes distance ℓ_Ω(x_t) (e.g., offroad distance, collision area) and its gradient ∇ℓ
  - Bridge Scheduler: Computes γ(t) (bridge scaling) and γ'(t) (conditioning scaling)
  - Score Network (Transformer/MLP): Takes x_t and concatenated ∇ℓ (conditioning)
  - Combiner: Adds the network output s_θ to the bridge offset b_Ω

- **Critical path:**
  1. Implementing the Ω-distance functions ℓ_Ω (specifically collision and offroad logic)
  2. Tuning the γ(t) schedule to ensure γ→∞ as t→0 without causing numerical overflow during integration
  3. Implementing the stop-gradient on the distance projection (as noted in Appendix E) to prevent destabilizing backprop through the constraint geometry

- **Design tradeoffs:**
  - MBM-arch vs. DB-arch: MBM requires implementing the conditioning path (modifying network inputs) but converges significantly faster (10x in experiments) than the offset-only DB-arch
  - Constraint complexity: Complex constraints (e.g., non-convex collision checks) require differentiable approximations or proxy functions, which may introduce approximation error

- **Failure signatures:**
  - "Garbage Safe" samples: Model outputs satisfy constraints but look unrealistic (low likelihood). This suggests the bridge is overpowering the data-driven score (check γ magnitude)
  - Constraint Violation: Infraction rate > 0 at t=0. Likely γ(t) is not ramping up fast enough or the SDE solver step size is too large
  - Training Divergence: Loss explodes. Likely caused by large bridge gradients at high noise levels; check the normalization trick in Appendix E (x' = x/√(1+σ²))

- **First 3 experiments:**
  1. 2D Checkerboard (Toy): Validate that the implementation can learn a non-uniform distribution on a discrete constrained manifold (replicate Fig 3)
  2. Traffic Scene - Single Agent: Verify road-constraint (ℓ_offroad) works in isolation on a single vehicle
  3. Traffic Scene - Multi-Agent Collision: Introduce the second bridge term (ℓ_collision) and verify that the additive bridge property holds (Prop 2) and agents don't overlap

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific mathematical conditions must manual bridges satisfy to ensure the sample paths of the reverse SDE strictly converge to the constraint set Ω, guaranteeing constraint satisfaction beyond just the modeled distribution p_θ(x, 0)?
- **Basis in paper:** [explicit] The authors state in Future Work: "We would ideally like to identify the mathematical conditions on manual bridges and their combinations that would give rise to formal guarantees of constraint satisfaction."
- **Why unresolved:** While Proposition 1 proves the modeled distribution places mass only on Ω at t=0, the authors note that the manual bridges do not necessarily satisfy the conditions (like the Polyak-Lojasiewicz condition) required to guarantee that the SDE solution itself remains in Ω.
- **What evidence would resolve it:** Derivation of necessary and sufficient conditions for bridge functions b_Ω (potentially involving a generalization of Itô's lemma for non-smooth functions) that mathematically ensure the SDE solution satisfies constraints almost surely.

### Open Question 2
- **Question:** How can the MBM framework be effectively extended to full trajectory space generation for autonomous vehicle planning while maintaining high sample fidelity and training stability?
- **Basis in paper:** [explicit] The Future Work section notes: "our approach should and could be extended to trajectory space as well. Static actor placements are, after all, merely short trajectories. Our initial attempts at this were promising, but, work remains to be done."
- **Why unresolved:** The paper validates MBM on static scene generation. Extending to trajectories introduces complex temporal dependencies and higher-dimensional constraints that may exacerbate the existing issues with bridge gradient scaling and training variance.
- **What evidence would resolve it:** Successful implementation of MBM for trajectory prediction tasks showing low infraction rates and realistic motion dynamics, comparable to the static scene results reported in Table 1.

### Open Question 3
- **Question:** Can the bridge scaling function γ(t) and noise integration schedules be automated or learned to eliminate the manual hyperparameter "fiddling" currently required for stable training?
- **Basis in paper:** [explicit] The Discussion states: "The proper functioning of manual bridges still requires a fair amount of hyperparameter fiddling, particularly integration schedule, minimum noise level, and bridge scaling... the asymptotic scaling of bridges and their gradients is extreme."
- **Why unresolved:** The extreme scaling of gradients as t→0 currently necessitates careful, manual tuning of γ(t) and architectural adjustments (like the MBM-arch) to prevent training instability.
- **What evidence would resolve it:** A proposed adaptive mechanism for γ(t) or a theoretical derivation of an optimal schedule that consistently yields stable convergence across diverse constraint sets without manual intervention.

## Limitations

- The theoretical guarantees rely on taking the SDE integration limit as t→0, but practical implementations use finite timesteps that may not fully satisfy constraints
- The distance function gradients must be carefully designed and normalized; incorrect scaling could cause instability
- The compositional property for multiple constraints assumes compatible constraint sets, which may not hold in practice

## Confidence

- **High**: The MBM-arch architecture effectively combines bridge offset with conditioning for stable training (supported by Figure 4 convergence comparison)
- **Medium**: The theoretical convergence guarantees at t=0 (Proposition 1) - practical timestep constraints may limit real-world performance
- **Medium**: The compositional property for multiple constraints (Proposition 2) - requires compatible constraint sets which may not always be guaranteed

## Next Checks

1. **Constraint Gradient Sensitivity**: Systematically vary γ(t) scaling and measure infraction rates to find optimal schedules that balance constraint satisfaction with data fidelity
2. **Cross-Domain Transfer**: Apply MBM to a different constrained generation task (e.g., molecule generation with chemical validity constraints) to test generalizability beyond traffic scenes
3. **Numerical Integration Verification**: Quantify constraint satisfaction at different SDE timestep resolutions to empirically validate the theoretical t→0 limit claim