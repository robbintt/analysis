---
ver: rpa2
title: 'Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image,
  Audio, and Video'
arxiv_id: '2510.03458'
source_url: https://arxiv.org/abs/2510.03458
tags:
- retrieval
- text
- video
- arxiv
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Omni-Embed-Nemotron is a unified multimodal retrieval model capable
  of processing text, image, audio, and video inputs within a shared embedding space.
  Built on the Qwen-Omni architecture, it uses a bi-encoder design with modality-specific
  encoding for audio and video, avoiding token interleaving to preserve full contextual
  information.
---

# Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video

## Quick Facts
- **arXiv ID**: 2510.03458
- **Source URL**: https://arxiv.org/abs/2510.03458
- **Reference count**: 34
- **Primary result**: Achieves NDCG@10 of 0.7064 on FineVideo video retrieval benchmark

## Executive Summary
Omni-Embed-Nemotron is a unified multimodal retrieval model capable of processing text, image, audio, and video inputs within a shared embedding space. Built on the Qwen-Omni architecture, it uses a bi-encoder design with modality-specific encoding for audio and video, avoiding token interleaving to preserve full contextual information. The model employs contrastive learning with hard-negative mining for training, leveraging text-text and text-image pairs, and achieving strong zero-shot performance on video and audio retrieval without requiring modality-specific training data. Evaluated on benchmarks including FineVideo and LPM for video retrieval, ViDoRe for image retrieval, and MTEB for text retrieval, Omni-Embed-Nemotron demonstrates competitive performance while supporting both cross-modal and joint-modal retrieval.

## Method Summary
Omni-Embed-Nemotron employs a bi-encoder architecture built on Qwen-Omni, processing each modality through separate encoding streams rather than token interleaving. Text and image inputs use the original Qwen-Omni encoder, while audio and video employ modality-specific encoders. For video, it extracts frames and generates corresponding audio transcripts, encoding them in separate streams. The model uses contrastive learning with hard-negative mining, trained on text-text and text-image pairs. Notably, it achieves strong performance on audio and video retrieval tasks without any audio or video training data, relying instead on zero-shot transfer capabilities from the unified embedding space.

## Key Results
- Achieves NDCG@10 of 0.7064 on FineVideo video retrieval benchmark
- Attains NDCG@10 of 0.5407 on audio-only retrieval and 0.4488 on video-only retrieval for open-domain video content
- Demonstrates strong zero-shot performance across all four modalities without modality-specific training data

## Why This Works (Mechanism)
The model's effectiveness stems from its unified embedding space that maps diverse modalities into a common semantic representation. By avoiding token interleaving and using modality-specific encoding, it preserves the full contextual information from each input type. The contrastive learning approach with hard-negative mining ensures robust feature learning across modalities. The zero-shot capability emerges from the model's ability to transfer knowledge from text-image pairs to handle audio and video inputs, leveraging the shared embedding space to capture cross-modal semantic relationships without explicit audio/video training.

## Foundational Learning
- **Bi-encoder architecture**: Separates query and document encoding for efficient retrieval, reducing computational complexity from O(n²) to O(n) during inference
- **Contrastive learning with hard-negative mining**: Improves discriminative power by selecting challenging negative samples that are semantically similar to positive examples
- **Zero-shot transfer**: Enables performance on unseen modalities by leveraging shared embedding spaces learned from related modalities
- **Modality-specific encoding**: Preserves full contextual information by processing each modality through dedicated encoders rather than forcing uniform tokenization
- **Unified embedding space**: Maps diverse modalities into a common representation space, enabling cross-modal retrieval operations
- **Hard-negative mining**: Selects semantically challenging negatives to improve model robustness and discrimination

## Architecture Onboarding

Component Map: Text/Image Encoder -> Shared Embedding Space <- Audio Encoder <- Audio Data; Video Encoder <- Video Frames/Audio Transcripts

Critical Path: Input Modality → Modality-Specific Encoder → Shared Embedding Space → Similarity Computation → Retrieval

Design Tradeoffs:
- Separate stream encoding preserves full context but increases model complexity compared to token interleaving
- Zero-shot approach eliminates need for modality-specific training data but may limit performance on complex audio/video tasks
- Bi-encoder architecture enables efficient retrieval but may miss cross-modal interactions during encoding

Failure Signatures:
- Poor performance on domain-specific video content despite strong general retrieval scores
- Inconsistent cross-modal retrieval quality across different modality pairs
- Latency issues when processing high-resolution video or long audio files

First Experiments:
1. Evaluate retrieval quality on held-out text-image pairs to verify baseline performance
2. Test cross-modal retrieval between text and image to validate embedding space alignment
3. Assess zero-shot transfer performance on audio/video retrieval using only text-image trained model

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can domain-specific video training data improve generalization to unseen video datasets, or does it primarily lead to overfitting on the target domain?
- Basis in paper: [explicit] The authors note that "Adding the FineVideo training set improved performance on the FineVideo evaluation benchmark... However, it did not enhance retrieval across other modalities or generalize better to unseen video datasets."
- Why unresolved: The paper demonstrates domain-specific fine-tuning helps on FineVideo (NDCG@5: 0.55→0.61) but does not investigate whether alternative training strategies or more diverse video corpora could yield transferable improvements.
- What evidence would resolve it: Systematic evaluation across multiple held-out video benchmarks after training on varied video datasets with different domain compositions.

### Open Question 2
- Question: What multimodal alignment strategies beyond separate-stream encoding could improve cross-modal retrieval performance?
- Basis in paper: [explicit] The conclusion explicitly points toward "future improvements through domain-specific fine-tuning and more sophisticated multimodal alignment strategies."
- Why unresolved: The paper compares only two fusion approaches (interleaved vs. separate streams) and does not explore intermediate architectures, attention mechanisms, or learned fusion weights.
- What evidence would resolve it: Ablation studies comparing additional fusion architectures (e.g., cross-attention layers, learned modality weights) on the same benchmarks.

### Open Question 3
- Question: Why does audio-only retrieval substantially outperform video-only retrieval on open-domain video content like FineVideo?
- Basis in paper: [inferred] Table 3 shows audio-only achieves 0.5407 NDCG@10 while video-only achieves only 0.4488, but the paper does not explain this 9-point gap or analyze what semantic information each modality captures.
- Why unresolved: The paper reports the finding but does not conduct analysis on frame sampling rates, visual content density, or audio-visual semantic alignment in the dataset.
- What evidence would resolve it: Controlled experiments varying frame sampling density, audio transcription quality, and per-category performance breakdowns to identify where visual signal fails.

### Open Question 4
- Question: What is the minimal training data composition required to achieve strong zero-shot retrieval across all four modalities?
- Basis in paper: [inferred] The model achieves strong video/audio performance "even without using audio or video data during training," relying only on text-text and text-image pairs, but the paper does not investigate whether this is optimal.
- Why unresolved: The paper does not ablate training data composition or test whether including modest amounts of audio/video pairs could further improve performance.
- What evidence would resolve it: Systematic training runs with varying proportions of text, image, audio, and video pairs, evaluated across all modalities.

## Limitations
- Zero-shot transfer approach may not capture full semantic richness of audio and video compared to models trained with dedicated multimodal data
- Performance claims require careful interpretation given potential differences in model scales and training regimes compared to baseline models
- Bi-encoder architecture's modality-specific encoding may introduce complexity affecting retrieval quality at scale
- Evaluation on limited benchmark samples may not represent full real-world multimodal retrieval scenarios

## Confidence
- High confidence: Model architecture and training methodology (bi-encoder design, contrastive learning with hard-negative mining, text-image pair training) are well-documented and technically sound
- Medium confidence: Performance claims on video and audio retrieval benchmarks, as zero-shot transfer capabilities can be sensitive to domain shifts and evaluation conditions
- Medium confidence: Claims about avoiding token interleaving preserving full contextual information, as this architectural choice's impact on retrieval quality requires further empirical validation

## Next Checks
1. Conduct controlled experiments comparing retrieval quality on audio and video tasks when using models trained with dedicated multimodal data versus the zero-shot approach
2. Test the model's retrieval performance on additional real-world multimodal datasets beyond the reported benchmarks to assess generalization
3. Evaluate the impact of modality-specific encoding on retrieval latency and scalability compared to token interleaving approaches