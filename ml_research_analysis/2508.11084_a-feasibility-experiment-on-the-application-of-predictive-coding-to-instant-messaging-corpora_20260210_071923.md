---
ver: rpa2
title: A Feasibility Experiment on the Application of Predictive Coding to Instant
  Messaging Corpora
arxiv_id: '2508.11084'
source_url: https://arxiv.org/abs/2508.11084
tags:
- text
- training
- chat
- instant
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of applying predictive coding
  to instant messaging data, which presents unique challenges due to its informal
  nature, small message sizes, and high dimensionality from abbreviations and numbers.
  The authors propose a solution that first groups individual messages into "day chat"
  documents by conversation and time, making manual review more feasible.
---

# A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora

## Quick Facts
- arXiv ID: 2508.11084
- Source URL: https://arxiv.org/abs/2508.11084
- Reference count: 8
- Primary result: Grouping instant messages into day-chat documents and replacing numeric tokens with tags improves predictive coding precision from 60.7% to 65% at 80% recall on financial IM data.

## Executive Summary
This paper addresses the challenge of applying predictive coding to instant messaging corpora, which are characterized by informal language, small message sizes, and high dimensionality from abbreviations and numbers. The authors propose a practical solution that groups individual messages into "day chat" documents by conversation and time, making manual review more feasible. They implement feature selection and logistic regression classification, with a focus on reducing dimensionality by replacing numeric tokens with "[NUM]" and "[TIMESTAMP]" tags. This preprocessing step significantly improves model performance, increasing precision at 80% recall from 60.7% to 65% on a dataset of 5,000 training documents.

## Method Summary
The approach groups individual chat messages into day-based document collections by conversation and timestamp, creating review units with sufficient semantic content for human coders and ML classifiers. Numeric tokens are replaced with generic tags ([NUM], [TIMESTAMP]) to reduce dimensionality, while information gain-based feature selection focuses the model on discriminative tokens. The method uses logistic regression with normalized token frequency features, minimum token length 3, and top 20,000 features selected by information gain score. The pipeline processes Bloomberg instant messaging data, applying text normalization to remove participant join/leave markers and silence timestamps before classification.

## Key Results
- Precision at 80% recall improves from 60.7% to 65% after numeric tagging on 5,000 training documents
- Information gain feature selection identifies domain-specific tokens like "isdafix," "done," and "fix" as highly discriminative
- Day-level grouping strategy makes predictive coding economically feasible for IM corpora by creating review units with sufficient semantic content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping individual chat messages into day-based document collections makes predictive coding economically feasible for instant messaging corpora.
- Mechanism: Individual chat lines are too granular for meaningful responsiveness coding. By consolidating messages by conversation and date into "day chat" documents, the workflow creates review units with sufficient semantic content for both human coders and ML classifiers to make informed judgments.
- Core assumption: Instant communication about a non-casual topic is highly likely to conclude within the day it was initiated.
- Evidence anchors: [abstract] "exploit a data management workflow to group messages into day chats...to provide an economically feasible predictive coding solution"; [section] "grouping individual chat lines into collections of text, which can be coded and assigned to custodians, thus making a human review economically feasible"
- Break condition: If conversations span multiple days with topic shifts mid-conversation, day-level grouping may mix responsive and non-responsive content in single documents.

### Mechanism 2
- Claim: Replacing numeric tokens with generic tags ([NUM], [TIMESTAMP]) improves classification precision by reducing dimensionality.
- Mechanism: Numeric tokens in financial chat data create sparse, high-dimensional feature spaces. Many unique numbers (amounts, rates, references) appear infrequently, causing the model to learn from noise. Tagging collapses all digit sequences into shared features, preserving the semantic signal "a number exists here" while reducing vocabulary size.
- Core assumption: Reviewers care whether messages contain quantitative information but not the specific numeric values.
- Evidence anchors: [abstract] "improve the solution's baseline model performance by dimensionality reduction, with focus on quantitative features"; [section] "the observed increase in performance is more likely to be attributed to the decrease in dimensionality caused by eliminating numbers, rather than the tags' predictive power itself"; Figure 5 shows near-identical results between tagged and number-removal approaches.
- Break condition: If specific numeric values (e.g., particular monetary thresholds) are discriminative signals, tagging removes that information.

### Mechanism 3
- Claim: Information gain-based feature selection before logistic regression training focuses the model on discriminative tokens.
- Mechanism: Tokens are ranked by information gain; the top N (20,000 here) are retained for model training. This filters low-information features that increase computational cost and overfitting risk without improving classification.
- Core assumption: The top features by information gain score are the most useful for separating responsive from non-responsive documents.
- Evidence anchors: [section] "Prior to training the model with LR, an information gain assessment was performed, based on which the tokens are ranked"; [section] Table II shows domain-specific tokens (e.g., "isdafix," "done," "fix") with high coefficients, indicating the selection surfaces meaningful features.
- Break condition: If discriminative power is distributed across many low-frequency tokens rather than concentrated in high-IG features, aggressive thresholding may exclude useful signals.

## Foundational Learning

- Concept: **Predictive Coding / Technology Assisted Review (TAR)**
  - Why needed here: This is the core application—binary document classification (responsive/not responsive) for legal eDiscovery, where model scores prioritize human review effort.
  - Quick check question: Given a model scoring documents 0.0–1.0 for responsiveness, which documents should a review team prioritize to maximize recall with minimal effort?

- Concept: **Dimensionality Reduction in Text Classification**
  - Why needed here: IM text has high vocabulary size due to abbreviations, typos, and numeric tokens; small training sets (5k documents) cannot reliably estimate parameters for large feature spaces.
  - Quick check question: Why might a model with 50,000 features trained on 5,000 documents overfit, and how does reducing to 20,000 features help?

- Concept: **Precision at Fixed Recall**
  - Why needed here: Legal review workflows often require minimum recall thresholds (e.g., 80%) for defensibility; precision at that recall measures efficiency (fewer false positives = lower cost).
  - Quick check question: At 80% recall with 65% precision, how many documents must reviewers read to find 80% of 10,000 relevant documents?

## Architecture Onboarding

- Component map:
  - Raw IM chat logs -> Deduplication/compression -> Day chat document creation -> Text normalization -> Numeric token tagging -> Tokenization -> Information gain ranking -> Top-N feature selection -> Logistic regression training -> Scored documents

- Critical path:
  1. Day chat grouping (determines document granularity; foundational for both human review and model training)
  2. Numeric token tagging (drives precision improvements: 60.7% → 65% in 5k scenario)
  3. Feature selection (constrains model to information-rich tokens)

- Design tradeoffs:
  - **Raw vs. normalized text**: Normalized text removes participant information, causing ~1% precision drop; retain raw text if participant identity may be discriminative.
  - **Single vs. dual tagging**: [NUM] alone performs comparably to [NUM]+[TIMESTAMP]; marginal gains may not justify added complexity.
  - **Training set size**: Larger training sets (29k vs. 5k) improve absolute performance (72.7% → 74% precision) but reduce relative benefit of tagging (dimensionality matters less with more data).

- Failure signatures:
  - **Numeric tokens with high coefficients in untagged models**: Indicates overfitting to noise features; triggers need for tagging intervention.
  - **Precision plateaus at high recall**: Model struggles to separate borderline cases; may need additional features or more training data.
  - **Normalized text underperforming raw**: Information loss from participant removal exceeds noise reduction benefit.

- First 3 experiments:
  1. **Baseline validation**: Train on raw day chat documents with no tagging; measure precision @ 80% recall to establish performance floor.
  2. **Tagging ablation**: Compare [NUM] single-tag vs. no-tag on same data; quantify precision gain attributable to dimensionality reduction.
  3. **Training size sensitivity**: Retain best tagging configuration; test with 5k, 29k, and full training sets to characterize how training data volume interacts with preprocessing choices.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does tagging numbers by their specific semantic value (e.g., monetary amount, date, reference) improve classification performance compared to a generic "[NUM]" tag?
  - Basis in paper: [explicit] The authors state in the Future Work section that they are currently examining "tagging numbers per their semantic value."
  - Why unresolved: The current study only evaluated a universal tag for all numbers or a separate tag for timestamps, without distinguishing the specific type of quantitative data.
  - What evidence would resolve it: A comparative study showing precision/recall metrics for models trained on semantically tagged datasets versus the generic tagging approach.

- **Open Question 2**: Does applying string matching algorithms (e.g., Levenshtein distance) to cluster similar words and spelling mistakes further reduce dimensionality and improve accuracy?
  - Basis in paper: [explicit] The authors suggest that processing the corpus vocabulary with string matching methods might be beneficial to "rationalise the vocabulary."
  - Why unresolved: Chat text is informal and contains many abbreviations and typos, but the study has not yet tested methods to normalize these variations into single features.
  - What evidence would resolve it: Experimental results comparing the baseline model against a model pre-processed using distance-based clustering for vocabulary normalization.

- **Open Question 3**: Are the observed performance gains from dimensionality reduction statistically significant, or are they random fluctuations caused by using different validation sets?
  - Basis in paper: [explicit] The authors note in Section III.C.1 that "a statistical significance test for the differences in the AUROC curves would be useful."
  - Why unresolved: The paper relies on intuitive interpretation of precision/recall curves and approximate AUROC values, acknowledging that results could be due to chance.
  - What evidence would resolve it: Statistical testing (e.g., p-values) across multiple cross-validation folds to confirm the reliability of the precision improvements.

- **Open Question 4**: Can combining tags (e.g., an <amount> followed by a <rate>) create predictive features that outperform single independent tags?
  - Basis in paper: [explicit] The Future Work section hypothesizes that "combinations of tags" might be correlated with responsive documents.
  - Why unresolved: The logistic regression model used treats tokens independently and cannot detect sequential relationships between tags without feature engineering.
  - What evidence would resolve it: A model evaluation incorporating tag n-grams (e.g., bigrams of semantic tags) to test for improved correlation with responsiveness.

## Limitations
- Evaluation relies on proprietary Bloomberg instant messaging dataset, preventing independent replication and verification
- Modest absolute improvement in precision (60.7% → 65%) may not justify implementation costs in all contexts
- Only tests one classification algorithm (logistic regression), leaving open whether benefits generalize to other models

## Confidence
- **High confidence**: The general framework of grouping IM messages into day-level documents for TAR feasibility, and the basic principle that numeric tagging reduces dimensionality.
- **Medium confidence**: The specific performance numbers (60.7% → 65% precision at 80% recall) and the claim that this represents meaningful cost savings, as these depend on the proprietary dataset and specific implementation details not fully disclosed.
- **Low confidence**: Claims about the superiority of the specific approach over alternatives, as no comparative studies with other feature selection methods, classification algorithms, or document grouping strategies are presented.

## Next Checks
1. **Replication with open data**: Replicate the numeric tagging and logistic regression pipeline on publicly available IM datasets (e.g., Enron email corpus with IM threads, or Reddit comment threads) to verify that precision improvements generalize beyond the proprietary Bloomberg dataset.
2. **Comparative algorithm study**: Test whether precision@recall improvements from numeric tagging extend to modern classifiers like Support Vector Machines, Random Forests, or neural networks, to determine if the benefit is specific to logistic regression or more general.
3. **Document grouping ablation**: Compare the day-chat grouping strategy against alternative segmentation methods (e.g., conversation-based grouping regardless of date, fixed-size message windows) to empirically validate that temporal consolidation is optimal for this classification task.