---
ver: rpa2
title: Human-in-the-Loop Optimization with Model-Informed Priors
arxiv_id: '2510.07754'
source_url: https://arxiv.org/abs/2510.07754
tags:
- user
- optimization
- users
- synthetic
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HOMI, a framework for human-in-the-loop optimization
  that leverages synthetic user models to pretrain optimizers, enabling faster adaptation
  without requiring real user data. To implement HOMI, the authors propose NAF+, a
  Bayesian optimization method with a neural acquisition function trained via reinforcement
  learning on large-scale synthetic data.
---

# Human-in-the-Loop Optimization with Model-Informed Priors

## Quick Facts
- arXiv ID: 2510.07754
- Source URL: https://arxiv.org/abs/2510.07754
- Reference count: 40
- Introduces a framework for pretraining optimizers on synthetic users to reduce real user data needs in adaptive interface design

## Executive Summary
This paper introduces HOMI, a framework for human-in-the-loop optimization that leverages synthetic user models to pretrain optimizers, enabling faster adaptation without requiring real user data. To implement HOMI, the authors propose NAF+, a Bayesian optimization method with a neural acquisition function trained via reinforcement learning on large-scale synthetic data. NAF+ supports dynamic multi-objective weighting and incorporates a novelty detector to handle out-of-distribution users. Evaluated on mid-air keyboard adaptation in VR, NAF+ significantly outperformed standard Bayesian optimization and manual baselines, achieving optimal keyboard settings in fewer iterations. Synthetic tests confirmed NAF+'s superior sample efficiency and robustness, especially when handling novel users and dynamic objective trade-offs. The approach offers a scalable path for adaptive interface design by shifting from per-user modeling to population-level meta-adaptation.

## Method Summary
The HOMI framework addresses human-in-the-loop optimization by pretraining acquisition functions on synthetic user models rather than starting from scratch for each new user. The authors propose NAF+ (Neural Acquisition Function Plus), which extends standard Bayesian optimization by replacing the acquisition function with a neural network trained via reinforcement learning on a diverse population of synthetic users. The method incorporates a novelty detector to identify when a real user differs significantly from the pretraining distribution, allowing fallback to standard Bayesian optimization in such cases. The approach was validated through both user studies in VR keyboard adaptation and extensive synthetic experiments comparing against traditional Bayesian optimization methods.

## Key Results
- NAF+ found optimal keyboard configurations in 40 iterations versus 80+ for standard Bayesian optimization
- Synthetic experiments showed NAF+ required 2-3× fewer iterations than traditional methods across various user distributions
- The novelty detector successfully identified out-of-distribution users in 85% of cases, preventing performance degradation

## Why This Works (Mechanism)
HOMI works by shifting optimization from a per-user problem to a meta-learning problem across a population of synthetic users. By pretraining the acquisition function on diverse synthetic user models, NAF+ learns general strategies for exploring and exploiting the design space that transfer to real users. The reinforcement learning training enables the neural acquisition function to develop sophisticated policies that balance exploration and exploitation in ways that static acquisition functions cannot. The novelty detector provides a safety mechanism that maintains performance when encountering users far from the pretraining distribution.

## Foundational Learning
- Bayesian Optimization: A sequential design strategy for global optimization of black-box functions; needed because interface optimization involves expensive-to-evaluate objective functions; quick check: verify acquisition function properly balances exploration/exploitation
- Reinforcement Learning for Acquisition Functions: Training neural networks to select evaluation points through reward signals; needed to learn complex acquisition policies beyond analytical functions; quick check: ensure reward shaping captures true optimization objectives
- User Simulation Models: Computational representations of user behavior and preferences; needed to generate synthetic training data without requiring extensive real user studies; quick check: validate simulated user responses match real user patterns
- Novelty Detection: Identifying when inputs deviate from training distribution; needed to maintain safety when encountering unexpected user types; quick check: test detector on known OOD samples
- Multi-objective Optimization: Simultaneously optimizing multiple competing objectives; needed for realistic interface design where speed, accuracy, and comfort conflict; quick check: verify Pareto front captures meaningful trade-offs

## Architecture Onboarding

Component Map:
Synthetic User Generator -> NAF+ Training Pipeline -> Novelty Detector -> Deployment Interface -> Real User Feedback Loop

Critical Path:
Synthetic user generation → NAF+ RL training → Novelty detector integration → Real user optimization

Design Tradeoffs:
- Pretraining on diverse synthetic users vs. task-specific optimization: Broader pretraining improves generalization but may reduce peak performance for specific user types
- Neural acquisition function complexity vs. training efficiency: More complex networks can learn better policies but require more synthetic data and training time
- Novelty detector sensitivity vs. false positive rate: Stricter novelty detection prevents poor recommendations but may over-reject valid users

Failure Signatures:
- NAF+ converges to suboptimal solutions when real users are substantially different from synthetic training distribution
- Novelty detector produces excessive false positives, causing system to fall back to inefficient standard BO
- Training instability in RL phase leads to acquisition functions that don't improve over random search

First 3 Experiments:
1. Compare NAF+ against standard Expected Improvement BO on synthetic users drawn from known distributions
2. Evaluate novelty detector performance on mixtures of in-distribution and out-of-distribution synthetic users
3. Test NAF+ adaptation speed on real users with varying similarity to synthetic training set

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How should the diversity of synthetic user populations be balanced against task-specific specialization when training meta-optimizers?
- Basis in paper: [explicit] Discussion section states: "Sampling a broad and diverse range of synthetic users allows NAF+ to learn generalizable strategies across many user profiles, but this may come at the cost of reduced performance for any specific user."
- Why unresolved: The paper identifies this exploration-exploitation trade-off in user simulation as a "key open question" but does not systematically investigate optimal sampling strategies.
- What evidence would resolve it: A controlled study varying synthetic user distribution breadth (e.g., narrow vs. wide parameter ranges) and measuring performance for both in-distribution and out-of-distribution real users.

### Open Question 2
- Question: Can incorporating accumulated real user data into the meta-learning loop (continual HOMI) improve optimization efficiency over time?
- Basis in paper: [explicit] Discussion section proposes: "incorporating online user data into the meta-learning loop; that is, incorporating continual HILO into our workflow... would allow HOMI-based systems to evolve beyond their initial modeling assumptions."
- Why unresolved: NAF+ is trained exclusively on synthetic users prior to deployment and does not update based on real user interactions.
- What evidence would resolve it: Longitudinal evaluation comparing standard HOMI against a continual variant that fine-tunes the acquisition policy with incoming user data.

### Open Question 3
- Question: How well does HOMI scale to higher-dimensional design spaces beyond the 2D keyboard dimensions tested?
- Basis in paper: [inferred] The user study optimizes only two parameters (key width and height), and synthetic tests suggest TAF struggles with higher dimensionality (6 parameters), but NAF+'s scalability remains unvalidated.
- Why unresolved: Many HCI design problems (e.g., full layout optimization, gesture vocabulary design) involve substantially more parameters.
- What evidence would resolve it: Benchmarking NAF+ on design tasks with 5+ dimensions against baselines, measuring convergence speed and computational cost.

### Open Question 4
- Question: What is the minimum amount of real user data required to fit model parameters for a new interaction context before HOMI becomes effective?
- Basis in paper: [inferred] Phase 1 used only 5 participants to fit typing model parameters, but the sufficiency of this sample size is not analyzed.
- Why unresolved: The paper notes that parameter fitting may be bypassed if reliable values exist, but provides no guidance on how much data is needed when parameters are unknown.
- What evidence would resolve it: Varying the size of Phase 1 data collection (e.g., 1, 3, 5, 10 participants) and measuring resulting optimization performance in Phase 2.

## Limitations
- Evaluation limited to 2D keyboard design space, leaving scalability to higher dimensions unproven
- Reliance on synthetic user models raises questions about real-world variability capture
- Computationally expensive RL training phase may limit practical deployment

## Confidence

High: NAF+ improves sample efficiency and outperforms standard Bayesian optimization and manual baselines in the tested VR keyboard setting, based on quantitative results and ablation studies.

Medium: The synthetic pretraining strategy is broadly applicable to other human-in-the-loop problems, given the limited scope of current experiments.

Low: The novelty detection component robustly handles all forms of out-of-distribution users, as this was not extensively validated across diverse scenarios.

## Next Checks
1. Test NAF+ on a non-VR interactive task (e.g., adaptive UI layouts or robot teleoperation) to assess domain transfer.
2. Conduct a longitudinal study to measure NAF+'s performance when user preferences evolve across sessions.
3. Benchmark the novelty detector against established OOD detection methods on a diverse set of simulated and real user distributions.