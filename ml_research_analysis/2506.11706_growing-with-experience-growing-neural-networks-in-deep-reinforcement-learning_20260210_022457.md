---
ver: rpa2
title: 'Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning'
arxiv_id: '2506.11706'
source_url: https://arxiv.org/abs/2506.11706
tags:
- network
- learning
- networks
- training
- grownn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training large neural networks
  in deep reinforcement learning, where current approaches struggle to scale up network
  complexity without compromising trainability. The authors propose GrowNN, a method
  that incrementally grows neural networks during training using network morphisms,
  specifically Net2Net transformations.
---

# Growing with Experience: Growing Neural Networks in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2506.11706
- **Source URL:** https://arxiv.org/abs/2506.11706
- **Authors:** Lukas Fehring; Marius Lindauer; Theresa Eimer
- **Reference count:** 4
- **Primary result:** GrowNN achieves up to 72% relative improvement over static networks in MuJoCo Ant by incrementally growing network depth during training.

## Executive Summary
This paper addresses the challenge of training large neural networks in deep reinforcement learning, where current approaches struggle to scale up network complexity without compromising trainability. The authors propose GrowNN, a method that incrementally grows neural networks during training using network morphisms, specifically Net2Net transformations. The approach starts with a small network to learn initial policies, then progressively adds layers without altering the encoded function, allowing subsequent updates to utilize the added capacity as policy complexity increases. GrowNN can be seamlessly integrated into existing RL agents and is algorithm-agnostic.

## Method Summary
GrowNN uses Net2DeeperNet transformations to add layers with identity initialization (weights = 1 for same-position neuron, 0 for others) while preserving the exact input-output function. The method starts training with a small network, then grows it at fixed interaction intervals by adding layers without bias terms, using ReLU activation to maintain the identity property. Growth occurs in the feature extractor (not policy head), and the approach can be integrated into any RL algorithm. Training uses modified BOHB with static fidelity scheduling for hyperparameter tuning.

## Key Results
- MiniHack Room: GrowNN-deeper networks achieved over 50% solution rate vs. 0% for static networks of same size (48% improvement)
- MuJoCo Ant: GrowNN showed 72% relative improvement over static counterparts, with agents learning to actively move forward vs. baselines that only avoided death states
- The method enables larger networks to solve previously impossible tasks without algorithmic changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Function-preserving network growth maintains learned policy quality while increasing capacity.
- Mechanism: Net2DeeperNet transformations add layers initialized as identity mappings (outgoing weights = 1 for same-position neuron, 0 for others). This preserves the exact input-output function θ(s) = f(θ)(s) ∀s ∈ S, allowing subsequent gradient updates to utilize new capacity without disrupting existing knowledge.
- Core assumption: The activation function must be idempotent (e.g., ReLU) and added layers must have no bias, ensuring double-application doesn't change outputs.
- Evidence anchors:
  - [abstract] "We start training a small network to learn an initial policy. Then we add layers without changing the encoded function."
  - [section 3] "Formally, a transformation f : Θ → Θ qualifies as a network morphism iff for each input vector s ∈ S the output after transformations is unchanged"
  - [corpus] Weak direct evidence; corpus neighbors address pre-training and replay strategies but not function-preserving growth mechanisms.
- Break condition: If activation functions are non-idempotent (e.g., sigmoid, tanh) or bias terms are included in new layers, the identity property fails and policy degrades immediately post-growth.

### Mechanism 2
- Claim: Small networks learn initial policies more reliably than large networks in deep RL.
- Mechanism: Smaller parameter spaces reduce optimization complexity during early training when the agent must discover fundamental skills (e.g., torque-state relationships in Ant). Growth adds capacity only when policy complexity demands it.
- Core assumption: RL tasks decompose into hierarchical skill acquisition where early skills are simpler and learnable with fewer parameters.
- Evidence anchors:
  - [section 3] "small networks may learn the simpler skills easily, putting them together and controlling the ant to move forward is rather complicated, which means a larger network would be preferred"
  - [section 4] "while on MiniHack, the baseline configured with a static depth 1 outperforms our growth approach, as soon as we increase the depth... GrowNN is superior"
  - [corpus] Assumption: "A Forget-and-Grow Strategy" paper suggests primacy bias from early experiences limits generalizability, indirectly supporting staged capacity approaches.
- Break condition: If tasks require high-capacity representations from the outset (e.g., complex visual processing), starting small may fail to learn any useful policy before growth triggers.

### Mechanism 3
- Claim: Environment interaction count serves as a viable heuristic for growth timing.
- Mechanism: Growth events are scheduled at fixed interaction intervals (shown as dotted vertical lines in Figure 2). This couples capacity expansion with experience accumulation rather than performance metrics.
- Core assumption: Policy complexity increases monotonically with environment interactions, making interaction count a proxy for when additional capacity becomes beneficial.
- Evidence anchors:
  - [section 3] "GrowNN starts with a small network for the initial training phase and incrementally grows it using the number of environment interactions as a heuristic"
  - [section 3] "we utilize a static fidelity schedule" for growth timing within a modified BOHB framework
  - [corpus] No direct corpus evidence for interaction-based growth scheduling in RL.
- Break condition: If early interactions are uninformative or if task complexity varies non-monotonically, fixed scheduling wastes capacity or grows too late.

## Foundational Learning

- Concept: **Network Morphisms (Net2Net)**
  - Why needed here: GrowNN's core operation relies on understanding how to add layers while preserving function outputs.
  - Quick check question: Can you explain why initializing new layer weights as identity matrices preserves the forward pass?

- Concept: **Policy Gradient Methods (PPO)**
  - Why needed here: Experiments use PPO as the base algorithm; understanding policy updates is required for integration.
  - Quick check question: How does PPO's clipped objective prevent large policy updates that could destabilize growth transitions?

- Concept: **Feature Extractors in RL**
  - Why needed here: Growth is applied to the feature extractor (not policy head), requiring understanding of where capacity matters most.
  - Quick check question: Why might growing the feature extractor have more impact than growing the policy head?

## Architecture Onboarding

- Component map: Base RL agent (PPO) -> Feature extractor (MLP with optional CNN) -> Policy head (unchanged) -> Growth controller: Monitors environment interactions, triggers Net2DeeperNet at scheduled intervals -> Hyperparameter tuner: Modified BOHB with static fidelity schedule

- Critical path:
  1. Initialize small network (e.g., 1-2 layers in feature extractor)
  2. Train until first growth trigger (fixed interaction count)
  3. Apply Net2DeeperNet: Add layer with identity weights, zero bias, verify ReLU activation
  4. Continue training; repeat steps 2-3 until final architecture size reached

- Design tradeoffs:
  - Growth frequency vs. stability: More frequent growth = finer capacity matching but more transitions to destabilize training
  - Initial size vs. final size: Too small initial network may fail early; too large wastes growth benefit
  - Growth location: Paper grows feature extractor; growing policy head is untested

- Failure signatures:
  - Performance drop immediately after growth → Check: Was bias incorrectly added? Is activation non-idempotent?
  - Static baselines outperform GrowNN at all sizes → Check: Is growth schedule too aggressive? Initial network too small?
  - No improvement despite growth → Check: Are new layers receiving gradient updates? Is learning rate appropriate post-growth?

- First 3 experiments:
  1. Reproduce MiniHack Room 10x10 with 1→4 layer growth; verify ~50% solution rate vs. static 4-layer baseline at 0%
  2. Ablation: Test different growth schedules (2 vs. 4 vs. 8 growth events) on MuJoCo Ant to find stability boundary
  3. Integration test: Apply GrowNN to a different RL algorithm (e.g., DQN or SAC) to validate algorithm-agnostic claim

## Open Questions the Paper Calls Out

- Question: Does the GrowNN approach improve performance when applied to standard deep CNN architectures (e.g., Impala CNN) used in visual RL?
  - Basis in paper: [explicit] The conclusion states, "more evaluation is necessary, especially on other network architectures like CNNs."
  - Why unresolved: The experiments were limited to Multi-Layer Perceptrons (MLPs) and a single CNN layer.
  - What evidence would resolve it: Benchmarks on complex visual environments (e.g., Atari) utilizing deep residual networks.

- Question: Can performance be further improved by replacing the static interaction-count heuristic with adaptive growth triggers?
  - Basis in paper: [inferred] The method grows networks using "environment interactions as a heuristic," contrasting with related work that uses gradient-based decisions.
  - Why unresolved: The paper does not compare the simple static schedule against dynamic triggers based on plasticity or loss.
  - What evidence would resolve it: Comparative analysis of static scheduling vs. adaptive growth based on neuron dormancy or gradient information.

- Question: Is increasing network depth strictly more effective than increasing width for maintaining trainability in RL?
  - Basis in paper: [inferred] The paper focuses almost exclusively on "GrowNN-deeper" transformations, leaving the efficacy of width expansion (Net2WiderNet) unexplored.
  - Why unresolved: Experiments only validated adding layers; the effect of adding neurons to existing layers remains unknown.
  - What evidence would resolve it: Ablation studies comparing the performance impact of growing depth versus width on the same tasks.

## Limitations
- The modified BOHB hyperparameter tuning procedure is underspecified, making exact reproduction difficult
- The algorithm-agnostic claim lacks experimental validation beyond PPO
- Limited environment diversity (only 2 tasks tested) constrains generalizability

## Confidence

- **High confidence** in the core mechanism (Net2DeeperNet function preservation) as it relies on well-established network morphism theory
- **Medium confidence** in performance claims due to limited environment diversity (2 tasks only) and unknown hyperparameter tuning details
- **Low confidence** in the "algorithm-agnostic" claim without evidence from other RL algorithms

## Next Checks
1. Replicate MiniHack Room 10x10 results with 1→4 layer growth to verify the claimed ~50% solution rate improvement
2. Test different growth schedules (2 vs. 4 vs. 8 growth events) on MuJoCo Ant to identify optimal frequency and timing
3. Apply GrowNN to a different RL algorithm (DQN or SAC) to empirically validate the algorithm-agnostic integration claim