---
ver: rpa2
title: 'T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning'
arxiv_id: '2510.23484'
source_url: https://arxiv.org/abs/2510.23484
tags:
- learning
- t-reg
- t-regs
- length
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T-REGS introduces a simple regularization framework for self-supervised
  learning based on maximizing the length of the Minimum Spanning Tree (MST) over
  learned representations. The method combines MST length maximization with a soft
  sphere constraint to prevent dimensional collapse while promoting uniform distribution
  of embeddings.
---

# T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning

## Quick Facts
- arXiv ID: 2510.23484
- Source URL: https://arxiv.org/abs/2510.23484
- Authors: Julie Mordacq; David Loiseaux; Vicky Kalogeiton; Steve Oudot
- Reference count: 40
- T-REGS introduces MST-based regularization for SSL that achieves 66.1% Top-1 accuracy on ImageNet-1k linear evaluation

## Executive Summary
T-REGS introduces a simple regularization framework for self-supervised learning based on maximizing the length of the Minimum Spanning Tree (MST) over learned representations. The method combines MST length maximization with a soft sphere constraint to prevent dimensional collapse while promoting uniform distribution of embeddings. Theoretical analysis shows that T-REGS satisfies four key uniformity properties and provably prevents dimensional collapse on compact Riemannian manifolds. Experiments demonstrate that T-REGS effectively spreads points uniformly on the sphere and achieves competitive performance on standard SSL benchmarks including CIFAR-10/100 and ImageNet-1k, both as a standalone regularizer and as an auxiliary loss to existing methods.

## Method Summary
T-REGS regularizes self-supervised learning by maximizing the length of the Minimum Spanning Tree over learned embeddings. The method computes MST for each batch using Kruskal's algorithm, then maximizes the normalized total edge length while maintaining embeddings on the unit sphere through a soft quadratic constraint. The combined loss includes an invariance term (MSE between augmented views), MST length maximization (negative normalized), and sphere constraint. The framework works with any backbone and projector architecture, using standard SSL augmentations and training procedures with LARS optimization and cosine learning rate decay.

## Key Results
- Achieves 66.1% Top-1 accuracy on ImageNet-1k linear evaluation as standalone method
- Improves existing SSL methods (BYOL, SimSiam, VICReg, MoCo) by 0.2-0.9% when combined as auxiliary loss
- Effectively spreads points uniformly on the sphere, converging to simplex-like configurations
- Prevents dimensional collapse while maintaining competitive performance across CIFAR-10/100 and ImageNet-100 benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing MST length induces repulsive forces between edge-connected points in the embedding space.
- Mechanism: The gradient of E(MST(Z)) with respect to each point sums unit vectors pointing away from its MST neighbors (Eq. 3), creating pairwise repulsion that spreads points apart during optimization.
- Core assumption: Points are free to move within the bounded manifold; MST edges recompute dynamically as points move.
- Evidence anchors:
  - [section 3] "E(MST(X)) is differentiable almost everywhere, with derivatives given by the following simple formula: ∇_x E(MST(X)) = Σ (x,z)∈edges ∇_x ||x-z||_2"
  - [section 4.1] Theorem 4.1 proves maximum MST length is achieved at regular simplex vertices (maximally spread configuration)
  - [corpus] Corpus lacks direct mechanistic evidence for T-REGS specifically; FAMST paper addresses MST computation efficiency but not regularization dynamics
- Break condition: If the sphere constraint coefficient λ is too small relative to γ, points diverge rather than converge to uniform distribution (Figure 2b).

### Mechanism 2
- Claim: The soft sphere constraint (LS) creates a quadratic penalty that bounds embeddings while preserving gradient information about embedding norms.
- Mechanism: LS(Z) = (1/n) Σ(||z_i||_2 - 1)² grows quadratically as points leave the unit sphere, counterbalancing the linear growth potential of MST length maximization.
- Core assumption: The unit sphere provides an appropriate scale for normalized representations; soft constraints enable better optimization than hard normalization.
- Evidence anchors:
  - [abstract] "T-REGS... combines MST length maximization with a soft sphere constraint to prevent dimensional collapse while promoting uniform distribution"
  - [section 4] Eq. (5-6) define the constraint and combined loss
  - [section E.3] Table 8 shows soft constraint (91.2%) outperforms hard constraint (89.2%) on CIFAR-10
- Break condition: If λ is too large relative to γ, embeddings cluster near the sphere surface without spreading uniformly.

### Mechanism 3
- Claim: MST length maximization is asymptotically equivalent to maximizing Rényi entropy, which uniquely achieves its maximum at the uniform distribution on compact manifolds.
- Mechanism: Theorem 4.4 connects E(MST(X_n)) asymptotically to ∫f^{(d-1)/d} dμ; Proposition 4.5 shows this integral is maximized only by uniform density via strict concavity of x^p for p∈(0,1).
- Core assumption: Large-sample asymptotics (n→∞) apply; the manifold is compact Riemannian.
- Evidence anchors:
  - [section 4.1.2] Theorem 4.4 and Corollary 4.6 establish the entropy connection and uniformity guarantee
  - [section 4.2] Figure 2d shows cosine similarity distribution converges toward zero (uniform/simplex-like configuration)
  - [corpus] Corpus does not provide corroborating evidence for this specific entropy-MST connection
- Break condition: For small batch sizes where n < d+1, asymptotic results don't apply; behavior is governed by Theorem 4.1 (simplex vertices) instead.

## Foundational Learning

- Concept: **Minimum Spanning Tree (MST)**
  - Why needed here: Core data structure whose total edge length serves as the regularizer objective
  - Quick check question: Given 5 points on a sphere, can you sketch which edges would be in the MST and why removing any one edge would disconnect the graph?

- Concept: **Dimensional Collapse vs. Complete Collapse**
  - Why needed here: T-REGS specifically targets dimensional collapse (embeddings in low-dim subspace), not just complete collapse (all embeddings identical)
  - Quick check question: If your 1024-dim embeddings have rank 16 after training, is this complete collapse, dimensional collapse, or neither?

- Concept: **Rényi Entropy and Uniformity on Compact Sets**
  - Why needed here: Provides theoretical justification for why MST maximization promotes uniformity
  - Quick check question: On a compact set, which distribution maximizes Shannon entropy, and why does this matter for representation learning?

## Architecture Onboarding

- Component map:
  - Encoder (f_θ) -> Projector (h_φ) -> Embeddings -> MST Computation -> Loss Aggregation

- Critical path:
  1. Sample two augmented views per image
  2. Forward pass: image → encoder → projector → embeddings z_1, z_2
  3. Compute MST(z_1), MST(z_2) and their lengths
  4. Compute L_E = -E(MST)/n, L_S = mean((||z||-1)²), L_MSE = mean(||z_1 - z_2||²)
  5. Backpropagate total loss: β·L_MSE + γ·L_E + λ·L_S

- Design tradeoffs:
  - **Soft vs. hard sphere constraint**: Soft enables gradient flow through norms; paper shows +2% improvement over hard normalization
  - **MST algorithm**: Kruskal's O(B² log B) dominates preprocessing; GPU-parallelizable distance matrix computation mitigates cost
  - **Batch size**: Smaller batches (256-512) work well; Table 7 shows stable 66.3-68.7% across 128-1024 batch sizes

- Failure signatures:
  - **Collapse**: All embeddings converge to identical vectors → check if β ≥ γ ≥ λ violated
  - **Divergence**: Embedding norms grow unbounded → increase λ relative to γ
  - **Non-uniform clustering**: High variance in pairwise cosine similarities → verify MST computation correctness
  - **OOM on large batches**: MST distance matrix is B×B → use gradient checkpointing or reduce batch size

- First 3 experiments:
  1. **Synthetic validation**: Optimize 256 random points in R³ with only L_T-REG; confirm final distribution covers sphere uniformly (cosine similarity centered near 0)
  2. **Ablation on loss coefficients**: On ImageNet-100 with 50 epochs, sweep β∈{1,10}, γ∈{0.02,0.2,0.5}, λ∈{8e-5,8e-4,2e-3} following Table 4's protocol
  3. **Integration test**: Add T-REGS to BYOL on CIFAR-10 for 500 epochs; compare Top-1 accuracy against BYOL baseline (target: +0.5% improvement per Table 1)

## Open Questions the Paper Calls Out
No specific open questions are explicitly called out in the paper.

## Limitations
- Computational complexity O(B² log B) may limit scalability to very large batch sizes (>1024)
- Theoretical guarantees rely on asymptotic behavior that may not fully apply to finite batch sizes
- All experiments focus on natural image datasets, leaving cross-domain generalization uncertain

## Confidence

**High Confidence:**
- T-REGS effectively prevents dimensional collapse through combined MST maximization and sphere constraint
- The soft sphere constraint outperforms hard normalization in empirical results
- T-REGS achieves competitive linear evaluation accuracy on standard SSL benchmarks

**Medium Confidence:**
- The asymptotic equivalence between MST length maximization and Rényi entropy maximization
- The claim that T-REGS works effectively as an auxiliary loss across diverse SSL methods
- The robustness of results across different batch sizes (128-1024)

**Low Confidence:**
- Performance extrapolation to extremely large batch sizes (>2048)
- Generalizability to domains beyond natural images (e.g., medical imaging, text)
- Long-term stability of representations beyond 500 training epochs

## Next Checks

1. **Batch Size Sensitivity Analysis**: Systematically evaluate T-REGS performance on ImageNet-1k with batch sizes ranging from 128 to 4096, measuring both computational efficiency and representation quality. This would validate the theoretical claim about asymptotic behavior and identify practical batch size limits.

2. **Cross-Domain Generalization Test**: Apply T-REGS to a non-natural image dataset (e.g., medical imaging or satellite imagery) with ResNet-50 backbone, comparing linear evaluation accuracy against baseline SSL methods. This would test the method's domain generalizability beyond the paper's natural image focus.

3. **Dynamic Coefficient Scheduling**: Implement and evaluate a learning rate-style scheduling strategy for γ and λ coefficients, starting from conservative values and increasing over training epochs. Compare against static coefficients to determine if adaptive regularization improves final representation quality.