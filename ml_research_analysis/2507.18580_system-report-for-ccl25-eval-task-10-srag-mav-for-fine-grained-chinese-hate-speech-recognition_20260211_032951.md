---
ver: rpa2
title: 'System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate
  Speech Recognition'
arxiv_id: '2507.18580'
source_url: https://arxiv.org/abs/2507.18580
tags:
- score
- hate
- speech
- task
- triplet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SRAG-MAV, a system for fine-grained Chinese
  hate speech recognition that integrates task reformulation, self-retrieval-augmented
  generation, and multi-round accumulative voting. The approach reformulates quadruplet
  extraction into triplet extraction, leverages the training set as a retrieval corpus
  to enhance contextual understanding, and employs iterative inference with voting
  to stabilize outputs.
---

# System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition

## Quick Facts
- arXiv ID: 2507.18580
- Source URL: https://arxiv.org/abs/2507.18580
- Reference count: 5
- Primary result: SRAG-MAV achieves 37.505 Average Score on STATE ToxiCN, outperforming GPT-4o (15.63) and fine-tuned Qwen2.5-7B (35.365)

## Executive Summary
SRAG-MAV is a system for fine-grained Chinese hate speech recognition that reformulates quadruplet extraction into triplet extraction, leverages self-retrieval-augmented generation using the training corpus, and employs multi-round accumulative voting to stabilize outputs. Evaluated on the STATE ToxiCN dataset, the system achieves state-of-the-art performance with a Hard Score of 26.66, Soft Score of 48.35, and Average Score of 37.505. The approach demonstrates strong performance in structured hate speech detection while maintaining computational efficiency through task reformulation.

## Method Summary
SRAG-MAV integrates three key innovations: Task Reformulation reduces quadruplet extraction to triplet extraction by inferring hatefulness from target group labels; Self-Retrieval-Augmented Generation retrieves semantically similar training examples using bge-large-zh-v1.5 embeddings and concatenates them with inputs for few-shot prompting; Multi-Round Accumulative Voting performs iterative inference across diverse retrieved prompts with frequency-based voting until a threshold is reached. The system uses a fine-tuned Qwen2.5-7B model with vLLM inference, requiring up to 20 forward passes per input at τ=200.

## Key Results
- Hard Score: 26.66 (exact quadruplet match F1)
- Soft Score: 48.35 (partial match F1 with >50% similarity)
- Average Score: 37.505 (mean of Hard and Soft)
- Outperforms GPT-4o baseline (15.63) and fine-tuned Qwen2.5-7B baseline (35.365)
- Component gains: Task Reformulation (+0.475), SRAG (+0.735), MAV (+0.93)

## Why This Works (Mechanism)

### Mechanism 1: Task Reformulation (TR)
Reducing quadruplet extraction to triplet extraction lowers structured generation complexity while preserving output integrity. The authors identified a deterministic dependency in the training data—hatefulness labels can be inferred from target groups ("no-hate" target group → "no-hate" hatefulness; otherwise → "hate"). This contracts the model's output space and improves precision.

### Mechanism 2: Self-Retrieval-Augmented Generation (SRAG)
Retrieving semantically similar training examples as in-context demonstrations improves triplet generation accuracy without external knowledge sources. The training corpus is encoded using bge-large-zh-v1.5 embeddings, and top-k similar samples are retrieved and concatenated with the input to form few-shot prompts.

### Mechanism 3: Multi-Round Accumulative Voting (MAV)
Iterative inference across diverse retrieved prompts with frequency-based voting stabilizes outputs and improves precision. For each input, k distinct prompts are constructed using different retrieved examples, and the model generates predictions iteratively until the most frequent triplet exceeds threshold τ=200.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: SRAG adapts RAG principles but uses the training set as the retrieval corpus rather than external knowledge bases
  - Quick check question: Given a hate speech input, can you identify what type of external knowledge would help classify it, versus what in-distribution examples would provide?

- **Concept: Structured Information Extraction**
  - Why needed here: The task requires extracting quadruplets (Target, Argument, Targeted Group, Hateful) from text—not just classification
  - Quick check question: For the sentence "These [immigrants] are destroying our [neighborhood]," can you identify Target, Argument, Targeted Group, and Hateful?

- **Concept: Ensemble/Voting Methods**
  - Why needed here: MAV is essentially an inference-time ensemble where diverse prompts create varied model perspectives
  - Quick check question: Why would running the same model with different prompts produce different outputs, and when would voting fail to improve results?

## Architecture Onboarding

### Component map:
Input Text → Embedding Encoder (bge-large-zh-v1.5) → Vector Database (training corpus) → Top-k Retrieval (k=10 similar samples) → Prompt Construction (k prompts) → Fine-tuned Qwen2.5-7B → Triplet Predictions → Accumulative Voting (until threshold τ) → Triplet → Quadruplet Conversion (rule-based) → Final Output

### Critical path:
1. Training phase: Build vector index from training set; fine-tune Qwen2.5-7B on triplet task with retrieved exemplars in prompts
2. Inference phase: Retrieve top-k → construct k prompts → iterate predictions → accumulate frequencies → output when threshold met → convert to quadruplet
3. The voting loop is the latency bottleneck—each iteration requires model inference

### Design tradeoffs:
- k (retrieval count): Higher k increases prompt diversity but also memory/compute. Paper uses k=10
- τ (voting threshold): Higher τ improves precision but linearly increases inference time. τ=200 requires ~20× compute vs. τ=1
- Embedding model choice: bge-large-zh-v1.5 is Chinese-optimized; swapping models requires re-indexing the full training corpus
- Base model size: 7B chosen for efficiency; larger models may reduce need for MAV's stabilization

### Failure signatures:
- Low Hard Score with acceptable Soft Score: Model captures coarse structure but fails on precise span boundaries—may need improved span extraction training
- Voting never converges (infinite loop): Set maximum iteration cap; may indicate ambiguous or out-of-distribution input
- Retrieved examples are irrelevant: Check embedding quality; verify corpus preprocessing; consider hybrid retrieval (keyword + semantic)
- Quadruplet conversion errors: If target group prediction is wrong, inferred hatefulness will also be wrong—trace back to triplet quality

### First 3 experiments:
1. **Baseline reproduction**: Fine-tune Qwen2.5-7B on triplet task without SRAG or MAV (τ=1, k=1). Verify your setup matches reported 35.365 Average Score
2. **Ablation by component**: Add TR → TR+SRAG → TR+SRAG+MAV sequentially. Confirm each step's contribution matches Table 2 (+0.475 → +0.735 → +0.93)
3. **Threshold sweep**: With k=10 fixed, test τ ∈ {1, 5, 20, 50, 100, 200}. Plot Hard/Soft/Average scores to validate the tradeoff curve and identify your optimal operating point for available compute

## Open Questions the Paper Calls Out

- **Cross-domain transfer learning**: Can SRAG-MAV maintain its performance advantage when transferred to hate speech detection in other domains (e.g., news comments, forums) or languages beyond Chinese social media? The conclusion states future work will explore this, as the model's domain-specific performance is listed as a limitation.

- **Computational efficiency optimization**: How can MAV's voting threshold mechanism be optimized to reduce computational costs while preserving accuracy gains? The conclusion identifies that MAV's high voting thresholds increase computational costs and lists optimization of MAV's computational efficiency as future work.

- **Task reformulation generalization**: Does the Task Reformulation assumption—that hatefulness can be deterministically inferred from target group—generalize to datasets where this correlation is weaker? The paper does not validate this correlation on external datasets or analyze cases where the pattern might break.

## Limitations

- The Task Reformulation assumption may not generalize beyond STATE ToxiCN dataset where the target-group-to-hatefulness correlation was observed
- SRAG's retrieval quality depends on embedding similarity aligning with structural similarity, which lacks quantitative validation
- MAV's voting mechanism introduces substantial computational overhead with linear scaling in latency

## Confidence

- **High Confidence**: Empirical results on STATE ToxiCN dataset are well-documented and reproducible
- **Medium Confidence**: Individual component effectiveness is supported by ablation studies but only on single dataset
- **Low Confidence**: Generalizability of target-group-to-hatefulness inference rule and SRAG retrieval effectiveness beyond STATE ToxiCN remains unverified

## Next Checks

1. **External Dataset Validation**: Evaluate SRAG-MAV on at least two additional Chinese hate speech datasets to test whether the target-group-to-hatefulness correlation and SRAG retrieval effectiveness generalize beyond STATE ToxiCN

2. **Component Ablation Under Distribution Shift**: Systematically introduce controlled perturbations to test data (sarcastic texts, domain-shifted social media platforms, different Chinese dialects) and measure how TR, SRAG, and MAV individually degrade

3. **Computational Efficiency Analysis**: Benchmark SRAG-MAV against alternative ensemble methods while measuring inference latency, memory usage, and performance trade-offs to identify optimal τ and k values for different deployment scenarios