---
ver: rpa2
title: Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time
arxiv_id: '2505.03668'
source_url: https://arxiv.org/abs/2505.03668
tags:
- macro-actions
- learning
- heuristics
- planning
- pomdp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning persistent macro-actions
  using Inductive Logic Programming (ILP) to improve Monte Carlo Tree Search (MCTS)-based
  solvers for Partially Observable Markov Decision Processes (POMDPs). The approach
  leverages Event Calculus in Answer Set Programming to generate temporally-aware
  macro-actions from execution traces, which guide planners like POMCP and DESPOT.
---

# Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time

## Quick Facts
- arXiv ID: 2505.03668
- Source URL: https://arxiv.org/abs/2505.03668
- Reference count: 18
- One-line primary result: Symbolic macro-actions learned via ILP improve MCTS-based POMDP solvers in Rocksample and Pocman domains

## Executive Summary
This paper presents a method for learning persistent macro-actions using Inductive Logic Programming (ILP) to enhance Monte Carlo Tree Search (MCTS)-based solvers for Partially Observable Markov Decision Processes (POMDPs). The approach leverages Event Calculus in Answer Set Programming to generate temporally-aware macro-actions from execution traces, which guide planners like POMCP and DESPOT. Experiments on Rocksample and Pocman domains show that the learned macro-actions outperform time-independent heuristics in terms of both computational efficiency and performance, especially in challenging settings.

## Method Summary
The method generates execution traces from POMCP runs in simple scenarios, extracts repeated-action sequences, and creates Context-Dependent Partial Interpretations (CDPIs) with init/contd atoms. These are used to learn Event Calculus theories via ILASP. The learned macro-actions are integrated into POMCP/DESPOT by biasing UCT initialization (N(ha)=10) and rollout sampling with coverage-weighted probabilities. The approach assumes prior knowledge of feature maps $F_F$ and $F_A$ that convert belief states and actions into logical predicates.

## Key Results
- Learned macro-actions significantly outperform time-independent heuristics in Rocksample and Pocman domains
- The method demonstrates superior generalization to larger, more complex scenarios (e.g., training on 12x12 generalizing to 17x19 grids)
- Computational efficiency is improved through reduced search depth in MCTS while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Persistent macro-actions reduce the effective search depth in Monte Carlo Tree Search (MCTS) by committing to action sequences that satisfy temporal logic conditions.
- **Mechanism:** By using Event Calculus (EC) to define `init` (start) and `contd` (continue) conditions, the planner treats a sequence of identical actions as a single extended step. This bypasses the need to re-evaluate the branching factor at every individual time step for sustained behaviors (e.g., moving in a straight line).
- **Core assumption:** The optimal policy contains segments where the same action is repeated over multiple time steps, and the state features required to validate this persistence are observable or derivable from the belief state.
- **Evidence anchors:**
  - [Abstract] Mentions generating "persistent (i.e., constant) macro-actions... over a time horizon."
  - [Section 4.2] Describes isolating traces where the action does not change to generate `contd` examples.
  - [Corpus] "Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific Temporal Planning Guidance" supports the general utility of symbolic guidance in temporal planning.
- **Break condition:** If the environment dynamics require frequent action switching (e.g., a maze with turns every 2 steps), the "persistent" assumption fails, and the overhead of checking EC conditions may outweigh the search reduction benefits.

### Mechanism 2
- **Claim:** Symbolic rule learning via Inductive Logic Programming (ILP) provides data-efficient generalization compared to neural approaches, specifically for state abstraction.
- **Mechanism:** ILP (specifically ILASP) learns logical axioms from sparse positive examples (execution traces). These rules map raw belief features (e.g., distance to rock) to abstract action conditions. Because the rules are relational (e.g., `dist(R, D)`), they inherently generalize to unseen instances (e.g., more rocks) without retraining.
- **Core assumption:** A minimal set of domain features (the "alphabet") defined in the POMDP transition model is sufficient to describe high-value behaviors; complex "common sense" concepts are not required.
- **Evidence anchors:**
  - [Abstract] Claims the method "generalizes well to larger, more complex scenarios" and is data-efficient.
  - [Section 5.2] Shows training on a 12x12 grid with 4 rocks generalizes to a 17x19 grid with 8 rocks (in Pocman/Rocksample).
  - [Corpus] "Neuro-Symbolic Contrastive Learning" highlights the general inference power of ILP over shallow heuristics.
- **Break condition:** If the provided feature map $F_F$ is incomplete (e.g., missing a crucial sensor reading), ILP will fail to find a consistent hypothesis, unlike a neural network which might fabricate a heuristic from raw data.

### Mechanism 3
- **Claim:** Soft-biasing the rollout policy preserves the theoretical optimality guarantees of POMCP while accelerating empirical convergence.
- **Mechanism:** Rather than forcing the macro-action, the architecture modifies the action selection probability $\rho$ based on the "coverage ratio" (confidence) of the learned rules. This steers the random rollouts toward promising subtrees while keeping the exploration probability non-zero ($\rho_a \neq 0$).
- **Core assumption:** The default UCT exploration strategy is too slow to converge, and a learned symbolic bias, even if imperfect, correlates with higher value regions of the search tree.
- **Evidence anchors:**
  - [Section 4.3] Explicitly states $\rho_a = cov_a$ if $a$ is in the macro-action set, preserving optimality guarantees.
  - [Page 2] "Biasing MCTS exploration... was proven more robust to bad heuristics" than reward shaping.
- **Break condition:** If the learned heuristic is consistently adversarial (suggesting worst actions) and the simulation budget is extremely low, the bias could mislead the limited search, though the paper claims robustness.

## Foundational Learning

- **Concept: POMDPs and Belief States**
  - **Why needed here:** The method operates on *belief-action* traces, not ground truth states. You must understand that the agent acts under uncertainty (e.g., "probability rock is valuable").
  - **Quick check question:** Can you explain how a particle filter approximates a belief distribution in POMCP?

- **Concept: Monte Carlo Tree Search (MCTS) & UCT**
  - **Why needed here:** The macro-actions modify the UCT value formula and the rollout phase. Understanding the exploration-exploitation trade-off is vital to seeing why "soft biasing" helps.
  - **Quick check question:** How does the UCT formula balance the average reward of a node against the number of times it has been visited?

- **Concept: Event Calculus (EC)**
  - **Why needed here:** The paper uses a specific fragment of EC (`init`/`contd`) to model time. This is the structural format of the learned rules.
  - **Quick check question:** In Event Calculus, what is the difference between an event initiating a fluent and a fluent holding by inertia?

## Architecture Onboarding

- **Component map:** Trace Generator -> Feature Mapper ($F_F, F_A$) -> ILASP Learner -> Planner Wrapper
- **Critical path:** The definition of the **Feature Map ($F_F$)**. The paper explicitly assumes this is known (Assumption 1). If your predicates do not capture the necessary spatial or temporal relations (e.g., `delta_x`), the learner will fail.
- **Design tradeoffs:**
  - **Interpretability vs. Complexity:** The system uses a "fragment" of LTL/EC to keep learning tractable. Full LTL might be too slow or require too much data.
  - **Macro-action length:** The method sorts by length in DESPOT to maximize horizon reduction. Longer macros are more efficient but riskier if the environment changes unexpectedly.
- **Failure signatures:**
  - **"Trivial" Macro-actions:** If the learned rules result in $|M_a| = 1$ (single-step actions), the temporal extension failed. Check the `contd` examples for noise.
  - **ASP Grounding Explosion:** If the feature map produces too many ground terms (e.g., continuous distances not discretized), the ASP solver will time out during online planning.
- **First 3 experiments:**
  1. **Sanity Check (Overfitting):** Train on a tiny $4 \times 4$ grid. Does the agent learn the specific optimal path? (Verifies the pipeline works).
  2. **Generalization Test:** Train on $N=11$, Test on $N=15$. Compare "Timed Heuristic" vs. "Local Heuristic" (time-independent). Look for performance cliffs.
  3. **Ablation on Coverage:** Set the weighting constant $N_{max}$ (simulations assigned to heuristic actions) to 0 (pure random) vs. $\infty$ (greedy). Plot the discount return vs. simulation count to find the sweet spot for soft-biasing.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the learning framework be effectively extended to handle full Linear Temporal Logic (LTL) specifications rather than the current restricted fragment based on Event Calculus?
- **Basis in paper:** [explicit] The conclusion states the intent to "extend the learning process to even more complex logical representations, such as LTL."
- **Why unresolved:** The current methodology is limited to generating *persistent* macro-actions (constant actions) using a specific Event Calculus fragment, which may not capture more complex, non-constant temporal dependencies.
- **What evidence would resolve it:** Successful derivation and application of non-persistent LTL-based macro-actions in the Rocksample or Pocman domains showing improved performance over the EC-based approach.

### Open Question 2
- **Question:** Does the symbolic macro-action approach maintain its computational efficiency and interpretability when applied to complex, physical robotics scenarios with continuous state spaces?
- **Basis in paper:** [explicit] The authors list plans to "validate our methodology in more challenging domains, e.g., robotics" and to account for "continuous domains."
- **Why unresolved:** The current validation is restricted to standard benchmark simulations (Rocksample, Pocman) which may not reflect the noise, continuous variables, or hardware constraints of real-world robotics.
- **What evidence would resolve it:** Demonstration of the method guiding a robotic platform (e.g., navigation or manipulation) in real-time with analysis of the learned rules' utility.

### Open Question 3
- **Question:** Can the reliance on priorly defined feature maps ($F_F$) be relaxed by integrating automatic symbol grounding without significantly degrading the quality of the learned heuristics?
- **Basis in paper:** [inferred] Assumption 1 states that feature maps must be "priorly known," though the text notes they could be "learned separately."
- **Why unresolved:** The paper relies on hand-crafted features derived from the transition model; the impact of learning these features simultaneously or sequentially within the pipeline is unknown.
- **What evidence would resolve it:** Experiments comparing the performance of heuristics learned from hand-crafted features versus those learned from automatically grounded symbols in a complex domain.

## Limitations
- The performance gain heavily depends on the quality of the feature map $F_F$, which is assumed to be known and correctly designed by domain experts. The paper does not address how to automatically derive or validate this feature map for new domains.
- The method's robustness to noisy or suboptimal training traces is not thoroughly evaluated. While the paper claims robustness, the ablation studies focus primarily on simulation count rather than trace quality.
- The computational overhead of grounding the Event Calculus theories during planning is not quantified. For domains with continuous state spaces, discretization could lead to combinatorial explosion in ASP grounding.

## Confidence
- **High confidence**: The general architecture of learning persistent macro-actions via ILP and integrating them with MCTS is sound and well-supported by the experimental results.
- **Medium confidence**: The claim of superior generalization to larger scenarios is supported but based on a limited set of domain variations (Rocksample and Pocman).
- **Medium confidence**: The assertion that symbolic rules are more data-efficient than neural approaches is plausible but not directly compared in the paper.

## Next Checks
1. **Feature Map Robustness Test**: Systematically degrade the feature map (e.g., remove predicates, add noise) and measure the impact on learned macro-action quality and planning performance.
2. **Training Trace Quality Analysis**: Compare performance when training on traces from optimal vs. heuristic policies to quantify sensitivity to training data quality.
3. **Computational Overhead Benchmark**: Measure the ASP grounding and solving time across varying discretization granularities and domain sizes to establish practical limits.