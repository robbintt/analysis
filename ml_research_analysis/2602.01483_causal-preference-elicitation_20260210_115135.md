---
ver: rpa2
title: Causal Preference Elicitation
arxiv_id: '2602.01483'
source_url: https://arxiv.org/abs/2602.01483
tags:
- posterior
- expert
- causal
- edge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic framework for causal preference
  elicitation (CaPE), where expert knowledge about local edge relations in a causal
  graph is actively queried and integrated into a Bayesian posterior over directed
  acyclic graphs (DAGs). The method models expert judgments as noisy three-way categorical
  responses using a hierarchical logistic likelihood, and selects queries by maximizing
  expected information gain (EIG) under the posterior.
---

# Causal Preference Elicitation

## Quick Facts
- arXiv ID: 2602.01483
- Source URL: https://arxiv.org/abs/2602.01483
- Reference count: 40
- One-line primary result: CaPE reduces SHD by 14 edges and improves orientation F1 by 0.38 after 40 queries on the Sachs protein signaling dataset.

## Executive Summary
This paper introduces a probabilistic framework for causal preference elicitation (CaPE) where expert knowledge about local edge relations in a causal graph is actively queried and integrated into a Bayesian posterior over DAGs. The method models expert judgments as noisy three-way categorical responses using a hierarchical logistic likelihood, and selects queries by maximizing expected information gain (EIG) under the posterior. Experiments on synthetic graphs, the Sachs protein signaling dataset, and a human gene perturbation benchmark show that CaPE accelerates posterior concentration and improves structural recovery under tight query budgets.

## Method Summary
CaPE maintains a weighted particle representation of the posterior over DAGs and iteratively updates it via expert queries about edge existence and orientation. At each round, it screens candidate variable pairs by edge-existence uncertainty, selects the pair maximizing EIG (BALD), queries the expert, and updates particle weights via a hierarchical logistic likelihood. When effective sample size drops, it resamples and rejuvenates particles with acyclicity-constrained MCMC moves. The approach is demonstrated on Sachs (11 proteins, 40 queries) and a human gene perturbation benchmark (35 genes, 200 queries), showing substantial gains in structural recovery over random and uncertainty baselines.

## Key Results
- On Sachs dataset, CaPE reduces SHD by 14 edges and improves orientation F1 by 0.38 after 40 queries compared to initial posterior.
- On a human gene perturbation benchmark (35 genes), CaPE achieves higher Top-K precision and AUPRC than uncertainty and random baselines within 200 queries.
- Proposition 7.1 shows EIG = Σy p̂_ij^t(y) KL(qt(W|Yij=y) || qt(W)), linking query selection to mutual information.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Expert judgments act as likelihood factors that concentrate posterior mass on the true DAG under non-adversarial feedback.
- **Mechanism:** Each expert response updates particle weights via the three-way hierarchical logistic likelihood pθ(Yij|W). The likelihood factors favor graphs consistent with expert input, driving posterior odds of incorrect graphs toward zero across iterations.
- **Core assumption:** The expert assigns strictly higher probability to the correct edge/orientation than to incorrect alternatives (non-adversarial condition).
- **Evidence anchors:**
  - [abstract] "experiments...show faster posterior concentration and improved recovery of directed effects"
  - [section 5.1] Bayesian update equation: qt(W) ∝ qt−1(W) pθ(Y(t)|W)
  - [section D.4] Theorem D.1 proves almost-sure convergence to W⋆ under infinite querying of ambiguous edges
- **Break condition:** If expert is adversarial or systematically biased (assigns higher probability to wrong orientations), posterior may concentrate on incorrect graphs.

### Mechanism 2
- **Claim:** Expected Information Gain (EIG) query selection maximizes expected posterior change, accelerating convergence relative to random or uncertainty sampling.
- **Mechanism:** EIG computes mutual information between current DAG posterior and expert response via BALD decomposition: EIG = H(Yij) − EW[H(Yij|W)]. It prioritizes pairs where candidate graphs make conflicting predictions (high epistemic uncertainty) but the expert model is confident for any fixed graph (low aleatoric uncertainty).
- **Core assumption:** Information-theoretic acquisition identifies queries with greatest global structural impact, not merely local edge uncertainty.
- **Evidence anchors:**
  - [section 6.2] EIGt(i,j) = H_ij^t − Σs wt^(s) H(pθ(Yij|W(s)))
  - [section 7.1] Proposition 7.1: EIG = Σy p̂_ij^t(y) KL(qt(W|Yij=y) || qt(W))
  - [corpus] Weak direct corpus evidence for BALD specifically in causal discovery; related work (Tong & Koller, 2001) uses information gain for interventions, not expert queries
- **Break condition:** If all particles agree on edge predictions (zero epistemic uncertainty), EIG provides no signal; falls back to uncertainty sampling behavior.

### Mechanism 3
- **Claim:** Particle rejuvenation with acyclicity-constrained MCMC maintains diversity while preserving valid DAG structure.
- **Mechanism:** After ESS-triggered resampling, each particle undergoes Metropolis-Hastings proposals (edge add/delete/flip). Proposals violating acyclicity are rejected immediately; accepted moves follow MH ratio under current posterior.
- **Core assumption:** Multi-modal DAG posteriors require diverse particles; pure resampling causes collapse.
- **Evidence anchors:**
  - [section 5.4] "This leaves qt invariant and restores diversity among particles as the posterior concentrates"
  - [algorithm 6] Explicit acyclicity rejection in proposal kernel
  - [corpus] No direct corpus comparison for rejuvenation in expert-in-the-loop causal discovery
- **Break condition:** If rejuvenation steps are too few or proposals too aggressive, particle diversity degrades; if too many, computational cost dominates.

## Foundational Learning

- **Concept: Bayesian Structure Learning over DAGs**
  - Why needed here: CaPE assumes access to samples from an initial observational posterior q0(W|X); understanding how posteriors over DAGs represent uncertainty is foundational.
  - Quick check question: Can you explain why two DAGs may be Markov equivalent and how this affects identifiability from observational data alone?

- **Concept: Sequential Monte Carlo (Particle Filtering)**
  - Why needed here: The method uses importance reweighting, ESS-based resampling, and MCMC rejuvenation—all SMC concepts.
  - Quick check question: What happens to particle weights if the likelihood becomes very peaked, and how does resampling address this?

- **Concept: Mutual Information and BALD**
  - Why needed here: Query selection relies on computing mutual information between the DAG posterior and expert response variable.
  - Quick check question: In the BALD formulation, what does the difference H(Y) − EW[H(Y|W)] represent in terms of uncertainty types?

## Architecture Onboarding

- **Component map:** Particle posterior store -> Candidate screener -> EIG evaluator -> Expert query interface -> Bayesian updater -> Resampler -> Rejuvenator

- **Critical path:** EIG computation dominates per-round cost at O(Sk); screening at O(SD²) is mitigated by aggressive k ≪ D². Memory is O(SD²) for storing particle graphs.

- **Design tradeoffs:**
  - Particle count S: Higher S improves posterior approximation but linearly increases compute
  - Screening budget k: Smaller k reduces EIG cost but may miss informative edges
  - Expert reliability (β_edge, β_dir): Higher values assume sharper expert; overconfidence harms calibration if expert is noisy

- **Failure signatures:**
  - ESS collapses to near-zero repeatedly → increase rejuvenation steps or reduce expert overconfidence
  - SHD plateaus early despite queries → check if screening excludes distinguishing edges
  - Cyclic graphs appear → verify acyclicity check in rejuvenation proposals

- **First 3 experiments:**
  1. **Synthetic sanity check:** D=20 Erdős–Rényi graph with known ground truth; verify posterior entropy decreases and ETCP increases with queries. Compare EIG vs Random vs Uncertainty policies (replicate Figure 1).
  2. **Sachs observational subset:** D=11 proteins, S=500 particles, T=40 queries; target ∼14-edge SHD reduction and orientation F1 improvement >0.35 vs initial posterior (Table 3).
  3. **Ablation on expert reliability:** Vary β_edge, β_dir to simulate noisy expert; confirm degradation is graceful (no catastrophic divergence) and that EIG policy remains superior to UNC under moderate noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to handle latent confounders, cyclic mechanisms, or partially observed systems?
- Basis in paper: [explicit] The authors list "settings with latent variables, cyclic causal mechanisms, or partially observed systems" as a primary avenue for future work to broaden applicability (Section 9).
- Why unresolved: The current method strictly enforces acyclicity and assumes causal sufficiency, constraints that do not hold in many complex biological or physical systems where feedback loops exist.
- What evidence would resolve it: A reformulation of the particle representation and likelihood to support acyclic directed mixed graphs (ADMGs) or cyclic graphs, demonstrated on synthetic data with feedback loops.

### Open Question 2
- Question: How do cognitive load and query interpretability affect performance when deploying CaPE with real human experts?
- Basis in paper: [explicit] The conclusion states that deploying with real humans "raises important questions about cognitive load, query interpretability, and human-AI interaction" (Section 9).
- Why unresolved: All experiments utilized simulated experts (ground-truth graphs with synthetic noise), whereas human experts exhibit fatigue, bias, and varying reliability that may not match the hierarchical logistic model.
- What evidence would resolve it: A user study with domain scientists measuring mental demand (e.g., NASA-TLX scores) and performance accuracy relative to the simulated baselines.

### Open Question 3
- Question: Can richer expert models that incorporate context-dependent reliability improve the efficiency of elicitation?
- Basis in paper: [explicit] The authors suggest "richer expert models could incorporate additional structural features or context-dependent reliability" to better represent domain knowledge (Section 9).
- Why unresolved: The current model uses fixed hyperparameters ($\beta_{edge}, \beta_{dir}$) for reliability, assuming uniform expertise across all variable pairs, which is often unrealistic for multidisciplinary domains.
- What evidence would resolve it: Experiments comparing static reliability parameters against an adaptive model that infers specific reliability scores for different sub-sections of the graph.

## Limitations

- Expert feedback model assumes three-way categorical judgments and may not capture continuous edge weights or richer relational feedback.
- Query efficiency under noisy experts is untested; all experiments assume near-perfect expert feedback.
- Particle rejuvenation uses only 2 MH steps; insufficient diversity preservation in multi-modal posteriors is not evaluated.
- Computational cost O(S·D² + S·k) limits application to D≫20 graphs without aggressive screening.

## Confidence

- **High confidence:** Core Bayesian update mechanism (Mechanism 1) and particle rejuvenation (Mechanism 3) are well-specified and empirically validated.
- **Medium confidence:** EIG acquisition (Mechanism 2) is theoretically grounded but lacks direct corpus evidence for BALD in expert-in-the-loop causal discovery.
- **Low confidence:** Generalization to noisy experts and very large DAGs is not demonstrated.

## Next Checks

1. **Noisy expert robustness:** Repeat Sachs experiment with β_edge=β_dir ∈ {1,5,10}, confirm that CaPE remains superior to baselines under moderate noise but degrades gracefully.
2. **Scalability test:** Apply CaPE to D=50 synthetic graphs; measure runtime and recovery quality vs. baseline methods; verify screening+k approximation holds.
3. **Alternative query policies:** Implement uncertainty sampling and random baselines; run head-to-head comparisons on ETCP and SHD improvement per query.