---
ver: rpa2
title: The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination
arxiv_id: '2505.20177'
source_url: https://arxiv.org/abs/2505.20177
tags:
- learning
- sfilt
- sinp
- scln
- contamination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of supervised learning from datasets
  contaminated with adversarial noise, both in the input space and labels. The authors
  propose a general iterative polynomial filtering algorithm that removes outliers
  from the dataset while preserving low-degree polynomial expectations, applicable
  to any hypercontractive distribution.
---

# The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination

## Quick Facts
- arXiv ID: 2505.20177
- Source URL: https://arxiv.org/abs/2505.20177
- Authors: Adam R. Klivans; Konstantinos Stavropoulos; Kevin Tian; Arsen Vasilyan
- Reference count: 40
- Primary result: General iterative polynomial filtering algorithm removes adversarial outliers while preserving low-degree polynomial expectations under hypercontractive distributions

## Executive Summary
This paper addresses supervised learning from datasets contaminated with adversarial noise in both inputs and labels. The authors propose an iterative polynomial filtering algorithm that removes outliers while preserving statistical properties of clean data. The framework applies to any hypercontractive distribution and resolves longstanding gaps between agnostic and contamination learning, showing that polynomial approximation implies efficient learning under bounded contamination. The work also establishes that sandwiching approximation enables learning under heavy contamination where the adversary controls more than half the dataset.

## Method Summary
The core method uses iterative polynomial filtering based on low-degree polynomials whose absolute expectations are small under the target distribution. The algorithm compares empirical averages of these polynomials between the contaminated input set and a clean reference set, iteratively identifying and removing points that cause large polynomial values. For bounded contamination, the filtered dataset maintains good approximation properties allowing standard L1 polynomial regression to achieve error 2η+ε. For heavy contamination, stronger sandwiching polynomials that bound the target function everywhere enable learning even when most data points are adversarial.

## Key Results
- Polynomial approximation implies efficient bounded contamination learning, resolving a gap between agnostic and contamination learning
- Sandwiching approximation enables efficient heavy contamination learning where majority of dataset can be corrupted
- First efficient algorithms for tolerant testable learning of functions of halfspaces under log-concave distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative filtering removes outliers while preserving statistical properties by constraining low-degree polynomial expectations
- Mechanism: Algorithm compares empirical averages of low-degree polynomials over input and clean reference sets, iteratively removing points causing large polynomial values
- Core assumption: Target distribution D* is A-hypercontractive (polynomials have controlled tail behavior)
- Evidence anchors: [abstract], [page 8, Theorem 3.2]
- Break condition: Non-hypercontractive distributions cause concentration bounds to fail

### Mechanism 2
- Claim: Low-degree L2 polynomial approximators are sufficient for learning with bounded contamination
- Mechanism: Filtering ensures expectation of (p*(x))²-1 remains small, limiting L1 error contribution of remaining outliers
- Core assumption: Concept class has low L2-approximate degree ℓ with respect to D*
- Evidence anchors: [page 1], [page 11, Theorem 4.2]
- Break condition: Heavy contamination makes simple L2 approximation insufficient

### Mechanism 3
- Claim: Sandwiching polynomial approximators enable learning under heavy contamination
- Mechanism: Algorithm requires polynomials p_down, p_up that sandwich target function everywhere, with filter tuned to preserve clean points
- Core assumption: Concept class admits low-degree sandwiching approximators
- Evidence anchors: [page 1], [page 15, Theorem 4.4]
- Break condition: Classes without efficient sandwiching polynomials require exponential sample complexity

## Foundational Learning

- **Hypercontractivity**: Controls higher moments of polynomials under target distribution; enables filter to distinguish signal from noise
  - Quick check: Are polynomial moments bounded by lower moments (e.g., Gaussian, log-concave distributions)?

- **Sandwiching vs. Approximating Polynomials**: Sandwiching requires polynomial to bound function everywhere, while approximation minimizes average error
  - Quick check: Does polynomial fit function well on average (L2) or strictly bound values at every point?

- **Bounded vs. Heavy Contamination Models**: BC limits adversary to replacing fraction η, while HC allows unlimited data addition (ratio Q)
  - Quick check: Is malicious data volume limited to fraction of total dataset or can it vastly outnumber clean data?

## Architecture Onboarding

- **Component map**: Input S_inp and S_ref -> Iterative Polynomial Filtering (Algorithm 1) -> L1 Regression on S_filt -> Hypothesis
- **Critical path**: Hyperparameter R in Algorithm 1
  - BC-Learning: R≈2 allows moderate removal
  - HC-Learning: R∝Q (larger R makes filter stricter)
- **Design tradeoffs**:
  - Generality vs. Efficiency: Heavy contamination requires strong sandwiching assumptions, excluding classes like Monotone Functions
  - Sample Complexity vs. Robustness: Higher Q forces higher reference set requirements
- **Failure signatures**:
  - Catastrophic Removal: Filter empties dataset (R too small or wrong D*)
  - No Convergence: Iterative filter loops indefinitely (violated degree constraints or non-hypercontractive distribution)
- **First 3 experiments**:
  1. Sanity Check (BC): Train halfspace intersections on Gaussian data with η=0.1 noise, verify error≈2η
  2. Stress Test (HC): Inject Q=5 contamination into decision tree of halfspaces, tune R to scale with Q, check >80% clean points retained
  3. Lower Bound Verification: Attempt to learn Monotone Functions under HC, confirm exponential scaling (2^Ω(d))

## Open Questions the Paper Calls Out

- **Open Question 1**: Can iterative filtering algorithms work universally for broad classes of distributions without requiring unlabeled samples from D*?
  - Basis: Authors ask whether D* samples can be relaxed for algorithms working universally
  - Evidence needed: Algorithm achieving stated guarantees for any distribution in broad family without target-specific unlabeled samples

- **Open Question 2**: Is existence of low-degree L1 approximating polynomials sufficient for efficient BC-learning?
  - Basis: Paper notes L1 approximation sufficient for agnostic learning but requires L2 for BC-learning
  - Evidence needed: Proof showing L1 approximators suffice for BC-learning or hardness result demonstrating impossibility

- **Open Question 3**: Can monotone functions be learned in time 2^(Õ_ε(√d)) under realizable heavy contamination?
  - Basis: Authors provide lower bound for general heavy contamination but ask about realizable case
  - Evidence needed: Algorithm for learning monotone functions under realizable heavy contamination with runtime 2^(Õ_ε(√d))

## Limitations
- Numerical stability of convex programs for high-degree polynomials is critical but not extensively validated
- Framework's reliance on hypercontractivity assumptions may limit applicability to distributions with heavy tails
- Sandwiching approximation requirement creates inherent limitation for natural concept classes like monotone functions

## Confidence
- **High Confidence**: Polynomial approximation for bounded contamination, iterative filtering algorithm structure
- **Medium Confidence**: Heavy contamination results dependent on sandwiching approximation assumption
- **Low Confidence**: Numerical implementation details for iterative filtering algorithm (solver algorithms, hyperparameter tuning)

## Next Checks
1. **Numerical Stability Test**: Implement Algorithm 1 with convex solver on synthetic data with varying dimensions (d=10, 50, 100) and degrees (ℓ=2, 4, 6), measuring convergence rates and solution quality
2. **Distribution Assumption Validation**: Test algorithm on distributions approaching hypercontractivity boundaries (sub-exponential with varying tail decay), quantifying breakdown point
3. **Sandwiching Necessity Demonstration**: Attempt to learn monotone functions under heavy contamination, verifying exponential scaling with dimension confirms theoretical lower bound