---
ver: rpa2
title: 'ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models
  with Limited GPU Memory'
arxiv_id: '2503.12668'
source_url: https://arxiv.org/abs/2503.12668
tags:
- memory
- computation
- data
- framework
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO2, a scalable zeroth-order fine-tuning
  framework for extremely large language models (LLMs) with limited GPU memory. The
  core method leverages CPU offloading to transfer inactive parameters from GPU to
  CPU, exploiting the unique dual forward pass architecture of zeroth-order (ZO) optimization
  to minimize communication overhead.
---

# ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory
## Quick Facts
- arXiv ID: 2503.12668
- Source URL: https://arxiv.org/abs/2503.12668
- Authors: Liangyu Wang; Jie Ren; Hang Xu; Junxiao Wang; Huanyi Xie; David E. Keyes; Di Wang
- Reference count: 37
- One-line primary result: Enables fine-tuning of 175B-parameter LLMs on a single 18GB GPU with no accuracy loss

## Executive Summary
ZO2 is a zeroth-order optimization framework designed to enable fine-tuning of extremely large language models with limited GPU memory. The core innovation leverages CPU offloading to transfer inactive model parameters from GPU to CPU, exploiting the unique two-forward-pass architecture of zeroth-order methods to minimize communication overhead. The framework achieves this with negligible time cost while maintaining model accuracy, making it practical for resource-constrained environments.

## Method Summary
The paper introduces ZO2, a scalable zeroth-order fine-tuning framework for extremely large language models with limited GPU memory. The core method leverages CPU offloading to transfer inactive parameters from GPU to CPU, exploiting the unique dual forward pass architecture of zeroth-order (ZO) optimization to minimize communication overhead. Key innovations include a random number generator state manager to ensure accuracy, a dynamic scheduler for overlapping computation and communication, reusable GPU memory blocks, efficient parameter updating, and low-bit precision support in AMP mode. ZO2 enables fine-tuning of models like OPT-175B (175B parameters) on a single 18GB GPU with almost no additional time cost and absolutely no accuracy loss compared to standard ZO methods.

## Key Results
- Enables fine-tuning of OPT-175B (175B parameters) on a single 18GB GPU
- Maintains almost no additional time cost compared to standard ZO methods
- Demonstrates consistent memory usage reductions across model sizes with throughput maintained or improved

## Why This Works (Mechanism)
ZO2 exploits the dual forward pass architecture of zeroth-order optimization, where parameters can be safely offloaded to CPU during inactive periods without affecting the computation flow. The framework uses a dynamic scheduler to overlap communication and computation, while a random number generator state manager ensures accuracy preservation across CPU-GPU transfers.

## Foundational Learning
1. Zeroth-Order Optimization: Optimization methods that don't require gradient computation, instead estimating gradients through function evaluations. Needed because traditional gradient-based methods become memory-prohibitive for extremely large models.
2. CPU-GPU Parameter Offloading: Technique to transfer inactive model parameters from GPU memory to CPU memory during computation. Required to overcome GPU memory limitations for large-scale model fine-tuning.
3. Dual Forward Pass Architecture: The two-pass computation pattern in zeroth-order methods where parameters can be temporarily inactive. Critical because it enables safe parameter offloading without breaking computation flow.
4. Memory Management in AMP Mode: Techniques for handling low-bit precision activations while maintaining numerical stability. Important for balancing memory savings with computational accuracy.
5. Random Number Generator State Management: Mechanisms to preserve RNG state across CPU-GPU transfers to ensure stochastic process consistency. Essential for maintaining the statistical properties of zeroth-order methods.

## Architecture Onboarding
- Component Map: Optimizer -> Memory Manager -> Scheduler -> Parameter Updater -> GPU Memory Blocks
- Critical Path: Forward Pass -> Parameter Offload (CPU) -> Backward Pass Computation -> Parameter Sync (GPU) -> Update
- Design Tradeoffs: Memory vs. Computation Time - CPU offloading saves memory but introduces transfer overhead; mitigated through overlapping techniques
- Failure Signatures: Memory overflow errors, accuracy degradation, synchronization stalls
- First Experiments: 1) Single-layer model offload timing test, 2) Dual forward pass state preservation validation, 3) Memory usage profiling across model scales

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on OPT-175B model and single-GPU scenarios, leaving questions about multi-GPU scaling
- CPU offloading efficiency may vary significantly depending on CPU memory bandwidth and system architecture
- The claim of "absolutely no accuracy loss" is based on comparisons with standard ZO methods, but broader downstream task performance remains unexplored

## Confidence
- Memory reduction claims: High - supported by systematic ablation studies across multiple model sizes
- Throughput maintenance/improved performance: Medium - consistent in most cases but with noted variations across configurations
- Accuracy preservation: High - demonstrated through controlled comparisons with baseline ZO methods
- Scalability claims: Low - limited experimental validation beyond tested scenarios

## Next Checks
1. Multi-GPU scaling evaluation: Test ZO2 performance when distributing computation across multiple GPUs with CPU offloading to assess communication overhead and synchronization bottlenecks.
2. Cross-architecture robustness: Evaluate the framework on diverse model architectures (e.g., GPT-3 variants, BLOOM, LLaMA) to verify general applicability beyond OPT-family models.
3. Long-sequence performance analysis: Conduct experiments with varying sequence lengths up to 8K tokens to identify potential memory bottlenecks or performance degradation in extended context scenarios.