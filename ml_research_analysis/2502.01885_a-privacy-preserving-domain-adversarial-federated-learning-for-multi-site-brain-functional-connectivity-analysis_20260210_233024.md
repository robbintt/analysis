---
ver: rpa2
title: A Privacy-Preserving Domain Adversarial Federated learning for multi-site brain
  functional connectivity analysis
arxiv_id: '2502.01885'
source_url: https://arxiv.org/abs/2502.01885
tags:
- learning
- data
- domain
- federated
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses privacy-preserving multi-site brain functional
  connectivity analysis in non-IID data settings, proposing a novel Domain Adversarial
  Federated Learning (DAFed) framework. DAFed integrates feature disentanglement,
  domain adversarial training, and contrastive learning to decompose latent features
  into domain-invariant and domain-specific components, enabling robust global learning
  while preserving local data specificity.
---

# A Privacy-Preserving Domain Adversarial Federated learning for multi-site brain functional connectivity analysis

## Quick Facts
- arXiv ID: 2502.01885
- Source URL: https://arxiv.org/abs/2502.01885
- Reference count: 40
- Primary result: DAFed achieves 0.764 ASD and 0.718 MCI classification accuracy while preserving data privacy in federated multi-site settings

## Executive Summary
This paper addresses the challenge of multi-site brain functional connectivity analysis in federated learning settings with non-IID data. The proposed DAFed framework integrates feature disentanglement, domain adversarial training, and contrastive learning to separate domain-invariant disease biomarkers from site-specific scanner noise. Evaluated on ABIDE (ASD) and ADNI (MCI) datasets, DAFed outperforms state-of-the-art federated learning methods while ensuring data confidentiality through gradient noise injection.

## Method Summary
DAFed implements a privacy-preserving federated learning framework that decomposes fMRI functional connectivity into domain-invariant and domain-specific components. The method uses a 4-layer GCN encoder with jumping knowledge aggregation to generate latent embeddings, which are then split by a disentanglement module into invariant and site-specific features. These components are merged via multi-head self-attention and processed by separate heads for classification, domain identification, and mutual information estimation. The framework employs adversarial training to align distributions across sites while contrastive learning stabilizes global knowledge transfer between federated rounds.

## Key Results
- Achieved average classification accuracy of 0.764 for ASD diagnosis on ABIDE dataset
- Achieved average classification accuracy of 0.718 for MCI classification on ADNI dataset
- Identified key brain regions including Right Frontal Medial Cortex and Left Frontal Operculum Cortex as site-specific biomarkers

## Why This Works (Mechanism)

### Mechanism 1: Feature Disentanglement
The feature disentangler isolates universal disease biomarkers from site-specific scanner noise by minimizing mutual information between domain-invariant ($f_{di}$) and domain-specific ($f_{ds}$) components. This forces the model to retain biological signal while discarding site-specific information like scanner hardware effects.

### Mechanism 2: Adversarial Training
Domain adversarial training aligns the distributions of domain-invariant features across sites by training a Domain Identifier to classify site origins while the feature generator attempts to fool it. This alignment acts as a regularizer preventing overfitting to source domain characteristics.

### Mechanism 3: Contrastive Learning
The contrastive learning module stabilizes federated aggregation by anchoring local updates to the global model's history through positive pairs between current and previous global domain-invariant features. This prevents catastrophic forgetting of global knowledge during federated rounds.

## Foundational Learning

- **Graph Convolutional Networks (GCN) on Functional Connectivity:** GCNs process the Functional Connectivity Network (FCN) graph structure where nodes represent ROIs and edges represent correlations. Quick check: Do you understand how Eq. (1) aggregates features from neighboring brain regions rather than pixels?

- **Domain Adversarial Neural Networks (DANN):** DANN handles non-IID data by using adversarial training to align feature distributions across domains. Quick check: Can you explain why we want to maximize the domain classification loss while minimizing the task classification loss?

- **Dynamic Functional Connectivity (dFC):** The sliding window approach on fMRI time series captures temporal dynamics rather than static average correlations. Quick check: Why does the paper use a "Jumping Knowledge" network to capture these temporal dynamics?

## Architecture Onboarding

- **Component map:** Input (dFC) -> GCN Encoder -> Embedding $Z$ -> Feature Disentangler ($f_{di}$, $f_{ds}$) -> Multi-head Self-Attention -> Classifier/Domain Identifier/MINE heads

- **Critical path:** The Feature Disentangler is the choke point. If this fails to separate $f_{di}$ from $f_{ds}$, the Adversarial training will remove signal and the global model will fail.

- **Design tradeoffs:** Noise injection ($\alpha=0.01$) for privacy destroys gradient-based explainability, requiring Score-CAM compensation. The dimensional split between $f_{di}$ and $f_{ds}$ is unspecified, creating a critical hyperparameter tuning challenge.

- **Failure signatures:** Monitor Domain Identifier Accuracy plateau; if $L_{DI}$ drops to 0 too quickly features aren't aligning, if it stays high the model isn't learning to fool it. Check t-SNE clustering of $f_{di}$ for class-based rather than site-based separation.

- **First 3 experiments:**
  1. Baseline Sanity Check: Run GCN backbone on pooled data to establish theoretical upper bound on accuracy
  2. Disentanglement Ablation: Run DAFed with $\lambda_1$ (MI loss) set to 0 to prove separating $f_{di}$ and $f_{ds}$ helps
  3. Visualization Check: Run Score-CAM on "Site" classification task to verify $f_{di}$ isn't contaminated with site-specific info

## Open Questions the Paper Calls Out

- How does DAFed's classification performance degrade when stricter differential privacy noise levels are applied compared to the low-noise setting ($\alpha=0.01$) used in the study?

- Are the site-specific biomarkers identified by improved Score-CAM, such as the Right Frontal Medial Cortex and Left Frontal Operculum Cortex, reproducible pathological features or artifacts of data heterogeneity?

- How sensitive is the federated model's convergence and accuracy to the selection of the "source domain" (central site) versus local sites?

## Limitations

- MINE-based mutual information estimation may suffer from numerical instability during training, potentially causing gradient explosion or vanishing gradients that compromise disentanglement quality.

- The dimensional split between $f_{di}$ and $f_{ds}$ components is unspecified, creating a critical hyperparameter tuning challenge that significantly affects performance.

- Small client handling is problematic with batch size set to 1/16 of dataset size, potentially causing BatchNorm failures or noisy gradient estimates that destabilize federated averaging.

## Confidence

- **High Confidence:** Framework architecture and FL training procedure are clearly specified; comparative performance claims are well-documented with appropriate baselines.

- **Medium Confidence:** MI estimation approach using MINE is theoretically sound but practical implementation details are underspecified, creating potential reproduction challenges.

- **Low Confidence:** Generalizability claim across "diverse brain disorders" extends beyond the two datasets tested, requiring additional validation on different neuroimaging modalities or disease types.

## Next Checks

1. **MI Loss Monitoring:** Implement real-time visualization of $L_{MI}$ during training to detect numerical instability, with automatic gradient clipping thresholds when MI estimates approach zero.

2. **Dimensionality Sweep:** Systematically test different dimensional splits for $f_{di}$ vs $f_{ds}$ (e.g., 50/50, 70/30, 90/10) to identify optimal configuration for both privacy preservation and diagnostic accuracy.

3. **Small Site Robustness:** Evaluate model performance when training on clients with varying minimum batch sizes, comparing BatchNorm against GroupNorm to identify most stable normalization strategy for heterogeneous client populations.