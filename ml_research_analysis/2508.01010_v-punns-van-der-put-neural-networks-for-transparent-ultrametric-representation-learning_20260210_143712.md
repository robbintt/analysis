---
ver: rpa2
title: 'v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation
  Learning'
arxiv_id: '2508.01010'
source_url: https://arxiv.org/abs/2508.01010
tags:
- hierarchical
- digit
- page
- ultrametric
- wordnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: v-PuNNs introduce a novel neural architecture that operates natively
  in ultrametric p-adic space, where neurons are characteristic functions of p-adic
  balls. This design enables lossless, white-box representation of hierarchical data,
  addressing the fundamental mismatch between hierarchical structures and conventional
  Euclidean embeddings.
---

# v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning

## Quick Facts
- arXiv ID: 2508.01010
- Source URL: https://arxiv.org/abs/2508.01010
- Reference count: 31
- Primary result: 99.96% leaf accuracy on WordNet nouns (52,427 leaves) in 16 minutes using p-adic ball neurons

## Executive Summary
v-PuNNs introduce a novel neural architecture that operates natively in ultrametric p-adic space, where neurons are characteristic functions of p-adic balls. This design enables lossless, white-box representation of hierarchical data, addressing the fundamental mismatch between hierarchical structures and conventional Euclidean embeddings. The framework is built on the Transparent Ultrametric Representation Learning (TURL) principle, where every parameter corresponds to a p-adic ball with explicit subtree semantics. A key theoretical contribution is the Finite Hierarchical Approximation Theorem, proving that depth-K v-PuNNs with Σ_{j=0}^{K-1} p^j neurons universally represent any K-level tree.

## Method Summary
v-PuNNs encode hierarchical data as characteristic functions of p-adic balls, with each neuron χ_B(x) returning 1 if input x is in ball B and a small leak value α otherwise. The architecture uses Valuation-Adaptive Perturbation Optimization (VAPO) to handle the discrete nature of p-adic space, maintaining real-valued latent variables that are projected to discrete p-adic digits via rounding. The method achieves state-of-the-art results on three benchmarks: 99.96% leaf accuracy on WordNet nouns (52,427 leaves) in 16 minutes, 96.9% leaf/100% root accuracy on Gene Ontology (27,638 proteins) in 50 seconds, and Spearman ρ=-0.96 correlation with taxonomic distance on NCBI Mammalia (12,205 taxa).

## Key Results
- 99.96% leaf accuracy on WordNet nouns (52,427 leaves) in 16 minutes
- 96.9% leaf / 100% root accuracy on Gene Ontology (27,638 proteins) in 50 seconds
- Spearman ρ=-0.96 correlation with taxonomic distance on NCBI Mammalia (12,205 taxa)

## Why This Works (Mechanism)

### Mechanism 1: Native p-adic Ball Representation
Encoding hierarchical data as characteristic functions of p-adic balls preserves the ultrametric distance of the original tree structure exactly, eliminating the geometric distortion inherent in Euclidean embeddings. Each neuron implements χ_B(x) for a ball B = B_k(a) ⊂ ℤ_p, where the p-adic distance |x - y|_p = p^{-k} encodes tree distance losslessly.

### Mechanism 2: Valuation-Adaptive Perturbation Optimization (VAPO)
VAPO enables gradient-based optimization in a discrete space where standard gradients vanish, by maintaining a real-valued latent variable and projecting to the discrete p-adic lattice via rounding. Each p-adic digit θ_i ∈ {0, ..., p-1} is associated with a real latent variable v_i, updated using standard Adam momentum and variance terms, then projected via Π(v) = round(v) mod p.

### Mechanism 3: Finite Hierarchical Approximation Theorem
A depth-K v-PuNN with N = Σ_{j=0}^{K-1} p^j = (p^K - 1)/(p - 1) parameters can exactly represent any function g: leaves(T) → ℚ_p on a K-level tree. The proof constructs coefficients c_B for each ball at depth K to match g(ℓ), then recursively defines parent coefficients as averages of children.

## Foundational Learning

- **Concept: Ultrametric spaces and the strong triangle inequality**
  - **Why needed here:** Understanding that d(x, z) ≤ max{d(x, y), d(y, z)} is fundamental to grasping why p-adic spaces naturally encode hierarchies.
  - **Quick check question:** If three points have distances d(A, B) = 3, d(B, C) = 3, and d(A, C) = 5, can this be an ultrametric?

- **Concept: p-adic valuation and norm**
  - **Why needed here:** The core encoding maps each leaf to a p-adic integer via its root-to-leaf path. The valuation ν_p(n) is the exponent of the highest power of p dividing n, and the norm ‖n‖_p = p^{-ν_p(n)}.
  - **Quick check question:** For p = 5, compute ν_5(125) and ‖125‖_5.

- **Concept: van der Put basis (classical)**
  - **Why needed here:** The architecture is named after van der Put's 1968 result that characteristic functions of p-adic balls form a basis for continuous functions on ℤ_p.
  - **Quick check question:** Why does the classical van der Put basis use differences of indicators rather than raw indicators?

## Architecture Onboarding

- **Component map:**
  CharacteristicNeuron χ_B(x) → AdamScalar (latent v, Adam state) → PrefixEncoder Γ(x) → PrefixDecoder Γ^{-1}(ẑ) → DenseMSEHead/TwoLogitCEHead

- **Critical path:**
  1. Encode: Map each leaf synset/protein/taxon to z(ℓ) ∈ ℤ_p via path digits
  2. Forward: Input x activates one ball per depth; coefficients are summed per digit head
  3. Predict: Each depth head outputs a digit d̂_k; reconstruct ẑ = Σ_k d̂_k p^k
  4. Decode: Apply Γ^{-1} to recover the predicted leaf
  5. Optimize: VAPO updates latent variables via projected Adam

- **Design tradeoffs:**
  - GIST-VAPO vs Adam-VAPO: GIST is faster (2 min vs 16 min on WordNet) but sacrifices root accuracy (37% vs 100%)
  - Full van der Put tensor vs conditional factorization: Full N_{vdp} parameters guarantee universality but are expensive
  - Leak parameter α: Values < 0.005 stall optimization; > 0.02 blur indicator sharpness

- **Failure signatures:**
  - Root accuracy << leaf accuracy: Indicates coarse-digit heads not converging
  - Triangle inequality violations: Should never occur; indicates incorrect p-adic reconstruction
  - Training stalls at high loss: Likely α too small; increase to 0.01-0.02

- **First 3 experiments:**
  1. Build a depth-3 binary tree (p=2, K=3), encode leaves as p-adic integers, train a minimal v-PuNN
  2. On NCBI Mammalia subset (1000 leaves), compare GIST-VAPO vs Adam-VAPO on time to 90% leaf accuracy
  3. After training on WordNet nouns, use describe_ball(θ, k) to extract synsets associated with a specific coefficient

## Open Questions the Paper Calls Out

### Open Question 1
Can mixed-radix or local-prime schemes be developed for v-PuNNs to reduce parameter waste when branching factors vary sharply across depths? Current v-PuNNs use a single global prime p ≥ max(b_k) + 1, which over-parameterizes shallow levels.

### Open Question 2
How can v-PuNNs jointly learn the hierarchy structure alongside the embedding, rather than assuming a fixed hierarchy? All experiments use pre-defined taxonomies, but incorporating tree inference remains open.

### Open Question 3
What are the theoretical limits of the conditional-head factorization used in HiPaN when hierarchies are not digit-separable? The paper proves universality for the full van der Put tensor but only empirically validates the more efficient HiPaN parameterization.

### Open Question 4
Can p-adic codes from v-PuNNs be effectively integrated into existing deep learning architectures (LLMs, GNNs, RL agents) for interpretable hierarchical reasoning? Tab-HiPaN shows promise for tabular data, but scaling to high-dimensional inputs requires different conditioning mechanisms.

## Limitations
- Requires input hierarchy to be a tree with known branching factor, making it inapplicable to networks with cycles or multiple inheritance
- Experimental results limited to three public benchmarks; generalization to other hierarchical domains remains unproven
- Adam-VAPO optimizer lacks formal convergence guarantees beyond the O(T^{-1/2}) rate for the projection step

## Confidence
- **High**: Native p-adic ball representation mechanism, ultrametric distance preservation, transparent white-box learning
- **Medium**: VAPO optimization effectiveness, universal approximation capability, practical scalability to 50K+ leaves
- **Low**: Joint encoding learning potential, applicability to non-tree hierarchies, long-term stability of learned representations

## Next Checks
1. Test v-PuNN robustness to corrupted hierarchies by randomly rewiring 5-20% of parent-child edges and measuring degradation in leaf accuracy and ultrametricity.
2. Compare v-PuNN performance against Euclidean embedding baselines (t-SNE, UMAP) on a synthetic hierarchical dataset where ground-truth ultrametric distances are known.
3. Implement the Joint v-PuNN variant on a small taxonomy (e.g., WordNet subset) and measure the trade-off between encoding flexibility and representation accuracy.