---
ver: rpa2
title: 'Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery
  to Scenario-Based Problem Solving'
arxiv_id: '2506.08349'
source_url: https://arxiv.org/abs/2506.08349
tags:
- medical
- llms
- tasks
- levels
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces a multi-cognitive-level evaluation framework
  (MultiCogEval) for assessing large language models'' (LLMs) medical capabilities
  across three cognitive levels: preliminary knowledge grasp, comprehensive knowledge
  application, and scenario-based problem solving. Built on existing medical datasets
  (MedQA, MedMCQA, MIMIC-IV), the framework constructs tasks targeting each cognitive
  level and normalizes performance metrics for comparability.'
---

# Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving

## Quick Facts
- **arXiv ID:** 2506.08349
- **Source URL:** https://arxiv.org/abs/2506.08349
- **Reference count:** 25
- **Primary result:** Systematic evaluation reveals significant performance declines as cognitive complexity increases, with 70B models outperforming 7B models by 2-3x on complex tasks.

## Executive Summary
This study introduces MultiCogEval, a framework for assessing large language models' medical capabilities across three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving. Built on existing medical datasets (MedQA, MedMCQA, MIMIC-IV), the framework constructs tasks targeting each cognitive level and normalizes performance metrics for comparability. Systematic evaluation of 32 state-of-the-art general and medical LLMs (2B-70B parameters) reveals that while top models achieve >60% accuracy on knowledge recall tasks, performance drops to 19.4% on full-path clinical diagnosis tasks. The findings highlight the need for enhanced LLM capabilities in clinical planning, information acquisition, and multi-step reasoning for real-world medical applications.

## Method Summary
The framework constructs three cognitive-level tasks using MedQA/MedMCQA datasets and MIMIC-IV EHRs. L1 tasks involve multiple-choice questions, L2 reformulates these into statement validation, multi-step rectification, and answer existence tasks, while L3 creates scenario-based full-path clinical diagnosis tasks using MIMIC-IV records. Performance is measured using normalized accuracy for L1/L2 and a weighted full-path accuracy metric for L3 that combines diagnosis correctness with examination recall. The framework evaluates 32 LLMs (2B-70B parameters) from six families using 5-shot in-context learning for L1/L2 and zero-shot agent-based interaction for L3.

## Key Results
- Performance declines significantly as cognitive complexity increases: L1 > L2 > L3
- Model size plays a critical role in higher-level performance, with 70B models outperforming 7B models by 2-3x on complex tasks
- Medical-domain fine-tuning improves low and mid-level performance by up to 15% but fails to significantly enhance high-level reasoning
- Inference-time scaling shows promise, particularly for complex problem-solving tasks, with DeepSeek-R1 improving L3 accuracy from 19.4% to 26.5%

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Load Isolation via Task Stratification
The framework isolates reasoning deficits from knowledge gaps by enforcing knowledge consistency across increasing complexity levels. MultiCogEval anchors all tasks (L1-L3) to a unified disease pool derived from MedQA/MedMCQA. By holding the "knowledge domain" constant while escalating the cognitive demand, performance drops can be attributed to cognitive limitations rather than a lack of encyclopedic knowledge.

### Mechanism 2: Inference-Time Compute for Complex Planning
Allocating additional compute during inference allows models to navigate larger decision spaces required for scenario-based problem solving. Inference-time scaling models utilize extended reasoning chains to evaluate possibilities before acting, significantly improving L2/L3 performance compared to standard models.

### Mechanism 3: Procedural Regularization via Recall Weighting
Weighting diagnosis accuracy by examination recall forces the model to demonstrate "due diligence" in information acquisition. The metric `Accuracy_proc` multiplies diagnosis correctness by the recall of relevant exam items, penalizing "lucky guesses" and rewarding systematic clinical planning.

## Foundational Learning

- **Concept: Bloom's Taxonomy (Revised)**
  - Why needed here: The entire MultiCogEval framework is structured around the cognitive hierarchy (Remembering -> Applying -> Creating/Planning).
  - Quick check question: Can you distinguish between a task requiring "recall" (e.g., selecting the right enzyme) and one requiring "planning" (e.g., determining the sequence of tests)?

- **Concept: Normalized Accuracy**
  - Why needed here: L1 tasks are multiple-choice (20-25% random chance), while L2 sub-tasks vary. Normalization is required to compare "60% on MCQ" vs "60% on Binary Classification" fairly.
  - Quick check question: Why would raw accuracy mislead you when comparing a 5-option MCQ task against a True/False statement validation task?

- **Concept: Agent-based Evaluation**
  - Why needed here: The High-Level (L3) task is not a single-pass prompt but a multi-turn interaction where the model acts as an agent requesting data.
  - Quick check question: How does evaluating a model as an "agent" (making sequential moves) differ from evaluating it as a "classifier" (one-shot answers)?

## Architecture Onboarding

- **Component map:** Disease Pool -> L1 Interface -> L2 Reformulator -> L3 Agent Environment -> Metric Engine
- **Critical path:** Constructing the High-Level (L3) dataset is the bottleneck. You must perform NER on source exams -> filter MIMIC-IV by ICD codes -> validate alignment.
- **Design tradeoffs:** The paper chooses "Recall" for exams, prioritizing safety over efficiency. Base models cannot be evaluated on the full-path task.
- **Failure signatures:** "Premature Diagnosis" (immediate output with 0% Exam Recall), "Hallucinated Exams" (tests not supported by database), "Instruction Amnesia" (models fail to follow agent protocol).
- **First 3 experiments:**
  1. Run Llama-3-8B and Llama-3-70B on the L1 task to replicate the "scaling helps" curve.
  2. Run a single top-tier model on 5 L3 samples and manually inspect if it orders tests logically or guesses randomly.
  3. Calculate L3 performance using only end-point accuracy vs. the full-path metric to quantify "lucky guessing."

## Open Questions the Paper Calls Out

### Open Question 1
How can medical-domain fine-tuning strategies be redesigned to specifically enhance high-level cognitive abilities like clinical planning and multi-step reasoning? The authors conclude that current medical-specific fine-tuning strategies are "insufficient for enhancing LLMs' reasoning abilities in complex real-world medical scenarios" and explicitly call for future efforts to focus on these high-level capabilities.

### Open Question 2
Do the observed cognitive performance declines hold across a wider range of medical domains, specifically rare diseases or complex multi-morbidity scenarios? The study limits the High-Level dataset to 42 diseases with single-discharge diagnoses, excluding the "long-tail" distribution of rare conditions and complex patient cases typical of real-world hospitals.

### Open Question 3
Can inference-time scaling effectively substitute for parameter scaling in high-level medical tasks, or are larger model sizes strictly necessary for scenario-based problem solving? While the paper notes inference-time scaling "shows promise," the performance gain on High-Level tasks was smaller than on Mid-Level tasks, and the gap between 7B and 70B models remains critical at higher levels.

## Limitations
- The "Knowledge Consistency" principle assumes perfect alignment between MedQA/MedMCQA diseases and MIMIC-IV records, which may not reflect clinical practice with comorbidities
- The agent-based evaluation protocol may disadvantage smaller models that follow instructions literally versus larger models that exploit implicit patterns
- The procedural regularization mechanism's clinical validity is questionable - the assumption that MIMIC-IV ground truth represents the only correct diagnostic path may not reflect real-world clinical practice

## Confidence

**High Confidence:** Claims about performance degradation across cognitive levels and general scaling benefits of larger models are well-supported by systematic evaluation.

**Medium Confidence:** The assertion that medical fine-tuning improves lower-level performance but not high-level reasoning is supported, though the evaluation protocol's sensitivity to instruction-following behavior introduces noise.

**Low Confidence:** The procedural regularization mechanism's clinical validity is questionable - the assumption that MIMIC-IV ground truth represents the only correct diagnostic path may not reflect real-world clinical practice where multiple valid approaches exist.

## Next Checks

1. **Knowledge Alignment Validation:** Manually audit 50 disease-to-MIMIC-IV record mappings to quantify alignment fidelity and identify systematic gaps that could explain performance drops unrelated to cognitive limitations.

2. **Instruction-Following Ablation:** Compare L3 performance of instruction-tuned medical models versus general models with explicit instruction-following fine-tuning to isolate whether observed differences reflect reasoning capability versus protocol compliance.

3. **Alternative Diagnostic Path Analysis:** For 20 L3 cases where models fail, have clinical experts evaluate whether alternative diagnostic sequences could have been valid, assessing whether the evaluation protocol's strictness may penalize correct but non-standard clinical reasoning.