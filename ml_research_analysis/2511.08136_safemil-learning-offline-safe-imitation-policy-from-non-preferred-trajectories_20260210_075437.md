---
ver: rpa2
title: 'SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories'
arxiv_id: '2511.08136'
source_url: https://arxiv.org/abs/2511.08136
tags:
- cost
- trajectories
- learning
- policy
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafeMIL, a novel offline safe imitation learning
  method that addresses the challenge of learning safe policies from limited non-preferred
  trajectories and large unlabeled datasets without explicit reward or cost information.
  SafeMIL formulates cost function learning as a multiple instance learning problem,
  using negative and unlabeled trajectory bags to train a cost predictor that identifies
  risky state-action pairs.
---

# SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories

## Quick Facts
- **arXiv ID**: 2511.08136
- **Source URL**: https://arxiv.org/abs/2511.08136
- **Reference count**: 40
- **Primary result**: Introduces SafeMIL, an offline safe imitation learning method that learns from non-preferred trajectories without explicit reward/cost labels, achieving 3.7× better safety metrics than baselines while maintaining high reward performance

## Executive Summary
SafeMIL addresses the challenge of learning safe policies from limited non-preferred trajectories and large unlabeled datasets without explicit reward or cost information. The method formulates cost function learning as a multiple instance learning problem, using negative and unlabeled trajectory bags to train a cost predictor that identifies risky state-action pairs. The learned cost function then guides the identification of preferred behaviors in the unlabeled dataset, enabling safe policy learning via behavior cloning. Empirically, SafeMIL outperforms state-of-the-art baselines in both velocity-constrained and navigation tasks, achieving superior safety metrics while maintaining high reward performance.

## Method Summary
SafeMIL operates in an offline setting where it receives negative trajectory bags (risky behaviors) and unlabeled trajectory bags (unknown quality) without explicit reward or cost labels. The method uses epistemic uncertainty estimation through Monte Carlo dropout to identify risky state-action pairs within negative bags, treating this uncertainty as a proxy for cost. It then formulates cost function learning as a multiple instance learning problem, training a cost predictor to distinguish risky from safe behaviors. The learned cost function is used to identify preferred trajectories in the unlabeled dataset by minimizing expected cost. Finally, a safe imitation policy is learned through behavior cloning on the identified preferred behaviors. The approach is tested across multiple simulation environments including Hopper, Ant, Car, and Franka kitchen tasks.

## Key Results
- Outperforms state-of-the-art baselines in velocity-constrained and navigation tasks
- Achieves median performance 3.7× better in terms of safety metrics
- Maintains high reward performance while improving safety
- Robust to variations in bag size and trajectory length
- Can learn effective policies using partial trajectories

## Why This Works (Mechanism)
SafeMIL works by leveraging the correlation between epistemic uncertainty and riskiness in state-action pairs. By treating uncertainty estimation as a proxy for identifying risky behaviors, the method can learn a cost function without explicit cost labels. The multiple instance learning formulation allows the cost predictor to be trained using only negative bags (containing risky behaviors) and unlabeled bags, without requiring individual state-action pair labels. This enables safe policy learning from limited non-preferred demonstrations and large amounts of unlabeled data, addressing the data efficiency challenge in safe imitation learning.

## Foundational Learning
- **Epistemic Uncertainty Estimation**: Why needed - To identify risky state-action pairs without explicit cost labels; Quick check - Verify uncertainty estimates correlate with actual risk in validation data
- **Multiple Instance Learning**: Why needed - To learn cost functions from bag-level labels without individual state-action pair annotations; Quick check - Ensure cost predictor generalizes to unseen state-action pairs
- **Behavior Cloning**: Why needed - To learn the final safe policy from identified preferred behaviors; Quick check - Validate policy performance on held-out safe demonstrations
- **Trajectory Segmentation**: Why needed - To handle partial trajectories and variable-length demonstrations; Quick check - Test performance with different trajectory segmentations
- **Cost-Aware Policy Learning**: Why needed - To ensure learned policy minimizes expected cost while maximizing reward; Quick check - Monitor both cost and reward metrics during training
- **Negative Bag Construction**: Why needed - To provide examples of risky behaviors for cost function learning; Quick check - Verify negative bags contain sufficiently diverse risky behaviors

## Architecture Onboarding

**Component Map:**
State-Action Pairs -> Uncertainty Estimation -> Negative Bag Processing -> Cost Predictor Training -> Unlabeled Bag Processing -> Preferred Behavior Identification -> Behavior Cloning -> Safe Policy

**Critical Path:**
1. Estimate epistemic uncertainty for state-action pairs in negative bags
2. Train cost predictor using multiple instance learning framework
3. Identify preferred behaviors in unlabeled dataset using learned cost function
4. Learn safe policy via behavior cloning on preferred behaviors

**Design Tradeoffs:**
- Using epistemic uncertainty as risk proxy trades explicit cost labeling for potential noise in uncertainty estimates
- Multiple instance learning enables training without individual labels but may require larger datasets
- Behavior cloning provides simple policy learning but may struggle with distribution shift

**Failure Signatures:**
- High uncertainty in safe state-action pairs indicates poor uncertainty estimation
- Cost predictor failing to distinguish risky from safe behaviors suggests inadequate training data
- Poor policy performance despite successful cost learning indicates behavior cloning issues
- Inability to identify preferred behaviors suggests cost function is not aligned with safety requirements

**First 3 Experiments to Run:**
1. Validate uncertainty-risk correlation by comparing epistemic uncertainty in negative vs. safe demonstrations
2. Test cost predictor performance on held-out state-action pairs from negative bags
3. Evaluate policy performance on safety-critical scenarios with known risk profiles

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily conducted in simulation environments; real-world performance unverified
- Relies on assumption that risky state-action pairs exhibit higher epistemic uncertainty
- Requires access to negative trajectory bags from risky behaviors, which may be difficult to obtain in safety-critical applications

## Confidence

**High Confidence:** The core algorithmic framework and its implementation are well-documented. The method's ability to outperform baselines in the tested simulation environments is well-supported by empirical results.

**Medium Confidence:** The theoretical justification for using epistemic uncertainty as a proxy for riskiness is reasonable but may not generalize to all environments. The method's performance in real-world applications is promising but unverified.

**Low Confidence:** The practical feasibility of obtaining negative trajectory bags in real-world safety-critical applications. The method's robustness to variations in environment dynamics and sensor noise is not thoroughly explored.

## Next Checks
1. Implement SafeMIL on a physical robotic system (e.g., a mobile robot or robotic arm) to validate its performance in real-world conditions with sensor noise and mechanical uncertainties

2. Design test cases where risky behaviors do not necessarily exhibit high epistemic uncertainty to evaluate the method's robustness to violations of its core assumption

3. Conduct experiments to determine the minimum number of negative trajectory bags required for effective cost function learning, and assess the method's performance when only a limited number of risky demonstrations are available