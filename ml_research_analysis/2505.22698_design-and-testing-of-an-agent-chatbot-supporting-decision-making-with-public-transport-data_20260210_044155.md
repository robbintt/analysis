---
ver: rpa2
title: Design and testing of an agent chatbot supporting decision making with public
  transport data
arxiv_id: '2505.22698'
source_url: https://arxiv.org/abs/2505.22698
tags:
- data
- chatbot
- query
- such
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an agent-based chatbot designed to support
  decision-making with public transport data by allowing users to interact with structured
  datasets using natural language. The chatbot employs an agent architecture where
  a core Large Language Model (LLM) generates SQL queries based on user questions,
  and a suite of tools execute these queries, verify their correctness, and visualize
  the results through maps and charts.
---

# Design and testing of an agent chatbot supporting decision making with public transport data

## Quick Facts
- arXiv ID: 2505.22698
- Source URL: https://arxiv.org/abs/2505.22698
- Reference count: 14
- Primary result: LLM-based agent chatbot achieves ~53% accuracy on simple public transport queries, with clear limitations on complex queries

## Executive Summary
This paper presents an agent-based chatbot designed to support decision-making with public transport data by allowing users to interact with structured datasets using natural language. The chatbot employs an agent architecture where a core Large Language Model (LLM) generates SQL queries based on user questions, and a suite of tools execute these queries, verify their correctness, and visualize the results through maps and charts. The system was tested on GTFS datasets from Bologna, Ferrara, and Milan, processing data into a relational database format. Performance evaluation involved a workflow that repeatedly tested the chatbot with 146 questions derived from templates, storing generated queries and comparing results to manually created "gold" queries.

## Method Summary
The chatbot uses a GPT-4-Turbo core LLM orchestrated by Langchain, which generates SQL queries based on user questions. The system provides the LLM with database schema information and retrieves relevant example queries to guide generation. A validation tool checks SQL for errors before execution, and results can be visualized as maps using Leaflet. The architecture processes GTFS data into a relational format and evaluates performance by comparing LLM-generated queries against manually created "gold" standard queries for 146 test questions derived from 7 templates.

## Key Results
- The chatbot provided correct answers in approximately 53% of cases for two simpler query templates
- System struggles significantly with complex queries requiring substantial adaptation of example patterns
- Common error types identified include SQL syntax issues and logical mistakes in query generation

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Agent Orchestration
- Claim: The agent architecture extends LLM capabilities by allowing it to invoke external tools for tasks the model cannot perform natively (SQL execution, visualization, error correction).
- Mechanism: A core LLM receives the user question, reasons about required steps, generates artifacts (SQL queries), and delegates execution to specialized tools. The orchestration layer manages state and tool invocation sequencing.
- Core assumption: The LLM can accurately decompose user intent into tool-callable subtasks; tool outputs are reliable enough to inform final responses.
- Evidence anchors:
  - [abstract] "agent architecture, which expands the capabilities of the core Large Language Model (LLM) by allowing it to interact with a series of tools that can execute several tasks, like performing SQL queries, plotting data and creating maps"
  - [section IV-A] "the agent can use a series of tools able to perform tasks that would otherwise be challenging or even infeasible for the LLM on its own"
  - [corpus] Weak direct corpus support; neighboring papers describe LLM chatbots but not specifically tool-augmented agent architectures for SQL.
- Break condition: If tool outputs are unreliable (e.g., SQL returns wrong schema), the LLM has no ground-truth signal to detect failure.

### Mechanism 2: Schema-Grounded Prompt Engineering with Example Retrieval
- Claim: Providing the LLM with explicit database schema (DDL, column comments, foreign keys) plus relevant example queries improves SQL generation accuracy.
- Mechanism: The prompt uses an XML-like structure containing task description, full schema definitions, and rules. Embedding-based retrieval selects the most relevant example queries from a library, which are appended to the prompt before generation.
- Core assumption: Retrieved examples are sufficiently similar to the target question; schema documentation is complete and accurate.
- Evidence anchors:
  - [section IV-B] "The model is guided with a textual prompt, written with an XML-like structure that uses ad-hoc tags"
  - [section IV-B] "text-embedding-ada-002 model employed to build the embeddings used to find the most relevant example queries for any input question"
  - [corpus] No corpus papers directly validate schema-grounding for SQL generation; this mechanism is paper-internal.
- Break condition: When questions require modifications beyond retrieved examples (e.g., complex multi-join queries), the system struggles—the paper explicitly notes this limitation.

### Mechanism 3: Iterative Query Validation Before Execution
- Claim: A dedicated error-checking tool reviews generated SQL for syntactic and logical errors before database execution, reducing runtime failures.
- Mechanism: After the LLM generates a query, a separate tool (using the same LLM with a different prompt containing common error patterns) inspects and corrects the query. Only then is it executed against the database.
- Core assumption: The error-checking prompt covers the majority of likely mistakes; corrections do not introduce new errors.
- Evidence anchors:
  - [section V] "the generated query is passed to a tool that looks for syntactical and logical errors and corrects them when necessary"
  - [section VI-B] 10 of 112 template 1-2 questions still produced syntax errors, indicating the validation tool is imperfect
  - [corpus] No corpus papers address SQL validation tools specifically.
- Break condition: Subtle semantic errors (e.g., comparing string direction values to integers as noted in VI-B) may pass syntactic validation but return incorrect results.

## Foundational Learning

- **Concept: GTFS Data Model and Relational Mapping**
  - Why needed here: The chatbot queries GTFS-derived tables (routes, trips, stops, shapes, etc.). Understanding entity relationships (e.g., routes → trips → stop_times → stops) is essential for debugging incorrect queries.
  - Quick check question: Can you sketch how `routes`, `trips`, and `stop_times` tables join to answer "which routes serve municipality X"?

- **Concept: Text-to-SQL Prompt Engineering**
  - Why needed here: The system's accuracy depends heavily on how schema and rules are presented to the LLM. Poorly structured prompts increase hallucination and syntax error rates.
  - Quick check question: Given a table with columns `trip_id`, `route_id`, and `direction`, what prompt rules would prevent the LLM from assuming `direction` is an integer?

- **Concept: Agent Tool Invocation Patterns**
  - Why needed here: The orchestrator must sequence tool calls correctly (generate → validate → execute → visualize). Misordering or missing error handling breaks the user experience.
  - Quick check question: What should happen if the SQL execution tool returns an empty result set—should the agent retry, report emptiness, or request clarification?

## Architecture Onboarding

- **Component map:**
  - User Question → Chainlit Frontend → Langchain Orchestrator → LLM (GPT-4-Turbo) → Validation Tool → Execution Tool → PostgreSQL → Map Tool → Leaflet Visualization → Response

- **Critical path:**
  1. User question received via Chainlit UI or API
  2. Langchain agent invokes LLM with schema + rules + retrieved examples
  3. LLM generates SQL query
  4. Validation tool checks/corrects query
  5. Execution tool runs query against PostgreSQL
  6. If visualization requested, map tool processes results
  7. LLM synthesizes natural language response from query results

- **Design tradeoffs:**
  - Single-agent vs. multi-agent: Current single-agent design is simpler but struggles with complex queries; paper indicates migration to multi-agent (Langgraph) for task separation
  - Example-based vs. from-scratch generation: Heavily relies on retrieved examples; fails when questions require substantial deviation
  - Post-hoc validation vs. constrained generation: Validation catches some errors but semantic mistakes (type mismatches) persist

- **Failure signatures:**
  - Empty responses with "I don't know" often indicate SQL syntax errors that escaped validation
  - Results returning wrong entity types (routes instead of municipalities) suggest schema misunderstanding
  - Zero-value aggregates for valid route-stop pairs indicate incorrect join logic or filter conditions

- **First 3 experiments:**
  1. **Schema exposure ablation:** Test query accuracy with full schema vs. minimal schema to quantify prompt grounding contribution.
  2. **Example retrieval threshold sweep:** Vary the number of retrieved examples (0, 1, 3, 5) and measure impact on template 1-2 accuracy.
  3. **Error taxonomy analysis:** Manually classify all 27 failed queries from templates 1-2 into syntax vs. semantic categories to prioritize validation tool improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does migrating to a multi-agent architecture (e.g., using Langgraph) significantly improve query accuracy and visualization reliability compared to the current single-agent tool-use approach?
- Basis in paper: [explicit] Section VII states the authors are repurposing the architecture to a multi-agent approach to "allow a considerable improvement of the accuracy."
- Why unresolved: The current paper only presents results for the single-agent architecture; the multi-agent results are part of ongoing work.
- Evidence: Comparative evaluation results showing the percentage of correct queries and successful visualizations in the multi-agent version versus the baseline.

### Open Question 2
- Question: How can the system be extended to integrate and query real-time transport data and service documents to compute quality metrics like Level of Service (LOS)?
- Basis in paper: [explicit] Section VII lists "integration of new data sources, such as real-time transport data and service documents" as an "open issue."
- Why unresolved: The current implementation relies solely on static GTFS schedules and does not handle dynamic data streams or unstructured documents.
- Evidence: A working module capable of joining static GTFS data with real-time feeds and answering questions regarding service quality with high temporal accuracy.

### Open Question 3
- Question: How can the chatbot be improved to explicitly verbalize implicit assumptions made during query generation (e.g., aggregation timeframes) to prevent user misinterpretation?
- Basis in paper: [inferred] Section V discusses instances where the chatbot must make arbitrary choices about data aggregation without user input, noting that "future improvement can consider... [specifying] all such assumptions."
- Why unresolved: The current system does not consistently expose these internal decisions to the user, potentially leading to incorrect conclusions.
- Evidence: Qualitative analysis or user studies verifying that users correctly understand the scope of the data when assumption disclosures are included in the response.

### Open Question 4
- Question: What specific prompt engineering or validation strategies are required to eliminate data type mismatch errors in SQL generation (e.g., treating direction codes as integers rather than strings)?
- Basis in paper: [inferred] Section VI-B identifies specific syntax errors where the LLM assumed incorrect data types (e.g., integer 0/1 for direction instead of strings 'andata'/'ritorno').
- Why unresolved: The paper reports these failures but does not propose or test a solution to prevent the LLM from hallucinating schema properties.
- Evidence: Post-intervention test results showing a reduction to near-zero for syntax errors specifically related to column data types.

## Limitations

- The system shows limited generalization capability, struggling significantly with complex query types that require substantial adaptation beyond retrieved examples
- The validation tool, while helpful, does not catch all semantic errors—particularly type mismatches like treating string direction values as integers
- Performance heavily depends on the quality and diversity of the example library, with heavy reliance on example-based generation

## Confidence

- **High confidence**: The agent architecture and tool orchestration mechanism are well-documented and replicable. The methodology for quantitative performance assessment (using template-derived questions with gold standard queries) is sound.
- **Medium confidence**: The reported 53% accuracy figure is specific to two relatively simple query templates. Generalization to broader query types remains uncertain. The error analysis is preliminary and based on a limited set of failed queries.
- **Low confidence**: The system's performance on queries requiring complex joins, aggregations, or significant adaptation of example patterns is poorly characterized and likely much lower than the reported 53%.

## Next Checks

1. Conduct schema exposure ablation testing: Compare query accuracy with full schema documentation versus minimal schema to quantify the contribution of comprehensive schema grounding to performance.
2. Perform example retrieval threshold analysis: Systematically vary the number of retrieved examples (0, 1, 3, 5) and measure impact on accuracy for template 1-2 questions to optimize retrieval parameters.
3. Execute comprehensive error taxonomy analysis: Manually classify all failed queries into syntactic versus semantic error categories to identify which validation tool improvements would yield the greatest accuracy gains.