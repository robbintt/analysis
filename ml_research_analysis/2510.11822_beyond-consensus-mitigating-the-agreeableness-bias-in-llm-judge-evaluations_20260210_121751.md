---
ver: rpa2
title: 'Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations'
arxiv_id: '2510.11822'
source_url: https://arxiv.org/abs/2510.11822
tags:
- validator
- feedback
- data
- outputs
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of using LLMs as judges to evaluate
  other LLM outputs, particularly in open-ended tasks like code feedback generation.
  The core issue is a strong positive bias in LLM validators: they reliably identify
  valid outputs (high TPR 96%) but poorly detect invalid ones (low TNR < 25%).'
---

# Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations

## Quick Facts
- arXiv ID: 2510.11822
- Source URL: https://arxiv.org/abs/2510.11822
- Authors: Suryaansh Jain; Umair Z. Ahmed; Shubham Sahai; Ben Leong
- Reference count: 23
- Primary result: Regression-based framework reduces maximum absolute error to 1.2%, achieving 2x improvement over best ensemble of 14 state-of-the-art LLMs

## Executive Summary
This paper addresses a critical limitation in using large language models (LLMs) as judges for evaluating other LLM outputs, particularly in open-ended tasks like code feedback generation. The core problem is a strong positive bias where LLM validators reliably identify valid outputs (high true positive rate > 96%) but poorly detect invalid ones (low true negative rate < 25%). This leads to overestimated precision estimates. The authors propose two main approaches: a minority-veto ensemble strategy that improves upon simple majority voting and is robust to missing data, and a novel regression-based framework that uses a small set of human-annotated ground truth data to model and correct validator bias. On a dataset of 366 high-school Python programs, the regression approach achieves a maximum absolute error of just 1.2%, representing a 2x improvement over the best ensemble of 14 state-of-the-art LLMs.

## Method Summary
The authors tackle LLM judge bias through two complementary approaches. First, they develop a minority-veto ensemble strategy that leverages diverse LLM validators to improve consensus accuracy while being robust to missing data. Second, they introduce a regression-based framework that models validator bias using a small set of human-annotated ground truth data. The framework learns the relationship between validator judgments and true output quality, then applies this learned model to correct bias in real-world evaluations. The regression approach assumes that validator bias patterns are stable enough to be learned from limited ground truth samples and can be applied to correct evaluations across different validator sets.

## Key Results
- LLM validators show high true positive rate (>96%) but low true negative rate (<25%) on code feedback generation tasks
- Minority-veto ensemble strategy improves upon simple majority voting while being robust to missing data
- Regression-based framework reduces maximum absolute error to 1.2%, achieving 2x improvement over best ensemble of 14 state-of-the-art LLMs
- The framework maintains effectiveness with small ground truth samples while correcting systematic validator bias

## Why This Works (Mechanism)
The regression framework works by modeling the systematic bias patterns in LLM validators and using these patterns to correct their judgments. Rather than relying on consensus alone, it learns the relationship between validator outputs and true quality through a small set of human-annotated examples. This learned model captures how each validator's tendency toward positive bias manifests across different types of outputs, allowing for individualized corrections. The framework effectively transforms biased validator judgments into more accurate estimates by applying learned bias-correction factors, achieving better precision than any single validator or simple ensemble approach.

## Foundational Learning

**LLM Validator Bias**: The systematic tendency of LLMs to give positive evaluations more frequently than warranted. Why needed: Understanding this bias is fundamental to recognizing why simple consensus approaches fail. Quick check: Examine TPR vs TNR metrics across different validation tasks.

**Ensemble Methods**: Techniques that combine multiple model predictions to achieve better performance than any individual model. Why needed: Provides baseline approach for comparison and understanding of voting dynamics. Quick check: Compare majority voting vs minority-veto performance on validation sets.

**Regression Analysis**: Statistical method for modeling relationships between variables. Why needed: Core technique for learning bias patterns and correction factors. Quick check: Validate regression model's predictive accuracy on held-out data.

**Ground Truth Annotation**: Human-labeled data serving as the reference standard for model evaluation. Why needed: Essential for training and validating bias-correction models. Quick check: Assess annotation consistency and inter-rater reliability.

## Architecture Onboarding

**Component Map**: Human ground truth data -> Regression model training -> Validator bias learning -> Bias correction application -> Improved evaluation outputs

**Critical Path**: The most time-consuming step is collecting and annotating the ground truth validation set, which serves as the foundation for the entire bias-correction framework.

**Design Tradeoffs**: The framework trades additional complexity (regression model training) for improved accuracy over simple ensemble methods. The key tradeoff is between the size of required ground truth data and the degree of bias correction achievable.

**Failure Signatures**: The framework may fail if validator bias patterns are too inconsistent across tasks, if ground truth samples are too small or unrepresentative, or if the relationship between validator outputs and true quality is too complex for regression modeling.

**3 First Experiments**:
1. Compare TPR/TNR metrics of individual validators vs ensemble approaches on a validation set
2. Train regression model on small ground truth sample and test bias correction accuracy
3. Vary ground truth sample size to determine minimum effective training set

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Results are specific to code feedback generation tasks and may not generalize to other domains without validation
- The regression framework assumes validator bias patterns are stable enough to be learned from small ground truth samples
- Performance may degrade if validator sets change significantly or if new LLM architectures exhibit different bias patterns
- The 2x improvement comparison is limited to 14 specific state-of-the-art LLMs and may not represent all possible configurations

## Confidence

**High Confidence**:
- Empirical validation of LLM validator bias (TPR > 96%, TNR < 25%) across multiple datasets
- 2x improvement claim over 14 state-of-the-art LLMs is well-supported by experimental results

**Medium Confidence**:
- Minority-veto ensemble approach effectiveness, which depends on task-specific optimal k parameter
- Regression framework's practical deployment, assuming stability of learned bias patterns across task types

## Next Checks

1. Test the regression framework's bias correction performance across diverse task domains (e.g., creative writing, summarization) to assess generalizability beyond code feedback generation.

2. Evaluate the framework's robustness to changes in LLM architecture versions and prompt engineering variations to determine if learned bias patterns remain stable over time.

3. Conduct ablation studies to quantify the minimum required ground truth sample size for effective bias correction, and test whether the framework maintains performance with smaller validation sets.