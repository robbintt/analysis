---
ver: rpa2
title: 'MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction'
arxiv_id: '2509.18813'
source_url: https://arxiv.org/abs/2509.18813
tags:
- extraction
- keyphrase
- mapex
- expert
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MAPEX, a multi-agent pipeline for unsupervised\
  \ keyphrase extraction using large language models. The framework employs three\
  \ LLM-based agents\u2014Expert Recruiter, Candidate Extractor, and Domain Expert\u2014\
  coordinated through a dual-path strategy that adapts to document length (knowledge-driven\
  \ for short texts, topic-guided for long ones)."
---

# MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction

## Quick Facts
- **arXiv ID**: 2509.18813
- **Source URL**: https://arxiv.org/abs/2509.18813
- **Authors**: Liting Zhang; Shiwan Zhao; Aobo Kong; Qicheng Li
- **Reference count**: 0
- **Key outcome**: MAPEX achieves 2.44% F1@5 improvement over state-of-the-art unsupervised methods across six benchmark datasets

## Executive Summary
MAPEX introduces a multi-agent LLM pipeline for unsupervised keyphrase extraction that dynamically adapts to document length through knowledge-driven and topic-guided extraction paths. The framework coordinates three specialized agents—Expert Recruiter, Candidate Extractor, and Domain Expert—to produce superior keyphrases compared to existing unsupervised methods. Experiments demonstrate consistent improvements across six benchmark datasets and three different 7B-parameter LLMs, with particular gains in scientific document domains.

## Method Summary
MAPEX employs a three-agent architecture with dual-path routing based on document length (threshold: 512 tokens). For short documents (<512 tokens), it uses knowledge-driven extraction with Wikipedia retrieval for each candidate keyphrase. For long documents (≥512 tokens), it employs topic-guided extraction that first distills core themes before re-ranking candidates. The Expert Recruiter assigns domain-specific roles to the Domain Expert, while the Candidate Extractor generates initial candidate pools. A post-processing pipeline handles deduplication, normalization, and filtering of non-grounded phrases. The framework was evaluated on six benchmark datasets using Mistral-7B-Instruct-v0.3, Qwen2-7B-Instruct, and Qwen2.5-7B-Instruct with greedy decoding.

## Key Results
- Achieves 2.44% average F1@5 improvement over PromptRank baseline across six datasets
- Outperforms standard LLM baselines by 4.01% F1@5 on average
- Shows consistent improvements across different model architectures (Mistral, Qwen2, Qwen2.5)
- Knowledge-driven path particularly effective for short documents (e.g., SemEval2010: 11.76→14.55 F1@5)

## Why This Works (Mechanism)

### Mechanism 1: Role-Based Expert Specialization
- Claim: Domain-specific role assignment improves keyphrase extraction quality through specialized terminology awareness
- Evidence: Expert Role+ improves F1@5 on NUS from 12.91 to 13.20 and Krapivin from 13.28 to 13.56 (long-text datasets)
- Break condition: Role assignment may mislead for multi-domain documents or those with ambiguous domain signals

### Mechanism 2: Length-Adaptive Dual-Path Routing
- Claim: Different extraction strategies for short vs. long texts address distinct failure modes
- Evidence: Knowledge-driven path shows substantial gains for short documents; topic-guided path prevents semantic dilution in long contexts
- Break condition: Fixed 512-token threshold may misroute documents near boundary or in domains with different optimal lengths

### Mechanism 3: Knowledge Augmentation via External Retrieval
- Claim: Wikipedia definitions improve semantic understanding and re-ranking accuracy for short documents
- Evidence: Knowledge+ path shows improvements on SemEval2010 (11.76→14.55 F1@5) and DUC2001 (23.01→24.00)
- Break condition: Wikipedia may lack coverage for domain-specific technical terms or novel concepts

## Foundational Learning

- **Multi-Agent LLM Coordination**: Understanding how three specialized agents coordinate through prompt chaining is prerequisite for grasping MAPEX architecture
  - Quick check: Can you explain why Expert Recruiter and Domain Expert are separate agents rather than combined?

- **Prompt Engineering for Role-Based Reasoning**: Effectiveness depends on carefully structured prompts that assign domain-specific roles
  - Quick check: What information must be included in Domain Expert re-ranking prompt for short vs. long documents?

- **Length Threshold Calibration**: The 512-token routing threshold requires empirical validation and may vary by domain
  - Quick check: How would you determine optimal length threshold for a new dataset? What metrics would you track?

## Architecture Onboarding

- **Component map**: Document → Expert Recruiter → Candidate Extractor → Dual-Path Router → (Knowledge Module OR Topic Module) → Domain Expert → Post-Processor → Final Keyphrases
- **Critical path**: 1) Document input → Expert Recruiter (role assignment), 2) Parallel: Candidate Extractor generates candidates, 3) Router checks length → selects path, 4) Domain Expert re-ranks using path-specific context, 5) Post-processor filters output
- **Design tradeoffs**: Knowledge path requires multiple Wikipedia API calls per candidate (latency overhead); 512-token threshold may not generalize across domains; role assignment adds inference step
- **Failure signatures**: Hallucinated keyphrases (post-processing filter failure), wrong domain role (Expert Recruiter misclassification), knowledge retrieval gaps (Wikipedia coverage issues), long document semantic dilution (topic-guided path too broad)
- **First 3 experiments**: 1) Threshold sensitivity analysis (sweep 256-1024 tokens), 2) Ablation by document length bucket (short/medium/long groups), 3) Knowledge retrieval coverage audit (check Wikipedia definition success rate)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the 512-token routing threshold optimal for diverse LLM architectures and domains?
- Basis: Authors set threshold based on Mistral-7B crossover region observation
- Why unresolved: Different models have varying context window capabilities; non-scientific domains may have different semantic dilution points
- Evidence needed: Sensitivity analysis across different model families and dataset types

### Open Question 2
- Question: Does Wikipedia reliance limit effectiveness in specialized technical domains?
- Basis: Knowledge-driven path uses WikiQuery for candidate definitions
- Why unresolved: Technical domains may contain proprietary terms or novel concepts not well-covered by Wikipedia
- Evidence needed: Comparison with domain-specific knowledge bases for short-text pipeline

### Open Question 3
- Question: Do multi-agent benefits diminish with larger, more capable LLMs?
- Basis: Experiments restricted to 7B parameter models
- Why unresolved: Larger models may handle context and reasoning without explicit multi-agent coordination
- Evidence needed: Evaluation on significantly larger models (GPT-4, Llama-3-70B)

## Limitations
- The 512-token routing threshold is empirically derived without theoretical justification and may not generalize across domains
- Wikipedia coverage limitations could degrade knowledge-driven path effectiveness for specialized technical terminology
- Single-role assignment per document may introduce bias for multi-domain or ambiguous documents

## Confidence

**High Confidence (4/5)**: Dual-path routing effectiveness, Expert Role contribution on long documents, overall F1@5 improvement over baselines
**Medium Confidence (3/5)**: Knowledge augmentation benefits for short documents, 512-token threshold optimality
**Low Confidence (2/5)**: Role assignment benefits on short documents, generalizability to non-benchmark domains

## Next Checks

1. **Threshold Sensitivity Analysis**: Sweep length threshold from 256 to 1024 tokens on validation split of target domain; plot F1@5 vs. threshold to identify optimal value and compare against paper's 512-token baseline

2. **Knowledge Coverage Audit**: Sample 50 documents from target domain; compute fraction of candidate keyphrases with useful Wikipedia definitions; if coverage <60%, implement domain-specific knowledge base fallback and assess impact

3. **Multi-Domain Stress Test**: Create or identify documents spanning multiple domains; run MAPEX and compare Expert Recruiter role assignment accuracy against human annotations; measure performance degradation with conflicting domain signals