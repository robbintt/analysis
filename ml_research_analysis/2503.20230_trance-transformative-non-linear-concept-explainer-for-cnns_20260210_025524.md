---
ver: rpa2
title: 'TraNCE: Transformative Non-linear Concept Explainer for CNNs'
arxiv_id: '2503.20230'
source_url: https://arxiv.org/abs/2503.20230
tags:
- concept
- faith
- explanations
- concepts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TraNCE, a novel concept-based explainability
  framework for CNNs that automatically discovers accurate and consistent concepts
  by leveraging variational autoencoders (VAEs) for non-linear decomposition of image
  activations. Unlike existing methods that assume linear reconstructability of image
  activations, TraNCE captures intricate relationships within the activations through
  its VAE-based approach, enabling more meaningful concept discovery.
---

# TraNCE: Transformative Non-linear Concept Explainer for CNNs

## Quick Facts
- **arXiv ID:** 2503.20230
- **Source URL:** https://arxiv.org/abs/2503.20230
- **Reference count:** 40
- **Primary result:** Introduces TraNCE, a VAE-based concept explainer achieving Faith scores exceeding 98% on fine-grained visual categorization tasks

## Executive Summary
TraNCE is a novel concept-based explainability framework for convolutional neural networks (CNNs) that addresses the limitations of linear concept explainers by employing variational autoencoders (VAEs) for non-linear decomposition of image activations. The framework introduces the Faith score, which combines Fidelity and Coherence metrics to provide a more comprehensive evaluation of explainer faithfulness and consistency. Experiments on fine-grained visual categorization tasks demonstrate that TraNCE outperforms baseline methods across multiple CNN architectures, while its Bessel function-based visualization module reduces concept duplication and ambiguity.

## Method Summary
TraNCE employs a VAE to project intermediate layer activations from a CNN into a latent space, where concept vectors (CAVs) are discovered through self-supervised training. Unlike linear reducers like NMF or PCA, this approach captures the intrinsic geometric structure of high-dimensional activations in fine-grained visual categorization tasks. The framework introduces the Faith score, which combines Fidelity (reconstruction agreement) and Coherence (normalized cross-spectral density of predictions) to evaluate explainer performance. A Bessel function is applied to the latent activations for smooth visualization, mitigating concept duplication issues found in threshold-based masking approaches.

## Key Results
- TraNCE achieves Faith scores exceeding 98% across multiple CNN architectures and image classes in fine-grained visual categorization tasks
- The VAE-based approach outperforms linear concept explainers (NMF, PCA) in both Fidelity and Coherence metrics
- Bessel function visualization reduces concept duplication and ambiguity compared to traditional threshold-based heatmaps
- The framework successfully addresses the assumption of linear reconstructability of image activations, which fails to capture intricate relationships in CNN feature maps

## Why This Works (Mechanism)

### Mechanism 1
If high-dimensional CNN activations contain non-linear manifold structures, then VAE-based decomposition yields more meaningful concept vectors than linear reducers (PCA/NMF). A Variational Autoencoder projects intermediate layer activations into a latent space via a probabilistic encoder and reconstructs them via a decoder, allowing the explainer to model complex, non-orthogonal relationships in the feature map. The core assumption is that discriminative features in Fine-Grained Visual Categorization tasks do not lie on a linear subspace and require manifold learning to disentangle.

### Mechanism 2
If concept explanations are evaluated solely on Fidelity (accuracy), they may be inconsistent; adding Coherence (consistency) to form the "Faith" score provides a more robust trust metric. The framework calculates Fidelity (reconstruction agreement) and Coherence (normalized cross-spectral density of predictions), with the Faith score being their arithmetic mean, forcing the explainer to be both accurate and consistent with the original model's logic. The assumption is that Coherence, defined via spectral density, serves as a valid proxy for human-perceivable consistency of concept prototypes.

### Mechanism 3
Smooth visualization of concept heatmaps via Bessel functions reduces the ambiguity and duplication found in threshold-based masking. Instead of a binary threshold to show "what the model saw," TraNCE applies a Bessel function to the latent activations, creating a sinusoidal color map that smoothly transitions between high and low activations, revealing not just the presence of a concept but the gradation of its influence. The assumption is that hard thresholding introduces human bias and artifacts that smooth interpolation can remove without losing semantic meaning.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs)**
  - **Why needed here:** The core of TraNCE is replacing linear reducers with a VAE. Understanding the balance between the reconstruction loss and KL-divergence (regularization) is necessary to tune the concept discovery process.
  - **Quick check question:** How does the VAE reconstruction loss differ from a standard autoencoder, and why does that matter for feature map decomposition?

- **Concept: Concept Activation Vectors (CAVs)**
  - **Why needed here:** TraNCE outputs CAVs as the explanation. You must understand that a CAV represents a direction in the activation space that corresponds to a human-understandable concept (e.g., "stripes" or "fur").
  - **Quick check question:** In the TraNCE architecture, does the CAV correspond to the input image, the latent vector z, or the decoder weights W?

- **Concept: Fine-Grained Visual Categorization (FGVC)**
  - **Why needed here:** The paper specifically targets FGVC (e.g., distinguishing bird species). These tasks have high inter-class similarity, justifying the move from linear to non-linear explainers.
  - **Quick check question:** Why would a linear explainer struggle to separate the concept "beak shape" for two different bird species if those features are highly correlated in the activation space?

## Architecture Onboarding

- **Component map:** Backbone CNN -> Extractor (hooks into intermediate layer) -> Reducer (VAE: Encoder q_θ, Decoder p_φ) -> Visualizer (Bessel function + MMD-Critic) -> Evaluator (Faith Score calculator)

- **Critical path:**
  1. Hook: Extract activations A_l from the target CNN layer (e.g., ResNet Layer4)
  2. Reshape: Flatten spatial dimensions to matrix G_l
  3. Train: Train the VAE on G_l (self-supervised) to converge on latent space z
  4. Invert: Use the decoder to reconstruct A'_l and compute the Faith score
  5. Visualize: Apply Bessel function to z and overlay on the original image

- **Design tradeoffs:**
  - Computational Cost: Training a VAE is significantly more expensive than running NMF. The paper argues this is necessary for FGVC tasks but may be overkill for simple classification.
  - Latent Dimension (c'): The paper uses 32 concepts. Increasing this adds granularity but risks concept duplication (ambiguity), while decreasing it risks losing discriminative features.

- **Failure signatures:**
  - Duplicate Concepts: If prototypes for different concepts look identical, the VAE latent dimension may be too small or the CNN backbone lacks discrimination for that class.
  - Low Faith Score (High Fidelity, Low Coherence): The explainer is accurate but inconsistent (concept prototypes don't match the query image class).
  - Sanity Check Failure: If image rotation or noise destroys the explanation, the explainer is overfitting to pixel data rather than semantic features.

- **First 3 experiments:**
  1. Reproducibility Check: Run TraNCE on ResNet50 with the "Australian Kelpie" class using the paper's provided code/weights to verify the reported Faith score (>98%).
  2. Ablation Study: Replace the Bessel visualization with a standard ReLU-based heatmap to qualitatively assess if "concept duplication" actually increases.
  3. Stress Test: Apply the "Sanity Check" experiment (adding Gaussian noise as per Fig. 9) to verify that the Faith score drops as noise increases, confirming the explainer reflects CNN logic rather than artifacts.

## Open Questions the Paper Calls Out
None

## Limitations
- The Bessel function visualization approach lacks empirical validation through ablation studies comparing it to standard heatmap methods
- The computational overhead of VAE training is not quantified relative to the performance gains
- The Faith score combines metrics with different scales and interpretations, requiring independent verification

## Confidence
- **High confidence**: The VAE-based decomposition mechanism and its advantages over linear reducers for non-linear activation manifolds
- **Medium confidence**: The Faith score as a comprehensive evaluation metric, pending validation on diverse CNN architectures and tasks
- **Low confidence**: The Bessel function visualization's effectiveness in reducing concept ambiguity, due to absence of comparative ablation studies

## Next Checks
1. **Faith Score Validation**: Test TraNCE on a simple linear dataset (e.g., MNIST digits) where NMF should perform comparably, verifying that Faith scores remain high (>95%) for both methods
2. **Computational Overhead Analysis**: Measure wall-clock training time and inference latency of TraNCE versus NMF-based explainers across multiple CNN architectures
3. **Bessel vs Standard Heatmap**: Conduct a user study where domain experts evaluate concept clarity and ambiguity between Bessel-function and ReLU-based visualizations on the same FGVC dataset