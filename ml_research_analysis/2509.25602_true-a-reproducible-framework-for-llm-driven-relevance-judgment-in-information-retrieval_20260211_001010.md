---
ver: rpa2
title: 'TRUE: A Reproducible Framework for LLM-Driven Relevance Judgment in Information
  Retrieval'
arxiv_id: '2509.25602'
source_url: https://arxiv.org/abs/2509.25602
tags:
- relevance
- 'true'
- information
- retrieval
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TRUE, a reproducible rubric-based framework
  for LLM-driven relevance judgment in IR, addressing the lack of standardized and
  interpretable workflows in existing automated evaluation methods. TRUE generates
  relevance labels using iterative data sampling and reasoning, constructing label-specific
  rubrics based on five key features: intent alignment, coverage, specificity, accuracy,
  and usefulness.'
---

# TRUE: A Reproducible Framework for LLM-Driven Relevance Judgment in Information Retrieval

## Quick Facts
- **arXiv ID:** 2509.25602
- **Source URL:** https://arxiv.org/abs/2509.25602
- **Reference count:** 40
- **Primary result:** TRUE achieves strong system-level correlation (NDCG@10: 0.988 for DL2020) using rubric-based LLM evaluation across five relevance dimensions.

## Executive Summary
This paper introduces TRUE, a reproducible rubric-based framework for LLM-driven relevance judgment in IR. TRUE generates relevance labels using iterative data sampling and reasoning, constructing label-specific rubrics based on five key features: intent alignment, coverage, specificity, accuracy, and usefulness. Evaluated on TREC DL 2019, 2020, and LLMJudge datasets, TRUE achieves strong leaderboard correlations, especially with Llama 3.3 70B (NDCG@10: 0.988 for DL2020), and shows moderate agreement with human judgments (Cohen's κ up to 0.456). The framework improves reproducibility and interpretability while highlighting the trade-off between model scale and topic-level consistency, offering a robust foundation for automated relevance assessment in IR.

## Method Summary
TRUE is a four-step framework that generates reproducible relevance labels through structured rubric construction. It begins with stratified sampling of 10% of query-document pairs (balanced across labels 0-3), then uses OpenAI's o1 to generate reasoning patterns across five dimensions (intent, coverage, specificity, accuracy, usefulness). These patterns are iteratively distilled into label-specific rubrics (N=3 iterations), which are then applied by LLaMA models (3 8B or 3.3 70B) to evaluate relevance. The framework separates reasoning generation from evaluation using different model families to reduce circularity and memorization risks, while maintaining interpretability through explicit rubric criteria.

## Key Results
- Llama 3.3 70B with TRUE achieves NDCG@10 correlation of 0.988 on TREC DL2020
- Moderate item-level agreement with human judgments (Cohen's κ up to 0.456)
- Higher system-level correlations than item-level consistency across topics
- Smaller LLaMA 3 8B benefits more from reasoning augmentation than larger 70B model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured rubrics derived from sampled data may improve reproducibility compared to direct prompting by standardizing evaluation criteria across prompts.
- Mechanism: The framework samples 10% of query-document pairs with equal label distribution, uses OpenAI's o1 to generate reasoning patterns, then iteratively distills these into label-specific rubrics across five features (intent alignment, coverage, specificity, accuracy, usefulness). This replaces ad-hoc prompting with explicit, auditable criteria.
- Core assumption: The reasoning model can reliably extract generalizable patterns from sampled examples that transfer to unseen query-document pairs.
- Evidence anchors:
  - [abstract] "TRUE generates relevance labels using iterative data sampling and reasoning, constructing label-specific rubrics based on five key features"
  - [Section 3.1] "We begin with batches of query document pairs from the datasets using 10% of each dataset with equal distribution for each relevance label (0-3)"
  - [corpus] Related work (Arabzadeh & Clarke) finds pairwise judgments align most closely with human labels, while graded methods like UMBRELA achieve stronger system-ranking correlations—suggesting structured approaches have precedent.
- Break condition: If rubrics overfit to sampled data and fail to generalize, or if label distributions in production differ significantly from the balanced sampling strategy.

### Mechanism 2
- Claim: Separating reasoning generation from evaluation (using different model families) may reduce circularity and memorization risks in LLM-as-judge setups.
- Mechanism: TRUE uses OpenAI's o1 for rubric generation but LLaMA models (3 8B or 3.3 70B) for final relevance labeling. Only 10% of data is exposed to the reasoning model, limiting memorization pathways.
- Core assumption: Cross-model separation meaningfully reduces data leakage and that patterns learned by o1 transfer to LLaMA without requiring shared training data.
- Evidence anchors:
  - [Section 5] "To mitigate circularity, we employ two different LLMs, for reasoning generation and evaluation. This also reduces memorization by exposing only a sample of the data to derive rubrics."
  - [corpus] LLM-Evaluation Tropes paper discusses validity concerns in LLM-based evaluations, including memorization and circularity, but does not provide direct evidence on mitigation effectiveness.
- Break condition: If o1 and LLaMA share training data with overlapping benchmarks, or if the rubric formulation itself encodes dataset-specific patterns.

### Mechanism 3
- Claim: Multi-dimensional rubric decomposition (intent, coverage, specificity, accuracy, usefulness) appears to produce higher leaderboard correlations but shows moderate agreement with human judgments at the item level.
- Mechanism: Rather than treating relevance as a single score, TRUE evaluates across five interpretable dimensions, potentially capturing nuance that single-metric approaches miss. However, item-level Cohen's κ (0.20–0.456) suggests the decomposition does not fully bridge human-LLM judgment gaps.
- Core assumption: Human relevance judgments implicitly weigh these five dimensions, and LLMs can approximate this weighting consistently.
- Evidence anchors:
  - [Section 4.2] "Overall agreement levels are moderate, Cohen's κ ≈ (0.20 − 0.45)... Agreement is higher when labels are collapsed into broader categories (0|123, 01|23)"
  - [Table 2] LLaMA 3.3 70B with TRUE achieves NDCG@10 correlation of 0.988 on DL2020
  - [corpus] Weak corpus evidence on multi-dimensional decomposition specifically—neighbor papers focus on direct prompting and pairwise methods.
- Break condition: If dimensions are redundant or if LLMs systematically misweight certain dimensions compared to humans (e.g., over-indexing on coverage vs. specificity).

## Foundational Learning

- Concept: **Cranfield Paradigm & TREC Evaluation**
  - Why needed here: TRUE is evaluated against TREC Deep Learning (DL) 2019, 2020, and LLMJudge benchmarks, which follow the Cranfield approach of pooling documents and having assessors label relevance.
  - Quick check question: Can you explain why system-level correlation (NDCG@10) might be high even when item-level agreement (Cohen's κ) is moderate?

- Concept: **Relevance as Multi-Faceted Construct**
  - Why needed here: TRUE decomposes relevance into five features (intent, coverage, specificity, accuracy, usefulness), building on Saracevic's framework cited in the paper.
  - Quick check question: Why might "usefulness" differ from "relevance" in search evaluation? When would they diverge?

- Concept: **LLM Judge Agreement Metrics**
  - Why needed here: The paper reports both correlation metrics (Spearman's ρ, Kendall's τ) and inter-rater agreement (Cohen's κ), which measure different things.
  - Quick check question: What does it mean if Spearman's ρ = 0.98 but Cohen's κ = 0.25? Is the evaluation "good"?

## Architecture Onboarding

- Component map: Sampling Module → Reasoning Generator (OpenAI o1) → Rubric Distiller (iterative, N=3) → Evaluator (LLaMA 3 8B or 3.3 70B) → Correlation Computer
- Critical path: Sampling → Reasoning (o1) → Rubric Finalization → Evaluation (LLAMA) → Correlation Analysis. The rubric generation happens once per dataset; evaluation runs per query-document pair.
- Design tradeoffs:
  - **Model scale vs. consistency**: LLaMA 3.3 70B shows higher system-level correlation; LLaMA 3 8B benefits more from reasoning augmentation (Figure 2)
  - **Rubric specificity vs. generalization**: Manual refinement improves interpretability but risks overfitting
  - **5-dimension coverage vs. annotation cost**: More dimensions may capture nuance but complicate rubric formulation
- Failure signatures:
  - Central tendency bias: LLMs concentrate scores around middle labels, depressing MAP/MRR (Section 5)
  - Topic-level inconsistency: High run-level correlations but lower topic-level correlations (Figure 2 scatter)
  - κ disagreement on fine-grained labels: 4-way agreement lower than collapsed binary (0|123, 01|23)
- First 3 experiments:
  1. **Reproduce rubric generation**: Sample TREC DL 2019 with 10% balanced split, run through o1 reasoning pipeline, verify rubric content matches paper's 5-dimension structure.
  2. **Ablate reasoning step**: Compare TRUE (direct rubric application) vs. TRUE Reasoning (CoT before scoring) on LLaMA 3 8B to verify reasoning helps smaller models more.
  3. **Cross-dataset rubric transfer**: Train rubrics on DL 2019, apply to LLMJudge without modification—measure correlation drop to assess generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the TRUE framework be effectively extended to task-specific and session-aware relevance judgment?
- Basis in paper: [explicit] The authors state, "for future work we plan to pursue task-specific and session aware relevance judgment... including fine-tuning."
- Why unresolved: The current implementation evaluates individual query-document pairs and does not model the dependencies or context of a multi-turn search session.
- What evidence would resolve it: Successful application of TRUE on session-based datasets (e.g., THUIR) showing high correlation with session-level human satisfaction judgments.

### Open Question 2
- Question: Can TRUE be adapted to enable real-time relevance and usefulness-based re-ranking in conversational retrieval systems?
- Basis in paper: [explicit] The authors plan to "extend TRUE to automated relevance judgments in text and conversational retrieval systems, enabling real-time relevance and usefulness based re-ranking."
- Why unresolved: The current rubric generation involves iterative sampling and reasoning which may introduce latency incompatible with real-time constraints.
- What evidence would resolve it: A modified framework demonstrating low-latency inference capabilities while maintaining high correlation with human relevance labels in a live conversational setting.

### Open Question 3
- Question: What methodological improvements are required to bridge the gap between high system-level correlation and lower topic-level consistency?
- Basis in paper: [inferred] The results section notes that "run-level correlations are high but topic-level correlations are consistently lower," suggesting a limitation in fine-grained consistency.
- Why unresolved: The rubrics may generalize well to system trends but fail to capture the nuances of specific queries, leading to variance at the topic level.
- What evidence would resolve it: An analysis showing improved Spearman's correlation at the individual topic level without degrading system-level NDCG rankings.

## Limitations

- Moderate inter-rater agreement (Cohen's κ up to 0.456) indicates the framework doesn't fully bridge the human-LLM judgment gap, particularly at the item level
- Reliance on specific model versions (OpenAI o1-2024-12-17, LLaMA variants) creates potential reproducibility barriers if model availability or behavior changes
- The 10% sampling strategy, while balancing label distribution, may not capture the full complexity of real-world relevance patterns

## Confidence

- **High confidence:** System-level correlation results (NDCG@10 reaching 0.988 for DL2020) and the general methodology of rubric-based decomposition into five dimensions
- **Medium confidence:** Claims about reduced circularity through cross-model separation, given limited direct evidence on memorization reduction
- **Medium confidence:** Item-level agreement metrics and their implications, as the moderate κ values suggest the framework captures system-level trends better than individual judgment accuracy

## Next Checks

1. Conduct ablation studies comparing TRUE with alternative rubric formulations (e.g., fewer dimensions, different weightings) to isolate which features drive the correlation improvements
2. Test rubric generalization by applying TRUE rubrics trained on TREC DL 2019 to completely different IR datasets (e.g., academic search or legal retrieval) to assess domain transferability
3. Perform human validation studies where assessors evaluate the same query-document pairs using the TRUE rubric dimensions to identify systematic gaps between human and LLM interpretations