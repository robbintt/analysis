---
ver: rpa2
title: 'Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning
  Benchmarks'
arxiv_id: '2509.22258'
source_url: https://arxiv.org/abs/2509.22258
tags:
- reasoning
- clinical
- evaluation
- neural-medbench
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Neural-MedBench, a compact diagnostic benchmark\
  \ designed to probe the reasoning capabilities of vision-language models (VLMs)\
  \ in clinical neurology. Unlike existing datasets focused on classification accuracy,\
  \ Neural-MedBench includes 120 expert-annotated multimodal cases\u2014combining\
  \ MRI scans, EHRs, and clinical notes\u2014structured into 200 reasoning-intensive\
  \ tasks spanning differential diagnosis, lesion recognition, and rationale generation."
---

# Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks

## Quick Facts
- **arXiv ID**: 2509.22258
- **Source URL**: https://arxiv.org/abs/2509.22258
- **Reference count**: 29
- **Key outcome**: Introduces Neural-MedBench, a compact multimodal diagnostic benchmark exposing reasoning gaps in VLMs that classification-focused datasets miss.

## Executive Summary
Neural-MedBench is a compact diagnostic benchmark designed to probe the reasoning capabilities of vision-language models in clinical neurology. Unlike existing datasets focused on classification accuracy, it includes 120 expert-annotated multimodal cases—combining MRI scans, EHRs, and clinical notes—structured into 200 reasoning-intensive tasks spanning differential diagnosis, lesion recognition, and rationale generation. A hybrid evaluation pipeline, validated by clinicians, uses LLM-based graders alongside semantic similarity metrics to assess reasoning fidelity. Evaluations of leading VLMs reveal a sharp performance drop compared to conventional benchmarks, with reasoning failures dominating error patterns. Human expert baselines significantly outperform models, especially on complex tasks. The results underscore the necessity of depth-oriented benchmarks alongside breadth-focused datasets for assessing clinically trustworthy AI.

## Method Summary
Neural-MedBench provides 120 expert-curated neurology cases yielding 200 multimodal reasoning tasks. Models are evaluated zero-shot using interleaved prompts with base64-encoded MRI/CT, EHR fields, and clinical notes. The pipeline scores responses with Pass@k accuracy, BERTScore for semantic fidelity, and an LLM grader calibrated against clinician consensus (Pearson's r > 0.9). The dataset is available at `https://huggingface.co/datasets/Reisen301/Neural-MedBench`, and evaluation uses temperature=0.7, top_p=0.95 decoding with the prompt template "You are an experienced neurology expert...".

## Key Results
- VLMs achieve near-human performance on conventional benchmarks but drop to <15% pass@1 on Neural-MedBench's complex tasks.
- Reasoning failures (51%) dominate over perceptual failures (27%) in error analysis, with 13% visual hallucinations.
- Human expert baselines significantly outperform models, especially on multi-turn and complex reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1: Two-Axis Independence Hypothesis
- **Claim:** Success on breadth-oriented benchmarks does not predict competence on depth-oriented reasoning tasks.
- **Mechanism:** Breadth benchmarks reward pattern recognition and statistical generalization across populations. Depth benchmarks require multimodal synthesis, ambiguity resolution, and structured justification under uncertainty. These cognitive demands activate different capabilities, creating an "evaluation illusion" where high breadth scores mask reasoning deficits.
- **Core assumption:** The two axes are largely uncorrelated—models can achieve high classification accuracy without developing integrative clinical reasoning.
- **Evidence anchors:**
  - [abstract] "A sharp performance drop compared to conventional datasets" with "reasoning failures dominating error patterns"
  - [Section 5.1] Pass@1 accuracy plummets to <15% on complex tasks despite near-human performance on standard benchmarks; Figure 1 visualizes this disconnect
  - [corpus] ClinDEF paper notes that "dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question answering"

### Mechanism 2: Reasoning Failure Cascade
- **Claim:** The primary bottleneck for VLMs in clinical neurology is not perception but integrative inference.
- **Mechanism:** Models correctly identify imaging features (e.g., FLAIR hyperintensity) but fail at (1) weighting evidence across MRI sequences, (2) temporal/causal modeling, and (3) adjudicating competing differential hypotheses. This produces fluent but diagnostically incorrect outputs.
- **Core assumption:** Perceptual and reasoning capabilities are separable; models can "see" without "understanding."
- **Evidence anchors:**
  - [abstract] "Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings"
  - [Section 5.3 / Appendix F] 51% of errors classified as Reasoning Failure vs. 27% Perceptual Failure; patterns include misweighting sequences, conflating acute/chronic findings, collapsing multi-lesion presentations
  - [corpus] "The Illusion of Clinical Reasoning" paper (FMR 0.57) finds similar pervasive gaps between exam performance and true clinical competency

### Mechanism 3: Hybrid Evaluation as Scalable Proxy
- **Claim:** LLM-based graders, calibrated against clinician consensus, can automate reasoning assessment with high fidelity.
- **Mechanism:** A detailed neurology-specific rubric guides an LLM grader (GPT-4o). Clinician-in-the-loop validation establishes correlation (Pearson's r > 0.9), enabling post-calibration automated evaluation without ongoing expert involvement.
- **Core assumption:** The rubric captures the essential dimensions of clinical reasoning, and the grader generalizes beyond calibration cases.
- **Evidence anchors:**
  - [Section 3 / Stage 1] "Very high correlation between the LLM grader's scores and the consensus of human experts (e.g., Pearson's r > 0.9)"
  - [Appendix E] No systematic advantage observed for GPT-4o outputs when graded by itself; hybridization with BERTScore adds robustness
  - [corpus] Limited direct corpus evidence on LLM-as-judge reliability in medical reasoning; mostly validation still emerging

## Foundational Learning

- **Concept: Multimodal Clinical Reasoning**
  - **Why needed here:** Neural-MedBench requires integrating MRI findings, EHR data, and clinical narratives—unlike single-modality benchmarks.
  - **Quick check question:** Can you explain why FLAIR hyperintensity without diffusion restriction changes the differential for a white matter lesion?

- **Concept: Differential Diagnosis Generation**
  - **Why needed here:** Pass@5 evaluates whether the correct diagnosis appears in the model's top-5 hypotheses; ranking matters.
  - **Quick check question:** Given headache + focal weakness + MRI showing ring-enhancing lesion, list 3 differential diagnoses in priority order.

- **Concept: Benchmark Validity Trade-offs**
  - **Why needed here:** Neural-MedBench prioritizes reasoning density (depth) over scale (breadth)—understanding this trade-off is essential for interpreting results.
  - **Quick check question:** Why might a 200-task benchmark reveal failure modes that a 12,000-task benchmark misses?

## Architecture Onboarding

- **Component map:** Input layer (MRI/CT, EHR, notes) -> Task layer (200 tasks across 3 families, stratified by difficulty) -> Evaluation layer (Pass@k, BERTScore, LLM grader, error taxonomy)
- **Critical path:**
  1. Load case → encode images → format interleaved multimodal prompt
  2. Generate response with structured role-prompting ("experienced neurologist")
  3. Score via automated pipeline (accuracy + BERTScore + LLM grader)
  4. Optional: manual error taxonomy annotation for failure mode analysis

- **Design tradeoffs:**
  - **Compact scale (120 cases, 200 tasks)** → high annotation quality, low compute cost (~10x cheaper than OmniMed-VQA), but limited statistical generalization
  - **Neurology-only scope** → depth in one domain, but unknown transfer to other specialties
  - **LLM grader reliance** → scalable evaluation, but requires calibration trust

- **Failure signatures:**
  - **Reasoning Failure:** Model cites correct findings but reaches wrong diagnosis (most common, 51%)
  - **Perceptual Failure:** Model misses or mischaracterizes visible lesions (27%)
  - **Visual Hallucination:** Model fabricates findings not present in images (13%)—distinct safety risk

- **First 3 experiments:**
  1. **Baseline run:** Evaluate your VLM on all 200 tasks with temperature=0.7, top_p=0.95; report pass@1, pass@5, and BERTScore by task family and difficulty level.
  2. **Ablation:** Test with only imaging (no EHR/notes) vs. only text (no images) to isolate multimodal integration gaps.
  3. **Error audit:** Sample 20 incorrect responses; manually classify by error taxonomy to determine if your model's failure profile matches the paper's 51/27/13/9 distribution or diverges.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific training interventions or architectural modifications can effectively mitigate the "Reasoning Failure" error mode identified as the primary bottleneck for VLMs?
- **Basis in paper:** [explicit] The error analysis reveals that 51% of failures stem from reasoning deficits rather than perceptual errors, yet the paper only diagnoses this issue without proposing remediation strategies.
- **Why unresolved:** The work focuses on defining the benchmark and exposing the "evaluation illusion," leaving the solution for bridging the gap between perception and clinical synthesis for future research.
- **What evidence would resolve it:** Studies demonstrating that specific fine-tuning methods (e.g., process-supervised rewards) improve pass@1 scores on Neural-MedBench without merely overfitting to the test cases.

### Open Question 2
- **Question:** Does the observed "Evaluation Illusion"—where high Breadth-axis scores obscure low Depth-axis competence—generalize to medical domains outside of neurology?
- **Basis in paper:** [inferred] The Two-Axis Evaluation Framework is proposed as a general paradigm, but empirical validation is restricted to clinical neurology cases involving MRI and EHR data.
- **Why unresolved:** It remains unclear if the "sobering chasm" between models and human experts is unique to the high multi-modal synthesis required in neurology or is a universal feature of medical VLMs.
- **What evidence would resolve it:** Application of the compact, depth-oriented benchmarking methodology to other specialties (e.g., dermatology or pathology) showing similar performance drops.

### Open Question 3
- **Question:** How can Depth-axis benchmarks integrate longitudinal data and multi-center variability while maintaining the evaluation efficiency and "reasoning density" central to Neural-MedBench?
- **Basis in paper:** [explicit] The Limitations section explicitly identifies the lack of longitudinal data and systematic domain shift analysis as current gaps, listing them as key targets for the "continually evolving resource."
- **Why unresolved:** The current design prioritizes a compact, static snapshot of diagnostic complexity; adding temporal dimensions may increase costs and reduce the "stress test" signal-to-noise ratio.
- **What evidence would resolve it:** A validated extension of the dataset that includes time-series patient data while preserving the high correlation ($r > 0.9$) between the automated grader and clinician consensus.

## Limitations
- **Limited generalization scope**: Neural-MedBench focuses exclusively on neurology, so results may not transfer to other specialties.
- **Scale vs. depth trade-off**: The dataset's compact size ensures high annotation quality but may not capture full statistical distribution of clinical reasoning scenarios.
- **Evaluation pipeline dependency**: Reliance on LLM-based grading introduces potential bias; calibration was validated on this dataset, but generalization to other domains is unproven.

## Confidence
- **High confidence**: The existence of a sharp performance drop for VLMs on Neural-MedBench versus conventional benchmarks; the dominance of reasoning failures (51%) over perceptual errors (27%) in error analysis.
- **Medium confidence**: The reliability of the hybrid LLM-based evaluation pipeline for clinical reasoning assessment; the claim that perceptual and reasoning capabilities are separable.
- **Low confidence**: Generalization of results beyond neurology; the long-term stability of LLM-as-judge reliability without clinician recalibration.

## Next Checks
1. **Cross-specialty transfer test**: Evaluate the same VLMs on a similar reasoning benchmark from a different medical specialty (e.g., radiology or oncology) to assess domain generalization.
2. **Human-LLM grader correlation replication**: Conduct an independent clinician-LLM grading correlation study on a held-out subset of Neural-MedBench to verify calibration robustness.
3. **Breadth-to-depth transfer experiment**: Train or fine-tune a VLM on a large-scale medical classification dataset, then re-evaluate on Neural-MedBench to test if breadth improves depth reasoning.