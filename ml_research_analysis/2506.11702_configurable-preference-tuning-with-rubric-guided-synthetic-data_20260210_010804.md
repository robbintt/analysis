---
ver: rpa2
title: Configurable Preference Tuning with Rubric-Guided Synthetic Data
arxiv_id: '2506.11702'
source_url: https://arxiv.org/abs/2506.11702
tags:
- preference
- text
- score
- system
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of static, monolithic preference
  models in language models by introducing Configurable Preference Tuning (CPT). CPT
  enables LLMs to dynamically adjust their behavior at inference time based on explicit,
  human-interpretable system prompts derived from structured rubrics.
---

# Configurable Preference Tuning with Rubric-Guided Synthetic Data

## Quick Facts
- arXiv ID: 2506.11702
- Source URL: https://arxiv.org/abs/2506.11702
- Authors: Víctor Gallego
- Reference count: 14
- Primary result: CPT enables LLMs to dynamically adjust behavior via system prompts, achieving 0.83 binned accuracy vs 0.60 baseline on rubric tasks.

## Executive Summary
This paper introduces Configurable Preference Tuning (CPT), a method to overcome the static nature of traditional preference models by enabling language models to dynamically adapt their behavior at inference time. CPT leverages structured rubrics to generate system prompts, which condition the generation of synthetic preference data. These preferences are then used to fine-tune a student model via Direct Preference Optimization (DPO). The approach is validated on creative writing tasks, showing significant improvements in the student model's ability to match target rubric-defined quality levels compared to baselines. Additionally, CPT enhances Best-of-N sampling by providing a more diverse initial distribution of responses.

## Method Summary
CPT uses structured rubrics to create system prompts that guide the generation of synthetic preference data. Teacher models (e.g., DeepSeek-R1, o3-mini) generate text conditioned on these prompts and rubric-defined score levels. The resulting preference pairs (chosen/rejected responses) are used to fine-tune a student LLM via DPO with LoRA adapters. At inference, the student model accepts system prompts derived from rubrics to dynamically adjust its output. Evaluation involves measuring accuracy in matching target quality bins (low ≤40, moderate 40–92.5, extremely high >92.5) and rank correlations (Kendall's τ, Spearman's ρ) between generated output scores and intended rubric levels, using an LLM judge (Claude 3.5 Sonnet).

## Key Results
- Student models trained with CPT achieved 0.83 binned accuracy versus 0.60 for baselines in matching target rubric-defined quality levels.
- Rank correlations (τ, ρ) between judge scores and target bins were significantly higher for CPT models across all tested base models.
- CPT-enhanced models improved Best-of-N sampling by providing a better initial distribution of responses, leading to higher quality outputs with fewer samples.

## Why This Works (Mechanism)
CPT works by embedding explicit, human-interpretable control into the preference tuning process. By conditioning synthetic preference generation on structured rubrics and system prompts, the model learns to associate specific behaviors with specific rubric-defined quality levels. This allows the fine-tuned model to dynamically adjust its outputs at inference time based on the provided rubric prompt, rather than being locked into a single static preference model.

## Foundational Learning
- Direct Preference Optimization (DPO): A method for fine-tuning language models using preference data. Why needed: Enables learning from pairwise comparisons without explicit reward modeling. Quick check: Verify DPO implementation matches standard formulation (e.g., β = 0.1, logistic loss).
- LoRA (Low-Rank Adaptation): A parameter-efficient fine-tuning technique. Why needed: Reduces computational cost and storage compared to full fine-tuning. Quick check: Confirm LoRA rank and target modules are appropriate for the base model.
- Synthetic Data Generation: Creating training data via model outputs rather than human annotation. Why needed: Scales preference data creation for diverse rubric conditions. Quick check: Validate synthetic data quality via rubric adherence scores.
- System Prompts: Structured inputs derived from rubrics that guide model behavior at inference. Why needed: Provides explicit control over output style/quality levels. Quick check: Ensure system prompt formatting matches rubric structure.
- LLM Judge: Using a language model to evaluate outputs against rubrics. Why needed: Scalable, consistent evaluation compared to human judges. Quick check: Verify judge instructions include rubric definitions verbatim.
- Best-of-N Sampling: Generating multiple outputs and selecting the best one. Why needed: Improves final output quality by exploring diversity. Quick check: Confirm N and selection criteria are clearly defined.

## Architecture Onboarding
- Component Map: Rubrics → System Prompts → Teacher Model Generation → Preference Pairs → DPO Fine-tuning → Student Model
- Critical Path: System prompt generation and synthetic data quality are critical; poor teacher outputs propagate through the pipeline.
- Design Tradeoffs: Manual rubric creation ensures quality but limits scalability; synthetic data is scalable but may inherit teacher biases.
- Failure Signatures: Model ignores system prompts (accuracy similar across bins); judge scores cluster near moderate regardless of target.
- First Experiments: 1) Generate and score teacher outputs for rubric adherence (Table 2 replication). 2) Train student with synthetic preferences and evaluate accuracy/τ/ρ. 3) Test Best-of-N sampling improvement with CPT model.

## Open Questions the Paper Calls Out
- Can CPT effectively enable compositional control over multiple attributes simultaneously (e.g., specific levels of formality combined with specific creativity scores) without interference between attributes? The current design only validates single rubric conditioning, and the authors identify multi-attribute control as future work.
- Can the process of rubric and system prompt generation be fully automated without degrading the fidelity of the preference tuning? The study relies on manual rubrics, and the authors note scalability challenges due to resource-intensive rubric creation.
- To what extent do inherent biases in the teacher models propagate through the CPT pipeline and influence the student model's behavior? The paper evaluates rubric adherence but does not isolate or measure transfer of specific teacher biases to the student.
- Is the CPT framework effective for multimodal alignment tasks, such as controlling attributes in image-text pairs? The methodology is text-only, and the authors suggest extending to vision-language models as future work.

## Limitations
- Synthetic dataset size (900 samples) is modest; generalization to broader rubric types or real human preferences is unproven.
- Evaluation relies entirely on LLM judges, which may not perfectly align with human judgment.
- Approach assumes rubric structure is known and well-defined, limiting applicability to tasks lacking formalization.
- LoRA and DPO hyperparameters are not fully specified, introducing potential variability in replication.

## Confidence
- Claim: CPT enables dynamic behavior adjustment via rubric-guided preferences. Confidence: **High** (supported by strong empirical results across multiple base models).
- Claim: CPT generalizes beyond tested rubrics. Confidence: **Medium** (limited by modest dataset and single rubric type).
- Claim: Human-annotated preferences would yield similar results. Confidence: **Low** (not tested; synthetic data may have biases).

## Next Checks
- Validate teacher model generation quality by reproducing Table 2 scores for rubric adherence.
- Test CPT performance with human-annotated preference data instead of synthetic.
- Evaluate robustness across additional rubric types and base model families.