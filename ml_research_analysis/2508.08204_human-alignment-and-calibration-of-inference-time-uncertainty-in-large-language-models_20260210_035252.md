---
ver: rpa2
title: Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language
  Models
arxiv_id: '2508.08204'
source_url: https://arxiv.org/abs/2508.08204
tags:
- uncertainty
- measures
- human
- language
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates inference-time uncertainty quantification measures
  in large language models for alignment with human uncertainty and calibration. Using
  two datasets (2998 Roper survey questions and 38 Pew questions), it compares model
  uncertainty measures against human disagreement patterns.
---

# Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models

## Quick Facts
- arXiv ID: 2508.08204
- Source URL: https://arxiv.org/abs/2508.08204
- Reference count: 14
- This work evaluates inference-time uncertainty quantification measures in large language models for alignment with human uncertainty and calibration.

## Executive Summary
This paper investigates inference-time uncertainty quantification measures in large language models, specifically examining their alignment with human uncertainty patterns and calibration performance. Using two survey datasets (2998 Roper questions and 38 Pew questions) and the MMLU benchmark, the study identifies specific entropy-based measures that correlate strongly with human group-level uncertainty while also demonstrating good calibration. The research reveals that choice entropy, top-k entropy, and top-p entropy measures show significant alignment with human disagreement patterns despite weak alignment with human answer preferences, and these same measures exhibit moderate to strong calibration through both correctness correlation and Jensen-Shannon distance shift analysis.

## Method Summary
The study evaluates inference-time uncertainty quantification measures by querying LLMs for full token probability distributions over the vocabulary. Uncertainty metrics include total entropy, top-k entropy (k ∈ {5,10,25,50,100}), choice entropy (restricted to answer choice tokens), and top-p entropy/size. For alignment with human uncertainty, the paper computes Pearson correlations between model uncertainty measures and human group disagreement entropy on survey datasets (Roper and Pew). For calibration assessment, it uses Spearman correlation with correctness on MMLU and introduces a Jensen-Shannon distance (JSD) shift metric comparing distributional similarity between high-certainty and low-certainty model answers against ground truth. The analysis includes 5 models (LLaMa-3.2 1B/3B, LLaMa-3.1 8B, Mistral 7B v0.1/v0.3) and runs 1,000-iteration permutation tests for statistical significance.

## Key Results
- Multiple entropy-based measures (choice entropy, top-k entropy, top-p entropy) show strong correlation (r ≥ 0.5) with human group uncertainty
- These measures demonstrate weak alignment with human answer preferences but strong alignment with human disagreement patterns
- Choice entropy emerges as the best-performing measure for both alignment and calibration across nearly all models and subjects
- The JSD shift metric successfully identifies calibration, with statistically significant results (p=0.000) across most models and measures
- Mistral 0.1 7B Instruct shows negative JSD shift, indicating systematic overconfidence in wrong answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entropy-based measures calculated over the model's token probability distribution serve as a proxy for human population-level disagreement.
- **Mechanism:** When a question induces high disagreement among humans (high entropy in human responses), the LLM's next-token probability distribution exhibits similar flatness (high choice entropy or top-k entropy). This alignment occurs despite the model not necessarily sharing the same preference ordering as humans; the magnitude of uncertainty correlates even if the direction does not.
- **Core assumption:** Human group-level disagreement on survey questions is a valid ground-truth approximation for "inherent question uncertainty" that an LLM should mirror.
- **Evidence anchors:**
  - [Abstract] "Multiple entropy-based measures... show strong correlation (r ≥ 0.5) with human group uncertainty, despite weak alignment with human answer preference."
  - [Results] "Every model shows significant (|r| ≥ 0.3) correlation for all top-k measures... The same is true of choice entropy."
- **Break condition:** This mechanism may fail if the model has memorized specific answers (collapsed entropy) to questions that are subjectively uncertain to humans, or if the domain shifts to open-ended generation where "choice entropy" cannot be easily defined.

### Mechanism 2
- **Claim:** Jensen-Shannon Distance (JSD) shift provides a robust distributional signal for model calibration, complementing simple correctness correlation.
- **Mechanism:** Rather than just checking if high confidence implies correctness, this mechanism compares the distance between the model's answer distribution and the ground truth distribution under conditions of high vs. low certainty. A well-calibrated measure implies that when the model is "certain" (low uncertainty score), its answer distribution (LM) is significantly closer to the ground truth (LA) than when it is "uncertain" (HM vs HA).
- **Core assumption:** The validity of JSD shift relies on the assumption that "certainty" bins can be reliably established via standardized uncertainty scores, and that the ground truth remains a stable reference point (e.g., MMLU benchmarks).
- **Evidence anchors:**
  - [Calibration] "We leverage the JSD to examine how closely the distribution of answers given by the model matches the distribution of correct answers at high and low entropy."
  - [Results] Table 2 shows statistically significant p-values (p=0.000) for JSD shift across most models and measures, indicating the measure successfully detects calibration.
- **Break condition:** This mechanism breaks down if the model is systematically overconfident in wrong answers (negative calibration, as seen in Mistral 0.1 7B Instruct), where the JSD shift may invert or show conflicting signals compared to correlation metrics.

### Mechanism 3
- **Claim:** Restricting entropy calculations to valid answer choice tokens ("Choice Entropy") improves alignment and calibration compared to vocabulary-wide measures.
- **Mechanism:** By masking the full vocabulary and computing entropy only over the normalized probabilities of the provided answer choices (e.g., A, B, C, D), the metric focuses exclusively on the decision boundary relevant to the task. This filters out noise from irrelevant tokens and aligns the uncertainty calculation with the multiple-choice structure of the human survey data.
- **Core assumption:** The model assigns significant probability mass to the valid answer token labels (A, B, C...) relative to the rest of the vocabulary, and that normalizing these specific tokens captures the true decision uncertainty.
- **Evidence anchors:**
  - [Inference-Time UQ] "We also measure the uncertainty as the entropy over the normalized probabilities of the target tokens... We call this measure the choice entropy."
  - [Results] "There is a clear winner in the choice entropy. Across nearly all models and subjects, choice entropy shows mild to moderate correlation with correctness."
- **Break condition:** If the model spreads probability mass across semantic variations not captured by the single-token label (e.g., answering "The first option" instead of "A"), choice entropy might misrepresent the model's true uncertainty.

## Foundational Learning

- **Concept:** **Shannon Entropy**
  - **Why needed here:** The primary metrics (Total Entropy, Choice Entropy, Top-k Entropy) are all derived from Shannon entropy over probability distributions. Understanding how entropy quantifies "flatness" or uncertainty in a distribution is required to interpret the correlation results.
  - **Quick check question:** If a model assigns 90% probability to token A and 10% to token B, is the entropy higher or lower than if it assigned 50% to A and 50% to B?

- **Concept:** **Jensen-Shannon Divergence (JSD)**
  - **Why needed here:** The paper introduces a novel calibration metric based on JSD shift. One must understand JSD as a symmetric, bounded measure of similarity between two probability distributions to grasp how calibration is being assessed beyond simple accuracy.
  - **Quick check question:** Why would JSD be preferred over Kullback-Leibler (KL) divergence when comparing model answer distributions to ground truth distributions?

- **Concept:** **Bayesian Highest Density Credible Set**
  - **Why needed here:** The paper draws a theoretical connection between Top-p (nucleus) sampling and this Bayesian concept. Understanding this helps explain why Top-p size and entropy were investigated as potential uncertainty measures.
  - **Quick check question:** In Top-p sampling, how does the size of the candidate set change as the distribution becomes flatter (more uncertain)?

## Architecture Onboarding

- **Component map:** Input -> Logit Extractor -> Selector/Masker -> Normalizer -> Metric Engine -> Comparator
- **Critical path:** The extraction and normalization of the Choice Entropy is the most critical path for replication. One must ensure that when calculating entropy, the probabilities of only the valid answer tokens (e.g., "A", "B", "C", "D") are extracted and renormalized, rather than using the raw softmax over the full vocabulary which dilutes the signal.
- **Design tradeoffs:**
  - Choice Entropy vs. Total Entropy: Choice entropy is more calibrated and aligned for multiple-choice tasks but requires knowing the answer space a priori. Total entropy is task-agnostic but noisier.
  - Inference-time vs. Multi-sample UQ: The paper champions inference-time metrics for low computational cost (O(1) generations), but acknowledges this might sacrifice the robustness seen in Monte Carlo or Ensemble methods (which require O(N) generations).
- **Failure signatures:**
  - Inverted Calibration: As seen with Mistral 0.1 7B Instruct, the model may exhibit significant but negative JSD shift, meaning the model is more confident when wrong. Watch for positive correlations where negative ones are expected.
  - Preference Misalignment: Do not expect the model to rank-order preferences matching humans (Kendall τ). The mechanism supports uncertainty alignment, not preference alignment.
- **First 3 experiments:**
  1. Reproduce Choice Entropy Correlation: Take a subset of 50 questions from a survey dataset (e.g., Pew). Compute the correlation between model "Choice Entropy" and human disagreement percentages to verify the r ≥ 0.5 claim.
  2. Verify JSD Shift on MMLU: Implement the JSD Shift calculation. Partition a small MMLU subset into high/low certainty based on the model's own Choice Entropy and confirm that JSD(LM, LA) is lower than JSD(HM, HA).
  3. Ablate Top-k vs. Choice: Compare the calibration correlation of "Choice Entropy"