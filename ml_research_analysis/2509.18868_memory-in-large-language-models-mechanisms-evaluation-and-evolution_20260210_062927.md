---
ver: rpa2
title: 'Memory in Large Language Models: Mechanisms, Evaluation and Evolution'
arxiv_id: '2509.18868'
source_url: https://arxiv.org/abs/2509.18868
tags:
- memory
- evaluation
- knowledge
- retrieval
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for evaluating and governing
  memory in large language models (LLMs). The authors define LLM memory as a persistent
  state written during pretraining, finetuning, or inference that can later be addressed
  and stably influences outputs.
---

# Memory in Large Language Models: Mechanisms, Evaluation and Evolution

## Quick Facts
- arXiv ID: 2509.18868
- Source URL: https://arxiv.org/abs/2509.18868
- Reference count: 40
- One-line primary result: A comprehensive framework for evaluating and governing memory in LLMs across four types using a three-regime protocol that decouples model capability from information availability.

## Executive Summary
This paper proposes a unified framework for evaluating and governing memory in large language models (LLMs). The authors define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and stably influences outputs. They introduce a four-way taxonomy (parametric, contextual, external, and procedural/episodic) and a "memory quadruple" (storage location—persistence—write/access path—controllability) to characterize different memory types. The core method is a three-setting evaluation protocol (parametric-only, offline retrieval, online retrieval) that decouples model capability from information availability on the same data slice and timeline, enabling apples-to-apples comparisons across different memory mechanisms and evaluation setups.

## Method Summary
The paper's methodology centers on a three-regime evaluation protocol: Parametric-Only (no retrieval/tools), Offline-Retrieval (fixed index), and Online-Retrieval (dynamic sources). This protocol requires the same data slice and timeline across regimes to enable controlled comparisons. For each memory type, the authors build layered evaluation metrics: parametric memory uses closed-book recall, edit differentials, and memorization/privacy risks; contextual memory focuses on position curves and the mid-sequence drop; external memory evaluates answer correctness alongside snippet-level attribution/faithfulness; and procedural/episodic memory examines cross-session consistency and timeline replay. The governance component, DMM-Gov, coordinates continued pretraining, parameter-efficient finetuning, model editing, and retrieval-augmented generation into an auditable loop covering admission thresholds, progressive rollout, online monitoring, reversible rollback, and change audit certificates.

## Key Results
- Introduces a unified framework with four memory types (parametric, contextual, external, procedural/episodic) characterized by a "memory quadruple"
- Proposes a three-regime evaluation protocol that decouples model capability from information availability
- Demonstrates the "lost in the middle" phenomenon in contextual memory with positional performance curves
- Shows that RAG correctness and faithfulness are distinct dimensions requiring separate evaluation
- Presents DMM-Gov governance framework addressing timeliness, conflict handling, and long-horizon consistency

## Why This Works (Mechanism)

### Mechanism 1: Localized Key-Value Storage in FFNs
The paper claims that factual associations in LLMs are localized within specific mid-layer Feed-Forward Networks (FFN), which function as key-value memories. The FFN layers act as storage where a subject token (key) triggers a specific weight vector to retrieve associated attributes (value), allowing targeted editing of specific facts without retraining the entire model. This relies on the "locality" assumption that knowledge can be isolated to specific neural substrates rather than being distributed. The break condition occurs when sequential edits or batch updates cause "neighborhood interference" or "model collapse," where updating one fact degrades the model's ability to recall semantically related but distinct facts.

### Mechanism 2: Positional Dilution in Contextual Memory
The paper demonstrates that increasing the context window does not linearly increase the model's ability to utilize information, particularly in the middle of long sequences. Attention mechanisms exhibit a positional bias (often U-shaped), preferentially weighing tokens at the start and end of the context while diluting attention to mid-sequence tokens. This relies on the assumption that the self-attention mechanism has a finite effective "attention budget" that gets spread too thin over long sequences. The break condition is when retrieval accuracy drops significantly when the "needle" (key information) is placed in the middle of a context window exceeding the model's effective attention span.

### Mechanism 3: Decoupled Correctness and Faithfulness in RAG
In Retrieval-Augmented Generation (RAG), the paper shows that correctness of an answer and its faithfulness to retrieved evidence are distinct dimensions. The model may ignore retrieved evidence in favor of strong parametric memory priors or confabulate connections, requiring metrics that separate retrieval quality from generation faithfulness. This assumes LLMs prioritize fluent generation over strict evidential adherence unless explicitly constrained. The break condition occurs when the model retrieves the correct document but generates an answer that contradicts it (unfaithful) or answers from pre-training knowledge despite the retrieved context (parametric override).

## Foundational Learning

- **Concept: Knowledge Localization (Causal Tracing)**
  - **Why needed here:** To understand the "Location" dimension of the Memory Quadruple. You cannot evaluate or edit parametric memory without understanding where facts physically reside in the network layers.
  - **Quick check question:** Can you identify which layer (e.g., mid-layer MLP) is primarily responsible for storing the fact "The capital of France is Paris"?

- **Concept: The Memory Quadruple**
  - **Why needed here:** This is the paper's core definitional framework. It shifts the focus from "Does it remember?" to "How does it remember?" across four axes: Location, Persistence, Write/Access Path, and Controllability.
  - **Quick check question:** Classify "a chat history in a 128k context window" using the quadruple (Answer: Contextual Memory, transient persistence, read via attention).

- **Concept: The Three Operating Regimes**
  - **Why needed here:** Evaluation is meaningless without controlled baselines. You must distinguish between the model's internal capability (PO) and its performance with external help (Offline/Online Retrieval).
  - **Quick check question:** If a model fails a test in "Online Retrieval" mode but passes in "Parametric-Only," is the model broken or is the retrieval system broken? (Answer: The retrieval system).

## Architecture Onboarding

- **Component map:** Parametric Memory (Weights/FFNs, persistent, hard to update) -> Contextual Memory (Activations/KV Cache, transient, limited by positional bias) -> External Memory (Vector DBs/Documents, non-parametric, high updatability) -> Procedural Memory (Event logs/Timelines, manages cross-session consistency)

- **Critical path:** The "Write -> Read -> Inhibit/Update" loop
  1. **Write:** Pretraining writes to Params; Ingestion writes to External Indices
  2. **Read:** Attention reads Context; Retrieval reads External
  3. **Inhibit/Update:** Editing modifies Params; RAG overrides Params; Unlearning suppresses Params

- **Design tradeoffs:**
  - **Params:** High latency efficiency vs. Low timeliness/controllability
  - **Context:** High flexibility vs. Positional fragility (Mid-sequence drop)
  - **External:** High timeliness/auditability vs. Latency overhead and Retrieval errors

- **Failure signatures:**
  - **"Lost in the Middle":** Model fails to answer questions using evidence placed in the middle of a long prompt
  - **"Parametric Override":** Model ignores retrieved RAG context in favor of incorrect internal knowledge
  - **"Model Collapse":** Sequential editing destroys general capabilities

- **First 3 experiments:**
  1. **Parametric Audit:** Run a LAMA-style probe (fill-in-the-blank) to establish a Parametric-Only (PO) baseline for core facts
  2. **Positional Stress Test:** Place a specific fact at the start, middle, and end of a 50k-token context to plot the "U-shaped" performance curve and identify the "mid-sequence drop"
  3. **RAG Decoupling:** Evaluate a RAG pipeline by measuring "Answer Correctness" and "Citation Faithfulness" separately on the same dataset to identify if errors stem from retrieval (recall) or generation (grounding)

## Open Questions the Paper Calls Out
None

## Limitations
- The DMM-Gov governance framework requires substantial infrastructure investment that may not be feasible for smaller organizations
- The memory quadruple framework may oversimplify complex memory interactions and feedback loops between different memory types
- The focus on Transformers may limit generalizability to other architectures like state-space models or hybrid approaches

## Confidence

- **High Confidence**: The three-regime evaluation protocol and its ability to decouple model capability from information availability
- **Medium Confidence**: The FFN localization hypothesis for parametric memory
- **Medium Confidence**: The positional dilution mechanism in contextual memory
- **Low Confidence**: The proposed DMM-Gov governance framework

## Next Checks

1. **Cross-Architecture Generalization Test**: Validate the memory quadruple framework and evaluation protocols on non-Transformer architectures (Mamba, RWKV, hybrid models) to assess architectural universality

2. **Long-Horizon Conflict Resolution Experiment**: Design a controlled study where sequential edits create semantic conflicts and measure propagation to semantically related facts over extended timelines

3. **Resource-Constrained DMM-Gov Implementation**: Develop and test a minimal viable version of the governance framework suitable for smaller organizations, focusing on high ROI components relative to implementation complexity