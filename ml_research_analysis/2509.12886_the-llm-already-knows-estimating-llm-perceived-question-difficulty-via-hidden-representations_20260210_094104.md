---
ver: rpa2
title: 'The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden
  Representations'
arxiv_id: '2509.12886'
source_url: https://arxiv.org/abs/2509.12886
tags:
- difficulty
- question
- arxiv
- wang
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for estimating the difficulty of input
  questions as perceived by large language models (LLMs) using only the hidden representations
  generated by the target LLM, without requiring any output generation. The method
  models the token-level generation process as a Markov chain and defines a value
  function to estimate the expected output quality from any hidden state.
---

# The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations

## Quick Facts
- **arXiv ID**: 2509.12886
- **Source URL**: https://arxiv.org/abs/2509.12886
- **Reference count**: 37
- **Primary result**: A method to estimate LLM-perceived question difficulty using only hidden representations, achieving high ROC-AUC (91-94%) and Macro-F1 (75-81%) scores across textual and multimodal tasks.

## Executive Summary
This paper introduces a novel approach to estimate the difficulty of input questions as perceived by large language models (LLMs) using only the hidden representations generated by the target LLM, without requiring any output generation. The method models the token-level generation process as a Markov chain and defines a value function to estimate the expected output quality from any hidden state. Experiments across both textual and multimodal tasks show that the proposed method outperforms existing baselines in difficulty estimation, achieving high ROC-AUC and Macro-F1 scores. For example, on ScienceQA, the method achieves 93.09% ROC-AUC and 79.48% Macro-F1. The method is also applied to guide adaptive reasoning strategies like Self-Consistency, Best-of-N, and Self-Refine, resulting in higher inference efficiency with fewer generated tokens.

## Method Summary
The proposed method estimates question difficulty by analyzing the hidden representations of LLMs without generating full outputs. It formulates autoregressive generation as a Markov chain over hidden states, where each state captures the model's internal reasoning process. A value function is defined using the Bellman equation to estimate expected output quality from any hidden state. This value function is learned via temporal difference learning, minimizing the TD error between predicted and actual rewards (based on answer correctness). The method extracts hidden states from the target LLM, builds Markov states by combining hidden representations with generated tokens, and trains a lightweight neural network to map these states to expected quality scores. At inference, only the initial hidden state is needed to estimate difficulty, enabling efficient difficulty-based adaptive reasoning strategies.

## Key Results
- Achieves 93.09% ROC-AUC and 79.48% Macro-F1 on ScienceQA, outperforming existing baselines
- Successfully applies difficulty estimation to guide adaptive reasoning strategies (Self-Consistency, Best-of-N, Self-Refine), reducing token generation while maintaining accuracy
- Shows strong cross-domain generalization, with ROC-AUC scores of 65-89% when trained on one dataset and tested on another
- Hidden representations clearly separate easy from hard questions, validated through t-SNE visualization

## Why This Works (Mechanism)

### Mechanism 1: Markov Chain Formulation of Generation
- Claim: Modeling autoregressive generation as a Markov chain over hidden representations enables structured estimation of expected output quality.
- Mechanism: Each token generation step is a state transition: $H_{t+1}, y_{t+1} = f_\theta(H_t, y_t)$. The sequence of hidden representations forms a Markov chain where states encode the model's evolving reasoning process. A value function $V(s_t)$ is defined over states using the Bellman equation to estimate expected cumulative reward (output quality).
- Core assumption: Hidden representations contain sufficient information to predict eventual output quality without completing generation.
- Evidence anchors:
  - [abstract] "We model the token-level generation process as a Markov chain and define a value function to estimate the expected output quality given any hidden state."
  - [section 3.1-3.2] Formal definition of Markov states and value function derivation
  - [corpus] Weak corpus connection—Markov chain modeling of LLM generation is mentioned in related work (Kao et al., 2025) but not extensively validated for difficulty estimation
- Break condition: If hidden states do not contain separable information about eventual output quality (Figure 1 shows this assumption holds for tested models)

### Mechanism 2: Value Function Learning via Temporal Difference
- Claim: A lightweight neural network can learn to predict expected output quality from hidden states using TD learning objectives.
- Mechanism: Train $\hat{F}_\phi$ to minimize squared TD error: $\delta_t = \gamma \hat{F}_\phi(s_{t+1}) - \hat{F}_\phi(s_t)$ for non-terminal states, and $\delta_t = \gamma \text{Reward}(y) - \hat{F}_\phi(s_t)$ for terminal states. The reward is determined by outcome correctness.
- Core assumption: The reward signal (answer correctness) can be reliably extracted and serves as a valid proxy for difficulty.
- Evidence anchors:
  - [abstract] "This allows for efficient and accurate difficulty estimation based solely on the initial hidden state, without generating any output tokens."
  - [section 3.3] Training objective formulation with TD error
  - [corpus] No direct corpus evidence for TD learning specifically applied to LLM difficulty estimation
- Break condition: If answer extraction is unreliable or reward labeling is noisy (Table 9 shows GPT-4o extractor yields similar results, suggesting robustness)

### Mechanism 3: Hidden Representation Separability
- Claim: Easy and hard questions produce distinguishable hidden representations in the model's internal state space.
- Mechanism: The initial hidden state $s_0 = \{H_0, x\}$ encodes the model's "perception" of input difficulty before any generation occurs. t-SNE visualization (Figure 1) shows clear clustering of easy vs. hard questions.
- Core assumption: Difficulty information is encoded in hidden representations and is learnable by a simple 2-layer network.
- Evidence anchors:
  - [section 1, Figure 1] "these representations clearly separate easy from hard questions"
  - [section 5.2] High ROC-AUC (91-94%) and Macro-F1 (75-81%) across datasets
  - [corpus] Related work (Zhang et al., 2024b; Yin et al., 2024) suggests hidden representations carry fine-grained semantic information
- Break condition: If representations do not cluster by difficulty or the learned function fails to generalize across domains (Table 3 shows strong cross-domain generalization)

## Foundational Learning

- **Markov Decision Processes and Bellman Equations**
  - Why needed here: The method frames LLM generation as a Markov chain and uses value functions—core RL concepts. Understanding state transitions and bootstrapped value estimation is essential.
  - Quick check question: Given a state $s_t$, can you explain why $V(s_t) = \mathbb{E}[R(s_t) + \gamma V(s_{t+1})]$ allows estimation without rolling out the full trajectory?

- **Temporal Difference (TD) Learning**
  - Why needed here: The training objective minimizes TD error to learn the value function. This is not standard supervised learning—it requires understanding bootstrapping.
  - Quick check question: How does TD learning differ from Monte Carlo estimation in terms of variance and bias?

- **Hidden State Extraction from Transformer LLMs**
  - Why needed here: Implementation requires extracting hidden representations from specific layers (the paper uses last-layer states $h_t$).
  - Quick check question: In a transformer decoder, where would you hook to extract the hidden state for the last generated token?

## Architecture Onboarding

- **Component map:**
  1. Hidden State Extractor -> Markov State Builder -> Value Network -> Difficulty Classifier
  2. Training Pipeline (collect trajectories, compute TD errors, update value network) -> Inference Module (extract $s_0$, apply $\hat{F}_\phi$, compare to threshold)

- **Critical path:**
  1. Ground truth labeling: Run 3 independent inference attempts, label as "easy" if all correct, "hard" otherwise (or use reward model for open-ended tasks)
  2. Training data collection: For each sample, collect trajectory of $(s_0, s_1, ..., s_T)$ with final reward
  3. TD error computation: For each timestep, compute $\delta_t$ using next-state prediction or terminal reward
  4. Value network training: Minimize $\mathcal{L}_{TD} = \mathbb{E}[\sum_t \delta_t^2]$ across all trajectories
  5. Threshold calibration: Use validation set to determine $\tau$ that best separates easy/hard

- **Design tradeoffs:**
  - **Markov order ($k=1$ vs higher)**: Table 8 shows minimal impact (<1 point) when using $k=2,3$—suggests first-order is sufficient
  - **Number of inference attempts for ground truth**: Table 7 shows 3 attempts sufficient; more yield marginal gains
  - **Model specificity**: Value network does NOT generalize across different LLM architectures (Table 5 shows ROC-AUC drops to ~48% when training on Qwen and testing on LLaMA)
  - **Layer selection**: Paper uses last layer; other layers not explored—potential for earlier layers to capture different difficulty signals

- **Failure signatures:**
  1. **Closed-model deployment**: Cannot extract hidden states from proprietary APIs (explicit limitation noted in Section 8)
  2. **Multi-turn conversations**: Current formulation assumes single-turn inputs; state definition may need extension for conversational contexts
  3. **Cross-model transfer**: ROC-AUC ~47-50% (random chance) when applying value network trained on one model to another (Table 5)
  4. **Severe class imbalance**: Macro-F1 degrades if easy/hard classes are highly imbalanced, though ROC-AUC remains informative

- **First 3 experiments:**
  1. **Sanity check—representation visualization**: Extract hidden states for 200-500 samples, plot t-SNE colored by difficulty label. If no clustering visible, the core assumption fails.
  2. **Ablation—value network capacity**: Compare 1-layer vs 2-layer vs 3-layer networks. Paper uses 2-layer; test if simpler suffices or deeper helps.
  3. **Cross-domain generalization test**: Train on one dataset (e.g., ScienceQA), test on another (e.g., commonsenseQA) without retraining. Compare to Table 3 baselines to verify reported ~65-89% ROC-AUC holds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Markov chain formulation and value function be adapted to handle multi-turn conversational contexts where the difficulty depends on accumulated history?
- Basis in paper: [explicit] The Limitations section states the approach "currently focuses on single-turn inputs and may require adaptation for multi-turn or conversational settings."
- Why unresolved: The current method estimates difficulty solely from the initial hidden state $s_0$ of a single input $x$, without accounting for how state transitions evolve across dialogue turns.
- What evidence would resolve it: A modified framework that successfully estimates difficulty in multi-turn dialogues, maintaining high ROC-AUC scores as context length increases.

### Open Question 2
- Question: Can this difficulty estimation framework be extended to closed-source models where internal hidden representations are inaccessible?
- Basis in paper: [explicit] The authors note the method "requires access to token-level hidden representations... which may not be readily accessible in certain closed-source systems."
- Why unresolved: The value function is trained directly on the target LLM's hidden states, which are unavailable in black-box APIs.
- What evidence would resolve it: A proxy method (e.g., using output logits or external encoder embeddings) that correlates strongly with the hidden-state method without requiring internal model access.

### Open Question 3
- Question: Is it possible to train a universal value function that generalizes across heterogeneous model architectures without retraining?
- Basis in paper: [inferred] Table 5 shows that a value function trained on Qwen2.5-7B-Instruct fails to transfer to LLaMA-3.1-8B-Instruct, with ROC-AUC dropping to near-random levels (~47-50%).
- Why unresolved: Distinct architectures encode information differently, causing the learned estimation of expected output quality to fail on models with different representation spaces.
- What evidence would resolve it: Successful cross-model transfer experiments where a value function trained on one architecture family maintains high Macro-F1 scores on a structurally different model family.

## Limitations

- **Closed-model constraint**: The method fundamentally requires access to hidden states, making it inapplicable to closed APIs like GPT-4 or Claude, limiting practical deployment to research settings or self-hosted models.
- **Cross-model transfer failure**: Value networks trained on one architecture perform at chance level on different architectures (ROC-AUC ~47-50%), suggesting the difficulty signal is model-specific rather than task-specific.
- **Reward signal reliability**: The method depends on accurate ground truth labeling through multiple inference attempts, and its robustness to ambiguous or multi-valid answers remains untested.

## Confidence

- **High Confidence**:
  - The core mechanism works within the same model architecture
  - Hidden representations do separate easy/hard questions (t-SNE visualization, high ROC-AUC/Macro-F1)
  - The method improves inference efficiency when guiding adaptive strategies

- **Medium Confidence**:
  - The Markov chain formulation is valid for modeling generation
  - TD learning effectively trains the value function
  - 3 inference attempts provide sufficient ground truth

- **Low Confidence**:
  - Cross-model generalization potential
  - Performance on open-ended generation tasks without clear ground truth
  - Behavior with severe class imbalance or ambiguous questions

## Next Checks

1. **Cross-model fine-tuning experiment**: Train the value network on data from model A, then fine-tune on just 100-500 samples from model B. Measure if this significantly improves cross-model performance versus training from scratch on model B. This would test whether the difficulty signal is transferable with minimal adaptation.

2. **Noise robustness test**: Introduce varying levels of label noise (e.g., 5%, 15%, 30% of ground truth labels flipped) and measure degradation in ROC-AUC and Macro-F1. This would quantify how sensitive the method is to imperfect reward signals, which is critical for real-world deployment.

3. **Computational overhead benchmarking**: Measure wall-clock time and memory usage for: (a) full generation with baseline, (b) hidden state extraction + value network inference, and (c) combined approach with adaptive stopping. Compare against reported inference efficiency gains to verify the method is net-positive.