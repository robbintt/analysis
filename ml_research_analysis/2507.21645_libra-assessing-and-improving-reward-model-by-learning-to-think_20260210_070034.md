---
ver: rpa2
title: 'Libra: Assessing and Improving Reward Model by Learning to Think'
arxiv_id: '2507.21645'
source_url: https://arxiv.org/abs/2507.21645
tags:
- reward
- arxiv
- reasoning
- bench
- libra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Libra Bench, a reasoning-oriented reward model
  benchmark constructed from challenging mathematical problems and responses from
  advanced reasoning models. To address limitations of existing reward models in complex
  reasoning tasks, the authors propose a learning-to-think approach combining rejection
  sampling and reinforcement learning, resulting in the Libra-RM series.
---

# Libra: Assessing and Improving Reward Model by Learning to Think

## Quick Facts
- arXiv ID: 2507.21645
- Source URL: https://arxiv.org/abs/2507.21645
- Authors: Meng Zhou, Bei Li, Jiahao Liu, Xiaowen Shi, Yang Bai, Rongxiang Weng, Jingang Wang, Xunliang Cai
- Reference count: 40
- Primary result: Libra-RM-32B-MATH achieves 81.7% accuracy on Libra Bench, outperforming existing models

## Executive Summary
This paper addresses the limitations of existing reward models in complex reasoning tasks by introducing Libra Bench, a reasoning-oriented benchmark constructed from challenging mathematical problems and responses from advanced reasoning models. The authors propose a novel learning-to-think approach that combines rejection sampling and reinforcement learning to create the Libra-RM series. The approach demonstrates state-of-the-art performance on mathematical reasoning tasks while showing strong generalizability to general reward model benchmarks.

## Method Summary
The authors construct Libra Bench using challenging mathematical problems from sources like MATH-500 and AIME, combined with responses from advanced reasoning models. They develop a learning-to-think approach that first uses rejection sampling to generate high-quality training data from a teacher model (DeepSeek-R1), then applies reinforcement learning to refine the reward model's ability to assess reasoning quality. This dual approach aims to teach the model not just to judge answers but to understand the reasoning process itself. The resulting Libra-RM series, particularly the 32B variant, achieves superior performance on both mathematical and general reasoning benchmarks.

## Key Results
- Libra-RM-32B-MATH achieves 81.7% accuracy on Libra Bench, significantly outperforming AceMath-72B-RM (66.6%) and GPT-4.1 (69.1%)
- Strong generalizability demonstrated across general RM benchmarks beyond mathematical reasoning
- Downstream DPO experiments show correlation between Libra Bench accuracy and improved reasoning performance on AIME problems
- Validates Libra-RM's potential for RL data scaling with unlabeled data

## Why This Works (Mechanism)
The learning-to-think approach works by teaching reward models to understand reasoning processes rather than just final answers. Through rejection sampling, the model learns from high-quality reasoning traces generated by advanced models. The reinforcement learning phase then refines this understanding by exposing the model to diverse reasoning patterns and teaching it to identify subtle differences in reasoning quality. This creates a reward model that can better assess complex chains of reasoning, which is particularly valuable for mathematical and logical problem-solving where intermediate steps matter as much as final answers.

## Foundational Learning
- **Rejection Sampling**: Needed to filter high-quality reasoning examples from a teacher model; quick check: verify the sampling criteria produces diverse yet correct reasoning traces
- **Reinforcement Learning for RMs**: Needed to refine the reward model's ability to assess reasoning quality; quick check: ensure the reward signal properly distinguishes between correct and incorrect reasoning paths
- **Benchmark Construction**: Needed to create challenging evaluation scenarios; quick check: validate that problems cover a wide range of mathematical reasoning difficulties
- **DPO (Direct Preference Optimization)**: Needed for downstream policy improvement; quick check: confirm that reward improvements translate to better reasoning outputs
- **Reasoning Verification**: Needed to assess complex reasoning chains; quick check: test the model's ability to identify subtle errors in intermediate reasoning steps

## Architecture Onboarding
- **Component Map**: Problem Input -> Libra Bench Generator -> Rejection Sampling Engine -> RL Trainer -> Libra-RM Model -> DPO Pipeline
- **Critical Path**: Data generation (rejection sampling) → Reward model training (RL) → Performance validation (benchmarks) → Downstream application (DPO)
- **Design Tradeoffs**: Computational intensity of RL training vs. performance gains; mathematical specificity vs. general reasoning capability; synthetic data quality vs. real-world applicability
- **Failure Signatures**: Poor performance on novel problem types; inability to distinguish between correct reasoning and correct answers; overfitting to specific reasoning styles from the teacher model
- **First 3 Experiments**: 1) Benchmark Libra-RM on pure mathematical problems to establish baseline; 2) Test generalizability on non-mathematical reasoning tasks; 3) Apply Libra-RM as reward signal in online RL training to assess stability

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the Verifiable Reasoning to Verifiable Judging (V2V) strategy be effectively generalized to non-mathematical reasoning domains such as coding or formal logic?
- Basis in paper: [inferred] The paper constructs Libra Bench exclusively from "challenging mathematical problems" (MATH-500, AIME) and validates Libra-RM primarily on mathematical reasoning tasks (Section 3).
- Why unresolved: While the method proves effective for math, the authors do not demonstrate if the "thinking" capabilities and training pipelines transfer to domains where correctness is defined by code execution or logical axioms rather than numerical answers.
- What evidence would resolve it: Application of the V2V pipeline to code generation benchmarks (e.g., HumanEval) or logical entailment tasks, showing comparable performance gains over non-reasoning RMs.

### Open Question 2
- Question: Does high performance on Libra Bench correlate with improved policy model performance in online reinforcement learning settings (e.g., PPO/GRPO), or is it limited to offline DPO?
- Basis in paper: [inferred] The downstream experiments explicitly focus on "offline methods like DPO" and validate the "potential of Libra-RM for RL data scaling" through DPO experiments (Section 2 & Section 6).
- Why unresolved: The paper establishes a correlation between Libra Bench accuracy and DPO downstream performance, but it leaves open the question of whether these generative RMs remain stable and effective as reward signals during the iterative updates of online RL training.
- What evidence would resolve it: Results from online RL experiments (e.g., GRPO or PPO) where Libra-RM serves as the reward signal, showing improved reasoning capabilities without reward hacking.

### Open Question 3
- Question: To what extent is Libra-RM's performance attributable to learning reasoning skills versus learning the specific output distributions of the teacher model (DeepSeek-R1) used for rejection sampling?
- Basis in paper: [inferred] The training data for rejection sampling is "based on DeepSeek-R1" and the "non-judging data" is also sampled from it (Section 4.2).
- Why unresolved: There is a risk that the model learns to mimic the specific phrasing or "reasoning style" of DeepSeek-R1 rather than generalizable verification logic, potentially biasing it against responses from models with different reasoning styles.
- What evidence would resolve it: Ablation studies evaluating Libra-RM's accuracy on responses generated by a diverse set of models not represented in the rejection sampling teacher set.

## Limitations
- Benchmark-based evaluation may not fully capture real-world reasoning capabilities
- Computational resource intensity of reinforcement learning approach
- Focus on mathematical reasoning may limit direct applicability to other domains
- Potential bias from relying heavily on DeepSeek-R1 for rejection sampling data

## Confidence
- **High Confidence**: The 81.7% accuracy claim on Libra Bench is well-supported by direct benchmark results and clear comparative metrics
- **Medium Confidence**: Generalizability claims are based on performance across multiple datasets but lack extensive domain-specific validation
- **Medium Confidence**: The correlation between Libra Bench accuracy and AIME performance is demonstrated but requires more extensive validation across different problem domains

## Next Checks
1. Conduct ablation studies isolating the contributions of rejection sampling versus reinforcement learning components
2. Test Libra-RM's performance on non-mathematical reasoning benchmarks (e.g., scientific reasoning, logical inference tasks)
3. Evaluate the computational efficiency and resource requirements compared to simpler reward modeling techniques