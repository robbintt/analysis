---
ver: rpa2
title: 'EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language
  Models'
arxiv_id: '2511.07193'
source_url: https://arxiv.org/abs/2511.07193
tags:
- context
- emoji
- llms
- disambiguation
- emodis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMODIS, a new benchmark for evaluating large
  language models on context-dependent emoji disambiguation. EMODIS provides sentence-emoji
  pairs with contrastive contexts that lead to different interpretations, requiring
  models to reason beyond surface-level cues.
---

# EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models

## Quick Facts
- arXiv ID: 2511.07193
- Source URL: https://arxiv.org/abs/2511.07193
- Reference count: 13
- Primary result: Even GPT-4 achieves only 58.8% pair-wise accuracy on context-dependent emoji disambiguation, significantly below human performance of 88.5%.

## Executive Summary
This paper introduces EMODIS, a benchmark designed to evaluate large language models' ability to disambiguate emoji meanings based on contextual cues. The benchmark presents sentence-emoji pairs with contrastive contexts that lead to different interpretations, requiring models to perform genuine pragmatic reasoning rather than relying on surface-level patterns. Through evaluation of multiple API-based and open-source models, the authors demonstrate that current LLMs struggle with this task, exhibiting systematic biases toward dominant emoji interpretations and limited sensitivity to contextual changes.

## Method Summary
EMODIS comprises 1,000 manually curated instances, each containing an ambiguous sentence with an emoji, two distinct contexts leading to divergent interpretations, a question, and groundtruth answers. The evaluation uses pair-wise accuracy (both contexts must be answered correctly) and context sensitivity metrics. Models generate open-form text responses at temperature=0.2, which are evaluated against groundtruth using GPT-4 as an automatic verifier. The benchmark employs a taxonomy of context types: temporal information, domain theme, cultural background, and social intent.

## Key Results
- GPT-4 achieves 58.8% pair-wise accuracy, while human performance reaches 88.5%.
- Open-source models like LLaMA-7B perform significantly worse at 35.4% pair-wise accuracy.
- Models show systematic bias toward figurative interpretations regardless of context.
- Context awareness and output variability scores reveal limited ability to distinguish and respond to contextual differences.

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Context Pairing
Pairing identical target sentences with minimally different contexts exposes whether models perform genuine contextual reasoning versus surface-level pattern matching. Each instance presents (T, Câ‚, Q) and (T, Câ‚‚, Q) where the same emoji-containing sentence and question receive two distinct contexts leading to divergent interpretations. Pair-wise accuracy requires getting both correct simultaneously, penalizing models that default to a single interpretation regardless of context.

### Mechanism 2: Interpretation Bias Detection via Balanced Sampling
Measuring model preferences across emojis with equally valid literal and figurative meanings reveals systematic priors that override contextual evidence. By selecting examples where groundtruth distributions are balanced (e.g., 17:17 literal vs. figurative for red lantern), skewed model predictions indicate reliance on pretraining associations rather than context integration.

### Mechanism 3: Context Sensitivity Metrics as Diagnostic Signals
Context awareness (Score_câ‚) and output variability (Score_oáµ¥) quantify the gap between perceiving context changes and generating context-appropriate responses. Context awareness measures how often Oâ‚ â‰  Oâ‚‚ across pairs; output variability measures response changes when context is removed entirely. Low scores indicate context is treated as peripheral rather than semantically active.

## Foundational Learning

- **Concept: Semantic Ambiguity and Polysemy**
  - Why needed here: Emojis function as polysemous symbols where interpretation depends on discourse context, cultural knowledge, and social intent.
  - Quick check question: Can you explain why "She just sent a ðŸ‘ last night" requires context to disambiguate, and identify two distinct contexts that would lead to different interpretations?

- **Concept: Pragmatic vs. Semantic Reasoning**
  - Why needed here: The paper distinguishes between surface-level semantic understanding and pragmatic reasoning (inferring speaker intent, social dynamics, cultural references).
  - Quick check question: Given the sentence "She posted a ðŸ’£ after the announcement," what pragmatic inferences distinguish interpreting the emoji as "shocking news" vs. "literal weapon"?

- **Concept: Contrastive Evaluation Design**
  - Why needed here: The benchmark's core methodology relies on paired examples with controlled variation.
  - Quick check question: Why does pair-wise accuracy provide a stricter test than query-wise accuracy for context-dependent disambiguation?

## Architecture Onboarding

- **Component map:**
  Data Layer: 1,000 manually curated instances -> Target sentences -> Context pairs (Câ‚, Câ‚‚) mapped to 4-category taxonomy -> Questions -> Groundtruth answers
  Evaluation Layer: Inference prompts -> GPT-4 verifier -> Metrics computation

- **Critical path:**
  1. Data verification (3 annotators per instance) -> ensures emoji ambiguity is genuine and contexts lead to distinct interpretations
  2. Model inference (temperature=0.2) -> generates open-form text responses
  3. GPT-4 evaluation -> binary correctness judgment via semantic matching
  4. Metric aggregation -> pair-wise accuracy as primary signal, context sensitivity as diagnostic

- **Design tradeoffs:**
  - Manual curation vs. scale: 1,000 instances with rigorous verification limits breadth but ensures quality
  - Brief contexts vs. realistic complexity: Contexts are deliberately minimal to match current LLM capabilities
  - GPT-4 as verifier: >90% agreement with human evaluators validates reliability but introduces dependency

- **Failure signatures:**
  - Low Accâ‚š with moderate Accáµ§ -> model can answer correctly but doesn't switch interpretations when context changes
  - High output variability with low accuracy -> model generates different responses but for wrong reasons
  - Systematic bias toward figurative interpretations -> model defaults to pretraining priors regardless of context evidence
  - Performance variance across taxonomy categories -> uneven capability in temporal, cultural, or social reasoning dimensions

- **First 3 experiments:**
  1. Baseline establishment: Run GPT-4 and two open-source models to replicate reported Accâ‚š gaps
  2. Context ablation study: Compare model performance with contexts removed entirely
  3. Taxonomy-stratified analysis: Compute Accâ‚š separately for each of the four context types

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LLMs improve disambiguation accuracy when provided with complex, layered contexts compared to the minimal contexts currently used?
- **Open Question 2:** To what extent do multimodal cues (e.g., speaker identity, emotional tone) aid in resolving emoji ambiguity for LLMs?
- **Open Question 3:** How can the systematic bias toward dominant emoji meanings be effectively mitigated during training or inference?

## Limitations
- The evaluation relies entirely on GPT-4 as a verifier, introducing a single point of failure.
- The taxonomy of context types is incomplete, omitting speaker identity and emotional tone.
- The 1,000-instance dataset remains relatively small for comprehensive benchmarking.

## Confidence
**High Confidence**: The core finding that current LLMs struggle with context-dependent emoji disambiguation is well-supported by the substantial gap between human (88.5%) and model (58.8% for GPT-4) performance.

**Medium Confidence**: Claims about systematic bias toward figurative interpretations are supported by the data but require additional validation across diverse emoji sets and cultural contexts.

**Low Confidence**: The interpretation that low context awareness directly indicates pragmatic reasoning deficits is plausible but not definitively proven, as alternative explanations exist.

## Next Checks
1. **Cross-verifier validation**: Run a subset of EMODIS instances through multiple independent human evaluators to establish true variance in interpretation and validate GPT-4's role as arbiter.

2. **Domain expansion**: Create a complementary EMODIS subset focusing on temporal and speaker-identity contexts to test whether performance gaps persist when extending beyond current taxonomy boundaries.

3. **Model ablation study**: Compare performance across temperature settings (0.0, 0.2, 0.5, 1.0) to determine whether deterministic outputs artificially constrain context-sensitive behavior.