---
ver: rpa2
title: 'GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt
  Optimized LLMs'
arxiv_id: '2502.10522'
source_url: https://arxiv.org/abs/2502.10522
tags:
- node
- graph
- neighbors
- text
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphiT introduces an efficient framework for node classification
  on text-attributed graphs by encoding graph structures into concise textual representations
  and optimizing LLM prompts programmatically. The approach uses neighbor keyphrases
  instead of lengthy summaries to capture neighborhood information, reducing token
  usage and avoiding the "lost-in-the-middle" effect.
---

# GraphiT: Efficient Node Classification on Text-Attributed Graphs with Prompt Optimized LLMs

## Quick Facts
- arXiv ID: 2502.10522
- Source URL: https://arxiv.org/abs/2502.10522
- Authors: Shima Khoshraftar; Niaz Abedini; Amir Hajian
- Reference count: 34
- Primary result: GraphiT achieves 79.84%, 93.28%, and 57.25% accuracy on Cora, PubMed, and Ogbn-arxiv respectively using only 3 nodes/class for training

## Executive Summary
GraphiT introduces an efficient framework for node classification on text-attributed graphs by encoding graph structures into concise textual representations and optimizing LLM prompts programmatically. The approach uses neighbor keyphrases instead of lengthy summaries to capture neighborhood information, reducing token usage and avoiding the "lost-in-the-middle" effect. GraphiT employs the DSPy framework to automatically optimize both prompt instructions and demonstrative examples without manual tuning or labeled training data. Experimental results show GraphiT outperforms vanilla LLM and LLM-based baselines while being more token-efficient than methods using neighbor summaries.

## Method Summary
GraphiT encodes each node by concatenating its text attributes with neighbor labels and neighbor keyphrases extracted using KeyBERT. The system optimizes prompts via DSPy's COPRO instruction refinement and BootstrapFewShot demonstration selection, using only 3 nodes/class for training and 2 nodes/class for validation. Keyphrases are extracted from concatenated neighbor text using semantic similarity ranking and diversity filtering (n-grams ∈ {1,2,3}), producing compact representations. The optimized prompt with chain-of-thought reasoning is then applied to test nodes for classification.

## Key Results
- Achieves 79.84%, 93.28%, and 57.25% accuracy on Cora, PubMed, and Ogbn-arxiv datasets respectively
- Outperforms vanilla LLM and LLM-based baselines across all three datasets
- Reduces token usage by 3-5× compared to neighbor summary approaches while maintaining accuracy
- Bridges performance gap between LLM-only approaches and GNNs while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1: Neighbor Keyphrase Compression
The system extracts ζ keyphrases from concatenated neighbor text attributes using KeyBERT with semantic similarity ranking and diversity filtering, producing compact representations that avoid the "lost-in-the-middle" phenomenon where LLMs overlook information in lengthy contexts. This compression preserves classification-relevant information while reducing token consumption compared to full summaries.

### Mechanism 2: DSPy-driven Prompt Optimization
Automated prompt optimization via COPRO instruction refinement and bootstrap few-shot selection outperforms manual prompt engineering without requiring labeled training data. COPRO iteratively refines task instructions using coordinate ascent; bootstrap random search accumulates successful demonstrations where prediction score μ(φ(x), x') ≥ λ, then selects top-K via validation-set RP@K ranking.

### Mechanism 3: Homophily-based Neighbor Feature Aggregation
Incorporating 1-hop neighbor labels and keyphrases alongside node text attributes improves classification by exploiting structural similarity. For node vᵢ, the system concatenates its text attributes with neighbor labels lNᵢ = {l₀,...,lₖ} and shared neighbor keyphrases pNᵢ, forming a unified textual prompt for the LLM.

## Foundational Learning

- **Text-Attributed Graphs (TAGs)**: GraphiT operates specifically on graphs where nodes carry text attributes; understanding TAG structure is prerequisite to grasping why encoding matters.
  - Why needed: Explains the domain where GraphiT applies and why textual encoding is relevant
  - Quick check: Can you explain why shallow embeddings (bag-of-words, word2vec) may fail to capture contextual semantics compared to LLM-based embeddings?

- **DSPy Programming Model**: GraphiT relies on DSPy for prompt optimization; you must understand signatures, modules, and compilers to reproduce or extend the framework.
  - Why needed: DSPy is the core optimization engine that makes GraphiT work without manual prompt tuning
  - Quick check: What is the difference between COPRO (instruction optimization) and BootstrapFewShot (demonstration selection) in DSPy?

- **Keyphrase Extraction (KPE)**: The efficiency claim hinges on KPE replacing summaries; understanding KeyBERT's semantic ranking and diversity filtering explains why compression doesn't degrade performance.
  - Why needed: Keyphrase extraction is the primary mechanism for reducing token usage while preserving information
  - Quick check: How does Maximum Marginal Relevance (MMR) balance relevance vs. redundancy in keyphrase selection?

## Architecture Onboarding

- Component map: Node Feature Preparation -> DSPy Optimization Pipeline -> LLM Inference Layer
- Critical path: 1) Sample 3 nodes/class for training, 2 nodes/class for validation; 2) Run COPRO + BootstrapFewShot compilation to generate optimized instructions and demonstrations; 3) Apply optimized prompt to test nodes encoded with text + neighbor labels + neighbor keyphrases
- Design tradeoffs:
  - 1-hop vs. multi-hop neighbors: Paper uses 1-hop for efficiency; GCN captures 2-hop, explaining GraphiT's gap on Cora/Ogbn-arxiv
  - Keyphrases vs. summaries: Keyphrases reduce tokens 3–5× with comparable accuracy; summaries may help when neighbor texts are highly distinct
  - Small validation set: Enables low-data regime but risks overfitting; consider increasing val size for production deployments
- Failure signatures:
  - Accuracy degrades on heterophilic graphs where neighbor labels/keyphrases conflict with target node class
  - Optimized prompts overfit to validation set if RP@K threshold λ is too permissive or validation distribution diverges from test
  - Keyphrase extraction yields empty or generic results when neighbor texts share no common terminology
- First 3 experiments:
  1. Ablation on encoding components: Test (a) text only, (b) text + neighbor labels, (c) text + neighbor labels + keyphrases, (d) text + neighbor labels + summaries on your target dataset
  2. Token cost benchmark: Measure average prompt token count for keyphrases vs. summaries across 100 random nodes
  3. Validation set sensitivity: Vary validation size (1, 2, 5, 10 nodes/class) and measure test accuracy stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating multi-hop neighbor information (beyond 1-hop) into GraphiT close the performance gap with GCN on datasets like Cora and Ogbn-arxiv?
- Basis in paper: "Exploring the incorporation of neighbors beyond 1-hop in GraphiT will be one of our future research directions."
- Why unresolved: GraphiT currently only uses 1-hop neighbors, while GCN aggregates 2-hop information, which may explain GCN's superior performance on Cora and Ogbn-arxiv.
- What evidence would resolve it: Ablation experiments with 2-hop and 3-hop neighbor keyphrases showing improved accuracy on these datasets.

### Open Question 2
- Question: Can the GraphiT framework be successfully extended to other graph prediction tasks such as link prediction?
- Basis in paper: "Furthermore, we will extend our approach to other graph prediction tasks, including link prediction."
- Why unresolved: The current framework and experiments focus exclusively on node classification; the encoding and prompt optimization approach may need modification for edge-level predictions.
- What evidence would resolve it: Modified GraphiT implementation for link prediction with benchmark comparisons against existing LLM-based and GNN baselines.

### Open Question 3
- Question: How does GraphiT perform on heterophilic graphs where the homophily assumption (connected nodes are similar) does not hold?
- Basis in paper: The method explicitly relies on the homophily assumption, but all evaluated datasets are citation networks which typically exhibit homophily.
- Why unresolved: The approach of aggregating neighbor keyphrases assumes neighbors provide relevant class information, which may fail when neighbors are dissimilar to the target node.
- What evidence would resolve it: Experiments on established heterophilic graph benchmarks comparing GraphiT against both LLM and GNN baselines.

## Limitations

- DSPy optimization is sensitive to validation set size and distribution; using only 2 nodes/class for validation may lead to overfitting or poor generalization
- The homophily assumption underlying neighbor feature aggregation is unverified for heterophilic graphs or datasets with mixed topical content
- The exact KeyBERT diversity module configuration is unspecified, potentially affecting keyphrase quality and downstream accuracy
- Dataset preprocessing differences may explain accuracy variations across runs not captured in the paper

## Confidence

- **High confidence** in token efficiency claims: The 3–5× reduction from keyphrases vs. summaries is measurable and directly observable in prompt construction
- **Medium confidence** in accuracy improvements: Results show consistent gains over baselines, but small validation sets and lack of hyperparameter details create uncertainty about reproducibility across domains
- **Low confidence** in cross-dataset generalization: DSPy optimizations appear dataset-specific, raising questions about performance transfer

## Next Checks

1. **Ablation study on validation set size**: Vary validation nodes per class (1, 2, 5, 10) and measure test accuracy and overfitting risk on Cora to determine minimum viable validation set
2. **Heterophily stress test**: Apply GraphiT to a known heterophilic graph (e.g., fraud detection network) and compare neighbor label performance vs. isolated node text classification
3. **Token cost verification**: Instrument the pipeline to measure average prompt token count for keyphrases vs. summaries across 100 random nodes; confirm the claimed 3–5× reduction holds across all datasets