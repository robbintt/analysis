---
ver: rpa2
title: Are Large Language Models Dynamic Treatment Planners? An In Silico Study from
  a Prior Knowledge Injection Angle
arxiv_id: '2508.04755'
source_url: https://arxiv.org/abs/2508.04755
tags:
- insulin
- glucose
- prior
- rate
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper rigorously compares zero-shot inference performance of
  open-source LLMs with small neural-network-based RL agents (SRAs) for dynamic insulin
  dosing in Type 1 diabetes using the SimGlucose simulator. Expert knowledge was injected
  into SRAs via conservative exploration and into LLMs via structured system prompts
  and chain-of-thought reasoning.
---

# Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle

## Quick Facts
- arXiv ID: 2508.04755
- Source URL: https://arxiv.org/abs/2508.04755
- Reference count: 32
- Primary result: Qwen2.5-7B achieves 62.8% normalized return in zero-shot dynamic insulin dosing, outperforming smaller neural-network-based RL agents

## Executive Summary
This paper rigorously compares zero-shot inference performance of open-source LLMs with small neural-network-based RL agents (SRAs) for dynamic insulin dosing in Type 1 diabetes using the SimGlucose simulator. Expert knowledge was injected into SRAs via conservative exploration and into LLMs via structured system prompts and chain-of-thought reasoning. SRAs showed contradictory performance with prior knowledge: DQN suffered in harder environments while PPO improved. Qwen2.5-7B achieved 62.8% normalized return, outperforming smaller SRAs, especially in adult cohorts. Chain-of-thought prompting improved larger models but led smaller models to overly aggressive dosing and increased hypoglycemia risk. LLMs demonstrated strong zero-shot performance and simpler prior knowledge integration but still face challenges with latent clinical states and temporal reasoning.

## Method Summary
The study uses SimGlucose simulator for Type 1 diabetes management with 15-minute timesteps over 16-hour episodes. Two SRA approaches (DQN with 11-bin discrete actions, PPO with continuous actions) were trained for 20 epochs. Four LLM families (Qwen2.5, LLaMA3, LLaMA2, Gemma) were evaluated zero-shot with structured prompts including basic, expert knowledge, chain-of-thought, and meal-aware variants. Expert knowledge injection used conservative exploration for DQN (biased zero-dose probability) and Tanh transformations for PPO. LLMs received structured natural language prompts encoding clinical heuristics. Performance was measured using survival rate, time-in-range, and normalized return across adult, adolescent, and child cohorts.

## Key Results
- Qwen2.5-7B achieved 62.8% normalized return, outperforming smaller SRAs especially in adult cohorts
- Chain-of-thought prompting improved larger models but caused smaller models (<10B) to produce overly aggressive dosing and increased hypoglycemia risk
- DQN with conservative exploration showed degraded performance in harder environments while PPO improved modestly
- Prior knowledge prompts significantly improved LLM performance, with expert knowledge prompt yielding highest returns

## Why This Works (Mechanism)

### Mechanism 1: Prior Knowledge Injection via Natural Language vs. Exploration Constraints
LLMs enable simpler prior knowledge integration through structured prompts compared to algorithm-specific exploration modifications in traditional RL. For RL agents, clinical knowledge is injected through domain-informed exploration strategies (biased action selection for DQN, Tanh transformations for PPO). For LLMs, the same knowledge is embedded through natural language instructions specifying safety thresholds, delayed insulin effects, and dosing constraints. This approach assumes clinical heuristics expressed in natural language can transfer to decision-making behavior without environment-specific training.

### Mechanism 2: Zero-Shot Decision-Making via Pre-trained Clinical Knowledge
Certain LLMs (particularly Qwen2.5-7B and larger) can achieve clinically meaningful insulin dosing decisions through zero-shot inference, leveraging pre-trained medical knowledge. LLMs encode implicit physiological relationships and clinical heuristics from pre-training on medical literature. When presented with structured patient observations, the model maps these to appropriate dosing decisions without gradient updates. This assumes pre-training corpora contain sufficient signal about insulin-glucose dynamics and clinical decision patterns to support reasonable dosing behavior.

### Mechanism 3: Chain-of-Thought Induces Aggressive Dosing in Smaller Models
Chain-of-thought prompting improves performance in larger models but causes smaller models (<10B parameters) to produce overly aggressive insulin dosing, increasing hypoglycemia risk. CoT prompts force models to generate intermediate reasoning steps before outputting a dose. In smaller models, this reasoning chain introduces arithmetic errors, circular logic, and inflated dose calculations that the model fails to self-correct before the final answer. Larger models maintain better numerical coherence throughout the reasoning trace.

## Foundational Learning

- **Concept: Markov Decision Processes and Partial Observability**
  - Why needed here: The SimGlucose environment presents partially observed dynamics—meal events and individual patient parameters are latent. Understanding why this challenges both RL agents (requiring careful exploration design) and LLMs (lacking explicit state estimation mechanisms) is essential for interpreting the results.
  - Quick check question: Given only glucose measurements and insulin history every 15 minutes, can you distinguish between a rising glucose trend caused by a recent meal versus decreased insulin sensitivity? If not, what information is missing?

- **Concept: On-Policy vs. Off-Policy Reinforcement Learning**
  - Why needed here: The paper shows contradictory effects of prior knowledge injection: DQN (off-policy) degrades in harder environments with conservative priors while PPO (on-policy) benefits modestly. This stems from how each algorithm uses experience—off-policy methods can be poisoned by inconsistent value estimates from misaligned exploration.
  - Quick check question: If an RL agent's exploration strategy produces many unsafe trajectories, would an on-policy or off-policy algorithm be more likely to have its learned value function corrupted by those experiences? Why?

- **Concept: Emergent Behavior vs. Parametric Knowledge in LLMs**
  - Why needed here: The paper tests whether pre-trained medical knowledge alone suffices for clinical decision-making, versus requiring fine-tuning or architectural modifications. Understanding the difference between what LLMs "know" parametrically versus what they can perform via in-context reasoning clarifies why zero-shot inference works here but may fail elsewhere.
  - Quick check question: When an LLM correctly recommends reducing insulin dose after observing declining glucose, is it applying a learned clinical rule or pattern-matching against similar scenarios in its training data? How would you distinguish these?

## Architecture Onboarding

- **Component map:**
  SimGlucose Simulator -> Observation Formatter -> [SRA Path (DQN/PPO trained with exploration modifications) OR LLM Path (Qwen/LLaMA zero-shot with structured prompts + optional CoT)] -> Action: Insulin rate [0-9] U/h -> Evaluation: Survival Rate, Time-In-Range, Normalized Return

- **Critical path:**
  1. Prompt design is the highest-leverage intervention for LLMs—expert knowledge prompts improve performance significantly over baseline
  2. Model selection (family + parameter count) matters more than expected; Qwen2.5 consistently outperforms LLaMA3, and 7B appears sufficient for this task
  3. Decoding temperature of 0.7 outperforms deterministic decoding, suggesting controlled stochasticity helps exploration-like behavior

- **Design tradeoffs:**
  - CoT vs. direct prompting: CoT provides interpretability but introduces failure modes (arithmetic hallucination, aggressive dosing) in smaller models. Use CoT only with models >14B parameters or when reasoning transparency justifies performance cost.
  - RL training vs. LLM inference: RL requires environment-specific training but can optimize directly for clinical objectives. LLMs require no training but cannot easily incorporate patient-specific physiological parameters beyond prompt context.
  - Discrete vs. continuous action space: DQN with discrete actions enables biased exploration but reduces dosing precision; PPO with continuous actions is more flexible but requires careful action transformation (Tanh vs. clip) for safety.

- **Failure signatures:**
  - Arithmetic hallucination: Intermediate calculations produce implausible values that are not caught before final output
  - Temporal myopia: Overreaction to short-term glucose fluctuations without considering cumulative insulin effects
  - Aggressive dosing under uncertainty: Recommending high insulin doses when glucose trends are ambiguous
  - Detached reasoning: Final dose recommendation contradicts intermediate reasoning steps without self-correction

- **First 3 experiments:**
  1. Reproduce baseline comparison: Take best-performing SRA (DQN trained on child environment without prior knowledge) and best-performing LLM (Qwen2.5-7B with expert prompt, temperature 0.7). Evaluate both on adult, adolescent, and child cohorts to verify performance gap.
  2. Ablate prompt components: Systematically remove sections from expert knowledge prompt to identify which components drive performance improvement.
  3. Test CoT on 14B+ model: Select Qwen2.5-14B or 32B and compare direct prompting vs. CoT vs. meal-aware CoT on child cohort specifically. Document whether larger models recover from aggressive dosing pattern seen in 7B.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs effectively infer latent clinical states (e.g., meal intake or insulin sensitivity) in dynamic treatment regimes, given that explicit chain-of-thought reasoning failed to capture these hidden dynamics?
- Basis in paper: The authors note that "Incorporating explicit reasoning about latent clinical states... yielded minimal performance gains" and conclude that "latent-state inference of meal effects remains an unsolved challenge."
- Why unresolved: The paper demonstrates that current prompting strategies cause models to misinterpret physiological cues, leading to aggressive dosing rather than accurate state estimation.
- What evidence would resolve it: A study demonstrating a method (e.g., tool-use, hybrid architectures, or fine-tuning) that successfully infers latent variables and improves clinical metrics over zero-shot baselines.

### Open Question 2
- Question: Can chain-of-thought (CoT) prompting be redesigned to mitigate the risk of overly aggressive dosing and arithmetic hallucination in safety-critical clinical applications?
- Basis in paper: The results show CoT prompting "led smaller models to overly aggressive dosing," and the discussion calls for "prompt designs that support... uncertainty awareness" to prevent unsafe interventions.
- Why unresolved: The paper evaluated standard CoT, finding it causes smaller models to panic and overdose; a safety-constrained reasoning framework was not tested.
- What evidence would resolve it: Evaluation of a modified CoT prompt that enforces "deferred intervention" logic, showing reduced hypoglycemia rates without compromising Time-in-Range.

### Open Question 3
- Question: Do hybrid architectures combining LLMs with structured physiological models outperform pure zero-shot LLMs in handling volatile patient cohorts (e.g., children/adolescents)?
- Basis in paper: The abstract and conclusion advocate for "hybrid approaches that combine linguistic reasoning with structured physiological modelling" as a necessary step for robust systems.
- Why unresolved: The current study focused on pure zero-shot inference; the proposed hybrid integration remains a theoretical suggestion to address poor performance in volatile cohorts.
- What evidence would resolve it: Benchmarks from a hybrid system (e.g., LLM + differential equation simulator) showing statistically significant improvements in survival rates and normalized returns in the "Child" cohort compared to Qwen2.5-7B.

## Limitations
- SimGlucose simulator represents simplified physiological model compared to real-world diabetes management
- Patient-specific parameters and meal events remain latent in observation space, requiring inference from glucose trajectories alone
- Expert knowledge prompts were hand-crafted and may not generalize to other clinical domains without domain expertise

## Confidence
- **High Confidence:** Zero-shot performance of Qwen2.5-7B surpassing smaller RL agents in stable patient cohorts (adults)
- **Medium Confidence:** Chain-of-thought degradation in smaller models (<14B)
- **Low Confidence:** Claims about simpler prior knowledge integration via natural language compared to RL exploration constraints

## Next Checks
1. Evaluate best-performing LLM and SRA on an independent Type 1 diabetes simulator (e.g., UVA/Padova) to test performance generalization beyond SimGlucose.
2. Have board-certified endocrinologists review a subset of LLM decisions (particularly failure cases) to assess clinical reasonableness and identify patterns not captured by simulator metrics.
3. Conduct small-scale simulation using real patient data with clinically annotated outcomes to assess whether simulator-based performance translates to clinically meaningful decision support.