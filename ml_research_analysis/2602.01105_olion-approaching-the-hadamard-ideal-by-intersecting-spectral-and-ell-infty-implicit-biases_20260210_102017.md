---
ver: rpa2
title: 'OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$
  Implicit Biases'
arxiv_id: '2602.01105'
source_url: https://arxiv.org/abs/2602.01105
tags:
- muon
- sign
- spectral
- olion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes OLion, an optimizer that combines spectral\
  \ control (via orthogonalized update directions) with \u2113\u221E-style coordinate\
  \ control (via sign-based updates). By applying a sign operation after Newton-Schulz\
  \ orthogonalization, OLion approximates the intersection of spectral and \u2113\u221E\
  \ constraint sets\u2014a scaled Hadamard-like set for matrix parameters\u2014providing\
  \ an efficient way to combine both implicit biases."
---

# OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases

## Quick Facts
- **arXiv ID:** 2602.01105
- **Source URL:** https://arxiv.org/abs/2602.01105
- **Reference count:** 34
- **Primary result:** OLion combines spectral control (orthogonalized updates) with ℓ∞ control (sign-based updates) to match or outperform AdamW/Muon across large-scale language and vision pretraining and fine-tuning.

## Executive Summary
This paper introduces OLion, an optimizer that approximates the intersection of spectral and ℓ∞ constraint sets for matrix parameters by applying a sign operation after Newton-Schulz orthogonalization. The method aims to capture complementary implicit biases—spectral norm control from orthogonalization and ℓ∞ norm control from sign updates—without requiring full SVD computation. Under a diagonal-isotropy assumption on gradients, the authors prove convergence and demonstrate strong empirical performance across diverse large-scale training tasks, including GPT-2 and Llama pretraining, SiT image pretraining, and Llama-3.1-8B fine-tuning.

## Method Summary
OLion combines Lion-style momentum (single momentum buffer, Nesterov mixing) with Newton-Schulz orthogonalization (K=5 iterations) followed by elementwise sign normalization. The update direction is scaled by an RMS alignment factor (γ_t = 0.2/RMS(S_t)) to stabilize training. This yields an efficient approximation to the intersection of column-orthonormal matrices (spectral constraint) and uniform-magnitude matrices (ℓ∞ constraint), providing both implicit biases simultaneously. The method uses only momentum-level optimizer state, reducing memory overhead compared to AdamW.

## Key Results
- Matches or exceeds AdamW and Muon performance across GPT-2, Llama-2-7B, SiT, and Llama-3.1-8B fine-tuning tasks
- Maintains smaller spectral norms than AdamW and smaller ℓ∞ norms than Muon during training
- Reduces optimizer mismatch when fine-tuning AdamW-pretrained checkpoints
- Robust across learning rates from 3e-4 to 2e-3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sign-after-orthogonalization approximates the intersection of spectral and ℓ∞ constraint sets for matrix parameters.
- **Mechanism:** Orthogonalization projects onto the spectral extreme (column-orthonormal set A), then sign projects onto the ℓ∞ extreme (uniform-magnitude set B). Their composition yields updates reflecting both implicit biases.
- **Core assumption:** The intersection A∩B (scaled Hadamard matrices) is meaningful; composed projection is a reasonable one-step approximation.
- **Evidence anchors:** [abstract] efficient approximation to intersection; [Section 4] formal sets A and B with Equation (6); [corpus] related work on sign descent but not intersection geometry.
- **Break condition:** If orthogonalization destroys structure needed for meaningful sign alignment, intersection approximation degrades.

### Mechanism 2
- **Claim:** Diagonal-isotropy ensures sign-after-orthogonalization preserves directional alignment with orthogonalized update direction.
- **Mechanism:** Under diagonal-isotropy, diagonal entries of U^T·sign(UV^T)·V are nearly uniform, so inner product ⟨G_t - αQ_t, sign(Q_t)⟩ is tightly bounded, enabling descent with geometry-aware stationarity measure Φ_t.
- **Core assumption:** Assumption 4.2 (diagonal-isotropy decomposition) holds with small ε along training trajectory.
- **Evidence anchors:** [Section 4.2] formal statement and cancellation-aware bound; [Appendix B] Theorem B.1 proves random Gaussian matrices satisfy diagonal-isotropy with ε=o(1) when d_1·d_2 ≫ r·log(r); [Appendix C, Figure 8] empirical verification during GPT-2 training.
- **Break condition:** If ε becomes large (highly structured/sparse gradient matrices with strongly non-uniform diagonal correlations), descent guarantee weakens.

### Mechanism 3
- **Claim:** Combined implicit bias yields weight matrices with simultaneously small spectral norm and small ℓ∞ norm.
- **Mechanism:** Orthogonalization flattens singular value profile (spectral control), while sign normalization suppresses coordinate outliers (ℓ∞ control). Composition prevents either bias from dominating.
- **Core assumption:** Two biases do not conflict destructively; sign step after orthogonalization preserves enough spectral structure to maintain its benefit.
- **Evidence anchors:** [Section 5.3, Figure 2] OLion maintains smaller spectral norms than AdamW/Lion and smaller ℓ∞ norms than Muon; [Section 5.3, Figure 6] end-of-training distributions show OLion shifts singular values downward vs. AdamW and tightens entry magnitudes vs. Muon.
- **Break condition:** If sign operation substantially corrupts spectral structure (e.g., for near-low-rank matrices), benefits may diminish.

## Foundational Learning

- **Concept:** Steepest descent under norm-induced geometries
  - **Why needed here:** Paper frames optimizers as maximal-update methods under different norm constraints. Understanding orthogonalization ≈ spectral geometry and sign ≈ ℓ∞ geometry is essential.
  - **Quick check question:** Can you explain why applying sign(·) to a momentum vector corresponds to steepest descent under an ℓ∞ constraint?

- **Concept:** Newton-Schulz orthogonalization (polar factor approximation)
  - **Why needed here:** OLion uses K Newton-Schulz iterations to approximate polar factor O(G)=UV^T. This is the computational core enabling spectral control without full SVD.
  - **Quick check question:** What is the polar factor of a matrix Z=UΣV^T, and why does Newton-Schulz converge to it iteratively?

- **Concept:** Hadamard matrices
  - **Why needed here:** "Hadamard ideal" refers to matrices that are both orthogonal and have uniform-magnitude entries (±1/√d). This intersection motivates algorithm design.
  - **Quick check question:** For a square matrix, what constraints define the intersection of column-orthonormal matrices and matrices with entries ±1/√d?

## Architecture Onboarding

- **Component map:** Gradient computation → Momentum update (β₂) → Nesterov mix → Newton-Schulz orthogonalization (K=5) → Sign operation → RMS alignment → Weight update

- **Critical path:**
  1. Compute mini-batch gradient g_t
  2. Update momentum: M_t = β₂·M_{t-1} + (1-β₂)·g_t
  3. Nesterov mix: Ḡ_t = (1-β₁)·g_t + β₁·M_t
  4. Orthogonalize: Q_t = NewtonSchulz(Ḡ_t, K)
  5. Sign: S_t = sign(Q_t)
  6. Align: D_t = γ_t·S_t where γ_t = 0.2/RMS(S_t)
  7. Update: X_{t+1} = X_t - η_t·D_t - λ·η_t·X_t

- **Design tradeoffs:**
  - K (Newton-Schulz steps): More iterations → better orthogonalization but higher compute. Default K=5 balances accuracy and cost.
  - RMS alignment vs. fixed scale: RMS alignment stabilizes step sizes across layers/shapes but adds per-layer normalization pass.
  - Memory vs. AdamW: OLion uses ~2× less optimizer state (momentum only, no second-moment accumulator).

- **Failure signatures:**
  - Training instability with very high learning rates: Sign operation caps coordinate contribution but may still amplify noise in early training.
  - Mismatch warning: While OLion reduces mismatch vs. Muon, extreme pretrain-finetune distribution shifts may still cause issues.
  - Non-matrix parameters: Method designed for 2D weight matrices; biases/embeddings need separate handling (as in MuonAll variants).

- **First 3 experiments:**
  1. **Validation sanity check:** Train GPT-2 small (124M) on OpenWebText for 10K steps with default hyperparameters (β₁=0.95, β₂=0.98, K=5, η=6e-4). Verify validation loss converges comparably to AdamW baseline.
  2. **Learning rate sweep:** On same setup, test η ∈ {3e-4, 1e-3, 2e-3, 5e-3}. Confirm OLion maintains advantage across range (per Figure 5).
  3. **Implicit bias probe:** Track spectral norm and ℓ∞ norm of representative weight matrix (e.g., layer 4 attention projection) throughout training. Verify both norms stay smaller than AdamW (ℓ∞) and Muon (spectral) respectively.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multi-step intersection methods (e.g., using alternating projections or Dykstra's algorithm) yield better optimization performance or stability than OLion's single-step composed projection?
- **Basis in paper:** [explicit] Conclusion states Hadamard-intersection viewpoint "suggests richer operator-splitting or multi-step intersection solvers beyond a single composed projection."
- **Why unresolved:** Authors approximate intractable intersection using single projection step for efficiency but do not evaluate if iterative projection schemes better approximate Hadamard ideal.
- **What evidence would resolve it:** Empirical comparison of convergence rates and final loss between OLion and variants utilizing iterative intersection-seeking algorithms.

### Open Question 2
- **Question:** To what extent does OLion's sign-based update structure enable communication efficiency (e.g., 1-bit compression) in distributed training without compromising convergence?
- **Basis in paper:** [explicit] Introduction notes sign updates "naturally support communication-efficient multi-node training," and conclusion calls for "deeper study of communication efficiency and quantization behavior."
- **Why unresolved:** While sign operation theoretically allows heavy compression, paper does not provide empirical scaling results or theoretical guarantees for distributed data-parallel settings.
- **What evidence would resolve it:** Multi-node training experiments comparing wall-clock time and bandwidth usage of compressed OLion updates against full-precision AdamW or Muon.

### Open Question 3
- **Question:** Is diagonal-isotropy assumption (Assumption 4.2) strictly necessary for convergence, or does OLion robustly handle update directions that violate this condition?
- **Basis in paper:** [inferred] Theorem 4.4 requires Assumption 4.2 to bound alignment error. While Appendix B/C verifies it for random/GPT-2 data, authors do not prove it holds universally for all architectures or training stages.
- **Why unresolved:** Convergence proof relies on specific geometric structure of gradient singular vectors; failure cases are not characterized.
- **What evidence would resolve it:** Synthetic experiments on loss landscapes designed to violate diagonal-isotropy to determine if OLion's convergence degrades relative to baselines.

## Limitations
- Diagonal-isotropy assumption lacks broader theoretical guarantees for arbitrary network architectures
- Limited quantitative analysis of approximation error or convergence rate to Hadamard ideal
- No empirical validation of communication efficiency benefits from sign-based updates

## Confidence
- **High Confidence:** Convergence theory under diagonal-isotropy (Lemma 4.3, Theorem B.1), empirical implicit bias measurements (Figures 2, 6)
- **Medium Confidence:** Practical effectiveness across benchmarks (Figure 5, Table 1), optimizer mismatch mitigation
- **Low Confidence:** Theoretical justification for Hadamard ideal as optimization target, quantitative bounds on approximation quality

## Next Checks
1. **Diagonal-isotropy stress test:** Systematically vary layer width/depth ratios and matrix ranks during training to map boundary where Assumption 4.2 breaks down, measuring ε(t) across different architectural configurations.

2. **Intersection approximation analysis:** Quantify distance between actual OLion update direction and true intersection of spectral/ℓ∞ constraint sets using polar decomposition residuals and entrywise deviation metrics.

3. **Cross-architecture transfer:** Test OLion on transformer variants (e.g., MLP-Mixer, ConvNeXt) and recurrent architectures to validate whether combined implicit bias consistently improves optimization beyond standard architectures.