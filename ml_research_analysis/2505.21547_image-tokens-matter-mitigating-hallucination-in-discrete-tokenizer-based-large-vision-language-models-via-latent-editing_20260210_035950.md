---
ver: rpa2
title: 'Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based
  Large Vision-Language Models via Latent Editing'
arxiv_id: '2505.21547'
source_url: https://arxiv.org/abs/2505.21547
tags:
- image
- tokens
- visual
- hallucination
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses hallucination in Large Vision-Language Models
  (LVLMs) that use discrete image tokenizers, identifying that visual priors from
  co-occurring image tokens induce such hallucinations. The authors propose a two-step
  approach: Context-Guided Clustering (CGC) uses a Graph Neural Network and contrastive
  learning to cluster image tokens based on their co-occurrence patterns in segmented
  images, capturing visual priors.'
---

# Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing

## Quick Facts
- arXiv ID: 2505.21547
- Source URL: https://arxiv.org/abs/2505.21547
- Reference count: 40
- Primary result: CGC+VTD reduces hallucinations while maintaining general capabilities, outperforming recent baselines and being computationally efficient

## Executive Summary
This paper addresses hallucination in Large Vision-Language Models (LVLMs) that use discrete image tokenizers by identifying that visual priors from co-occurring image tokens induce such hallucinations. The authors propose a two-step approach: Context-Guided Clustering (CGC) uses a Graph Neural Network and contrastive learning to cluster image tokens based on their co-occurrence patterns in segmented images, capturing visual priors. Visual Token Decontamination (VTD) then mitigates hallucinations by suppressing the influence of visually absent tokens from dominant clusters during autoregressive decoding. Experiments on three LVLMs (Chameleon-7B, Janus-Pro-7B, Emu3-13B) across three benchmarks (AMBER, Object HalBench, MME) show CGC+VTD reduces hallucinations while maintaining general capabilities, outperforming recent baselines and being computationally efficient.

## Method Summary
The method works in two phases. First, Context-Guided Clustering (CGC) constructs a graph from co-occurring image tokens in segmented COCO images, using spatial proximity and semantic segmentation to weight edges. A GNN trained with InfoNCE contrastive learning learns embeddings that capture these co-occurrence patterns, which are then clustered using K-means. Second, Visual Token Decontamination (VTD) identifies absent tokens from dominant clusters during inference and suppresses their influence by subtracting their latent representations from present token representations at intermediate decoder layers. This reduces the model's tendency to "evoke" visually absent tokens that frequently co-occur with present ones.

## Key Results
- CGC+VTD reduces CHAIR hallucination scores across all three LVLMs (Chameleon-7B, Janus-Pro-7B, Emu3-13B) on AMBER, Object HalBench, and MME benchmarks
- The method maintains general capabilities as measured by MME scores while slightly reducing object coverage
- VTD layer selection shows intermediate-to-deep layers (20-25) perform best, avoiding the last 2 layers
- Ablation studies confirm both spatial and semantic components in CGC are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Image token co-occurrence patterns create visual priors that induce hallucinations
- Mechanism: When certain image tokens frequently co-occur during tokenizer training, they form strong associations. During inference, if some tokens from a cluster appear in an image, the model may "evoke" absent tokens from the same cluster due to these learned associations, leading to hallucinations of objects not visually present.
- Core assumption: Co-occurrence patterns in training data create statistical biases that persist in the tokenizer's codebook embeddings.
- Evidence anchors:
  - [abstract] "When certain image tokens frequently co-occur... they become strongly associated with the verbalizations of those objects. As a result, the model may hallucinate by evoking visually absent tokens that often co-occur with present ones."
  - [Section 3.2] "C2—tokens that do not appear in the image but belong to the most dominant cluster—consistently achieves significantly higher HitRate values (5–10% higher) than both C1 and C3"
  - [corpus] Related work on token-level hallucination detection (EAZY, PAINT) similarly identifies specific tokens as hallucination sources, but focuses on attention/logits rather than co-occurrence priors
- Break condition: If hallucinations were uncorrelated with dominant cluster absent tokens, this mechanism would not hold

### Mechanism 2
- Claim: Spatial and semantic co-occurrence signals improve clustering for hallucination detection
- Mechanism: CGC combines (1) spatial proximity (tokens within 3×3 grids) and (2) semantic coherence (tokens within same segmentation mask) to weight graph edges. This captures both local visual patterns and object-level associations better than raw embedding similarity.
- Core assumption: Tokens representing the same object or nearby regions carry meaningful co-occurrence patterns that embedding similarity alone misses.
- Evidence anchors:
  - [Section 3.1] "These two signals together capture both fine-grained spatial context and broader semantic relationships"
  - [Table 4] Ablation shows removing either spatial or semantic components degrades CHAIR-s from 38.58 to 42.15/43.67 on Chameleon
  - [corpus] Sparse autoencoder approaches (SAVE) similarly aim to isolate hallucinatory features but require different training; CGC remains post-hoc
- Break condition: If naive K-means on codebook embeddings achieved comparable HitRate@K, the graph construction would be unnecessary

### Mechanism 3
- Claim: Latent subtraction of absent token contributions suppresses hallucinative influence
- Mechanism: VTD identifies absent tokens from dominant clusters, computes their hidden representations at a specific layer, and subtracts a normalized projection from present image token representations. This reduces the "pull" of co-occurrence-driven absent tokens during generation.
- Core assumption: Hallucinatory influence can be isolated to specific token representations at intermediate layers and removed via vector subtraction.
- Evidence anchors:
  - [Section 3.3] Equation 4: "g(l)(vi) := g(l)(vi) − γ · ĝ(l)(vi) · ĝ(l)(vhal)/||ĝ(l)(vhal)||²² · g(l)(vhal)"
  - [Figure 5] "editing intermediate-to-deep layers (avoiding the last 2) yields optimal results... early layers aggregate information while deeper layers form conceptual representations"
  - [corpus] PROJECTAWAY uses similar latent editing for continuous-feature LVLMs; CGC+VTD adapts this for discrete tokenizers
- Break condition: If editing degraded general capabilities (MME scores dropped significantly), the intervention would be over-aggressive

## Foundational Learning

- Concept: **Discrete image tokenization (VQ-VAE/VQGAN)**
  - Why needed here: The paper's mechanism depends on understanding how images become discrete tokens from a finite codebook
  - Quick check question: Can you explain why a finite codebook creates token reuse across different visual concepts?

- Concept: **Graph Neural Networks with contrastive learning**
  - Why needed here: CGC uses GNN+InfoNCE to learn token embeddings that reflect co-occurrence patterns
  - Quick check question: How does contrastive learning encourage connected nodes (positive pairs) to cluster together?

- Concept: **Autoregressive decoding in transformers**
  - Why needed here: VTD operates during generation by modifying intermediate hidden states at each step
  - Quick check question: At which layer would you expect "conceptual representations" to be most formed—early, middle, or late layers?

## Architecture Onboarding

- Component map:
  ```
  Offline (run once per model):
    COCO Panoptic → Image tokenizer → Co-occurrence graph → GNN training → K-means clusters
  
  Inference (per image):
    Input image → Tokenize → Identify dominant clusters → Find absent tokens in those clusters
                                                                          ↓
    For each generation step: Extract layer-l hidden states → Subtract absent token projections → Continue
  ```

- Critical path:
  1. Graph construction quality (top-10% edges, spatial+semantic weighting)
  2. Cluster identification (HitRate@K analysis validates C2 tokens correlate with hallucinations)
  3. Layer selection for editing (intermediate-to-deep layers, not last 2)
  4. Magnitude coefficient γ (too high degrades coverage, too low fails to suppress)

- Design tradeoffs:
  - **Cluster size**: Larger clusters capture more co-occurrence but include irrelevant tokens; paper uses 10
  - **# dominant clusters**: More clusters catch more hallucination sources but risk over-suppression; varies 2-4 by model
  - **Editing layer**: Earlier layers may miss conceptual associations; later layers too close to output
  - **Coverage vs. hallucination**: Table 1 shows slight Cover drops, indicating trade-off

- Failure signatures:
  - MME scores drop significantly → γ too high or wrong layer
  - CHAIR doesn't improve → clustering failed to capture meaningful co-occurrence (check CGC training convergence)
  - Coverage drops sharply → over-aggressive suppression of absent tokens
  - VCD combination fails → contrastive decoding's noise perturbation conflicts with targeted subtraction (Table 2)

- First 3 experiments:
  1. **Validate CGC clustering quality**: Run HitRate@K analysis (Section 3.2) on 100 images; verify C2 tokens (absent from dominant clusters) show higher hallucination correlation than C1/C3. If not, debug graph construction.
  2. **Layer sweep**: Fix γ=0.5, test layers {15, 20, 25, 28} on AMBER generative task. Confirm intermediate layers outperform early/late (Figure 5 pattern).
  3. **Ablate spatial vs. semantic**: Compare w.o. Spatial vs. w.o. Semantic variants (Table 4) on your target model. Both should degrade performance if CGC is working correctly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the trade-off between minimizing object hallucination and maintaining high visual coverage (recall) be optimized?
- Basis in paper: [explicit] The authors note in the Limitations section that they "observe a slight drop in coverage scores (Cover ↑ in Table 1) with our proposed method, suggesting a trade-off between reducing hallucinations and maintaining comprehensive object recognition."
- Why unresolved: The Visual Token Decontamination (VTD) method suppresses latent embeddings associated with hallucination-prone tokens, which effectively reduces false positives but inadvertently suppresses some correct visual features, leading to lower object coverage.
- What evidence would resolve it: A modified mitigation technique that maintains or improves the Cover metric on the AMBER benchmark while simultaneously achieving state-of-the-art CHAIR (hallucination) scores.

### Open Question 2
- Question: Can the visual prior editing framework be generalized to Large Vision-Language Models that utilize continuous visual features (e.g., LLaVA) rather than discrete image tokenizers?
- Basis in paper: [explicit] The Limitations section states: "Our method is specifically designed for LVLMs with discrete image tokenizers and is not directly applicable to earlier models such as LLaVA... Extending our approach to bridge this gap and generalize across different visual encoding paradigms presents an important direction."
- Why unresolved: The current Context-Guided Clustering (CGC) relies on constructing a graph from a finite codebook of discrete tokens. Continuous models lack this discrete vocabulary, making the current co-occurrence clustering mechanism inapplicable without developing a method to identify equivalent semantic clusters in continuous space.
- What evidence would resolve it: A successful adaptation of the method applied to a continuous-feature LVLM (e.g., LLaVA-v1.6) demonstrating statistically significant hallucination reduction.

### Open Question 3
- Question: Why do Contrastive Decoding (CD) methods fail on discrete tokenizer-based LVLMs, and can they be adapted to avoid inducing overgeneration?
- Basis in paper: [explicit] The experiments section notes that "CD-based methods, VCD and SID, tend to underperform" and "consistently yield higher CHAIR scores." The authors highlight this in Appendix D.4, observing that VCD increases response length significantly (up to 223%), causing instability.
- Why unresolved: The paper identifies the failure mode—noise perturbation appears to destabilize discrete token generation leading to verbose and hallucinated outputs—but does not propose a fix for CD in this specific model paradigm.
- What evidence would resolve it: A theoretical explanation or a modified contrastive decoding objective that lowers the CHAIR score for discrete-token models (like Chameleon or Emu3) below the Nucleus Sampling baseline.

### Open Question 4
- Question: To what extent do the visual priors captured by Context-Guided Clustering (CGC) on the COCO dataset transfer to out-of-distribution domains?
- Basis in paper: [inferred] The method trains CGC exclusively on the COCO 2017 Panoptic dataset to capture co-occurrence patterns. However, visual priors (objects co-occurring) in general web data (COCO) may differ significantly from specialized domains not evaluated in the paper.
- Why unresolved: It is unclear if the "dominant clusters" identified by CGC remain valid or if the suppression logic remains beneficial when the input images contain visual relationships rarely seen in the training distribution (e.g., abstract art or medical imaging).
- What evidence would resolve it: Evaluation of the CGC+VTD pipeline on benchmarks involving domain shifts, showing that hallucination reduction persists compared to the baseline.

## Limitations
- Model Dependency: The approach requires careful calibration (cluster size, dominant clusters, editing layer) for each specific LVLM and tokenizer combination
- Coverage Trade-off: The method reduces hallucinations but slightly decreases object coverage, indicating a fundamental tension between these goals
- Single Dataset Evaluation: All experiments use COCO-derived datasets; performance on non-standard visual domains (medical imaging, abstract art) remains unknown

## Confidence

**High Confidence** (Supporting evidence is strong and consistent):
- Mechanism 1: Visual priors from co-occurrence patterns induce hallucinations (validated by HitRate@K showing C2 tokens have 5-10% higher hallucination correlation)
- Mechanism 3: Latent subtraction at intermediate layers suppresses hallucinatory influence (validated by layer sweep showing optimal performance at layers 20-25, not early/late)

**Medium Confidence** (Supporting evidence exists but has limitations):
- Mechanism 2: Spatial+semantic co-occurrence signals improve clustering (validated by ablation showing degradation when either component is removed, but CGC's superiority over simpler methods is less rigorously tested)
- General capability preservation: MME scores remain stable, but the coverage trade-off suggests some degradation is inevitable

**Low Confidence** (Evidence is limited or indirect):
- Computational efficiency claims: No runtime comparisons with baselines are provided
- Scalability to larger tokenizers: The 10-cluster size and 2-4 dominant clusters are empirically chosen but not theoretically justified for scalability

## Next Checks

1. **Cross-dataset hallucination correlation**: Run HitRate@K analysis on 500 images from diverse datasets (COCO, ImageNet, medical imaging, abstract art) to verify that C2 tokens consistently show higher hallucination correlation across domains. This validates whether the co-occurrence prior assumption holds beyond object-centric images.

2. **Runtime efficiency benchmarking**: Measure wall-clock time for CGC+VTD vs. baseline methods (PROJECTAWAY, VCD) on identical hardware, including both offline GNN training and online inference overhead. Compare the trade-off between hallucination reduction and computational cost across different image resolutions and batch sizes.

3. **Extreme coverage stress test**: Design prompts that intentionally request discussion of absent objects (e.g., "Describe what's NOT in this image") and measure whether CGC+VTD suppresses valid non-hallucinatory content. This quantifies the coverage trade-off threshold and identifies scenarios where the method should be disabled.