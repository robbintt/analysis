---
ver: rpa2
title: Interactive Real-Time Speaker Diarization Correction with Human Feedback
arxiv_id: '2509.18377'
source_url: https://arxiv.org/abs/2509.18377
tags:
- speaker
- correction
- feedback
- diarization
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an LLM-assisted real-time speaker diarization
  correction system that enables users to verbally fix speaker attribution errors
  during conversations. The method integrates streaming ASR and diarization, uses
  LLM summarization to display concise speaker-attributed content, and accepts brief
  verbal corrections via a wake-word interface.
---

# Interactive Real-Time Speaker Diarization Correction with Human Feedback

## Quick Facts
- arXiv ID: 2509.18377
- Source URL: https://arxiv.org/abs/2509.18377
- Reference count: 0
- Reduces diarization error rate by 9.92% and speaker confusion by 44.23% using verbal corrections

## Executive Summary
This paper presents an LLM-assisted real-time speaker diarization correction system that enables users to verbally fix speaker attribution errors during conversations. The method integrates streaming ASR and diarization, uses LLM summarization to display concise speaker-attributed content, and accepts brief verbal corrections via a wake-word interface. Two key techniques are introduced: a split-when-merged (SWM) method that detects and splits mixed-speaker segments using window voting and smoothing, and online enrollments (OE) that proactively update speaker embeddings based on user corrections. Experiments on the AMI test set show that the system reduces diarization error rate (DER) by 9.92% and speaker confusion error by 44.23% compared to baseline.

## Method Summary
The system implements a 5-stage pipeline: streaming ASR produces transcripts with word-level timestamps, ECAPA-TDNN embeddings are extracted and assigned via cosine similarity, GPT-5o-mini summarizes recent turns per speaker every I segments, users provide verbal corrections via wake-word ("Hey Cobi"), and an LLM parser reassigns speakers. SWM post-processes segments by splitting merged multi-speaker turns using sliding-window voting (W=1.0s, stride=0.2s) with dominance threshold θ=0.7. OE updates speaker embedding pools with corrected segment embeddings. The system was evaluated on the AMI Meeting Corpus Headset-mix test split using simulated user feedback based on ground truth.

## Key Results
- DER reduced by 9.92% and speaker confusion error by 44.23% compared to baseline
- SWM alone improves DER by 1.39% and speaker error by 6.22%
- OE effectiveness peaks at 1-2 enrollments; more cause diminishing returns
- Summary views outperform conversation views for correction accuracy

## Why This Works (Mechanism)

### Mechanism 1: Split-When-Merged (SWM) Boundary Refinement
SWM detects and splits merged multi-speaker segments using sliding-window speaker voting with word-level labels. It searches for optimal split points by maximizing label consistency on each side, using a dominance threshold (θ=0.7) to suppress unnecessary splits when one speaker dominates. This is needed because ASR segmentation frequently merges adjacent turns from different speakers, and segment-level speaker assignment cannot correct mid-segment speaker changes.

### Mechanism 2: Online Enrollments (OE) for Proactive Embedding Updates
When users correct speaker attributions, OE adds the corrected segment's embedding to that speaker's enrollment pool. Subsequent speaker assignments use max cosine similarity across all pooled embeddings. This works because corrected segments contain speaker-specific acoustic information, and user corrections are assumed accurate. OE is particularly effective when SWM first cleans segment boundaries to prevent mixed-speaker contamination.

### Mechanism 3: Summary-Based Correction Interface Reduces Cognitive Load
LLM-generated summaries improve correction accuracy by condensing recent turns per speaker (~32-41 words vs 80-281 word transcripts). Users can spot errors faster in condensed views, enabling targeted verbal corrections. The system displays I=15 segments with max 30 corrections, showing that longer excerpts make it harder to quickly localize errors.

## Foundational Learning

- **Speaker Diarization and DER Metrics**: DER = (T_miss + T_fa + T_conf) / T_total combines detection and attribution errors. Understanding that Speaker Confusion is the addressable component is essential for interpreting improvements. Quick check: Given DER=68.40% baseline and ImDER=9.92%, what is the absolute DER after improvement? (Answer: ~61.62%)

- **Streaming ASR Segmentation Behavior**: ASR segments by silence, often merging rapid speaker turns. The SWM mechanism exists because segment-level speaker assignment cannot correct mid-segment speaker changes. Quick check: Why does one-label-per-segment assumption limit correction effectiveness?

- **Speaker Embedding Similarity**: The system uses ECAPA-TDNN embeddings and cosine similarity for speaker assignment. OE adds embeddings to pools, requiring understanding of how max-similarity voting works. Quick check: If a segment embedding is added to Speaker A's pool, how does that affect future assignments?

## Architecture Onboarding

- **Component map**: Streaming ASR -> ECAPA-TDNN Embeddings -> SWM Post-processor -> Speaker Assignment -> LLM Summarizer -> User Feedback Interface -> LLM Corrector -> Online Enrollment Store
- **Critical path**: ASR → Embedding → SWM → Speaker Assignment → LLM Summary → User Correction → LLM Correction Parser → OE Update → Future Speaker Assignment
- **Design tradeoffs**: Correction frequency vs. user burden (optimal at 2-5 minute intervals); OE count (1-2 sufficient); Summary interval (I=15 optimal with max 30 corrections yields best DER)
- **Failure signatures**: Mixed-speaker segment contamination in OE if SWM disabled; long conversation display degrades correction; remaining DER gap largely due to front-end detection and segmentation
- **First 3 experiments**: 1) SWM ablation: Run baseline + SWM only (expect ~1.39% DER improvement); 2) OE without SWM: Add 1-2 online enrollments without SWM pre-processing (expect contamination effects, ~3-5% DER improvement); 3) Summary vs. Full Transcript: Compare correction accuracy at I=5,10,15,20 with both display modes

## Open Questions the Paper Calls Out

### Open Question 1
How does the system's performance compare when using actual human verbal feedback versus the LLM-based simulation used in the experiments? The authors simplify feedback acquisition by simulating user corrections without frontend user audio, bypassing variability and potential errors from real human users.

### Open Question 2
How does the system's performance degrade when the streaming ASR incorrectly transcribes the user's spoken correction command? The simulation feeds text directly to the LLM Corrector, ignoring the ASR step required to transcribe the user's spoken "Hey Cobi" command.

### Open Question 3
Does the Split-When-Merged (SWM) algorithm sufficiently handle segments containing more than two speaker turns? The algorithm allows at most one split per segment, which may fail to resolve segments with rapid turn-taking involving 3 or more distinct speaker turns.

## Limitations
- Performance relies on controlled AMI meeting data (4 speakers, headset audio) limiting generalizability to noisy, multi-party scenarios
- SWM effectiveness depends on ASR producing merge-prone segments, which may not occur in pause-rich conversations
- LLM summary quality and correction accuracy are assumed but not directly measured
- Online enrollment contamination risk only partially mitigated by SWM

## Confidence

- **High Confidence**: DER improvement metrics (9.92% absolute), SWM split detection logic, OE embedding update mechanism
- **Medium Confidence**: Summary-based correction effectiveness, optimal correction frequency (2-5 minutes), SWM dominance threshold impact
- **Low Confidence**: Real-time latency constraints, summary quality measurement, generalization to non-meeting domains

## Next Checks

1. **Generalization Test**: Apply the system to multiparty conversational datasets (e.g., CALLHOME, DIHARD) to measure DER improvement degradation across varying speaker counts and acoustic conditions.
2. **Summary Quality Analysis**: Implement automated metrics (ROUGE, semantic similarity) comparing LLM summaries to ground-truth speaker-attributed transcripts to quantify information retention vs. condensation.
3. **Contamination Assessment**: Analyze speaker embedding distributions in enrollment pools before/after SWM processing to measure the actual reduction in mixed-speaker contamination rates.