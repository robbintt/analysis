---
ver: rpa2
title: 'SurveyGen: Quality-Aware Scientific Survey Generation with Large Language
  Models'
arxiv_id: '2508.17647'
source_url: https://arxiv.org/abs/2508.17647
tags:
- survey
- surveys
- generation
- citation
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurveyGen dataset addresses the lack of standardized benchmarks
  for scientific survey generation by providing over 4,200 human-written surveys with
  242,143 cited references and quality-related metadata. The proposed QUAL-SG framework
  extends standard RAG by incorporating quality-aware indicators for literature retrieval,
  improving citation reliability and survey quality.
---

# SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models

## Quick Facts
- arXiv ID: 2508.17647
- Source URL: https://arxiv.org/abs/2508.17647
- Reference count: 40
- Primary result: QUAL-SG achieves 16.73% F1 score for citation quality, outperforming baselines by 8.97%

## Executive Summary
This paper addresses the challenge of generating high-quality scientific surveys by introducing SurveyGen, a dataset of over 4,200 human-written surveys with 242,143 cited references, and QUAL-SG, a quality-aware framework that extends RAG with bibliometric indicators for literature retrieval. The QUAL-SG framework improves citation reliability and survey quality by incorporating co-citation expansion, bibliometric re-ranking, and outline-first generation. Experiments show significant improvements in citation quality, content quality, and structural consistency compared to baselines.

## Method Summary
The QUAL-SG framework extends standard RAG by incorporating quality-aware indicators for literature retrieval. It first retrieves topically relevant papers via semantic similarity, then expands the candidate set through co-citation analysis (adding papers cited by at least two candidates). Retrieved papers are re-ranked using a weighted formula incorporating normalized citation counts, author h-index, and venue h-index combined with topical relevance and diversity scores. The generation process uses an outline-first approach where the LLM first creates a structured outline, then generates content for each section. The system uses only abstracts and metadata to avoid copyright issues, with evaluation conducted on 120 surveys across four domains.

## Key Results
- QUAL-SG achieves 16.73% F1 score for citation quality, outperforming Naive-RAG by 8.97%
- Improves content quality by 0.73% in semantic similarity and 3.66% in key point recall
- Task 2 (RAG-based) shows improved structural overlap (24.76%) compared to Fully-LLMGen (14.89%)
- Human evaluations reveal LLM-generated surveys become more acceptable with accurate references and structured outlines

## Why This Works (Mechanism)

### Mechanism 1: Co-citation Expansion for Semantically Distant Retrieval
The framework addresses the limitation of semantic similarity alone by expanding candidate pools via co-citation analysis. By identifying papers cited by at least two topically relevant candidates, the system retrieves influential foundational works that lack direct lexical overlap with the survey topic, solving the problem of missing seminal papers like "Backpropagation" in a "Deep Learning" survey.

### Mechanism 2: Bibliometric Re-ranking to Filter Low-Impact Sources
A weighted re-ranking step using bibliometric indicators (citations, author influence, venue reputation) aligns the final reference list closer to human-selected "gold standards." This addresses the gap between semantic relevance and survey quality by prioritizing high-impact sources that humans would naturally select for authoritative surveys.

### Mechanism 3: Outline-First Generation for Structural Coherence
Decomposing the generation task into explicit outline generation followed by parallel section expansion improves structural consistency. This approach leverages LLMs' strength in summarizing when constrained by pre-existing hierarchy, avoiding the brief, unstructured outputs typical of single-pass generation attempts.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: Understanding that standard RAG retrieves text chunks based purely on vector similarity is essential to understanding why "quality-aware" indicators are necessary
  - Quick check: If I ask a standard RAG system for papers on "Transformers," will it prioritize the original "Attention is All You Need" paper or just papers that use the word "Transformer" frequently?

- **Concept: Bibliometrics (H-Index & Citation Count)**
  - Why needed: The core logic of the QUAL-SG re-ranker depends on weighing author influence and venue reputation using h-index and citation counts as quality proxies
  - Quick check: Does a paper with 1,000 citations from a predatory conference score higher on "venue reputation" than a paper with 50 citations from a top-tier journal like Nature?

- **Concept: Co-citation Analysis**
  - Why needed: This mechanism expands the search space beyond direct semantic matches to find influential papers that are frequently co-cited by topically relevant literature
  - Quick check: If Paper A and Paper B both cite Paper C, but Paper C has zero keyword overlap with the survey topic, how does the system identify Paper C as relevant?

## Architecture Onboarding

- **Component map:** Survey Topic -> S2ORC API (Retrieve top-$n$ abstracts) -> OpenAlex (Identify 2nd-level references) -> OpenAlex (Fetch metadata) -> Weighted Scoring (Relevance + Impact + Diversity) -> Rank -> Outline Agent -> Section Agent (Parallel execution)

- **Critical path:** The Re-rank module (Step 4) is where the paper claims its advantage over baselines. If the weights (α, β, γ) or bibliometric normalization are flawed, retrieval quality degrades to Naive-RAG levels.

- **Design tradeoffs:** Uses abstracts and metadata only to avoid copyright issues and reduce context window load, sacrificing "fine-grained details" and depth. LLM-as-judge for topical relevance increases API cost but improves precision.

- **Failure signatures:** "Long-Tail" trap (excessive diversity leads to obscure citations), hallucination (poor retrieval causes invented facts), recency bias (citation metrics disadvantage new papers).

- **First 3 experiments:** 1) Re-ranker weight sensitivity ablation on 10 surveys, 2) Retrieval limit test varying initial retrieval count from 50 to 300, 3) Abstract vs. full-text impact comparison on a single survey.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating full-text paper content (rather than abstracts alone) significantly improve information coverage and depth in LLM-generated surveys?
- **Open Question 2:** Would training a dedicated reference selection model on human-annotated data outperform the current heuristic re-ranking approach (QUAL-SG) in aligning with human citation preferences?
- **Open Question 3:** To what extent does data contamination (LLMs having seen test surveys during pre-training) inflate performance estimates in this benchmark?
- **Open Question 4:** Can incorporating citation intent and contextual citation behavior improve the quality and relevance of automatically selected references for survey generation?

## Limitations
- Relies entirely on abstracts and metadata rather than full-text content, resulting in loss of fine-grained details and depth
- Quality-aware retrieval may systematically disadvantage new papers with low citation counts despite high relevance
- Bibliometric indicators as quality proxies may vary significantly across academic disciplines
- Effectiveness of co-citation expansion for emerging fields with less stable citation patterns remains unproven

## Confidence
- **High Confidence:** SurveyGen dataset creation methodology and basic statistics are well-documented and reproducible
- **Medium Confidence:** Core claim that quality-aware indicators improve citation reliability is supported by 16.73% F1 score improvement
- **Medium Confidence:** Superiority of outline-first generation for structural coherence demonstrated through improved structural overlap metrics
- **Low Confidence:** Generalizability of bibliometric-based re-ranking across all scientific domains remains unproven

## Next Checks
1. Conduct domain-specific analysis of QUAL-SG performance to determine if bibliometric indicators equally predict survey quality in humanities versus STEM fields
2. Test co-citation expansion threshold sensitivity across fields with different citation densities
3. Evaluate the trade-off between using abstracts-only versus full-text content on survey depth and accuracy metrics