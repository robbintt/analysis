---
ver: rpa2
title: 'Lucy: edgerunning agentic web search on mobile with machine generated task
  vectors'
arxiv_id: '2508.00360'
source_url: https://arxiv.org/abs/2508.00360
tags:
- tool
- reasoning
- think
- task
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of stabilizing reasoning processes
  in small language models (SLMs) for knowledge-intensive tasks, where traditional
  approaches struggle with inconsistent and divergent reasoning trajectories during
  multi-step tool use. The core method introduces the concept of treating the model's
  internal reasoning, delimited by <toolcall and <toolcall tags, as a dynamic task
  vector machine that actively constructs and refines its own task representations
  during inference.
---

# Lucy: edgerunning agentic web search on mobile with machine generated task vectors

## Quick Facts
- arXiv ID: 2508.00360
- Source URL: https://arxiv.org/abs/2508.00360
- Authors: Alan Dao; Dinh Bach Vu; Alex Nguyen; Norapat Buppodom
- Reference count: 23
- One-line primary result: 1.7B-parameter Lucy achieves 78.3% SimpleQA accuracy, matching DeepSeek-V3 performance

## Executive Summary
Lucy addresses the challenge of stabilizing reasoning processes in small language models for knowledge-intensive tasks. The core innovation treats the model's internal reasoning, delimited by  tags, as a dynamic task vector machine that actively constructs and refines its own task representations during inference. This is achieved through a two-stage reinforcement learning framework with behavior-centric rewards that optimize reasoning efficiency and correctness, coupled with a structured XML dialogue format. The result is a 1.7B-parameter model that achieves 78.3% accuracy on SimpleQA, performing on par with much larger models while exhibiting emergent behaviors like dynamic skipping of redundant thinking steps.

## Method Summary
Lucy employs a two-stage reinforcement learning framework to train a 1.7B-parameter model for agentic web search. The method uses structured XML dialogue format with  tags as dynamic task vector construction zones. Stage 1 uses behavior-centric rewards with correctness gating (R1 = r_correct × log(1.001 + r_correct × b)) to prevent reward hacking, while Stage 2 enforces strict compliance with binary gates. The training data consists of 10,325 question-answer pairs from MuSiQue-Ans, and the model uses MCP servers for web search and scraping tools. The approach requires no supervised fine-tuning and directly optimizes for tool use proficiency and reasoning efficiency.

## Key Results
- Lucy achieves 78.3% accuracy on SimpleQA benchmark, matching DeepSeek-V3 performance
- 19.1 percentage point improvement over baseline (59.2% → 78.3%)
- Emergent behavior: model learns to dynamically skip reasoning steps for predictable actions

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Task Vector Construction via Structured Reasoning Tags
The generation process within ˋˋ tags actively constructs and refines internal task representations during inference. The model uses delimited reasoning blocks as a computational substrate where task vectors are dynamically updated through token generation, creating a self-modifying loop where each reasoning step refines the model's internal objective representation. This mechanism is supported by template ablation experiments showing 81.6% → 63.0% accuracy degradation across template variants.

### Mechanism 2: Two-Stage Reward Shaping with Correctness-Gated Behavioral Credit
Separating foundational skill acquisition from format refinement, with correctness as a prerequisite for behavioral rewards, stabilizes multi-step tool use learning. Stage 1 uses R₁ = r_correct × log(1.001 + r_correct × b), where b combines secondary rewards. If r_correct = 0, the entire reward collapses to zero—preventing reward hacking on formatting without correctness. This achieves 78.3% accuracy vs. 59.2% baseline.

### Mechanism 3: Efficiency-Penalized Reasoning Induces Strategic Skipping
Rewarding concise thinking with a negatively skewed distribution causes models to allocate deliberation capacity based on action predictability. The r_think(x) = SN(x; μ=35, σ=150, α=-5) reward penalizes verbose reasoning. The model learned to omit ˋˋ tags for low-uncertainty operations (reading retrieved content) while retaining them for query formulation—emerging without explicit training.

## Foundational Learning

- **Task Vectors** (Hendel et al., 2023)
  - Why needed here: The entire framework reinterprets reasoning blocks as task vector manipulation. Without understanding that models encode task-specific information in latent space before generation, the mechanism is opaque.
  - Quick check question: Can you explain why moving tool responses inside vs. outside ˋˋ tags would affect model performance if task vectors didn't exist?

- **Reinforcement Learning from Verifiable Rewards (RLVR)**
  - Why needed here: The two-stage training pipeline uses RLVR with automatically computable rewards (substring match for correctness, XML validity checks). Understanding reward shaping fundamentals is prerequisite to grasping why the gated formulation matters.
  - Quick check question: Why would a model learn to game format rewards without correctness gates?

- **Chain-of-Thought Reasoning in Tool-Augmented Models**
  - Why needed here: Lucy's innovation is optimizing—not removing—CoT. Prior work (ReAct, ToolFormer) showed value but suffered from inconsistency. The paper positions itself as stabilizing CoT trajectories.
  - Quick check question: What specific failure modes does "overthinking" refer to in small models using tools?

## Architecture Onboarding

- Component map:
User Query → ˋˋ Block (Task Vector Construction) → <tool_call/> → MCP Server (Web Search/Scrape) → <tool_response/> → ˋˋ Block (Refinement) → <answer/>

- Critical path:
1. Initialize reasoning in ˋˋ tags with query decomposition
2. Generate <tool_call/> with search parameters
3. Process <tool_response/> (may or may not trigger new ˋˋ deliberation depending on learned skipping behavior)
4. Iterate until <answer/> generation
5. XML validity + correctness determine reward during training

- Design tradeoffs:
- **Thinking tag scope**: Placing tool calls inside ˋˋ tags (Template 3) achieved 77.4% vs. 82.2% with open-ended think blocks—suggesting tool actions should remain outside reasoning context
- **Reward skew parameters**: μ=35 tokens, α=-5 were set heuristically; different tool contexts may benefit from different distributions
- **Two-stage vs. single-stage**: Added complexity but prevents reward hacking; paper doesn't ablate this

- Failure signatures:
- **Overthinking loops**: Model generates verbose reasoning questioning whether to use tools (addressed by r_think penalty)
- **Format gaming**: High format scores with wrong answers (addressed by correctness gating)
- **Excessive searching**: More queries than document visits (penalized by r_visit/search)
- **Knowledge barriers**: Model can't formulate correct queries when lacking entity knowledge—test-time compute can't compensate

- First 3 experiments:
1. **Template ablation**: Replicate Appendix A experiments—test all 5 prompt templates on a held-out QA set to verify task vector sensitivity
2. **Reward component isolation**: Train with r_think disabled to confirm overthinking re-emerges; compare reasoning trace lengths
3. **Stage collapse test**: Attempt single-stage training with combined rewards; measure if format gaming occurs and accuracy drops

## Open Questions the Paper Calls Out
None

## Limitations
- Framework's reliance on specific prompt templates suggests brittleness to implementation details, with performance varying by up to 19.4 percentage points across templates
- Task vector interpretation as "dynamic machine" remains largely theoretical without direct evidence of computational task representation
- Emergent skipping behavior requires systematic study across diverse tool contexts to validate as general principle

## Confidence

- **High confidence**: Lucy's 78.3% SimpleQA accuracy is empirically validated and reproducible given the same training pipeline and data
- **Medium confidence**: The correctness-gated reward shaping mechanism prevents reward hacking—supported by ablation logic but not directly tested against single-stage alternatives
- **Low confidence**: The task vector interpretation as a "dynamic machine" remains largely theoretical—the paper shows template sensitivity but doesn't provide direct evidence that reasoning tags function as computational task representations rather than simply structured context

## Next Checks

1. Conduct systematic template ablation across diverse tool types (not just web search) to test whether task vector sensitivity generalizes beyond the current MCP setup
2. Implement direct visualization of internal representations during reasoning to verify that task vectors are being actively constructed and modified, rather than the XML format serving as a passive scaffold
3. Test Lucy on out-of-distribution knowledge queries requiring entity synthesis to measure whether the claimed "knowledge barriers" limitation manifests as predicted, or if the model develops compensatory reasoning strategies