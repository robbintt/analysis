---
ver: rpa2
title: 'Epidemiology of Large Language Models: A Benchmark for Observational Distribution
  Knowledge'
arxiv_id: '2511.03070'
source_url: https://arxiv.org/abs/2511.03070
tags:
- race
- education
- knowledge
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark designed to evaluate
  whether large language models (LLMs) can access and accurately represent real-world
  observational distributions across diverse domains such as health, economics, and
  social behavior. Using datasets from U.S.
---

# Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge

## Quick Facts
- **arXiv ID:** 2511.03070
- **Source URL:** https://arxiv.org/abs/2511.03070
- **Reference count:** 40
- **Primary result:** LLMs score significantly below optimal (average 22/100 for low-dimensional tasks) on estimating real-world observational distributions, indicating limited internalization of population-level statistics.

## Executive Summary
This paper introduces the first benchmark to evaluate whether large language models can accurately represent real-world observational distributions across domains like health, economics, and social behavior. Using US national survey datasets, the authors construct tasks requiring LLMs to estimate conditional probabilities over population statistics. Results show that even the best-performing models score significantly below optimal, with no substantial improvement from fine-tuning or instruction-tuning. The findings suggest current LLMs lack robust Layer 1 (observational) knowledge in Pearl's Causal Hierarchy, limiting their reliability for higher-level causal or counterfactual reasoning. The publicly available benchmark provides a foundation for evaluating and improving model distributional knowledge.

## Method Summary
The benchmark evaluates LLMs on estimating conditional probabilities $P(V_Y|V_X)$ using 10 US survey datasets (ACS, NHANES, BRFSS, etc.) with 169 total tasks. For low-dimensional tasks, ground truth distributions are computed via bin-counting; high-dimensional tasks use LightGBM cross-validation. Models are evaluated using two prompting strategies: QA prompting (next-token probabilities averaged over permutations) and likelihood prompting (selecting probability bins). Scores are normalized using L1 distance between model and ground-truth distributions, with baselines set by uniform/0-1 distributions and bootstrap sampling of ground truth. The framework supports both open- and closed-weight models through HuggingFace integration or API calls.

## Key Results
- Best-performing models score 22/100 on low-dimensional tasks and 17/100 on high-dimensional tasks
- Fine-tuning and instruction-tuning show inconsistent and modest improvements
- RAG (retrieval-augmented generation) fails to improve performance on zero-score tasks
- No clear evidence of "curse of dimensionality" in model performance degradation
- Models perform only marginally better than predicting marginal statistics (mean baseline scores 46/100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can be probed for internal probability distributions via next-token probability inspection on structured answer choices.
- Mechan: Multiple-choice prompts with permuted answer labels (A/B/C) are fed to the model; next-token probabilities for each label are extracted, normalized across choices, and averaged over permutations to control for ordering bias. This yields $\tilde{P}(V_Y|V_X)$ representing the model's conditional distribution.
- Core assumption: The model's next-token probability distribution over answer labels reflects its internal representation of the underlying real-world probability distribution for that query.
- Evidence anchors: [abstract] "comparing model responses against true population distributions using an L1-based scoring system"; [section 2.2.1] Eq. (2) defines how model probability is computed; [corpus] Weak corpus support—no neighbor papers directly validate this elicitation method.
- Break condition: If models develop sophisticated prompt-response heuristics that decouple token probabilities from genuine distributional knowledge, the elicitation mechanism would measure strategic response patterns rather than internal distributions.

### Mechanism 2
- Claim: The L1 distance between model and ground-truth distributions provides a robust score that accounts for sampling uncertainty in survey data.
- Mechan: Bootstrap samples from ground-truth datasets establish the distribution of distances $D(P||P^{(b)})$. The 95th percentile defines the "perfect" score ($S=100$); distance from a uniform baseline (or 0/1 baseline for binary outcomes) defines $S=0$. Model scores are linearly interpolated between these bounds via Eq. (6).
- Core assumption: Bootstrap sampling adequately captures uncertainty in the ground-truth distribution, and the uniform/0/1 baselines represent meaningful "no knowledge" reference points.
- Evidence anchors: [section 2.2.1] "The upper 5% quantile of the spread of $D(P||P^{(b)})$ is labeled with the perfect score $S=100$"; [section 2.2.1] Eq. (6): $S_T = 100 \times \max(1 - D(\tilde{P}||P) / \min\{D(P_{unif}||P), D(P_{0,1}||P)\}, 0)$; [corpus] No direct corpus validation of this specific scoring scheme.
- Break condition: If the ground-truth datasets have systematic biases not captured by bootstrap resampling (e.g., non-response bias, sampling frame issues), the "perfect" score baseline may be misaligned with true population distributions.

### Mechanism 3
- Claim: Poor performance on Layer 1 (observational) distributions implies unreliable performance on Layer 2 (interventional) and Layer 3 (counterfactual) reasoning, per the Causal Hierarchy Theorem.
- Mechan: The CHT (Proposition 1) establishes that observational distributions $P(V)$ underdetermine interventional and counterfactual distributions without additional causal assumptions. Corollary 2 extends this: if a model's $\tilde{P}(V)$ differs from true $P(V)$, no guarantees exist for higher-layer inferences.
- Core assumption: LLMs do not possess correct causal structure (assumptions A in Proposition 1) that could compensate for incorrect observational distributions.
- Evidence anchors: [section 1] "Corollary 2: If the model's distribution $\tilde{P}(V)$ differs from the true $P(V)$, no guarantees can be provided for the validity of the model's interventional or counterfactual inferences"; [abstract] "language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited"; [corpus] Neighbor paper "Counterfactual Realizability" discusses related PCH constraints.
- Break condition: If models possess correct causal structure A through other means (e.g., explicit causal graph encoding in weights), the implication from poor L1 to poor L2/L3 may not hold—the paper does not test this directly.

## Foundational Learning

- Concept: **Pearl's Causal Hierarchy (PCH)**
  - Why needed here: The paper frames its entire evaluation around whether LLMs possess Layer 1 knowledge, which is prerequisite for higher-level causal reasoning. Understanding the three-layer distinction (seeing/doing/imagining) is essential to interpret why the benchmark matters.
  - Quick check question: Given only observational data on smoking and lung cancer, can you determine whether smoking *causes* cancer or whether a latent factor causes both? (Answer: No—this requires Layer 2 reasoning and causal assumptions.)

- Concept: **Curse of Dimensionality in Distribution Learning**
  - Why needed here: The paper references Stone (1982) to explain why learning high-dimensional distributions is fundamentally hard—convergence rates degrade as $O(n^{-1/(2+d)})$. This contextualizes why even large models struggle with high-dimensional conditional distributions.
  - Quick check question: If estimating a distribution over $d=5$ variables requires 10,000 samples for acceptable error, how many samples might $d=10$ require? (Answer: Exponentially more—Stone's result implies the rate degrades sharply with dimension.)

- Concept: **Observational vs. Factual Knowledge**
  - Why needed here: The paper distinguishes factual knowledge (single ground truth, e.g., "capital of England") from probabilistic knowledge (distributions over outcomes, e.g., "sex of a CS graduate"). Benchmarks like MMLU test the former; this benchmark tests the latter.
  - Quick check question: "What percentage of US adults have diabetes?" requires what type of knowledge? What about "Is insulin used to treat diabetes?" (Answer: First requires probabilistic/observational knowledge; second requires factual knowledge.)

## Architecture Onboarding

- Component map:
  - **Dataset layer**: 10 preprocessed US survey datasets (ACS 2023, NHANES 2021-2023, BRFSS 2023, MEPS 2023, NSDUH 2023, SCF 2022, GSS 2022, IPEDS, BLS, FBI UCR) with sample weights for national representativeness
  - **Task layer**: 75 low-dimensional tasks ($|V_X|=1$, $|V_Y|=1$) and 94 high-dimensional tasks ($|V_X|\in\{2,3,4,5\}$, $V_Y$ binary)
  - **Prompting layer**: QA prompting (multiple-choice with permutation averaging) and likelihood prompting (direct probability bins)
  - **Evaluation layer**: L1 distance computation, bootstrap-based scoring normalization, baseline comparison (uniform, 0/1, mean)
  - **Model interface**: HuggingFace integration for open-weights models; API calls for closed models

- Critical path:
  1. Load dataset → extract $P(V_Y|V_X)$ via bin-counting (low-dim) or LightGBM cross-validation (high-dim)
  2. For each task, generate prompts from template $\pi(v_X)$ for all values in $\text{dom}(V_X)$
  3. Query model → extract $\tilde{P}(V_Y|V_X)$ via next-token probabilities (open) or likelihood bins (closed)
  4. Compute L1 distance $D(\tilde{P}||P)$ → normalize against baselines → assign score $S_T$
  5. Aggregate across tasks for model leaderboard

- Design tradeoffs:
  - **QA vs. likelihood prompting**: QA exploits token probabilities directly but requires open-weights; likelihood works for closed models but bins probabilities coarsely. Paper shows likelihood prompting yields higher scores (41 vs. 22/100), suggesting either better elicitation or different failure modes.
  - **L1 vs. KL divergence**: Paper chooses L1 to avoid sensitivity to low-probability events and support mismatch (Gibbs & Su, 2002). Trade-off: L1 is less informative about tail behavior.
  - **Mean baseline exclusion**: Paper uses uniform/0/1 for $S=0$, but shows mean baseline would score 46/100, making all models appear near-zero. Trade-off: current baselines may overstate model capability; mean baseline would show how little models know beyond marginal statistics.

- Failure signatures:
  - **All scores near zero with QA prompting**: May indicate ordering bias not fully controlled by permutation averaging, or model calibration issues.
  - **Large score variance across task permutations**: Suggests prompt sensitivity dominates genuine distribution knowledge.
  - **Fine-tuned models scoring worse than base models**: Observed in Appendix C for some tasks—indicates fine-tuning on narrative text doesn't transfer to probability elicitation.
  - **RAG showing zero improvement**: Paper reports GPT-4.1 with web access still scored 0 on 11 targeted tasks, suggesting either retrieval failure or inability to extract relevant statistics from retrieved documents.

- First 3 experiments:
  1. **Baseline validation**: Run the benchmark on a trivial model (e.g., always predict marginal mean) and verify scores match expected baseline values. This confirms scoring logic before investing in full model evaluation.
  2. **Ablation on permutation count**: Test whether 120 permutations (or fewer) adequately controls ordering bias by comparing scores with 10, 50, 120, and $|dom(V_Y)|!$ permutations on a subset of tasks. Large variance indicates insufficient permutation.
  3. **Cross-dataset transfer test**: Fine-tune a model on one dataset (e.g., NHANES) and evaluate on related but unseen datasets (e.g., BRFSS health measures). This tests whether observational knowledge generalizes across domains or remains dataset-specific—a key question for practical deployment.

## Open Questions the Paper Calls Out

- **Geographic generalization**: All datasets describe US populations; future work should aim for wider geographic coverage to determine if poor performance is specific to American observational data or reflects general inability to internalize real-world distributions.

- **Training methodology improvements**: Standard fine-tuning on synthetic narratives yields only inconsistent and modest gains, suggesting that encoding population-level probabilistic knowledge may require fundamentally different model architectures, training objectives, or data representations.

- **RAG scaling and optimization**: Initial tests show RAG yields zero improvement on zero-score tasks; evaluating RAG on the full benchmark suite with varied retrieval corpora and prompt formulations could determine if and where it provides consistent advantages.

- **Curse of dimensionality observability**: The paper notes that theoretical $O(n^{-1/(2+d)})$ relationship is not clearly observable with current task design; expanding the benchmark with more tasks across a wider range of dimensionalities could test for the expected performance degradation.

## Limitations

- **Elicitation mechanism validity**: Core assumption that next-token probabilities reflect true internal distributions remains weakly validated; permutation averaging may not fully control for ordering bias or prompt sensitivity.

- **Ground truth estimation quality**: High-dimensional tasks rely on LightGBM models whose cross-validation performance is not reported, raising questions about whether "ground truth" estimates themselves are accurate for sparse conditions.

- **Scoring baseline appropriateness**: Mean baseline scores 46/100, suggesting most models perform only marginally worse than predicting marginal statistics, potentially making normalized scores misleading about true capability gaps.

## Confidence

- **High confidence**: The benchmark is technically well-constructed, the evaluation pipeline is reproducible, and the finding that models perform poorly on observational distribution tasks is robust across multiple models and prompting strategies.

- **Medium confidence**: The implications for higher-layer causal reasoning (L2/L3) follow logically from the Causal Hierarchy Theorem, but empirical validation of this implication is not provided. The relative performance differences between models (e.g., RAG vs. base) are real but may reflect dataset-specific factors.

- **Low confidence**: The claim that LLMs fundamentally lack Layer 1 knowledge rather than failing at the specific elicitation task. The ground truth estimates for high-dimensional tasks, and the normalized scoring system's ability to meaningfully differentiate capability levels.

## Next Checks

1. **Permutation ablation study**: Systematically vary the number of prompt permutations (10, 50, 120, full factorial) on a subset of tasks to determine whether ordering bias is actually controlled and whether the current 120-permutation standard is necessary or excessive.

2. **Cross-dataset transfer validation**: Fine-tune models on one survey dataset (e.g., NHANES) and evaluate on structurally similar but distinct datasets (e.g., BRFSS). This would reveal whether poor performance reflects inability to learn observational distributions generally or failure to internalize specific dataset statistics.

3. **Factual vs. distributional knowledge dissociation**: Create tasks where factual knowledge (e.g., "What percentage of adults smoke?") is required versus tasks where only distributional knowledge matters (e.g., "Given age 30-40 and income $50k, what's the probability of smoking?"). This would clarify whether LLMs can separate these knowledge types or if factual knowledge transfers to distribution estimation.