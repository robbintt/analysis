---
ver: rpa2
title: 'K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive
  Language in Korean'
arxiv_id: '2506.13513'
source_url: https://arxiv.org/abs/2506.13513
tags:
- language
- dataset
- offensive
- implicit
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detoxifying offensive language,
  particularly implicitly offensive language that contains slang and cultural nuances.
  The authors introduce K/DA, an automated pipeline that generates paired neutral-toxic
  datasets by leveraging retrieval-augmented generation and two-stage filtering for
  context preservation and implicit offensiveness.
---

# K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean

## Quick Facts
- **arXiv ID:** 2506.13513
- **Source URL:** https://arxiv.org/abs/2506.13513
- **Reference count:** 40
- **Primary result:** K/DA pipeline generates 7.5K Korean paired datasets with high consistency (4.06/5) and implicit offensiveness (2.62/5), outperforming existing datasets and improving detoxification model performance.

## Executive Summary
This paper introduces K/DA, an automated pipeline for generating paired neutral-toxic datasets to detoxify implicitly offensive language in Korean. The method leverages retrieval-augmented generation with two-stage LLM filtering to produce high-quality datasets exhibiting both consistency and implicit offensiveness. The pipeline successfully generates a Korean dataset with 7,555 pairs, achieving superior performance compared to existing datasets. Models trained on K/DA data show improved detoxification performance across multiple test sets, and the pipeline demonstrates potential for generalization to other languages and open-source models.

## Method Summary
The K/DA pipeline generates paired neutral-toxic datasets through a three-stage process: (1) RAG-based generation using a vector database of Korean online community data to retrieve slang-contextually relevant toxic responses to neutral inputs, (2) two-stage LLM filtering—first for context preservation and second for implicit offensiveness—to ensure quality and relevance, and (3) instruction fine-tuning of detoxification models using the filtered paired data. The pipeline uses GPT-4 Turbo for both generation and filtering, with open-source alternatives also tested. The method specifically targets implicit offensiveness by leveraging contemporary slang and cultural nuances through retrieval from community data.

## Key Results
- Generated 7,555 Korean paired examples with 4.06/5 consistency and 2.62/5 implicit offensiveness scores
- K/DA-trained models outperformed K-OMG and translated CADD on K/DA test set (Ours: 1.145 vs K-OMG: 1.657 vs CADD: 1.802)
- Achieved 1.606 overall detoxification score on KOLD test set vs vanilla model's 1.741
- Pipeline successfully applied to other languages and open-source models (Trillion-7B, Gemma2-9B)

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Generation for Trend-Aligned Slang
RAG enables generation of contextually-relevant, contemporary offensive language that static LLMs cannot produce. A vector database (92,953 sentences from Korean online communities, embedded via SBERT) is queried with cosine similarity to retrieve slang relevant to neutral inputs. Multiple retrieval counts (n ∈ {0,3,5,7,9}) maximize diversity before filtering removes irrelevant outputs.

### Mechanism 2: Two-Stage LLM-Based Filtering
LLM-based filtering improves pair consistency and implicit offensiveness compared to unfiltered generation. Stage 1 (pair consistency) uses prompts to classify context preservation vs. context shift/QA responses. Stage 2 (implicit offensiveness) uses definitions of trend-aligned slang and implicit offensiveness with few-shot examples. The "Context Shift" + "Derogatory Detection" prompt combination achieved 4.06/5 consistency and 2.62/5 implicit offensiveness while retaining 63.24% of generations.

### Mechanism 3: Paired Dataset Structure for Instruction Tuning
Paired neutral-toxic data enables more effective detoxification training than non-paired alternatives. The 7,555 paired examples allow direct instruction fine-tuning (toxic input → neutral output). Models trained on K/DA outperformed those trained on K-OMG and translated CADD on the K/DA test set and KOLD, though not on BEEP.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for injecting contemporary slang into generation without retraining the base LLM
  - Quick check question: Can you explain why varying the retrieval count (n) before filtering improves output diversity?

- **Concept: Implicit vs. Explicit Offensiveness**
  - Why needed here: K/DA specifically targets implicit offensiveness (sarcasm, cultural slang, profanity variations), which constitutes ~53% of online offensive content per the paper's analysis
  - Quick check question: How would you distinguish "disregard and mockery" from "trend-aligned slang" in Korean text?

- **Concept: LLM-as-Judge Evaluation**
  - Why needed here: The filtering pipeline and G-Eval evaluation rely on LLM judgments; understanding reliability (86-90% agreement with humans per Section 5.1) is critical
  - Quick check question: What are the failure modes when using an LLM to evaluate subjective constructs like "implicit offensiveness"?

## Architecture Onboarding

- **Component map:** Vector Database (FAISS + KR-SBERT embeddings) -> RAG Module (cosine similarity with n ∈ {0,3,5,7,9}) -> Generation LLM (GPT-4 Turbo) -> Filter 1 (Context Shift) -> Filter 2 (Derogatory Detection) -> Paired Dataset

- **Critical path:** 1. Neutral sentence input → RAG retrieval (multiple n) → generate toxic candidates 2. Pass through Consistency Filter → retain context-preserving pairs 3. Pass through Implicit Offensiveness Filter → retain implicitly offensive outputs 4. Aggregate filtered outputs → instruction-tune detoxification model

- **Design tradeoffs:** Higher n in RAG → more diverse candidates but more irrelevant retrievals (requires stronger filtering); Stricter filtering → higher quality but lower retention (Multi-meaning Relationship prompt: 3.2% retention); Open-source models (Trillion-7B, Gemma2-9B) → lower cost but lower consistency than GPT-4 Turbo

- **Failure signatures:** Answer generation (LLM responds to neutral input instead of transforming it); Irrelevant generation (retrieved slang mismatches context); Inoffensive generation (topic too neutral, retrieval lacks toxicity); Over-explicit outputs (controversial topics yield explicit profanity instead of implicit offensiveness)

- **First 3 experiments:** 1. Baseline filter comparison: Run unfiltered generation vs. each filtering prompt (Context Shift, QA and Paraphrasing, Derogatory Detection, Tone Classification) on 500 samples; measure G-Eval scores and retention rate 2. RAG n-value ablation: Fix all other parameters; generate with n ∈ {0,3,5,7,9} individually and combined; compare implicit offensiveness and diversity of final filtered outputs 3. Cross-dataset detoxification evaluation: Train instruction-tuned models on K/DA, K-OMG, and translated CADD; evaluate on K/DA test set, KOLD, and BEEP; measure consistency retention (context preserved after detoxification)

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning open-source LLMs effectively replace proprietary models like GPT-4 Turbo as both data generators and filtering agents within the K/DA pipeline? While the paper demonstrates that off-the-shelf open-source models (Trillion-7B, Gemma2-9B) can run the pipeline, the authors note that larger models still yield better consistency. It is untested whether fine-tuning these smaller models specifically for this task can bridge the performance gap with proprietary models while reducing costs.

### Open Question 2
How does the diversification of the neutral seed sentence corpus impact the model's ability to generalize detoxification to unseen datasets? The paper notes that the performance decline on the BEEP test set "is primarily due to the limited coverage of the neutral sentence from the dataset used, a limitation that can be easily addressed by diversifying the neutral sentence data."

### Open Question 3
Does the K/DA pipeline effectively capture implicit offensiveness in languages with significantly different cultural or linguistic mechanisms for sarcasm and slang than Korean or English? The pipeline relies on a definition of "trend-aligned slang" derived largely from Korean online community structures (agglutinative language features). It is unclear if the retrieval and filtering mechanisms transfer effectively to languages where implicit offense relies less on neologisms and more on, for example, honorifics or tonal nuances.

## Limitations

- The evaluation relies heavily on GPT-4 Turbo as both judge and generator, raising concerns about inherent bias in filtering decisions and the transferability of results to human judgment (86-90% agreement is good but not perfect)
- The RAG mechanism depends on the quality and currency of the crawled community corpus—if offensive slang evolves rapidly, the vector database may become stale
- The method shows strong performance on Korean test sets but fails to improve on BEEP (English), indicating potential domain or language-specific limitations

## Confidence

- **High confidence:** The paired dataset structure improves detoxification performance (supported by multiple test set comparisons showing K/DA outperforming K-OMG and CADD)
- **Medium confidence:** The two-stage LLM filtering approach effectively selects high-quality pairs (supported by quantitative scores but limited human validation beyond the 86-90% agreement rate)
- **Medium confidence:** RAG-based generation produces contemporary slang relevant to neutral inputs (supported by the retrieval mechanism description but lacking independent validation of slang currency)
- **Low confidence:** The pipeline generalizes robustly to other languages and open-source models (supported only by single experiments with limited evaluation)

## Next Checks

1. **Human validation study:** Have 3-5 native Korean speakers evaluate 100 randomly selected K/DA pairs using the same G-Eval criteria (implicit offensiveness 1-5, consistency 1-5) to verify alignment with GPT-4 Turbo judgments and identify systematic LLM biases

2. **Cross-lingual detoxification test:** Train Ko-LLaMA3-Luxia-8B on K/DA and evaluate detoxification performance on English datasets (BEEP and any available Korean-to-English translated test sets) to assess language transfer capabilities and identify failure modes

3. **Temporal robustness check:** Rebuild the vector database with community data crawled 6+ months after the original, regenerate 1,000 pairs, and compare implicit offensiveness and consistency scores to detect degradation in slang relevance and generation quality over time