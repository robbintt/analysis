---
ver: rpa2
title: Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric
  Understanding
arxiv_id: '2510.21356'
source_url: https://arxiv.org/abs/2510.21356
tags:
- gaze
- attention
- visual
- human
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Gaze-VLM, a framework that uses gaze as a
  training-time regularizer to improve Vision-Language Models (VLMs) for egocentric
  understanding tasks. Unlike prior approaches, gaze is not used during inference.
---

# Gaze-VLM:Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding

## Quick Facts
- **arXiv ID:** 2510.21356
- **Source URL:** https://arxiv.org/abs/2510.21356
- **Authors:** Anupam Pani; Yanchao Yang
- **Reference count:** 40
- **Primary result:** Uses gaze as training-time regularizer to improve Vision-Language Models for egocentric understanding without requiring gaze at inference

## Executive Summary
This paper introduces Gaze-VLM, a framework that leverages human gaze patterns as a training-time regularizer to improve Vision-Language Models for egocentric understanding tasks. The key insight is that gaze can guide model attention during training without requiring gaze input at inference, making the approach practical for real-world deployment. By aligning model attention distributions with human gaze patterns through KL divergence regularization, the method achieves significant improvements in fine-grained future event prediction (up to 11%) and current activity understanding (around 7%) across multiple VLM architectures.

## Method Summary
Gaze-VLM employs gaze-regularized attention during training to align model focus with human visual gaze patterns. The method processes raw gaze coordinates into spatial heatmaps using Gaussian smoothing, then aggregates them temporally while filtering out occluded gaze points through bidirectional optical flow consistency checks. During training, the model computes patch-wise gaze distributions and minimizes KL divergence between these distributions and the model's attention weights, with the regularization term added to the standard cross-entropy loss. Critically, gaze is not used during inference, allowing the framework to maintain practical efficiency while benefiting from gaze supervision during training.

## Key Results
- Improves fine-grained future event prediction by up to 11% compared to baselines
- Achieves around 7% improvement in current activity understanding
- Demonstrates generalization to out-of-distribution data with consistent gains (7-9% future prediction, 5-6% activity understanding on EGTEA+ Gaze without fine-tuning)
- Reduces visual hallucinations by grounding predictions in attended regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning model attention distributions with human gaze patterns through KL divergence regularization during training can improve egocentric activity understanding and future prediction without requiring gaze at inference.
- **Mechanism:** The approach computes patch-wise gaze distributions from spatial heatmaps and minimizes KL divergence between these distributions and the model's attention weights. This is formulated as: `DKL(At∥H̃t) = Σ At,i log(At,i / H̃t,i)`, where `At` represents model attention and `H̃t` represents the normalized patch-wise gaze distribution. The regularization term is added to the standard cross-entropy loss with a weighting hyperparameter λ.
- **Core assumption:** Human gaze patterns reflect task-relevant visual attention that correlates with meaningful activity understanding and prediction capabilities.
- **Evidence anchors:**
  - [abstract] "introduces a gaze-regularized attention mechanism that aligns model focus with human visual gaze... using a KL divergence loss"
  - [section 3.3] "We thus divide the domain Ω of the (aggregated) heatmap Ht into a grid of P patches that correspond to the same spatial partition used by the vision encoder... and compute the gaze score for each patch"
  - [corpus] Related work "Eyes on Target" (arXiv:2511.01237) injects gaze-derived features into attention mechanisms for egocentric object detection, suggesting gaze-attention coupling is a promising direction, though using different integration strategies.
- **Break condition:** If human gaze patterns do not reliably correlate with task-relevant features (e.g., in scenarios where peripheral vision or task-irrelevant fixations dominate), the regularization may misguide the model. The paper notes that "excessive regularization may constrain the model's ability to attend to regions that are task-relevant but not explicitly captured by human gaze" (Section 4.3).

### Mechanism 2
- **Claim:** Temporal aggregation of gaze points with occlusion-aware filtering produces more stable and reliable supervision signals compared to single-frame gaze points.
- **Mechanism:** Individual gaze points are transformed into spatial heatmaps via Gaussian smoothing, then aggregated over a temporal window (δ ≈ 200ms). Crucially, bidirectional optical flow consistency checks identify occluded gaze points, which are excluded from aggregation. The occlusion check computes: `Δ = ||fτ→t(p) + ft→τ(p̂)||`, and if the proportion of pixels exceeding threshold ε is greater than η (0.60), the frame's gaze points are discarded.
- **Core assumption:** Individual gaze points contain noise (particularly during saccades), and gaze points directed at occluded regions would create misleading supervision signals.
- **Evidence anchors:**
  - [section 3.1] "Since saccades reflect transitional eye movements rather than focused attention, relying on individual gaze points... can introduce noise into the supervision signal"
  - [section 3.1] "considering that a point in the scene fixated upon may become occluded due to camera movement... including such occluded points would create misleading supervision signals"
  - [corpus] Limited direct corpus evidence for this specific occlusion-handling mechanism in gaze-VLM contexts. Related work on egocentric gaze (e.g., "Beyond Gaze Overlap," arXiv:2509.12419) focuses on joint attention dynamics but does not address temporal aggregation with occlusion filtering.
- **Break condition:** If optical flow estimation is unreliable (e.g., rapid motion, textureless regions) or if the scene changes exceed the flow model's capacity, the occlusion check may incorrectly include or exclude gaze points.

### Mechanism 3
- **Claim:** Attention patterns learned through gaze regularization during training can generalize to inference scenarios where gaze is unavailable, maintaining performance improvements.
- **Mechanism:** During training, the gaze-regularized attention block shapes the model's attention patterns to align with human visual focus. At inference, this learned attention behavior persists without explicit gaze input. The framework uses a global query derived from the entire input sequence (rather than frame-specific queries), which the paper suggests supports better temporal generalization and reduces overfitting to frame-specific visual patterns.
- **Core assumption:** The attention patterns learned through gaze supervision capture generalizable priors about task-relevant visual regions that transfer to unseen data without gaze.
- **Evidence anchors:**
  - [abstract] "gaze is not used during inference" and the method "improves fine-grained future event prediction by up to 11% and current activity understanding by around 7% compared to baselines"
  - [table 4] Out-of-distribution evaluation on EGTEA+ Gaze shows consistent gains (7-9% future prediction, 5-6% activity understanding) without fine-tuning
  - [corpus] "StreamGaze" (arXiv:2512.01707) explores gaze-guided temporal reasoning for streaming video understanding, suggesting temporal gaze patterns are valuable, though it does not specifically address training-only supervision transfer.
- **Break condition:** If the model overfits to gaze patterns specific to the training distribution, performance gains may not transfer to out-of-distribution scenarios. The paper provides OOD results suggesting some generalization, but the mechanism's robustness across diverse deployment contexts remains an assumption.

## Foundational Learning

- **Concept: Transformer Attention Mechanisms (Query-Key-Value)**
  - **Why needed here:** The gaze regularization operates directly on attention weights (`At = softmax(QK^T / √dk)`). Understanding how queries, keys, and values interact is essential to grasp how gaze distributions are matched to model attention and why patch-level alignment is necessary.
  - **Quick check question:** Given a vision transformer processing an image as 16×16 patches, how would you derive a scalar attention weight for each patch from a global query?

- **Concept: KL Divergence for Distribution Alignment**
  - **Why needed here:** The core regularization mechanism uses KL divergence to measure the difference between model attention and gaze distributions. Understanding its asymmetric nature (DKL(P||Q) ≠ DKL(Q||P)) is critical for interpreting how the loss guides the model.
  - **Quick check question:** If the model's attention distribution is uniform across all patches but the gaze distribution is highly peaked on one patch, what would the KL divergence term penalize?

- **Concept: Optical Flow and Occlusion Detection**
  - **Why needed here:** The temporal aggregation mechanism relies on bidirectional optical flow consistency to identify occluded gaze points. Understanding flow forward-backward consistency checks is necessary to implement or debug the occlusion filtering module.
  - **Quick check question:** If forward flow predicts a pixel moves from position p to p̂, what does the backward flow from the target frame need to satisfy for the pixel to be considered non-occluded?

## Architecture Onboarding

- **Component map:**
Input: RGB frames {It} + Gaze coordinates (training only)
         ↓
    Vision Encoder (ViT)
         ↓
    Visual tokens {ψi}
         ↓
[Gaze-Regularized Attention Block] ← Gaze heatmaps Ht (training only)
    - Global query from sequence
    - Keys/values from visual tokens
    - Attention weights At
    - KL divergence: DKL(At || H̃t)
         ↓
    Language Decoder
         ↓
Output: Text description (activity or future prediction)

- **Critical path:**
  1. **Gaze preprocessing:** Raw gaze coordinates → Gaussian heatmaps → Temporal aggregation with occlusion filtering → Patch-wise distributions H̃t
  2. **Attention computation:** Visual tokens → Global query + patch keys → Attention weights At
  3. **Regularization application:** KL divergence loss added to cross-entropy loss during training only
  4. **Inference:** Identical path but without gaze input or KL term

- **Design tradeoffs:**
  - **λ (regularization strength):** Higher values enforce tighter gaze alignment but may over-constrain the model. Paper finds λ=100 optimal; λ=1000 shows marginal decline (Table 2).
  - **Temporal aggregation window δ:** Longer windows capture more stable attention but risk including outdated gaze if scene changes rapidly. Paper uses 200ms.
  - **Gaussian kernel size σ:** Larger overlays spread gaze influence but may blur precise localization. Temporal aggregation appears more important than individual overlay size (Table 9).

- **Failure signatures:**
  - **Over-regularization (high λ):** Model attends only to gaze regions, missing task-relevant peripheral information → Performance plateau or decline
  - **Noisy gaze data:** Misaligned or poorly calibrated eye-tracking → Supervision signal degrades, gains diminish
  - **Occlusion detection failures:** Optical flow errors cause incorrect inclusion/exclusion of gaze points → Heatmap quality drops
  - **Frame-level queries (vs. global):** Model overfits to frame-specific patterns → Poor temporal generalization (Table 19)

- **First 3 experiments:**
  1. **Baseline comparison without gaze regularization:** Train the VLM with λ=0 (standard cross-entropy only) on the egocentric dataset. Measure semantic similarity scores for future prediction and activity understanding to establish baseline performance.
  2. **Regularization strength ablation:** Train with λ ∈ {10, 100, 1000} while keeping all other hyperparameters fixed. Evaluate on validation set to identify optimal λ (paper finds 100 works best across architectures).
  3. **Ablation of temporal aggregation:** Compare (a) single-frame gaze heatmaps, (b) temporally aggregated heatmaps without occlusion filtering, and (c) full aggregation with occlusion filtering. This isolates the contribution of temporal aggregation and occlusion handling separately.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can joint modeling of gaze and action improve predictive capabilities more effectively than using gaze solely as an attention regularizer?
- **Basis in paper:** [explicit] The Conclusion states, "Future work may explore joint modeling of gaze and action."
- **Why unresolved:** The current framework treats gaze strictly as an independent supervisory signal (regularizer) for attention, rather than modeling the intrinsic, coupled relationship between gaze dynamics and action generation.
- **What evidence would resolve it:** A multi-task architecture that explicitly conditions action prediction on latent gaze features, evaluated against the regularization-only baseline on future prediction tasks.

### Open Question 2
- **Question:** Does aligning model attention with task-specific gaze patterns (e.g., visual search vs. passive viewing) yield better performance than using a generic gaze prior?
- **Basis in paper:** [explicit] Appendix A.6 suggests exploring "task-specific attention modeling" to help models "adapt their internal focus in a task-aware manner."
- **Why unresolved:** The current method applies a uniform KL-divergence loss based on aggregated gaze, ignoring psychological evidence that human attention strategies differ fundamentally by task intent.
- **What evidence would resolve it:** Comparative experiments regularizing models with gaze data collected under distinct task instructions (e.g., search vs. classification) compared to the current aggregated approach.

### Open Question 3
- **Question:** How does gaze regularization transfer to non-egocentric, temporally grounded tasks where the correlation between gaze and intent may differ?
- **Basis in paper:** [explicit] The Conclusion proposes "extending the framework to other temporally grounded tasks beyond egocentric understanding."
- **Why unresolved:** The method was validated exclusively on egocentric datasets (Ego4D, EGTEA+), relying on the tight coupling between the camera wearer's gaze and hand-object interaction.
- **What evidence would resolve it:** Application of the framework to third-person video datasets (e.g., ActivityNet) using mapped or predicted gaze signals, measuring performance deltas on temporal grounding tasks.

## Limitations

- **Data Dependency and Gaze Quality:** The framework's effectiveness is fundamentally tied to the quality and representativeness of gaze data. The paper assumes gaze patterns reliably reflect task-relevant attention, but this may not hold across all scenarios or populations.
- **Regularization Sensitivity:** The optimal regularization strength (λ=100) was determined empirically across architectures but may require re-tuning for different datasets or tasks.
- **Temporal Generalization Assumption:** While the paper demonstrates OOD performance gains, the assumption that gaze-regularized attention patterns generalize to all inference scenarios without gaze input remains unverified across diverse deployment contexts.

## Confidence

**High Confidence:**
- The mathematical formulation of gaze regularization (KL divergence between attention and gaze distributions) is sound and correctly implemented.
- The temporal aggregation with occlusion filtering methodology is technically rigorous and well-described.

**Medium Confidence:**
- Performance improvements (11% for future prediction, 7% for activity understanding) are well-supported by the experiments but may vary with different architectures or datasets.
- The claim that gaze regularization reduces visual hallucinations is supported but could benefit from more extensive qualitative analysis.

**Low Confidence:**
- Generalization to completely unseen distributions beyond the tested OOD scenarios.
- The assumption that human gaze patterns universally represent task-relevant attention across all egocentric scenarios.

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate the trained Gaze-VLM model on a completely different egocentric dataset (e.g., EPIC-KITCHENS) without fine-tuning to assess true OOD generalization beyond EGTEA+ Gaze.

2. **Gaze Quality Sensitivity Analysis:** Systematically degrade gaze data quality (simulated calibration errors, noise injection, missing data patterns) and measure performance degradation to quantify the framework's robustness to real-world gaze data imperfections.

3. **Attention Pattern Visualization Study:** Generate attention maps for both gaze-regularized and baseline models across diverse egocentric scenarios, comparing them to actual gaze patterns to verify that learned attention behaviors align with human visual attention and identify potential failure modes.