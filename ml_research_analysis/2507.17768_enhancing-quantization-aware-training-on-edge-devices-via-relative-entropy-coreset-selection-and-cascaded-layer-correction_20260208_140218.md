---
ver: rpa2
title: Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy
  Coreset Selection and Cascaded Layer Correction
arxiv_id: '2507.17768'
source_url: https://arxiv.org/abs/2507.17768
tags:
- quantized
- coreset
- training
- quantization
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of Quantization-Aware
  Training (QAT) on edge devices by proposing a coreset-based framework. The authors
  identify that traditional coreset selection methods fail to capture quantization
  errors, leading to performance degradation when training on small subsets.
---

# Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction

## Quick Facts
- **arXiv ID**: 2507.17768
- **Source URL**: https://arxiv.org/abs/2507.17768
- **Reference count**: 40
- **Primary result**: Achieves up to 9.52% higher Top-1 accuracy on CIFAR-100 and 5.72% on ImageNet-1K with 1% coreset using Relative Entropy Score and Cascaded Layer Correction

## Executive Summary
This paper addresses the computational inefficiency of Quantization-Aware Training (QAT) on edge devices by proposing a coreset-based framework. The authors identify that traditional coreset selection methods fail to capture quantization errors, leading to performance degradation when training on small subsets. To overcome this, they introduce the Relative Entropy Score (RES) to select samples that best reflect quantization errors. Additionally, they propose a Cascaded Layer Correction (CLC) strategy to mitigate error accumulation in intermediate layers during training. The method is evaluated on MobileNetV2 and ResNet-18 across CIFAR-100 and ImageNet-1K datasets, showing significant improvements in accuracy and training efficiency.

## Method Summary
The method consists of two key components: Relative Entropy Score (RES) coreset selection and Cascaded Layer Correction (CLC). RES measures the KL divergence between full-precision and quantized model outputs to identify samples where quantization induces significant behavioral drift. The framework selects a small subset (1-10%) of training data based on RES and gradient scores, refreshing the selection periodically. CLC aligns intermediate layer outputs between full-precision and quantized models to prevent error accumulation. The total loss combines knowledge distillation with CLC regularization, trained using the STE optimizer. The approach is implemented with LSQ+ quantization and evaluated on edge-device architectures.

## Key Results
- Achieved 9.52% higher Top-1 accuracy on CIFAR-100 with 1% coreset compared to state-of-the-art baselines
- Demonstrated 5.72% higher Top-1 accuracy on ImageNet-1K with 1% coreset
- Reduced training time by 7× in UAV-based crack detection case study
- Showed steep accuracy drops below 5% coreset size without RES+CLC compensation

## Why This Works (Mechanism)

### Mechanism 1: Relative Entropy Score Selection
- **Claim**: Selecting samples based on KL divergence between FP and quantized outputs targets specific information loss from bit-width reduction
- **Mechanism**: RES prioritizes samples where quantization induces significant behavioral drift, focusing limited training budget on correcting quantization-specific non-linearities
- **Core assumption**: Output logits divergence reliably proxies quantization error propagation through intermediate layers
- **Evidence anchors**: Spearman correlation of 0.867 between relative entropy and model accuracy; abstracts mention RES captures quantization errors effectively
- **Break condition**: Extreme quantization (e.g., binary weights) may select only outliers that harm generalization

### Mechanism 2: Cascaded Layer Correction
- **Claim**: Aligning intermediate layer outputs prevents error accumulation missed by final-layer distillation in small-data regimes
- **Mechanism**: CLC enforces alignment at intermediate layers during training, maintaining representational fidelity throughout the backbone, not just at classifier head
- **Core assumption**: FP model's intermediate activations represent an ideal target the quantized model can approximate
- **Evidence anchors**: CLC defined to mitigate intermediate layer error accumulation; loss function explicitly targets intermediate layers
- **Break condition**: If student model capacity is too low to represent teacher's intermediate features, forcing alignment may degrade performance

### Mechanism 3: Dynamic Coreset Re-selection
- **Claim**: Periodic re-selection adapts training data distribution to evolving quantization error landscape
- **Mechanism**: Framework re-evaluates RES and gradient scores every R epochs as quantization error changes during training
- **Core assumption**: Overhead of periodic inference on full dataset is acceptable relative to full backpropagation cost
- **Evidence anchors**: Implementation details show periodic updates (R=50, R=10); complexity analysis argues forward pass overhead is minor
- **Break condition**: Very short training windows or strictly limited edge device storage may make periodic selection infeasible

## Foundational Learning

- **Knowledge Distillation (KD)**: Required to understand teacher-student relationship in RES calculation and CLC alignment
  - *Quick check*: Can you explain why minimizing KL divergence between teacher and student might fail if student capacity is significantly lower than teacher?

- **Straight-Through Estimator (STE)**: Essential for understanding how QAT handles non-differentiable quantization operations
  - *Quick check*: In Eq. (2), how does STE approximate gradient for rounding operation, and what are implications for weight updates?

- **Coreset Selection**: Needed to evaluate QuaRC's improvement over baseline coreset methods
  - *Quick check*: How does gradient-based coreset method differ from error-based method like RES?

## Architecture Onboarding

- **Component map**: Teacher (FP Model) -> Selector -> Student (Quantized Model) -> Correction Head -> Output
- **Critical path**: 
  1. Initialization: Clone FP weights to Quantized model
  2. Selection Phase: Pass full dataset through both models to calculate RES, select top S%
  3. Training Loop: Forward pass selected batch, compute LTOTAL = LKD + β·LCLC, backprop via STE, update student weights
  4. Repeat: Re-select coreset every R epochs to adapt to new errors

- **Design tradeoffs**: 
  - Coreset Size (S) vs. Accuracy: Below 5% requires strong RES/CLC compensation; 10% is safer but slower
  - Interval (R) vs. Overhead: Smaller R adapts better to error drift but increases forward-pass overhead
  - Correction Weight (β): Too high forces exact mimicry, potentially ignoring quantization-specific adaptation

- **Failure signatures**:
  - Divergence of CLC: LCLC remains high while LKD drops indicates intermediate layers failing to quantize effectively
  - Stagnant Accuracy: No improvement after re-selection at epoch R suggests RES selecting uninformative outliers

- **First 3 experiments**:
  1. Metric Ablation: Run QAT on 1% CIFAR-100 using only Gradient scores vs. only RES to validate RES contribution
  2. CLC Impact: Compare standard KD vs. KD+CLC on 1% coreset, examine KL divergence of intermediate layers
  3. Scaling Test: Run QuaRC on ImageNet-1K with ResNet-18 at 2-bit to verify mechanisms on large-scale datasets

## Open Questions the Paper Calls Out

- **Adapting to object detection and semantic segmentation**: The authors explicitly identify adapting RES and CLC techniques to object detection and semantic segmentation as future work, noting the current study is restricted to image classification.

- **Applicability to Vision Transformers**: The authors suggest CLC is applicable to hierarchical networks like Transformers but note hardware limitations prevented implementation, leaving performance on attention-based architectures unverified.

- **Reducing computational overhead**: While improving training efficiency, the authors acknowledge that calculating RES requires dual-inference overhead that "can still be a bottleneck for edge devices with extremely limited resources," without proposing reduction methods.

## Limitations

- Sample efficiency threshold not established - dramatic accuracy drops below 5% coreset size but no theoretical lower bound identified
- Generalizability limited to tested architectures - results only demonstrated on MobileNetV2 and ResNet-18
- Computational overhead concerns - periodic full-dataset evaluation may become prohibitive for very large datasets on memory-constrained devices

## Confidence

- **High Confidence**: RES mechanism and CLC trend are well-supported by empirical results (Spearman 0.867 correlation)
- **Medium Confidence**: Scalability to larger architectures and datasets is plausible but not definitively proven beyond tested cases
- **Low Confidence**: "First to leverage relative entropy" claim is difficult to verify without exhaustive literature review

## Next Checks

1. **Architecture Transfer Test**: Implement QuaRC on EfficientNet-B0/CIFAR-100 to verify accuracy improvements on architectures with compound scaling
2. **Extreme Compression Test**: Evaluate QuaRC at 1-bit weights and 2-bit activations on ImageNet-1K to determine practical lower bound where method fails
3. **Overhead Benchmarking**: Measure wall-clock time of periodic RES evaluation on 1M-sample dataset on actual edge device (Raspberry Pi) to quantify real deployment speedup