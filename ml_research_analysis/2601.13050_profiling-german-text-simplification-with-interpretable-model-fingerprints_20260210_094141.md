---
ver: rpa2
title: Profiling German Text Simplification with Interpretable Model-Fingerprints
arxiv_id: '2601.13050'
source_url: https://arxiv.org/abs/2601.13050
tags:
- simplification
- text
- https
- profiler
- german
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Simplification Profiler, a diagnostic
  toolkit that generates multidimensional, interpretable fingerprints of simplified
  texts to diagnose the nuanced behavior of Large Language Models (LLMs) in text simplification.
  Instead of relying on monolithic evaluation metrics, the Profiler uses compositional
  analysis of specific linguistic properties like semantic fidelity, linguistic quality,
  and coherence to provide granular insights.
---

# Profiling German Text Simplification with Interpretable Model-Fingerprints

## Quick Facts
- arXiv ID: 2601.13050
- Source URL: https://arxiv.org/abs/2601.13050
- Authors: Lars Klöser; Mika Beele; Bodo Kraft
- Reference count: 40
- A diagnostic toolkit (Simplification Profiler) generates multi-dimensional fingerprints of simplified texts, validated via classification of model configurations with F1 up to 71.9%.

## Executive Summary
This paper introduces the Simplification Profiler, a diagnostic toolkit that generates multidimensional, interpretable fingerprints of simplified texts to diagnose the nuanced behavior of Large Language Models (LLMs) in text simplification. Instead of relying on monolithic evaluation metrics, the Profiler uses compositional analysis of specific linguistic properties like semantic fidelity, linguistic quality, and coherence to provide granular insights. A meta-evaluation demonstrates that the Profiler's complete feature set enables a linear classifier to distinguish high-level variations (e.g., prompting strategies) and fine-grained changes (e.g., few-shot examples) in generated simplifications, achieving classification F1-scores up to 71.9%, improving upon simple baselines by over 48 percentage points. The tool offers developers a transparent, actionable analysis to build more effective and adaptive text simplification systems, particularly valuable for languages like German where data scarcity limits traditional evaluation methods.

## Method Summary
The method implements a reference-free diagnostic toolkit that decomposes simplification quality into 23 interpretable features across semantic fidelity, linguistic quality, and heuristics. It processes German source-simplification pairs through NLI models for semantic metrics (COR, COV), LanguageTool for linguistic quality (SIM, LNG), FBR readability index, cosine similarity for coherence, and simple heuristics (LEN, ENT, ASL). A logistic regression classifier validates the feature space by distinguishing 18 simplification configurations (3 models × 4 prompts × few-shot variants) on a corpus of 14,868 German Wikipedia simplifications.

## Key Results
- The complete 23-feature fingerprint achieves classification F1-scores up to 71.9%, outperforming simple baselines by over 48 percentage points
- Ablation studies show average sentence length (ASL) has disproportionately high diagnostic power, with removal reducing F1 by 11-13 percentage points for key prompt types
- Binary classification distinguishes prompt strategies (Rules 64.6%, Content 67.1% F1) more effectively than the full 18-class setting (Rules 64.6%, Content 67.1%)

## Why This Works (Mechanism)

### Mechanism 1
A compositional fingerprint of multiple linguistic properties can distinguish ATS system configurations more effectively than single monolithic metrics. The Profiler decomposes simplification quality into 9+ interpretable dimensions (COR, COV, SIM, LNG, FBR, COH, ASL, LEN, ENT). A linear classifier trained on these 23 features identifies model/prompt configurations with F1 up to 71.9%, outperforming simple length baselines by 48+ points. This works because different strategies create distinct trade-off patterns (e.g., Target prompts prioritize readability over coverage). Core assumption: The selected dimensions capture the meaningful variation space of simplification behavior, and linear separability indicates real behavioral differences.

### Mechanism 2
NLI models can proxy semantic fidelity by detecting contradictions and measuring entailment between source and simplified texts. Content Correctness (COR) aggregates 1−P(contradiction) across source sentences (premise) vs. simplification (hypothesis). Content Coverage (COV) combines P(entailment) with semantic similarity. High scores require both logical entailment and semantic overlap, reducing single-metric failure modes. Core assumption: The underlying NLI model generalizes to German simplification contexts and doesn't introduce systematic bias toward certain rewriting patterns.

### Mechanism 3
Simple heuristics (sentence length, named entity ratio) carry diagnostic power disproportionate to their complexity for identifying simplification strategies. Average Sentence Length (ASL) emerged empirically as a critical fingerprint component. Ablation shows removing ASL drops Target F1 from 48.3% to 24.9% and Rules from 64.6% to 52.1%. These heuristics capture strategy signatures (e.g., sentence-splitting vs. vocabulary simplification). Core assumption: Heuristic correlations with strategy reflect genuine behavioral differences, not artifacts of the test set construction.

## Foundational Learning

- **Natural Language Inference (NLI)**: Core to semantic fidelity metrics (COR, COV); must understand entailment/contradiction probabilities. Quick check: Given "The weather was bad" as premise, would "Experts predict higher prices" be labeled contradiction, neutral, or entailment?
- **Reference-free evaluation**: Profiler is explicitly reference-less; understanding why avoids confusion with BLEU/SARI paradigms. Quick check: What's the advantage of reference-free metrics when evaluating adaptive simplification for diverse target groups?
- **Linear separability as validation proxy**: The meta-evaluation uses classification performance to argue fingerprint informativeness. Quick check: If a linear classifier achieves 90% accuracy distinguishing prompts, what does that imply about the feature space?

## Architecture Onboarding

- **Component map**: Source text + simplified text -> spaCy segmentation -> 23 metrics (NLI, LanguageTool, FBR, embeddings, heuristics) -> fingerprint vector -> (optional) Logistic Regression classification or spider diagram visualization
- **Critical path**: 1) Input source and simplified texts, 2) Segment into sentences using spaCy, 3) Compute all 23 metrics including NLI calls, LanguageTool rules, FBR index, embedding similarity, and heuristics, 4) Aggregate into fingerprint vector, 5) (Optional) Classify configuration or visualize as spider diagram
- **Design tradeoffs**: Compositional vs. monolithic (reuses robust, well-trained components rather than training new evaluation model from scarce German ATS data); Interpretability vs. coverage (dimensions are transparent but not exhaustive); Generality vs. language-specificity (FBR and LanguageTool rules are German-specific)
- **Failure signatures**: NLI model domain gap causing false contradictions in technical/encyclopedic text; COH metric missing discourse-level coherence (high scores for locally smooth but globally incoherent text); FBR gaming (models optimize sentence length without genuine simplification); Classifier "middle-ground" problem (4B model poorly distinguished in multi-class setting despite pairwise separability)
- **First 3 experiments**: 1) Reproduce classification experiment on German Wikipedia subset using open-source repo and verify F1 ranges match Table 2, 2) Ablation study: Remove COV and observe which prompt strategies become harder to identify; hypothesize why, 3) Cross-domain test: Apply Profiler to German news articles (not Wikipedia); compare fingerprint distributions and note any metric failures

## Open Questions the Paper Calls Out

- How does the Simplification Profiler's multi-dimensional fingerprint correlate with human quality judgments across diverse target groups? The authors state this is "a crucial but distinct subsequent step" since the current meta-evaluation only validates the profiler's ability to distinguish model configurations, not its alignment with human preferences.
- Can the fingerprinting methodology be effectively generalized to other languages and text domains without extensive re-engineering? The tool relies on German-specific components (FBR readability index, LanguageTool rules), making immediate applicability to other linguistic contexts or domains unknown.
- How can the fingerprint be enriched to capture pragmatic qualities such as tone, register, and user engagement? The current feature set focuses on structural and semantic properties, lacking metrics for the affective and stylistic nuance required for truly adaptive systems.

## Limitations

- The Profiler relies on language-specific components (FLEX rules, FBR index) that require non-trivial re-implementation for other languages
- NLI-based semantic metrics depend on an unspecified German NLI model, raising questions about generalizability and potential biases
- The evaluation corpus is exclusively German Wikipedia, limiting confidence in cross-domain applicability
- The tool doesn't capture pragmatic qualities like tone, register, or user engagement that may be crucial for adaptive simplification

## Confidence

- **High confidence**: The compositional analysis approach and general methodology of using interpretable dimensions for diagnostic purposes; ablation studies demonstrating feature importance are convincing
- **Medium confidence**: The meta-evaluation showing linear separability of configurations; while classification results are strong, the binary setup may overestimate real-world utility
- **Low confidence**: The absolute performance numbers and their interpretation across different prompt types; Target prompts achieve only 48.2% F1 in the full 18-class setting, suggesting some strategy fingerprints may be less distinct

## Next Checks

1. **Cross-NLI validation**: Repeat the classification experiment using two different German NLI models (e.g., mDeBERTa and XLM-R) to assess robustness of COR/COV metrics to model choice
2. **Domain transfer test**: Apply the Profiler to a non-Wikipedia German corpus (e.g., news articles or legal documents) and compare fingerprint distributions. Document which metrics degrade most significantly and hypothesize why
3. **Multi-class to binary mapping**: For configurations showing poor 18-class F1 (particularly 4B models), create binary classifiers distinguishing only the four main prompt strategies. Compare performance to the 4-class results reported for "no few-shot" scenarios to identify if the issue is fundamentally about model size or prompt complexity