---
ver: rpa2
title: A Logic of General Attention Using Edge-Conditioned Event Models (Extended
  Version)
arxiv_id: '2505.14539'
source_url: https://arxiv.org/abs/2505.14539
tags:
- event
- attention
- definition
- update
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first general logic of attention that
  overcomes key limitations of existing approaches. The authors present edge-conditioned
  event models that generalize both standard event models and generalized arrow updates,
  achieving exponential succinctness compared to standard models while maintaining
  equal expressivity.
---

# A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)

## Quick Facts
- arXiv ID: 2505.14539
- Source URL: https://arxiv.org/abs/2505.14539
- Reference count: 12
- One-line primary result: First general logic of attention using edge-conditioned event models, achieving exponential succinctness while maintaining equal expressivity to standard models.

## Executive Summary
This paper introduces the first general logic of attention that overcomes key limitations of existing approaches. The authors present edge-conditioned event models that generalize both standard event models and generalized arrow updates, achieving exponential succinctness compared to standard models while maintaining equal expressivity. By treating attention as a modality, the framework allows agents to attend to arbitrary formulas, not just atomic propositions, enabling modeling of complex attention scenarios such as social learning and attentional biases. The paper provides sound and complete axiomatizations and introduces attention principles (closure properties) that govern the behavior of the attention modality.

## Method Summary
The method involves extending dynamic epistemic logic with edge-conditioned event models where edges are conditioned on formulas rather than being unconditional. The framework defines attention as a modality Aaφ meaning "agent a attends to formula φ," with attention functions assigning sets of formulas to agents at each world. The core mechanism is a transformation that replaces unconditional edges with formula-conditioned edges, separating what is revealed from what agents attend to. The authors prove soundness and completeness of the axiomatization and demonstrate exponential succinctness through explicit construction and bisimulation arguments.

## Key Results
- Edge-conditioned event models achieve exponential succinctness while preserving expressivity relative to standard models
- Attention as a modality enables modeling of agents who attend to complex structured information including higher-order beliefs
- Attention principles are preserved through dynamic updates, enabling stable axiomatization of specific attention behaviors
- The framework allows AI agents to reason about human attentional biases with practical applicability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge-conditioned event models achieve exponential succinctness while preserving expressivity relative to standard event models.
- Mechanism: By replacing unconditional edges (e,f) ∈ Qa with formula-conditioned edges (e:φ, f:ψ) ∈ Qa, the framework separates *what is revealed* (preconditions) from *what agents attend to* (edge conditions). This allows a single event to represent multiple epistemic distinctions that would otherwise require exponentially many distinct events.
- Core assumption: Agents' epistemic accessibility relations can be determined by evaluating formula conditions at both source and target worlds during product update.
- Evidence anchors:
  - [abstract] "edge-conditioned event models... achieving exponential succinctness compared to standard models while maintaining equal expressivity"
  - [Section 3, Theorem 6] "H(p) is of size O(n). Any standard event model that is update equivalent to H(p) has at least 2^n events."
  - [corpus] Weak corpus support; neighboring papers address different formalisms (standpoint logic, constraint logic programming) without direct comparison.
- Break condition: If attention conditions become arbitrarily complex nested formulas, edge-condition size may grow to offset event-count savings.

### Mechanism 2
- Claim: Treating attention as a modality (Aaφ) enables modeling of agents who attend to complex structured information including higher-order beliefs and others' attention.
- Mechanism: The attention function A: Ag × W → P(LGA) assigns each agent at each world a set of formulas they attend to. During revelation event R(Γ), each agent learns only Γ ∩ Aa(w). Edge conditions encode this: agent a transitions from event S to T iff T ⊆ S and T = {φ ∈ S : Aaφ true at source}.
- Core assumption: Attention is truth-tracking; agents who attend to φ learn whether φ holds when it is revealed.
- Evidence anchors:
  - [Section 5] "If ψ ∈ Γ ∩ Aa(w), then agent a learns ψ at world w."
  - [Section 5, Def. 13] Event model construction with source conditions ∧φ∈T Aaφ and target conditions mirroring attended formulas.
  - [corpus] No direct corpus validation; this modality-based approach appears novel relative to neighbors.
- Break condition: If agents attend to logically incompatible formula sets, attention sets may become inconsistent unless constrained by attention principles.

### Mechanism 3
- Claim: Attention principles (closure properties) are preserved through dynamic updates, enabling stable axiomatization of specific attention behaviors.
- Mechanism: The product update preserves attention sets: A(a, (w,e)) = A(a,w). Since dynamics don't modify attention assignments, any closure property satisfied pre-update (e.g., conjunctive closure, attention introspection) remains satisfied post-update.
- Core assumption: Attention is a standing disposition, not altered by the revelation event itself (only beliefs update).
- Evidence anchors:
  - [Section 5] "the dynamics do not modify agents' attention set"
  - [Section 5] "attention introspection property is immediately preserved by this event model, as the target condition Aaφ ensures that an agent knows what she attends to"
  - [corpus] No corpus comparison; attention preservation is framework-internal claim.
- Break condition: If modeling attention *shifts* during events (e.g., attentional capture), this preservation property becomes a limitation requiring extension.

## Foundational Learning

- Concept: **Product Update in DEL**
  - Why needed here: The core operation M ⊗ E that transforms epistemic states when events occur; edge-conditioned version modifies accessibility relation computation.
  - Quick check question: Given a Kripke model with worlds w,v connected by Ra and an edge-conditioned event with edge (e:φ, f:ψ) ∈ Qa, when do (w,e) and (v,f) connect in the updated model?

- Concept: **Modal Logic Semantics (Kripke Models)**
  - Why needed here: Understanding belief modality Baφ, accessibility relations, and satisfaction ⊨ is prerequisite for reading the axiomatization.
  - Quick check question: What does (M,w) ⊨ Baφ mean, and how does it differ from (M,w) ⊨ Aaφ?

- Concept: **Bisimulation and Update Equivalence**
  - Why needed here: The succinctness claims hinge on showing edge-conditioned and standard models produce bisimilar results across all base models.
  - Quick check question: Two models are bisimilar if they satisfy [Atom], [Forth], and [Back] conditions—can you sketch why bisimilar models validate the same formulas?

## Architecture Onboarding

- Component map:
  Base Kripke Model (W, R, V) -> Attention Model (W, R, V, A) -> Edge-Conditioned Event Model (E, Q, pre) -> Product Update M ⊗ C -> Updated Attention Model

- Critical path: Understanding transformation T1 (standard → edge-conditioned) and T1' (edge-conditioned → standard) is essential for verifying equivalence claims. The bijection in the proof shows how maximally consistent condition sets at each event determine standard-event counterparts.

- Design tradeoffs:
  - Succinctness vs. condition complexity: Fewer events but more complex edge formulas
  - Generality vs. tractability: Arbitrary attention sets allow any pattern but no built-in resource constraints
  - Static attention vs. attention dynamics: Current framework preserves attention; capturing attention shifts requires extension

- Failure signatures:
  - Exponential blowup when translating edge-conditioned → standard (by design: mc(e) can be exponential in |Φ(e)|)
  - Underspecified attention leading to update ambiguity when conditions don't fully determine unique transitions
  - Inconsistent attention sets if principles aren't enforced

- First 3 experiments:
  1. Implement H(φ) for a simple revelation with 3 agents and 2 atoms; compare event count to F(φ) to verify O(n) vs. exponential scaling.
  2. Construct an attention model where agent a attends to Bbφ but not φ directly; trace product update to confirm a learns b's belief without learning φ's truth value.
  3. Define attention principle "agent a ignores agent b" (Bbφ ∉ Aa(w) for all φ); verify through model checking that a never updates based on b's announcements across multiple revelation events.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should attention capacity constraints be formalized in the logic of general attention, and what ordering mechanisms should determine which formulas agents prioritize when revealed information exceeds their attentional capacity?
- Basis in paper: [explicit] "In future work, we would like to further investigate the properties of our general attention framework and extend it to incorporate additional features of attention, such as capacity constraints... To address this issue, we need an ordering of the formulas that determines which ones are attended first and thus learned. We will explore this topic in our next paper."
- Why unresolved: The current framework places no restrictions on the size of attention sets, allowing agents to attend to arbitrarily many formulas simultaneously, which is cognitively unrealistic.
- What evidence would resolve it: A formal extension incorporating capacity bounds and priority orderings with a complete axiomatization.

### Open Question 2
- Question: How can the logic of general attention be adapted to plausibility models to enable belief revision and recovery from false beliefs?
- Basis in paper: [explicit] "A standard limitation of classical DEL, inherited by our framework, is that agents cannot recover from false beliefs... Adapting our logic of general attention to that framework is another interesting direction for future work."
- Why unresolved: The current framework uses Kripke models where agents cannot revise beliefs when confronted with contradictory information—they instead come to believe every formula.
- What evidence would resolve it: An extension of the edge-conditioned event model framework to plausibility models with appropriate update mechanisms.

### Open Question 3
- Question: What is the precise formal relationship between attention and awareness as modalities in epistemic logic?
- Basis in paper: [explicit] "Going beyond them, the correspondence to awareness logic allows to study the relation between attention and awareness, rarely discussed in the literature."
- Why unresolved: While the framework is structurally similar to awareness logic, attention differs in being intrinsically dynamic—it determines what agents learn from events rather than restricting standing beliefs.
- What evidence would resolve it: A formal comparison establishing correspondences, translations, or expressiveness relationships between the two logics.

## Limitations

- The framework assumes static attention, preserving attention sets through updates rather than modeling attention shifts or capture phenomena.
- While exponential succinctness is proven theoretically, the edge conditions may become complex enough to offset event-count savings in practical applications.
- The framework inherits DEL's limitation of no belief revision—agents cannot recover from false beliefs when confronted with contradictory information.

## Confidence

- Exponential succinctness claim: High confidence - rigorously proven through explicit construction and bisimulation arguments
- Static attention assumption: Medium confidence - well-justified for modeling stable attentional dispositions but limits capturing attention dynamics
- Completeness proof: High confidence - follows standard DEL techniques but not mechanically verified
- Practical applicability: Medium confidence - demonstrated through examples but lacks empirical validation in real-world scenarios

## Next Checks

1. Implement H(φ) for increasing numbers of atoms and agents; empirically verify the O(n) vs. exponential scaling in event counts.
2. Extend the framework to model attention shifts by modifying the attention preservation assumption; assess impact on axiomatization and completeness.
3. Apply the framework to a human-in-the-loop AI reasoning scenario; empirically evaluate whether edge-conditioned event models provide computational advantages over standard models in practice.