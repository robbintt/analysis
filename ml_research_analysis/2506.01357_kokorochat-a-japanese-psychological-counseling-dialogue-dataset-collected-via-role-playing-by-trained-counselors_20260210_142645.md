---
ver: rpa2
title: 'KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected
  via Role-Playing by Trained Counselors'
arxiv_id: '2506.01357'
source_url: https://arxiv.org/abs/2506.01357
tags:
- uni00000011
- uni0000000f
- uni00000017
- uni00000018
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the need for high-quality psychological counseling
  dialogue data by developing KokoroChat, the largest manually collected Japanese
  dataset of its kind, constructed via role-playing by trained counselors. The approach
  mitigates privacy concerns while ensuring authenticity and professional quality,
  and includes detailed client feedback across 20 evaluation dimensions.
---

# KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors

## Quick Facts
- arXiv ID: 2506.01357
- Source URL: https://arxiv.org/abs/2506.01357
- Reference count: 23
- Japanese psychological counseling dialogue dataset collected via role-playing by trained counselors with 20-dimension client feedback

## Executive Summary
This paper introduces KokoroChat, the largest manually collected Japanese dataset of psychological counseling dialogues, constructed through role-playing by trained counselors. The dataset addresses privacy concerns while maintaining authenticity and professional quality through detailed client feedback across 20 evaluation dimensions. Experiments demonstrate that fine-tuning open-source LLMs with KokoroChat improves both response generation quality and automatic dialogue evaluation accuracy, with the high-quality subset (Kokoro-High) outperforming larger but noisier datasets.

## Method Summary
The KokoroChat dataset consists of 6,589 Japanese counseling dialogues (avg. 91.2 utterances each) collected via an online platform where trained counselors perform role-plays. Each dialogue is paired with 20-dimensional client feedback scores on a 0-5 scale. The authors fine-tune Llama-3.1-Swallow-8B-Instruct-v0.3 using QLoRA for two tasks: response generation and score prediction. Response generation uses LoRA adapters with specific configurations, while score prediction is trained to output evaluations across all 20 dimensions. The dataset is split into Kokoro-Low (score <70), Kokoro-High (70-98), and Kokoro-Full (≤98) for controlled experiments.

## Key Results
- Fine-tuned models with KokoroChat data outperform non-fine-tuned models in both response quality and evaluation accuracy
- Kokoro-High (2,601 dialogues) outperforms Kokoro-Full (6,471 dialogues), demonstrating quality over quantity
- Score prediction model achieves 35.35% accuracy and 0.8283 MAE across 20 evaluation dimensions
- Human evaluation shows enhanced response quality compared to non-fine-tuned models, particularly in reducing excessive questioning behavior

## Why This Works (Mechanism)
The approach works by leveraging high-quality, professionally-constructed role-play data that captures authentic counseling dynamics while avoiding privacy issues. The dual-task fine-tuning (response generation and score prediction) creates a system where the evaluation model can assess the quality of generated responses, enabling automated quality control. The 20-dimensional evaluation framework provides granular feedback that captures multiple aspects of counseling effectiveness beyond simple satisfaction scores.

## Foundational Learning
- **Role-played counseling data**: Understanding why simulated interactions can provide high-quality training data while avoiding privacy concerns
  - *Why needed*: Real counseling data is difficult to obtain due to privacy regulations
  - *Quick check*: Compare role-play data characteristics with real counseling session transcripts if available

- **20-dimensional evaluation framework**: Recognizing the importance of multi-faceted quality assessment in counseling
  - *Why needed*: Single-dimension satisfaction scores miss crucial aspects of counseling effectiveness
  - *Quick check*: Verify that the 20 dimensions cover key counseling competencies (empathy, questioning, advice-giving, etc.)

- **QLoRA fine-tuning for dialogue systems**: Understanding efficient parameter-efficient fine-tuning for large language models
  - *Why needed*: Full fine-tuning is computationally expensive and may not generalize well
  - *Quick check*: Compare parameter counts and performance between full fine-tuning and QLoRA approaches

## Architecture Onboarding

- **Component map:** Data Collection & Curation Pipeline -> Fine-tuning for Response Generation -> Inference & Deployment, plus separate Fine-tuning for Score Prediction -> Evaluation System

- **Critical path:** Data Curation -> Fine-tuning for Response Generation. The paper demonstrates that data quality (high client feedback scores) is a stronger driver of model performance than data quantity alone, making the creation of high-quality training subsets the most critical upstream process.

- **Design tradeoffs:**
  - Quality vs. Quantity in Data: Using Kokoro-High (2,601 dialogues) yields better performance than Kokoro-Full (6,471 dialogues)
  - Role-Play Authenticity vs. Real-World Data: Simulated authenticity trades perfect authenticity for scalability and safety
  - Human vs. Automated Evaluation: Automated evaluation is faster and cheaper but less granular than human assessment

- **Failure signatures:**
  - Model asks too many questions, disrupting conversation flow
  - Model generates advice without establishing empathy first
  - Score prediction is inaccurate, indicating failure to learn human evaluation criteria

- **First 3 experiments:**
  1. Reproduce the Quality Split Experiment: Create Low, High, and Full datasets; fine-tune models; verify that High model performs best
  2. Evaluation Model Correlation Check: Calculate Spearman correlation between predicted and actual scores across all 20 dimensions
  3. Out-of-Distribution Test: Evaluate model responses with counselors who weren't part of original data collection

## Open Questions the Paper Calls Out
- How well does automatic evaluation accuracy on role-played counseling dialogues transfer to real client-counselor interactions?
- Do gender and age imbalances (80% female, 80% aged 30–59) introduce systematic bias in model outputs?
- Does fine-tuning on KokoroChat reduce excessive questioning behavior observed in non-fine-tuned models?
- How does dialogue act annotation and strategy analysis reveal mechanisms behind effective counseling interactions?

## Limitations
- Role-play data may not fully capture real counseling dynamics despite methodological rigor
- Dataset's Japanese-language focus limits generalizability to other cultural contexts
- Human evaluation sample size (100 responses, 5 counselors) is relatively modest for robust claims

## Confidence

*High Confidence Claims:*
- Dataset construction methodology is clearly described and reproducible
- Technical fine-tuning approach (QLoRA with specified parameters) is well-documented
- Comparative results showing Kokoro-High outperforming Kokoro-Full are supported

*Medium Confidence Claims:*
- Kokoro-High is "the largest manually collected Japanese dataset" - likely accurate but comprehensive verification is challenging
- Generalizability to real-world counseling scenarios - supported by rigor but limited by role-play nature

*Low Confidence Claims:*
- Broader claims about clinical applicability and safety in real therapeutic contexts are not empirically validated
- Robustness of score prediction model across diverse counseling scenarios remains untested

## Next Checks
1. **Real-world Generalization Test**: Compare fine-tuned model responses against practicing counselors in actual client sessions using the same 20-dimension evaluation framework

2. **Cross-cultural Validation**: Translate and adapt KokoroChat methodology to collect similar datasets in different cultural contexts to assess universality and identify cultural-specific patterns

3. **Longitudinal Safety Assessment**: Implement systematic monitoring to evaluate model responses over extended conversations for potential safety issues or therapeutic ineffectiveness