---
ver: rpa2
title: Uncertainty-Aware Explainable Federated Learning
arxiv_id: '2503.05194'
source_url: https://arxiv.org/abs/2503.05194
tags:
- uncertainty
- rule
- rules
- uncertainxfl
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UncertainXFL, the first explainable federated
  learning framework that explicitly incorporates uncertainty information into the
  explanation generation process. The framework generates logical rules as explanations
  while quantifying their uncertainty, which arises from the labeller's confidence
  in identifying features.
---

# Uncertainty-Aware Explainable Federated Learning

## Quick Facts
- **arXiv ID:** 2503.05194
- **Source URL:** https://arxiv.org/abs/2503.05194
- **Reference count:** 10
- **Primary result:** First XFL framework incorporating uncertainty into explanation generation, achieving 90.34% model accuracy and 90.84% rule accuracy on CUB dataset

## Executive Summary
This paper introduces UncertainXFL, the first explainable federated learning framework that explicitly incorporates uncertainty information into the explanation generation process. The framework generates logical rules as explanations while quantifying their uncertainty, which arises from the labeller's confidence in identifying features. During FL training, clients upload both their model updates and rule sets to the server, which aggregates them in a conflict-free manner while weighting clients based on the reliability of their explanations (measured by both accuracy and uncertainty). The uncertainty information guides the FL aggregation process, prioritizing more reliable clients.

## Method Summary
The framework uses a ResNet backbone to extract features, which are then modulated by labeller uncertainty scores (1.0 for "definitely", 0.7 for "probably", 0.5 for "guessing", 0 for "not visible") through element-wise multiplication. A concept-based network generates logical rules from these uncertainty-weighted features. During federated learning, the server aggregates rules using a ranking score combining accuracy and uncertainty, resolving conflicts by prioritizing higher-scoring rules or using OR operators for semantically related features. Clients are weighted in model aggregation based on how frequently their rules are selected for the global set, creating a feedback loop where better explanations lead to greater influence on the global model.

## Key Results
- Achieves 90.34% model accuracy and 90.84% rule accuracy on CUB dataset
- Outperforms state-of-the-art baseline without uncertainty consideration by 2.71% and 1.77% respectively
- Demonstrates that uncertainty-aware aggregation improves both model performance and explanation quality
- Shows sensitivity to artificial uncertainty injection, with MNIST experiments showing lower accuracy than baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modulating feature vectors with labeller confidence scores (aleatoric uncertainty) improves the reliability of generated logical rules and downstream model performance.
- **Mechanism:** The framework alters the input to the concept-based network. Instead of binary feature vectors, it uses an augmented vector $\hat{v}_i = v_i \odot u_i$ (element-wise multiplication of feature values with uncertainty values). This suppresses the activation of features in the logic rule extraction process if the labeller was uncertain, preventing low-confidence features from defining the decision boundary.
- **Core assumption:** Labeller confidence scores (e.g., "definitely" vs. "guessing") are accurate proxies for feature noise and validity.
- **Evidence anchors:** [abstract] "includes uncertainty scores derived from labeller confidence... surpassing baselines by 2.71%"; [section 4.2] "feature vector augmented with uncertainty information... is then calculated as: $\hat{v}_i = v_i \odot u_i$"
- **Break condition:** If uncertainty is artificial or does not reflect actual data noise (e.g., simulated uncertainty via image overlay in MNIST), the model may discard useful signal, leading to reduced accuracy compared to non-uncertainty baselines.

### Mechanism 2
- **Claim:** Weighting client contributions to the global model based on the quality of their local explanations (accuracy + uncertainty) enhances global model robustness.
- **Mechanism:** The server aggregates rules using a ranking score $R_k$ combining rule accuracy and uncertainty. It tracks how often a client's rules are selected for the global set ($t_k$). This frequency count determines the aggregation weight $w_k$ for the client's model parameters, prioritizing clients with high-fidelity, high-confidence explanations.
- **Core assumption:** Clients with better local explanations (logical rules) possess better local data quality or model states, making their weight updates more valuable for the global model.
- **Evidence anchors:** [abstract] "guides the FL training process by prioritizing clients with the most reliable explanations through higher weights during model aggregation"; [section 4.4] "The weight assigned to k for model aggregation... reflects the frequency for which a client's rules are regarded as important"
- **Break condition:** If high rule fidelity does not correlate with generalization performance on the server's validation set, weighting by explanation quality could amplify overfitting clients.

### Mechanism 3
- **Claim:** A "positive-only" logic formulation combined with conflict-aware aggregation produces more human-interpretable rules than standard logic-based XAI.
- **Mechanism:** The system excludes the logical NOT ($\neg$) operator to avoid double negatives and confusing descriptions (e.g., "NOT very small"). It resolves conflicts between correlated features (e.g., two different wing colors) by prioritizing the rule with the highest accuracy $\times$ uncertainty product, or using the OR operator if semantically valid.
- **Core assumption:** Humans prefer positive assertions (what the object *is*) over negative assertions (what the object *is not*) for interpretability.
- **Evidence anchors:** [section 3] "people generally prefer defining rules using the positive form of a feature because it is easier to understand"; [section 5.5] "omitting the $\neg$ form... offers a more logical and direct description"
- **Break condition:** In datasets where the absence of a feature is the primary differentiator, excluding the $\neg$ operator may fail to capture the decision logic.

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here:** The paper explicitly distinguishes between data noise (aleatoric) and model ignorance (epistemic). UncertainXFL relies heavily on *aleatoric* uncertainty (labeller confidence). A practitioner must understand that this framework addresses noise in the data, not just model architecture limitations.
  - **Quick check question:** Does the UncertainXFL framework primarily reduce uncertainty from noisy data labels (aleatoric) or model architecture limitations (epistemic)?

- **Concept: Concept Bottleneck Models (CBMs)**
  - **Why needed here:** The architecture does not explain raw pixels directly. It relies on a neural network that first predicts "concepts" (features) which are then mapped to classes. Understanding that the "explanation" is derived from this intermediate concept layer is critical.
  - **Quick check question:** In the UncertainXFL architecture, are the logical rules extracted from the raw pixel input layer or an intermediate feature/concept layer?

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** The proposed mechanism modifies the standard FedAvg weight calculation ($\frac{n_k}{n}$). To understand the contribution of this paper, one must first understand that standard FL weights clients by data size, whereas UncertainXFL weights them by explanation quality.
  - **Quick check question:** How does UncertainXFL deviate from the standard FedAvg weighting strategy during the server aggregation phase?

## Architecture Onboarding

- **Component map:** Client Node: ResNet (Feature Extractor) -> Concept Network (Predicts features/classes) -> Logic Rule Extractor; Server Node: Validation Set -> Rule Ranker (calculates $R_k$) -> Conflict Resolver -> Model Aggregator (Weighted by $t_k$); Data Flow: Local models/rules -> Server -> Global Rules -> Global Model updates

- **Critical path:**
  1. Local training generates a feature vector
  2. **Crucial Step:** Vector is modulated by uncertainty scalar ($v \cdot u$)
  3. Rules are extracted and uploaded
  4. Server ranks rules using $acc \cdot u$
  5. Server updates model weights based on rule selection frequency, *not* just data volume

- **Design tradeoffs:**
  - **Positive-only Logic:** Sacrifices potential logical completeness (using NOT) for higher user interpretability and reduced cognitive load
  - **Greedy vs. Beam Search:** The paper moves from Beam Search (used in prior work LR-XFL) to a Greedy uncertainty-guided aggregation. This is computationally cheaper (lower memory cost) but risks missing globally optimal rule combinations
  - **Simulated vs. Real Uncertainty:** The paper notes a performance dip on MNIST (simulated uncertainty) vs. success on CUB (real labeller uncertainty), suggesting the system is sensitive to the fidelity of the uncertainty signal

- **Failure signatures:**
  - **Artificial Uncertainty Injection:** If uncertainty is simulated via image overlay (as in the MNIST experiment), model accuracy may drop (e.g., 95.71% vs 97.40% for baselines) because the signal-to-noise ratio is effectively lowered
  - **Validation Set Drift:** If the server's validation set is not representative of client data distributions, the rule ranking mechanism may prioritize spurious rules

- **First 3 experiments:**
  1. **Baseline Comparison:** Run UncertainXFL vs. LR-XFL (logic rules without uncertainty) on CUB to verify if uncertainty weighting improves accuracy (Target: +2.71% improvement)
  2. **Ablation on Aggregation:** Run UncertainXFL using standard FedAvg aggregation (ignoring rule weights) to prove the value of the uncertainty-weighted aggregation mechanism
  3. **Logic Analysis:** Qualitatively compare rules for a specific class (e.g., "Common Yellowthroat") with and without the $\neg$ operator to verify interpretability claims

## Open Questions the Paper Calls Out

- **Open Question 1:** How can epistemic (model) uncertainty be effectively quantified and integrated into the UncertainXFL framework alongside aleatoric uncertainty? The paper states plans to incorporate model uncertainty alongside the aleatoric uncertainty currently used, but this presents a different technical challenge for aggregation.

- **Open Question 2:** Does the exclusion of negative logical connectives (¬) reduce explanation fidelity in datasets where feature co-occurrence is not mutually exclusive? The paper asserts that positive features are sufficient for datasets like CUB, but does not validate if this simplification fails in domains where the *absence* of a feature is a critical, independent predictor.

- **Open Question 3:** How does UncertainXFL perform when applied to naturally occurring uncertainty versus the artificially simulated uncertainty used in the MNIST experiments? The paper notes that MNIST uncertainty was artificially simulated and models without uncertainty actually performed better, suggesting the artificial noise did not reflect true labeling ambiguity.

## Limitations

- The framework assumes accurate uncertainty labels from human labellers, which may not hold in practice and could propagate errors through the system
- Simulated uncertainty experiments (MNIST) show degraded performance compared to baselines, suggesting the approach may be sensitive to the quality of uncertainty signals
- The greedy aggregation strategy for rules is computationally efficient but may miss globally optimal rule combinations compared to more exhaustive search methods

## Confidence

- **High confidence:** The core mechanism of weighting clients by explanation quality (accuracy × uncertainty) is well-supported by experimental results showing 2.71% accuracy improvement on CUB
- **Medium confidence:** The interpretability benefits of positive-only logic formulation are supported qualitatively but lack systematic human evaluation studies
- **Medium confidence:** The sensitivity to artificial vs. real uncertainty signals is demonstrated but not fully explained mechanistically

## Next Checks

1. **Ablation study:** Run experiments removing the uncertainty weighting from client aggregation to quantify the exact contribution of this mechanism versus other components
2. **Cross-dataset validation:** Test the framework on datasets with varying levels of feature correlation to evaluate the robustness of the conflict resolution mechanism
3. **Human evaluation:** Conduct systematic studies measuring whether positive-only rules are indeed more interpretable than complete logical rules including negation operators