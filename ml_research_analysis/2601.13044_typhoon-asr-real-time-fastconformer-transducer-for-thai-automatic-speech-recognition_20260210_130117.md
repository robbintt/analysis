---
ver: rpa2
title: 'Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech
  Recognition'
arxiv_id: '2601.13044'
source_url: https://arxiv.org/abs/2601.13044
tags:
- thai
- typhoon
- isan
- data
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition

## Quick Facts
- **arXiv ID**: 2601.13044
- **Source URL**: https://arxiv.org/abs/2601.13044
- **Reference count**: 7
- **Primary result**: 115M-parameter streaming model achieves 45× fewer FLOPs than Whisper Large-v3 with comparable accuracy

## Executive Summary
Typhoon ASR Real-time presents a 115M-parameter FastConformer-Transducer model optimized for streaming Thai speech recognition with low latency. The paper argues that rigorous text normalization can match the impact of model scaling for low-resource ASR, enabling a compact model to achieve competitive accuracy while dramatically reducing computational cost. A two-stage curriculum learning approach enables dialect adaptation to Isan (north-eastern Thai) while preserving performance on Central Thai. The model demonstrates strong performance across multiple benchmarks while maintaining real-time streaming capability.

## Method Summary
The approach combines a FastConformer-Transducer architecture with aggressive text normalization and curriculum-based dialect adaptation. The model uses 8× depthwise convolutional subsampling to reduce sequence length before attention layers, enabling efficient streaming inference. Training employs consensus pseudo-labeling from three Whisper models followed by normalization of training targets to spoken-form Thai. Isan dialect adaptation uses a two-stage curriculum: Stage 1 applies low learning rate fine-tuning to the full model, while Stage 2 freezes the encoder and applies high learning rate fine-tuning to the decoder and joint network. The model is trained on ~11,000 hours of Thai audio data.

## Key Results
- 115M FastConformer-Transducer achieves 45× reduction in computational cost compared to Whisper Large-v3
- Model maintains competitive accuracy (5.69% CER on FLEURS with normalization vs 9.98% without)
- Isan dialect adaptation reduces CER from 16.22% to 10.65% while preserving Central Thai performance
- Streaming inference enables real-time applications with minimal latency

## Why This Works (Mechanism)

### Mechanism 1
Rigorous text normalization eliminates training-target noise from ambiguous orthographic forms, reducing the hypothesis space the model must learn. This allows a smaller model to achieve comparable accuracy to larger models trained on inconsistent labels. The normalization converts Arabic numerals, repetition markers, and symbolic ranges into canonical spoken-form Thai.

### Mechanism 2
FastConformer-Transducer architecture enables streaming inference through frame-synchronous output generation. The 8× depthwise convolutional subsampling reduces sequence length before attention layers, while the Transducer decoder produces outputs without autoregressive decoding latency. This combination enables real-time processing with 45× fewer FLOPs than Whisper Large-v3.

### Mechanism 3
Two-stage curriculum learning decouples acoustic and linguistic adaptation during dialect fine-tuning. Stage 1 uses low learning rate for full-model adaptation to Isan tonal patterns, while Stage 2 freezes the encoder and uses high learning rate for rapid learning of Isan vocabulary. This preserves Central Thai performance while achieving strong Isan recognition.

## Foundational Learning

- **Concept: Streaming vs. Offline ASR Architectures**
  - **Why needed here:** The paper's core contribution is a streaming-optimized model; understanding why Whisper fails for streaming is essential context.
  - **Quick check question:** Can you explain why autoregressive decoder inference introduces unpredictable latency that makes Whisper unsuitable for real-time applications?

- **Concept: Transducer (RNN-T) Decoding**
  - **Why needed here:** The model uses a Transducer decoder instead of Transformer decoder; this is the architectural enabler for streaming.
  - **Quick check question:** How does frame-synchronous emission in Transducer differ from token-autoregressive generation in Whisper?

- **Concept: Text Normalization for ASR Training Targets**
  - **Why needed here:** The paper argues data quality equals model scaling impact; normalization is their primary data intervention.
  - **Quick check question:** Why would inconsistent number representation (digits vs. words) create training noise for an acoustic model?

## Architecture Onboarding

- **Component map:**
  Audio Input → 8× Depthwise Conv Subsampling → FastConformer Encoder (local attention) → Joint Network → Transducer Decoder → Token Emission (frame-synchronous)

- **Critical path:**
  1. Text normalization pipeline (Section 2.2) — incorrect normalization creates inconsistent training targets
  2. Consensus pseudo-labeling quality (Section 2.1) — garbage labels propagate to model
  3. Learning rate schedule for curriculum stages (Section 3.3) — wrong LR causes catastrophic forgetting

- **Design tradeoffs:**
  - Phonetic fidelity vs. readability: Model outputs spoken forms (หนึ่งศูนย์) not digits (10)—requires ITN post-processing for display
  - Streaming latency vs. accuracy: Local attention limits long-range context; offline models (Table 6) still achieve lower CER
  - Dialect coverage vs. model count: Single multi-dialect model vs. per-dialect adapters not explored

- **Failure signatures:**
  - Numbers output as mixed digit-word forms → normalization pipeline incomplete
  - Isan audio transcribed with Central Thai particles → Stage 2 decoder fine-tuning failed or insufficient Isan data
  - High latency on long audio → attention window misconfiguration or streaming buffer overflow
  - Hallucinations on silence/near-silence → Transducer blank token calibration issue (not discussed in paper)

- **First 3 experiments:**
  1. **Reproduce normalization impact:** Train identical architecture with raw vs. normalized transcripts on held-out subset; measure CER delta
  2. **Streaming latency benchmark:** Measure real-time factor (RTF) and latency distribution on 30s audio chunks vs. Whisper baseline
  3. **Dialect ablation:** Skip Stage 2 (decoder-only fine-tuning) and measure Isan CER to validate the 5.57% improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
How can robust Inverse Text Normalization (ITN) be developed to convert the model's phonetic "spoken form" outputs back into context-aware written formats (e.g., postal codes, currency) for end-user applications? The model is currently trained on a strict normalization pipeline that prioritizes phonetic fidelity (e.g., outputting "nueng sun" instead of "10"), creating outputs that are unsuitable for display without post-processing.

### Open Question 2
Can the FastConformer-Transducer architecture be optimized for heavy Thai-English code-switching without sacrificing streaming efficiency? The current model forces phonetic mapping to Thai characters rather than switching to Latin script, limiting utility in technical domains where mixed-script output is preferred.

### Open Question 3
Can unified streaming models effectively perform zero-shot identification and adaptation for diverse regional Thai dialects (Northern, Southern) alongside Central Thai? The current approach requires a specific two-stage curriculum learning strategy for each dialect; it is unknown if a single efficient streaming model can generalize across dialects without explicit flagging.

### Open Question 4
To what extent can shallow fusion or contextual biasing improve the recognition of under-represented domain-specific vocabulary (e.g., proper names, technical jargon) in this streaming architecture? The paper identifies a "semantic gap" where the compact 115M model lacks the world knowledge of massive foundation models, leading to difficulties with low-frequency entities or homophones.

## Limitations
- Internal datasets (Curated Media, Isan gold-standard, TTS data) are not publicly accessible, preventing exact reproduction
- Complete text normalization pipeline is referenced but not fully specified or released
- Model architecture details (attention head configuration, joint network specifics) are not fully detailed
- Generalizability of text normalization impact to other languages not empirically validated

## Confidence

**High Confidence**: Architectural efficiency claims (45× reduction) are supported by specific parameter counts and FLOPs comparisons, using well-established mechanisms (depthwise convolution, Transducer decoding).

**Medium Confidence**: Text normalization impact is supported by internal evidence (FLEURS results) but relies on proprietary data and unpublished normalization rules.

**Low Confidence**: Two-stage curriculum learning for dialect adaptation is least validated, relying on internal human A/B testing without rigorous ablation studies on the specific architecture.

## Next Checks

1. **Independent Normalization Impact Study**: Train FastConformer-Transducer on publicly available Thai dataset with and without normalization pipeline; measure CER delta to isolate data quality effect from model capacity effects.

2. **Streaming Latency Benchmarking**: Conduct real-time factor (RTF) and end-to-end latency measurement comparing Typhoon (115M) against Whisper Large-v3 and CarelessWhisper on identical hardware; measure 50th and 95th percentile latency on 30-second audio chunks.

3. **Dialect Adaptation Ablation**: Train three variants (no adaptation, Stage 1 only, both stages) and evaluate CER on Isan and Central Thai test sets to quantify Stage 2 contribution and monitor Central Thai performance for catastrophic forgetting.