---
ver: rpa2
title: 'Why Synthetic Isn''t Real Yet: A Diagnostic Framework for Contact Center Dialogue
  Generation'
arxiv_id: '2508.18210'
source_url: https://arxiv.org/abs/2508.18210
tags:
- stage
- call
- transcript
- generation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a diagnostic framework for evaluating synthetic
  dialogue generation in contact centers, where privacy and data scarcity limit access
  to real transcripts. Unlike prior work focused on open-domain or medical dialogues,
  this framework addresses goal-oriented, role-asymmetric, and behaviorally complex
  conversations featuring disfluencies, ASR noise, and compliance-driven agent actions.
---

# Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation

## Quick Facts
- **arXiv ID:** 2508.18210
- **Source URL:** https://arxiv.org/abs/2508.18210
- **Authors:** Rishikesh Devanathan; Varun Nathan; Ayush Kumar
- **Reference count:** 40
- **Primary result:** No single generation method excels across all 18 linguistic and behavioral traits; synthetic dialogues fail notably on disfluency, sentiment, and behavioral realism.

## Executive Summary
This paper introduces a diagnostic framework for evaluating synthetic dialogue generation in contact centers, addressing privacy and data scarcity constraints. Unlike prior work on open-domain or medical dialogues, the framework targets goal-oriented, role-asymmetric conversations with disfluencies, ASR noise, and compliance-driven agent actions. The authors propose conditioning generation on structured supervision signals—intent summaries, topic flows, QA forms—and injecting ASR noise and disfluencies to simulate realistic conditions. A diagnostic framework of 18 linguistically and behaviorally grounded metrics is introduced to compare real and synthetic transcripts. Benchmarking four language-agnostic generation strategies (single-stage, dual-stage turn/count, characteristic-aware) against baselines shows no method excels across all traits, with notable deficits in disfluency, sentiment, and behavioral realism. The diagnostic tool enables fine-grained evaluation and stress testing of synthetic dialogue across languages, exposing specific gaps in realism and guiding future improvements.

## Method Summary
The paper benchmarks four synthetic dialogue generation strategies using GPT-4.1-mini, conditioned on structured supervision signals (intent summaries, topic flows, QA forms) extracted from real call center data. The Dual Stage - Turn Count pipeline splits conversations into chunks for fine-grained noise injection, while the Characteristic-Aware pipeline targets specific behavioral traits. Evaluation uses 18 metrics computed by Claude-3.5-Sonnet, measuring linguistic complexity, behavioral realism, and semantic fidelity. Generation is optimized using DSPy (MIPROv2) to maximize a weighted Reconstruction Score, but the core finding is that high input fidelity does not guarantee behavioral realism. Distribution matching via Chi-square/G-tests and Jensen-Shannon divergence is used to compare synthetic and real data across languages (English, Spanish, French, French-Canadian).

## Key Results
- No generation method excels across all 18 traits; all show significant deficits in disfluency, sentiment, and behavioral realism.
- Single Stage performs best on semantic fidelity (intent, topic flow) but worst on surface realism (disfluency, ASR noise).
- Dual Stage - Turn Count achieves the highest scores for disfluency, ASR noise, and interruption but lags on semantic traits.
- High Reconstruction Scores (input fidelity) do not correlate with high behavioral realism, indicating the need for evaluation-aware tuning.
- Language-specific challenges emerge: French and French-Canadian data show higher JS Divergence, suggesting weaker supervision signal coverage.

## Why This Works (Mechanism)

### Mechanism 1: Structured Supervision Anchoring
Conditioning generation on structured metadata (intent summaries, QA forms) reduces hallucination and improves semantic fidelity by acting as semantic anchors. These inputs force the LLM to adhere to specific events and resolution paths defined by real operational data rather than probabilistic guessing. Core assumption: the structured inputs accurately represent the ground truth and are not themselves noisy or biased. Break condition: if supervision signals are generic or contradictory, the model may force unnatural or "scripted" resolutions.

### Mechanism 2: Decoupling Semantic Planning from Surface Realization
Separating the generation of clean dialogue logic from the injection of noisy speech artifacts (ASR errors, disfluencies) improves realism. The Dual Stage pipeline first generates coherent base dialogue, then applies perturbations in a second pass, allowing focused control over surface-level noise. Core assumption: post-hoc insertion effectively mimics natural co-occurrence of cognitive load and disfluency. Break condition: if noise injection lacks diversity or context-awareness, the transcript may read as disjointed or parodic despite improved statistical scores.

### Mechanism 3: Behavioral Distribution Matching over Reconstruction
High fidelity to input prompts does not guarantee realism; effective evaluation requires comparing the generated data's distribution against real data across behavioral metrics. The framework evaluates success by whether the statistical distribution of traits (sentiment arcs, repetition, disfluency) matches real call center distributions. Core assumption: real transcripts represent the optimal target distribution for synthetic data. Break condition: if the real dataset is small or biased, synthetic data may correctly match a skewed distribution, reinforcing dataset bias.

## Foundational Learning

- **Concept: Role Asymmetry in Dialogue**
  - *Why needed here:* Contact center dialogues have strict role constraints (Agent vs. Customer) where agents follow compliance scripts and customers drive intent.
  - *Quick check:* Does your synthetic pipeline enforce different vocabularies and constraints for the Agent (compliance-focused) versus the Customer (complaint-focused)?

- **Concept: Surface Realism vs. Semantic Fidelity**
  - *Why needed here:* The paper proves these are orthogonal; a transcript can perfectly capture "reason for call" but fail to include "fillers" or "ASR errors."
  - *Quick check:* If you optimize only for "Intent Summary" accuracy, which specific traits (according to Table 4) are likely to fail?

- **Concept: Statistical Distance Metrics (JS Divergence/Chi-Square)**
  - *Why needed here:* Standard metrics like BLEU fail here; you must measure if synthetic data has "too much" or "too little" of a trait using distribution tests.
  - *Quick check:* Why is a p-value > 0.05 in a Chi-square test desirable when comparing synthetic vs. real trait distributions?

## Architecture Onboarding

- **Component map:** Structured Supervision -> Generator (Single/Dual Stage) -> Noise Layer (Disfluency/ASR) -> Evaluator (18 metrics)
- **Critical path:** The "Dual Stage - Turn Count" pipeline is the current sweet spot; the critical dependency is the Structured Supervision inputs. If QA forms or Topic Flows are missing or low-quality, the pipeline defaults to hallucination.
- **Design tradeoffs:**
  - *Single Stage:* Cheap, fast, high semantic fidelity, but very low realism.
  - *Dual Stage:* 2x slower/costlier, handles noise better, but risks "drift" if segmentation fails.
  - *Characteristic Aware:* Best for specific control (Repetition), but underperforms on general Linguistic Complexity.
- **Failure signatures:**
  - *The "Polite Robot" Effect:* High Reconstruction Score (>0.9) but low Sentiment/Disfluency realism.
  - *Language Collapse:* Good performance in English/Spanish, but JS Divergence spikes in French/French-Canadian due to lack of language-specific noise models.
- **First 3 experiments:**
  1. Run 50 calls through Single vs. Dual Stage; plot "Disfluency" metric distribution. If Dual Stage doesn't shift toward real baseline, noise prompts are failing.
  2. Remove "QA Form" input and measure drop in "Proactivity" and "Solution" metric alignment to verify behavioral supervision.
  3. Target "ASR Noise" failure mode; adjust noise injection prompts to increase "Substitution" errors and re-evaluate against real distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can reinforcement learning or differentiable objectives bridge the gap between input reconstruction fidelity and behavioral realism?
- **Basis:** Section 7 states the pipeline does not leverage RL or differentiable objectives and suggests policy-gradient methods might iteratively refine traits like emotion or noise better than sequential prompting.
- **Why unresolved:** High Reconstruction Scores (based on topic/intent adherence) do not correlate with high performance on the 18 realism metrics, suggesting the current optimization target is misaligned with desired output style.
- **What evidence would resolve it:** A comparison of generation quality when optimizing via RL using diagnostic metrics as rewards, versus the current method of optimizing for the composite Reconstruction Score.

### Open Question 2
- **Question:** Does the generation of realistic disfluency and ASR noise require multimodal or acoustically grounded inputs rather than text-only prompting?
- **Basis:** Section 5.4 notes disfluency and ASR noise remain difficult to replicate, and Section 7 suggests multimodal inputs, phonetic modeling, or tighter ASR integration might be necessary.
- **Why unresolved:** Current text-only LLM generation struggles to simulate speech-driven artifacts authentically, resulting in statistically significant divergences from real data.
- **What evidence would resolve it:** An experiment benchmarking text-conditioned generation against speech-conditioned generation (e.g., using audio tokens or acoustic embeddings) on the Disfluency and ASR Noise metrics.

### Open Question 3
- **Question:** Can a hybrid pipeline selectively combining single-stage semantic generation and dual-stage behavioral induction outperform individual strategies?
- **Basis:** Section 7 states the paper does not investigate hybrid approaches that selectively combine strengths—e.g., using single-stage for semantic fidelity and characteristic-aware for behavioral induction.
- **Why unresolved:** Results show a trade-off: Single Stage excels at intent/topic flow, while Dual Stage handles interruptions/noise better. No single method excels across all 18 metrics.
- **What evidence would resolve it:** Implementing and evaluating a modular pipeline that routes specific supervision tasks (content vs. style) to the respective sub-models that performed best in isolation experiments.

## Limitations
- Proprietary source dataset and supervision signal extraction systems create significant reproducibility barriers.
- Evaluation relies on distribution matching rather than individual instance quality, potentially missing edge cases or rare compliance scenarios.
- Characteristic-aware generation requires extensive manual prompt engineering per trait dimension, limiting scalability.

## Confidence
- **High Confidence:** The diagnostic framework's 18-metric evaluation approach and the general observation that no generation strategy dominates across all behavioral traits.
- **Medium Confidence:** The specific superiority of Dual Stage - Turn Count for disfluency and ASR noise injection (results show improvement but quality depends heavily on prompt engineering).
- **Medium Confidence:** The claim that high reconstruction scores don't imply behavioral realism (supported by correlation analysis but the practical threshold for "good enough" realism remains unclear).

## Next Checks
1. **Noise Injection Validation:** Run the Dual Stage pipeline with varying noise injection probabilities (0%, 25%, 50%, 75%) and measure whether JS Divergence on disfluency/ASR metrics follows expected monotonic improvement patterns.
2. **Structured Supervision Ablation:** Generate transcripts using Single Stage with and without each supervision signal (Intent Summary, Topic Flow, QA Form) independently removed to quantify each signal's contribution to behavioral realism.
3. **Language-Specific Bias Test:** Using only English and Spanish data, train the same pipeline and evaluate whether the JS Divergence patterns (particularly for sentiment and proactivity) remain consistent across languages or reveal systematic bias in the supervision signals.