---
ver: rpa2
title: 'MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search
  Directions'
arxiv_id: '2510.20872'
source_url: https://arxiv.org/abs/2510.20872
tags:
- mobo-osd
- number
- optimization
- pareto
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOBO-OSD is a novel multi-objective Bayesian optimization algorithm
  that generates a diverse set of Pareto optimal solutions by solving multiple constrained
  optimization subproblems along orthogonal search directions (OSDs) defined with
  respect to an approximated convex hull of individual objective minima. The algorithm
  employs a well-distributed set of OSDs to ensure broad coverage of the objective
  space, enhancing both solution diversity and hypervolume performance.
---

# MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search Directions

## Quick Facts
- arXiv ID: 2510.20872
- Source URL: https://arxiv.org/abs/2510.20872
- Authors: Lam Ngo; Huong Ha; Jeffrey Chan; Hongyu Zhang
- Reference count: 40
- Primary result: Outperforms state-of-the-art algorithms in both sequential and batch settings for generating diverse Pareto optimal solutions.

## Executive Summary
MOBO-OSD is a novel multi-objective Bayesian optimization algorithm that generates a diverse set of Pareto optimal solutions by solving multiple constrained optimization subproblems along orthogonal search directions (OSDs) defined with respect to an approximated convex hull of individual objective minima. The algorithm employs a well-distributed set of OSDs to ensure broad coverage of the objective space, enhancing both solution diversity and hypervolume performance. To further improve solution density without requiring excessive subproblems, MOBO-OSD leverages a Pareto Front Estimation technique to generate additional solutions in the neighborhood of existing solutions. Additionally, the algorithm supports batch optimization through parallel function evaluations using the Kriging Believer technique. Extensive experiments on synthetic and real-world benchmark functions with two to six objectives demonstrate that MOBO-OSD consistently outperforms state-of-the-art algorithms in both sequential and batch settings.

## Method Summary
MOBO-OSD addresses multi-objective Bayesian optimization by generating diverse Pareto optimal solutions through orthogonal search directions (OSDs) defined relative to an approximated convex hull of individual minima (CHIM). The algorithm trains independent Gaussian Process surrogates for each objective, calculates boundary points to approximate the CHIM, and generates well-distributed OSDs using Riesz s-Energy. For each OSD, it solves a constrained optimization subproblem to find candidate solutions, then applies Pareto Front Estimation (PFE) to generate additional local candidates. In batch mode, it selects points using Hypervolume Improvement while enforcing diversity constraints across exploration spaces, employing Kriging Believer for parallel evaluations. The method is validated on 5 synthetic and 4 real-world benchmarks with 2-6 objectives, showing superior hypervolume performance compared to baselines.

## Key Results
- Consistently outperforms state-of-the-art algorithms (qEHVI, MESMO, PESMO, SMS-EGO) on both synthetic and real-world benchmarks.
- Generates more diverse Pareto solutions with better hypervolume coverage compared to competing methods.
- Effective in both sequential (batch=1) and batch settings (4, 8, 10) with parallel function evaluations.
- Maintains performance across problems with 2 to 6 objectives, demonstrating scalability.

## Why This Works (Mechanism)

### Mechanism 1: Geometric Coverage via Approximated CHIM
The algorithm uses orthogonal search directions normal to an approximated convex hull of individual minima (CHIM) to generate diverse Pareto candidates. Instead of random scalarization, it defines a hyperplane using boundary points derived from observed ideal and nadir points, then generates well-distributed weight vectors and projects search lines normal to this hyperplane. This forces search to probe distinct regions of the trade-off frontier, assuming the true Pareto front intersects these normal vectors or lies in their immediate neighborhood.

### Mechanism 2: Budget-Aware Local Refinement (PFE)
After finding candidate solutions via OSD subproblems, the algorithm employs First Order Approximation to define local exploration spaces and samples additional points rather than spawning new OSDs. This leverages the assumption that Pareto optimal solutions are locally connected, efficiently increasing solution density without requiring excessive expensive subproblem optimizations.

### Mechanism 3: Diversity-Constrained Batch Selection
The algorithm enforces selection from distinct exploration spaces in batch mode by maximizing Hypervolume Improvement while applying constraints that limit the number of selected points from any single local exploration space. This prevents redundant evaluations and improves hypervolume per evaluation, using Kriging Believer to update the surrogate model with fantasized observations for subsequent picks.

## Foundational Learning

- **Concept: Normal Boundary Intersection (NBI)** - Why needed: MOBO-OSD adapts NBI's use of the Convex Hull of Individual Minima (CHIM) and normal vectors. Understanding this foundation is critical to grasp the geometric approach.
  - Quick check: How does the "quasi-normal" direction used in this paper differ from a standard geometric normal vector?

- **Concept: Gaussian Processes (GPs) & Uncertainty** - Why needed: The MOBO-OSD subproblem relaxes hard constraints using confidence bounds (μ ± δσ). Understanding how GP variance defines these search tunnels is critical.
  - Quick check: What happens to the feasible region of the MOBO-OSD subproblem if the GP uncertainty (σ) collapses to zero prematurely?

- **Concept: Hypervolume Indicator** - Why needed: This is the primary metric for both the acquisition function (HVI) and experimental evaluation. The algorithm specifically seeks to maximize this volume.
  - Quick check: Why is the Hypervolume Improvement used for selection rather than just the raw Hypervolume of the candidate?

## Architecture Onboarding

- **Component map:** Input data → Train GPs → Calculate Ideal/Nadir → Approximated CHIM → Generate OSDs → Solve subproblems → PFE sampling → Aggregate candidates → Batch selection with HVI + constraints
- **Critical path:** The definition of the Approximated CHIM. If boundary points (ideal/nadir) are poorly estimated, OSDs will point in wrong directions, causing the subproblem solver to explore irrelevant regions.
- **Design tradeoffs:** Riesz s-Energy vs. Uniform Sampling adds computational overhead for optimized uniformity; Hard vs. Soft Constraints uses soft confidence bounds tolerating model error but may allow solutions to drift from exact orthogonal lines.
- **Failure signatures:** Search Space Collapse if CHIM shrinks too quickly; Solver Stagnation if SLSQP gets stuck in local optima or confidence region is too tight.
- **First 3 experiments:**
  1. Run MOBO-OSD on ZDT1 (convex) vs DTLZ2 (non-convex) and visualize OSD lines and candidate points to verify geometric coverage.
  2. Disable PFE and run with minimal OSDs (nβ=10) to confirm it compensates for sparse directions.
  3. Execute batch optimization (b=10) on Water Planning (M=6) and monitor if diversity constraint prevents clustering while maintaining hypervolume improvement.

## Open Questions the Paper Calls Out

- **Noisy Functions Extension:** The method focuses on noiseless observations and would need to employ acquisition functions that handle noise and improve the MOBO-OSD subproblem to extend to noisy settings.
- **Many-Objective Scaling:** While claiming to scale to arbitrary objectives, empirical validation is limited to 2-6 objectives, leaving questions about performance in many-objective settings (M > 10).
- **Alternative Acquisition Functions:** The method uses Hypervolume Improvement but is compatible with alternative MO acquisition functions like JES or MESMO, which could potentially improve sample efficiency.

## Limitations
- Assumes noiseless observations, limiting applicability to real-world noisy scenarios without modifications.
- Computational cost scales with number of objectives and search directions, potentially becoming prohibitive in many-objective settings.
- Geometric coverage mechanism may fail if the true Pareto front is highly non-convex or disconnected relative to the approximated CHIM scaffold.

## Confidence
- **High Confidence:** Algorithm's ability to generate diverse Pareto solutions via OSDs when approximated CHIM provides reasonable scaffold (supported by consistent improvement over baselines in convergence plots).
- **Medium Confidence:** PFE mechanism's effectiveness in compensating for sparse OSDs (limited ablation evidence; corpus papers don't validate this specific combination).
- **Low Confidence:** Batch performance claims relative to qEHVI on higher-dimensional (M=6) problems (only 2 real-world benchmarks tested; batch diversity constraint impact not fully characterized).

## Next Checks
1. **Geometry Validation:** Run MOBO-OSD on DTLZ2 (non-convex) vs ZDT1 (convex) and visualize OSD lines and candidate points to verify geometric coverage.
2. **PFE Ablation Test:** Disable PFE and run with minimal OSDs (nβ=10) to confirm it compensates for sparse directions.
3. **Batch Diversity Stress Test:** Execute batch optimization (b=10) on Water Planning (M=6) and monitor if diversity constraint prevents clustering while maintaining hypervolume improvement.