---
ver: rpa2
title: 'MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages'
arxiv_id: '2509.04111'
source_url: https://arxiv.org/abs/2509.04111
tags:
- languages
- language
- question
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiWikiQA, a multilingual reading comprehension
  dataset covering 306 languages, generated using Wikipedia articles and LLM-generated
  questions. The authors conducted crowdsourced human evaluations of question fluency
  across 30 languages, showing high quality with mean ratings above 2.0 (mostly natural).
---

# MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages

## Quick Facts
- **arXiv ID**: 2509.04111
- **Source URL**: https://arxiv.org/abs/2509.04111
- **Reference count**: 0
- **Primary result**: MultiWikiQA covers 306 languages with LLM-generated extractive QA pairs; human evaluation shows high fluency; model performance varies widely across languages.

## Executive Summary
MultiWikiQA is a new multilingual reading comprehension benchmark covering 306 languages, generated from Wikipedia articles using LLM-based question generation with verbatim answer constraints. The dataset includes 5,000 QA pairs per language where possible, with questions rephrased to prevent lexical shortcut exploitation. Human evaluation of question fluency across 30 languages showed high quality with mean ratings above 2.0. The benchmark was evaluated on 261 languages using 6 language models, revealing significant performance variation that correlates with language resource availability. The dataset and evaluation data are publicly available to address the scarcity of reading comprehension resources for low-resource languages.

## Method Summary
The dataset was constructed using Wikipedia 20231101 dump, extracting articles per language and generating 2-10 QA pairs per article using Gemini-1.5-pro with structured JSON output. Answers were required to appear verbatim in source documents, ensuring extractive format. Questions were then rephrased without context access to reduce lexical matching opportunities. Languages were split for Mandarin and Portuguese variants. Human fluency evaluation covered 30 languages via crowdsourcing. Model evaluation used EuroEval framework with 2-shot prompting for decoders and training with early stopping for encoders, requiring minimum 1,024 training samples per language.

## Key Results
- Human evaluation of question fluency across 30 languages showed mean ratings above 2.0, indicating mostly natural questions.
- Model performance varied significantly across languages, ranging from near-zero F1 on low-resource languages to ~80% on high-resource ones.
- The benchmark successfully differentiated between encoder and decoder model types, with consistent performance rankings across model architectures.

## Why This Works (Mechanism)

### Mechanism 1: Structured LLM-based QA Generation with Verbatim Answer Constraints
Constraining LLM-generated answers to appear verbatim in source documents creates extractive QA pairs that test comprehension rather than generation. Gemini-1.5-pro generates questions with JSON-structured output, and filtering validates that each answer appears exactly in source text. This ensures the task remains extractive rather than abstractive, though for extremely low-resource languages the LLM may fail to generate fluent questions.

### Mechanism 2: Question Rephrasing to Reduce Lexical Shortcut Exploitation
After initial QA generation, the same LLM rephrases each question without context access, using synonyms and structural variation while preserving meaning. This breaks direct lexical overlap between question and answer span, though rephrasing may distort meaning for languages where the LLM has weak fluency.

### Mechanism 3: Resource-Level Correlation with Model Performance
Performance correlates with language resource availability, revealing gaps in multilingual model coverage. Models trained on high-resource languages transfer poorly to low-resource ones, with F1 scores ranging from near-zero to ~80% depending on training exposure. Performance may also reflect script complexity or tokenization quality rather than pure resource level.

## Foundational Learning

- **Extractive vs. Abstractive Question Answering**: MultiWikiQA is explicitly extractive—answers must be verbatim spans. Understanding this distinction is critical for interpreting why verbatim filtering and SQuAD-format structure matter. Quick check: If a model paraphrases the correct answer rather than extracting the exact span, should it receive full credit on MultiWikiQA?

- **Lexical Overfitting in Reading Comprehension**: The rephrasing stage exists specifically to prevent models from matching keywords rather than understanding. Without this concept, the two-stage pipeline seems redundant. Quick check: Why might a model achieve high accuracy on a dataset where questions reuse article wording, yet fail on real-world queries?

- **Resource-Level Taxonomy in Multilingual NLP**: Interpreting performance discrepancy requires distinguishing high-resource from low-resource languages. The paper doesn't explicitly label these, but results cluster accordingly. Quick check: What factors beyond training data size might cause a "low-resource" language to achieve unexpectedly high scores?

## Architecture Onboarding

- **Component map**: Wikipedia dump (20231101) → Language-specific article extraction → [Stage 1] Gemini-1.5-pro QA generation (2-10 pairs/article) → JSON validation + verbatim answer filter → [Stage 2] Question rephrasing (same LLM, no context) → Final dataset (SQuAD-format triples) → Human fluency evaluation (crowdsourced, 30 languages) → Model evaluation (6 models, 261 languages)

- **Critical path**: The verbatim filtering step is the bottleneck—if answers don't match exactly, the pair is discarded. For low-resource languages with limited Wikipedia coverage, this can cap dataset size (101 languages ran out of articles before reaching 5,000 samples).

- **Design tradeoffs**: Quality vs. Scale (human evaluation covers only 30/306 languages); Rephrasing vs. Meaning Preservation (no check confirms semantic equivalence); Encoder vs. Decoder Evaluation (encoders require training, excluding 45 languages).

- **Failure signatures**: Zero F1 on specific languages likely indicates tokenization failure; high variance in results reflects small test sets; fluency ratings below 2.0 would indicate LLM generation failure.

- **First 3 experiments**:
  1. Ablate rephrasing stage: Compare model performance on original vs. rephrased questions to quantify lexical shortcut reduction impact.
  2. Cross-lingual transfer analysis: Fine-tune encoder model on high-resource languages only, then evaluate on low-resource languages from same family.
  3. Human semantic validation: For 100 rephrased questions across 5 low-resource languages, have native speakers judge semantic preservation.

## Open Questions the Paper Calls Out

- **Does the high fluency observed in the 30 surveyed languages generalize to the remaining ~90% of languages in the dataset?**: The authors explicitly state they "cannot guarantee that the conclusions from the surveys generalise to the remaining languages" due to unavailability of crowdsourcing resources for most languages.

- **Is the large performance discrepancy across languages primarily driven by model tokenizer limitations or noise in the LLM-generated questions?**: It is unclear if low F1-scores reflect poor model understanding or flawed gold-standard data from potentially unnatural LLM-generated questions in low-resource settings.

- **How effective is the automated rephrasing step at preventing word-matching heuristics in typologically diverse languages?**: The authors mention rephrasing to prevent "cheating" but do not validate if this perturbation is semantically preserved or structurally distinct in non-English languages, particularly agglutinative or non-segmenting languages.

## Limitations

- Human evaluation covers only 30 of 306 languages, leaving 96% of the dataset unverified for linguistic naturalness.
- The relationship between Wikipedia article availability and language model performance may conflate multiple factors including script complexity and tokenization quality.
- Rephrasing mechanism's effectiveness in reducing lexical shortcuts lacks direct validation, particularly for low-resource languages where semantic preservation is uncertain.

## Confidence

- **High confidence**: Dataset construction pipeline is technically sound and reproducible; evaluation methodology is clearly specified and methodologically appropriate.
- **Medium confidence**: Quality assessment from human evaluations provides reasonable evidence of fluency despite limited language coverage; observed performance discrepancies align with expected resource-level patterns.
- **Low confidence**: Claims about rephrasing stage's effectiveness lack direct validation; assumption that 30-language evaluation generalizes to all 306 languages is unverified; relationship between performance and resource availability may conflate multiple uncontrolled factors.

## Next Checks

1. **Ablation study on rephrasing effectiveness**: Systematically compare model performance on original versus rephrased questions across different resource levels to quantify whether rephrasing actually reduces lexical shortcut exploitation or adds noise.

2. **Cross-linguistic semantic preservation validation**: For 100 randomly selected rephrased questions across 5 low-resource languages, conduct human evaluations specifically focused on semantic equivalence rather than fluency to test whether rephrasing preserves meaning.

3. **Multi-factor analysis of performance variance**: Control for Wikipedia article count, script complexity, and tokenization quality separately to isolate the true drivers of performance discrepancies and distinguish comprehension difficulties from technical limitations.