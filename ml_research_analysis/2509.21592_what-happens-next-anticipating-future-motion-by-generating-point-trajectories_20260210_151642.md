---
ver: rpa2
title: What Happens Next? Anticipating Future Motion by Generating Point Trajectories
arxiv_id: '2509.21592'
source_url: https://arxiv.org/abs/2509.21592
tags:
- motion
- video
- trajectories
- ours
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of forecasting motion from a single
  image by generating dense point trajectories across the entire scene. The core idea
  is to adapt modern video generation architectures to directly predict 2D pixel coordinates
  of a grid of points over time, rather than generating pixels and then tracking motion.
---

# What Happens Next? Anticipating Future Motion by Generating Point Trajectories

## Quick Facts
- arXiv ID: 2509.21592
- Source URL: https://arxiv.org/abs/2509.21592
- Reference count: 40
- One-line primary result: The paper presents a method for forecasting motion from a single image by generating dense point trajectories, showing superior physical plausibility, diversity, and accuracy compared to regression-based and video generator baselines.

## Executive Summary
This paper addresses the problem of forecasting motion from a single image by generating dense point trajectories across the entire scene. The core idea is to adapt modern video generation architectures to directly predict 2D pixel coordinates of a grid of points over time, rather than generating pixels and then tracking motion. This approach models the distribution of possible futures and captures scene-wide dynamics, offering improved accuracy and diversity compared to regression-based and video generator baselines. The method was evaluated on synthetic and real-world datasets, showing superior performance in physical plausibility, diversity, and accuracy, while being more computationally efficient than RGB-based video generators. Results include strong performance in motion distributional metrics and user preference studies.

## Method Summary
The approach uses a two-stage pipeline: first, a trajectory VAE compresses ground truth trajectories into latent codes; second, a Latent DiT denoiser with Rectified Flow generates latents conditioned on DINOv2 image features. The model predicts dense grid trajectories rather than sparse points, forcing it to reason about global scene geometry. This is trained on datasets including Kubric MOVi-A, LIBERO, and Physics101, with evaluation metrics including Best-of-K MSE, FVMD, and LRTL for rigidity.

## Key Results
- Trajectory generation outperforms RGB-based video generators on physical plausibility metrics while being more computationally efficient
- The method shows superior performance in motion distributional metrics and user preference studies
- Dense grid prediction captures scene-wide dynamics better than sparse point regression approaches
- VAE regularization prevents mode collapse in synthetic data, though ablation shows mixed results on real-world Physics101 scenarios

## Why This Works (Mechanism)

### Mechanism 1: Modality-Induced Physical Bias
Generating point trajectories instead of RGB pixels appears to enforce physical inductive biases—specifically object permanence and temporal coherence—more effectively than video generation. By changing the output modality to 2D coordinates, the optimization landscape is simplified, focusing capacity on physics rather than low-level appearance factors.

### Mechanism 2: Latent Trajectory Regularization
Compressing trajectories into a VAE latent space prevents the "single mode collapse" often observed in coordinate-based generation. The Flow Matching process operates on these latent codes, interpolating between futures rather than averaging them, which maintains diversity in predictions.

### Mechanism 3: Dense Spatio-Temporal Context Aggregation
Forecasting dense grids (rather than sparse "active" points) improves accuracy by capturing scene-wide interactions. This allows the model to capture "non-local" interactions where distant objects might collide later in time, which sparse predictors might ignore.

## Foundational Learning

**Rectified Flow / Flow Matching**
- **Why needed here:** This is the generative engine replacing standard diffusion, learning to transport noise to the target distribution via straight paths, which the paper claims is more efficient than standard DDPM.
- **Quick check question:** Can you explain why learning a straight path (ODE) from noise to data is sample-efficient compared to the iterative denoising of standard diffusion?

**β-VAE (Variational Autoencoder)**
- **Why needed here:** This creates the latent space where generation happens, balancing reconstruction fidelity against latent smoothness to prevent jitter.
- **Quick check question:** If the β term is too high, the latent space becomes too smooth; what would happen to the specific shape of a predicted falling object?

**Factorized Spatio-Temporal Attention**
- **Why needed here:** The architecture separates attention over time and attention over space to manage compute for handling long sequences.
- **Quick check question:** Does the model attend to all pixels across all frames simultaneously, or does it alternate between spatial and temporal attention?

## Architecture Onboarding

**Component map:** DINOv2 (frozen) -> Trajectory VAE (encoder/decoder) -> Latent DiT denoiser (with gated cross-attention) -> Decoder -> Output trajectories

**Critical path:** The Flow Matching Loss. The model predicts the velocity field connecting noise to latent trajectory, not the next frame directly.

**Design tradeoffs:**
- Grid Stride (s): Smaller stride increases density but explodes token count; defaults to every other pixel
- Latent vs. Pixel: Explicitly rejects pixel generation in favor of coordinates
- Patch size: VAE uses patch size 2 or 4 to compress the grid

**Failure signatures:**
- Rigidity Loss: High LRTL indicates the model is breaking object shapes (non-rigid motion on rigid bodies)
- Mode Collapse: Low scene sample variance indicates the model ignores conditioning and predicts the same future for every scene

**First 3 experiments:**
1. VAE Reconstruction Test: Train only the VAE to reconstruct ground truth trajectories; if this cannot achieve low error, the latent space is too compressed
2. Static Scene Sanity Check: Feed an image with no moving objects; the model should predict near-zero velocity for all points
3. Modality Ablation (Toy Task): Replicate the "Falling Blocks" experiment to confirm that a trajectory-based model converges faster than an RGB-based model on the same architecture

## Open Questions the Paper Calls Out

**Open Question 1:** How does performance and physical plausibility scale when trained on large-scale, in-the-wild video datasets compared to current synthetic and small-scale real-world data? The authors state they are "currently limited from achieving broader in-the-wild generalisation" and list "scaling up datasets" as a primary goal.

**Open Question 2:** What architectural modifications are necessary to improve spatial accuracy around object boundaries without increasing computational cost? The authors note that "lower conditioning image resolution and further downsampling reduce the accuracy of predicted tracks around object boundaries."

**Open Question 3:** Is the "overhead of generating pixels" a fundamental limitation preventing video generators from acting as accurate world models, or can architectural improvements allow RGB-based models to match trajectory generators? The paper concludes that video generator failure "arises from the overhead of generating pixels rather than directly modeling motion," yet also shows that fine-tuning video models improves them.

**Open Question 4:** Under what conditions is the VAE latent space representation preferable to raw trajectory generation, given mixed results where the VAE-less model performed better on certain real-world Physics101 scenarios? In ablation studies, the model without a VAE performed better on 3 out of 5 Physics101 scenarios.

## Limitations
- The core mechanism of modality-induced physical bias is compelling but primarily supported by qualitative efficiency claims rather than direct causal evidence
- The VAE regularization's effectiveness is demonstrated empirically but lacks theoretical analysis of why the latent space is better suited for Flow Matching
- All datasets involve controlled physical interactions, leaving the generalization to complex, multi-object scenarios with significant occlusion unproven

## Confidence
- **High Confidence:** Core technical implementation (VAE + Flow Matching pipeline, training procedures, evaluation metrics) is well-documented and reproducible
- **Medium Confidence:** Proposed mechanisms explaining why trajectory generation works better than RGB generation are plausible but lack direct causal evidence
- **Low Confidence:** Generalization of results to real-world, unconstrained environments due to reliance on controlled physical interaction datasets

## Next Checks
1. **Causal Efficiency Experiment:** Compare model capacity usage and training convergence speed between coordinate-based and pixel-based architectures on the same motion forecasting task to directly measure the "overhead" claim.
2. **Latent Space Analysis:** Visualize the VAE latent space distribution across different physical scenarios to verify it captures meaningful motion modes rather than just compressing coordinates.
3. **Occlusion Robustness Test:** Evaluate the model on sequences with significant object occlusion or camera motion to determine whether the dense grid approach maintains accuracy when global context becomes misleading.