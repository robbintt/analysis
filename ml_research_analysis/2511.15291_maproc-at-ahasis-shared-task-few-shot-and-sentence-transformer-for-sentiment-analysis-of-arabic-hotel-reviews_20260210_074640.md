---
ver: rpa2
title: 'MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment
  Analysis of Arabic Hotel Reviews'
arxiv_id: '2511.15291'
source_url: https://arxiv.org/abs/2511.15291
tags:
- arabic
- sentiment
- training
- sentence
- dialects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sentiment analysis of Arabic dialects in hotel
  reviews, which presents challenges due to linguistic diversity and data scarcity.
  The proposed solution employs SetFit (Sentence Transformer Fine-tuning), a few-shot
  learning method that fine-tunes sentence transformers using contrastive learning
  and trains a simple classification head.
---

# MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews

## Quick Facts
- arXiv ID: 2511.15291
- Source URL: https://arxiv.org/abs/2511.15291
- Reference count: 8
- Achieved 73% F1 score on official evaluation set using only 64 examples per class

## Executive Summary
This paper addresses sentiment analysis of Arabic dialects in hotel reviews, tackling challenges from linguistic diversity and data scarcity. The proposed solution employs SetFit (Sentence Transformer Fine-tuning), a few-shot learning method that fine-tunes sentence transformers using contrastive learning and trains a simple classification head. Experiments on the AHaSIS dataset containing Moroccan and Saudi dialect reviews achieved competitive results with minimal training data, ranking 12th among 26 participants. The approach demonstrates that efficient few-shot learning can handle nuanced dialectal Arabic text in specialized domains.

## Method Summary
The method employs SetFit framework with Arabic-SBERT-100K as the sentence transformer model. The approach uses supervised contrastive learning to adapt pre-trained embeddings to the sentiment classification task, generating positive and negative pairs from limited training data. The process involves two stages: first fine-tuning the sentence transformer with contrastive objective for 3 epochs, then training a simple classification head (logistic regression) on frozen embeddings. The model uses only 64 examples per class (192 total) from the AHaSIS dataset, demonstrating few-shot efficiency while handling both Moroccan and Saudi Arabic dialects in hotel review sentiment analysis.

## Key Results
- Achieved 73% F1 score on official AHaSIS evaluation set
- Demonstrated strong performance with only 64 examples per class (192 total)
- Arabic-SBERT-100K outperformed ArabicBERT_Finetuned-AR-500 by ~15 percentage points
- Showed consistent improvement with increased few-shot samples up to 64 per class

## Why This Works (Mechanism)

### Mechanism 1: Supervised Contrastive Learning for Embedding Adaptation
Contrastive learning adapts pre-trained sentence embeddings to the target task by clustering same-class sentences and separating different-class sentences in embedding space. SetFit generates positive pairs (same label) and negative pairs (different labels) from limited training data, minimizing distance between same-label embeddings while maximizing distance between different-label embeddings using CosineSimilarityLoss. Core assumption: pre-trained Arabic-SBERT-100K encodes meaningful semantic representations that can be refined with minimal task-specific signal.

### Mechanism 2: Two-Stage Decoupled Training
Separating embedding fine-tuning from classification head training enables data-efficient transfer learning. Stage 1 fine-tunes the sentence transformer with contrastive objective (3 epochs). Stage 2 freezes embeddings and trains a simple classification head (logistic regression) on the resulting vectors. Core assumption: embeddings learned via contrastive learning are sufficiently task-aligned that a linear classifier can separate classes effectively.

### Mechanism 3: Sample Scaling Within Few-Shot Regime
Increasing few-shot samples per class improves performance up to a point, even within the few-shot regime. More samples provide richer contrastive pairs during fine-tuning, enabling better representation of class distributions and decision boundaries. Core assumption: training data quality is sufficient; additional samples capture meaningful variation rather than noise.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Understanding how positive/negative pair construction drives embedding space organization is essential for debugging representation quality.
  - Quick check question: Given three sentences with labels [positive, positive, negative], which pairs are positive pairs for contrastive learning?

- Concept: Sentence Transformers vs. Standard BERT
  - Why needed here: The architecture depends on pre-trained sentence embeddings, not token-level BERT outputs. Misunderstanding this leads to wrong model selection.
  - Quick check question: What is the output dimension of Arabic-SBERT-100K for a single input sentence, and how does it differ from AraBERT's output?

- Concept: Few-Shot Learning Constraints
  - Why needed here: The method's viability hinges on achieving acceptable performance with limited labeled data; understanding this regime prevents misaligned expectations.
  - Quick check question: If you have 860 labeled samples, why might you choose to use only 192 (64 per class) for training?

## Architecture Onboarding

- Component map:
  Input: Raw Arabic hotel review text → Minimal preprocessing (punctuation removal, letter normalization) → Encoder: Arabic-SBERT-100K (sentence transformer) → Contrastive Fine-tuning: 3 epochs, batch size 16, 20 iterations for pair generation, CosineSimilarityLoss → Classification Head: Logistic regression on frozen 768-dim embeddings → Output: Three-class sentiment (positive/negative/neutral)

- Critical path:
  1. Preprocessing: Normalize Arabic letters (ا, أ, إ) and remove punctuation
  2. Sample selection: Ensure balanced class sampling for few-shot subset
  3. Pair generation: Verify 20 iterations produce sufficient positive/negative pairs
  4. Epoch control: 3 epochs observed as optimal; more epochs showed degradation in early experiments

- Design tradeoffs:
  - Arabic-SBERT-100K vs. ArabicBERT_Finetuned-AR-500: Paper reports 67.48% vs. 52.83% F1 with 8 samples/3 epochs—base model choice significantly impacts performance
  - Samples vs. training time: 64 samples/class required ~2h 16m; 8 samples/class required ~15m
  - Epochs: 3 epochs outperformed 1 and 5 epochs at 8 samples, suggesting overfitting risk with more epochs at low data

- Failure signatures:
  - Dialect imbalance: Table 6 shows Darija neutral class at 61.3% F1 vs. Saudi neutral at 80%—likely underrepresentation in pre-training data
  - Overfitting: Performance degradation at 5 epochs with 8 samples suggests contrastive phase memorizes limited pairs
  - Base model mismatch: ArabicBERT_Finetuned-AR-500 underperformed by ~15 percentage points

- First 3 experiments:
  1. Replicate baseline: Fine-tune Arabic-SBERT-100K with 64 samples/class, 3 epochs, measure F1 on held-out set; verify ~73-79% range
  2. Ablate sample count: Test 8, 16, 32, 64 samples/class with fixed 3 epochs; plot learning curve to identify saturation point
  3. Cross-dialect evaluation: Train on Saudi-only subset, evaluate on Darija, and vice versa; quantify dialect transfer gap to diagnose representation limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or annotation inconsistencies cause the significant performance gap in neutral sentiment detection for Moroccan Darija compared to Saudi dialects?
- Basis in paper: [inferred] The results show the model struggles with Darija neutral class (61.3% F1), while Saudi neutral is 80%, yet the paper offers no linguistic explanation for this disparity.
- Why unresolved: The authors acknowledge the need for analysis but provide no qualitative breakdown of the error types or specific morphological challenges unique to Darija neutrality.
- What evidence would resolve it: A detailed error analysis correlating false negatives with specific Darija syntactic structures or lexical choices.

### Open Question 2
- Question: How does the performance of Arabic-specific sentence transformers compare to multilingual models when using SetFit for dialectal sentiment analysis?
- Basis in paper: [explicit] The conclusion states: "Future work could explore... experimenting with a wider range of pre-trained Arabic or multilingual sentence transformers."
- Why unresolved: The study only tested two Arabic-specific models (Arabic-SBERT-100K and ArabicBERT_Finetuned-AR-500), leaving the potential transfer learning capabilities of massive multilingual models unexplored.
- What evidence would resolve it: Benchmarking multilingual models (e.g., mBERT, XLM-R) within the same SetFit configuration on the AHaSIS dataset.

### Open Question 3
- Question: Does increasing the few-shot sample size beyond 64 examples per class yield diminishing returns or approach the performance of the fully fine-tuned AraBERT baseline?
- Basis in paper: [inferred] The authors note a positive correlation between sample size and performance up to 64 samples but did not test the "Full training dataset" (860 samples) within the SetFit framework to find the saturation point.
- Why unresolved: It is unclear if SetFit's efficiency is strictly a low-data phenomenon or if it can compete with the baseline (75.07%) when given access to the full training set.
- What evidence would resolve it: A curve plotting SetFit performance from 64 to 860 samples compared against the baseline.

## Limitations
- Limited to single dataset (AHASIS) with specific dialectal distribution
- Significant performance gap in Darija neutral class (61.3% vs 80% Saudi neutral) unexplained
- 73% official evaluation score appears inconsistent with higher internal test performance (78.87%)

## Confidence

**High Confidence** (Empirical support, minimal assumptions):
- SetFit framework can achieve sentiment classification on Arabic dialects
- Few-shot learning with 64 samples per class is viable for this task
- Contrastive fine-tuning + classification head architecture works as described

**Medium Confidence** (Logical but under-specified):
- Contrastive learning specifically adapts embeddings to dialectal variation
- 3 epochs represents optimal training duration across all sample sizes
- Performance gap between dialects stems from pre-training data imbalance

**Low Confidence** (Speculative or weakly supported):
- The 73% official evaluation score accurately reflects model capability
- Sample scaling benefits will persist beyond 64 samples per class
- The approach generalizes to other Arabic dialect classification tasks

## Next Checks

1. **Replication with multiple seeds**: Run the 64 samples/class configuration with 3 epochs using 5 different random seeds for sample selection. Report mean and standard deviation of F1 scores on both internal and official evaluation sets to quantify variance and verify if 73% is reproducible.

2. **Dialect transfer experiment**: Train the model on Saudi dialect only (216 samples), evaluate on Darija test set, and vice versa. Compare performance to bilingual training to quantify how much dialect-specific adaptation is occurring versus general sentiment learning.

3. **Base model ablation**: Replace Arabic-SBERT-100K with ArabicBERT_Finetuned-AR-500 using identical SetFit configuration (64 samples/class, 3 epochs). Measure performance difference to isolate whether the 15-point gap stems from pre-training data, architecture, or fine-tuning dynamics.