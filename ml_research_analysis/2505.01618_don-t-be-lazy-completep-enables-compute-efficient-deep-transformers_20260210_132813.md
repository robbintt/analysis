---
ver: rpa2
title: 'Don''t be lazy: CompleteP enables compute-efficient deep transformers'
arxiv_id: '2505.01618'
source_url: https://arxiv.org/abs/2505.01618
tags:
- arxiv
- depth
- base
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the compute efficiency of LLM training under
  different parameterizations, which are rules for scaling model and optimizer hyperparameters
  with model size. Some parameterizations fail to transfer optimal hyperparameters
  like learning rate across changes in model depth, requiring expensive re-tuning
  or resulting in sub-optimal training.
---

# Don't be lazy: CompleteP enables compute-efficient deep transformers

## Quick Facts
- arXiv ID: 2505.01618
- Source URL: https://arxiv.org/abs/2505.01618
- Reference count: 40
- This paper studies the compute efficiency of LLM training under different parameterizations, which are rules for scaling model and optimizer hyperparameters with model size.

## Executive Summary
This paper studies the compute efficiency of LLM training under different parameterizations, which are rules for scaling model and optimizer hyperparameters with model size. Some parameterizations fail to transfer optimal hyperparameters like learning rate across changes in model depth, requiring expensive re-tuning or resulting in sub-optimal training. Even when hyperparameter transfer is achieved, parameterizations may still cause "lazy learning" where layers only learn features close to their linearization, preventing effective use of depth and nonlinearity. The authors identify and adopt a parameterization called CompleteP that achieves both depth-wise hyperparameter transfer and non-lazy learning in all layers. CompleteP enables a wider range of width-to-depth ratios to remain compute-efficient, unlocking model shapes better suited for different hardware settings. It also enables 12-34% compute efficiency improvements over the prior state-of-the-art, with experiments run on Cerebras CS-3 systems. The paper provides theoretical justification for why CompleteP works and demonstrates its effectiveness through extensive experiments on models ranging from 75M to 1.9B parameters.

## Method Summary
The authors propose CompleteP, a parameterization that extends maximal update parameterization (μP) to depth. CompleteP scales residual branch outputs by L^(-1), enables hyperparameter transfer across depths, and prevents lazy learning. The method involves scaling residual branch outputs, weight initialization, learning rates, biases, LayerNorm parameters, AdamW epsilon, and weight decay with specific depth and width scalings. The authors validate CompleteP through extensive experiments, showing it achieves 12-34% FLOP savings compared to μP and 25-40% compared to standard parameterization (SP).

## Key Results
- CompleteP achieves depth-wise hyperparameter transfer, enabling stable training across varying depths without re-tuning.
- CompleteP prevents lazy learning, ensuring all layers utilize nonlinearity effectively for better depth utilization.
- CompleteP enables 12-34% FLOP savings compared to μP, with deeper, narrower models showing the most significant improvements.
- Compute-optimal models under CompleteP are deeper and narrower than under μP, better suited for hardware with weight streaming capabilities.

## Why This Works (Mechanism)

### Mechanism 1: Depth-wise Hyperparameter Transfer via α=1 Residual Scaling
- Claim: Setting the residual branch scaling to L^(-1) (α=1) enables learning rates and initialization variances to transfer across model depths without re-tuning.
- Mechanism: The residual update h_ℓ+1 = h_ℓ + L^(-α)·F_ℓ(h_ℓ) requires learning rate η ∝ L^(α-1) to maintain Θ(1/L) contribution per residual block (Desideratum 2). Only α=1 makes η independent of depth, enabling HP transfer.
- Core assumption: Optimal hyperparameters for shallow models should generalize to deeper models when dynamics are properly normalized.
- Evidence anchors:
  - [abstract] "CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers"
  - [section 4, Finding 2] "Only CompleteP (α=1) achieves reliable depth-wise HP transfer"
  - [corpus] Related work (Mlodozeniec et al., "Completed Hyperparameter Transfer") independently validates extended HP transfer, supporting the general direction but not this specific α mechanism.
- Break condition: If your optimizer is not AdamW-like (e.g., vanilla SGD), learning rate scaling changes (see Appendix D.1, category 2).

### Mechanism 2: Complete Feature Learning Avoids Lazy Regime
- Claim: α=1 is the unique value ensuring all layers remain non-linear with respect to parameters as N,L→∞; α<1 asymptotically linearizes residual blocks.
- Mechanism: Taylor expansion of layer updates reveals second-order terms scale as L^(α-2). For α<1, these vanish relative to linear terms (∝L^(-1)), pushing learning toward the lazy/NTK regime. For α=1, linear and nonlinear terms are both Θ(L^(-1)), preserving feature learning.
- Core assumption: Non-linear feature learning across all layers is desirable for effective depth utilization.
- Evidence anchors:
  - [section 6] "Only α=1 ensures complete feature learning... we dub it CompleteP"
  - [Figure 6] Shows α=0.5 converges to linearization at rate 1/√L; α=1 maintains distance
  - [corpus] Weak/absent direct corpus validation of the laziness mechanism specifically for transformers.
- Break condition: If your architecture already has strong nonlinear normalization (e.g., certain specialized ResNet variants), the lazy regime may be less relevant, though this remains untested in this paper.

### Mechanism 3: Extended Parameterization for Stability (LayerNorm, Bias, AdamW ε)
- Claim: CompleteP requires depth-aware scaling of LayerNorm LR, bias LR, AdamW ε, and weight decay to maintain stable training, especially for α=0.5.
- Mechanism: Bias and LayerNorm parameters must contribute Θ(1/L) updates to residual stream (like weights). This requires η_bias, η_LN ∝ L^(α-1). AdamW ε must match gradient scale: ε ∝ L^(-α)·N^(-1) for residual blocks.
- Core assumption: Modern pre-LN transformers with AdamW are the target architecture.
- Evidence anchors:
  - [Table 1] Complete prescriptions for all parameter scalings
  - [Figure 7] α=0.5 fails stability without bias/LN scaling; succeeds with it
  - [section D.5, Figure 8] AdamW ε scaling enables wider stable range
- Break condition: Architectures without LayerNorm (e.g., some post-LN variants) or with different normalization schemes require modified prescriptions (Appendix D.1, category 1).

## Foundational Learning

- **Maximal Update Parameterization (μP)**: Rules for scaling hyperparameters with width so feature learning dynamics remain consistent across model sizes.
  - Why needed here: CompleteP extends μP's width-wise transfer to depth. You must understand μP's core idea (maintaining Θ(1) update scales) to grasp why α=1 matters.
  - Quick check question: Can you explain why μP scales learning rates as η ∝ N^(-1) for hidden layers?

- **Residual Network Signal Propagation**: How activations evolve through residual connections; the tradeoff between residual branch contributions and main path.
  - Why needed here: The paper's core modification is to the residual branch scaling (L^(-α)). Understanding why α≥0.5 is required for stability is essential.
  - Quick check question: Why does α<0.5 cause activation variance to explode as L→∞?

- **Lazy Learning / Neural Tangent Kernel Regime**: A training regime where the network learns only features close to its linearization; limits the effective use of depth and nonlinearity.
  - Why needed here: The paper's key theoretical contribution (Desideratum 3) distinguishes α=1 as uniquely avoiding lazy learning.
  - Quick check question: Why might lazy learning be undesirable when scaling depth?

## Architecture Onboarding

- **Component map**:
  - Residual branch: Scale outputs by mL^(-1) = (L/L_base)^(-1) for both MHA and MLP blocks
  - Hidden layer weights: LR ∝ N^(-1)·L^(0) (no depth correction for α=1)
  - Hidden layer biases: LR ∝ L^(0) = 1 (same as base)
  - LayerNorm (gain/bias): LR ∝ L^(0) = 1
  - AdamW ε (residual blocks): ε ∝ N^(-1)·L^(-1)
  - Weight decay: λ ∝ N (no depth correction)
  - Unembedding output: Scale by N^(-1)

- **Critical path**:
  1. Implement residual branch scaling (Table 1, "MHA/MLP Residual" rows)
  2. Adjust hidden layer learning rates (mL^(α-1) = 1 for α=1)
  3. Scale AdamW ε per layer type
  4. Verify with coordinate check (Figure 7): activation norms should remain stable across depths

- **Design tradeoffs**:
  - Deeper models (low N:L): Better FLOP savings with CompleteP (up to 34% vs μP) but higher latency
  - Shallower models (high N:L): More conventional, latency-friendly; CompleteP advantage smaller (~12%)
  - Hardware considerations: Low-memory devices benefit from deep-narrow shapes (N:L≈10-20) with weight streaming

- **Failure signatures**:
  - Loss divergence at large depth → Check residual scaling factor; verify α=1, not α=0.5 without bias/LN corrections
  - HP sensitivity increases with depth → Likely using μP or SP instead of CompleteP
  - Deeper models underperform shallower at same FLOPs → HP transfer failed; verify base model tuning

- **First 3 experiments**:
  1. **Coordinate check validation**: Train models at depths {2, 8, 32, 128} for ~82K tokens; plot ||h_L,t - h_L,1||_2. Should show stable activation changes across depths (replicate Figure 7).
  2. **HP transfer sweep**: Grid search η_base ∈ [2^(-11), 2^(-5)] at depths {2, 8, 32, 128} with fixed tokens. Verify optimal η_base is stable for α=1 but shifts for alternatives (replicate Figure 2).
  3. **Shape efficiency test**: Train models at fixed P_non-emb (e.g., 50M) with N:L ranging from ~4 to ~700. Fit scaling law and identify compute-optimal N:L range. Compare FLOP savings vs μP baseline (replicate Figure 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CompleteP maintain its compute-efficiency and hyperparameter transfer properties when scaling to state-of-the-art model sizes (e.g., >10B parameters)?
- Basis in paper: [explicit] Section 8 states, "In future work, we plan to train large, state-of-the-art LLMs." Additionally, Section 7 notes that scaling law fits were limited to models up to 1.5B parameters due to budget constraints.
- Why unresolved: The empirical validation was restricted to 75M–1.9B parameter models, and the authors acknowledge that fits based on limited data points may make extrapolation unreliable.
- What evidence would resolve it: Successful application of CompleteP to train models exceeding 10B parameters, demonstrating consistent FLOP savings and stable hyperparameter transfer without retuning.

### Open Question 2
- Question: How does CompleteP theoretically extend to architectures utilizing Mixture of Experts (MoE) or specific optimizer variants like LAMB?
- Basis in paper: [explicit] Section D.1 explicitly lists "MoE" and "LAMB" under components requiring "more theoretical derivations" because they may alter the scale of updates relative to width and depth.
- Why unresolved: The current derivations assume dense transformers and standard AdamW updates; MoE introduces sparsity and routing dynamics not covered by the current theory.
- What evidence would resolve it: A theoretical derivation of parameterization rules for MoE layers and empirical validation showing HP transfer across varying numbers of experts.

### Open Question 3
- Question: What is the correct theoretical scaling for batch size and training duration when simultaneously scaling width and depth with CompleteP?
- Basis in paper: [explicit] Section D.1 lists "batch size (and schedule)" as a scaling dimension requiring more theoretical work. [inferred] Section 7 notes the theoretical analysis assumes a fixed token count, limiting direct applicability to compute-optimal regimes where tokens scale with parameters.
- Why unresolved: The current theory focuses on model scaling at fixed data, whereas modern training requires joint scaling of batch size, data, and model size.
- What evidence would resolve it: A derivation of the functional form for batch size scaling $B(N, L)$ and empirical verification that optimal batch size transfers across depth under CompleteP.

## Limitations
- **Limited to 75M–1.9B parameters**: The empirical validation was restricted to models up to 1.9B parameters, leaving questions about scalability to state-of-the-art sizes.
- **Theoretical gaps in extensions**: The paper acknowledges that architectures like MoE, optimizers like LAMB, and joint scaling of batch size require further theoretical work.
- **Fixed token assumption**: The theoretical analysis assumes a fixed token count, which may not directly apply to compute-optimal regimes where tokens scale with parameters.

## Confidence
- **Mechanism 1 (Depth-wise HP transfer)**: High - Directly validated in Figure 2 and supported by the core theoretical framework.
- **Mechanism 2 (Non-lazy learning)**: Medium - Theoretical derivation is clear, but empirical validation is limited to specific metrics (Figure 6).
- **Mechanism 3 (Extended parameterization)**: High - Explicitly required for stability in Figure 7 and detailed in Table 1.
- **Overall empirical claims**: High - Extensive experiments across multiple parameterizations and model sizes support the main findings.

## Next Checks
1. **Coordinate check validation**: Train models at depths {2, 8, 32, 128} for ~82K tokens; plot ||h_L,t - h_L,1||_2. Should show stable activation changes across depths (replicate Figure 7).
2. **HP transfer sweep**: Grid search η_base ∈ [2^(-11), 2^(-5)] at depths {2, 8, 32, 128} with fixed tokens. Verify optimal η_base is stable for α=1 but shifts for alternatives (replicate Figure 2).
3. **Shape efficiency test**: Train models at fixed P_non-emb (e.g., 50M) with N:L ranging from ~4 to ~700. Fit scaling law and identify compute-optimal N:L range. Compare FLOP savings vs μP baseline (replicate Figure 4).