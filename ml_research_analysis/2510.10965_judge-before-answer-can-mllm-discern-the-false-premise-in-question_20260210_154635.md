---
ver: rpa2
title: 'Judge Before Answer: Can MLLM Discern the False Premise in Question?'
arxiv_id: '2510.10965'
source_url: https://arxiv.org/abs/2510.10965
tags:
- premise
- 'false'
- premises
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of multimodal large language
  models (MLLMs) in recognizing false premises within questions. The authors identify
  two key gaps: the lack of comprehensive benchmarks for evaluating false premise
  detection and the absence of targeted training frameworks to enhance this capability.'
---

# Judge Before Answer: Can MLLM Discern the False Premise in Question?

## Quick Facts
- arXiv ID: 2510.10965
- Source URL: https://arxiv.org/abs/2510.10965
- Authors: Jidong Li; Lingyong Fang; Haodong Zhao; Sufeng Duan; Gongshen Liu
- Reference count: 0
- One-line primary result: MLLMs struggle with false premise detection, but JBA-GRPO training significantly improves performance.

## Executive Summary
This paper addresses the challenge of multimodal large language models (MLLMs) in recognizing false premises within questions. The authors identify two key gaps: the lack of comprehensive benchmarks for evaluating false premise detection and the absence of targeted training frameworks to enhance this capability. To address these issues, they introduce JBA, a fully automated pipeline for constructing a large-scale dataset that systematically categorizes false premises into three levels and thirteen subtypes. Additionally, they propose JBA-GRPO, a reinforcement learning framework designed to improve MLLMs' ability to detect and reject false premises by incorporating a novel reasoning reward mechanism. Experiments demonstrate that existing MLLMs struggle with this task, while models trained with JBA-GRPO achieve significant improvements, highlighting the effectiveness of the proposed approach.

## Method Summary
The JBA framework consists of a two-stage training approach on Qwen2.5-VL-7B-Instruct. First, supervised fine-tuning (SFT) on a structured dataset with reasoning tags (<think>) that enforce premise verification before answer generation. Second, Group Relative Policy Optimization (GRPO) reinforcement learning with three rewards: format reward (correct tag structure), answer reward (correct refusal or response), and reasoning reward (LLM-evaluated logical coherence). The JBA dataset is constructed via an automated pipeline from Visual Genome images, involving visual premise extraction, premise-aware captioning, and target question generation to create positive and negative samples across 13 false premise subtypes.

## Key Results
- Existing MLLMs (LLaVA-v1.5-7B, Qwen2.5-VL-7B-Instruct) perform poorly on false premise detection, with low FPC and FPDP scores.
- JBA-GRPO significantly improves model performance, achieving high False Premise Coverage (FPC) and False Premise Detection Precision (FPDP).
- The reasoning reward mechanism is crucial, with ablation studies showing degraded performance when removed.
- Models trained with JBA-GRPO maintain reasonable True Premise Identification Rate (TPIR), though this remains lower than FPDP.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reasoning tags (<think>) enforce explicit premise verification before answer generation.
- Mechanism: The tag creates a mandatory pre-analysis phase where the model systematically compares visual content against the textual premise, making discrepancies detectable before committing to an answer. This separates verification from generation.
- Core assumption: Models can learn to meaningfully populate reasoning tags that genuinely reflect their verification process, not just perform pattern completion.
- Evidence anchors:
  - [abstract] "incorporating a novel reasoning reward mechanism"
  - [section 4.3] "The notable performance improvement of JBA can be attributed to... a structured reasoning mechanism by incorporating a <think> tag into the model's inference pipeline"
  - [corpus] Related work on premise verification (arXiv:2504.06438) similarly uses explicit reasoning chains, suggesting the approach generalizes.
- Break condition: If reasoning tags become superficial pattern-matching without genuine verification, the mechanism degrades to theater without substance.

### Mechanism 2
- Claim: Automated pipeline generates diverse false premise types that expose systematic model weaknesses.
- Mechanism: By decomposing premise generation into extraction → captioning → replacement, the pipeline systematically varies which cognitive ability (perceptual/cognitive/reasoning) the false premise targets, creating coverage across the 13-subtype taxonomy.
- Core assumption: The synthetic false premises genuinely test the same capabilities that natural false premises would require.
- Evidence anchors:
  - [abstract] "systematically categorizes the premises into three main types and thirteen subtypes"
  - [section 2.1.3] "Replace the original premise p with an incorrect or contradictory (but never identical) premise p′"
  - [corpus] MultiHoax dataset (arXiv:2506.00264) uses multi-hop false premises, suggesting premise complexity matters—JBA's hierarchical taxonomy may not capture all complexity dimensions.
- Break condition: If synthetic premises follow predictable patterns, models may learn surface cues rather than genuine verification.

### Mechanism 3
- Claim: Reasoning reward (evaluated by external LLM) enforces logical coherence between verification reasoning and final answer.
- Mechanism: An LLM evaluator scores the reasoning block on logical coherence, relevance, and support for the conclusion. This prevents reward hacking where models reach correct answers through flawed reasoning chains.
- Core assumption: The evaluator LLM reliably distinguishes coherent from incoherent reasoning, and its judgments transfer to improved verification behavior.
- Evidence anchors:
  - [section 3.2] "A model could potentially arrive at the correct answer through a flawed or coincidental chain of thought... we employ a large language model (LLM) as an evaluator"
  - [section 3.2] "The LLM evaluates aspects such as logical coherence, relevance to the question, and whether the step-by-step reasoning genuinely supports the final answer."
  - [corpus] No direct corpus evidence on LLM-as-reasoning-evaluator effectiveness; this remains an open question.
- Break condition: If evaluator LLM has systematic blind spots or biases, the reward signal misaligns with actual reasoning quality.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper's RL framework uses GRPO (from DeepSeek-R1), which standardizes rewards across groups of candidate outputs and uses KL divergence to prevent policy drift.
  - Quick check question: Can you explain why GRPO uses group-wise reward standardization rather than absolute rewards?

- Concept: **Multimodal grounding**
  - Why needed here: False premise detection requires verifying whether textual claims match visual evidence—this is fundamentally about vision-language alignment.
  - Quick check question: What would happen if a model could detect false premises in text-only settings but failed when the same premise required visual verification?

- Concept: **Reward design for reasoning**
  - Why needed here: The paper's key innovation is the reasoning reward. Understanding how to shape rewards for process (not just outcome) is critical.
  - Quick check question: Why might an answer reward alone be insufficient for training reliable premise detection?

## Architecture Onboarding

- Component map:
  [Visual Genome Images] → [Stage 1: Visual Premise Extraction] → [Stage 2: Premise-Aware Captioning] → [Stage 3: Target Question Generation] → [JBA Dataset] → [Training: SFT → GRPO with 3 rewards]

- Critical path: The reasoning reward evaluator is the single point of failure—if it produces noisy or biased judgments, the entire RL optimization can misalign.

- Design tradeoffs:
  - **Synthetic vs. human-annotated**: Automated pipeline scales but may miss edge cases humans would catch.
  - **13-subtype taxonomy**: Enables fine-grained analysis but introduces classification ambiguity at boundaries.
  - **Two-stage training (SFT→RL)**: SFT establishes reasoning structure; RL refines it. Skipping SFT risks unstable RL; skipping RL leaves potential unrealized.

- Failure signatures:
  - High FPDP but low TPIR: Model over-rejects, flagging valid premises as false.
  - High performance on synthetic data, poor on real-world: Pipeline artifacts, not genuine capability.
  - Reasoning tag contradicts final answer: Reward hacking; reasoning reward not working.

- First 3 experiments:
  1. **Baseline probe**: Run existing MLLMs (LLaVA-v1.5-7B, Qwen2.5-VL-7B-Instruct) on JBA dataset without any training to establish floor performance across all 13 subtypes.
  2. **Ablation on reasoning reward**: Train with format+answer rewards only vs. full 3-reward setup to isolate the reasoning reward's contribution.
  3. **Cross-type generalization**: Train on perceptual-level premises only, evaluate on cognitive/reasoning levels to test whether verification skill transfers across the taxonomy hierarchy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the JBA-GRPO framework's effectiveness transfer to MLLMs of different scales beyond the 7B parameter model tested?
- Basis in paper: [inferred] The experiments only report fine-tuning results on Qwen2.5-VL-7B-Instruct, leaving the scalability of the approach unexplored.
- Why unresolved: No experiments were conducted on larger (e.g., 72B) or smaller (e.g., 2B) model variants to determine if the framework's benefits are consistent across scales.
- What evidence would resolve it: Training and evaluation results across multiple model sizes using identical JBA-GRPO hyperparameters.

### Open Question 2
- Question: To what extent does the automated pipeline's reliance on Qwen2.5-VL-72B and Qwen3-32B introduce systematic blind spots in the generated dataset?
- Basis in paper: [inferred] The construction pipeline depends entirely on these models for premise extraction, captioning, and question generation, but their potential limitations in certain premise types could propagate to the benchmark.
- Why unresolved: No analysis examines whether the generating models' weaknesses create gaps in the 13-subtype taxonomy.
- What evidence would resolve it: Human validation of dataset coverage across all subtypes, or comparison with datasets generated using different foundation models.

### Open Question 3
- Question: Is there an inherent trade-off between False Premise Detection Precision (FPDP) and True Premise Identification Rate (TPIR) when optimizing for false premise recognition?
- Basis in paper: [inferred] Table 1 shows TPIR scores are consistently lower than FPDP scores across all evaluated models, suggesting models may become overly skeptical.
- Why unresolved: The paper does not investigate whether improved false premise detection comes at the cost of rejecting valid questions.
- What evidence would resolve it: A parametric study varying reward function weights between detection and acceptance, measuring both metrics simultaneously.

### Open Question 4
- Question: Does the reasoning reward mechanism, which uses an LLM evaluator, introduce circular dependencies or biases that limit generalization to out-of-distribution false premise types?
- Basis in paper: [explicit] The paper states "we employ a large language model (LLM) as an evaluator to assess the quality of the reasoning," but does not address whether this creates evaluator-specific biases.
- Why unresolved: The evaluator LLM's own blind spots may reinforce certain reasoning patterns while penalizing valid alternative approaches.
- What evidence would resolve it: Ablation studies using different evaluator LLMs, or human evaluation of reasoning quality compared to LLM-assigned rewards.

## Limitations
- The automated pipeline's reliance on specific foundation models (Qwen2.5-VL-72B, Qwen3-32B) may introduce systematic blind spots in the generated dataset.
- The reasoning reward mechanism depends entirely on the reliability of an LLM evaluator, which is not thoroughly validated.
- The hierarchical taxonomy of 13 subtypes may oversimplify the nuanced nature of premise verification tasks.

## Confidence
- **High confidence**: The structured reasoning mechanism with <think> tags provides clear benefit for enforcing premise verification before answer generation, as evidenced by improved performance metrics.
- **Medium confidence**: The automated pipeline successfully generates diverse false premise types, though synthetic data may not fully capture real-world complexity.
- **Low confidence**: The reasoning reward mechanism's effectiveness depends entirely on the evaluator LLM's reliability, which is not thoroughly validated in the paper.

## Next Checks
1. **Cross-architecture validation**: Test JBA-GRPO on multiple MLLM architectures (not just Qwen2.5-VL-7B) to assess generalizability.
2. **Real-world transfer**: Evaluate trained models on human-annotated false premise datasets (like MultiHoax) to verify synthetic training translates to real-world performance.
3. **Evaluator reliability audit**: Conduct ablation studies removing the reasoning reward or using different evaluator LLMs to quantify its impact and potential biases.