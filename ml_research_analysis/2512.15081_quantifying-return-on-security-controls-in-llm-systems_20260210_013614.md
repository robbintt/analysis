---
ver: rpa2
title: Quantifying Return on Security Controls in LLM Systems
arxiv_id: '2512.15081'
source_url: https://arxiv.org/abs/2512.15081
tags:
- control
- loss
- injection
- language
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying return on investment
  for security controls in large language model (LLM) systems by introducing a decision-oriented
  framework that converts adversarial probe outcomes into financial risk estimates.
  The core method idea combines automated adversarial testing using Garak with Laplace's
  Rule of Succession to estimate attack success probabilities, then uses Monte Carlo
  simulation with calibrated loss distributions to produce expected loss and loss
  exceedance curves for different vulnerability classes.
---

# Quantifying Return on Security Controls in LLM Systems

## Quick Facts
- arXiv ID: 2512.15081
- Source URL: https://arxiv.org/abs/2512.15081
- Reference count: 40
- Primary result: ABAC reduces total simulated expected loss by ~94% with RoC of 9.83

## Executive Summary
This paper introduces a decision-oriented framework for quantifying return on investment for security controls in LLM systems. The approach converts adversarial probe outcomes into financial risk estimates by combining Laplace's Rule of Succession with Monte Carlo simulation and calibrated loss distributions. The methodology is demonstrated on three common controls—ABAC, NER redaction, and NeMo Guardrails—showing that upstream controls achieve substantially higher risk reduction than downstream filtering approaches.

## Method Summary
The framework integrates automated adversarial testing using Garak probes with Laplace's Rule of Succession to estimate attack success probabilities, then combines these with Monte Carlo simulation using triangle loss distributions calibrated from IBM breach data. The system under test is a RAG pipeline built on DeepSeek-R1 Distill Qwen 1.5B with a FAISS vector store containing 100,000 synthetic PII records. Five probe families target PII leakage, latent injection, prompt injection, attack generation, and divergence. Expected loss is computed across 10,000 simulation runs, and Return on Control is calculated using an assumed $30,000 implementation cost for all controls.

## Key Results
- ABAC reduces total simulated expected loss by ~94% with RoC of 9.83
- NER redaction achieves 57% reduction with RoC of 5.97
- NeMo Guardrails provides only marginal benefit with RoC of 0.05
- Three controls reduce total expected loss from $313,000 to between $17,000-$131,000

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ABAC achieves higher risk reduction by controlling information flow upstream of model inference.
- **Mechanism:** Attribute-based policies deny retrieval of sensitive documents before they enter the generation pipeline. By restricting which documents the model can access based on request attributes, the control prevents harmful context from being incorporated into reasoning, collapsing attack success probabilities for PII leakage (0.980→0.019) and prompt injection (0.998→0.001).
- **Core assumption:** Access policies can reliably classify which documents contain sensitive or manipulable content before retrieval.
- **Evidence anchors:**
  - [abstract] "ABAC collapses success probabilities for PII and prompt-related attacks to near zero and reduces the total expected loss by ~94%, achieving an RoC of 9.83."
  - [Section IV-B] "ABAC eliminates all observed failures for PII leakage (50 to 0) and prompt injection (500 to 0), and dramatically reduces latent-injection failures from 160 to 18."
  - [corpus] Related work on prompt injection (arXiv:2306.05499) demonstrates that controlling input context is a recognized defense strategy, though corpus lacks direct ABAC comparisons.

### Mechanism 2
- **Claim:** NER redaction provides targeted protection against PII leakage but leaves other vulnerability classes unaffected.
- **Mechanism:** Named Entity Recognition scans retrieved documents and replaces sensitive entities (names, SSNs) before content reaches the model. This removes PII from the context window but does not alter prompts, injection payloads, or model reasoning—leaving latent injection, prompt injection, and divergence success rates unchanged.
- **Core assumption:** NER accurately detects all entity types of concern, and redaction does not introduce new vulnerabilities through incomplete masking.
- **Evidence anchors:**
  - [abstract] "NER redaction likewise eliminates PII leakage and attains an RoC of 5.97."
  - [Section IV-C] "NER eliminates all observed PII leakage, reducing the number of failures from 50 to 0... In contrast, the control leaves divergence, latent injection, and prompt injection unchanged."
  - [corpus] LeakSealer (arXiv:2508.00602) addresses both prompt injection and data leakage, suggesting NER alone is insufficient for comprehensive defense.

### Mechanism 3
- **Claim:** Combining Laplace's Rule of Succession with Monte Carlo simulation converts sparse probe outcomes into actionable financial risk estimates.
- **Mechanism:** Laplace's Rule (P = (s+1)/(n+2)) transforms binary probe results into conservative probability estimates that avoid zero-confidence issues. These probabilities gate whether a loss is sampled from vulnerability-specific triangle distributions (calibrated from IBM breach data) across 10,000 Monte Carlo trials, producing loss exceedance curves and expected loss values.
- **Core assumption:** Probe success rates generalize to real attack conditions, and triangle distributions reasonably approximate actual breach costs.
- **Evidence anchors:**
  - [abstract] "Attack success probabilities are estimated via Laplace's Rule of Succession and combined with loss triangle distributions... in 10,000-run Monte Carlo simulations to produce loss exceedance curves."
  - [Section III-B-5] "The Laplace Rule of Succession provides a conservative estimate of attack success probability in the presence of limited data."
  - [corpus] Limited corpus evidence on this specific methodology; closest is metric-driven security analysis (arXiv:2502.08610) addressing quantification gaps in AI standards.

## Foundational Learning

- **Concept:** Laplace's Rule of Succession
  - **Why needed here:** Converts limited binary trial data into probability estimates that avoid overconfident zeros. Essential for understanding how the paper derives attack success probabilities from probe results.
  - **Quick check question:** If 0 failures occur in 50 trials, what probability does Laplace's Rule produce, and why is this preferable to P=0?

- **Concept:** Loss Exceedance Curves
  - **Why needed here:** Visualizes tail risk by showing P(Loss > L) across loss magnitudes. Critical for interpreting the Monte Carlo outputs and understanding why some controls reduce tail risk while others don't.
  - **Quick check question:** What does it mean when a loss exceedance curve shifts downward across all probabilities versus shifting only at the tail?

- **Concept:** Retrieval-Augmented Generation (RAG) Architecture
  - **Why needed here:** The system under test is a RAG pipeline; understanding where controls integrate (retrieval vs. output stages) determines their effectiveness.
  - **Quick check question:** At which points in the RAG pipeline can security controls intercept harmful content, and how does control placement affect which vulnerability classes are addressed?

## Architecture Onboarding

- **Component map:** FastAPI endpoint -> FAISS vector store (100k synthetic PII records) -> DeepSeek-R1 Distill Qwen 1.5B model -> Security control layer (varies by scenario) -> Response

- **Critical path:** Request received -> Similarity threshold check -> Document retrieval (if threshold met) -> Control application (ABAC/NER/Guardrails) -> Context assembly -> Model inference -> Output filtering (if Guardrails) -> Response returned

- **Design tradeoffs:**
  - Upstream controls (ABAC, NER) prevent harmful content from reaching the model but may block legitimate queries
  - Downstream controls (Guardrails) preserve model access but cannot undo reasoning that incorporated malicious context
  - The paper assumes equal $30k implementation cost for all controls; real costs likely vary significantly

- **Failure signatures:**
  - Baseline: PII leakage 50/50, prompt injection 500/500, latent injection 160/160 (near-certain success)
  - ABAC failure pattern: Slight increases in divergence and attack generation, suggesting potential over-restriction
  - NER failure pattern: Only PII affected; other vulnerability rates identical to baseline
  - Guardrails failure pattern: All vulnerability rates nearly identical to baseline (RoC 0.05)

- **First 3 experiments:**
  1. Replicate baseline probing on your RAG system using Garak with the five probe families to establish your own Laplace-adjusted success probabilities
  2. Implement ABAC-style access restriction on your retrieval corpus and measure changes in attack success rates and simulated expected loss
  3. Calibrate triangle distributions using your organization's breach cost data rather than IBM averages, then re-run Monte Carlo simulations to produce context-specific RoC metrics

## Open Questions the Paper Calls Out

- **Open Question 1:** How do the effectiveness rankings and RoC values of security controls change when evaluated across different LLM architectures, scales, and training paradigms?
  - **Basis in paper:** [explicit] Future Work states: "Testing across models of varying scale, architecture, and training paradigms, including open-source and proprietary systems, would yield insights into how vulnerability profiles differ and which mitigations generalize effectively."
  - **Why unresolved:** Only DeepSeek R1 Distill Qwen 1.5B was tested; generalizability to other models remains unknown.
  - **What evidence would resolve it:** Replicating the methodology across diverse LLMs (e.g., GPT-4, LLaMA, Claude) and comparing resulting RoC metrics.

- **Open Question 2:** Can NeMo Guardrails achieve meaningful risk reduction with improved Colang policy configuration or deeper integration earlier in the RAG pipeline?
  - **Basis in paper:** [explicit] Authors state: "Further tuning, broader policy coverage, or deeper integration earlier in the RAG pipeline may be necessary for the control to produce meaningful reductions in model vulnerability."
  - **Why unresolved:** NeMo showed negligible benefit (RoC = 0.05) in one configuration only.
  - **What evidence would resolve it:** Systematic testing of varied NeMo configurations with different policy coverage and integration points.

- **Open Question 3:** How do security controls perform against additional failure modes beyond the five tested, such as multilingual leakage, model inversion, or bias amplification?
  - **Basis in paper:** [explicit] Future Work notes: "new detectors and probes could be developed to assess additional failure modes, such as multilingual leakage, model inversion, or bias amplification."
  - **Why unresolved:** Study limited to PII leakage, latent injection, prompt injection, attack generation, and divergence.
  - **What evidence would resolve it:** Developing additional probes targeting untested vulnerability classes and measuring control effectiveness.

## Limitations