---
ver: rpa2
title: Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using
  Generalizability Theory
arxiv_id: '2507.19980'
source_url: https://arxiv.org/abs/2507.19980
tags:
- raters
- human
- scoring
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used generalizability theory to compare the reliability
  of human and large language model (LLM) scoring on AP Chinese writing tasks. Human
  raters demonstrated higher generalizability coefficients (e.g., 0.81 vs.
---

# Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory

## Quick Facts
- **arXiv ID:** 2507.19980
- **Source URL:** https://arxiv.org/abs/2507.19980
- **Reference count:** 0
- **Primary result:** Human raters achieved higher generalizability coefficients (0.81) than LLM raters (0.71) in AP Chinese writing assessments, with composite scoring showing potential for hybrid scoring models.

## Executive Summary
This study employed generalizability theory to systematically compare the reliability of human and large language model (LLM) scoring on AP Chinese writing tasks. The research found that human raters consistently demonstrated superior reliability across both holistic and analytic scoring approaches compared to AI raters. Story narration tasks produced more reliable scores than email responses, particularly for AI scoring. The study revealed that composite scoring combining human and AI raters could improve reliability under certain conditions, suggesting hybrid scoring models may offer advantages for large-scale writing assessments.

## Method Summary
The study utilized generalizability theory to evaluate scoring reliability, comparing human raters against LLM scoring across AP Chinese writing tasks. Researchers examined multiple scoring conditions including holistic and analytic approaches, different task types (story narration versus email responses), and various rater combinations. The analysis incorporated domain-specific reliability measures and tested composite scoring models that combined human and AI raters. Generalizability coefficients were calculated to quantify reliability differences between human and AI scoring approaches.

## Key Results
- Human raters achieved higher generalizability coefficients (0.81) than AI raters (0.71) with two tasks and raters
- Story narration tasks yielded more reliable scores than email responses, especially for AI scoring
- Domain-specific reliability was highest for task completion scores
- Composite scoring combining human and AI raters improved reliability in certain conditions

## Why This Works (Mechanism)
The superior reliability of human raters stems from their ability to understand nuanced language use, cultural context, and subtle rhetorical elements that current LLMs struggle to consistently evaluate. Humans can apply holistic judgment across multiple dimensions simultaneously, while LLMs may exhibit systematic biases or inconsistencies in scoring. The improved reliability of composite scoring suggests that human and AI raters capture complementary aspects of writing quality, with humans providing more consistent evaluations of higher-order writing features.

## Foundational Learning
1. **Generalizability Theory**: Statistical framework for examining reliability across multiple facets (why needed: provides more comprehensive reliability analysis than classical test theory; quick check: examine variance components for different rater combinations)
2. **Holistic vs Analytic Scoring**: Holistic scoring evaluates overall writing quality while analytic scoring assesses specific dimensions (why needed: different scoring approaches yield different reliability profiles; quick check: compare generalizability coefficients across scoring types)
3. **Domain-Specific Reliability**: Reliability estimates for specific scoring dimensions rather than overall scores (why needed: identifies which aspects of writing are most reliably scored; quick check: analyze variance components by scoring domain)
4. **Composite Scoring Models**: Combining multiple rater types to improve overall reliability (why needed: leverages complementary strengths of different scoring approaches; quick check: evaluate reliability gains from different rater combinations)

## Architecture Onboarding
**Component Map:** Writing Tasks → Raters (Human/AI) → Scoring Models → Generalizability Analysis → Reliability Coefficients

**Critical Path:** Task administration → Rater scoring → Reliability analysis → Interpretation of generalizability coefficients

**Design Tradeoffs:** 
- Human scoring provides higher reliability but lower scalability and higher cost
- AI scoring offers scalability but lower consistency across task types
- Composite scoring balances reliability and efficiency but requires careful calibration

**Failure Signatures:**
- Low generalizability coefficients indicate poor reliability across facets
- Task-specific performance differences suggest model limitations for certain writing genres
- Inconsistent domain-specific reliability points to scoring dimension challenges

**3 First Experiments:**
1. Compare generalizability coefficients across multiple LLM models using identical prompts
2. Test cross-cultural generalizability by applying the scoring framework to different language assessments
3. Evaluate longitudinal consistency by implementing the same scoring protocol across multiple testing cycles

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on Chinese language writing tasks, limiting generalizability to other languages
- Relies on a single LLM model, not accounting for potential performance differences across AI systems
- Fixed task types (story narration and email responses) may not represent the full spectrum of writing assessment scenarios

## Confidence
- **High:** Human raters achieve consistently higher generalizability coefficients than AI raters
- **High:** Story narration tasks produce more reliable scores than email responses
- **Medium:** Generalizability of results to other languages and cultural contexts
- **Medium:** Performance consistency across different LLM models and prompting strategies

## Next Checks
1. Replicate the study with multiple LLM models and prompting strategies to assess model dependency
2. Test the findings across different languages and cultural contexts to evaluate cross-cultural generalizability
3. Examine long-term scoring consistency through longitudinal studies with the same raters and models over extended periods