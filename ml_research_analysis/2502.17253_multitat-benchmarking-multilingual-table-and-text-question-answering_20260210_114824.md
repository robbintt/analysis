---
ver: rpa2
title: 'MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering'
arxiv_id: '2502.17253'
source_url: https://arxiv.org/abs/2502.17253
tags:
- question
- english
- languages
- answer
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MULTITAT is the first multilingual benchmark for question answering
  over tables and text. The dataset is created by translating existing English TATQA
  data into 10 languages.
---

# MULTITAT: Benchmarking Multilingual Table-and-Text Question Answering

## Quick Facts
- **arXiv ID:** 2502.17253
- **Source URL:** https://arxiv.org/abs/2502.17253
- **Reference count:** 33
- **Primary result:** First multilingual benchmark for TATQA, showing 19.4% performance drop in non-English languages

## Executive Summary
MULTITAT introduces the first multilingual benchmark for question answering over tables and text (TATQA), created by translating English TATQA data into 10 languages. The benchmark reveals significant performance disparities between English and non-English languages, with a 19.4% average performance drop across all non-English languages. The authors develop OURS, a baseline that links relevant information from hybrid contexts and reasons in English, achieving 3.3 average improvement over other baselines and reducing the performance gap by 23.2%. Error analysis indicates that performance declines are primarily due to challenges in linking relevant information, applying formulas, and following instructions.

## Method Summary
The authors constructed MULTITAT by translating existing English TATQA data into 10 languages without native speaker validation. They developed OURS, a baseline approach that extracts relevant information from both tables and text, translates non-English questions to English for reasoning, and generates answers. The benchmark was evaluated using automated metrics across multiple language models, comparing performance between English and the 10 target languages to quantify cross-lingual transfer capabilities.

## Key Results
- Non-English TATQA performance drops by 19.4% compared to English
- OURS baseline improves performance by 3.3 on average over other baselines
- OURS reduces the performance gap between languages by 23.2%
- Performance decline primarily due to challenges in linking information, formula application, and instruction following

## Why This Works (Mechanism)
The benchmark works by providing a standardized multilingual evaluation framework that exposes language-specific challenges in table-and-text question answering. By translating English TATQA data into multiple languages, it reveals systematic performance differences that stem from cross-lingual transfer limitations, formula understanding, and instruction-following capabilities across linguistic contexts.

## Foundational Learning
- **Cross-lingual transfer learning** - why needed: to understand how knowledge transfers between languages; quick check: compare performance of multilingual vs monolingual models
- **Table-text reasoning** - why needed: core capability for answering questions combining structured and unstructured data; quick check: test on synthetic mixed-context questions
- **Formula application in QA** - why needed: many TATQA questions require arithmetic or logical operations; quick check: evaluate on formula-only questions
- **Instruction following** - why needed: TATQA questions often include specific requirements; quick check: measure compliance with explicit constraints
- **Translation quality assessment** - why needed: benchmark validity depends on translation fidelity; quick check: compare machine vs human translations
- **Performance gap analysis** - why needed: to identify specific failure modes across languages; quick check: error type distribution analysis

## Architecture Onboarding

**Component Map:** Input Questions -> Translation Layer -> English Reasoning Engine -> Answer Generation -> Output

**Critical Path:** Question processing → Information extraction → Formula application → Answer generation

**Design Tradeoffs:** Machine translation vs native data collection (speed vs quality), automated evaluation vs human validation (scalability vs accuracy), English reasoning vs multilingual reasoning (consistency vs language-specific optimization)

**Failure Signatures:** Performance drops when questions require precise numerical calculations, when instructions are complex or ambiguous, and when cultural/linguistic nuances affect interpretation

**First Experiments:** 1) Test OURS baseline on held-out English questions to establish upper bound; 2) Run ablation study removing formula components to isolate reasoning impact; 3) Compare performance across language families to identify systematic patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset constructed via translation without native speaker validation, potentially introducing artifacts
- Limited to 10 languages, restricting generalizability to other linguistic families
- Relies on automated metrics without human evaluation for answer correctness

## Confidence
- Performance gap findings: Medium confidence (well-supported but translation effects not isolated)
- OURS baseline effectiveness: Medium confidence (reproducible results but limited ablation analysis)
- Cross-lingual transfer conclusions: Medium confidence (methodologically sound but scope limited)

## Next Checks
1. Conduct human evaluation study with native speakers for each language to verify answer quality and identify translation artifacts
2. Perform ablation experiments comparing machine-translated versus human-translated versions of the same dataset to quantify translation impact
3. Expand evaluation to include languages from different linguistic families not represented in the current 10-language set to test generalizability of findings