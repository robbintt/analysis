---
ver: rpa2
title: Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic
  Training Data
arxiv_id: '2505.10551'
source_url: https://arxiv.org/abs/2505.10551
tags:
- data
- real
- feasible
- image
- infeasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether feasibility (real-world plausibility)
  of synthetic training images impacts classifier performance. The authors define
  feasible images as those with attributes that could exist in reality, and infeasible
  images as those with unrealistic attributes.
---

# Does Feasibility Matter? Understanding the Impact of Feasibility on Synthetic Training Data

## Quick Facts
- arXiv ID: 2505.10551
- Source URL: https://arxiv.org/abs/2505.10551
- Reference count: 40
- Key outcome: Feasibility of synthetic training images minimally impacts CLIP classifier performance, with differences typically less than 0.3% in top-1 accuracy.

## Executive Summary
This paper investigates whether the real-world plausibility of synthetic training images affects classifier performance. The authors introduce VariReal, a pipeline that minimally edits real images to include specified feasible or infeasible attributes across background, color, and texture categories. Experiments on three fine-grained datasets show that feasibility has minimal impact on LoRA-fine-tuned CLIP performance, challenging the assumption that only feasible synthetic data should be used for training. Background modifications consistently improve performance regardless of feasibility, while color and texture edits are less effective and behave similarly to out-of-distribution data.

## Method Summary
The paper proposes VariReal, a pipeline that edits real images with specified feasible or infeasible attributes using a combination of inpainting and ControlNet models. For background edits, SDXL Inpainting with Real Prior and dilated masks is used. For color and texture edits, a two-stage approach combines SDXL Inpainting with ControlNet using Canny edge conditioning and IP-Adapter. The method uses Stable Diffusion 2.1 to generate prior images, Grounding DINO plus SAM2 for object masks, and LLaVA-1.6-7B for automatic filtering. CLIP ViT-B/16 is fine-tuned using LoRA (rank 16) on both image and text encoders with AdamW optimizer and cosine annealing scheduler.

## Key Results
- Feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy
- Background modifications consistently improve performance regardless of feasibility, yielding +1.2% (feasible) and +0.6% (infeasible) gains in synthetic-only setting
- Mixing feasible and infeasible images in training sets does not significantly impact performance compared to using purely feasible or infeasible datasets
- Color and texture edits are less effective than background edits and behave similarly to out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1: Background Augmentation Improves Object-Centric Classification
- **Claim:** Background modifications improve classification performance regardless of feasibility because classification tasks are object-centric, relying primarily on foreground features.
- **Mechanism:** Altering backgrounds adds data diversity while preserving foreground class-relevant features. The model learns invariant representations of the object independent of context.
- **Core assumption:** Class-discriminative features reside in foreground, not background.
- **Evidence anchors:**
  - Table 1 shows background modifications yield +1.2% (feasible) and +0.6% (infeasible) gains in synthetic-only setting vs. real baseline.
  - Observation 2: "background modifications consistently improve performance, whereas synthetic foreground edits (color and texture) are less effective."

### Mechanism 2: CLIP Representations Are Robust to Real-World Infeasibility
- **Claim:** Feasibility minimally impacts CLIP fine-tuning because learned representations capture structural and semantic patterns, not real-world plausibility judgments.
- **Mechanism:** CLIP's vision-language alignment trained on internet-scale data learns object identity from visual patterns; it does not explicitly encode "real-world likelihood" as a supervised signal.
- **Core assumption:** CLIP's pre-training does not strongly couple visual features to real-world occurrence priors for fine-grained classes.
- **Evidence anchors:**
  - "feasibility minimally affects LoRA-fine-tuned CLIP performance, with mostly less than 0.3% difference in top-1 accuracy."
  - 78% of Δ₁ (feasible vs. infeasible gap) values remain within 0.3% in real+synthetic setting.

### Mechanism 3: Foreground Modifications Risk Class-Relevant Feature Disruption
- **Claim:** Color and texture edits are less effective than background edits because they modify foreground regions containing class-discriminative features.
- **Mechanism:** Changing an object's color or texture may conflict with class-defining visual properties, introducing noise rather than useful augmentation.
- **Core assumption:** Fine-grained classes have foreground attributes that are semantically tied to class identity.
- **Evidence anchors:**
  - Table 3: Color/texture edits have higher CLIP/DINO similarity to real images than background edits, yet Table 1 shows they perform worse.
  - Observation 4: "Classification tasks are object-centric: although foreground modifications align more closely with real data distributions, changing them may deviate from meaningful class-relevant features."

## Foundational Learning

- **Concept: CLIP Vision-Language Alignment**
  - **Why needed here:** The entire methodology fine-tunes a pre-trained CLIP classifier using LoRA; understanding how CLIP maps images and text to shared embeddings is essential for interpreting why feasibility doesn't impact performance.
  - **Quick check question:** Can you explain why CLIP encodes "a photo of a green dog" and "a photo of a brown dog" to similar semantic regions despite color differences?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The paper uses LoRA to fine-tune CLIP with limited data; understanding the decomposition h = W₀x + BAx is critical for grasping why synthetic data effects are observable despite minimal parameter updates.
  - **Quick check question:** Why does LoRA's low-rank constraint potentially limit the model's ability to overfit to infeasible attributes?

- **Concept: Out-of-Distribution (OOD) Data Effects**
  - **Why needed here:** The paper frames infeasible data as a form of OOD data and references prior work on OOD benefits at small scales; distinguishing covariance shift from semantic shift clarifies why infeasibility may not harm generalization.
  - **Quick check question:** Is a "green Yorkshire terrier" a semantic shift or a covariance shift relative to the real distribution? Why does this distinction matter for training?

## Architecture Onboarding

- **Component map:** Real image -> mask + Canny edge generation -> prompt (feasible/infeasible) -> prior image generation -> prior + mask + prompt -> inpainting + ControlNet editing -> MLLM filtering -> final synthetic dataset -> LoRA fine-tuning
- **Critical path:**
  1. Real image → mask + Canny edge generation
  2. Prompt (feasible/infeasible) → prior image generation
  3. Prior + mask + prompt → inpainting + ControlNet editing
  4. MLLM filtering → final synthetic dataset
  5. LoRA fine-tuning on synthetic-only or mixed real+synthetic
- **Design tradeoffs:**
  - **Inpainting vs. ControlNet:** Inpainting preserves realism but struggles with attribute changes; ControlNet preserves structure but can produce unnatural results. The paper combines both for foreground edits.
  - **Raw vs. Real Prior:** Real Prior (prior merged with original object region) maintains spatial coherence but limits attribute change magnitude.
  - **Mask dilation:** Prevents "floating object" artifacts but may bleed unwanted context into edits.
- **Failure signatures:**
  - **Floating objects:** Occurs without proper mask dilation in background edits.
  - **Shape distortion:** Inpainting alone can alter foreground geometry during color/texture edits.
  - **Unnatural results:** ControlNet alone yields less realistic textures/colors.
  - **Filter rejection:** MLLM may reject images where fine-grained textures are not clearly rendered.
- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune CLIP with LoRA on real-only data for Oxford Pets; confirm baseline accuracy matches Table 1.
  2. **Feasibility ablation:** Generate feasible vs. infeasible background edits using VariReal; train synthetic-only models and compare Δ₁ (should be <0.3% per paper claims).
  3. **Attribute comparison:** Repeat for color and texture edits; verify that background outperforms foreground modifications and that mixing feasible/infeasible yields Δ₂ within 0.2% of individual settings.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the feasibility of synthetic data impact performance when modifying structural or lighting attributes, as opposed to texture, color, or background? The conclusion states that due to resource constraints, only three attributes were explored, and "future work could extend to others, such as lighting."

- **Open Question 2:** Is the minimal impact of feasibility specific to CLIP-based models with frozen pre-trained weights, or does it hold for models trained from scratch? The methodology restricts evaluation to "LoRA-fine-tuned CLIP," relying on pre-existing robust features rather than training new feature extractors from random initialization.

- **Open Question 3:** Does the feasibility of synthetic data affect performance in dense prediction tasks like object detection or semantic segmentation? The paper focuses exclusively on "object-centric classification" and acknowledges the generation pipeline supports tasks like detection and segmentation, but does not evaluate them.

## Limitations

- Findings based on three fine-grained classification datasets with specific model architectures (CLIP + LoRA), may not generalize to other domains like medical imaging or object detection.
- Study does not investigate semantic shifts versus covariance shifts systematically, leaving unclear when feasibility might become critical.
- Automatic filtering approach using MLLM may introduce selection bias that isn't fully characterized.

## Confidence

- **High Confidence:** The empirical observation that background modifications consistently improve performance regardless of feasibility (supported by Table 1 and multiple dataset evaluations)
- **Medium Confidence:** The mechanism that CLIP representations are robust to real-world infeasibility (empirical but lacks theoretical grounding)
- **Medium Confidence:** The finding that mixing feasible and infeasible images doesn't significantly impact performance (statistically supported but requires broader validation)

## Next Checks

1. **Cross-Domain Validation:** Apply VariReal to non-fine-grained datasets (e.g., ImageNet) and alternative architectures (e.g., ResNet, ViT without CLIP pre-training) to test generalizability of feasibility findings.

2. **Semantic Shift Analysis:** Systematically vary the degree of attribute change from covariance shift to semantic shift in foreground edits to identify the threshold where feasibility impacts performance.

3. **Robustness Testing:** Evaluate trained models on adversarial examples and out-of-distribution test sets to determine if infeasible training data affects generalization beyond accuracy metrics.