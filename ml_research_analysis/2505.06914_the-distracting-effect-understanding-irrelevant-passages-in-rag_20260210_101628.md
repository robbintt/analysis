---
ver: rpa2
title: 'The Distracting Effect: Understanding Irrelevant Passages in RAG'
arxiv_id: '2505.06914'
source_url: https://arxiv.org/abs/2505.06914
tags:
- distracting
- passages
- effect
- passage
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of distracting passages in Retrieval
  Augmented Generation (RAG) systems, where irrelevant but semantically related passages
  mislead language models during answer generation. The authors formalize a quantifiable
  measure of a passage's distracting effect with respect to a query and LLM, demonstrating
  robustness across different models through high correlation in distraction scores.
---

# The Distracting Effect: Understanding Irrelevant Passages in RAG

## Quick Facts
- arXiv ID: 2505.06914
- Source URL: https://arxiv.org/abs/2505.06914
- Authors: Chen Amiraz; Florin Cuconasu; Simone Filice; Zohar Karnin
- Reference count: 40
- Primary result: Fine-tuning LLMs on hard distracting passages improves RAG accuracy by up to 7.5% on ungrounded examples

## Executive Summary
This paper addresses the issue of distracting passages in Retrieval Augmented Generation (RAG) systems, where irrelevant but semantically related passages mislead language models during answer generation. The authors formalize a quantifiable measure of a passage's distracting effect with respect to a query and LLM, demonstrating robustness across different models through high correlation in distraction scores. They introduce multiple methods to obtain hard distracting passages, including answer-skewed retrieval and synthetic generation across four categories (related topic, hypothetical, negation, modal statement). Experiments show that retrieved passages become more distracting at higher ranks, and combining diverse retrieval and generation methods yields significantly more distracting passages than any single approach. The authors demonstrate practical value by fine-tuning LLMs on their hard distracting dataset, achieving up to 7.5% improvement in answering accuracy over conventional RAG training, particularly for ungrounded examples.

## Method Summary
The paper quantifies distracting effect using DE_q(p) = 1 - p(NO-RESPONSE|q,p), measuring an LLM's tendency to answer rather than abstain when presented with irrelevant passages. They create hard distracting passages through six methods: standard retrieval (Rst), reranked retrieval (Rst+), answer-skewed retrieval (Rsk) that retrieves passages related to query but unrelated to answer, and four synthetic generators (G_rel, G_hypo, G_neg, G_modal) using Claude 3.5 Sonnet. Passages containing the ground-truth answer are filtered using an NLI model. For training, they construct datasets with 5 passages per query (50% grounded with 1 relevant + 4 distracting, 50% ungrounded with 5 distracting) and fine-tune using LoRA on Llama-3.2-3B or Llama-3.1-8B with specified hyperparameters.

## Key Results
- Combined retrieval and synthetic generation methods produce significantly more distracting passages than any single approach
- Hard distracting passages are more effective when combined with relevant passages than when used alone
- Fine-tuning on hard distracting passages improves ungrounded accuracy by 5.3-16.1 points for Llama-3.2-3B and 3.6-11.0 for Llama-3.1-8B
- The distracting effect measure shows high correlation (>0.5 Spearman) across different LLM architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The distracting effect of a passage can be quantified by measuring an LLM's probability of not abstaining when asked to answer from that passage alone.
- Mechanism: Given query q and irrelevant passage p, prompt the LLM to answer or output "NO-RESPONSE" if no answer exists. Compute DE_q(p) = 1 - p(NO-RESPONSE|q,p). Higher scores indicate higher distraction potential. This isolates passage-specific effects by testing each passage independently.
- Core assumption: The LLM's tendency to generate an answer rather than abstain reflects genuine distraction, not just instruction-following failures.
- Evidence anchors:
  - [abstract] "We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs."
  - [section 3.1] Equation 1 defines DE_q(p); advantages listed include interpretability and cheap implementation cost via single token probability.
  - [corpus] Related work on positional bias (arxiv 2505.15561) examines how context position affects RAG accuracy, suggesting distraction interacts with ordering—mechanism may require calibration for positional effects.
- Break condition: If an LLM systematically ignores abstention instructions (e.g., Falcon-3-3B per Appendix A.1), DE scores become inflated and unreliable as relative rankings.

### Mechanism 2
- Claim: Combining retrieval-based and synthetic generation methods yields more distracting passages than any single method alone.
- Mechanism: Use standard retrieval (Rst), answer-skewed retrieval (Rsk), and four synthetic generators (G_rel, G_hypo, G_neg, G_modal) to produce candidate passages. For each query, select the passage with highest DE score across all methods. This diversifies beyond what standard retrieval provides.
- Core assumption: Distracting passages are diverse in type; no single source captures all high-DE passages.
- Evidence anchors:
  - [section 4.3, Figure 3] Rst+ provides most distracting passage for ~52% of queries, but other methods win for ~48%. G_modal, G_neg, and G_hypo contribute unique wins.
  - [section 3.2.1] Answer-skewed retrieval modifies query embedding: E_sub(q,a) = E_Q(q) - λ·E_D(a), retrieving passages related to query but unrelated to answer.
  - [corpus] "Dynamic Context Selection for RAG" (arxiv 2512.14313) proposes context selection to mitigate distractors—suggests complementary approach where distraction scoring informs selection rather than training.
- Break condition: If synthetic passages diverge too far from real retrieval distribution, training on them may not generalize to inference-time retrieval noise.

### Mechanism 3
- Claim: Fine-tuning LLMs with hard distracting passages improves robustness, particularly on ungrounded examples where no relevant passage is present.
- Mechanism: Construct training sets mixing grounded (1 relevant + 4 hard distracting) and ungrounded (5 hard distracting) examples. Fine-tune with LoRA. The model learns to recognize and resist distraction when no gold passage exists, improving parametric knowledge reliance.
- Core assumption: Exposure to maximally distracting passages during training induces robustness that transfers to unseen distracting passages at inference.
- Evidence anchors:
  - [section 5.2, Table 2] Hard training improves acc_u (ungrounded accuracy) by 5.3-16.1 points for Llama-3.2-3B and 3.6-11.0 for Llama-3.1-8B across benchmarks.
  - [section 5.2] "When the answer is present only in the parametric memory of the LLM, a prompt with only distracting passages is much more likely to result in an error."
  - [corpus] Weak corpus evidence for this specific mechanism; no directly comparable training approaches found in neighbors.
- Break condition: If grounded accuracy (acc_g) degrades significantly, the tradeoff may be unacceptable for use cases requiring retrieval grounding.

## Foundational Learning

- Concept: **Dense retrieval and reranking pipeline**
  - Why needed here: The paper uses E5-base embeddings + BGE-M3 cross-encoder reranking; understanding these components is prerequisite to implementing Rst and Rsk methods.
  - Quick check question: Can you explain why reranking increases average distracting effect (Section 4.2) despite improving retrieval quality?

- Concept: **Probability-based LLM evaluation via next-token logits**
  - Why needed here: DE_q(p) computation requires extracting p(NO-RESPONSE) from logits, not just generation.
  - Quick check question: Given a prompt ending with "Answer:", how would you extract the probability that the next token is "NO-RESPONSE"?

- Concept: **LoRA fine-tuning for instruction-tuned LLMs**
  - Why needed here: Section 5 uses LoRA (rank 64-128) with specific hyperparameters; understanding adapter tuning is necessary for reproduction.
  - Quick check question: Why might LoRA be preferred over full fine-tuning for this task given the paper's experimental constraints?

## Architecture Onboarding

- Component map:
  1. **DE Scorer**: Takes (query, passage, LLM) → computes 1 - p(NO-RESPONSE) via single-token logit extraction (Algorithm 1, Figure 6 prompt)
  2. **Passage Candidate Generator**: Six parallel sources—Rst (standard retrieval), Rst+ (with reranking), Rsk/Rsk+ (answer-skewed), G_rel/G_hypo/G_neg/G_modal (synthetic via Claude 3.5 Sonnet with few-shot prompts, Figures 22-25)
  3. **False Negative Filter**: NLI model (Honovich 2022) excludes passages containing or entailing the ground-truth answer (Section 3.2)
  4. **Hard Selector**: For each query, ranks candidates by DE score and selects highest
  5. **Training Set Builder**: Constructs (q, a*, P) triplets with 5 passages per query (50% grounded, 50% ungrounded for Hard strategy)
  6. **LoRA Fine-tuner**: Trains Llama-3.2-3B or Llama-3.1-8B with specified hyperparameters (Appendix B)

- Critical path: DE Scorer → Passage Candidate Generator → False Negative Filter → Hard Selector → Training Set Builder → LoRA Fine-tuner. The DE scorer is the bottleneck; computing DE for all candidate passages per query requires N LLM forward passes where N = total candidates across all methods.

- Design tradeoffs:
  - λ in answer-skewed retrieval (Eq. 2-3): λ=1 balances distraction vs. topic drift; λ>1 yields weak/irrelevant passages; λ<1 makes Rsk too similar to Rst (Appendix A.2)
  - Grounded/ungrounded mix ratio: 50/50 split chosen for Hard strategy; increasing ungrounded proportion may further improve acc_u at cost of acc_g
  - DE threshold for hard/weak classification: 0.8/0.2 thresholds capture ~72% of probability mass for Llama-3.1-8B (Section 4.4); different models may require calibration

- Failure signatures:
  - **Inflated DE scores**: Model ignores abstention instructions (Falcon-3-3B in Appendix A.1); diagnose by checking if DE distribution is heavily concentrated near 1.0
  - **Low unique wins for synthetic methods**: If G_* methods rarely produce highest-DE passages, check prompt quality and ensure generated passages don't accidentally contain correct answer
  - **Training divergence**: If fine-tuned model shows lower acc_g than baseline, reduce ungrounded example proportion or lower learning rate

- First 3 experiments:
  1. **Validate DE metric correlation**: Compute DE scores for 100 query-passage pairs across 2+ LLMs; verify Spearman correlation >0.5 as claimed in Section 4.3/Figure 4
  2. **Ablate candidate sources**: Train models using each source (Rst, Rst+, Rsk, G_modal, etc.) in isolation; confirm Rst+ and G_modal are top individual performers per Figure 1
  3. **Test generalization gap**: Train on NQ, evaluate OOD on PopQA/TriviaQA/WebQA; verify Hard strategy maintains 3-7 point advantage over Retrieve/Rerank baselines per Table 2

## Open Questions the Paper Calls Out
None

## Limitations
- The DE metric reliability depends on LLMs consistently following abstention instructions, which fails for certain models like Falcon-3-3B
- Synthetic passage generation depends on unspecified few-shot prompts for Claude 3.5 Sonnet, creating reproducibility concerns
- The 50/50 grounded/ungrounded training mix may not generalize across all RAG use cases with different retrieval success rates

## Confidence

- **High Confidence**: DE metric computation via abstention probability (mechanism and equations clearly specified); LoRA fine-tuning hyperparameters (explicit values provided); overall experimental methodology and benchmark evaluation
- **Medium Confidence**: Correlation robustness across LLMs (claimed >0.5 but only 3 models tested); effectiveness of answer-skewed retrieval (λ=1 empirically chosen); synthetic generator prompt quality (structure shown but few-shot examples missing)
- **Low Confidence**: Synthetic passages' distributional similarity to real retrieval noise; generalizability of 50/50 grounded/ungrounded training mix; specific NLI model thresholds for false negative filtering

## Next Checks

1. **DE Metric Correlation Validation**: Compute DE scores for 100 query-passage pairs across Llama-3.1-8B, Llama-3.2-3B, and Gemma-2-9B; verify Spearman correlation exceeds 0.5 between all model pairs as claimed in Section 4.3.

2. **Ablation of Synthetic Generators**: Train separate models using each synthetic generator (G_rel, G_hypo, G_neg, G_modal) in isolation; confirm that G_modal and G_neg contribute unique high-DE passages and that combined usage outperforms any single source as shown in Figure 1.

3. **OOD Generalization Test**: Train on NQ using Hard strategy, then evaluate on PopQA, TriviaQA, and WebQA; verify that acc_u improvements (3-7 points over baseline) transfer to out-of-domain datasets as claimed in Table 2, and measure any degradation in acc_g.