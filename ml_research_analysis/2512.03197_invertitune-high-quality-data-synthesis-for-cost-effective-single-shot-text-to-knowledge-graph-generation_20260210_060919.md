---
ver: rpa2
title: 'InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge
  Graph Generation'
arxiv_id: '2512.03197'
source_url: https://arxiv.org/abs/2512.03197
tags:
- knowledge
- triples
- text
- graph
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic knowledge graph
  construction from text (Text2KG), where existing methods rely on computationally
  expensive iterative LLM prompting and lack high-quality, realistically sized datasets
  for supervised fine-tuning. The proposed InvertiTune framework introduces a data
  generation pipeline that extracts noise-minimized, semantically coherent subgraphs
  from a large knowledge base and uses an LLM to generate corresponding textual descriptions,
  a task more aligned with LLM capabilities than direct KG generation from text.
---

# InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation

## Quick Facts
- **arXiv ID**: 2512.03197
- **Source URL**: https://arxiv.org/abs/2512.03197
- **Reference count**: 21
- **Key outcome**: Achieves 82.02 G-BLEU, 82.67 G-ROUGE, and 92.58 G-BERTScore on CE12k test set using a small fine-tuned model, outperforming larger non-fine-tuned LLMs and state-of-the-art baselines.

## Executive Summary
InvertiTune addresses the challenge of efficient, high-quality knowledge graph construction from text by introducing a data synthesis pipeline that extracts noise-minimized subgraphs and uses LLMs to generate textual descriptions, better aligning with LLM strengths than direct KG generation. The framework enables the creation of realistic, large-scale datasets for single-shot Text2KG, enabling effective fine-tuning of lightweight models. Experimental results demonstrate that InvertiTune achieves superior performance compared to much larger non-fine-tuned models and state-of-the-art baselines on CE12k and CrossEval-1200 datasets. The approach highlights that dataset quality is more important than quantity for advancing efficient Text2KG systems.

## Method Summary
InvertiTune introduces a novel data generation pipeline for single-shot Text2KG by reversing the typical generation direction: instead of generating KGs from text, it extracts noise-minimized, semantically coherent subgraphs from a large knowledge base and uses an LLM to generate corresponding textual descriptions. This process is more aligned with LLM strengths and enables the creation of longer texts paired with larger KGs that reflect real-world scenarios. The framework fine-tunes a lightweight Qwen2.5-1.5B Instruct model on the generated dataset, achieving strong performance with significantly fewer parameters than non-fine-tuned baselines.

## Key Results
- Achieves 82.02 G-BLEU, 82.67 G-ROUGE, and 92.58 G-BERTScore on the CE12k test set.
- Outperforms significantly larger non-fine-tuned LLMs and state-of-the-art Text2KG baselines.
- Demonstrates strong cross-dataset generalization on CrossEval-1200.
- Shows that high-quality data is more important than quantity, with comparable performance achievable with fewer than 12k samples.

## Why This Works (Mechanism)
InvertiTune leverages the strengths of LLMs by using them for text generation from structured knowledge rather than attempting direct KG generation from text, a task more aligned with LLM capabilities. By extracting noise-minimized subgraphs and generating corresponding textual descriptions, the pipeline produces high-quality, semantically coherent datasets that better reflect real-world scenarios. This approach enables efficient fine-tuning of lightweight models, achieving superior performance with fewer parameters and less computational overhead than larger non-fine-tuned models.

## Foundational Learning
- **Subgraph extraction from knowledge bases**: Why needed? To isolate semantically coherent and noise-minimized portions of the KG for training. Quick check: Evaluate subgraph coherence and noise levels on held-out KGs.
- **LLM-based text generation from structured data**: Why needed? To leverage LLM strengths in natural language generation rather than KG construction. Quick check: Compare text quality metrics (e.g., fluency, coherence) for LLM-generated vs. human-written descriptions.
- **Single-shot learning for Text2KG**: Why needed? To enable efficient, low-cost KG construction without iterative prompting. Quick check: Measure performance on few-shot vs. single-shot scenarios.
- **Fine-tuning lightweight models**: Why needed? To achieve high performance with reduced computational cost. Quick check: Compare FLOPs and inference time for fine-tuned vs. non-fine-tuned models.
- **Cross-dataset generalization**: Why needed? To validate robustness across diverse KG and text domains. Quick check: Test on additional, real-world KGs and longer, more complex texts.
- **Dataset quality vs. quantity**: Why needed? To optimize resource use in data synthesis. Quick check: Perform ablation studies varying dataset size and quality.

## Architecture Onboarding
- **Component map**: Knowledge base -> Subgraph extraction -> LLM text generation -> Dataset synthesis -> Fine-tuning of Qwen2.5-1.5B -> Inference
- **Critical path**: Subgraph extraction and LLM text generation are the core pipeline steps enabling high-quality dataset synthesis for fine-tuning.
- **Design tradeoffs**: Lightweight fine-tuning vs. larger non-fine-tuned models; quality-focused data synthesis vs. quantity; single-shot vs. iterative prompting.
- **Failure signatures**: Poor subgraph extraction leading to noisy or incoherent training data; LLM text generation failing to capture KG semantics; overfitting on synthetic data.
- **First 3 experiments**: (1) Ablation study isolating subgraph extraction vs. LLM generation impact; (2) Evaluation on additional, diverse real-world KGs; (3) Controlled comparison of dataset quality vs. quantity on performance.

## Open Questions the Paper Calls Out
None.

## Limitations
- Evaluation relies heavily on a single synthetic dataset (CE12k) and a small, synthetically constructed benchmark (CrossEval-1200), raising concerns about real-world applicability.
- No comparison to more recent, larger Text2KG systems or formal error analysis provided.
- Cost-effectiveness claims are not rigorously quantified (e.g., FLOPs or token counts across baselines).
- Lack of ablation studies isolating the impact of subgraph extraction versus LLM-based generation.

## Confidence
- **High**: Outperforms larger non-fine-tuned models on CE12k; cross-dataset generalization observed.
- **Medium**: Reverse generation pipeline is novel and effective; dataset quality more important than quantity.
- **Low**: Cost-effectiveness and broad real-world applicability claims not fully substantiated.

## Next Checks
1. Evaluate InvertiTune on additional, diverse real-world knowledge graphs and longer, more complex texts to test scalability and robustness.
2. Conduct a controlled ablation comparing the subgraph extraction process with direct LLM generation from full graphs, isolating its contribution to performance gains.
3. Quantify the computational cost (e.g., FLOPs, tokens) for both data generation and inference across all baselines, including detailed runtime breakdowns for model fine-tuning and inference.