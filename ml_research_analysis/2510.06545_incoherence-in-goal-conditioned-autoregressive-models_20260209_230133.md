---
ver: rpa2
title: Incoherence in goal-conditioned autoregressive models
arxiv_id: '2510.06545'
source_url: https://arxiv.org/abs/2510.06545
tags:
- policy
- prior
- reward
- have
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes incoherence in autoregressive goal-conditioned
  policies, which arises when conditioning a generative model on goals without considering
  that the derived policy will be used autoregressively. This leads to suboptimal
  behavior, as the policy doesn't account for its own future actions.
---

# Incoherence in goal-conditioned autoregressive models

## Quick Facts
- arXiv ID: 2510.06545
- Source URL: https://arxiv.org/abs/2510.06545
- Authors: Jacek Karwowski; Raymond Douglas
- Reference count: 35
- The paper analyzes incoherence in autoregressive goal-conditioned policies and shows it can be eliminated through iterative retraining, leading to optimal behavior.

## Executive Summary
This paper identifies a fundamental problem in autoregressive goal-conditioned models where the policy becomes incoherent with its own future actions. The core issue arises when conditioning a generative model on goals without accounting for the fact that the derived policy will be used autoregressively. The authors formalize this incoherence as a KL divergence between the current policy and its future-return-influenced version, and prove that iterative retraining can eliminate it, leading to optimal behavior. The work provides a unified theoretical framework connecting common RL fine-tuning methods as coherence-restoring procedures.

## Method Summary
The paper analyzes goal-conditioned autoregressive policies through a control-as-inference framework, where rewards are treated as log-probabilities of optimality. It identifies incoherence as the misalignment between the prior used for conditioning and the policy used for action. Three equivalent mechanisms for eliminating incoherence are proposed: iterative retraining on self-generated actions, temperature annealing in KL-regularized RL, and folding the posterior into the reward. The authors prove these converge to the same coherent policy under deterministic dynamics, with monotonic return improvement.

## Key Results
- Incoherence in autoregressive goal-conditioned policies leads to suboptimal behavior
- Iterative retraining on self-generated actions monotonically reduces incoherence and improves return
- Three equivalent perspectives for coherence restoration: fine-tuning, temperature annealing, and reward folding
- Under deterministic dynamics, these methods converge to the same policy sequence with optimal behavior
- Theoretical characterization of convergence rate for the coherence-restoring process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative re-training on self-generated actions (control-as-inference) monotonically reduces incoherence and improves return.
- Mechanism: By conditioning the model on trajectories sampled from its own policy π_i rather than a fixed prior p, the agent forces the generative model to answer Question (2) ("what action leads to the goal if I follow my current policy?") instead of Question (1) ("what action leads to the goal if I act randomly?"). This aligns the posterior with the marginal visitation distribution.
- Core assumption: The environment dynamics τ remain stationary, and the initial prior has full support over optimal actions.
- Evidence anchors:
  - [abstract] "We focus on the process of re-training models on their own actions... We prove that it decreases incoherence and leads to an improvement in return."
  - [section] Proposition 5.4 (Strong return improvement lemma): "The sequence of policies... given by the control-by-inference improves its return monotonically."
  - [corpus] Not explicitly validated by neighbor papers; primarily a theoretical contribution of this work.
- Break condition: If the prior assigns zero probability to an optimal action, the sequence may converge to a suboptimal policy (Section 5, discussion of Prop 5.5).

### Mechanism 2
- Claim: In deterministic environments, decreasing the temperature (sharpening the softmax) is equivalent to iteratively re-training on self-generated actions.
- Mechanism: As temperature decreases (α → ∞), the policy becomes "greedier" regarding its own value estimate. The paper proves this is mathematically equivalent to the "folding" of the posterior into the reward, provided transitions are deterministic. This creates a bridge between inference-time manipulation (temperature) and training-time manipulation (re-training).
- Core assumption: Transition dynamics τ are deterministic. This equivalence breaks down in stochastic environments (Theorem 5.9, Example 3).
- Evidence anchors:
  - [abstract] "...in the deterministic case, as decreasing the temperature parameter; the correspondence has computational content via the training-inference trade-off."
  - [section] Theorem 5.9: "For all priors... and deterministic transition dynamics... π_{α(k)} = π^G_{2k} = π^F_{2k}."
  - [corpus] Neighbor "Why Goal-Conditioned Reinforcement Learning Works" discusses optimal control but does not confirm this specific temperature-equivalence claim.
- Break condition: In stochastic environments, lowering temperature diverges from the posterior-folding behavior (Example 3), potentially leading to suboptimal "luck"-based policies.

### Mechanism 3
- Claim: Incoherence can be eliminated by folding the posterior probability into the reward function.
- Mechanism: Instead of just maximizing the environment reward r, the agent maximizes a modified reward r̂(s,a) = r(s,a) + log π(a|s). This internalizes the cost of deviation from the policy. By iteratively updating the reward to include the log-probability of the agent's own actions, the system converges to a coherent fixed point where the "prior" used for inference matches the "policy" used for action.
- Core assumption: The reward function is non-positive (treated as log-probabilities of optimality).
- Evidence anchors:
  - [section] Definition 5.7 (Folded sequence): "This technically modifies the MDP... distributes [reward] around the MDP."
  - [section] Theorem 5.9 establishes equivalence with re-training (π^F_k = π^G_k) even in stochastic settings.
  - [corpus] Weak external validation; primarily an internal theoretical derivation.
- Break condition: Infinite computational horizon or improper reward scaling could destabilize the iterative update loop.

## Foundational Learning

- Concept: **Control-as-Inference (Soft Q-Learning)**
  - Why needed here: The paper frames RL entirely as probabilistic inference, where rewards are treated as log-probabilities of a binary optimality variable O_t. Understanding this mapping is required to interpret "incoherence" as a misalignment between the inference prior and the generative policy.
  - Quick check question: Can you explain why maximizing E[r] is equivalent to maximizing the log-likelihood of an "optimal" binary variable in this framework?

- Concept: **KL Divergence & Entropy Regularization**
  - Why needed here: Incoherence is formally defined as a KL divergence between the trajectory distribution of the policy and the trajectory distribution of the conditioned model. The "fix" relies on manipulating this divergence.
  - Quick check question: What does it mean for two policies to have zero KL divergence over trajectories versus zero KL divergence over single-step actions?

- Concept: **Autoregressive Goal-Conditioning**
  - Why needed here: The mechanism fails specifically because autoregressive models condition on the past to predict the future, but RL requires conditioning on the future (goal) to select the current action. The paper analyzes the mismatch between these two directions.
  - Quick check question: How does a Decision Transformer differ from a standard Transformer in terms of conditioning context during inference?

## Architecture Onboarding

- Component map:
  - Generative Prior (p) -> Inference Engine -> Optimality Variable (O_t) -> Re-training Loop

- Critical path:
  1. Define the Prior p(a|s) and Reward r(s,a)
  2. Calculate the goal-conditioned policy π(a|s) ∝ p(a|s) · p(O_{1:T}=1|s,a)
  3. **Detect Incoherence:** Check if π produces trajectories inconsistent with the prior used to derive it (e.g., the Mountain Race example)
  4. **Apply Restoration:** Either (a) Retrain on π's trajectories, or (b) Modify reward to r̂ = r + log π
  5. Repeat until convergence (Coherence)

- Design tradeoffs:
  - **Compute vs. Performance:** Re-training (Mechanism 1) is computationally expensive but works in stochastic settings. Temperature annealing (Mechanism 2) is cheap (inference-time) but strictly valid only for deterministic dynamics.
  - **Exploration:** The initial prior must have "full support" (non-zero probability everywhere). If the offline data lacks coverage, the coherent policy may converge to a local optimum (Corollary 4.15).

- Failure signatures:
  - **"Safe" Suboptimality:** The agent consistently chooses a lower-reward "safe" path because the conditioning logic incorrectly assumes it will act randomly (like the prior) in future steps (like the Mountain Race example).
  - **Stochastic Divergence:** If using temperature annealing in a noisy environment, the policy may over-commit to actions that were "lucky" in hindsight rather than robustly optimal (Example 3).

- First 3 experiments:
  1. **Deterministic MDP Validation (Mountain Race):** Implement Example 1. Verify that naive conditioning chooses the "safe" 0.75 reward path, while the coherent/folded policy chooses the "risky" 1.0 path.
  2. **Stochastic Divergence Check (Example 3):** Construct a simple 3-state stochastic MDP. Demonstrate that Temperature Annealing (π_α) produces different policies than Posterior Folding (π_F) after k steps.
  3. **Rate of Convergence:** Measure the return improvement J(π_k) - J(π_{k-1}) against the theoretical bound in Corollary 5.10 to see if the "coherence gap" closes as predicted.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes stationary dynamics and full-support prior, with explicit breakdown in stochastic environments when applying temperature annealing
- Theoretical convergence guarantees are asymptotic and don't address finite-sample computational costs
- Framework requires reward to be non-positive (log-probability form), limiting application to arbitrary reward structures

## Confidence
- **High confidence:** Proposition 5.4 (return improvement under re-training), Theorem 5.9 (deterministic case equivalence), and formal definition of incoherence
- **Medium confidence:** Deterministic temperature-equivalence claim (theoretically sound but may face practical implementation challenges)
- **Medium confidence:** Rate of convergence bound in Corollary 5.10 (derived from theoretical assumptions that may not hold exactly in practice)

## Next Checks
1. Implement the Mountain Race MDP from Example 1 to empirically verify that naive conditioning selects the suboptimal "safe" path (0.75 reward) while the coherent policy selects the optimal "risky" path (1.0 reward).
2. Construct the 3-state stochastic MDP from Example 3 to demonstrate divergence between temperature annealing and posterior folding policies, showing that stochastic dynamics break the equivalence.
3. Measure empirical return improvement J(π_k) - J(π_{k-1}) against the theoretical bound in Corollary 5.10 to validate the predicted convergence rate in a simple grid-world environment.