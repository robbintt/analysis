---
ver: rpa2
title: 'HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under
  Human-AI Coauthoring'
arxiv_id: '2506.02959'
source_url: https://arxiv.org/abs/2506.02959
tags:
- detection
- text
- methods
- performance
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies fine-grained detection of machine-generated
  text (MGT) under human-AI coauthoring. The authors propose a new dataset HACo-Det
  that simulates realistic human-AI collaboration by using LLMs to paraphrase human-written
  texts over multiple turns, with word-level attribution labels.
---

# HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring

## Quick Facts
- arXiv ID: 2506.02959
- Source URL: https://arxiv.org/abs/2506.02959
- Authors: Zhixiong Su; Yichen Wang; Herun Wan; Zhaohan Zhang; Minnan Luo
- Reference count: 40
- Primary result: Finetuned models (DeBERTa) achieve F1-W up to 0.831 for word-level MGT detection, far outperforming metric-based methods (F1 ~0.462)

## Executive Summary
This paper studies fine-grained detection of machine-generated text (MGT) under human-AI coauthoring scenarios. The authors introduce HACo-Det, a dataset simulating realistic human-AI collaboration through multi-turn LLM paraphrasing of human-written texts with word-level attribution labels. They adapt seven existing MGT detectors for fine-grained word-level and sentence-level detection tasks and evaluate them on HACo-Det. Results show metric-based methods perform poorly at word-level detection (average F1 0.462), while finetuned models like DeBERTa achieve much higher accuracy (F1 up to 0.831), though challenges remain in generalization and robustness.

## Method Summary
The study constructs the HACo-Det dataset by iteratively paraphrasing human-written text spans with LLMs across multiple turns, creating realistic human-AI coauthored texts with word-level attribution labels. The pipeline segments human texts into sentences, selects continuous spans, and applies LLM paraphrasing (using Llama-3, Mixtral, GPT-4o, GPT-4o-mini) repeatedly. Word-level labels are assigned based on lexical matching - unchanged words in the same tense retain human attribution while paraphrased words receive MGT labels. The authors adapt seven existing MGT detectors (both metric-based and finetuned models) for fine-grained detection by modifying them to output per-word predictions. They evaluate performance using macro F1-score, AUC-ROC, and precision/recall metrics across in-domain, out-of-domain, and out-of-model settings.

## Key Results
- Finetuned models (DeBERTa-v3) achieve F1-W of 0.831 on in-domain word-level detection, significantly outperforming metric-based methods (F1 ~0.462)
- Metric-based detectors (DetectGPT, Fast-DetectGPT, etc.) struggle at word-level detection but perform better at sentence-level (F1-S ~0.889-0.892)
- Performance degrades substantially in out-of-domain (F1-S drops to 0.769-0.881) and out-of-model (F1-S drops to 0.764-0.890) settings
- Longer context windows improve detection accuracy, with performance increasing from F1-S 0.913 (512 tokens) to 0.955 (2048 tokens)

## Why This Works (Mechanism)

### Mechanism 1: Multi-turn Paraphrasing as Coauthoring Simulation
Iterative LLM paraphrasing of human-written spans produces realistic human-AI coauthored texts with controllable attribution. The pipeline selects continuous sentence spans from human-written text, instructs LLMs to paraphrase them, and repeats across multiple turns. Each revision creates MGT fragments while preserving surrounding human context, enabling word-level attribution study.

### Mechanism 2: Grounded Word-Level Attribution via Lexical Matching
Word-level authorship is attributed by tracking which words survive paraphrasing unchanged versus newly generated. After each paraphrasing turn, words in the revised span default to MGT labels, but words that appear identically (same tense) in the original span retain human attribution. This prevents misattribution of preserved vocabulary.

### Mechanism 3: Semantic Representations Enable Fine-Grained Detection; Metric-Based Signals Do Not
Finetuned models with learned semantic representations substantially outperform metric-based methods for word-level MGT detection because metrics computed at fine granularity lose discriminative power. Metric-based detectors rely on probability distributions that become indistinguishable at word level in coauthored contexts, while finetuned models learn contextual semantic features that capture subtle authorship patterns across the sequence.

## Foundational Learning

- **Sequence Labeling (Token Classification)**: The task requires assigning a binary label (HWT/MGT) to each word in a document, not just document-level classification. Understanding BIO tagging, CRFs, or token-level classifiers is essential.
  - Why needed here: Fine-grained detection requires per-token predictions rather than document-level classification
  - Quick check question: Can you explain how a model outputs per-token predictions versus a single document label?

- **Authorship Attribution & Boundary Detection**: Fine-grained detection is fundamentally about attributing segments to different sources. Prior work on boundary detection and MGT localization informs the problem formulation.
  - Why needed here: The core challenge is determining where human authorship ends and AI authorship begins within a text
  - Quick check question: How would you define the boundary between human and AI authorship in a paraphrased sentence?

- **Generalization in Detection (Domain/Model Shift)**: The paper evaluates out-of-domain and out-of-model settings. Understanding distribution shift, overfitting to generator patterns, and robustness is critical for real deployment.
  - Why needed here: Detectors must work across different writing styles, domains, and AI generators
  - Quick check question: Why might a detector trained on GPT-4o outputs fail on Llama-3 outputs?

## Architecture Onboarding

- **Component map:** Raw HWT → Sentence segmentation → Span selection → LLM paraphrasing (multi-turn) → Word-level lexical matching → Label assignment → Aggregation to sentence level
- **Critical path:**
  1. Construct coauthored dataset with grounded word-level labels
  2. Adapt detectors: metric-based (compute per-word, threshold) and finetuned (sequence labeling head)
  3. Train/validate on in-domain split; test on held-out domains/generators
  4. Analyze failure modes: context window limits, zero-shot capability, revision mode generalization
- **Design tradeoffs:**
  - Metric-based vs. Finetuned: Metrics require white-box access, are zero-shot but fail at fine granularity; finetuned models need labeled data, generalize better but require retraining for new domains/generators
  - Word vs. Sentence Level: Word-level provides precise attribution but is harder; sentence-level via ensembling is coarser but more tractable for metric methods
  - Context Window: Longer context improves performance but increases compute and may exceed model limits; chunking degrades accuracy
- **Failure signatures:**
  1. Metric-based near-random performance at word level (F1 ~0.44-0.50) → signals metrics lack fine-grained discriminability
  2. Large OOD/OOM performance drops for finetuned models → overfitting to training generator/domain
  3. Frozen encoder failure in zero-shot (Table 6: F1 ~0.53-0.60) → semantic representations not general enough without tuning
- **First 3 experiments:**
  1. Reproduce in-domain word-level detection: Train DeBERTa on HACo-Det training split; evaluate F1-W/AUC-W on test set. Compare to metric baselines. Confirm ~0.83 F1.
  2. Ablate context window: Truncate inputs to 512, 1024, 2048 tokens; measure performance drop. Validate Figure 5 trend.
  3. Test OOD generalization: Train on one domain (e.g., news), test on others (paper, story, Wikipedia). Measure F1-S gap vs. in-domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can expanding the context window beyond 2048 tokens significantly improve performance for fine-grained detection in long coauthored texts?
- Basis in paper: The authors state in the Introduction and Section 7.3 that the "context window of the classification model is a bottleneck" and suggest "larger context windows" as a future direction.
- Why unresolved: Current detectors often truncate or chunk texts exceeding standard limits (e.g., 512 or 2048), which destroys long-range semantic dependencies and degrades detection accuracy.
- What evidence would resolve it: Evaluating the performance of detectors (like DeBERTa) adapted to handle larger input sequences (e.g., 4,096 or 8,192 tokens) on the full-length texts within HACo-Det without chunking.

### Open Question 2
- Question: How can detection models be trained to generalize effectively across different human-AI coauthoring revision modes (e.g., paraphrasing vs. continuation)?
- Basis in paper: Section 7.1 notes "It is still hard for DeBERTa to generalize between different revision modes," showing a significant performance gap when testing on the SeqXGPT-Bench dataset compared to the HACo-Det dataset.
- Why unresolved: Current SOTA models are sensitive to the specific generation pipeline used in training, failing to transfer learned features to datasets constructed with different revision operations.
- What evidence would resolve it: Developing detectors trained on a mixture of revision strategies (paraphrasing, continuation, polishing) and measuring performance consistency across datasets with differing construction pipelines.

### Open Question 3
- Question: Can fine-tuning the base language models used for metric computation enable effective zero-shot fine-grained detection?
- Basis in paper: Section 7.2 suggests that metric-based methods fail because their distributions are indistinguishable at lower granularities, proposing a "patch" of "fine-tuning the base model for metrics computing."
- Why unresolved: Metric-based detectors currently rely on standard pre-trained models that do not capture the conditional probability distributions needed for word-level attribution without supervised adaptation.
- What evidence would resolve it: Adapting the generator or proxy models used in metric-based detectors (like DetectGPT or Fast-DetectGPT) to the specific task of fine-grained attribution and evaluating the resulting zero-shot F1 scores.

## Limitations

- Dataset Realism: The multi-turn paraphrasing simulation may not capture all forms of human-AI collaboration, particularly those involving ideation, editing, or structural changes beyond paraphrasing.
- Detector Generalization: Significant performance drops in out-of-domain and out-of-model settings suggest detectors may overfit to specific generator patterns rather than learning generalizable authorship features.
- Context Window Constraints: Performance improvements with longer context windows are limited by model position embedding constraints, and the paper doesn't fully explore sliding window approaches or their impact on detection accuracy.

## Confidence

**High Confidence:** The core finding that finetuned semantic models (DeBERTa) significantly outperform metric-based methods at word-level detection (F1 0.831 vs ~0.46) is well-supported by experimental results across multiple domains and generators.

**Medium Confidence:** The claim that multi-turn paraphrasing better simulates realistic coauthoring than single-turn methods is reasonable but not empirically validated against actual human-AI collaboration data.

**Low Confidence:** The assertion that these findings generalize to all forms of human-AI coauthoring remains speculative, as the dataset construction may not capture the full complexity of collaborative writing processes.

## Next Checks

1. **Validate Attribution Scheme:** Conduct a human annotation study comparing the lexical matching-based labels against human judgments of authorship boundaries in paraphrased texts to test whether the heuristic captures true semantic authorship.

2. **Test Real Collaboration Data:** Apply the adapted detectors to datasets of actual human-AI coauthoring (if available) rather than simulated paraphrasing to validate whether performance trends generalize to real collaborative writing scenarios.

3. **Analyze Failure Cases Systematically:** Conduct detailed error analysis on OOD/OOM failure cases to identify whether detectors fail due to style shifts, vocabulary differences, or fundamental semantic indistinguishability.