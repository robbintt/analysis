---
ver: rpa2
title: Benchmarking Large Language Models on Homework Assessment in Circuit Analysis
arxiv_id: '2506.06390'
source_url: https://arxiv.org/abs/2506.06390
tags:
- student
- answer
- final
- correct
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks large language models (LLMs) on homework
  assessment for an undergraduate circuit analysis course. The authors develop a novel
  dataset of real student solutions and official reference solutions in LaTeX format,
  and evaluate three LLMs (GPT-3.5 Turbo, GPT-4o, and Llama 3 70B) across five metrics:
  completeness, method, final answer, arithmetic error, and units.'
---

# Benchmarking Large Language Models on Homework Assessment in Circuit Analysis

## Quick Facts
- arXiv ID: 2506.06390
- Source URL: https://arxiv.org/abs/2506.06390
- Reference count: 26
- Key outcome: LLMs show significant potential for automated homework assessment in circuit analysis, with GPT-4o and Llama 3 70B outperforming GPT-3.5 Turbo across all evaluation metrics

## Executive Summary
This paper presents a comprehensive benchmark of large language models for automated assessment of circuit analysis homework. The authors developed a novel dataset of 29 problems with 232 student solutions in LaTeX format, covering real student work from an undergraduate course. Three LLMs (GPT-3.5 Turbo, GPT-4o, and Llama 3 70B) were evaluated across five metrics: completeness, method, final answer, arithmetic error, and units. The study reveals that while GPT-4o and Llama 3 70B significantly outperform GPT-3.5 Turbo, current models still struggle with circuit diagram recognition and handwritten solutions, highlighting important limitations for practical implementation in educational settings.

## Method Summary
The authors created a benchmark dataset consisting of 29 circuit analysis problems from an undergraduate course, with 232 student solutions provided in LaTeX format alongside official reference solutions. Three LLMs (GPT-3.5 Turbo, GPT-4o, and Llama 3 70B) were evaluated using a multi-metric scoring system across five dimensions: completeness, method, final answer, arithmetic error, and units. Each solution was assessed by human experts who rated the LLM assessments for accuracy and reliability. The evaluation framework was designed to simulate real-world homework grading scenarios, with LLMs tasked with identifying correct approaches, detecting errors, and providing appropriate feedback to students.

## Key Results
- GPT-4o and Llama 3 70B significantly outperformed GPT-3.5 Turbo across all five evaluation metrics
- GPT-4o demonstrated more consistent performance compared to Llama 3 70B, which had higher false positive rates
- All models showed significant limitations in circuit diagram recognition and struggled with handwritten solution assessment
- The study achieved higher assessment accuracy than previous work in the domain, with improvements of 9-22% across different metrics

## Why This Works (Mechanism)
The success of GPT-4o and Llama 3 70B in this benchmark can be attributed to their larger model architectures and more sophisticated reasoning capabilities, which enable better understanding of complex circuit analysis problems and more accurate identification of student errors. These models demonstrated superior ability to parse mathematical notation in LaTeX format and apply domain-specific knowledge to evaluate solution methods.

## Foundational Learning
- Circuit analysis fundamentals: Understanding basic circuit components, Kirchhoff's laws, and analysis techniques is essential for evaluating student solutions
  - Why needed: LLMs must comprehend the underlying physics and mathematics to assess solution correctness
  - Quick check: Can the model identify when a student incorrectly applies Kirchhoff's current law?

- LaTeX mathematical notation: Proficiency in parsing and interpreting LaTeX-formatted equations and circuit diagrams
  - Why needed: The benchmark dataset uses LaTeX format for all solutions, requiring models to understand mathematical syntax
  - Quick check: Can the model correctly parse complex mathematical expressions with subscripts and superscripts?

- Educational assessment principles: Understanding grading rubrics and common student misconceptions in circuit analysis
  - Why needed: Effective assessment requires knowledge of what constitutes a complete solution and common error patterns
  - Quick check: Can the model identify partial credit scenarios where method is correct but final answer has calculation errors?

## Architecture Onboarding
Component map: Dataset (LaTeX problems/solutions) -> LLM (GPT-3.5 Turbo, GPT-4o, Llama 3 70B) -> Evaluation metrics (completeness, method, final answer, arithmetic error, units) -> Human expert validation

Critical path: Problem input → Solution parsing → Error detection → Metric scoring → Feedback generation

Design tradeoffs: The use of LaTeX format enables precise mathematical representation but limits applicability to handwritten solutions; larger models provide better accuracy but require more computational resources; expert human grading ensures quality but introduces subjectivity and scalability limitations.

Failure signatures: Incorrect circuit diagram interpretation, arithmetic calculation errors, failure to recognize valid alternative solution methods, overconfidence in incorrect assessments (false positives), particularly evident in Llama 3 70B's higher false positive rates.

First experiments:
1. Test model performance on a subset of problems with varying complexity levels to establish baseline accuracy
2. Compare assessment consistency across multiple human graders to quantify inter-rater reliability
3. Evaluate model performance on handwritten solution images to assess real-world applicability limitations

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation framework relies heavily on subjective expert human assessment, introducing variability in grading across the five metrics
- The dataset size of 29 problems with 232 solutions represents a moderate sample that may not capture full complexity of circuit analysis education
- The focus on typed LaTeX solutions limits generalizability to handwritten student submissions, where LLMs demonstrated particular weakness

## Confidence
- High confidence: GPT-4o and Llama 3 70B significantly outperform GPT-3.5 Turbo across all five evaluation metrics
- Medium confidence: GPT-4o demonstrates more consistent performance compared to Llama 3 70B
- Medium confidence: Current LLMs show fundamental limitations in circuit diagram recognition and handwritten solution processing
- Low confidence: The proposed evaluation framework is sufficient for comprehensive assessment of LLMs in educational contexts

## Next Checks
1. Conduct statistical significance testing on performance differences between models using appropriate tests (ANOVA or pairwise comparisons) to determine whether observed metric differences are meaningful
2. Expand the evaluation dataset to include handwritten solutions and circuit diagrams to better assess real-world applicability of LLM assessment tools
3. Implement inter-rater reliability analysis among human graders to quantify subjectivity in the current assessment methodology and establish confidence intervals for each metric