---
ver: rpa2
title: 'Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level
  Machine Translation'
arxiv_id: '2509.03809'
source_url: https://arxiv.org/abs/2509.03809
tags:
- evaluation
- source
- rankings
- translation
- align-then-slide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Align-then-Slide, a complete evaluation framework
  for ultra-long document-level machine translation. The framework addresses the challenge
  of evaluating document-level translations, where whole-document outputs break traditional
  sentence-by-sentence alignment assumptions.
---

# Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation

## Quick Facts
- arXiv ID: 2509.03809
- Source URL: https://arxiv.org/abs/2509.03809
- Reference count: 12
- Primary result: Pearson correlation of 0.929 between proposed method and expert MQM rankings on WMT benchmark

## Executive Summary
Align-then-Slide introduces a two-stage framework for evaluating ultra-long document-level machine translation where traditional sentence-by-sentence alignment fails due to omissions and many-to-one mappings. The framework first aligns source and target sentences using dynamic programming over similarity scores, reconstructing the target to match source sentence count with placeholders. It then applies n-chunk sliding evaluation (k=1,2,3,4) to calculate averaged quality scores across multiple granularities. Experiments demonstrate strong correlation with human judgments and successful application in reinforcement learning for document-level translation systems.

## Method Summary
The framework operates in two stages: (1) Align, which uses dynamic programming to find optimal sentence-level correspondences between source and target documents, reconstructing the target with placeholders for omissions and concatenation for one-to-many mappings; (2) n-Chunk Sliding Evaluate, which scores consecutive sentence chunks (k=1,2,3,4) with stride 1 and averages results across granularities using COMET-based quality estimators. The method produces preference data that enables effective CPO training and GRPO reward modeling, yielding translations preferred over vanilla SFT baselines in human evaluation.

## Key Results
- Pearson correlation of 0.929 between proposed method and expert MQM rankings on WMT 2020 benchmark
- Strong alignment with human judgments on newly curated real-world test set of 50 document pairs
- Preference data enables effective CPO training and GRPO reward modeling, producing translations preferred over SFT baseline
- Framework successfully handles ultra-long documents with systematic omissions and many-to-one mappings

## Why This Works (Mechanism)

### Mechanism 1
Dynamic programming over similarity matrix reconstructs one-to-one sentence alignment from misaligned pairs. After segmentation, an m×n similarity matrix is computed, and DP search finds maximum-sum path with constraints (y advances by 1, x non-negative). Anchoring on source, unmatched positions get placeholders; multiple targets are concatenated. Core assumption: similarity metric captures cross-lingual correspondence. Evidence: section 2.1, Algorithm 1. Break condition: noisy or inverted similarity scores produce incorrect alignments.

### Mechanism 2
Averaging scores across 1-, 2-, 3-, 4-chunk spans mitigates residual alignment conflicts. After Stage 1, consecutive k sentences form chunks with stride 1. For each k, all (m-k+1) chunks are scored and averaged. 1-chunk penalizes omissions; larger chunks merge adjacent source sentences, allowing single target to match multi-sentence source. Core assumption: quality estimators can meaningfully score multi-sentence chunks. Evidence: section 2.2, Figure 4. Break condition: systematic concentration of omissions in multi-sentence spans.

### Mechanism 3
Preference pairs and reward signals enable effective reinforcement learning. Framework scores candidates; higher-scoring outputs become preferred. These pairs train CPO offline or serve as GRPO reward model online. Both produce models preferred over SFT baselines. Core assumption: metric ranking correlates sufficiently with human judgment. Evidence: abstract, Table 4. Break condition: systematic metric bias (e.g., favoring longer outputs) leads to reward hacking.

## Foundational Learning

- **Reference-free quality estimation** (e.g., COMET-Kiwi, LaBSE): Needed for Stage 1 alignment to score source-target pairs without gold references; DP path depends on similarity scores. Quick check: Would COMET-Kiwi return high score if translation omits key information?

- **Dynamic programming for sequence alignment**: Needed to find optimal alignment path through similarity matrix under movement constraints; understanding recurrence relations is essential. Quick check: If dp[i-1][j-1] = 0.8, dp[i-1][j-2] = 0.6, and score[i][j] = 0.2, what is dp[i][j]?

- **Offline vs online RL for LLMs (CPO vs GRPO)**: Needed to understand downstream training utility; preference optimization vs reward-model-guided sampling clarifies when to use each. Quick check: In CPO, do you need to sample new translations during training, or is preference dataset fixed?

## Architecture Onboarding

- **Component map**: Sentence segmenter → Similarity matrix builder → DP path finder → Target reconstructor → Chunk generator → Quality estimator → Score averager → Preference pair extractor → CPO trainer/GRPO reward model

- **Critical path**: Similarity matrix quality → DP alignment accuracy → Chunk scoring stability → Aggregate metric reliability → RL training signal. Errors propagate forward; Stage 1 alignment mistakes cannot be fully corrected by Stage 2.

- **Design tradeoffs**: Stride 1 increases omission sensitivity but multiplies estimator calls O(m-k+1). Anchoring on source neutralizes length discrepancies but prevents detecting source omissions. Computational cost is O(m×n) for matrix plus O(k×(m-k+1)) estimator calls; long documents need batching or pruning.

- **Failure signatures**: Large number of empty placeholders suggests systematic omission/alignment failure. High variance across chunk sizes indicates unresolved many-to-one mappings. RL-trained model produces verbose/repetitive outputs, suggesting reward hacking.

- **First 3 experiments**: 1) Validate alignment on held-out set with known alignment; report precision/recall vs gold. 2) Correlate ASD20/ASD22/ASDKiwi with human MQM on WMT2020; confirm 0.929 Pearson and test sensitivity to backbone choice. 3) Train small CPO model using preference pairs; compare against SFT baseline via human side-by-side evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
Can batching or pruning heuristics effectively mitigate O(m × n) memory and latency bottlenecks for ultra-long documents without degrading alignment accuracy? Basis: authors explicitly state GPU memory/latency can become prohibitive without these optimizations. Unresolved because current framework computes full matrix without proposed algorithms. Evidence needed: comparison against modified framework using sparse techniques on documents >10,000 words.

### Open Question 2
Does fixed range of n-chunk sizes (1–4) generalize to documents requiring larger context windows for accurate disambiguation? Basis: method fixes k ∈ {1,2,3,4} without ablation study or theoretical justification. Unresolved because discourse phenomena may span more than four sentences, potentially diluting scores for longer dependencies. Evidence needed: results varying maximum chunk size (up to 8 or 16) to measure impact on human correlation.

### Open Question 3
How robust is alignment stage when applied to low-resource or morphologically rich languages where segmentation tools are less reliable? Basis: experiments limited to Chinese-English; ablation only compares mature tools assuming high segmentation accuracy. Unresolved because systematic segmentation errors could propagate through Align stage, breaking required one-to-one reconstruction. Evidence needed: evaluation on diverse multilingual test set where segmentation tools vary in quality.

## Limitations
- Memory and latency bottlenecks for very long documents without batching or pruning heuristics
- Computational cost O(m×n) for similarity matrix plus O(k×(m-k+1)) estimator calls
- Framework assumes source sentences are always present, making it blind to legitimate source omissions
- Aggregation method for combining multi-chunk scores remains underspecified

## Confidence

- **High confidence**: Core DP alignment mechanism and chunking approach are well-defined and algorithmically sound
- **Medium confidence**: Pearson correlation (0.929) with MQM is strong but based on single dataset; real-world test set validation provides additional support
- **Low confidence**: Effectiveness of derived RL signals across diverse translation tasks; framework's robustness to different similarity metric choices

## Next Checks

1. Conduct cross-linguistic validation using AFRIDOC-MT dataset to test alignment accuracy and correlation with human judgment beyond Chinese-English

2. Perform ablation studies comparing different similarity metrics (LaBSE vs COMET-Kiwi) and chunk size combinations to isolate contribution of each component

3. Test framework's sensitivity to alignment errors by systematically introducing controlled omissions and reordering in test documents and measuring metric stability