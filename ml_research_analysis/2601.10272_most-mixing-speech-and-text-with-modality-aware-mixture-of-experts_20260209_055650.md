---
ver: rpa2
title: 'MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts'
arxiv_id: '2601.10272'
source_url: https://arxiv.org/abs/2601.10272
tags:
- most
- experts
- text
- speech
- mamoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoST introduces Modality-Aware Mixture of Experts (MAMoE) to integrate
  speech and text processing in a single model. By routing tokens to modality-specific
  expert groups (text or audio) and shared experts for cross-modal interaction, MAMoE
  addresses the limitations of uniform parameter processing across different modalities.
---

# MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts

## Quick Facts
- arXiv ID: 2601.10272
- Source URL: https://arxiv.org/abs/2601.10272
- Authors: Yuxuan Lou; Kai Yang; Yang You
- Reference count: 27
- Key outcome: MoST achieves state-of-the-art or competitive performance on ASR (2.0% WER on LibriSpeech-clean), TTS (6.0% WER), audio language modeling (71.94 average accuracy), and spoken question answering (best results on WebQ).

## Executive Summary
MoST introduces Modality-Aware Mixture of Experts (MAMoE) to integrate speech and text processing in a single model. By routing tokens to modality-specific expert groups (text or audio) and shared experts for cross-modal interaction, MAMoE addresses the limitations of uniform parameter processing across different modalities. The training pipeline adapts a pretrained MoE LLM through ASR/TTS post-training followed by instruction fine-tuning on curated speech-text data. MoST achieves state-of-the-art or competitive performance on ASR, TTS, audio language modeling, and spoken question answering tasks.

## Method Summary
MoST builds on DeepSeek-v2 Lite (64 experts, top-2 routing) by introducing MAMoE, where text and audio tokens are routed to separate expert groups (indices 0-31 for text, 32-63 for audio) while a shared MLP processes all tokens. The model uses frozen HuBERT features for audio and standard token embeddings for text, with modality indicators gating the routing scores. Training occurs in two stages: Stage 1 post-trains on ASR/TTS data (LibriHeavy, Common Voice, VoxPopuli) mixed with RefinedWeb text, then Stage 2 fine-tunes on synthesized speech-text instructions from SmolTalk mixed with ASR/TTS data. The model is trained on 48× A100 GPUs with AdamW optimizer.

## Key Results
- Achieves 2.0% WER on LibriSpeech-clean test-other for ASR, competitive with state-of-the-art models
- Reaches 6.0% WER for TTS on LJSpeech, matching top-performing systems
- Obtains 71.94 average accuracy across audio language modeling benchmarks (sWUGGY, sBLIMP, sTopic-StoryCloze, sStoryCloze)
- Sets new state-of-the-art results on WebQ for spoken question answering while maintaining competitive performance on Llama Q and Trivial QA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modality-aware routing improves performance by preventing representational interference between speech and text.
- **Mechanism:** A modality indicator (m_t) gates the routing scores via element-wise multiplication with a modality-specific mask, ensuring text tokens only compete for text experts and audio tokens only compete for audio experts. Shared experts process all tokens to maintain cross-modal alignment.
- **Core assumption:** Speech and text have inherently different statistical properties that benefit from specialized processing pathways.
- **Evidence anchors:**
  - [abstract] "MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities."
  - [section 6.2] Full MAMoE consistently outperforms MoST-VanillaMoE (standard MoE routing) and MoST-NoShared (no shared experts) across ASR, TTS, ALM, and SQA tasks.
  - [corpus] Mixture-of-Mamba (arXiv:2501.16295) independently validates modality-aware sparsity for multimodal state-space models, suggesting the principle generalizes beyond transformers.

### Mechanism 2
- **Claim:** The two-stage training pipeline (ASR/TTS post-training → instruction tuning) efficiently adapts a text-only MoE LLM to speech-text processing.
- **Mechanism:** Stage 1 primes modality-specific experts on large-scale paired ASR/TTS data (LibriHeavy, Common Voice) with a 0.4/0.4/0.2 ASR/TTS/Text LM mixing ratio. Stage 2 fine-tunes on a curated instruction dataset mixed with 10% ASR/TTS data to prevent catastrophic forgetting.
- **Core assumption:** The pretrained MoE LLM's text capabilities can be preserved while adding speech capabilities through targeted post-training rather than full multimodal pretraining from scratch.
- **Evidence anchors:**
  - [abstract] "The training pipeline adapts a pretrained MoE LLM through ASR/TTS post-training followed by instruction fine-tuning on curated speech-text data."
  - [section 4.2] "This mixed-task learning approach enhances complex instruction-following capabilities while preventing catastrophic forgetting of foundational speech processing abilities."
  - [corpus] DeepOmni (arXiv:2506.21864) uses a similar adaptive modality-specific MoE approach for speech interaction, corroborating the pipeline's effectiveness, though direct comparison is limited.

### Mechanism 3
- **Claim:** Shared experts are critical for cross-modal tasks (ASR/TTS) and preserving text capabilities during multimodal training.
- **Mechanism:** A parallel MLP block (E_shared) processes all tokens regardless of modality, providing a common pathway for information exchange. The final MAMoE output is y_mamoe = y_routed + E_shared(h).
- **Core assumption:** Some knowledge must be shared across modalities for tasks requiring alignment (e.g., speech-to-text transcription).
- **Evidence anchors:**
  - [section 3.3.2] "Shared experts act as a bridge, facilitating knowledge transfer and cross-modal alignment crucial for tasks like Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) synthesis."
  - [section 6.2] MoST outperforms MoST-NoShared across all tasks, with notable gaps in TTS (Figure 5). Text benchmarks (Table 5) show MoST retains 55.4 MMLU vs. MinMo's 58.5, indicating shared experts help preserve general capabilities.
  - [corpus] SMAR (arXiv:2506.06406) emphasizes soft modality-aware routing to preserve language capabilities, aligning with the shared expert design, though direct architectural comparison is absent.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) with Sparse Activation**
  - **Why needed here:** MoST builds on DeepSeek-V2 Lite, an MoE LLM where each token activates only a subset of experts (top-K routing). Understanding load balancing losses, expert capacity, and routing entropy is essential to interpret MAMoE's modifications.
  - **Quick check question:** Can you explain why standard MoE routing might cause some experts to be underutilized, and how auxiliary load balancing losses address this?

- **Concept: Multimodal Representation Alignment**
  - **Why needed here:** MoST uses continuous HuBERT features for audio and standard token embeddings for text, projecting both to d_model=2048. Understanding how different modalities are mapped to a shared space—and why this is harder for speech-text than image-text—is key to appreciating the routing design.
  - **Quick check question:** Why might directly concatenating HuBERT features and text embeddings in a single transformer cause representational interference?

- **Concept: Instruction Tuning and Catastrophic Forgetting**
  - **Why needed here:** MoST's Stage 2 mixes instruction data with ASR/TTS data to prevent forgetting. If you've worked with LLM fine-tuning, this tradeoff should be familiar, but the multimodal context adds complexity (e.g., text-only benchmarks must be preserved).
  - **Quick check question:** What would happen if MoST were fine-tuned on speech-text instructions without mixing in any original ASR/TTS data?

## Architecture Onboarding

- **Component map:**
  - Waveform → Frozen HuBERT encoder → Linear projection → Audio embeddings (H_audio). Text → Tokenizer → Text embeddings (H_text). Modality indicators (0 for text, 1 for audio) are attached to each token.
  - MoST transformer blocks: 27 layers with standard attention, but designated layers replace the FFN with MAMoE. Each MAMoE layer has: (1) modality-specific routed experts (32 text, 32 audio), (2) one shared expert (always active), (3) modality-aware router using m_t to mask gating scores.
  - Output heads: LM head for text, HuBERT token prediction head for audio → HifiGAN vocoder for waveform synthesis.

- **Critical path:**
  1. Understand the standard MoE routing (softmax(h·W_g) → top-K selection).
  2. Trace how modality masking (Algorithm 1, lines 4–12) restricts text tokens to E_text and audio tokens to E_audio.
  3. Note the shared expert is *not* routed—it's always added: y_mamoe = y_routed + E_shared(h).
  4. Follow the two-stage training: Stage 1 (500k steps, 0.4/0.4/0.2 ASR/TTS/Text LM) → Stage 2 (10k steps, 0.4/0.4/0.1/0.1 Speech Instruct/Text Instruct/ASR/TTS).

- **Design tradeoffs:**
  - **50% index-based partition:** Simple but arbitrary. The paper acknowledges clustering-based or knowledge-preserving partitions might be better (Appendix D).
  - **Single shared expert:** Lightweight, but capacity might be insufficient for complex cross-modal tasks. No ablation on shared expert count.
  - **Frozen HuBERT encoder:** Reduces training cost but limits end-to-end optimization. The paper notes this preserves acoustic information better than discrete tokenization.

- **Failure signatures:**
  - **High routing entropy:** Indicates experts aren't specializing. Check modality mask implementation and load balancing loss weight (aux_loss_alpha=0.001).
  - **Degraded text benchmarks:** Suggests shared expert capacity is insufficient or ASR/TTS mixing ratio in Stage 2 is too low.
  - **Poor TTS quality:** Could stem from HuBERT quantizer issues (L11_quantizer_500.pt) or HifiGAN vocoder misalignment.

- **First 3 experiments:**
  1. **Reproduce the controlled comparison (Section 6.1):** Initialize from Llama3.2 3B, train Dense, Traditional MoE, and MoST-style variants on a small ASR/TTS subset. Verify MoST-style outperforms the others before scaling.
  2. **Ablate shared expert capacity:** Replace the single shared MLP with 2 or 4 parallel shared experts. Measure impact on cross-modal tasks (ASR/TTS) and text benchmarks.
  3. **Visualize routing patterns:** Replicate Figure 6 heatmaps on a held-out sample. Confirm text/audio tokens route to their respective expert groups and entropy decreases over training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can clustering-based or knowledge-preserving expert partitioning strategies outperform the index-based 50% split used in MoST?
- **Basis:** [explicit] Appendix D.1 states that the index-based partition is a "simple initialization strategy" and lists clustering-based initialization and knowledge-preserving partitioning as promising future directions.
- **Why unresolved:** The current implementation relies on a statistically neutral, random index split rather than analyzing pretrained expert activation patterns to assign modalities.
- **Evidence:** Comparative studies measuring convergence speed and final performance when experts are grouped by their inherent pre-training specializations versus random assignment.

### Open Question 2
- **Question:** Can the MAMoE architecture effectively generalize to incorporate vision, creating a unified audio-visual-language model?
- **Basis:** [explicit] Appendix D identifies extending the MAMoE concept to additional modalities, "particularly vision," as a primary direction for future work.
- **Why unresolved:** The current architecture and modality-aware routing mechanism are specifically validated only for the speech-text domain.
- **Evidence:** Extending the framework to include visual experts and evaluating performance on multimodal benchmarks (e.g., video QA) without degrading speech or text capabilities.

### Open Question 3
- **Question:** How does the MAMoE architecture scale when applied to significantly larger parameter counts beyond the DeepSeek-v2 Lite base?
- **Basis:** [explicit] Appendix D explicitly calls for investigating the scaling of MoST to larger parameter counts as a key area for further research.
- **Why unresolved:** The empirical results are limited to the "Lite" configuration, leaving the efficiency and capability trade-offs of larger MoE models unexplored.
- **Evidence:** Training larger variants of MoST (e.g., 7B or 70B active parameters) and benchmarking the performance gains relative to the increased computational cost.

## Limitations

- **Scalability concerns:** The paper reports using 48× A100 GPUs but lacks comprehensive runtime and energy consumption metrics, leaving efficiency claims largely theoretical.
- **Limited ablation studies:** The 50/50 expert partition is arbitrary, and the single shared expert's capacity is not explored through ablation, potentially limiting cross-modal transfer.
- **Evaluation scope:** Results are limited to English benchmarks without testing cross-lingual performance or robustness to domain shifts, which are critical for real-world deployment.

## Confidence

**High Confidence:** The core mechanism of modality-aware routing (separating text and audio expert groups while maintaining shared experts for cross-modal alignment) is well-supported by controlled ablation studies showing consistent improvements across all four task categories.

**Medium Confidence:** The claim that MoST is the "first fully open-source speech-text LLM" built on MoE architecture is accurate based on the literature review, though the field evolves rapidly.

**Low Confidence:** Claims about scalability benefits and efficiency gains are weakly supported, as the paper lacks comprehensive runtime analysis and cost per task metrics.

## Next Checks

1. **Ablate Expert Partitioning Strategy:** Implement clustering-based expert partitioning based on initial routing patterns rather than the current 50/50 index split. Compare performance on ASR/TTS tasks to determine if knowledge-aware partitioning improves cross-modal alignment compared to the arbitrary partition.

2. **Scale Shared Expert Capacity:** Replace the single shared expert MLP with 2-4 parallel shared experts while maintaining the same total parameter count. Measure impact on cross-modal task performance (ASR/TTS) and text benchmark retention to determine the optimal balance between routed and shared capacity.

3. **Evaluate Cross-Lingual Generalization:** Test MoST on multilingual speech-text benchmarks (e.g., FLEURS for speech, XL-SR for audio) to assess whether the modality-aware routing provides similar benefits across languages or if performance degrades due to English-centric pretraining.