---
ver: rpa2
title: 'Seeing Culture: A Benchmark for Visual Reasoning and Grounding'
arxiv_id: '2509.16517'
source_url: https://arxiv.org/abs/2509.16517
tags:
- cultural
- visual
- questions
- image
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Seeing Culture Benchmark (SCB) introduces a two-stage evaluation
  framework for cultural reasoning in vision-language models, requiring both visual
  option selection and artifact segmentation as evidence. The benchmark addresses
  limitations in existing cultural datasets by focusing on underrepresented Southeast
  Asian cultures across five categories with 1,065 images and 3,178 questions.
---

# Seeing Culture: A Benchmark for Visual Reasoning and Grounding

## Quick Facts
- arXiv ID: 2509.16517
- Source URL: https://arxiv.org/abs/2509.16517
- Reference count: 40
- Primary result: VLMs perform worst on same-country visual options and best on cross-country options, with a significant gap between visual reasoning accuracy and spatial grounding

## Executive Summary
The Seeing Culture Benchmark (SCB) introduces a two-stage evaluation framework for cultural reasoning in vision-language models, requiring both visual option selection and artifact segmentation as evidence. The benchmark addresses limitations in existing cultural datasets by focusing on underrepresented Southeast Asian cultures across five categories with 1,065 images and 3,178 questions. Evaluation reveals VLMs perform worst on same-country visual options and best on cross-country options, suggesting cultural context aids reasoning. Notably, there's a significant gap between visual reasoning accuracy and spatial grounding, indicating models can select correct answers without adequately justifying them through segmentation.

## Method Summary
The SCB framework evaluates VLMs through two stages: (1) Multiple-choice visual question answering where models select the correct image from four visual options based on a text question, and (2) Visual grounding where models segment the cultural artifact in the correctly selected image. The benchmark contains 1,065 images, 138 concepts, and 3,178 questions across 7 Southeast Asian countries. Questions are organized into three types based on visual option similarity: Type 1 (same-country options), Type 2 (cross-country options), and Type 3 (mixed). Ground truth uses polygon masks, though evaluation uses bounding boxes for compatibility with current VLM capabilities.

## Key Results
- VLMs exhibit their poorest performance when visual options originate from the same country (Type 1), with accuracy significantly lower than cross-country options (Type 2)
- There is a notable performance gap between visual reasoning accuracy and spatial grounding, with models achieving high selection accuracy but low segmentation mIoU
- GPT-o3 achieves over 90% accuracy in visual reasoning but its mIoU score does not surpass 33%, demonstrating the reasoning-grounding gap
- The benchmark reveals current VLMs lack fine-grained cultural knowledge for distinguishing artifacts within the same cultural context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Questions containing explicit country or regional cultural references help VLMs eliminate mismatched visual options, improving selection accuracy.
- Mechanism: When a question mentions a specific cultural context (e.g., "Thai wedding"), VLMs can use textual-visual alignment to reject options showing artifacts from clearly different countries. This contextual filtering reduces the effective search space among distractor images.
- Core assumption: VLMs encode sufficient visual features to distinguish cross-country cultural artifacts (e.g., clothing styles, instrument shapes) even if they cannot distinguish within-country variants.
- Evidence anchors:
  - [abstract] "VLMs perform worst on same-country visual options and best on cross-country options, suggesting cultural context aids reasoning."
  - [section] "This pattern can largely be explained by the contextual clues embedded in the questions that pertain to specific countries or cultures. As a result, VLMs are more adept at eliminating alternative visual options that may include indicators from diverse countries."
  - [corpus] Related benchmarks (VULCA-Bench, RICE-VL) similarly assume cultural context provides discriminative signals, but none directly validate this filtering mechanism empirically.
- Break condition: If questions are stripped of country references, or if visual options from different countries share highly similar visual features, this mechanism would weaken or fail.

### Mechanism 2
- Claim: Visual selection accuracy and spatial grounding (segmentation) are partially decoupled capabilities in current VLMs.
- Mechanism: A VLM can correctly associate a question with an image via high-level semantic matching without forming precise spatial representations of the target artifact. Selection relies more on global image-question similarity; grounding requires fine-grained localization that may not be activated during selection training.
- Core assumption: The training objectives for VQA and segmentation are not sufficiently joint; models may learn shortcuts for selection that bypass detailed spatial reasoning.
- Evidence anchors:
  - [abstract] "There's a significant gap between visual reasoning accuracy and spatial grounding, indicating models can select correct answers without adequately justifying them through segmentation."
  - [section] "While GPT-o3 achieves an accuracy exceeding 90%, its mIoU score does not surpass 33%... grounding by reasoning results in an average drop of 16% in the mean IoU."
  - [corpus] Corpus papers do not directly address this reasoning-grounding gap; evidence is limited to this paper's empirical finding.
- Break condition: If models were trained with explicit joint objectives linking selection to spatial grounding (e.g., requiring segmentation as intermediate supervision), the gap would narrow.

### Mechanism 3
- Claim: Within-culture visual discrimination (Type 1) requires finer-grained cultural knowledge than cross-culture discrimination (Type 2).
- Mechanism: When all visual options belong to the same country and category (e.g., Indonesian wedding artifacts), distinguishing the correct answer requires understanding nuanced symbolic or contextual differences rather than broad visual disparities. This exceeds the cultural knowledge encoded in current VLMs.
- Core assumption: VLMs have limited fine-grained cultural knowledge for underrepresented regions, especially Southeast Asia, due to training data biases.
- Evidence anchors:
  - [section] "VLMs, both open-source and closed-source, exhibit their poorest performance when the visual options originate from the same country."
  - [section] "The dance category primarily features specific dancer characters, while the celebration category encompasses cultural artifacts that represent intangible concepts." (implying variable difficulty based on concept tangibility)
  - [corpus] RICE-VL and related work similarly highlight Western-centric biases in VLMs, indirectly supporting this assumption.
- Break condition: If training data included dense, region-specific cultural annotations, within-culture discrimination would improve, reducing the Type 1 performance penalty.

## Foundational Learning

- Concept: **Visual Question Answering (VQA) with Visual Options**
  - Why needed here: SCB's first stage uses images as answer choices rather than text, requiring understanding of multimodal comparison rather than text-only selection.
  - Quick check question: Given a question "Which image shows a Thai wedding headband?" and four images, can you explain how a model would score each image-question pair?

- Concept: **Semantic Segmentation vs. Bounding Box Grounding**
  - Why needed here: SCB uses polygon segmentation for ground truth but evaluates with bounding box IoU due to current VLM limitations; understanding this gap is critical for interpreting results.
  - Quick check question: If a model outputs a bounding box that covers 80% of a segmented region but includes 30% extra area, how would IoU be affected?

- Concept: **Cultural Knowledge Representation in VLMs**
  - Why needed here: The benchmark assumes VLMs encode cultural associations; understanding how and where this knowledge is stored (or fails to be) informs model improvement.
  - Quick check question: Name two ways a VLM might associate "Barong dance" with Indonesian culture without explicit geographic labels in training.

## Architecture Onboarding

- Component map:
  - Stage 1 Evaluator: Takes (question, 4 image options) → outputs selected option index → accuracy metric
  - Stage 2 Evaluator: Takes (correctly selected image, grounding prompt) → outputs segmentation mask → IoU vs. ground truth polygon (converted to bounding box for compatibility)
  - Question Type Generator: Algorithms 1-3 for Type 1/2/3 option sampling; enforces category consistency, country constraints, and avoid-list exclusions
  - Dataset: 1,065 images / 138 concepts / 3,178 questions organized by country/category/concept hierarchy

- Critical path:
  1. Load image-question pairs with precomputed visual option sets
  2. Run VQA inference; filter only correct responses to Stage 2
  3. Run grounding inference; compute mIoU against ground truth masks
  4. Report accuracy (Stage 1) and mIoU (Stage 2) stratified by type/country/category

- Design tradeoffs:
  - Human-curated questions ensure authenticity but limit scalability; semi-automated generation is proposed for future work
  - Polygon segmentation captures fine details but is reduced to bounding boxes for evaluation due to current VLM capabilities
  - Type 3 questions allow repetition (up to 2x) to balance sample sizes, introducing minor data overlap

- Failure signatures:
  - **Low Type 1 accuracy, high Type 2 accuracy**: Model lacks fine-grained cultural knowledge but can use country-level cues
  - **High accuracy, low mIoU**: Model selects correctly but cannot spatially ground its reasoning (reasoning-grounding gap)
  - **Near-random accuracy across all types (~25%)**: Model has no meaningful cultural knowledge for target regions

- First 3 experiments:
  1. **Baseline evaluation**: Run GPT-4.1, Qwen2.5-VL, and one smaller open-source model (e.g., InternVL2.5) on all three question types; confirm Type 1 < Type 2 accuracy pattern.
  2. **Ablation on question phrasing**: Remove explicit country references from a subset of Type 2 questions; measure whether accuracy drops toward Type 1 levels.
  3. **Grounding-only control**: For correctly answered Type 1 questions, compare IoU when grounding is prompted with the cultural concept name directly vs. the original reasoning-based question; quantify the 16% mIoU gap reported.

## Open Questions the Paper Calls Out

- **Can a semi-automated approach using human-crafted questions as seeds effectively scale the dataset while maintaining cultural authenticity and reasoning complexity?**
  - Basis: Authors note current reliance on human-generated questions is not suitable for scaling and propose semi-automated approaches for future work.
  - Why unresolved: Manual curation by native speakers limits data volume and cultural coverage.
  - What evidence would resolve it: A follow-up study demonstrating comparable performance between models trained on semi-automatically expanded vs. human-curated data.

- **How does VLM performance on the two-stage SCB framework quantitatively compare to human expert performance?**
  - Basis: Authors plan to include human evaluations for comparison in future comprehensive challenges.
  - Why unresolved: Current study evaluates VLMs against ground truth but lacks human baseline for the same tasks.
  - What evidence would resolve it: Human annotators performing the same multiple-choice VQA and segmentation tasks, providing baseline accuracy and mIoU.

- **What specific architectural or training modifications are required to bridge the performance gap between visual reasoning (selecting an option) and spatial grounding (segmenting the artifact)?**
  - Basis: Authors conclude there's a "notable discrepancy between visual reasoning and spatial grounding" and frame SCB as a tool for identifying these shortcomings.
  - Why unresolved: High VQA accuracy (>90%) doesn't correlate with high segmentation quality (mIoU <33%), suggesting current models lack cross-modal mechanisms to link semantic reasoning to pixel-level evidence.
  - What evidence would resolve it: Development of a VLM demonstrating statistically significant reduction in the performance delta between Stage 1 and Stage 2 on SCB.

## Limitations
- The benchmark relies entirely on human-generated questions, which limits scalability and cultural coverage
- Ground truth uses polygon masks but evaluation uses bounding boxes, potentially understating true grounding performance
- Focus on Southeast Asian cultures, while valuable for representation, limits generalizability to other cultural domains

## Confidence

- **High**: The empirical finding that Type 1 (same-country) questions yield lower accuracy than Type 2 (cross-country) questions is directly supported by the data and has clear mechanistic explanation through country-based filtering.
- **Medium**: The reasoning-grounding gap claim is supported by the reported mIoU statistics, but the exact prompting methodology and baseline comparisons for direct grounding are not fully detailed.
- **Low**: The assumption that current VLMs encode insufficient fine-grained cultural knowledge for Southeast Asian artifacts is plausible but not directly measured through knowledge probes or training data analysis.

## Next Checks

1. **Prompt Ablation Study**: Systematically remove country and cultural references from Type 2 questions to test whether accuracy drops toward Type 1 levels, directly validating the cultural context filtering mechanism.
2. **Direct Grounding Comparison**: For a subset of Type 1 questions, compare mIoU when grounding is prompted with the cultural concept name alone versus the full reasoning-based question to quantify the reported 16% gap.
3. **Training Data Analysis**: Analyze the presence and distribution of Southeast Asian cultural artifacts in training corpora of top-performing VLMs (GPT-4.1, Gemini-2.5-Pro) to empirically support or refute the fine-grained knowledge assumption.