---
ver: rpa2
title: Tiny, On-Device Decision Makers with the MiniConv Library
arxiv_id: '2512.19726'
source_url: https://arxiv.org/abs/2512.19726
tags:
- latency
- learning
- decision
- on-device
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiniConv, a library of small convolutional
  encoders designed to compile to OpenGL fragment shaders for deployment on embedded
  GPUs. The core idea is a split-policy architecture where a lightweight on-device
  encoder processes visual observations and transmits a compact feature tensor to
  a remote policy head, reducing bandwidth usage and decision latency compared to
  transmitting raw observations.
---

# Tiny, On-Device Decision Makers with the MiniConv Library

## Quick Facts
- **arXiv ID:** 2512.19726
- **Source URL:** https://arxiv.org/abs/2512.19726
- **Reference count:** 30
- **Primary result:** Introduces MiniConv, a library of small convolutional encoders designed to compile to OpenGL fragment shaders for deployment on embedded GPUs, achieving competitive learning performance while reducing bandwidth and latency in distributed reinforcement learning systems.

## Executive Summary
This paper introduces MiniConv, a library of small convolutional encoders designed to compile to OpenGL fragment shaders for deployment on embedded GPUs. The core idea is a split-policy architecture where a lightweight on-device encoder processes visual observations and transmits a compact feature tensor to a remote policy head, reducing bandwidth usage and decision latency compared to transmitting raw observations. The method is evaluated on three visual control tasks (Walker2d, Hopper, Pendulum) across three edge devices (NVIDIA Jetson Nano, Raspberry Pi 4B, Raspberry Pi Zero 2 W). Learning results show that MiniConv encoders achieve competitive performance compared to a Full-CNN baseline, with modest trade-offs in mean return. System measurements demonstrate that split-policy execution significantly reduces end-to-end decision latency in bandwidth-limited settings (e.g., 540ms to 145ms at 10 Mbps) and improves server scalability (12 to 36 concurrent clients at 10Hz under a 100ms p95 latency budget). The approach provides a practical pathway for deploying RL policies on resource-constrained devices while maintaining learning performance.

## Method Summary
The method uses a split-policy architecture where visual observations are processed by a lightweight on-device encoder (MiniConv) that transmits compact feature tensors to a remote policy head. MiniConv encoders are trained end-to-end with standard RL algorithms (PPO, SAC, DDPG) using Stable-Baselines3 and Gymnasium environments with pixel observations. The encoders are designed to compile to OpenGL fragment shaders for execution on embedded GPUs lacking dedicated ML accelerators. The approach uses 3-frame stacking, spatial downsampling through stride-two layers, and channel projection to create compact representations. System measurements evaluate decision latency, server scalability, and learning performance across three edge devices and three visual control tasks.

## Key Results
- MiniConv encoders achieve competitive learning performance compared to Full-CNN baseline, with modest trade-offs in mean return (e.g., 2384 vs 2543 for Walker2d, K=4)
- Split-policy execution reduces end-to-end decision latency in bandwidth-limited settings (540ms to 145ms at 10 Mbps)
- Server scalability improves from 12 to 36 concurrent clients at 10Hz under a 100ms p95 latency budget
- The latency benefit reverses at higher bandwidths (>50 Mbps), where server-only execution becomes faster

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Split-policy inference reduces end-to-end decision latency when bandwidth is limited relative to the compression ratio and on-device compute cost.
- Mechanism: A lightweight encoder spatially downsamples observations through stride-two layers (reducing X×X to (X/2ⁿ)×(X/2ⁿ)) and projects to K channels, yielding a K·(X/2²ⁿ)-byte transmission versus 4X² bytes for raw RGBA. When transmission time dominates processing time, the smaller payload reduces total latency.
- Core assumption: The link bandwidth B is sufficiently low that B < 32X²(1 - K/(4·2²ⁿ))/j (derived in Section 4.2), where j is on-device processing time and n is the number of stride-two layers.
- Evidence anchors:
  - [abstract]: "reduces end-to-end decision latency in bandwidth-limited settings (e.g., 540ms to 145ms at 10 Mbps)"
  - [section 4.3, Table 5]: Shows crossover behavior—at 10 Mb/s split-policy is 145ms vs 540ms server-only; at 100 Mb/s it reverses to 137ms vs 90ms.
  - [corpus]: Related work on remote inference compression (arXiv:2501.02521) reports similar latency reductions via task-oriented vector quantization, supporting the general principle but not MiniConv specifically.
- Break condition: When bandwidth exceeds the break-even point (~50.4 Mb/s for the Pi Zero 2 W configuration tested) or when on-device compute j increases substantially (larger encoders, higher-resolution inputs), the latency benefit disappears or reverses.

### Mechanism 2
- Claim: Fragment shaders provide a widely portable execution target for small CNN encoders on embedded GPUs that lack dedicated ML accelerators.
- Mechanism: Each convolution or pooling operation is implemented as a shader pass that samples a local neighborhood from an input texture and writes to an output texture. RGBA output provides 4 channels per pass; K>4 requires multiple passes with texture binding constraints.
- Core assumption: The target GPU supports OpenGL ES with sufficient texture sampling budget (64 samples on Pi Zero 2 W) and texture binding slots (8 max on Pi Zero 2 W).
- Evidence anchors:
  - [abstract]: "small convolutional encoders designed to compile to OpenGL fragment shaders for deployment on embedded GPUs"
  - [section 3, Figure 1]: Illustrates the mapping from CNN layers to shader passes via texture sampling.
  - [corpus]: No direct corpus evidence for OpenGL-based RL encoders; related systems use specialized hardware (Coral) or model compression rather than graphics pipeline integration.
- Break condition: Encoders requiring >8 texture bindings or >64 samples per shader cannot execute on devices with these limits; kernel sizes and layer compositions must be constrained accordingly.

### Mechanism 3
- Claim: Moving early visual processing to edge devices reduces per-request server compute, improving server scalability under concurrent load.
- Mechanism: The server receives pre-compressed K-channel feature tensors rather than high-dimensional images, skipping the initial convolutional layers. With fixed per-client decision rates, more clients can be served within the same latency budget.
- Core assumption: Server-side inference time is non-negligible and scales with input dimensionality; the bottleneck is compute rather than I/O at the server.
- Evidence anchors:
  - [abstract]: "improves server scalability (12 to 36 concurrent clients at 10Hz under a 100ms p95 latency budget)"
  - [section 4.4, Table 6]: Quantifies 3× improvement in concurrent clients under the experimental setup.
  - [corpus]: VLAgents (arXiv:2601.11250) similarly addresses policy server efficiency for distributed inference, corroborating the scalability motivation but not the encoder-offload approach.
- Break condition: If the policy head remains computationally heavy or if server batching is inefficient, scalability gains may be smaller than reported.

## Foundational Learning

- **Concept:** Reinforcement learning with visual (pixel) observations
  - Why needed here: The encoder replaces standard CNN feature extractors; understanding how pixel-based RL learns is essential for interpreting the learning results and trade-offs.
  - Quick check question: Can you explain why frame stacking (3 frames) is used and what temporal information it provides to the policy?

- **Concept:** OpenGL fragment shaders and texture memory model
  - Why needed here: The deployment pathway depends on understanding how shaders execute, how textures map to tensors, and what constraints (bindings, samples) apply.
  - Quick check question: Given a shader that outputs RGBA to a texture, how would you implement an 8-channel feature map?

- **Concept:** Bandwidth-latency tradeoffs in distributed inference
  - Why needed here: The core value proposition hinges on when transmission savings outweigh compute overhead; system designers must model this for their deployment context.
  - Quick check question: For a 200×200 input with n=3 stride-two layers and K=4, what is the compression ratio relative to raw RGBA transmission?

## Architecture Onboarding

- **Component map:** Gymnasium environment -> pixel observation -> 100×100 RGB render -> 84×84 crop -> 3-frame stack (9 channels) -> MiniConv encoder -> K-channel feature tensor -> network transmission -> remote policy head -> action output

- **Critical path:**
  1. Observation rendered to 100×100 RGB frame → cropped to 84×84 → normalized to [0,1]
  2. 3-frame stack assembled on device (9 channels)
  3. MiniConv encoder processes stack through n stride-two conv layers (fragment shader passes)
  4. K-channel feature tensor transmitted to server
  5. Policy head produces action → returned to device

- **Design tradeoffs:**
  - **K (output channels):** Lower K reduces bandwidth but may lose task-relevant information; paper shows K=4 works better for locomotion, K=16 for Pendulum.
  - **Number of layers (n):** More layers increase spatial compression (smaller transmission) but increase on-device latency j; drives break-even bandwidth higher.
  - **Input resolution:** Larger inputs increase both j and transmission; Pi Zero 2 W requires X≤500 for 5 FPS.

- **Failure signatures:**
  - **Throttling under sustained load:** Jetson Nano shows increased frame times after initial period under power limits (Figure 3a).
  - **Texture binding overflow:** Encoder designs exceeding 8 bindings fail on Pi Zero 2 W.
  - **Sample budget exceeded:** Shaders requiring >64 texture lookups will not compile or produce incorrect output.
  - **Latency crossover:** Deployments with >50 Mb/s bandwidth may see worse latency than server-only approach.

- **First 3 experiments:**
  1. **Latency break-even measurement:** For your target device and network, measure server-only vs split-policy latency across bandwidth levels (10, 25, 50, 100 Mb/s) to identify the crossover point.
  2. **Encoder capacity sweep:** Train policies with K∈{4, 8, 16} and n∈{2, 3, 4} on your target task to characterize the representation-size vs performance trade-off (single seed initially, multi-seed for final selection).
  3. **Sustained load thermal test:** Run 5,000+ consecutive inferences at target resolution on your deployment device, logging frame time, temperature, and power to verify throttling behavior fits within your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are MiniConv learning results across multiple random seeds and environment variations?
- Basis in paper: [explicit] The authors state: "A limitation of the current learning evaluation is that results are reported for single fixed-seed runs; future work should characterise variance across seeds and environments."
- Why unresolved: Only one fixed-seed run per condition was evaluated, so performance differences between encoders (e.g., K=4 vs K=16) could reflect seed-specific variance rather than true architectural effects.
- What evidence would resolve it: Multi-seed experiments with statistical analysis showing mean returns and confidence intervals across at least 5–10 seeds per condition.

### Open Question 2
- Question: Would integrating image compression shift the bandwidth break-even point at which split-policy inference becomes beneficial?
- Basis in paper: [explicit] In the bandwidth model discussion: "Image compression would shift the break-even point and is left to future work."
- Why unresolved: Both raw observations and encoded features are transmitted as uncompressed uint8 buffers; compression would alter the latency trade-off between transmission and on-device computation.
- What evidence would resolve it: Empirical latency measurements comparing JPEG/WebP-compressed raw frames against uncompressed and compressed feature tensors across bandwidth settings.

### Open Question 3
- Question: Do compact MiniConv feature representations leak recoverable information about raw visual observations?
- Basis in paper: [inferred] The Discussion notes that "compact feature representations can still leak information in principle; stronger privacy claims require explicit objectives or adversarial reconstruction testing."
- Why unresolved: The paper suggests privacy benefits from not transmitting raw frames but does not evaluate whether feature tensors could be inverted to reconstruct visual information.
- What evidence would resolve it: Adversarial reconstruction experiments attempting to recover input frames from transmitted K-channel feature tensors, measuring reconstruction error or perceptual similarity.

### Open Question 4
- Question: How well do MiniConv encoders transfer to real-world camera pipelines with sensor noise, variable lighting, and heterogeneous hardware?
- Basis in paper: [explicit] Future work should "extend evaluation to real camera pipelines and broader environments."
- Why unresolved: All experiments use simulated MuJoCo/Gymnasium rendering with controlled camera settings; real-world deployment may introduce distribution shift and additional latency from camera capture.
- What evidence would resolve it: Benchmarking on physical robot platforms or real camera feeds, measuring both learning performance and end-to-end decision latency including capture overhead.

## Limitations
- Learning results are based on single-seed runs without variance characterization, making performance comparisons uncertain
- Exact MiniConv encoder architecture is not fully specified in the paper and must be inferred from code
- Server hardware specifications for scalability experiments are underspecified (only "Intel CPU and NVIDIA GPU" stated)

## Confidence
- **High confidence** in the latency and scalability measurements, which are well-documented and reproducible through the measurement harness
- **Medium confidence** in the learning performance claims due to single-seed experiments and underspecified encoder architecture
- **Medium confidence** in the mechanism explanations, which are supported by system measurements but rely on simplifying assumptions about network behavior and device constraints

## Next Checks
1. **Latency break-even measurement:** For your target device and network, measure server-only vs split-policy latency across bandwidth levels (10, 25, 50, 100 Mb/s) to identify the crossover point.
2. **Encoder capacity sweep:** Train policies with K∈{4, 8, 16} and n∈{2, 3, 4} on your target task to characterize the representation-size vs performance trade-off (single seed initially, multi-seed for final selection).
3. **Sustained load thermal test:** Run 5,000+ consecutive inferences at target resolution on your deployment device, logging frame time, temperature, and power to verify throttling behavior fits within your latency budget.