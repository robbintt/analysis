---
ver: rpa2
title: When is dataset cartography ineffective? Using training dynamics does not improve
  robustness against Adversarial SQuAD
arxiv_id: '2503.18290'
source_url: https://arxiv.org/abs/2503.18290
tags:
- dataset
- squad
- adversarial
- training
- cartography
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates dataset cartography's effectiveness for
  improving adversarial robustness in extractive question answering using the SQuAD
  dataset. The author analyzes annotation artifacts in SQuAD and evaluates model performance
  on two adversarial datasets (AddSent and AddOneSent) using an ELECTRA-small model.
---

# When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD

## Quick Facts
- **arXiv ID**: 2503.18290
- **Source URL**: https://arxiv.org/abs/2503.18290
- **Reference count**: 8
- **Primary result**: Dataset cartography provides minimal improvement for adversarial robustness in SQuAD-style QA tasks

## Executive Summary
This paper investigates whether dataset cartography—a technique that partitions training examples based on learning dynamics—can improve adversarial robustness for extractive question answering on SQuAD. Using an ELECTRA-small model trained on SQuAD, the author partitions the dataset into easy-to-learn, ambiguous, and hard-to-learn subsets based on confidence and variability metrics tracked during training. The study evaluates model performance on two adversarial datasets (AddSent and AddOneSent) and compares results against random subset baselines. Results show that training on cartography-based subsets provides minimal improvement, with only slight gains on AddOneSent for the hard-to-learn subset. The findings suggest dataset cartography has limited benefit for adversarial robustness in SQuAD-style QA tasks compared to its effectiveness on simpler classification tasks like SNLI.

## Method Summary
The study uses dataset cartography to partition SQuAD v1.1 training data into three subsets based on learning dynamics. An ELECTRA-small model is trained while logging per-example prediction confidence and variability across epochs. These metrics are used to create a data map that divides examples into easy-to-learn, ambiguous, and hard-to-learn regions. Three separate ELECTRA-small models are trained on each subset, and their performance is evaluated on SQuAD validation, AddSent, and AddOneSent adversarial datasets. Results are compared against models trained on random 33% subsets and the full 100% training data.

## Key Results
- Dataset cartography provides little benefit for adversarial robustness in SQuAD-style QA tasks
- Hard-to-learn subset yields slightly higher F1 score on AddOneSent (56.56 vs. 55.14 for random baseline), but gains are limited
- SQuAD exhibits high variance in training dynamics compared to SNLI, reducing the effectiveness of clean partitioning
- No significant improvement observed on AddSent adversarial dataset

## Why This Works (Mechanism)

### Mechanism 1: Training Dynamics-Based Data Partitioning
- Claim: Dataset cartography can partition training examples into meaningful categories based on learning behavior
- Mechanism: Track model confidence and prediction variability across training epochs, then map examples onto a 2D space (variability × confidence) to identify easy-to-learn (high confidence, low variability), ambiguous (high variability, moderate confidence), and hard-to-learn (low confidence, low variability) regions
- Core assumption: Examples with similar training dynamics share similar informational properties relevant to generalization
- Evidence anchors:
  - [abstract]: "Using training dynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn subsets"
  - [section 3]: "By monitoring metrics like confidence, variability, and correctness, dataset cartography constructs a three-dimensional map of the training data. This map divides examples into regions such as easy-to-learn, ambiguous, or hard-to-learn"
  - [corpus]: Limited direct support; neighbor papers discuss training dynamics in multi-task settings but not this specific partitioning mechanism
- Break condition: High variance in training dynamics (as observed in SQuAD vs SNLI) reduces distinction between categories, diminishing effectiveness

### Mechanism 2: Artifact Detection via Learnability Patterns
- Claim: Annotation artifacts may manifest as anomalous learning patterns detectable through cartography
- Mechanism: Easy-to-learn examples with abnormally high confidence early in training may indicate exploitable artifacts; focusing on harder examples may reduce reliance on spurious patterns
- Core assumption: Annotation artifacts create distinct training signatures that differ from genuine learning signal
- Evidence anchors:
  - [abstract]: "dataset cartography provides little benefit for adversarial robustness in SQuAD-style QA tasks"
  - [section 6]: "In SNLI, artifacts arise from human-generated annotations, which often introduce predictable patterns... In contrast, SQuAD artifacts are more nuanced and embedded within the task of predicting exact answers from a passage"
  - [corpus]: Weak corpus support; related papers do not directly validate artifact detection via cartography
- Break condition: When artifacts are nuanced, context-dependent, and embedded in complex reasoning tasks rather than surface-level patterns

### Mechanism 3: Hard-Example Training for Distractor Robustness
- Claim: Training on hard-to-learn examples may selectively improve robustness to certain adversarial perturbations
- Mechanism: Hard-to-learn examples may require more substantive reasoning, aligning better with adversarial tasks involving distracting but plausible sentences
- Core assumption: Hard-to-learn examples capture reasoning patterns transferable to adversarial scenarios
- Evidence anchors:
  - [abstract]: "the hard-to-learn subset yields a slightly higher F1 score on the AddOneSent dataset, the overall gains are limited"
  - [section 5/Table 4]: Hard-to-train subset achieved 56.56 F1 on AddOneSent vs. 55.14 for 33% random baseline (marginal +1.42 improvement)
  - [corpus]: Neighbor paper "Adversarial Question Answering Robustness" studies AddSent but uses different mitigation approaches
- Break condition: Improvement is inconsistent—no gains observed on AddSent or SQuAD validation; effect size is marginal

## Foundational Learning

- **Concept: Dataset Cartography (Swayamdipta et al., 2020)**
  - Why needed here: Core framework for categorizing training examples based on learning dynamics
  - Quick check question: Given a plot of variability vs. confidence across epochs, which region would you identify as "ambiguous" and why?

- **Concept: Adversarial SQuAD (AddSent vs. AddOneSent)**
  - Why needed here: Understanding how each adversarial dataset challenges models differently is essential for interpreting robustness results
  - Quick check question: What is the key difference between AddSent and AddOneSent in how they construct adversarial examples?

- **Concept: Annotation Artifacts in NLP Datasets**
  - Why needed here: The paper's central hypothesis is that cartography mitigates artifacts; understanding what artifacts are is prerequisite
  - Quick check question: Why did the hypothesis-only baseline achieve 67% accuracy on SNLI, and what does this imply about dataset quality?

## Architecture Onboarding

- **Component map:**
  - ELECTRA-small discriminator -> Training dynamics logger -> Data map generator -> Subset partitioner -> Model trainer -> Evaluation harness

- **Critical path:**
  1. Train ELECTRA-small on full SQuAD while logging per-example prediction probabilities per epoch
  2. Compute mean confidence and variability for each training example
  3. Generate data map; partition into three subsets using threshold boundaries
  4. Train three separate models, one per subset
  5. Evaluate all models on standard and adversarial test sets

- **Design tradeoffs:**
  - Subset size (~33% each): Smaller subsets reduce training cost but may lack coverage
  - ELECTRA-small vs. larger models: Efficiency vs. potential ceiling effects on what dynamics reveal
  - Single-run training (no std dev): Faster iteration but reduced statistical confidence (acknowledged limitation)

- **Failure signatures:**
  - High scatter in data map with no clear clustering (see Figure 3 for SQuAD vs. Figure 2 for SNLI)
  - Cartography-based subsets perform similarly to random 33% baseline
  - Inconsistent results across adversarial datasets (gain on AddOneSent, no gain on AddSent)

- **First 3 experiments:**
  1. **Visual diagnostic**: Generate the data map for your target dataset; assess whether clear regions emerge or if points are highly scattered
  2. **Baseline comparison**: Train on cartography-based subsets vs. random subsets of equal size; measure gap on validation and adversarial sets
  3. **Threshold sensitivity**: Vary the confidence/variability thresholds defining each region; test whether results are stable or highly sensitive to boundary choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would adaptations to dataset cartography metrics improve its effectiveness for high-variance datasets like SQuAD?
- Basis in paper: [explicit] "Future research could explore adaptations of dataset cartography to address datasets with high variance or tasks requiring nuanced reasoning, such as question answering."
- Why unresolved: The paper applies standard cartography (confidence/variability) without modifications; whether alternative metrics could better capture meaningful subsets in complex QA tasks remains untested.
- What evidence would resolve it: Designing modified cartography approaches (e.g., incorporating gradient-based measures or multi-span predictions) and demonstrating improved robustness gains on Adversarial SQuAD.

### Open Question 2
- Question: Why does the hard-to-learn subset improve performance on AddOneSent but not AddSent?
- Basis in paper: [inferred] Table 4 shows hard-to-learn achieves 56.56 F1 on AddOneSent (vs. 55.14 baseline), while Table 5 shows no such advantage on AddSent (65.47 vs. 65.99). The paper does not explain this asymmetry.
- Why unresolved: The discussion attributes ineffectiveness to "nuanced artifacts" and "high variance" generally, but does not address why these factors would differentially affect the two adversarial datasets.
- What evidence would resolve it: Analyzing what distinguishes examples where hard-to-learn training helps versus hurts, potentially through error analysis or probing whether AddOneSent distractors require different reasoning than AddSent distractors.

### Open Question 3
- Question: Are the observed performance differences statistically significant given the lack of multiple training runs?
- Basis in paper: [explicit] "Unlike Swayamdipta et al. (2020), I did not train our models multiple times to calculate the standard deviation of our model performance."
- Why unresolved: The marginal improvement on AddOneSent (56.56 vs. 55.14 F1) and variations across subsets could fall within normal training variance, making conclusions uncertain.
- What evidence would resolve it: Running multiple seeds for each subset training condition and reporting means with confidence intervals to determine whether differences exceed statistical noise thresholds.

## Limitations

- High variance in SQuAD training dynamics reduces effectiveness of clean partitioning into easy/ambiguous/hard subsets
- Only one model architecture (ELECTRA-small) tested with limited adversarial dataset evaluation
- Undefined partition thresholds make precise reproduction difficult

## Confidence

- **Primary claim (Low confidence)**: Dataset cartography provides little benefit for adversarial robustness in SQuAD-style QA
- **Secondary claim (Medium confidence)**: SQuAD artifacts are more nuanced than SNLI artifacts
- **Tertiary claim (Medium confidence)**: Hard-to-learn subsets may yield slight improvements on certain adversarial tasks

## Next Checks

1. **Replicate data map analysis**: Generate the variability vs. confidence data map for SQuAD using the described methodology. Verify that the high scatter pattern matches Figure 3 before proceeding with partitioning.
2. **Test multiple architectures**: Repeat the experiment with larger models (e.g., ELECTRA-base or RoBERTa) to assess whether architecture scale affects the effectiveness of dataset cartography for adversarial robustness.
3. **Expand adversarial evaluation**: Test on additional adversarial datasets beyond AddSent and AddOneSent to determine if results are consistent across different types of adversarial perturbations.