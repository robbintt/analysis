---
ver: rpa2
title: 'Reliable Decision Support with LLMs: A Framework for Evaluating Consistency
  in Binary Text Classification Applications'
arxiv_id: '2505.14918'
source_url: https://arxiv.org/abs/2505.14918
tags:
- reliability
- llms
- classification
- llama3
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of established methods for evaluating
  the reliability of large language models (LLMs) in binary text classification. The
  authors introduce a framework that adapts psychometric principles to determine sample
  size requirements, develop metrics for invalid responses, and evaluate intra- and
  inter-rater reliability.
---

# Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications

## Quick Facts
- **arXiv ID:** 2505.14918
- **Source URL:** https://arxiv.org/abs/2505.14918
- **Reference count:** 40
- **Primary result:** Introduces a psychometric framework for evaluating LLM reliability in binary text classification, achieving high consistency across 14 models while revealing task constraints on predictive validity

## Executive Summary
This study addresses the critical gap in established methods for evaluating large language model (LLM) reliability in binary text classification tasks. The authors develop a comprehensive framework that adapts psychometric principles to determine statistically justified sample sizes, employs multiple complementary reliability metrics, and conducts dual-reference validity analysis against both benchmark labels and external market criteria. In a case study classifying financial news sentiment, the framework demonstrates that models can achieve high consistency (90-98% perfect agreement) while smaller models often outperform larger ones, yet all models fail to predict actual market movements, suggesting inherent task constraints rather than model limitations.

## Method Summary
The framework adapts psychometric sample size determination to LLM evaluation, using Gwet's approach to calculate minimum sample sizes (216-1317 articles depending on metric) for binary classification with five replicates. It employs five chance-corrected reliability metrics (Conger's Kappa, Fleiss' Kappa, Gwet's AC1, Brennan-Prediger, Krippendorff's Alpha) to assess both intra- and inter-rater reliability across 14 LLMs. The method uses a structured prompt with chain-of-thought reasoning and two-shot examples, processes 1,350 financial news articles with balanced classes, and evaluates validity against both benchmark sentiment labels and next-day market returns. Invalid responses are handled through both NA-dropped and NA-penalized analyses to ensure robust reliability estimates.

## Key Results
- LLMs achieved high intra-rater consistency with 90-98% perfect agreement and reliability coefficients clustering between 0.93-0.99
- Smaller models (gemma3:1B, llama3.2:3B) outperformed larger counterparts on benchmark accuracy (0.76-0.88) with minimal cost differences
- All models performed at chance level (~0.5) when predicting actual market movements, indicating task constraints rather than model limitations
- Deepseek-r1:1.5B showed lower reliability (~0.85) as the only notable outlier among tested models

## Why This Works (Mechanism)

### Mechanism 1: Psychometric Sample Size Determination Enables Resource-Efficient Evaluation
The framework treats LLM annotation tasks as measurement experiments, borrowing sample size formulas that control the standard error of agreement coefficients. For binary classification with r replicates, the formula n₀ = (z²_α · C) / E²₀ yields n values of 216 (AC1), 847 (PA), and 1317 (Brennan-Prediger) for E₀ = 0.10, α = 0.10, r = 5; the conservative maximum ensures adequate power across all metrics.

### Mechanism 2: Multi-Metric Reliability Aggregation Reduces Single-Index Blind Spots
Using five complementary reliability metrics (Conger's Kappa, Fleiss' Kappa, Gwet's AC1, Brennan-Prediger, Krippendorff's Alpha) mitigates individual metric biases, especially under unbalanced class distributions. Different metrics correct for chance in distinct ways, and requiring high values across all guards against spuriously high agreement driven by class imbalance.

### Mechanism 3: Dual-Reference Validity Analysis Exposes Semantic-Real-World Gaps
Validating LLM annotations against both benchmark labels and external market criteria reveals high semantic consistency but low predictive validity. The framework shows LLMs can achieve 0.76-0.88 accuracy against benchmark labels while performing at chance (~0.5) on actual market prediction, suggesting models capture sentiment semantics but not causal market dynamics.

## Foundational Learning

- **Concept:** Intra- vs. Inter-Rater Reliability
  - Why needed: The framework evaluates both within-model consistency and across-model agreement, essential for selecting dependable annotators and understanding whether multiple LLMs converge
  - Quick check: Can you explain why a model could have high intra-rater reliability but low inter-rater reliability with other models?

- **Concept:** Chance-Corrected Agreement Metrics
  - Why needed: Simple percent agreement can be inflated by class imbalance; metrics like Cohen's Kappa, Gwet's AC1, and Krippendorff's Alpha adjust for expected chance agreement
  - Quick check: Given a binary classification task with 90% positive labels, would simple accuracy or a chance-corrected metric be more informative for assessing rater agreement?

- **Concept:** Validity vs. Reliability in Measurement
  - Why needed: High reliability does not guarantee validity; the case study shows LLMs can be consistent yet fail to predict real outcomes
  - Quick check: If an LLM achieves 95% agreement with a benchmark label set but 50% accuracy predicting actual outcomes, which type of evaluation (reliability or validity) does each number represent?

## Architecture Onboarding

- **Component map:** Planning Phase (Sample size calculator → LLM selector → Prompt designer) → Data Collection Phase (Dataset curator → LLM interaction layer → Replication controller) → Reliability Analysis Phase (Agreement scorers → NA-handlers → Coefficient aggregators) → Validity Analysis Phase (Benchmark comparator → External criterion evaluator → Ensemble validator)

- **Critical path:** Sample size determination → Prompt design → Multi-replicate data collection → Intra-rater reliability check → (if passing) Inter-rater reliability check → Benchmark validation → External criterion validation

- **Design tradeoffs:** Conservative sample sizing (Brennan-Prediger's n=1317) increases confidence but raises compute cost vs. smaller n (AC1's 216); NA-dropped analysis gives optimistic reliability while NA-penalized penalizes invalid responses

- **Failure signatures:** Low intra-rater reliability (e.g., deepseek-r1:1.5B at ~0.85) → model is unstable; high benchmark agreement but chance-level external criterion → task may be semantically well-defined but not predictive

- **First 3 experiments:** 1) Small-scale pilot (≤50 examples) with 2-3 candidate LLMs to validate prompt parsing; 2) Intra-rater reliability experiment on n_final examples with 5 replicates per model; 3) Dual validity check against benchmark labels and external criterion

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt wording and few-shot examples are not fully specified, making faithful replication challenging
- Psychometric sample size determination relies on assumptions that may not hold for LLM behavior due to shared training data or temperature effects
- External validity comparison assumes market returns are influenced by article sentiment, but numerous confounding factors may dominate

## Confidence
- **High Confidence:** Methodological framework for multi-metric reliability assessment and empirical finding that smaller models can outperform larger ones
- **Medium Confidence:** Psychometric sample size determination approach and its claimed resource efficiency
- **Low Confidence:** Interpretation that all models performing at chance on market prediction indicates "task constraints rather than model limitations"

## Next Checks
1. **Prompt Sensitivity Analysis:** Systematically vary the prompt wording, few-shot examples, and chain-of-thought instructions across 3-5 alternative versions to identify which elements most strongly affect model consistency.

2. **Cross-Domain Reliability Transfer:** Apply the same framework to a non-financial binary classification task (e.g., product review sentiment or medical note classification) to compare whether smaller models consistently outperform larger ones.

3. **Benchmark Label Quality Assessment:** Conduct a small-scale human annotation study on a subset of articles to independently verify the StockNewsAPI labels and determine if the benchmark itself is a reliable reference standard.