---
ver: rpa2
title: 'GradPower: Powering Gradients for Faster Language Model Pre-Training'
arxiv_id: '2505.24275'
source_url: https://arxiv.org/abs/2505.24275
tags:
- adam
- adampower
- loss
- learning
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GradPower is a lightweight gradient transformation technique that
  accelerates language model pre-training by applying an elementwise sign-power transformation
  to gradients before feeding them to a base optimizer. Applied to Adam (AdamPower),
  it consistently achieves lower terminal loss across diverse architectures (LLaMM,
  Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate
  schedules (cosine, warmup-stable-decay).
---

# GradPower: Powering Gradients for Faster Language Model Pre-Training

## Quick Facts
- arXiv ID: 2505.24275
- Source URL: https://arxiv.org/abs/2505.24275
- Reference count: 40
- Primary result: Elementwise sign-power gradient transformation (p=1.2) consistently accelerates LLM pre-training across architectures, scales, and schedules

## Executive Summary
GradPower is a lightweight gradient transformation technique that accelerates language model pre-training by applying an elementwise sign-power transformation to gradients before feeding them to a base optimizer. Applied to Adam (AdamPower), it consistently achieves lower terminal loss across diverse architectures (LLaMM, Qwen2MoE), parameter scales (66M to 2B), datasets (C4, OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The most pronounced gains occur with mixture-of-experts models and warmup-stable-decay schedules. GradPower also integrates seamlessly with other state-of-the-art optimizers like Muon, yielding further improvements. Theoretically, it is shown to accelerate convergence in both low- and high-noise regimes by enhancing slow dynamics along flat directions critical for loss reduction.

## Method Summary
GradPower applies an elementwise sign-power transformation φ_p(g) = sign(g_i)|g_i|^p to gradients before passing them to a base optimizer like Adam. The transformation amplifies effective update magnitude along flat optimization directions under high gradient noise conditions, particularly benefiting mixture-of-experts models and warmup-stable-decay schedules. The technique is orthogonal to and composable with other optimizer improvements, requiring no modifications to internal optimizer state or hyperparameters.

## Key Results
- Consistent terminal loss improvements across LLaMA and Qwen2MoE architectures (0.022-0.028 absolute reduction)
- Most pronounced gains on MoE models with warmup-stable-decay schedules
- Optimal performance at power p=1.2 for standard batch sizes (512)
- Seamless integration with other optimizers like Muon yields additive improvements
- Reduced training instability and improved scaling laws

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Elementwise power transformation with p > 1 amplifies effective update magnitude along flat optimization directions under high gradient noise conditions.
- **Mechanism:** The sign-power transformation φ_p(g) = sign(g_i)|g_i|^p is applied before the base optimizer. For small gradient magnitudes typical in flat directions, p > 1 reduces both the numerator (first moment) and denominator (second moment) in Adam's adaptive update, but the denominator reduction dominates, yielding larger net updates along these critical directions.
- **Core assumption:** Gradient noise follows approximately symmetric distributions around the true gradient, and flat directions carry disproportionate importance for loss reduction in LLM training landscapes.
- **Evidence anchors:**
  - [abstract] "we provide theoretical analyses that reveal the underlying mechanism of GradPower and highlights the influence of gradient noise"
  - [Section 4.1, Proposition 4.3] In high-noise regime (μ ≪ σ), optimal p satisfies p* = Θ(log(ε log(1/σ))/log σ), with p* > 1 when ε log(1/σ) < 1
  - [corpus] SPAM paper confirms gradient spikes cause training instability, suggesting noise structure matters for optimizer design
- **Break condition:** When gradient noise is low (large batch regimes), p < 1 becomes optimal; using p > 1 would reduce update magnitude unnecessarily.

### Mechanism 2
- **Claim:** GradPower's benefit is architecture-dependent, with strongest gains on MoE models under warmup-stable-decay (WSD) schedules.
- **Mechanism:** MoE training exhibits higher gradient noise due to sparse expert routing. WSD schedules maintain constant high learning rate during the stable phase, where GradPower's noise-dependent amplification provides cumulative benefit over extended training.
- **Core assumption:** MoE gradient noise structure is sufficiently uniform across parameters that a single global p value remains effective.
- **Evidence anchors:**
  - [abstract] "The most pronounced gains are observed when training modern mixture-of-experts models with warmup-stable-decay schedules"
  - [Section 3.3] "the absolute improvement achieved by AdamPower on Qwen2MoE-2B (0.028) is more significant than that on LLaMA-2B (0.022)"
  - [corpus] Universal Dynamics of WSD paper establishes WSD effectiveness beyond transformers, supporting schedule-optimizer interactions
- **Break condition:** If gradient noise distribution becomes highly non-uniform across parameters (e.g., extreme expert imbalance), elementwise transformation may need per-layer p adaptation.

### Mechanism 3
- **Claim:** GradPower is orthogonal to and composable with other optimizer improvements (Blockwise LR, Muon).
- **Mechanism:** GradPower operates as a gradient preprocessor before any base optimizer, requiring no modifications to internal optimizer state or hyperparameters. Gains from different techniques can accumulate additively.
- **Core assumption:** The base optimizer's internal mechanics (momentum, adaptive rates, preconditioning) remain effective on transformed gradients.
- **Evidence anchors:**
  - [abstract] "GradPower also integrates seamlessly with other state-of-the-art optimizers, such as Muon, yielding further improvements"
  - [Section 3.4] "AdamPower with Blockwise LR achieves a lower terminal loss than both AdamPower and Adam with Blockwise LR individually. The observed improvement (∼0.45) is nearly the sum of the gains"
  - [corpus] Effective Quantization of Muon Optimizer States confirms Muon's convergence advantages, providing a complementary baseline for composition testing
- **Break condition:** If the base optimizer makes explicit assumptions about gradient scale (e.g., trust-region methods with adaptive trust radius), transformed gradients may violate those assumptions.

## Foundational Learning

- **Concept: Edge of Stability (EoS) dynamics**
  - Why needed here: GradPower's theoretical justification relies on understanding "fast-slow" dynamics where oscillations occur along sharp directions while steady progress occurs along flat "river" directions.
  - Quick check question: Can you explain why gradient descent can remain stable despite the largest eigenvalue of the Hessian exceeding 2/learning_rate?

- **Concept: Signal-to-noise ratio in stochastic gradients**
  - Why needed here: The optimal power p depends critically on σ/μ (noise-to-signal ratio). Section 3.5 shows p decreases as batch size increases.
  - Quick check question: If you double the batch size from 512 to 1024, would you expect the optimal p to increase or decrease, and why?

- **Concept: Adam's adaptive update mechanics**
  - Why needed here: GradPower's benefit emerges from how the power transformation affects the ratio m_t/(√v_t + ε), not just the raw gradient.
  - Quick check question: Why does linear gradient rescaling (multiplying all gradients by constant c) not change Adam's update direction?

## Architecture Onboarding

- **Component map:** Raw gradient g → [GradPower: φ_p(g)] → Powered gradient g' → [Base optimizer (Adam/Muon)] → Parameter update
- **Critical path:**
  1. Compute mini-batch gradient as usual
  2. **Before** passing to optimizer, apply: `g = torch.sign(g) * torch.abs(g).pow(p)`
  3. Feed transformed gradient to unmodified base optimizer
  4. Single hyperparameter to tune: p (paper recommends p=1.2 as default)

- **Design tradeoffs:**
  - Higher p: More aggressive noise suppression, better for small batches/high noise; risk of over-compressing informative gradients in low-noise regimes
  - Lower p (<1): Better for large batches; may not provide sufficient noise filtering
  - p=1: Degenerates to standard gradient (identity transformation)

- **Failure signatures:**
  - Training loss spikes with p >> 1.2 on large-batch training (over-amplification of remaining noise)
  - No improvement over baseline with p ≈ 1 (effectively disabled)
  - Worse performance with p < 1 on small-batch training (under-correction for noise)

- **First 3 experiments:**
  1. **Sanity check:** Train small model (e.g., 100M params) on C4 subset with Adam vs. AdamPower (p=1.2) for ~10K steps. Expect visible loss gap by step 5K.
  2. **Hyperparameter sweep:** On same setup, sweep p ∈ {0.8, 1.0, 1.2, 1.4, 1.6} to verify p≈1.2 is optimal for batch size 512. Compare against Section 3.1 Figure 2 curves.
  3. **Batch size interaction:** At batch sizes 512, 2048, 4096, 8192, identify optimal p for each. Verify that optimal p decreases as batch size increases (per Section 3.5 Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GradPower yield particularly significant gains for Mixture-of-Experts (MoE) architectures and warmup-stable-decay (WSD) schedules compared to dense models?
- Basis in paper: [explicit] The conclusion asks to "investigate why AdamPower exhibits particular potential for MoE models and wsd LR scheduler."
- Why unresolved: While Section 3.3 notes reduced loss spikes in MoEs, the paper provides no theoretical or ablation evidence explaining the specific interaction between the power transformation and the unique dynamics of MoE/WSD training.
- What evidence would resolve it: Analysis of Hessian spectra or gradient noise distributions specific to MoE models to determine if GradPower dampens routing instabilities or exploits distinct landscape geometry.

### Open Question 2
- Question: Does the sign-power transformation explicitly suppress vibrations along sharp (valley) directions to reduce training instability?
- Basis in paper: [explicit] Section 3.3 states, "We hypothesize that the gradient power transformation in AdamPower may help suppress the vibrations along these directions. We leave a detailed investigation of this phenomenon to future work."
- Why unresolved: The paper observes reduced loss spikes empirically but does not measure the "fast vibrations" or provide theoretical proof that the transformation alters optimizer dynamics in the sharp directions of the loss landscape.
- What evidence would resolve it: Trajectory analysis measuring oscillation amplitudes along high-curvature directions in GradPower vs. baseline Adam.

### Open Question 3
- Question: Can GradPower improve convergence and scaling laws in non-LLM domains such as computer vision or reinforcement learning?
- Basis in paper: [explicit] The conclusion states, "exploring the applicability of GradPower beyond LLMs... could further extend its impact."
- Why unresolved: All empirical evaluations in Sections 3.2 through 3.5 are restricted to decoder-only LLM architectures (LLaMA, Qwen2MoE) on language tasks.
- What evidence would resolve it: Benchmarks on standard vision tasks (e.g., ResNet/ViT on ImageNet) demonstrating whether the optimal power $p$ transfers or requires re-tuning for different data modalities.

## Limitations
- Empirical scope primarily limited to transformer-based architectures with standard positional encoding schemes
- Optimal power parameter p=1.2 is derived for batch size 512 and may require adjustment for extreme batch sizes
- Theoretical analysis assumes symmetric gradient noise distributions which may not capture all real-world noise structures

## Confidence

- **High Confidence:** Claims about consistent terminal loss improvements across architectures and datasets; mechanism of noise-dependent amplification in high-noise regimes; composability with other optimizers
- **Medium Confidence:** Theoretical convergence acceleration bounds; optimal power parameter selection (p=1.2); scaling law improvements
- **Low Confidence:** Architecture-agnostic performance guarantees; optimal p values for all batch sizes and learning rate schedules; interaction effects with all potential optimizer combinations

## Next Checks

1. **Batch Size Sensitivity Test:** Systematically vary batch size from 128 to 8192 while tracking optimal p values to validate the noise-dependent mechanism and identify any regime shifts in optimal transformation parameters.

2. **Architecture Transfer Test:** Apply GradPower to non-transformer architectures (e.g., MLP-Mixer, ConvNeXt) to verify the claimed architecture-agnostic benefits and identify any architecture-specific failure modes.

3. **Long-Horizon Stability Test:** Extend training beyond typical pre-training horizons (20× parameters) to 50-100× parameters to verify that GradPower's stability benefits persist over extended training and do not introduce long-term pathological behaviors.