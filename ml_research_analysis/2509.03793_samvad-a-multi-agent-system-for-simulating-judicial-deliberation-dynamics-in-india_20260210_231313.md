---
ver: rpa2
title: 'SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics
  in India'
arxiv_id: '2509.03793'
source_url: https://arxiv.org/abs/2509.03793
tags:
- legal
- agents
- deliberation
- judicial
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SAMVAD, a multi-agent system that simulates
  judicial deliberation in the Indian legal context using large language models (LLMs)
  and retrieval-augmented generation (RAG). The system models a judge, prosecution
  and defense counsels, and adjudicators deliberating on criminal cases grounded in
  Indian legal documents like the IPC and Constitution.
---

# SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation Dynamics in India

## Quick Facts
- arXiv ID: 2509.03793
- Source URL: https://arxiv.org/abs/2509.03793
- Authors: Prathamesh Devadiga; Omkaar Jayadev Shetty; Pooja Agarwal
- Reference count: 13
- Primary result: Multi-agent system simulating Indian judicial deliberation with RAG integration achieves high consensus and improved legal grounding

## Executive Summary
This paper presents SAMVAD, a multi-agent system that simulates judicial deliberation in the Indian legal context using large language models (LLMs) and retrieval-augmented generation (RAG). The system models a judge, prosecution and defense counsels, and adjudicators deliberating on criminal cases grounded in Indian legal documents like the IPC and Constitution. The key innovation is integrating RAG with a vector-based legal knowledge base to provide source-cited, legally precise reasoning for the judge and counsel agents. Simulation results across five cases showed high adjudicator participation, strong consensus (>90% agreement), and improved deliberation quality with RAG enabled. Ablation studies confirmed RAG consistently improved grounding scores (0.21→0.42) and verdict consistency. This demonstrates SAMVAD's effectiveness in producing explainable, legally-grounded judicial simulations.

## Method Summary
SAMVAD implements a modular Python system where agents (Judge, Prosecution Counsel, Defense Counsel, and multiple Adjudicators) interact through an Orchestrator to simulate judicial deliberation. The system uses ChromaDB as a vector store populated with Indian legal texts (IPC, Constitution, CrPC). Judge and Counsel agents employ RAG to retrieve relevant legal provisions before generating arguments. Adjudicators deliberate in multiple rounds, updating their leanings based on peer arguments until reaching consensus. The system was tested with various LLMs including Qwen-2.5-7B, LLaMA-3-8B, Mistral-7B, and Gemma-7B across five criminal case scenarios.

## Key Results
- RAG integration improved argument grounding scores from 0.21 to 0.42 (roughly doubling them)
- Verdict consistency increased significantly with RAG enabled across ablation studies
- Cases reached consensus in 1-2 deliberation rounds with >90% agreement ratios
- System demonstrated high adjudicator participation and balanced deliberation dynamics

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Grounding for Legal Precision
Integrating RAG with a domain-specific vector database reduces hallucinations and improves the legal validity of agent arguments compared to parametric knowledge alone. When Judge or Counsel agents require legal context, the system converts their query into an embedding, performs a similarity search against a vector store populated with the Indian Penal Code (IPC) and Constitution, and injects the retrieved chunks into the LLM context window. This forces the generation step to cite specific source text. The core assumption is that semantic search correctly maps case facts to relevant legal statutes, and the LLM can successfully synthesize these retrieved chunks into coherent arguments. Evidence shows RAG improved grounding scores (0.21→0.42) and verdict consistency. Break condition: colloquial case facts failing to match formal legal terminology may lead to irrelevant statute retrieval.

### Mechanism 2: Role-Based Adversarial Dialectics
Simulating distinct, opposing roles (Prosecution vs. Defense) creates a dialectic process that surfaces relevant facts and legal interpretations that a single monolithic reasoning agent might miss. Separate agent instances are prompted with identical case files but diametrically opposed objectives (conviction vs. acquittal). Both are augmented by RAG to find statutes supporting their specific bias. The Adjudicators then observe this conflict to form a more "balanced" view. The core assumption is that the LLM can maintain the persona and objective of a specific legal role without "drifting" toward the opposing argument. Evidence shows this architecture produces balanced deliberations, but break condition: if the "Judge" agent's instructions are not impartial, or if one Counsel agent fails to retrieve key evidence, the dialectic becomes asymmetrical.

### Mechanism 3: Iterative Consensus Convergence
Multi-round deliberation among Adjudicator agents increases consensus stability by allowing "undecided" agents to integrate peer reasoning before finalizing a verdict. Adjudicators emit a state (Guilty/Not Guilty/Undecided) and a justification. In subsequent rounds, they consume the justifications of peers. The Orchestrator terminates the loop only when an agreement threshold (e.g., >80%) is met. The core assumption is that LLMs are capable of updating "beliefs" (outputs) based on new textual context (peer arguments) in a way that mimics rational consensus-building rather than random oscillation. Evidence shows cases reached consensus in 1-2 rounds, but break condition: if peer arguments are persuasive but legally unsound, the consensus mechanism may amplify errors rather than correct them.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Standard LLMs lack up-to-date or jurisdiction-specific legal knowledge and often hallucinate case law. RAG anchors the SAMVAD system in the specific text of the Indian Penal Code.
  - **Quick check question:** If the vector database contained only the Constitution and not the IPC, would the system still be able to judge criminal cases accurately?

- **Concept: Agent Orchestration (MAS)**
  - **Why needed here:** Judicial deliberation is a process, not a single inference. You need a state machine (The Orchestrator) to manage the sequence of Opening -> Arguments -> Deliberation -> Verdict.
  - **Quick check question:** Why can't the "Judge" agent simply generate the verdict immediately after reading the case file? What is lost without the deliberation loop?

- **Concept: Semantic Similarity Search**
  - **Why needed here:** To implement the RAG mechanism, one must understand how text is converted to vectors and how "distance" implies relevance. This is the bridge between the case facts and the legal knowledge base.
  - **Quick check question:** If a case involves "theft," but the IPC uses the term "larceny" or specific sections (e.g., 378 IPC), how does the vector search ensure a match?

## Architecture Onboarding

- **Component map:** Case JSON File -> Orchestrator -> Judge Agent -> Counsel Agents (2) -> Adjudicator Agents (N) -> ChromaDB Vector Store -> LLM Interface

- **Critical path:**
  1. **Initialization:** Load JSON case file -> Connect to ChromaDB
  2. **Preparation:** Judge Agent retrieves relevant laws (RAG) -> Generates instructions
  3. **Argumentation:** Pros/Cons Agents retrieve precedents/laws (RAG) -> Generate arguments
  4. **Deliberation:** Adjudicators read Instructions + Arguments -> Generate Leanings
  5. **Loop:** If consensus < threshold, repeat Deliberation with previous leanings in context

- **Design tradeoffs:**
  - **Latency vs. Robustness:** Running 5 Adjudicators + Judge + Counsel requires ~8 LLM calls per round (minimum). This is slow and expensive compared to single-prompt classification but provides the "simulation" value.
  - **RAG Precision:** The system chunks legal texts. Small chunks lose context; large chunks introduce noise. The paper does not specify chunk size, requiring tuning.

- **Failure signatures:**
  - **Hung Panel:** Agents oscillate between Guilty/Not Guilty for >10 rounds (Consensus threshold too high or case too ambiguous)
  - **Citation Hallucination:** Agent invents a case law not present in the vector store (RAG retrieval failure)
  - **Context Overflow:** Case files + Retrieved Laws + Peer Arguments exceed the LLM's context window, causing truncation of early arguments

- **First 3 experiments:**
  1. **RAG Ablation:** Run a single case with RAG enabled vs. disabled (parameters: grounding score). Verify the 0.21 -> 0.42 delta reported in Table 2.
  2. **Consensus Threshold Sensitivity:** Run a complex case with consensus thresholds set at 60%, 80%, and 100% to measure how strictly "Hung Panels" occur.
  3. **Model Robustness:** Swap the backing LLM (e.g., GPT-4 vs. Mistral-7B) while keeping the case file constant to evaluate reasoning consistency across model families.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the keyword-frequency-based Argument Grounding Score correlate with qualitative human expert assessments of legal reasoning quality?
- **Basis in paper:** Section 5.1 defines the Argument Grounding Score based on the "frequency of predefined case keywords," a metric that may fail to capture semantic nuance or logical coherence.
- **Why unresolved:** Lexical overlap metrics often struggle to accurately evaluate the complex, inferential reasoning required in legal deliberation.
- **What evidence would resolve it:** A comparative study benchmarking SAMVAD's automated grounding scores against blind evaluations performed by legal professionals.

### Open Question 2
- **Question:** How does the absence of case law precedents in the knowledge base impact the system's ability to simulate stare decisis (binding precedent)?
- **Basis in paper:** Section 3.4 explicitly limits the knowledge base to statutory documents (IPC, Constitution, CrPC), omitting the judicial precedents that heavily influence Indian court rulings.
- **Why unresolved:** In common law systems like India, judges often rely on past judgments (case law) to interpret statutes; relying solely on the text of the law may limit realism.
- **What evidence would resolve it:** Ablation studies comparing the current system against a version where the RAG pipeline is augmented with a corpus of landmark Supreme Court judgments.

### Open Question 3
- **Question:** Can the current consensus algorithm accurately simulate judicial deadlock or "hung panel" scenarios in cases with high factual ambiguity?
- **Basis in paper:** Results in Table 1 show rapid consensus (often 1.00 agreement ratio in 1-2 rounds), suggesting the system may converge too easily compared to real-world deliberations.
- **Why unresolved:** The high consensus rates imply the simulation might lack the mechanisms to model persistent disagreement or minority dissent effectively.
- **What evidence would resolve it:** Testing the system on a dataset of historically contentious or "split verdict" cases to observe if the agents can maintain distinct positions without premature convergence.

## Limitations
- Legal knowledge base coverage limited to statutory documents (IPC, Constitution, CrPC) without case law precedents
- Grounding score methodology lacks transparency in keyword extraction and frequency calculation
- Consensus mechanism may amplify persuasive but legally unsound arguments ("Groupthink")
- Limited case sample size (n=5) prevents strong generalization claims

## Confidence
- **High Confidence:** RAG improves grounding scores (0.21→0.42) and verdict consistency - directly supported by ablation results in Table 2
- **Medium Confidence:** Multi-agent deliberation produces more balanced verdicts than single-agent judgment - supported by participation metrics but lacks comparative baselines
- **Low Confidence:** The system can generalize to diverse Indian criminal cases beyond the five tested - limited case sample size (n=5) prevents strong generalization claims

## Next Checks
1. **Legal Expert Review:** Have practicing Indian lawyers evaluate SAMVAD's verdicts on 10+ new criminal cases for legal soundness, not just consensus rates
2. **RAG Retrieval Audit:** Test retrieval relevance by providing case facts with synonyms/idioms and verifying the system retrieves correct IPC sections (e.g., "theft" vs. formal legal terminology)
3. **Consensus Stability Test:** Run adversarial cases where one Counsel agent is given clearly weaker arguments to measure if Adjudicators can identify and discount poor reasoning despite high consensus thresholds