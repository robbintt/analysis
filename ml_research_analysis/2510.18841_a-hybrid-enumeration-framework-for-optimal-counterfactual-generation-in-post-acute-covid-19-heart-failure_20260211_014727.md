---
ver: rpa2
title: A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute
  COVID-19 Heart Failure
arxiv_id: '2510.18841'
source_url: https://arxiv.org/abs/2510.18841
tags:
- counterfactual
- features
- framework
- clinical
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a hybrid counterfactual inference framework
  that integrates exhaustive enumeration and optimization-based search to generate
  patient-specific risk interventions for post-acute COVID-19 heart failure (PASC-HF).
  Using EHR data from over 2,700 patients with pre-existing heart failure and confirmed
  SARS-CoV-2 infection, the framework identifies modifiable clinical features that
  could alter predicted risk.
---

# A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure

## Quick Facts
- arXiv ID: 2510.18841
- Source URL: https://arxiv.org/abs/2510.18841
- Reference count: 14
- Primary result: Hybrid framework identifies modifiable comorbidities (hypertension, CKD, diabetes) that could reduce predicted PASC-HF risk from 72% to 15% in individual patients

## Executive Summary
This study introduces a hybrid counterfactual inference framework that combines exhaustive enumeration and optimization-based search to generate patient-specific risk interventions for post-acute COVID-19 heart failure (PASC-HF). Using EHR data from over 2,700 patients with pre-existing heart failure and confirmed SARS-CoV-2 infection, the framework identifies modifiable clinical features that could alter predicted risk. The predictive model achieved strong discrimination (AUROC = 0.88, 95% CI: 0.84-0.91), and counterfactual analyses revealed that modifying comorbidities such as hypertension, chronic kidney disease, or diabetes could substantially reduce predicted PASC-HF risk. This approach bridges individualized prediction and causal reasoning, providing interpretable, actionable insights for clinical decision-making and hypothesis generation in complex biomedical systems.

## Method Summary
The framework uses a hybrid algorithm that first trains a gradient-boosted classifier on temporal EHR features (diagnoses, labs, medications) extracted using TLDR. For counterfactual generation, it identifies binary actionable features and exhaustively enumerates all 2^m - 1 non-trivial combinations when m ≤ 16, scoring candidates by sparsity versus probability shift. When enumeration is infeasible (m > 16 or features are non-binary), it falls back to NICE (searching nearest training instances of target class) then MOC (genetic algorithm for synthetic counterfactuals). The method guarantees optimal counterfactuals under enumeration conditions while providing principled approximations otherwise.

## Key Results
- Predictive model achieved AUROC = 0.88 (95% CI: 0.84-0.91) for PASC-HF risk
- Exhaustive enumeration over binary features guarantees optimal counterfactuals when m ≤ 16
- Counterfactual analysis showed comorbidity modifications could reduce predicted risk from 72% to 15% in individual patients
- Framework bridges individualized prediction with interpretable intervention recommendations

## Why This Works (Mechanism)

### Mechanism 1
Exhaustive enumeration over binary actionable features guarantees discovery of optimal sparse counterfactuals when the feature count is tractable. The algorithm identifies binary features (|unique(x_j)| = 2) and, when m ≤ m_max (default 16), enumerates all 2^m - 1 non-trivial combinations. Each candidate is scored by S(x') = α·|S| - β·|f_y*(x') - f_y*(x_0)|, balancing sparsity (fewer changes) against probability shift toward the target class. The lowest-scoring valid candidates are returned. Core assumption: Binary feature toggles represent clinically meaningful and achievable interventions; the classifier's probability estimates remain reliable under feature perturbation.

### Mechanism 2
A two-stage optimization fallback (NICE → MOC) provides principled approximation when enumeration is infeasible or features are non-binary. Stage 1 (NICE) searches for nearest training instances of the target class using Gower distance, restricted to match x_0 on all fixed features. If NICE fails due to insufficient restricted samples, Stage 2 (MOC) uses a genetic algorithm to evolve synthetic counterfactuals optimizing proximity, sparsity, and target probability simultaneously, returning Pareto-optimal solutions. Core assumption: The training distribution contains representative counterfactual instances (for NICE), or synthetic perturbations remain within clinically plausible regions (for MOC).

### Mechanism 3
Patient-specific counterfactuals reveal that modifying comorbidities (hypertension, CKD, diabetes) could substantially reduce predicted PASC-HF risk. The trained classifier (AUROC = 0.88) outputs risk probabilities. Counterfactual inference toggles binary comorbidity features from present to absent, recomputes predicted probability, and quantifies risk reduction. Example: changing "diabetes present" to "diabetes absent" reduced one patient's predicted risk from 72% to 15%. Core assumption: The association between comorbidity presence and PASC-HF risk reflects a modifiable relationship; removing the comorbidity (if clinically possible) would yield the predicted risk reduction.

## Foundational Learning

- **Counterfactual Inference**: Why needed here - The framework's core objective is estimating "what-if" scenarios under hypothetical interventions, requiring understanding of how counterfactuals differ from standard predictions. Quick check question: Given a patient with predicted PASC-HF risk of 74%, what does a counterfactual "no hypertension" scenario actually estimate?

- **Combinatorial Enumeration with Pruning**: Why needed here - The hybrid approach relies on recognizing when exhaustive search (2^m - 1 evaluations) is tractable versus when approximation is required. Quick check question: If you have 20 binary actionable features, should the algorithm use enumeration or fallback to optimization? Why?

- **Multi-Objective Optimization (Pareto Front)**: Why needed here - MOC returns counterfactuals from a Pareto-optimal front balancing proximity, sparsity, and probability shift—understanding trade-offs is essential for interpreting outputs. Quick check question: If two counterfactuals have identical probability shifts but one requires 3 feature changes and the other requires 5, which is preferred under the default scoring?

## Architecture Onboarding

- **Component map**: EHR data (diagnoses, labs, medications) -> TLDR temporal feature extraction -> gradient-boosted classifier -> Hybrid counterfactual algorithm (enumeration branch OR NICE → MOC fallback) -> Top-k counterfactuals ranked by scoring function S(·)

- **Critical path**: 1) Identify binary actionable features (B) and fixed features (F) 2) If |B| ≤ m_max: enumerate all 2^|B| - 1 combinations → score → return top-k 3) If |B| > m_max: attempt NICE with fixed-feature restriction → if empty, run MOC with genetic algorithm → return Pareto-optimal counterfactuals

- **Design tradeoffs**: Enumeration vs. approximation (guarantees vs. scalability), Sparsity vs. effectiveness (α/β ratio control), Fixed vs. actionable features (plausibility vs. search space restriction)

- **Failure signatures**: Empty counterfactual set (probability threshold too narrow or fixed-feature constraints too restrictive), Implausible counterfactuals (actionable features include non-modifiable attributes), Computational timeout (m_max set too high causing enumeration explosion)

- **First 3 experiments**: 1) Reproduce enumeration branch on synthetic binary data with m = 8 features; verify all 255 candidates evaluated and top-k selection matches manual scoring 2) Test NICE fallback by artificially restricting training pool to <10 instances matching fixed features; confirm MOC activates and returns Pareto-optimal solutions 3) Validate constraint enforcement: inject a feature marked as "fixed" and verify it remains unchanged (tolerance ε = 10^-8) across all returned counterfactuals

## Open Questions the Paper Calls Out

- **Extension to continuous variables**: How can the framework be extended to handle continuous clinical variables, such as laboratory values or medication dosages, while preserving interpretability? The current methodology is optimized for binary features (presence/absence), limiting the granularity of recommendations for continuous metrics common in routine care.

- **Clinical achievability validation**: Can the counterfactual interventions generated by the model be validated as clinically achievable, or do they represent mathematical artifacts? The framework assumes that feature modifications represent achievable clinical interventions, which may not hold for all therapeutic scenarios.

- **In-silico clinical trials**: Can this framework effectively simulate in-silico clinical trials to prioritize drug candidates for repurposing? The paper suggests potential applications in supporting in-silico clinical trials and drug repurposing efforts by modeling alternative treatment regimens.

## Limitations
- Enumeration optimality guarantee applies only when binary actionable features are ≤16 and the 2^m - 1 search space remains computationally tractable
- Clinical interpretability depends critically on correct identification of truly modifiable features and assumes classifier probability estimates remain valid under feature perturbation
- Association between comorbidity presence and predicted risk reduction represents correlation under the model, not proven causal effects

## Confidence
- **High confidence**: Predictive model performance (AUROC = 0.88 with 95% CI: 0.84-0.91), enumeration optimality guarantee conditions, and NICE/MOC fallback activation criteria
- **Medium confidence**: Clinical interpretation of comorbidity counterfactuals and assumed modifiability of identified risk factors
- **Low confidence**: Generalization to populations beyond MGB P2RC cohort and real-world achievability of recommended interventions

## Next Checks
1. Implement Algorithm 1 on synthetic binary data (m=8 features) and verify all 255 candidates are evaluated with correct top-k selection matching manual scoring
2. Test NICE fallback by artificially restricting training pool to <10 instances matching fixed features; confirm MOC activates and returns Pareto-optimal solutions
3. Validate constraint enforcement by injecting a "fixed" feature and verifying it remains unchanged (tolerance ε = 10^-8) across all returned counterfactuals