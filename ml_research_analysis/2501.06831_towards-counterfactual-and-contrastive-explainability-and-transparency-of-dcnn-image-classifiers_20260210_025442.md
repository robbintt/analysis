---
ver: rpa2
title: Towards Counterfactual and Contrastive Explainability and Transparency of DCNN
  Image Classifiers
arxiv_id: '2501.06831'
source_url: https://arxiv.org/abs/2501.06831
tags:
- filters
- class
- image
- explanations
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for generating counterfactual
  and contrastive explanations for deep convolutional neural networks (DCNNs). The
  method identifies the most important filters in the top convolution layer of a pre-trained
  DCNN that separate the model's decision between classifying an image to the original
  inferred class or some other specified alter class.
---

# Towards Counterfactual and Contrastive Explainability and Transparency of DCNN Image Classifiers

## Quick Facts
- arXiv ID: 2501.06831
- Source URL: https://arxiv.org/abs/2501.06831
- Reference count: 14
- Provides interpretable explanations by identifying minimal filter sets that maintain or alter DCNN predictions

## Executive Summary
This paper proposes a method for generating counterfactual and contrastive explanations for deep convolutional neural networks by analyzing filter activations in the top convolutional layer. The approach identifies the minimum set of filters necessary to maintain a prediction (contrastive) or to change it to a different class (counterfactual). The method trains separate neural networks to output binary masks or additive vectors applied to Global Average Pooled features, optimized for sparsity and prediction fidelity. Results on the Caltech-UCSD Birds dataset show the method can provide meaningful, interpretable explanations that enhance model transparency.

## Method Summary
The method works by taking a pre-trained DCNN (VGG-16) and analyzing its top convolutional layer through Global Average Pooling. Two separate explanation heads are trained: one to find Minimum Correct (MC) filters that maintain the predicted class with minimal filter usage, and another to find Minimum Incorrect (MI) filters that can be modified to change the prediction to a target class. The MC head uses a composite loss function combining cross-entropy, L1 sparsity, and negative logits, while the MI head uses cross-entropy with L1 regularization. The identified filters are visualized through their receptive fields to provide interpretable explanations.

## Key Results
- The proposed method generates meaningful counterfactual and contrastive explanations for DCNN image classifiers
- Visualizations show identified filters correspond to semantically meaningful image regions (e.g., bird eye, wing patterns)
- The approach successfully identifies minimal filter sets that can maintain or alter model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If the model maintains its prediction using only a sparse subset of filters (Minimum Correct), those filters likely represent the critical concepts for the inferred class.
- Mechanism: The method trains a "Contrastive Explanation Model" to output a binary mask over the Global Average Pooled (GAP) features. It minimizes a composite loss: Cross-Entropy (ensure prediction holds), L1 sparsity (minimize active filters), and negative logits (force high contribution from selected filters).
- Core assumption: The top convolutional layer contains semantically complete features where a subset is sufficient for class identity.
- Evidence anchors:
  - [abstract] "...identifying the minimum set of filters necessary to predict the input image to the inferred class."
  - [section 3.1] Equation 3 defines the loss $L_{MC} = L_{CE} + \lambda L_{l1} - L_{logits}$ to find optimal sparse filters.
  - [corpus] Neighbor papers like *Leveraging counterfactual concepts for debugging* support the general premise of concept-based debugging, though they do not validate this specific masking mechanism.
- Break condition: If disabling non-MC filters causes the prediction confidence to drop significantly or shift classes, the sparsity constraint ($\lambda$) may be too high, or the features are too distributed.

### Mechanism 2
- Claim: If specific activation values are added to selected filters (Minimum Incorrect), the model decision can be pivoted to a target "alter" class.
- Mechanism: The method trains a separate head to predict an additive residual vector (MI filters) applied to the GAP output. It optimizes for the target class using Cross-Entropy while minimizing the magnitude of the addition (L1 loss).
- Core assumption: The decision boundary between classes is smooth enough that additive offsets in the filter space can traverse it without needing structural changes in earlier layers.
- Evidence anchors:
  - [abstract] "...specifying the minimal changes necessary in such filters so that a contrastive output is obtained."
  - [section 3.2] Equation 8 describes the modified prediction $\hat{c}_i = h(g_i + F_{MI})$.
  - [corpus] General corpus support for counterfactual generation exists (e.g., *GenFacts*), but specific validation for GAP-layer filter injection is weak in the provided neighbors.
- Break condition: If the required additive values become extremely large, the input is likely an outlier or the classes are not well-separated in the learned embedding space.

### Mechanism 3
- Claim: If filters are visualized via their Receptive Fields (RF) on highly activating images, they align with human-interpretable semantic concepts.
- Mechanism: The system does not just output filter indices; it visualizes the spatial region (RF) in the image that activates the specific MC/MI filters to link abstract filter indices to physical features (e.g., "red eye" vs "wing pattern").
- Core assumption: Individual filters in late layers act as "concept detectors" rather than distributed feature mixers.
- Evidence anchors:
  - [page 3] "It has been shown that filters in the top convolution layer of a DCNN tend to learn more abstract, high-level features..."
  - [section 4.2.1] Figure 3 shows filter 15 activating on the "red-colored eye" of a bird.
  - [corpus] *P-TAME* also relies on trainable attention maps, reinforcing the viability of learned explanation layers.
- Break condition: If the visualized RF highlights incoherent or background regions (like the misclassification example in Figure 6), the filter is likely unreliable or detecting bias.

## Foundational Learning

- Concept: **Global Average Pooling (GAP)**
  - Why needed here: The method operates directly on the output of the GAP layer, treating the resulting vector as a list of filter activations (concepts). Understanding how GAP collapses spatial dimensions is required to interpret the filter masks.
  - Quick check question: If a filter has a high activation value in the GAP vector, does it mean it fired strongly everywhere or strongly in a specific location?

- Concept: **Loss Function Composition (Sparsity vs. Fidelity)**
  - Why needed here: The mechanism relies on balancing a trade-off: keeping the prediction correct (Cross-Entropy) while minimizing the number of filters used (L1 Sparsity).
  - Quick check question: What happens to the explanation if you increase the weight $\lambda$ of the L1 loss too high?

- Concept: **Receptive Fields (RF)**
  - Why needed here: To interpret the "Filter X" returned by the model, you must map the filter's activation back to the input image pixel region (RF) to see *what* it is looking at.
  - Quick check question: Does a filter in the top layer have a small or large receptive field relative to the input image?

## Architecture Onboarding

- Component map:
  - **Backbone:** A frozen Pre-trained DCNN (e.g., VGG-16) up to the Top Convolution Layer.
  - **Projection:** Global Average Pooling (GAP) Layer.
  - **Heads:** Two separate small neural networks (CFE models) taking GAP output as input:
    1. **MC Head:** Dense -> Sigmoid -> Thresholded ReLU (Outputs Binary Mask).
    2. **MI Head:** Dense -> ReLU (Outputs Additive Vector).
  - **Classifier:** Original frozen Fully Connected + Softmax layers.

- Critical path:
  1. Forward pass input to get GAP vector $g_i$.
  2. Pass $g_i$ through CFE head to get mask/offset.
  3. Modify $g_i$ (multiply for MC, add for MI).
  4. Pass modified vector to frozen Classifier.
  5. Compute composite loss and update *only* the CFE head weights.

- Design tradeoffs:
  - **Sparsity ($\lambda$):** Higher $\lambda$ yields simpler explanations (fewer filters) but risks dropping confidence (Table 3).
  - **Logits Loss:** Included for MC filters to ensure selected filters are "highly weighted" for the class, not just sufficient for a weak prediction.

- Failure signatures:
  - **Sudden confidence drop:** The sparsity constraint is too tight; the model cannot maintain the class with so few filters.
  - **Nonsensical visualization:** The MC filters highlight background noise. This indicates the model relies on context/background rather than the object (dataset bias).
  - **Oscillating loss:** The threshold $t=0.5$ in the Thresholded ReLU might be fighting the Sigmoid activation if not initialized or scaled carefully.

- First 3 experiments:
  1. **Baseline MC Verification:** Train the MC head on a single class with low $\lambda$. Verify that the predicted MC filters (when applied) maintain >95% of the original confidence.
  2. **Logits Loss Ablation:** Train two MC heads (with and without negative logits loss). Compare the visualization quality of the top-3 filters to see if the "logits" version highlights more distinct features (Table 4).
  3. **Class Sensitivity:** For a misclassified image (Paper Fig 6), generate MC filters for the incorrect class and MI filters for the correct class. Check if the MI filters correspond to features missing in the input (e.g., "red spot").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can robust quantitative evaluation metrics be developed for counterfactual explanation models when current synthetic ground truths fail to capture all semantically distinguishing features?
- **Basis in paper:** [explicit] The authors state, "There is a need for developing clear and robust evaluation metrics for counterfactual explanations," noting that synthesized ground truths often miss features like wing patterns.
- **Why unresolved:** Current benchmarks based on part annotations do not fully capture the high-level concepts or "filters" actually used by the DCNN for decision-making, limiting comparative analysis.
- **What evidence would resolve it:** The introduction of evaluation metrics that align with human judgment or a benchmark dataset specifically annotated for counterfactual reasoning.

### Open Question 2
- **Question:** How does the efficacy of the Counterfactual Explanation (CFE) model change when applied to intermediate or shallow convolutional layers rather than the top layer?
- **Basis in paper:** [explicit] The paper notes that while the current method is restricted to the top convolution layer, it "can be used to identify counterfactual and contrastive filters from any layer."
- **Why unresolved:** It is unclear if the filter manipulation strategy remains interpretable and effective in lower layers where features represent textures or edges rather than high-level concepts.
- **What evidence would resolve it:** Empirical analysis comparing the sparsity and prediction accuracy of the CFE model when targeting different network depths.

### Open Question 3
- **Question:** Can the identified minimum incorrect (MI) and minimum correct (MC) filters be reliably utilized for detecting adversarial attacks?
- **Basis in paper:** [explicit] The authors identify "adversarial attack detection" as a specific application of the proposed methodology to be "explored in the future."
- **Why unresolved:** The paper demonstrates the ability to explain classifications but does not validate if the identified filter patterns act as reliable signatures for differentiating adversarial perturbations from natural misclassifications.
- **What evidence would resolve it:** Experiments showing distinct divergence in MI/MC filter activation profiles between clean inputs and adversarial examples.

## Limitations

- The method relies on the assumption that top-layer filters encode coherent semantic concepts, which may not hold for distributed representations
- No quantitative comparison with existing counterfactual methods on the same dataset
- Reliance on user studies (Explanation Satisfaction) without comparison to baseline explanations

## Confidence

- Core mechanism of filter-level counterfactual and contrastive explanation generation: **Medium confidence**
- The sparsity-constrained optimization is theoretically sound, but relies heavily on the assumption that top-layer filters encode coherent semantic concepts
- The visualization validation (RF mapping) provides partial evidence, but the paper acknowledges instances where filters highlight background features rather than the object itself

## Next Checks

1. Ablation study: Remove the negative logits loss term and compare explanation quality (filter visualization coherence)
2. Distribution analysis: For a random sample of images, compare the distribution of selected MC filter counts vs. random baseline to verify sparsity constraint is meaningful
3. Counterfactual validation: For correctly classified images, verify that applying MI filter modifications actually changes the predicted class in the intended direction