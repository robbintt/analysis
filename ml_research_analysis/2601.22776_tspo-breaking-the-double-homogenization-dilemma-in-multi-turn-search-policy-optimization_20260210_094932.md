---
ver: rpa2
title: 'TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy
  Optimization'
arxiv_id: '2601.22776'
source_url: https://arxiv.org/abs/2601.22776
tags:
- reward
- tspo
- arxiv
- groups
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a "Double Homogenization Dilemma" in multi-turn
  RL for search-augmented reasoning: (1) process-level homogenization, where intermediate
  successes are ignored, and (2) intra-group homogenization, where uniform rewards
  cause vanishing gradients. To address this, the authors propose TSPO with a First-Occurrence
  Latent Reward (FOLR) mechanism that assigns partial rewards to the turn where the
  ground-truth first appears, without requiring external models or annotations.'
---

# TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization

## Quick Facts
- **arXiv ID**: 2601.22776
- **Source URL**: https://arxiv.org/abs/2601.22776
- **Reference count**: 34
- **Primary result**: TSPO with FOLR mechanism achieves 24% and 13.6% average performance gains on Qwen2.5-3B and 7B models respectively across seven QA datasets

## Executive Summary
This paper addresses a fundamental challenge in multi-turn reinforcement learning for search-augmented reasoning: the "Double Homogenization Dilemma." This dilemma manifests as process-level homogenization (ignoring intermediate successes) and intra-group homogenization (uniform rewards causing vanishing gradients in GRPO). The authors propose TSPO (Turn-based Search Policy Optimization) with a First-Occurrence Latent Reward (FOLR) mechanism that assigns partial rewards based on when ground-truth first appears in retrieved feedback, eliminating the need for external models or annotations. Extensive experiments on seven QA datasets demonstrate significant performance improvements over state-of-the-art baselines.

## Method Summary
TSPO addresses the Double Homogenization Dilemma in multi-turn RL by introducing a turn-level reward mechanism called First-Occurrence Latent Reward (FOLR). For each trajectory, FOLR detects the first turn t* where the ground-truth answer appears in retrieved documents and assigns rewards accordingly: ri,k = 1 if the answer is correct, α (0<α≤1) if k ≤ t* (first turn where ground-truth appears), else 0. The method uses turn-level advantage normalization Âi,k = (ri,k - mean(r·,k)) / (std(r·,k) + ε) and applies only to all-wrong groups based on ablation findings. Built on Search-R1/Verl frameworks, TSPO trains for 500 steps with group size G=5 and batch size 512 on 8×A100 GPUs, achieving significant performance gains across multiple model scales and datasets.

## Key Results
- TSPO achieves average performance gains of 24% on Qwen2.5-3B and 13.6% on 7B models
- Outperforms state-of-the-art baselines across seven QA datasets including NQ, TriviaQA, and HotpotQA
- Ablation study confirms FOLR's effectiveness in addressing both process-level and intra-group homogenization

## Why This Works (Mechanism)
The FOLR mechanism works by identifying the first turn where ground-truth appears in retrieved feedback and assigning partial rewards to all turns up to that point. This creates meaningful reward variance within groups, eliminating vanishing gradients while preserving credit assignment for intermediate successes. By operating at the turn level rather than only on final outcomes, TSPO captures the learning signal from successful intermediate steps that would otherwise be ignored.

## Foundational Learning
- **Multi-turn search-augmented reasoning**: Why needed - Foundation for understanding the sequential decision-making process where models iteratively retrieve and reason. Quick check - Can you describe the typical 3-5 turn workflow in search-augmented QA?
- **Double Homogenization Dilemma**: Why needed - Core problem TSPO addresses involving process-level and intra-group reward homogenization. Quick check - Can you explain how uniform rewards cause vanishing gradients in RL?
- **GRPO (Group Relative Policy Optimization)**: Why needed - Baseline RL framework that TSPO modifies. Quick check - Can you describe how GRPO computes advantages using group-relative normalization?
- **First-Occurrence Latent Reward**: Why needed - TSPO's novel mechanism for assigning partial rewards based on ground-truth detection timing. Quick check - Can you calculate FOLR rewards for a trajectory where ground-truth first appears at turn 3?
- **Reward variance within groups**: Why needed - Key insight that creating variance prevents gradient vanishing. Quick check - Can you explain why zero variance within groups causes optimization failures?
- **String/substring matching for ground-truth detection**: Why needed - Practical implementation detail for identifying when answers appear in retrieved context. Quick check - Can you describe edge cases where exact string matching might fail?

## Architecture Onboarding

**Component map**: Qwen2.5 model -> Search-R1/Verl framework -> FOLR reward computation -> GRPO advantage calculation -> Policy update

**Critical path**: Trajectory generation → Ground-truth detection in retrieved docs → FOLR reward assignment → Turn-level advantage computation → Policy gradient update

**Design tradeoffs**: FOLR vs external verifier models (simplicity vs potential accuracy), turn-level vs trajectory-level rewards (credit assignment vs stability), exact string matching vs semantic matching (implementation ease vs robustness)

**Failure signatures**: All-wrong groups yielding zero gradients despite near-misses, policy entropy collapse indicating over-rewarding, inconsistent t* detection across similar contexts

**First experiments**: 1) Implement basic FOLR with exact string matching on a single dataset, 2) Verify reward variance creation in mixed-success groups, 3) Compare TSPO vs baseline GRPO on small subset before full training

## Open Questions the Paper Calls Out

**Open Question 1**: Can the Double Homogenization theory and TSPO be effectively transferred to other domains beyond search-augmented reasoning, such as code generation or complex multi-step tool usage?
- **Basis in paper**: Limitations section explicitly states this remains open
- **Why unresolved**: Experiments focused exclusively on seven search-based QA benchmarks
- **What evidence would resolve it**: Demonstration of TSPO improving performance on code synthesis or agentic tool-use benchmarks

**Open Question 2**: How can the FOLR mechanism be adapted for tasks where the correct answer is not explicitly contained within the retrieved evidence or where the retrieval system is imperfect?
- **Basis in paper**: Limitations section mentions exploring FOLR for imperfect or absent retrieval
- **Why unresolved**: Current FOLR relies on exact string matching between ground truth and retrieved context
- **What evidence would resolve it**: Modified reward function using semantic similarity or model-based verification when exact matching fails

**Open Question 3**: How should TSPO's reward structure be modified to handle tasks where the model can answer correctly using internal parametric knowledge without retrieving the ground truth?
- **Basis in paper**: Limitations section notes tasks solvable without direct retrieval may require modified rewards
- **Why unresolved**: Current FOLR assumes successful trajectories always involve retrieval
- **What evidence would resolve it**: Framework balancing rewarding successful retrieval against correct internal reasoning

## Limitations
- Ground-truth detection mechanism unspecified (exact, case-insensitive, or fuzzy matching unclear)
- Missing critical hyperparameters including learning rate and optimizer settings
- Dataset preprocessing details incomplete (splits, normalization, filtering criteria)
- RL configuration gaps in KL divergence coefficient and entropy parameters

## Confidence

**High confidence**: The theoretical framework and FOLR mechanism are clearly defined; empirical results showing consistent improvements across multiple datasets and model scales are compelling with specific EM improvements (24% and 13.6% gains)

**Medium confidence**: Experimental methodology appears sound but lacks hyperparameter transparency; ablation study results presented without statistical significance measures

**Low confidence**: Exact implementation details necessary for faithful reproduction remain uncertain, particularly regarding reward computation in edge cases and interaction with Search-R1/Verl components

## Next Checks

1. **Reward assignment verification**: Implement logging to verify FOLR correctly identifies t* values across different datasets, with particular attention to edge cases where ground-truth appears partially in retrieved documents

2. **Gradient flow analysis**: Monitor distribution of advantage values Âi,k across training steps to confirm FOLR eliminates vanishing gradient problem in all-wrong groups while maintaining stable learning in mixed-success groups

3. **Hyperparameter sensitivity testing**: Conduct controlled experiments varying FOLR parameter α and KL coefficient to establish sensitivity of TSPO performance to these critical but under-specified parameters