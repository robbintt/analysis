---
ver: rpa2
title: 'Reasoning by Exploration: A Unified Approach to Retrieval and Generation over
  Graphs'
arxiv_id: '2510.07484'
source_url: https://arxiv.org/abs/2510.07484
tags:
- reasoning
- arxiv
- graph
- graphs
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multi-hop reasoning over large
  knowledge graphs using Large Language Models (LLMs). Existing GraphRAG approaches
  struggle with incomplete or noisy retrieval and poor generalization to unseen graphs.
---

# Reasoning by Exploration: A Unified Approach to Retrieval and Generation over Graphs

## Quick Facts
- arXiv ID: 2510.07484
- Source URL: https://arxiv.org/abs/2510.07484
- Reference count: 40
- Key outcome: Unified framework that integrates retrieval and generation into a single process, significantly outperforming state-of-the-art baselines with up to 3.8% improvement in Hit and F1 metrics

## Executive Summary
This paper addresses the challenge of multi-hop reasoning over large knowledge graphs using Large Language Models (LLMs). Existing GraphRAG approaches struggle with incomplete or noisy retrieval and poor generalization to unseen graphs. To overcome these limitations, the authors propose Reasoning by Exploration (RoE), a unified framework that integrates retrieval and generation into a single process by framing reasoning as step-by-step graph exploration. RoE trains LLMs to actively explore the graph, construct reasoning paths, and generate answers simultaneously.

## Method Summary
RoE is a two-stage training framework using Llama-3.1-8B-Instruct with LoRA fine-tuning. Stage 1 (SFT) trains on gold reasoning trajectories mined via breadth-first search, including all valid paths below a length threshold. Stage 2 (RL) uses Group Relative Policy Optimization with five rule-based rewards to enhance exploration and generalization. The model performs depth-first exploration with batching to handle context limits, maintaining current paths and frontier neighbors as state representation.

## Key Results
- Achieves up to 3.8% improvement in Hit and F1 metrics over state-of-the-art baselines
- Demonstrates strong generalization across different knowledge graphs, including transfer to MetaQA
- Outperforms GraphRAG approaches that separate retrieval and generation stages

## Why This Works (Mechanism)

### Mechanism 1
Unified exploration-retrieval-generation reduces information loss inherent in two-stage GraphRAG pipelines. RoE treats reasoning as step-wise graph exploration where the LLM incrementally expands frontier nodes, selects edges, and outputs partial answers at each step—eliminating the retriever-generator handoff that can drop relevant paths or inject noise.

### Mechanism 2
Supervised fine-tuning on gold exploration trajectories instills basic graph traversal skills. SFT uses gold actions derived from all reasoning paths under a length threshold, teaching the model multi-route exploration and reducing ambiguity when multiple neighbors share a relation.

### Mechanism 3
Reinforcement learning with rule-based rewards refines exploration policy and enhances generalization. GRPO optimization uses five rule-based rewards (format, answer recall, answer discovery, exploration recall, exploration discovery) to encourage structured outputs, correct answers, and novel valid paths while penalizing hallucinations.

## Foundational Learning

- **Knowledge Graph Question Answering (KGQA)**
  - Why needed: RoE is evaluated on KGQA benchmarks (WebQSP, CWQ) requiring multi-hop reasoning over entity-relation paths
  - Quick check: Can you trace a 2-hop path from "Winston Churchill" to "nephew" entities via sibling/parent relations?

- **Graph Retrieval-Augmented Generation (GraphRAG)**
  - Why needed: RoE addresses GraphRAG's two-stage limitation by unifying retrieval and generation
  - Quick check: What are the typical failure modes of separating retriever and generator in multi-hop KGQA?

- **Reinforcement Learning with Rule-Based Rewards**
  - Why needed: Stage II uses GRPO with five rule-based rewards to refine exploration beyond SFT imitation
  - Quick check: How do rule-based rewards differ from learned reward models in RLHF?

## Architecture Onboarding

- **Component map**: Input (Question, KG, Seed entities) -> State Encoder (s_d = (q, P_d, N_Fd)) -> Action Predictor (LLM) -> Reward Calculator (R_total) -> Policy Optimizer (GRPO)

- **Critical path**:
  1. Mine gold-consistent paths for SFT dataset
  2. SFT on step-wise (s_d, a*_d) pairs to learn basic exploration
  3. RL with GRPO on remaining data using rule-based rewards
  4. Inference: depth-first exploration up to D_max=5, batching neighbors per step

- **Design tradeoffs**:
  - Unified vs. two-stage: Unified avoids retriever-generator mismatch but requires whole-graph access and more compute per step
  - Exploration breadth vs. context limits: Batching neighbors mitigates context overflow but may lose global view
  - SFT path diversity vs. dataset specificity: Multi-path gold trajectories improve robustness but may not cover unseen graph patterns

- **Failure signatures**:
  - Low Hit on transfer tasks: SFT overfitting to training graph patterns; check RL discovery rewards
  - High hallucination rate: Insufficient penalty weight β in R_ans-dis or R_exp-dis
  - Premature termination: D_max too low or model learns conservative stopping; increase depth or adjust rewards

- **First 3 experiments**:
  1. Ablate RL stage (RoE w/o RL) on WebQSP → MetaQA transfer to isolate generalization benefit from RL
  2. Vary β in discovery rewards to measure hallucination-exploration trade-off on CWQ
  3. Test different max path thresholds in gold trajectory construction to assess SFT diversity impact

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Performance degradation with supernodes (entities with extremely high degrees) where neighborhood context must be split into multiple batches
- Computational latency trade-off between accuracy gains of iterative exploration and single-pass dense retrieval methods
- Sensitivity of RL stage to specific weighting of penalty parameter β used to discourage hallucinated predictions

## Confidence
- **High confidence**: The two-stage training approach (SFT followed by RL) is well-established in the literature
- **Medium confidence**: Experimental results showing 3.8% improvement over baselines are compelling
- **Low confidence**: Claims about strong generalization across different knowledge graphs are based on limited transfer experiments

## Next Checks
1. Ablate RL stage (RoE w/o RL) on WebQSP → MetaQA transfer to quantify the specific contribution of reinforcement learning to generalization performance
2. Systematically vary the penalty weight β in the hallucination discovery rewards and measure the trade-off between exploration breadth and hallucination rate on CWQ
3. Evaluate RoE trained on WebQSP on a completely different knowledge graph (e.g., NELL or NELL-995) to test the claim of strong generalization beyond the reported MetaQA transfer