---
ver: rpa2
title: 'Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges'
arxiv_id: '2512.11258'
source_url: https://arxiv.org/abs/2512.11258
tags:
- intent
- slot
- multi-intent
- language
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey provides the first comprehensive review of multi-intent\
  \ spoken language understanding (SLU) research since 2019, systematically analyzing\
  \ decoding paradigms and modeling approaches. It categorizes methods based on information\
  \ flow direction\u2014intent-guided slot filling, slot-guided intent detection,\
  \ and bidirectional interaction\u2014and evaluates their performance across classification-based\
  \ and generation-based frameworks."
---

# Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges

## Quick Facts
- arXiv ID: 2512.11258
- Source URL: https://arxiv.org/abs/2512.11258
- Reference count: 40
- First comprehensive survey of multi-intent SLU research since 2019, analyzing decoding paradigms and modeling approaches

## Executive Summary
This survey provides the first comprehensive review of multi-intent spoken language understanding research since 2019, systematically analyzing decoding paradigms and modeling approaches. The authors categorize methods based on information flow direction—intent-guided slot filling, slot-guided intent detection, and bidirectional interaction—and evaluate their performance across classification-based and generation-based frameworks. The survey finds that bidirectional interactive models generally outperform unidirectional ones, with PTMs and LLMs providing significant performance gains.

## Method Summary
The survey synthesizes research from multiple publications to create a comprehensive taxonomy of multi-intent SLU approaches. It analyzes decoding paradigms including classification-based and generation-based methods, categorizing them by information flow direction (intent-guided, slot-guided, bidirectional). The review evaluates performance across various datasets and benchmarks, identifying trends in model architecture, pretraining strategies, and evaluation metrics. The authors systematically compare model characteristics, computational efficiency, and practical deployment considerations.

## Key Results
- Bidirectional interactive models generally outperform unidirectional ones in multi-intent SLU tasks
- PTMs and LLMs provide significant performance gains across all model types
- Generation-based approaches show greater potential for accuracy, though classification-based models are more computationally efficient
- Limited real-world and multilingual datasets remain a significant challenge for the field

## Why This Works (Mechanism)
Multi-intent SLU models leverage the inherent relationship between intents and slots to improve performance. By modeling the bidirectional interaction between intent detection and slot filling, these systems can use intent information to guide slot prediction and slot information to disambiguate intent classification. Pretrained language models provide rich semantic representations that capture contextual dependencies, while generation-based approaches can produce more nuanced outputs by modeling the full conditional probability distribution.

## Foundational Learning

**Spoken Language Understanding (SLU)**: Why needed - Core task of extracting semantic information from spoken language. Quick check - Can identify intents and slots in single-intent utterances.

**Multi-Intent Recognition**: Why needed - Real-world utterances often express multiple intents simultaneously. Quick check - Handles utterances with overlapping or hierarchical intents.

**Slot Filling**: Why needed - Extracts specific semantic entities from utterances. Quick check - Accurately labels tokens with semantic tags.

**Intent Detection**: Why needed - Classifies the overall purpose of user utterances. Quick check - Correctly identifies primary and secondary intents.

**Bidirectional Information Flow**: Why needed - Captures dependencies between intent and slot predictions. Quick check - Improves performance by sharing contextual information.

**Pretrained Language Models (PTMs)**: Why needed - Provide rich contextual representations. Quick check - Improves performance on low-resource and complex utterances.

## Architecture Onboarding

**Component Map**: Text Input -> Encoding Layer -> Intent Decoder + Slot Decoder -> Output Layer (Intents + Slots)
Critical Path: Input -> Encoding -> Dual Decoding (Intent + Slot) -> Output

**Design Tradeoffs**: Classification-based models prioritize efficiency over accuracy, while generation-based models favor accuracy at computational cost. Bidirectional models require more parameters but achieve better performance than unidirectional approaches.

**Failure Signatures**: Performance degradation occurs with rare intents, ambiguous slot boundaries, and complex nested structures. Limited multilingual datasets cause poor cross-lingual generalization.

**First Experiments**:
1. Benchmark classification vs generation models on standard multi-intent SLU datasets
2. Compare unidirectional vs bidirectional information flow architectures
3. Evaluate PTM integration strategies across different model sizes

## Open Questions the Paper Calls Out
- How to improve model interpretability and explainability in multi-intent SLU systems
- What strategies can address the limited availability of real-world and multilingual datasets
- How to optimize model size and computational efficiency for practical deployment
- What continual