---
ver: rpa2
title: Improve LLM-based Automatic Essay Scoring with Linguistic Features
arxiv_id: '2502.09497'
source_url: https://arxiv.org/abs/2502.09497
tags:
- essay
- features
- linguistic
- language
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper combines linguistic features with large language model
  (LLM)-based automatic essay scoring to improve performance. By incorporating linguistic
  features into the zero-shot prompting approach, the proposed hybrid method achieves
  better alignment with human judgments compared to baseline models.
---

# Improve LLM-based Automatic Essay Scoring with Linguistic Features

## Quick Facts
- arXiv ID: 2502.09497
- Source URL: https://arxiv.org/abs/2502.09497
- Authors: Zhaoyi Joey Hou; Alejandro Ciuba; Xiang Lorraine Li
- Reference count: 35
- Primary result: Hybrid LLM-linguistics method improves zero-shot AES, approaching GPT-4 performance

## Executive Summary
This paper proposes a hybrid approach to automatic essay scoring (AES) that combines large language models (LLMs) with linguistic features to improve zero-shot performance. The method integrates explicit linguistic metrics like unique word count, readability scores, and lemma count into LLM prompts, providing concrete calibration signals that help the model better align with human grading. Experiments on both in-domain (ASAP) and out-of-distribution (ELLIPSE) essay datasets demonstrate that this approach significantly improves scoring accuracy, with open-source models like Mistral-7B approaching the performance of GPT-4 when linguistic features are included.

## Method Summary
The approach uses zero-shot prompting where linguistic features extracted from essays are explicitly included in the prompt template. The authors adopt an educational researcher persona and use an explanation-first structure (explanation before scoring) based on prior work showing this improves reasoning alignment. They select 10 linguistic features with Pearson's correlation ≥0.6 with essay scores, extracted using spaCy, NLTK, and readability packages. The LLM outputs are parsed using a few-shot template to extract scores in the correct range. The method is evaluated on two datasets (ASAP and ELLIPSE) using Quadratic Weighted Kappa (QWK) as the primary metric.

## Key Results
- Mistral-7B with linguistic features achieves 0.492 QWK on ASAP dataset (vs. 0.454 baseline)
- The approach successfully generalizes to out-of-distribution ELLIPSE dataset with similar improvements
- LLM performance approaches GPT-4 levels when linguistic features are included
- Top-10 linguistic features consistently outperform models without features across datasets

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Feature Injection as Explicit Calibration Signal
Providing explicit linguistic metrics in the prompt helps LLMs calibrate scoring decisions against quantifiable essay properties. The prompt includes an "Additional Information" section listing features like unique word count with framing that these features are "highly, positively correlated with the grade," giving the model concrete anchors to reference when reasoning about score assignment.

### Mechanism 2: Persona + Explanation-First Prompt Structure
Structuring the prompt with an educational researcher persona and requiring explanation-before-score improves reasoning alignment. The template follows persona pattern → essay prompt → analysis task → student essay → additional information → format instruction, forcing the model to generate reasoning before committing to a score.

### Mechanism 3: Feature Selection via Correlation Filtering
Restricting to high-correlation features (Pearson's r ≥ 0.6) avoids prompt bloat while retaining predictive signal. The authors select 10 features from Ridley et al. (2020b) that showed strong correlation with essay scores, reducing prompt length while preserving the most informative metrics.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK)**: Primary evaluation metric measuring agreement between predicted and human scores while accounting for chance agreement and penalizing larger disagreements more heavily. Quick check: If a model consistently scores 1 point higher than human raters, would QWK be low or high? (Answer: Low—systematic bias reduces agreement even if rank order is preserved.)

- **Zero-shot Prompting**: The approach relies on zero-shot inference—no task-specific training. Understanding how to structure prompts for zero-shot evaluation is essential. Quick check: What is the key difference between zero-shot prompting and fine-tuning for AES? (Answer: Zero-shot uses frozen model weights with task instructions; fine-tuning updates weights on labeled essay-score pairs.)

- **Linguistic Feature Engineering**: Understanding features like Dale-Chall readability, lemma count, and stopword ratio is essential to interpret why they help. Quick check: Why might unique word count correlate with essay quality more than total word count? (Answer: Unique words reflect vocabulary diversity; total words could reflect verbosity without quality.)

## Architecture Onboarding

- Component map: Input Essay + Prompt Description -> Feature Extraction (spaCy, NLTK, readability pkg) -> Prompt Construction (template + linguistic feats) -> LLM Scoring (Mistral-7B / GPT-4) -> Output Parsing (few-shot LLM JSON extractor) -> Score Output

- Critical path: Feature extraction → Prompt construction → LLM inference → Parsing. If any feature extraction fails, the prompt lacks calibration signals. If parsing fails (7% error rate noted), scores are lost.

- Design tradeoffs: Mistral-7B (open, controllable) vs. GPT-4 (higher baseline, costly, black-box); Top-3 vs. Top-10 features: shorter prompts vs. more signal; Zero-shot efficiency vs. supervised accuracy ceiling.

- Failure signatures: Open-source LLM underperformance (Mistral baseline 0.454 vs. GPT-4 0.499); GPT-4 feature paradox (adding features sometimes hurt GPT-4 performance, suggesting larger models may already encode these signals); Prompt length sensitivity (authors warn that "overly long prompts could negatively impact LLM performance").

- First 3 experiments: 1) Ablation by feature count: Run zero-shot AES with {0, 1, 3, 10} linguistic features on a held-out prompt. Measure QWK and latency. 2) Cross-dataset transfer test: Train prompt template on ASAP, evaluate zero-shot on ELLIPSE. Check if feature importance transfers. 3) Parsing robustness check: Feed 100 LLM outputs through the parsing module. Manually verify the 7% error rate and characterize failure modes.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset bias and generalizability: Limited diversity in prompt types and scoring distributions may restrict generalization to different essay formats, score ranges, or non-English languages.
- Feature selection stability: The 10-feature set based on fixed correlation threshold (r ≥ 0.6) is not tested for stability across domains or essay types.
- GPT-4 anomaly: Adding linguistic features sometimes hurts GPT-4 performance, suggesting larger models may already encode linguistic patterns implicitly, but this discrepancy is not deeply investigated.

## Confidence
- Linguistic features improve zero-shot AES alignment with human judgments: High
- Feature selection (Top-10) is critical to avoid prompt bloat: Medium
- The approach generalizes to out-of-distribution essays: Low

## Next Checks
1. **Dataset Diversity Test**: Apply the prompt template to essays from a third, structurally distinct dataset (e.g., TOEFL or GRE prompts) to test feature generalization. Compare QWK for Top-3 vs. Top-10 features and analyze which features remain predictive.

2. **Ablation of Feature Subsets**: Systematically remove one linguistic feature at a time (e.g., readability, unique words, lemma count) to identify which features drive performance gains. This would clarify whether gains come from specific metrics or cumulative signal.

3. **Cross-Lingual Transfer Test**: Translate the ASAP dataset into a non-English language (e.g., Spanish or Chinese), extract the same linguistic features, and run zero-shot AES. Measure QWK to assess whether the approach generalizes beyond English.