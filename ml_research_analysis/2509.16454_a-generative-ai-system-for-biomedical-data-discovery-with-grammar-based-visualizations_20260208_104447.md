---
ver: rpa2
title: A Generative AI System for Biomedical Data Discovery with Grammar-Based Visualizations
arxiv_id: '2509.16454'
source_url: https://arxiv.org/abs/2509.16454
tags:
- visualization
- data
- language
- system
- visualizations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a prototype system that combines generative
  AI with grammar-based visualizations to support biomedical data discovery. The system
  uses a multi-agent architecture to interpret natural language queries, generate
  visualization specifications, and apply interactive filters across linked views.
---

# A Generative AI System for Biomedical Data Discovery with Grammar-Based Visualizations

## Quick Facts
- arXiv ID: 2509.16454
- Source URL: https://arxiv.org/abs/2509.16454
- Reference count: 40
- Prototype system combining generative AI with grammar-based visualizations for biomedical data discovery

## Executive Summary
This paper presents a prototype system that combines generative AI with grammar-based visualizations to support biomedical data discovery. The system uses a multi-agent architecture to interpret natural language queries, generate visualization specifications, and apply interactive filters across linked views. A fine-tuned LLM produces specifications for a custom visualization grammar that supports multi-table linking and table displays, enabling users to progressively build interactive dashboards. The interface transparently shows filtering actions through generated UI widgets, allowing users to adjust or correct the AI's decisions.

## Method Summary
The system implements a multi-agent architecture where an orchestrator agent classifies user intent and routes to specialized agents - a filter agent for data predicates and a visualization agent for chart specifications. The visualization agent is a fine-tuned LLM trained on a domain-specific visualization grammar (udi-grammar) using the DQVis dataset. The system layer injects interactivity into generated specifications to create linked views across multiple tables. The frontend renders visualizations and filter widgets with coordinated state, allowing users to progressively build interactive dashboards through natural language interaction.

## Key Results
- Multi-agent architecture successfully routes natural language queries to appropriate processing agents
- Fine-tuned LLM generates valid JSON specifications for custom visualization grammar supporting multi-table linking
- Generated filter widgets provide transparency and enable user correction of LLM filter decisions
- Case study demonstrates utility in exploring donor metadata and filtering datasets based on age, sex, and death events

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-agent architecture with role separation may improve the handling of distinct query intents (filtering vs. visualization) in data discovery tasks.
- Mechanism: An orchestrator agent classifies user intent and routes to specialized agents—a filter agent for data predicates and a visualization agent for chart specifications. Structured outputs enforce valid responses.
- Core assumption: Intent can be cleanly categorized into filter vs. visualization tasks, and role-specialized agents outperform a single generalist model (unproven; author preference noted for iteration speed).
- Evidence anchors:
  - [abstract] "In our prototype, we use a multi-agent system to generate visualization specifications and apply filters."
  - [section 5.1] "The orchestrator first determines if the user request necessitates filtering the existing data, creating a visualization, or both. Then, additional agents are called."
  - [corpus] YAC (Lange et al., related work) also uses multi-agent coordination for biomedical discovery, suggesting architectural convergence; no comparative evidence exists.
- Break condition: Ambiguous or multi-intent queries that require simultaneous coordination across agents may fail if orchestration logic is brittle.

### Mechanism 2
- Claim: Fine-tuning an LLM on a domain-specific visualization grammar likely improves specification generation over off-the-shelf models.
- Mechanism: Supervised fine-tuning (SFT) trains a model to emit structured JSON visualization specs for a custom grammar that supports tables and cross-entity linking. Training uses reasoning-and-action traces with tool-call tokens.
- Core assumption: The custom grammar and SFT dataset (DQVis) adequately represent the biomedical metadata visualization space.
- Evidence anchors:
  - [abstract] "A fine-tuned LLM produces specifications for a custom visualization grammar that supports multi-table linking and table displays."
  - [section 5.2] "We found that using existing models was not very successful at constructing visualization specifications for our grammar. Therefore, we fine-tuned a model."
  - [corpus] No corpus evidence directly compares fine-tuned vs. base models for custom grammars; this remains an untested claim.
- Break condition: Distribution shift in queries or schema changes may degrade fine-tuned performance; grammar extensions require retraining.

### Mechanism 3
- Claim: System-injected interactivity into LLM-generated specifications enables progressive construction of linked dashboards without requiring the model to manage cross-view state.
- Mechanism: The fine-tuned LLM generates single-view specifications; the system layer automatically injects selection and linking logic (brushing/linking, global filter state) to create coordinated multi-view behavior.
- Core assumption: Linking semantics can be derived deterministically from schema metadata (entities, foreign keys) without LLM reasoning about coordination.
- Evidence anchors:
  - [abstract] "These visualizations are linked together, resulting in an interactive dashboard that is progressively constructed."
  - [section 5.4] "Since the fine-tuned LLM was trained to create individual visualizations, the system injects additional logic into the specifications to create interactive linked visualizations."
  - [corpus] InterChat supports linked chat-visualization updates but for single views; multi-view linking via system injection is not compared in literature.
- Break condition: Complex cross-entity relationships or non-standard linking patterns may not be captured by injected logic.

### Mechanism 4
- Claim: Generated filter widgets provide transparency and enable user correction of LLM filter decisions.
- Mechanism: The filter agent emits structured filter specs (interval/point filters with entity, field, values); the system renders these as interactive UI widgets allowing users to adjust bounds or selections.
- Core assumption: Users will recognize errors and know how to correct them via widgets.
- Evidence anchors:
  - [abstract] "The interface transparently shows filtering actions through generated UI widgets, allowing users to adjust or correct the AI's decisions."
  - [section 5.4] "The filter component is interactive, allowing the user to adjust the initial range selected by the agent (Resolve)."
  - [corpus] DynaVis uses dynamically generated widgets for visualization editing; this system adapts the pattern for filter ambiguity resolution.
- Break condition: Complex compound filters or interdependent filter logic may be difficult to represent or adjust via simple widgets.

## Foundational Learning

- Concept: Grammar of Graphics / Visualization Grammars (e.g., Vega-Lite)
  - Why needed here: The system uses a custom grammar for visualization specifications; understanding grammar composition (marks, encodings, transforms) is prerequisite to modifying or extending it.
  - Quick check question: Given a Vega-Lite spec, can you identify the mark type, encoding channels, and data fields?

- Concept: Multi-Agent LLM Architectures
  - Why needed here: The orchestrator delegates to specialized agents; understanding agent roles, structured outputs, and delegation patterns is essential.
  - Quick check question: How does an orchestrator agent differ from a router in a traditional microservices architecture?

- Concept: Linked Views / Brushing and Linking
  - Why needed here: The system injects cross-view interactivity; understanding selection state propagation and global filter coordination is critical.
  - Quick check question: What happens to a scatterplot when a bar chart selection filters a shared dataset?

## Architecture Onboarding

- Component map: Frontend (Vue + Quasar) -> Orchestrator Agent (GPT-4.1) -> Filter Agent (GPT-4.1) or Visualization Agent (Fine-tuned LLM) -> Grammar Renderer -> System Layer (inject interactivity) -> Frontend (renders views and widgets)
- Critical path:
  1. User query enters chat → Orchestrator classifies intent (filter/visualize/both)
  2. Orchestrator routes to Filter Agent and/or Visualization Agent
  3. Filter Agent outputs filter spec → System applies filter, generates UI widget
  4. Visualization Agent outputs grammar spec → System injects linking logic
  5. Frontend renders visualizations and widgets with coordinated state
- Design tradeoffs:
  - Fine-tuning vs. multi-agent with base models: Authors chose hybrid (fine-tuned visualization agent, base GPT-4.1 for orchestration/filtering) for faster iteration; fine-tuning both would increase training overhead.
  - Grammar-based specs vs. code generation: Grammar provides guardrails (JSON Schema validation) and easier system integration; code generation offers more flexibility but higher risk of invalid outputs.
  - System-injected linking vs. LLM-managed linking: Reduces LLM complexity but limits expressiveness of custom coordination patterns.
- Failure signatures:
  - Orchestrator misclassification: Filter requests routed to visualization agent (or vice versa) produce incorrect outputs.
  - Filter agent value hallucination: Filter uses values not present in the provided field context.
  - Visualization spec schema violation: Generated JSON fails grammar validation; frontend silently ignores or crashes.
  - Cross-entity filter mismatch: Filtering one entity doesn't propagate correctly to related entities due to incorrect join assumptions.
  - Widget adjustment ignored: User modifies filter widget but state doesn't sync with visualization views.
- First 3 experiments:
  1. **Intent classification accuracy test**: Construct a test set of 50+ queries labeled as filter/visualize/both; measure orchestrator routing precision/recall to identify misclassification patterns.
  2. **Specification validation rate**: Feed 100 diverse queries through the visualization agent; measure percentage of outputs that pass grammar schema validation without post-hoc correction.
  3. **Filter accuracy spot-check**: For 20 filter queries with known ground-truth predicates, compare agent-generated filter specs against expected values; identify common error modes (wrong entity, hallucinated values, incorrect bounds).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the fine-tuned model and overall system stability when evaluated beyond preliminary "spot checking"?
- Basis in paper: [explicit] The Conclusion states, "The next critical piece of work is a more careful evaluation of the fine-tuned model, the interface design, and the integrated system," noting that current checks only confirm the code runs without errors.
- Why unresolved: The authors have not yet quantified the system's reliability, visualization accuracy, or utility compared to existing baselines.
- What evidence would resolve it: Results from formal user studies and quantitative benchmarks measuring specification accuracy and task completion rates.

### Open Question 2
- Question: Does a single fine-tuned model for both visualization and filtering outperform the current multi-agent architecture?
- Basis in paper: [explicit] The Discussion poses the trade-off between fine-tuning versatility versus the "longer iteration cycle" of retraining, asking if a unified model is preferable to the modular agent approach.
- Why unresolved: The team prioritized the speed of multi-agent development for the prototype, leaving the performance comparison with a unified model untested.
- What evidence would resolve it: Ablation studies comparing the accuracy and latency of a unified fine-tuned model against the multi-agent system.

### Open Question 3
- Question: How can the system support filtering on high-cardinality categorical fields without exceeding the LLM's context window?
- Basis in paper: [inferred] Section 5.1 notes that categorical fields with many unique values (e.g., ID columns) are currently excluded from the prompt to avoid context overflow.
- Why unresolved: Real-world biomedical data often relies on high-cardinality identifiers, so excluding them limits the system's ability to perform precise data discovery on complex metadata.
- What evidence would resolve it: Successful integration of retrieval-augmented generation (RAG) or data summarization techniques that allow filtering on these fields.

## Limitations

- No quantitative evaluation metrics are provided - the paper relies on preliminary spot checking rather than systematic assessment of intent classification accuracy, specification validity rates, or filter accuracy.
- The custom grammar specification is not publicly available in the submission, making independent validation of generated specifications difficult.
- Critical implementation details are missing, including the specific base model used for fine-tuning, prompt structures and schema context formats, and access to the DQVis training dataset.

## Confidence

- **High confidence**: The architectural approach (multi-agent system with specialized roles) is technically sound and follows established patterns in AI systems.
- **Medium confidence**: The mechanism of system-injected interactivity for progressive linked dashboard construction is plausible given grammar-based visualization frameworks, but effectiveness depends on implementation details not fully specified.
- **Low confidence**: Claims about fine-tuning superiority and overall system utility lack empirical support through quantitative evaluation.

## Next Checks

1. **Intent classification accuracy test**: Construct a test set of 50+ queries labeled as filter/visualize/both; measure orchestrator routing precision/recall to identify misclassification patterns.
2. **Specification validation rate**: Feed 100 diverse queries through the visualization agent; measure percentage of outputs that pass grammar schema validation without post-hoc correction.
3. **Filter accuracy spot-check**: For 20 filter queries with known ground-truth predicates, compare agent-generated filter specs against expected values; identify common error modes (wrong entity, hallucinated values, incorrect bounds).