---
ver: rpa2
title: 'RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems'
arxiv_id: '2510.02263'
source_url: https://arxiv.org/abs/2510.02263
tags:
- abstractions
- reasoning
- abstraction
- solution
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RLAD jointly trains an abstraction generator and a solution generator\
  \ via RL to improve reasoning in large language models. The abstraction generator\
  \ proposes concise natural language insights\u2014such as procedural steps or useful\
  \ lemmas\u2014that guide the solution generator."
---

# RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems

## Quick Facts
- arXiv ID: 2510.02263
- Source URL: https://arxiv.org/abs/2510.02263
- Reference count: 40
- Primary result: RLAD achieves 44% improvement on AIME 2025 over prior RL methods by training abstraction and solution generators jointly via RL

## Executive Summary
RLAD introduces a two-player reinforcement learning framework that trains large language models to discover and utilize abstractions for reasoning problems. The approach decouples learning signals by training an abstraction generator to propose concise natural language insights and a solution generator to use these abstractions when solving problems. Both components are trained cooperatively using rewards tied to solution accuracy. A warmstart phase using supervised fine-tuning provides initial abstraction examples, and RL training optimizes both proposing and utilizing abstractions. This approach leads to consistent performance gains across math reasoning benchmarks, with abstractions enabling more diverse and structured exploration than standard RL methods.

## Method Summary
RLAD jointly trains an abstraction generator and solution generator via reinforcement learning to improve reasoning in large language models. The abstraction generator proposes concise natural language insights—such as procedural steps or useful lemmas—that guide the solution generator. Both components are trained cooperatively using rewards tied to solution accuracy when conditioned on the proposed abstractions. A warmstart phase using supervised fine-tuning provides initial abstraction examples, and RL training optimizes both proposing and utilizing abstractions. This approach leads to consistent performance gains across math reasoning benchmarks (e.g., 44% improvement on AIME 2025 over prior RL methods), with abstractions enabling more diverse and structured exploration. RLAD also scales well with inference compute, showing greater benefits from increasing abstraction diversity than solution sampling alone.

## Key Results
- RLAD achieves 44% improvement on AIME 2025 over prior RL methods
- Abstractions enable broader strategy exploration, producing semantically distinct solution traces
- RLAD scales well with inference compute, showing greater benefits from increasing abstraction diversity than solution sampling alone

## Why This Works (Mechanism)

### Mechanism 1: Cooperative Two-Player RL Decouples Learning Signals
- Claim: Jointly training an abstraction generator and solution generator via RL creates specialized learning signals that prevent degenerate exploration.
- Mechanism: The abstraction generator receives reward based on the *improvement* in solution accuracy when solutions are conditioned on its proposed abstractions (Eq. 4), while the solution generator receives direct accuracy reward. This decoupling prevents the model from optimizing only for depth (sequential token prediction) and instead incentivizes proposing *diverse strategies* that help a solution generator succeed.
- Core assumption: Solution generators will not spontaneously discover and reuse procedural abstractions through standard RL on long chains of thought; explicit training to propose abstractions is necessary.
- Evidence anchors:
  - [abstract]: "This results in a two-player RL training paradigm... that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation."
  - [section 5.1]: "The core principle behind our approach is that an abstraction z is successful at a given problem x if it can maximally help π_sol(·|x,z) find correct responses... without actually leaking the answer itself."
  - [corpus]: Weak/missing direct corpus evidence on two-player RL specifically for abstractions; related work on "Retrospective Replay" addresses exploration but not the cooperative game formulation.
- Break condition: If the solution generator is too weak to solve problems even with good abstractions, or too strong to need them, the reward signal for the abstraction generator becomes uninformative (Section 5.1 explicitly notes this challenge).

### Mechanism 2: Abstractions Enable Breadth-First Exploration of Strategy Space
- Claim: Conditioning on diverse abstractions produces semantically distinct solution traces, increasing the effective coverage of the solution space per unit of compute.
- Mechanism: Each abstraction encodes a different procedural or factual approach (e.g., "use quadratic formula in modular arithmetic" vs. "check multiplicative inverse existence first"). When the solution generator conditions on different abstractions, it produces solutions that are measurably less semantically similar (Fig. 6, left) than solutions generated without abstractions. This structured diversity means that with a fixed compute budget, generating N abstractions × M solutions per abstraction outperforms generating N×M solutions without abstractions (Table 3).
- Core assumption: The model's default failure mode is committing to a plausible but incorrect strategy and failing to switch ("underthinking"), rather than making local computation errors.
- Evidence anchors:
  - [abstract]: "While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration."
  - [section 6.2]: "Two solutions generated by conditioning on different abstractions are visibly less similar than the other two cases, implying that conditioning on and adhering to abstractions improves diversity of reasoning traces generated."
  - [corpus]: "Programming by Backprop" (arxiv 2506.18777) finds that code training induces "reusable algorithmic abstractions," providing indirect support that explicit abstraction mechanisms aid generalization.
- Break condition: If problems primarily require computational precision rather than strategic choice (e.g., arithmetic-heavy but single-method problems), abstraction diversity provides limited marginal benefit over solution sampling.

### Mechanism 3: Reward Masking Forces Abstraction Utilization
- Claim: Zeroing rewards for solution traces generated without abstractions forces the solution generator to learn to rely on abstractions, rather than ignoring them.
- Mechanism: The solution generator is trained on a mixture of prompts with and without abstractions. For traces without abstractions, rewards are set to 0 (Eq. 3), but KL regularization to the reference model is retained. This means the model learns to match the reference distribution when no abstraction is provided, but must actively optimize accuracy when an abstraction is available. This prevents the "ignoring abstractions" failure mode.
- Core assumption: The base model already has sufficient instruction-following capability to utilize abstractions if properly incentivized.
- Evidence anchors:
  - [section 5.1]: "To address these potential failure modes, we make a small but consequential change... we simply zero out rewards for any trace generated on x without abstractions."
  - [section B.2]: Table 6 shows that removing reward masking drops AIME 2025 w/ abs (best) from 48.33 to 42.50.
  - [corpus]: No direct corpus evidence on this specific reward masking technique.
- Break condition: If the base model lacks sufficient instruction-following capacity to incorporate the abstraction even when incentivized, this mechanism fails. Figure 2 (middle/right) shows this: even good abstractions only help if the solution model is sufficiently capable (Qwen-4B benefits more than Qwen-0.6B).

## Foundational Learning

- Concept: **Reinforcement Learning from Outcome Rewards (RLOR)**
  - Why needed here: RLAD builds on DAPO/GRPO-style RL, where models learn from binary success/failure signals rather than process supervision. You must understand policy gradients, KL-constrained RL, and clipping to follow the training setup.
  - Quick check question: Can you explain why RLOR requires balancing exploration (trying diverse strategies) against exploitation (refining known-good approaches)?

- Concept: **Chain-of-Thought Reasoning and "Underthinking"**
  - Why needed here: The paper positions abstractions as a solution to the "underthinking" problem where models commit to poor strategies early. Understanding CoT limitations helps you see why abstractions provide value.
  - Quick check question: When a model generates a long chain of thought that builds on an initial wrong assumption, is this a depth problem or a breadth problem?

- Concept: **Curriculum Learning for RL**
  - Why needed here: RLAD uses a two-stage curriculum (easy → medium problems) to stabilize training. The ablation in Table 6 shows curriculum improves w/ abs (best) from 43.33 to 48.33.
  - Quick check question: Why might training on hard problems from the start cause the abstraction generator to propose trivial or answer-leaking abstractions?

## Architecture Onboarding

- Component map:
  - **Abstraction Generator (π_abs)**: LLM (e.g., Qwen3-1.7B) fine-tuned via SFT on synthetically generated abstractions, then via RL using RFT/offline RL. Input: problem x. Output: natural language abstraction z.
  - **Solution Generator (π_sol)**: LLM (e.g., Qwen3-1.7B) fine-tuned via DAPO-style online RL. Input: problem x + optional abstraction z. Output: solution y.
  - **Warmstart Data Pipeline**: Uses stronger model (o4-mini) to generate abstractions from solution traces, filtered by efficacy (abstraction must improve solution accuracy when conditioned on).
  - **Reward Functions**: r(x,z,ỹ) = Acc(ỹ, y*) if z ≠ ∅, else 0. r_π_sol(x,z) = E[Acc] over solutions conditioned on z.

- Critical path:
  1. Collect warmstart abstractions via summarization (Section 4.1).
  2. SFT π_abs for 5 epochs on filtered abstractions.
  3. Joint RL training: π_abs proposes → π_sol generates → rewards computed → both updated (Algorithm 1).
  4. Inference: sample N abstractions, generate M solutions per abstraction, select best.

- Design tradeoffs:
  - **Single vs. separate models**: Paper attempted single-model training but found it "quickly lose[s] the ability of proposing abstractions over the course of RL training" (Section 7). Separate models are more stable but increase serving complexity.
  - **Online vs. offline RL for π_abs**: Paper uses offline RFT for π_abs due to compute constraints, but online RL (rolling out π_sol on the fly) would provide more accurate reward signals.
  - **Number of abstractions at inference**: Table 3 suggests diminishing returns after ~16 abstractions for AIME 2025, but optimal k varies by benchmark.

- Failure signatures:
  - **Abstraction leaking answer**: Detected via LLM judge; abstractions that allow base model to solve without problem context are filtered.
  - **Solution generator ignoring abstraction**: Detected via low adherence rate (Fig. 6, right); addressed by reward masking.
  - **Abstraction generator proposing trivial hints**: Caused by weak warmstart or insufficient RL signal; addressed by curriculum and efficacy filtering.

- First 3 experiments:
  1. **Validate warmstart quality**: Generate abstractions for a held-out problem set using your distilled pipeline, then measure whether π_sol conditioned on these abstractions improves over baseline (replicate Fig. 2 for your model size).
  2. **Ablate reward masking**: Train π_sol with and without the zero-reward-for-no-abstraction modification; verify that removal drops abstraction adherence (replicate Table 6, row 3 vs. row 4).
  3. **Compute allocation sweep**: For a fixed compute budget on AIME-style problems, vary the abstraction/solution ratio (m solutions per n abstractions where m×n = constant) and plot iso-compute frontiers (replicate Fig. 5) to identify optimal allocation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can RLAD be adapted to train a single unified model to both propose and utilize abstractions without experiencing catastrophic forgetting of the proposal capability?
- **Basis in paper:** [explicit] The authors state in Section 7 that they attempted to train a single model for both tasks, but it would "very quickly lose the ability of proposing abstractions over the course of RL training," and suggest targeted mid-training as a potential future direction.
- **Why unresolved:** The current two-player setup decouples the learning signals to prevent the solution generator's gradient from overwhelming the abstraction generator; balancing these within one set of weights remains an unsolved engineering challenge.
- **What evidence would resolve it:** A demonstration of a single model maintaining high pass@k accuracy on abstractions while simultaneously improving solution accuracy throughout the RL training process, without the need for separate policy heads.

### Open Question 2
- **Question:** What are the mechanistic explanations for why training with abstractions improves performance even when abstractions are withheld during inference?
- **Basis in paper:** [explicit] Section 7 notes that the improvement in the "w/o abs" setting (Table 2) is intriguing and likely a result of generalization, but states that "understanding the mechanisms behind this phenomena and how to amplify them is also interesting for future work."
- **Why unresolved:** The paper currently provides empirical evidence of the improvement but lacks a theoretical or causal explanation of how the model internalizes the abstraction-guided reasoning traces to solve problems independently.
- **What evidence would resolve it:** Mechanistic interpretability studies (e.g., probing specific attention heads) showing that abstraction-trained models develop more robust internal representations of procedural knowledge than standard RL models.

### Open Question 3
- **Question:** Can the reasoning abstraction framework be effectively applied to open-ended reasoning domains that lack the rigid procedural structure of mathematics?
- **Basis in paper:** [explicit] The authors acknowledge in Section 7 that "we limited our evaluation to mathematical reasoning tasks, leaving open-ended reasoning unexplored."
- **Why unresolved:** Mathematical reasoning often relies on specific lemmas and formulas ("factual knowledge"), and it is uncertain if the "procedural knowledge" captured by abstractions translates to domains like qualitative logic or creative planning.
- **What evidence would resolve it:** Successful application of the RLAD training paradigm to non-mathematical benchmarks (e.g., ARC-AGI full test sets or qualitative logic puzzles) showing similar performance gains over baseline RL methods.

## Limitations
- Warmstart dependency: RLAD relies on a strong warmstart phase using o4-mini to generate abstraction examples, constraining scalability to larger models.
- Reward signal quality: The abstraction generator's reward depends on solution accuracy, creating credit assignment problems that may become uninformative if solution generators are too weak or too strong.
- Domain specificity: While demonstrating strong results on math reasoning benchmarks, the abstractions are problem-specific and may not generalize to other reasoning domains.
- Computational overhead: RLAD requires generating multiple abstractions and solutions per problem at inference time, though it is more compute-efficient than generating many solutions without abstractions.

## Confidence
- **High confidence**: The mechanism of cooperative two-player RL for abstraction proposal and utilization is well-supported by ablation studies (Table 6 shows reward masking is critical, curriculum improves performance). The compute-efficiency claim is validated through iso-compute analysis (Figure 5, Table 3).
- **Medium confidence**: The claim that abstractions enable broader strategy exploration is supported by semantic similarity analysis (Figure 6, left) and qualitative examples, but could benefit from more systematic diversity metrics. The scalability limitations for larger models are observed but not fully explained.
- **Low confidence**: The long-term generalization of learned abstractions beyond the training distribution is not extensively tested. The paper's focus on math reasoning leaves open questions about performance in other domains requiring different types of abstractions.

## Next Checks
1. **Ablation on warmstart quality**: Systematically vary the strength of the warmstart model (e.g., use different versions of o1/o3 vs. o4-mini) and measure the impact on final RLAD performance to quantify the warmstart dependency.
2. **Cross-domain transfer**: Apply RLAD-trained abstractions from math problems to physics, coding, or logical reasoning benchmarks to test whether the learned abstraction mechanisms transfer beyond their training domain.
3. **Scaling analysis**: Train RLAD on progressively larger models (8B, 32B, 70B parameters) and measure whether the marginal gains over base DAPO decrease as predicted, or whether architectural modifications could maintain the benefits at scale.