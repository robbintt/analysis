---
ver: rpa2
title: 'MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data
  through Question Answering'
arxiv_id: '2508.11163'
source_url: https://arxiv.org/abs/2508.11163
tags:
- data
- trajectory
- mobility
- question
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MobQA, a benchmark dataset for evaluating
  semantic understanding of human mobility data using natural language question answering.
  The dataset contains 5,800 question-answer pairs across three types: factual retrieval,
  multiple-choice reasoning, and free-form explanation, covering both daily and weekly
  trajectory granularities.'
---

# MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering

## Quick Facts
- **arXiv ID**: 2508.11163
- **Source URL**: https://arxiv.org/abs/2508.11163
- **Reference count**: 40
- **Primary result**: State-of-the-art LLMs show strong performance (>80%) on factual retrieval but significant limitations on semantic reasoning tasks, with free-form answers scoring only 2.0-3.0/5 on faithfulness and informiveness metrics.

## Executive Summary
MobQA is a benchmark dataset designed to evaluate semantic understanding of human mobility data through natural language question answering. The dataset contains 5,800 question-answer pairs across three types: factual retrieval, multiple-choice reasoning, and free-form explanation, covering both daily and weekly trajectory granularities. Experiments reveal that while LLMs excel at explicit factual retrieval from GPS trajectories, they struggle significantly with semantic reasoning tasks that require inferring hidden attributes like mobility modes and visit purposes. Performance degrades substantially for longer weekly trajectories, highlighting current models' challenges in processing extended sequences and extracting meaningful semantic insights from raw GPS data.

## Method Summary
The MobQA dataset was constructed using the Geolife GPS dataset from Microsoft Research, containing 5,800 QA pairs (2,000 factual retrieval, 2,000 multiple-choice, 1,800 free-form) built on anonymized GPS trajectories. Trajectories were converted to text format as `(hh:mm, x, y)` tuples with 2 decimal place coordinate offsets. Daily trajectories average 97.2 points while weekly trajectories average 787.5 points. The benchmark evaluates both closed-source models (GPT-4o-mini, GPT-4o, o3-mini, Gemini-1.5/2.0-Flash) via API and fine-tuned open-source models (Llama-3.2-1B/3B, Qwen3-1.7B/4B) using QLoRA. Free-form answers are evaluated using LLM-as-a-Judge with Gemini-2.0-Flash scoring faithfulness and informativeness on a 1-5 scale, validated against human ratings with 0.50-0.60 correlation.

## Key Results
- Factual retrieval accuracy exceeds 80% for most models on daily trajectories
- Free-form answers score only 2.0-3.0 out of 5 on faithfulness and informativeness metrics
- Performance degrades substantially for longer weekly trajectories (average 787.5 points)
- Multiple-choice reasoning shows less sensitivity to trajectory length than factual retrieval
- Semantic augmentation (mobility modes, POIs) improves some models (o3-mini) while harming others (gpt-4o-mini)

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Length Degradation
Longer trajectories increase search space complexity, causing attention mechanisms to disperse focus across more tokens. Models struggle to identify specific information among many similar entries, creating a needle-in-haystack problem that predictably degrades performance.

### Mechanism 2: Semantic Abstraction Gap
Raw numerical trajectories lack explicit semantic signals. Models must infer modes and purposes solely from coordinate sequences through multi-step reasoning (detect speed patterns → infer transportation mode → reason about location context → infer purpose), with each step compounding error risk.

### Mechanism 3: Task-Specific Robustness Divergence
Multiple-choice questions provide candidate answers that act as implicit anchors, allowing models to verify hypotheses against trajectory data rather than freely generating from scratch. The five-option constraint reduces output entropy and provides length-robustness.

## Foundational Learning

- **Concept: Semantic Trajectory Enrichment**
  - Why needed: Understanding how to augment raw GPS with POI labels, mobility modes, and purpose annotations to enable semantic reasoning
  - Quick check: Given raw coordinates (09:06, -0.27, 8.12) → (09:07, -0.06, 8.14), what speed and mobility mode can be inferred?

- **Concept: Long-Context Attention Distribution**
  - Why needed: Understanding why LLMs lose accuracy on long sequences is essential for designing trajectory chunking or hierarchical processing strategies
  - Quick check: If a daily trajectory averages 97 data points and weekly averages 787, at what sequence length does your target model show >10% accuracy drop on retrieval tasks?

- **Concept: LLM-as-a-Judge Evaluation**
  - Why needed: Free-form QA cannot be evaluated with exact-match metrics; understanding how to use LLMs to score faithfulness and informativeness is critical
  - Quick check: When using GPT-4o to judge Gemini-generated answers, what steps are needed to detect and mitigate self-enhancement bias?

## Architecture Onboarding

- **Component map**: Raw GPS → downsampled 1-min intervals → anonymized offsets → textual tuples "(hh:mm, x, y)\n..." → Question Generation → LLM Evaluator → Benchmark Interface

- **Critical path**:
  1. Validate trajectory tokenization matches training (each coordinate tuple = ~20 tokens)
  2. Confirm GPU memory for target trajectory length (<1000 points for open-source on A100)
  3. Run LLM-as-a-judge correlation check against human ratings (target r > 0.5)

- **Design tradeoffs**:
  - Daily vs. Weekly granularity: Daily preserves accuracy; weekly provides richer behavioral context
  - Raw vs. semantically-augmented input: Raw tests pure inference; augmented improves semantic task performance but may over-specify
  - Template vs. free-form generation: Templates are reproducible; free-form captures nuance but requires manual filtering

- **Failure signatures**:
  - Accuracy drops sharply at specific sequence lengths (attention ceiling)
  - Free-form answers contain hallucinated POIs not present in trajectory
  - Multiple-choice models select semantically plausible but trajectory-inconsistent options

- **First 3 experiments**:
  1. Establish baseline: Run GPT-4o-mini on daily factual retrieval to confirm ~0.69 accuracy; if >0.10 deviation, check prompt format
  2. Length sensitivity test: Bin daily trajectories by length (100, 200, 400, 800 points) and plot accuracy curve to identify degradation threshold
  3. Semantic augmentation ablation: Add mobility mode labels to raw trajectories and measure delta on multiple-choice accuracy (expect +5-15% for capable models like o3-mini)

## Open Questions the Paper Calls Out

**Open Question 1**: Can architectural modifications (e.g., hierarchical attention, memory-augmented architectures) enable LLMs to maintain factual retrieval accuracy on weekly trajectories exceeding 600+ data points? The authors report that fine-tuning failed to resolve performance degradation with longer sequences.

**Open Question 2**: How can semantic augmentation be integrated to consistently improve performance across different model families rather than benefiting some models while harming others? Figure 6 shows o3-mini improved with semantic augmentation while gpt-4o-mini performed worse.

**Open Question 3**: What training approaches beyond supervised fine-tuning can improve free-form explanation quality (currently scoring only 2.0-3.0/5)? The authors suggest high-level reasoning may be difficult to acquire through supervised fine-tuning alone.

**Open Question 4**: To what extent do findings from Beijing-based GPS trajectories generalize to mobility patterns in other cultural, geographic, and urban contexts? The authors acknowledge the geographic and demographic scope remains limited to Beijing.

## Limitations
- API version dependency may introduce variability in reproducibility due to inevitable ceiling on exact replicability from API drift
- Single judge validation relies entirely on Gemini-2.0-Flash with moderate correlation (0.50-0.60) to human ratings
- Hardware constraints limited open-source evaluation to trajectories with fewer than 1,000 data points due to GPU memory limitations

## Confidence
- **High Confidence**: Factual retrieval performance degradation with sequence length
- **Medium Confidence**: Semantic abstraction gap mechanism
- **Medium Confidence**: Task-specific robustness divergence between multiple-choice and factual retrieval

## Next Checks
1. **API Version Sensitivity Test**: Run the same MobQA benchmark on at least two different versions of GPT-4o to quantify the impact of model updates on reported performance metrics.

2. **Alternative Judge Validation**: Implement a cross-judge evaluation framework using different LLM-as-a-Judge models to score the same free-form answers and compute inter-judge agreement scores.

3. **Full Trajectory Evaluation**: Re-run the benchmark using models with larger context windows (e.g., Gemini-1.5-Pro with 2M token context) on complete weekly trajectories without truncation to determine if observed degradation is an artifact of sequence length limitations.