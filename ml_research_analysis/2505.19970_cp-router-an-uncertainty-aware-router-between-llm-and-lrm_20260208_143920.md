---
ver: rpa2
title: 'CP-Router: An Uncertainty-Aware Router Between LLM and LRM'
arxiv_id: '2505.19970'
source_url: https://arxiv.org/abs/2505.19970
tags:
- prediction
- routing
- cp-router
- accuracy
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CP-Router is a training-free routing framework that dynamically
  selects between a Large Language Model (LLM) and a Large Reasoning Model (LRM) using
  uncertainty estimates from Conformal Prediction (CP). It introduces Full and Binary
  Entropy (FBE) to adaptively select the CP error rate, improving uncertainty differentiation.
---

# CP-Router: An Uncertainty-Aware Router Between LLM and LRM

## Quick Facts
- arXiv ID: 2505.19970
- Source URL: https://arxiv.org/abs/2505.19970
- Authors: Jiayuan Su; Fulin Lin; Zhaopeng Feng; Han Zheng; Teng Wang; Zhenyu Xiao; Xinlong Zhao; Zuozhu Liu; Lu Cheng; Hongwei Wang
- Reference count: 40
- Primary result: Reduces token usage while maintaining or improving accuracy by routing between LLM and LRM based on Conformal Prediction uncertainty

## Executive Summary
CP-Router introduces a training-free framework that dynamically routes queries between a Large Language Model (LLM) and a Large Reasoning Model (LRM) based on uncertainty estimates from Conformal Prediction (CP). The system uses the size of CP prediction sets as a proxy for query difficulty, routing uncertain queries to the LRM while handling confident queries with the more efficient LLM. A novel Full and Binary Entropy (FBE) mechanism adaptively selects the CP error rate to maximize uncertainty differentiation. Evaluated across multiple-choice QA benchmarks including mathematics, logical reasoning, and chemistry, CP-Router achieves significant token savings without sacrificing accuracy compared to using LRM alone.

## Method Summary
CP-Router extracts logits from answer choices using an LLM, applies softmax to obtain probability distributions, and defines nonconformity scores as 1 minus the probability. Conformal Prediction generates prediction sets by including all answers whose nonconformity scores fall below a quantile threshold computed from a calibration set. The FBE mechanism scans candidate error rates (α values) to find the one that maximizes the entropy of the prediction set size distribution, using β=3 weighting between full and binary entropy components. For test queries, if the prediction set size is small (typically singleton), the LLM's top answer is returned; otherwise, the query is routed to the LRM. This approach suppresses overthinking by preventing LRMs from generating verbose reasoning traces for simple queries.

## Key Results
- Reduces token usage by routing simple queries to LLM, avoiding unnecessary LRM reasoning traces
- Maintains or improves accuracy compared to LRM-only approaches across multiple MCQA benchmarks
- Introduces FBE mechanism that adaptively selects CP error rate for better uncertainty differentiation
- Generalizes well across different model pairings and demonstrates robustness on open-ended QA tasks

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Proxy via Prediction Set Size
The size of Conformal Prediction sets serves as a reliable proxy for query difficulty, indicating when an LLM is likely insufficient. CP converts LLM logits into plausible answer sets—small sets indicate confidence while large sets indicate uncertainty. The router directs queries with large sets to the LRM. This works because the LLM's probability distribution over options correlates with correctness likelihood; low confidence implies the need for deeper reasoning. Break condition: If the LLM is miscalibrated (confidently wrong), the prediction set will be small, causing erroneous LLM selection for difficult queries.

### Mechanism 2: Adaptive Separation via FBE
Dynamically selecting the CP error rate using Full and Binary Entropy creates better separation between easy and hard queries than fixed thresholds. FBE scans candidate α values to maximize the entropy of prediction set size distribution, preventing the router from sending too many or too few queries to the LRM. This works because there exists an optimal error rate balancing coverage guarantees and routing utility. Break condition: On uniformly difficult or easy datasets, entropy maximization may fail to find discriminative thresholds, leading to unstable routing.

### Mechanism 3: Efficiency via Overthinking Suppression
Routing simple queries to the LLM prevents LRMs from generating unnecessarily long chains of thought, saving tokens without sacrificing accuracy. LRMs like DeepSeek-R1 default to long reasoning traces even for trivial inputs. By intercepting low-uncertainty queries, the system returns direct LLM answers, bypassing verbose LRM generation. This works because LRMs consistently produce verbose outputs for trivial inputs while LLMs are significantly more concise. Break condition: If CP computation overhead approaches LRM latency for simple queries, efficiency gains are negated.

## Foundational Learning

- **Concept: Conformal Prediction (CP)**
  - Why needed: CP converts raw LLM logits into statistically rigorous prediction sets, making the router's threshold logic mathematically sound
  - Quick check: How does the calibration set size affect the width of prediction set intervals?

- **Concept: Uncertainty Quantification (Entropy)**
  - Why needed: FBE mechanism relies on interpreting entropy as a signal for distribution shape; understanding why high entropy in set sizes implies better routing capability
  - Quick check: Why does the paper maximize entropy of prediction set size distribution rather than minimizing it?

- **Concept: Model Routing / Cascades**
  - Why needed: This is the system architecture; understanding trade-offs between cascade approach vs parallel execution
  - Quick check: What is the latency cost added by routing step compared to direct LRM call?

## Architecture Onboarding

- **Component map:** Input Prompt -> LLM (Encoder) -> CP Module -> FBE Optimizer -> Router -> LLM (if |C(x)| ≤ τ) OR LRM (if |C(x)| > τ)

- **Critical path:** The FBE Optimizer determining optimal α. If miscalibrated, the system either over-uses expensive LRM (wasting tokens) or over-trusts LLM (losing accuracy).

- **Design tradeoffs:** Token Savings vs Accuracy controlled by α threshold—stricter α favors token savings but risks accuracy. Offline vs Online Calibration requires representative calibration data or held-out validation set for dynamic α setting.

- **Failure signatures:** "Router Always On" (all prediction sets large) indicates LLM is undertrained or calibration/test data are non-exchangeable. "Accuracy Drop" (all sets size 1 but LLM is wrong) indicates LLM overconfidence/miscalibration.

- **First 3 experiments:** 1) Sanity Check with fixed α=0.2: plot prediction set size distribution to check if varied or flat. 2) FBE Validation: compare fixed α vs FBE-selected α on validation split to verify improvement in Token Utility metric. 3) Calibration Drift: swap calibration dataset (e.g., Math data for Chemistry questions) to test coverage guarantee retention.

## Open Questions the Paper Calls Out

### Open Question 1
Can CP-Router be adapted for free-form generation tasks without requiring heuristic conversion to multiple-choice format? The current framework relies on finite candidate options to compute softmax probabilities and prediction sets. A native application to open-ended text remains unexplored. Evidence would require a modified nonconformity score function operating on semantic embeddings or text generation likelihoods rather than discrete option indices.

### Open Question 2
How does the framework perform when applied to black-box API models where internal logits are inaccessible? The method currently depends on white-box access to extract logits for answer choices. Many production models via paid APIs do not expose logits, preventing direct calculation of nonconformity scores. Evidence would require experiments using sampling-based uncertainty estimation (e.g., self-consistency) as a substitute for logit-based CP.

### Open Question 3
How sensitive is the conformal coverage guarantee to violations of the exchangeability assumption in real-world data streams? The paper evaluates on standard benchmarks but does not test scenarios with distribution shift. In deployment, user queries may drift over time, violating the exchangeability assumption required for the formal coverage guarantee. Evidence would require evaluation of accuracy and coverage retention when test set distribution differs significantly from calibration set.

## Limitations

- Critical dependence on calibration set quality—distribution mismatch leads to miscalibrated prediction sets and routing errors
- FBE mechanism lacks comparison to simpler baseline calibration methods (fixed α or grid search with coverage validation)
- No systematic analysis of routing failure scenarios (miscalibrated LLM, uniformly difficult/easy queries)
- Efficiency gains assume CP computation overhead is negligible, which may not hold in all deployment scenarios

## Confidence

- **High Confidence**: Core mechanism of using CP prediction set size as uncertainty proxy is well-grounded and empirically consistent across benchmarks
- **Medium Confidence**: FBE method for adaptive α selection improves uncertainty differentiation but needs ablation studies against simpler baselines
- **Low Confidence**: No systematic analysis of failure modes; efficiency assumptions about CP overhead need validation

## Next Checks

1. **Ablation on Calibration Method**: Compare FBE-selected α against (a) fixed α chosen via coverage validation on held-out data, and (b) simple grid search maximizing token utility directly to validate FBE's added value.

2. **Calibration Drift Analysis**: Systematically evaluate routing performance when calibration set is drawn from different domain or difficulty level than test set, measuring coverage guarantee violations and accuracy drops.

3. **Latency Overhead Measurement**: Instrument system to measure wall-clock time for CP computation on LLM versus direct LRM call for varying query complexities to determine breakeven point where routing becomes detrimental.