---
ver: rpa2
title: 'User Perception of Attention Visualizations: Effects on Interpretability Across
  Evidence-Based Medical Documents'
arxiv_id: '2508.10004'
source_url: https://arxiv.org/abs/2508.10004
tags:
- attention
- biomedical
- perceived
- explanations
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether attention weights from Transformer
  models serve as effective explanations for biomedical document classification. Researchers
  conducted a user study with medical experts who classified articles into evidence
  types while using an interface displaying attention-based word highlights and model
  probability scores.
---

# User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents

## Quick Facts
- arXiv ID: 2508.10004
- Source URL: https://arxiv.org/abs/2508.10004
- Reference count: 0
- XLNet achieved F1 scores up to 0.97 for biomedical document classification

## Executive Summary
This study investigates whether attention weights from Transformer models serve as effective explanations for biomedical document classification. Researchers conducted a user study with medical experts who classified articles into evidence types while using an interface displaying attention-based word highlights and model probability scores. The XLNet model achieved high accuracy (F1 scores up to 0.97), but attention visualizations were not consistently perceived as helpful. Users preferred simpler encodings like background color and word luminance over bar length, which aligns with cognitive load findings showing lower mental effort for intuitive formats. Predicted probabilities were consistently rated as highly useful across all conditions. Results suggest attention weights alone are insufficient for explanation, but their perceived helpfulness depends on how they are visually presented.

## Method Summary
The study fine-tuned XLNet on 399,737 biomedical documents for multi-class classification into five evidence types (Broad Synthesis, Excluded, PS-RCT, PS-NRCT, Systematic Review). Attention weights were extracted from the final encoder layer and averaged across attention heads to create word-level importance scores. A Chrome extension interface displayed these attention weights using four visualization conditions: control (no visualization), background color saturation, word luminance, and bar length. Five medical experts classified 200 articles each while providing Likert ratings for explanation helpfulness and completing NASA-TLX cognitive load assessments.

## Key Results
- XLNet achieved high classification accuracy with F1 scores up to 0.97
- Attention visualizations received low helpfulness ratings (peaking around 3.0 on 5-point scale)
- Users preferred intuitive encodings (background color, luminance) over precise bar length encoding
- Predicted probability scores were consistently rated highly useful across all conditions
- Bar length visualization produced highest cognitive load and frustration scores

## Why This Works (Mechanism)

### Mechanism 1: Visualization Format Moderates Attention Explanation Usefulness
- Claim: The perceived helpfulness of attention-based explanations depends significantly on visual encoding format, independent of the underlying attention weights.
- Mechanism: Different visual encodings impose varying cognitive loads. Intuitive encodings (background color, luminance) require less mental effort to process than precise-but-demanding encodings (bar length), enabling faster integration into decision workflows.
- Core assumption: Users can map highlighted words to decision-relevant information when visualization overhead doesn't consume available cognitive capacity.
- Evidence anchors:
  - [abstract]: "Users preferred simpler encodings like background color and word luminance over bar length, which aligns with cognitive load findings showing lower mental effort for intuitive formats"
  - [section 6]: "Bar Length—despite being the most perceptually accurate channel—less helpful than more intuitive encodings like Background or Luminance. This contradicts the expected effectiveness hierarchy in visualization literature."
  - [corpus]: Limited direct corpus support; corpus papers focus on LLM interpretability tools (KnowThyself) and Shapley-based explanations (MultiSHAP) rather than attention visualization formats
- Break condition: If users lack domain expertise for the classification task, format preference may not translate to improved decision accuracy.

### Mechanism 2: Model Confidence Provides Stable Decision Support
- Claim: Predicted probability scores are perceived as highly useful across all visualization conditions and document types, independent of attention explanation quality.
- Mechanism: Numerical confidence provides a single, interpretable signal users can integrate without needing to interpret which features drove the prediction.
- Core assumption: Users understand probability semantics and the model is sufficiently calibrated.
- Evidence anchors:
  - [abstract]: "Predicted probabilities were consistently rated as highly useful across all conditions"
  - [section 6]: "The predicted probability was consistently rated as highly useful across all article types, with mean scores above 4.0... no statistically significant differences between visual encodings"
  - [corpus]: Preliminary trust study (arXiv:2510.15769) suggests explainability correlates with trust, but no direct corpus evidence on probability scores specifically
- Break condition: If model probabilities become poorly calibrated in deployment, user trust will degrade with repeated use.

### Mechanism 3: Attention Weights Alone Are Insufficient for Domain Expert Explanation
- Claim: Raw attention weights do not consistently provide helpful explanations for biomedical classification, contrary to proposals that attention serves as a natural explanation mechanism.
- Mechanism: Attention weights capture token importance within the model's learned representation space, but this importance may not align with domain experts' conceptual frameworks for evidence classification.
- Core assumption: Higher attention weights indicate features relevant to the model's prediction, but this relevance may not match human-interpretable reasoning.
- Evidence anchors:
  - [abstract]: "attention weights were not perceived as particularly helpful for explaining the predictions"
  - [section 6]: "Overall, users rated the usefulness of highlighted words relatively low, with scores peaking around 3.0 on a 5-point scale"
  - [corpus]: "Investigating Training and Generalization in Faithful Self-Explanations" (arXiv:2512.07288) finds LLM self-explanations often lack faithfulness—relevant to the broader explanation validity debate
- Break condition: If attention can be mapped to domain concepts (e.g., "randomized" → RCT), perceived helpfulness may improve.

## Foundational Learning

- Concept: Transformer Self-Attention Extraction
  - Why needed here: Understanding how attention weights are extracted (final layer, averaged across heads) is essential before interpreting user perception data or modifying the approach.
  - Quick check question: Can you explain what averaging across attention heads produces and what information is lost versus preserved?

- Concept: Cognitive Load vs. Perceptual Precision Tradeoff
  - Why needed here: The finding that bar length (more perceptually accurate) was less preferred than background color requires understanding why theoretical precision can produce worse real-world outcomes.
  - Quick check question: Why might a "more accurate" visualization encoding result in lower perceived helpfulness and higher frustration?

- Concept: Evidence-Based Medicine Document Hierarchy
  - Why needed here: Classification targets have different evidence levels (systematic reviews > RCTs > non-randomized trials), which affects what features should be highlighted for each type.
  - Quick check question: What distinguishes a systematic review from an RCT in the medical evidence hierarchy, and how might this affect attention patterns?

## Architecture Onboarding

- Component map:
  - Document input → XLNet encoding → [CLS] classification → Attention extraction (final layer, head-averaged) → Visualization rendering → User decision → Likert feedback collection

- Critical path: Document input → XLNet encoding → [CLS] classification → Attention extraction (final layer, head-averaged) → Visualization rendering → User decision → Likert feedback collection

- Design tradeoffs:
  - Encoder vs. autoregressive LLM: XLNet chosen for inference efficiency, fine-tuning stability, and direct attention interpretability (vs. LLMs where attention is less accessible)
  - Precision vs. cognitive load: Bar length is theoretically optimal (Munzner's principle) but increases NASA-TLX frustration (49.4) vs. background color (30.3)
  - Single vs. multi-layer attention: Only final layer used—simpler extraction but potentially loses hierarchical patterns from earlier layers

- Failure signatures:
  - Low helpfulness ceiling (~3.0/5.0) across all attention visualizations indicates attention-highlighted words don't align with expert reasoning
  - High variance across document types (e.g., SR Background M=2.58 vs. PS-RCT Bar M=1.62) suggests attention interpretability is category-dependent
  - Bar length consistently underperforms despite theoretical precision advantages—signals theory-practice gap in visualization design

- First 3 experiments:
  1. Compare attention extraction strategies: Test final-layer-only vs. weighted multi-layer aggregation to determine if richer attention patterns improve perceived helpfulness.
  2. Map attention to domain concepts: Aggregate word-level attention to medical concepts (via UMLS/MeSH) and test if domain-aligned explanations improve ratings.
  3. Assess calibration before trust: Measure model probability calibration (reliability diagrams); run longitudinal study to detect if helpfulness degrades when probabilities are miscalibrated.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the preference for intuitive visual encodings (background color) over precise encodings (bar length) generalize to broader user groups beyond the small sample of five medical experts?
- Basis in paper: [explicit] The authors explicitly list "small sample size" as a limitation and state that "Future work will... extend the study to broader user groups."
- Why unresolved: The study involved only five participants, which limits the statistical power to generalize the finding that users prefer lower-precision visualizations despite visualization theory favoring bar length.
- What evidence would resolve it: A replication of the user study with a significantly larger sample size of medical practitioners to confirm if the preference for background color over bar length holds.

### Open Question 2
- Question: How do interactive attention explanations impact perceived helpfulness and cognitive load compared to the static visualizations evaluated in this study?
- Basis in paper: [explicit] The Conclusion states that "Future work will explore interactive explanations" to build upon the findings regarding static visualizations.
- Why unresolved: The current study only evaluated static representations of attention weights; it did not test if allowing users to manipulate or toggle explanations changes their utility.
- What evidence would resolve it: A comparative user study measuring NASA-TLX scores and perceived helpfulness between the static interface conditions and a new condition featuring interactive attention features.

### Open Question 3
- Question: How does the perceived helpfulness of attention weights compare to other explanation mechanisms, such as gradient-based methods or feed-forward layer attributions?
- Basis in paper: [inferred] The authors note the limitation of "the use of one explanation mechanism" and cite recent studies questioning attention in favor of feed-forward layers.
- Why unresolved: It remains unclear if the low helpfulness ratings were due to the nature of attention weights themselves or the specific visual implementation; comparing attention against fundamentally different XAI methods would isolate the cause.
- What evidence would resolve it: An experiment presenting users with different explanation types (e.g., attention vs. LIME vs. feed-forward attribution) for the same model predictions to assess relative preference and task performance.

## Limitations
- Small expert sample size (n=5) limits statistical power and generalizability of user perception findings
- Single model and attention extraction strategy (XLNet final layer) may not represent broader attention-based explanation approaches
- Static visualizations only tested; interactive explanations were not evaluated

## Confidence

- **High Confidence:** Model performance metrics (F1 scores up to 0.97) and probability score helpfulness ratings across all conditions. These findings are based on large-scale classification evaluation with clear numerical outcomes.
- **Medium Confidence:** Cognitive load findings showing bar length encoding produced higher frustration (NASA-TLX=49.4) than simpler formats. While statistically reported, the small sample size and potential learning effects during the study reduce confidence.
- **Low Confidence:** Attention visualization format preferences and their impact on decision accuracy. The ceiling effect at ~3.0/5.0 helpfulness ratings suggests limited alignment between attention weights and expert reasoning, but the small sample prevents definitive conclusions about format superiority.

## Next Checks

1. **Expand Expert Sample Size:** Replicate the user study with 15-20 medical experts to establish statistically robust differences between visualization conditions and validate the cognitive load findings.

2. **Multi-Layer Attention Analysis:** Compare single-layer attention extraction with weighted combinations of multiple encoder layers to determine if richer attention patterns improve perceived helpfulness and decision accuracy.

3. **Calibration Assessment:** Measure model probability calibration using reliability diagrams on held-out data, then conduct longitudinal user studies to detect whether helpfulness ratings degrade when probabilities are poorly calibrated.