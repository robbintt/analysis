---
ver: rpa2
title: 'Fake & Square: Training Self-Supervised Vision Transformers with Synthetic
  Data and Synthetic Hard Negatives'
arxiv_id: '2509.02029'
source_url: https://arxiv.org/abs/2509.02029
tags:
- synthetic
- data
- learning
- negatives
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates synthetic data and synthetic hard negatives
  to enhance self-supervised vision transformers. The authors generate a synthetic
  clone of ImageNet-100 using diffusion models and create synthetic hard negatives
  in representation space to improve contrastive learning.
---

# Fake & Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives

## Quick Facts
- **arXiv ID**: 2509.02029
- **Source URL**: https://arxiv.org/abs/2509.02029
- **Reference count**: 34
- **Primary result**: Synthetic data and synthetic hard negatives improve self-supervised vision transformers, with DeiT-S reaching 82.12% top-1 accuracy when combining both approaches

## Executive Summary
This paper investigates synthetic data and synthetic hard negatives to enhance self-supervised vision transformers. The authors generate a synthetic clone of ImageNet-100 using diffusion models and create synthetic hard negatives in representation space to improve contrastive learning. Their Syn2Co framework combines both approaches, training DeiT-S and Swin-T architectures. Results show synthetic data alone can achieve reasonable performance (79.02% top-1 accuracy for DeiT-S), though real data still provides advantages. Synthetic hard negatives further improve representations, with DeiT-S reaching 82.12% top-1 accuracy when using both synthetic components. Swin-T benefits primarily from synthetic negatives. The study demonstrates that modern diffusion models can generate high-quality synthetic images that capture substantial semantic information, while synthetic negatives provide computationally efficient enhancement to contrastive learning, though with architecture-specific responses.

## Method Summary
The authors propose Syn2Co, a framework combining synthetic data and synthetic hard negatives for self-supervised vision transformer training. They first generate a synthetic ImageNet-100 clone (130K images) using class-conditional diffusion models. For synthetic hard negatives, they select the top-N hardest negatives from a memory queue by cosine similarity and apply synthesis functions (interpolation, extrapolation, mixing, perturbation) to create additional challenging negatives in representation space. The framework uses MoBY's momentum encoder and InfoNCE loss, combining real negatives from the queue with synthetic negatives. Experiments compare DeiT-S and Swin-T architectures across different combinations of real/synthetic data and hard negative configurations.

## Key Results
- DeiT-S trained with 100% synthetic data achieves 79.02% top-1 accuracy, approaching the 79.36% baseline with real data
- Adding synthetic hard negatives improves DeiT-S to 82.12% top-1 accuracy with full Syn2Co
- Swin-T shows minimal benefit from synthetic data alone (83.45% vs 83.90% baseline) but gains from synthetic hard negatives (84.04%)
- Architecture-specific responses observed: DeiT benefits from both synthetic components while Swin benefits primarily from synthetic negatives

## Why This Works (Mechanism)

### Mechanism 1: Diffusion-Generated Synthetic Data Captures Semantic Structure
- Claim: Modern diffusion models can generate synthetic images that encode sufficient semantic information to support self-supervised representation learning, though with diminishing returns as real data is replaced.
- Mechanism: Class-conditional diffusion models trained on ImageNet learn to generate images preserving class-specific visual features; when used for contrastive pre-training, these synthetic images provide valid augmentation diversity for learning invariances.
- Core assumption: The semantic content and visual diversity in synthetic images approximates the distributional properties needed for learning transferable features.
- Evidence anchors:
  - [abstract] "modern diffusion models can generate high-quality synthetic images that capture substantial semantic information"
  - [section 4.3] "The relatively modest performance gap between fully synthetic and fully real training regimes suggests that modern diffusion models can generate high-quality samples"
  - [corpus] Related work (Sariyildiz et al., SynCLR) supports diffusion clones for representation learning, but corpus provides limited mechanistic validation for this specific framework.
- Break condition: If distribution shift between synthetic and real data exceeds a threshold, fine-tuning transfer degrades; Figure 3 shows ~5-8% gap between 100% real and 100% synthetic.

### Mechanism 2: Representation-Space Hard Negative Synthesis Improves Discriminability
- Claim: Generating synthetic hard negatives by operating directly on learned representations creates more challenging contrastive signals that improve feature discriminability.
- Mechanism: Selecting the top-N hardest negatives (highest cosine similarity to query) and applying synthesis functions (interpolation, extrapolation, mixing, perturbation) creates boundary cases that force the encoder to refine semantic boundaries without requiring additional real data.
- Core assumption: Synthesized negatives remain semantically valid (not false negatives) and provide gradient signals that improve rather than confuse the embedding space.
- Evidence anchors:
  - [abstract] "synthetic hard negatives provide computationally efficient enhancement to contrastive learning"
  - [section 3.4] "This addresses a key limitation in contrastive learning, i.e., the scarcity of informative negative examples that effectively capture semantic boundaries"
  - [corpus] SynCo and hard negative mixing papers provide precedent, but corpus evidence is weak for ViT-specific validation of this mechanism.
- Break condition: Excessive perturbation or incorrect synthesis creates false negatives that degrade performance; hyperparameter sensitivity observed in Figure 4.

### Mechanism 3: Architecture-Specific Synthetic Component Utilization
- Claim: Transformer architectures with different inductive biases leverage synthetic components differently—DeiT benefits from both synthetic data and negatives, while Swin benefits primarily from negatives.
- Mechanism: DeiT's global patch-level attention may require more data diversity to learn effectively, whereas Swin's hierarchical windowed attention creates stronger locality priors that benefit more from harder discrimination tasks than data augmentation.
- Core assumption: Architectural structure determines how synthetic augmentations are utilized during representation learning.
- Evidence anchors:
  - [abstract] "Swin-T benefits primarily from synthetic negatives... with architecture-specific responses"
  - [section 4.2/Table 1] Swin: 84.04% (negatives only) vs 83.70% (full Syn2Co); DeiT: 82.12% (full) > 81.86% (data only)
  - [corpus] Absent—corpus does not provide comparative validation across ViT architectures.
- Break condition: Generalization to other architectures (larger models, ConvNeXt, etc.) is untested; mechanism may not transfer.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**:
  - Why needed here: The entire framework builds on contrastive self-supervised learning; understanding why hard negatives matter requires grasping how InfoNCE shapes the embedding space.
  - Quick check question: Why do easy negatives (low similarity to query) contribute less gradient signal than hard negatives in InfoNCE?

- **Vision Transformer Architectures (DeiT vs Swin)**:
  - Why needed here: Interpreting architecture-specific results requires understanding DeiT's global patch attention versus Swin's hierarchical shifted-window attention.
  - Quick check question: How does Swin's window-based attention differ from DeiT's global attention, and what inductive bias does this create?

- **Momentum Encoders and Memory Queues**:
  - Why needed here: The framework uses MoBY's momentum encoder and queue; synthetic negatives are generated from queued representations.
  - Quick check question: Why does a momentum-updated target encoder paired with a memory queue improve contrastive learning over single-encoder approaches?

## Architecture Onboarding

- **Component map**:
  - Data sources: Real dataset X (ImageNet-100) + Synthetic dataset Xs (130K diffusion-generated images)
  - Augmentation pipeline: Family T producing two views (xq, xk) per image
  - Encoders: Online encoder fq (trained via backprop) + Target encoder fk (momentum-updated: θk ← m·θk + (1-m)·θq)
  - Negative management: Memory queue Q (real negatives) + Synthetic hard negatives Qs (generated per batch)
  - Synthesis module: F(q, n) with 6 strategies—interpolation, extrapolation, mixing, noise jittering, perturbation, adversarial
  - Objective: InfoNCE loss over combined negative set (K real + L synthetic)

- **Critical path**:
  1. Pre-generate synthetic dataset Xs using diffusion model (one-time; ~130K images)
  2. Each training iteration: sample from X ∪ Xs, create two augmented views
  3. Encode views, retrieve top-N hardest negatives from queue by cosine similarity
  4. Apply synthesis functions F to generate L synthetic negatives
  5. Compute InfoNCE loss with combined negatives, backpropagate to online encoder

- **Design tradeoffs**:
  - **N (hard negatives selected)**: Higher N = more synthesis candidates but increased compute; paper tests {256, 512, 1024}
  - **Synthetic ratio L/(K+L)**: Architecture-dependent; DeiT shows sensitivity (Figure 4a), Swin more robust
  - **Real/synthetic data mix**: Diminishing returns beyond ~50% synthetic; Figure 3 shows gradual decline
  - **Training duration**: DeiT benefits from 300 epochs vs 150 (81.86% → 82.12%); Table 1

- **Failure signatures**:
  - DeiT performance degrades with misconfigured hard negative ratios (Figure 4a shows non-monotonic response)
  - Swin shows minimal gain from synthetic data alone (Table 1: 83.45% vs 83.90% baseline)
  - Both architectures: excessive hard negative selection (high N with aggressive synthesis) risks false negatives

- **First 3 experiments**:
  1. **Baseline validation**: Reproduce MoBY with DeiT-S on ImageNet-100 (no synthetic components); target ~79.36% top-1 to confirm setup matches Table 1.
  2. **Synthetic negatives ablation**: Add hard negative synthesis (N=512) with varying synthetic ratios on DeiT-S; plot accuracy curve to verify Figure 4a pattern.
  3. **Architecture comparison**: Run Syn2Co (full) on both DeiT-S and Swin-T for 300 epochs; confirm DeiT benefits from both components while Swin peaks with negatives only.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to ImageNet-100 with only DeiT-S and Swin-T architectures
- Optimal synthesis function selection not established; no comparison to learned synthesis strategies
- Architecture-specific hyperparameter sensitivity requires careful tuning
- Distribution shift between synthetic and real data impact not fully characterized

## Confidence
- **High confidence**: Diffusion-generated synthetic data captures sufficient semantic information for representation learning
- **Medium confidence**: Synthetic hard negatives provide computationally efficient enhancement to contrastive learning
- **Medium confidence**: Architectural differences drive differential utilization of synthetic components

## Next Checks
1. **Generalization to larger datasets**: Validate Syn2Co on full ImageNet-1K and object detection benchmarks (COCO, LVIS) to assess scalability beyond ImageNet-100.
2. **Cross-architecture robustness**: Test synthetic hard negatives on ConvNeXt and larger ViT variants (B, L) to determine if the architecture-specific utilization pattern generalizes.
3. **Synthesis function optimization**: Conduct systematic ablation of the six synthesis functions (interpolation, extrapolation, mixing, perturbation, noise jittering, adversarial) to identify optimal strategies and potentially learn synthesis parameters rather than using fixed functions.