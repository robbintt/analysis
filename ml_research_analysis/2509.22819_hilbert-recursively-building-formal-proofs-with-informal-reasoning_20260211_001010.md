---
ver: rpa2
title: 'Hilbert: Recursively Building Formal Proofs with Informal Reasoning'
arxiv_id: '2509.22819'
source_url: https://arxiv.org/abs/2509.22819
tags:
- proof
- theorem
- subgoals
- subgoal
- theorems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HILBERT is a hierarchical agentic framework that bridges informal
  mathematical reasoning with formal theorem proving by recursively decomposing complex
  problems into simpler subgoals. The system orchestrates general-purpose reasoning
  LLMs with specialized prover LLMs, leveraging verifier feedback to refine proofs.
---

# Hilbert: Recursively Building Formal Proofs with Informal Reasoning

## Quick Facts
- arXiv ID: 2509.22819
- Source URL: https://arxiv.org/abs/2509.22819
- Authors: Sumanth Varambally; Thomas Voice; Yanchao Sun; Zhifeng Chen; Rose Yu; Ke Ye
- Reference count: 40
- Key outcome: HILBERT achieves 70.0% pass rate on PutnamBench (462/660 problems), outperforming proprietary SeedProver by nearly 20 percentage points

## Executive Summary
HILBERT is a hierarchical agentic framework that bridges informal mathematical reasoning with formal theorem proving by recursively decomposing complex problems into simpler subgoals. The system orchestrates general-purpose reasoning LLMs with specialized prover LLMs, leveraging verifier feedback to refine proofs. On the challenging PutnamBench dataset, HILBERT achieves 70.0% pass rate, outperforming proprietary SeedProver (50.4%) by nearly 20 percentage points and demonstrating a 422% improvement over the best publicly available baseline. On miniF2F, HILBERT attains 99.2% pass rate, 6.6 points above the best public method.

## Method Summary
HILBERT is an inference-only agentic framework that recursively decomposes theorem proving problems into subgoals. The system uses a Reasoner LLM (Gemini 2.5 Pro/Flash) for informal reasoning, sketch generation, and subgoal extraction, and a Prover LLM (DeepSeek-Prover-V2-7B or Goedel-Prover-V2-32B) for formal proof synthesis. The framework retrieves relevant Mathlib theorems using FAISS similarity search, validates generated proofs with Lean 4 verification, and iteratively refines sketches based on error feedback. The core innovation is recursive decomposition that splits problems into subgoals, solves them independently (using prover or reasoner), and recursively decomposes any remaining unsolved subgoals up to depth D=5.

## Key Results
- Achieves 70.0% pass rate on PutnamBench (462/660 problems), outperforming proprietary SeedProver by nearly 20 percentage points
- Demonstrates 422% improvement over best publicly available baseline on PutnamBench
- Attains 99.2% pass rate on miniF2F, 6.6 points above the best public method
- Generates substantially longer proofs (average 1,454 lines on PutnamBench, some exceeding 15,000 lines) while maintaining formal correctness

## Why This Works (Mechanism)

### Mechanism 1
Recursive decomposition enables solving problems that exceed single-model context capacity. The system generates proof sketches with `have` subgoals marked as `sorry` placeholders, extracts each subgoal as an independent theorem, solves them with Prover or Reasoner, and recursively decomposes any remaining unsolved subgoals up to depth D. Complex proofs decompose into simpler subgoals that remain mathematically valid when extracted with relevant context from preceding subgoals.

### Mechanism 2
Orchestrating complementary agents (general-purpose reasoner + specialized prover) outperforms either paradigm alone. The Reasoner handles informal proof generation, sketch creation, subgoal extraction, and error correction; the Prover handles direct formal proof attempts. General-purpose LLMs excel at decomposition and language-intensive tasks while specialized provers excel at standalone formal proof synthesis.

### Mechanism 3
Retrieval-augmented generation reduces inference compute while improving pass rate. The Retriever generates search queries, fetches top-m theorems from Mathlib via FAISS similarity search, and the Reasoner selects relevant ones to include in proof generation prompts. Accessing relevant Mathlib theorems during sketch generation and error correction prevents failures from incorrect theorem names and simplifies proof strategies.

## Foundational Learning

- Concept: Lean 4 proof structure and `have` statements
  - Why needed here: The decomposition strategy relies on generating proof sketches with `have` subgoals that must compile correctly with `sorry` placeholders before extraction.
  - Quick check question: Can you explain what a `have` statement does in Lean and why `sorry` is a valid placeholder?

- Concept: Semantic search with embedding similarity
  - Why needed here: The Retriever uses cosine similarity over sentence embeddings to find relevant Mathlib theorems.
  - Quick check question: How would you interpret a low-relevance retrieval result, and what prompt adjustments might help?

- Concept: Formal verification feedback loops
  - Why needed here: Error correction depends on interpreting Lean compiler errors to guide Reasoner refinements across multiple passes.
  - Quick check question: Given a Lean error about "unknown identifier," what retrieval or correction strategy should trigger?

## Architecture Onboarding

- Component map: GENERATEPROOF -> (Prover fails) -> SUBGOALDECOMPOSITION -> RETRIEVETHEOREMS -> GENERATEPROOFSKETCH -> REFINEANDVALIDATESKETCH -> SOLVEALLSUBGOALS -> (per subgoal) SOLVESUBGOAL -> (if needed) recursive SUBGOALDECOMPOSITION

- Critical path: GENERATEPROOF → (Prover fails) → SUBGOALDECOMPOSITION → RETRIEVETHEOREMS → GENERATEPROOFSKETCH → REFINEANDVALIDATESKETCH → SOLVEALLSUBGOALS → (per subgoal) SOLVESUBGOAL → (if needed) recursive SUBGOALDECOMPOSITION

- Design tradeoffs:
  - Prover first vs Reasoner first: Prover is cheaper; paper shows ~50-75% of subgoals solve via Prover alone.
  - Shallow solve line limit (30): Long proofs indicate need for further decomposition; prevents wasted compute.
  - Max recursion depth (D=5): Prevents infinite decomposition; ablation shows most gains by D=3.

- Failure signatures:
  - Subgoal passes syntax check but Reasoner flags as mathematically incorrect → triggers sketch refinement loop.
  - Shallow solve exceeds 30 lines and still fails → triggers recursive decomposition.
  - All K_sketch_attempts exhausted without valid assembled proof → returns failure for theorem.

- First 3 experiments:
  1. Run HILBERT on 10 MiniF2F problems with D=0 to establish Prover-only baseline, then D=2 to measure decomposition gain.
  2. Disable retrieval and measure change in reasoner calls and tokens on same 10 problems to quantify retrieval efficiency.
  3. Trace a single PutnamBench problem through the full pipeline, logging which strategy (Prover/shallow/recursive) succeeds at each subgoal level to understand depth distribution.

## Open Questions the Paper Calls Out

- Can HILBERT-generated proofs and reasoning traces effectively train improved prover and reasoner models to create a self-improving cycle?
- What are the fundamental failure modes preventing HILBERT from solving the remaining 30% of PutnamBench problems and closing the gap to the 83% informal reasoning ceiling?
- How does HILBERT's performance generalize beyond competition mathematics to research-level mathematics and other formal systems?
- What is the optimal trade-off between recursion depth, inference-time compute, and solution quality?

## Limitations

- Hyperparameter Transparency: The paper does not fully specify all algorithmic hyperparameters, particularly the number of correction attempts and Lean tactic hints used in prompts.
- Context Window Scaling: While HILBERT generates substantially longer proofs, the framework's effectiveness at even larger scales remains untested.
- Synthetic Data Generation: The dataset augmentation process using GPT-4 to generate additional problems raises questions about distribution shift.

## Confidence

**High Confidence Claims**:
- HILBERT's recursive decomposition approach demonstrably outperforms baseline methods on both miniF2F (99.2% vs 92.6%) and PutnamBench (70.0% vs 50.4%)
- The multi-agent orchestration (Prover + Reasoner) provides complementary capabilities that exceed single-agent approaches
- Retrieval augmentation reduces computational cost while improving success rates

**Medium Confidence Claims**:
- The specific hyperparameter choices (D=5, 30-line shallow solve limit) are optimal for general theorem proving
- The performance gap versus proprietary systems will persist across different mathematical domains
- The framework's scalability to larger, more complex proofs beyond PutnamBench

**Low Confidence Claims**:
- Claims about HILBERT being "the first framework" to combine recursive decomposition with specialized provers at inference time
- Predictions about performance on completely different theorem proving domains not tested
- Generalizability of the specific prompt engineering approaches to other formal systems

## Next Checks

1. **Ablation Study Replication**: Systematically reproduce the ablation experiments from Section 4.3 by running HILBERT with D=0, D=2, and D=3 on miniF2F to verify the reported pass rate improvements and confirm that most gains occur by D=3.

2. **Retrieval Impact Measurement**: Disable the retrieval component and measure changes in reasoner call counts and total tokens consumed across a representative sample of PutnamBench problems to quantify the computational efficiency gains reported in the paper.

3. **Failure Mode Analysis**: Select 20 PutnamBench problems that HILBERT solved successfully and 20 that it failed, then trace through the complete execution pipeline to categorize failure modes and validate the paper's characterization of subgoal extraction errors versus mathematical impossibility versus type system issues.