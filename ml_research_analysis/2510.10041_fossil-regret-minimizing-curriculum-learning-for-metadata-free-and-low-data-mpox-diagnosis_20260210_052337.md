---
ver: rpa2
title: 'FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data
  Mpox Diagnosis'
arxiv_id: '2510.10041'
source_url: https://arxiv.org/abs/2510.10041
tags:
- curriculum
- learning
- difficulty
- across
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FOSSIL, a regret-minimizing curriculum learning
  framework that adaptively weights training samples based on their estimated difficulty
  using softmax-based uncertainty. Applied to Mpox skin lesion diagnosis, FOSSIL significantly
  improves classification performance across diverse deep learning architectures without
  metadata, manual curation, or synthetic augmentation.
---

# FOSSIL: Regret-Minimizing Curriculum Learning for Metadata-Free and Low-Data Mpox Diagnosis

## Quick Facts
- arXiv ID: 2510.10041
- Source URL: https://arxiv.org/abs/2510.10041
- Reference count: 40
- Primary result: FOSSIL improves Mpox skin lesion diagnosis with AUC=0.9573 and ECE=0.053 without metadata or augmentation.

## Executive Summary
This study introduces FOSSIL, a regret-minimizing curriculum learning framework that adaptively weights training samples based on their estimated difficulty using softmax-based uncertainty. Applied to Mpox skin lesion diagnosis, FOSSIL significantly improves classification performance across diverse deep learning architectures without metadata, manual curation, or synthetic augmentation. The proposed method achieves state-of-the-art discrimination (AUC = 0.9573), calibration (ECE = 0.053), and robustness under real-world perturbations, while maintaining interpretability through localized activation patterns. Statistical validation confirms significant improvements over baseline training across all models tested. FOSSIL provides a generalizable, data-efficient solution for small and imbalanced biomedical datasets, with potential applications extending beyond dermatology to radiology, histopathology, genomics, and other data-constrained domains.

## Method Summary
FOSSIL is a regret-minimizing curriculum learning framework that weights training samples based on estimated difficulty. The method computes difficulty as d_i = 1 - max_c p(c)_i, where p(c)_i is the softmax probability for sample i. Training samples are weighted exponentially as w_i = exp(-d_i/T), where T is a temperature parameter. The framework uses a Weighted Binary Cross-Entropy loss and incorporates architecture-specific regularization (SAM for transformers, freezing for CNNs). FOSSIL is metadata-free and requires no synthetic augmentation, making it suitable for low-data biomedical applications.

## Key Results
- Achieves state-of-the-art discrimination with AUC = 0.9573 and calibration with ECE = 0.053
- Demonstrates significant improvements over baseline training across all tested architectures (DenseNet, ConvNeXt, DeiT, MaxViT)
- Maintains interpretability through localized activation patterns and shows robustness under real-world perturbations
- Statistical validation confirms significant improvements with p-values indicating reliable performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exponentially decaying sample weights based on estimated difficulty minimize cumulative regret and stabilize gradient descent.
- **Mechanism:** The framework assigns a weight w_i = exp(-d_i/T) where d_i is difficulty. By down-weighting high-loss or high-uncertainty samples (large d_i) during early training (low T), the optimizer takes gradient steps primarily on "reliable" data. This reduces gradient variance, preventing the model from overfitting to noisy or ambiguous labels in the small-data regime, effectively smoothing the optimization landscape.
- **Core assumption:** The "difficulty" metric (d_i) correlates with noise or class overlap rather than intrinsic signal, and the loss landscape benefits from initial smoothing.
- **Evidence anchors:** Abstract: "...regret-minimizing weighting framework that adaptively balances training emphasis according to sample difficulty." Section 2.2/2.4: "The proposed regret-minimizing weighting scheme smooths gradient variance and ensures consistent convergence under imbalance or noise." Corpus: The related "FOSSIL" preprint (arXiv:2509.13218) establishes the theoretical bounds (O(sqrt(T)) regret) leveraged here.
- **Break condition:** If the difficulty metric is inversely correlated with label noise (e.g., hard samples are the only source of signal), this weighting will discard informative gradients, leading to underfitting.

### Mechanism 2
- **Claim:** Softmax-based uncertainty provides a class-agnostic, metadata-free proxy for sample difficulty.
- **Mechanism:** Difficulty is defined as d_i = 1 - max_c p(c)_i. Samples where the model's predicted probability is low (high uncertainty) are treated as "hard." This allows the curriculum to be derived dynamically from the model's own confidence rather than external heuristics (like lesion size), ensuring the curriculum adapts to the specific architecture's failure modes.
- **Core assumption:** Model uncertainty (softmax entropy/probability) reflects true sample ambiguity or overlap with other classes, not just model miscalibration.
- **Evidence anchors:** Section: Difficulty Estimation: "This measure provides a class-agnostic scalar of model confidence... Empirical quantiles... defined four curriculum stages." Section: Results: "In the Very Hard case... difficulty here captures not visual obscurity but the model's intrinsic uncertainty in differentiating visually analogous dermatological patterns." Corpus: "Bandit Guided Submodular Curriculum" (arXiv:2511.22944) supports the general difficulty of defining robust difficulty metrics, validating the need for this uncertainty-based approach.
- **Break condition:** On uncalibrated models (e.g., deep transformers with sharp softmax distributions), this metric may falsely classify out-of-distribution samples as "Easy" (high confidence but wrong).

### Mechanism 3
- **Claim:** Architecture-specific regularization (SAM vs. Freezing) is required to augment the curriculum in low-data regimes.
- **Mechanism:** The paper observes that CNNs and Transformers respond differently to data scarcity. CNNs (DenseNet, ConvNeXt) benefit from partial freezing to stabilize feature extractors. Transformers (DeiT, MaxViT) require Sharpness-Aware Minimization (SAM) to flatten the loss landscape, preventing them from overfitting to the limited variety of samples even when weighted.
- **Core assumption:** Generic curriculum weighting alone is insufficient to stabilize specific architectural inductive biases in the small-data limit.
- **Evidence anchors:** Table 5: "Convolutional models... incorporated partial backbone freezing... whereas transformer-based models... employed [SAM] to promote flatter minima." Section: Results: "DeiT-S experienced severe degradation under the SAM optimizer [in specific contexts]... highlighting the need for calibration-aware strategies." Corpus: "RSwinV2-MD" (arXiv:2601.01835) reinforces that Mpox detection relies heavily on architectural choice for lesion classification.
- **Break condition:** Applying SAM to models that are already well-calibrated or under-parametrized may unnecessarily slow convergence without improving generalization.

## Foundational Learning

- **Concept:** **Online Convex Optimization (Regret Minimization)**
  - **Why needed here:** FOSSIL is grounded in theoretical regret bounds (R_T). Understanding that the algorithm minimizes the difference between cumulative loss and the "hindsight optimum" is crucial to interpreting why it generalizes well despite limited data.
  - **Quick check question:** Can you explain why minimizing an upper bound on regret (O(sqrt(T))) implies stable convergence in a non-convex deep learning setting?

- **Concept:** **Curriculum Learning (Self-Paced Learning)**
  - **Why needed here:** The method relies on organizing data from "Easy" to "Very Hard." Distinguishing between predefined curricula (static) and self-paced learning (dynamic based on current loss) is necessary to implement the weight scheduling correctly.
  - **Quick check question:** How does the temperature parameter T in w_i = exp(-d_i/T) control the trade-off between focusing on easy samples versus treating all samples equally?

- **Concept:** **Model Calibration (Expected Calibration Error - ECE)**
  - **Why needed here:** The study prioritizes ECE alongside AUC. You must understand that a model can have high discrimination (AUC) but poor reliability (miscalibrated probabilities), which is dangerous in clinical screening.
  - **Quick check question:** If a model has high ECE, does that mean its AUC is necessarily low? (Answer: No; they are independent aspects of performance).

## Architecture Onboarding

- **Component map:**
  1. **Backbone:** Pretrained ImageNet model (DenseNet121, ConvNeXt-T, etc.)
  2. **Difficulty Estimator:** Uses the backbone's softmax output p(c) to compute scalar d_i
  3. **Weighting Engine:** Calculates w_i = exp(-d_i/T)
  4. **Loss Aggregator:** Computes weighted BCE loss (1/N) sum w_i l_i
  5. **Stabilizer:** Optional SAM (Transformers) or Freezing (CNNs)

- **Critical path:**
  1. **Initialize:** Load pretrained weights. Set initial Temperature T high or low depending on strategy (paper uses T as an exploration/exploitation balance)
  2. **Estimate:** Pass training batch through model; calculate d_i and w_i for the batch
  3. **Update:** Backpropagate using weighted loss
  4. **Schedule:** Adjust T (or stage boundaries) if using dynamic curriculum progression

- **Design tradeoffs:**
  - **Entropy vs. Softmax:** The paper abandoned entropy H(x) for softmax d_i = 1-max p(c) because entropy was statistically unstable across folds (Appendix A.2)
  - **SAM vs. Freezing:** Transformers without SAM or CNNs without freezing showed "borderline" or "yes" overfitting in Table 7

- **Failure signatures:**
  - **Skyrocketing Loss:** If T is too low, weights for hard samples approach zero, effectively removing data and causing the model to see only a subset of the distribution
  - **Class Bias:** If the difficulty metric accidentally correlates with the minority class (e.g., all Mpox samples are "Hard"), the model will down-weight the target disease. (The paper validates against this with a Mann-Whitney U test, p=0.544)

- **First 3 experiments:**
  1. **Baseline vs. Weighted:** Train a single architecture (e.g., ConvNeXt-T) with standard BCE vs. FOSSIL weights. Confirm AUC and ECE improvements on the MSLD v2 dataset
  2. **Metric Ablation:** Compare Entropy-based difficulty vs. Softmax-based difficulty on fold stability. Verify that Entropy yields skewed distributions (Appendix A.1)
  3. **External Validation:** Train on MSLD v2 and test zero-shot on MCSI. Check if the "Easy" samples in MSLD correlate with high confidence on MCSI to ensure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would adaptive temperature scheduling for the exploration–exploitation parameter T improve FOSSIL's convergence and final performance compared to the fixed-T setting used in this study?
- Basis in paper: [explicit] Section 4.3 states: "Future work will include... adaptive temperature-scheduling strategies." The Conclusion reiterates: "Future research should explore adaptive temperature scheduling."
- Why unresolved: The current implementation uses a fixed temperature T throughout training, but Lemma A.5.2 shows that monotonically decreasing T would create a self-paced schedule. The theoretical benefits of adaptive scheduling remain untested empirically.
- What evidence would resolve it: A comparative study evaluating FOSSIL with scheduled T decay (e.g., exponential or step-wise reduction) versus fixed T across multiple architectures and datasets, measuring convergence speed, final AUC, and calibration metrics.

### Open Question 2
- Question: Can FOSSIL's performance gains be replicated in prospective, multi-institutional clinical deployments, or do they depend on the retrospective validation setup?
- Basis in paper: [explicit] Section 4.3 acknowledges: "Despite promising results, the current evaluation remains retrospective. Future work will include multi-institutional external validation." And "Prospective clinical trials should assess the safety, interpretability, and decision impact of models trained under the proposed weighting framework in real-world workflows."
- Why unresolved: External validation used only one additional dataset (MCSI), and all experiments were conducted retrospectively on pre-collected images. Real-world deployment involves distribution shifts, labeling noise, and workflow constraints not captured in the study.
- What evidence would resolve it: A prospective multi-center trial where FOSSIL-trained models are deployed in clinical settings across geographically diverse hospitals, with comparison to baseline models on patient-level outcomes, calibration reliability, and clinician trust metrics.

### Open Question 3
- Question: Why does the combination of FOSSIL with Sharpness-Aware Minimization (SAM) degrade performance in transformer-based models like DeiT-S?
- Basis in paper: [inferred] Table 8 and Section 3 show that DeiT-S under FOSSIL + SAM ("FROZEN" setting) experienced severe degradation (AUC 0.4838, marked as overfitting), while FOSSIL alone improved performance. The paper notes "transformer-based models may be more sensitive to curriculum–optimizer interactions" but does not investigate the mechanism.
- Why unresolved: The interaction between regret-minimizing weighting and sharpness-aware optimization is theoretically unclear. Whether this is due to gradient conflict, learning rate incompatibility, or architectural sensitivity remains unknown.
- What evidence would resolve it: An ablation study isolating the effects of SAM and FOSSIL across transformer architectures, analyzing gradient statistics, loss landscape curvature, and convergence behavior to identify the failure mode.

### Open Question 4
- Question: Does FOSSIL generalize to other data-constrained biomedical domains (e.g., histopathology, radiology, genomics) with comparable gains in discrimination and calibration?
- Basis in paper: [explicit] Table 10 and Section 4.2 claim the framework is applicable to radiology, histopathology, microscopy, omics, and longitudinal monitoring, but all empirical validation in this study is limited to dermatological Mpox images.
- Why unresolved: Different modalities have distinct noise characteristics, class imbalance patterns, and spatial structures. Whether softmax-based difficulty estimation and regret-minimizing weighting transfer effectively is untested.
- What evidence would resolve it: Benchmarking FOSSIL on at least two additional biomedical domains (e.g., chest X-ray classification, histopathology slide analysis) with small and imbalanced datasets, reporting AUC, ECE, and robustness metrics against domain-appropriate baselines.

## Limitations
- Temperature parameter T is not explicitly specified, which could significantly impact curriculum balance
- Architecture-specific stabilizers (SAM vs. Freezing) suggest the method may not be universally applicable without careful tuning
- All validation is retrospective; prospective multi-institutional clinical deployment remains untested

## Confidence
- **High Confidence:** The core mechanism of using softmax-based uncertainty as a difficulty metric and the general improvement in AUC (0.9573) and ECE (0.053) are well-supported by the experimental results
- **Medium Confidence:** The claim that the method is "metadata-free" and the assertion of generalizability to other biomedical domains are plausible but require more extensive validation beyond Mpox and the two datasets tested
- **Low Confidence:** The theoretical justification for the O(sqrt(T)) regret bound in the non-convex deep learning setting is not rigorously established in this paper

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary the temperature parameter T (e.g., T = 0.1, 0.5, 1.0, 2.0) and report the impact on both performance (AUC, ECE) and training stability (loss curves) to identify an optimal range
2. **Architectural Ablation Study:** Implement FOSSIL on a purely CNN architecture (e.g., ResNet) without any freezing and on a purely Transformer architecture (e.g., ViT) without SAM to isolate the effect of the curriculum weighting from the architectural stabilizers
3. **Cross-Domain Generalization Test:** Apply the FOSSIL-weighted model, trained on MSLD v2, to a completely different biomedical image classification task (e.g., a small histopathology dataset) and report both in-distribution and out-of-distribution performance to test the claim of broad applicability