---
ver: rpa2
title: 'Translation Analytics for Freelancers: I. Introduction, Data Preparation,
  Baseline Evaluations'
arxiv_id: '2504.14619'
source_url: https://arxiv.org/abs/2504.14619
tags:
- translation
- evaluation
- language
- machine
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates how freelance translators can adopt advanced
  automatic evaluation metrics like BLEU, chrF, TER, and COMET to assess MT and LLM
  outputs with rigor and precision. Using a trilingual medical corpus, the authors
  show that automatic scores strongly correlate with human judgments, even for small
  document samples, validating their practical utility for quality assessment.
---

# Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations

## Quick Facts
- **arXiv ID**: 2504.14619
- **Source URL**: https://arxiv.org/abs/2504.14619
- **Reference count**: 40
- **Primary result**: Automatic metrics (BLEU, chrF, TER, COMET) correlate strongly with human judgment for translation quality, validating their use by freelancers

## Executive Summary
This study demonstrates how freelance translators can adopt advanced automatic evaluation metrics like BLEU, chrF, TER, and COMET to assess MT and LLM outputs with rigor and precision. Using a trilingual medical corpus, the authors show that automatic scores strongly correlate with human judgments, even for small document samples, validating their practical utility for quality assessment. The findings highlight that neural-based metrics, especially COMET, align closely with human evaluations and are effective across diverse language pairs. This approach empowers freelancers to make informed decisions about translation tools, optimize workflows, and add technical value to their services. The research underscores the importance of integrating human expertise with automated methods to adapt to evolving translation technologies.

## Method Summary
The study evaluates MT and LLM translation quality using automatic metrics (BLEU, chrF2, TER, COMET) and correlates results with human judgment. The Christopher & Dana Reeve Foundation Trilingual Corpus (EN-RU-JA, medical domain) serves as the test bed, with 3555-4528 segments depending on version. Translations are generated via CAT tool pre-translation for MT engines and REST API calls for LLMs, with sentence-by-sentence processing and temperature=0. Metrics are computed using MATEO with language-specific configurations. Human correlation analysis involves grading segments stratified by COMET score, with both Pearson and Spearman correlations calculated to assess metric-human alignment.

## Key Results
- Document-level metric rankings remain stable across non-overlapping samples (229, 1143, 2183 segments), with correlations r > 0.85 for most pairs
- Neural metrics, especially COMET, show stronger correlation with human judgment than string-based metrics for EN-RU and EN-JA pairs
- Sentence-level COMET scores correlate moderately to strongly (0.55-0.93 Pearson) with human grades, supporting their use for prioritizing review

## Why This Works (Mechanism)

### Mechanism 1: Neural metrics capture semantic similarity beyond surface form
- Claim: COMET correlates more strongly with human judgment than string-based metrics, especially for linguistically distant pairs.
- Mechanism: COMET uses a pre-trained neural model fine-tuned on human quality assessments, enabling it to evaluate meaning rather than exact word overlap. This allows valid translations using synonyms or restructuring to score well even when surface forms diverge.
- Core assumption: Human quality judgments used to train COMET generalize to new domains and language pairs not in training data.
- Evidence anchors:
  - [abstract] "neural-based metrics, especially COMET, align closely with human evaluations"
  - [section 6.3] "Most of the r and ρ values suggest moderate to strong correlation" between sentence-level COMET and human grades
  - [corpus] Related work HiMATE discusses MQM-based LLM evaluation but lacks direct comparison for COMET; corpus evidence is thin on cross-domain generalization.
- Break condition: If COMET's training data is contaminated by the test corpus, correlations would be inflated; low-resource domains may show weaker alignment.

### Mechanism 2: Stratified sampling preserves system rankings across document scales
- Claim: Automatic metric rankings remain stable even when computed on small non-overlapping document partitions.
- Mechanism: By computing metrics on disjoint subsections (229, 1143, 2183 segments) and comparing system rankings, the approach shows that relative quality judgments are robust to sample size. Pearson correlations between subsection scores are very strong (r > 0.85 for most pairs).
- Core assumption: The corpus is internally coherent (same domain, register) so that subsections are representative of the whole.
- Evidence anchors:
  - [section 5] "correlations are very strong in all cases, across all the metrics" with r values 0.86-0.99
  - [tables 13-14] EN-RU and EN-JA correlations across three document splits show p < 0.05 in all cases
  - [corpus] No corpus neighbor directly validates sample-size robustness for MT evaluation; evidence is internal to the paper.
- Break condition: If subsections differ significantly in difficulty or topic, rankings could diverge; power analysis needed to determine minimal viable sample.

### Mechanism 3: Sentence-level COMET enables targeted human review prioritization
- Claim: Segment-level COMET scores can identify high/low-quality outputs for selective human grading.
- Mechanism: By ranking segments by COMET score and sampling from top/median/bottom tiers, evaluators can efficiently characterize system performance without grading all outputs. Correlation between sentence-level COMET and human grades supports this prioritization.
- Core assumption: Sentence-level metric scores are reliable enough for ranking despite higher variance than corpus-level scores.
- Evidence anchors:
  - [section 6.1] Segments selected by COMET rank (10 highest, 10 median, 10 lowest) for human grading
  - [section 6.3] Moderate-to-strong Pearson (0.55-0.93) and Spearman (0.44-0.91) correlations for sentence-level COMET vs. human grades
  - [corpus] HiMATE paper discusses hierarchical LLM-based evaluation but does not validate sentence-level metric-human correlation; corpus support is weak.
- Break condition: If sentence-level scores have high noise, ranking-based sampling may misidentify problem segments; aggregating over small batches may be safer.

## Foundational Learning

- Concept: String-based vs. neural evaluation metrics
  - Why needed here: The paper uses four metrics with distinct mechanisms; understanding BLEU/chrF/TER (surface matching) vs. COMET (semantic embedding) is essential for interpreting discrepancies.
  - Quick check question: If a translation uses correct synonyms but no shared n-grams with the reference, which metric family would likely score it higher?

- Concept: Pearson vs. Spearman correlation
  - Why needed here: The paper reports both for metric-human alignment; Pearson assumes linearity while Spearman uses ranks and is robust to outliers.
  - Quick check question: If human grades are ordinal (A/B/C/D/F) rather than continuous, which correlation coefficient is more appropriate?

- Concept: Bootstrap resampling for metric confidence intervals
  - Why needed here: MATEO computes metric scores with bootstrap confidence intervals; understanding this helps interpret whether score differences are significant.
  - Quick check question: If BLEU scores of 41.1 ± 1.5 and 42.3 ± 1.8 overlap at their bounds, can you confidently claim one system is better?

## Architecture Onboarding

- Component map: Corpus preparation (Excel → aligned TMX/text files, segment ID tracking) -> Translation generation (MT engines via CAT tool pre-translation; LLMs via REST API with structured prompts) -> Metric computation (MATEO for BLEU/chrF/TER/COMET; language-specific tokenization for Japanese) -> Correlation analysis (Pearson/Spearman between metric scores and human grades)

- Critical path:
  1. Clean corpus → remove tags, short segments, number-only segments
  2. Generate translations → one sentence per API call, temperature=0, explicit prompt to suppress extraneous output
  3. Compute metrics → COMET takes ~16 min per output; configure tokenization per language
  4. Sample and grade → stratify by COMET score, blind grading by ID not score

- Design tradeoffs:
  - Sentence-by-sentence LLM calls vs. batch processing: sentence-level gives consistent formatting but loses context; batch is faster but output formatting is inconsistent.
  - Sample size vs. confidence: 229 segments (6.4% of corpus) yields stable rankings, but smaller samples need power analysis.
  - Human grading scale: 0-4 linear scale is intuitive but less rigorous than MQM; faster for freelancers.

- Failure signatures:
  - LLM outputs include extraneous text ("Here is the translation: ") → prompt insufficiently explicit
  - COMET scores flat across systems → possible data contamination or metric saturation
  - Metric-human correlation near zero → grading not blind to system identity, or domain mismatch between metric training and test data
  - Japanese BLEU unusually low → tokenization misconfigured (should use ja-mecab)

- First 3 experiments:
  1. Replicate correlation analysis on a held-out section of the corpus to validate that sample-size findings generalize.
  2. Compare string-based vs. neural metric rankings for a language pair with high morphological complexity to stress-test chrF vs. COMET.
  3. Test whether adding a brief domain context to LLM prompts improves metric scores (e.g., "You are translating a medical guide for patients with spinal cord injuries").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does maintaining a running context window in LLM API requests yield significant improvements in translation quality compared to isolated, sentence-by-sentence processing?
- **Basis in paper**: [explicit] The authors acknowledge they instantiated a *de novo* context for every request and state, "We did not attempt to evaluate the impact of context windows... Still, this may be worth exploring in future research" (Section 3.3).
- **Why unresolved**: The study deliberately restricted its methodology to isolated sentences to limit scope and variable complexity, leaving the potential benefits of conversational context untested.
- **What evidence would resolve it**: A comparative experiment evaluating identical source texts using isolated API calls versus context-aware sessions, measuring the delta in automatic metric scores (e.g., COMET) and human evaluation.

### Open Question 2
- **Question**: What is the minimal sample size required to achieve statistically significant power when evaluating translation quality in this specific freelance context?
- **Basis in paper**: [explicit] The authors note that their correlation experiments "need to be complemented with power analysis to determine the minimal size of the statistically significant sample" (Section 7).
- **Why unresolved**: While the authors empirically found strong correlations with a 229-segment sample, they did not perform formal power analysis to define the precise lower bound for reliability.
- **What evidence would resolve it**: Formal statistical power analysis (using toolkits like NLPStatTest) across varying sample sizes to identify the threshold where automatic metric reliability degrades.

### Open Question 3
- **Question**: Can glossaries automatically extracted from parallel sentences by LLMs be effectively utilized to improve the quality of subsequent translations in a freelancer's workflow?
- **Basis in paper**: [explicit] The authors list future plans to "explore the potential... for (i) extracting a bilingual glossary from a set of parallel sentences, and (ii) using a glossary thus obtained to improve the quality of translation" (Section 7).
- **Why unresolved**: This paper focused solely on baseline evaluations and metric correlations; the utility of LLMs for resource extraction and workflow integration remains untested in this framework.
- **What evidence would resolve it**: Experiments using the RFTC corpus to prompt LLMs for glossary extraction, followed by translations prompted with these glossaries, evaluated via automatic metrics and human review.

## Limitations
- Data contamination risk: COMET's training corpus may overlap with the Reeve medical corpus, potentially inflating correlations
- Sample size uncertainty: No established minimum for reliable sentence-level metric-human correlations
- Limited language pair scope: Findings primarily validated for EN-RU and EN-JA, not morphologically complex languages beyond Japanese

## Confidence
- **High Confidence**: Document-level metric rankings remain stable across non-overlapping samples when corpus is internally coherent; neural metrics (COMET) show stronger correlation with human judgment than string-based metrics for EN-RU and EN-JA pairs; the overall approach of combining automatic metrics with selective human review is practically implementable
- **Medium Confidence**: Sentence-level COMET scores can reliably prioritize segments for human review; findings generalize to similar medical-domain translation tasks; the 6.4% sample size is sufficient for stable document-level rankings
- **Low Confidence**: Cross-domain generalization of COMET correlations (e.g., to legal or technical domains); minimum viable sample size for sentence-level metric reliability; metric-human correlation strength for morphologically complex languages beyond Japanese

## Next Checks
1. **Cross-Domain Validation**: Apply the same evaluation methodology to a non-medical corpus (e.g., legal or technical documentation) to test whether COMET's superior correlation with human judgment generalizes beyond the medical domain.

2. **Sample Size Power Analysis**: Systematically reduce the document sample size and recompute metric rankings to determine the minimum number of segments required for stable system-level comparisons, particularly for sentence-level analysis.

3. **Blind Grading Verification**: Conduct a controlled experiment where the same human graders evaluate segments with and without knowledge of the translation system source to quantify the impact of rater bias on metric-human correlation results.