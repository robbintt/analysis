---
ver: rpa2
title: It's 2025 -- Narrative Learning is the new baseline to beat for explainable
  machine learning
arxiv_id: '2510.09723'
source_url: https://arxiv.org/abs/2510.09723
tags:
- learning
- narrative
- data
- dataset
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Narrative Learning is a new explainable ML method that defines
  models entirely in natural language and iteratively refines classification rules
  using LLM-based overseers and underlings. Experiments on 3 synthetic and 3 natural
  datasets showed that Narrative Learning ensembles achieved higher accuracy than
  traditional explainable baselines (logistic regression, decision trees, rulefit,
  etc.) on 5 out of 6 datasets by 2025, with accuracy improvements ranging from 6.5%
  to 27.1% depending on the dataset.
---

# It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning

## Quick Facts
- arXiv ID: 2510.09723
- Source URL: https://arxiv.org/abs/2510.09723
- Authors: Gregory D. Baker
- Reference count: 40
- Key outcome: Narrative Learning ensembles achieved higher accuracy than traditional explainable baselines on 5 out of 6 datasets by 2025, with accuracy improvements ranging from 6.5% to 27.1%

## Executive Summary
Narrative Learning introduces a novel explainable ML approach that defines models entirely in natural language through iterative refinement of classification rules using LLM-based overseers and underlings. The method demonstrates statistically significant accuracy improvements over traditional explainable baselines (logistic regression, decision trees, rulefit) on 5 of 6 tested datasets, with gains ranging from 6.5% to 27.1%. By maintaining interpretability through lexically stable, human-readable narratives while achieving competitive accuracy, Narrative Learning establishes itself as a new baseline for explainable machine learning in 2025.

## Method Summary
Narrative Learning is a supervised binary classification method that uses LLMs to iteratively refine natural language classification rules. The system employs an overseer LLM to generate classification prompts and an underling LLM to apply these rules to data points. Performance metrics and misclassification examples flow back to the overseer, which refines rules in subsequent rounds. The process terminates when validation scores stop improving for 3 rounds, and final predictions are made through majority voting of three independently-trained narrative models. The approach was tested on 3 synthetic and 3 natural datasets with anonymized features.

## Key Results
- Narrative Learning ensembles outperformed traditional explainable baselines on 5 out of 6 datasets tested
- Accuracy improvements ranged from 6.5% to 27.1% compared to logistic regression, decision trees, and rulefit methods
- Statistical analysis showed significant accuracy trends over time (p=0.002) with improvements stabilizing after 5 rounds
- Herdan's coefficient analysis confirmed that explanation complexity remained stable despite accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1: LLM as Rule Generator (Overseer-Underling Division of Labor)
Separating rule generation (overseer) from rule execution (underling) enables iterative refinement analogous to hypothesis testing in science. The overseer LLM generates natural language classification rules while the underling LLM applies these rules to individual data points, with performance metrics and misclassification examples flowing back to the overseer for refinement.

### Mechanism 2: Early-Stopping Feedback Loop
Validation-based early stopping prevents overfitting while allowing sufficient rule refinement. After each iteration, validation accuracy is checked, and the process terminates when validation scores stop improving for 3 rounds, ensuring the model generalizes beyond the training data.

### Mechanism 3: Ensemble Variance Reduction via Majority Vote
Ensembling three independently-trained narrative models reduces high individual variance and improves predictive stability. Three overseer-underling pairs are trained independently with different random seeds, and their predictions are combined via majority vote, with the best-performing trio selected on validation data.

## Foundational Learning

- **Supervised Binary Classification with Train/Validation/Test Splits**: Required because Narrative Learning is a supervised binary classifier; proper data partitioning prevents information leakage and enables early stopping. *Quick check*: Why must validation data never influence the final model selection more than once?

- **LLM Instruction-Following and Prompt Engineering**: Critical because the system's success depends on both overseer generating coherent rules and underling correctly interpreting them. *Quick check*: What properties make a natural language instruction "executable" by an LLM?

- **Lexicostatistics and Readability Metrics**: Important because the paper uses Herdan's coefficient to verify that explanations remain comprehensible as accuracy improves. *Quick check*: Why might increasing model accuracy correlate with decreasing explanation comprehensibility?

## Architecture Onboarding

- Component map: Training Data -> Overseer LLM -> Narrative Prompt -> Underling LLM -> Performance Metrics -> Feedback to Overseer -> Ensemble Selection (3 models) -> Test Data -> Final Predictions

- Critical path:
  1. Transform/anonymize dataset to prevent LLM from using prior knowledge
  2. Initialize with random-guess prompt
  3. Run 3-iteration refinement loop (overseer → underling → metrics → overseer)
  4. Train multiple (3+) independent narrative models
  5. Select best ensemble triple on validation data
  6. Evaluate final ensemble on held-out test data

- Design tradeoffs:
  - Model selection: Larger frontier models for overseer vs. cheaper models for underling (phi-4 failed to follow instructions)
  - Example count per round: Tested 3 vs 10 examples; Wilcoxon p=0.753 showed no significant difference
  - Ensemble size: Triples used; larger ensembles may improve accuracy but increase cost
  - Patience: 3 rounds chosen empirically; longer patience may be needed for noisier datasets

- Failure signatures:
  - Underling misinterprets rules → use frontier models instead of smaller models
  - Rules are incomplete or impossible to apply → overseer prompt engineering needed
  - High variance across runs → increase ensemble size or reduce temperature
  - LLM uses prior knowledge → strengthen data anonymization/transformation

- First 3 experiments:
  1. Reproduce on a simple 2D synthetic dataset (similar to "Espionage" with 0% noise) to validate pipeline functionality with minimal cost
  2. Ablate underling model choice (gpt-4o-mini vs gpt-4.1-mini vs alternatives) to measure sensitivity to instruction-following capability
  3. Vary temperature settings on the underling to quantify inference stochasticity effects on accuracy and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the ensemble size beyond three members yield accuracy benefits that justify the additional training cost and complexity?
- Basis: The authors state in Section 2: "An open question is identifying where there is benefit in using larger ensembles: there will be an additional training cost, but presumably a larger ensemble will be more accurate."
- Why unresolved: Experiments uniformly used ensembles of three models to manage variance; larger groups were not tested.

### Open Question 2
- Question: Can the Narrative Learning methodology be effectively extended to multi-class classification or regression tasks?
- Basis: Section 6 explicitly lists as future work: "How would a multi-class Narrative Learning classifier work?" and "How would a Narrative Learning regressor work?"
- Why unresolved: The algorithm definition in Section 2 and all subsequent experiments were restricted to "supervised binary classification."

### Open Question 3
- Question: How stable are the generated narratives and accuracy scores across multiple identical runs of the same model on the same dataset?
- Basis: Section 6 asks: "If we run Narrative Learning multiple times using the same model on the same dataset, do we get similar results? Are the narratives similar?"
- Why unresolved: While the paper mentions that "individual narratives have high variance," it does not quantify the reproducibility of the final ensemble's performance or the semantic consistency of the rules generated.

### Open Question 4
- Question: Do human experts improve the narrative quality when collaborating with the overseer, compared to purely AI-driven processes?
- Basis: Section 6 asks: "Are there ways for human experts to work with the overseers to guide them to better hypotheses? How do the results compare to purely AI-driven overseers?"
- Why unresolved: The current system is fully automated; the interaction loop currently excludes human domain expertise.

## Limitations
- The study relies on small synthetic datasets (200 points each) and only 3 natural domains, limiting generalizability
- Computational costs are substantial due to reliance on frontier LLMs for both rule generation and execution
- Early-stopping configuration (patience=3 rounds) was empirically chosen without systematic validation across diverse data distributions
- The relationship between Herdan's coefficient stability and actual human interpretability remains unverified beyond statistical measures

## Confidence

- **High Confidence**: The core mechanism of iterative rule refinement through overseer-underling LLM interaction is well-documented and reproducible
- **Medium Confidence**: The claimed accuracy improvements (6.5%-27.1%) over traditional baselines are based on specific datasets and experimental conditions
- **Low Confidence**: The optimal configuration parameters (ensemble size, patience period, example count) lack systematic exploration

## Next Checks
1. **Ablation Study on Underling Models**: Systematically test different LLM capabilities (phi-4, gpt-4o-mini, gpt-4.1-mini, frontier models) as underlings to quantify the impact of instruction-following reliability on final accuracy
2. **Cross-Dataset Generalization Test**: Apply the complete Narrative Learning pipeline to 3-5 additional real-world binary classification datasets from different domains to assess whether accuracy improvements hold across varied data distributions
3. **Interpretability Validation with Human Evaluators**: Conduct user studies where domain experts assess the comprehensibility and trustworthiness of narrative explanations across different accuracy levels, measuring whether Herdan's coefficient stability correlates with actual human understanding