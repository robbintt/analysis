---
ver: rpa2
title: You Only Train Once
arxiv_id: '2506.04349'
source_url: https://arxiv.org/abs/2506.04349
tags:
- loss
- optimization
- which
- yoto
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces You Only Train Once (YOTO), a method that
  automatically optimizes loss weight hyperparameters during a single training run
  by treating them as regular parameters of the network. The key innovation is reformulating
  the composite loss as a differentiable layer parameterized by a softmax operation,
  allowing standard gradient-based optimization to tune both regular parameters and
  loss weights simultaneously.
---

# You Only Train Once
## Quick Facts
- arXiv ID: 2506.04349
- Source URL: https://arxiv.org/abs/2506.04349
- Reference count: 40
- Key outcome: YOTO achieves 0.7% improvement in δ1 and 0.5% in FA metrics over grid search for monocular 3D estimation, and 0.77% mIoU improvement on semantic segmentation

## Executive Summary
This paper introduces You Only Train Once (YOTO), a method that automatically optimizes loss weight hyperparameters during a single training run by treating them as regular parameters of the network. The key innovation is reformulating the composite loss as a differentiable layer parameterized by a softmax operation, allowing standard gradient-based optimization to tune both regular parameters and loss weights simultaneously. The method also includes a novel regularization scheme that promotes uniformity among loss weights while ensuring boundedness.

Experiments on two key vision tasks—monocular 3D estimation using UniDepth and semantic segmentation using CISS—demonstrate that YOTO consistently outperforms brute-force grid search. The method shows robustness to initialization and stochasticity, converging to stable hyperparameter values across different runs.

## Method Summary
YOTO reformulates the composite loss function by introducing a learnable weight vector λ that parameterizes the loss combination through a softmax operation. This transforms the weighted sum of losses into a differentiable layer where both model parameters θ and loss weights λ can be optimized simultaneously using standard gradient-based methods. The paper introduces a novel regularization term R(λ) = λ1‖∇L‖2 + λ2‖λ‖2 that encourages uniformity among loss weights while maintaining boundedness. The total loss becomes L_total = L(θ, λ) + R(λ), where the first term is the standard composite loss and the second is the regularization. During training, gradients flow through both the model parameters and the loss weights, allowing the network to automatically discover optimal weight configurations.

## Key Results
- YOTO achieves 0.7% improvement in δ1 and 0.5% in FA metrics over the best grid-search model on monocular 3D estimation (UniDepth)
- YOTO achieves a 0.77% mIoU improvement on the ACDC test set for semantic segmentation (CISS)
- The method shows robustness to initialization, with consistent convergence patterns across different random seeds

## Why This Works (Mechanism)
The key mechanism behind YOTO is treating loss weights as learnable parameters that can be optimized through backpropagation alongside model weights. By reformulating the weighted loss as a softmax-normalized combination, the method creates a fully differentiable pipeline where the network can adapt its own training objective based on task-specific needs. The regularization scheme ensures that the learned weights remain well-behaved (uniform and bounded) while still allowing flexibility to prioritize different loss components as needed during training.

## Foundational Learning
- **Loss weight optimization**: Why needed - manual tuning of loss weights is time-consuming and suboptimal; Quick check - verify that multiple loss components exist in the target task
- **Softmax parameterization**: Why needed - ensures weights are positive and sum to one while maintaining differentiability; Quick check - confirm the implementation uses softmax over raw weight values
- **Gradient-based hyperparameter tuning**: Why needed - enables end-to-end optimization of both model and training configuration; Quick check - verify gradients flow through loss weights during backpropagation
- **Regularization for weight uniformity**: Why needed - prevents extreme weight values and promotes balanced learning; Quick check - monitor weight distribution during training for stability

## Architecture Onboarding
Component map: Input data → Backbone network → Task-specific heads → Composite loss layer (with learnable weights) → Regularization → Total loss

Critical path: Data input → Forward pass through network → Compute individual losses → Combine via softmax-weighted sum → Apply regularization → Backpropagate through both network weights and loss weights

Design tradeoffs: YOTO trades off simplicity of grid search for automation and potentially better performance. The method requires careful tuning of regularization hyperparameters (λ1, λ2) but eliminates the need to search over loss weight configurations.

Failure signatures: Poor convergence of loss weights (all weights collapsing to extreme values), instability in training loss curves, or degradation in primary task metrics compared to baseline training.

First experiments: 1) Test on a simple multi-task learning problem with known optimal weight configuration, 2) Apply to a single-task problem with auxiliary losses, 3) Compare convergence patterns with and without regularization terms

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit ones emerge from the work: generalizability to non-vision domains, optimal strategies for setting regularization hyperparameters, and scalability to problems with many loss components.

## Limitations
- Effectiveness demonstrated only on two vision tasks (monocular 3D estimation and semantic segmentation), limiting generalizability claims
- Regularization scheme introduces two new hyperparameters (λ1, λ2) that require tuning
- Computational overhead and memory usage implications are not thoroughly analyzed
- Assumes optimal loss weights can be represented as a softmax distribution, which may not hold for all loss combinations

## Confidence
High confidence in effectiveness for the tested vision tasks and metrics. Medium confidence in robustness claims due to limited seed testing. Medium confidence in computational efficiency claims without detailed timing analysis. Low confidence in generalizability beyond vision tasks based on current evidence.

## Next Checks
1. Test YOTO on non-vision tasks (e.g., NLP or audio processing) to verify generalizability across domains
2. Conduct experiments with more random seeds (e.g., 10+ seeds) to better establish robustness claims
3. Measure and report detailed computational overhead metrics including training time per epoch and memory usage compared to standard training