---
ver: rpa2
title: 'Eat your own KR: a KR-based approach to index Semantic Web Endpoints and Knowledge
  Graphs'
arxiv_id: '2508.08713'
source_url: https://arxiv.org/abs/2508.08713
tags:
- endpoints
- sparql
- dcat
- knowledge
- indegx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents IndeGx, a framework for indexing knowledge
  graphs (KGs) and their SPARQL endpoints using knowledge representation (KR) techniques.
  The approach uses a reflexive KR-based method to index and characterize existing
  KGs with respect to their usage of KR methods and techniques.
---

# Eat your own KR: a KR-based approach to index Semantic Web Endpoints and Knowledge Graphs

## Quick Facts
- arXiv ID: 2508.08713
- Source URL: https://arxiv.org/abs/2508.08713
- Authors: Pierre Maillot; Catherine Faron; Fabien Gandon; Franck Michel; Pierre Monnin
- Reference count: 5
- One-line primary result: IndeGx uses declarative KR rules to index 300+ open KGs, revealing gaps in SPARQL 1.1 and OWL RL compliance

## Executive Summary
This paper presents IndeGx, a framework for indexing knowledge graphs (KGs) and their SPARQL endpoints using knowledge representation (KR) techniques. The approach uses a reflexive KR-based method to index and characterize existing KGs with respect to their usage of KR methods and techniques. The framework employs a declarative representation of procedural knowledge and collaborative environments to provide an agile, customizable, and expressive KR approach for building and maintaining an index of KGs.

## Method Summary
The framework employs a declarative, rule-based approach to index knowledge graphs accessible via SPARQL endpoints. It uses a stratified workflow: extraction rules probe endpoints with SPARQL queries to capture features, augmentation rules normalize heterogeneous data and compute additional metadata, and an OWL RL reasoner infers complex classifications. The system logs execution traces as a reflexive knowledge graph, enabling transparency and debugging. Results are stored in an RDF index served via a public endpoint.

## Key Results
- Analyzed over 300 open knowledge graph endpoints, providing insights into the state of the Semantic Web regarding supported standard languages, ontology usage, and diverse quality evaluations
- Identified significant gaps in SPARQL 1.1 and OWL RL compliance across public endpoints
- Demonstrated the potential of KR technologies for indexing and characterizing KGs in the wild

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If knowledge graphs (KGs) are indexed using declarative SPARQL-based rules rather than hardcoded scripts, the resulting framework allows for extensible and transparent metadata extraction from diverse endpoints.
- **Mechanism:** The system uses a "Test & Action" pattern. A Test query (typically `ASK` or `SELECT`) probes a remote endpoint for a specific capability or data pattern. If the test passes, an Action query (`INSERT` or `CONSTRUCT`) executes to materialize the finding into the local index. This decouples the extraction logic from the execution engine.
- **Core assumption:** Remote SPARQL endpoints partially comply with standard query protocols and return consistent error handling so that "failure" can be reliably distinguished from "absence of data."
- **Evidence anchors:**
  - [abstract]: "...framework leveraging an extensible base of rules to index the content of KGs... relies on a declarative representation of procedural knowledge..."
  - [section 3.2]: "Tests are SPARQL queries sent to characterize the datasets... Actions are SPARQL queries designed to generate a part of a KG's description."
  - [corpus]: Weak relevance; corpus neighbors focus on generative AI and graph learning, not rule-based indexing.
- **Break condition:** If an endpoint returns syntactically correct HTTP responses for failing queries (e.g., empty result sets interpreted as false positives) or imposes aggressive rate limits that prevent the completion of the test suite.

### Mechanism 2
- **Claim:** Applying a stratified reasoning pipeline (Extraction $\to$ Augmentation $\to$ Inference) enables the system to derive high-level logical classifications (e.g., "SPARQL 1.1 Compliant") from low-level feature checks.
- **Mechanism:** Raw data is first extracted via rules. Augmentation rules then normalize this data (e.g., mapping heterogeneous properties like `dcterms:created` and `schema:dateCreated` to a common view). Finally, an OWL RL reasoner applies ontological definitions to infer complex types or equivalence relations that would be computationally expensive or verbose to express via direct SPARQL queries.
- **Core assumption:** The specific subset of OWL RL used (e.g., intersection classes, property restrictions) is sufficient to capture the relevant characteristics of the indexed domain without requiring full OWL DL expressivity.
- **Evidence anchors:**
  - [abstract]: "Using SPARQL rules and an OWL RL ontology of the indexing domain, IndeGx can now build and reason over an index..."
  - [section 3.4]: "The OWL definitions introduced in the ontology are typically useful to provide additional dimensions... [e.g.] defining concepts using restrictions in description logic to trigger a classification."
  - [corpus]: N/A.
- **Break condition:** If the reasoning engine encounters inconsistencies in the aggregated data (e.g., contradictory type assertions from different extraction rules) that cause the OWL reasoner to fail or produce an empty model.

### Mechanism 3
- **Claim:** Storing the execution traces of the indexing process as a Knowledge Graph (in dedicated named graphs) enables post-hoc debugging and quality assessment (accountability) of the index itself.
- **Mechanism:** The framework logs every rule execution, including start/end times and outcomes, using provenance vocabularies (PROV-O, EARL). This effectively creates a "reflexive" KG where the metadata about the indexing process is queryable alongside the index data.
- **Core assumption:** The volume of log data generated by tracing hundreds of rules across hundreds of endpoints does not overwhelm the storage capacity or query performance of the local triple store.
- **Evidence anchors:**
  - [section 3.5]: "The resulting graph also includes a declarative representation of the rules themselves... and the provenance information of all the generated KG descriptions."
  - [listing 5]: Demonstrates querying `kgi:Logs` to calculate average execution time using provenance data (`prov:Activity`, `earl:result`).
  - [corpus]: N/A.
- **Break condition:** If the logging mechanism introduces significant latency, causing timeouts that skew the performance metrics being measured (observer effect).

## Foundational Learning

- **Concept:** **SPARQL 1.1 Graph Store HTTP Protocol & UPDATE**
  - **Why needed here:** The framework relies heavily on `INSERT` and `INSERT DATA` operations to construct the index locally (Listings 1, 2, 3). Understanding the difference between updating a default graph and a named graph is critical for managing the index and log separation.
  - **Quick check question:** How does the `INSERT` clause in a SPARQL Update query differ from the `CONSTRUCT` query form in terms of side effects?

- **Concept:** **OWL RL (Rule Language) Profiles**
  - **Why needed here:** The system uses an OWL RL ontology for the inference step (Mechanism 2). You must understand that RL is designed for scalable reasoning via rule-based engines, unlike OWL DL which may require tableaux algorithms.
  - **Quick check question:** Why is OWL RL suitable for a pipeline that uses SPARQL-based processing steps before the final inference?

- **Concept:** **Vocabulary Heterogeneity (Void vs. DCAT vs. Schema.org)**
  - **Why needed here:** The paper explicitly addresses the need to handle different vocabularies describing the same thing (e.g., `void:Dataset` vs `dcat:Dataset` in Listing 1). The augmentation rules (Listing 3) are designed to bridge these gaps.
  - **Quick check question:** If an endpoint describes its license using `dcterms:license` but the index schema expects `schema:license`, at which step (Extraction, Augmentation, Inference) should this mapping ideally occur?

## Architecture Onboarding

- **Component map:** Catalog of SPARQL Endpoint URLs -> IndeGx Runner (Controller) -> Knowledge Base (Rules, Ontology, Logs) -> Index KG
- **Critical path:** 1. Load Rule Set and Catalog 2. Extraction Loop: Iterate endpoints → Execute Test → If success, Execute Action 3. Augmentation: Apply local normalization rules 4. Inference: Run OWL RL reasoner 5. Publication: Serialize and serve Index KG
- **Design tradeoffs:**
  - *Declarative vs. Imperative:* Rules are easier to share and compose (Listing 1) but harder to debug than standard Python scripts.
  - *Remote vs. Local Compute:* Pushing complex `SELECT` queries to remote endpoints (Extraction) risks timeouts; moving logic to local `INSERT` rules (Augmentation) shifts load to the indexer's infrastructure.
- **Failure signatures:**
  - **"Ghost" Features:** An endpoint passes a test for a feature (e.g., `NOT EXISTS`) but the subsequent action fails due to timeouts, leaving the index with partial/inconsistent metadata.
  - **Non-standard RAND/NOW:** As noted in Section 4.6, endpoints may return incorrect values for basic functions, leading to erroneous data in the index (e.g., duplicate random numbers).
  - **Vocabulary Silencing:** Extraction rules look for specific classes (e.g., `dcat:Dataset`). If a KG uses a custom ontology without mapping, it effectively becomes invisible to the index.
- **First 3 experiments:**
  1. **Basic Connectivity:** Run the IndeGx workflow against a single, highly stable endpoint (e.g., Wikidata or DBpedia) to verify the Extraction-to-Index pipeline.
  2. **Rule Addition:** Write a new rule (Listing 1 style) to extract a specific property not currently indexed (e.g., `dbo:abstract`) and observe if it populates in the local store.
  3. **Reasoning Validation:** Manually insert a feature triple (e.g., `sd:feature sparqles:SELNOTEXISTS`) and query the resulting KG to see if the reasoner correctly classified the endpoint as `kgi:SPARQL11Compliant` (Listing 4).

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the observed lack of server-side RDFS entailment in over 83% of public endpoints impact the correctness and performance of federated query processing? Basis in paper: [explicit] The authors explicitly report that "at least 83% of publicly available endpoints do not support basic RDFS entailment" in Section 4.4. Why unresolved: The paper identifies the existence of this limitation but does not investigate the downstream consequences for applications or query algorithms that rely on inferred triples. What evidence would resolve it: An empirical study measuring the discrepancy in query results and execution time when federated engines must compensate for missing entailments versus when they access compliant endpoints.

- **Open Question 2:** To what extent does the non-standard behavior of SPARQL engines (e.g., inconsistent RAND and NOW functions) degrade the reliability of automated source selection in Semantic Web applications? Basis in paper: [inferred] The conclusion suggests these results help identify "KR research opportunities" for applications like GraphRAG and source selection, while Section 4.6 details that 49% of endpoints fail RAND consistency tests. Why unresolved: The paper quantifies the technical non-compliance but leaves the causal link between these specific implementation variances and the failure rates of higher-level KR tasks untested. What evidence would resolve it: A user study or benchmark suite tracking the failure rate of automated source selection tools when exposed to endpoints with known non-standard SPARQL behaviors versus compliant ones.

- **Open Question 3:** Can the declarative, rule-based approach of IndeGx be efficiently adapted to index knowledge graphs accessible only via static RDF dumps rather than live SPARQL endpoints? Basis in paper: [inferred] The introduction notes KGs are "accessible in various ways," yet the methodology (Section 3) explicitly restricts the input to "a catalog of SPARQL endpoints." Why unresolved: The current framework relies on dynamic SPARQL queries (Test & Action rules) which cannot be directly applied to static files without significant architectural changes. What evidence would resolve it: An extension of the IndeGx framework that successfully parses local dumps using the same declarative rules, along with a performance comparison to the endpoint-based approach.

## Limitations

- The evaluation relies on a snapshot of 870 SPARQL endpoints, of which only 300-400 were active during testing, raising questions about reproducibility of reported statistics.
- Vocabulary heterogeneity and non-standard SPARQL extensions persist as challenges, suggesting the index may underrepresent KGs using non-standard ontologies.
- The provenance logging, while valuable for debugging, could introduce observer effects that distort performance metrics if the overhead is significant.

## Confidence

- **High confidence:** The mechanism of using Test/Action SPARQL rules for extraction (Mechanism 1) is clearly described and implementable. The architecture is sound, and the logging system is well-specified.
- **Medium confidence:** The augmentation and inference pipeline (Mechanism 2) is conceptually valid but depends on the specific OWL RL ontology used, which is not fully detailed in the paper. The effectiveness of the inference rules is not empirically validated.
- **Low confidence:** The reproducibility of the full evaluation is low due to the redacted endpoint list and unspecified execution environment (e.g., the specific reasoner and LDScript engine version).

## Next Checks

1. **Pipeline Validation:** Run the IndeGx workflow against a small, curated list of stable endpoints (e.g., Wikidata, DBpedia) to verify the basic extraction-to-index pipeline functions correctly.
2. **Rule Addition:** Add a custom extraction rule to capture a specific property not currently indexed and confirm it populates the local graph.
3. **Reasoning Test:** Manually insert a feature triple and query the resulting KG to verify the OWL RL reasoner correctly infers the expected classifications.