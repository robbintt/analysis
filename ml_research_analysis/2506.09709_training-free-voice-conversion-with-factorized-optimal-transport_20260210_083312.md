---
ver: rpa2
title: Training-Free Voice Conversion with Factorized Optimal Transport
arxiv_id: '2506.09709'
source_url: https://arxiv.org/abs/2506.09709
tags:
- conversion
- voice
- transport
- knn-vc
- mkl-vc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Factorized MKL-VC, a training-free modification
  for kNN-VC pipeline. The method replaces kNN regression with a factorized optimal
  transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear
  solution.
---

# Training-Free Voice Conversion with Factorized Optimal Transport

## Quick Facts
- arXiv ID: 2506.09709
- Source URL: https://arxiv.org/abs/2506.09709
- Authors: Alexander Lobashev; Assel Yermekova; Maria Larchenko
- Reference count: 0
- Key outcome: MKL-VC achieves voice conversion with short references (5-10s), outperforming kNN-VC and matching FACodec quality

## Executive Summary
This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline that replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces. The method leverages the Monge-Kantorovich Linear solution to transform source embedding distributions to match target distributions analytically, enabling synthesis of novel tokens rather than retrieving existing ones. Experiments on LibriSpeech and FLEURS datasets demonstrate significant improvements in content preservation and robustness with short reference audio, achieving performance comparable to FACodec, especially in cross-lingual voice conversion.

## Method Summary
The method uses WavLM-Large encoder to extract embeddings from source and reference audio, then factorizes the 1024-dimensional space into K-dimensional blocks sorted by variance. For each block, it computes the Monge-Kantorovich Linear map T(x) = μ₂ + Σ₁^(-1/2)(Σ₁^(1/2)Σ₂Σ₁^(1/2))^(1/2)Σ₁^(-1/2)(x−μ₁) that transports Gaussian distributions, applies this transformation, concatenates the transformed blocks, and decodes with HiFi-GAN vocoder. The factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation while preserving information from low-variance dimensions that kNN-based distance metrics ignore.

## Key Results
- MKL-VC significantly improves content preservation with short reference audio (5-10s vs 5 min required by kNN-VC)
- Optimal K=2 achieves best total score (0.105) balancing WER, CER, and speaker similarity
- MKL-VC achieves performance comparable to FACodec, particularly excelling in cross-lingual German-French conversion
- The method works effectively with reference audio as short as 5 seconds

## Why This Works (Mechanism)

### Mechanism 1
Replacing kNN regression with factorized optimal transport enables voice conversion with dramatically shorter reference audio (5-10s vs 5 min). The Monge-Kantorovich Linear map analytically transforms source embedding distributions to match target distributions, synthesizing novel tokens rather than retrieving existing ones. The map T(x) = μ₂ + Σ₁^(-1/2)(Σ₁^(1/2)Σ₂Σ₁^(1/2))^(1/2)Σ₁^(-1/2)(x - μ₁) transports one Gaussian distribution to another. This works because WavLM embeddings within each K-dimensional subspace approximately follow multivariate Gaussian distributions.

### Mechanism 2
Factorizing the 1024-dimensional WavLM space into K-dimensional blocks preserves information from low-variance dimensions that kNN-based distance metrics ignore. Dimensions sorted by standard deviation reveal that ~900/1024 components have minimal variance. Factorization solves independent OT problems per K-dimensional block, preventing high-variance dimensions from dominating the transport cost function. This works under the assumption that the covariance matrix is approximately block-diagonal, meaning K-dimensional subspaces are statistically independent.

### Mechanism 3
Smaller MKL dimension K improves content preservation (lower WER/CER) at the cost of speaker similarity; optimal K=2 for cross-lingual scenarios. Smaller K yields lower Wasserstein distance to Gaussian, making the Gaussian assumption more valid. Lower K also prevents overfitting to speaker-specific artifacts that may not generalize across languages. This trade-off can be tuned via K without breaking the overall system.

## Foundational Learning

- **Monge-Kantorovich Optimal Transport**: Why needed here: The core algorithm is built on the closed-form solution for quadratic-cost transport between Gaussians. Without understanding that T maps distribution p₀ to p₁ with minimal expected squared distance, the factorization strategy is opaque. Quick check: Given two 1D Gaussians N(0,1) and N(2,4), what is the MKL map that transports samples from the first to the second?

- **WavLM Embedding Structure and Self-Supervised Speech Representations**: Why needed here: The method exploits specific WavLM properties—non-uniform variance, approximate Gaussianity in subspaces, and meaningful cosine distance for perceptually similar sounds. Using a different encoder requires re-validating these properties. Quick check: Why does WavLM trained with contrastive learning yield embeddings where cosine distance correlates with perceptual similarity?

- **Covariance Matrix Square Root and Symmetric Positive Definite Matrices**: Why needed here: The MKL map requires computing Σ^(1/2) and Σ^(-1/2). These operations are only defined for symmetric positive definite matrices and require eigendecomposition or Cholesky-like methods. Quick check: Given covariance matrix Σ = [[4, 1], [1, 2]], how would you compute Σ^(1/2)?

## Architecture Onboarding

- **Component map**: Source Audio → WavLM-Large Encoder → Source Embeddings (T×1024) → [Variance-Sorted Factorization into N/K blocks of dim K] → [Per-block MKL Map] → [Concatenate transformed blocks → T×1024] → HiFi-GAN Vocoder → Converted Audio

- **Critical path**: 1) Validate Gaussian assumption on your encoder (compute Wasserstein distance as in Fig. 3 for your data). 2) Determine optimal K via ablation on a validation set (start with K=2, 4, 8, 16). 3) Pre-compute per-dimension standard deviations on a representative corpus to establish sorting order (done once per encoder).

- **Design tradeoffs**: K (MKL dimension): Lower K → better content preservation, worse speaker similarity; Higher K → inverse trade-off. Paper recommends K=2–8 for general use. Training-free vs. quality: No training required, but quality bounded by pre-trained WavLM/HiFi-GAN capabilities. Reference length: Works with 5-10s references, but longer references may still improve speaker similarity.

- **Failure signatures**: High WER with K>64: Gaussian assumption violated; reduce K. Robotic/unnatural output: Check if WavLM embeddings have abnormal statistics; verify HiFi-GAN vocoder compatibility. Cross-lingual pronunciation leakage: Increase K to preserve less speaker-specific articulation. Reconstruction artifacts when replacing low-variance dimensions with constants: Do not discard dimensions—factorization preserves them.

- **First 3 experiments**: 1) Gaussianity validation: Compute Wasserstein-2 distance between empirical embedding distributions and fitted Gaussians for various K on your target encoder and language data. 2) K ablation: Run MKL-VC with K ∈ {2, 4, 8, 16, 32, 64} on a held-out set, measuring WER, CER, and SIM to identify optimal operating point. 3) Reference length sensitivity: Test voice conversion with 3s, 5s, 10s, 30s, and 60s reference audio to quantify robustness.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the Gaussian assumption required for the Monge-Kantorovich Linear (MKL) map hold effectively for self-supervised speech encoders other than WavLM? The authors state the scheme is not universal because it "relies on specific properties of WavLM embeddings" and that "for a new encoder, the Gaussianity assumptions must be explicitly verified, as they are not guaranteed to hold in general." This remains unresolved as the paper only validates the Gaussian assumption for WavLM embeddings.

- **Open Question 2**: Can the factorization dimension K be determined automatically or adaptively to optimize the trade-off between speaker similarity and content preservation? The authors note that "MKL-VC has only one tuning parameter, K, which governs the trade-off... a higher MKL dimension K corresponds to higher speaker similarity but lower content scores." Currently, K appears to be a fixed hyperparameter set manually, requiring an adaptive method for varying reference audio lengths or linguistic complexities.

- **Open Question 3**: Does the assumption of block-diagonal covariance matrices result in the loss of essential cross-dimensional correlations present in the full WavLM embedding space? The method relies on the assumption that covariance matrices are "approximately block-diagonal" to make the transport map tractable, effectively decoupling sub-vectors. While factorization prevents numerical instability, it ignores potential correlations between factorized blocks that may encode speaker-specific nuances.

## Limitations

- The method critically relies on the Gaussian assumption for WavLM embedding subspaces, which may not hold universally across different languages, speaker demographics, or newer WavLM variants
- Cross-lingual voice conversion results are limited to German-French pairs, with generalization to other language pairs unproven
- Performance comparison with FACodec uses FACodec's own X-Vector model for similarity scoring, potentially introducing bias
- Computational cost analysis is incomplete, lacking wall-clock timing comparisons and memory requirements analysis

## Confidence

- **High confidence**: The core mathematical formulation of the Monge-Kantorovich Linear map for Gaussian distributions is correct and well-established. The improvement over kNN-VC with short reference audio is demonstrated on two datasets.
- **Medium confidence**: The variance-sorted factorization strategy effectively preserves information while enabling transport. The optimal K=2 recommendation for cross-lingual scenarios is supported by the presented experiments but may not generalize.
- **Low confidence**: The claim that MKL-VC achieves performance "comparable to FACodec" is based on limited comparisons and potentially biased evaluation metrics. The robustness claims across diverse linguistic scenarios need broader validation.

## Next Checks

1. **Gaussianity verification across domains**: Apply the Wasserstein distance analysis from Figure 3 to test whether WavLM embeddings from diverse languages (e.g., Mandarin, Arabic, Finnish) and different speaker demographics maintain approximate Gaussianity in K-dimensional subspaces. This directly validates the foundational assumption across realistic deployment scenarios.

2. **Controlled ablation of low-variance dimensions**: Design an experiment that systematically tests three approaches: (a) standard kNN-VC with all 1024 dimensions, (b) kNN-VC with only top-64 dimensions by variance, and (c) MKL-VC with full factorization. This would provide direct evidence for whether factorization truly preserves information that kNN-VC discards.

3. **Cross-lingual generalization study**: Extend the cross-lingual evaluation beyond German-French to include at least two additional language pairs with different phonetic properties (e.g., Spanish to Japanese, Hindi to English). This would test the robustness of the K=2 recommendation and identify potential failure modes in more challenging language combinations.