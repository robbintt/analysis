---
ver: rpa2
title: Generate Logical Equivalence Questions
arxiv_id: '2510.12001'
source_url: https://arxiv.org/abs/2510.12001
tags:
- questions
- generation
- students
- question
- equivalence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Generate Logical Equivalence Questions

## Quick Facts
- arXiv ID: 2510.12001
- Source URL: https://arxiv.org/abs/2510.12001
- Authors: Xinyu Wang; Haoming Yu; Yicheng Yang; Zhiyuan Li
- Reference count: 28
- Key outcome: None

## Executive Summary
This paper presents an automated question generator (AQG) for creating pairs of logically equivalent propositional logic expressions for discrete mathematics education. The system uses a formal language approach with dual syntax tree generation to ensure equivalence-by-construction, avoiding the verification step required by AI-based generators. The method employs paired production rules where one tree creates structure and the other applies semantic equivalence laws, controlled by MD5-derived deterministic randomness for per-student uniqueness and configurable difficulty parameters.

## Method Summary
The generator operates in three phases: (1) creates a deterministic random seed from student identifiers using MD5 hashing with cyclic offset, (2) generates parallel syntax trees using rule sets—R1 for structural generation and R2 for semantic equivalence laws—with roulette selection, and (3) constructs expressions via DFS traversal with an attribute system for precedence-based parenthesization. Difficulty is controlled through minimum proposition length, probability increment for equivalence law application, and maximum iterations, with a hardcoded Medium→Hard→Easy law category cycling pattern based on institutional feedback.

## Key Results
- AQG achieves 65.24% accuracy on 111 questions compared to historical textbook accuracy of 51.78%-68.33%
- Step count distribution centers on 3-4 steps (31 and 21 questions respectively) matching textbook patterns
- ChatGPT-3.5 produces 25/52 incorrect/nonequivalent questions, ChatGPT-4.o produces 29/49 incorrect
- MD5-based randomness ensures deterministic per-student uniqueness while preventing regeneration gaming

## Why This Works (Mechanism)

### Mechanism 1
Dual syntax tree generation with paired production rules produces provably equivalent propositions by construction, avoiding post-hoc verification. The system maintains two parallel syntax trees using paired rules (R1 for structural generation, R2 for semantic equivalence transformations). Each rule pair specifies simultaneous substitutions: the first creates structure in Proposition 1, while the second applies the corresponding equivalence law in Proposition 2. For example, the absorption rule rAb∨ produces `Ei ::= Ej ∨ Eh` and `Eh ::= Ej ∧ Ek` in the first tree, while the second tree gets only `Ei' ::= Ej'`—directly encoding `p ∨ (p ∧ q) ≡ p`.

### Mechanism 2
MD5-derived deterministic randomness ensures per-student uniqueness while preventing regeneration gaming. Student identifying information is hashed to a 16-digit hexadecimal MD5 code. Each digit drives one pseudo-random decision (rule selection via roulette wheel). When exhausted, the system cycles with a prime offset to avoid short-period repetition. This makes questions deterministic for a given student (no "regenerate until easy" exploits) but statistically unique across students due to low hash collision probability.

### Mechanism 3
User-defined difficulty control via recursive depth limiting and law-category cycling produces narrow step-count distributions matching textbook questions. Three parameters govern difficulty: minimum proposition length `m`, probability increment `pc` (forces R2 equivalence-law application), and maximum iterations `max`. The algorithm preferentially cycles through Median → Hard → Easy law categories, avoiding both trivial and impossibly complex derivations. Attribute grammars handle parenthesization to ensure unambiguous expressions.

## Foundational Learning

- Concept: **Propositional Logic Equivalence Laws**
  - Why needed here: The generator's R2 rules directly encode 21 standard equivalence laws (Identity, De Morgan, Absorption, etc.). Understanding these is prerequisite to reading the production rules or validating generated questions.
  - Quick check question: Given `p ∨ (p ∧ q)`, which equivalence law reduces this to `p`?

- Concept: **Context-Free Grammars and Attribute Systems**
  - Why needed here: Questions are defined as sentences in a formal language with BNF syntax. The attribute system (synthesized/inherited attributes) handles expression serialization with correct parenthesization based on operator precedence.
  - Quick check question: In an L-attributed grammar, can inherited attributes depend on siblings to the right?

- Concept: **Pseudo-random Number Generation from Hash Functions**
  - Why needed here: The MD5-to-seed conversion and roulette selection mechanism underpin the uniqueness guarantee. Understanding bias in hexadecimal-digit-driven selection is necessary to evaluate collision risk.
  - Quick check question: If you use hex digits 0-F as uniform random values modulo 10, what bias is introduced?

## Architecture Onboarding

- Component map: MD5 hash generator → Algorithm 1 syntax tree generation → Algorithm 2 expression construction → two equivalent expressions
- Critical path: Seed → Algorithm 1 recursive tree building → Algorithm 2 attribute synthesis → two string expressions. The guarantee of equivalence is maintained only if R2 rule pairs are correctly specified; any asymmetry breaks the invariant.
- Design tradeoffs:
  - Ambiguous grammar (parentheses omitted during generation) simplifies rule specification but requires precedence-aware reconstruction
  - Law-category cycling is hardcoded based on local institutional feedback; not dynamically adapted to individual student performance
  - Linear time complexity is achieved by bounding recursion depth; this limits maximum question complexity
- Failure signatures:
  - Trivial questions: If `pc` is too low or `max` too small, R2 rules rarely fire, producing nearly identical propositions
  - Unsolvable questions: If R2 rule pairs are mis-specified, the two propositions may not be equivalent
  - Predictable structure: If students recognize the Medium→Hard→Easy cycling, they may anticipate which law to apply next
- First 3 experiments:
  1. Unit test rule pairs: For each of the 21 R2 rules, generate 100 pairs and verify equivalence via truth table enumeration; log any mismatches
  2. Collision simulation: Generate 10,000 random student IDs, compute MD5 seeds, and measure hash collision rate to validate the uniqueness claim empirically
  3. Difficulty calibration: Run the generator with varying `pc` and `max` values; plot step-count distributions against textbook benchmarks to find parameter ranges that match the 3-4 step cluster

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed formal language framework be generalized to create a universal generator for other mathematical question types beyond logical equivalence? The conclusion states, "One major future work is to generalize the generator from logical equivalence to all question types," proposing a universal generator that accepts a question type definition. This remains unimplemented as the current approach is tailored specifically to propositional logic syntax and equivalence laws.

### Open Question 2
How can an auto-grader be effectively designed to evaluate the step-by-step logical proofs provided by students for these generated questions? The authors state in the conclusion: "Another future work can be the design of an auto-grader," noting that human marking becomes infeasible given the high volume of unique questions. The system currently generates questions and the final answer but lacks capability to parse or validate intermediate logical steps.

### Open Question 3
Does the manual classification of equivalence laws into "Easy," "Median," and "Hard" categories correlate more strongly with student performance than the "minimum steps" metric? Section 3.4 notes that reasoning difficulty is "relatively subjective" relying on "historical feedback," while Section 4 evaluates difficulty using "minimum number of steps," creating ambiguity regarding which metric better defines difficulty.

## Limitations
- Critical gap in rule specifications: Only two absorption rules are explicitly defined; remaining 19 semantic generation rules are referenced but not provided
- Difficulty calibration may not generalize: Law-category cycling relies on institution-specific historical data that may not transfer to other populations
- Evaluation cohort mismatch: AQG tested in 2023 against textbooks from 2021-2022 introduces potential cohort effects
- Weak domain overlap: Corpus analysis shows max FMR 0.67, suggesting limited transferability to other mathematical domains

## Confidence
- High confidence: MD5-based deterministic uniqueness mechanism is clearly specified and verifiable
- Medium confidence: Dual-syntax-tree framework is well-described but incomplete rule specifications limit verification
- Low confidence: Difficulty calibration strategy lacks explicit implementation details and depends on unprovided institutional data

## Next Checks
1. Implement all 21 ℛ2 equivalence law pairs and verify by exhaustive truth-table testing that generated proposition pairs are always equivalent
2. Systematically vary `pc` (0.05 to 0.5) and `max` (5 to 15) parameters; measure resulting step-count distributions and compare against the reported 3-4 step cluster
3. Deploy AQG in a different institution with a different student population; compare accuracy rates and difficulty distributions to determine if difficulty calibration generalizes