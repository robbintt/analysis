---
ver: rpa2
title: Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective
  Responses
arxiv_id: '2601.13024'
source_url: https://arxiv.org/abs/2601.13024
tags:
- cultural
- language
- narrative
- emotion
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CEDAR introduces a multimodal benchmark for assessing culturally
  grounded emotion alignment in large language models. Unlike existing cultural benchmarks
  that focus on factual knowledge, CEDAR curates scenarios where identical situations
  elicit distinct emotional responses across cultures.
---

# Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses

## Quick Facts
- arXiv ID: 2601.13024
- Source URL: https://arxiv.org/abs/2601.13024
- Reference count: 40
- Models struggle with culturally grounded emotion alignment, showing systematic biases toward high-arousal emotions

## Executive Summary
CEDAR introduces a multimodal benchmark for assessing culturally grounded emotion alignment in large language models. Unlike existing cultural benchmarks that focus on factual knowledge, CEDAR curates scenarios where identical situations elicit distinct emotional responses across cultures. Using a pipeline that combines LLM-based candidate selection with rigorous human annotation, the benchmark comprises 10,962 instances across seven languages and 14 emotion categories. Evaluations of 17 multilingual models reveal significant challenges in cross-cultural affective understanding, with performance varying substantially across languages and showing systematic biases toward high-arousal emotions while underrepresenting deactivated states.

## Method Summary
The CEDAR benchmark construction employs a two-stage pipeline combining LLM-based candidate selection with human annotation validation. LLMs generate provisional emotion predictions across languages for candidate scenarios, with instances showing cross-language disagreement retained for human evaluation. Native speakers provide ground truth labels through majority voting with at least five annotators per instance. The benchmark covers 7 languages (English, Chinese, Japanese, Arabic, Swahili, Hindi, Portuguese) with 10,962 total instances across 14 emotion categories, including 400 multimodal examples. Evaluation uses three metrics: Standard Accuracy, Global Emotion Prediction Propensity (GEPP), and Russell's Quadrant Bias (RQB) to analyze arousal-valence distributions.

## Key Results
- Models exhibit systematic bias toward high-arousal emotions (surprise, amusement, fear, anger) while underrepresenting deactivated states (contentment, embarrassment, relief, sympathy)
- English prompts consistently outperform native-language prompts, even for culture-specific content (Japanese accuracy: 30.46% with Japanese prompts vs 44.14% with English)
- Multimodal performance lags behind text-only across all languages, indicating challenges in integrating visual symbolic meaning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Culturally distinct emotional responses can be isolated through a two-stage pipeline combining LLM-based candidate selection with human annotation validation.
- **Mechanism:** LLMs (Claude4.5-Sonnet, Gemini2.5-Flash, GPT-4.5) generate provisional emotion predictions across languages; instances with cross-language disagreement are retained as candidates. Human native speakers then establish ground truth through majority voting with at least 5 annotators per instance.
- **Core assumption:** LLMs can approximate cultural perspectives sufficiently to identify divergent cases, even if they cannot accurately predict the correct emotion.
- **Evidence anchors:**
  - [section 2.3] "We first impose within-language agreement by aggregating predictions from the three models for each language... We then enforce cross-language variation by comparing these provisional labels across languages and removing instances with uniform predictions."
  - [section 2.5] "Ground-truth labels are obtained through annotation by local native speakers recruited on Prolific, with at least five annotators per instance."
  - [corpus] Weak direct evidence; corpus neighbors address LLM cultural simulation but not this specific pipeline architecture.
- **Break condition:** If LLM provisional labels are systematically biased toward certain emotions, the filtering stage may miss valid culturally distinct scenarios or select spurious ones.

### Mechanism 2
- **Claim:** LLMs exhibit systematic bias toward high-arousal emotions and underrepresent deactivated emotional states across languages.
- **Mechanism:** Models disproportionately predict Quadrant I/II emotions (surprise, amusement, fear, anger) over Quadrant III/IV (contentment, embarrassment, relief, sympathy). This bias persists across both multimodal and text-only settings and intensifies for low-resource languages.
- **Core assumption:** Assumption: The bias reflects training data distribution and Western-centric affective norms rather than task design artifacts.
- **Evidence anchors:**
  - [abstract] "showing systematic biases toward high-arousal emotions while underrepresenting deactivated states"
  - [section 4.3] "LLMs exhibit a systematic preference for high-arousal emotions, while consistently underrepresenting affective states associated with low arousal."
  - [corpus] "Reasoning Beyond Labels" (arXiv:2508.04199) confirms LLM sentiment challenges in culturally nuanced contexts.
- **Break condition:** If benchmark emotion distribution is itself skewed toward high-arousal categories, observed bias may partially reflect label imbalance rather than model behavior.

### Mechanism 3
- **Claim:** Language consistency between prompt and dataset degrades rather than improves cultural alignment for some languages.
- **Mechanism:** English prompts consistently outperform native-language prompts, even when evaluating culture-specific content. Japanese prompts on Japanese data yield 30.46% accuracy vs. 44.14% with English prompts. Models may use English as a latent reasoning pivot.
- **Core assumption:** Assumption: Models have not internalized the cultural values associated with native language expressions despite linguistic proficiency.
- **Evidence anchors:**
  - [section 4.4] "English prompts consistently yield better performance compared to prompts in other languages."
  - [section 4.4] "Gemma3-27B-It...Japanese prompt with Japanese data yields an accuracy of only 30.46%...lagging significantly behind English (44.14%)"
  - [corpus] No direct corpus support for this specific dissociation pattern.
- **Break condition:** If English-centric evaluation framing (e.g., emotion category labels) advantages English prompts independent of cultural reasoning, the observed effect may be methodological rather than mechanistic.

## Foundational Learning

- **Concept:** Russell's Circumplex Model (valence-arousal emotion space)
  - **Why needed here:** All quadrant-based bias analysis (GRQB, LSRQB) maps emotions to this 2D space; understanding QI-QIV classification is essential for interpreting results.
  - **Quick check question:** Which quadrant contains "contentment"—high or low arousal?

- **Concept:** Cross-cultural affective processing
  - **Why needed here:** The benchmark's core premise is that identical stimuli elicit culturally contingent emotions; practitioners must understand why semantic equivalence ≠ emotional equivalence.
  - **Quick check question:** Why would white lanterns signal mourning in one culture and festivity in another?

- **Concept:** Multimodal grounding for emotion inference
  - **Why needed here:** Multimodal instances require integrating visual symbolic meaning with textual context; performance gaps (multimodal < text-only) reflect this added complexity.
  - **Quick check question:** What additional information does the image provide that the narrative deliberately omits?

## Architecture Onboarding

- **Component map:** Seed data sources (ArabCulture, JETHICS, CulturalBench, etc.) → NQ pair generation (GPT-4.5) → Contextual refinement (Llama3.3-70B) → Basic filtering (length, social-relevance, toxicity via PolyGuard) → Consistency/variation filtering (3 LLMs voting) → Cultural variation selection (Qwen3-Embedding + Russell quadrant scoring) → Human annotation (Prolific native speakers) → Final benchmark (10,962 instances)

- **Critical path:** The LLM-based variation filtering is the gating step—without cross-language disagreement, instances are discarded. Human annotation is the validation bottleneck requiring 5+ native speakers per instance per language.

- **Design tradeoffs:**
  - Scale vs. depth: 7 languages with rigorous validation rather than broad language coverage
  - Categorical vs. dimensional: 14 discrete categories for evaluation precision, Russell's model for analysis
  - High-consensus scenarios only: Prioritizes prototypical cases over individual variation

- **Failure signatures:**
  - Models over-predicting surprise/amusement with high variance → check GEPP scores
  - Language-prompt mismatch causing degradation → examine cross-lingual variance in Table 1
  - Japanese deactivated-emotion underrepresentation → review LSRQB negative clustering (~-20%)

- **First 3 experiments:**
  1. **Baseline evaluation:** Run GPT-4o on text-only subset across all 7 languages; compute SA, GEPP, and GRQB to establish benchmark proficiency.
  2. **Prompt-language ablation:** Test whether English-prompt-as-pivot pattern holds for your target model by cross-matching all prompt/dataset language pairs (7×7 = 49 conditions).
  3. **Quadrant-specific analysis:** Isolate instances in Quadrant IV (high valence, low arousal) and measure per-model accuracy to confirm deactivated-emotion underperformance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific training interventions are required to resolve the dissociation between language consistency and cultural alignment observed when using native-language prompts?
- **Basis in paper:** [inferred] The authors find that using the "correct" native language prompt (e.g., Japanese) often yields lower accuracy than English prompts, suggesting models may rely on English as a latent reasoning pivot rather than internalizing the target culture's values.
- **Why unresolved:** The paper identifies the failure mode (performance degradation with native prompts) but does not propose or validate a method (e.g., specific fine-tuning or architecture changes) to ensure native prompts effectively elicit the correct cultural response.
- **What evidence would resolve it:** A study showing that models fine-tuned on culturally-aligned preference datasets maintain or improve performance when switching from English to native-language prompts, rather than degrading.

### Open Question 2
- **Question:** Can data rebalancing strategies effectively mitigate the systematic bias towards high-arousal emotions and improve model sensitivity to deactivated states?
- **Basis in paper:** [explicit] The evaluation reveals that models consistently prioritize high-arousal emotions (Quadrant I & II) and systematically underrepresent deactivated emotions (Quadrant III & IV) across both multimodal and text-only settings.
- **Why unresolved:** While the bias is quantified via Russell's Quadrant Bias (RQB) metrics, the paper does not test whether this is an artifact of training data distribution or a fundamental architectural limitation in processing low-intensity affect.
- **What evidence would resolve it:** Experiments demonstrating that models trained on a dataset balanced across arousal dimensions show a significant reduction in RQB scores for deactivated quadrants without losing accuracy in high-arousal categories.

### Open Question 3
- **Question:** To what extent do the observed failures in cross-cultural affective understanding generalize to low-resource languages outside the seven representative languages studied?
- **Basis in paper:** [explicit] The Limitations section states that the study restricted its scope to seven languages to prioritize high-fidelity validation, leaving the performance on "under-represented languages" unconfirmed.
- **Why unresolved:** It is unclear if the specific patterns of bias (such as the "language modulates emotional distributional shifts" observed in Arabic and Swahili) are consistent across all non-Western languages or if they vary by resource availability.
- **What evidence would resolve it:** Expanding the CEDAR pipeline to low-resource languages (e.g., Yoruba, Quechua) and comparing the magnitude of the performance gap against the current baseline of mid-to-high resource languages.

## Limitations
- Benchmark construction relies heavily on LLM-based candidate selection, which may introduce systematic bias in identifying culturally distinct cases
- Human annotation bottleneck requires 5+ native speakers per instance, limiting scalability and potential language coverage
- Observed English-prompt advantage may reflect evaluation artifacts rather than genuine cross-lingual reasoning deficiencies

## Confidence

- **High confidence:** The Russell quadrant bias analysis (Q1/Q2 overrepresentation, Q3/Q4 underrepresentation) is empirically grounded and consistent across models
- **Medium confidence:** The claim that identical language prompts degrade performance for some cultures (e.g., Japanese) is supported but requires careful disentanglement from evaluation framing effects
- **Low confidence:** The LLM-based filtering pipeline reliably isolates culturally distinct cases without introducing bias—this is fundamentally unverifiable without independent validation data

## Next Checks

1. **Cross-validation of the filtering pipeline:** Apply the LLM-based variation detection method to a held-out set of culturally validated scenarios and measure precision/recall of culturally distinct case identification
2. **Ablation on prompt framing:** Test whether the English-prompt advantage persists when emotion categories are embedded in the target language or when prompts are culture-neutral
3. **Bias decomposition:** Separate model-induced quadrant bias from benchmark-induced bias by comparing predicted vs. actual emotion distributions in a balanced affective stimulus set