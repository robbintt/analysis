---
ver: rpa2
title: Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token
  and Parameter Levels
arxiv_id: '2509.16596'
source_url: https://arxiv.org/abs/2509.16596
tags:
- subject
- uni00000013
- data
- performance
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how supervised fine-tuning (SFT) affects
  the factual knowledge of large language models (LLMs) on the closed-book question
  answering (CBQA) task. By systematically varying the category and scale of fine-tuning
  data, we observe that models fine-tuned on 1,920 samples can perform up to 14% worse
  than those fine-tuned on only 240 samples.
---

# Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels

## Quick Facts
- arXiv ID: 2509.16596
- Source URL: https://arxiv.org/abs/2509.16596
- Reference count: 32
- Fine-tuning 1,920 samples can degrade CBQA performance by up to 14% compared to 240 samples

## Executive Summary
This study investigates how supervised fine-tuning (SFT) affects the factual knowledge of large language models (LLMs) on closed-book question answering (CBQA) tasks. By systematically varying the category and scale of fine-tuning data, the authors observe that models fine-tuned on 1,920 samples can perform up to 14% worse than those fine-tuned on only 240 samples. Further analysis at both token and parameter levels reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these unnecessary updates significantly improves model performance, with gains exceeding 10% in some cases. These findings highlight the potential for optimizing fine-tuning strategies to better preserve and strengthen model knowledge.

## Method Summary
The study fine-tunes multiple LLaMA and LLaMA-3 models (7B, 13B, 70B) on the ENTITYQUESTIONS dataset, categorizing data by mastery level using multi-template completion. Models are trained with batch_size=8, 1 epoch, AdamW optimizer with cosine LR scheduling, and learning rate 1e-5. Performance is evaluated on in-domain and out-of-domain CBQA accuracy. KL divergence between fine-tuned and pre-trained models is computed using top-10 logit renormalization. Parameter restoration is performed by ranking updates by relative change magnitude and reverting the top-k% to pre-trained values.

## Key Results
- Performance peaks at 240 samples and degrades significantly with 1,920 samples (up to 14% worse)
- High KL divergence between fine-tuned and pre-trained models correlates with performance decline
- Restoring up to 90% of parameter updates to pre-trained values consistently improves CBQA accuracy by 10%+
- Low-mastery data (R=0) causes the highest KL divergence and worst performance

## Why This Works (Mechanism)

### Mechanism 1: Inverse Scaling of Factual Knowledge
Increasing SFT data volume beyond 240 samples causes performance degradation in factual recall by driving parameters toward overfitting the fine-tuning distribution, overriding pre-trained knowledge structures. This "over-adaptation" is most pronounced when fine-tuning on data the model has low mastery of.

### Mechanism 2: Token-Level Distributional Shift (KL Divergence)
Performance degradation correlates with high KL divergence between fine-tuned and pre-trained token logits. As SFT scales, the model shifts probability mass away from factual entities toward formatting tokens, with high divergence signaling excessive deviation from original knowledge embeddings.

### Mechanism 3: Parameter Redundancy and Restoration
Up to 90% of parameter updates during SFT are unnecessary for fitting training data and actively harm generalization. These "interference noise" updates are highly concentrated in a small subset of parameters, while factual knowledge is distributed more broadly and vulnerable to global updates.

## Foundational Learning

- **Concept: Closed-Book Question Answering (CBQA)**
  - Why needed here: Primary evaluation metric relying solely on internal model weights
  - Quick check question: If I change a model's weights and its CBQA score drops, does that mean it "forgot" facts or just forgot how to answer questions? (The paper argues it primarily indicates knowledge degradation).

- **Concept: KL Divergence (Relative Entropy)**
  - Why needed here: Quantifies how far the fine-tuned model has shifted from its pre-trained parent in token probability predictions
  - Quick check question: A high KL divergence between Model A (pre-trained) and Model B (fine-tuned) on the same input implies Model B's output distribution has changed significantly. Is this always bad? (The paper suggests yes, for knowledge retention).

- **Concept: Parameter Restoration / Ablation**
  - Why needed here: Core intervention involving selectively reverting weights to previous values based on magnitude of change
  - Quick check question: If resetting 50% of changed weights improves performance, what does that imply about the efficiency of standard gradient descent in SFT? (It implies standard SFT is noisy and inefficient regarding knowledge preservation).

## Architecture Onboarding

- **Component map:**
  Pre-trained Base (M) -> Mastery Filter -> Delta Calculator -> Restoration Engine -> Restored Model

- **Critical path:**
  1. Data Selection: Categorize data by mastery level
  2. Training: Perform standard SFT (1-3 epochs, AdamW)
  3. Diagnosis: Calculate magnitude of parameter change relative to pre-trained model
  4. Restoration: Identify top 20-40% of most changed parameters and revert them

- **Design tradeoffs:**
  - More data (1920 samples) increases knowledge collapse risk but allows greater restoration improvements
  - Restoration improves both training set and test set performance, suggesting redundant updates weren't helping training task

- **Failure signatures:**
  - Catastrophic Drop: 1920 samples result in 14% lower accuracy than 240 samples
  - Low-Mastery Poisoning: R=0 data causes highest KL divergence and worst performance

- **First 3 experiments:**
  1. Mastery Bucket Ablation: Train on Mastery Level 0 vs Level 4 data to verify low-mastery data is harmful
  2. Scale Threshold Test: Train with 60, 240, 960, 1920 samples to confirm U-shaped accuracy curve
  3. Restoration Sweep: After 1920-sample training, iteratively restore 1-40% of most changed parameters

## Open Questions the Paper Calls Out

### Open Question 1
How can adaptive fine-tuning strategies be designed to proactively minimize unnecessary parameter updates during training, rather than relying on post-hoc restoration?
- Basis: Limitations section mentions future work should focus on "designing adaptive fine-tuning strategies that minimize unnecessary updates"
- Unresolved because: Paper demonstrates restoring parameters improves performance but doesn't propose preventing unnecessary updates during initial training
- Evidence needed: Novel regularization technique or dynamic learning rate adjustment maintaining CBQA performance without post-training restoration

### Open Question 2
Why does fine-tuning on low-mastery data induce higher magnitude of detrimental parameter updates compared to high-mastery data?
- Basis: Finding 2 observes models fine-tuned on low-mastery data suffer steeper performance drops and allow more parameter restoration
- Unresolved because: Analysis links effect to KL divergence but doesn't explain why "unknown" knowledge causes more parameter noise
- Evidence needed: Mechanistic interpretability study mapping how gradients from low-mastery samples disproportionately affect knowledge neurons

### Open Question 3
Do findings regarding parameter redundancy generalize to tasks requiring complex reasoning or creative generation, as opposed to factual recall?
- Basis: Authors note analysis is limited by resources to specific model families and hint at generalization
- Unresolved because: Study relies on CBQA accuracy to define "knowledge enhancement" which may not correlate with parameter updates for logical reasoning
- Evidence needed: Reproduction on reasoning benchmarks showing similar proportions of parameter updates can be restored without degrading reasoning accuracy

## Limitations
- Mastery quantification reliability: Multi-template completion with synonym mapping may introduce noise in categorizing data by knowledge mastery
- KL divergence interpretation: Assumes pre-trained model's distribution is ideal knowledge state, potentially problematic if pre-trained model contains hallucinations
- Parameter restoration generalization: Findings specific to factual recall may not apply to tasks requiring novel skill acquisition or procedural reasoning

## Confidence
- **High Confidence**: Inverse scaling of factual knowledge with increased SFT data volume is well-supported by systematic experiments across multiple model sizes
- **Medium Confidence**: Token-level KL divergence correlation with performance degradation is observed but relies on specific implementation choices
- **Low Confidence**: Generalization of findings to non-factual tasks or scenarios requiring new capability acquisition is uncertain

## Next Checks
1. Cross-Domain Generalization Test: Replicate mastery-based categorization and parameter restoration on non-location factual dataset (e.g., historical events) to verify inverse scaling effect persists across domains

2. Novel Skill Acquisition Baseline: Fine-tune models on tasks requiring new procedural reasoning not present in pre-training, compare parameter restoration outcomes to factual recall case

3. Pre-Trained Model Quality Ablation: Conduct controlled experiment with pre-trained baseline containing known hallucinations, measure whether KL divergence minimization from flawed parent model still correlates with performance improvement