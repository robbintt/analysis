---
ver: rpa2
title: 'Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings'
arxiv_id: '2502.04386'
source_url: https://arxiv.org/abs/2502.04386
tags:
- embeddings
- debiasing
- cancer
- demographic
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of demographic bias encoded in
  self-supervised 3D CT embeddings used for medical imaging tasks. The authors propose
  a VAE-based adversarial debiasing framework to remove demographic information (age,
  sex) from embeddings while preserving downstream task performance.
---

# Towards Fair Medical AI: Adversarial Debiasing of 3D CT Foundation Embeddings

## Quick Facts
- arXiv ID: 2502.04386
- Source URL: https://arxiv.org/abs/2502.04386
- Reference count: 13
- Authors: Guangyao Zheng; Michael A. Jacobs; Vladimir Braverman; Vishwa S. Parekh
- Primary result: Debiased 3D CT embeddings reduce sex classification accuracy from 0.994 to 0.647 while maintaining lung cancer prediction accuracy at 0.986 (1-year) and 0.977 (2-year)

## Executive Summary
This paper presents a VAE-based adversarial debiasing framework to remove demographic information (age, sex) from self-supervised 3D CT embeddings while preserving downstream clinical task performance. The authors apply this to lung cancer screening embeddings from the NLST dataset, demonstrating that demographic information can be effectively removed from embeddings without sacrificing cancer prediction accuracy. The debiased embeddings also show improved fairness metrics and robustness against adversarial data poisoning attacks.

## Method Summary
The authors develop a VAE-based adversarial debiasing framework that transforms self-supervised 3D CT embeddings into a new latent space where demographic information is no longer encoded. The method uses a VAE encoder to produce a 500-dimensional latent representation from 1408-dimensional CT foundation embeddings, with two adversary branches predicting sex (binary classification) and age (regression). The encoder is trained to minimize reconstruction loss while simultaneously maximizing the adversary's prediction error for demographics. The framework was evaluated on the NLST lung cancer screening dataset, using 1-year and 2-year cancer prediction as downstream tasks.

## Key Results
- Sex classification accuracy dropped from 0.994 to 0.647 after debiasing
- Age prediction error increased from MAE 2.734 to 4.169 while maintaining task performance
- Lung cancer prediction accuracy remained high at 0.986 (1-year) and 0.977 (2-year)
- Fairness metrics improved with Equal Opportunity Difference approaching zero
- Debiased embeddings showed robustness against adversarial data poisoning attacks

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Gradient Conflict Forces Demographic Erasure
The competing optimization objectives between the VAE encoder and adversary cause demographic information to be removed from the latent space. The adversary branch predicts demographic attributes from latent representations, while the encoder receives gradients that minimize this predictability while simultaneously minimizing reconstruction loss. This gradient conflict creates pressure to encode information that reconstructs well but cannot predict demographics.

### Mechanism 2: Information Bottleneck via Latent Dimension Constraint
Constraining latent dimensionality to 500 dimensions forces selective information retention, prioritizing task-relevant features over demographic signals. The compression from 1408-dimensional inputs to 500-dimensional latent space reduces capacity for encoding auxiliary information while reconstruction loss forces retention of information critical for recreating the original embedding.

### Mechanism 3: Reconstruction Loss Anchors Task-Relevant Representations
MSE reconstruction loss between original and decoded embeddings preserves feature directions needed for downstream tasks. Features with large variance in the original embeddings contribute disproportionately to MSE loss, creating pressure to retain them. Demographic features that occupy smaller variance subspaces can be sacrificed.

## Foundational Learning

- **Concept: Variational Autoencoder (VAE) Architecture**
  - Why needed here: The entire debiasing framework builds on VAE mechanics—encoder producing mean/log-variance, reparameterization trick for backpropagation, KL divergence for regularization
  - Quick check question: Can you explain why the reparameterization trick (sampling ε ~ N(0,1) and computing z = μ + σε) enables gradient flow through the sampling step?

- **Concept: Adversarial Training / Gradient Reversal**
  - Why needed here: The encoder must learn to produce representations that fool the demographic predictor. This requires understanding minimax optimization where one component's loss is another's gain
  - Quick check question: In standard adversarial training, what happens to encoder gradients when the adversary becomes too strong or too weak?

- **Concept: Fairness Metrics (Equal Opportunity Difference)**
  - Why needed here: The paper evaluates success using EOD—the difference in true positive rates between demographic groups. Interpreting results requires understanding why EOD near zero indicates fairness
  - Quick check question: If a lung cancer predictor has TPR of 0.90 for females and 0.70 for males, what is the EOD? Which direction indicates bias against males?

## Architecture Onboarding

- **Component map**: Input embedding → Encoder (μ, log σ²) → Reparameterization (z) → split to [Decoder (reconstruction loss)] and [Adversary branches (demographic prediction loss)]

- **Critical path**: Input embedding → Encoder (μ, log σ²) → Reparameterization (z) → split to [Decoder (reconstruction loss)] and [Adversary branches (demographic prediction loss)]. Encoder receives combined gradients; adversary receives only its branch gradients

- **Design tradeoffs**:
  - Latent dimension (500): Lower values improve debiasing but risk losing task-relevant information; empirical tuning required per dataset
  - Learning rates (encoder/decoder: 0.0005, adversary: 0.002): Asymmetric rates prevent adversary from dominating too quickly
  - Batch size (32): Standard; smaller batches may add noise beneficial for disentanglement but were not ablated

- **Failure signatures**:
  - Convergence failure (oscillating losses): Adversary too strong; reduce adversary learning rate or increase encoder capacity
  - Demographic information persists (sex AUC > 0.8 after training): Latent dimension too large or adversary undertrained
  - Task performance drops (cancer accuracy < 0.95): Latent dimension too small; reconstruction loss weight insufficient
  - Mode collapse (all outputs similar): KL weight too high relative to reconstruction

- **First 3 experiments**:
  1. Baseline demographic predictability: Train linear classifiers on original embeddings to predict sex and age. Record AUC and MAE as upper bounds
  2. Latent dimension sweep: Train VAE with adversary across dimensions [100, 200, 300, 400, 500, 600]. Plot sex AUC vs. cancer AUC
  3. Robustness check via data poisoning: Following Section 3.2, flip cancer labels for 50% of one sex. Compare EOD between original and debiased embeddings

## Open Questions the Paper Calls Out
- Can the VAE-based adversarial debiasing framework effectively remove race or geographic location information from 3D CT embeddings while maintaining clinical utility?
- Does the debiasing framework generalize to other clinical tasks and imaging domains beyond lung cancer risk prediction in 3D CT scans?
- Is the empirically determined latent dimension of 500 robust across diverse patient populations and scanner protocols, or does it require task-specific tuning?

## Limitations
- Method validated on a single lung cancer screening dataset (NLST), limiting generalizability claims
- No analysis of whether debiasing introduces correlated artifacts or reduces embedding diversity in clinically meaningful ways
- Did not evaluate against other attack vectors beyond data poisoning that might exploit adversarial training dynamics

## Confidence
- Mechanism 1 (Adversarial Gradient Conflict): High confidence - well-supported by ablation results and clear gradient flow architecture
- Mechanism 2 (Latent Dimension Constraint): Medium confidence - empirical tuning shown, but theoretical justification for 500 dimensions is absent
- Mechanism 3 (Reconstruction Loss Preservation): Medium confidence - correlation between reconstruction fidelity and task performance observed, but causation not rigorously established

## Next Checks
1. **Cross-Dataset Validation**: Apply the debiased embeddings to an independent lung cancer screening dataset (e.g., PLCO) and evaluate whether fairness improvements and task performance generalize beyond NLST
2. **Latent Space Interpretability**: Use techniques like TCAV or supervised UMAP to identify whether specific latent dimensions correspond to clinically meaningful features that might be inadvertently removed during debiasing
3. **Adversarial Attack Analysis**: Conduct a gradient-based inversion attack to recover demographic attributes from the debiased embeddings. Measure success rate and compare against original embeddings to quantify the actual security improvement