---
ver: rpa2
title: 'VIKING: Deep variational inference with stochastic projections'
arxiv_id: '2510.23684'
source_url: https://arxiv.org/abs/2510.23684
tags:
- should
- viking
- posterior
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of applying variational inference\
  \ to overparameterized deep neural networks, where traditional mean-field approximations\
  \ fail due to strong parameter correlations induced by model reparameterizations.\
  \ The authors propose a novel variational family that explicitly reflects this geometry\
  \ by decomposing the parameter space into two orthogonal subspaces\u2014one capturing\
  \ uncertainty over the training data and another capturing general model uncertainty\u2014\
  via projections onto the kernel and image of the Fisher-Rao metric."
---

# VIKING: Deep variational inference with stochastic projections

## Quick Facts
- arXiv ID: 2510.23684
- Source URL: https://arxiv.org/abs/2510.23684
- Reference count: 40
- Primary result: State-of-the-art variational inference for deep networks using stochastic projections onto kernel/image of Fisher-Rao metric

## Executive Summary
This paper addresses the fundamental challenge of performing accurate variational inference in overparameterized deep neural networks, where traditional mean-field approaches fail due to strong parameter correlations. The authors propose VIKING, a novel variational family that explicitly respects the geometric structure induced by model reparameterizations. By decomposing the parameter space into orthogonal subspaces via projections onto the kernel and image of the Fisher-Rao metric, VIKING achieves superior uncertainty quantification and predictive performance across image classification, out-of-distribution detection, and generative modeling tasks.

## Method Summary
VIKING introduces a geometrically-informed variational family that decomposes the parameter space using the Fisher-Rao metric. The method projects parameters onto orthogonal subspaces corresponding to data-dependent and data-independent uncertainties, then applies stochastic alternating projections to approximate the posterior efficiently. To scale to large networks, the authors develop matrix-free solvers and reparameterized sampling techniques. The variational distribution is parameterized to capture correlations in the model's overparameterized regime while maintaining computational tractability through stochastic optimization.

## Key Results
- Achieves state-of-the-art or competitive performance on image classification benchmarks
- Demonstrates superior out-of-distribution detection capabilities compared to standard variational methods
- Shows strong performance in generative modeling tasks while maintaining efficient inference

## Why This Works (Mechanism)
The method succeeds by explicitly accounting for the overparameterization structure inherent in deep networks. Traditional mean-field variational inference assumes independent parameters, which fails when model reparameterizations induce strong correlations. VIKING's geometric decomposition separates uncertainty components that are orthogonal under the Fisher-Rao metric, allowing the variational family to capture the true posterior structure. The stochastic projection framework enables this sophisticated approach to scale to large models by avoiding explicit computation of high-dimensional matrix operations.

## Foundational Learning
- Fisher-Rao metric: Why needed - captures the geometry of statistical manifolds and parameter sensitivity; Quick check - verify that the metric captures meaningful parameter correlations in your network
- Overparameterization in deep learning: Why needed - understanding why traditional VI fails in modern architectures; Quick check - confirm your model exhibits multiple equivalent parameterizations
- Variational inference fundamentals: Why needed - basis for understanding the approximation framework; Quick check - ensure you can derive ELBO for simple models
- Alternating projections: Why needed - enables decomposition of parameter space into orthogonal subspaces; Quick check - test convergence on simple matrix decomposition problems
- Matrix-free solvers: Why needed - critical for scaling to large networks; Quick check - verify computational efficiency gains on small-scale problems

## Architecture Onboarding

Component map: Variational family -> Fisher-Rao decomposition -> Stochastic projections -> Matrix-free solvers -> Inference engine

Critical path: The core computational path involves computing projections onto kernel and image subspaces, which requires efficient matrix operations and gradient propagation through the variational parameters.

Design tradeoffs: The method trades computational complexity for improved posterior approximation quality. The stochastic projection approach enables scalability but introduces approximation error. The choice of variational family must balance expressiveness with tractability.

Failure signatures: Poor performance may indicate that the Fisher-Rao metric doesn't capture relevant correlations in the data, or that the stochastic projection approximation is insufficient for the problem complexity. Computational bottlenecks may arise from inefficient matrix operations in the projection steps.

First experiments:
1. Verify Fisher-Rao metric computation on a simple linear model with known correlations
2. Test alternating projection convergence on a synthetic parameter space with clear subspace structure
3. Benchmark basic VIKING implementation against mean-field VI on a small convolutional network

## Open Questions the Paper Calls Out
The paper acknowledges that the theoretical grounding of the variational family decomposition relies on assumptions about Fisher-Rao metric structure that may not hold universally across different architectures and datasets. The empirical validation is primarily focused on classification and generative modeling tasks, leaving uncertainty about generalization to other domains.

## Limitations
- Theoretical assumptions about Fisher-Rao metric structure may not hold across all architectures
- Empirical validation limited to classification and generative modeling domains
- Computational overhead and memory requirements for very large models (>100M parameters) remain unclear

## Confidence
High: The method achieves strong empirical performance on tested benchmarks and the stochastic projection framework is technically sound
Medium: The theoretical justification for the variational family decomposition is complete and the approach will scale to very large models without modification
Low: The robustness of results to hyperparameter choices and initialization schemes is thoroughly explored

## Next Checks
1. Test robustness across diverse initialization schemes and verify sensitivity to hyperparameter choices through systematic ablation studies
2. Apply the method to non-vision domains (e.g., language modeling or reinforcement learning) to assess generalizability beyond classification/generative tasks
3. Evaluate computational overhead and memory requirements for scaling to models with >100M parameters, particularly for the matrix-free solver components