---
ver: rpa2
title: 'The Geometry of Thought: How Scale Restructures Reasoning In Large Language
  Models'
arxiv_id: '2601.13358'
source_url: https://arxiv.org/abs/2601.13358
tags:
- reasoning
- arxiv
- scale
- structure
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether neural scaling laws produce uniform
  capability gains or domain-specific geometric reorganizations in reasoning. The
  core method analyzes 25,000+ chain-of-thought trajectories from 8B and 70B parameter
  models across four domains (Law, Science, Code, Math) using geometric measurements
  of trajectory structure and learned endpoint operators.
---

# The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models

## Quick Facts
- arXiv ID: 2601.13358
- Source URL: https://arxiv.org/abs/2601.13358
- Authors: Samuel Cyrenius Anderson
- Reference count: 40
- Primary result: Legal reasoning "crystallizes" with 45% dimensional collapse and 31% alignment increase at 70B scale, enabling endpoint prediction without traversing intermediate states (63.6% accuracy).

## Executive Summary
This paper investigates whether neural scaling produces uniform capability gains or domain-specific geometric reorganizations in reasoning. Analyzing 25,000+ chain-of-thought trajectories from 8B and 70B Llama-3 models across Law, Science, Code, and Math domains reveals three distinct phases: Legal reasoning "crystallizes" with dramatic dimensional collapse, forming low-dimensional manifolds; Scientific and mathematical reasoning remain "liquid" with scale-invariant geometry; Code reasoning forms discrete "lattice" structures with high clustering. A universal oscillatory signature (coherence ≈ -0.4) persists across all domains. The Neural Reasoning Operator achieves 63.6% accuracy on held-out legal tasks by predicting reasoning endpoints without traversing intermediate states, demonstrating that favorable geometry enables amortized inference.

## Method Summary
The study extracts chain-of-thought trajectories from Llama-3-8B and Llama-3.1-70B models using a two-pass generate-then-extract pipeline that captures final-layer hidden states at each token. Geometric analysis measures intrinsic dimensionality (Levina-Bickel MLE), embedding dimensionality (PCA-based d95), trajectory alignment, step-to-step coherence, and clustering structure. Neural Reasoning Operators learn direct mappings from initial to terminal hidden states using linear, MLP, and DeepONet architectures, with a Turbo variant conditioned on initial velocity. Performance is evaluated via probe decoding accuracy on held-out trajectories.

## Key Results
- Legal reasoning undergoes Crystallization at 70B scale: 45% dimensional collapse (d95: 501→274), 31% alignment increase, and 10× manifold untangling (G/L ratio 9.82→0.98)
- Universal oscillatory signature (coherence ≈ -0.4) invariant across domains and scales
- Neural Reasoning Operator achieves 63.6% probe accuracy on held-out legal tasks without traversing intermediate states
- Three-phase taxonomy: Crystalline (Law), Liquid (Science/Math), Lattice (Code)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scale triggers geometric reorganization (Crystallization) in rule-governed domains, not uniform capability gains.
- Mechanism: As parameters increase from 8B to 70B in legal reasoning, the Global-to-Local dimension ratio collapses from 9.82× to 0.98×—the manifold "untangles." This 10× reduction indicates the model discovers latent compressible structure (precedent hierarchies, formal rules) and flattens previously folded representations into linear, traversable geometry.
- Core assumption: Rule-governed domains possess inherent low-dimensional structure that only becomes exploitable at sufficient parameter scale.
- Evidence anchors: Legal reasoning shows 45% dimensional collapse (d95: 501→274) and 10× manifold untangling at 70B scale.
- Break condition: If a domain lacks rule-governed structure (open-ended conceptual synthesis like GPQA science problems), crystallization fails—geometry remains liquid regardless of scale.

### Mechanism 2
- Claim: Transformer architectures produce a universal oscillatory signature (coherence ≈ -0.4) independent of domain or scale.
- Mechanism: Successive velocity vectors along reasoning trajectories show negative cosine similarity, indicating "zig-zag" dynamics. The paper proposes this reflects architectural tension: attention layers aggregate context (pull) while feedforward layers transform representations (push), creating oscillation rather than smooth flow toward attractors.
- Core assumption: The -0.4 constant encodes relative strengths of attention vs. feedforward processing in Llama-family architectures.
- Evidence anchors: Universal -0.4 coherence across all eight domain/scale conditions (Law, Science, Code, Math at 8B and 70B).
- Break condition: If transformer architecture fundamentally changes (e.g., removing residual connections or altering attention/feedforward alternation), the signature may shift or disappear.

### Mechanism 3
- Claim: Favorable trajectory geometry enables amortized inference—predicting reasoning endpoints without traversing intermediate states.
- Mechanism: Neural Reasoning Operators learn direct mappings F_c: h₀ → h_T from initial to terminal hidden states. In crystalline domains (low d95, high alignment, G/L ≈ 1), the endpoint occupies a predictable region of representation space. The Turbo operator conditions on initial velocity (h₁ - h₀) to capture early trajectory direction.
- Core assumption: Terminal states lie on sufficiently low-dimensional manifolds that early-state information can specify them without full trajectory simulation.
- Evidence anchors: Turbo operator achieves 74.7% latent similarity and 63.6% decoding accuracy on legal classification tasks when evaluated on held-out trajectories.
- Break condition: In liquid domains (Science, Math) with high intrinsic curvature (G/L ≈ 10×), operator accuracy should degrade substantially—terminal states become unpredictable from initial states alone.

## Foundational Learning

- **Intrinsic vs. Global Dimensionality**
  - Why needed: The paper reports stable d_mle (≈20-25) across all conditions while d95 varies dramatically (92-501). Misunderstanding this distinction leads to incorrect conclusions about whether reasoning manifolds are "low-dimensional."
  - Quick check: If d95 collapses from 501 to 274 but d_mle stays at 22, what does this tell you about the manifold's embedding vs. its intrinsic structure?

- **Manifold Hypothesis for Dynamics**
  - Why needed: The paper extends the manifold hypothesis from static data to reasoning trajectories—treating hidden state evolution as paths through representation space with measurable geometry.
  - Quick check: What does a G/L ratio of 9.8× vs. 0.98× indicate about how the manifold is folded in ambient space?

- **Trajectory Coherence**
  - Why needed: Coherence measures stepwise smoothness of reasoning dynamics. The universal -0.4 signature is central to the paper's claim about architectural invariants.
  - Quick check: If coherence were +0.8 instead of -0.4, what would that imply about how representations evolve during reasoning?

## Architecture Onboarding

- **Component map:**
  - Trajectory extraction -> Geometric analysis suite -> Neural Reasoning Operators -> Decoder adapters
  - (Two-pass generate-then-extract) -> (PCA, Levina-Bickel, alignment, coherence) -> (Linear, MLP, DeepONet, Turbo) -> (Probe decoding)

- **Critical path:**
  1. Extract trajectories from Llama-3-8B-Instruct and Llama-3.1-70B-Instruct (requires 8×B200 GPUs for 70B)
  2. Compute geometric metrics per domain/scale combination
  3. Train operators on 70/15/15 split; evaluate probe decoding accuracy on held-out test

- **Design tradeoffs:**
  - Probe decoding vs. frozen-unembedding: Paper uses probe as headline metric; frozen decoding is diagnostic only
  - Single model family: All results from Llama-3; generalization to Mistral/Qwen unverified
  - Two scale points only: Cannot distinguish continuous compression from sharp phase transition

- **Failure signatures:**
  - Empty generations (T=0) filtered out
  - Coherence requires ≥4 states; local PCA requires ≥10 states per trajectory
  - Delimiter localization failures cause incorrect answer-token targeting

- **First 3 experiments:**
  1. Replicate crystallization on LexGLUE-SCOTUS: Extract 8B/70B trajectories, verify d95 collapse (501→274) and G/L ratio drop (9.82→0.98)
  2. Validate oscillatory signature: Compute coherence across all eight domain/scale conditions; confirm ≈-0.4 universality
  3. Pilot Neural Reasoning Operator: Train Turbo architecture on legal (h₀, h_T) pairs with 70/15/15 split; target 63.6% probe accuracy baseline on held-out trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what critical parameter scale does Crystallization initiate in legal reasoning?
- Basis in paper: Section 4.6.3 notes only two scale points (8B, 70B) prevent characterizing the transition shape.
- Why unresolved: The study lacks intermediate scale measurements (14B, 33B) needed to determine whether Crystallization is a sharp phase transition or continuous compression.
- What evidence would resolve it: Geometric measurements at 5+ intermediate scales between 8B–70B, plotting d95, alignment, and G/L ratio as functions of parameter count.

### Open Question 2
- Question: Does the Crystal/Liquid/Lattice taxonomy generalize across model architectures beyond Llama-3?
- Basis in paper: Section 4.7 states all experiments used the Llama-3-Instruct series, making geometric signatures potentially architecture-specific.
- Why unresolved: Single-architecture study cannot distinguish universal transformer properties from Llama-specific training or architectural choices.
- What evidence would resolve it: Replication of geometric measurements on Mistral, Qwen, and other families at equivalent scales.

### Open Question 3
- Question: Does Crystallization reflect genuine expertise or merely efficient encoding?
- Basis in paper: Section 4.1 presents two interpretations: Expertise Interpretation (deeper understanding) vs. Compression Interpretation (efficient encoding of rule-governed structure).
- Why unresolved: Dimensional collapse and alignment gains are consistent with both deeper understanding and more efficient compression.
- What evidence would resolve it: Transfer tests measuring whether crystallized representations generalize to novel legal domains, or analysis probing whether collapsed dimensions encode task-irrelevant vs. task-relevant information.

### Open Question 4
- Question: Can Neural Reasoning Operators trained at 70B scale achieve higher amortized inference accuracy than the 63.6% observed with 8B operators?
- Basis in paper: Section 4.1 states full validation of 70B operator performance remains an important direction for future work.
- Why unresolved: The paper demonstrates deeper geometric organization at 70B but does not directly test whether this improved geometry translates to better operator performance.
- What evidence would resolve it: Training and evaluating Neural Reasoning Operators on 70B trajectories across domains, comparing decoding accuracy against 8B baselines.

## Limitations

- Single model family (Llama-3) at only two scale points limits conclusions about whether crystallization represents sharp phase transitions versus gradual compression.
- Universal oscillatory signature (-0.4 coherence) lacks mechanistic validation through ablation studies or cross-architecture testing.
- Probe decoding evaluation complicates interpretation of operator quality versus probe learning capacity.

## Confidence

**High confidence**: Geometric measurements themselves (d95, G/L ratios, alignment scores, coherence values) are reproducible given the code, as they rely on standard statistical methods applied to hidden states. The crystallization phenomenon in legal reasoning appears robust across geometric metrics.

**Medium confidence**: The three-phase taxonomy follows logically from geometric measurements but assumes these phases represent distinct reasoning modes rather than points along continuous spectra. The Neural Reasoning Operator's 63.6% accuracy demonstrates feasibility but probe-decoding evaluation complicates interpretation.

**Low confidence**: The universal oscillatory signature (-0.4 coherence) and its proposed architectural origin lack mechanistic validation. Claims about "low-dimensional manifolds" require careful distinction between intrinsic and embedding dimensionality that the paper doesn't fully explore.

## Next Checks

1. **Architecture Ablation**: Train and extract trajectories from Mistral-7B and Qwen2.5-7B models on the same legal tasks. Verify whether crystallization (d95 collapse, G/L ≈1) and the -0.4 oscillatory signature persist across architectures, or if these are Llama-3-specific phenomena.

2. **Intrinsic vs. Embedding Dimensionality Analysis**: For domains showing crystallization (Law) versus liquid behavior (Science), systematically compare intrinsic dimension (d_mle) to embedding dimension (d95) across multiple k values in the Levina-Bickel estimator. This would clarify whether crystalline domains possess genuinely lower intrinsic complexity or merely more favorable embedding geometry.

3. **Operator Generalization Test**: Train Neural Reasoning Operators on legal trajectories from 8B scale and evaluate on 70B trajectories (and vice versa), testing whether operators transfer across scales within the same domain. Additionally, compare probe-decoding accuracy against frozen-unembedding baselines to isolate operator contribution from probe learning effects.