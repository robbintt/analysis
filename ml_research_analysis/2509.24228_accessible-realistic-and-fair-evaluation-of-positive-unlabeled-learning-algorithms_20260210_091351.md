---
ver: rpa2
title: Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms
arxiv_id: '2509.24228'
source_url: https://arxiv.org/abs/2509.24228
tags:
- learning
- data
- positive
- algorithms
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first benchmark for comparing positive-unlabeled
  (PU) learning algorithms. The authors identify key issues affecting fair evaluation:
  (1) many algorithms use negative data in validation, which contradicts PU learning
  assumptions, and (2) one-sample and two-sample PU settings have different data distributions
  that existing evaluations fail to account for.'
---

# Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms

## Quick Facts
- arXiv ID: 2509.24228
- Source URL: https://arxiv.org/abs/2509.24228
- Reference count: 40
- Authors: Wei Wang; Dong-Dong Wu; Ming Li; Jingxiong Zhang; Gang Niu; Masashi Sugiyama
- Primary result: Presents the first benchmark for comparing positive-unlabeled (PU) learning algorithms

## Executive Summary
This paper addresses fundamental evaluation challenges in positive-unlabeled learning by identifying critical biases in current benchmarking practices. The authors demonstrate that many existing PU algorithms violate core assumptions by using negative data during validation, and that evaluations fail to distinguish between one-sample and two-sample PU settings which have different data distributions. They propose proxy metrics for validation without negative data and introduce calibration techniques to address label shift issues. Their comprehensive benchmark reveals that no single algorithm dominates across all scenarios, highlighting the importance of context-dependent algorithm selection.

## Method Summary
The authors establish a new evaluation framework for PU learning algorithms that addresses two major issues: the inappropriate use of negative data in validation and the failure to account for differences between one-sample and two-sample PU settings. They propose using proxy accuracy and proxy AUC score for model selection when negative data is unavailable, and introduce a calibration approach to handle internal label shift in the one-sample setting. The framework is validated through experiments on 13 UCI datasets and synthetic data, comparing 12 PU learning algorithms across multiple metrics and settings.

## Key Results
- No single PU learning algorithm dominates across all datasets and metrics
- Early PU learning algorithms perform competitively with newer methods
- Proposed proxy validation metrics and calibration techniques significantly improve fair comparisons across PU learning algorithm families
- The exclusion of negative data from validation reveals substantial performance differences compared to traditional evaluation methods

## Why This Works (Mechanism)
The proposed framework works by ensuring that evaluation metrics align with the fundamental assumptions of PU learning. By eliminating negative data from validation, the proxy metrics force algorithms to rely solely on positive and unlabeled data, creating a more realistic assessment of their capabilities. The calibration approach addresses the internal label shift problem specific to one-sample PU settings, where the distribution of positive samples in the training set differs from that in the test set. This alignment between evaluation conditions and algorithm assumptions leads to more reliable and comparable performance assessments.

## Foundational Learning
- PU learning fundamentals: Understanding the distinction between one-sample and two-sample settings is crucial because they have different data distributions and require different algorithmic approaches.
- Evaluation bias identification: Recognizing how negative data in validation violates PU assumptions is essential for fair algorithm comparison.
- Proxy metric construction: Learning to design validation metrics that work without negative data is key to realistic PU learning evaluation.
- Calibration techniques: Understanding how to correct for label shift between training and test distributions in one-sample settings is necessary for accurate performance assessment.
- Benchmark design principles: Knowing how to create comprehensive, fair benchmarks that account for multiple settings and metrics is critical for advancing the field.

## Architecture Onboarding

**Component map:** Proxy metrics -> Model selection -> Calibration -> Performance evaluation -> Algorithm comparison

**Critical path:** The evaluation pipeline begins with proxy metrics for validation, proceeds through calibration (when needed), and culminates in performance comparison across multiple metrics and settings. The calibration step is critical only for one-sample PU scenarios.

**Design tradeoffs:** The exclusion of negative data from validation creates more realistic assessments but may sacrifice some statistical power compared to traditional methods. The framework prioritizes methodological purity over maximum performance measurement.

**Failure signatures:** Algorithms that rely heavily on negative data during training may show artificially inflated performance in traditional evaluations but fail under the proposed framework. Poor calibration can lead to significant performance degradation in one-sample settings.

**First experiments:** 1) Test proxy metrics on simple synthetic PU datasets with known class priors. 2) Compare calibrated versus uncalibrated performance in one-sample settings. 3) Evaluate algorithm ranking stability across different validation approaches.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Experiments primarily conducted on standard UCI and synthetic datasets, limiting generalization to diverse real-world PU scenarios
- The exclusion of negative data from validation may not reflect practical applications where some negative information is available
- Performance gains from proposed methods vary substantially across datasets and algorithm families, suggesting context-dependent effectiveness

## Confidence
- High: Identification of existing evaluation biases in PU learning regarding negative data use in validation and one-sample versus two-sample setting treatment
- Medium: Effectiveness of proposed proxy metrics and calibration techniques, demonstrated but potentially dataset-dependent
- Low: Claims about relative performance of specific algorithms, as results show no clear winners and early algorithms may already be competitive

## Next Checks
1. Test the proposed proxy metrics and calibration approach on large-scale real-world datasets with varying class priors and contamination levels
2. Evaluate whether exclusion of negative data in validation leads to degraded performance in production settings where some negative information may be available
3. Investigate sensitivity of proposed methods to hyperparameter choices and their robustness across different data distributions and feature spaces