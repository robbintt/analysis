---
ver: rpa2
title: Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models
arxiv_id: '2508.19720'
source_url: https://arxiv.org/abs/2508.19720
tags:
- uni00000011
- uni00000013
- knowledge
- uni00000014
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CSKS, a framework for continuously steering\
  \ the sensitivity of large language models (LLMs) to contextual knowledge. The key\
  \ idea is to use two small proxy models\u2014one fine-tuned to be faithful to contextual\
  \ knowledge and the other to its own parametric knowledge\u2014and adjust the LLM's\
  \ output distribution based on the difference between these two proxy models."
---

# Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models

## Quick Facts
- arXiv ID: 2508.19720
- Source URL: https://arxiv.org/abs/2508.19720
- Reference count: 28
- Key outcome: Framework achieves up to 47.67-point increase in sensitivity score and maintains strong performance across varying knowledge conflict types.

## Executive Summary
This paper introduces CSKS, a framework for continuously steering the sensitivity of large language models (LLMs) to contextual knowledge. The key idea is to use two small proxy models—one fine-tuned to be faithful to contextual knowledge and the other to its own parametric knowledge—and adjust the LLM's output distribution based on the difference between these two proxy models. This allows precise and continuous control over the LLM's reliance on either contextual or parametric knowledge via a hyperparameter α. Experiments on synthetic datasets (MuSiQue, PopQA) and real-world benchmarks (DynamicQA, 2WikiMultiHopQA) demonstrate that CSKS significantly improves the LLM's ability to integrate contextual knowledge, outperforming state-of-the-art baselines.

## Method Summary
CSKS fine-tunes two small proxy models (P for context-faithful, N for parametric-faithful) on ECQA dataset with opposing objectives. At inference, it computes the difference vector $(D_P - D_N)$ in output logit distributions and adds this to the target LLM's logits: $\tilde{X}_t \sim \text{softmax}[D_L + (D_P - D_N) \cdot \alpha]$. The framework uses greedy decoding and allows α to span $(-\infty, +\infty)$ for bidirectional control. For black-box models, it applies top-K token reweighting when full logit access is unavailable.

## Key Results
- Achieves up to 47.67-point increase in sensitivity score compared to baselines
- Works effectively across different LLM families (Llama, Qwen, Gemma) with 3B proxy models matching 7B performance
- Maintains strong performance on real-world benchmarks while enabling continuous control over knowledge source preference
- Shows trade-off: larger α values increase contextual sensitivity but may slightly reduce general reasoning ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logit-space distribution steering via contrastive proxy models enables continuous control over knowledge source preference.
- Mechanism: The framework fine-tunes two small proxy models (positive P, negative N) on identical data with opposing objectives: P is trained to prefer contextual knowledge, while N is trained to prefer parametric knowledge. At inference, the difference vector $(D_P - D_N)$ in output logit distributions represents a directional signal toward contextual reliance, scaled by hyperparameter $\alpha$ and added to the target LLM's logits: $\tilde{X}_t \sim \text{softmax}[D_L + (D_P - D_N) \cdot \alpha]$.
- Core assumption: The divergence $(D_P - D_N)$ isolates "context-sensitive" semantic dimensions, and this signal generalizes from proxy to target vocabulary distributions.
- Evidence anchors:
  - [abstract] "tune two small LMs (i.e. proxy models) and use the difference in their output distributions to shift the original distribution of an LLM"
  - [section 2.1] "$\tilde{X}_t \sim \text{softmax}[D_L + (D_P - D_N) \alpha]$"
  - [corpus] ContextFocus uses activation steering for contextual faithfulness; corpus does not directly validate this specific proxy-based logit steering mechanism.
- Break condition: If proxy training data diverges from deployment distribution, or vocabulary alignment is poor, $(D_P - D_N)$ may inject noise.

### Mechanism 2
- Claim: Small models' higher plasticity makes them effective "steering wheels" for larger, more rigid models.
- Mechanism: Large LLMs (70B) exhibit greater "stubbornness" to knowledge conflicts than small LLMs (8B); CAD/COIECD improve small models significantly but minimally affect large ones. CSKS exploits this by training adaptable small models to capture desired behavior, then projecting this as an external steering signal onto the large model.
- Core assumption: Behavioral patterns learned by small proxies transfer meaningfully to guide large targets despite capacity and representation differences.
- Evidence anchors:
  - [section 2.3] "larger LMs exhibit greater rigidity... small models' superior adaptability as proxies to guide LLMs"
  - [section 3.3] "large models are 'stubborn' and resistant... 'self-guidance' signal is often too weak"
  - [corpus] Corpus discusses knowledge conflict behavior broadly but does not directly validate proxy-model transfer.
- Break condition: If steering signal is too weak relative to target LLM's parametric bias, or proxy training collapses (P and N converge), the differential signal becomes ineffective.

### Mechanism 3
- Claim: Bidirectional control (positive/negative $\alpha$) enables both amplification and suppression of contextual sensitivity.
- Mechanism: $\alpha$ spans $(-\infty, +\infty)$. Positive $\alpha$ steers toward context-faithfulness; negative $\alpha$ steers toward parametric reliance. Figure 3 shows monotonic sensitivity increase with positive $\alpha$ and decrease with negative $\alpha$.
- Core assumption: $(D_P - D_N)$ consistently encodes a "more vs. less context" semantic axis across token vocabulary.
- Evidence anchors:
  - [section 3.3] "extending $\alpha$ to negative values reveals an inverse mode of action... transforming the target model into a parametric knowledge conservative"
  - [Figure 3] Monotonic sensitivity score changes with $\alpha$ variation.
  - [corpus] CoCoA addresses adaptive decoding for conflicts but does not validate this bidirectional mechanism.
- Break condition: Extreme $|\alpha|$ distorts output distributions, degrading general reasoning (Table 2 shows ~4% Humanities drop at $\alpha=+2.0$).

## Foundational Learning

- **Concept: Knowledge Conflict in LLMs**
  - Why needed here: CSKS manages scenarios where retrieved context contradicts parametric knowledge; understanding this conflict is foundational.
  - Quick check question: If parametric memory says "Eiffel Tower is in Rome" but context says "Paris," what does CSKS control?

- **Concept: Logit Distribution and Steering**
  - Why needed here: The intervention operates in logit space; additive vectors shift next-token probability distributions.
  - Quick check question: In $\text{softmax}[D_L + (D_P - D_N) \cdot \alpha]$, what happens to context-aligned token probabilities as $\alpha$ increases?

- **Concept: Proxy Models and Transfer Learning**
  - Why needed here: Small models are trained to capture behavioral signals that steer larger models without modifying their weights.
  - Quick check question: Why might a small model be easier to fine-tune for a specific behavioral preference than a large model?

## Architecture Onboarding

- **Component map:** ECQA preprocessing -> Proxy P fine-tuning (context-faithful) -> Proxy N fine-tuning (parametric-faithful) -> Inference steering module -> LLM output

- **Critical path:** Ensure proxy and target share vocabulary (same tokenizer) -> Verify proxy divergence: P sensitivity ~84, N ~25 (Figure 8) -> Compute all three distributions in parallel at inference -> Apply steering equation

- **Design tradeoffs:**
  - **Proxy Size:** 3B matches 7B effectiveness (Figure 5); smaller proxies have marginal impact
  - **Alpha Range:** Stronger control at extreme $\alpha$ risks reasoning degradation; recommended range $[-1.5, +1.5]$
  - **Black-Box:** API-only models constrained to top-K token reweighting

- **Failure signatures:**
  1. **Proxy Collapse:** P and N sensitivity converge (~50), yielding near-zero steering signal
  2. **Excessive Degradation:** >5% drops on MMLU/2WikiMultiHopQA at chosen $\alpha$
  3. **No Sensitivity Change:** Varying $\alpha$ produces no monotonic effect—check vocabulary alignment
  4. **Misaligned Steering:** Positive $\alpha$ increases errors on ground-truth context

- **First 3 experiments:**
  1. **Proxy Validation:** Fine-tune P/N; target P sensitivity >80, N <30 on held-out conflict set
  2. **Alpha Sweep:** Run $\alpha \in \{-2.0, -1.0, 0.0, +1.0, +2.0\}$ on MuSiQue/PopQA; plot Sensitivity Score vs. $\alpha$; identify range with <3% MMLU degradation
  3. **Real-World Benchmark:** Apply selected $\alpha$ to DynamicQA; compare Static/Temporal/Disputable accuracy against baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the steering hyperparameter $\alpha$ be determined dynamically and automatically rather than manually, to optimize the trade-off between contextual faithfulness and general reasoning capability in real-time applications?
- Basis in paper: [explicit] The authors state in the Limitations section that "the optimal calibration of the guiding hyperparameter $\alpha$ may vary in real scenarios" and suggest future research explore "methods for automatically or more adaptively determining the value of $\alpha$."
- Why unresolved: Currently, $\alpha$ is a static hyperparameter; high values increase sensitivity but degrade general capabilities (as shown in MMLU results), requiring manual tuning for specific datasets.
- What evidence would resolve it: A mechanism that adjusts $\alpha$ per query based on context confidence or ambiguity, maintaining high sensitivity scores without the observed drop in MMLU/2WikiMultiHopQA performance.

### Open Question 2
- Question: Why does the efficacy of the CSKS framework saturate with smaller proxy models (e.g., 3B vs. 7B for a 72B target), and does this hold for target models of varying scales?
- Basis in paper: [inferred] Section 3.5 notes that a 3B proxy's impact is "comparable, occasionally slightly better, than a 7B proxy" and speculates this efficiency stems from "selective steering," but provides no theoretical proof or test for the lower bound of proxy size.
- Why unresolved: The paper observes this phenomenon empirically but does not investigate if the proxy model only needs to capture "knowledge conflicts" rather than full knowledge, leaving the minimal sufficient proxy size undefined.
- What evidence would resolve it: Experiments mapping proxy parameter counts against steering vector quality (DP - DN) for various target model sizes to establish a scaling law for proxy efficacy.

### Open Question 3
- Question: To what extent does the "top-5 token reweighting" constraint limit the theoretical efficacy of CSKS when applied to black-box models compared to white-box full-distribution steering?
- Basis in paper: [inferred] Section 3.5 mentions that for GPT-3.5-Turbo, "CSKS only reweights the five tokens" due to API limitations, but it does not quantify the performance gap against a hypothetical full-access baseline.
- Why unresolved: While the method works for black-box models, it is unclear if the truncated view of the distribution introduces errors or limits the granularity of continuous control.
- What evidence would resolve it: A comparison of steering performance on an open model (e.g., Llama) using full logits vs. a simulated "top-k only" API restriction to isolate the performance loss.

## Limitations

- Vocabulary alignment dependency: Critical effectiveness relies on perfect vocabulary alignment between proxy and target models
- Proxy-to-target generalization: Behavioral pattern transfer from small to large models lacks direct experimental validation
- Extreme α behavior: Large values may degrade general reasoning without theoretical framework for understanding when degradation occurs

## Confidence

**High confidence:** The core mechanism of using proxy model differences for steering is well-supported by mathematical formulation and consistent with established logit-space intervention techniques.

**Medium confidence:** The claim that small models' higher plasticity makes them effective steering wheels is supported by comparisons to CAD/COIECD but lacks direct experimental validation of the transfer mechanism.

**Low confidence:** The exact threshold where α-induced degradation becomes problematic across different LLM families is not precisely characterized.

## Next Checks

1. **Vocabulary robustness test:** Evaluate CSKS performance when proxy and target models use different tokenizers (e.g., GPT-3.5 tokenizer with Llama-3 model) to quantify the impact of vocabulary misalignment on steering effectiveness.

2. **Extreme α ablation study:** Systematically measure degradation across all MMLU subjects at α ∈ {-3.0, -2.0, -1.0, 0.0, +1.0, +2.0, +3.0} to establish precise degradation thresholds and identify whether certain knowledge domains are more vulnerable.

3. **Cross-architecture proxy transfer:** Train proxies on one LLM family (e.g., Llama) and test steering effectiveness on a different family (e.g., Qwen or GPT) to validate whether the steering signal generalizes beyond architectural similarities.