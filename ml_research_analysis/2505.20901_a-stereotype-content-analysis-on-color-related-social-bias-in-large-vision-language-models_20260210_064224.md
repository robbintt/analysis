---
ver: rpa2
title: A Stereotype Content Analysis on Color-related Social Bias in Large Vision
  Language Models
arxiv_id: '2505.20901'
source_url: https://arxiv.org/abs/2505.20901
tags:
- stereotypes
- confident
- quality
- suitable
- bright
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BASIC, a benchmark for assessing gender,
  race, and color stereotypes in large vision-language models (LVLMs), and proposes
  SCM-based metrics to complement sentiment-based evaluations. The study reveals that
  LVLMs exhibit stereotypes based on color tones, with blue and white images associated
  with higher warmth and competence compared to red.
---

# A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models

## Quick Facts
- arXiv ID: 2505.20901
- Source URL: https://arxiv.org/abs/2505.20901
- Reference count: 29
- Key outcome: LVLMs exhibit significant stereotypes based on color tones, gender, and race, with SCM-based metrics revealing nuanced biases that sentiment analysis misses

## Executive Summary
This paper introduces BASIC, a benchmark for assessing gender, race, and color stereotypes in large vision-language models (LVLMs), and proposes SCM-based metrics to complement sentiment-based evaluations. The study reveals that LVLMs exhibit stereotypes based on color tones, with blue and white images associated with higher warmth and competence compared to red. Gender stereotypes show male images scoring lower in competence but higher in warmth than female images. Race-based stereotypes are also significant, with Asian and White individuals generally rated lower in warmth. The results demonstrate that model architecture and parameter size have complex interactions affecting stereotypes, with no single factor dominating across all metrics.

## Method Summary
The authors constructed the BASIC benchmark with controlled synthetic images varying one attribute at a time (color, gender, race). They prompted 14 open-source LVLMs to describe these images and measured stereotype content using both SCM-based metrics (via projection onto warmth/competence axes) and VADER sentiment analysis. Statistical significance was assessed through paired t-tests comparing matched image pairs across attribute categories.

## Key Results
- LVLMs show color-based stereotypes: blue and white images score higher in warmth and competence than red images
- Gender stereotypes: male images score lower in competence but higher in warmth than female images
- Race stereotypes: Asian and White individuals generally rated lower in warmth compared to Black and Indian individuals
- SCM-based metrics detect significantly more stereotype instances than VADER sentiment analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCM-based metrics capture stereotypes that sentiment analysis misses because positive-sentiment text can still encode implicit bias
- Mechanism: Project sentence embeddings onto orthogonal warmth and competence axes derived from representative word dictionaries. The projection coordinates (αw, αc) quantify how much each dimension contributes to the sentence meaning, revealing stereotypical associations that sentiment polarity obscures
- Core assumption: Stereotype content in LVLM outputs maps onto the warmth-competence plane from social psychology (Fiske et al., 2018), and word embeddings preserve these semantic relationships
- Evidence anchors: [abstract] "SCM-based evaluation is effective in capturing stereotypes"; [section 6.1] "VADER is independent from SCM-based metrics and measures different aspects... a sentence with positive polarity does not necessarily imply high competence or warmth"
- Break condition: If LVLM outputs consistently use negation or context that flips word meaning (e.g., "not competent"), projection scores may not reflect actual stereotype expression

### Mechanism 2
- Claim: Color tones influence stereotype expression in LVLM outputs because models internalize sociocultural associations from image-text training data
- Mechanism: Blue and white images receive higher warmth/competence scores than red images. The paper hypothesizes this reflects learned associations: blue with calmness/trust, white with purity/professionalism, red with warning/aggression
- Core assumption: These color associations are learned from human-labeled training data and transfer to LVLM outputs during generation
- Evidence anchors: [abstract] "LVLMs exhibit color stereotypes in the output along with gender and race ones"; [section 6.2] PMI results show red associated with "COMPETITIVE," blue with "WELCOMING," white with "EXPERTISE"
- Break condition: If color effects disappear when controlling for background complexity or image quality differences, the finding may reflect visual confounds rather than learned stereotypes

### Mechanism 3
- Claim: Safeguard mechanisms in LVLMs distort sentiment-based bias detection by enforcing positive tone regardless of underlying stereotypical content
- Mechanism: RLHF/safety training teaches models to avoid negative language on sensitive topics, artificially inflating sentiment scores while SCM dimensions remain sensitive to subtle competence/warmth distinctions
- Core assumption: Safeguards primarily target sentiment polarity rather than semantic content dimensions
- Evidence anchors: [section 6.1] "Safeguard mechanisms in LVLMs can distort or mask the model's internal stereotypes... sentiment polarity-based metrics, such as VADER, may not sufficiently capture bias"; [table 3] VADER shows far fewer significant stereotypes (6-14/19) compared to competence (7-18/19) and warmth (10-16/19)
- Break condition: If newer models are explicitly trained to debias on SCM dimensions, this mechanism's explanatory power diminishes

## Foundational Learning

- Concept: Stereotype Content Model (SCM) from social psychology
  - Why needed here: This is the theoretical foundation for the warmth/competence metrics. Understanding that stereotypes decompose into these two independent dimensions is essential for interpreting results
  - Quick check question: Can a group be rated high on both warmth and competence simultaneously, or are they inversely correlated?

- Concept: Orthogonal projection in embedding space
  - Why needed here: The SCM metrics use vector projection. You need to understand how to compute basis vectors from word lists and project sentences onto them
  - Quick check question: Why must warmth and competence basis vectors be non-parallel for the projection to work?

- Concept: Pointwise Mutual Information (PMI)
  - Why needed here: PMI identifies which words are most associated with each attribute (color, gender, race), validating the quantitative SCM findings qualitatively
  - Quick check question: A PMI score of 0 means what relationship between a word and an attribute?

## Architecture Onboarding

- Component map: Image Encoder -> Vision-Language Connector -> LLM Backbone -> SCM Metric Pipeline
- Critical path: 1. Generate controlled images (BASIC dataset) varying one attribute at a time; 2. Prompt LVLM: "Describe this person objectively"; 3. Extract description text; 4. Compute SCM scores via projection and VADER sentiment; 5. Paired t-test across matched image pairs
- Design tradeoffs:
  - Projection vs. word frequency: Projection captures semantic context; frequency is simpler but misses meaning
  - Three colors only: Limits generalizability but enables controlled experiments
  - Open-source models only: Enables architecture comparison; excludes proprietary systems with potentially different bias profiles
- Failure signatures:
  - High correlation between VADER and SCM scores suggests both metrics capture the same phenomenon (redundancy)
  - Non-significant paired t-tests with high variance may indicate image quality issues rather than absence of bias
  - Opposite trends between models with similar architectures suggests confounding factors (training data, hyperparameters)
- First 3 experiments:
  1. Replicate color stereotype finding on a held-out occupation set from BASIC to validate generalization
  2. Ablate the projection basis vectors: test if random word lists produce different scores than SCM dictionaries
  3. Cross-validate SCM scores against human annotations on a subset (n=100) to establish ground-truth correspondence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do other visual attributes (e.g., yellow or green tones, facial expressions, brightness, posture) influence stereotype formation in LVLMs, and do they interact with the color-based stereotypes observed in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that "other color tones (e.g., yellow or green) may show different patterns of stereotypes" and "facial expression, brightness, or posture may still have a potential influence on the model outputs"
- Why unresolved: The study intentionally limited color manipulation to white, red, and blue based on prior human perception research, and synthetic image generation controlled for but did not systematically vary other visual confounds
- What evidence would resolve it: Extending BASIC to include additional color tones and systematically manipulating other visual attributes in controlled image pairs, then applying the same SCM-based metrics

### Open Question 2
- Question: What causal mechanisms link specific architectural choices (backbone LLM, training data composition, safeguard mechanisms) to stereotype expression in LVLMs?
- Basis in paper: [explicit] The authors note their "result only provides correlation between them" and call for "further research on identifying such possible causes from model architecture"
- Why unresolved: Identifying causality requires pre-training models from scratch with controlled variations, which demands substantial computational resources beyond the scope of this observational study
- What evidence would resolve it: Ablation studies on models trained with controlled variations in architecture, training data, and alignment procedures, measuring stereotype metrics at each stage

### Open Question 3
- Question: Do LVLMs exhibit different stereotype patterns across languages and cultural contexts, and how do language-specific biases interact with visual stereotypes?
- Basis in paper: [explicit] The authors state in the Limitations that they "conducted our experiment only in English" and that "conducting similar experiment with different languages may provide different insights"
- Why unresolved: The current study used only English prompts and SCM dictionaries derived from English-language social psychology research
- What evidence would resolve it: Multilingual evaluation using the BASIC benchmark with prompts and SCM-adapted metrics in multiple languages, combined with culturally diverse annotator validation

## Limitations
- Color stereotype findings limited to only three colors (blue, red, white) with unknown generalizability to other hues
- Paired t-test approach assumes independence between image pairs, but background similarities could confound results
- SCM projection method critically depends on quality of warmth/competence word dictionaries, which may contain cultural bias

## Confidence

- **High Confidence**: The core finding that SCM-based metrics detect more nuanced stereotypes than sentiment analysis is well-supported by the statistical evidence showing VADER's limited sensitivity compared to competence/warmth metrics
- **Medium Confidence**: The color stereotype mechanism linking blue/white to higher warmth/competence appears plausible but requires stronger causal evidence
- **Medium Confidence**: The safeguard mechanism explanation for VADER's poor bias detection is plausible but weakly supported

## Next Checks
1. Cross-cultural validation: Test whether color stereotypes persist when using images with different cultural contexts or when evaluated by human annotators from diverse backgrounds
2. Temporal robustness check: Evaluate whether SCM scores remain stable when models are prompted with temporally varied language to test whether safeguards systematically mask stereotypes
3. Dictionary sensitivity analysis: Systematically replace SCM word dictionaries with randomly generated word lists to quantify how much projection scores depend on dictionary quality versus the projection method itself