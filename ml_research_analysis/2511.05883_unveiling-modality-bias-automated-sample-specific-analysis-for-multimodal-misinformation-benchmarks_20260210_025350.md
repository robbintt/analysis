---
ver: rpa2
title: 'Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal
  Misinformation Benchmarks'
arxiv_id: '2511.05883'
source_url: https://arxiv.org/abs/2511.05883
tags:
- modality
- bias
- automated
- multimodal
- misinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates automated sample-specific modality bias
  analysis for multimodal misinformation benchmarks. The authors propose three bias
  quantification methods at different granularities: modality benefit (coarse, Shapley
  value-based), modality flow (medium, saliency-based attention analysis), and modality
  causal effect (fine, counterfactual reasoning).'
---

# Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks

## Quick Facts
- arXiv ID: 2511.05883
- Source URL: https://arxiv.org/abs/2511.05883
- Reference count: 40
- Primary result: Multi-view ensemble achieves 79.33% (Fakeddit) and 83.33% (MMFakeBench) accuracy in detecting sample-specific modality bias

## Executive Summary
This paper investigates automated sample-specific modality bias analysis for multimodal misinformation benchmarks. The authors propose three bias quantification methods at different granularities: modality benefit (coarse, Shapley value-based), modality flow (medium, saliency-based attention analysis), and modality causal effect (fine, counterfactual reasoning). A human evaluation on 300 samples from Fakeddit and MMFakeBench shows that ensembling these views (multi-view analysis) achieves 79.33% and 83.33% accuracy respectively, outperforming single-view approaches. The analysis is vulnerable to detector-induced fluctuations when changing misinformation detectors, and shows higher agreement on modality-balanced samples versus biased ones. The findings suggest automated sample-specific bias analysis has practical potential for cleaning biased benchmarks by retaining modality-balanced samples.

## Method Summary
The authors develop a three-view framework for automated sample-specific modality bias analysis in multimodal misinformation detection. The "Modality Benefit" view uses Shapley values to quantify each modality's marginal contribution to correct predictions. The "Modality Flow" view analyzes gradient-weighted attention to identify which modality the model relies on during inference. The "Modality Causal Effect" view employs counterfactual reasoning via structural causal models to determine if models use genuine fusion or shortcut paths. These views are combined through majority voting to classify samples as modality-balanced or biased (uni-text or uni-image).

## Key Results
- Multi-view ensemble achieves 79.33% accuracy on Fakeddit and 83.33% on MMFakeBench
- Single-view approaches show significant divergence on biased samples but convergence on modality-balanced samples
- Modality-balanced samples exhibit higher agreement across all three views
- Detector sensitivity causes accuracy fluctuations exceeding 10% when switching misinformation detectors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Quantifying the "fair contribution" of each modality via cooperative game theory provides a coarse-grained bias signal.
- **Mechanism**: The framework calculates Shapley values (φ) to measure the marginal benefit of adding a specific modality (e.g., Image) to a coalition. If the Shapley value for text (φ_text) significantly outweighs image (φ_image) in achieving a correct prediction, the sample is classified as "Uni-text."
- **Core assumption**: The detector's performance on uni-modal inputs reflects the inherent information density of that modality for that specific sample.
- **Evidence anchors**:
  - [Section 3.1] Defines the benefit function V and marginal benefit calculation using Shapley values to determine φ_m1.
  - [Table 1] Shows Modality Benefit identifies bias but fluctuates significantly (71.67% vs 48.67%) across datasets.
- **Break condition**: If the underlying detector achieves near-100% accuracy on both uni-modal and multi-modal inputs (Saturation Effect), the Shapley value difference collapses to zero.

### Mechanism 2
- **Claim**: Analyzing the gradient-weighted attention flow from input tokens to the output token reveals which modality the model "looks at" during inference.
- **Mechanism**: This view computes Saliency Scores (S) by taking the Hadamard product of attention matrices (A^h) and the gradient of the loss (∂L/∂A^h). It aggregates the flow from image tokens (S_it) vs. text tokens (S_tt) to the final output token (OT).
- **Core assumption**: Higher accumulated gradient-weighted attention implies greater decision-making influence.
- **Evidence anchors**:
  - [Section 3.2] Details the saliency score calculation S and the aggregation into S_it,norm and S_tt,norm using a threshold ε.
  - [Appendix 10.2] demonstrates that "Average" or "Max" strategies fail due to the "modality gap" (token count imbalance), validating the "Sum" approach.
- **Break condition**: Fails if the LVLM does not use a Transformer architecture with accessible attention heads or if the text is extremely short compared to dense image patches.

### Mechanism 3
- **Claim**: Constructing a structural causal model (SCM) and performing counterfactual reasoning isolates whether the model uses "shortcuts" (direct paths) or genuine fusion (indirect paths).
- **Mechanism**: The system builds a causal graph (I → C/E → O, T → W/R → O). It uses Total Indirect Effect (TIE) to measure fusion reliance and Natural Direct Effect (NDE) to measure bias.
- **Core assumption**: The extraction of "core information" (Entity E, Core Words W) via external models (MiniCPM-V, Llama3) is accurate enough to serve as variables in the causal graph.
- **Evidence anchors**:
  - [Section 3.3] Formulates the causal inference graph and defines TIE/NDE for the variables W, E, C, R.
  - [Figure 2(d)] Visualizes the causal paths where bias is defined by the dominance of direct paths (I → O) over fusion paths (F → O).
- **Break condition**: If the core information extraction models hallucinate or fail to identify key entities, the causal graph structure is invalid.

## Foundational Learning

- **Concept**: Shapley Value (Game Theory)
  - **Why needed here**: It provides the mathematical basis for the "Modality Benefit" view, allowing the system to fairly distribute the "credit" for a correct prediction between the image and text modalities.
  - **Quick check question**: If a model predicts correctly using only text, what is the Shapley value of the image modality? (Answer: Zero or negligible).

- **Concept**: Saliency Maps (Gradient x Input)
  - **Why needed here**: This underpins the "Modality Flow" view. You must understand how backpropagation from the output to the input attention weights reveals which tokens the model actually relied upon.
  - **Quick check question**: Why does the paper use the Hadamard product of attention and gradients, rather than just raw attention weights? (Answer: Raw attention shows where the model looks, but gradients weight that attention by its impact on the final decision).

- **Concept**: Counterfactual Reasoning (Causal Inference)
  - **Why needed here**: Required to understand the "Modality Causal Effect." You need to grasp the difference between observing a correlation and calculating the effect of an intervention (e.g., "What would the output be if the text were replaced by a placeholder, holding the image constant?").
  - **Quick check question**: In this context, what does a high "Natural Direct Effect" (NDE) of text on the output indicate? (Answer: The model is making predictions based on the text alone, bypassing fusion—indicating text bias).

## Architecture Onboarding

- **Component map**: Detectors (Image-only, Text-only, Multimodal, LVLM) -> Analyzers (Benefit, Flow, Causal) -> Ensembler (majority voting) -> Extractors (external LLMs for Causal view)
- **Critical path**: The Modality Causal Effect path is the most complex and expensive. It requires running inference on external LLMs to extract core entities/words before constructing the causal graph and computing counterfactuals.
- **Design tradeoffs**:
  - **Detector Sensitivity**: The system is accurate but brittle; changing the detector causes accuracy fluctuations >10%.
  - **Granularity vs. Cost**: "Modality Flow" requires a large LVLM and is computationally heavy, whereas "Modality Benefit" is faster but produces coarser signals.
- **Failure signatures**:
  - **Saturation Effect**: If the backbone detectors are too strong (100% accuracy on validation), the Shapley values equalize, and the system defaults to "Modality Balance" for all samples.
  - **Modality Gap**: In "Modality Flow," if you average saliency scores instead of summing them, the system falsely detects "Uni-text" bias because image tokens are more numerous but individually less semantic.
- **First 3 experiments**:
  1. **Sanity Check (Saturation)**: Run the Modality Benefit view on a validation set. If φ_image ≈ φ_text for >90% of samples, check detector accuracy; if accuracy is 100%, the detectors are "saturated" and must be replaced with slightly weaker models.
  2. **Threshold Calibration**: On the "Modality Flow" view, run a sweep on the hyperparameter ε using a small human-annotated set (e.g., 20 samples) to find the optimal balance point between "Uni-image" and "Uni-text" classification.
  3. **Ablation on Views**: Run the full pipeline vs. single views (Benefit, Flow, Causal) on a subset of Fakeddit. Confirm that single views diverge significantly on biased samples but converge on modality-balanced samples, validating the need for the multi-view ensemble.

## Open Questions the Paper Calls Out
None

## Limitations
- The automated bias detection system shows promising accuracy (79-83%) but exhibits critical vulnerabilities
- The analysis is highly sensitive to detector choice, with accuracy fluctuations exceeding 10% when switching between misinformation detectors
- The causal analysis depends heavily on the accuracy of external LLM-based core information extraction, which can hallucinate and invalidate the causal graph structure
- The Shapley value-based benefit analysis collapses when detectors achieve near-perfect accuracy, limiting its applicability to high-performing systems

## Confidence
- **High confidence**: The existence of modality bias in multimodal misinformation benchmarks is well-established; the multi-view ensemble consistently outperforms single-view approaches across different datasets
- **Medium confidence**: The specific accuracy numbers (79.33% on Fakeddit, 83.33% on MMFakeBench) are reliable within the context of the human evaluation methodology used
- **Low confidence**: The causal mechanism's reliance on external LLM extraction for core information creates significant uncertainty about whether the identified bias reflects true sample characteristics versus extraction errors

## Next Checks
1. **Detector Robustness Test**: Systematically evaluate the same samples across 5-7 different misinformation detectors (varying architectures) to quantify detector-induced variance and establish confidence intervals for bias classifications
2. **Extraction Error Analysis**: For samples where the causal view and other views disagree, manually examine the LLM-extracted core entities/words to determine hallucination rates and their correlation with disagreement patterns
3. **Saturation Boundary Characterization**: Identify the exact detector accuracy threshold (likely between 90-95%) where Shapley values equalize and the benefit view becomes ineffective, establishing clear operational boundaries for the system