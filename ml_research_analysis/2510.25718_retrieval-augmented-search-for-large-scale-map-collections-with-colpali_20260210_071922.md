---
ver: rpa2
title: Retrieval-Augmented Search for Large-Scale Map Collections with ColPali
arxiv_id: '2510.25718'
source_url: https://arxiv.org/abs/2510.25718
tags:
- search
- collections
- digital
- maps
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces map-RAS, a retrieval-augmented search system
  for large-scale historic map collections, specifically demonstrating on 101,233
  map images from the Library of Congress. The system addresses the challenge of searching
  maps, which combine visual and textual elements that traditional metadata or OCR-based
  methods cannot fully capture.
---

# Retrieval-Augmented Search for Large-Scale Map Collections with ColPali

## Quick Facts
- arXiv ID: 2510.25718
- Source URL: https://arxiv.org/abs/2510.25718
- Reference count: 19
- Primary result: map-RAS enables multimodal search over 101,233 historic map images with sub-second query response times using ColPali embeddings and late-interaction scoring

## Executive Summary
This paper introduces map-RAS, a retrieval-augmented search system for large-scale historic map collections, specifically demonstrating on 101,233 map images from the Library of Congress. The system addresses the challenge of searching maps, which combine visual and textual elements that traditional metadata or OCR-based methods cannot fully capture. By applying ColPali, a document-retrieval framework that uses multiple patch embeddings to parse different sections of documents, the system enables multimodal queries and dynamic corpus expansion. Users can search via text or image, receive ranked results with metadata, and generate LLM-based summaries of findings. The system processes embeddings on consumer-grade hardware, with query response times averaging under one second against the 101,233-item corpus. The paper also discusses use cases for archivists, curators, and researchers, and highlights the potential for inter-collection search across digital cultural heritage collections.

## Method Summary
The system uses ColQwen2 to generate 768 patch embeddings (128-dimensional each) per map image, capturing fine-grained visual features rather than collapsing the entire document into one vector. At query time, the MaxSim late-interaction mechanism computes maximum similarity between each query token and all document patches, enabling partial matches and cross-modal alignment. A Flask REST API loads all embeddings into memory and exposes `/search` and `/upload` endpoints, with query response times averaging under one second for the 101,233-item corpus. The system supports dynamic corpus expansion by processing user-contributed images through the identical ColQwen2 pipeline, projecting them into the same embedding space as the base corpus.

## Key Results
- Successfully processes 101,233 historic map images from Library of Congress using ColPali framework
- Achieves sub-second query response times (average under 1 second) for the full corpus
- Enables multimodal search via text and image queries with ranked results and LLM-generated summaries
- Supports dynamic corpus expansion where user-contributed images can be searched alongside base collection

## Why This Works (Mechanism)

### Mechanism 1: Multi-Vector Patch Embeddings for Fine-Grained Retrieval
Multi-vector patch embeddings enable retrieval of specific visual features in maps that single global embeddings miss. ColQwen2 generates 768 patch embeddings per map image (128-dimensional each), allowing the model to attend to specific regions rather than collapsing the entire document into one vector. At query time, MaxSim computation finds the maximum similarity between each query token and all document patches. This works because maps contain localized features (e.g., illustrations, annotations) that are semantically meaningful in isolation and benefit from region-specific matching.

### Mechanism 2: Late-Interaction Scoring for Cross-Modal Alignment
Late-interaction scoring via MaxSim enables better text-to-image retrieval than single-vector similarity. Instead of pre-computing a single similarity score, the system computes maximum similarity per query token against all document patches, then aggregates. This allows partial matches (e.g., "ships" matching only patches containing ship illustrations). This works because query tokens correspond to visually localizable features in the target images, and the late-interaction preserves token-patch relationships that single-vector approaches discard.

### Mechanism 3: Dynamic Corpus Expansion via Unified Embedding Space
User-contributed images can be searched alongside the base corpus without retraining or centralization. All images—base corpus or user-uploaded—are processed through the identical ColQwen2 pipeline, projecting them into the same 128-dimensional space. The system stores only embeddings (not images), enabling federated search while respecting data sovereignty. This works because ColQwen2's embedding space is sufficiently general to handle out-of-distribution map styles from user uploads.

## Foundational Learning

- **Vision-Language Models (VLMs) and Shared Embedding Spaces**: ColQwen2 is a VLM that maps both text and images to a common vector space; understanding this is prerequisite to grasping how text queries retrieve images. Quick check: Can you explain why a text query and an image can be compared mathematically in a VLM?

- **Late Interaction vs. Bi-Encoder Retrieval**: The paper's MaxSim scoring differs from standard bi-encoder approaches; late interaction preserves token-patch relationships that bi-encoders discard. Quick check: How does MaxSim differ from computing cosine similarity between two single vectors?

- **IIIF (International Image Interoperability Framework)**: The pipeline uses IIIF for image handling; practitioners need this to replicate or extend the system with cultural heritage collections. Quick check: What advantage does IIIF provide over static image files for a retrieval system?

## Architecture Onboarding

- **Component map**: IIIF image URLs → CSV metadata → batch download (500 images/batch) → ColQwen2 model (5.62GB VRAM) → 768 × 128-dim patch embeddings per image → distributed pickle files (28GB total for corpus) → Flask REST API → loads all embeddings into memory → exposes `/search`, `/upload` endpoints → Query text → ColQwen2 embedding → MaxSim scoring → top-k results → Llama 3.2-1B summarization (optional)

- **Critical path**: 1) Batch embedding generation (3-4 sec/image on T4 GPU) 2) Loading embeddings into memory at API startup 3) MaxSim scoring against full corpus at query time (~6 sec for 120K corpus)

- **Design tradeoffs**: ColQwen2 vs. ColPali: Authors chose ColQwen2 (768 patches) over ColPali (1024 patches) to reduce computational cost; tradeoff is fewer patches for fine-grained matching. In-memory embeddings: 28GB corpus loaded at startup enables sub-second retrieval for 25K+ images but limits scalability; inference time grows linearly with corpus size. Summary from metadata only: Llama 3.2 summarizes metadata, not raw image content; future work proposes direct image-based summarization.

- **Failure signatures**: Query latency spikes when corpus exceeds available RAM (paging to disk). Low similarity scores for image-to-image queries (paper notes scores are order-of-magnitude higher than text-to-image—may indicate calibration issue). User uploads fail silently if IIIF endpoints are unreachable or malformed.

- **First 3 experiments**: 1) **Baseline retrieval quality**: Query the demo with 10-20 known map features (e.g., "compass rose," "ships," "topographic contours") and measure precision@5 against ground-truth metadata. This validates patch-level matching. 2) **Latency scaling test**: Create synthetic corpora of 10K, 50K, 100K, 200K embeddings and measure query latency to confirm linear scaling and identify memory thresholds. 3) **Cross-collection transfer**: Upload a small external map collection (e.g., 50 maps from a different institution) and test whether retrieval quality degrades compared to in-corpus queries. This tests the dynamic expansion assumption.

## Open Questions the Paper Calls Out

- Can approximate nearest neighbor techniques or hierarchical index structures achieve sub-second query latency for map corpora exceeding 500,000 items? The paper notes that inference time grows linearly with corpus size, demanding new techniques for small-scale inference once corpus size exceeds a few hundred thousand.

- Does fine-tuning ColQwen2 on digitized historical maps improve retrieval precision for domain-specific features such as cartographic symbols, period-specific typography, and historical place names? The paper plans to explore finetuning ColPali for specific use with digital cultural heritage collections.

- How does map-RAS compare to traditional metadata search and CLIP-based retrieval in terms of user task success, time-on-task, and result relevance for archivist workflows? The paper hopes to conduct user evaluations to compare search paradigms on standardized tasks.

- Do LLM summaries generated from directly extracted visual map content (rather than metadata) produce more accurate and informative descriptions of search results? The paper plans to expand into a full vision language model-based RAG system by generating summaries based on what the model finds from the map images directly.

## Limitations

- Dynamic corpus expansion validation gap: The paper claims user-contributed images can be searched alongside the base corpus, but provides no empirical validation of this capability. The assumption that ColQwen2 embeddings generalize to out-of-distribution map styles remains unvalidated.

- Cross-modal score calibration: The paper notes that image-to-image similarity scores are order-of-magnitude higher than text-to-image scores, but doesn't address whether this reflects genuine semantic differences or scoring miscalibration.

- No quantitative retrieval quality metrics: While the system claims to enable "fine-grained semantic matching," no precision/recall, MRR, or other standard IR metrics are reported to validate retrieval effectiveness against ground truth.

## Confidence

- **High confidence**: ColPali's multi-vector patch embeddings enable localized feature matching in maps (supported by mechanism description and related ColPali work).
- **Medium confidence**: Late-interaction MaxSim scoring improves text-to-image retrieval over single-vector methods (supported by design rationale but no ablation studies).
- **Low confidence**: Dynamic corpus expansion works robustly across heterogeneous map collections (claimed but not empirically validated).

## Next Checks

1. **Retrieval quality validation**: Query the demo with 10-20 known map features (e.g., "compass rose," "ships," "topographic contours") and measure precision@5 against ground-truth metadata to empirically validate patch-level matching effectiveness.

2. **Latency scaling verification**: Create synthetic corpora of 10K, 50K, 100K, 200K embeddings and measure query latency to confirm linear scaling, identify memory thresholds, and validate the sub-second response claim for 25K+ images.

3. **Cross-collection transfer test**: Upload a small external map collection (e.g., 50 maps from a different institution) and test whether retrieval quality degrades compared to in-corpus queries to validate the dynamic expansion assumption.