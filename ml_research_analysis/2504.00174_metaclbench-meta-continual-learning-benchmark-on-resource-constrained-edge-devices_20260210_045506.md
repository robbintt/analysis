---
ver: rpa2
title: 'MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge
  Devices'
arxiv_id: '2504.00174'
source_url: https://arxiv.org/abs/2504.00174
tags:
- learning
- accuracy
- methods
- anml
- meta-cl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MetaCLBench, a comprehensive benchmark framework
  designed to evaluate Meta-Continual Learning (Meta-CL) methods on resource-constrained
  edge devices. The framework assesses six representative Meta-CL methods across three
  network architectures (CNN, YAMNet, ViT) and five datasets spanning image and audio
  modalities, measuring not only accuracy but also deployment-critical metrics including
  memory footprint, latency, and energy consumption on devices ranging from 512 MB
  to 4 GB RAM.
---

# MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge Devices

## Quick Facts
- arXiv ID: 2504.00174
- Source URL: https://arxiv.org/abs/2504.00174
- Authors: Sijia Li; Young D. Kwon; Lik-Hang Lee; Pan Hui
- Reference count: 40
- Key outcome: MetaCLBench evaluates six Meta-Continual Learning methods on edge devices, revealing significant memory and energy costs while showing lightweight CNNs can outperform complex architectures for Meta-CL tasks.

## Executive Summary
This paper introduces MetaCLBench, a comprehensive benchmark framework designed to evaluate Meta-Continual Learning (Meta-CL) methods on resource-constrained edge devices. The framework assesses six representative Meta-CL methods across three network architectures (CNN, YAMNet, ViT) and five datasets spanning image and audio modalities, measuring not only accuracy but also deployment-critical metrics including memory footprint, latency, and energy consumption on devices ranging from 512 MB to 4 GB RAM. The study reveals that while many Meta-CL methods can learn new classes effectively for both image and audio modalities, they impose significant computational and memory costs on edge devices, with up to three methods causing out-of-memory failures on sub-1 GB devices. The research demonstrates that LifeLearner achieves near-oracle accuracy while consuming 2.54-7.43× less energy than the Oracle method, and that larger or more sophisticated architectures like ViT and YAMNet do not necessarily yield better Meta-CL performance, challenging conventional assumptions about model complexity. The paper provides practical deployment guidelines for researchers and practitioners implementing Meta-CL in resource-constrained environments and plans to make the benchmark framework and tools publicly available to enable fair evaluation across both accuracy and system-level metrics.

## Method Summary
MetaCLBench evaluates six Meta-Continual Learning methods (OML, ANML, OML+AIM, ANML+AIM, LifeLearner, Latent OML) using three architectures (3-layer CNN with 112 channels, YAMNet with pretrained weights, ViT with pretrained weights) on five datasets (CIFAR-100, MiniImageNet, GSCv2, UrbanSound8K, ESC-50). The framework measures accuracy, peak memory footprint, end-to-end latency, and energy consumption on edge devices ranging from 512 MB to 4 GB RAM. Meta-training uses batch size 1 for inner loop and 64 for outer loop over 20,000 steps with learning rates α=β=0.001, while pre-training CNNs until validation convergence. Meta-testing evaluates with 10 learning rates, reporting best results. Audio data is resampled to 22kHz/32kHz with 128×85 or 157×64 spectrogram features, while images use standard CIFAR/MiniImageNet preprocessing.

## Key Results
- Meta-CL methods achieve 60-80% accuracy on new classes for both image and audio modalities, with LifeLearner matching Oracle performance while using 2.54-7.43× less energy
- AIM-enhanced methods (ANML+AIM, OML+AIM) require 608-2,648 MB peak memory, causing out-of-memory failures on sub-1GB devices (RPi 3B+ and Zero 2)
- Well-tuned 3-layer CNNs outperform more sophisticated architectures like ViT and YAMNet under Meta-CL conditions, challenging assumptions about model complexity
- All methods except Latent OML incur 10-1,000× higher energy consumption than inference-only Oracle baseline due to meta-training overhead

## Why This Works (Mechanism)
None

## Foundational Learning
- **Meta-Continual Learning**: Framework for sequential class learning with few-shot samples while avoiding catastrophic forgetting - needed for lifelong learning on edge devices where new classes arrive continuously; quick check: verify methods can learn new classes without forgetting old ones using rehearsal buffers.
- **Catastrophic forgetting**: Neural networks rapidly lose previously learned knowledge when trained on new tasks - critical challenge in continual learning scenarios; quick check: measure accuracy drop on previously learned classes after new class training.
- **Rehearsal-based methods**: Techniques that maintain small buffers of past samples to prevent forgetting - fundamental approach in Meta-CL; quick check: verify replay buffer size and its impact on memory usage and accuracy.
- **AIM (Average Gradient Outer Loop Meta-Learning)**: Method for learning task-agnostic feature representations in Meta-CL - used in ANML+AIM and OML+AIM but causes high memory consumption; quick check: monitor memory during AIM computation to identify bottlenecks.
- **8-bit quantization**: Technique to reduce model size and computation through integer arithmetic - used in LifeLearner to achieve energy efficiency; quick check: compare quantized vs floating-point model performance and energy usage.

## Architecture Onboarding
- **Component map**: Meta-training (batch size 1 inner, 64 outer) -> Meta-testing (10 learning rates) -> Deployment (device measurement: accuracy, memory, latency, energy)
- **Critical path**: Pre-training source classes -> Meta-training with rehearsal buffers -> Meta-testing with few-shot adaptation -> Resource measurement on target device
- **Design tradeoffs**: Model complexity vs memory constraints (ViT/YAMNet vs CNN), rehearsal buffer size vs accuracy (larger buffers prevent forgetting but increase memory), quantization vs accuracy (8-bit reduces energy but may impact performance)
- **Failure signatures**: OOM errors on sub-1GB devices for AIM-enhanced methods, accuracy collapse without proper pre-training, energy spikes during meta-training vs stable inference-only operation
- **First experiments**: 1) Implement basic CNN Meta-CL baseline on CIFAR-100 to verify catastrophic forgetting prevention, 2) Measure memory usage of AIM-enhanced methods during backpropagation to identify OOM causes, 3) Compare quantized vs unquantized LifeLearner energy consumption on Jetson Nano

## Open Questions the Paper Calls Out
- **Long-term deployment stability**: Can Meta-CL methods maintain stable performance and resource efficiency during weeks or months of continuous deployment on edge devices? All experiments evaluate fixed task sequences with limited duration, but real-world IoT deployments require continuous operation where memory fragmentation and thermal throttling may emerge. Resolved by longitudinal studies (30+ days) monitoring accuracy drift, memory patterns, and energy consumption over time.

- **Hardware accelerator generalization**: Do observed Meta-CL performance and resource trade-offs generalize to platforms with neural accelerators like Google Coral or specialized NPUs? The benchmark evaluates only CPU-based inference and training, but accelerators offer different memory hierarchies and quantization support that may alter viability of memory-intensive methods like AIM-enhanced approaches. Resolved by replicating evaluation on accelerator-equipped devices and comparing against CPU-only results.

- **Architectural performance drivers**: What mechanisms explain why well-tuned lightweight 3-layer CNNs outperform more sophisticated architectures like ViT and YAMNet under difficult Meta-CL conditions? The phenomenon is observed empirically but underlying causes (meta-training dynamics, overfitting in high-capacity models, inductive bias mismatches) remain unexplored. Resolved by ablation studies isolating factors like meta-training sample efficiency and architectural inductive biases across wider range of architectures.

## Limitations
- Limited architectural specifications for AIM module and exact layer configurations beyond channel counts
- Unknown meta-training/test class splits and replay buffer sizes affecting exact reproducibility
- Evaluation assumes idealized conditions and may not capture all real-world edge deployment scenarios

## Confidence
- **High**: Benchmark framework design, device measurement methodology, and comparative accuracy trends across methods
- **Medium**: Absolute performance values and energy consumption comparisons due to potential implementation differences
- **Low**: Specific architectural details for AIM module and exact meta-training configurations

## Next Checks
1. Verify reproducibility of LifeLearner's 2.54-7.43× energy advantage by implementing 8-bit quantization and comparing against unquantized Oracle baseline on identical hardware
2. Test the claim that ViT and YAMNet do not improve Meta-CL performance by training all six methods with each architecture on CIFAR-100 and MiniImageNet
3. Validate the OOM behavior of AIM-enhanced methods by measuring peak memory usage during backpropagation across different replay buffer sizes on RPi 3B+ and Zero 2