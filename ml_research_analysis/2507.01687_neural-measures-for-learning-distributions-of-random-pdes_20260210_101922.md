---
ver: rpa2
title: Neural Measures for learning distributions of Random PDEs
arxiv_id: '2507.01687'
source_url: https://arxiv.org/abs/2507.01687
tags:
- neural
- probability
- networks
- measures
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Neural Measures, a novel framework that integrates
  Scientific Machine Learning (SciML) with uncertainty quantification (UQ) for random
  partial differential equations (PDEs). The approach combines Physics-Informed Neural
  Networks (PINNs) with generative modeling techniques to represent and control uncertainty
  in forward problems while maintaining predictive accuracy.
---

# Neural Measures for learning distributions of Random PDEs

## Quick Facts
- arXiv ID: 2507.01687
- Source URL: https://arxiv.org/abs/2507.01687
- Reference count: 40
- Primary result: Integrates PINNs with generative modeling to represent uncertainty in random PDEs via pushforward measures

## Executive Summary
This work introduces Neural Measures, a framework combining Scientific Machine Learning with uncertainty quantification for random PDEs. The approach represents solutions as pushforward measures from reference distributions, enabling systematic uncertainty quantification while maintaining predictive accuracy. Three architectures are proposed that integrate PINNs with generative modeling, validated on three representative problems showing accurate capture of both mean and higher-order statistical properties.

## Method Summary
The method constructs discrete neural measure spaces parameterized by neural network functions and reference probability measures. Solutions are represented as pushforward measures where neural networks learn deterministic maps from parameter space to solution space. Three architectures are developed: fully network-based, PCE-NN, and Galerkin-NN. The framework uses Wasserstein distances between probability measures on function spaces, which simplify to tractable L² residuals when target distributions are atomic. Universal approximation theorems establish density of these neural measure spaces under Wasserstein metric.

## Key Results
- Neural measure spaces can approximate any probability measure in relevant function spaces under Wasserstein metric
- Three architectures (Fully-NN, PCE-NN, Galerkin-NN) successfully capture distributions in bistable ODE, diffusion PDE, and reaction-diffusion PDE
- The atomic simplification of Wasserstein distance to L² residuals enables practical training
- Method handles infinite-dimensional probability distributions while maintaining uncertainty control

## Why This Works (Mechanism)

### Mechanism 1: Pushforward Measure Representation
Neural networks approximate probability distributions over infinite-dimensional function spaces by learning deterministic maps from reference distributions. Given reference measure γ on parameter space Ξ, a neural network X_θ: Ξ → H induces distribution ν_θ = [X_θ]#γ on solution space H via pushforward, transforming "learn a distribution" into "learn a function that generates the target distribution when composed with random inputs."

### Mechanism 2: Wasserstein-to-L2 Simplification via Atomic Measures
When target distribution is atomic (Dirac delta), Wasserstein distance reduces to tractable L² residuals. For residual operators, W_p(Ã⊙μ_θ, δ_0) simplifies to E[||Ã_ξX_θ(ξ)||^p_H]—an integral rather than optimal transport problem, making loss computation practical.

### Mechanism 3: Universal Approximation via Bochner Space Separability
Under separability of L²(γ;H), neural networks with non-polynomial activations approximate coefficient functions in spectral expansion. Since L² norm controls W₂ metric, L² approximation implies W₂ approximation, establishing density of neural measure spaces.

## Foundational Learning

- **Concept: Pushforward Measures**
  - Why needed here: Core representation mechanism—understanding how deterministic functions transform probability distributions
  - Quick check question: Given measure γ on A and function f: A → B, what is P(f(x) ∈ S) for S ⊆ B?

- **Concept: Bochner Spaces L²(γ;H)**
  - Why needed here: Paper formulates solutions as functions from parameter space to Hilbert space, requiring Bochner integration
  - Quick check question: What ensures U: Ξ → H has finite second moment in Bochner sense?

- **Concept: Wasserstein Distance**
  - Why needed here: Loss function built on W₂ distances between probability measures on function spaces
  - Quick check question: How does W₂(μ, ν) behave when μ and ν have disjoint support compared to KL divergence?

## Architecture Onboarding

- **Component map**:
  Input: (x,t) ∈ D×[0,T], ξ ∼ γ → Three variants: Fully-NN, PCE-NN, Galerkin-NN → Loss: W₂(A⊙μ_θ, δ_f) + W₂(B⊙μ_θ, δ_g) → L² residuals (via atomic simplification)

- **Critical path**:
  1. Specify reference measure γ on parameter space Ξ
  2. Choose architecture (Fully-NN simplest, PCE-NN for separation)
  3. Compute residuals for sampled (x,t,ξ) points
  4. Optimize via L-BFGS with periodic resampling

- **Design tradeoffs**:
  - Fully-NN vs PCE-NN: Fully-NN simpler but couples variables; PCE-NN separates stochastic/deterministic but requires truncation degree K
  - Resampling frequency: Paper uses domain every 50 iterations, parameters every 100—higher frequency captures physics but destabilizes training
  - Snake vs tanh: Snake activation σ(x;a) = x + sin²(ax)/a captures periodicity better but adds per-layer hyperparameter a

- **Failure signatures**:
  - Sharply peaked distributions show increasing W₂ despite decreasing training loss
  - Boundary errors at initial times (Figure 5.3b)
  - Testing loss rising while training drops signals overfitting (Figure A.1)

- **First 3 experiments**:
  1. Bistable ODE (Section 5.1): Single ODE, two random parameters, solution bifurcates to two stable equilibria—tests multi-modal distribution capture
  2. Linear diffusion (Section 5.2): Simple PDE with analytical solution, two random parameters—compare PINN vs PCE-NN convergence
  3. Reaction-diffusion (Section 5.3): Nonlinear PDE, four random parameters—tests boundary handling and higher complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can neural measures be adapted to accurately capture sharply peaked distributions that concentrate toward Dirac delta masses, as observed in the bistable ODE problem?
- Basis in paper: [explicit] The authors state "This sharp concentration makes convergence challenging, since the Wasserstein distance is highly sensitive to samples that have not fully collapsed onto the Dirac masses" and note that the Wasserstein distance increases over time for the bistable ODE
- Why unresolved: The current framework uses L2-type losses derived from Wasserstein distances, which become poorly suited for distributions approaching atomic measures
- What evidence would resolve it: Development of modified loss functionals or adaptive sampling strategies that maintain accuracy as distributions sharpen; successful numerical experiments on problems with known delta-limiting behavior

### Open Question 2
- Question: What systematic criteria should guide the selection among the three neural measure architectures (fully network-based, PCE-NN, Galerkin-NN) for a given random PDE problem?
- Basis in paper: [inferred] The diffusion experiments show PINN-PCE sometimes produces higher errors than plain PINN, while the authors state different architectures "can be implemented" without providing selection guidelines
- Why unresolved: The paper presents three architectures but does not analyze their relative strengths, weaknesses, or problem-dependent performance characteristics
- What evidence would resolve it: Comparative studies across problem classes identifying when each architecture excels; theoretical analysis relating architecture choice to properties of the random parameter distribution and PDE structure

### Open Question 3
- Question: Can adaptive training point selection based on residual errors effectively address the boundary accuracy degradation observed in the reaction-diffusion experiments?
- Basis in paper: [explicit] The authors note: "The error shown in Figure 5.3b shows larger values at the boundaries of the domain for initial times. This flaw can probably be fixed by using adaptive selection of the training points based on the error."
- Why unresolved: The suggestion is offered as speculation without implementation or testing
- What evidence would resolve it: Implementation of adaptive resampling strategies with demonstrated reduction in boundary errors; convergence analysis showing improved accuracy at domain boundaries

## Limitations

- The universal approximation theorems rely on separability of Bochner spaces and Polish parameter spaces, which may not hold for all relevant physical problems
- The atomic simplification of Wasserstein distances is elegant but fundamentally limited to Dirac target measures; extension to general distributions requires alternative divergences
- Numerical convergence rates are not rigorously established, with observed sensitivity to architecture choice and resampling frequency

## Confidence

- **High confidence** in the core mathematical framework (pushforward measures, Bochner space formulation) and numerical results for the three test cases
- **Medium confidence** in the universal approximation claims, which depend on assumptions not explicitly verified in practice
- **Low confidence** in scalability claims, as experiments use relatively low-dimensional parameter spaces (2-4 dimensions)

## Next Checks

1. **Parameter dimension scaling test**: Apply the framework to a random PDE with 10+ random parameters to assess curse-of-dimensionality effects
2. **Non-Dirac target distribution**: Replace Dirac loss with standard Wasserstein computation (e.g., Sinkhorn) to verify method works beyond the atomic simplification
3. **Boundary condition sensitivity**: Systematically vary boundary conditions in the reaction-diffusion problem to identify when distribution learning fails