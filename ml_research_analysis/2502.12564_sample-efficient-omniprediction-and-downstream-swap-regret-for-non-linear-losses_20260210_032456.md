---
ver: rpa2
title: Sample Efficient Omniprediction and Downstream Swap Regret for Non-Linear Losses
arxiv_id: '2502.12564'
source_url: https://arxiv.org/abs/2502.12564
tags:
- decision
- functions
- loss
- regret
- swap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of making online predictions that
  are useful to all downstream agents, even when agents have non-linear loss functions
  over multi-dimensional outcomes. The authors introduce "decision swap regret," which
  generalizes both omniprediction and swap regret, and develop algorithms to achieve
  it for arbitrary multi-dimensional Lipschitz loss functions in online adversarial
  settings.
---

# Sample Efficient Omniprediction and Downstream Swap Regret for Non-Linear Losses

## Quick Facts
- arXiv ID: 2502.12564
- Source URL: https://arxiv.org/abs/2502.12564
- Authors: Jiuyao Lu; Aaron Roth; Mirah Shi
- Reference count: 40
- Provides first polynomial sample-complexity bounds for omniprediction with Lipschitz loss functions

## Executive Summary
This paper introduces decision swap regret, a novel framework that unifies omniprediction and swap regret concepts to address online prediction problems where downstream agents have non-linear loss functions over multi-dimensional outcomes. The authors develop algorithms that achieve low decision swap regret for arbitrary multi-dimensional Lipschitz loss functions in online adversarial settings. A key innovation is the construction of basis functions that can uniformly approximate non-linear loss functions, transforming the problem into one with linear losses in a higher-dimensional space.

The work provides exponentially improved sample complexity bounds compared to previous approaches and establishes the first polynomial sample-complexity bounds for omniprediction with Lipschitz loss functions. The framework yields improved bounds for specific economically-relevant loss functions including CES (constant elasticity of substitution), Cobb-Douglas, and Leontief functions, demonstrating practical relevance beyond the general theoretical results.

## Method Summary
The core approach involves constructing a basis function expansion that can uniformly approximate arbitrary Lipschitz loss functions over multi-dimensional outcome spaces. By transforming non-linear losses into linear losses in a higher-dimensional space through this basis expansion, the problem becomes amenable to existing techniques from the omniprediction literature. The authors extend decision calibration methods to the online adversarial setting, enabling them to guarantee low swap regret across all possible downstream loss functions. This transformation allows the application of online learning algorithms that can achieve the desired regret bounds. The framework also includes an online-to-batch reduction that provides sample complexity bounds for the offline setting, making the results applicable to both sequential prediction and batch learning scenarios.

## Key Results
- First polynomial sample-complexity bounds for omniprediction with Lipschitz loss functions (previous bounds scaled exponentially with error parameter)
- First algorithm guaranteeing swap regret bounds for all downstream agents with non-linear loss functions over multi-dimensional outcomes
- Sample complexity and regret bounds that scale exponentially with outcome space dimension in general, but show improved bounds for specific economic loss functions: CES (polynomial in both dimension and error), Cobb-Douglas (pseudo-polynomial), and Leontief (exponential in dimension but polynomial in error)

## Why This Works (Mechanism)
The mechanism relies on the fundamental insight that non-linear loss functions can be uniformly approximated by linear combinations of basis functions. By constructing an appropriate basis that spans the space of Lipschitz loss functions, any non-linear loss can be expressed as a linear function in the expanded space. This transformation enables the application of linear loss techniques to non-linear problems. The decision calibration framework ensures that predictions remain calibrated across all possible downstream loss functions, which is crucial for achieving low swap regret. The online-to-batch reduction bridges the gap between sequential prediction guarantees and batch learning sample complexity, making the theoretical results practically applicable.

## Foundational Learning
- **Decision calibration**: Ensures predictions are well-calibrated with respect to all possible loss functions. Needed because omniprediction requires predictions to be useful regardless of the downstream agent's specific loss function. Quick check: Verify that the calibration error decreases as the number of samples increases.
- **Swap regret**: Measures performance against sequences of different loss functions, not just a fixed one. Essential for handling changing or unknown downstream agent preferences. Quick check: Compare swap regret against external regret to understand the additional benefit.
- **Lipschitz continuity**: Ensures that small changes in predictions lead to small changes in loss, which is crucial for uniform approximation. Required for the theoretical guarantees on approximation quality. Quick check: Verify that the loss functions satisfy the Lipschitz condition with the claimed constants.
- **Basis function approximation**: Allows non-linear functions to be represented as linear combinations in an expanded space. Critical for transforming non-linear problems into linear ones. Quick check: Test the approximation quality of the basis functions for different loss functions.
- **Online-to-batch reduction**: Converts online learning guarantees into sample complexity bounds for batch learning. Needed to apply sequential prediction results to traditional machine learning settings. Quick check: Compare the sample complexity bounds obtained through the reduction with direct batch learning approaches.

## Architecture Onboarding

**Component Map**: Input outcomes → Basis function expansion → Linear loss transformation → Online learning algorithm → Decision calibration → Low swap regret guarantees

**Critical Path**: The most time-critical path is the basis function construction and transformation, as errors here propagate through the entire system and affect all downstream guarantees. The approximation quality of the basis functions directly determines the achievable regret bounds.

**Design Tradeoffs**: The exponential dependence on outcome space dimension represents a fundamental tradeoff between generality and computational efficiency. While the framework works for arbitrary Lipschitz loss functions, the computational cost grows exponentially with dimension. This is balanced by improved polynomial bounds for specific economic loss functions that are commonly encountered in practice.

**Failure Signatures**: 
- If the basis functions fail to uniformly approximate the true loss functions, swap regret bounds will degrade
- Violations of Lipschitz continuity assumptions can cause the approximation guarantees to fail
- In the online setting, adversarial sequences that exploit the approximation error can lead to higher than expected regret
- The online-to-batch reduction may produce suboptimal sample complexity if the online algorithm's regret guarantees are loose

**First Experiments**:
1. Implement the algorithm for CES loss functions on a 2D dataset and measure the achieved swap regret against ground truth downstream loss functions
2. Test the basis function approximation quality for different economic loss functions by measuring the uniform approximation error
3. Evaluate the online-to-batch reduction on a streaming data scenario with synthetic non-linear downstream loss functions, comparing sample complexity against direct batch approaches

## Open Questions the Paper Calls Out
The paper identifies several open questions: extending the results to settings where loss functions are not exactly Lipschitz but have other regularity properties, exploring whether the exponential dependence on dimension can be avoided for broader classes of loss functions, and investigating the practical performance of the algorithms beyond theoretical guarantees. The authors also suggest that developing more efficient basis function constructions or alternative approximation schemes could yield improved bounds.

## Limitations
- Exponential dependence on outcome space dimension in the general case, making the approach computationally challenging for high-dimensional problems
- Requires multi-dimensional Lipschitz loss functions, excluding scenarios with discontinuous or non-Lipschitz losses
- Online-to-batch reduction, while theoretically sound, may face practical implementation challenges in real-world streaming scenarios
- The framework assumes full knowledge of the outcome space structure, which may not hold in all applications

## Confidence

**High Confidence Claims**:
- Decision swap regret framework provides a valid unification of omniprediction and swap regret concepts
- Sample complexity bounds scale exponentially with dimension in the general case
- The theoretical foundations for the online-to-batch reduction are sound

**Medium Confidence Claims**:
- Polynomial scaling for CES and Cobb-Douglas loss functions in practice
- Practical performance of the algorithms matches theoretical guarantees
- The specific basis function constructions are optimal for the given problem class

**Low Confidence Claims**:
- The exponential dependence on dimension is fundamentally unavoidable for all possible Lipschitz loss functions
- The approach extends seamlessly to settings with approximate rather than exact Lipschitz continuity

## Next Checks

1. Implement the algorithm for CES loss functions on a multi-dimensional real-world dataset to empirically verify the claimed polynomial scaling in both dimension and error parameter

2. Test the robustness of the approach when loss functions are approximately rather than exactly Lipschitz by introducing controlled violations and measuring the degradation in performance

3. Evaluate the online-to-batch reduction on a realistic streaming data scenario with synthetic non-linear downstream loss functions, comparing the achieved sample complexity against theoretical predictions and direct batch learning approaches