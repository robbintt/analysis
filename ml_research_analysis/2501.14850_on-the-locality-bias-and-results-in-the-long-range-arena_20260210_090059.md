---
ver: rpa2
title: On the locality bias and results in the Long Range Arena
arxiv_id: '2501.14850'
source_url: https://arxiv.org/abs/2501.14850
tags:
- transformer
- performance
- task
- long
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the nature of tasks in the Long Range Arena
  (LRA) benchmark and their implications for long-range dependency modeling. The authors
  find that the tasks in LRA are primarily positional and local, which favors architectures
  like SSMs and MEGA that incorporate inductive biases for locality.
---

# On the locality bias and results in the Long Range Arena

## Quick Facts
- **arXiv ID:** 2501.14850
- **Source URL:** https://arxiv.org/abs/2501.14850
- **Reference count:** 26
- **Primary result:** Small convolutional models with limited receptive fields achieve near state-of-the-art results on LRA, questioning the benchmark's validity for long-range dependency modeling

## Executive Summary
This paper investigates the Long Range Arena (LRA) benchmark and reveals that its tasks are primarily solvable through local and positional patterns rather than genuine long-range dependencies. Through systematic experimentation, the authors demonstrate that small convolutional models with restricted receptive fields can achieve competitive performance, while showing that standard Transformers can match specialized architectures like SSMs when provided with appropriate training techniques. The findings suggest that LRA's current design inadvertently favors architectures with strong locality inductive biases, and that the benchmark should be redesigned to better assess true long-range dependency modeling capabilities.

## Method Summary
The authors evaluate Transformers on LRA using improved training techniques including rotary position embeddings, data augmentation (AutoAugment-derived for CIFAR10), and multi-task denoising objectives (30% token masking). They systematically test convolutional models with varying kernel sizes to assess locality requirements, and compare performance against specialized architectures like SSMs and MEGA. The experiments cover all six LRA tasks: CIFAR10, Pathfinder, Pathfinder-X, Text Classification, Text Retrieval, and ListOps, with particular attention to how training strategies affect different architectural approaches.

## Key Results
- Small convolutional models with kernel sizes as small as 61 tokens achieve near state-of-the-art results on LRA tasks
- Transformers with rotary embeddings and multitask denoising achieve 85.69% average accuracy, comparable to MEGA's 86.25%
- Unrestricted convolution models (gMLP) match SSM performance when provided with appropriate training techniques and data augmentation
- Vanilla Transformers fail on LRA primarily due to overfitting on high-dimensional, scarce data rather than attention complexity limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LRA tasks are solvable primarily via short-range dependencies, causing architectures with strong locality inductive biases to outperform global attention models.
- **Mechanism:** A restricted convolutional model with kernel size 61 (and as small as 5 for some text tasks) can capture necessary features to achieve near-SOTA accuracy, implying the "long-range" signal is actually local or positional.
- **Core assumption:** Performance of restricted models implies information outside their receptive field is not strictly necessary for the defined objective function.
- **Evidence anchors:** Small kernel convolutions achieving near-SOTA results; Table 1 showing Text Classification accuracy dropping only slightly with kernel size 5 vs 61.
- **Break condition:** If tasks are introduced where target variables depend on token interactions separated by >1000 steps, this mechanism fails.

### Mechanism 2
- **Claim:** SSM structural constraints function primarily as regularization and learning efficiency tools for small datasets rather than strict requirements for modeling long sequences.
- **Mechanism:** When training strategies (augmentation, multitask denoising) are applied, unrestricted convolution models match SSM performance, suggesting complex SSM parameterization solves data scarcity rather than sequence length problems.
- **Core assumption:** Training strategies don't introduce harmful biases that artificially inflate gMLP performance.
- **Evidence anchors:** gMLP with training techniques achieves 85.98% average accuracy comparable to S4/S5; Table 3 showing unrestricted convolution performance.
- **Break condition:** If sequence length scales significantly beyond training window, unrestricted models may fail where SSMs with decay priors remain stable.

### Mechanism 3
- **Claim:** Standard Transformers fail on LRA due to overfitting on high-dimensional, scarce data, remedied by injecting relative positional priors and robust regularization.
- **Mechanism:** Rotary Embeddings inject decay prior similar to SSMs; multitask denoising forces robust representation learning from bytes, closing performance gap with SSMs without architectural changes to attention.
- **Core assumption:** Poor baseline Transformer performance stems from optimization landscape rather than quadratic attention complexity.
- **Evidence anchors:** Transformer + Techniques reaches 85.69% vs MEGA 86.25%; Table 4 showing performance drop without training techniques.
- **Break condition:** If sequence length requires strict O(N) inference latency, this mechanism works in training but fails in deployment.

## Foundational Learning

- **Concept: Receptive Field vs. Global Attention**
  - **Why needed here:** To understand why small kernels can solve "long-range" benchmarks
  - **Quick check question:** If a model with receptive field of 60 tokens solves a 4000-token task, is the task truly measuring long-range dependency?

- **Concept: Inductive Bias (Decay/Locality)**
  - **Why needed here:** To explain why SSMs and RoPE-Transformers succeed where vanilla Transformers fail
  - **Quick check question:** Does adding a "decay" mechanism (distant tokens matter less) help or hurt a model trying to connect two distant points in an image?

- **Concept: Data Efficiency & Regularization**
  - **Why needed here:** To grasp why architectural "improvements" might just be masking data scarcity
  - **Quick check question:** If Model A beats Model B on small dataset but Model B wins on massive dataset, which has better inductive bias for small data?

## Architecture Onboarding

- **Component map:** Input sequences -> Positional Encoding (Rotary) -> Backbone Processing (Transformer/Convolution) -> Auxiliary Loss (Denoising) + Task Loss
- **Critical path:** Data Augmentation (domain-specific) -> Positional Encoding (RoPE) -> Backbone Processing -> Auxiliary Loss (Denoising) + Task Loss
- **Design tradeoffs:**
  - **Vanilla Transformer:** High expressiveness, low data efficiency (fails on LRA)
  - **SSM:** High efficiency & locality bias, lower expressiveness (harder to train on complex data?)
  - **RoPE-Transformer:** High expressiveness, medium efficiency (requires regularization techniques)
- **Failure signatures:**
  - **Text Tasks:** Accuracy stalls at ~64% (chance level) without denoising
  - **ListOps:** Model predicts only mode of distribution if overfitting occurs
  - **General:** Large gap between Training and Validation accuracy indicating memorization
- **First 3 experiments:**
  1. **Kernel Sweep:** Train Conv1D on LRA text tasks with kernel sizes [5, 11, 31, 61] to confirm locality signal
  2. **Embedding Ablation:** Train Transformer with (a) Learned Embeddings vs (b) Rotary Embeddings on Retrieval task to quantify bias impact
  3. **Regularization Check:** Train gMLP on Pathfinder (a) without augmentation and (b) with "easy" set mixing to validate data efficiency hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a benchmark be effectively designed to genuinely assess long-range dependencies without being solvable primarily via short-range positional patterns?
- **Basis in paper:** [explicit] The authors conclude that their "insights indicate that LRA results should be interpreted with caution and call for a redesign of the benchmark."
- **Why unresolved:** The paper demonstrates current LRA tasks are susceptible to local heuristics but does not propose a specific replacement benchmark architecture.
- **What evidence would resolve it:** Creation and validation of a new task suite where models with bounded receptive fields cannot achieve competitive accuracy compared to global attention models.

### Open Question 2
- **Question:** Do the locality biases and short-range dependency heuristics identified in LRA generalize to real-world long-context tasks?
- **Basis in paper:** [inferred] The paper notes the "success of the Transformer in real-world settings has not been replicated" by architectures that dominate LRA.
- **Why unresolved:** The study is confined to specific synthetic and byte-level tasks within LRA; it does not verify if simple convolutional models perform similarly well on real-world data like long-document summarization.
- **What evidence would resolve it:** Comparative study showing small-kernel convolutional models match performance of SSMs and Transformers on established real-world long-sequence datasets.

### Open Question 3
- **Question:** What specific mechanisms enable models to solve structural ListOps operations (MIN, MAX, MED) while failing to learn arithmetic SUM MOD (SM) operation?
- **Basis in paper:** [inferred] The authors observe models "are still guessing randomly in the SUM MOD operation" even when achieving high accuracy on other operators.
- **Why unresolved:** The analysis identifies failure mode but doesn't explain if bottleneck is lack of specific arithmetic inductive bias or inability to model specific dependency path required for summation.
- **What evidence would resolve it:** Ablation study analyzing attention maps or convolutional kernel behaviors specifically during SM operations to identify if model fails to aggregate information or simply fails to compute modulus.

## Limitations

- The claim that LRA tasks are "primarily local" lacks formal theoretical justification, resting on empirical demonstrations without proving long-range information is redundant rather than difficult to extract
- Data efficiency claims are compelling but incomplete, as the paper doesn't systematically explore whether SSMs maintain advantages at extreme scales (100k+ tokens) where their decay priors might provide stability benefits
- The assertion that LRA should be "redesigned" is a strong normative claim requiring broader community validation beyond this single paper's findings

## Confidence

**High Confidence:** Empirical results showing Transformers with rotary embeddings and multitask denoising achieving 85.69% average accuracy on LRA are well-supported by presented data; ablation studies demonstrating necessity of these techniques are rigorous and convincing.

**Medium Confidence:** Claim that LRA tasks are "primarily local/positional" is strongly supported by kernel size experiments but lacks formal theoretical justification; interpretation that SSM restrictions serve as data efficiency mechanisms is plausible but could benefit from additional scaling experiments.

**Low Confidence:** Assertion that LRA should be "redesigned" as a benchmark is a strong normative claim that, while logically following from analysis, requires broader community validation and consensus beyond this paper's findings.

## Next Checks

1. **Kernel Size Scalability Test:** Systematically evaluate performance drop as receptive field size decreases from 61 to 3 on each LRA task; measure point where accuracy degradation becomes severe (>10% drop) to establish quantitative bounds on locality requirements.

2. **Scaling Law Analysis:** Train unrestricted convolution (gMLP) model with extensive augmentation on progressively larger versions of LRA tasks (5x, 10x dataset size) to determine whether performance plateaus or continues improving, and whether SSMs show different scaling behavior.

3. **Long-Range Dependency Injection:** Create synthetic variants of LRA tasks where target labels depend on token interactions separated by 500+ positions (e.g., parity of tokens at distance 600); test whether small kernels fail completely while SSMs and properly-trained Transformers maintain performance, validating existence of genuine long-range dependencies.