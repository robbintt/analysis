---
ver: rpa2
title: 'You''ve Changed: Detecting Modification of Black-Box Large Language Models'
arxiv_id: '2504.12335'
source_url: https://arxiv.org/abs/2504.12335
tags:
- text
- generated
- features
- llms
- distributions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for detecting changes in black-box
  Large Language Models (LLMs) provided via API by comparing distributions of linguistic
  and psycholinguistic features from generated text. The approach uses the Kolmogorov-Smirnov
  test to compare feature distributions from two samples of LLM-generated text, enabling
  developers to identify when an LLM has changed.
---

# You've Changed: Detecting Modification of Black-Box Large Language Models

## Quick Facts
- arXiv ID: 2504.12335
- Source URL: https://arxiv.org/abs/2504.12335
- Reference count: 40
- Primary result: Detects changes in black-box LLMs by comparing linguistic feature distributions using Kolmogorov-Smirnov testing

## Executive Summary
This paper presents a method for detecting changes in black-box Large Language Models (LLMs) provided via API by comparing distributions of linguistic and psycholinguistic features from generated text. The approach uses the Kolmogorov-Smirnov test to compare feature distributions from two samples of LLM-generated text, enabling developers to identify when an LLM has changed. Experiments with five OpenAI completion models and Meta's Llama 3 70B chat model demonstrate that simple text features coupled with statistical testing can reliably distinguish between different LLMs. The method can detect LLM changes as small as 3% in mixture scenarios and is sensitive enough to detect subtle prompt injection attacks that alter sentiment distributions. This enables frequent LLM monitoring without computationally expensive benchmark evaluations.

## Method Summary
The method generates text samples from the target LLM using standardized prompts, extracts linguistic and psycholinguistic features from the generated text, and compares feature distributions between baseline and current samples using the Kolmogorov-Smirnov test. The approach focuses on distributional changes rather than semantic similarity, making it robust to prompt variations. The authors evaluated five OpenAI completion models and Meta's Llama 3 70B chat model, using features such as word frequency, sentiment scores, and psycholinguistic properties. The detection framework operates by establishing a baseline distribution from an initial sample, then periodically collecting new samples to compare against this baseline using statistical testing to flag significant deviations.

## Key Results
- Successfully distinguishes between five different OpenAI completion models with high statistical significance
- Detects LLM changes as small as 3% in controlled mixture scenarios
- Identifies prompt injection attacks that alter sentiment distributions through feature distribution shifts
- Achieves reliable detection without requiring computationally expensive benchmark evaluations

## Why This Works (Mechanism)
The method works by leveraging the fact that different LLMs, even when trained on similar data, develop distinct statistical patterns in their generated text. These patterns manifest in measurable linguistic and psycholinguistic features that remain relatively stable for a given model but shift when the underlying model changes. The Kolmogorov-Smirnov test provides a non-parametric way to detect these distributional shifts without making assumptions about the underlying feature distributions. By focusing on feature distributions rather than individual responses, the method achieves robustness to prompt variations while maintaining sensitivity to genuine model changes.

## Foundational Learning

**Kolmogorov-Smirnov Test**: Non-parametric statistical test for comparing two distributions - needed for detecting distributional shifts without assuming normality; quick check: verify test statistic calculation and p-value interpretation

**Linguistic Feature Extraction**: Process of quantifying text properties like word frequency, sentiment, and psycholinguistic characteristics - needed to create measurable representations of LLM behavior; quick check: validate feature extraction consistency across different text samples

**Statistical Power Analysis**: Determining sample size needed to detect effects of given magnitude - needed to balance detection sensitivity with monitoring cost; quick check: verify that 250-response sample size provides adequate power for expected effect sizes

## Architecture Onboarding

**Component Map**: Prompt Generator -> LLM API -> Text Processor -> Feature Extractor -> Distribution Comparator -> Change Detector

**Critical Path**: The core detection pipeline involves generating text samples, extracting features, computing distributions, and applying KS testing. Each component must maintain consistency to ensure reliable change detection.

**Design Tradeoffs**: The authors chose simple linguistic features over complex embeddings to reduce computational overhead and improve interpretability, trading some detection sensitivity for practical deployability and faster monitoring cycles.

**Failure Signatures**: False positives may occur from prompt drift, environmental factors, or temporary API issues; false negatives may result from insufficient sample sizes or subtle model changes that don't significantly affect feature distributions.

**First Experiments**: 1) Test detection capability on real LLM updates from major providers, 2) Evaluate feature stability across diverse prompt types and domains, 3) Compare proposed feature set against alternative feature families for optimization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on synthetic mixture scenarios and controlled prompt injection attacks, raising questions about real-world deployment effectiveness
- The feature set was not optimized or compared against alternative feature families, potentially missing more sensitive detection approaches
- The approach assumes API providers maintain consistent prompt-response behavior, which may not hold across different API endpoints or versions

## Confidence
- **High confidence** in the statistical methodology (Kolmogorov-Smirnov testing on feature distributions)
- **Medium confidence** in detection thresholds and practical deployment recommendations
- **Low confidence** in real-world effectiveness without external validation

## Next Checks
1. Test detection capability on real-world LLM updates from major providers (e.g., GPT-4â†’GPT-4o transitions) to validate against actual API changes
2. Evaluate detection robustness across diverse prompt types and domains to assess feature stability in production scenarios
3. Compare the proposed feature set against alternative feature families (e.g., embedding-based, syntactic, or semantic features) to optimize detection sensitivity while minimizing false positives