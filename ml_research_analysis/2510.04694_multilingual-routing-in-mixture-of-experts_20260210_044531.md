---
ver: rpa2
title: Multilingual Routing in Mixture-of-Experts
arxiv_id: '2510.04694'
source_url: https://arxiv.org/abs/2510.04694
tags:
- experts
- layers
- routing
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates how multilingual tokens are routed in Mixture-of-Experts
  (MoE) large language models. Through a systematic analysis of routing divergence
  from English across multiple layers, languages, and models, the authors reveal a
  consistent U-shaped pattern: higher divergence in early and late layers, with lower
  divergence (i.e., more cross-lingual expert sharing) in middle layers.'
---

# Multilingual Routing in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2510.04694
- Source URL: https://arxiv.org/abs/2510.04694
- Reference count: 30
- Primary result: Steering interventions yield 1-2% performance gains across 15+ languages in three state-of-the-art MoE models

## Executive Summary
This work systematically investigates how multilingual tokens are routed through Mixture-of-Experts (MoE) large language models, revealing that routing divergence from English follows a consistent U-shaped pattern across layers. Higher divergence occurs in early and late layers, while middle layers show lower divergence, indicating more cross-lingual expert sharing. This middle-layer expert sharing strongly correlates with language performance, suggesting it's crucial for multilingual generalization. The authors demonstrate that inference-time steering interventions—which promote activation of task-specific experts frequently used in English—can consistently improve multilingual task performance by 1-2% on two benchmarks across three MoE models without requiring retraining.

## Method Summary
The authors conducted a comprehensive analysis of routing patterns across multiple multilingual MoE models including Qwen2.5-MoE-72B-Instruct, LLaMA-3.2-8B-Instruct, and PHI-3.5-MOE-3B-Instruct. They measured routing divergence by computing the KL-divergence between the routing probability distributions of non-English languages and English across different layers. This analysis revealed a consistent U-shaped pattern of routing divergence. Based on these findings, they implemented inference-time steering interventions that adjust routing probabilities at test time to promote activation of task-specific experts frequently used in English, particularly in middle layers. The steering was applied multiplicatively to the routing logits with a hyperparameter γ that controls the steering intensity. They evaluated these interventions on two multilingual benchmarks: MGSM and Global-MMLU, covering 15+ languages.

## Key Results
- Routing divergence follows a consistent U-shaped pattern across all tested MoE models: higher in early and late layers, lower in middle layers
- Middle-layer expert sharing strongly correlates with language performance, suggesting its importance for multilingual generalization
- Inference-time steering interventions yield consistent 1-2% performance improvements across three state-of-the-art MoE models
- The steering interventions are model-agnostic and require no retraining, making them broadly applicable
- Expert specialization analysis reveals a modular separation between language-specific experts in middle layers and task-specific experts in other layers

## Why This Works (Mechanism)
The steering interventions work by exploiting the discovered pattern of expert specialization in MoE models. The analysis reveals that middle layers show lower routing divergence, indicating cross-lingual expert sharing, while other layers show higher divergence with language-specific expert activation. By steering routing probabilities toward task-specific experts that are frequently used in English, particularly in the middle layers where cross-lingual sharing is most prominent, the model can leverage the same reasoning pathways for multiple languages. This intervention effectively amplifies the natural modularity between language-specific and task-specific expert pools, improving multilingual generalization without requiring architectural changes or additional training.

## Foundational Learning

**KL-divergence**: A measure of how one probability distribution differs from another, used here to quantify routing divergence between languages. Needed to objectively measure how differently languages are routed through the MoE architecture.

**Routing mechanism in MoE**: The process by which tokens are assigned to different expert networks based on gating networks. Needed to understand how multilingual tokens traverse the model differently.

**Expert specialization**: The phenomenon where different experts in an MoE model develop distinct functional roles. Needed to explain why certain experts handle specific languages or tasks.

**Load balancing loss**: A regularization term in MoE training that ensures all experts receive roughly equal token assignment. Needed to understand how the training process shapes routing patterns.

**Multilingual generalization**: The ability of a model to perform well across multiple languages, often without direct training on all languages. Needed to frame the research question about cross-lingual transfer.

**Inference-time intervention**: Techniques that modify model behavior during inference without retraining. Needed to understand the practical applicability of the proposed method.

## Architecture Onboarding

**Component map**: Token input → Gating network → Expert selection (uniform MoE blocks) → Expert computation → Output aggregation. The steering intervention modifies the gating network's output at inference time.

**Critical path**: Gating network routing decisions → Expert computation → Final prediction. The steering intervention targets the gating network's routing decisions, particularly in middle layers.

**Design tradeoffs**: The uniform MoE architecture provides flexibility but may not be optimal for multilingual tasks. The steering intervention offers a lightweight alternative to specialized multilingual architectures.

**Failure signatures**: Poor performance in languages with high routing divergence, particularly in early and late layers. Models showing uniform routing patterns across all layers may indicate poor expert specialization.

**First experiment**: Measure routing divergence between English and other languages across all layers to identify the U-shaped pattern.

**Second experiment**: Analyze expert specialization by tracking which experts are activated for different languages and tasks.

**Third experiment**: Apply steering interventions with varying intensities to identify optimal steering parameters for different language families.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can cross-lingual routing alignment be effectively enforced during the pre-training or post-training phase?
- **Basis in paper:** The conclusion states the findings "motivate future work on methods that enhance cross-lingual routing alignment... such as during training," rather than just at inference time.
- **Why unresolved:** The current study only validates inference-time steering interventions on fully trained models.
- **What evidence would resolve it:** A training objective or regularization technique that inherently produces low routing divergence in middle layers without requiring test-time intervention.

### Open Question 2
- **Question:** How can MoE architectures be explicitly designed to exploit the modularity between language-specific and task-specific experts?
- **Basis in paper:** The authors conclude that the "distinct and modular separability... suggests opportunities for architectural or training approaches that exploit this natural division."
- **Why unresolved:** The separation is currently an emergent property of standard, uniform MoE blocks rather than a structural feature.
- **What evidence would resolve it:** A "Split-MoE" architecture with distinct expert pools for language processing versus reasoning that demonstrates improved efficiency or generalization.

### Open Question 3
- **Question:** What causes the anomalous first-layer routing pattern in the PHI-3.5-MOE model?
- **Basis in paper:** Section 4.3 notes that PHI-3.5-MOE "perplexingly activates the same few experts in the first layer for all languages... requires further investigation."
- **Why unresolved:** This behavior is an outlier compared to other models, and the authors could not determine if it stems from poor load-balancing or another factor.
- **What evidence would resolve it:** Ablation studies on the model's load-balancing loss coefficients or weight initialization to identify the causal mechanism.

## Limitations

- The steering interventions are applied at inference time without addressing potential trade-offs in model calibration or out-of-distribution generalization
- The analysis focuses on task-specific expert sharing patterns but does not fully explore whether these patterns emerge from training data distribution, model initialization, or optimization dynamics
- The 1-2% improvement range suggests modest gains that may not scale to more challenging tasks or languages
- Effectiveness may depend on specific routing mechanisms and expert specialization patterns that could vary between different MoE implementations

## Confidence

High confidence in the empirical findings regarding the U-shaped routing divergence pattern across layers, as this observation is consistently reproduced across multiple models and languages. Medium confidence in the correlation between middle-layer expert sharing and language performance, as the causal relationship requires further investigation. Medium confidence in the effectiveness of steering interventions, as the 1-2% improvement range suggests modest but consistent gains that may not scale to more challenging tasks or languages.

## Next Checks

1. Test the steering interventions across a broader range of MoE architectures with different routing mechanisms (e.g., hash-based, load-balancing, or confidence-based routing) to establish architecture-agnostic effectiveness.

2. Evaluate the long-term stability and potential degradation effects of repeated steering interventions across multiple inference batches to assess whether the performance gains persist over extended usage.

3. Conduct ablation studies varying the steering intensity and targeting different layer ranges to identify optimal intervention parameters for each language family and task type.