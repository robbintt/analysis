---
ver: rpa2
title: 'Not There Yet: Evaluating Vision Language Models in Simulating the Visual
  Perception of People with Low Vision'
arxiv_id: '2508.10972'
source_url: https://arxiv.org/abs/2508.10972
tags:
- tell
- vision
- what
- image
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether vision language models can simulate
  the visual perception of people with low vision. A benchmark dataset is collected
  from 40 low vision participants, including vision information and responses to open-ended
  and multiple-choice image perception tasks.
---

# Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision

## Quick Facts
- **arXiv ID**: 2508.10972
- **Source URL**: https://arxiv.org/abs/2508.10972
- **Reference count**: 40
- **Primary result**: Vision language models can simulate low vision perception with agreement scores up to 0.70 when given both vision information and a single combined example, but remain "not there yet" compared to human accuracy of 0.94.

## Executive Summary
This paper evaluates whether vision language models (VLMs) can simulate the visual perception of people with low vision. The authors created a benchmark dataset with 40 low vision participants who provided both vision information and responses to open-ended and multiple-choice image perception tasks. They tested VLMs with varying levels of prompting, from minimal vision information to combinations of detailed profiles and example responses. Results show that while VLMs struggle with minimal prompting (agreement of 0.59), providing both vision information and a single combined example significantly improves performance (agreement of 0.70, p < 0.0001). Additional examples provide minimal benefit. The study concludes that VLMs show promise but are currently insufficient for accurate low vision simulation.

## Method Summary
The researchers collected data from 40 low vision participants responding to 25 images across 5 content types and 5 usage scenarios. Each participant provided brief and detailed vision information along with responses to open-ended descriptions and six multiple-choice questions per image. The study used GPT-4o as the VLM, testing 16 distinct prompt configurations varying vision information (none, diagnosis, brief+detailed) and examples (none, single, multiple). The primary metric was agreement score, calculated as the percentage of MCQ responses that exactly matched the target human participant's responses. Temperature was set to 0 for all experiments.

## Key Results
- VLMs default to "sighted" responses with minimal prompting, achieving only 0.59 agreement with human responses
- Combining vision information with a single example of both open-ended and multiple-choice responses achieves 0.70 agreement (p < 0.0001)
- Additional examples beyond the first provide minimal performance gains (p > 0.05)
- Unprompted VLMs achieve 0.94 accuracy against ground truth, highlighting their tendency to "hallucinate" details when asked to simulate low vision

## Why This Works (Mechanism)

### Mechanism 1: The "Sighted Prior" Conflict
VLMs default to "ideal observer" descriptions unless explicitly constrained by behavioral examples, as they cannot reliably "unlearn" their training to simulate perception deficits based on text instructions alone. The model's latent space is dominated by training data where "describe the image" equates to "list all visible details accurately." Minimal prompts act as weak soft constraints that are overridden by strong visual features, leading models to "hallucinate" details a low-vision user wouldn't see.

### Mechanism 2: Semantic-Perceptual Anchoring
Combining semantic vision profiles with behavioral examples is statistically necessary to achieve significant agreement (>0.70). The vision profile provides the "rules" (e.g., "I have no central vision"), while the example demonstrates the "style" (e.g., "I describe things as 'vague' or 'possibly'"). The example grounds the abstract medical condition in a concrete input-output mapping, teaching the model how to fail or be uncertain.

### Mechanism 3: Rapid Context Convergence
Simulation performance saturates with a single combined example; additional examples provide negligible improvement. The "style" and "capability" of the persona are captured in the first effective demonstration. Because VLMs are already powerful reasoners, they require only one "calibration" point to align their output distribution to the user's specific error mode.

## Foundational Learning

- **Concept: Low Vision Heterogeneity**
  - *Why needed here:* Engineers often treat "accessibility" as a binary toggle (blind vs. sighted). This paper proves that simulation requires nuance because low vision is diverse.
  - *Quick check question:* If I prompt a model to "be blind," why is the agreement score low (0.35) compared to the participant?

- **Concept: Agreement vs. Accuracy**
  - *Why needed here:* Standard VLM evaluation optimizes for accuracy (getting the "right" answer). This system optimizes for *agreement* (getting the *same wrong* answer as the user).
  - *Quick check question:* If the image shows a red apple and the user says "green ball," should the VLM say "red apple" (High Accuracy) or "green ball" (High Agreement)?

- **Concept: Few-Shot Prompting (In-Context Learning)**
  - *Why needed here:* The solution relies on the VLM learning a specific behavior from a prompt without weight updates.
  - *Quick check question:* Why does adding a text description of a disability fail to change the model's output, while adding an example response succeeds?

## Architecture Onboarding

- **Component map:** User Profile Store -> Prompt Assembler -> Simulation Agent (VLM) -> Agreement Calculator
- **Critical path:** The Prompt Assembler is the bottleneck. It must correctly inject the user's specific uncertainty style. If it defaults to a generic "You are visually impaired" prompt, the system falls back to the 0.59 baseline.
- **Design tradeoffs:**
  - *Fidelity vs. Privacy:* Achieving 0.70 agreement requires PII-level detail (medical history + specific responses).
  - *Cost vs. Performance:* The paper finds "diminishing returns" on examples. Stick to 1-shot to minimize token costs.
- **Failure signatures:**
  - **The "Super-Perceiver":** The agent responds with highly detailed visual descriptions when the user profile indicates severe impairment.
  - **The "Generic Assistant":** The agent prefaces answers with "As an AI..." or refuses to roleplay the impairment.
- **First 3 experiments:**
  1. **Baseline Calibration:** Run the "Minimal Prompt" condition on your target VLM to confirm the "Sighted Prior" (accuracy ~0.94, agreement ~0.59).
  2. **Ablation Study:** Test "Vision Info Only" vs. "Example Only" vs. "Combined." Confirm that "Combined" is the only condition yielding >0.65 agreement.
  3. **Format Test:** Test "Open-Ended Example Only" vs. "MCQ Only" vs. "Combined Example." Verify that the combined format yields the statistically significant gain.

## Open Questions the Paper Calls Out

- **What defines an acceptable fidelity threshold for VLM simulations to be effectively used in accessibility design?**
  - *Basis in paper:* [explicit] The authors state future work should focus on "defining acceptable fidelity thresholds â€“ what level of agreement is 'good enough' for various design contexts."
  - *Why unresolved:* The study establishes a 0.70 agreement ceiling but does not validate if this level is sufficient for tasks like prototyping or accessibility auditing.
  - *What evidence would resolve it:* User studies with designers and low vision individuals validating the utility of simulations at different agreement levels for identifying barriers.

- **How do blind and low vision individuals perceive and consent to the simulation of their specific visual experiences?**
  - *Basis in paper:* [explicit] The authors highlight that "Understanding how BLV users perceive and consent to the simulation of their experiences is also important to ensure transparency."
  - *Why unresolved:* While the technical capability is measured, the ethical framework and user acceptance of creating digital twins of their perception are not explored.
  - *What evidence would resolve it:* Qualitative interviews assessing participant comfort, trust, and consent requirements regarding AI agents modeling their vision profiles.

- **Can advanced reasoning models improve simulation performance by better adhering to vision constraints than non-reasoning models?**
  - *Basis in paper:* [inferred] The authors note they used a "non-reasoning model" (GPT-4o) but suggest in Section 6.4 that "Advances in model capabilities are likely to improve consistency."
  - *Why unresolved:* It is unclear if reasoning capabilities specifically address the paper's finding that models tend to "infer beyond the specified vision ability."
  - *What evidence would resolve it:* A comparative benchmark of reasoning-based VLMs against the dataset to check for improved agreement scores.

## Limitations

- The agreement metric captures exact MCQ matching rather than nuanced perceptual similarity, potentially missing semantically equivalent responses.
- The heterogeneous nature of low vision conditions means simulation performance likely varies significantly across different impairment types, though the paper doesn't provide this breakdown.
- Privacy constraints that limited dataset access prevent independent validation of the exact vision profiles and example responses used.

## Confidence

- **High Confidence**: The core finding that minimal prompting yields poor agreement (0.59) is well-supported by baseline calibration and explicit observations about unprompted model accuracy.
- **Medium Confidence**: The claim that additional examples provide minimal benefit assumes the single example captures the user's perceptual style consistently across images.
- **Low Confidence**: The generalizability of the 0.70 agreement ceiling to VLMs beyond GPT-4o or to different image domains remains untested.

## Next Checks

1. **Cross-VLM Validation**: Test the same prompting strategy on Claude-3, Gemini-1.5, and LLaVA to determine if the 0.70 agreement ceiling is VLM-specific or a fundamental limitation.

2. **Per-Impairment Analysis**: Segment the 40 participants by impairment type and re-calculate agreement scores to identify whether certain conditions are easier or harder to simulate.

3. **Longitudinal Stability Test**: Run the same 10 images through the simulation pipeline twice with different random seeds and temperature variations to measure consistency of agreement scores.