---
ver: rpa2
title: 'JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry
  in Model-Based Reinforcement Learning'
arxiv_id: '2505.19698'
source_url: https://arxiv.org/abs/2505.19698
tags:
- uni00000014
- uni00000015
- uni00000016
- uni00000013
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a significant performance asymmetry in pixel-based
  MBRL agents across the Atari100k benchmark, where agents dramatically outperform
  humans on tasks humans perform poorly (agent-optimal tasks) but underperform on
  tasks where humans excel (human-optimal tasks). The authors hypothesize this asymmetry
  arises from the lack of temporally-structured latent space in pixel-based methods,
  which forces agents to learn representations solely through temporal-difference
  updates without the self-consistency objective present in latent-based approaches.
---

# JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.19698
- Source URL: https://arxiv.org/abs/2505.19698
- Authors: Jing Yu Lim; Zarif Ikram; Samson Yu; Haozhe Ma; Tze-Yun Leong; Dianbo Liu
- Reference count: 40
- Primary result: Latent diffusion world model achieves state-of-the-art on human-optimal Atari tasks while remaining competitive on agent-optimal tasks, with 3× faster inference and 43% lower memory than pixel-based diffusion baseline

## Executive Summary
This paper addresses a significant performance asymmetry in pixel-based model-based reinforcement learning (MBRL) agents on the Atari100k benchmark, where agents dramatically outperform humans on tasks humans perform poorly but underperform on tasks where humans excel. The authors hypothesize this asymmetry arises from the lack of temporally-structured latent representations in pixel-based methods, which rely solely on temporal-difference updates without the self-consistency objective present in latent-based approaches. They propose JEDI (Joint Embedding Diffusion), a novel latent diffusion world model trained end-to-end with a self-consistency objective inspired by Joint Embedding Predictive Architecture (JEPA). JEDI predicts compressed latent representations directly through diffusion denoising without reconstruction or hidden state extraction.

## Method Summary
JEDI is a latent diffusion world model that learns temporally-structured representations through end-to-end training with a self-consistency objective. The encoder maps 64×64×3 image observations to compressed latents z ∈ [-3,3]^16×8×8, which are then processed by a diffusion model to predict next-state latents. The training objective uses stop-gradient on targets and randomly switches between encoder and diffusion outputs for subsequent timesteps to balance temporal structure propagation. The method is trained with REINFORCE policy updates using an LSTM actor-critic, with 3-step Euler denoising at inference for efficiency.

## Key Results
- Achieves state-of-the-art performance on human-optimal Atari tasks, outperforming baselines by over 2×
- Maintains competitive performance on agent-optimal tasks while improving overall balance
- Provides 3× faster inference and 43% lower memory usage compared to pixel-based diffusion baseline
- Mitigates performance asymmetry between human-optimal and agent-optimal task categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end latent diffusion with self-consistency objective enables temporally-structured representations that support action-temporal reasoning.
- Mechanism: The encoder learns through both dynamics and reward/termination losses, with gradients flowing back through current latents but not through target latents (stop-gradient), forcing predictive representations without collapse.
- Core assumption: Temporal structure emerges when representations must support prediction of future states conditioned on actions.
- Evidence anchors: Abstract mentions novel latent diffusion world model with self-consistency objective; section 4 details stop-gradient training; no direct corpus evidence found.

### Mechanism 2
- Claim: Latent space compression (12×) enables efficient diffusion while maintaining sufficient information for policy learning.
- Mechanism: 64×64×3 observations compressed to 16×8×8 latents with tanh clamp provides stability during iterative denoising that pixel-space lacks naturally.
- Core assumption: Compressed latents preserve task-relevant features better than pixels when trained with prediction objectives.
- Evidence anchors: Abstract states 3× faster inference and 43% lower memory; section 4 confirms 12× compression as sole reason for efficiency gains; weak corpus connections found.

### Mechanism 3
- Claim: Random switching between encoder output and diffusion output for subsequent timesteps captures near-horizon temporal information.
- Mechanism: With 0.5 probability, denoised predictions are used as input for next timestep's loss instead of fresh encoder output, balancing teaching encoder from observations vs. propagating temporal structure.
- Core assumption: Gradients from future timesteps should influence current encoder representations.
- Evidence anchors: Section 4 describes random switching mechanism; section 5.4 ablation shows this outperforms reconstruction alternatives; no corpus evidence found.

## Foundational Learning

- Concept: **Joint Embedding Predictive Architecture (JEPA)**
  - Why needed here: JEDI's training objective derives from JEPA principles—learning representations by predicting target embeddings in the same latent space, avoiding reconstruction.
  - Quick check question: Can you explain why stop-gradient on targets prevents representation collapse while still training the predictor?

- Concept: **Diffusion denoising with preconditioning (EDM framework)**
  - Why needed here: JEDI uses Karras et al.'s preconditioning with c_skip, c_out, c_in scalars and logit-normal noise sampling for stable training.
  - Quick check question: Why does sampling τ from logit-normal (favoring middle noise levels) help diffusion training?

- Concept: **Temporal Difference (TD) learning vs. World Model objectives**
  - Why needed here: The paper's core hypothesis is that pixel-based agents rely solely on TD updates, lacking the self-consistency objective that latent world models provide.
  - Quick check question: How does a world model's next-state prediction objective differ from TD(0) in terms of what gradients train the encoder?

## Architecture Onboarding

- Component map: Environment → Encoder → Latent → Diffusion → Reward/Termination → Latent trajectories → LSTM Actor-Critic → Actions
- Critical path: Encoder → Latent Diffusion → Actor-Critic imagination rollout → REINFORCE update. The encoder must produce stable latents for all downstream components.
- Design tradeoffs:
  - Latent size 16×8×8: Chosen for 12× compression; smaller loses task info, larger reduces efficiency gains
  - 3-step Euler denoising at inference: Fewer steps reduce quality; more steps slow sampling
  - REINFORCE vs. backprop-through-denoising: REINFORCE avoids expensive gradients through multi-step denoising but may have higher variance
  - No reconstruction decoder: Saves computation but limits interpretability of latents
- Failure signatures:
  - Representation collapse: All latents converge to similar values. Check: encoder output variance across diverse inputs
  - Denoising instability: Latents explode during inference. Check: tanh clamp magnitude during rollout
  - Reward hacking without temporal reasoning: Agent finds exploits but behaves erratically. Check: action consistency across trajectories
- First 3 experiments:
  1. Ablate clamp + random switching: Train JEDI without tanh clamp and random switching on 7 agent-optimal games; expect degraded performance per Fig 8
  2. Latent dimension sweep: Test 8×4×4, 16×8×8, 32×16×16 on 3 human-optimal games; measure HNS, memory, and inference speed to verify 16×8×8 is Pareto-optimal
  3. Compare stop-gradient variants: Test (a) stop-gradient on target only, (b) no stop-gradient, (c) stop-gradient on both; expect (a) to outperform due to encoder learning from prediction pressure without target collapse

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance asymmetry between human-optimal and agent-optimal tasks be fully eliminated, or is an order-of-magnitude gap fundamentally inherent to current MBRL approaches?
- Basis in paper: The authors note in Section 6 that "the performance is still around one order of magnitude smaller than the performance on the Agent-Optimal tasks, indicating a wide gap in performance in these two tasks" even after applying JEDI.
- Why unresolved: JEDI only mitigates asymmetry but does not eliminate it; the underlying causes of the remaining gap remain unclear.
- What evidence would resolve it: Achieving comparable human-normalized scores across both task categories on Atari100k, or identifying theoretical bounds on achievable symmetry.

### Open Question 2
- Question: Will the JEDI approach generalize to larger-scale benchmarks beyond Atari100k, such as the full Atari benchmark with 57 games?
- Basis in paper: The authors state in Section 6: "limited to the small environment steps regime and small set of tasks in Atari100k, as opposed to the larger set of tasks in Atari Benchmark."
- Why unresolved: JEDI was only evaluated on 26 Atari100k tasks; scaling behavior remains untested.
- What evidence would resolve it: Demonstrating comparable or improved asymmetry mitigation on the full Atari57 benchmark with similar sample efficiency.

### Open Question 3
- Question: Can the multi-step denoising inference bottleneck in JEDI be eliminated to match the speed of non-diffusion latent MBRL baselines?
- Basis in paper: Section 6 states: "we are slower than existing latent MBRL baselines due to the nature of the multi-step denoising inference with every next state sampled."
- Why unresolved: The diffusion process inherently requires iterative denoising; it is unclear if one-step latent prediction can match expressiveness.
- What evidence would resolve it: Developing a single-step variant that maintains JEDI's temporal reasoning capabilities while matching DreamerV3's inference speed.

## Limitations
- Performance asymmetry mitigation is incomplete—significant gap remains between human-optimal and agent-optimal task performance
- Evaluation limited to Atari100k benchmark, with unclear generalization to larger-scale or different domains
- Multi-step denoising inference remains slower than non-diffusion latent MBRL baselines despite efficiency improvements

## Confidence
- **High confidence**: JEDI achieves superior performance on human-optimal tasks while maintaining competitiveness on agent-optimal tasks; architectural claims about faster inference and lower memory are well-supported
- **Medium confidence**: The mechanism by which temporal structure in latent space improves performance on human-optimal tasks; paper provides plausible hypothesis but lacks direct causal evidence
- **Low confidence**: Generalization claims beyond Atari to other RL domains; method's effectiveness in non-Atari environments remains unknown

## Next Checks
1. **Temporal structure ablation**: Train JEDI variants that systematically vary temporal dependencies and measure performance changes specifically on human-optimal vs agent-optimal tasks to isolate the temporal hypothesis
2. **Cross-domain validation**: Evaluate JEDI on non-Atari MBRL benchmarks (e.g., DeepMind Control Suite, Procgen) to test generalizability claims and verify asymmetry mitigation holds across domains
3. **Latent space analysis**: Conduct qualitative and quantitative analysis of learned latent representations to validate that JEDI produces more temporally-structured representations than pixel-based alternatives