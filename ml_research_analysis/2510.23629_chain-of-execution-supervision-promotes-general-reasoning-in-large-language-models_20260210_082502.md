---
ver: rpa2
title: Chain of Execution Supervision Promotes General Reasoning in Large Language
  Models
arxiv_id: '2510.23629'
source_url: https://arxiv.org/abs/2510.23629
tags:
- code
- reasoning
- tracepile
- should
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TracePile, a large-scale corpus of 2.6 million
  samples that transforms code execution into explicit, step-by-step chain-of-thought-style
  rationales called Chain of Execution (CoE). The corpus covers mathematics, algorithms,
  and algorithmic competition, and is enriched with variable-tracing questions and
  code rewritings to enhance logical granularity and code diversity.
---

# Chain of Execution Supervision Promotes General Reasoning in Large Language Models
## Quick Facts
- **arXiv ID:** 2510.23629
- **Source URL:** https://arxiv.org/abs/2510.23629
- **Reference count:** 40
- **Primary result:** Introduces TracePile, a 2.6M-sample corpus that transforms code execution into Chain of Execution (CoE) rationales, boosting reasoning performance across math and coding benchmarks.

## Executive Summary
This paper introduces TracePile, a large-scale synthetic corpus of 2.6 million samples that transforms code execution into step-by-step Chain of Execution (CoE) rationales, designed to enhance reasoning in large language models. By covering mathematics, algorithms, and algorithmic competition, and enriching with variable-tracing and code rewriting, TracePile consistently improves model reasoning, notably increasing LLaMA3.1-8B's average math performance by 7.1%. The approach demonstrates gains across math, coding, and general reasoning benchmarks under two-stage fine-tuning.

## Method Summary
TracePile is a 2.6 million-sample synthetic corpus that converts code execution into explicit, step-by-step Chain of Execution (CoE) rationales, enabling large language models to learn reasoning through supervised execution traces. The corpus spans mathematics, algorithms, and algorithmic competition, and is enriched with variable-tracing questions and code rewriting to improve logical granularity and code diversity. The approach is evaluated across three training setups (CoE, standard CoT, and no CoT) and four base models, consistently improving reasoning performance on math, coding, and general reasoning benchmarks.

## Key Results
- TracePile boosts LLaMA3.1-8B by 7.1% on average across nine math datasets.
- Improvements are observed on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.
- Consistent gains across three training setups (CoE, standard CoT, no CoT) and four base models.

## Why This Works (Mechanism)
TracePile leverages the structured, traceable nature of code execution to provide explicit, step-by-step rationales (Chain of Execution, CoE) for model reasoning. By transforming execution traces into natural language rationales, the corpus enables models to learn logical progression and intermediate reasoning steps, similar to Chain of Thought but grounded in actual code behavior. The enrichment with variable-tracing and code rewriting further enhances logical granularity and diversity, allowing models to generalize better across reasoning tasks.

## Foundational Learning
- **Code Execution Traces**: Why needed: Provide verifiable, step-by-step intermediate states for training. Quick check: Can the model reproduce execution steps for unseen code.
- **Chain of Execution (CoE)**: Why needed: Transforms execution traces into natural language rationales for reasoning. Quick check: Does the model generate correct intermediate reasoning steps.
- **Variable-Tracing**: Why needed: Enhances logical granularity by focusing on variable states. Quick check: Are variable states accurately tracked in rationales.
- **Code Rewriting**: Why needed: Increases code diversity for better generalization. Quick check: Does the model handle rewritten code with similar performance.
- **Two-Stage Fine-Tuning**: Why needed: Stabilizes training and improves reasoning performance. Quick check: Does fine-tuning order affect final performance.

## Architecture Onboarding
- **Component Map**: TracePile Corpus -> CoE Rationale Generation -> Variable-Tracing/ Rewriting Enrichment -> Model Fine-Tuning
- **Critical Path**: Synthetic corpus creation (execution trace to CoE) → enrichment (variable-tracing, code rewriting) → model training (two-stage fine-tuning)
- **Design Tradeoffs**: Large-scale synthetic corpus enables broad coverage but may introduce domain-specific bias; enrichment techniques improve granularity but require careful balance to avoid overfitting.
- **Failure Signatures**: Poor performance on out-of-distribution prompts; overfitting to algorithmic competition patterns; degradation on non-math reasoning tasks.
- **First Experiments**:
  1. Evaluate model performance on math benchmarks with and without CoE supervision.
  2. Test variable-tracing enrichment by comparing reasoning accuracy on variable-focused questions.
  3. Assess code rewriting impact by training on original vs. rewritten code samples.

## Open Questions the Paper Calls Out
None.

## Limitations
- Evaluation focuses mainly on math datasets, with limited coverage of other reasoning domains or real-world applications.
- Corpus construction relies heavily on algorithmic competition data, potentially introducing domain-specific biases.
- Relative contributions of variable-tracing and code rewriting are not isolated in ablation studies.
- Two-stage fine-tuning setup does not explore potential overfitting or generalization limits with larger or more diverse datasets.

## Confidence
- **High**: TracePile construction and its positive impact on math reasoning benchmarks are well-supported by quantitative results.
- **Medium**: Claims about the general applicability of CoE supervision across reasoning tasks are plausible but not exhaustively validated.
- **Medium**: The enrichment techniques (variable-tracing, code rewriting) are shown to help, but their individual contributions and scalability are not fully isolated.

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of variable-tracing and code rewriting to performance gains.
2. Evaluate TracePile-trained models on broader reasoning domains (e.g., scientific reasoning, commonsense QA) to assess generalization beyond math.
3. Test for overfitting or performance saturation when scaling TracePile to much larger or more diverse datasets, and assess robustness to out-of-distribution prompts.