---
ver: rpa2
title: Emotion-Aware Speech Generation with Character-Specific Voices for Comics
arxiv_id: '2509.15253'
source_url: https://arxiv.org/abs/2509.15253
tags:
- emotion
- character
- speaker
- recognition
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an end-to-end pipeline for generating character-specific,
  emotion-aware speech from comics. The system integrates scene graph generation,
  character identification, emotion intensity estimation, LLM-based speaker and emotion
  prediction, and TTS synthesis.
---

# Emotion-Aware Speech Generation with Character-Specific Voices for Comics

## Quick Facts
- arXiv ID: 2509.15253
- Source URL: https://arxiv.org/abs/2509.15253
- Reference count: 0
- Generates character-specific, emotion-aware speech from comics using end-to-end pipeline

## Executive Summary
This paper presents an end-to-end pipeline for generating character-specific, emotion-aware speech from comics. The system integrates scene graph generation, character identification, emotion intensity estimation, LLM-based speaker and emotion prediction, and TTS synthesis. The LLM leverages both visual and textual information to infer dialogue attribution and emotional tone. The approach is evaluated on Manga109 and KangaiSet datasets, achieving 79.2% speaker prediction accuracy and 42.9% emotion classification accuracy.

## Method Summary
The paper introduces a comprehensive pipeline that processes comics through multiple stages to generate speech. The system first generates scene graphs and identifies characters, then estimates emotion intensity and uses an LLM to predict both the speaker and emotional tone of each dialogue. Finally, a text-to-speech system synthesizes the speech with appropriate character voices and emotional expression. The LLM component is particularly novel as it integrates both visual information from the comic panels and textual information from the dialogue to make predictions about speaker attribution and emotional content.

## Key Results
- 79.2% accuracy for speaker prediction on Manga109 and KangaiSet datasets
- 42.9% accuracy for emotion classification on the same datasets
- 20.4% joint accuracy for both speaker and emotion prediction

## Why This Works (Mechanism)
The system works by creating a multimodal pipeline that combines visual scene understanding with language modeling and speech synthesis. The scene graph generation provides structured representation of the visual content, character identification links faces to specific characters, and emotion intensity estimation provides emotional context. The LLM then uses this combined visual-textual information to make informed predictions about who is speaking and with what emotional tone. This integrated approach allows the system to handle the complex task of generating contextually appropriate speech from static comic images.

## Foundational Learning
- Scene graph generation: Why needed? To extract structured visual relationships from comic panels. Quick check: Verify scene graphs capture character positions and interactions.
- Character identification: Why needed? To associate faces with specific character identities. Quick check: Test accuracy on character recognition across different art styles.
- Emotion intensity estimation: Why needed? To quantify emotional content for speech synthesis. Quick check: Validate emotion intensity correlates with human judgments.
- LLM inference: Why needed? To combine visual and textual information for speaker/emotion prediction. Quick check: Compare LLM predictions against human annotations.
- TTS synthesis: Why needed? To convert predicted text and emotion into natural speech. Quick check: Evaluate speech naturalness and emotional expressiveness.

## Architecture Onboarding

Component map: Comic panels -> Scene Graph Generation -> Character Identification -> Emotion Intensity Estimation -> LLM Inference -> TTS Synthesis -> Generated Speech

Critical path: Visual processing (scene graph + character ID) → LLM inference (speaker/emotion prediction) → TTS synthesis (speech generation)

Design tradeoffs: The system prioritizes integration of multiple pretrained models over end-to-end training, which allows faster development but may limit optimization across components. The choice of LLM for multimodal inference provides flexibility but introduces complexity in prompt engineering.

Failure signatures: Low speaker prediction accuracy when characters have similar appearances, poor emotion classification when visual cues are ambiguous, unnatural speech when TTS model cannot capture emotional nuances.

3 first experiments:
1. Evaluate each component's accuracy individually on a validation set
2. Test LLM performance with different prompt templates and input modalities
3. Conduct ablation studies removing scene graph generation or emotion estimation

## Open Questions the Paper Calls Out
None

## Limitations
- Pre-trained models may not generalize well to comics with different artistic styles
- 20.4% joint accuracy indicates LLM inference remains a bottleneck
- Evaluation metrics don't capture perceptual quality of generated speech

## Confidence

**High confidence**: Technical feasibility of end-to-end pipeline integration, reported accuracy metrics on Manga109 and KangaiSet datasets

**Medium confidence**: Effectiveness of LLM-based speaker and emotion prediction approach, generalizability across different comic styles

**Low confidence**: Perceptual quality of generated speech and its ability to convey character-specific voices and emotional nuances

## Next Checks

1. Conduct perceptual evaluation studies with human readers to assess naturalness, emotional expressiveness, and character distinctiveness of generated speech
2. Test system on diverse comic datasets with varying artistic styles, panel layouts, and dialogue complexity to evaluate cross-domain robustness
3. Implement ablation studies to quantify contribution of each component (scene graph, character identification, emotion estimation, LLM inference) to overall system performance