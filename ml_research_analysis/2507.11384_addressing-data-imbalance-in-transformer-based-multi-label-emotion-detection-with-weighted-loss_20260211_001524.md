---
ver: rpa2
title: Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection
  with Weighted Loss
arxiv_id: '2507.11384'
source_url: https://arxiv.org/abs/2507.11384
tags:
- emotion
- multi-label
- loss
- classes
- bart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies a weighted loss function to Transformer-based
  models for multi-label emotion detection in SemEval-2025 Shared Task 11. The approach
  addresses data imbalance by dynamically adjusting class weights to enhance performance
  on minority emotion classes without the computational burden of resampling methods.
---

# Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss

## Quick Facts
- arXiv ID: 2507.11384
- Source URL: https://arxiv.org/abs/2507.11384
- Authors: Xia Cui
- Reference count: 11
- Primary result: Weighted loss improves performance on high-frequency emotion classes but shows limited impact on minority classes

## Executive Summary
This paper addresses class imbalance in multi-label emotion detection by applying weighted loss functions to Transformer-based models for the SemEval-2025 Shared Task 11. The approach dynamically adjusts class weights during training to enhance performance on minority emotion classes without the computational burden of traditional resampling methods. BERT, RoBERTa, and BART models were evaluated on the BRIGHTER dataset, with weighted loss improving performance on high-frequency emotion classes but showing limited impact on minority classes. BERT and BART demonstrated significant gains with weighted loss, while RoBERTa's performance was less affected, suggesting inherent robustness to class imbalance.

## Method Summary
The method applies weighted Binary Cross-Entropy (BCE) loss to multi-label emotion detection. Class weights are computed as normalized inverse frequencies (wj = max(W)/wj where wj = fj/N), applied per-class within the BCE formulation. The models (BERT-base, RoBERTa-base, BART-base) output C logits (one per emotion), passed through element-wise sigmoid to produce independent probabilities. A threshold (τ=0.5) determines final assignments. The approach is compared against baseline BCE loss on the BRIGHTER dataset using Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.

## Key Results
- Weighted loss improved performance on high-frequency emotion classes across all models
- BERT and BART showed significant gains with weighted loss, while RoBERTa's performance was less affected
- Limited impact on minority emotion classes despite weighting adjustments
- RoBERTa demonstrated notable resilience to data imbalance even without reweighting strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inverse-frequency class weights shift gradient updates to increase model sensitivity on under-represented emotion classes
- **Mechanism:** Class weights computed as normalized inverse frequencies (wj = max(W)/wj where wj = fj/N) increase loss contribution for minority classes during backpropagation
- **Core assumption:** Inverse-frequency weighting appropriately compensates for gradient dominance by majority classes
- **Evidence anchors:**
  - [abstract] "Our approach addresses data imbalance by dynamically adjusting class weights, thereby enhancing performance on minority emotion classes"
  - [section 3.2.2] Defines weighted loss formula and normalization of weights
  - [corpus] Limited direct validation; related work supports importance-weighted loss for imbalance but in different context

### Mechanism 2
- **Claim:** BCE loss with independent sigmoid outputs enables multi-label prediction but presumes label independence
- **Mechanism:** Model outputs C logits (one per emotion), passed through element-wise sigmoid to produce independent probabilities
- **Core assumption:** Emotions are conditionally independent given the input
- **Evidence anchors:**
  - [section 3.2.1] "Under the assumption that emotions are independent, the BCE loss is calculated separately for each label"
  - [section 2] Prior work models label dependencies but is computationally expensive
  - [corpus] Related work addresses multi-label learning but doesn't validate independence assumptions

### Mechanism 3
- **Claim:** RoBERTa's robust pretraining methodology reduces sensitivity to class imbalance
- **Mechanism:** Enhanced pretraining (larger corpus, longer training, dynamic masking, no NSP) provides stronger foundation for fine-tuning on imbalanced data
- **Core assumption:** Pretraining differences cause RoBERTa's imbalance robustness
- **Evidence anchors:**
  - [table 1] RoBERTa+w shows minimal improvement over baseline
  - [appendix B] "RoBERTa demonstrates notable resilience to data imbalance even without reweighting strategies"
  - [corpus] No external validation of this specific claim

## Foundational Learning

- **Concept:** Multi-label classification with Binary Relevance
  - **Why needed here:** Task requires assigning multiple emotion labels per instance
  - **Quick check question:** Given an instance with labels {joy, surprise}, would a binary relevance approach optimize both labels independently or jointly?

- **Concept:** Class-weighted loss functions
  - **Why needed here:** Core intervention is weighting BCE loss
  - **Quick check question:** If class A appears 10x more frequently than class B, what weight ratio would equalize their total contribution to the loss?

- **Concept:** Transformer pretraining variations (BERT vs. RoBERTa vs. BART)
  - **Why needed here:** Comparing three architectures with different pretraining objectives
  - **Quick check question:** Which pretraining difference between BERT and RoBERTa is most likely to affect fine-tuning robustness on small, imbalanced datasets?

## Architecture Onboarding

- **Component map:**
  - Input text -> AutoTokenizer (WordPiece for BERT, BPE for RoBERTa/BART) -> Transformer encoder (BERT-base, RoBERTa-base, or BART-base) -> Linear classification head -> C logits -> Sigmoid activation -> Weighted BCE loss -> Optimization

- **Critical path:**
  1. Tokenize input via model-specific tokenizer
  2. Forward pass through Transformer backbone to obtain pooled output
  3. Project to C logits via classification head
  4. Compute weighted BCE loss using precomputed class weights
  5. Backpropagate and update; monitor Macro F1 on dev set for early stopping

- **Design tradeoffs:**
  - Weighted loss vs. resampling: Avoids distorting multi-label co-occurrence patterns
  - Label independence assumption: Simplifies implementation but ignores emotion correlations
  - Base vs. large models: Computational constraints forced use of base models

- **Failure signatures:**
  - Weighted loss improves Macro F1 but not per-class F1 on minority emotions
  - High-frequency classes improve while minority class precision/recall stagnates
  - RoBERTa shows no improvement or slight degradation with weighted loss

- **First 3 experiments:**
  1. Reproduce baseline + weighted loss comparison on BRIGHTER English Track A with reported hyperparameters
  2. Ablate weight scaling strategy: inverse frequency vs. square-root inverse frequency vs. effective number of samples
  3. Implement per-class threshold tuning instead of fixed τ=0.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the weighted loss function generalize to the other 27 languages in the BRIGHTER dataset beyond English?
- Basis in paper: [explicit] Conclusion states future work could explore impact across different languages and datasets
- Why unresolved: Authors restricted experiments to English subset as representative example
- What evidence would resolve it: Evaluation metrics for non-English tracks of BRIGHTER dataset

### Open Question 2
- Question: How can the weighted loss formulation be adjusted to improve detection of minority classes?
- Basis in paper: [inferred] Section 5 notes improvements were "primarily driven by gains in high-frequency classes"
- Why unresolved: Current frequency-based weighting failed to translate into better recall for minority classes
- What evidence would resolve it: Comparative study implementing alternative re-balancing strategies

### Open Question 3
- Question: Why is RoBERTa's performance slightly degraded by weighted loss while BERT and BART improve?
- Basis in paper: [inferred] Section 5 reports RoBERTa+w led to "slight underperformance in Micro F1"
- Why unresolved: Authors hypothesize this is due to pretraining methodology without empirical validation
- What evidence would resolve it: Ablation study analyzing gradient updates and decision boundaries

## Limitations
- Weighted loss mechanism shows limited effectiveness on truly rare emotion classes
- Attribution of RoBERTa's robustness to pretraining methodology is observational, not experimentally validated
- Small test set (116 instances) may limit statistical power for per-class comparisons

## Confidence

- **High confidence:** Weighted loss mechanism and general impact on high-frequency emotion classes
- **Medium confidence:** Attribution of RoBERTa's robustness to pretraining methodology
- **Low confidence:** Long-term generalization to different emotion datasets, languages, or emotion taxonomies

## Next Checks

1. **Controlled pretraining ablation:** Train RoBERTa with and without NSP objective on same pretraining corpus, then fine-tune on imbalanced emotion data

2. **Minority-class specific intervention:** Implement effective number of samples weighting and evaluate whether this scaling strategy provides better gradient signal for extremely rare emotion classes

3. **Label correlation modeling:** Replace independent BCE with learned label correlation matrix to capture emotion co-occurrence patterns and compare performance against independence baseline