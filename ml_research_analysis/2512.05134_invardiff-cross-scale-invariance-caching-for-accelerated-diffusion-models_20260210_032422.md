---
ver: rpa2
title: 'InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models'
arxiv_id: '2512.05134'
source_url: https://arxiv.org/abs/2512.05134
tags:
- step
- reuse
- flux
- diffusion
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InvarDiff accelerates diffusion model inference by exploiting temporal
  invariance in deterministic sampling. It computes a per-timestep, per-layer, per-module
  binary cache plan matrix from a small calibration set, enabling both step-level
  and layer-wise reuse.
---

# InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models

## Quick Facts
- arXiv ID: 2512.05134
- Source URL: https://arxiv.org/abs/2512.05134
- Reference count: 40
- Primary result: 2–3× end-to-end speedup on DiT and FLUX with minimal perceptual quality loss

## Executive Summary
InvarDiff accelerates diffusion model inference by exploiting temporal invariance in deterministic sampling. The method computes a per-timestep, per-layer, per-module binary cache plan matrix from a small calibration set, enabling both step-level and layer-wise reuse. A resampling correction prevents drift from consecutive reuse. Applied to DiT and FLUX, InvarDiff achieves 2–3× end-to-end speedups with minimal impact on perceptual quality metrics while being training-free and generalizable across prompts.

## Method Summary
InvarDiff is a training-free method that accelerates deterministic diffusion model inference by exploiting temporal feature invariance. It computes rate metrics ρ between consecutive timesteps for each module, applies quantile thresholds to create a binary cache plan, and uses a resampling correction to prevent drift from chained reuse. The method combines step-level and layer-level caching, operating on DiT-family models using ODE-based samplers like DDIM and DPM-Solver.

## Key Results
- Achieves 2–3× end-to-end speedup on DiT-XL/2 and FLUX.1-dev
- Minimal impact on perceptual quality metrics (LPIPS, SSIM, PSNR)
- Training-free approach generalizes across classes and prompts
- Works with various deterministic samplers (DDIM, DPM-Solver, flow matching)

## Why This Works (Mechanism)

### Mechanism 1: Temporal Feature Invariance in Deterministic Sampling
The method exploits the observation that adjacent timesteps in deterministic diffusion sampling exhibit highly correlated internal activations. By measuring change rates between consecutive steps using ρ = ||Z_{t+1} - Z_t||_1 / ||Z_t - Z_{t-1}||_1 for each layer/module, modules with low change rates can be safely cached and reused. This assumption holds for deterministic samplers like DDIM η=0 but breaks for stochastic samplers like DDPM.

### Mechanism 2: Complementary Step-Level and Layer-Level Caching
Step-level caching allows entire forward passes to be reused when temporal invariance is strong, while layer-level caching selectively reuses individual module outputs within active steps. The combination captures different redundancy patterns - step gates capture long constant intervals while layer gates recover residual redundancy within active steps. This cross-scale approach yields greater speedups than either scale alone.

### Mechanism 3: Resampling Correction for Consecutive Reuse Drift
A second calibration pass simulates chained reuse by substituting cached values during rate computation, revealing how approximation errors accumulate across consecutive cached steps. This correction identifies modules that appear safe to cache in isolation but cause drift when reused repeatedly, refining the cache plan to exclude false positives.

## Foundational Learning

- **Diffusion Transformers (DiT) and deterministic sampling**: Understanding patch tokenization, MHSA/FFN blocks, and why deterministic samplers enable feature tracking is prerequisite. Quick check: Can you explain why DDIM with η=0 allows feature caching but DDPM does not?

- **Quantile-based thresholding for cache planning**: The cache plan derives from pooling rate statistics and applying quantile thresholds. Understanding how quantiles translate to binary reuse decisions is central. Quick check: Given a distribution of ρ values, what does selecting the 0.7 quantile threshold mean for caching aggressiveness?

- **Error accumulation in approximate computing**: Resampling correction addresses chained reuse drift. Understanding how small approximation errors compound across steps informs why Phase 2 is necessary. Quick check: Why might a module that appears safe to cache in isolation cause quality degradation when cached across 5 consecutive steps?

## Architecture Onboarding

- **Component map**: Rate computation module -> Cache plan generator -> Inference scheduler -> Cache storage
- **Critical path**: 1. Calibration: Run deterministic passes → compute rates → initial plan → resampling correction → final plan. 2. Inference: For each t, check c_step_t → skip or execute layer-wise with per-module reuse
- **Design tradeoffs**: Higher thresholds → more aggressive caching → faster but risk quality degradation. Step threshold controls overall aggressiveness; module thresholds tune residual redundancy. Attention modules tolerate higher thresholds; FFN thresholds affect texture quality.
- **Failure signatures**: Spatial layout distortion: MHSA threshold too high (>0.8). Texture degradation/speckles: FFN threshold too high. Patch-edge artifacts after upscaling: Single-stream FFN threshold too aggressive. Early-step blur: Insufficient warm-up (especially for FLUX).
- **First 3 experiments**: 1. Reproduce ablation on DiT-XL/2: Test step-only vs. layer-only vs. combined caching. 2. Threshold sensitivity sweep on FLUX: Vary τ_step ∈ {0.4, 0.5, 0.6, 0.7, 0.75}. 3. Cross-prompt generalization test: Calibrate on 5 prompts, evaluate on held-out GenEval prompts.

## Open Questions the Paper Calls Out
- Can the caching mechanism be effectively extended to stochastic samplers (SDEs) such as DDPM?
- Can the cache plan be made adaptive to varying timestep schedules and lengths without re-calibration?
- How does InvarDiff compose with aggressive step-reduction techniques or model distillation?
- Do the observed cross-scale invariance patterns hold for long-horizon video generation?

## Limitations
- The method is explicitly limited to deterministic samplers and excludes stochastic methods like DDPM
- Calibration overhead is significant, approximately doubling calibration time due to the resampling correction phase
- Tensor hooking points and exact threshold semantics are not fully specified in the paper

## Confidence
- **High Confidence**: Step-level caching mechanism and temporal invariance demonstration in deterministic sampling
- **Medium Confidence**: Cross-scale combination benefits and resampling correction effectiveness
- **Medium Confidence**: Generalizability across different DiT variants and prompt distributions

## Next Checks
1. **Cross-model generalization test**: Apply InvarDiff calibration from DiT-XL/2 to FLUX.1-dev and vice versa, measuring LPIPS stability and speedup retention
2. **Stochastic sampler edge case**: Run InvarDiff on modified DDIM with small η>0 to test deterministic assumption boundaries
3. **Calibration overhead accounting**: Measure total end-to-end time including resampling correction, comparing effective speedup against reported inference-only speedups