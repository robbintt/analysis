---
ver: rpa2
title: 'Advancing Intoxication Detection: A Smartwatch-Based Approach'
arxiv_id: '2510.09916'
source_url: https://arxiv.org/abs/2510.09916
tags:
- data
- alcohol
- accuracy
- intoxication
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a smartwatch-based approach for intoxication
  detection using machine learning. The researchers collected a three-week dataset
  of accelerometer, gyroscope, heart rate, and TAC data from 30 participants.
---

# Advancing Intoxication Detection: A Smartwatch-Based Approach

## Quick Facts
- arXiv ID: 2510.09916
- Source URL: https://arxiv.org/abs/2510.09916
- Reference count: 40
- Binary intoxication detection using smartwatch sensors with 76.1% accuracy

## Executive Summary
This study presents a smartwatch-based approach for intoxication detection using machine learning on sensor data from accelerometers, gyroscopes, and heart rate monitors. The researchers collected a three-week dataset from 30 participants and evaluated seven state-of-the-art classifiers including Transformer, bi-LSTM, GRU, 1D-CNN, SVM, LightGBM, and Hyperdimensional Computing (HDC). The 1D-CNN model achieved the best balance of accuracy (76.1%) and efficiency, with minimal computational requirements (0.035 MB model size) suitable for edge deployment. When deployed on a Samsung Galaxy S20, the 1D-CNN achieved inference times of 0.0121 seconds with 52.3 MB memory usage and 0.315 W power consumption.

## Method Summary
The study collected sensor data from Apple Watch Series 8 and BACtrack Skyn Bracelet from 30 participants over three weeks. Data was downsampled to 40 Hz after low-pass filtering (cutoff ~5 Hz) and segmented into 20-second windows. Labels were assigned as "Drunk" when Transdermal Alcohol Concentration exceeded 35µg/L. The researchers used subject-wise stratified group K-fold cross-validation on 14 subjects, evaluating seven classifiers including 1D-CNN, HDC, bi-LSTM, Transformer, SVM, LightGBM, and GRU. The 1D-CNN architecture consisted of three 1D-Conv layers (32 filters each) with kernel sizes 3, 5, and 7.

## Key Results
- 1D-CNN model achieved 76.1% accuracy with minimal computational requirements (0.035 MB model size)
- HDC model achieved highest F1 score of 0.665 but was 1000x larger (36.7 MB) than 1D-CNN
- On Samsung Galaxy S20: 1D-CNN achieved 0.0121s inference time, 52.3 MB memory usage, 0.315 W power consumption
- Several models exhibited complete class collapse (predicting only "Drunk" or only "Sober")

## Why This Works (Mechanism)

### Mechanism 1
Alcohol consumption induces detectable physiological and motor control deviations (gait instability, heart rate changes) that correlate with transdermal alcohol concentration (TAC). The system captures these deviations by fusing accelerometer, gyroscope, and heart rate data, assuming intoxication levels >35µg/L produce distinct signal signatures compared to sober baseline.

### Mechanism 2
1D-Convolutional Neural Networks efficiently extract local temporal features from sensor streams, offering superior balance of accuracy and computational cost for edge devices compared to recurrent architectures. The 1D-CNN applies sliding filters across time-series data to identify local patterns without sequential bottlenecks of RNNs or heavy attention mechanisms of Transformers.

### Mechanism 3
Hyperdimensional Computing enables robust classification on resource-constrained hardware by mapping sensor data into high-dimensional space where similar states cluster together. HDC encodes input features into large hypervectors (3,000 dimensions) and uses simple vector operations for classification, which are computationally cheaper than floating-point matrix multiplications in deep learning.

## Foundational Learning

- **Concept:** Transdermal Alcohol Concentration (TAC) vs. Blood Alcohol Concentration (BAC)
  - **Why needed here:** Ground truth is TAC (from wrist bracelet), not BAC (blood). TAC has lag time relative to BAC. Understanding this lag is critical for accurate data labeling and windowing.
  - **Quick check question:** If a user takes a drink at t=0, when do you expect the peak TAC signal relative to peak BAC, and how does this affect label alignment?

- **Concept:** Time-Series Windowing & Frequency Filtering
  - **Why needed here:** Paper downsamples to 40Hz and uses 20-second windows. Must understand aliasing and Nyquist frequencies to replicate preprocessing pipeline correctly.
  - **Quick check question:** Why did authors choose 40Hz sampling frequency after observing frequency peaks were below 5Hz?

- **Concept:** Edge Inference (Executorch & Quantization)
  - **Why needed here:** Paper benchmarks on Samsung Galaxy S20. Understanding Executorch is required to bridge gap between PyTorch model and mobile-ready binary.
  - **Quick check question:** Why did bi-LSTM and GRU models fail to export while 1D-CNN succeeded, and what does "unrolling the graph" mean in this context?

## Architecture Onboarding

- **Component map:** Smartwatch (Accelerometer, Gyroscope, HR) @ 50Hz -> Preprocessing (Low-pass filter, 40Hz downsample, 20s windows) -> Feature Extractor (1D-CNN layers or HDC Encoder) -> Classifier (Linear layer / Hamming distance) -> Output: Binary Intoxication State (>35µg/L)

- **Critical path:** Data Collection -> Windowing -> Inference (0.0121s on S20) -> JITAI Intervention

- **Design tradeoffs:** HDC vs. 1D-CNN: HDC offers slightly better F1 (0.665 vs 0.655) and "sober" recall but is 1000x larger (36.7 MB vs 0.035 MB). 1D-CNN recommended for strict memory constraints; HDC for accuracy priority. Bi-LSTM/Transformer disqualified for mobile deployment due to high latency or export incompatibility.

- **Failure signatures:** Class Collapse (Sober): SVM failed by classifying 100% as "Sober" (Drunk Accuracy = 0%). Class Collapse (Drunk): LightGBM and Transformer failed by classifying 100% as "Drunk" (Sober Accuracy = 0%). Export Failure: RNN-based models failed to compile for edge device due to graph unrolling errors.

- **First 3 experiments:**
  1. Reproduce Preprocessing: Load raw accelerometer data, apply low-pass filter, downsample to 40Hz, slice into 20-second windows. Verify "High/Low" TAC label alignment.
  2. Baseline Replication: Train 1D-CNN and LightGBM on provided dataset. Confirm LightGBM exhibits "Zero Sober Accuracy" failure mode described in Table II.
  3. Mobile Latency Test: Export 1D-CNN model to Executorch, deploy to Android device, measure inference time. Verify meets <0.1s benchmark.

## Open Questions the Paper Calls Out
- Does incorporating user profiles and demographic variables into the model architecture significantly improve detection accuracy and generalizability across diverse populations?
- What specific gains in latency and energy efficiency can be achieved by implementing the 1D-CNN or HDC models using C and vectorized operations compared to the current PyTorch-based prototype?
- How does the system perform in rigorous real-world scenarios regarding data privacy and the efficacy of Just-in-Time Adaptive Interventions (JITAIs)?

## Limitations
- Dataset of 30 participants is relatively small for deep learning applications, raising concerns about model generalizability
- 76.1% accuracy may not be sufficient for reliable real-world deployment where false negatives could have safety implications
- Reliance on TAC as ground truth creates potential temporal misalignment issues in training data

## Confidence
- **High Confidence:** Computational efficiency claims for 1D-CNN (0.035 MB model size, 0.0121s inference time) are well-supported by mobile deployment benchmarks
- **Medium Confidence:** Accuracy metrics (76.1%) are credible but require independent validation on larger, more diverse datasets
- **Low Confidence:** HDC model's practical utility is questionable given its 1000x larger size (36.7 MB) compared to 1D-CNN despite similar performance

## Next Checks
1. Verify whether the custom dataset can be accessed through IRB-approved channels or if a synthetic dataset can be created following the same sampling and labeling protocols
2. Analyze the TAC-to-sensor data alignment process to quantify potential lag-induced labeling errors and their impact on model performance
3. Deploy the 1D-CNN model on multiple Android devices with different hardware specifications to confirm inference time and power consumption consistency with Samsung Galaxy S20 results