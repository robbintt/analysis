---
ver: rpa2
title: An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data
arxiv_id: '2502.01789'
source_url: https://arxiv.org/abs/2502.01789
tags:
- cognitive
- workflow
- concerns
- prompt
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed and validated a fully automated, multi-agent
  AI workflow using LLaMA 3 8B to detect cognitive concerns in clinical notes. The
  agentic workflow, which employs task-specific agents that collaborate to analyze
  clinical notes, was compared to an expert-driven benchmark workflow.
---

# An Agentic AI Workflow for Detecting Cognitive Concerns in Real-world Data

## Quick Facts
- arXiv ID: 2502.01789
- Source URL: https://arxiv.org/abs/2502.01789
- Reference count: 0
- Multi-agent AI workflow achieves expert-level accuracy in detecting cognitive concerns from clinical notes

## Executive Summary
This study presents a fully automated, multi-agent AI workflow using LLaMA 3 8B to detect cognitive concerns in clinical notes. The workflow employs task-specific agents that collaborate to analyze clinical text, achieving classification performance comparable to an expert-driven benchmark workflow. Both approaches demonstrated high accuracy with F1-scores of 0.90 (agentic) and 0.91 (expert-driven). The agentic workflow showed improved specificity (1.00) and required fewer prompt refinement iterations (2 vs. 4). While promising, the study's findings need validation in larger, more diverse clinical settings to confirm scalability and real-world applicability.

## Method Summary
The study developed an agentic workflow that uses multiple specialized AI agents working collaboratively to process clinical notes. The workflow was trained and evaluated on 301 clinical notes with an imbalanced distribution of cognitive concerns. Performance was compared against an expert-driven benchmark workflow using the same underlying dataset and prompts. The agentic workflow demonstrated comparable classification accuracy while requiring fewer iterations for prompt refinement and achieving perfect specificity.

## Key Results
- Both agentic and expert-driven workflows achieved high classification performance (F1-scores of 0.90 and 0.91, respectively)
- Agentic workflow demonstrated improved specificity (1.00 vs 0.96) compared to expert-driven approach
- Agentic workflow required fewer prompt refinement iterations (2 vs. 4) for optimization
- Both workflows showed reduced performance on validation data, with agentic maintaining perfect specificity

## Why This Works (Mechanism)
The multi-agent approach works by dividing the cognitive concern detection task into specialized subtasks, each handled by dedicated agents. This division of labor allows for more focused processing of clinical note components, with agents specializing in different aspects of cognitive assessment. The collaborative nature enables agents to cross-verify findings and refine outputs iteratively, leading to more accurate and consistent classifications compared to monolithic approaches.

## Foundational Learning
- Clinical note processing - Understanding medical terminology and documentation patterns in clinical notes is essential for accurate cognitive concern detection. Quick check: Review sample clinical notes to identify key indicators of cognitive concerns.
- Multi-agent coordination - Agents must communicate effectively and share context to produce coherent assessments. Quick check: Map agent communication flows and identify potential information bottlenecks.
- Prompt engineering - Effective prompts are crucial for guiding AI agents in clinical decision-making. Quick check: Test prompt variations with small sample sets to optimize performance.
- Classification metrics - Understanding precision, recall, and F1-score helps evaluate model performance in imbalanced datasets. Quick check: Calculate metrics on test data to verify expected performance ranges.

## Architecture Onboarding

Component Map:
Clinical Note Input -> Pre-processing Agent -> Analysis Agent 1 -> Analysis Agent 2 -> Validation Agent -> Classification Output

Critical Path:
Clinical Note Input → Pre-processing Agent → Analysis Agents → Validation Agent → Final Classification

Design Tradeoffs:
- Accuracy vs. Speed: Multi-agent approach provides thorough analysis but may increase processing time
- Complexity vs. Interpretability: Multiple agents enable sophisticated processing but make the decision path harder to trace
- Automation vs. Control: Full automation reduces human intervention but limits manual oversight opportunities

Failure Signatures:
- Performance degradation when clinical notes use non-standard terminology
- Classification errors on minority class (positive cases) due to dataset imbalance
- Inconsistent outputs when agents fail to communicate effectively

Three First Experiments:
1. Test workflow performance on clinical notes with varying levels of detail and terminology standardization
2. Evaluate agent collaboration efficiency by measuring communication overhead and decision-making time
3. Assess classification performance on synthetic data with controlled cognitive concern indicators

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (301 notes) with imbalanced distribution limits generalizability
- Focus on classification performance without clinical outcomes or implementation challenges
- Need for validation in diverse clinical settings and documentation styles
- Lack of assessment on real-world integration with electronic health records

## Confidence
- High confidence in technical implementation and classification performance
- Medium confidence in efficiency gains due to controlled experimental conditions
- Low confidence in scalability and real-world applicability claims

## Next Checks
1. Test the agentic workflow on a larger, more diverse clinical dataset (>1000 notes) from multiple healthcare institutions to assess generalizability across different documentation styles and clinical settings.

2. Conduct a time-motion study comparing the agentic workflow to expert-driven approaches in a real clinical workflow, measuring not just classification accuracy but also integration with electronic health records and impact on clinical decision-making.

3. Perform an external validation using an independent dataset with known ground truth diagnoses to verify the reported F1-scores and specificity rates, particularly examining performance on the minority class (positive cases).