---
ver: rpa2
title: 'ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient
  Question Answering for Top Factuality'
arxiv_id: '2506.12886'
source_url: https://arxiv.org/abs/2506.12886
tags:
- clinical
- question
- sentences
- patient
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents three approaches to the ArchEHR-QA 2025 shared
  task on patient question answering. The methods include an end-to-end prompt-based
  baseline and two two-step approaches that first extract essential sentences from
  clinical text and then generate answers.
---

# ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality

## Quick Facts
- arXiv ID: 2506.12886
- Source URL: https://arxiv.org/abs/2506.12886
- Authors: Adrián Cuadrón; Aimar Sagasti; Maitane Urruela; Iker De la Iglesia; Ane G Domingo-Aldama; Aitziber Atutxa; Josu Goikoetxea; Ander Barrena
- Reference count: 37
- Top Factuality: Ranked 8th overall, achieved highest factuality score among all submissions

## Executive Summary
This paper presents three approaches to the ArchEHR-QA 2025 shared task on patient question answering using clinical notes. The best-performing method uses a two-step divide-and-conquer strategy: first extracting essential sentences via a specialized re-ranker, then generating answers using a smaller domain-appropriate LLM. This approach achieves the highest factuality score among all submissions while ranking 8th overall. The results demonstrate that task decomposition combined with appropriate method selection for each subtask significantly improves performance, even surpassing organizers' zero-shot baseline without external knowledge.

## Method Summary
The paper evaluates three approaches: an end-to-end prompt-based baseline, and two two-step methods. The two-step approaches first extract essential sentences from clinical text using either prompting or similarity ranking, then generate answers based on those sentences. The best-performing method uses a Jina re-ranker to score sentence relevance against the query, applies thresholding to select essential sentences, and employs Aloe 8B to generate answers from the selected evidence. Post-processing adds citation IDs and enforces word limits. All approaches rely on few-shot prompting rather than fine-tuning.

## Key Results
- The re-ranker based two-step system achieved the highest factuality score among all 30 submissions
- The system ranked 8th overall on the leaderboard with an overall score of 0.44
- Aloe 8B (8B parameters) outperformed larger models including Llama 3.3 70B on both factuality and relevance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task decomposition into sentence retrieval and answer generation improves factuality scores.
- Mechanism: A specialized re-ranker model first identifies essential sentences from clinical notes using a learned relevance threshold, then an LLM synthesizes only those sentences into a response. This constrains the generation space, reducing hallucination risk.
- Core assumption: The re-ranker's relevance scores generalize from the dev set to the test set.
- Evidence anchors:
  - [abstract] "Both two step approaches first extract essential sentences from the clinical text... Results indicate that the re-ranker based two-step system performs best"
  - [section 6.2.1.2] "We leverage this method as it aligns with the task's goal of identifying the sentences of the clinical text that are relevant to answer the patient's query."
  - [corpus] A neighboring paper (UTSA-NLP at ArchEHR-QA 2025) also uses a two-step approach with LLMs, finding improvement via self-consistency prompting for sentence relevance, supporting the decomposition hypothesis.
- Break condition: If the re-ranker's optimal threshold (tuned on dev) does not generalize to test data due to distribution shift, sentence retrieval precision/recall will drop, degrading the final answer's factuality.

### Mechanism 2
- Claim: Specialized similarity-based retrieval (re-ranker) outperforms general-purpose LLM prompting for evidence extraction.
- Mechanism: A dedicated re-ranker model, trained on semantic similarity tasks, directly compares query-sentence pairs to produce relevance scores. This is more effective than prompting a general LLM to either list essential IDs or classify sentences individually, as the re-ranker's training objective directly aligns with the retrieval task.
- Core assumption: The re-ranker model has been effectively pre-trained or fine-tuned on tasks similar to clinical sentence relevance ranking.
- Evidence anchors:
  - [abstract] "Both two step approaches first extract essential sentences from the clinical text—by prompt or similarity ranking—... Results indicate that the re-ranker based two-step system performs best"
  - [section 7] "This suggests that more critical than merely dividing the task into smaller and simpler subtasks, is the choice of an appropriate method for each subtask. In this case, using a reranker... proves to be more effective than the prompting-based selection."
  - [corpus] Corpus signals show multiple ArchEHR-QA 2025 papers explore different retrieval and ranking strategies (e.g., heiDS uses RAG), indicating broad interest in optimizing this specific subtask. Direct comparison data is limited in the provided neighbors.
- Break condition: If the clinical domain uses highly specialized terminology or note structures not well-represented in the re-ranker's training data, its similarity scores may be noisy, causing it to underperform against a medical-domain LLM.

### Mechanism 3
- Claim: Smaller, domain-appropriate models (8B parameters) can outperform larger, general-purpose models (70B) on this specific clinical QA task.
- Mechanism: The task requires a combination of medical understanding, strict output formatting, and summarization. A specialized 8B model (Aloe Beta), potentially fine-tuned on healthcare data, may have better calibrated priors for this domain and task format than a much larger, but more general, model. This reduces the need for the model to "learn" the domain from scratch via in-context examples.
- Core assumption: The Aloe 8B model's superior performance is due to its specialized training and not an artifact of the specific prompting strategy or evaluation set.
- Evidence anchors:
  - [section 6.1] "Table 1 shows that the smaller Aloe Beta (8B) model outperforms larger models in both Factuality and Relevance."
  - [section 7] "...surpass the organizers' zero-shot baseline... despite relying on a significantly larger model (Llama 3.3 70B). This further supports our conclusion in Section 6.1 that larger models do not necessarily outperform smaller ones..."
  - [corpus] Weak direct corpus evidence on model size comparison for this specific task. Most neighboring papers focus on methodology (e.g., agentic prompts, RAG) rather than a direct size ablation.
- Break condition: If task complexity increases significantly (e.g., requiring multi-hop reasoning across many notes), a smaller model's capacity may become a bottleneck, causing larger models to regain an advantage.

## Foundational Learning

- Concept: **Reranking / Two-Stage Retrieval**
  - Why needed here: This is the core of the paper's best-performing method. Understanding how a re-ranker refines an initial set of items (here, sentences) based on a query is critical to reproducing the results.
  - Quick check question: Given a user query and a set of 10 sentences, what is the re-ranker's output, and how is it used to select sentences for the next stage?

- Concept: **Prompting Strategies (Role, CoT, Few-Shot)**
  - Why needed here: The paper compares multiple prompting techniques for both the end-to-end baseline and the first step of a two-step approach. Knowing how to design and evaluate these prompts is essential for understanding the baselines.
  - Quick check question: What is the key difference between the "Extract list" and "Determine essentials individually" prompting strategies, and what is a potential trade-off of each?

- Concept: **One-Shot / Few-Shot In-Context Learning**
  - Why needed here: All approaches in the paper rely on providing the model with one or a few input-output examples (shots) in the prompt, as no fine-tuning was performed. This is a fundamental technique used throughout.
  - Quick check question: Why did the authors choose one-shot/few-shot learning over fine-tuning for this task, and how does that choice impact the prompts you would design?

## Architecture Onboarding

- Component map:
  1. **Input**: Patient narrative, clinical question, annotated EHR (sentences with IDs)
  2. **Stage 1 - Evidence Extraction**:
      - Option A (Baseline): End-to-end LLM
      - Option B (Prompting): LLM prompted to list essential IDs
      - Option C (Best): Jina re-ranker scores all sentences; thresholding selects subset of "essential" sentences
  3. **Stage 2 - Answer Generation**: Aloe 8B generates answer from selected essential sentences
  4. **Post-Processing**: Truncates to 75 words, matches generated sentences back to original essential sentences, appends citation IDs

- Critical path: The **Stage 1 Re-ranker** is the most critical component, as its output directly gates the information available to the generator. Errors in retrieval cannot be fixed later. The subsequent post-processing to add citations is also critical for the factuality metric.

- Design tradeoffs:
  - **Re-ranker vs. LLM for Retrieval**: Re-ranker is faster and more specialized but may lack deep reasoning. LLM is slower but can perform complex relevance judgment.
  - **Model Size**: A smaller, specialized model (8B) is computationally cheaper and performed best here, but may lack the broad knowledge of a 70B model for complex edge cases.
  - **Thresholding**: A fixed threshold is simple but brittle to data distribution shifts. A dynamic threshold or top-k selection would be alternative designs.

- Failure signatures:
  - **Low Factuality Score**: Indicates the re-ranker is selecting irrelevant sentences or missing essential ones. Check the threshold and re-ranker's performance on error cases.
  - **Low Relevance Score**: Indicates the generator LLM is failing to synthesize the selected sentences into a coherent answer or is hallucinating new information.
  - **Format Errors**: Output exceeds 75 words or citations are malformed. Check post-processing rules and the generator's instruction following.

- First 3 experiments:
  1. **Re-ranker Ablation**: Run Stage 1 using the re-ranker versus the best prompting method. Measure precision, recall, and F1 on the sentence classification task to isolate the contribution of the retrieval component.
  2. **Threshold Sensitivity Analysis**: Instead of a fixed Youden index threshold, experiment with a top-k selection or a percentile-based threshold. Evaluate how this changes the trade-off between precision and recall on the dev set.
  3. **Generator Model Swap**: Keep the re-ranker based Stage 1 fixed. Swap the Stage 2 generator model (e.g., from Aloe 8B to Gemma 2 9B or Mistral 7B) to measure the impact of the generation model on the final Relevance and Factuality scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does incorporating external medical knowledge via Retrieval Augmented Generation (RAG) enhance system performance compared to the current reliance on in-context data?
- Basis in paper: [explicit] The authors state in the Limitations section that "Information Retrieval methods such as Retrieval Augmented Generation (RAG) are not employed, even though they could be beneficial given the limited data available."
- Why unresolved: The study explicitly limited its scope to the provided data without external retrieval to focus on prompt-engineering and re-ranking strategies.
- What evidence would resolve it: A comparative study integrating a medical knowledge base (e.g., PubMed) against the current baseline on the ArchEHR-QA dataset.

### Open Question 2
- Question: Can text-to-text generative models (e.g., T5) improve the Relevance score in the second step without compromising the high Factuality scores achieved by decoder-only models?
- Basis in paper: [explicit] The Conclusion and Limitations suggest there is "room for improvement in order to enhance the Relevance overall score—for instance, by trying text-to-text models like T5."
- Why unresolved: The authors relied on prompt-based generative LLMs (Aloe, Mistral) which excelled at factuality but left a gap in argumentative relevance.
- What evidence would resolve it: Implementing a T5-based generator for the answer formulation step and comparing Relevance metrics (BLEU, ROUGE, etc.) against the current Aloe-based generator.

### Open Question 3
- Question: How robust is the static re-ranker thresholding strategy when applied to datasets with different distributions of clinical note lengths or relevance densities?
- Basis in paper: [inferred] Section 6.2.1.2 notes a potential limitation: "the threshold is determined based on the dev set and may not generalize well to the test set if the data distribution is different."
- Why unresolved: The optimization relies on the Youden index calculated on a specific development set, potentially overfitting the threshold to that specific distribution of essential sentences.
- What evidence would resolve it: Evaluating the re-ranker's F1-score on out-of-domain clinical datasets or using adaptive thresholding techniques to measure stability.

## Limitations
- The optimal threshold selection method (Youden index) may overfit to the dev set and not generalize to test data
- The paper does not explore domain adaptation of the re-ranker model or systematic sensitivity analysis of the evidence extraction threshold
- While the 8B model outperforms the 70B baseline, the analysis does not isolate whether this advantage stems from model architecture, domain knowledge, or prompt engineering differences

## Confidence

**High Confidence**: The comparative performance results showing the re-ranker approach outperforming both end-to-end baselines and the zero-shot baseline, as well as the superiority of the 8B model over the 70B model for this specific task.

**Medium Confidence**: The mechanism claims that task decomposition and specialized retrieval improve factuality, as the paper provides strong correlational evidence but does not conduct ablation studies isolating the individual contributions of each component or test threshold robustness across different datasets.

**Low Confidence**: The generalization claims about model size effects, as the comparison between 8B and 70B models lacks systematic ablation and may be influenced by domain-specific factors not explored in the paper.

## Next Checks

1. **Threshold Robustness Analysis**: Implement cross-validation or bootstrap sampling on the dev set to evaluate how stable the Youden index threshold is across different data subsets, and test alternative selection methods (top-k, percentile-based) to assess sensitivity to distribution shifts.

2. **Component Ablation Study**: Systematically disable or replace each component (re-ranker, generator model, post-processing) while keeping others fixed to quantify the marginal contribution of each element to the final factuality and relevance scores.

3. **Domain Adaptation Experiment**: Fine-tune the Jina re-ranker on a small subset of the clinical data (if available) versus using it zero-shot, and measure the impact on sentence retrieval precision/recall to determine whether domain adaptation provides meaningful improvements over the current approach.