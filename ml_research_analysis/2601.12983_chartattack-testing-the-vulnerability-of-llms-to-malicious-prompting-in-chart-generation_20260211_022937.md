---
ver: rpa2
title: 'ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart
  Generation'
arxiv_id: '2601.12983'
source_url: https://arxiv.org/abs/2601.12983
tags:
- chart
- misleading
- charts
- correct
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ChartAttack, the first framework to automatically
  generate misleading charts using multimodal large language models (MLLMs). ChartAttack
  systematically injects design-level misleaders into chart specifications to induce
  incorrect interpretations of data.
---

# ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation

## Quick Facts
- arXiv ID: 2601.12983
- Source URL: https://arxiv.org/abs/2601.12983
- Reference count: 40
- First framework to automatically generate misleading charts using MLLMs

## Executive Summary
ChartAttack introduces a systematic framework for automatically generating misleading charts using multimodal large language models (MLLMs). By injecting design-level misleaders into chart specifications, the system induces incorrect interpretations of data visualizations. The authors create AttackViz, a chart QA dataset with structured annotations specifying effective misleaders and induced incorrect answers. Experiments show MLLMs drop 19.6 points on in-domain data and 14.9 points on cross-domain data, while human participants drop 20.2 points when exposed to misleading charts.

## Method Summary
ChartAttack operates through two modules: Demonstration Selection and Misleader-generator. The system first simplifies chart JSON annotations and retrieves similar examples using SBERT embeddings fine-tuned with Multiple Negative Ranking Loss. The Misleader-generator module, a code-based MLLM, then selects contextually appropriate misleaders and produces JSON modifications to apply these misleaders. The framework generates charts via Matplotlib rendering, targeting 8 types of design-level misleaders including 3D effects, stacked bars, inverted axes, and log scales. The AttackViz dataset is built by applying rule-based misleaders to PlotQA and ChartQA, then filtering for instances where models show consistent incorrect responses to misleading variants.

## Key Results
- MLLMs drop 19.6 points on in-domain data and 14.9 points on cross-domain data when exposed to misleading charts
- Human participants drop 20.2 points on misleading charts versus correct charts
- 3D effects remain most impactful misleader, reducing accuracy to 25.3% (58.3 pp drop) in cross-domain evaluation
- Larger InternVL models (8B-38B) showed larger accuracy drops (23-38 pp) than smaller ones (15-16 pp)

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting with retrieved demonstrations enables MLLMs to systematically select and apply misleaders to chart specifications. The Demonstration Selection module retrieves similar (chart annotation, question, misleader) examples via SBERT embeddings trained with Multiple Negative Ranking Loss. These demonstrations guide the Misleader-generator module to select contextually appropriate misleaders and produce minimal JSON modifications. The 5-shot configuration yielded the best Macro F1 scores in oracle experiments.

### Mechanism 2
Modifying chart JSON annotations rather than raw images enables precise, reproducible misleader injection while preserving data consistency. ChartAttack operates on structured chart specifications (JSON) containing data tables, axis configurations, and visual encoding parameters. Misleaders are applied by modifying specific fields—for example, `direction: "top-to-bottom"` for inverted axis, `"3D effect": True` for 3D distortion, or `scaling_factors` dictionaries for misrepresentation—without altering the underlying data values.

### Mechanism 3
Design-level misleaders that alter visual perception without changing data values transfer across both MLLM architectures and human readers. Certain misleaders (3D, stacked, inverted axis, log scale) create systematic perceptual biases—e.g., 3D perspective makes closer objects appear larger, inverted axes reverse perceived trends. These biases exploit universal visual cognition patterns rather than model-specific weaknesses, explaining similar degradation in humans (20.2 pp) and MLLMs (19.7 pp).

## Foundational Learning

- **Few-shot in-context learning with retrieval augmentation**: ChartAttack's Misleader-generator relies on demonstration examples to teach the model which misleaders apply to which chart-question contexts. Without understanding how retrieval quality affects generation, you cannot debug poor misleader selection. *Quick check: If your retriever returns demonstrations with mismatched misleader sets, would you expect the generator to over-apply or under-apply techniques?*

- **Chart specification formats (JSON annotations)**: The attack surface is the structured representation, not the rendered image. Understanding the schema—axes, data fields, visual encoding parameters—is required to extend ChartAttack to new chart types or visualization libraries. *Quick check: To apply the "misrepresentation" misleader, which JSON field must be added versus modified?*

- **Visualization perception and misleader taxonomy**: Not all misleaders are equally effective. 3D and stacked bar manipulations caused >30 pp drops, while ineffective color schemes had near-zero impact. Defense prioritization requires knowing which techniques pose real risk. *Quick check: Why might "dual axis" show only a 2.2 pp average drop but a 10.3% deception rate on originally correct answers?*

## Architecture Onboarding

- **Component map**: Data table + question → Annotation simplification → Demonstration retrieval → Misleader-generator inference → JSON modification → Chart rendering → QA evaluation
- **Critical path**: The filtering step is critical—without it, noise from plotting artifacts or model-in-the-loop biases contaminates AttackViz. The pipeline must ensure majority-correct on clean charts and majority-incorrect on misleading variants.
- **Design tradeoffs**: SBERT vs. BM25 retrieval: Semantic similarity (SBERT) outperforms lexical (BM25) but requires fine-tuning on anchor-positive pairs with exact misleader matches. Model size vs. vulnerability: Larger InternVL models (8B-38B) showed larger accuracy drops (23-38 pp) than smaller ones (15-16 pp), suggesting higher capability may increase susceptibility to well-crafted misleading charts.
- **Failure signatures**: Low deception rate on originally incorrect answers (~1-3%): Misleader does not reinforce existing errors; it primarily converts correct → incorrect. High variance across misleader types: Ineffective color scheme (0.5% deception) vs. 3D (20.1% deception); if all techniques show similar rates, check if rendering pipeline is actually applying modifications.
- **First 3 experiments**: 
  1. Validate retriever quality: Compute Accuracy@5 on held-out validation set using your trained SBERT model; compare against BM25 baseline. Expect 40-80% depending on chart type.
  2. Ablate few-shot count: Run Misleader-generator with 0, 1, 3, 5 shots on AttackViz validation split; report Macro F1 per chart type. Confirm 5-shot is optimal or identify if your target chart type benefits from fewer shots.
  3. Cross-library rendering test: Take 20 modified JSON annotations, render with Matplotlib and an alternative library (e.g., Vega-Altair), and verify that the same misleader produces visually similar distortions.

## Open Questions the Paper Calls Out

- **Fine-tuning for robustness**: Can fine-tuning MLLMs on paired correct and misleading charts from AttackViz improve robustness against misleaders while maintaining chart understanding performance? The authors explicitly identify this as a defense direction, but the dataset is released without fine-tuning experiments conducted.

- **Multiple misleader combinations**: How does the effectiveness of attacks change when multiple misleaders are applied simultaneously to a single chart? The limitations section states charts containing multiple misleaders represent an important direction for future work.

- **Model characteristics beyond size**: What model characteristics beyond parameter count explain why vulnerability to misleading charts does not scale monotonically with size? Results show InternVL-3.5 8B drops 37.5 pp while 38B drops 25.4 pp, but the paper does not analyze architectural differences as potential explanatory factors.

- **Reasoning-level misleaders**: How effective are reasoning-level misleaders (manipulated titles, annotations, or context) compared to the design-level misleaders studied here? The limitations section states reasoning misleaders remain underexplored, though the taxonomy includes them.

## Limitations

- The generalizability of MLLM vulnerability patterns across different chart types remains uncertain, as experiments were primarily conducted on PlotQA and ChartQA datasets.
- Human evaluation was conducted with limited participants and simplified questions, which may not capture the full complexity of real-world chart interpretation scenarios.
- 3D and dual-axis charts may have reduced readability due to suppressed ticks or perspective issues, potentially confounding misleader impact with plotting artifacts.

## Confidence

- **High confidence**: The systematic framework design, the quantitative MLLM accuracy drops (19.6 pp in-domain, 14.9 pp cross-domain), and the parallel human vulnerability (20.2 pp drop) are well-supported by controlled experiments.
- **Medium confidence**: The mechanism transfer between humans and MLLMs is plausible but not fully explained—shared perceptual vulnerabilities are assumed rather than empirically validated across cognitive models.
- **Low confidence**: The assertion that larger MLLMs are more vulnerable is based on limited model comparisons and may reflect dataset bias rather than intrinsic architectural vulnerability.

## Next Checks

1. **Cross-dataset robustness test**: Apply ChartAttack to an independent chart corpus (e.g., financial reports or scientific papers) and measure whether the same misleader effectiveness hierarchy holds.

2. **Ablation of human-in-the-loop filtering**: Remove the consistency filtering step and quantify how many instances are incorrectly classified as "successfully misleading" due to plotting artifacts versus genuine perceptual bias.

3. **Model capability vs. vulnerability analysis**: Systematically vary both model size and reasoning complexity (simple vs. multi-step questions) to determine whether observed vulnerability patterns are driven by capability gaps or fundamental perceptual weaknesses.