---
ver: rpa2
title: 'Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation
  Graph'
arxiv_id: '2510.10976'
source_url: https://arxiv.org/abs/2510.10976
tags:
- reasoning
- arxiv
- video
- spatial
- spatio-temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of precise spatio-temporal reasoning
  in multimodal large language models (MLLMs), which are strong in semantic understanding
  but struggle with tasks requiring detailed physical information from videos, such
  as object layouts and motion. The core method introduces Video-STR, a graph-based
  reinforcement learning framework.
---

# Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph

## Quick Facts
- arXiv ID: 2510.10976
- Source URL: https://arxiv.org/abs/2510.10976
- Authors: Wentao Wang; Heqing Zou; Tianze Luo; Rui Huang; Yutian Zhao; Zhuochen Wang; Hansheng Zhang; Chengwei Qin; Yan Wang; Lin Zhao; Huaijian Zhang
- Reference count: 16
- One-line primary result: Achieves SOTA performance on STI-Bench with 13% improvement over base model using graph-based reinforcement learning

## Executive Summary
This paper addresses the challenge of precise spatio-temporal reasoning in multimodal large language models, which excel at semantic understanding but struggle with tasks requiring detailed physical information from videos. The authors introduce Video-STR, a graph-based reinforcement learning framework that extends GRPO with a graph reasoning mechanism to infer underlying spatio-temporal topology using inter-object relation graphs. These graphs capture spatial relationships among objects, providing rotation-invariant representations robust to viewpoint changes.

The approach constructs STV-205k, a dataset of 205k question-answer pairs derived from TAO, KITTI, and ScanNet, covering both static spatial and dynamic temporal reasoning tasks. Experiments show that Video-STR achieves state-of-the-art results across multiple benchmarks, outperforming the base model by 13% on STI-Bench and demonstrating superior performance in spatial, temporal, and spatio-temporal reasoning tasks.

## Method Summary
Video-STR employs Qwen2.5-VL-7B-Instruct as the base model, enhanced through Group Relative Policy Optimization (GRPO) with a graph reasoning mechanism. The method prompts the model to generate explicit graph structures during the thinking process, where nodes represent objects and edges encode pairwise distances and directional angles. This creates rotation-invariant representations that capture inter-object relationships rather than absolute coordinates. The framework uses verifiable rewards including format checking, answer accuracy (multiple choice, numerical, IoU), graph structure accuracy (node and edge), and length penalties. Training employs 8 responses per sample on a subset of STV-205k, with efficiency optimizations limiting frames to 16 during training and 32 during inference.

## Key Results
- Achieves state-of-the-art performance on STI-Bench with 13% improvement over base model
- Outperforms supervised fine-tuning on multiple benchmarks including V-STaR, VSI-Bench, SPAR-Bench, and Video-MME
- Demonstrates superior numerical QA accuracy (40.2% vs 38.1%) compared to base model
- Shows better generalization across spatial, temporal, and spatio-temporal tasks than SFT alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Inter-object relation graphs provide rotation-invariant spatio-temporal representations that outperform 2D cognitive maps and pixel-level localization.
- **Mechanism**: Instead of tracking object-to-camera coordinates (which shift with ego-motion), the model constructs graphs where nodes = objects, edges = pairwise distances and directional angles. Since rotating all objects preserves their relative distances and angles, the representation remains stable under viewpoint changes.
- **Core assumption**: Spatio-temporal reasoning tasks primarily depend on relative object configurations rather than absolute camera-centric coordinates.
- **Evidence anchors**:
  - [abstract]: "rotation-invariant graph representation" enabling modeling of "inter-object relationships"
  - [Page 5, Section 4.2]: Theorem 1 formally proves Φ(RX, C) = Φ(X, C) for rotation matrix R
  - [Page 2]: States 2D cognitive maps fail because "changes in camera viewpoints and non-rotation-invariant object coordinates can lead to errors"
  - [corpus]: TimeBlind (arXiv:2602.00288) highlights brittleness of temporal reasoning in current MLLMs, supporting the need for structured representations
- **Break condition**: Tasks requiring absolute egocentric coordinates (e.g., "how far is the cup from *me*") may not benefit from purely inter-object representations.

### Mechanism 2
- **Claim**: Explicit graph supervision during the thinking process forces genuine spatial understanding rather than answer memorization.
- **Mechanism**: The model is prompted to generate graph predictions (node positions, edge distances/angles) as intermediate reasoning steps. Separate rewards (R_n for nodes, R_e for edges) directly assess topology accuracy, creating gradient signals tied to spatial structure rather than just final answer correctness.
- **Core assumption**: Models can learn to generate accurate intermediate graph representations that causally improve final answer accuracy.
- **Evidence anchors**:
  - [Page 5, Section 4.2]: R_graph = (R_n + R_e) computed across k frames provides "explicit reward signals to ensure a genuine understanding"
  - [Page 7, Table 2]: Ablation shows removing graph reasoning drops STI-Bench from 39.3 to 36.9
  - [Page 8, Table 3]: Numerical QA accuracy improves from 38.1% → 40.2%, suggesting genuine understanding beyond multiple-choice guessing
  - [corpus]: Insufficient direct evidence; no corpus papers specifically examine intermediate reasoning supervision
- **Break condition**: If graph generation overhead exceeds computational budget, or if tasks require only single-object reasoning where graphs add no information.

### Mechanism 3
- **Claim**: GRPO-based reinforcement learning generalizes across diverse spatio-temporal tasks while supervised fine-tuning overfits to training distributions.
- **Mechanism**: GRPO samples multiple responses per question, computes relative advantages within groups, and optimizes policy via clipped objectives with KL regularization. This rewards reasoning processes that generalize rather than memorizing specific question-answer patterns. Verifiable rewards (exact match, IoU, numerical accuracy) provide non-subjective supervision.
- **Core assumption**: Verifiable rewards sufficiently capture task success; exploration of reasoning paths leads to transferable strategies.
- **Evidence anchors**:
  - [Page 2, Section 1]: Explicitly claims "RLVR generalizes, whereas SFT memorizes"
  - [Page 7, Table 1]: SFT improves STI-Bench (34.7→35.3) but degrades TempCompass (71.1→69.8); Video-STR improves both
  - [Page 7]: Attributes generalization to not "overfitting" compared to SFT
  - [corpus]: MUSEG (arXiv:2505.20715) similarly uses RL for video temporal understanding with timestamp-aware rewards, achieving comparable gains
  - [corpus]: SpaceR (cited in paper) explores RLVR for spatial reasoning but lacks graph-based mechanism
- **Break condition**: When ground-truth rewards are unavailable or noisy; when training data is so abundant and diverse that SFT suffices; when RL training instability outweighs benefits.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and GRPO**
  - **Why needed here**: GRPO extends PPO by eliminating the critic model through group-based advantage estimation. Understanding PPO's clipping mechanism (preventing destructive policy updates) is essential for tuning GRPO hyperparameters (ϵ, β, group size G).
  - **Quick check question**: Given 8 candidate responses with rewards [0.2, 0.4, 0.4, 0.6, 0.8, 0.8, 1.0, 1.0], compute the normalized advantage for the response with reward 0.6.

- **Concept: Graph neural representations vs. sequential reasoning**
  - **Why needed here**: Video-STR prompts the model to generate explicit graph structures (nodes, edges, attributes) rather than implicitly reasoning through chain-of-thought. Distinguishing structured from unstructured intermediate representations clarifies why this approach suits multi-object spatial tasks.
  - **Quick check question**: For a scene with 4 objects, how many edge attributes (d_ij, θ_ij) must the model predict? What computational cost does this add compared to pure text CoT?

- **Concept: Rotation invariance in computer vision**
  - **Why needed here**: The paper's core claim hinges on SO(3) invariance. Understanding why inter-object distances/angles are rotation-invariant while absolute coordinates are not explains the representation choice and its limitations.
  - **Quick check question**: If a camera rotates 90° clockwise, does the distance between two objects change? Does the angle of the vector from object A to B relative to the world frame change? Relative to the camera frame?

## Architecture Onboarding

- **Component map**: Input Video/Image → Vision Encoder (Qwen2.5-VL) → Visual Tokens → Question Template → Prompt Construction → LLM Backbone → Output: <think process with graph> + <answer> → Reward Computation (R_format, R_ans, R_graph, R_length) → GRPO Loss → Policy Update

- **Critical path**:
  1. **Graph extraction/parsing**: During training, the model's `<think/>` output must be parsed to extract predicted node positions (x_i) and edge attributes (d_ij, θ_ij). Failure here breaks R_graph computation.
  2. **Ground-truth graph construction**: STV-205k must provide object center coordinates and categories to compute ground-truth distances/angles for reward calculation.
  3. **Multi-response sampling**: GRPO requires G=8 responses per question during training. If all responses are identical (low temperature, high repetition penalty), advantage estimates collapse and learning stalls.

- **Design tradeoffs**:
  - **Frame count (16 training / 32 inference)**: Fewer frames speed training but lose temporal detail. Paper shows 32 frames improve STI-Bench performance.
  - **Response length bounds [320, 512] with R_l=0.2**: Encourages sufficient reasoning depth without runaway verbosity. Narrower bounds risk truncating valid reasoning; wider bounds increase compute.
  - **KL penalty β=0.04**: Prevents catastrophic forgetting of base model capabilities. Higher β stabilizes but slows learning; lower β risks divergence.
  - **Graph reward weight**: Paper sums R_graph directly into R_total without weighting. If R_graph magnitudes differ significantly from R_ans, one may dominate learning.

- **Failure signatures**:
  - **Reward collapse**: R_graph stays near 0 despite training → check ground-truth graph construction, parsing logic, or increase learning rate.
  - **Format violations**: R_format frequently 0 → prompt template not enforced; check tokenizer/special token handling.
  - **Length penalty gaming**: Model produces correct but minimally short answers to secure R_l without genuine reasoning → tighten length bounds or remove R_l.
  - **Generalization gap**: Training rewards increase but benchmark scores stagnate → overfitting to STV-205k distribution; add regularization or diversify data.
  - **Degraded temporal understanding**: Spatial benchmarks improve but TempCompass drops → temporal subset insufficient; rebalance STV-205k sampling.

- **First 3 experiments**:
  1. **Baseline validation**: Train with only R_format + R_ans (no R_graph, no R_l) on a 10k subset of STV-205k. Verify GRPO improves over SFT baseline on STI-Bench. This isolates RLVR's contribution from graph reasoning.
  2. **Graph reward ablation**: Add R_graph alone, then R_l alone, then both. Measure impact on spatial (VSI-Bench), temporal (TempCompass), and spatio-temporal (STI-Bench) tasks. Confirm R_graph primarily aids spatial/spatio-temporal, not pure temporal tasks.
  3. **Rotation invariance stress test**: Evaluate on videos with simulated camera rotations (crop/rotate augmentations). Compare Video-STR against a coordinate-prediction baseline (predicting absolute positions). Expect Video-STR to show smaller performance degradation, validating the invariance claim.

## Open Questions the Paper Calls Out
- How does Video-STR perform when extended to richer modalities beyond standard RGB video, such as depth or thermal data?
- How does the computational overhead of the inter-object relation graph scale in environments with high object density (e.g., crowded scenes with >50 objects)?
- Does the restriction to 16 frames during training limit the model's ability to generalize to long-term temporal reasoning required for complex trajectory prediction?

## Limitations
- The construction of STV-205k dataset is not fully specified - exact templates, filtering thresholds, and sampling strategies remain unclear
- The graph generation mechanism during the thinking process lacks detailed implementation specifics
- The specific contribution of graph reasoning versus other components of the reinforcement learning pipeline is not fully isolated in ablation studies

## Confidence
- **High confidence**: The GRPO framework with verifiable rewards improves over supervised fine-tuning across multiple benchmarks (Table 1). The mathematical proof of rotation invariance (Theorem 1) is sound. The 13% improvement on STI-Bench over the base model is empirically demonstrated.
- **Medium confidence**: The claim that inter-object graph representations outperform 2D cognitive maps requires further validation, particularly on tasks requiring absolute egocentric coordinates. The generalization benefits over SFT are demonstrated but could be influenced by dataset distribution shifts.
- **Low confidence**: The specific contribution of graph reasoning versus other components of the reinforcement learning pipeline is not fully isolated in ablation studies. The mechanism by which explicit graph supervision improves spatial understanding versus answer memorization needs more direct evidence.

## Next Checks
1. **Rotation invariance validation**: Create a controlled test set with systematic camera rotations and compare Video-STR performance against a baseline that uses absolute object coordinates. Measure the degradation in spatial reasoning accuracy under rotation to quantify the practical benefit of rotation-invariant representations.

2. **Graph representation necessity**: Implement an ablation that replaces graph-based reasoning with direct text-based spatial reasoning (similar to chain-of-thought) while maintaining the GRPO framework. Compare performance on spatial tasks to determine whether the graph structure itself provides benefits beyond the reinforcement learning approach.

3. **Reward signal isolation**: Train a variant that removes R_graph but keeps all other GRPO components, then another variant that removes R_ans but keeps R_graph. Compare these ablations to the full model on STI-Bench to determine the relative importance of graph supervision versus task-specific answer rewards in driving performance improvements.