---
ver: rpa2
title: 'SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M
  Parameters'
arxiv_id: '2509.15490'
source_url: https://arxiv.org/abs/2509.15490
tags:
- spatial
- reasoning
- smolrgpt
- depth
- warehouse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SmolRGPT is a compact vision-language model that achieves strong
  spatial reasoning in warehouse environments using only 600M parameters. It integrates
  RGB and depth information through separate connectors and refiners, and employs
  a three-stage training curriculum: global alignment on a caption dataset, spatial
  warmup on a large spatial dataset, and task-specific fine-tuning on warehouse data.'
---

# SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters

## Quick Facts
- arXiv ID: 2509.15490
- Source URL: https://arxiv.org/abs/2509.15490
- Authors: Abdarahmane Traore; Ã‰ric Hervet; Andy Couturier
- Reference count: 32
- Primary result: 600M-parameter model achieving 90.68 S1 score on AI City Challenge 2025 Warehouse Track, excelling at left-right reasoning (99.80% accuracy)

## Executive Summary
SmolRGPT is a compact vision-language model designed for spatial reasoning in warehouse environments that achieves competitive performance with only 600M parameters. The model processes RGB and depth information through separate connectors and refiners before fusion, and employs a three-stage curriculum training approach: global alignment on caption data, spatial warmup on a large spatial dataset, and task-specific fine-tuning on warehouse data. Evaluated on the AI City Challenge 2025 Warehouse Track, it placed third with an S1 score of 90.68, demonstrating that efficient architectures can match or exceed larger models on qualitative spatial relations while using 13x-57x fewer parameters.

## Method Summary
SmolRGPT uses a SigLIP2-base visual encoder (frozen) combined with a SmolLM2-360M LLM backbone. The architecture processes RGB and depth inputs through separate pixel-shuffle-based connectors and transpose-convolution refiners, with region features extracted via mask pooling and injected as special tokens into the LLM. The three-stage curriculum trains first on LLaVA-CC3M for global alignment (20 epochs, LR 1e-4), then on 1M OSD samples for spatial warmup (1 epoch, LR 1e-4), and finally on warehouse data for task-specific fine-tuning (early stopping at epoch 4, LR 5e-5). Evaluation uses Longformer classifiers for answer normalization and Qwen2.5-14B for final answer extraction.

## Key Results
- Achieved 90.68 S1 score on AI City Challenge 2025 Warehouse Track (3rd place)
- 99.80% accuracy on left-right spatial reasoning task
- Matches or exceeds larger models on qualitative spatial relations while using 13x-57x fewer parameters
- Strong distance estimation performance attributed to separate RGB/depth refiners

## Why This Works (Mechanism)

### Mechanism 1: Modality-Isolated Refinement
Processing RGB and depth through separate connectors and refiners prevents feature entanglement, allowing the limited capacity of a 600M model to learn distinct spatial and appearance features. This forces the model to learn specialized representations for visual texture and geometric structure independently, rather than mixing them in a single projection layer.

### Mechanism 2: Progressive Curriculum Freezing
A three-stage training schedule with strategic freezing prevents catastrophic forgetting during spatial specialization. The approach builds spatial competence progressively by first establishing a stable global visual anchor (RGB) before introducing complex geometric features (depth/regions), with specific learning rates and freezing patterns for each stage.

### Mechanism 3: Region-Token Injection via Mask Pooling
Converting spatial regions into special tokens allows a standard LLM to process geometric relations using its existing attention mechanisms without architectural redesign. The mask pooling extracts features for specific objects and inserts them as tokens, enabling the self-attention layers to relate spatial entities linguistically.

## Foundational Learning

- **Pixel Shuffle (Space-to-Depth)**: Reduces spatial resolution while increasing channel depth to maintain rich feature information in the 600M parameter budget without expensive projection matrices. *Quick check: How does pixel shuffling differ from strided convolution in terms of information loss?*

- **Mask Pooling (RoI Pooling)**: Extracts feature vectors specifically for pixels covered by binary masks, enabling region-level reasoning for questions about specific objects. *Quick check: If a mask covers two separate objects, how would mask pooling affect the resulting feature vector?*

- **Catastrophic Forgetting in Fine-Tuning**: The model uses strategic freezing and learning rate adjustments to preserve pre-trained knowledge while adapting to the warehouse domain. *Quick check: Why does lowering the learning rate in the final stage help preserve global alignment learned earlier?*

## Architecture Onboarding

- **Component map**: Input (Image + Depth + Masks) -> SigLIP2 -> Connectors -> Refiners -> Mask Pooling -> LLM
- **Critical path**: The most fragile link is the synchronization between the Masks and the Refined Feature Maps; if the upsampling in the Refiner misaligns with the Mask resolution, the region features will be garbage.
- **Design tradeoffs**: The model uses pixel shuffling to save parameters, reducing spatial resolution which may explain weaker absolute size estimation performance. Heavy reliance on the OSD dataset for spatial warmup may limit generalization to environments not covered by this data.
- **Failure signatures**: Depth ignoring (check depth loss during Stage 2), catastrophic forgetting (monitor text coherence during Stage 3), overfitting (validation loss increases after epoch 4 in Stage 3).
- **First 3 experiments**: Ablate depth pathway to quantify its specific contribution; verify mask replacement logic by inputting identical masks for different objects; bypass refiners to measure performance drop and validate architectural choices.

## Open Questions the Paper Calls Out

1. Can architectural modifications improve performance on quantitative width and height estimation where the model currently lags behind larger models?

2. Can the external answer normalization pipeline be internalized within the 600M parameter model for fully standalone deployment?

3. How robust is the model's spatial reasoning when deployed in environments with layouts significantly different from the training data?

## Limitations

- Absolute size estimation (width and height) still trails state of the art
- The answer extraction pipeline is external to the model, requiring additional components
- Robustness to domain shift or changing layouts remains untested

## Confidence

- **High Confidence**: The core architectural innovation of separate RGB/depth connectors and refiners is well-supported by ablation studies showing depth contribution to performance.
- **Medium Confidence**: Claims of competitive performance with 13x-57x fewer parameters are supported but comparisons mix different task formulations and evaluation protocols.
- **Low Confidence**: The assertion that mask pooling and token injection is optimal lacks comparative evidence against alternative region-to-token conversion methods.

## Next Checks

1. Conduct systematic ablation studies isolating the contribution of each component in the depth pathway rather than just confirming depth helps overall.

2. Evaluate the model on spatial reasoning datasets outside the warehouse domain to assess curriculum training generalization.

3. Systematically vary input resolution and pixel shuffling parameters to quantify the tradeoff between parameter efficiency and spatial reasoning accuracy, particularly for tasks requiring precise geometric computation.