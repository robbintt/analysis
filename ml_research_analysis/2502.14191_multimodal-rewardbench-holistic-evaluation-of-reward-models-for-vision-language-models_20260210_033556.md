---
ver: rpa2
title: 'Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language
  Models'
arxiv_id: '2502.14191'
source_url: https://arxiv.org/abs/2502.14191
tags:
- response
- reward
- arxiv
- responses
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Multimodal RewardBench, a comprehensive
  benchmark for evaluating reward models in vision-language models (VLMs). The benchmark
  addresses the lack of holistic evaluation frameworks for multimodal reward models
  by providing expert-annotated (prompt, chosen response, rejected response) triplets
  across six domains: general correctness, preference, knowledge, reasoning, safety,
  and visual question-answering.'
---

# Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models

## Quick Facts
- arXiv ID: 2502.14191
- Source URL: https://arxiv.org/abs/2502.14191
- Authors: Michihiro Yasunaga; Luke Zettlemoyer; Marjan Ghazvininejad
- Reference count: 29
- Key outcome: Top models achieve only 72% overall accuracy on this challenging benchmark for VLM reward models.

## Executive Summary
This paper introduces Multimodal RewardBench, a comprehensive benchmark for evaluating reward models in vision-language models (VLMs). The benchmark addresses the lack of holistic evaluation frameworks for multimodal reward models by providing expert-annotated (prompt, chosen response, rejected response) triplets across six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. The dataset comprises 5,211 triplets collected from various VLMs and annotated by domain experts. The authors evaluate multiple VLM judges including proprietary models (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) and open models (Llama 3.2 Vision Instruct, Molmo, Aria, Llava-1.5-13B). The top-performing models achieve only 72% overall accuracy, with significant challenges in reasoning and safety domains.

## Method Summary
The benchmark evaluates VLM-as-a-judge accuracy on ranking response pairs (chosen vs rejected) across 6 domains. The evaluation uses 5,211 expert-annotated triplets from various VLM datasets. The zero-shot prompting approach presents VLMs with a prompt + two candidate responses and asks them to output [[A]] or [[B]] based on which better follows instructions. Accuracy is computed against human-annotated ground truth. The benchmark spans six dimensions: correctness, preference, knowledge, reasoning, safety, and VQA.

## Key Results
- Top-performing models achieve only 72% overall accuracy, indicating significant room for improvement
- Most models struggle on reasoning tasks (both math and coding) and safety tasks (especially toxicity detection)
- Larger performance gaps between models compared to existing VLM benchmarks
- Claude 3.5 Sonnet achieves highest overall accuracy (72%) and excels at VQA (85.6%)
- Open models perform near or below random on bias detection tasks while proprietary models achieve >75% accuracy

## Why This Works (Mechanism)

### Mechanism 1: VLM-as-a-Judge with Zero-Shot Comparative Prompting
- Claim: Reward model evaluation can be framed as binary response comparison using off-the-shelf VLMs.
- Mechanism: Present the VLM with a prompt + two candidate responses (A/B), ask it to output `[[A]]` or `[[B]]` based on which better follows instructions. Random ordering prevents position bias. Accuracy is computed against human-annotated ground truth.
- Core assumption: VLMs' text generation capabilities transfer to judgment tasks without task-specific fine-tuning.
- Evidence anchors: Top models achieve only 72% overall accuracy; zero-shot prompting with standardized template; RewardBench 2 extends similar methodology.

### Mechanism 2: Expert Annotation with Focused Error Taxonomy
- Claim: High inter-annotator agreement requires constraining judgment scope to objectively verifiable errors.
- Mechanism: Annotators evaluate responses on three tasks: R1 correctness, R2 correctness, comparative preference. They focus on major errors only—visual errors, reasoning errors, knowledge errors—rather than subjective quality differences.
- Core assumption: Expert annotators can reliably distinguish "major" from "minor" errors across 30+ domains.
- Evidence anchors: Inter-annotator agreement improved from 0.61 to 0.75 with focused error instructions; 40% of knowledge/reasoning samples discarded due to flawed reasoning.

### Mechanism 3: Stratified Coverage Across Capability Dimensions
- Claim: Benchmark difficulty arises from spanning diverse capability types that may require different underlying skills.
- Mechanism: Construct triplets across 6 dimensions (correctness, preference, knowledge, reasoning, safety, VQA) with different response formats and judgment types.
- Core assumption: Weaknesses in one dimension are partially independent from others.
- Evidence anchors: Performance varies dramatically across categories; Claude achieves 0.856 on VQA but only 0.606 on Safety/Toxicity; domain-specific gaps support dimension-independence.

## Foundational Learning

- Concept: **Reward Models in RLHF**
  - Why needed here: The entire benchmark assumes understanding that reward models provide scalar signals for alignment training. Without this, the evaluation task seems arbitrary.
  - Quick check question: Can you explain why a reward model's ranking accuracy matters for downstream VLM behavior?

- Concept: **Vision-Language Model Architectures**
  - Why needed here: The paper evaluates proprietary models (GPT-4o, Gemini, Claude) and open models (Llama Vision, Molmo, Aria). Understanding their different capabilities helps interpret results.
  - Quick check question: What input modalities do VLMs process, and what output do they produce?

- Concept: **Inter-Annotator Agreement (IAA)**
  - Why needed here: The benchmark's credibility rests on annotation quality. Understanding IAA metrics is essential for judging reliability.
  - Quick check question: Why might unanimous agreement be harder to achieve for "preference" judgments than "correctness" judgments?

## Architecture Onboarding

- Component map: Benchmark Dataset -> Annotation Pipeline -> Evaluation Harness
- Critical path: Source prompts -> Generate responses -> Expert annotation -> Filter for agreement -> Evaluate with standardized prompt -> Report accuracy by dimension
- Design tradeoffs: Expert vs. crowd annotation (quality vs. scalability); VLM-as-judge vs. regression-based RM (practicality vs. potential accuracy); Chain-of-thought instruction increases annotation complexity
- Failure signatures: Models scoring ~0.5 on bias detection may exhibit U-shaped scaling; toxicity detection hovers near 0.5 even for top models; coding evaluation limited to Python/LaTeX tasks
- First 3 experiments: 1) Reproduce baseline with new open VLM; 2) Ablate prompt template for dimension-specific optimization; 3) Analyze error correlations to test dimension independence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do regression/classifier-based VLM reward models compare to VLM-as-a-judge approaches on Multimodal RewardBench?
- Basis in paper: [explicit] "Another limitation is that our benchmark currently evaluates only VLM-as-a-judge approaches... future work will evaluate regression/classifier-based VLM reward models as well."
- Why unresolved: Few regression-based VLM reward models exist publicly to evaluate.
- What evidence would resolve it: Training and evaluating regression-based VLM reward models on Multimodal RewardBench, comparing accuracy across six dimensions to VLM-as-a-judge baselines.

### Open Question 2
- Question: What training interventions can improve VLM judge performance on toxicity detection, where even top models achieve only ~60% accuracy?
- Basis in paper: [explicit] "Most models also struggle with toxicity detection. Even the top-performing model, Claude 3.5 Sonnet, achieves only 0.606 accuracy..."
- Why unresolved: The paper identifies the problem but does not investigate causes or solutions.
- What evidence would resolve it: Ablation studies varying safety-focused training data, comparing toxicity detection accuracy before and after targeted fine-tuning.

### Open Question 3
- Question: Why do open models perform near or below random on bias detection tasks while proprietary models achieve >75% accuracy?
- Basis in paper: [explicit] "For bias detection, many open models score below 0.5... In contrast, Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o perform well, achieving an accuracy of 0.75 or above."
- Why unresolved: The paper observes the gap but does not isolate whether it stems from training data, model scale, or explicit safety alignment procedures.
- What evidence would resolve it: Controlled experiments matching model scale and training data composition between open and proprietary models.

### Open Question 4
- Question: How can the benchmark be extended to cover additional safety dimensions such as prompt refusal, NSFW content, and harmful response identification?
- Basis in paper: [explicit] "As more datasets become available, future work can explore additional safety-related aspects, including prompt refusal, NSFW content detection, and harmful response identification."
- Why unresolved: Current VLM safety datasets are scarce.
- What evidence would resolve it: Constructing and validating new (prompt, chosen, rejected) triplets covering these additional safety dimensions.

## Limitations
- Evaluation scope limited to VLM-as-a-judge methodology, may not generalize to regression-based reward models
- Exclusion of Hateful Memes data due to licensing restricts complete safety evaluation
- Expert annotation at $250/hr creates scalability constraints with 40% of knowledge/reasoning samples discarded
- Coding evaluation limited to Python/LaTeX rendering tasks rather than broader algorithmic reasoning

## Confidence

**High Confidence**: Benchmark construction methodology (5,211 triplets, expert annotation, stratified dimensions) and reported performance gaps between models

**Medium Confidence**: Claim that VLM-as-a-judge is the most practical evaluation approach given lack of open-source alternatives

**Low Confidence**: Assertion that benchmark reveals "larger performance gaps" compared to existing VLM benchmarks

## Next Checks

1. Reproduce the evaluation pipeline with a new open VLM (e.g., Qwen-VL-2.5) to verify reproducibility and establish independent baseline performance

2. Conduct ablation studies on the prompt template to determine whether dimension-specific prompts improve accuracy over the standardized approach

3. Compute inter-annotator agreement metrics on a held-out subset of the dataset to verify the reported 0.75 unanimous agreement rate is representative