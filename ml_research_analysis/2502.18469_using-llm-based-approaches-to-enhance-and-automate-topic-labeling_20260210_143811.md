---
ver: rpa2
title: Using LLM-Based Approaches to Enhance and Automate Topic Labeling
arxiv_id: '2502.18469'
source_url: https://arxiv.org/abs/2502.18469
tags:
- topic
- labels
- documents
- labeling
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) to automatically
  generate meaningful topic labels from topic modeling outputs. The authors first
  use BERTopic to generate topics as keyword lists, then extract document summaries
  and keywords to feed into GPT-3.5-Turbo-Instruct for label generation.
---

# Using LLM-Based Approaches to Enhance and Automate Topic Labeling

## Quick Facts
- arXiv ID: 2502.18469
- Source URL: https://arxiv.org/abs/2502.18469
- Reference count: 6
- Primary result: GPT-3.5-Turbo-Instruct generates meaningful topic labels from BERTopic outputs; Approach 3 (largest subtopic sampling) performs best on BBC News, Approach 2 (TF-IDF similarity) performs best on 20 Newsgroups

## Executive Summary
This paper proposes using large language models to automatically generate meaningful topic labels from topic modeling outputs. The approach combines LLM-generated document summaries with BERTopic keyword extraction, then uses GPT-3.5-Turbo-Instruct to generate 2-5 word labels. Four document selection strategies are explored, with different approaches performing best on datasets with varying topic separability. A novel semantic similarity metric using sentence embeddings provides standardized evaluation, though it remains unvalidated against human judgment.

## Method Summary
The method uses GPT-3.5-Turbo-Instruct to generate 20-40 word summaries of documents, which are then processed by BERTopic to extract topics and top 10 keywords. One of four document selection strategies identifies 10 representative documents per topic, whose summaries are concatenated with topic keywords and fed to GPT-3.5-Turbo-Instruct with a persona-based prompt to generate labels. Performance is evaluated using a novel metric that measures semantic similarity between labels and documents using Sentence-BERT embeddings and cosine similarity.

## Key Results
- Approach 3 (selecting from largest subtopic) yields highest performance for BBC dataset due to well-defined categories
- Approach 2 (TF-IDF-based similarity) performs best for 20 Newsgroups due to high topic overlap making dominant keywords insufficient
- The proposed semantic similarity metric provides standardized evaluation, with higher values indicating better label quality
- Approach 4 (diversity sampling) consistently underperforms across both datasets

## Why This Works (Mechanism)

### Mechanism 1
Pre-summarizing documents with an LLM before topic modeling improves downstream label generation quality. GPT-3.5-Turbo-Instruct generates 20-40 word summaries that capture core themes while reducing noise from document length variation. These summaries provide richer context for BERTopic keyword extraction and subsequent labeling.

### Mechanism 2
Document selection strategy significantly impacts label quality, with optimal strategy depending on dataset structure. Four strategies prioritize different signals—keyword overlap, TF-IDF centrality, dominant subtopic sampling, and diversity sampling. Well-separated topics favor subtopic sampling while overlapping topics require centrality-based selection.

### Mechanism 3
A semantic similarity metric using sentence embeddings provides standardized, quantitative evaluation of label quality. The metric generates embeddings for both labels and documents, calculates pairwise cosine similarity, averages at topic level, then computes weighted average across topics. Higher cosine similarity indicates better label representativeness.

## Foundational Learning

- Concept: BERTtopic topic modeling
  - Why needed here: Core pipeline component that generates keyword lists from document collections. Understanding that BERTopic produces topics as keyword sets (not labels) is essential for grasping why the labeling step matters.
  - Quick check question: Given a corpus of customer reviews, what would BERTopic output for a topic about delivery issues—descriptive keywords or a human-readable label?

- Concept: TF-IDF vectorization and cosine similarity
  - Why needed here: Approach 2 uses TF-IDF representations to compute document-document similarity matrices. Understanding how TF-IDF weights terms and how cosine similarity measures vector alignment is necessary to implement this strategy.
  - Quick check question: Why might a document with high average TF-IDF similarity to all other documents in a topic be a good candidate for representing that topic?

- Concept: Sentence embeddings (Sentence-BERT)
  - Why needed here: Both the evaluation metric and BERTopic's internal mechanism rely on semantic embeddings. Understanding that similar meanings produce similar vector representations is foundational.
  - Quick check question: If two documents have high cosine similarity in Sentence-BERT embedding space, does this guarantee they share the same topic? Why or why not?

## Architecture Onboarding

- Component map: Document ingestion -> GPT-3.5-Turbo-Instruct summarization -> BERTopic clustering -> Document selection (Approach 3 recommended) -> Keyword + summary concatenation -> GPT-3.5-Turbo-Instruct label generation -> Sentence-BERT evaluation

- Critical path: Document ingestion → LLM summarization → BERTopic clustering → document selection (start with Approach 3) → keyword + summary concatenation → LLM label generation → embedding-based evaluation

- Design tradeoffs:
  - Approach 3 (largest subtopic) vs. Approach 2 (TF-IDF centrality): Well-separated topics favor subtopic sampling; overlapping topics favor centrality-based selection
  - Summary length (20-40 words): Shorter summaries may lose nuance; longer summaries may introduce noise and hit context window limits
  - Closed-source LLM dependency: Limited control over summarization/labeling quality; cannot inspect internal reasoning

- Failure signatures:
  - Very short source documents (tweets, headlines): Summarization step degrades performance
  - High topic overlap in corpus: Approach 3 may select documents from shared subthemes, producing generic labels
  - Context window overflow: Large documents or many selected summaries may exceed LLM input limits

- First 3 experiments:
  1. Baseline validation: Run BERTopic on BBC News, implement Approach 3 with GPT-3.5-Turbo-Instruct, compute evaluation metric—verify you can reproduce scores in Table 1 (target: ~0.1200 for BBC with Approach 3).
  2. Dataset characteristic test: Apply all four approaches to a corpus with known topic overlap (e.g., multi-category product reviews). Hypothesis: Approaches 1-2 should outperform Approach 3 if categories blur together.
  3. Metric sanity check: Generate labels using Approach 3, then have 2-3 humans rate label quality on a 1-5 scale for 20 topics. Compute correlation between human ratings and the proposed metric to assess alignment.

## Open Questions the Paper Calls Out

- Does the proposed semantic representativeness metric correlate with human judgments of topic label quality? The novel metric using sentence embeddings and cosine similarity was only evaluated automatically; no human evaluation was conducted to validate alignment with perceived label quality.

- How well does the proposed approach generalize to non-news document domains? Both experimental datasets (BBC News, 20 Newsgroups) are news article collections with relatively structured writing; the approach may perform differently on scientific papers, social media, or technical documentation.

- Can dataset characteristics predict which document selection strategy will perform best before experimentation? The authors observe that Approach 3 works best for BBC (well-defined categories) while Approach 2 works best for 20 Newsgroups (overlapping categories), but provide no principled framework for approach selection.

## Limitations

- Exact prompts for summarization and label generation are unspecified, impacting reproducibility
- The proposed evaluation metric remains unvalidated against human judgment despite being central to results
- Dataset size discrepancy exists between paper description and actual 20 Newsgroups dataset size used

## Confidence

- High confidence: The core pipeline architecture is clearly specified and technically sound. The claim that document selection strategy impacts label quality is well-supported by experimental results.
- Medium confidence: The performance claims for Approach 3 on BBC News and Approach 2 on 20 Newsgroups are supported by the reported metrics, but exact reproducibility is uncertain due to unspecified prompts and hyperparameters.
- Low confidence: The effectiveness of the proposed evaluation metric as a proxy for human judgment—while theoretically sound—lacks empirical validation.

## Next Checks

1. **Human validation study**: Recruit 3-5 annotators to rate topic label quality (1-5 scale) for 20 topics generated using Approach 3. Compute correlation between human ratings and the proposed metric to assess alignment.

2. **Prompt sensitivity analysis**: Test 3-4 variations of the summarization and labeling prompts (varying length constraints, persona descriptions, instruction specificity). Compare metric scores to determine prompt robustness.

3. **Cross-dataset generalizability test**: Apply all four approaches to a third dataset with different characteristics (e.g., short documents like tweets, or multi-lingual corpus). Verify whether the BBC/20 Newsgroups pattern (Approach 3 for well-separated topics, Approach 2 for overlapping topics) holds.