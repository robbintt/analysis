---
ver: rpa2
title: 'LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization'
arxiv_id: '2506.16738'
source_url: https://arxiv.org/abs/2506.16738
tags:
- semantic
- speech
- lm-spt
- mimi
- speechtokenizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LM-SPT, a speech tokenization method that
  improves semantic alignment with language models through a novel reconstruction-driven
  distillation approach. Instead of directly matching features from a self-supervised
  learning teacher, LM-SPT uses a pretrained ASR encoder to supervise the reconstruction
  of speech from semantic tokens, enabling better alignment even at low frame rates.
---

# LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization

## Quick Facts
- **arXiv ID**: 2506.16738
- **Source URL**: https://arxiv.org/abs/2506.16738
- **Reference count**: 40
- **Primary result**: Introduces LM-SPT, a speech tokenization method that improves semantic alignment with language models through a novel reconstruction-driven distillation approach, achieving superior reconstruction fidelity and TTS performance while maintaining competitive STT results.

## Executive Summary
This paper introduces LM-SPT, a speech tokenization method that improves semantic alignment with language models through a novel reconstruction-driven distillation approach. Instead of directly matching features from a self-supervised learning teacher, LM-SPT uses a pretrained ASR encoder to supervise the reconstruction of speech from semantic tokens, enabling better alignment even at low frame rates. The method incorporates a split RVQ architecture with dual semantic and acoustic encoders, and supports multiple frame rates (25Hz, 12.5Hz, 6.25Hz). Experimental results show that LM-SPT achieves superior reconstruction fidelity compared to baselines and consistently outperforms prior methods on text-to-speech tasks, while maintaining competitive performance on speech-to-text.

## Method Summary
LM-SPT introduces a novel semantic distillation approach for speech tokenization that improves alignment with language models. The method uses a dual-encoder architecture (semantic and acoustic streams) with split RVQ quantization (1 semantic VQ + 7 acoustic RVQs). Instead of directly matching teacher features, it reconstructs speech from semantic tokens using a lightweight auxiliary decoder and minimizes the MSE between the Whisper encoder's outputs for the original and reconstructed audio. This reconstruction-driven approach enables better semantic alignment even at low frame rates (6.25Hz, 12.5Hz, 25Hz). The system is trained with a combination of distillation loss, codec losses (L1/L2 Mel, adversarial, commitment), and optimized using AdamW. Extensive experiments on LibriSpeech demonstrate superior reconstruction quality and TTS performance compared to prior methods.

## Key Results
- LM-SPT achieves superior reconstruction fidelity (UTMOS, DNSMOS) compared to baselines at all tested frame rates
- Consistently outperforms prior methods on text-to-speech tasks across different LM sizes (Qwen2.5-0.5B, LLaMA3.2-3B)
- Maintains competitive speech-to-text performance while enabling 6.25Hz operation
- Large-scale pretraining (60K hours) provides additional quality gains beyond LibriSpeech

## Why This Works (Mechanism)
The key innovation is reconstruction-driven semantic distillation. By forcing the tokenizer to generate semantic tokens that can reconstruct speech accurately enough for the Whisper encoder to produce matching features, LM-SPT creates a stronger semantic alignment than direct feature matching. This approach is particularly effective at low frame rates because it ensures semantic tokens retain enough information for both reconstruction and ASR compatibility. The split RVQ architecture allows the system to separately optimize semantic and acoustic information, with the semantic branch focusing on high-level linguistic content while the acoustic branch handles fine-grained details.

## Foundational Learning
- **Split RVQ quantization**: Separates semantic and acoustic information into different codebooks, allowing specialized optimization for each. Needed because pure semantic tokens lose acoustic details while pure acoustic tokens lack linguistic structure. Quick check: Verify codebook sizes (1 semantic + 7 acoustic) match Table 1 specifications.
- **Dual-encoder architecture**: Uses separate semantic and acoustic encoders to process different aspects of speech. Needed to balance semantic alignment with reconstruction fidelity. Quick check: Confirm both encoders have identical architecture but separate parameters.
- **Reconstruction-driven distillation**: Uses a lightweight decoder to reconstruct speech from semantic tokens, then matches Whisper encoder features. Needed because direct feature matching fails at low frame rates. Quick check: Ensure auxiliary decoder is not frozen during training.
- **Whisper encoder supervision**: Uses pretrained ASR encoder as feature extractor for distillation loss. Needed to ensure semantic tokens are compatible with downstream ASR systems. Quick check: Verify Whisper encoder remains frozen during training.
- **Striding factors for frame rates**: Uses specific conv block strides ([8,5,4,2] for 25Hz, [8,8,5,4] for 6.25Hz) to achieve target frame rates. Needed to control temporal resolution while maintaining information density. Quick check: Confirm output frame rates match target specifications.

## Architecture Onboarding

**Component map**: Audio -> Dual Encoder (Semantic + Acoustic) -> Split RVQ -> Main Decoder + Auxiliary Decoder -> Reconstructed Audio; Whisper Encoder (frozen) extracts features for distillation loss

**Critical path**: Audio → Semantic Encoder → Semantic VQ → Auxiliary Decoder → Reconstructed Audio → Whisper Encoder → Feature Extraction → MSE Loss

**Design tradeoffs**: The lightweight auxiliary decoder enables efficient distillation but must balance capacity with speed. Split RVQ allows specialized optimization but increases codebook management complexity. Multiple frame rates require careful striding configuration.

**Failure signatures**: High WER and low UTMOS at 6.25Hz indicate incorrect striding factors or insufficient semantic information. NaN distillation loss suggests decoder instability or learning rate issues.

**3 first experiments**:
1. Implement dual-encoder with correct striding factors and verify output frame rates
2. Test semantic distillation with frozen vs. unfrozen auxiliary decoder to confirm impact
3. Compare reconstruction quality with and without split RVQ architecture

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design relies on LM-agnostic metrics that may not fully capture semantic alignment benefits for downstream LM integration
- ASR WER evaluation limited to single Whisper model without cross-system ablation
- TTS evaluation uses fixed LM sizes without exploring scaling relationships
- Auxiliary decoder architecture underspecified, making exact replication challenging

## Confidence
- **High confidence**: Reconstruction fidelity improvements at 6.25Hz and 12.5Hz; split RVQ contribution; semantic distillation outperforming direct feature matching
- **Medium confidence**: ASR performance parity claims; TTS quality improvements with Qwen2.5-0.5B; scaling benefit from 60K-hour pretraining
- **Low confidence**: Claims about semantic alignment benefiting arbitrary LMs without LM-specific evaluation; robustness to domain shift beyond LibriSpeech

## Next Checks
1. Implement and validate the auxiliary decoder architecture with exact striding factors from Table 10 for each frame rate
2. Conduct ablation studies comparing semantic distillation vs. direct feature matching across different Whisper encoder layers
3. Evaluate semantic token quality by fine-tuning a small LM on the discrete tokens and measuring perplexity vs. baseline tokenizers