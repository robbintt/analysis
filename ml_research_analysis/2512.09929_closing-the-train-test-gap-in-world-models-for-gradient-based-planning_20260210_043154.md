---
ver: rpa2
title: Closing the Train-Test Gap in World Models for Gradient-Based Planning
arxiv_id: '2512.09929'
source_url: https://arxiv.org/abs/2512.09929
tags:
- world
- planning
- adversarial
- gradient-based
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the poor performance of gradient-based planning
  (GBP) with world models, which stems from a train-test gap: world models are trained
  for next-state prediction but used at test time to optimize action sequences. The
  authors propose two methods to close this gap: Online World Modeling, which finetunes
  the model on corrected planning trajectories using simulator feedback, and Adversarial
  World Modeling, which trains the model on adversarially perturbed trajectories to
  smooth the optimization landscape.'
---

# Closing the Train-Test Gap in World Models for Gradient-Based Planning

## Quick Facts
- **arXiv ID**: 2512.09929
- **Source URL**: https://arxiv.org/abs/2512.09929
- **Reference count**: 40
- **Primary result**: Adversarial World Modeling enables gradient-based planning to match or exceed CEM performance while requiring only 10% of the computation time.

## Executive Summary
This paper addresses a fundamental limitation of gradient-based planning (GBP) with world models: the train-test gap where models trained for next-state prediction perform poorly when used to optimize action sequences. The authors identify that world models encounter out-of-distribution states during GBP, causing compounding prediction errors. They propose two complementary methods to close this gap: Online World Modeling, which finetunes the model on corrected planning trajectories using simulator feedback, and Adversarial World Modeling, which trains the model on adversarially perturbed trajectories to smooth the optimization landscape. Experiments on three tasks show that Adversarial World Modeling enables GBP to match or exceed the performance of the cross-entropy method (CEM) while requiring only 10% of the computation time.

## Method Summary
The paper proposes two methods to close the train-test gap in world models for gradient-based planning. Online World Modeling iteratively corrects trajectories produced by GBP using simulator feedback and finetunes the world model on these corrected trajectories, expanding the training distribution to cover planning-induced states. Adversarial World Modeling uses FGSM-style perturbations on latent states (λ_z ≤ 0.5) and actions (λ_a ≤ 1.0) to train the model on adversarially perturbed trajectories, smoothing the induced action loss landscape for better gradient-based optimization. The base model is DINO-WM with frozen DINOv2 encoder and ViT transition model, finetuned using AdamW (LR 1e-5, 1-2 epochs). At test time, GBP uses Adam optimizer with weighted goal loss (exponential weighting to intermediate states) over 300 optimization steps for open-loop or 100 steps × 10 iterations for MPC planning.

## Key Results
- Adversarial World Modeling enables GBP to match or exceed CEM performance while requiring only 10% of the computation time
- Both OWM and AWM methods significantly reduce world model error on planning trajectories compared to the pretrained baseline
- GBP with AWM achieves success rates comparable to CEM across PushT (78%), PointMaze (90%), and Wall (74%) tasks
- The train-test gap (higher error on planning vs expert trajectories) is reversed after AWM finetuning

## Why This Works (Mechanism)

### Mechanism 1: Distribution Shift Between Training and Planning Trajectories
- Claim: World models trained on expert trajectories encounter out-of-distribution (OOD) states during gradient-based planning, causing compounding prediction errors.
- Mechanism: The paper hypothesizes that during GBP, "the intermediate sequence of actions explored by gradient descent drive the world model into states that were not encountered during training" (Page 2, Column 2). These OOD states cause model errors to accumulate over the planning horizon, degrading long-horizon performance.
- Core assumption: The expert trajectory distribution does not cover the action sequences that gradient descent explores during optimization.
- Evidence anchors:
  - [abstract] "world models are trained for next-state prediction but used at test time to optimize action sequences"
  - [section 2.2] "Optimizing through learned models under such conditions is known to induce adversarial inputs"
  - [corpus] Weak direct evidence; related work on distribution shift exists but not specifically for GBP with latent world models.
- Break condition: If expert trajectories already cover the planning trajectory distribution, the gap would be minimal.

### Mechanism 2: Online World Modeling via Simulator-Corrected Trajectories
- Claim: Finetuning on corrected trajectories expands the training distribution to cover planning-induced states.
- Mechanism: Online World Modeling "iteratively corrects the trajectories produced by GBP and finetunes the world model on the resulting rollouts" (Page 4, Algorithm 2). By executing planned actions in the simulator and training on the resulting state transitions, the model learns to predict in regions it will encounter during planning.
- Core assumption: Access to a simulator for generating ground-truth state transitions during training.
- Evidence anchors:
  - [abstract] "Online World Modeling, which finetunes the model on corrected planning trajectories using simulator feedback"
  - [section 2.2] "Re-training on these corrected trajectories expands the training distribution to cover the regions of latent space induced by gradient-based planning"
  - [corpus] DAgger (Ross et al., 2011) provides precedent for this approach in imitation learning.
- Break condition: If simulation is expensive or infeasible, this mechanism cannot operate.

### Mechanism 3: Landscape Smoothing via Adversarial Training
- Claim: Adversarial training produces smoother optimization landscapes, improving gradient-based optimization stability.
- Mechanism: Adversarial World Modeling trains on perturbations that maximize prediction error. The paper states: "Adversarial finetuning smooths the induced action loss landscape, making it easier to optimize via gradient-based planning" (Page 5). Figure 2 visualizes this smoothing effect.
- Core assumption: FGSM-style perturbations generalize to the types of state deviations encountered during planning.
- Evidence anchors:
  - [abstract] "Adversarial World Modeling... trains the model on adversarially perturbed trajectories to smooth the optimization landscape"
  - [section 2.3] "Adversarial training has been shown to result in better behaved input gradients"
  - [corpus] Prior work (Mejia et al., 2019; Madry et al., 2018) links adversarial training to gradient smoothness.
- Break condition: If the perturbation radius is too large, semantic content may be distorted (Section D.2).

## Foundational Learning

- Concept: **World Models (Latent Dynamics Models)**
  - Why needed here: The entire approach builds on learning f_θ: Z × A → Z that predicts next latent state from current latent state and action.
  - Quick check question: Can you explain why the encoder Φ_μ is kept frozen while only f_θ is finetuned?

- Concept: **Gradient-Based Planning (GBP)**
  - Why needed here: The paper specifically addresses GBP failure modes; understanding how gradients flow through the rollout is essential.
  - Quick check question: How does ∇_{â_t} ẑ_{H+1} = ∇_{â_t} rollout_f(z_1, {â_t}) enable end-to-end action optimization?

- Concept: **Model Predictive Control (MPC)**
  - Why needed here: The paper evaluates both open-loop and MPC (receding-horizon) planning; MPC replans after executing K ≤ H actions.
  - Quick check question: Why might MPC be more robust than open-loop planning when world model errors exist?

## Architecture Onboarding

- Component map:
  - Encoder (Φ_μ): DINOv2 (frozen) maps observations to latent space Z
  - Transition Model (f_θ): ViT-based predictor, finetuned via OWM or AWM
  - Planning Module: Implements Algorithm 1 (GBP) using Adam or GD optimizers
  - Decoder (VQ-VAE): Optional, for visualization only—not used in planning

- Critical path:
  1. Pretrain f_θ on offline expert trajectories using next-state prediction loss (Equation 3)
  2. Apply Adversarial World Modeling (Algorithm 3) with FGSM perturbations
  3. Run GBP (Algorithm 1) at test time with weighted goal loss (Equation 9)

- Design tradeoffs:
  - **Online vs. Adversarial WM**: Online requires simulator access; Adversarial is simulator-free but may not cover planning distribution as precisely.
  - **FGSM vs. PGD**: FGSM is 2× backprop cost; PGD (K-step) requires (K+1)× but shows no consistent improvement (Table 11).
  - **Perturbation radii**: λ_z > 0.5 degrades performance; 0 ≤ λ_z ≤ 0.5 is stable (Figure 9).

- Failure signatures:
  - **Decoder reconstruction failure**: Visual artifacts indicate OOD latent states (Figure 10a, DINO-WM column).
  - **Negative train-test error gap**: If planning trajectory error exceeds expert trajectory error, the model is unreliable for planning (Figure 4).
  - **Gradient magnitude collapse**: Flat loss landscapes cause GBP to stall.

- First 3 experiments:
  1. Reproduce baseline GBP failure: Train DINO-WM on PushT, run GBP with GD, verify low success rate (Table 1, DINO-WM row).
  2. Apply AWM finetuning: Use Algorithm 3 with λ_a = 0.05, λ_z = 0.02, compare success rate improvement.
  3. Measure train-test gap: Compute world model error on expert vs. planning trajectories (Equation 8), confirm gap reversal after AWM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can these world model finetuning methods be effectively deployed on real-world robotic systems without relying on high-fidelity simulator feedback?
- Basis in paper: [explicit] The conclusion states, "An important direction for future work is to evaluate these methods on real-world systems."
- Why unresolved: The experiments are conducted entirely in simulation (PushT, PointMaze, Wall), where Online World Modeling has access to exact ground-truth dynamics for trajectory correction.
- What evidence would resolve it: Demonstration of successful planning on physical hardware (e.g., a robot arm) using these methods, particularly comparing the sim-to-real transfer performance of Online versus Adversarial World Modeling.

### Open Question 2
- Question: Does the adversarial training regime improve the robustness of gradient-based planning to environmental stochasticity or adversarial dynamics?
- Basis in paper: [explicit] The conclusion hypothesizes that "Adversarial training may additionally improve a world model’s robustness to environmental adversaries or stochasticity."
- Why unresolved: The evaluated tasks are deterministic, and the paper does not measure performance under conditions where transition dynamics are noisy or subject to external perturbations.
- What evidence would resolve it: Empirical evaluation of success rates in environments with stochastic transition noise or adversarial physics parameter changes (e.g., sudden mass/friction shifts).

### Open Question 3
- Question: What is the optimal method for selecting the weight sequence in the Weighted Goal Loss (WGL) for different planning horizons?
- Basis in paper: [explicit] In Section A.4, regarding the weighting of intermediate states, the authors state: "We leave the optimal selection of this sequence of weights as future work."
- Why unresolved: The paper manually tunes weights (using either $2^i$ or $1/2^i$) based on empirical observation for specific tasks, but lacks a general rule or adaptive mechanism.
- What evidence would resolve it: An ablation study correlating weight profiles with task properties (e.g., control frequency, horizon length) or an algorithm that adapts weights automatically during training or planning.

## Limitations

- The evaluation focuses on moderate-complexity tasks with 25-step horizons; performance on longer horizons or more complex tasks remains unclear
- The mechanism of landscape smoothing via adversarial training is theoretically supported but empirical evidence is limited to visual inspection of loss landscapes
- The claim that adversarial perturbations "generalize" to planning-induced states is supported by performance improvements but lacks quantitative validation of which specific state regions are smoothed

## Confidence

- **High**: The train-test gap problem is well-defined and the empirical improvements from both OWM and AWM are clearly demonstrated across three tasks with multiple metrics (success rate, error reduction, computation time)
- **Medium**: The mechanism of landscape smoothing via adversarial training is theoretically plausible and supported by prior work, but the specific claim about generalization to planning states needs more direct validation
- **Medium**: The comparison to CEM shows computational advantages, but the evaluation is limited to specific task configurations without exploring parameter sensitivity or failure modes of GBP vs CEM

## Next Checks

1. **Train-test gap quantification**: Systematically measure world model error on expert vs planning trajectories before and after AWM/OWM finetuning, computing the error gap (Equation 8) across different perturbation radii and task difficulties

2. **Landscape visualization validation**: Generate quantitative metrics of optimization landscape properties (eigenvalues of Hessian, gradient variance) to verify that AWM produces smoother landscapes, not just visual inspection

3. **Cross-task perturbation transfer**: Test whether perturbation radii learned on one task (e.g., PushT) transfer to similar tasks, or if task-specific tuning is required, to validate the claim that FGSM perturbations "generalize" to planning states