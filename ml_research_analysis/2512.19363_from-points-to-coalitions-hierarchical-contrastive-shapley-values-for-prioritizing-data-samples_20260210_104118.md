---
ver: rpa2
title: 'From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing
  Data Samples'
arxiv_id: '2512.19363'
source_url: https://arxiv.org/abs/2512.19363
tags:
- data
- valuation
- shapley
- hcdv
- leaf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the computational challenge of applying Shapley
  value theory to large-scale data valuation, which traditionally suffers from factorial
  complexity. HCDV introduces a three-stage framework: learning contrastive geometry-preserving
  embeddings, recursively clustering data into a balanced hierarchy, and computing
  Shapley-style payoffs for coalitions via local Monte-Carlo games with budget propagation.'
---

# From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples

## Quick Facts
- arXiv ID: 2512.19363
- Source URL: https://arxiv.org/abs/2512.19363
- Authors: Canran Xiao; Jiabao Dou; Zhiming Lin; Zong Ke; Liwei Hou
- Reference count: 33
- Primary result: Reduces Shapley computation from factorial to logarithmic complexity while improving predictive accuracy by up to 5 percentage points and valuation speed by up to 100x.

## Executive Summary
This paper addresses the computational challenge of applying Shapley value theory to large-scale data valuation, which traditionally suffers from factorial complexity. HCDV introduces a three-stage framework: learning contrastive geometry-preserving embeddings, recursively clustering data into a balanced hierarchy, and computing Shapley-style payoffs for coalitions via local Monte-Carlo games with budget propagation. Theoretically, HCDV approximates Shapley axioms with logarithmic surplus loss, enjoys sub-Gaussian concentration, and guarantees bounded regret for top-k selection. Empirically, HCDV improves predictive accuracy by up to 5 percentage points and reduces valuation time by up to 100x compared to state-of-the-art methods across tabular, vision, streaming, and large-scale CTR tasks, while supporting downstream applications like augmentation filtering, streaming updates, and fair data marketplace allocation.

## Method Summary
HCDV operates in three stages: (1) train a contrastive encoder to learn geometry-preserving embeddings where cross-label pairs are pushed apart; (2) recursively cluster data into a balanced hierarchy using k-means; (3) compute local Shapley games at each level with Monte-Carlo sampling and budget propagation from parent to children. The method approximates the full n-player Shapley game by restricting combinatorial search to K_l clusters per level, yielding O(T·K_max·log n) complexity versus O(T·n) for flat Monte-Carlo Shapley. The characteristic function combines base utility M(S) with a contrastive dispersion term Δ_c(S) that rewards samples increasing inter-class separation.

## Key Results
- Achieves up to 5 percentage points improvement in predictive accuracy compared to MCDS and SHAP-RL across multiple benchmarks
- Reduces valuation time by up to 100x compared to state-of-the-art methods
- Demonstrates effective performance on diverse tasks: tabular data (UCI Adult), vision (Fashion-MNIST), streaming data, and large-scale CTR prediction (Criteo-1B)
- Successfully applies to downstream tasks including augmentation filtering, streaming updates, and fair data marketplace allocation

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition reduces Shapley computation from factorial to logarithmic complexity by recursively clustering data and confining combinatorial search to K_l clusters per level. Monte-Carlo sampling with T permutations yields O(T·K_max·log n) total cost versus O(T·n) flat MC-Shapley. Core assumption: cluster structure approximates the true data manifold and within-cluster interactions dominate cross-cluster marginal contributions.

### Mechanism 2
Contrastive dispersion rewards samples that sharpen class boundaries in representation space. The characteristic function v_l(S) = M(S) + λ·Δ_c(S) adds normalized cross-label embedding distance, making high-value samples those that increase inter-class separation when included. Core assumption: geometric separation correlates with downstream generalization and the encoder preserves discriminative structure.

### Mechanism 3
Budget down-propagation preserves global efficiency with bounded approximation error by distributing parent coalition value to children via normalized weights. Children then play their own local Shapley game, capturing fine-grained interactions invisible at coarser levels. Core assumption: hierarchical game decomposition approximates the full n-player game and local marginal contributions are sufficient statistics.

## Foundational Learning

- **Shapley Value Axioms (Efficiency, Symmetry, Dummy, Additivity)**: Why needed: HCDV approximates these axioms; understanding them is required to interpret theoretical guarantees. Quick check: If two data points i and j contribute identically to all subsets, what should their Shapley values satisfy?
- **Contrastive Representation Learning**: Why needed: Stage I learns embeddings where cross-label pairs are pushed apart; this geometric signal directly influences valuation via Δ_c(S). Quick check: How does the contrastive dispersion term differ from standard supervised loss?
- **Monte-Carlo Concentration (Hoeffding's Inequality)**: Why needed: Proposition 1 establishes sub-Gaussian deviation for coalition Shapley estimates; this underpins sample complexity analysis. Quick check: What does the bound O_P(B·√(log K_l / T)) imply for choosing T?

## Architecture Onboarding

- **Component map**: Encoder Training -> Hierarchy Construction -> Local Shapley Games with Budget Propagation
- **Critical path**: Train encoder on full dataset → Build balanced k-means hierarchy → Compute root surplus B_0 → For each level l=0..L: estimate ψ̂_G via T permutations; propagate budgets to children → At leaves: exact Shapley or uniform split
- **Design tradeoffs**: T (permutations per level) - higher T reduces ε_MC but increases cost linearly; M (leaf size) - smaller M improves point-level precision but increases leaf-level exact computation; K_l (branching factor) - wider trees reduce depth L but increase per-level MC cost
- **Failure signatures**: Unbalanced clusters - some leaves exceed M significantly → ε_leaf dominates; Encoder collapse - low Δ_c across all subsets → contrastive term contributes little; Insufficient T - high ε_MC manifests as unstable valuations across runs
- **First 3 experiments**: 1) Sanity check on synthetic data - generate 2-class Gaussian blobs with known cluster structure; verify HCDV recovers ground-truth important points faster than MCDS; 2) Ablation on T and M - sweep T∈{64,128,256,512} and M∈{32,64,128,256}; plot valuation stability vs. runtime; 3) Streaming stress test - simulate batch arrivals with concept drift; compare HCDV-INC vs. HCDV-FULL on latency and final AUC degradation

## Open Questions the Paper Calls Out

### Open Question 1
How can HCDV be adapted for federated learning environments where data is distributed across silos and cannot be centralized for the initial contrastive embedding and hierarchy construction? The conclusion explicitly lists "federated valuation" as a primary direction for future work. The current HCDV framework is centralized, requiring access to the full dataset in Stage I to train the geometry-preserving encoder and in Stage II to build the balanced hierarchy.

### Open Question 2
Does the reliance on a fixed encoder in the streaming setting cause performance degradation under severe concept drift? The streaming protocol fixes the encoder learned on the original data, assigning new points to existing leaves based on cosine distance. If the data distribution shifts significantly, the frozen geometry may no longer represent the new data manifold, potentially leading to incorrect leaf assignments and misvaluation.

### Open Question 3
To what extent do errors or biases in the initial contrastive representation propagate to the final valuations, and is the method robust to encoder failure? The theoretical analysis bounds the error based on Monte-Carlo samples and hierarchy depth, but assumes the embedding and clustering effectively capture the data geometry. If the Stage I encoder fails to separate classes, the resulting hierarchy may group disparate points, rendering the "contrastive" characteristic function uninformative.

## Limitations
- Encoder-dependent performance: HCDV's accuracy hinges on the quality of learned embeddings, but no ablation or sensitivity analysis is provided for encoder architectures or training hyperparameters
- Approximation error accumulation: Hierarchical decomposition assumes local Shapley games approximate global interactions, but real-world data with complex inter-sample dependencies may violate these assumptions
- Scalability vs. precision trade-offs: While achieving O(T·K_max·log n) complexity, the constants T and K_max can be large, potentially limiting scaling to billion-scale datasets with constrained compute

## Confidence
- **High Confidence**: Claims about computational complexity reduction and Monte-Carlo concentration bounds are mathematically derived and internally consistent
- **Medium Confidence**: Empirical improvements are reported across four benchmarks, but replication details (encoder specs, λ, α tuning) are sparse, limiting independent verification
- **Low Confidence**: Claims about fair data marketplace allocation and robust streaming updates are demonstrated on synthetic or limited datasets; real-world validation is absent

## Next Checks
1. **Encoder Ablation**: Systematically vary encoder architectures (MLP → ResNet) and training hyperparameters; measure downstream valuation stability and predictive utility
2. **Error Propagation Analysis**: On synthetic data with known ground-truth Shapley values, quantify how ε_MC and ε_leaf accumulate across hierarchy levels
3. **Large-Scale Stress Test**: Deploy HCDV on a 100M+ sample tabular dataset (e.g., Click Logs) with constrained compute (single GPU); report runtime, memory, and valuation quality