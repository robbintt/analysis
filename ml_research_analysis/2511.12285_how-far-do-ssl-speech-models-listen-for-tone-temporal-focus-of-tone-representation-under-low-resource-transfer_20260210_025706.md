---
ver: rpa2
title: How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation
  under Low-resource Transfer
arxiv_id: '2511.12285'
source_url: https://arxiv.org/abs/2511.12285
tags:
- tone
- speech
- languages
- temporal
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how self-supervised learning (SSL) speech
  models encode lexical tone in low-resource languages, focusing on Burmese, Thai,
  Lao, and Vietnamese. The authors first establish that optimal temporal spans for
  tone classification are ~100 ms for Burmese and Thai, and ~180 ms for Lao and Vietnamese,
  using logistic regression on acoustic features.
---

# How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer

## Quick Facts
- **arXiv ID:** 2511.12285
- **Source URL:** https://arxiv.org/abs/2511.12285
- **Reference count:** 0
- **Key outcome:** ASR fine-tuning aligns SSL temporal spans with language-specific tone cues, while prosody/voice tasks produce overly broad spans and weaker tone encoding.

## Executive Summary
This study investigates how self-supervised learning (SSL) speech models encode lexical tone in low-resource languages. The authors establish optimal temporal spans for tone classification (100 ms for Burmese/Thai, 180 ms for Lao/Vietnamese) using logistic regression on acoustic features. They then analyze SSL models via layer-wise probes and gradient-based sensitivity to measure temporal focus. Results show that fine-tuning on automatic speech recognition (ASR) in the target language yields the best tone representation, with effective spans closely matching the language-specific baselines. In contrast, fine-tuning for prosody- or voice-related tasks produces overly broad spans and weaker tone encoding.

## Method Summary
The authors establish language-specific optimal temporal spans for tone classification using logistic regression on log-Mel features (20-300 ms windows). They then fine-tune SSL models (wav2vec 2.0, XLS-R, MMS, mHuBERT) on various downstream tasks (ASR, emotion, gender, speaker verification) in the target language. For each fine-tuned model, they extract frozen hidden states at each layer and train linear probes for tone classification. They also compute input gradients with respect to tone-class logits to derive center-of-mass radius as a quantitative proxy for temporal focus.

## Key Results
- ASR fine-tuning aligns SSL temporal receptive fields with language-specific tone spans (100 ms for Burmese/Thai, 180 ms for Lao/Vietnamese)
- Optimal tone representation occurs in mid-to-high encoder layers (12-24 in XLS-R), not early layers
- Prosody/voice fine-tuning tasks produce overly broad temporal spans and weaker tone encoding compared to ASR

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ASR fine-tuning aligns SSL temporal receptive fields with language-specific tone spans, while prosody/voice tasks produce overly broad spans.
- **Mechanism:** ASR objectives require segmental discrimination (distinguishing phonemes/tones), forcing the model to sharpen temporal focus to the minimal span needed for lexical contrast. Prosody tasks (emotion, speaker ID) operate over longer durations, pushing gradients toward wider temporal windows that dilute tone-specific sensitivity.
- **Core assumption:** The downstream task loss shapes gradient flow during fine-tuning, which in turn reorganizes temporal sensitivity in encoder representations.
- **Evidence anchors:**
  - [abstract] "automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans"
  - [section 4.3] "Target-language ASR fine-tuning yields the most stable localization: in higher layers, spans match the language-specific baselines... prosody- and voice-related fine-tuning pushes spans overly long"
  - [corpus] SITA paper (arxiv:2601.09050) confirms tone-aware representations require explicit invariance to speaker/voice factors, supporting the task-dependence finding.

### Mechanism 2
- **Claim:** Tone information is most accessible in mid-to-high encoder layers (layers 12-24 in XLS-R), not in early layers.
- **Mechanism:** Early layers encode low-level acoustic features (spectral edges, short-term energy) that are speaker-dependent and insufficient for abstract tone categories. Higher layers integrate information over longer contexts and build more invariant representations suitable for lexical tone classification.
- **Core assumption:** Layer-wise functional specialization emerges from SSL pre-training, with suprasegmental abstractions forming deeper in the network.
- **Evidence anchors:**
  - [section 4.3] "Probe performance usually peaks in mid-to-high layers, showing that lexical tone information is mainly captured at those layers"
  - [section 4.3] "early layers lack such concentration, explaining weaker tone probe results in 0-11"
  - [corpus] Tone recognition in NE India languages paper (arxiv:2506.03606) similarly reports layer-wise analysis showing optimal tone performance at specific layers.

### Mechanism 3
- **Claim:** Gradient-based center-of-mass radius (r_com) provides a quantitative proxy for the model's effective temporal span for tone processing.
- **Mechanism:** Input gradients with respect to the tone-class logit reveal which input frames most influence the classification decision. Squared gradient energy, binned by temporal offset from tone center, forms a histogram whose center-of-mass indicates average temporal focus—smaller radius = sharper localization.
- **Core assumption:** Gradient magnitude correlates with functional importance; frames with higher gradient energy are more causally involved in the tone decision.
- **Evidence anchors:**
  - [section 3.2] Equations (1) and (2) define gradient energy computation and center-of-mass radius derivation
  - [section 4.2] "Burmese and Thai show sharp concentration around tone centers, indicating narrow focus, whereas Lao and Vietnamese display a bit broader spreads"
  - [corpus] No direct corpus validation of this specific gradient method; assumes transferability from interpretability literature.

## Foundational Learning

- **Concept: Lexical Tone as Suprasegmental Feature**
  - **Why needed here:** Understanding that tone is a pitch-based contrast that unfolds over time (not instantaneous like a phoneme burst) explains why temporal span matters.
  - **Quick check question:** Can a 20 ms window reliably distinguish Mandarin tone 2 (rising) from tone 4 (falling)? (Answer: No—pitch trajectory requires longer observation.)

- **Concept: SSL Pre-training vs. Fine-tuning Transfer**
  - **Why needed here:** The paper's core claim depends on understanding how representations learned during self-supervised pre-training are reshaped by downstream task objectives.
  - **Quick check question:** If a model is pre-trained on English only, will it encode Mandarin tone patterns? (Answer: Unlikely—English lacks lexical tone, so tone-specific representations must emerge from transfer.)

- **Concept: Layer-wise Probing**
  - **Why needed here:** The methodology depends on training separate classifiers at each layer to identify where tone information is most accessible.
  - **Quick check question:** Why freeze the encoder weights when probing? (Answer: To measure what the representation already contains, not what the probe can learn through end-to-end adaptation.)

## Architecture Onboarding

- **Component map:**
  Audio (16kHz) -> Log-Mel (baseline) -> Logistic Regression (span estimation)
  ↓
  SSL Encoder (wav2vec2/XLS-R/MMS/mHuBERT)
  ↓ (layer-wise hidden states)
  Linear Probe (frozen encoder) -> Tone Classification
  ↓ (gradient backprop to input)
  Gradient Energy Histogram -> Center-of-Mass Radius

- **Critical path:**
  1. Establish language-specific baseline spans using logistic regression on log-Mel windows (20-300 ms).
  2. Fine-tune SSL model on target task (ASR recommended; prosody tasks as ablation).
  3. Extract frozen hidden states at each layer for tone-aligned segments.
  4. Train linear probes per layer; record macro-F1.
  5. Compute input gradients w.r.t. correct tone logit; derive r_com per layer.

- **Design tradeoffs:**
  - ASR fine-tuning data requirement: Requires labeled transcriptions in target language (low-resource constraint noted in paper).
  - Cross-lingual transfer: Mandarin ASR fine-tuning provides partial benefit (second-best), but target-language ASR is optimal.
  - Probe complexity: Linear probes are interpretable but may underestimate representation quality; nonlinear probes could show higher F1 but obscure layer-wise differences.

- **Failure signatures:**
  - Probe F1 near chance at all layers -> SSL model not fine-tuned, or tone labels misaligned.
  - Gradient energy flat across temporal offsets -> Task objective doesn't engage temporal structure (e.g., speaker verification).
  - r_com ≫ baseline span -> Wrong fine-tuning task (prosody/voice instead of ASR).

- **First 3 experiments:**
  1. Replicate baseline span estimation: Train logistic regression on log-Mel features for a held-out tonal language (e.g., Mandarin) to confirm the method generalizes.
  2. Ablate fine-tuning data size: Fine-tune ASR with 10%, 50%, 100% of target-language data to measure data-efficiency of temporal span alignment.
  3. Test task mixing: Fine-tune with multi-task objective (ASR + speaker ID) to quantify tradeoff between span alignment and speaker invariance.

## Open Questions the Paper Calls Out
None

## Limitations
- The gradient-based temporal focus metric (r_com) lacks direct validation against ground-truth temporal dependencies, relying on gradient magnitude as a proxy for functional importance.
- The probe-based layer-wise analysis assumes linear classifiers can adequately extract tone information, potentially underestimating representation quality in early layers.
- The study focuses on four tonal languages; generalization to other tonal systems (e.g., register tones in West Africa) remains untested.

## Confidence
- **High Confidence:** The finding that ASR fine-tuning produces optimal tone representation with spans matching language-specific baselines.
- **Medium Confidence:** The layer-wise pattern showing tone information peaks in mid-to-high layers.
- **Medium Confidence:** The claim that prosody/voice tasks produce overly broad temporal spans.

## Next Checks
1. Conduct controlled experiments where tone is artificially embedded in speech with known temporal offsets, then verify whether r_com accurately recovers these ground-truth spans.
2. Replace linear probes with nonlinear classifiers (e.g., small MLPs) to determine if probe linearity underestimates early-layer tone encoding capability.
3. Test the fine-tuning methodology on additional tonal languages (e.g., Cantonese, Yoruba) to verify that the observed task-dependence and optimal spans generalize beyond the four studied languages.