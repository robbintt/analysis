---
ver: rpa2
title: 'TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification
  in Telugu'
arxiv_id: '2508.01486'
source_url: https://arxiv.org/abs/2508.01486
tags:
- telugu
- sentiment
- classification
- language
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TeSent, the first comprehensive Telugu sentiment
  classification benchmark dataset, designed to address the scarcity of high-quality
  annotated resources for the Telugu language. TeSent includes 21,119 sentences with
  sentiment labels and human-annotated rationales, sourced from diverse domains such
  as social media, news, and blogs.
---

# TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu
## Quick Facts
- arXiv ID: 2508.01486
- Source URL: https://arxiv.org/abs/2508.01486
- Reference count: 40
- Introduces TeSent, the first comprehensive Telugu sentiment classification benchmark dataset with fairness and explainability features.

## Executive Summary
TeSent is introduced as the first comprehensive benchmark dataset for sentiment classification in Telugu, addressing the scarcity of high-quality annotated resources for this low-resource language. The dataset comprises 21,119 sentences with sentiment labels and human-annotated rationales, sourced from social media, news, and blogs. It supports both explainability and fairness evaluation, offering tools for bias analysis across gender and religion, and a suite of plausibility and faithfulness metrics for six post-hoc explainers. Experimental results demonstrate that training models with human rationales improves alignment with human reasoning and often enhances model accuracy, though fairness improvements are inconsistent. TeSent aims to advance NLP research in low-resource languages and foster inclusive, interpretable, and fair machine learning systems.

## Method Summary
TeSent was developed through a multi-step process involving data collection, annotation, and quality control. First, 21,119 Telugu sentences were sourced from social media, news, and blogs. These were annotated by native speakers for sentiment polarity (positive, negative, neutral) and accompanied by human rationales explaining the sentiment. To support fairness analysis, the dataset was annotated for gender and religion-related biases. A suite of post-hoc explainers (LIME, SHAP, etc.) was applied to benchmark explainability, and both plausibility and faithfulness metrics were used to evaluate the quality of explanations. The dataset is publicly available, along with tools for bias and explainability evaluation, enabling researchers to assess both model performance and fairness in low-resource settings.

## Key Results
- TeSent is the first comprehensive benchmark dataset for Telugu sentiment classification with integrated fairness and explainability features.
- Training models with human rationales improves alignment with human reasoning and often enhances model accuracy.
- Fairness improvements are inconsistent across demographic dimensions, highlighting challenges in bias mitigation for low-resource languages.

## Why This Works (Mechanism)
The effectiveness of TeSent stems from its integration of human-annotated rationales and fairness annotations, which enable models to learn not only sentiment but also the underlying reasoning and potential biases. By providing explicit explanations for sentiment labels, models can better mimic human reasoning, improving interpretability. The inclusion of fairness annotations allows for targeted bias analysis and mitigation, addressing a critical gap in low-resource NLP. The use of post-hoc explainers and standardized metrics further ensures that both the plausibility and faithfulness of explanations can be rigorously evaluated.

## Foundational Learning
- **Sentiment classification**: Classifying text into positive, negative, or neutral sentiment is fundamental for understanding user opinions and emotions in NLP.
- **Explainability in NLP**: Human-annotated rationales help models produce interpretable outputs, aligning machine reasoning with human understanding.
- **Fairness evaluation**: Bias analysis across demographic dimensions (gender, religion) is essential for developing inclusive and equitable AI systems.
- **Post-hoc explainers**: Methods like LIME and SHAP provide post-hoc explanations for model predictions, crucial for interpreting black-box models.
- **Low-resource language challenges**: Limited data and resources necessitate careful annotation and evaluation strategies to ensure robust model performance.

## Architecture Onboarding
- **Component map**: Data collection -> Annotation (sentiment + rationale + fairness) -> Model training with rationales -> Evaluation (accuracy, fairness, explainability)
- **Critical path**: High-quality annotations are the foundation; their integration into model training directly impacts both accuracy and interpretability.
- **Design tradeoffs**: Balancing dataset size with annotation quality in a low-resource language; choosing post-hoc explainers that are both interpretable and stable.
- **Failure signatures**: Inconsistent fairness improvements suggest that bias mitigation strategies may not generalize across all demographic groups or contexts.
- **First experiments**: 1) Evaluate model accuracy with and without human rationales. 2) Analyze bias across gender and religion using fairness metrics. 3) Compare plausibility and faithfulness of explanations across post-hoc explainers.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset size (21,119 sentences) is relatively small for a low-resource language, potentially limiting generalizability and robustness.
- Fairness improvements are inconsistent across demographic dimensions, suggesting incomplete bias mitigation.
- Reliance on post-hoc explainers raises questions about stability and reliability, especially in low-resource settings.

## Confidence
- Confidence in the dataset's contribution to advancing explainable and fair NLP for low-resource languages: **High**
- Confidence in the experimental results showing improved alignment with human reasoning: **Medium**
- Confidence in the generalizability of fairness improvements: **Low**

## Next Checks
1. Evaluate model fairness and explainability on a larger, independently curated Telugu dataset to assess robustness and generalizability.
2. Conduct ablation studies to isolate the impact of human rationales on both accuracy and fairness, and to determine which aspects of the rationale annotation process are most critical.
3. Test the stability and consistency of post-hoc explainers across different random seeds and training runs, particularly in the low-resource Telugu context.