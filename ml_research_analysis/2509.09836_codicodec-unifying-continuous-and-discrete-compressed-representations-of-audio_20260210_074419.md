---
ver: rpa2
title: 'CoDiCodec: Unifying Continuous and Discrete Compressed Representations of
  Audio'
arxiv_id: '2509.09836'
source_url: https://arxiv.org/abs/2509.09836
tags:
- audio
- continuous
- embeddings
- discrete
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoDiCodec unifies continuous and discrete audio compression by
  producing both compressed continuous embeddings (~11 Hz) and discrete tokens (2.38
  kbps) from a single trained model. It uses summary embeddings to efficiently capture
  global features, consistency training with a single loss, and Finite Scalar Quantization
  (FSQ) with FSQ-dropout to enable high-quality continuous decoding.
---

# CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio

## Quick Facts
- arXiv ID: 2509.09836
- Source URL: https://arxiv.org/abs/2509.09836
- Reference count: 0
- Primary result: CoDiCodec achieves superior audio quality (FAD 0.0112, FAD_clap 0.344) compared to existing autoencoders at similar compression ratios, with faster decoding speeds and end-to-end training simplicity.

## Executive Summary
CoDiCodec introduces a unified audio compression framework that produces both high-quality continuous embeddings and discrete tokens from a single trained model. The key innovation is FSQ-dropout, which enables dual-mode representation learning without quality degradation. By using summary embeddings to capture global features efficiently and implementing a novel parallel decoding strategy, CoDiCodec achieves state-of-the-art reconstruction quality while enabling faster inference. The model demonstrates strong performance across both music and speech domains with a single loss function and simplified training pipeline.

## Method Summary
CoDiCodec processes audio through an encoder-decoder architecture that produces compressed representations at ~11 Hz. The encoder uses convolutional patchifiers followed by transformers to process audio chunks, which are then reduced to 128 summary embeddings. These embeddings undergo either FSQ quantization (discrete mode) or continuous transformation (continuous mode) with FSQ-dropout during training. The upsampler and consistency decoder work together to reconstruct audio spectrograms, with the decoder trained as a consistency model that maps noisy inputs to clean outputs in one step. The model supports both autoregressive and parallel decoding strategies, with the latter achieving superior quality and speed through iterative pair shifting.

## Key Results
- Achieves FAD 0.0112 (continuous) and FAD 0.0127 (discrete) on MusicCaps
- Parallel decoding with s=3 achieves 2.23s vs. 3.22s for autoregressive on 60s audio
- Superior FAD_clap score of 0.344 compared to baseline autoencoders
- Single-model training supports both continuous (~11 Hz) and discrete (2.38 kbps) representations

## Why This Works (Mechanism)

### Mechanism 1: FSQ-Dropout Enables Dual-Mode Representations
During training, with probability p=0.75, FSQ rounding is bypassed to feed continuous tanh(z) values directly to the upsampler, while with probability 0.25, standard FSQ rounding is applied. This forces the encoder to produce informative embeddings across the full [-1,1] range and trains the decoder to accept both input types. The dual-mode capability is achieved without degrading quality in either representation mode.

### Mechanism 2: Summary Embeddings Capture Global Features Efficiently
K=128 learnable summary embeddings are concatenated with audio embeddings and processed through transformers. These summary embeddings attend to all audio embeddings, aggregating global context. Only the K summary embeddings are retained as the compressed representation (reshaped to 8 embeddings × 64 channels at ~11 Hz), effectively reducing temporal redundancy while preserving critical reconstruction information.

### Mechanism 3: Parallel Decoding Mitigates Boundary Artifacts
Parallel decoding with shifted pairs achieves faster inference and better quality than autoregressive decoding. Adjacent pairs of summary embeddings are decoded in parallel starting from noise, with pairs shifted by one position at each denoising step. This iterative shifting allows information to propagate across sequence boundaries without sequential dependencies, eliminating boundary artifacts that typically affect autoregressive approaches.

## Foundational Learning

- **Concept: Consistency Models**
  - Why needed: CoDiCodec's decoder is trained as a consistency model mapping noisy spectrograms to clean ones in one step
  - Quick check: Can you explain why consistency models enable one-step generation while diffusion models require iterative denoising?

- **Concept: Finite Scalar Quantization (FSQ)**
  - Why needed: FSQ is the core quantization mechanism using bounded rounding to create implicit codebooks without explicit codebook learning
  - Quick check: For N=5 and d_lat=4, what is the implicit codebook size, and why does FSQ claim better codebook utilization than VQ?

- **Concept: Straight-Through Estimator**
  - Why needed: The rounding operation in FSQ is non-differentiable; gradients are approximated by passing them through unchanged during backpropagation
  - Quick check: Why does the straight-through estimator work reasonably well for FSQ despite the gradient mismatch?

## Architecture Onboarding

- **Component map:**
Input Audio (44.1kHz stereo) -> STFT + amplitude transformation -> Encoder: Conv Patchifier → [Audio Embeddings + Summary Embeddings] → Transformers -> (retain only K=128 summary embeddings) -> FSQ (discrete) OR tanh (continuous) <- FSQ-dropout during training -> Upsampler: Summary Embeddings + Mask Embeddings → Transformers → Conv De-Patchifier -> (cross-connections to decoder) -> Consistency Decoder: Noisy Spectrogram → Conv Patchifier + Cross-connections → Transformers → De-Patchifier -> Reconstructed Spectrogram -> iSTFT -> Audio

- **Critical path:**
  1. Cross-connections from upsampler to decoder (essential for one-step decoding)
  2. Chunked causal masking in decoder transformers (enables both AR and parallel decoding modes)
  3. FSQ-dropout probability p=0.75 (calibrated in ablation; wrong value breaks dual-mode capability)

- **Design tradeoffs:**
  - Transformer-heavy vs. convolution-heavy: 12 transformer blocks per component with reduced convolutional levels (5 vs. 7) for easier scaling, but increases memory usage
  - 128 × 4-dim vs. 8 × 64-dim summary embeddings: Same total dimensionality, but finer granularity improves FAD
  - Parallel vs. AR decoding: Parallel is faster (2.23s vs. 3.22s for 60s audio) but memory scales linearly with sequence length

- **Failure signatures:**
  - Continuous embeddings cluster near quantization levels (FSQ-dropout not working; check dropout implementation)
  - Boundary artifacts in decoded audio (parallel decoding steps too few; try s=4-5)
  - Poor discrete reconstruction despite good continuous (FSQ dropout probability too high; reduce p)
  - Training instability (consistency loss diverging; check noise schedule and Δσ exponential schedule)

- **First 3 experiments:**
  1. Reproduce FSQ-dropout ablation (Table 1): Train with p∈{0.25, 0.5, 0.75} for 400k iterations; verify continuous FAD improves while discrete FAD remains stable
  2. Compare AR vs. parallel decoding on 60s audio: Measure both FAD scores and inference time; parallel should achieve lower FAD with s=3-4 while being ~30% faster
  3. Downstream generative modeling test: Train a Rectified Flow model on FSQ-dropout embeddings vs. pure continuous embeddings; verify FSQ-dropout embeddings achieve comparable or better FAD_clap with fewer denoising steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implicit regularization provided by FSQ-dropout consistently enhance the robustness and performance of downstream latent generative models compared to standard continuous latents?
- Basis in paper: Section 5.2 states the authors hypothesize FSQ-dropout's implicit regularization can be beneficial for latent generative modeling
- Why unresolved: The paper observes improved FAD scores with fewer denoising steps in a preliminary test, but the exact mechanism and consistency of this benefit across different generative frameworks remain unverified
- What evidence would resolve it: A comparative study of downstream generative models trained on FSQ-dropout latents versus standard continuous latents, analyzing convergence rates and output quality under varying noise conditions

### Open Question 2
- Question: How do CoDiCodec's unified continuous and discrete representations perform on a broad range of downstream Music Information Retrieval (MIR) tasks compared to specialized baselines?
- Basis in paper: The Conclusion states the authors plan on investigating representations for a broader range of MIR tasks
- Why unresolved: Current evaluation focuses primarily on reconstruction quality and a specific downstream generation task, leaving utility for tasks like classification or tagging unexplored
- What evidence would resolve it: Benchmarking the learned representations on standard MIR tasks (e.g., instrument classification, tempo estimation) against existing discrete (DAC) and continuous (Music2Latent) autoencoders

### Open Question 3
- Question: How does CoDiCodec's performance and stability change when scaling up the model parameters and training on diverse audio domains beyond the music-focused datasets used in this study?
- Basis in paper: The Conclusion explicitly lists scaling up the model and applying it to diverse audio domains as future work
- Why unresolved: The paper demonstrates results for a ~150M parameter model trained on a specific mix of music and some speech; performance on significantly larger scales or purely non-music data is unknown
- What evidence would resolve it: Training scaled variants of CoDiCodec and evaluating reconstruction fidelity on out-of-domain datasets (e.g., environmental sounds, pure speech) to assess generalization

## Limitations

- Temporal resolution trade-off: Summary embedding approach compresses 32-frame chunks into single tokens, potentially limiting fine-grained temporal tasks
- Evaluation scope: Primary testing on MusicCaps dataset leaves performance on speech-only, environmental sounds, or multi-domain audio unverified
- Computational overhead: 150M parameters with 12 transformer blocks per component creates significant resource requirements despite parallel decoding benefits

## Confidence

**High Confidence:**
- FSQ-dropout successfully enables dual-mode representations from a single model
- Summary embeddings effectively reduce temporal redundancy while maintaining reconstruction quality
- Parallel decoding with pair shifting achieves both faster inference and improved FAD scores

**Medium Confidence:**
- The unified model architecture provides practical advantages over separate continuous/discrete models
- FSQ with N=5 levels provides optimal balance between compression ratio and reconstruction quality
- The model's performance generalizes well beyond the MusicCaps evaluation dataset

**Low Confidence:**
- Downstream generative modeling capabilities without direct comparison to alternative embeddings
- Memory scaling benefits of parallel decoding for very long sequences (>5 minutes)
- Robustness to domain shift (e.g., from music to speech or environmental sounds)

## Next Checks

1. **Cross-Dataset Evaluation:** Test CoDiCodec on non-music datasets including LibriSpeech (speech), ESC-50 (environmental sounds), and AudioSet. Compare FAD and downstream task performance (speech recognition, sound classification) against specialized models.

2. **Fine-Grained Temporal Analysis:** Evaluate reconstruction quality on transient-rich audio (drum solos, percussive music) using objective metrics sensitive to temporal precision. Compare summary embedding compression (32 frames/chunk) against alternatives with finer temporal resolution.

3. **Memory-Constrained Deployment Test:** Implement and benchmark the parallel decoding strategy on resource-limited hardware (e.g., GPU with 8GB memory). Measure memory usage and decoding speed for sequences of varying lengths (30s, 2min, 10min) to quantify the practical limits of the linear memory scaling.