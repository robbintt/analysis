---
ver: rpa2
title: 'Paper2Web: Let''s Make Your Paper Alive!'
arxiv_id: '2510.15842'
source_url: https://arxiv.org/abs/2510.15842
tags:
- arxiv
- content
- preprint
- website
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Paper2Web introduces a benchmark dataset and evaluation framework\
  \ for converting academic papers into interactive, multimedia-rich webpages. It\
  \ addresses the challenge that current methods\u2014templates, direct HTML conversion,\
  \ or LLM generation\u2014fail to produce layout-aware, interactive sites with balanced\
  \ visual-textual composition."
---

# Paper2Web: Let's Make Your Paper Alive!

## Quick Facts
- **arXiv ID:** 2510.15842
- **Source URL:** https://arxiv.org/abs/2510.15842
- **Reference count:** 40
- **Primary result:** Introduces PWAgent, a multi-agent pipeline that converts academic papers to interactive webpages, achieving 28% higher completeness and 18% better holistic scores than baselines.

## Executive Summary
Paper2Web introduces a benchmark dataset and evaluation framework for converting academic papers into interactive, multimedia-rich webpages. Current methods—templates, direct HTML conversion, or LLM generation—fail to produce layout-aware, interactive sites with balanced visual-textual composition. The authors construct a large-scale dataset linking 10,716 papers to their project homepages, then propose PWAgent, an autonomous multi-agent pipeline that parses papers into structured assets and iteratively refines layout and interactivity via MCP tools. Evaluations using rule-based metrics (Connectivity, Completeness), LLM-as-a-Judge, and PaperQuiz show PWAgent achieves a 28% improvement over arXiv HTML baselines in completeness, 18% better holistic scores, and triples knowledge transfer scores while maintaining low generation cost (~$0.025 per site).

## Method Summary
PWAgent is a multi-agent pipeline that converts academic papers (PDF) into interactive project homepages (HTML). The process begins with document parsing using Docling or Marker to convert PDFs to Markdown, followed by LLM semantic parsing to extract structured JSON assets (textual, visual, link). These assets are ingested into an MCP server with stable resource IDs and standardized tools. An Orchestrator Agent (Qwen2.5-VL-32B) generates an initial HTML draft, segments the rendered page into visual tiles linked to HTML fragments, and iteratively critiques and refines layout via tool calls until convergence or iteration limit.

## Key Results
- PWAgent achieves 28% improvement over arXiv HTML baselines in completeness.
- Holistic scores improve by 18% (91% of ground-truth aesthetics, 94% informativeness).
- Knowledge transfer scores triple compared to arXiv HTML, while maintaining low generation cost (~$0.025 per site).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured asset decomposition from PDF to structured resources reduces context fragmentation and improves content fidelity.
- Mechanism: PDF → Markdown (Docling/Marker) → LLM semantic parsing → JSON assets (textual, visual, link). By normalizing heterogeneous PDF structures into a unified schema with cross-modal semantics, downstream agents operate on queryable, grounded resources rather than raw text, reducing hallucination and improving completeness.
- Core assumption: Document parsing tools (Docling/Marker) preserve layout order and LLM can reliably extract/align assets; assumes papers follow standard academic sections.
- Evidence anchors:
  - [abstract] "PWAgent... parses papers into structured assets and iteratively refines layout and interactivity via MCP tools."
  - [section 4.1] "The document is converted to Markdown... An LLM then performs semantic decomposition that extracts metadata, reconstruct tables, and model detailed page layout."
  - [corpus] Weak direct evidence; neighboring papers focus on academic automation (e.g., AutoPR, APEX) but not on PDF-to-asset pipelines.
- Break condition: If document parsing fails on LaTeX-heavy or non-standard layouts, or if LLM cannot disambiguate section boundaries, asset quality degrades.

### Mechanism 2
- Claim: MCP-based resource centralization enables tool-accessible, queryable state for multi-step agent refinement.
- Mechanism: Assets are ingested into an MCP server with stable resource IDs and standardized tools (enumerate, access, typed references). This provides a consistent API for agents to retrieve content, metadata, and layout hints, enabling iterative editing without re-parsing.
- Core assumption: MCP server correctly maintains resource state across tool calls; agents can reliably invoke tools without protocol errors.
- Evidence anchors:
  - [abstract] "...iteratively refines layout and interactivity via MCP tools..."
  - [section 4.2] "The server is responsible for resource construction, materializing assets with relational metadata... and for tool registration, exposing a minimal, consistent API."
  - [corpus] Limited; Paper2Agent (neighbor) mentions MCP but for agent-building, not webpage generation.
- Break condition: If MCP tool contracts change or if resource IDs become stale during iteration, agents retrieve mismatched content, causing layout edits on wrong sections.

### Mechanism 3
- Claim: Orchestrator-driven iterative refinement with local-global reasoning reduces visual hallucination and improves aesthetic balance.
- Mechanism: Initial page draft → Orchestrator MLLM segments page into tiles linked to HTML → sequential local analysis → merge-adjacent optimization → global pass. This hierarchical inspection localizes errors and enables precise tool-based edits.
- Core assumption: MLLM can accurately map visual tiles to HTML fragments; assumes iterative loop converges within cost/iteration limits.
- Evidence anchors:
  - [abstract] "...iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality."
  - [section 4.3] "The Orchestrator performs joint global–local reasoning... segments the rendered page into independent visual tiles linked to their corresponding HTML fragments... After each round... adjacent tiles are merged."
  - [corpus] No direct evidence; iterative multi-agent refinement is common (e.g., APEX for posters), but not with tile-HTML mapping.
- Break condition: If MLLM hallucinates visual layout issues or if tile segmentation misaligns with HTML structure, edits may introduce new bugs or fail to converge.

## Foundational Learning

- **Concept: Model Context Protocol (MCP)**
  - Why needed here: MCP standardizes how agents access external resources (paper assets) and tools (editing functions). Understanding MCP is critical to debugging tool-call failures and resource state issues in PWAgent.
  - Quick check question: Can you explain how MCP differs from a simple REST API in terms of resource lifetime and tool discovery?

- **Concept: Multi-agent orchestration with global-local reasoning**
  - Why needed here: PWAgent uses an Orchestrator MLLM to perform hierarchical visual inspection. This pattern—segment, analyze locally, merge, global assess—is key to understanding the refinement loop.
  - Quick check question: Why would a pure global optimization (analyzing the whole page at once) be more prone to hallucination than a local-global approach?

- **Concept: Document parsing and layout recovery**
  - Why needed here: Converting PDF to Markdown while preserving reading order, figure-table associations, and cross-references is non-trivial. Errors here cascade into asset quality.
  - Quick check question: What are two common failure modes when parsing two-column LaTeX PDFs into structured Markdown?

## Architecture Onboarding

- **Component map:** PDF input → Decomposition (Docling/Marker + LLM parser) → MCP ingestion (MCP server + tools) → Initial HTML draft → Iterative refinement (Orchestrator + MCP tools) → Final HTML
- **Critical path:** PDF input → Decomposition → MCP ingestion → Initial HTML draft → Iterative refinement (Orchestrator + MCP tools) → Final HTML. The loop continues until no edits or iteration limit.
- **Design tradeoffs:**
  - Cost vs. quality: More iterations improve aesthetics but increase token cost (~$0.025/site reported). Early stopping trades refinement for speed.
  - Asset granularity: Fine-grained assets (per-paragraph) improve precision but increase MCP overhead; coarse assets reduce state management but limit edit precision.
  - Model choice: Larger MLLM (e.g., Qwen2.5-VL-32B) improves visual reasoning but costs more; smaller models may fail to detect subtle layout issues.
- **Failure signatures:**
  1. **Asset misalignment:** Figures placed in wrong sections → check LLM parser alignment prompts, verify caption-to-section linking.
  2. **Non-converging edits:** Iteration loop exceeds limit without quality gain → inspect Orchestrator’s fix_suggest logs for repeated issues, may need better tile-HTML mapping.
  3. **Tool-call errors:** MCP tools return stale resources → verify resource IDs are regenerated or versioned after edits, check MCP server state management.
- **First 3 experiments:**
  1. **Baseline comparison with controlled papers:** Run PWAgent on 5 papers from PAPER2WEB dataset with diverse layouts (single-column, two-column, heavy math). Compare connectivity, completeness, and PaperQuiz scores against arXiv-HTML and Gemini-Template baselines to validate reported improvements.
  2. **Ablation on iteration count:** Fix all parameters, vary iteration limit (1, 3, 5, 10) on 3 papers. Measure aesthetic score (MLLM-as-Judge) and cost to identify Pareto point for quality/cost tradeoff.
  3. **MCP tool debugging harness:** Inject synthetic asset errors (e.g., wrong figure IDs) into MCP repository. Run Orchestrator refinement and log tool-call sequences to identify if agents can detect/recover from corrupted state. This tests robustness of MCP state management.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can multimedia elements' contribution to effective academic communication be robustly quantified beyond the current verbosity-penalty approach?
  - Basis in paper: [explicit] "Nonetheless, evaluating how multimedia elements contribute to effective academic communication remains an open challenge, which we plan to address through more robust agentic workflows and comprehensive evaluation methods in future work."
  - Why unresolved: The PaperQuiz metric penalizes verbose text but may undervalue rich multimedia (videos, animations); ground-truth sites scored lower than expected partly because they include such content, indicating the metric does not fully capture multimedia-driven knowledge transfer.
  - What evidence would resolve it: A new evaluation protocol that systematically scores multimedia enrichment (e.g., video understanding, animation interactivity) and correlates with user comprehension studies or task-based assessments.

- **Open Question 2:** Can agentic webpage generation systems close the remaining quality gap to human-designed project homepages, and what architectural improvements are required?
  - Basis in paper: [inferred] PWAgent achieves 91% of ground-truth aesthetics and 94% informativeness, yet the authors note "there is still room for improvement when compared to the human-designed version."
  - Why unresolved: The gap suggests current decomposition-MCP-refinement pipelines lack nuanced design intuition (spacing, typography hierarchy, creative layouts) that human designers apply; it is unclear whether scaling agents or adding design-specific modules would help.
  - What evidence would resolve it: Ablation studies isolating design-aware components, plus human preference rankings showing generated sites statistically indistinguishable from expert-designed pages.

- **Open Question 3:** What is the optimal balance between interactivity and static content for knowledge retention in academic webpages?
  - Basis in paper: [inferred] Figure 5 shows only ~10% of human-created pages are interactive, while PWAgent achieves 59% higher interactivity than alphaXiv; yet excessive interactivity may distract or increase cognitive load.
  - Why unresolved: The paper introduces interactivity metrics but does not test whether higher interactivity scores correlate with improved knowledge transfer or user satisfaction; the optimal level may vary by paper type (theory vs. systems).
  - What evidence would resolve it: Controlled user studies varying interactivity levels on identical content, measuring comprehension (via PaperQuiz), time-on-task, and subjective preference across paper categories.

## Limitations
- **Major uncertainty:** Dependency on document parsing fidelity—complex LaTeX or non-standard layouts may fail to parse reliably, degrading asset quality.
- **Performance trade-off:** Iterative refinement improves quality but increases cost; early stopping may sacrifice refinement for speed.
- **Evaluation gaps:** Rule-based and LLM-as-a-Judge metrics may not fully capture user experience or accessibility nuances.

## Confidence
- **High Confidence:** The dataset construction (10,716 papers with verified homepages) and the overall pipeline architecture (decomposition → MCP ingestion → iterative refinement) are well-specified and reproducible.
- **Medium Confidence:** The reported improvements over baselines (28% completeness, 18% holistic scores, 3x knowledge transfer) are plausible given the described mechanisms, but the exact magnitude may depend on the specific papers and evaluation prompts used.
- **Low Confidence:** The robustness of the iterative refinement loop to complex or noisy layouts is not fully demonstrated. The paper does not provide extensive ablation studies on iteration count or asset granularity, leaving some design choices empirically under-validated.

## Next Checks
1. **Layout Robustness Test:** Run PWAgent on a set of 10 papers from PAPER2WEB with diverse and challenging layouts (e.g., two-column, heavy math, multi-section figures). Measure connectivity, completeness, and PaperQuiz scores, and compare against the reported baselines to confirm generalization.
2. **Iteration-Aesthetic Pareto Analysis:** Fix all parameters and run PWAgent on 5 papers with iteration limits of 1, 3, 5, and 10. Plot aesthetic score (MLLM-as-Judge) against generation cost to identify the optimal trade-off point and validate the claimed cost-efficiency (~$0.025 per site).
3. **MCP State Management Stress Test:** Inject synthetic errors (e.g., wrong figure IDs, missing text chunks) into the MCP repository and run the refinement loop. Log tool-call sequences and final output quality to assess whether agents can detect and recover from corrupted asset states, or if the loop fails silently.