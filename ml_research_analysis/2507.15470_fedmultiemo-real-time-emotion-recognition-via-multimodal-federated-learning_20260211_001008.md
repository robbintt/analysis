---
ver: rpa2
title: 'FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning'
arxiv_id: '2507.15470'
source_url: https://arxiv.org/abs/2507.15470
tags:
- emotion
- physiological
- data
- recognition
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedMultiEmo introduces a multimodal federated learning framework
  for real-time in-vehicle emotion recognition, combining facial expressions (CNN-based)
  and physiological signals (Random Forest-based) with decision-level fusion. This
  system addresses challenges of modality fragility, physiological variability, and
  privacy risks in centralized approaches by keeping raw data local and distributing
  computation across edge devices.
---

# FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning

## Quick Facts
- arXiv ID: 2507.15470
- Source URL: https://arxiv.org/abs/2507.15470
- Reference count: 34
- Primary result: Federated multimodal emotion recognition achieves 87% accuracy with 18 rounds of training in under 200MB memory footprint per client

## Executive Summary
FedMultiEmo presents a privacy-preserving federated learning framework for real-time in-vehicle emotion recognition that combines facial expressions and physiological signals without centralizing sensitive data. The system employs a CNN for facial expression analysis and Random Forest for physiological signal processing, with decision-level fusion achieving 87% accuracy while maintaining data privacy through edge computing. The federated approach eliminates the need for raw data transmission, addressing both privacy concerns and the variability of physiological responses across different individuals in automotive environments.

## Method Summary
The framework integrates facial expression analysis using a CNN model trained on FER2013 data with physiological signal processing via Random Forest on custom-collected physiological datasets. These modalities are combined at the decision level through weighted averaging based on confidence scores. The federated learning architecture enables training across multiple edge devices while keeping raw data local, with model updates aggregated through a central server. The system specifically addresses the challenges of in-vehicle emotion recognition where environmental factors and individual physiological variations can affect signal quality and interpretation.

## Key Results
- CNN achieves 77% accuracy on facial expressions, Random Forest 74% on physiological signals
- Decision-level fusion of modalities reaches 87% accuracy, matching centralized baseline performance
- System converges in 18 rounds with 120-second average round time and <200MB memory footprint per client

## Why This Works (Mechanism)
The federated approach works by distributing computation across edge devices while aggregating only model updates rather than raw data, preserving privacy while enabling collaborative learning. The decision-level fusion leverages the complementary strengths of visual and physiological modalities, with facial expressions providing immediate emotional cues and physiological signals offering deeper, less visible emotional states. The combination addresses the inherent fragility of individual modalities in real-world conditions - facial expressions can be obscured by lighting or movement, while physiological signals vary significantly between individuals and can be affected by stress unrelated to emotional state.

## Foundational Learning

**Federated Learning** - A machine learning approach where models are trained across multiple decentralized edge devices without exchanging raw data. Why needed: Enables privacy-preserving collaborative learning in environments where data cannot be centralized due to privacy regulations or data sensitivity. Quick check: Verify that only model parameters, not raw data, are transmitted between clients and server.

**Decision-Level Fusion** - Combining outputs from multiple models by integrating their predictions rather than combining raw features. Why needed: Allows integration of heterogeneous data sources with different characteristics and preprocessing requirements while maintaining modularity. Quick check: Confirm fusion weights are based on model confidence scores and validated through ablation studies.

**CNN for Facial Expression Recognition** - Convolutional neural networks specialized for processing visual data to identify facial expressions. Why needed: Captures spatial hierarchies and patterns in facial features that indicate emotional states, particularly effective for real-time processing. Quick check: Validate that the CNN architecture includes appropriate pooling and normalization layers for robustness.

## Architecture Onboarding

**Component Map**: Edge devices (facial sensors, physiological sensors) -> Local models (CNN, Random Forest) -> Federated aggregation server -> Decision-level fusion module -> Emotion classification output

**Critical Path**: Sensor data collection → Local model inference → Model parameter updates → Federated aggregation → Decision fusion → Final emotion classification

**Design Tradeoffs**: Privacy preservation through local data processing versus potential accuracy loss from not sharing raw data; computational load distributed across edge devices versus communication overhead for model updates; decision-level fusion simplicity versus potential information loss compared to feature-level fusion.

**Failure Signatures**: Poor lighting conditions causing CNN degradation; physiological signal noise from movement or sensor displacement; model drift from non-IID data distributions across clients; communication failures during federated aggregation rounds.

**First Experiments**: 1) Test individual modality performance on isolated datasets to establish baseline accuracy. 2) Evaluate fusion performance with varying confidence weightings to optimize decision-level combination. 3) Assess federated learning convergence with different numbers of clients and communication frequencies.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited client diversity with only 10 clients in federated experiments, raising scalability concerns
- Insufficient cross-validation with independent datasets to verify generalizability beyond test data
- Lack of real-world driving condition testing to validate robustness under varying environmental factors

## Confidence
- High Confidence: Architectural design combining CNN and Random Forest with decision-level fusion; memory footprint estimates
- Medium Confidence: Reported accuracy metrics (77%, 74%, 87%) and convergence time (18 rounds, 120 seconds)
- Low Confidence: Scalability claims to hundreds of clients; robustness under real-world driving conditions

## Next Checks
1. Conduct experiments with 50-100 diverse clients to validate scalability and performance across demographic groups
2. Perform cross-dataset validation using independent emotion recognition datasets (DEAP, AMIGOS) to verify generalizability
3. Test system under real-world driving conditions with varying lighting, road conditions, and driver behaviors to assess robustness