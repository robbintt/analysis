---
ver: rpa2
title: A Comparative Approach to Assessing Linguistic Creativity of Large Language
  Models and Humans
arxiv_id: '2507.12039'
source_url: https://arxiv.org/abs/2507.12039
tags:
- creativity
- llms
- humans
- language
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a linguistic creativity test for humans and
  Large Language Models (LLMs), assessing their ability to generate novel words and
  phrases using derivation, compounding, and metaphorical language. The test was administered
  to 24 humans and 24 LLMs, with answers automatically evaluated using OCSAI tool
  for Originality, Elaboration, and Flexibility.
---

# A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans

## Quick Facts
- **arXiv ID:** 2507.12039
- **Source URL:** https://arxiv.org/abs/2507.12039
- **Reference count:** 39
- **Key result:** LLMs outperformed humans in linguistic creativity test with mean total score 0.52 vs 0.48 for humans

## Executive Summary
This paper introduces a linguistic creativity test comparing humans and Large Language Models (LLMs) in generating novel words and phrases. The test evaluated both groups on derivation, compounding, and metaphorical language using eight tasks. LLMs significantly outperformed humans on all criteria (Originality, Elaboration, Flexibility) in six out of eight tasks, with humans showing slightly higher uniqueness scores in some cases. Manual analysis revealed humans favor extending creativity (breaking rules for expressive effect) while LLMs favor fixed creativity (following established patterns).

## Method Summary
The study administered an 8-task linguistic creativity test to 24 humans (non-native English speakers, B2+ proficiency) and 24 LLMs (mix of open-source and proprietary). Each task had 2 items, with subjects providing 3 answers per item. Responses were automatically evaluated using the OCSAI tool, which measures Originality, Elaboration, and Flexibility based on semantic distances and GPT-4 integration. Flexibility was computed as the mean OCSAI score of all pairwise combinations of an individual's 3 answers. Statistical analysis included t-tests and OLS regression on normalized 0-1 scores.

## Key Results
- LLMs achieved significantly higher total scores (0.52) compared to humans (0.48) across all evaluation criteria
- LLMs outperformed humans in Originality (t-statistic -4.21), Elaboration (t-statistic -3.71), and Flexibility (t-statistic -2.24)
- Humans showed higher uniqueness scores in some tasks, particularly when employing rule-breaking "E-creativity"
- Manual analysis revealed humans tend to extend creativity by breaking morpho-syntactic rules while LLMs demonstrate rule-based creativity

## Why This Works (Mechanism)

### Mechanism 1: F-Creativity via Morphological Pattern Recombination
LLMs outperform humans in "Fixed-creativity" tasks because they can more effectively recombine valid morphological rules to generate statistically plausible, novel word forms. The models leverage learned probability distributions of affixes and compounding structures, optimizing for structural validity rather than semantic transgression.

### Mechanism 2: Semantic Distance Optimization in Evaluation
The superior scores of LLMs are partially an artifact of the OCSAI evaluation method, which relies on semantic distance and GPT-4 integration. LLM outputs, which are algorithmically generated to be contextually appropriate and distinct, naturally achieve high semantic distance scores without drifting into the incoherence sometimes found in human "E-creative" responses.

### Mechanism 3: Human E-Creativity as High-Variance Deviation
Humans underperform on aggregate metrics but dominate the "uniqueness" dimension because they employ "E-creativity"â€”intentionally violating linguistic norms to express pragmatic intent (humor, offense, identity). While this generates lower average consistency, it produces specific responses that are semantically distant from all other responses.

## Foundational Learning

- **Concept:** **F-Creativity vs. E-Creativity**
  - **Why needed here:** Essential for diagnosing why LLMs scored higher on standard metrics (F-creativity strength) while humans showed higher top-tier uniqueness (E-creativity strength)
  - **Quick check question:** Does the generated output strictly follow known derivation rules (F), or does it break semantic/syntactic norms to create new meaning (E)?

- **Concept:** **Divergent Thinking Metrics (Originality, Flexibility, Elaboration)**
  - **Why needed here:** The study abandoned "Fluency" (quantity) to avoid bias. Understanding the remaining three metrics is required to replicate the scoring logic
  - **Quick check question:** When scoring a response, are you measuring the rarity of the idea (Originality), the diversity of categories used (Flexibility), or the detail/word count (Elaboration)?

- **Concept:** **Automated Creativity Scoring (OCSAI)**
  - **Why needed here:** The results depend entirely on this tool. One must understand it uses semantic distance (embeddings) and LLM-judgment, not human manual scoring
  - **Quick check question:** If two answers are distinct but one is nonsensical, would a semantic distance metric still score them as highly original? (Answer: Yes, unless filtered)

## Architecture Onboarding

- **Component map:** Google Forms (Human) -> OCSAI Tool -> Python (Pandas/Scipy) -> Statistical Analysis
- **Critical path:** The transition from raw text answers to normalized 0-1 scores. If OCSAI prompt formatting is incorrect, the semantic distance calculation fails
- **Design tradeoffs:** 
  - Removed "Fluency" to prevent volume-based bias but ignores human capacity for sustained creative output
  - Used B2+ English students as accessible sample but may underrepresent native-speaker "E-creativity"
- **Failure signatures:**
  - LLM Alliteration Loops: Repetitive patterns that score high on distinctiveness but represent low variance
  - Human "Quirkiness": High variance outputs that sometimes score very low due to unconventional approaches
- **First 3 experiments:**
  1. **Temperature Sensitivity Analysis:** Replicate LLM cases with maxed-out temperature/top-p to verify stochasticity's effect on creativity scores
  2. **Native vs. Non-Native Comparison:** Run test on native English speakers to isolate "E-creativity" variable
  3. **Human-in-the-Loop Evaluation:** Re-score subset of answers using human annotators rather than OCSAI

## Open Questions the Paper Calls Out

- **Question:** How would performance gap change with native English speakers?
  - **Basis in paper:** Section 10 explicitly states this remains unanswered
  - **Why unresolved:** Study limited to 24 non-native speakers; unclear if B2+ proficiency captured full creative potential of native speakers

- **Question:** Do automated scoring tools inherently favor F-creativity over E-creativity?
  - **Basis in paper:** Section 8 notes humans tend towards E-creativity while LLMs favor F-creativity, yet LLMs scored higher on "Originality" via OCSAI
  - **Why unresolved:** Potential discrepancy between manual analysis (humans more unconventional) and automated scores favoring LLMs

- **Question:** Can sentiment analysis and semantic clustering distinguish human vs LLM creativity?
  - **Basis in paper:** Section 10 lists this as specific future work
  - **Why unresolved:** Current analysis relied on mean scores and manual inspection without deeper computational linguistic techniques

## Limitations
- Non-native English speakers may underestimate human creative potential, particularly in nuance and slang
- OCSAI evaluation may favor statistically plausible LLM outputs over unconventional human creativity
- Exact phrasing of all 16 test items remains unspecified, limiting precise replication

## Confidence
- **High Confidence:** LLMs outperformed humans on aggregate metrics under OCSAI evaluation framework
- **Medium Confidence:** F-creativity vs E-creativity distinction explains overall scoring patterns
- **Low Confidence:** Conclusions about human-like creative capabilities in LLMs, given potential evaluation bias

## Next Checks
1. **Human-in-the-Loop Validation:** Re-score 50-100 randomly selected creative responses using human annotators rather than OCSAI
2. **Native Speaker Comparison:** Run the full test on native English speakers to isolate whether uniqueness gap widens
3. **Evaluation Sensitivity Analysis:** Test alternative creativity metrics (surprise, contextual appropriateness) to verify if LLM advantages persist across different frameworks