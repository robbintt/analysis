---
ver: rpa2
title: 'EQA-RM: A Generative Embodied Reward Model with Test-time Scaling'
arxiv_id: '2506.10389'
source_url: https://arxiv.org/abs/2506.10389
tags:
- reasoning
- reward
- eqa-rm
- score
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EQA-RM is a generative multimodal reward model tailored for Embodied
  Question Answering (EQA), addressing the need for nuanced evaluation of agents'
  spatial, temporal, and logical understanding. It uses Contrastive Group Relative
  Policy Optimization (C-GRPO) with rule-based contrastive rewards from data augmentations
  to learn fine-grained behavioral distinctions.
---

# EQA-RM: A Generative Embodied Reward Model with Test-time Scaling

## Quick Facts
- **arXiv ID:** 2506.10389
- **Source URL:** https://arxiv.org/abs/2506.10389
- **Authors:** Yuhang Chen; Zhen Tan; Tianlong Chen
- **Reference count:** 26
- **Primary result:** 61.9% accuracy on EQARewardBench with 700 samples, outperforming Gemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, RoVRM, and VisualPRM

## Executive Summary
EQA-RM introduces a generative multimodal reward model for Embodied Question Answering that addresses the need for nuanced evaluation of agents' spatial, temporal, and logical understanding. The model uses a two-stage training approach with Contrastive Group Relative Policy Optimization (C-GRPO) and achieves strong performance with minimal training data. A key innovation is test-time scaling via generative sampling, allowing dynamic adjustment of evaluation granularity without retraining. The model demonstrates high sample efficiency and scalability, achieving state-of-the-art results on a new benchmark while maintaining interpretability through structured critiques.

## Method Summary
EQA-RM employs a two-stage training pipeline: first, Rejective Filtering Training (RFT) learns output formatting using a filtered dataset of correct-but-not-too-easy evaluations; second, C-GRPO uses contrastive rewards from structured perturbations (temporal shuffling, spatial masking, and reasoning step jumbling) to instill substantive evaluative understanding. The generative nature enables test-time scaling where K diverse reasoning paths are sampled and aggregated via majority voting or averaging. The model uses Qwen2-VL-2B-Instruct as backbone and is trained on 697 ScanNet instances, achieving 61.9% accuracy on a 1,546-instance benchmark with K=32 scaling.

## Key Results
- Achieves 61.9% accuracy on EQARewardBench with only 700 training samples
- Outperforms strong proprietary baselines including Gemini-2.5-Flash (59.8%) and GPT-4o
- Demonstrates test-time scaling benefits: accuracy improves from 42.47% to 61.86% when increasing K from 1 to 32
- Shows sample efficiency: 2B parameter model matches or exceeds larger models without task-specific training

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Rewards from Structured Perturbations
C-GRPO training improves evaluative acuity by forcing the model to distinguish coherent trajectories from synthetically degraded ones through three perturbation types: shuffled video frames disrupt temporal ordering, spatially masked frames degrade fine-grained visual grounding, and jumbled reasoning steps break logical coherence. The model receives boosted rewards only when it scores original instances more accurately than perturbed counterparts, learning fine-grained behavioral distinctions that transfer to genuine EQA evaluation.

### Mechanism 2: Test-time Scaling via Generative Sampling
Sampling K diverse reasoning paths at inference and aggregating scores improves evaluation accuracy without retraining by leveraging the model's generative diversity. More samples provide more perspectives for reliable assessment, with accuracy improving monotonically from K=1 to K=32. This approach is uniquely enabled by the generative nature of EQA-RM, allowing dynamic adjustment of evaluation granularity.

### Mechanism 3: Two-Stage Training (RFT → C-GRPO)
RFT establishes output formatting by teaching the critique-score structure through rejective filtering, while C-GRPO instills substantive evaluative understanding that RFT alone cannot achieve. This staged approach provides a stable initialization that enables effective RL; without RFT pre-training, C-GRPO may fail to converge and accuracy degrades below the base model.

## Foundational Learning

- **Reward Models in RLHF:** Understanding how reward models provide scalar or generative feedback for policy optimization is prerequisite for EQA-RM's evaluation framework.
  - *Quick check:* Can you explain why a reward model is trained separately from the policy model it will guide?

- **Group Relative Policy Optimization (GRPO):** C-GRPO extends GRPO with contrastive rewards; understanding group-based advantage normalization is essential for the RL training procedure.
  - *Quick check:* How does GRPO differ from PPO in computing advantages, and why might relative comparisons help for subjective evaluation tasks?

- **Vision-Language Models for Video Understanding:** EQA-RM processes episode memory videos and reasoning traces; understanding VLM capabilities and limitations with long video sequences is crucial.
  - *Quick check:* What are the typical failure modes of VLMs on long video sequences, and why might keyframe extraction help?

- **Embodied Question Answering (EQA):** The reward model must evaluate spatial, temporal, and logical understanding specific to embodied agents—generic RMs fail here.
  - *Quick check:* What makes EQA fundamentally different from standard VQA, and why would a generic RM struggle?

## Architecture Onboarding

- **Component map:** {question, answer, reasoning_trace, video_frames} -> [Keyframe Selection: N=5] -> [Qwen2-VL-2B-Instruct] -> [RFT] -> [C-GRPO] -> [Test-time Scaling: K samples] -> {critique: str, score: int (0-10)}

- **Critical path:**
  1. Dataset construction using Gemini-2.5-Pro for ground truth scores and human verification
  2. RFT data filtering with tolerance τ to select "correct but not too easy" candidate evaluations
  3. C-GRPO training with perturbation augmentation and contrastive reward optimization
  4. Test-time scaling configuration with K selection and aggregation method

- **Design tradeoffs:**
  - Model size vs. sample efficiency: 2B parameter base achieves 61.9% with only 700 training samples
  - RFT filtering threshold (τ): Tighter filtering may reduce diversity but improve quality
  - Perturbation intensity: Spatial masking at 15% ratio balances signal strength vs. task triviality
  - Test-time K: Scaling from K=1 to K=32 yields +19% accuracy but 32× inference cost

- **Failure signatures:**
  - RFT-only: Outputs well-formatted critiques with poor accuracy (style over substance)
  - C-GRPO without RFT: Training unstable, accuracy degrades below base model
  - Spatial reward alone: Ablation shows -3.25% accuracy—may over-regularize without complementary signals
  - Low test-time diversity: If samples are near-identical, scaling plateaus

- **First 3 experiments:**
  1. Reproduce RFT filtering analysis: Train with/without rejective filtering on the 700-sample D_F to verify "too easy" examples degrade downstream C-GRPO
  2. Ablate perturbation types individually: Train three models with R_t, R_s, R_r alone (post-RFT) to confirm reasoning reward provides largest single gain (+9.32%)
  3. Test-time scaling curve: Evaluate K∈{1,2,4,8,16,32} on held-out EQARewardBench subset to verify monotonic improvement and identify saturation point

## Open Questions the Paper Calls Out

- **Generalization to diverse EQA tasks:** The broader generalization of EQA-RM to EQA tasks, visual styles, or interaction paradigms substantially different from those in OpenEQA remains an open question due to evaluation being limited to two indoor household environment types (HM3D and ScanNet).

- **Faithfulness of generated critiques:** A systematic evaluation of their fine-grained faithfulness or their direct utility in, for example, few-shot learning for policy adaptation, was beyond the scope of this paper—score accuracy is evaluated but critique utility is untested.

- **Adaptive augmentation strategies:** The current set of contrastive augmentations in C-GRPO, while effective, is predefined and may not encompass the full spectrum of nuanced behaviors or subtle failure modes; learned or environment-specific augmentations could be superior.

- **Bias propagation from ground truth:** Any inherent biases, limitations, or specific characteristics of the Gemini-2.5-Pro model used for score generation could be subtly reflected in the score values, thereby influencing the RFT filtering process and accuracy reward component.

## Limitations

- **Dataset scale and generalization:** Strong results with 700 samples are promising but limited to two EQA benchmarks (HM3D, ScanNet), leaving true out-of-distribution performance untested.
- **Ablation completeness:** Key ablation results are presented but don't isolate all individual perturbation effects or exhaustively test test-time scaling dynamics.
- **Reproducibility constraints:** Critical implementation details remain underspecified, including exact RFT candidate counts, keyframe selection algorithm, and perturbation generation procedures.

## Confidence

**High Confidence:**
- Two-stage training (RFT → C-GRPO) improves performance over single-stage approaches
- Test-time scaling demonstrates measurable accuracy gains with increased K
- Contrastive rewards from structured perturbations provide meaningful training signal

**Medium Confidence:**
- Sample efficiency claims hold across EQA task variations
- The specific perturbation types contribute optimally to performance
- RFT filtering threshold selection is robust across different EQA benchmarks

**Low Confidence:**
- Generalizability to non-EQA embodied tasks
- Long-term stability of test-time scaling benefits
- Optimal configuration of test-time sampling parameters

## Next Checks

1. **Perturbation Ablation Study:** Systematically evaluate individual perturbation types (temporal, spatial, reasoning) with varying intensities to identify optimal contrastive signal strength and verify the reported +9.32% gain from reasoning rewards.

2. **Cross-Domain Generalization:** Test EQA-RM on embodied tasks outside the EQA domain (e.g., navigation-only or manipulation tasks) to assess whether the learned evaluative capabilities transfer beyond question-answering contexts.

3. **Ground Truth Verification:** Manually audit a subset of the 700 training samples and 1,546 test samples to verify the quality and consistency of Gemini-2.5-Pro generated ground truth critiques and scores, checking for systematic biases or inconsistencies.