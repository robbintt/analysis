---
ver: rpa2
title: 'GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents'
arxiv_id: '2505.15810'
source_url: https://arxiv.org/abs/2505.15810
tags:
- grounding
- arxiv
- training
- reward
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the R1-Zero-like training paradigm for
  GUI agents, focusing on visual grounding tasks. The authors identify three key challenges:
  excessive reasoning harms grounding accuracy, reward hacking occurs due to box size
  bias in reward functions, and GRPO introduces length and difficulty biases.'
---

# GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents

## Quick Facts
- arXiv ID: 2505.15810
- Source URL: https://arxiv.org/abs/2505.15810
- Reference count: 40
- Primary result: State-of-the-art GUI grounding model achieving 90.3% on ScreenSpot and 37.1% on ScreenSpot-Pro using only 17K training samples

## Executive Summary
This paper investigates R1-Zero-like training for GUI agents performing visual grounding tasks. The authors identify three key challenges: excessive reasoning harms grounding accuracy, reward hacking occurs due to box size bias in reward functions, and GRPO introduces length and difficulty biases. To address these, they propose three solutions: a Fast Thinking Template that eliminates reasoning, a box size-constrained reward function to mitigate hacking, and a difficulty-aware GRPO objective that removes length normalization. Trained on only 17K samples, their GUI-G1-3B model achieves state-of-the-art performance, scoring 90.3% on ScreenSpot and 37.1% on ScreenSpot-Pro, surpassing larger models while using fewer tokens and training stages.

## Method Summary
The authors train a Qwen2.5-VL-3B-Instruct model using VLM-R1 framework with three key modifications. First, they replace standard reasoning templates with a Fast Thinking Template that prompts for direct bounding box output without chain-of-thought reasoning. Second, they implement a composite reward function combining hit accuracy (RHit), IoU (RIoU), and box size constraints (RBox) with coefficients α=0.25 and β=0.125. Third, they modify GRPO to remove length normalization (replacing 1/|oi| with 1/Max_Tokens) and add difficulty weighting inversely proportional to relative box size. The model is trained for 3 days on 4×H800 GPUs using 17K filtered samples from UI-BERT and OS-Atlas datasets, with 8 rollouts per sample and no KL regularization.

## Key Results
- GUI-G1-3B achieves 90.3% accuracy on ScreenSpot (vs 88.6% for 34B reasoning model)
- GUI-G1-3B achieves 37.1% accuracy on ScreenSpot-Pro (vs 34.2% for 34B reasoning model)
- Model uses 41.6% fewer training tokens and 50% fewer training stages than baseline
- Fast Thinking Template outperforms Slow Thinking across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Fast Thinking Template for Grounding
Removing explicit chain-of-thought reasoning improves grounding accuracy by reducing interference with visual token processing. The template prompts for direct bounding box output without intermediate deliberation, preventing text tokens from competing with image tokens for attention capacity. This works because grounding relies more heavily on visual features than textual reasoning.

### Mechanism 2: Composite Reward with Box Size Regularization
Combining hit, IoU, and box-size rewards prevents opposing forms of reward hacking. RHit encourages smaller boxes (higher center-hit accuracy), RIoU encourages larger boxes (greater overlap), and RBox constrains predicted dimensions to match ground truth. The composite R = RHit + αRIoU + βRBox balances these competing pressures.

### Mechanism 3: Difficulty-Weighted GRPO without Length Normalization
Removing per-token length normalization and adding difficulty weighting shifts optimization focus toward harder samples. Standard GRPO divides by |oi|, creating length bias. Replacing with Max_Tokens constant removes this. Multiplying the objective by difficulty weight wq (inversely proportional to relative box size) assigns larger gradients to small-box samples.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: Core RL algorithm; understanding advantage normalization, token-level objective, and why length normalization introduces bias.
  - Quick check question: Given advantage scores [0.3, -0.1, 0.5] for three candidate responses of lengths [50, 100, 80] tokens, which response receives the highest per-token gradient under standard GRPO?

- **Concept: Visual Grounding in Multimodal Models**
  - Why needed here: Task formulation—mapping text instructions to bounding box coordinates as a generation problem.
  - Quick check question: If a model outputs `[x1, y1, x2, y2]` vs `{"bbox_2d": [x, y, w, h]}`, what changes are needed in the reward function?

- **Concept: Reward Hacking in RL**
  - Why needed here: Recognizing when models exploit reward signal structure rather than solving the intended task.
  - Quick check question: A grounding model achieves 95% center-hit accuracy but IoU averages 0.15. What reward component might it be overfitting, and what constraint could help?

## Architecture Onboarding

- **Component map**: Qwen2.5-VL-3B-Instruct -> Fast Thinking Template -> VLM-R1 framework -> Modified GRPO -> GUI-G1 model

- **Critical path**:
  1. Filter dataset: Prompt each sample 8× with base model; discard if all correct or all incorrect
  2. Rollout: Generate N=8 candidates per sample during training
  3. Compute rewards: RHit (binary), RIoU (continuous), RBox (size match)
  4. Normalize advantages: (r - μ) / σ across candidates
  5. Apply difficulty weight: wq = 0.5 + normalized_inverse(λq)
  6. Update policy: wq × GRPO objective with Max_Tokens constant

- **Design tradeoffs**:
  - α=0.25, β=0.125: IoU weighted lower than hit; box constraint is auxiliary. Tune if boxes drift.
  - No KL regularization: Faster but less stable; works here due to strong rule-based reward.
  - 17K samples: Low ceiling (acknowledged in Limitations); sufficient for analysis but scaling needs larger datasets.

- **Failure signatures**:
  - Boxes shrinking to points → RHit dominating (increase β)
  - Boxes expanding beyond targets → RIoU dominating (decrease α, increase β)
  - Correct responses too short / incorrect too long → Length normalization not removed
  - Small-box samples stuck at low accuracy → Difficulty weighting not applied or wq incorrectly computed

- **First 3 experiments**:
  1. Template ablation: Compare Fast vs Slow Thinking on held-out split; log accuracy, IoU, and output token count.
  2. Reward sweep: Grid search α∈{0.1, 0.25, 0.5}, β∈{0.05, 0.125, 0.25}; monitor relative box size trajectory during training.
  3. Difficulty weighting validation: Stratify test set by box size (small/medium/large); compare uniform vs difficulty-weighted training on each stratum's accuracy.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the "Fast Thinking" paradigm, which discourages reasoning, be effectively integrated with the deliberative requirements of action prediction and long-horizon planning? The paper states this work focuses exclusively on grounding and doesn't cover tasks like action prediction or long-horizon planning. It's unclear how to reconcile these conflicting optimal training regimes within a single GUI agent.

**Open Question 2**: Do the benefits of difficulty-aware weighting and the removal of length normalization persist when scaling training data beyond 17K samples? The model is trained on a "relatively small set of public datasets, which constrains its performance ceiling," and scaling up data is listed as future work. The inverse-size difficulty weighting may become noisy with massive datasets.

**Open Question 3**: Does the box-size constraint ($R_{Box}$) inadvertently penalize valid predictions in scenarios with ambiguous or loosely annotated ground truth boundaries? While $R_{Box}$ prevents extreme sizes, it forces conformity to specific aspect ratios. In high-resolution professional environments, visually correct predictions might reasonably differ in tightness from annotations.

## Limitations
- Trained on only 17K samples, constraining performance ceiling and generalization to new GUI contexts
- Fixed reward coefficients (α=0.25, β=0.125) may not generalize to datasets with different box size distributions
- Difficulty weighting assumes smaller boxes always indicate harder samples, which may not hold for all GUI contexts

## Confidence

**High confidence**: The Fast Thinking Template's effectiveness is well-supported by controlled experiments showing accuracy degradation with longer outputs and higher text-to-image ratios. The reward hacking analysis (shrinking/expanding boxes) is clearly demonstrated through training curves.

**Medium confidence**: The difficulty-weighted GRPO without length normalization shows promising results, but exact implementation details (Max_Tokens constant value, clipping epsilon) are unspecified, making exact reproduction challenging. The improvement from 34.2% to 37.1% on ScreenSpot-Pro is meaningful but uses a relatively small test set.

**Low confidence**: Claims about the general applicability of the R1-Zero-like paradigm to other visual grounding tasks beyond GUI agents are not directly tested. The paper doesn't explore whether the three identified problems appear in non-GUI contexts.

## Next Checks

**Check 1**: Test the Fast Thinking Template on a non-GUI visual grounding dataset (e.g., Flickr30k Entities or ReferItGame) to verify if reasoning elimination improves accuracy across different domains. Compare accuracy, IoU, and output token count between Fast and Slow templates.

**Check 2**: Conduct an ablation study varying the reward coefficients α and β across a wider range (e.g., α∈{0.1, 0.25, 0.5, 0.75}, β∈{0.05, 0.125, 0.25, 0.5}) to identify optimal settings for different box size distributions. Track relative box size trajectories to confirm reward hacking is mitigated across all settings.

**Check 3**: Validate the difficulty weighting assumption by creating a stratified test set with known difficulty levels (e.g., using box size, visual similarity, or human annotation). Compare training with uniform vs. difficulty-weighted objectives on each stratum to confirm that harder samples receive appropriate gradient allocation.