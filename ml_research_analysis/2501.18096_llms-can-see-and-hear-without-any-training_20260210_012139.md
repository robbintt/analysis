---
ver: rpa2
title: LLMs can see and hear without any training
arxiv_id: '2501.18096'
source_url: https://arxiv.org/abs/2501.18096
tags:
- image
- mils
- captioning
- generation
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MILS (Multimodal Iterative LLM Solver) is a training-free approach
  that leverages the reasoning capabilities of large language models (LLMs) to solve
  multimodal tasks across images, videos, and audio. The method works by iteratively
  generating candidate outputs (using an LLM as a GENERATOR) and scoring them with
  a multimodal model (SCORER), refining the outputs until convergence.
---

# LLMs can see and hear without any training

## Quick Facts
- arXiv ID: 2501.18096
- Source URL: https://arxiv.org/abs/2501.18096
- Authors: Kumar Ashutosh; Yossi Gandelsman; Xinlei Chen; Ishan Misra; Rohit Girdhar
- Reference count: 24
- Primary result: MILS achieves state-of-the-art zero-shot captioning results through iterative LLM reasoning

## Executive Summary
MILS (Multimodal Iterative LLM Solver) demonstrates that large language models can effectively process multimodal data without task-specific training. The approach iteratively generates and refines candidate outputs using an LLM as a generator and a multimodal model as a scorer. This enables emergent zero-shot performance across images, videos, and audio for tasks like captioning, image generation, style transfer, and cross-modal arithmetic. The method achieves competitive results on established benchmarks while requiring no training data for the target tasks.

## Method Summary
MILS operates through an iterative refinement process where an LLM generates candidate outputs and a multimodal scorer evaluates them. The system alternates between GENERATOR (LLM producing outputs) and SCORER (multimodal model evaluating candidates) phases until convergence. This framework supports various multimodal tasks including zero-shot captioning, text-to-image generation optimization, style transfer via Gram matrix optimization, and cross-modal arithmetic by inverting embeddings into text. The approach leverages the reasoning capabilities of LLMs while relying on pretrained multimodal models for evaluation.

## Key Results
- Achieves state-of-the-art zero-shot captioning performance on established benchmarks
- Improves text-to-image generation quality through iterative prompt optimization
- Enables style transfer by optimizing Gram matrix representations without paired training data
- Demonstrates cross-modal arithmetic capabilities by mapping between text, images, and audio

## Why This Works (Mechanism)
The approach succeeds by leveraging the compositional reasoning capabilities of LLMs combined with the perceptual capabilities of multimodal models. The iterative refinement process allows the LLM to explore the solution space systematically, while the SCORER provides feedback that guides improvement. This creates a self-supervised optimization loop where the LLM can discover effective strategies for multimodal tasks without explicit training. The method works because LLMs can reason about multimodal relationships when provided with appropriate scoring feedback.

## Foundational Learning
- **Multimodal embedding spaces**: Understanding how vision-language models like CLIP and BLIP represent cross-modal relationships is essential for interpreting SCORER feedback. Quick check: Visualize embedding similarity distributions for matched vs mismatched image-text pairs.
- **Gram matrix optimization**: Style transfer relies on matching feature correlations between content and style representations. Quick check: Verify Gram matrix similarity correlates with perceptual style similarity.
- **Iterative optimization dynamics**: The convergence behavior depends on the interaction between generator exploration and scorer feedback. Quick check: Plot score improvement curves across iterations for different tasks.

## Architecture Onboarding
**Component map**: LLM GENERATOR -> SCORER -> LLM GENERATOR (iterative loop)
**Critical path**: The iterative refinement loop where LLM outputs are scored and refined is the core mechanism driving performance improvements.
**Design tradeoffs**: Balances computational cost (multiple LLM calls) against zero-shot flexibility and performance. Relies heavily on the quality of the underlying multimodal scorer.
**Failure signatures**: Poor convergence when scorer provides noisy or inconsistent feedback; performance limited by the representational capacity of the multimodal model.
**First experiments**: 1) Compare MILS zero-shot captioning against frozen multimodal models. 2) Test iterative refinement on simple arithmetic tasks before scaling to complex modalities. 3) Measure score improvement per iteration to identify optimal stopping criteria.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- High computational cost due to iterative LLM inference required for each refinement step
- Strong dependence on the quality and capabilities of the underlying multimodal scorer models
- Performance may be optimizing for specific metric definitions rather than genuine understanding

## Confidence
- Zero-shot captioning performance: Medium confidence (relies on scorer model quality)
- Cross-modal arithmetic capabilities: Medium confidence (limited to well-defined relationships)
- Generalizability across domains: Low confidence (tested primarily on benchmark datasets)

## Next Checks
1. Ablation studies comparing MILS performance when using different SCORER models (CLIP vs BLIP vs other vision-language models) to quantify the approach's dependence on specific model choices
2. Cost-benefit analysis measuring the trade-off between iteration count and performance gains, including wall-clock time and computational resource comparisons to trained baselines
3. Robustness testing across diverse domain shifts (different artistic styles, languages, or audio characteristics) to evaluate generalization beyond the benchmark datasets used in validation