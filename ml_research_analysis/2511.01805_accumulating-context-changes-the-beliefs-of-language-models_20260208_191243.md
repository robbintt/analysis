---
ver: rpa2
title: Accumulating Context Changes the Beliefs of Language Models
arxiv_id: '2511.01805'
source_url: https://arxiv.org/abs/2511.01805
tags:
- belief
- arxiv
- support
- oppose
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) change
  their beliefs as context accumulates through interaction or reading. The authors
  propose a three-stage framework to measure stated beliefs and behaviors before and
  after context accumulation, distinguishing between intentional tasks (debate, persuasion)
  and non-intentional tasks (in-depth reading, research).
---

# Accumulating Context Changes the Beliefs of Language Models

## Quick Facts
- **arXiv ID:** 2511.01805
- **Source URL:** https://arxiv.org/abs/2511.01805
- **Authors:** Jiayi Geng; Howard Chen; Ryan Liu; Manoel Horta Ribeiro; Robb Willer; Graham Neubig; Thomas L. Griffiths
- **Reference count:** 40
- **Primary result:** LLM beliefs and behaviors shift substantially through context accumulation, with stated beliefs changing earlier than tool-use behaviors

## Executive Summary
This paper investigates whether large language models change their beliefs as context accumulates through interaction or reading. The authors propose a three-stage framework to measure stated beliefs and behaviors before and after context accumulation, distinguishing between intentional tasks (debate, persuasion) and non-intentional tasks (in-depth reading, research). They find substantial belief and behavior shifts across both types of tasks: GPT-5 exhibits a 54.7% shift in stated beliefs after 10 rounds of debate, while Grok 4 shows a 27.2% shift on political issues after reading opposing viewpoints. Belief shifts occur earlier than behavioral changes in conversation, and reading length amplifies shifts for longer, more coherent content. The findings reveal that LLM assistants' opinions and actions become unreliable after extended use, raising concerns about their long-term reliability in real-world applications.

## Method Summary
The authors use a three-stage framework to measure belief and behavior shifts: (1) elicit initial belief and behavior, (2) accumulate context through task-specific modules (debate, persuasion, reading, or research), and (3) re-elicit belief and behavior. For intentional tasks, they pair models from different families, run 3 seeds on 30 safety + 30 moral queries. For non-intentional tasks, they present curated documents (up to 80k words) or use LangChain's Open Deep Research agent with default workflow. Shift percentages are computed by comparing pre/post responses, with LLM-as-judge (GPT-5-mini) mapping behaviors to stances.

## Key Results
- GPT-5 exhibits a 54.7% shift in stated beliefs after 10 rounds of debate
- Grok 4 shows a 27.2% shift on political issues after reading opposing viewpoints
- Belief shifts occur earlier than behavioral changes in conversation
- Reading length amplifies shifts for longer, more coherent content

## Why This Works (Mechanism)

### Mechanism 1: Persuasion-Induced Malleability
- Claim: Intentional persuasive interaction causes significant shifts in stated beliefs, often early in the conversation.
- Mechanism: Models update output distributions based on accumulating context of debate, predicting responses aligned with new context rather than initial priors.
- Evidence anchors: GPT-5 shows 54.7% shift after 10 rounds; persuasion induces stronger shifts (72.7% with information technique); BASIL corpus suggests sycophancy drives belief shifts.

### Mechanism 2: Contextual Framing via Reading
- Claim: Passive reading of long, coherent texts shifts beliefs via "contextual framing" rather than specific factual updates.
- Mechanism: As context length grows, the model's representation space orients around the document's narrative structure, shifting probability mass toward source material's stance.
- Evidence anchors: Reading length amplifies shifts for coherent content; masking topically relevant sentences doesn't consistently reduce shifts; representation geometry research supports framing effects.

### Mechanism 3: Behavioral-Belief Decoupling
- Claim: Stated beliefs shift faster and more significantly than actual tool-use behaviors.
- Mechanism: Stated beliefs are verbal outputs directly conditioned on persuasive context; behaviors require integrating beliefs with task constraints and safety policies, creating lag between what models say and do.
- Evidence anchors: Belief shifts occur earlier than behavioral changes; magnitudes sometimes diverge indicating partial misalignment; ABBEL corpus suggests bottlenecks in translating context to action.

## Foundational Learning
- **In-Context Learning (ICL)**: Why needed: The entire phenomenon relies on the model adapting responses based on context sequence without weight updates. Quick check: Does the model retain the belief shift if context history is cleared?
- **Sycophancy**: Why needed: Understanding whether the model is truly "changing its mind" or merely agreeing to be helpful. Quick check: Does the shift persist when the user changes their stance in the next turn?
- **Agentic Action Space**: Why needed: To distinguish between generating text (stated belief) and executing API calls/tools (behavior), which have different constraint profiles. Quick check: Can a model verbally support an action while selecting a contradictory tool?

## Architecture Onboarding
- **Component map**: Stage 1 (Probing) -> Stage 2 (Accumulation) -> Stage 3 (Re-Probing) -> Evaluator
- **Critical path**: The definition of "Shift" (d_rescale) is the key metric; ensure Likert scale or binary choice is consistently administered before and after context accumulation
- **Design tradeoffs**: Using tool-use as behavior proxy is robust but expensive; using only verbal queries is cheaper but misses "agentic" drift
- **Failure signatures**: Models refusing safety queries (common in Claude-4) lowers effective sample size; open-source models showing low sensitivity suggests context window limitations
- **First 3 experiments**:
  1. Sanity Check: Replicate "Information" persuasion technique on GPT-5 to verify 72.7% shift claim
  2. Decoupling Test: Measure time-lag between verbal shift and behavioral shift by probing every 2 rounds
  3. Framing Validation: Run "In-depth reading" task with shuffled text to confirm if "coherence" is required for shift

## Open Questions the Paper Calls Out
- **Open Question 1**: How long do belief shifts persist after context accumulation, and do subsequent interactions reinforce or counteract them? [explicit] Section 6 states that "temporal dynamics... remain unexplored" and asks "how long belief shifts persist, whether they decay over time, and how subsequent interactions might reinforce or counteract initial shifts."
- **Open Question 2**: Which specific pieces of contextual evidence are causally responsible for driving belief changes? [explicit] Section 6 notes that "the detailed underlying mechanisms remain unclear" and suggests using "causal-tracing methods to better identify which pieces of contextual evidence are most relevant... and responsible."
- **Open Question 3**: Do belief shifts manifest differently in collaborative problem-solving or creative writing tasks compared to debate and research? [explicit] Section 6 lists "collaborative problem-solving, creative writing, or multi-agent collaborations" as "alternative settings" that "may produce qualitatively different belief shift dynamics and warrant further investigation."

## Limitations
- The findings rely heavily on proprietary models (GPT-5, Claude-4-Sonnet, Grok-4, Gemini-2.5-Pro) that were not yet publicly available at publication
- Dataset completeness is uncertainâ€”only partial examples are provided in appendices, with full moral dilemma and safety query pairs not released
- Research task implementation using LangChain's Open Deep Research agent lacks specific configuration details beyond "default workflow"

## Confidence
- **High Confidence**: In-context learning driving belief shifts; stated beliefs shifting more readily than behaviors; coherent narrative framing driving shifts
- **Medium Confidence**: Specific magnitude of shifts (54.7%, 27.2%); shifts occurring earlier in conversation than reading; shifts not primarily driven by specific topic-relevant information
- **Low Confidence**: Generalizability to open-source models; behavioral belief-decoupling mechanism requiring more granular temporal analysis

## Next Checks
1. Cross-Model Validation: Replicate persuasion task with accessible models (GPT-4o, Claude-3.5-Sonnet) using information technique to verify if 72.7% shift magnitude is achievable
2. Temporal Decoupling Analysis: Implement probing every 2 rounds during debate to map exact timeline of belief vs. behavior shifts
3. Coherence Control Test: Run in-depth reading task with original coherent text, shuffled text of equal length, and randomly selected topic-relevant sentences to definitively test whether narrative coherence drives framing effect