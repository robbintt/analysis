---
ver: rpa2
title: 'Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity'
arxiv_id: '2510.21303'
source_url: https://arxiv.org/abs/2510.21303
tags:
- data
- multiplicity
- datasets
- train
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for understanding how data processing
  choices influence predictive multiplicity, the phenomenon where multiple models
  achieve similar training performance but make conflicting predictions. By reframing
  data processing as choosing between neighbouring datasets, the authors provide theoretical
  and empirical insights into how overlap in class distributions affects downstream
  multiplicity.
---

# Data as a Lever: A Neighbouring Datasets Perspective on Predictive Multiplicity

## Quick Facts
- arXiv ID: 2510.21303
- Source URL: https://arxiv.org/abs/2510.21303
- Reference count: 40
- Key outcome: A framework for understanding how data processing choices influence predictive multiplicity through the lens of neighbouring datasets

## Executive Summary
This paper introduces a novel framework for understanding predictive multiplicity—the phenomenon where multiple models achieve similar training performance but make conflicting predictions—by reframing data processing as a choice between neighbouring datasets. The authors demonstrate that the Rashomon parameter, which measures the number of well-performing models in the hypothesis space, is a unifying quantity that links data processing choices to multiplicity. By formalizing the relationship between distribution overlap in class densities and the Rashomon parameter, they show that greater inter-class distribution overlap leads to lower multiplicity under a shared Rashomon parameter, reversing trends observed in prior work. The framework is applied to active learning and data imputation, where multiplicity-aware algorithms are shown to effectively steer multiplicity up or down without sacrificing accuracy.

## Method Summary
The authors formalize predictive multiplicity by introducing the concept of neighbouring datasets, where data processing choices define a set of datasets that differ from the original in a measurable way. They prove that the Rashomon parameter is a unifying quantity linking data processing to multiplicity, and derive theoretical results showing that under a shared Rashomon parameter, greater inter-class distribution overlap leads to lower multiplicity. The framework is applied to two practical scenarios: active learning, where the goal is to select samples that maximize multiplicity, and data imputation, where missing values are imputed to steer multiplicity up or down. Empirical validation is conducted on synthetic and real-world datasets, demonstrating the effectiveness of the proposed algorithms in controlling multiplicity without sacrificing accuracy.

## Key Results
- The Rashomon parameter is shown to be a unifying quantity linking data processing choices to predictive multiplicity.
- Under a shared Rashomon parameter, greater inter-class distribution overlap leads to lower multiplicity, reversing prior trends.
- Multiplicity-aware algorithms in active learning and data imputation can effectively steer multiplicity up or down without sacrificing accuracy.
- Higher missing data ratios amplify the steerability of data imputation, making the framework more impactful in practice.

## Why This Works (Mechanism)
The framework works by reframing data processing as a choice between neighbouring datasets, where each dataset is a small perturbation of the original. By formalizing the relationship between distribution overlap in class densities and the Rashomon parameter, the authors show that the Rashomon parameter acts as a unifying quantity that governs multiplicity. This allows them to derive theoretical results about how changes in data processing affect multiplicity, and to design algorithms that can actively steer multiplicity in desired directions. The key insight is that by controlling the overlap in class distributions, one can control the Rashomon parameter and, consequently, the multiplicity.

## Foundational Learning
- **Predictive Multiplicity**: The phenomenon where multiple models achieve similar training performance but make conflicting predictions. Why needed: Central to understanding the limitations of model selection and evaluation. Quick check: Can be observed by training multiple models on the same dataset and comparing their predictions on test data.
- **Rashomon Parameter**: A measure of the number of well-performing models in the hypothesis space. Why needed: Acts as a unifying quantity linking data processing to multiplicity. Quick check: Can be estimated by training multiple models and measuring the diversity of their predictions.
- **Neighbouring Datasets**: Datasets that differ from the original in a measurable way, defined by data processing choices. Why needed: Allows formalization of how data processing affects multiplicity. Quick check: Can be generated by applying small perturbations to the original dataset.
- **Distribution Overlap**: The degree of overlap between class densities in the feature space. Why needed: Governs the Rashomon parameter and, consequently, multiplicity. Quick check: Can be measured using divergence metrics like KL divergence or Wasserstein distance.
- **Active Learning**: A scenario where the goal is to select samples that maximize multiplicity. Why needed: Demonstrates the practical application of the framework. Quick check: Can be implemented by training models on selected samples and measuring their diversity.
- **Data Imputation**: The process of imputing missing values to steer multiplicity up or down. Why needed: Shows how the framework can be applied to real-world data processing challenges. Quick check: Can be evaluated by comparing multiplicity before and after imputation.

## Architecture Onboarding

**Component Map:**
Original Dataset -> Neighbouring Datasets -> Rashomon Parameter -> Multiplicity

**Critical Path:**
1. Define data processing choices that generate neighbouring datasets.
2. Measure the Rashomon parameter for each neighbouring dataset.
3. Relate the Rashomon parameter to multiplicity using theoretical results.
4. Design algorithms to steer multiplicity by controlling the Rashomon parameter.

**Design Tradeoffs:**
- Theoretical rigor vs. practical applicability: The framework is mathematically sound but may not generalize to all data processing scenarios.
- Simplicity vs. expressiveness: The Rashomon parameter is a simple unifying quantity but may not capture all nuances of multiplicity.
- Control vs. accuracy: Algorithms can steer multiplicity but may sacrifice accuracy in some cases.

**Failure Signatures:**
- Multiplicity does not change as expected after data processing.
- The Rashomon parameter does not correlate with multiplicity in practice.
- Algorithms fail to control multiplicity without sacrificing accuracy.

**First Experiments:**
1. Apply the framework to feature selection and measure its effect on multiplicity.
2. Test the framework on high-dimensional datasets to assess its scalability.
3. Evaluate the framework's effectiveness on ensemble methods and neural networks.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's reliance on the Rashomon parameter as a unifying quantity may not capture all nuances of multiplicity in real-world data.
- The assumption of a constant Rashomon parameter across neighbouring datasets may not hold in practice.
- The empirical validation is limited to specific domains (active learning and data imputation), and results may not generalize to other data processing scenarios or model classes.

## Confidence
- **High**: Theoretical framework and core mathematical proofs are rigorous and well-founded.
- **Medium**: Empirical results are promising but based on specific use cases and datasets.
- **Low**: Framework's generalizability beyond tested scenarios, particularly in complex or high-dimensional settings, remains untested.

## Next Checks
1. Test the framework's applicability to other data processing methods (e.g., feature selection, data augmentation) and model classes (e.g., neural networks, ensemble methods).
2. Validate the assumption of a constant Rashomon parameter across neighbouring datasets in diverse real-world datasets with varying levels of multiplicity.
3. Extend the empirical evaluation to high-dimensional or complex data distributions to assess the framework's robustness and scalability.