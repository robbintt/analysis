---
ver: rpa2
title: Online Multi-LLM Selection via Contextual Bandits under Unstructured Context
  Evolution
arxiv_id: '2506.17670'
source_url: https://arxiv.org/abs/2506.17670
tags:
- context
- cost
- user
- regret
- linucb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a contextual bandit framework for online multi-LLM
  selection under unstructured prompt evolution. It addresses the challenge of selecting
  the best LLM in a sequential, multi-step interaction where the prompt context evolves
  dynamically based on prior LLM outputs.
---

# Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution

## Quick Facts
- **arXiv ID**: 2506.17670
- **Source URL**: https://arxiv.org/abs/2506.17670
- **Reference count**: 40
- **Primary result**: Proposed contextual bandit algorithms achieve 74.84% average accuracy on multi-step LLM selection tasks, outperforming existing routing methods

## Executive Summary
This paper introduces a contextual bandit framework for online multi-LLM selection under unstructured prompt evolution. The method addresses the challenge of selecting the best LLM in a sequential, multi-step interaction where the prompt context evolves dynamically based on prior LLM outputs. The proposed approach uses a myopic regret minimization strategy with a Greedy LinUCB algorithm, maintaining separate linear models for each LLM and selecting the one with the highest upper confidence bound at each step. The framework includes budget-aware and positionally-aware extensions to handle variable costs and prioritize early-stage user satisfaction.

## Method Summary
The method employs a contextual bandit approach where each query is embedded into a 384-dimensional vector using sentence transformers. For each of K LLMs, the algorithm maintains a linear model (A_k, b_k) for ridge regression. At each step h, it computes the UCB score for each LLM as the predicted reward plus an exploration bonus based on the context's uncertainty. The algorithm selects the LLM with the highest UCB, receives binary feedback, and updates only the selected LLM's parameters. The context evolves by appending the previous LLM's output to the prompt. The framework extends to budget-aware selection using cost-aware scoring and positionally-aware selection using a knapsack heuristic that prioritizes early high-quality responses.

## Key Results
- Positionally-aware knapsack heuristic achieves highest average accuracy of 74.84% across benchmarks
- Budget-aware variant offers strong cost-efficiency with average cost of 2.97e-4 USD versus Greedy's 9.11e-4 USD
- Greedy LinUCB outperforms existing methods like MetaLLM (72% vs 86% accuracy on MMLU-Pro)
- Most queries resolve in 1.5-2.5 steps on average with cascade depth H=4

## Why This Works (Mechanism)

### Mechanism 1
**Claim**: Per-step LinUCB decisions achieve sublinear myopic regret even without modeling how prompts evolve over time.
**Mechanism**: At each step h, maintain separate ridge regression models (A_k, b_k) per LLM. Compute UCB_k = ⟨x_{t,h}, θ̂_k⟩ + α·||x_{t,h}||_{A_k^{-1}} and select argmax. Update only the selected LLM's parameters after receiving binary feedback. The exploration bonus ensures under-explored LLMs get chances, while exploitation uses learned preferences.
**Core assumption**: Expected feedback is linear in context: E[r_{t,h}] = ⟨x_{t,h}, θ*_{a_{t,h}}⟩ (surrogate model for discrete binary signals).
**Evidence anchors**:
- [abstract]: "We formalize a notion of myopic regret and develop a LinUCB-based algorithm that provably achieves sublinear regret without relying on future context prediction."
- [Section 4, Theorem 1]: R(T) = O(√(KdT H) · (SL + √λS) · log(KT L²/λδ))
- [corpus]: Weak direct evidence; corpus papers address routing/scheduling but don't validate myopic vs. trajectory optimization trade-offs.
**Break condition**: If linear model severely mis-specifies true reward, or if long-horizon dependencies dominate (myopic choices consistently harm later steps).

### Mechanism 2
**Claim**: Explicit budget-awareness with two-level confidence (optimistic in reward, conservative in cost) enables operation within per-query budget constraints while preserving learning.
**Mechanism**: Compute Score_k = UCB_k / max{ĉ_k - β_k, ε} where ĉ_k is empirical mean cost and β_k is Hoeffding-based confidence width. Select highest-scoring arm whose upper-bound cost (ĉ_k + β_k) fits remaining budget. This encourages exploration of potentially high reward/cost LLMs while ensuring budget feasibility.
**Core assumption**: Costs c_{t,h,k} are i.i.d. per arm with unknown mean μ_k and sub-Gaussian noise (Assumption 5).
**Evidence anchors**:
- [abstract]: "We further introduce budget-aware...extensions to accommodate variable query costs"
- [Section 5.1, Theorem 2]: Regret bound includes both reward regret O(SL√KdT H) and cost regret O(Σ C_max/μ²_k · √(T log(TK/δ)))
- [corpus]: No direct corpus validation of budget-aware bandits for LLM selection specifically.
**Break condition**: If costs are highly non-stationary, or budget is so tight that meaningful exploration is blocked (Figure 3 shows zero performance at 0.00005 budget).

### Mechanism 3
**Claim**: Prioritizing high-UCB LLMs early in multi-step interactions improves effective user satisfaction by aligning with positional preference (earlier success = higher utility).
**Mechanism**: Solve 0-1 knapsack at each step using UCB scores as rewards and empirical costs as weights. From the knapsack solution, immediately select the arm with highest UCB. Repeat until budget exhausted. This front-loads capable (often expensive) models rather than deferring them.
**Core assumption**: Users exhibit positional bias—early high-quality responses are more valuable than late ones.
**Evidence anchors**:
- [abstract]: "...positionally-aware (favoring early-stage satisfaction) extensions..."
- [Section 6.1.2, Table 3]: Knapsack heuristic achieves 71.10% accuracy in first step (95% of total 74.84%), vs. Greedy LinUCB's 47.89% first-step (71% of total).
- [corpus]: No direct corpus validation of positional utility in multi-LLM settings.
**Break condition**: If positional preference assumption fails for specific user populations, or noisy cost estimates cause knapsack to make systematically poor selections.

## Foundational Learning

**Concept: Linear Contextual Bandits / LinUCB**
- **Why needed here**: Core algorithmic engine; must understand UCB exploration-exploitation trade-off, ridge regression updates, and confidence ellipsoids to implement and debug.
- **Quick check question**: Given context x_t and arms with estimates (θ̂_k, A_k), why does LinUCB add α||x_t||_{A_k^{-1}} to the predicted reward instead of just using θ̂_k^T x_t?

**Concept: Sublinear Regret and Its Implications**
- **Why needed here**: Paper's theoretical contribution is proving R(T) = Õ(√T); need to interpret what this guarantees about learning efficiency.
- **Quick check question**: If cumulative regret is O(√T), what happens to average per-round regret as T → ∞? What does this imply about convergence?

**Concept: Black-Box Context Evolution and Why RL Fails**
- **Why needed here**: Central motivation for the myopic approach; understand why trajectory optimization is infeasible here.
- **Quick check question**: The context evolution function g(x, a, R, r) is "unstructured and unlearnable." Why does this violate standard RL assumptions, and why does myopic regret bypass this issue?

## Architecture Onboarding

**Component map**: Query → Embed (384-d) → Compute UCB for all K LLMs → Select top (or knapsack-filtered) → Call LLM API → Receive response → Collect binary feedback → Update (A_{a}, b_{a}) for selected LLM → If r=0 and budget remains: evolve context via g → Loop to next step (max H=4)

**Critical path**: Query → Embed (384-d) → Compute UCB for all K LLMs → Select top (or knapsack-filtered) → Call LLM API → Receive response → Collect binary feedback → Update (A_{a}, b_{a}) for selected LLM → If r=0 and budget remains: evolve context via g → Loop to next step (max H=4)

**Design tradeoffs**:
- **Greedy vs. Budget-Aware vs. Knapsack**: Greedy (highest accuracy, no cost control), Budget-Aware (strict cost limits, slightly lower accuracy), Knapsack (best early-stage success, moderate cost)
- **Exploration parameter α=0.675**: Balances exploration speed vs. exploitation confidence; higher α increases early diversity but slows convergence
- **Cascade depth H=4**: More steps allow error recovery but increase latency/cost; Table 3 shows most queries resolve in 1.5-2.5 steps on average

**Failure signatures**:
- **Cold start**: Initial queries select poorly before feedback accumulates; mitigated by 20% offline initialization
- **Cost overrun (Greedy only)**: Average cost 9.11e-4 USD vs. Budget-Aware's 2.97e-4 USD (Table 2)
- **Context pollution**: Appendix B shows 3/60 cases where prior context hurt subsequent performance
- **Feedback noise**: Grader LLM may misclassify; binary discretization loses gradient signal

**First 3 experiments**:
1. **Replicate main result on single dataset**: Implement Greedy LinUCB, run on MMLU-Pro 80% split, verify ~86% accuracy vs. MetaLLM ~72% (Table 1). Log per-step accuracy breakdown.
2. **Ablate cascade depth**: Run Greedy LinUCB on Math-500 with H ∈ {1, 2, 3, 4}. Plot accuracy vs. average steps; identify diminishing returns point.
3. **Budget sensitivity reproduction**: Run Budget-Aware LinUCB on AIME with budgets [0.00005, 0.00015, 0.001, 0.00214, 0.02]. Reproduce Figure 3 curve; confirm crossover where Knapsack surpasses Budget-Aware.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question**: Can the framework be extended to incorporate non-myopic planning under partially learnable prompt dynamics?
- **Basis in paper**: [explicit] The conclusion states future work includes "incorporating non-myopic planning under partially learnable prompt dynamics."
- **Why unresolved**: The current method relies on a myopic regret minimization strategy (Greedy LinUCB) because it assumes the context evolution function g is entirely unstructured and unlearnable.
- **What evidence would resolve it**: A theoretical and empirical demonstration of an algorithm that optimizes trajectory-level rewards without requiring full simulation of the transition function g.

**Open Question 2**
- **Question**: How does the algorithm's performance degrade or adapt when user feedback is noisy or delayed rather than immediate?
- **Basis in paper**: [explicit] The authors note "the framework assumes access to reliable user feedback, which may be noisy or delayed in practice."
- **Why unresolved**: The current theoretical regret bounds rely on the assumption that binary feedback r_{t,h} is observed immediately after the LLM response to update model parameters A and b.
- **What evidence would resolve it**: Regret analysis and experiments showing sublinear regret bounds in a "delayed feedback" bandit setting.

**Open Question 3**
- **Question**: How can the selection mechanism adapt to a dynamic LLM pool where models have varying availability or changing costs over time?
- **Basis in paper**: [explicit] The conclusion lists as a limitation: "we treat the LLM pool as fixed, whereas practical deployments may involve dynamic model availability."
- **Why unresolved**: The Algorithm 1 initialization sets fixed matrices A_k, b_k for a static set [K] LLMs, lacking a mechanism to handle arms that appear or disappear during deployment.
- **What evidence would resolve it**: An extension of the algorithm to the "sleeping bandits" setting with theoretical guarantees for non-stationary arm sets.

**Open Question 4**
- **Question**: Can a mechanism be developed to robustly filter or transform context to prevent performance degradation caused by misleading prior outputs?
- **Basis in paper**: [explicit] Appendix B notes that context sometimes hurts performance and states this "motivate[s] further research into robustly determining when and how context should be passed or filtered."
- **Why unresolved**: The current framework always passes the previous output R_{t,h} to the context evolution function g, even though experiments showed this misled the subsequent LLM in specific cases.
- **What evidence would resolve it**: A "context filtering" heuristic or learned function that improves cumulative accuracy by selectively withholding context compared to the baseline "always-pass" method.

## Limitations

- **Black-box assumption mismatch**: The paper claims LLM APIs are black boxes justifying the myopic approach, but implementation uses template-based context evolution that contradicts this theoretical foundation.
- **Binary feedback restriction**: Framework assumes binary rewards (correct/incorrect) limiting applicability to tasks where partial credit or continuous quality scores exist.
- **Offline initialization dependence**: 20% offline phase is critical for avoiding cold start problems, but sensitivity to initialization quality and size is not thoroughly analyzed.

## Confidence

**High Confidence**: The core regret bounds for Greedy LinUCB (Theorem 1) and the algorithmic implementation are well-founded. The sublinear regret claim follows standard contextual bandit theory, and the empirical results consistently show the proposed methods outperforming baselines across multiple benchmarks.

**Medium Confidence**: The budget-aware extension (Theorem 2) has reasonable theoretical grounding, but the practical cost estimates depend heavily on empirical averages that may not capture real-world cost variability. The knapsack heuristic for positional awareness shows strong results but relies on an assumed positional utility that may not generalize.

**Low Confidence**: The claim that "LLM APIs are black boxes" justifying the myopic approach is questionable given that the implementation uses template-based context evolution. The theoretical motivation doesn't fully align with the practical approach.

## Next Checks

1. **Cold start sensitivity**: Run experiments with varying offline initialization sizes (0%, 5%, 10%, 20%, 40%) to quantify performance degradation and identify minimum viable initialization requirements.

2. **Cost estimate validation**: Implement the system with real-time cost tracking (rather than offline empirical estimates) and measure how often the budget-aware variant actually prevents cost overruns versus false positives that block exploration.

3. **Context evolution impact**: Systematically compare performance when context evolution is template-based versus when it's truly unstructured (e.g., using different random perturbations or black-box transformations) to validate whether the theoretical assumptions match practical requirements.