---
ver: rpa2
title: 'Evaluating the Evaluators: Are readability metrics good measures of readability?'
arxiv_id: '2508.19221'
source_url: https://arxiv.org/abs/2508.19221
tags:
- readability
- metrics
- human
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates readability metrics for plain language summarization.
  It finds that traditional metrics like Flesch-Kincaid Grade Level (FKGL) correlate
  poorly with human judgments of readability, with Pearson correlations below 0.3
  for most metrics.
---

# Evaluating the Evaluators: Are readability metrics good measures of readability?

## Quick Facts
- arXiv ID: 2508.19221
- Source URL: https://arxiv.org/abs/2508.19221
- Reference count: 25
- Key outcome: Language models significantly outperform traditional readability metrics for evaluating plain language summaries, with best models achieving 0.56 Pearson correlation with human judgments versus 0.37 for best traditional metric.

## Executive Summary
This paper systematically evaluates readability metrics for plain language summarization by comparing traditional metrics (FKGL, DCRS, CLI, etc.) against language model evaluators using human judgments as ground truth. The study reveals that popular metrics like Flesch-Kincaid Grade Level correlate poorly with human perceptions of readability for scientific plain language summaries, while language models capture deeper attributes like required background knowledge and concept accessibility. The authors recommend discontinuing FKGL use and instead combining better-performing traditional metrics (DCRS and CLI) with language model evaluators for assessing plain language summaries.

## Method Summary
The study evaluates 8 traditional readability metrics using the `py-readability-metrics` package against a human-annotated dataset of 60 summaries rated 1-5 for reading ease. Five language models (Mistral 7B, Mixtral 7B, Gemma 7B, Llama 3.1 8B, Llama 3.3 70B) are prompted to score summaries using three different prompt styles, with the "Own Reasoning" prompt yielding the best results. Pearson and Kendall-Tau correlations with human judgments are computed, and statistical significance between metrics is tested using Williams' test. The analysis extends to 10 popular summarization datasets to examine readability variation across domains.

## Key Results
- FKGL, the most popular readability metric, achieves only 0.16 Pearson correlation with human judgments
- Language models outperform traditional metrics, with Llama 3.3 70B achieving 0.56 Pearson correlation
- PLS datasets show wide readability variation, with some datasets intended for general audiences showing similar readability to expert-targeted datasets
- The authors recommend using DCRS and CLI metrics combined with LM evaluators instead of FKGL

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Traditional readability metrics correlate poorly with human judgments for PLS because they rely on surface-level lexical features.
- **Mechanism:** Metrics like FKGL count syllables, word length, and sentence length as proxies for complexity. These proxies fail to capture whether technical terms are explained, whether context is provided, or whether background knowledge is required—attributes humans actually use to judge readability.
- **Core assumption:** Human readability judgments in PLS depend on semantic and contextual factors, not just lexical complexity.
- **Evidence anchors:**
  - [abstract] "most correlate poorly with human judgments, including the most popular metric, FKGL"
  - [section 3.2] "All of the metrics, except for DCRS and Spache, use lexical features such as number of syllables or length of sentences"
  - [section 5.1] "They measure lexical properties, such as number of syllables in a word, which penalizes summaries for using clearly defined technical terms"
  - [corpus] Neighbor paper "Readability Reconsidered" confirms "measurements that rely on surface-level text properties" as a key limitation.
- **Break condition:** If texts with identical lexical profiles but different explanations/context show similar metric scores, the mechanism holds. If FKGL correlation with humans exceeds 0.4 on new PLS datasets, mechanism may not generalize.

### Mechanism 2
- **Claim:** Language models outperform traditional metrics by reasoning over deeper readability attributes such as required background knowledge and concept explanations.
- **Mechanism:** LMs encode linguistic knowledge that enables them to evaluate whether technical terms are defined, whether concepts are accessible to non-experts, and whether sufficient context is provided—factors traditional metrics ignore.
- **Core assumption:** LM performance on this evaluation task transfers from their pre-trained language understanding capabilities.
- **Evidence anchors:**
  - [abstract] "LMs better capture deeper measures of readability, such as required background knowledge"
  - [section 4.3] Best LM (Llama 3.3 70B) achieves 0.56 Pearson correlation vs. 0.37 for best traditional metric (DCRS)
  - [section 4.4/keyword analysis] LM reasoning references "explanations for the non-expert" and "concepts in an accessible manner" for high-scoring summaries
  - [corpus] Neighbor paper "Human-Aligned Code Readability Assessment with Large Language Models" reports similar LM advantages in code readability, suggesting cross-domain pattern.
- **Break condition:** If smaller LMs significantly underperform larger ones, mechanism depends on model scale. If prompts that explicitly ask LMs to compute FKGL-style features reduce performance gap, mechanism weakens.

### Mechanism 3
- **Claim:** The poor correlation between traditional metrics and human judgments stems from definitional mismatch—education-grade complexity vs. non-expert accessibility.
- **Mechanism:** Traditional metrics were designed to match text complexity to years of schooling (e.g., 9th-grade reading level). PLS requires a different definition: whether a non-expert adult can understand the content. These definitions diverge when technical terms are well-explained but lexically complex, or when acronyms appear short but are unexplained.
- **Core assumption:** The August et al. (2024) human judgment dataset represents PLS-appropriate readability definitions.
- **Evidence anchors:**
  - [section 5.1] "Traditional readability metrics typically define a 'readable' text as one with an appropriate text complexity for the number of years of education...In contrast, the field of PLS typically defines a 'readable' text as one that gives a non-expert, adult reader an overall understanding"
  - [section 4.4/Table 6] Example shows FKGL rating a well-explained kids' summary as "college-graduate" level due to long words like "harvesting" and "electricity"
  - [corpus] Neighbor paper "Readability Measures and Automatic Text Simplification" discusses construct validity issues in readability measurement.
- **Break condition:** If traditional metrics perform well on PLS datasets targeting specific grade levels (e.g., K-12), definitional mismatch is domain-specific rather than fundamental.

## Foundational Learning

- **Pearson vs. Kendall-Tau Correlation**
  - Why needed here: The paper reports both correlations; understanding their differences clarifies result interpretation. Pearson measures linear relationships; Kendall-Tau measures rank-order agreement and is more robust to outliers.
  - Quick check question: If a metric ranks all summaries correctly but with non-linear scaling (e.g., scores 1,2,3 where humans rate 1,4,5), which correlation would be higher?

- **Readability Metric Formulas (FKGL, DCRS, CLI)**
  - Why needed here: Understanding what each metric measures explains their differing correlations with human judgment. FKGL uses syllables and sentence length; DCRS uses word familiarity lists; CLI uses characters per word and sentences.
  - Quick check question: Why would DCRS (word familiarity) correlate better with human PLS judgments than FKGL (syllable count)?

- **Language Model Evaluation Protocols**
  - Why needed here: The paper uses LMs as evaluators via prompted scoring. Understanding prompt design (Simple, ASCB, Own Reasoning) and output parsing is essential for replication.
  - Quick check question: Why might explicitly instructing an LM "do not use Flesch-Kincaid" improve its correlation with human readability judgments?

## Architecture Onboarding

- **Component map:**
  ```
  Traditional Metrics Layer: py-readability-metrics package → FKGL, DCRS, CLI, GFI, ARI, FRE, Spache, LW
  LM Evaluator Layer: Mistral 7B, Mixtral 7B, Gemma 7B, Llama 3.1 8B, Llama 3.3 70B → prompted scoring (1-5 scale)
  Human Judgment Ground Truth: August et al. (2024) dataset → 60 summaries, 593 annotators, 1-5 reading ease
  Analysis Pipeline: Correlation computation → Pearson, Kendall-Tau → significance testing (Williams test)
  ```

- **Critical path:**
  1. Load human-annotated dataset (60 summaries with averaged scores)
  2. Compute traditional metric scores (invert sign for metrics where lower = more readable)
  3. Run LM evaluators with "Own Reasoning" prompt (best-performing variant)
  4. Calculate Pearson and Kendall-Tau correlations against human judgments
  5. Run Williams test for statistical significance between metrics

- **Design tradeoffs:**
  - **LM size vs. cost:** Smaller models (Mistral 7B, Gemma 7B) perform nearly as well as Llama 3.3 70B (0.52-0.54 vs. 0.56 Pearson), suggesting cost-efficient options may suffice.
  - **Prompt simplicity vs. guidance:** Overly specific prompts (ASCB guidelines) caused LMs to over-rely on checklists. Minimal prompts with "use your own judgment" worked best.
  - **Traditional vs. LM metrics:** Authors recommend combining both—DCRS/CLI capture some signal, LMs capture deeper attributes, neither is perfect alone.

- **Failure signatures:**
  - FKGL assigns low readability to well-explained technical content with long words (e.g., "electricity," "harvesting")
  - FKGL assigns high readability to unexplained acronyms (short words like "GCDC," "SPE")
  - LMs occasionally attempt to compute FKGL internally when not explicitly told not to
  - Cohen's Kappa between FKGL and LM evaluators is only 0.17 (fair agreement)

- **First 3 experiments:**
  1. **Baseline replication:** Compute FKGL and DCRS scores on the August et al. dataset, verify correlations match reported values (FKGL: 0.16, DCRS: 0.37 Pearson).
  2. **LM evaluator test:** Run Llama 3.3 70B with the "Own Reasoning" prompt on a 10-summary subset, verify scores cluster around human judgments and reasoning mentions concept accessibility.
  3. **Dataset sanity check:** Evaluate arXiv (expert) and SJK (kids) datasets with both FKGL and LM evaluator—verify arXiv receives low LM scores and SJK receives high LM scores, regardless of FKGL rankings.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can LM-based readability evaluators be improved to reduce bias and increase interpretability while maintaining correlation with human judgments?
  - Basis in paper: [explicit] "LM-evaluators are an imperfect solution since they are subject to bias and a lack of interpretability... LMs are promising and worthy of future work that can decrease bias and improve interpretability."
  - Why unresolved: The authors demonstrate LM evaluators outperform traditional metrics but do not address known issues with LMs as evaluators (bias, lack of interpretability, hallucination).
  - What evidence would resolve it: A study comparing different LM architectures, prompt strategies, or calibration methods that demonstrates improved interpretability (e.g., through better reasoning explanations) and reduced bias (e.g., consistent scoring across text domains) while maintaining or improving Pearson correlation above 0.56.

- **Open Question 2:** Do the findings regarding readability metrics generalize to plain language summarization in domains beyond scientific articles, such as legal documents or clinical notes?
  - Basis in paper: [explicit] "Our human judgments and experiments focused on the summarization of scientific articles, and may not generalize to PLS in other domains, such as law or clinical notes."
  - Why unresolved: The validation dataset (August et al., 2024) and all 10 evaluated datasets are scientific in nature; no experiments were conducted on legal, financial, or medical plain language texts.
  - What evidence would resolve it: Replication of the correlation analysis between traditional metrics, LM evaluators, and human judgments on plain language summaries from legal or clinical domains.

- **Open Question 3:** How should traditional readability metrics (DCRS, CLI) and LM evaluators be optimally combined for evaluating plain language summaries?
  - Basis in paper: [inferred] The authors recommend using "a combination of traditional readability metrics and LM evaluators" and specifically DCRS and CLI with LMs, but do not specify an aggregation method.
  - Why unresolved: The paper evaluates metrics individually and reports their separate correlations but does not explore ensemble methods, weighted combinations, or decision frameworks for when metrics disagree.
  - What evidence would resolve it: An empirical comparison of combination strategies (e.g., linear weighted averaging, voting, meta-learner) showing which approach yields the highest correlation with human readability judgments.

## Limitations
- The study's findings are based on a single human-annotated dataset of 60 summaries, limiting generalizability across different domains and writing styles.
- The LM evaluators were tested with a specific prompt format that may not transfer to other contexts.
- The study does not address potential cultural or linguistic variations in readability judgments.

## Confidence
- Traditional metrics poorly correlate with human judgments: **High** (supported by clear statistical evidence across multiple metrics)
- Language models outperform traditional metrics: **Medium** (correlation difference is significant but based on a single dataset)
- PLS datasets show inconsistent readability: **Medium** (analysis covers many datasets but doesn't account for domain-specific factors)

## Next Checks
1. Replicate findings on a larger, independently collected human-annotated PLS dataset from a different domain
2. Test LM evaluator performance with alternative prompt formats and smaller model variants
3. Compare results using different implementations of traditional readability metrics to verify implementation consistency