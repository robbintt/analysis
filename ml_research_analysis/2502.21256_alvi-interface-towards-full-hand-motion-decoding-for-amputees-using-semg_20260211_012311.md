---
ver: rpa2
title: 'ALVI Interface: Towards Full Hand Motion Decoding for Amputees Using sEMG'
arxiv_id: '2502.21256'
source_url: https://arxiv.org/abs/2502.21256
tags:
- hand
- system
- real-time
- data
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ALVI Interface, a system for decoding hand movements
  using surface EMG signals for upper limb amputees. The system provides real-time
  reconstruction of finger joint angles across 20 degrees of freedom at 25 Hz using
  eight sEMG sensors and a VR training environment.
---

# ALVI Interface: Towards Full Hand Motion Decoding for Amputees Using sEMG

## Quick Facts
- arXiv ID: 2502.21256
- Source URL: https://arxiv.org/abs/2502.21256
- Authors: Aleksandr Kovalev; Anna Makarova; Petr Chizhov; Matvey Antonov; Gleb Duplin; Vladislav Lomtev; Viacheslav Gostevskii; Vladimir Bessonov; Andrey Tsurkan; Mikhail Korobok; Aleksejs Timƒçenko
- Reference count: 13
- Primary result: Real-time hand motion decoding from sEMG with 0.80 correlation and 14.50¬∞ mean angular error

## Executive Summary
ALVI Interface is a system for decoding hand movements using surface EMG signals for upper limb amputees. The system provides real-time reconstruction of finger joint angles across 20 degrees of freedom at 25 Hz using eight sEMG sensors and a VR training environment. The core method uses a transformer-based architecture called HandFormer, which processes 8-channel EMG data through an encoder-decoder structure with 32 learnable queries for movement prediction. The system includes a VR-based data collection platform and real-time calibration module that enables interactive adaptation. Offline analysis shows 0.80 correlation between predicted and actual hand movements for amputee participants, with a mean angular error of 14.50¬∞. The system achieves real-time performance with 51.2 ms latency and demonstrates significant user adaptation through interactive training, where both the model and users learn from each other to improve control quality.

## Method Summary
ALVI Interface uses a transformer-based HandFormer architecture to decode 20 degrees of freedom finger joint angles from 8-channel sEMG signals at 200 Hz. The system employs a two-stage training approach: first pretraining with masked autoencoders (70% token masking) then supervised fine-tuning with L1 loss. Data collection uses VR hand tracking to capture target positions from intact hands, which are mirrored to create ground truth for amputee training. The model processes 256-sample windows (1.28 seconds) through patch tokenization into 256 tokens, then uses a Perceiver-style decoder with 32 learnable queries for non-autoregressive prediction. Real-time inference runs at 25 Hz with exponential moving average smoothing, and the system supports continuous adaptation by fine-tuning model weights every 10 seconds based on user interaction.

## Key Results
- 0.80 correlation between predicted and actual hand movements for amputee participants
- Mean angular error of 14.50¬∞ across 20 degrees of freedom
- Real-time performance with 51.2 ms latency at 25 Hz
- Significant user adaptation through interactive training sessions
- 72 gestures tested across 22 participants in 3 sessions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mirror training via intact-hand tracking generates usable ground-truth labels for amputee sEMG decoding.
- **Mechanism:** The VR system tracks the intact hand's finger positions via Oculus Quest, mirrors them to create a virtual model of the absent hand, and synchronizes these target positions with sEMG signals from the residual limb using Lab Streaming Layer (LSL).
- **Core assumption:** Residual limb muscle activations during attempted movements correlate with the intended (mirrored) hand positions.
- **Evidence anchors:**
  - [abstract]: "VR-based data collection platform" for training data
  - [section 2.1]: "hand reflection module was specifically designed to precisely capture target finger positions for the amputated hand... tracking the coordinates of the fingers on the participant's intact hand using the Oculus Quest hand tracking system. These coordinates are then mirrored to create a virtual 3D model of the absent hand"
  - [corpus]: No direct comparison papers use mirror training for amputee sEMG; corpus focuses on high-density EMG spatial features rather than data generation methods
- **Break condition:** If phantom limb muscle patterns do not systematically correspond to intended finger positions; or if bilateral coordination is impaired.

### Mechanism 2
- **Claim:** Non-autoregressive Perceiver-style decoding enables efficient parallel prediction of 32 movement frames from sEMG tokens.
- **Mechanism:** EMG signals are patch-tokenized (1 electrode √ó 8 time steps per patch ‚Üí 256 total tokens), processed by a transformer encoder, then cross-attended with 32 learnable queries in the decoder. Each query maps to one predicted movement frame.
- **Core assumption:** Parallel prediction outperforms sequential/autoregressive approaches for EMG-to-motion tasks.
- **Evidence anchors:**
  - [abstract]: "transformer-based architecture called HandFormer... with 32 learnable queries for movement prediction"
  - [section 2.2]: "Decoder employs a Perceiver-like architecture with 32 learnable queries that match the number of movement frames. Notably, we use non-autoregressive prediction, which empirically outperforms autoregressive approaches"
  - [corpus]: MyoText (arXiv:2601.03098) applies transformer-based decoding to sEMG-to-text, supporting transformer efficacy for EMG sequence modeling
- **Break condition:** If complex sequential dependencies in fine motor movements require autoregressive modeling.

### Mechanism 3
- **Claim:** Bidirectional co-adaptation‚Äîmodel finetuning and user motor learning‚Äîimproves control quality within and across sessions.
- **Mechanism:** Users observe real-time virtual hand feedback and unconsciously adjust muscle activation patterns. The system continuously finetunes the pretrained model on incoming sEMG data, updating weights every 10 seconds. Historical data is combined with new data per session.
- **Core assumption:** Users can learn to modulate residual muscle activity based on visual feedback; the model can adapt faster than signal drift occurs.
- **Evidence anchors:**
  - [abstract]: "both the model and users learn from each other to improve control quality"
  - [section 3]: "participants reported a mutual learning phenomenon... users noticed they were unconsciously adjusting their muscle activation patterns to match the model's expected inputs, while the system continuously refined its predictions... decreasing adaptation time across sessions"
  - [corpus]: Hahne et al. (2017) cited in paper for "User adaptation in myoelectric man-machine interfaces" (Scientific Reports)
- **Break condition:** If user fatigue or day-to-day sEMG variability exceeds adaptation capacity.

## Foundational Learning

- **Concept: Surface EMG Signal Characteristics**
  - **Why needed here:** sEMG is inherently noisy and affected by electrode placement, skin impedance, muscle fatigue, and time-varying factors. Understanding normalization and windowing choices is critical.
  - **Quick check question:** Why is min-max normalization to [-1, 1] applied per-channel before model input, and what happens if electrode position shifts between sessions?

- **Concept: Perceiver Architecture and Learnable Queries**
  - **Why needed here:** HandFormer's decoder uses cross-attention between learned query vectors and encoded EMG tokens, not standard autoregressive decoding.
  - **Quick check question:** What is the functional role of the 32 learnable queries, and why might non-autoregressive prediction reduce latency compared to autoregressive alternatives?

- **Concept: Quaternion Joint Representation**
  - **Why needed here:** Target hand poses are encoded as quaternions for 21 joint orientations, then reduced to 4 angles per finger.
  - **Quick check question:** Why use quaternions rather than Euler angles for representing joint rotations in a learning pipeline?

## Architecture Onboarding

- **Component map:**
  Myo Armband (8 sEMG channels, 200 Hz) -> LSL stream -> emg_buffer.py -> predict.py (256-sample window) -> HandFormer inference -> Predicted joint angles (20 DOF, 25 Hz) -> VR Client via LSL -> AI Server (train.py) receives streamed data, finetunes model, returns updated weights every 10 seconds

- **Critical path:**
  1. Acquire 8-channel sEMG at 200 Hz via LSL
  2. Accumulate 256-sample window (1.28 seconds)
  3. Normalize to [-1, 1], tokenize into 256 patches
  4. Run HandFormer encoder-decoder inference
  5. Select most recent of 32 predicted frames, apply exponential moving average smoothing
  6. Stream to VR headset for real-time visualization

- **Design tradeoffs:**
  - **Latency vs. temporal context:** 1.28-second window provides signal context but sets minimum latency floor (~51 ms reported after inference)
  - **DOF count vs. prediction difficulty:** 20 DOF enables fine-grained finger control but increases output dimensionality and potential error accumulation
  - **Adaptation frequency vs. stability:** 10-second weight updates enable fast personalization but may introduce transient instability

- **Failure signatures:**
  - **Correlation drops significantly mid-session:** Likely electrode shift or poor skin contact; recalibrate or reposition armband
  - **High angular error on specific fingers:** Insufficient training coverage for those movement types; augment dataset with targeted gestures
  - **Jerky or oscillating output:** EMA smoothing coefficient may be too low; increase smoothing or check for inference jitter
  - **Latency exceeds ~100 ms:** Check LSL synchronization, network buffer between laptop and VR client, or server communication overhead

- **First 3 experiments:**
  1. **Validate data acquisition pipeline:** Stream from Myo Armband, confirm 8-channel data at 200 Hz in LSL, verify min-max normalization output range
  2. **Benchmark inference latency:** Load pretrained HandFormer weights, measure end-to-end inference time for 256-sample input on target hardware
  3. **Single-subject calibration test:** Run one 10-minute interactive session with real-time feedback, log correlation and angular error at 1-minute intervals to quantify adaptation trajectory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ALVI Interface maintain decoding accuracy across days and weeks without requiring recalibration, given inherent sEMG signal variability from electrode placement shifts and muscle fatigue?
- Basis in paper: [explicit] Authors state: "A key challenge remains the inherent variability of sEMG signals‚Äîaffected by electrode placement, muscle fatigue, and day-to-day fluctuations‚Äînecessitating regular calibration."
- Why unresolved: The study only tested across 3 sessions on different days; long-term stability with donning/doffing cycles remains uncharacterized.
- What evidence would resolve it: Longitudinal study tracking decoding performance over weeks with daily donning/doffing, measuring calibration persistence.

### Open Question 2
- Question: How well do findings generalize to larger amputee populations with varying amputation levels, residual limb conditions, and time since amputation?
- Basis in paper: [explicit] "While the results are promising, larger clinical trials with more amputees are needed to generalize these findings."
- Why unresolved: Only 2 amputee participants were tested; both may have atypical characteristics not representative of broader populations.
- What evidence would resolve it: Multi-site clinical trial with diverse amputee participants (n>30) analyzing performance correlation with clinical variables.

### Open Question 3
- Question: Will the co-adaptation strategy transfer effectively to physical prostheses, where visual-VR feedback is replaced by proprioceptive and tactile feedback?
- Basis in paper: [explicit] "Further research should also investigate integrating ALVI with physical prostheses for real-world tasks."
- Why unresolved: Current system only demonstrated in VR; physical prostheses introduce different feedback modalities, weight, and mechanical constraints.
- What evidence would resolve it: Comparative study of ALVI performance in VR versus physical prosthesis control on standardized functional tasks.

## Limitations

- The study only tested unilateral transradial amputees, limiting generalizability to other amputation levels and bilateral cases
- Real-time performance metrics were measured under controlled VR conditions and may degrade in unconstrained environments
- The adaptation mechanism's long-term stability remains unclear, as the paper reports improvements over 10-minute sessions but does not address day-to-day or week-to-week reliability

## Confidence

**High Confidence (‚òëÔ∏è):** The transformer-based HandFormer architecture can process sEMG signals to predict hand movements in real-time with measurable accuracy. The VR-based training environment successfully enables data collection and provides useful feedback for users. The system achieves sub-100ms latency for 20-DOF control using 8-channel sEMG.

**Medium Confidence (üü°):** The mirror training approach generates sufficient ground truth for effective model training across diverse amputee populations. The non-autoregressive decoding strategy provides measurable benefits over autoregressive alternatives for this specific task. The bidirectional adaptation mechanism produces meaningful improvements in control quality within training sessions.

**Low Confidence (üî¥):** The system maintains performance consistency across multiple days of use without recalibration. The 0.80 correlation metric represents robust real-world performance outside controlled VR conditions. The adaptation mechanism scales effectively to more complex hand movements beyond the tested 72 gestures.

## Next Checks

1. **Cross-session reliability test:** Evaluate system performance across 5 consecutive days with the same users, measuring correlation decay and required recalibration frequency. This addresses the critical unknown of long-term adaptation stability.

2. **Amputation type generalization:** Test the system with transhumeral, wrist disarticulation, and bilateral amputation cases to validate the mirror training mechanism's robustness across different residual limb configurations and phantom mapping patterns.

3. **Real-world task performance:** Move beyond VR-controlled environments to assess performance during practical activities (typing, object manipulation, tool use) in varied lighting and environmental conditions to validate the claimed real-world applicability.