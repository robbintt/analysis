---
ver: rpa2
title: 'The Illusionist''s Prompt: Exposing the Factual Vulnerabilities of Large Language
  Models with Linguistic Nuances'
arxiv_id: '2504.02865'
source_url: https://arxiv.org/abs/2504.02865
tags:
- llms
- original
- factual
- arxiv
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces The Illusionist\u2019s Prompt, a novel adversarial\
  \ attack that induces factual hallucinations in Large Language Models (LLMs) by\
  \ incorporating linguistic nuances into user queries. Unlike prior jailbreak attacks\
  \ that aim for harmful outputs, this attack preserves semantic integrity while triggering\
  \ factual errors through LLM-based rephrasing that reduces readability, formality,\
  \ and concreteness."
---

# The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances

## Quick Facts
- **arXiv ID:** 2504.02865
- **Source URL:** https://arxiv.org/abs/2504.02865
- **Reference count:** 40
- **Primary result:** Novel adversarial attack that induces factual hallucinations in LLMs by rephrasing queries with linguistic nuances, achieving up to 68.57% increase in hallucination scores

## Executive Summary
This paper introduces The Illusionist's Prompt, a novel adversarial attack that induces factual hallucinations in Large Language Models by incorporating linguistic nuances into user queries. Unlike prior jailbreak attacks that aim for harmful outputs, this attack preserves semantic integrity while triggering factual errors through LLM-based rephrasing that reduces readability, formality, and concreteness. The attack achieves a 100% pass rate against five fact-enhancing strategies and adaptive defenses, demonstrating significant vulnerabilities in current LLM architectures.

## Method Summary
The Illusionist's Prompt uses GPT-4o to rephrase truthful questions with six mutation guidelines targeting syntactic complexity, component rearrangement, uncommon structures, grammatical ingenuity, disguised sensitive terms, and emoji artistry. The attack reduces readability (FRES scores), formality, and concreteness while maintaining semantic similarity above 0.6. The mutation process involves a single-pass transformation with JSON output formatting, applied to questions from the TruthfulQA benchmark. Evaluation uses GPT-4o to assess factual hallucination scores, semantic logicality, and semantic similarity across four prominent LLMs including GPT-4o and Gemini-2.0.

## Key Results
- Achieved up to 68.57% increase in factual hallucination scores across four LLMs
- Maintained 100% pass rate against five fact-enhancing strategies and adaptive defenses
- Demonstrated significant reduction in semantic logicality while preserving semantic similarity above 0.6 threshold
- Showed effectiveness across both open-ended generation and single-true multiple-choice tasks

## Why This Works (Mechanism)
The attack exploits the trade-off between semantic preservation and linguistic complexity in LLM processing. By reducing readability, formality, and concreteness while maintaining semantic similarity, the prompts create cognitive load that overwhelms the model's factual reasoning capabilities without triggering refusal mechanisms. The six mutation guidelines systematically introduce linguistic nuances that confuse the model's fact-checking processes while preserving the core semantic intent of the question.

## Foundational Learning
- **Factual Hallucination Score (0-10):** Measures the degree of factual errors in LLM responses. Needed to quantify attack effectiveness. Quick check: Verify score distribution ranges from 0 (completely accurate) to 10 (completely hallucinated).
- **Semantic Similarity (0-1):** Cosine similarity between original and mutated prompts using Sentence-BERT. Needed to ensure semantic preservation. Quick check: Calculate similarity scores for 10 random mutations to verify >0.6 threshold.
- **Readability Metrics (FRES):** Flesch Reading Ease Score measuring text readability. Needed to quantify linguistic complexity reduction. Quick check: FRES scores below 60 indicate medium readability suitable for triggering hallucinations.
- **TruthfulQA Benchmark:** 817 questions across 38 categories testing factual knowledge. Needed as standardized evaluation dataset. Quick check: Verify all 817 questions load correctly with proper category distribution.
- **Fact-enhancing Strategies:** Five specific defense mechanisms including TruthX, ICD, and FreshPrompt. Needed to evaluate attack robustness. Quick check: Implement all five strategies and verify they reduce baseline hallucination rates.
- **Perplexity-based Filtering:** Adaptive defense measuring text complexity to block adversarial prompts. Needed as final defense evaluation. Quick check: Test filtering threshold of 50 and verify 100% pass rate.

## Architecture Onboarding

**Component Map:** User Query → GPT-4o Mutation (6 guidelines) → Adversarial Prompt → LLM → Factuality Score → Semantic Metrics

**Critical Path:** Mutation (GPT-4o API) → Adversarial Prompt Generation → LLM Evaluation (4 models) → Score Calculation

**Design Tradeoffs:** The attack prioritizes semantic preservation over maximum hallucination induction, choosing a 0.6 semantic similarity threshold that balances effectiveness with stealth. This creates a narrower attack window compared to unrestricted jailbreaks but enables bypassing fact-enhancing defenses.

**Failure Signatures:** 
- Low semantic similarity (<0.6) indicates over-aggressive mutation
- Model refusal suggests readability too low (FRES <20)
- Minimal hallucination increase (<10%) suggests ineffective mutation

**First 3 Experiments:**
1. Apply mutation to 10 random TruthfulQA questions and verify semantic similarity >0.6
2. Test mutated prompts on LLaMA-2-7B-chat with greedy decoding to measure baseline hallucination increase
3. Implement perplexity-based filtering with threshold 50 and verify 100% pass rate

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on GPT-4o for prompt mutation, creating potential model-specific optimization
- Limited evaluation to 817 TruthfulQA questions, restricting generalizability to broader domains
- Defense evaluation covers only five specific fact-enhancing strategies, missing broader landscape of mitigation techniques
- Semantic similarity threshold of 0.6 is somewhat arbitrary and may not capture all semantic-preserving mutations

## Confidence
- **High confidence:** Core attack methodology and effectiveness in increasing hallucination scores across multiple experiments and models
- **Medium confidence:** Claim that attack preserves semantic integrity while reducing readability/formality/concreteness, dependent on subjective metric thresholds
- **Medium confidence:** Effectiveness against specific fact-enhancing strategies, given limited scope of defenses tested

## Next Checks
1. **Cross-model mutation validation:** Test whether using different LLMs (Claude, Gemini) for mutation produces similar hallucination rates, or if attack is specifically tuned to GPT-4o's rephrasing patterns
2. **Expanded defense testing:** Evaluate attack against additional fact-checking mechanisms including RAG systems with real-time retrieval, confidence thresholding, and ensemble verification approaches
3. **Semantic preservation stress test:** Systematically vary semantic similarity threshold (0.5 to 0.9) and measure corresponding changes in hallucination scores to establish true relationship between semantic preservation and hallucination induction