---
ver: rpa2
title: 'Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive
  Dual-System Theory'
arxiv_id: '2507.18178'
source_url: https://arxiv.org/abs/2507.18178
tags:
- reasoning
- llms
- knowledge
- high
- fast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a cognition attribution framework to disentangle
  knowledge and reasoning in LLMs. The method uses dual-system cognitive theory to
  decompose inference into knowledge retrieval (Phase 1) and reasoning adjustment
  (Phase 2), separated by prompting models under fast-thinking and slow-thinking modes.
---

# Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory

## Quick Facts
- arXiv ID: 2507.18178
- Source URL: https://arxiv.org/abs/2507.18178
- Reference count: 40
- Primary result: Framework separates knowledge retrieval and reasoning adjustment in LLMs, revealing reasoning is domain-specific, benefits from scaling primarily through reduced overthinking, and operates in higher layers.

## Executive Summary
This paper proposes a cognition attribution framework that disentangles knowledge and reasoning in large language models using dual-system cognitive theory. The method decomposes inference into two phases: knowledge retrieval (Phase 1) and reasoning adjustment (Phase 2), separated by prompting models under fast-thinking and slow-thinking modes. Experiments on 15 LLMs across three datasets reveal that reasoning adjustment is domain-specific, benefiting reasoning-intensive domains like mathematics and physics while potentially harming knowledge-intensive domains. The framework provides new insights into scaling laws, hierarchical knowledge editing, and small-model limitations.

## Method Summary
The framework uses dual-system cognitive theory to decompose LLM inference into two phases: knowledge retrieval (fast thinking) and reasoning adjustment (slow thinking). Fast thinking prompts direct models to output answers immediately without Chain-of-Thought, engaging only knowledge retrieval. Slow thinking prompts encourage CoT generation, engaging both knowledge retrieval and reasoning adjustment. The accuracy difference between modes quantifies reasoning contribution. Layer activations are analyzed using Centered Kernel Alignment (CKA) to localize knowledge and reasoning functions across network layers.

## Key Results
- Reasoning adjustment is domain-specific: benefits reasoning-intensive domains (mathematics, physics) but may harm knowledge-intensive domains (history, political science)
- Parameter scaling enhances both knowledge and reasoning, with knowledge improving more than reasoning
- Reasoning adjustment is significantly more "prudent" (less overthinking) and moderately more "intelligent" with larger models
- Knowledge primarily resides in lower network layers while reasoning operates in higher layers

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Mode Separation via Prompting
- Claim: Knowledge retrieval and reasoning adjustment can be isolated by prompting LLMs under two distinct cognitive modes.
- Mechanism: Fast thinking prompts direct models to output answers immediately without CoT, engaging only Phase 1 (knowledge retrieval). Slow thinking prompts encourage CoT generation, engaging both Phase 1 and Phase 2 (reasoning adjustment). The accuracy difference between modes quantifies reasoning contribution.
- Core assumption: Prompting can reliably suppress or activate reasoning processes, and fast thinking genuinely reflects pure knowledge retrieval without implicit reasoning.
- Evidence anchors:
  - [abstract]: "LLMs are prompted to generate answers under two different cognitive modes, fast thinking and slow thinking, respectively."
  - [Section 2, Step 2]: Defines fast thinking as y_fast = argmax P(y | x; C_knowledge) and slow thinking as y_slow = argmax P(y | y_fast, x; C_knowledge, C_reasoning).
  - [corpus]: Related work "Learning, Reasoning, Refinement" (arXiv 2506.17913) similarly applies Kahneman's dual-system theory to GUI agents, suggesting cross-domain validity of the separation approach.
- Break condition: If models engage in implicit reasoning during fast thinking, or if slow thinking fails to meaningfully adjust initial answers, the separation collapses.

### Mechanism 2: Reasoning Gain Decomposition into Correction and Overthinking
- Claim: Reasoning adjustment produces both beneficial corrections and harmful overthinking, with their balance determining net reasoning gain.
- Mechanism: δ = δ_c - δ_o, where δ_c measures how often reasoning fixes incorrect fast answers (correction rate × error pool), and δ_o measures how often reasoning overrides correct fast answers (overthinking rate × correct pool). Small models exhibit low correction and high overthinking.
- Core assumption: The accuracy change between modes is entirely attributable to reasoning adjustment, not to other factors like prompt sensitivity.
- Evidence anchors:
  - [abstract]: "Reasoning adjustment is significantly more 'prudent' (less overthinking) and moderately more 'intelligent' with larger models."
  - [Section 2, Step 4, Eq. 9]: "δ arises from two opposing components: the benefit from correction δ_c and the loss from overthinking δ_o."
  - [Section 3.2]: "The overthinking rate r_o of LLaMA 1B is 45.4% higher than that of LLaMA 70B, while its correction rate r_c is merely 8.7% lower."
  - [corpus]: No direct corpus validation for this specific decomposition; evidence is primarily internal to the paper.
- Break condition: If correction and overthinking interact non-linearly (e.g., reasoning both fixes and breaks the same answer type), the additive decomposition becomes invalid.

### Mechanism 3: Hierarchical Layer Specialization
- Claim: Knowledge retrieval primarily operates in lower network layers while reasoning adjustment activates higher layers.
- Mechanism: Centered Kernel Alignment (CKA) measures similarity between corresponding layers under fast vs. slow thinking. Lower layers show high similarity (shared knowledge processing), while higher layers diverge (reasoning-specific activation in slow thinking).
- Core assumption: CKA similarity directly reflects functional role, and layer divergence indicates reasoning-specific computation rather than noise or task-irrelevant variation.
- Evidence anchors:
  - [abstract]: "Knowledge primarily resides in lower network layers, while reasoning operates in higher layers."
  - [Section 3.6]: "All CKA curves exhibit an initial plateau in lower layers, followed by a decline in higher layers."
  - [corpus]: Related work on knowledge editing (Meng et al., cited as [20-21]) identifies knowledge in lower layers; this paper extends that finding to reasoning localization.
- Break condition: If higher-layer divergence reflects attention pattern shifts unrelated to reasoning, or if knowledge also requires higher-layer integration, the localization claim weakens.

## Foundational Learning

- Concept: Dual-System Cognitive Theory (Kahneman's System 1/System 2)
  - Why needed here: The entire framework maps LLM cognition to two phases: fast/intuitive (System 1) and slow/deliberative (System 2). Without understanding this analogy, the experimental design appears arbitrary.
  - Quick check question: Can you explain why System 1 is associated with knowledge retrieval and System 2 with reasoning adjustment?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: The slow thinking mode relies on CoT generation to activate reasoning adjustment. Understanding CoT mechanics helps interpret why reasoning benefits math but may harm knowledge-intensive domains.
  - Quick check question: Why might CoT introduce noise that leads to overthinking in small models?

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: CKA quantifies layer-wise similarity between fast and slow thinking modes, enabling the hierarchical localization claim. Without CKA literacy, the Section 3.6 results are opaque.
  - Quick check question: If CKA between two layers is 0.9, what does that imply about their functional similarity?

## Architecture Onboarding

- Component map:
  - Input → Fast thinking prompt → Direct answer (Phase 1 only)
  - Input → Slow thinking prompt → CoT → Refined answer (Phase 1 + Phase 2)
  - Accuracy comparison → δ (reasoning gain) → Decompose into δ_c (correction) and δ_o (overthinking)
  - Layer activations → CKA computation → Hierarchical localization

- Critical path:
  1. Design prompts that reliably trigger fast vs. slow thinking (see Appendix A for templates).
  2. Evaluate accuracy for both modes using identical datasets (MMLU, MathQA, MedQA).
  3. Compute δ, δ_c, δ_o, r_c, r_o using Equations 5-9.
  4. (Optional) Extract layer activations and compute CKA for localization analysis.

- Design tradeoffs:
  - Domain selection: Reasoning adjustment benefits math/physics but may harm history/political science—choose domains based on whether knowledge or reasoning is bottleneck.
  - Model scale: Small models (<3B) show negative δ due to overthinking; scaling primarily reduces overthinking rather than increasing correction.
  - Evaluation method: GLM-4-PLUS used as evaluator for slow thinking (Appendix D.2)—introduces potential evaluator bias.

- Failure signatures:
  - Negative δ: Indicates overthinking dominates correction; common in small models or knowledge-intensive domains.
  - Flat CKA curves: Suggests reasoning adjustment is not activating distinct layers; may indicate prompt failure.
  - High token consumption with low δ: Small models generate CoT length similar to large models but gain little—sign of superficial reasoning imitation.

- First 3 experiments:
  1. Replicate fast/slow thinking comparison on MMLU subset with Qwen-7B to validate δ computation; expect δ ≈ 4.0% (Table 1).
  2. Test domain specificity: Run the framework on mathematics vs. political science questions; expect δ(mat) >> δ(pol), potentially δ(pol) < 0 (Table 2).
  3. Scale sensitivity test: Compare LLaMA-1B vs. LLaMA-8B on overthinking rate r_o; expect r_o(1B) >> r_o(8B) (Section 3.5).

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the decomposition of reasoning gain into correction and overthinking hold for open-ended generation tasks, or does the framework require modification for non-multiple-choice outputs?
  - Basis in paper: [explicit] The authors identify the limitation to multiple-choice QA datasets and explicitly call for extending the analysis to open-ended generation in future work.
  - Why unresolved: The current metrics (A_fast, δ_c, δ_o) rely on exact string matches against ground-truth options, which is infeasible for free-form text.
  - What evidence would resolve it: Adapting the framework to generative benchmarks using semantic similarity or LLM-based evaluation to quantify correction and overthinking rates.

- **Open Question 2**: Do the findings regarding scaling—specifically that it improves knowledge more than reasoning and increases "prudence"—persist in ultra-large models exceeding 100 billion parameters?
  - Basis in paper: [explicit] The authors note that while 15 models were studied, ultra-large models (>100B) were not included, leaving their behavior under this framework untested.
  - Why unresolved: Scaling laws can be non-linear; properties observed in the 1B–70B range may not extrapolate to models of significantly different capacities.
  - What evidence would resolve it: Applying the cognition attribution architecture to models like GPT-4 or Llama 3 405B to verify if the scaling curves for δ_c and δ_o remain consistent.

- **Open Question 3**: Can the negative impact of reasoning adjustment in knowledge-intensive domains (like history and political science) be prevented through targeted training or prompting?
  - Basis in paper: [explicit] The results show reasoning adjustment yields negative δ in domains like political science and history, yet the paper does not propose methods to mitigate this impairment.
  - Why unresolved: It is unclear if this "overthinking" is an inevitable byproduct of CoT in uncertain domains or a correctable alignment issue.
  - What evidence would resolve it: Experiments involving domain-aware prompting (e.g., suppressing CoT for knowledge queries) to see if it eliminates the negative reasoning gain without harming reasoning-intensive tasks.

- **Open Question 4**: Is the increase in reasoning "prudence" (reduction in overthinking) a direct result of parameter scaling or a byproduct of the larger training datasets typically used for larger models?
  - Basis in paper: [inferred] The paper attributes the reduction in overthinking to parameter scaling, but does not isolate the effect of data scale vs. model scale.
  - Why unresolved: Without disentangling these variables, it is unknown if a small model trained on massive data would achieve similar prudence.
  - What evidence would resolve it: Ablation studies comparing models of identical size trained on varying data volumes.

## Limitations

- The framework's validity hinges on the assumption that fast-thinking prompts genuinely suppress reasoning processes, without rigorous ablation studies varying prompt designs.
- The decomposition of reasoning gain into correction and overthinking components assumes additive independence, but real LLM reasoning may exhibit non-linear interactions.
- The CKA-based localization claims rely on the assumption that layer similarity directly indicates functional role, which remains to be validated across diverse architectures and tasks.

## Confidence

- **High confidence**: Domain specificity of reasoning adjustment (δ benefits math/physics, harms knowledge-intensive domains) - supported by multiple datasets and clear statistical patterns
- **Medium confidence**: Scaling effects on overthinking correction - the trend is consistent but small models' negative reasoning gain suggests fundamental limitations
- **Medium confidence**: Layer-wise localization (knowledge in lower layers, reasoning in higher layers) - CKA patterns are observed but functional interpretation remains inferential
- **Low confidence**: The additive decomposition of reasoning gain into correction vs. overthinking components - while mathematically elegant, this assumes independence without experimental validation

## Next Checks

1. **Prompt ablation study**: Systematically vary fast-thinking prompt designs (adding partial CoT, time pressure, different answer formats) to determine the minimum prompt structure that reliably suppresses reasoning, validating the core separation assumption.

2. **Cross-architecture replication**: Apply the framework to transformer variants with different layer structures (Mixture-of-Experts, sparse attention) to test whether the observed layer-wise localization generalizes beyond standard transformers.

3. **Knowledge-editing intervention**: Use layer-specific knowledge editing techniques on lower layers to empirically demonstrate that modifying these layers affects fast-thinking accuracy while leaving reasoning adjustment capabilities intact, providing causal evidence for the localization claim.