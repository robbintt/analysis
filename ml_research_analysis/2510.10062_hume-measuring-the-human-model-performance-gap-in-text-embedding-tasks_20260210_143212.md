---
ver: rpa2
title: 'HUME: Measuring the Human-Model Performance Gap in Text Embedding Tasks'
arxiv_id: '2510.10062'
source_url: https://arxiv.org/abs/2510.10062
tags:
- human
- performance
- tasks
- agreement
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HUME fills a critical gap in embedding evaluation by benchmarking
  human performance across 16 MTEB tasks spanning reranking, classification, clustering,
  and semantic textual similarity in multiple languages. Humans achieve 77.6% average
  performance, ranking 4th among 13 evaluated models (80.1% best model), with substantial
  variation by task and language.
---

# HUME: Measuring the Human-Model Performance Gap in Text Embedding Tasks

## Quick Facts
- **arXiv ID**: 2510.10062
- **Source URL**: https://arxiv.org/abs/2510.10062
- **Reference count**: 40
- **Primary result**: Humans achieve 77.6% average performance across 16 MTEB tasks, ranking 4th among 13 evaluated models

## Executive Summary
HUME introduces a comprehensive framework for measuring human performance across 16 MTEB text embedding tasks spanning reranking, classification, clustering, and semantic textual similarity in multiple languages. The study establishes that humans achieve 77.6% average performance, placing them 4th among 13 evaluated models (with the best model at 80.1%). Models outperform humans on 14 of 26 tasks, particularly in high-resource languages and technical domains, while humans maintain advantages on non-English sentiment analysis and Arabic semantic similarity tasks, suggesting potential cultural advantages in certain contexts.

The framework reveals that low human performance often indicates task ambiguity rather than human limitations, providing crucial insights for benchmark interpretation. Nine evaluated LLMs (76.1%) fall short of human performance (81.2%) despite their scalability benefits. This work establishes human baselines for meaningful model comparison and identifies quality issues in existing benchmarks, offering actionable insights for future model development and evaluation practices.

## Method Summary
HUME establishes human baselines across 16 MTEB tasks spanning four categories: reranking, classification, clustering, and semantic textual similarity. Human annotators complete these tasks in multiple languages including English, German, French, Spanish, Chinese, and Arabic. The framework includes inter-annotator agreement analysis to distinguish between task ambiguity and genuine human limitations. Performance is measured relative to ground truth annotations, with models and humans evaluated on identical task instances. The study compares human performance against 13 embedding models, identifying domains where each excels or falls short.

## Key Results
- Humans achieve 77.6% average performance, ranking 4th among 13 evaluated models (80.1% best model)
- Models exceed human performance on 14 of 26 tasks, particularly on high-resource languages and technical domains
- Humans outperform models on non-English sentiment analysis and Arabic semantic similarity, suggesting cultural advantages
- Inter-annotator agreement analysis reveals that low human performance often indicates task ambiguity rather than human limitations
- Nine evaluated LLMs (76.1%) fall short of human performance (81.2%) despite scalability benefits

## Why This Works (Mechanism)
The framework works by providing standardized human baselines that enable meaningful comparison between human and model performance across diverse embedding tasks. By measuring human performance directly on the same tasks as models, HUME eliminates the confounding factor of using different evaluation criteria. The inter-annotator agreement analysis serves as a diagnostic tool to identify task ambiguity, distinguishing between genuine human limitations and poorly defined tasks. This approach reveals that models can outperform humans in some domains while humans maintain advantages in others, creating a more nuanced understanding of the human-model performance gap.

## Foundational Learning

**Text Embedding Tasks**: The fundamental operations in embedding-based systems including reranking (reordering items by relevance), classification (categorizing text), clustering (grouping similar items), and semantic textual similarity (measuring meaning similarity). *Why needed*: These represent the core capabilities that embedding models must master. *Quick check*: Can you name the four task categories and provide an example of each?

**Human Baseline Establishment**: The process of systematically measuring human performance on machine learning tasks to create reference points for model evaluation. *Why needed*: Without human baselines, we cannot determine whether model performance represents genuine capability or merely overfitting to benchmarks. *Quick check*: What is the human average performance baseline established by HUME?

**Inter-Annotator Agreement Analysis**: Statistical measures of consistency between different human annotators on the same task. *Why needed*: Low agreement indicates task ambiguity rather than human limitations, crucial for interpreting benchmark results. *Quick check*: How does HUME use agreement analysis to interpret low human performance?

**Cross-Lingual Benchmarking**: Evaluating models and humans across multiple languages to assess generalization beyond English-centric performance. *Why needed*: Embedding models must function effectively across diverse linguistic contexts. *Quick check*: Which languages are included in HUME's evaluation framework?

## Architecture Onboarding

**Component Map**: Human annotators -> Task completion interface -> Performance measurement -> Model comparison -> Benchmark quality analysis -> Insights generation

**Critical Path**: The sequence of evaluating human performance, comparing against models, analyzing agreement patterns, and deriving benchmark quality insights represents the core workflow for establishing meaningful human baselines.

**Design Tradeoffs**: The framework trades comprehensive coverage (16 tasks across 6 languages) for potential depth in any single domain. This breadth enables broad insights but may miss nuanced task-specific behaviors. The decision to use identical tasks for humans and models ensures fair comparison but may not capture human advantages in task adaptation or context utilization.

**Failure Signatures**: Low human performance coupled with high inter-annotator agreement suggests genuine human limitations or unfair tasks. Low agreement across both humans and models indicates task ambiguity. Models significantly outperforming humans on tasks where humans typically excel may indicate benchmark artifacts or evaluation metric misalignment.

**First 3 Experiments**: 
1. Establish baseline human performance on a single MTEB task to validate the evaluation methodology
2. Compare human performance across languages on identical task types to assess cross-lingual consistency
3. Measure inter-annotator agreement on ambiguous tasks to develop diagnostic criteria for task quality assessment

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Substantial variance in human performance across tasks suggests the 77.6% average baseline may mask significant heterogeneity in human capabilities
- Results may not generalize beyond MTEB tasks to other embedding benchmarks or real-world applications
- Inter-annotator agreement analysis cannot fully disambiguate whether low human performance reflects task ambiguity or genuine human limitations
- The study does not deeply explore cultural factors underlying human advantages on certain non-English tasks

## Confidence
- **High**: The core finding that human baselines enable meaningful model comparison is well-supported by relative model rankings and benchmark quality identification
- **Medium**: The claim that cultural advantages explain human superiority on certain non-English tasks lacks deep exploration of cultural factors
- **Medium**: The assertion that nine LLMs fall short of human performance may be influenced by task-specific characteristics rather than general model limitations

## Next Checks
1. Replicate human evaluation with larger and more diverse annotator pools to assess stability of the 77.6% baseline across different populations
2. Conduct systematic error analysis comparing human and model mistakes on identical tasks to identify whether errors stem from different reasoning types or knowledge gaps
3. Test human baselines on MTEB tasks with modified prompts or instructions to determine how much performance variance is attributable to task ambiguity versus human capability