---
ver: rpa2
title: The 2025 Planning Performance of Frontier Large Language Models
arxiv_id: '2511.09378'
source_url: https://arxiv.org/abs/2511.09378
tags:
- planning
- tasks
- llms
- performance
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the end-to-end planning performance of three
  frontier LLMs (DeepSeek R1, Gemini 2.5 Pro, and GPT-5) on PDDL-based planning tasks,
  comparing them to the classical planner LAMA. The models are tested on standard
  and obfuscated versions of tasks from the IPC 2023 Learning Track, with the latter
  designed to assess pure symbolic reasoning by replacing all symbolic names with
  random strings.
---

# The 2025 Planning Performance of Frontier Large Language Models

## Quick Facts
- arXiv ID: 2511.09378
- Source URL: https://arxiv.org/abs/2511.09378
- Reference count: 28
- Primary result: GPT-5 solves 205/360 PDDL tasks, matching LAMA's performance on standard tasks but requiring orders of magnitude more compute

## Executive Summary
This paper evaluates the end-to-end planning performance of three frontier LLMs (DeepSeek R1, Gemini 2.5 Pro, and GPT-5) on PDDL-based planning tasks, comparing them to the classical planner LAMA. The models are tested on standard and obfuscated versions of tasks from the IPC 2023 Learning Track, with the latter designed to assess pure symbolic reasoning by replacing all symbolic names with random strings. GPT-5 solves 205 out of 360 tasks, matching LAMA's performance on standard tasks, while Gemini 2.5 Pro shows the smallest performance drop on obfuscated tasks (146 vs. 155 solved). DeepSeek R1's performance degrades most significantly on obfuscated tasks (93 vs. 157 solved). These results indicate that frontier LLMs have made substantial progress in planning, with GPT-5 now competitive with LAMA, though all models still rely on semantic information and are far less efficient computationally.

## Method Summary
The study evaluates LLMs on 360 tasks across 8 IPC 2023 Learning Track domains using both standard PDDL and obfuscated versions where all symbols are replaced with random strings. Models are prompted with domain definitions, task descriptions, and two example plans using few-shot learning. Each LLM generates a complete plan via a single API call with default parameters, and outputs are validated using the VAL tool. LAMA serves as a baseline classical planner with 30-minute timeout and 8GiB memory limits. Performance is measured by coverage (tasks solved) and plan length.

## Key Results
- GPT-5 achieves 205/360 solved tasks, matching LAMA's performance on standard PDDL tasks
- All LLMs show significant performance degradation on obfuscated tasks, indicating reliance on semantic information
- Gemini 2.5 Pro exhibits the smallest drop in obfuscated setting (146 vs. 155 solved) while DeepSeek R1 drops most severely (93 vs. 157 solved)
- LLMs require 10-30 minutes per task on GPU clusters versus milliseconds for LAMA on CPU

## Why This Works (Mechanism)

### Mechanism 1: Semantic Prior Exploitation
Frontier LLMs solve PDDL tasks by leveraging semantic associations between natural language tokens and likely action consequences, rather than relying solely on formal logical axioms. The model acts as a fuzzy semantic matcher, retrieving common-sense physics and causal patterns from pre-training data to constrain the search space when predicate names are meaningful.

### Mechanism 2: Test-Time Compute Scaling for Reasoning
LLMs compensate for lack of semantic cues or high problem complexity by increasing "reasoning effort" (generation length) at inference time. The model utilizes increased context window and generation steps to "think" through the state space, effectively simulating a search trace when semantic shortcuts are removed.

### Mechanism 3: Pattern Matching vs. State-Space Search
LLMs excel in domains where solution structure resembles common planning tropes found in training data, but fail in domains requiring deep, novel state-space exploration. The model retrieves policy fragments based on domain familiarity and struggles with domains like Sokoban or Rovers where dead-ends require look-ahead beyond the model's effective window.

## Foundational Learning

- **Concept: PDDL (Planning Domain Definition Language)**
  - Why needed here: This is the input format. Understanding the split between *Domain* (actions/predicates) and *Task* (objects/goal) is required to interpret why obfuscation breaks the model's semantic priors.
  - Quick check question: Can you distinguish between the `:precondition` of an action and the `:goal` of a task file?

- **Concept: Soundness vs. Completeness**
  - Why needed here: The paper explicitly validates LLM output using VAL to ensure soundness (plans are valid), but notes LLMs are not complete (they miss solvable tasks).
  - Quick check question: If an LLM generates a plan that achieves the goal but takes unnecessary steps, is it sound? (Yes).

- **Concept: Semantic Leakage / Data Contamination**
  - Why needed here: The core experimental design relies on "obfuscated" tasks to prevent the model from using pre-trained knowledge of "Blocksworld" logic.
  - Quick check question: Why does renaming "Truck" to "x7f2" force the model to rely on reasoning rather than memory?

## Architecture Onboarding

- **Component map:** PDDL Domain + Task -> LLM Generator -> VAL Validator -> Sound Plan (or rejection)
- **Critical path:** The prompt construction → LLM generation → VAL verification loop. If the LLM hallucinates a non-existent action, VAL rejects it.
- **Design tradeoffs:** Efficiency vs. Generality: LAMA solves tasks in milliseconds on a CPU; GPT-5 requires massive GPU clusters and minutes of generation. Robustness vs. Semantics: Using meaningful names aids LLM performance but introduces bias; obfuscation ensures pure reasoning but severely degrades capability.
- **Failure signatures:** Syntax Errors: LLM generates PDDL that is syntactically malformed (caught by parser). Semantic Hallucation: LLM invents actions (e.g., "teleport") not defined in the domain file. Context Overflow: Long plans (>1000 steps) may exceed output token limits or cause the model to lose track of the initial state.
- **First 3 experiments:** 1) Obfuscation A/B Test: Run a single domain (e.g., Logistics) standard vs. obfuscated to verify the semantic dependency delta locally. 2) Token Analysis: Plot "Plan Length" vs. "Reasoning Tokens" for Gemini to verify the linear compute scaling hypothesis shown in Figure 2b. 3) Validator Integration: Implement the VAL tool wrapper to filter raw LLM outputs; measure the "hallucination rate" (percentage of generated plans that are syntactically valid but logically unsound).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based planning achieve computational efficiency comparable to classical planners?
- Basis in paper: The Discussion notes that LLMs are "orders of magnitude less efficient" than LAMA, requiring massive GPU clusters versus a single CPU core.
- Why unresolved: The paper benchmarks capability (coverage) but highlights the efficiency gap as a "critical trade-off" for practical application without proposing a solution.
- What evidence would resolve it: A study comparing time-to-solution and energy consumption between LLMs and classical planners on identical hardware, or the demonstration of a distilled LLM capable of planning on edge devices.

### Open Question 2
- Question: Why does Gemini 2.5 Pro exhibit significantly higher robustness to symbolic obfuscation than other frontier models?
- Basis in paper: Table 2 and the text note that Gemini 2.5 Pro's performance drops only slightly on obfuscated tasks, whereas DeepSeek R1's performance collapses.
- Why unresolved: The paper reports the performance divergence but does not investigate the specific architectural traits or training data (e.g., chain-of-thought reinforcement learning) that confer this stability.
- What evidence would resolve it: Ablation studies on model architectures and training methodologies to isolate factors that improve performance on "pure reasoning" tasks lacking semantic cues.

### Open Question 3
- Question: Are frontier LLMs capable of generating optimal or near-optimal plans rather than just satisficing ones?
- Basis in paper: The authors explicitly state they consider "satisficing planning, where... optimality is not required" and the evaluation metric is primarily coverage.
- Why unresolved: Figure 2a shows plan lengths, but the paper does not benchmark these lengths against optimal solutions or analyze the "plan quality" in terms of cost efficiency.
- What evidence would resolve it: An evaluation using metrics such as plan length ratio compared to a provably optimal planner, or a cost-quality analysis on metric-sensitive domains.

## Limitations
- Computational unfairness: Comparison between GPU-based LLM inference (10-30 minutes) and CPU-based LAMA (milliseconds) conflates efficiency with capability
- Prompt sensitivity: The paper relies on few-shot prompting without ablation studies on prompt design
- Data contamination: The study doesn't verify whether training data included PDDL structures or planning concepts

## Confidence

- **High Confidence**: The empirical finding that semantic obfuscation degrades LLM performance while leaving LAMA unchanged. The magnitude of performance drops (DeepSeek R1: 157→93, Gemini 2.5 Pro: 155→146) is reproducible.
- **Medium Confidence**: The interpretation that semantic shortcuts explain LLM performance. While supported by the data, alternative explanations exist (e.g., semantic names provide better context for state tracking within the model's context window).
- **Low Confidence**: Claims about "substantial progress" in LLM planning capability. The paper shows GPT-5 matches LAMA on standard tasks, but this benchmark is narrow and doesn't demonstrate advantages in domains where classical planners excel.

## Next Checks
1. **Prompt Ablation Study**: Systematically vary prompt structure (examples, instructions, pitfalls checklist) across all three LLMs to isolate the effect of prompt design from model capability differences
2. **Temporal Reasoning Test**: Create a novel domain with temporal dependencies that couldn't plausibly exist in training data (e.g., complex time-window constraints) to distinguish pattern matching from genuine planning reasoning
3. **Efficiency Benchmarking**: Measure wall-clock time, energy consumption, and cost per solved task for both LLMs and LAMA across all domains to provide a complete efficiency-capability tradeoff analysis