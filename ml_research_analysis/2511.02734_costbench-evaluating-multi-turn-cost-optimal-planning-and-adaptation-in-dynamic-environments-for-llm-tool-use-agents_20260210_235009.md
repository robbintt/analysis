---
ver: rpa2
title: 'CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic
  Environments for LLM Tool-Use Agents'
arxiv_id: '2511.02734'
source_url: https://arxiv.org/abs/2511.02734
tags:
- tool
- cost
- wang
- tools
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CostBench is a new benchmark for evaluating LLM agents' ability
  to plan and adapt cost-optimal solutions in dynamic environments. It features a
  travel-planning domain with atomic and composite tools, each with randomized costs,
  and introduces four types of runtime blocking events (e.g., cost changes, tool failures)
  to test adaptive replanning.
---

# CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents

## Quick Facts
- arXiv ID: 2511.02734
- Source URL: https://arxiv.org/abs/2511.02734
- Authors: Jiayu Liu; Cheng Qian; Zhaochen Su; Qing Zong; Shijue Huang; Bingxiang He; Yi R. Fung
- Reference count: 37
- Primary result: Even leading LLMs achieve <75% exact match on hardest static tasks, with ~40% performance drop under dynamic conditions.

## Executive Summary
CostBench is a new benchmark for evaluating LLM agents' ability to plan and adapt cost-optimal solutions in dynamic environments. It features a travel-planning domain with atomic and composite tools, each with randomized costs, and introduces four types of runtime blocking events (e.g., cost changes, tool failures) to test adaptive replanning. Across ten leading LLMs, even the best model, GPT-5, achieves less than 75% exact match on the hardest static tasks, with performance dropping by around 40% under dynamic conditions, revealing significant gaps in cost-aware reasoning and adaptability. CostBench provides a scalable framework for diagnosing these weaknesses and advancing robust, economically rational LLM agents.

## Method Summary
CostBench evaluates LLM agents in a travel-planning domain where tasks require selecting sequences of atomic and composite tools to minimize total cost. Tools have randomized costs per instance (atomic: uniform [15,25], composite: sum + Gaussian noise, std=0.1). The benchmark operates in static mode (pre-calculated optimal path) and dynamic mode (runtime blocking events like cost changes or tool failures). Evaluation uses exact match ratio, cost gap, and average normalized edit distance. The system simulates tool execution using graph-based shortest path algorithms rather than real APIs, allowing precise ground truth calculation.

## Key Results
- GPT-5 achieves <75% exact match on hardest static tasks, dropping ~40% under dynamic conditions
- Cost change blocking events cause steepest performance decline (~38% drop in EMR)
- Even high-performing models struggle with implicit blocking events requiring environmental awareness

## Why This Works (Mechanism)

### Mechanism 1: Cost-Randomized Tool Decoupling
- **Claim:** Randomizing tool costs per instance forces explicit economic reasoning rather than memorization
- **Core assumption:** Agents can reason over numeric costs when static paths are neutralized
- **Evidence anchors:** Abstract states diverse customizable costs; Section 3.1 details randomization prevents memorization
- **Break condition:** Narrow cost ranges or excessive Gaussian noise may cause random selection

### Mechanism 2: Runtime Perturbation for Replanning
- **Claim:** Dynamic blocking events disrupt initial plans, testing adaptive replanning capability
- **Core assumption:** Optimal planning requires state-aware policies that can re-initialize search upon environment shifts
- **Evidence anchors:** Abstract mentions four types of dynamic blocking events; Section 5.2 shows cost change yields steepest performance decline
- **Break condition:** Events occurring too early or late fail to test adaptive planning phase

### Mechanism 3: Simulation-Based Ground Truthing
- **Claim:** Graph-based simulation (Dijkstra's algorithm) isolates planning capability from tool execution noise
- **Core assumption:** Logical planning can be decoupled from tool execution itself
- **Evidence anchors:** Section 3 describes static/dynamic modes; Appendix B.2 emphasizes simulation using Dijkstra's algorithm
- **Break condition:** If logical state graph doesn't accurately model real tool dependencies, optimal plan may be practically infeasible

## Foundational Learning

- **Concept:** Shortest Path Algorithms (Dijkstra)
  - **Why needed here:** Essential for understanding how the benchmark calculates ground truth using weighted directed graphs
  - **Quick check question:** Can you explain how adding an edge with negative weight would break Dijkstra's algorithm, and why CostBench adds Gaussian noise only to positive costs?

- **Concept:** Markov Decision Processes (MDPs) / POMDPs
  - **Why needed here:** The dynamic mode transforms the task into sequential decision-making with changing environment
  - **Quick check question:** In dynamic setting, does the agent have full observability of state changes (Explicit Blocks) or partial observability (Implicit Blocks like Cost Change)?

- **Concept:** JSON Schema & Tool Definition
  - **Why needed here:** Benchmark relies on strict input/output typing to constrain agent and calculate tool graph edges
  - **Quick check question:** If composite tool T takes TypeA and outputs TypeC, what intermediate types must its atomic components handle?

## Architecture Onboarding

- **Component map:** Query Generator -> Tool Library -> Environment -> Evaluator -> Graph Simulation
- **Critical path:** Tool Definition -> Graph Construction -> Cost Randomization -> Agent Interaction -> Trajectory Comparison
- **Design tradeoffs:**
  - Simulation vs. Real Execution: Trades API realism for mathematical certainty of ground truth
  - Atomic vs. Composite Granularity: Increases search space exponentially, testing planning depth but increasing prompt size
- **Failure signatures:**
  - Unaccessible Calls: Attempting tools without required input datatypes (state awareness failure)
  - Redundant/Extra Calls: Repeating tools or continuing after goal (progress/termination awareness failure)
  - High Cost Gap with Low Edit Distance: Valid path but not cheapest one (weak cost-reasoning)
- **First 3 experiments:**
  1. Static Baseline (Seq=5): Run agent on static setting to establish EMR and Cost Gap baseline
  2. Noise Sensitivity Analysis: Vary noise_std to determine reliance on bundling discounts vs. atomic cost summing
  3. Dynamic Robustness Test: Enable "Cost Change" blocking to measure EMR decrease and analyze recovery rate

## Open Questions the Paper Calls Out
None

## Limitations
- Model access uncertainty: GPT-5 is not publicly available for independent verification
- Simulation vs. reality gap: Graph simulation may not capture practical challenges like network latency
- Fixed Gaussian noise parameter (std=0.1) without sensitivity analysis could affect difficulty balance
- Domain generalizability untested beyond travel planning context

## Confidence
- **High Confidence:** Cost-randomized tool decoupling mechanism is well-specified and reproducible
- **Medium Confidence:** Runtime perturbation mechanism is theoretically valid but uniform interval triggering may not reflect realistic scenarios
- **Low Confidence:** GPT-5 superiority claims cannot be independently verified; effectiveness at diagnosing specific reasoning weaknesses requires more diverse domain testing

## Next Checks
1. **Noise Sensitivity Analysis:** Vary Gaussian noise standard deviation (0.01, 0.1, 1.0) to measure impact on model performance and identify meaningful cost differentiation threshold
2. **Cross-Domain Transferability:** Implement CostBench framework in non-travel domain (e.g., software development) to test generalizability of observed planning weaknesses
3. **Real API Integration:** Replace graph simulation with actual API calls for subset of tools to measure performance gap between theoretical optimal planning and practical execution