---
ver: rpa2
title: Rethinking Expert Trajectory Utilization in LLM Post-training
arxiv_id: '2512.11470'
source_url: https://arxiv.org/abs/2512.11470
tags:
- data
- performance
- arxiv
- training
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Plasticity-Ceiling Framework to analyze
  expert trajectory utilization in LLM post-training, decomposing performance into
  SFT performance and RL plasticity. It establishes the sequential SFT-then-RL pipeline
  as the superior paradigm over synchronized approaches, which suffer from stability
  issues.
---

# Rethinking Expert Trajectory Utilization in LLM Post-training

## Quick Facts
- arXiv ID: 2512.11470
- Source URL: https://arxiv.org/abs/2512.11470
- Reference count: 32
- This paper introduces the Plasticity-Ceiling Framework to analyze expert trajectory utilization in LLM post-training, decomposing performance into SFT performance and RL plasticity

## Executive Summary
This paper introduces the Plasticity-Ceiling Framework to analyze expert trajectory utilization in LLM post-training, decomposing performance into SFT performance and RL plasticity. It establishes the sequential SFT-then-RL pipeline as the superior paradigm over synchronized approaches, which suffer from stability issues. The optimal SFT-to-RL transition occurs during the Stable or Mild Overfitting Sub-phase, maximizing both SFT performance and RL plasticity. Regarding data properties, larger SFT data scale determines the primary post-training potential ceiling, while trajectory difficulty acts as a performance multiplier. The minimum SFT validation loss serves as a robust predictor of the final post-training ceiling.

## Method Summary
The paper proposes a sequential SFT-then-RL pipeline for LLM post-training, using DAPOdc (DAPO with asymmetric clipping) as the RL algorithm. SFT is performed with batch size 512, learning rate 1e-5, and 8-9 epochs, while RL uses batch size 64, learning rate 1e-6, and rollout number 8. The framework introduces a systematic approach to determine the optimal SFT-to-RL transition point by identifying Stable and Mild Overfitting sub-phases based on validation loss trajectories. Data properties are analyzed across different SFT datasets (SFT889K, Uniform102K/Easy102K/Hard102K variants) and RL datasets (RL62K), with difficulty classification based on win rates from a 1.5B model. The method employs robust curve-fitting with sigmoidal power law models and evaluates performance across multiple mathematical reasoning benchmarks.

## Key Results
- Sequential SFT-then-RL pipeline outperforms synchronized approaches (SRFT, UPT, LUFFY) with 2.6× lower performance standard deviation
- Optimal SFT-to-RL transition occurs during Stable (L≤1.02×Lmin) or Mild Overfitting (1.02×Lmin<L<1.1×Lmin) sub-phases
- Larger SFT data scale determines the primary post-training potential ceiling, while trajectory difficulty acts as a performance multiplier
- Minimum SFT validation loss serves as a robust predictor of final post-training ceiling (Pearson correlation: 0.93)

## Why This Works (Mechanism)
The Plasticity-Ceiling Framework works by decomposing post-training performance into two components: SFT performance (the knowledge foundation) and RL plasticity (the ability to refine and adapt). The sequential pipeline allows the model to first establish a strong knowledge base through SFT, then refine reasoning capabilities through RL when the model has reached an optimal balance point. The Mild Overfitting sub-phase provides this sweet spot where the model has sufficient knowledge but retains enough plasticity for RL to make meaningful improvements. The framework also shows that data properties create a multiplicative effect on performance rather than additive, with larger datasets providing the ceiling and trajectory difficulty amplifying the final results.

## Foundational Learning
- **SFT Validation Loss Tracking**: Understanding how to monitor and classify SFT phases using validation loss curves is critical for determining the optimal transition point to RL. Quick check: Verify you can identify Stable vs Mild Overfitting phases by checking if validation loss is below 1.02×Lmin.
- **DAPOdc Algorithm**: This RL algorithm with asymmetric clipping is essential for stable training. Quick check: Confirm you understand the asymmetric clipping parameters (ϵhigh=0.28, ϵlow=0.2) and how they differ from standard DAPO.
- **Trajectory Difficulty Classification**: The method of classifying trajectories by win rate (Easy WR=1.0, Hard WR=0/0.25) using a 1.5B model is crucial for data property analysis. Quick check: Verify your difficulty classification matches the win rate thresholds.
- **Curve Fitting with Sigmoidal Power Law**: Robust fitting of validation loss curves using Modified Z-score thresholds and LTS parameters is needed for phase identification. Quick check: Ensure your curve fitting produces the expected sub-phase boundaries.
- **Dynamic Difficulty Sampling**: The RL training uses dynamic difficulty sampling with 128 responses per inference. Quick check: Confirm your sampling strategy matches the specified parameters.

## Architecture Onboarding
- **Component Map**: Qwen2.5-7B -> SFT Phase (batch 512, lr 1e-5) -> Validation Loss Monitoring -> RL Phase (batch 64, lr 1e-6, DAPOdc) -> Performance Evaluation
- **Critical Path**: The most critical sequence is SFT training → validation loss monitoring → phase classification (Stable/Mild Overfitting) → RL transition. Any deviation from this path, particularly starting RL too early in the Adaptive phase, leads to suboptimal performance ceilings.
- **Design Tradeoffs**: The framework trades off between SFT performance and RL plasticity. Starting RL too early sacrifices SFT performance; starting too late reduces RL plasticity. The Mild Overfitting sub-phase provides the optimal balance.
- **Failure Signatures**: If synchronized methods (SRFT, UPT, LUFFY) show performance fluctuations >2.6× standard deviation compared to DAPO baseline, this indicates instability issues with the synchronization paradigm on general-purpose models.
- **3 First Experiments**: 1) Run SFT validation loss tracking to identify Stable/Mild Overfitting phases using Eq. 13 thresholds. 2) Test RL transition timing by starting from checkpoints in different sub-phases. 3) Compare performance of sequential vs synchronized approaches on Qwen2.5-7B.

## Open Questions the Paper Calls Out
- Does the Plasticity-Ceiling Framework and the superiority of the sequential SFT-then-RL pipeline generalize to domains without verifiable, rule-based rewards? The experimental setup exclusively utilizes mathematical reasoning benchmarks and binary correctness rewards, which provide dense, objective feedback that may not exist in open-ended dialogue or creative writing tasks.
- Do the data scaling laws (refuting "Less is More") and the definition of the "Mild Overfitting" sub-phase hold for models with significantly larger parameter counts (e.g., 70B+)? Larger models possess stronger pre-trained priors and may exhibit different saturation dynamics.
- Can Synchronized SFT-RL (Syn-SFT-RL) methods be stabilized to compete with the sequential pipeline, or is their instability fundamental to the paradigm? The paper benchmarks existing implementations but does not explore if architectural modifications could mitigate the observed instability.

## Limitations
- The study focuses exclusively on mathematical reasoning tasks using Qwen2.5-7B models, limiting applicability to other reasoning domains like code, science, or general knowledge tasks.
- The framework assumes sequential SFT-then-RL as optimal, but this conclusion is based on comparisons with synchronized methods that showed instability on general-purpose models.
- The optimal transition timing and data property relationships may not generalize to different task types or model scales beyond the tested 7B parameter range.

## Confidence
- **High Confidence**: Sequential SFT-then-RL pipeline superiority for mathematical reasoning, importance of starting RL during Stable or Mild Overfitting sub-phases, and minimum SFT validation loss as predictor of post-training ceiling.
- **Medium Confidence**: Claim that larger SFT data scale determines primary post-training potential ceiling, as relationship could be influenced by other factors like trajectory quality distribution.
- **Low Confidence**: Generalization of trajectory difficulty as performance multiplier across different domains and specific numerical thresholds (1.02×Lmin) may not transfer without empirical validation.

## Next Checks
1. **Cross-domain validation**: Apply the framework to non-mathematical reasoning tasks (e.g., code generation, scientific reasoning) to test whether SFT-to-RL transition timing and data property relationships hold across domains.
2. **Model scale verification**: Test the plasticity-ceiling relationship on larger models (e.g., 70B+ parameters) to determine if optimal transition phases and minimum validation loss thresholds scale consistently.
3. **Synchronized method re-evaluation**: Implement and test alternative synchronized approaches with enhanced stability mechanisms to determine if observed instability is implementation-specific rather than paradigm-inherent.