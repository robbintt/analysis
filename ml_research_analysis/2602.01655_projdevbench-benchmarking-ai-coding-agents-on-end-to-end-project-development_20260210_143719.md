---
ver: rpa2
title: 'ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development'
arxiv_id: '2602.01655'
source_url: https://arxiv.org/abs/2602.01655
tags:
- code
- agents
- project
- coding
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProjDevBench introduces a benchmark for evaluating AI coding agents
  on end-to-end project development, combining Online Judge (OJ) testing with LLM-assisted
  code review to assess system architecture design, functional correctness, and iterative
  refinement. The benchmark curates 20 programming problems across 8 categories, evaluating
  six coding agents on different LLM backends.
---

# ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development

## Quick Facts
- **arXiv ID**: 2602.01655
- **Source URL**: https://arxiv.org/abs/2602.01655
- **Reference count**: 40
- **Primary result**: AI coding agents achieve 27.38% acceptance rate on end-to-end project development, with prolonged interaction correlating negatively with performance.

## Executive Summary
ProjDevBench introduces a benchmark for evaluating AI coding agents on end-to-end project development, combining Online Judge (OJ) testing with LLM-assisted code review to assess system architecture design, functional correctness, and iterative refinement. The benchmark curates 20 programming problems across 8 categories, evaluating six coding agents on different LLM backends. Results show agents handle basic functionality but struggle with complex system design, time complexity optimization, and resource management, with extended interaction (average 138 turns, 4.81M tokens per problem) correlating negatively with performance.

## Method Summary
The benchmark evaluates six coding agents (Cursor, Copilot, Claude Code, Augment, Codex CLI, Gemini CLI) across 20 programming problems using C++ with CMake build requirements. Each agent-model configuration runs once per problem with maximum 2-18 submissions allowed. Evaluation combines OJ verdicts (Accepted, Wrong Answer, Time Limit Exceeded, Runtime Error, Compile Error, Memory Limit Exceeded) with LLM-based code review, producing a final weighted score (80% execution, 20% review). The Docker environment uses Ubuntu 24.04 with gcc-13/g++-13, cmake, python3.12, and Node.js 20.

## Key Results
- Overall acceptance rate of 27.38% across 20 problems
- Codex with GPT-5 achieves highest final weighted score of 77.85
- Extended interaction (average 138 turns, 4.81M tokens per problem) correlates negatively with performance (Spearman ρ=-0.734)
- Open-source models lag behind closed-source models in performance
- Agents succeed at basic functionality (41.86% wrong answer) but fail at specification alignment, time complexity optimization, and resource management

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual evaluation (OJ + LLM code review) catches failures invisible to execution testing alone.
- Mechanism: Online Judge provides fine-grained verdicts enabling iterative debugging, while LLM-based code review detects specification violations that pass tests but break rules. Final score weights execution 80%, code review 20%.
- Core assumption: LLM-based code review approximates human judgment reliably enough for scalable evaluation.
- Evidence anchors:
  - [abstract] "Combining Online Judge (OJ) testing with LLM-assisted code review"
  - [Section 5.4] LLM judge achieves 85.2% accuracy with Cohen's κ=0.710 vs human annotations
  - [corpus] E2EDevBench uses hybrid test-case and LLM-based verification

### Mechanism 2
- Claim: Extended interaction length signals agent difficulty and predicts failure.
- Mechanism: Task difficulty manifests through prolonged multi-turn interaction rather than static code complexity. Agents averaging 138 turns and 4.81M tokens per problem show strong negative correlation between interaction length and final score.
- Core assumption: Efficient problem-solving should converge with fewer interaction cycles.
- Evidence anchors:
  - [abstract] "Extended interaction (average 138 turns, 4.81M tokens per problem) correlates negatively with performance"
  - [Section 5.3] Spearman correlations show ρ=-0.668 for turns vs. score, ρ=-0.734 for tokens vs. score

### Mechanism 3
- Claim: From-scratch project construction exposes system-level capability gaps hidden by patch-based benchmarks.
- Mechanism: Requiring agents to design directory structures, configure build systems, and maintain inter-file consistency reveals failures invisible to single-file or patch-based tasks.
- Core assumption: End-to-end construction is a superset of issue-resolution capability.
- Evidence anchors:
  - [abstract] "struggle with complex system design, time complexity optimization, and resource management"
  - [Section 5.1] Detailed failure mode analysis: specification misalignment, time complexity gaps, exception safety failures

## Foundational Learning

- Concept: **Online Judge (OJ) Systems**
  - Why needed here: OJ provides automated compilation, execution, and fine-grained verdict classification that enables interpreting the 27.38% acceptance rate and failure mode breakdown.
  - Quick check question: Given a submission that passes all test cases but uses O(N²) where O(N log N) is required, what OJ verdict would you expect?

- Concept: **Build Systems (CMake)**
  - Why needed here: ProjDevBench requires agents to produce valid CMakeLists.txt configurations, and many failures stem from incorrect build artifacts.
  - Quick check question: What minimal CMakeLists.txt would compile main.cpp and utils.cpp into an executable named "code"?

- Concept: **Spearman Rank Correlation**
  - Why needed here: Section 5.3 uses Spearman ρ to quantify the negative relationship between interaction length and performance.
  - Quick check question: If turns and tokens both show strong negative correlation with score, but static code complexity shows weak correlation, what does this suggest about what makes problems hard for agents?

## Architecture Onboarding

- Component map: Natural language task specification → Agent layer → Docker execution → OJ submission → OJ verdicts + Code review → Final weighted score
- Critical path:
  1. Agent receives task specification (README.md, constraints)
  2. Agent designs project structure, implements files, configures CMakeLists.txt
  3. Agent commits to Git, pushes to remote repository
  4. Agent submits to OJ via oj_client.py
  5. OJ returns verdict + diagnostic signals
  6. Code review executes rule-based checks + LLM assessment
  7. Final score computed; agent iterates within submission limit

- Design tradeoffs:
  - 20 problems vs. scale: Depth of evaluation vs. breadth and statistical power
  - C++ focus: Language-specific insights vs. cross-language generalization
  - Fully autonomous vs. human-in-loop: Isolates end-to-end capability vs. interactive workflows
  - Single evaluation pass: Reduces compute cost vs. capturing variance

- Failure signatures:
  - Specification misalignment: High Wrong Answer rate with locally passing tests
  - Time complexity gaps: Time Limit Exceeded on large test cases
  - Exception safety failures: Memory Leaks, Runtime Errors (SIGSEGV, Aborted)
  - Version control workflow gaps: Discrepancy between local git history and submitted state
  - Prolonged interaction without progress: Extended turns/tokens with low final score

- First 3 experiments:
  1. Baseline agent comparison: Run all six agents with GPT-5 backend on 5 easiest problems
  2. Interaction length analysis: Log turn-by-turn actions for a single hard problem
  3. Code review validation: Run LLM-based code review alongside human expert review

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed failure modes in C++ extend to other programming languages like Python or Java?
- Basis in paper: The paper states it "focuses primarily on C++, and it remains unclear whether observed agent behaviors generalize to other languages."
- Why unresolved: The current benchmark tasks are designed specifically for C++ build systems and conventions.
- What evidence would resolve it: Evaluation of agents on a translated version of ProjDevBench or a new dataset using different languages.

### Open Question 2
- Question: Can architectural improvements allow agents to convert prolonged debugging interactions into successful solutions?
- Basis in paper: The authors note that "Extended interaction indicates difficulty and correlates negatively with performance."
- Why unresolved: Current agents struggle to make forward progress during long interactions, often spinning wheels rather than converging on a fix.
- What evidence would resolve it: A study showing a positive correlation between interaction turns and success rate, or an agent architecture that specifically leverages extended context for debugging.

### Open Question 3
- Question: How does the inclusion of human feedback during the development loop affect performance on project-level tasks?
- Basis in paper: The paper excludes human-in-the-loop workflows to isolate end-to-end capabilities but notes that "extending to interactive settings is a promising future direction."
- Why unresolved: The current evaluation setup forces fully autonomous execution without external guidance.
- What evidence would resolve it: Comparative results between fully autonomous runs and runs with intermittent human validation or guidance.

## Limitations

- Scalability remains unclear due to curation effort required for multi-file problems with robust test suites
- C++-only focus with CMake requirements may not generalize to other language ecosystems
- Single evaluation pass per agent-model configuration doesn't capture variance across runs
- Limited problem count constrains statistical power and generalizability

## Confidence

- **High Confidence**: Mechanisms combining execution testing with LLM review creating orthogonal diagnostic signals; extended interaction correlating with failure
- **Medium Confidence**: Claims about system-level capability gaps due to C++-specific context and limited problem count
- **Low Confidence**: Practical significance of 27.38% acceptance rate without baseline human performance context

## Next Checks

1. **LLM Code Review Validation**: Run code review pipeline on 10 submitted repositories alongside human expert review to measure accuracy, Cohen's κ, and specific failure category detection
2. **Cross-Language Generalization**: Replicate benchmark methodology with a small subset of problems in Python/JavaScript using their respective build systems
3. **Interaction Length Analysis**: For a single hard problem, log turn-by-turn agent actions to identify when agents first achieve passing submissions and correlate extended turns with specific failure modes