---
ver: rpa2
title: Reducing Tokenization Premiums for Low-Resource Languages
arxiv_id: '2601.13328'
source_url: https://arxiv.org/abs/2601.13328
tags:
- tokens
- language
- languages
- vocabulary
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes tokenization premiums for low-resource languages
  across ten popular language models, finding that non-Latin scripts like Bangla,
  Hindi, and Urdu incur 3-5x higher tokenization costs than English. The authors propose
  a method to reduce these premiums by adding single-token encodings for multi-token
  characters in frozen models.
---

# Reducing Tokenization Premiums for Low-Resource Languages

## Quick Facts
- **arXiv ID**: 2601.13328
- **Source URL**: https://arxiv.org/abs/2601.13328
- **Reference count**: 4
- **Primary result**: Llama 3.2 1B achieves 0.70-0.98 cosine similarity when compressing inputs for 12 low-resource languages by adding single-token encodings for multi-token characters

## Executive Summary
This paper analyzes tokenization premiums for low-resource languages across ten popular language models, finding that non-Latin scripts like Bangla, Hindi, and Urdu incur 3-5x higher tokenization costs than English. The authors propose a method to reduce these premiums by adding single-token encodings for multi-token characters in frozen models. Using Llama 3.2 1B, they demonstrate that compressed inputs with new character tokens achieve high similarity (0.70-0.98 cosine similarity) to original inputs across 12 low-resource languages, with linear regression at layer 0 being the most effective embedding strategy.

## Method Summary
The authors propose a method to reduce tokenization premiums in frozen language models by augmenting the vocabulary with single-token encodings for multi-token Unicode characters in low-resource languages. For each missing character, they extract the original token sequence's embeddings, compute hidden representations at a chosen layer, and use linear regression (or k-NN/local methods) to predict a single input embedding. These new tokens are added to the vocabulary, enabling compressed tokenization that maintains high cosine similarity (0.70-0.98) to original representations when averaged at layer 16.

## Key Results
- Tokenization premiums for low-resource languages are 3-5x higher than English in modern LMs
- Linear regression at layer 0 achieves the best average cosine similarity (0.809-0.982) across 12 languages
- Deeper layers (15-16) provide better preservation for worst-case languages like Amharic and Shan
- The approach maintains high similarity while reducing token counts for Hindi, Bangla, and Urdu

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BPE tokenizers trained on English-dominated corpora systematically underserve non-Latin scripts, creating tokenization premiums.
- Mechanism: BPE iteratively merges the most frequent adjacent token pairs into new vocabulary entries. When training corpora are dominated by English and other Latin-script languages, merge operations rarely create tokens for non-Latin characters. These characters must then be encoded as byte sequences split across multiple tokens, inflating token counts by 3-4x for languages like Hindi, Bangla, and Urdu.
- Core assumption: Tokenization premiums are primarily driven by vocabulary composition, not architectural constraints in the transformer itself.
- Evidence anchors:
  - [abstract] "low-resource languages suffer from substantial tokenization premiums in modern LMs... This tokenization premium results in increased API and energy costs and reduced effective context windows"
  - [section 1] "As a data-driven procedure, BPE learns a vocabulary that reflects the training corpus. High-resource languages, especially English, typically dominate available training data"
  - [corpus] Related work (Petrov et al., 2023; Ahia et al., 2023) confirms downstream cost impacts of tokenizer imbalances across multiple model families
- Break condition: If the model's training corpus contained no content in the target language, derived embeddings would have no semantic foundation to approximate.

### Mechanism 2
- Claim: Averaging input embeddings of multi-token character sequences produces a viable single-token embedding replacement.
- Mechanism: When a Unicode character is encoded as multiple tokens, each constituent token has a learned embedding. The linear regression approach at layer 0 learns an affine transformation from hidden representations back to input embeddings. At layer 0, hidden states equal input embeddings (Es = Hs), so the learned transformation is the identity—making this equivalent to simple averaging. This preserves the centroid of the original embedding subspace.
- Core assumption: The frozen model's internal representations for multi-token characters are sufficiently captured by their embedding centroid.
- Evidence anchors:
  - [section 4.2] "Linear regression at layer 0 is the best strategy on average. It is actually equivalent to simply averaging the input embeddings, as the learned transformation is the identity"
  - [section 4.2] Table 4 shows layer 0 LinReg achieving 0.809-0.982 cosine similarity across languages
  - [corpus] Related work on cross-lingual tokenizer inequities (arxiv 2510.21909) corroborates that vocabulary design, not model architecture, is the primary efficiency bottleneck
- Break condition: For languages where multi-token character constituents share semantic overlap with unrelated tokens (high embedding collision), averaging introduces noise that degrades downstream representations.

### Mechanism 3
- Claim: Deeper layer hidden representations better preserve semantics for poorly-tokenized languages when computing replacement embeddings.
- Mechanism: Later transformer layers contextualize token representations, separating semantic content from suboptimal tokenization artifacts. Local linear regression at layers 15-16 outperforms layer 0 methods for Amharic (amh) and Shan (shn)—languages with the highest tokenization premiums—suggesting the model learns to normalize fragmented encodings.
- Core assumption: Contextualized representations at deeper layers are more robust to tokenization variation than raw embeddings.
- Evidence anchors:
  - [section 4.2] "The worst languages for local strategies at layer 0 (amh and shn) are better preserved by the same strategies at later layers, suggesting that... the model determines appropriate contextualized representations for the constituents of multi-token characters"
  - [section 4.2] Table 4 shows Amharic improving from ~0.05 (layer 0 KNN) to 0.68-0.70 (layer 16 local methods)
  - [corpus] Weak direct evidence; related papers focus on tokenizer design rather than layer-wise embedding derivation
- Break condition: If deeper layer representations become too task-specific, they may not generalize as input embeddings for arbitrary new contexts.

## Foundational Learning

- Concept: Byte-Pair Encoding (BPE) vocabulary construction
  - Why needed here: Understanding how vocabularies become skewed requires knowing that BPE is frequency-driven and corpus-dependent.
  - Quick check question: Given a corpus with 90% English and 10% Hindi text, would BPE create more English or Hindi word-level tokens?

- Concept: Embedding matrices as input representations
  - Why needed here: The mitigation strategy requires predicting new rows in the embedding matrix that behave similarly to multi-token sequences.
  - Quick check question: If a character is encoded as 3 tokens with embeddings [e1, e2, e3], what operation produces a single candidate replacement embedding?

- Concept: Cosine similarity for representation comparison
  - Why needed here: The evaluation methodology relies on cosine similarity between hidden states to measure preservation quality.
  - Quick check question: Two vectors with cosine similarity 0.85 are more or less aligned than vectors with similarity 0.95? What does 0.0 indicate?

## Architecture Onboarding

- Component map:
  - Tokenizer (BPE/WordPiece/ULM) -> Vocabulary (set of tokens with IDs)
  - Embedding Matrix V₀ (|vocab| × d_model) -> Maps token IDs to vectors
  - Transformer Blocks (layers 1-16 in Llama 3.2 1B) -> Contextualize embeddings
  - Hidden State Matrix Vₗ -> Per-layer representations of vocabulary tokens
  - Augmented Vocabulary -> Original vocab + new character tokens
  - Predicted Embeddings -> Derived vectors for new tokens

- Critical path:
  1. Identify missing Unicode characters in target language (those requiring multi-token encoding)
  2. Extract original token sequences for each missing character
  3. Compute hidden representations at chosen layer for each sequence
  4. Train prediction model (LinReg/KNN/LocalLinReg) mapping hidden states -> input embeddings
  5. Add new tokens to vocabulary with predicted embeddings
  6. Evaluate via cosine similarity between original and compressed sentence representations

- Design tradeoffs:
  - Layer 0 (simple averaging) vs. deeper layers (contextualized but less stable): Layer 0 is simplest and best overall; deeper layers help worst-case languages
  - KNN vs. Linear Regression: KNN adapts locally but requires hyperparameter k; LinReg is global and stable
  - Input-only vs. input+output augmentation: Output augmentation causes "vote-splitting" where probability mass fragments between new and old tokens

- Failure signatures:
  - Very low cosine similarity (<0.3) indicates predicted embeddings fall outside the model's learned representation space
  - Amharic at layer 0 with local methods (0.05-0.06 similarity) suggests constituent token embeddings are semantically incoherent when averaged
  - Negative similarity scores (e.g., LinReg at layer 16 for Vietnamese: -0.026) indicate learned transformations fail to generalize across the full vocabulary distribution

- First 3 experiments:
  1. Replicate layer 0 linear regression on Llama 3.2 1B for Hindi: Extract multi-token character sequences, average their embeddings, add as new tokens, measure cosine similarity on FLORES-200 dev split.
  2. Compare layer 0 vs. layer 16 local linear regression for Amharic to validate the "deeper layers help worst-case languages" finding; use k=5 neighbors.
  3. Test "vote-splitting" hypothesis: Run text generation with augmented vocabulary and measure probability mass assigned to new tokens vs. original sub-character tokens for a held-out test set.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can vocabulary selection be improved by re-weighting tokens according to estimated first-language speaker populations rather than raw corpus frequency? Current BPE prioritizes popular substrings from internet corpora, which are skewed toward dominant languages regardless of speaker populations.

- **Open Question 2**: Can a lightweight pseudo-translation mapping between embedding spaces enable frozen models to process low-resource language inputs more efficiently? The paper proposes this approach but doesn't test whether simple models can create useful continuous-space pseudo-translations.

- **Open Question 3**: Which strategy best mitigates the vote-splitting problem when augmenting vocabularies: input-only compression, removing merged sub-character tokens, or winner-takes-all probability assignment? The authors identify the problem and propose solutions but don't experimentally compare them.

## Limitations

- **Single-language optimization bias**: The method optimizes for individual languages by adding new tokens for their specific OOV characters, creating vocabulary bloat when applying to multiple languages simultaneously.

- **No downstream task evaluation**: The paper only measures cosine similarity between compressed and original representations at the model level, without validating whether these compressed inputs maintain performance on actual downstream tasks.

- **Training data contamination concerns**: The FLORES-200 dataset used for evaluation was likely part of the training corpus for Llama 3.2 1B, creating a risk of inflated similarity scores due to memorization rather than genuine representation preservation.

## Confidence

**High confidence**: The core observation that BPE tokenizers create systematic inequities for non-Latin scripts is well-established in the literature and supported by extensive evidence. The mechanism linking corpus composition to vocabulary coverage is straightforward and the empirical findings on tokenization premiums (3-5x increases) are consistent with related work.

**Medium confidence**: The proposed mitigation strategy works as described for the specific experimental setup (Llama 3.2 1B, FLORES-200, layer 0 linear regression). However, the claim that this approach generalizes to other model families, sizes, and languages requires additional validation. The observation that deeper layers help worst-case languages (Amharic, Shan) is supported but based on limited evidence.

**Low confidence**: Claims about practical deployment implications (reduced API costs, energy savings, context window improvements) are speculative. The paper doesn't measure actual cost reductions or test the approach in production settings. The "vote-splitting" phenomenon is mentioned but not empirically validated.

## Next Checks

1. **Downstream task preservation**: Apply the compressed tokenization approach to a multilingual classification or translation task using the same 12 languages. Measure whether task performance degrades when using compressed inputs versus original tokenizations, and compare against baseline multilingual models.

2. **Cross-lingual embedding transfer**: Instead of training separate embedding predictors for each language, train a single model to predict embeddings for all OOV characters across languages. Evaluate whether this approach maintains representation quality while reducing vocabulary bloat, and test if embeddings learned for one language transfer to others.

3. **Larger model validation**: Replicate the entire experimental pipeline on Llama 3.1 8B and 70B models. Compare whether layer 0 linear regression remains optimal or if deeper layer strategies become more effective, and measure how the relative performance of different prediction methods scales with model size.