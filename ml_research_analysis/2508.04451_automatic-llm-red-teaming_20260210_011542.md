---
ver: rpa2
title: Automatic LLM Red Teaming
arxiv_id: '2508.04451'
source_url: https://arxiv.org/abs/2508.04451
tags:
- target
- teaming
- language
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hierarchical reinforcement learning framework
  for automated red teaming of large language models (LLMs). The approach addresses
  the limitations of single-turn attacks by modeling adversarial interactions as multi-turn
  dialogues through a Markov Decision Process (MDP).
---

# Automatic LLM Red Teaming

## Quick Facts
- arXiv ID: 2508.04451
- Source URL: https://arxiv.org/abs/2508.04451
- Authors: Roman Belaire; Arunesh Sinha; Pradeep Varakantham
- Reference count: 23
- Primary result: 97.0% ASR@30 against Llama-3.1-8B-Instruct in context-aware settings

## Executive Summary
This paper presents a hierarchical reinforcement learning framework for automated red teaming of large language models (LLMs) that addresses the limitations of single-turn attacks by modeling adversarial interactions as multi-turn dialogues. The system achieves state-of-the-art performance on multiple safety benchmarks, demonstrating significant improvements over existing methods like Rainbow Teaming, Ferret, and WildTeaming. The approach successfully transfers to larger and closed-source models including Llama-3.1-70B, Mistral-8x22B, and GPT-4o.

## Method Summary
The approach employs a hierarchical reinforcement learning architecture with two levels: a high-level policy that generates strategic attack guides and a low-level policy that generates coherent utterances token-by-token. The system models adversarial interactions as multi-turn dialogues using a Markov Decision Process (MDP) framework. A novel token-level marginal contribution reward function is introduced to attribute credit for successful attacks at the token level, addressing the challenge of credit assignment in multi-turn dialogues. The method is evaluated on HarmBench, JailbreakBench, and WildBench datasets, demonstrating superior performance in context-aware settings.

## Key Results
- Achieves 97.0% attack success rate at 30 steps (ASR@30) against Llama-3.1-8B-Instruct
- Outperforms Rainbow Teaming (11.0%), Ferret (82.5%), and WildTeaming (76.1%) on benchmark datasets
- Successfully transfers to larger models (Llama-3.1-70B) and closed-source models (GPT-4o, Mistral-8x22B)

## Why This Works (Mechanism)
The hierarchical reinforcement learning approach works by decomposing the complex problem of automated red teaming into manageable components. The high-level policy creates strategic attack plans that guide the overall adversarial interaction, while the low-level policy focuses on generating natural, coherent dialogue tokens that execute these strategies. This separation allows the system to maintain both strategic coherence and linguistic quality. The token-level marginal contribution reward function enables precise credit assignment, allowing the system to learn which specific tokens and strategies are most effective at circumventing safety measures, rather than just attributing success to entire utterances.

## Foundational Learning
- **Hierarchical Reinforcement Learning**: Needed for decomposing complex multi-turn dialogue tasks into strategic planning and token generation; Quick check: Verify the high-level and low-level policies are properly coordinated and that the hierarchical structure improves performance over flat RL approaches.
- **Markov Decision Process for Dialogue**: Needed to formally model the sequential nature of adversarial interactions; Quick check: Confirm that the state representation captures sufficient context for decision-making across multiple turns.
- **Token-level Credit Attribution**: Needed to identify which specific tokens contribute to attack success in multi-turn dialogues; Quick check: Validate that the marginal contribution rewards correctly identify influential tokens and improve learning efficiency.
- **Context-aware Attack Generation**: Needed to adapt attacks based on the specific model's responses and conversation history; Quick check: Test whether attacks maintain effectiveness when the target model's behavior changes during interaction.

## Architecture Onboarding

**Component Map**: High-level policy -> Low-level policy -> Token generation -> Model response -> Reward calculation

**Critical Path**: State observation → High-level policy → Strategic guide generation → Low-level policy → Token generation → Model response → Reward calculation → Policy update

**Design Tradeoffs**: The hierarchical structure trades computational complexity for improved strategic coherence and token-level precision. While single-turn attacks are computationally simpler, they fail to capture the nuanced, multi-turn nature of real adversarial interactions. The token-level reward mechanism adds complexity but provides more granular learning signals compared to utterance-level rewards.

**Failure Signatures**: 
- Low attack success rates despite high reward values indicate reward function misalignment
- High variance in attack outcomes suggests poor policy generalization
- Inability to transfer to larger models indicates overfitting to specific model architectures
- Token generation failures suggest low-level policy collapse or reward hacking

**First Experiments**:
1. Validate the hierarchical structure by comparing performance against a flat RL baseline
2. Test token-level reward attribution by conducting ablation studies with utterance-level rewards
3. Evaluate transfer learning by testing attacks on models with different architectures and parameter counts

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on specific safety benchmarks that may not represent all adversarial scenarios
- Token-level reward mechanism requires validation across diverse tasks beyond safety violations
- Limited analysis of failure modes when attacks do not succeed on larger or closed-source models

## Confidence

**Major Claim Confidence:**
- **High confidence**: Hierarchical RL framework architecture and token-level reward attribution method
- **Medium confidence**: State-of-the-art performance claims on benchmark datasets
- **Medium confidence**: Transferability results to larger and closed-source models

## Next Checks
1. Test the attack framework on non-safety-related tasks (e.g., task completion, reasoning accuracy) to evaluate generalizability beyond jailbreaking scenarios
2. Conduct ablation studies removing the hierarchical structure to quantify the specific contribution of multi-turn dialogue modeling versus other components
3. Evaluate attack success rates under different computational budgets (varying max steps beyond 30) to understand practical cost-benefit trade-offs of the method