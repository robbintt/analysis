---
ver: rpa2
title: Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models
arxiv_id: '2503.23523'
source_url: https://arxiv.org/abs/2503.23523
tags:
- knowledge
- question
- graph
- subgraph
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of knowledge-intensive Multiple
  Choice Question Answering (MCQA) for Large Language Models (LLMs), where traditional
  methods struggle due to missing or noisy knowledge graph (KG) information. The proposed
  Question-Aware Knowledge Graph Prompting (QAP) method enhances LLM reasoning by
  integrating question-aware GNN aggregation (QNA) to dynamically assess KG relevance
  and global attention-derived prompting (GTP) to infer missing knowledge across answer
  options.
---

# Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models

## Quick Facts
- arXiv ID: 2503.23523
- Source URL: https://arxiv.org/abs/2503.23523
- Authors: Haochen Liu, Song Wang, Chen Chen, Jundong Li
- Reference count: 8
- Key outcome: QAP improves MCQA accuracy by 3-8% over state-of-the-art methods, with question-aware GNN aggregation and cross-option knowledge transfer

## Executive Summary
This paper addresses knowledge-intensive Multiple Choice Question Answering (MCQA) for Large Language Models (LLMs) by proposing Question-Aware Knowledge Graph Prompting (QAP). The method enhances LLM reasoning by integrating question-aware Graph Neural Network (GNN) aggregation to dynamically assess KG relevance and global attention-derived prompting to infer missing knowledge across answer options. QAP outperforms state-of-the-art methods across multiple datasets, achieving accuracy improvements of up to 8.3% over baseline approaches while maintaining parameter efficiency through frozen LLM weights.

## Method Summary
QAP processes MCQA tasks by first extracting 2-hop subgraphs around entities in the question and answer options from a knowledge graph. A specialized 3-layer GNN with question-aware attention (QNA) computes node representations that incorporate question context through multi-head attention combining node-to-node, node-to-question, and question-to-node interactions. Global Attention-Derived Prompting (GTP) then captures inter-option relationships by computing attention between subgraph node embeddings and all question-option text sequences, producing a soft prompt that enriches the LLM input. The entire system is trained end-to-end with cross-entropy loss while keeping LLM parameters frozen, optimizing only the GNN and attention modules.

## Key Results
- QAP achieves 81.6% accuracy on OBQA, 77.8% on Riddle, and 83.8% on MedQA
- Outperforms strongest baseline GNP by 3.0-8.3% accuracy across all datasets
- Ablation studies show QNA contributes 6-12% improvement and GTP contributes 3-6% improvement
- Consistent performance across both encoder-decoder (Flan-T5) and decoder-only (Llama2) LLM architectures

## Why This Works (Mechanism)

### Mechanism 1: Question-Aware Neighborhood Aggregation (QNA)
Incorporating question embeddings into GNN attention weights improves KG relevance assessment compared to structure-only aggregation. Multi-head attention combines three components via weighted sum: node-to-node attention (captures graph structure), node-to-question attention, and question-to-node attention. Parameter γ ∈ (0, 0.5) controls the balance. This enables the GNN to upweight edges relevant to the specific question rather than treating all edges equally.

### Mechanism 2: Global Attention-Derived Prompting (GTP)
Cross-option attention enables inference of missing KG knowledge by transferring information from well-represented to poorly-represented answer options. For each subgraph, computes attention between its node embeddings and ALL n question-option text sequences, then concatenates and transforms via FFN. MaxPooling produces one embedding per option, concatenated into the soft prompt.

### Mechanism 3: End-to-End Soft Prompt Optimization with Frozen LLM
Optimizing only the QNA and GTP parameters via cross-entropy loss while freezing LLM weights effectively aligns KG-derived prompts with reasoning. Soft prompt prepended to question/option input guides LLM output. Gradients flow through frozen LLM to update only the GNN and attention modules.

## Foundational Learning

- **Graph Neural Networks (message passing)**:
  - Why needed here: QNA is fundamentally a GNN that aggregates neighbor information with attention-weighted messages
  - Quick check question: Given a node with three neighbors, how would you compute its updated representation using attention-weighted aggregation?

- **Multi-head attention mechanisms**:
  - Why needed here: Both QNA (4 heads) and GTP use attention to compute relevance weights
  - Quick check question: Why might multi-head attention capture different aspects of relevance compared to single-head attention?

- **Soft prompting vs. fine-tuning tradeoffs**:
  - Why needed here: QAP freezes the LLM and only trains soft prompts
  - Quick check question: What information can soft prompts encode that might be lost compared to full fine-tuning, and vice versa?

## Architecture Onboarding

- **Component map**: Entity linking -> Subgraph Retrieval -> QNA attention computation -> Node updates -> Global attention -> FFN fusion -> MaxPool -> Prompt concatenation -> LLM forward -> Cross-entropy

- **Critical path**: Entity linking → Subgraph extraction → QNA attention computation (Eqs. 3-5) → Node updates (Eqs. 1-2) → Global attention (Eq. 6) → FFN fusion (Eq. 7) → MaxPool (Eq. 8) → Prompt concatenation (Eq. 9) → LLM forward → Cross-entropy

- **Design tradeoffs**:
  - γ parameter: Higher values (0.4) favor question context; lower values (0.2) preserve graph structure
  - Decoder-only LLMs (Llama2) show instability with soft prompts—encoder-decoder (Flan-T5) is more robust
  - Computational overhead: GNN + global attention adds inference latency

- **Failure signatures**:
  - Llama2 with SP baseline drops to 23-28% accuracy—decoder-only models may misalign with prepended prompts
  - Ablation without QNA: ~6-12% drop
  - Ablation without GTP: ~3-6% drop
  - Performance degrades on sparse KGs

- **First 3 experiments**:
  1. **Ablation study**: Remove QNA, remove GTP, remove both. Validate each component's contribution
  2. **γ parameter sweep**: Test 0.0–0.5 on OBQA and MedQA to find optimal balance
  3. **Baseline comparison**: Run against LLM-only, PE, KGEP, SP, GNP on all three datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades significantly on sparse or disconnected KGs when subgraphs lack sufficient information
- Decoder-only LLM architectures (Llama2) show fundamental incompatibility with prepended soft prompts
- Computational overhead from 3-layer GNN and global attention may limit scalability for production systems

## Confidence

**High Confidence (Mechanistic Claims)**:
- QNA's question-aware attention mechanism correctly implements the described multi-component attention
- GTP's cross-option attention follows standard attention computation patterns
- End-to-end training with frozen LLM is technically feasible

**Medium Confidence (Performance Claims)**:
- 3-8% accuracy improvements over baselines are reproducible on the three tested datasets
- Optimal γ parameter values generalize within tested datasets
- Component ablations are correctly measured

**Low Confidence (Generalization Claims)**:
- Performance extends to other KG types beyond ConceptNet and UMLS
- Architecture-agnostic effectiveness across different LLM families
- Scalability to much larger KGs or questions with more entities

## Next Checks

1. **Sparse KG Robustness Analysis** - Systematically remove edges from ConceptNet/UMLS and measure QAP accuracy degradation to establish practical limits

2. **Cross-Domain Transferability** - Test QAP on a dataset from a completely different domain (e.g., scientific literature QA) with its own KG to validate generalization

3. **Memory and Latency Benchmarking** - Measure inference time and GPU memory usage of QAP vs baselines on identical hardware, including breakdown of costs for subgraph retrieval, QNA, and GTP