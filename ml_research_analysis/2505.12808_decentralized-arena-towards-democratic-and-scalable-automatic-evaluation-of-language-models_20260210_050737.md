---
ver: rpa2
title: 'Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation
  of Language Models'
arxiv_id: '2505.12808'
source_url: https://arxiv.org/abs/2505.12808
tags:
- llms
- ranking
- arena
- de-arena
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Decentralized Arena (De-Arena), a fully\
  \ automated framework for evaluating large language models (LLMs) that leverages\
  \ collective intelligence by having all LLMs evaluate each other in a democratic\
  \ pairwise manner. The method addresses key limitations in existing LLM evaluation\
  \ approaches\u2014such as bias from single-judge methods and scalability challenges\
  \ in human-based evaluations\u2014by implementing a coarse-to-fine incremental ranking\
  \ algorithm and an automatic representative question selection strategy."
---

# Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models

## Quick Facts
- arXiv ID: 2505.12808
- Source URL: https://arxiv.org/abs/2505.12808
- Reference count: 40
- This paper introduces De-Arena, a fully automated framework for evaluating LLMs that leverages collective intelligence by having all models evaluate each other in a democratic pairwise manner.

## Executive Summary
This paper introduces Decentralized Arena (De-Arena), a fully automated framework for evaluating large language models (LLMs) that leverages collective intelligence by having all LLMs evaluate each other in a democratic pairwise manner. The method addresses key limitations in existing LLM evaluation approaches—such as bias from single-judge methods and scalability challenges in human-based evaluations—by implementing a coarse-to-fine incremental ranking algorithm and an automatic representative question selection strategy. Across extensive experiments involving 66 LLMs, De-Arena achieves up to 97% correlation with human-judged Chatbot Arena rankings while significantly reducing evaluation costs.

## Method Summary
De-Arena implements a democratic pairwise evaluation system where all participating models act as both contestants and judges. The framework uses a coarse-to-fine incremental ranking algorithm with binary search for efficient insertion of new models into the ranking, combined with in-window reranking for refinement. Judge votes are weighted by each model's Elo score, calculated through pairwise comparisons. The system automatically selects representative questions from open-ended datasets for each of 9 fine-grained dimensions (math algebra, math geometry, math probability, logic reasoning, social reasoning, science chemistry, science biology, science physics, MT-bench) based on their correlation with overall rankings.

## Key Results
- Achieves up to 97% Spearman correlation with human-based Chatbot Arena rankings across 66 LLMs
- Reduces evaluation costs by requiring only O(kn log n) comparisons versus O(n²) for full pairwise comparison
- Demonstrates high reliability and stability, with performance improving as more models participate
- Shows strong resistance to single-judge and group biases through democratic aggregation of preferences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating judgments from all participant models reduces single-judge bias.
- **Mechanism:** The system leverages the "wisdom of crowds" where LLMs evaluate peer outputs. Because models have distinct training data and biases, the aggregation of preferences cancels out individual idiosyncrasies (e.g., a judge favoring its own output style).
- **Core assumption:** Judge models must exhibit diverse biases; if all judges share a similar bias (homogeneity), aggregation will fail to correct it.
- **Evidence anchors:** [abstract] "...mitigates single-model judge bias by democratic, pairwise evaluation..."; [section] Page 7, Table 3 shows LLaMA-3-70B favors itself, while De-Arena aligns better with human rankings.
- **Break condition:** The judge pool is homogeneous (e.g., all from the same model family), reintroducing group bias.

### Mechanism 2
- **Claim:** Coarse-to-fine incremental ranking achieves sub-quadratic efficiency without significant accuracy loss.
- **Mechanism:** The algorithm decouples the ranking process. First, it performs a coarse binary search ($O(kn \log n)$) to find a rough rank position. Second, it performs a fine-grained "in-window" reranking only among neighbors with similar capabilities, avoiding the expensive comparison of vastly different models.
- **Core assumption:** The ranking list is monotonic; binary search is valid because a model that beats the median likely beats the lower half.
- **Evidence anchors:** [abstract] "...coarse-to-fine incremental ranking algorithm... sub-quadratic efficiency."; [section] Page 8, Table 4 shows "Full Ranking" requires $\approx 4\times$ the judge counts compared to the coarse-to-fine method.
- **Break condition:** The "seed" list is poorly ranked initially, causing binary search to diverge or require excessive window correction.

### Mechanism 3
- **Claim:** Adaptive Elo-based weighting of judge votes improves reliability.
- **Mechanism:** The system calculates the Elo score for each model (as a participant). It then uses this score as a weight in the loss function when that model acts as a judge. Stronger models (higher Elo) exert more influence on the final ranking.
- **Core assumption:** High-performing models (participants) are also high-quality judges (evaluators).
- **Evidence anchors:** [section] Page 4, Section 3.1: "...utilize the normalized Elo score as the weight in the loss function."; [section] Page 9, Table 7 shows a drop in correlation when weights are removed ("No Weights").
- **Break condition:** High-performing models are poor judges (inverse correlation between ability and evaluation skill).

## Foundational Learning

- **Concept:** **Elo Rating System**
  - **Why needed here:** It is the mathematical core used to convert pairwise "win/loss" data into a scalar leaderboard score, handling the transitivity of model strength.
  - **Quick check question:** If Model A beats Model B, and Model B beats Model C, does the Elo system guarantee A > C? (Answer: Not strictly, but probabilistically yes).

- **Concept:** **Pairwise Comparison**
  - **Why needed here:** This is the atomic unit of data collection in the Arena. Instead of scoring a single response, models vote on "Response A vs. Response B."
  - **Quick check question:** Why is pairwise comparison often more robust than single-point scoring for LLMs? (Answer: It is easier for a model to identify relative quality than absolute quality).

- **Concept:** **Spearman Correlation**
  - **Why needed here:** This is the primary metric used to validate the system. It measures how well the automatic ranking order matches the human-derived Chatbot Arena ranking.
  - **Quick check question:** If De-Arena swaps the rank of Model #5 and #6, does it significantly impact the Spearman correlation? (Answer: No, it measures rank monotonicity, not absolute error).

## Architecture Onboarding

- **Component map:** Seed Ranker -> Binary Search Module -> Window Reranker -> Judge Pool -> Elo Calculator
- **Critical path:** 1. Initialize seed list (expensive step). 2. For every new model: Generate responses → Run Binary Search → Run Window Rerank → Update Elo for all.
- **Design tradeoffs:**
  - **Window Size ($W$):** Size 1 is cheaper but may miss subtle ranking inversions; larger windows are more accurate but costly (Table 9).
  - **Seed Size:** Small seeds are unstable; large seeds are expensive to bootstrap.
  - **Judge Count:** Using all models is robust but slow; sub-sampling judges is faster but risks bias.
- **Failure signatures:**
  - **Rank Oscillation:** If the "coarse" binary search error is too large, the "fine" window might fail to stabilize the rank.
  - **Self-Bias Accumulation:** If judge weighting is disabled, strong but biased models (e.g., favoring their own outputs) may distort the entire leaderboard.
- **First 3 experiments:**
  1. **Sanity Check:** Run De-Arena on a subset of 10 models and compare the resulting ranking against a "Full Sample" (all-pairs) ranking to verify algorithmic consistency.
  2. **Bias Ablation:** Compare ranking quality using "Single Strong Judge" (e.g., GPT-4) vs. "All Models" to reproduce the bias reduction claim (Table 3).
  3. **Scaling Test:** Measure the time/compute cost as $N$ (number of models) increases to verify the claimed $O(kn \log n)$ complexity.

## Open Questions the Paper Calls Out
- How can a decentralized evaluation framework accurately assess models that possess "super-human" intelligence when the judge pool consists of models with lower capabilities?
- Can the framework evolve to support fully automatic dimension discovery without relying on manual data collection?
- Does the assumption that higher Elo scores reliably indicate superior judgment capability hold across different model architectures and evaluation dimensions?
- To what extent does the framework remain robust against systemic "group bias" where a majority of participating models share similar training data contamination?

## Limitations
- The framework's effectiveness may be limited when evaluating models that exceed the collective intelligence of all participating judges
- Current implementation requires manual dimension definition and question collection from existing datasets
- The assumption that higher Elo scores indicate better judgment capability is not empirically validated across all dimensions

## Confidence
- Correlation claims: High (97% with human rankings is explicitly measured)
- Efficiency claims: Medium (O(kn log n) vs O(n²) is theoretically derived but computational cost measurements are limited)
- Bias reduction claims: Medium (demonstrated against single-judge bias but group bias potential acknowledged)

## Next Checks
1. Verify the implementation by running De-Arena on a small subset (10 models) and comparing against full pairwise ranking to ensure algorithmic consistency
2. Test robustness by comparing rankings using all models as judges versus a single strong judge to reproduce bias reduction findings
3. Validate efficiency claims by measuring computational cost as the number of models scales up to verify the O(kn log n) complexity claim