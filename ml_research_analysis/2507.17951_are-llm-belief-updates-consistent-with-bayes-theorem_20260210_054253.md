---
ver: rpa2
title: Are LLM Belief Updates Consistent with Bayes' Theorem?
arxiv_id: '2507.17951'
source_url: https://arxiv.org/abs/2507.17951
tags:
- evidence
- class
- more
- arxiv
- updates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric, the Bayesian Coherence Coefficient
  (BCC), to measure how consistently large language models update their beliefs in
  accordance with Bayes' theorem when presented with evidence. The authors generate
  a dataset with multiple categories, each containing classes, evidences, and conversation
  histories, and compute the BCC as the correlation between expected and observed
  credence updates.
---

# Are LLM Belief Updates Consistent with Bayes' Theorem?

## Quick Facts
- arXiv ID: 2507.17951
- Source URL: https://arxiv.org/abs/2507.17951
- Reference count: 34
- Key outcome: BCC increases log-linearly with parameter count (r = 0.906, p < 10^-6) and correlates with benchmark performance

## Executive Summary
This paper introduces the Bayesian Coherence Coefficient (BCC) to measure how consistently LLMs update beliefs in accordance with Bayes' theorem. The authors generate a dataset with categories, classes, evidences, and conversation histories, then compute the correlation between expected (log likelihood ratios) and observed (log odds ratios) credence updates. Testing five model families up to 14B parameters, they find that larger models exhibit higher BCC values and that all models systematically under-update relative to Bayesian optimal updates. The results suggest scaling improves Bayesian coherence, which has implications for AI alignment and governance.

## Method Summary
The authors create a novel dataset using GPT-4o to generate categories with classes, evidences, and conversation histories. For each proposition-evidence pair, they compute priors P(c|h,k), likelihoods P(x|c,h,k), and posteriors P(c|x,h,k) using cumulative token probabilities from the target LLM. The Bayesian Coherence Coefficient (BCC) is then calculated as the correlation between expected updates (log likelihood ratios) and observed updates (log odds ratios) across all 6,460 tuples. They evaluate multiple pre-trained LLMs across five model families, testing models ranging from 125M to 14B parameters.

## Key Results
- BCC increases log-linearly with parameter count (r = 0.906, p < 10^-6) across five model families
- Larger models show significantly higher BCC values, suggesting improved Bayesian coherence with scale
- All tested models systematically under-update relative to Bayesian optimal updates
- BCC shows significant positive correlations with BIG-Bench Hard, GPQA, MMLU-PRO, and Math Lvl 5 benchmark scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Correlation-based metrics capture Bayesian coherence better than error-based metrics because they are not confounded by output entropy.
- Mechanism: The Bayesian Coherence Coefficient (BCC) computes the correlation between expected updates (log likelihood ratios) and observed updates (log odds ratios) across proposition-evidence pairs. Unlike error-based metrics, BCC remains invariant to temperature scaling and avoids rewarding uniform distributions that trivially minimize error.
- Core assumption: Cumulative token probabilities assigned by the LLM accurately proxy credence in propositions.
- Evidence anchors:
  - [abstract] "We formulate a Bayesian Coherence Coefficient (BCC) metric... BCC increases log-linearly with the number of model parameters (r = 0.906, p < 10^-6)"
  - [Appendix A.2] BCE exhibits undesirable bias toward high-entropy distributions; a uniform distribution achieves BCE = 0 despite providing no meaningful information.
  - [corpus] Related work on Martingale Score (arxiv 2512.02914) proposes an unsupervised metric for Bayesian rationality, corroborating interest in correlation-based coherence metrics.
- Break condition: If cumulative token probabilities do not reflect action-relevant belief states, BCC may measure something other than true Bayesian coherence.

### Mechanism 2
- Claim: Larger models exhibit higher BCC because scale enables more coherent internal world models that better approximate Bayesian updating.
- Mechanism: As parameter count increases, models appear to learn more consistent relationships between priors, likelihoods, and posteriors. The paper finds a log-linear relationship between BCC and parameter count across five model families.
- Core assumption: The scaling relationship generalizes beyond 14B parameters to larger frontier models.
- Evidence anchors:
  - [Section 4] "We found a significant positive correlation between BCC and the log of model parameter counts in the models we tested ((r = 0.906, p < 10−6))"
  - [Table 1] Shows consistent BCC increases within each model family as parameters scale (e.g., Llama 1.24B → 8.03B: BCC 0.658 → 0.739)
  - [corpus] Evidence is limited; neighboring papers focus on belief representation in RL agents rather than LLM scaling.
- Break condition: If the correlation is driven by benchmark performance rather than scale itself, or if instruction-tuning breaks the relationship, scaling predictions may not hold.

### Mechanism 3
- Claim: All tested models systematically under-update relative to Bayesian optimal updates.
- Mechanism: The gradient of observed vs. expected updates is consistently below 1. This under-updating correlates with low evidence likelihood—when evidence is improbable given the classes, models adjust beliefs less than Bayes' theorem would prescribe.
- Core assumption: Under-updating reflects a systematic bias rather than a dataset artifact.
- Evidence anchors:
  - [Section 4] "For all tested models, the gradient of observed vs. expected updates is less than 1"
  - [Section 5] "The inverse correlation between this update gradient and the negative evidence log likelihood averaged over the class pair suggests that this is related to the evidences used in our analysis being a lot less likely compared to the classes"
  - [corpus] No direct corpus evidence on under-updating mechanisms.
- Break condition: If under-updating is an artifact of how evidence texts were generated (favoring improbable evidence), the pattern may not generalize.

## Foundational Learning

- Concept: Bayes' theorem and belief updating
  - Why needed here: The entire metric assumes understanding how optimal belief updating works—prior × likelihood normalized by evidence across all hypotheses.
  - Quick check question: Given P(c₁|x) / P(c₂|x) = [P(x|c₁)P(c₁)] / [P(x|c₂)P(c₂)], what does multiplying the prior ratio by the likelihood ratio yield?

- Concept: Log-odds and log-likelihood ratios
  - Why needed here: BCC operates in log space to convert multiplicative updates into additive comparisons and avoid numerical instability.
  - Quick check question: Why use log(P(c₁|x)/P(c₂|x)) - log(P(c₁)/P(c₂)) rather than the raw probability ratios?

- Concept: Correlation vs. error metrics
  - Why needed here: The paper argues correlation-based BCC is superior to error-based BCE because error metrics can be gamed by high-entropy outputs.
  - Quick check question: If a model outputs uniform probabilities for everything, would it score well on BCE? On BCC?

## Architecture Onboarding

- Component map: Dataset generator (GPT-4o) -> Evaluator (target model) -> BCC calculator
- Critical path: Dataset generation → prior/likelihood/posterior elicitation (separate model instances for each) → compute ratios → correlate. Temperature must be set to 1 (training temperature).
- Design tradeoffs:
  - Correlation-based BCC vs. error-based BCE: BCC is robust to entropy manipulation; BCE can be trivially minimized by uniform outputs
  - Cumulative token probability vs. alternative belief proxies: Token probability is tractable but may not reflect action-relevant beliefs
  - Pre-trained-only vs. fine-tuned models: This study limits scope to pre-trained; RLHF effects unknown
- Failure signatures:
  - Models with high BCE but low BCC may be exploiting entropy rather than exhibiting genuine coherence
  - Category-specific BCC variance (Figure 6) suggests some domains elicit more coherent behavior than others
  - GPT-2 family shows anomalous non-monotonic scaling (Large > XL for BCC)
- First 3 experiments:
  1. Replicate BCC computation on a single category to validate pipeline—verify prior, likelihood, and posterior extraction matches the paper's format.
  2. Test temperature sensitivity: run BCC at T=0.5, T=1.0, T=1.5 to confirm the paper's claim that BCC is temperature-invariant (per Appendix Figure 7).
  3. Extend to a model >14B parameters (e.g., Llama 70B) to test whether the log-linear scaling relationship continues or plateaus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does supervised fine-tuning (SFT) or reinforcement learning (RL) alter the Bayesian Coherence Coefficient (BCC) compared to pre-training alone?
- Basis in paper: [explicit] "Future research should repeat our results for... fine-tuned LLMs. Both instruction-tuned and reinforcement-learning-tuned models are interesting candidates for study."
- Why unresolved: The study strictly evaluated pre-trained-only models up to 14 billion parameters, leaving the effects of alignment training unknown.
- What evidence would resolve it: Calculating BCC across model checkpoints before and after SFT/RL tuning.

### Open Question 2
- Question: Why do observed updates consistently show an "under-updating" gradient (less than 1) relative to expected updates?
- Basis in paper: [explicit] "It is also unclear why all models tested seem to under-update... we leave... the observed under-updating phenomena to future research."
- Why unresolved: While the authors identify a correlation with negative evidence log likelihood, the precise mechanism causing the conservative update magnitude remains undetermined.
- What evidence would resolve it: Analysis correlating update gradients with evidence probability distributions and model calibration.

### Open Question 3
- Question: Are cumulative token probabilities a valid proxy for an LLM's actual "credence" or belief state regarding a proposition?
- Basis in paper: [explicit] "It is unclear whether this is an accurate proxy for action-relevant belief states."
- Why unresolved: The methodology assumes this equivalence without verifying if token probability reflects the model's internal reasoning or behavioral intent.
- What evidence would resolve it: Comparative studies correlating BCC results with alternative belief elicitation techniques or downstream behavioral consistency.

## Limitations

- The study only tested models up to 14B parameters, leaving uncertainty about whether scaling relationships continue to frontier models
- All models tested were pre-trained only, with unknown effects from instruction-tuning or RLHF on Bayesian coherence
- The under-updating phenomenon could be an artifact of dataset construction that favors improbable evidence

## Confidence

- **Low** confidence on whether scaling generalizes beyond 14B parameters - log-linear correlation compelling but limited to tested range
- **Medium** confidence on under-updating interpretation - could be dataset artifact rather than systematic bias
- **High** confidence on BCC as a superior metric - convincingly demonstrates correlation-based approach avoids entropy gaming

## Next Checks

1. Test scaling beyond 14B parameters: Evaluate BCC on frontier models (70B+ parameters) to determine whether the log-linear relationship continues, plateaus, or reverses.
2. Validate dataset construction: Generate a subset of evidences with explicit likelihood constraints and verify that the observed under-updating persists.
3. Test temperature invariance rigorously: Run the full BCC computation across a range of temperatures (T=0.1 to T=2.0) on multiple models to confirm that the metric remains stable.