---
ver: rpa2
title: Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps
arxiv_id: '2502.14829'
source_url: https://arxiv.org/abs/2502.14829
tags:
- reasoning
- unlearning
- steps
- faithfulness
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for measuring the faithfulness
  of language model chain-of-thought reasoning by intervening on model parameters
  rather than just context. The proposed Faithfulness by Unlearning Reasoning steps
  (FUR) method uses preference optimization-based unlearning to remove information
  from reasoning steps and measures how this affects model predictions.
---

# Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps

## Quick Facts
- arXiv ID: 2502.14829
- Source URL: https://arxiv.org/abs/2502.14829
- Reference count: 40
- Introduces parameter-based intervention framework for measuring CoT faithfulness

## Executive Summary
This paper presents a novel framework for measuring chain-of-thought (CoT) faithfulness by intervening on language model parameters rather than just context. The proposed Faithfulness by Unlearning Reasoning steps (FUR) method uses preference optimization-based unlearning to remove information from reasoning steps and measures how this affects model predictions. Experiments demonstrate that unlearning key reasoning steps frequently changes model predictions, indicating when CoTs are parametrically faithful. The approach achieves high efficacy in unlearning while maintaining model capabilities, though a human study reveals tension between parametric faithfulness and human plausibility assessments.

## Method Summary
The framework introduces a new approach to measuring CoT faithfulness by intervening on model parameters through preference optimization-based unlearning. Rather than just analyzing context or attention patterns, FUR removes information from specific reasoning steps by unlearning their influence on model parameters. The method measures faithfulness by observing how unlearning affects model predictions - if removing a reasoning step changes the prediction, that step is considered parametrically faithful. The approach is tested across four language models and five multi-choice question answering datasets, with additional analysis of verbalization patterns and human plausibility assessments.

## Key Results
- Unlearning key reasoning steps frequently changes model predictions, indicating parametric faithfulness
- FUR achieves high unlearning efficacy while maintaining model specificity and general capabilities
- Human study shows steps identified as important by FUR are not considered plausible by humans
- Unlearning causes models to verbalize reasoning supporting different answers

## Why This Works (Mechanism)
The method works by exploiting the causal relationship between reasoning steps and model predictions through parameter-level intervention. By using preference optimization to unlearn specific reasoning steps, the framework can isolate the contribution of each step to the final prediction. This intervention-based approach reveals whether reasoning steps are genuinely influencing predictions (faithful) or merely appearing relevant while being confabulated. The preference optimization technique allows targeted removal of information without broadly degrading model capabilities, enabling precise measurement of individual step contributions.

## Foundational Learning
- Preference optimization: Why needed - to enable targeted parameter-level unlearning without catastrophic forgetting; Quick check - verify unlearning only affects targeted steps while preserving general reasoning
- Chain-of-thought faithfulness: Why needed - to distinguish genuine reasoning from confabulation in language model outputs; Quick check - compare parameter intervention effects with context-only interventions
- Multi-choice question answering: Why needed - provides clear ground truth for evaluating reasoning faithfulness; Quick check - ensure datasets have unambiguous correct answers for reliable measurement

## Architecture Onboarding
- Component map: Reasoning steps -> Preference optimization unlearning -> Parameter intervention -> Prediction change measurement
- Critical path: Identify important steps → Apply unlearning → Measure prediction changes → Assess faithfulness
- Design tradeoffs: Parameter intervention vs. context manipulation; targeted vs. broad unlearning; parametric faithfulness vs. human plausibility
- Failure signatures: Unlearning causes general capability degradation; prediction changes don't align with step importance; human and parametric measures diverge significantly
- First experiments: 1) Ablation study varying unlearning strength across different step types; 2) Cross-model comparison of unlearning effects; 3) Human validation study comparing parametric vs. human plausibility ratings

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Preference optimization unlearning may introduce confounding effects beyond targeted information removal
- Tension exists between parametric faithfulness measures and human plausibility assessments
- Artificial nature of unlearning intervention may not fully capture natural reasoning processes
- Limited analysis of potential side effects on model reasoning patterns

## Confidence
- FUR efficacy claims: Medium confidence due to artificial unlearning intervention
- Parametric vs. human plausibility distinction: Medium confidence requiring clearer interpretation
- Unlearning artifact concerns: Medium confidence needing more thorough investigation
- Theoretical framework validity: High confidence in conceptual contribution

## Next Checks
1. Conduct ablation studies varying the strength and scope of preference optimization to establish bounds on unlearning effectiveness and potential interference with general reasoning capabilities.

2. Design controlled experiments comparing FUR-identified important steps against human-annotated reasoning steps across multiple annotators to better understand the parametric vs. human plausibility gap.

3. Test the framework on reasoning tasks where ground truth faithfulness can be established through alternative means (e.g., symbolic reasoning tasks) to validate that FUR correctly identifies truly faithful vs. confabulated reasoning.