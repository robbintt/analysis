---
ver: rpa2
title: 'SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding'
arxiv_id: '2506.07600'
source_url: https://arxiv.org/abs/2506.07600
tags:
- scenerag
- scene
- video
- segmentation
- caching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SceneRAG addresses the challenge of understanding long-form videos
  by introducing human-inspired scene segmentation and multimodal knowledge graphs.
  It segments videos into coherent scenes using transcripts and visual cues, refines
  boundaries with heuristics, and constructs a graph of scene-level entities and relations.
---

# SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding

## Quick Facts
- **arXiv ID**: 2506.07600
- **Source URL**: https://arxiv.org/abs/2506.07600
- **Reference count**: 40
- **Primary result**: Achieves 70.8% win rate vs. graph-based baselines on LongerVideos benchmark

## Executive Summary
SceneRAG introduces a human-inspired scene segmentation approach to long-form video understanding, replacing fixed-length chunking with narrative-consistent scene boundaries. The system constructs a multimodal knowledge graph fusing visual and textual entities, enabling multi-hop retrieval and generation. Evaluated on the LongerVideos benchmark, SceneRAG outperforms state-of-the-art baselines including VideoRAG and large vision-language models, demonstrating superior long-range reasoning and contextual understanding across multiple domains.

## Method Summary
SceneRAG segments videos into coherent scenes using LLM-based transcript analysis and visual cues, then refines boundaries with heuristics. For each scene, it extracts entities and relations from both visual descriptions (generated by VLM) and transcripts to build a knowledge graph. Given a query, the system retrieves relevant scenes via multi-hop reasoning under token constraints, generates query-focused captions, and produces answers using an LLM. The framework uses ASR (Distil-Whisper) for transcripts, GPT-4o-mini for segmentation, MiniCPM-V for visual captioning, and ImageBind for embeddings.

## Key Results
- Achieves 72.5% win rate on generation tasks against competitive baselines
- Outperforms VideoRAG and other graph-based methods with 70.8% overall win rate
- Demonstrates superior long-range reasoning and contextual understanding compared to large vision-language models

## Why This Works (Mechanism)

### Mechanism 1: Human-Inspired Scene Segmentation
SceneRAG uses LLMs to identify narrative-consistent scene boundaries from transcripts, then applies heuristics to refine them. This aligns retrieval units with human-perceived scenes rather than arbitrary time windows, improving semantic coherence within retrieval units.

### Mechanism 2: Multimodal Knowledge Graph Construction
The system fuses visual and textual information to build a knowledge graph where nodes represent scenes, entities, and events. This graph structure enables multi-hop retrieval and long-range reasoning by modeling temporal and semantic relationships between entities.

### Mechanism 3: Token-Budgeted, Query-Focused Retrieval and Generation
SceneRAG retrieves relevant scenes under token constraints and generates query-focused visual captions. This approach balances context richness with computational efficiency while improving signal-to-noise for final LLM generation.

## Foundational Learning

- **Concept: Graph-based Retrieval (GraphRAG)**
  - **Why needed here**: SceneRAG's core innovation is building a knowledge graph over scenes, requiring understanding of how nodes, edges, and community detection enable multi-hop reasoning
  - **Quick check question**: How does retrieving information by traversing edges in a knowledge graph differ from standard vector similarity search? (Answer: It allows for multi-hop reasoning and finding related entities that are not semantically similar to the query itself)

- **Concept: Multimodal Fusion**
  - **Why needed here**: The system fuses information from video frames and audio transcripts, requiring understanding of challenges like temporal alignment and different embedding spaces
  - **Quick check question**: What are two common methods for fusing visual and textual information? (Answer: Early fusion: combining raw features; Late fusion: combining model outputs or scores)

- **Concept: LLM-based Evaluation (e.g., LLM-as-a-Judge)**
  - **Why needed here**: The paper's primary results are "win rates" judged by GPT-4 variants, requiring understanding of strengths and limitations of this evaluation method
  - **Quick check question**: What is a potential bias when using an LLM to judge the output of another LLM? (Answer: Position bias - preferring the first answer presented; Length bias - preferring longer, more verbose answers)

## Architecture Onboarding

- **Component map**: Video → 5-min Chunks → ASR (Distil-Whisper) → Timestamped Transcript → LLM Segmentation → Heuristics → Scene Boundaries → VLM Captioning → Entity/Relation Extraction → Knowledge Graph → Scene Embeddings → Vector DB + Graph DB → Query Retrieval → LLM Generation

- **Critical path**: Scene Segmentation is the most critical and fragile path. Errors here cascade into the graph builder, creating incoherent "scenes" that degrade all downstream retrieval and generation.

- **Design tradeoffs**:
  - LLM vs. Heuristics for Segmentation: Uses prompt-based LLM for flexibility but adds latency and cost, with brittleness to prompt formatting
  - Dual-Path Extraction: Extracts entities from visual and textual paths separately before fusing, adding computational overhead but reducing modality bias
  - Token Budget Assumption: Uses 2400 tokens as a heuristic for balancing context and cost, not a theoretically derived limit

- **Failure signatures**:
  - Fragmented Narrative: Many very short scenes (<15 seconds) suggest LLM segmentation prompt needs tuning or silence threshold is too low
  - Hallucinated Entities: Generated answers contain information not in video, pointing to noisy entity extraction from VLM or LLM
  - Empty Retrieval: System fails to find relevant scenes for queries, indicating poor multimodal embeddings or sparse knowledge graph

- **First 3 experiments**:
  1. Sanity Check Segmentation: Manually inspect scene boundaries on 5-minute video sample with ground truth, comparing LLM-only vs. LLM+Heuristic boundaries against human judgment
  2. Component Ablation: Disable visual entity extraction (only use transcript) and measure performance drop on QA task to quantify visual modality contribution
  3. Retrieve vs. Generate Stress Test: Provide query requiring information from two temporally distant, semantically unrelated scenes to evaluate multi-hop graph retrieval success

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on LLM-as-a-judge methodology without human verification of factual accuracy
- Knowledge graph quality and multi-hop reasoning capabilities are not explicitly tested in isolation
- Performance generalization beyond the LongerVideos benchmark (134 hours) remains unknown

## Confidence
- **High Confidence**: Segmentation method works for LongerVideos dataset and improves over fixed-length chunking
- **Medium Confidence**: Multimodal knowledge graph contributes to performance gains over text-only methods
- **Low Confidence**: "Multi-hop reasoning" claim is not validated; 72.5% win rate relies entirely on automated scoring

## Next Checks
1. **Factual Accuracy Audit**: Manually verify 20 random answers from top-performing condition for hallucinated information not present in source video
2. **Multi-Hop Retrieval Test**: Design 10 queries requiring information from two non-adjacent scenes to test actual multi-hop reasoning capability
3. **Cross-Dataset Generalization**: Evaluate SceneRAG on different video dataset (HowTo100M, TVR, or diverse manually curated set) to test method generalization beyond LongerVideos