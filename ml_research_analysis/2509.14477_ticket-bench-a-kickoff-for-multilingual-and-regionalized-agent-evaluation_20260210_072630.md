---
ver: rpa2
title: 'Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation'
arxiv_id: '2509.14477'
source_url: https://arxiv.org/abs/2509.14477
tags:
- game
- team
- languages
- across
- ticket
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ticket-Bench is a multilingual benchmark for evaluating large language
  models in task-oriented function calling, using soccer ticket purchasing as a domain.
  It includes over 1000 scenarios in six languages (Portuguese, English, Spanish,
  German, Italian, and French) with localized entities, schedules, and user profiles.
---

# Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation

## Quick Facts
- arXiv ID: 2509.14477
- Source URL: https://arxiv.org/abs/2509.14477
- Reference count: 40
- Ticket-Bench evaluates large language models in multilingual task-oriented function calling using soccer ticket purchasing as a domain, revealing systematic cross-lingual performance disparities.

## Executive Summary
Ticket-Bench introduces a multilingual benchmark for evaluating large language models on task-oriented function calling, using soccer ticket purchasing as a domain. The benchmark includes over 1000 scenarios across six languages (Portuguese, English, Spanish, German, Italian, and French) with localized entities, schedules, and user profiles. Models interact with a fixed set of functions to book tickets under various constraints, and success is measured by whether the final environment state matches the expected outcome across multiple runs. Results show that reasoning models like GPT-5 and Qwen3-235B perform best, with pass^3 accuracy up to 0.91, but notable cross-lingual disparities persist, indicating uneven multilingual performance.

## Method Summary
Ticket-Bench creates localized environments for six languages using real league rosters and schedules, then generates 1,020 queries from 17 templates with varying constraint combinations. Models are provided with translated function schemas and must execute the correct sequence of function calls to satisfy user requests. Evaluation is LLM-free and programmatic, comparing the final environment state to ground truth. A pass^3 metric requiring success across three independent runs per query measures consistency and robustness beyond single-attempt accuracy.

## Key Results
- Reasoning models (GPT-5, Qwen3-235B) achieve pass^3 accuracy up to 0.91, significantly outperforming non-reasoning models
- Cross-lingual disparities persist even among top models, with no language being uniformly "easy" or "hard"
- Performance gaps reflect differences in model training data distributions rather than task difficulty
- Pass^3 consistency reveals stochastic behavior masked by single-run accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Optimized Inference Improves Multilingual Function-Calling Accuracy
Reasoning models allocate additional compute per query to decompose complex constraint satisfaction tasks before generating function calls, reducing errors in constraint interpretation across languages. Performance gains stem from architectural inference-time reasoning rather than merely larger training datasets.

### Mechanism 2: Localized Entity Instantiation Creates Controlled Cross-Lingual Comparability
Replacing naively translated entities with region-authentic alternatives forces models to reason over culturally grounded knowledge distributions, exposing training data biases that monolingual benchmarks miss.

### Mechanism 3: Pass^k Consistency Metric Exposes Brittleness That Single-Run Accuracy Masks
Requiring models to succeed across multiple independent attempts better predicts deployment reliability by penalizing stochastic or temperature-sensitive behavior that single-run metrics cannot detect.

## Foundational Learning

- **Function-calling as structured tool invocation**: Understanding how LLMs translate natural language requests into correct API calls with proper arguments is prerequisite knowledge. Quick check: Can you explain how an LLM determines which function to call and what arguments to pass given a user request like "Book the cheapest available ticket for next week"?

- **Cross-lingual transfer and training data bias**: The paper's central finding is that model families exhibit systematic language-specific performance asymmetries tied to training data distributions. Quick check: Why might a model trained predominantly on English data perform worse on Portuguese queries even when function schemas are translated?

- **Constraint satisfaction in multi-step reasoning**: Queries combine up to five constraint types requiring models to filter game listings, join with leaderboard data, and apply temporal filters. Quick check: For the query "Buy the cheapest ticket I can afford for a midweek game in Milan against a top-3 team," what sequence of function calls would be required?

## Architecture Onboarding

- **Component map**: Environment layer (localized league data) -> Function interface (5 translated functions) -> Query generation (17 templates × 10 queries) -> Evaluation engine (state comparison + pass^3 scoring)

- **Critical path**: Load localized environment → Present translated function schemas → Model generates function calls → Execute calls against simulation → Compare final state to ground truth → Repeat 3× per query for pass^3 scoring

- **Design tradeoffs**: Synthetic data enables controlled comparison but may not reflect real-world distributions; programmatic evaluation is unambiguous but cannot reward near-miss solutions; limited runs may provide unreliable variance estimates

- **Failure signatures**: Calling Buy Game Ticket without checking user balance; failing to handle "no valid solution" cases; high single-run accuracy but low pass^3 indicating temperature-sensitive behavior; family-specific language drops >10 points

- **First 3 experiments**: 1) Baseline comparison: non-reasoning vs. reasoning model on English-only subset 2) Language ablation: plot pass^3 vs. parameter count for each language per model family 3) Constraint complexity analysis: compare accuracy degradation by number of constraints across languages

## Open Questions the Paper Calls Out

### Open Question 1
Why does specialized function-calling fine-tuning degrade cross-lingual generalization compared to base instruction-tuned models? The paper identifies performance drops in xLAM models but doesn't isolate whether this stems from data scarcity or catastrophic forgetting of multilingual reasoning capabilities.

### Open Question 2
What specific training data distributions drive the family-specific language asymmetries? While the paper establishes correlation between model families and language performance, it lacks access to proprietary training corpora to confirm causal links.

### Open Question 3
Do cross-lingual reasoning failures generalize to other complex, culturally localized domains? The current single-domain focus leaves unclear whether failures stem from domain-specific knowledge or fundamental multilingual planning deficits.

## Limitations

- Synthetic schedules and leaderboards may not generalize to real-world data distributions
- LLM-free evaluation cannot distinguish semantically equivalent solutions from incorrect ones
- Only M=3 runs per query may provide unreliable variance estimates for pass^3 scores
- Critical implementation details are not fully specified, requiring significant reverse-engineering

## Confidence

**High Confidence**: Reasoning models outperform non-reasoning models of similar scale on multilingual function-calling tasks is well-supported by direct experimental evidence across multiple model families and languages.

**Medium Confidence**: Localized entity instantiation reveals training data biases is plausible given systematic family-specific disparities, but could also reflect cultural familiarity or language-specific complexity.

**Low Confidence**: Pass^3 consistency directly predicts deployment reliability lacks external validation—no evidence links pass^3 scores to real-world performance metrics.

## Next Checks

1. **Real Data Validation**: Replace synthetic schedules and leaderboards with actual historical soccer data from the same leagues and re-evaluate top-performing models to determine if performance patterns persist on real-world distributions.

2. **Semantic Tolerance Analysis**: Implement a more permissive evaluation that accepts near-miss solutions (e.g., slightly more expensive tickets, different but equivalent game selections) and measure how this affects relative model rankings and cross-lingual patterns.

3. **Temporal Stability Test**: Run the same models on the benchmark at different time intervals (e.g., 1 month apart) to measure score variance and determine whether pass^3 truly captures stable capabilities or is influenced by model version drift and training updates.