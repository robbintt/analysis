---
ver: rpa2
title: 'DiFR: Inference Verification Despite Nondeterminism'
arxiv_id: '2511.20621'
source_url: https://arxiv.org/abs/2511.20621
tags:
- h200
- incorrect
- inference
- qwen3-30b-a3b
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiFR (Divergence-From-Reference), a method
  for verifying LLM inference outputs despite nondeterminism. The core idea is to
  synchronize random seeds between provider and verifier, enabling near-deterministic
  generation where token-level outputs themselves serve as auditable evidence of correct
  inference at zero provider overhead.
---

# DiFR: Inference Verification Despite Nondeterminism

## Quick Facts
- arXiv ID: 2511.20621
- Source URL: https://arxiv.org/abs/2511.20621
- Reference count: 40
- Primary result: Method for verifying LLM inference outputs despite nondeterminism with 4-bit quantization detection at AUC > 0.999

## Executive Summary
DiFR (Divergence-From-Reference) enables verification of LLM inference outputs despite inherent nondeterminism by synchronizing random seeds between provider and verifier, making token-level outputs themselves auditable evidence. The framework introduces two complementary methods: Token-DiFR compares claimed tokens against reference predictions using shared randomness, while Activation-DiFR uses random orthogonal projections to compress activations into compact fingerprints for sample-efficient forward-pass verification. The approach achieves near-deterministic verification with zero provider overhead for Token-DiFR and 25-75% communication reduction versus existing methods for Activation-DiFR.

## Method Summary
DiFR provides two verification methods for LLM inference: Token-DiFR synchronizes PRNG seeds between provider and verifier to constrain outputs to a near-deterministic set, where token-level outputs serve as auditable evidence through post-Gumbel logit margin comparison; Activation-DiFR compresses hidden states via random orthogonal projections to preserve distance information for detecting misconfigurations with far fewer tokens. Both methods aggregate per-token scores over batches using calibrated thresholds trained on pooled honest baselines, enabling binary classification between honest and misconfigured deployments while maintaining sample efficiency.

## Key Results
- Token-DiFR detects 4-bit quantization with AUC > 0.999 within 300 tokens using synchronized seeds
- Activation-DiFR detects same quantization with AUC > 0.999 using just 2 tokens while reducing communication by 25-75%
- Temperature-zero spot-checking enables practical deployment through third-party provider verification

## Why This Works (Mechanism)

### Mechanism 1: Token-DiFR (Seed-Synchronized Gumbel-Max Verification)
Synchronizing PRNG seeds between provider and verifier constrains valid outputs to a near-deterministic set, making token sequences themselves auditable evidence. With shared seed σ, Gumbel-Max sampling produces deterministic noise vectors, so any token disagreement arises solely from numerical differences in computed logits. The verification score is the post-Gumbel logit margin: δ_logit = (ℓ[v_token] + T·g[v_token]) - (ℓ[c_token] + T·g[c_token]). Break condition: Provider refuses seed synchronization, uses different sampling algorithm, or adversarially modifies both model output and sampling procedure.

### Mechanism 2: Activation-DiFR (Random Orthogonal Projection Fingerprinting)
Compressing hidden states via random orthogonal projections preserves sufficient distance information to detect forward-pass misconfigurations with far fewer tokens than token-only methods. Both parties generate the same random orthogonal projection matrix P from shared seed, compute f = P·a, and compare ℓ₂ distances. The Johnson–Lindenstrauss lemma guarantees approximate distance preservation. Break condition: Projection dimension k is too small, or model's hidden states are inherently noisy across implementations.

### Mechanism 3: Statistical Aggregation with Calibrated Thresholds
Aggregating per-token scores over batches (via mean, clipped mean, or tail-focused pooling) enables binary classification between honest and misconfigured deployments when thresholds are calibrated on pooled honest baselines. Collect scores from multiple honest configurations, fit 1D classifier to distinguish these from misconfigured scores, and flag batches exceeding threshold. Break condition: Honest baseline is too broad, causing subtle misconfigurations to fall within null distribution.

## Foundational Learning

- Concept: **Gumbel-Max Sampling**
  - Why needed here: This is the sampling algorithm that Token-DiFR assumes; understanding it is essential to compute the post-Gumbel logit margin.
  - Quick check question: Given logits [1.0, 2.0, 0.5], temperature 1.0, and fixed Gumbel noise vector [0.1, -0.2, 0.3], which token would be sampled?

- Concept: **Floating-Point Non-Associativity**
  - Why needed here: Explains why exact reproducibility fails—different GPU kernels or batch sizes change reduction order, producing different logits even for identical model weights.
  - Quick check question: Why does (a + b) + c ≠ a + (b + c) in floating-point arithmetic? What implication does this have for tensor-parallel inference?

- Concept: **Johnson–Lindenstrauss Lemma**
  - Why needed here: Provides theoretical guarantee that Activation-DiFR's random projections approximately preserve distances, enabling compression with minimal information loss.
  - Quick check question: If you project 4096-dimensional vectors to 32 dimensions via random orthogonal projection, what does JL guarantee about pairwise distances?

## Architecture Onboarding

- Component map: Provider inference server -> Verifier (trusted model weights) -> Calibration dataset -> Threshold selector
- Critical path: 1) Provider generates output sequence with known seed σ or logs activation fingerprints 2) Verifier receives output + metadata, runs single prefill pass over prompt + output to recompute logits/activations 3) Verifier computes per-token scores, aggregates (mean with clipping), compares to calibrated threshold 4) Flag batch if aggregated score > τ; otherwise accept
- Design tradeoffs: Token-DiFR has zero communication overhead but requires seed synchronization and more tokens (~300–5000); Activation-DiFR is sample-efficient (2–4 tokens for 4-bit quantization), no seed sync needed, but requires ~0.3–7 bytes/token communication and provider-side instrumentation
- Failure signatures: High false positives from calibration baseline too narrow or implementation differences; low true positives on subtle bugs with mean aggregation; cross-entropy attack vulnerability
- First 3 experiments: 1) Temperature-zero spot-check on third-party provider with greedy outputs 2) Quantization detection benchmark with 300-token outputs and 4-bit/FP8 models 3) Activation-DiFR vs TOPLOC comparison on 32-token batches

## Open Questions the Paper Calls Out

- **Question**: Can DiFR verification methods be extended to speculative decoding while preserving efficiency advantages?
  - Basis: Paper has not evaluated approach on speculative decoding; vLLM 0.10.0 doesn't support it yet
  - Status: Authors sketch verification strategy in Appendix F but don't implement or evaluate

- **Question**: What causes unusually high behavioral variability in Qwen3-30B-A3B across different inference implementations?
  - Basis: Unclear whether differences driven by mixture-of-experts architecture or implementation details
  - Status: Exact-match rate varies dramatically (99.9% for matched H200/vLLM vs ~95% for HuggingFace)

- **Question**: How can verification schemes defend against selective cheating where providers serve high-quality inference only for verification queries?
  - Basis: Temperature-zero spot-checking is vulnerable to selective cheating
  - Status: Paper notes attack vector but doesn't propose countermeasures

## Limitations
- Token-DiFR determinism assumption depends on provider faithfully using same sampling algorithm and not introducing additional nondeterminism
- Calibration baseline construction doesn't fully address how to handle providers using different implementations (vLLM vs HuggingFace)
- Activation-DiFR compression limits effectiveness depends on chosen dimensionality k and specific model activation space characteristics

## Confidence
- **High Confidence**: Token-DiFR's core mechanism of using synchronized seeds; Activation-DiFR's ability to detect 4-bit quantization with high AUC; overall framework's effectiveness in distinguishing honest vs misconfigured deployments
- **Medium Confidence**: Specific hyperparameters (Δ_max clipping, winsorization percentiles) not always specified; communication overhead estimates assume specific serialization schemes; calibration approach for σ in likelihood-based variants not fully specified
- **Low Confidence**: Performance in adversarial settings with deliberate obfuscation; generalization to models outside tested family; behavior with extreme misconfigurations causing complete generation failures

## Next Checks
1. **Cross-implementation baseline validation**: Generate Token-DiFR scores comparing vLLM and HuggingFace implementations of same model on identical inputs; measure variance distribution to determine if excluding mismatched implementations from pooled baseline is necessary
2. **Adversarial temperature tuning test**: Systematically vary temperature and sampling parameters on provider side to minimize Token-DiFR scores while maintaining semantic similarity; determine if adversarial configurations evade detection
3. **Extreme misconfiguration stress test**: Test Token-DiFR and Activation-DiFR against more extreme misconfigurations like completely wrong weight files, model architecture mismatches, or catastrophic decoding bugs; measure detection sensitivity and false positive rates at boundaries