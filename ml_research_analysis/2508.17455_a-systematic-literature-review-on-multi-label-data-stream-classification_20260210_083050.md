---
ver: rpa2
title: A Systematic Literature Review on Multi-label Data Stream Classification
arxiv_id: '2508.17455'
source_url: https://arxiv.org/abs/2508.17455
tags:
- data
- multi-label
- classification
- labels
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review provides a comprehensive analysis
  of multi-label data stream classification methods published between 2016 and 2024.
  The review identifies 58 primary studies and categorizes them according to their
  approach (algorithm adaptation, problem transformation, or ensemble), evaluation
  strategies, and how they handle concept drift and concept evolution.
---

# A Systematic Literature Review on Multi-label Data Stream Classification

## Quick Facts
- **arXiv ID:** 2508.17455
- **Source URL:** https://arxiv.org/abs/2508.17455
- **Reference count:** 40
- **Primary result:** Identifies 58 primary studies on multi-label data stream classification, revealing gaps in concept evolution handling, label latency, and evaluation metric standardization.

## Executive Summary
This systematic literature review comprehensively analyzes multi-label data stream classification methods published between 2016 and 2024. The review categorizes approaches into algorithm adaptation, problem transformation, and ensemble methods, while examining how they handle concept drift and evolution. Key findings reveal that while concept drift detection has received significant attention, concept evolution remains under-explored. The review also highlights the limited consideration of label latency, with only six studies addressing infinite label latency. Evaluation metrics predominantly focus on F1-score, accuracy, and hamming loss, with example-based metrics being more commonly used in streaming contexts.

## Method Summary
The review employed systematic literature review methodology using PICOC criteria to search five digital libraries (ACM, IEEE Xplore, Science Direct, Springer Link, Scopus). From 192 initial studies, 58 primary studies were identified through filtering and quality assessment. Quality scores were calculated using a citation-based formula and a 4-point checklist. The methodology included snowballing to identify additional relevant studies, though the exact depth and rejection rules for this step were not fully specified.

## Key Results
- Multi-label data stream classification methods are categorized into algorithm adaptation, problem transformation, and ensemble approaches
- Concept drift detection has received significant attention while concept evolution remains under-explored
- Only six studies explicitly address infinite label latency scenarios
- F1-score, accuracy, and hamming loss dominate evaluation metrics, with example-based metrics preferred in streaming contexts
- Limited availability of appropriate multi-label streaming datasets hinders comprehensive evaluation

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Diversity for Drift Robustness
- **Claim:** Maintaining an ensemble of classifiers with different temporal views allows the system to retain relevant historical knowledge while adapting to new concepts
- **Mechanism:** Uses weighting strategy (e.g., Prequential accuracy or McNemar tests) to prioritize recent classifiers while keeping older ones available for recurring concepts
- **Core assumption:** Concept drift is gradual or recurrent, and hypothesis space contains models capable of fitting new distribution without discarding all prior knowledge
- **Evidence anchors:** Section 7.1 describes AESAKNNS and ROSE using ADWIN detectors to trigger new expert training when drift is identified

### Mechanism 2: Label Space Transformation for Dependency Capture
- **Claim:** Transforming multi-label problem into multi-class (Label Powerset) captures correlations between labels implicitly
- **Mechanism:** Maps combination of active labels to unique class ID (e.g., {y₁, y₃} → Class_A) and learns decision boundaries for these combinations
- **Core assumption:** Frequently occurring label combinations are significantly lower than theoretical power set |P(L)|
- **Evidence anchors:** Section 2.1.1 defines Label Powerset and its advantage in preserving relationships between labels

### Mechanism 3: Outlier Detection for Concept Evolution
- **Claim:** New labels can be detected by identifying instances outside decision boundaries of all known classes
- **Mechanism:** Maintains decision model (micro-clusters or tree forest) and flags instances as "novel" if isolation score exceeds threshold
- **Core assumption:** Novel class instances form cohesive clusters in feature space and are distinguishable from noise
- **Evidence anchors:** Section 8 states concept evolution is "under-explored" and describes methods like MINAS-BR using distance thresholds for novel class detection

## Foundational Learning

- **Concept: Prequential (Interleaved Test-Then-Train) Evaluation**
  - **Why needed here:** In streaming contexts, you cannot hold out static test set because distribution changes over time
  - **Quick check question:** If you use static holdout set from time t₀ to evaluate model at time t₁₀₀, what two phenomena might invalidate your results? (Answer: Concept Drift and Concept Evolution)

- **Concept: Label Latency (Immediate vs. Infinite)**
  - **Why needed here:** Review identifies this as critical gap; most methods assume immediate labels (d=0), which is unrealistic
  - **Quick check question:** Why does "Infinite Label Latency" prevent use of standard supervised drift detectors like ADWIN? (Answer: ADWIN monitors error rates based on ground truth; without labels, error cannot be calculated directly)

- **Concept: Example-based vs. Label-based Metrics**
  - **Why needed here:** Multi-label performance is nuanced; "Accuracy" is often too strict (Subset Accuracy)
  - **Quick check question:** Model predicts 3 out of 5 correct labels for instance plus 2 false positives. Is Hamming Loss or Subset Accuracy more sensitive to false positives? (Answer: Hamming Loss penalizes every incorrect label prediction)

## Architecture Onboarding

- **Component map:** Stream Source -> Preprocessor -> Buffer/Window -> Core Classifier -> Novelty/Drift Module -> Evaluator
- **Critical path:** Selection of Memory Strategy (Fixed Window vs Fading Factor vs Self-Adjusting Memory) dictates trade-off between stability and plasticity
- **Design tradeoffs:**
  - Binary Relevance (BR) vs Classifier Chains (CC): BR is O(L) and parallelizable but ignores label correlations; CC is sequential and captures correlations but increases latency
  - Active vs Passive Drift: Active detection allows precise resetting but requires labels; Passive adaptation is robust to latency but may react slowly
- **Failure signatures:**
  - Concept Drift: Sudden, sustained drop in Prequential accuracy; ADWIN warning signals
  - Concept Evolution: High "Unknown Rate" where instances rejected by all classifiers
  - High Latency Impact: Model weights diverge significantly from current concept
- **First 3 experiments:**
  1. Baseline Stability Test: Run standard algorithm (ML-kNN) on static multi-label stream using Prequential evaluation
  2. Drift Injection: Introduce synthetic stream with known abrupt drift; compare Passive (sliding window) vs Active (ADWIN + reset) methods
  3. Latency Stress Test: Artificially delay ground truth labels; compare standard learner vs semi-supervised/infinite-latency method

## Open Questions the Paper Calls Out

- **Open Question 1:** How can multi-label data stream classifiers be developed to handle finite, delayed ground truth label latency (0 < d < ∞)?
  - **Basis in paper:** Review identifies "limited consideration of label latency" as key gap, noting no studies found for feasible time delays
  - **Why unresolved:** Literature focuses almost exclusively on immediate labeling (d=0) or infinite latency; intermediate problem remains unaddressed
  - **Evidence:** New algorithm or adaptation demonstrating stable performance when labels arrive with fixed/variable delay d

- **Open Question 2:** What strategies can detect and manage recurring labels (concept evolution) in multi-label streams?
  - **Basis in paper:** Section 8 states neither of 8 evolution studies considered recurring class possibility
  - **Why unresolved:** Current methods detect novel labels but lack mechanisms to recognize reappearing labels
  - **Evidence:** Method identifying re-emerging labels and utilizing memory to recall previous model states

- **Open Question 3:** Can consensus be established on evaluation metrics and standardized prequential procedures for multi-label stream classification?
  - **Basis in paper:** Abstract identifies "lack of unity and consensus on evaluation metrics" as gap
  - **Why unresolved:** High variability in metric usage and lack of standardization make fair comparison difficult
  - **Evidence:** Comprehensive benchmarking study evaluating state-of-the-art methods on shared datasets using unified prequential metrics

## Limitations
- Conclusions drawn from relatively small set of primary studies (58), with only 6 addressing infinite label latency
- PICOC criteria may have excluded relevant work on related topics like semi-supervised streaming
- Systematic methodology relies on qualitative assessments introducing reviewer subjectivity
- Claims about "exponential increase in dimension" for Label Powerset methods lack quantitative bounds

## Confidence

- **High Confidence:** Claims about predominance of concept drift detection methods and dominance of Hamming Loss/F1-score as evaluation metrics
- **Medium Confidence:** Conclusions regarding under-exploration of concept evolution are reasonable but severity depends on actual frequency in real-world applications
- **Low Confidence:** Specific claims about "exponential increase in dimension" for Label Powerset methods lack comparative analysis

## Next Checks

1. **Dataset Analysis:** Validate claim about limited multi-label streaming datasets by compiling comprehensive list of publicly available datasets and testing suitability for streaming evaluation with concept drift

2. **Metric Robustness:** Re-analyze evaluation results from subset of primary studies using wider range of metrics (example-based precision/recall, ranking loss) to confirm F1-score and Hamming Loss are truly representative

3. **Latency Impact Study:** Implement controlled experiment systematically varying label latency in streaming scenario and measuring performance degradation across multiple algorithms to quantify real-world impact