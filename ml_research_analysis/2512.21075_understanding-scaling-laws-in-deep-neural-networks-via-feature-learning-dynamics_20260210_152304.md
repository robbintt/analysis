---
ver: rpa2
title: Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics
arxiv_id: '2512.21075'
source_url: https://arxiv.org/abs/2512.21075
tags:
- learning
- neural
- limit
- training
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a rigorous analysis of feature learning in deep
  ResNets by introducing Neural Feature Dynamics (NFD), a coupled forward-backward
  SDE system describing training in the joint infinite-width and infinite-depth limit.
  Unlike prior scaling-law theories that focus on fixed-kernel regimes, NFD captures
  the full co-evolution of features and gradients, revealing a depth-induced vanishing
  of forward-backward correlations that restores the gradient-independence assumption
  (GIA) during training.
---

# Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics

## Quick Facts
- arXiv ID: 2512.21075
- Source URL: https://arxiv.org/abs/2512.21075
- Authors: Zihan Yao; Ruoyu Wu; Tianxiang Gao
- Reference count: 40
- Key outcome: This work provides a rigorous analysis of feature learning in deep ResNets by introducing Neural Feature Dynamics (NFD), a coupled forward-backward SDE system describing training in the joint infinite-width and infinite-depth limit. Unlike prior scaling-law theories that focus on fixed-kernel regimes, NFD captures the full co-evolution of features and gradients, revealing a depth-induced vanishing of forward-backward correlations that restores the gradient-independence assumption (GIA) during training. This restoration enables tractable analysis of feature learning dynamics beyond kernel methods. Using NFD, the paper explains why depth-wise hyperparameter transfer fails for multi-layer residual blocks and proposes a simple depth-aware learning-rate correction that restores effective feature learning and improves empirical performance. Experiments on CIFAR-10 validate the theory, demonstrating improved training stability and accuracy in deeper ResNets.

## Executive Summary
This paper introduces Neural Feature Dynamics (NFD), a coupled forward-backward stochastic differential equation system that rigorously describes feature learning in deep residual networks under the joint infinite-width and infinite-depth limit. Unlike prior theories that focus on fixed-kernel regimes, NFD captures the co-evolution of forward features and backward gradients, revealing a depth-induced vanishing of their correlations that restores the Gradient Independence Assumption (GIA). This theoretical advance explains why standard hyperparameter transfer fails for multi-layer residual blocks and provides a depth-aware learning rate correction that restores effective feature learning. Experiments on CIFAR-10 validate the theory, demonstrating improved training stability and accuracy in deeper ResNets.

## Method Summary
The paper derives Neural Feature Dynamics (NFD) by analyzing deep ResNets in the joint infinite-width and infinite-depth limit under depth-Î¼P parameterization. The method involves proving convergence of finite networks to a coupled forward-backward McKean-Vlasov SDE system, analyzing the vanishing of forward-backward correlations to restore GIA, and using this framework to diagnose and correct depth-wise hyperparameter transfer failures in multi-layer residual blocks. The theoretical analysis is validated through experiments on CIFAR-10 using pre-activation ResNets with width-128 and varying depths from 3 to 128.

## Key Results
- Introduces NFD, a coupled forward-backward SDE system that rigorously describes feature learning in deep ResNets beyond kernel methods
- Reveals depth-induced vanishing of forward-backward correlations that restores GIA, enabling tractable analysis of feature learning dynamics
- Explains why depth-wise hyperparameter transfer fails for multi-layer residual blocks and proposes a depth-aware learning rate correction
- Validates the theory through CIFAR-10 experiments showing improved training stability and accuracy in deeper ResNets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under specific depth scaling ($1/\sqrt{L}$) and infinite limits, the correlation between forward features and backward gradients vanishes, effectively restoring the Gradient Independence Assumption (GIA) during training.
- Mechanism: The interaction term coupling forward and backward passes (due to weight reuse) scales as $\tau^2$ (where $\tau = T/L$). As depth $L \to \infty$, this term vanishes, allowing the system to be treated as driven by independent Brownian motions.
- Core assumption: The network operates in the joint infinite-width and infinite-depth limit under depth-$\mu$P.
- Evidence anchors:
  - [abstract] "reveals a depth-induced vanishing of forward-backward correlations... restoration enables tractable analysis"
  - [page 8] "stochastic calculus shows that this correlation term is of order $\tau^2$ and hence vanishes... GIA becomes asymptotically valid"
  - [corpus] Weak/General: Related corpus discusses scaling laws and NTK foundations but does not explicitly cover this specific mechanism of GIA restoration via depth.

### Mechanism 2
- Claim: In multi-layer residual blocks, standard depth parameterization causes feature learning to collapse in the first internal layer while the second layer remains active.
- Mechanism: The first internal layer controls representation $x_\ell$. Under standard depth-$\mu$P, the gradient update to $x_\ell$ scales as $O(1/\sqrt{L})$, causing learning to vanish as depth increases. The second layer controls residual-stream dynamics and does not suffer this suppression.
- Core assumption: The residual block has $>1$ internal layer (e.g., two-layer blocks common in Transformers).
- Evidence anchors:
  - [page 9] "feature learning in the first internal layer collapses... internal representations learning to collapse"
  - [page 37] "feature update contributed by $\Delta W_{\ell,1}$ decays as $1/\sqrt{L}$"
  - [corpus] Weak: Corpus mentions $\mu$P for ResNets but does not detail the structural separation of learning roles within blocks.

### Mechanism 3
- Claim: Pre-activation ResNets maintain stable feature propagation at large depth, whereas post-activation variants can diverge for common non-linearities like ReLU.
- Mechanism: Post-activation designs apply the non-linearity before the skip connection. For "positive dominant" activations (e.g., ReLU), this creates a recursive amplification of expectations, leading to divergence. Pre-activation avoids this by preserving the identity path.
- Core assumption: Activation functions satisfy the "positive dominance" condition (e.g., ReLU, GELU).
- Evidence anchors:
  - [page 4, Prop 1] "post-act ResNet can diverge even under $1/\sqrt{L}$ scaling... Pre-act design remains stable"
  - [page 16] Proof shows expectation of hidden states grows exponentially in $L$ for post-act.
  - [corpus] "On residual network depth" (Corpus ID 62230) discusses ensemble-like behavior but does not explicitly verify the pre/post-act divergence mechanism.

## Foundational Learning

- Concept: **Tensor Programs (TP) & Master Theorem**
  - Why needed here: The paper relies on the TP framework (specifically the Master Theorem) to formally prove that network outputs converge to the Neural Feature Dynamics (NFD) limit.
  - Quick check question: Can you explain how the Master Theorem allows converting finite-width expectations into infinite-width Gaussian expectations?

- Concept: **McKean-Vlasov SDEs**
  - Why needed here: The derived NFD is a coupled forward-backward system of McKean-Vlasov type, where the drift and diffusion coefficients depend on the law (distribution) of the solution itself.
  - Quick check question: How does a McKean-Vlasov process differ from a standard Ito diffusion?

- Concept: **Depth-$\mu$P (Maximal Update Parameterization)**
  - Why needed here: The theory assumes the network is parameterized specifically to preserve feature learning magnitude at infinite depth ($\alpha=1/\sqrt{n}$, residual scaling $1/\sqrt{L}$).
  - Quick check question: What is the specific scaling factor applied to the residual branch in depth-$\mu$P to ensure signal stability?

## Architecture Onboarding

- Component map: Pre-activation ResNet with single-layer or two-layer residual blocks. Key components are the internal representation layer ($W_1$), the residual stream output layer ($W_2$), and the skip connection.
- Critical path: Initialize with depth-$\mu$P scalings $\to$ Monitor internal layer updates ($\Delta x$) $\to$ Detect collapse $\to$ Apply depth-aware LR correction ($\eta_1 \propto \sqrt{L}$) if using multi-layer blocks.
- Design tradeoffs: Increasing the SDE time horizon $T$ expands model capacity (Prop 5) but risks training instability (Fig 2). Pre-act is theoretically stable; Post-act is unstable for ReLU-class activations.
- Failure signatures:
  - **Depth-wise HP transfer failure:** Optimal learning rates drift significantly as depth increases (Fig 5d vs 5e).
  - **Internal learning freeze:** Internal representation updates $\|x^{(k)} - \tilde{x}^{(k)}\|$ stagnate despite active training loss changes.
  - **Feature explosion:** Norm of hidden states growing exponentially with depth (Post-act failure mode).
- First 3 experiments:
  1. **Validate GIA Restoration:** Train a deep ResNet (depth 64+) comparing standard backprop against a "decoupled" version where backward weights are independent copies. Check if losses converge (Fig 4).
  2. **Diagnose Collapse:** Train a 2-layer block ResNet. Plot the norm of updates for the first internal layer vs. the second layer across varying depths to confirm the $1/\sqrt{L}$ suppression.
  3. **Restore HP Transfer:** Apply the depth-aware learning rate ($\eta_1 = \eta_c n\sqrt{L}$) to the first layer of a 2-layer block ResNet and verify that the optimal learning rate remains constant across different depths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Neural Feature Dynamics (NFD) framework and the restoration of the Gradient Independence Assumption (GIA) be generalized to Transformer architectures with attention mechanisms?
- Basis in paper: [explicit] The Conclusion states that these results "open avenues for extending NFD to broader architectures," specifically noting the current focus is on ResNets.
- Why unresolved: The NFD derivation relies on specific properties of pre-activation ResNets (specifically the $1/\sqrt{L}$ residual scaling), and it is not proven if attention layers preserve the "vanishing mechanism" required to restore GIA.
- What evidence would resolve it: A theoretical derivation of NFD for a Transformer block or empirical validation showing GIA restoration and depth-wise hyperparameter transfer in deep Transformers.

### Open Question 2
- Question: Do the NFD limiting dynamics and the proposed depth-aware learning rate correction hold for adaptive optimization algorithms like Adam or AdamW?
- Basis in paper: [explicit] The Conclusion explicitly lists extending NFD to "optimizers" as an open avenue for future work.
- Why unresolved: The paper derives the NFD system and convergence proofs exclusively for online SGD (Section 2). Adaptive methods alter the gradient noise structure and scaling, which could disrupt the $\tau^2$ vanishing mechanism.
- What evidence would resolve it: Theoretical analysis of NFD under adaptive gradient updates or empirical demonstration that the depth-aware correction ($\eta_1 = \eta_c n\sqrt{L}$) successfully restores HP transfer in deep networks trained with Adam.

### Open Question 3
- Question: Can the convergence of finite ResNets to the NFD limit be rigorously proven for the joint simultaneous limit (min(n, L) -> infinity) rather than the sequential limit?
- Basis in paper: [inferred] Theorem 1 establishes convergence for sequential limits (width, then depth). Page 8 notes that commutativity is "demonstrated" empirically (Figure 3) but does not provide a rigorous theorem for the simultaneous limit.
- Why unresolved: Proving commutativity requires establishing uniform convergence rates with respect to both dimensions simultaneously, which is distinct from the sequential analysis provided in Appendix G.
- What evidence would resolve it: A theoretical extension of Proposition 17 proving that the approximation error decays as $O(1/n + 1/L)$ in the joint simultaneous limit without ordering the limits.

### Open Question 4
- Question: How does the "internal learning collapse" manifest in residual blocks with three or more internal layers (e.g., standard Bottleneck blocks)?
- Basis in paper: [inferred] Section 5 explicitly analyzes single-layer and "two-layer residual blocks," identifying a specific collapse mechanism in the first layer, but does not extend this structural analysis to blocks with depth greater than two.
- Why unresolved: The paper's diagnosis relies on a specific separation between the first layer (internal representation) and second layer (residual stream). In 3+ layer blocks, the interaction between multiple internal layers and the residual stream is unknown.
- What evidence would resolve it: An extension of the NFD analysis to 3-layer blocks to determine if additional collapse modes exist or if the proposed depth-aware correction is sufficient for deeper blocks.

## Limitations

- The theoretical analysis critically relies on the joint infinite-width and infinite-depth limit, and the depth-$\mu$P parameterization scaling may not hold in finite regimes.
- The feature collapse mechanism and depth-aware LR correction for multi-layer blocks is empirically validated but lacks rigorous theoretical proof of complete restoration of original dynamics.
- The stability distinction between pre- and post-activation designs assumes "positive dominant" activations and may not generalize to other architectures or normalization schemes.

## Confidence

- **High Confidence**: GIA restoration mechanism (Mechanism 1) - rigorously proven via stochastic calculus in the SDE framework.
- **Medium Confidence**: Feature collapse in multi-layer blocks and depth-aware LR correction (Mechanism 2) - theoretical derivation supported by empirical validation.
- **Medium Confidence**: Pre-activation stability vs. post-activation divergence (Mechanism 3) - mathematically proven under stated assumptions, but real-world generalization uncertain.

## Next Checks

1. **Finite-Depth GIA Validation**: Systematically measure forward-backward correlation terms at finite depths (L=8, 16, 32, 64) to verify the predicted $\tau^2$ scaling and quantify the deviation from the infinite-depth limit.

2. **Alternative Architecture Test**: Apply the depth-aware learning rate correction to a two-layer Transformer block (instead of ResNet) to test whether the feature collapse mechanism is architecture-specific or more general.

3. **Activation Function Stress Test**: Replace ReLU with strictly odd activations (e.g., tanh) or non-positive-dominant activations in post-activation ResNets to verify the predicted divergence behavior and test the boundary of the "positive dominance" assumption.