---
ver: rpa2
title: A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted
  Return Optimization
arxiv_id: '2601.13435'
source_url: https://arxiv.org/abs/2601.13435
tags:
- sharpe
- wavelet
- trading
- loss
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WaveLSFormer addresses the challenge of profitable intraday long-short
  equity trading from noisy, non-stationary financial time series by learning multi-scale
  representations via a learnable wavelet front-end and fusing them with a Transformer
  backbone. It introduces a low-guided high-frequency injection module to stabilize
  cross-frequency integration and optimizes directly for trading performance under
  a fixed risk budget.
---

# A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization

## Quick Facts
- arXiv ID: 2601.13435
- Source URL: https://arxiv.org/abs/2601.13435
- Reference count: 33
- Key outcome: WaveLSFormer achieves 0.607±0.045 ROI and 2.157±0.166 Sharpe on five years of hourly U.S. equity data, substantially outperforming MLP, LSTM, and standard Transformer baselines.

## Executive Summary
WaveLSFormer addresses the challenge of profitable intraday long-short equity trading from noisy, non-stationary financial time series by learning multi-scale representations via a learnable wavelet front-end and fusing them with a Transformer backbone. It introduces a low-guided high-frequency injection module to stabilize cross-frequency integration and optimizes directly for trading performance under a fixed risk budget. On five years of hourly U.S. equity data across six industries, WaveLSFormer achieves 0.607±0.045 ROI and 2.157±0.166 Sharpe, substantially outperforming MLP, LSTM, and standard Transformer baselines, with and without fixed wavelet front-ends.

## Method Summary
WaveLSFormer combines a learnable wavelet front-end with a Transformer encoder to process multi-scale financial time series for long-short equity trading. The model uses FIR convolution kernels trained end-to-end as low-pass and high-pass filters, guided by spectral regularizers that enforce clean band separation while allowing task-driven adaptation. A low-guided high-frequency injection (LGHI) module stabilizes cross-frequency fusion by computing attention weights from low-frequency representations and injecting high-frequency values through a gated residual pathway. The model is trained with a trading-aligned soft-label loss and Sharpe regularization, optimizing directly for risk-adjusted returns rather than point-wise prediction errors. The architecture achieves significant performance gains on hourly U.S. equity data across six industries.

## Key Results
- WaveLSFormer achieves 0.607±0.045 ROI and 2.157±0.166 Sharpe, outperforming baselines by 2-3x in ROI
- Learnable wavelet front-end shows 2x ROI improvement over fixed wavelets across all six industries
- LGHI module enables stable training with low-frequency stability and high-frequency refinement
- Soft-label loss with Sharpe regularization improves risk-adjusted returns over traditional regression losses

## Why This Works (Mechanism)

### Mechanism 1: Learnable Wavelet Filters
Learnable wavelet filters improve signal-to-noise ratio by adapting frequency decomposition to the trading task. FIR convolution kernels are trained end-to-end as low-pass and high-pass filters, with spectral regularizers enforcing clean band separation while allowing task-driven adaptation. This extracts more tradable signals than fixed wavelets by adapting to the specific frequency characteristics of each industry's returns.

### Mechanism 2: Low-Guided High-Frequency Injection
Low-guided high-frequency injection stabilizes cross-frequency fusion by preventing noisy high-frequency components from dominating learning. Attention weights are computed solely from low-frequency representations, while high-frequency representations are injected as values through a gated residual pathway. This preserves the stable low-frequency backbone while selectively incorporating high-frequency cues, preventing gradient instability.

### Mechanism 3: Trading-Aligned Loss Function
Trading-aligned soft-label loss with Sharpe regularization improves risk-adjusted returns over point-wise regression losses. Hard binary labels are replaced with soft targets reflecting directional confidence, and a Sharpe regularizer penalizes low risk-adjusted returns. This aligns the optimization objective with actual trading performance rather than prediction accuracy.

## Foundational Learning

- **Discrete-time Fourier transform (DTFT) of FIR filters**: The learnable wavelet filters are FIR convolution kernels; understanding that their frequency response H(e^jω) = Σ θ_k e^{-jωk} is the DTFT of parameters is essential for interpreting spectral regularization. Quick check: If you increase filter kernel length L, how does this affect frequency resolution in the learned filter?

- **Signal-to-noise ratio decomposition in financial returns**: The paper decomposes log returns ℓ_t = s_t + n_t where s_t is low-frequency trend and n_t is high-frequency noise. Effective SNR is defined as Var[s_t]/Var[n_t]. Quick check: If a learned low-pass filter passes too much high-frequency energy, what happens to SNR and why does this hurt trading?

- **Gradient flow through gated residual connections**: LGHI uses Y = L + β·Z where β controls gradient scaling to LGHI parameters. Understanding ∂L/∂θ_Z = β·(∂L/∂Y)·(∂Z/∂θ_Z) is critical for debugging training instability. Quick check: If β → 0 during training, what is the effective capacity of the model, and what does this suggest about the high-frequency branch's utility?

## Architecture Onboarding

- **Component map**: Input → Neural wavelet (FIR conv) → Low-freq branch + High-freq branch → LGHI fusion → Transformer encoder → Position output → Risk-budget scaling → Loss computation (soft-label + Sharpe + spectral)

- **Critical path**: Input layer receives rolling window X_t ∈ R^{d×L} of log returns. Neural wavelet front-end generates low-/high-frequency components via learnable FIR filters. LGHI module fuses these through gated residual injection. Transformer encoder processes the fused representation, producing scalar position output mapped to [-1,1]. Risk-budget rescaler enforces fixed leverage, and trading objective with spectral regularization computes final loss.

- **Design tradeoffs**: λ_spec (spectral regularization strength) balances clean band separation against filter adaptability; gate initialization γ controls LGHI stability; soft-label scaling k affects prediction confidence calibration; model capacity vs. computational cost trade-offs favor the wavelet front-end's minimal parameter overhead.

- **Failure signatures**: Gradient vanishing in LSTM when β is too large; gradient explosion in Transformer with large β; overfitting to training ROI when penalty weight is too small; spectral collapse when regularization is removed entirely.

- **First 3 experiments**:
  1. Ablate wavelet front-end: Compare Neural Wavelet vs. Classic Wavelet vs. Plain Transformer to isolate benefit of learnable filters.
  2. Vary LGHI gate initialization: Test different γ values to confirm stability window and performance impact.
  3. Compare loss functions: Train with MSE, MAE, and soft-label loss to validate the trading-aligned objective's advantage.

## Open Questions the Paper Calls Out

- **Transaction costs and market frictions**: The authors acknowledge their evaluation uses idealized trading assumptions without modeling transaction costs, slippage, or bid-ask spreads, which could significantly impact real-world profitability.

- **Generalization across asset classes**: The study is limited to U.S. hourly equity data, requiring further validation for daily data, foreign exchange, or high-frequency futures to assess broader applicability.

- **Downside-risk-aware objectives**: The authors propose exploring objectives beyond Sharpe-style regularization, such as Sortino ratio or CVaR, to potentially improve robustness against tail risk and drawdowns.

## Limitations

- Exact hyperparameter values for Sharpe regularizer strength and wavelet loss energy-ratio bounds are unspecified, leaving model stability ambiguous
- Limited cross-validation without formal statistical significance testing for performance differences between baselines
- Fixed 96-timestep window assumption may miss longer-term dependencies relevant to equity dynamics
- No evaluation under realistic market frictions like transaction costs or slippage

## Confidence

- **High confidence**: Learnable wavelet filters outperform fixed wavelets in multi-scale decomposition (direct comparison in Table IV)
- **Medium confidence**: LGHI module stabilizes training without degrading performance (gradient analysis and initialization sweep support this)
- **Medium confidence**: Soft-label loss with Sharpe regularization improves risk-adjusted returns (empirical gains shown, but causal attribution unclear)

## Next Checks

1. **Spectral regularization sweep**: Vary λ_spec ∈ {1, 5, 10, 20, 50} across all six industries to validate optimal value and industry dependence.

2. **LGHI gate sensitivity analysis**: Systematically test γ initialization values {-5, -3, -1, 0, 1} with both LSTM and Transformer backbones to confirm stability window.

3. **Loss component ablation**: Train identical architectures with (a) MSE loss only, (b) soft-label loss without Sharpe regularizer, (c) soft-label + Sharpe without wavelet loss to isolate contribution of each component.