---
ver: rpa2
title: Route Experts by Sequence, not by Token
arxiv_id: '2511.06494'
source_url: https://arxiv.org/abs/2511.06494
tags:
- seqtopk
- arxiv
- experts
- routing
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SeqTopK, a routing mechanism that shifts\
  \ the expert selection budget from the token level to the sequence level in mixture-of-experts\
  \ language models. By selecting the top T\xB7K experts across all T tokens in a\
  \ sequence, SeqTopK enables dynamic expert allocation\u2014assigning more experts\
  \ to difficult tokens and fewer to easy ones\u2014while preserving the same overall\
  \ budget as standard TopK routing."
---

# Route Experts by Sequence, not by Token

## Quick Facts
- **arXiv ID:** 2511.06494
- **Source URL:** https://arxiv.org/abs/2511.06494
- **Authors:** Tiansheng Wen; Yifei Wang; Aosong Feng; Long Ma; Xinyang Liu; Yifan Wang; Lixuan Guo; Bo Chen; Stefanie Jegelka; Chenyu You
- **Reference count:** 15
- **Key outcome:** SeqTopK improves MoE routing by shifting from token-level to sequence-level expert selection, achieving 2.1–16.9% gains across tasks with <1% overhead.

## Executive Summary
This paper introduces SeqTopK, a routing mechanism that shifts the expert selection budget from the token level to the sequence level in mixture-of-experts language models. By selecting the top T·K experts across all T tokens in a sequence, SeqTopK enables dynamic expert allocation—assigning more experts to difficult tokens and fewer to easy ones—while preserving the same overall budget as standard TopK routing. The method requires only a few lines of code, adds less than 1% overhead, and remains fully compatible with pretrained MoE models. Experiments show consistent improvements over TopK and prior parameter-free adaptive methods across diverse tasks, with gains becoming substantially larger under higher sparsity (up to 16.9%). The authors also introduce online SeqTopK with Expert Cache for efficient autoregressive decoding.

## Method Summary
SeqTopK modifies standard TopK routing by computing expert scores for all tokens in a sequence, then selecting the top T·K experts globally across the entire sequence rather than top-K per token. This enables difficult tokens to receive more experts while easy tokens receive fewer, maintaining the same total budget. During training, global SeqTopK uses full sequence context; during inference, online SeqTopK with Expert Cache approximates this using only past tokens. The method adds minimal overhead (<1%) and requires only simple modifications to existing MoE implementations. Token-level bounds (min 1, max K+2 experts) prevent degenerate allocations.

## Key Results
- SeqTopK improves zero-shot GSM8K accuracy by 2.1–16.9% across sparsity levels (K=2 to K=8)
- Achieves 139.41 tokens/s vs. 141.23 for TopK during inference (~1% overhead)
- Shows consistent gains across GSM8K, HumanEval, MBPP, Summary, and Law tasks
- Outperforms TopK and prior parameter-free adaptive routing methods

## Why This Works (Mechanism)

### Mechanism 1: Sequence-Level Budget Redistribution
Shifting from per-token fixed budgets to sequence-level flexible budgets improves expert utilization when tokens vary in complexity. Instead of selecting top-K experts independently for each token, SeqTopK selects the top T·K experts across all T tokens in a sequence. High-entropy tokens (harder predictions) naturally attract more experts, while low-entropy tokens receive fewer—end-to-end training learns this allocation via likelihood maximization. Core assumption: Tokens within a sequence have heterogeneous computational needs that a fixed per-token budget cannot satisfy optimally.

### Mechanism 2: Context-Aware Relative Score Comparison
Routing decisions benefit from comparing expert scores across tokens within the same context, not just within each token independently. SeqTopK computes a score matrix S ∈ R^{T×N} and selects globally top entries. A token receives more experts not just because its absolute scores are high, but because its scores are *relatively* important within the sequence context—similar to how attention contextualizes across positions. Core assumption: Token importance is context-relative, not absolute; the same token may need different expert allocation in different sequences.

### Mechanism 3: Online SeqTopK with Expert Cache for Causal Decoding
The non-causal nature of global SeqTopK can be approximated efficiently during autoregressive generation using a cached, incremental score aggregation. Expert Cache stores routing scores s_1, ..., s_m for all previously generated tokens. At step m, Online SeqTopK selects top m·K experts from the accumulated cache, ensuring the cumulative budget never exceeds T·K. Memory overhead is modest (B×T×N vs. B×T×H for KV cache, where N≪H). Core assumption: Expert allocation for earlier tokens need not be revised once made; the online approximation sufficiently tracks the offline distribution.

## Foundational Learning

- **TopK Routing in MoEs**
  - Why needed here: SeqTopK is a minimal modification of TopK; understanding the baseline (fixed K experts per token, softmax-normalized scores) is prerequisite to grasping what changes.
  - Quick check question: In standard TopK, if K=4 and there are 64 experts, how many experts are activated per token? (Answer: exactly 4, regardless of token difficulty.)

- **Token Heterogeneity and Entropy**
  - Why needed here: The paper's core hypothesis is that tokens vary in predictive difficulty; token entropy (output distribution uncertainty) is used as a proxy for difficulty.
  - Quick check question: A function word like "the" typically has [high/low] next-token entropy compared to a rare domain term. Which would SeqTopK allocate more experts to? (Answer: Low entropy for "the"; more experts to the rare term.)

- **Autoregressive Causality Constraints**
  - Why needed here: Understanding why SeqTopK is "non-causal" and requires the Online variant with Expert Cache for generation is critical for deployment.
  - Quick check question: Why can't vanilla SeqTopK be used directly during autoregressive decoding? (Answer: It requires scores for all future tokens x_{m+1:T}, which are unavailable at step m.)

## Architecture Onboarding

- **Component map:** Router score computation -> TopK selection (modified to sequence-level) -> Expert Cache (for inference) -> Token-level bounds enforcement

- **Critical path:**
  1. **Training:** Replace TopK indexing with sequence-level TopK; use Global SeqTopK (full sequence access)
  2. **Fine-tuning:** Load pretrained TopK checkpoint, fine-tune with SeqTopK for ~100 steps (paper shows convergence is fast)
  3. **Inference:** Switch to Online SeqTopK with Expert Cache; co-locate cache with KV cache for memory efficiency

- **Design tradeoffs:**
  - **Global vs. Online SeqTopK at training:** Global (non-causal, better performance) vs. Online (causal, matches inference). Paper uses Global for training (Table 5), Online for inference
  - **Token-level bounds:** Unbounded allocation maximizes flexibility but risks some tokens receiving 0 experts. K+2 upper bound is a safe default (ablation in Table 7)
  - **Shared experts:** Models like Qwen1.5-MoE have shared experts; SeqTopK gains are smaller (3.6% vs. 5.9%) because flexibility is constrained

- **Failure signatures:**
  - **Degenerate allocation:** Some tokens consistently receive 0 experts → check token-level lower bound is enforced
  - **BatchTopK-style batch sensitivity:** If implementing batch-level instead of sequence-level routing, performance becomes unstable across batch sizes (Figure 6b)
  - **Memory blowup:** Expert Cache should be O(B×T×N); if approaching KV cache size, check N is correctly set (should be 8–128, not hidden dimension)

- **First 3 experiments:**
  1. **Validation parity:** Fine-tune a small pretrained MoE (e.g., 182M) with SeqTopK vs. TopK on a single downstream task (GSM8K). Expect 2–5% gain at default sparsity
  2. **Sparsity scaling:** Ablate across K=8, 4, 2 on the same model/task. Gains should increase as sparsity increases (paper shows ~7.5% gain at K=2)
  3. **Expert Cache overhead:** Profile inference throughput and peak memory with Online SeqTopK vs. TopK. Target: <2% throughput degradation, <2% memory increase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between Global SeqTopK and Online SeqTopK during training be closed through improved training strategies or architectural modifications?
- Basis in paper: [explicit] Table 5 shows Global SeqTopK consistently outperforms Online SeqTopK (e.g., 46.09 vs 45.80 on GSM8k). Authors state: "Online SeqTopK... consistently underperforms compared to Global SeqTopK" and attribute this to "richer contextual information" available to the global variant.
- Why unresolved: The non-causal nature of Global SeqTopK makes it incompatible with autoregressive decoding at inference, creating a train-test mismatch that may limit performance.
- What evidence would resolve it: A training strategy that closes the gap to within statistical insignificance, or an improved online formulation that better approximates global context.

### Open Question 2
- Question: How should the optimal upper bound for per-token expert allocation be selected, and can it be learned adaptively rather than set as a hyperparameter?
- Basis in paper: [inferred] Table 7 shows performance fluctuates with different upper bounds (K+1 through K+4), with no single configuration dominating across all tasks. The authors note "slight fluctuations depending on the specific upper bound chosen."
- Why unresolved: The paper empirically tests fixed bounds but provides no principled method for selection, and different tasks favor different bounds.
- What evidence would resolve it: A learnable or adaptive bound selection mechanism that matches or exceeds the best fixed-bound performance across all evaluated tasks.

### Open Question 3
- Question: What are the theoretical scaling properties of SeqTopK when combined with expert-level specialization methods such as expert pruning, merging, or grouping?
- Basis in paper: [explicit] The conclusion states SeqTopK "is orthogonal to the aforementioned methods and can be easily combined with them," but no experiments validate this combination.
- Why unresolved: Expert-level specialization reduces capacity while SeqTopK redistributes capacity—interactions between these mechanisms are unexplored.
- What evidence would resolve it: Experiments combining SeqTopK with at least one expert-level specialization method, demonstrating whether gains are additive, synergistic, or conflicting.

## Limitations

- Focuses primarily on decoder-only models with fixed sequence lengths during training; results may not transfer directly to encoder-decoder architectures or variable-length generation scenarios
- Reported gains are measured primarily on benchmark datasets with synthetic evaluations; real-world long-form generation tasks with diverse sequence lengths and topics remain untested
- The Expert Cache mechanism, while efficient, introduces an approximation that deviates from the optimal global routing used during training—the magnitude of this gap in practical deployments is not fully characterized

## Confidence

**High Confidence**: The claim that SeqTopK improves expert utilization and task performance over TopK routing across multiple benchmarks is strongly supported. Multiple experiments consistently show gains of 2.1–16.9% across different sparsity levels, model scales, and tasks. The mechanism is well-understood, implementation is straightforward, and ablation studies validate the design choices.

**Medium Confidence**: The assertion that SeqTopK's benefits increase substantially under higher sparsity (K=2) is supported by data but requires more extensive validation. The GSM8K improvement at K=2 (16.9%) is impressive but based on a single model/task combination. The claim about reduced expert imbalance is indirect—measured via normalized entropy—but the practical significance for model stability needs further investigation.

**Low Confidence**: The characterization of Online SeqTopK as a negligible-overhead approximation of global routing is partially supported but has gaps. While Table 4b shows only 1% throughput overhead and Table 5 shows comparable performance, the 0.2–0.5% gap in zero-shot GSM8K suggests the approximation is not perfect. The paper does not characterize when this gap becomes problematic or how it scales with sequence length and model size.

## Next Checks

1. **Sequence Length Sensitivity**: Evaluate SeqTopK performance across varying sequence lengths (e.g., 256, 1024, 2048 tokens) to determine if the dynamic allocation advantage persists or diminishes for longer contexts where expert cache management becomes more complex.

2. **Real-World Generation Tasks**: Test SeqTopK on long-form content generation tasks (e.g., story completion, code generation with context) rather than benchmark datasets to assess practical benefits for actual deployment scenarios involving variable-length, diverse content.

3. **Expert Cache Approximation Gap**: Systematically measure the performance difference between Global SeqTopK (training) and Online SeqTopK (inference) across multiple model scales and sequence lengths to quantify when and why the approximation degrades, establishing guidelines for when exact global routing might be necessary despite the overhead.