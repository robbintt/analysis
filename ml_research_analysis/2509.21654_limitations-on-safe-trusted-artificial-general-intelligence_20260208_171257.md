---
ver: rpa2
title: Limitations on Safe, Trusted, Artificial General Intelligence
arxiv_id: '2509.21654'
source_url: https://arxiv.org/abs/2509.21654
tags:
- system
- program
- safe
- turing
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a fundamental incompatibility between safety,
  trust, and Artificial General Intelligence (AGI) under strict mathematical definitions.
  The authors define safety as never making false claims, trust as assuming safety,
  and AGI as matching or exceeding human capability on all provable instances.
---

# Limitations on Safe, Trusted, Artificial General Intelligence

## Quick Facts
- arXiv ID: 2509.21654
- Source URL: https://arxiv.org/abs/2509.21654
- Reference count: 33
- One-line primary result: Safe, trusted AI systems cannot be AGI systems due to fundamental mathematical impossibility results.

## Executive Summary
This paper establishes a fundamental incompatibility between safety, trust, and Artificial General Intelligence (AGI) under strict mathematical definitions. The authors define safety as never making false claims, trust as assuming safety, and AGI as matching or exceeding human capability on all provable instances. They prove that a system satisfying both safety and trust cannot be an AGI system, demonstrating this impossibility for program verification, planning, and graph reachability through self-referential constructions inspired by Gödel's incompleteness theorems and Turing's undecidability proof.

## Method Summary
The paper uses self-referential program constructions to prove impossibility results for safe, trusted AGI systems. The approach involves defining an abstract AI system interface with a predict method that can output correct answers or abstain, then constructing task instances that humans can provably solve but safe, trusted AI systems must either abstain from or solve incorrectly. The key constructions include Gödel_program, Turing_program, and Turing_T, which use self-referential logic to create contradictions when a safe AI system attempts to answer certain verification questions about these programs.

## Key Results
- Theorem 1.5/3.2: Safe, trusted AI systems cannot achieve AGI-level performance for program verification tasks
- Theorem 3.6: Similar impossibility results hold for planning tasks
- Theorem 3.11: Graph reachability with time bounds also cannot be solved by safe, trusted AGI systems
- Theorem 4.2: Limitations persist even for calibration-safety variants with probabilistic guarantees

## Why This Works (Mechanism)
The impossibility results work through self-referential program constructions that create logical contradictions when a safe AI system attempts to verify them. When a safe AI system is asked to verify whether these programs are well-behaved, the construction ensures that either the AI must make a false claim (violating safety) or abstain from answering (failing to achieve AGI-level performance). This is analogous to Gödel's incompleteness theorems, where self-reference creates statements that cannot be proven within a formal system.

## Foundational Learning
- **Self-referential program constructions**: Why needed: Creates logical contradictions that expose limitations of safe AI systems. Quick check: Verify that Gödel_program correctly implements the self-referential logic in Algorithm 1.
- **Formal definitions of safety and trust**: Why needed: Provides precise mathematical framework for impossibility proofs. Quick check: Ensure safety definition (never false claims) and trust definition (assuming safety) are correctly implemented.
- **Computability theory and undecidability**: Why needed: Underpins the theoretical foundation for impossibility results. Quick check: Confirm that the constructions properly leverage Turing's undecidability proof.
- **Program verification concepts**: Why needed: Core application domain for the impossibility results. Quick check: Verify that termination verification is correctly modeled as the target task.

## Architecture Onboarding
**Component Map**: AI System (interface) -> Self-referential Construction (Gödel/Turing programs) -> Verification Task -> Contradiction Proof

**Critical Path**: The critical path involves constructing the self-referential program, then having the AI system attempt to verify it. The contradiction arises when the AI's safety property conflicts with the need to correctly answer about the self-referential program.

**Design Tradeoffs**: The paper uses strict definitions of safety and trust to establish impossibility results, but these definitions may be too restrictive for practical applications. The tradeoff is between mathematical rigor and real-world applicability.

**Failure Signatures**: Failure occurs when the AI system either makes a false claim (violating safety) or abstains from answering (failing AGI performance). The self-referential construction ensures that safe AI systems cannot correctly answer about themselves.

**First 3 Experiments**:
1. Implement Algorithm 1's Gödel_program construction and verify it creates the expected logical contradiction
2. Test a concrete safe-by-construction AI system against the Gödel_program to demonstrate the impossibility
3. Extend the construction to program verification tasks beyond termination (analogous to Rice's theorem)

## Open Questions the Paper Calls Out
- Can the impossibility results be extended to other semantic properties of programs beyond termination verification, analogous to Rice's theorem?
- What alternative, practical definitions of safety and trust could circumvent the impossibility results while remaining useful for deployment?
- For calibration-safety, does the impossibility result depend on the specific calibration threshold (±0.25), or do similar limitations persist for arbitrarily tight calibration requirements?

## Limitations
- The paper uses very strict definitions of safety and trust that may not map to practical AI systems
- Self-referential constructions rely on idealized capabilities (perfect verification, infinite reasoning time)
- The paper doesn't address potential loopholes or relaxed definitions that might permit safe, trusted AGI
- Calibration-safety results assume strict probabilistic guarantees that may be difficult to achieve in practice

## Confidence
- Mathematical proofs of impossibility: High
- Applicability to practical AI safety: Low-Medium
- Calibration-safety variant robustness: Medium

## Next Checks
1. Verify the self-referential program constructions (Gödel_program, Turing_program) with a concrete, formally verified safe-by-construction AI system implementation
2. Test whether relaxing safety to allow bounded error rates changes the impossibility results
3. Cross-reference the best-arm identification parameters from Karnin et al. 2013 to ensure correct application in the calibration-safety proof