---
ver: rpa2
title: Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input
arxiv_id: '2510.05864'
source_url: https://arxiv.org/abs/2510.05864
tags:
- harmful
- harm
- context
- sentences
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically evaluates how large language models\
  \ (LLMs) detect harmful content\u2014including toxic, offensive, and hate speech\u2014\
  in long-context settings. Using LLaMA-3, Qwen-2.5, and Mistral, the study varies\
  \ harmful content type (explicit vs."
---

# Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input

## Quick Facts
- arXiv ID: 2510.05864
- Source URL: https://arxiv.org/abs/2510.05864
- Authors: Faeze Ghorbanpour; Alexander Fraser
- Reference count: 40
- This paper systematically evaluates how large language models detect harmful content in long-context settings, revealing systematic biases and dilution effects.

## Executive Summary
This paper systematically evaluates how large language models detect harmful content—including toxic, offensive, and hate speech—in long-context settings. Using LLaMA-3, Qwen-2.5, and Mistral, the study varies harmful content type (explicit vs. implicit), position (beginning, middle, end), prevalence (0.01–0.50 of the prompt), and context length (600–6000 tokens). Results show that model performance peaks at moderate harmful prevalence (~0.25), declines with longer contexts, is strongest for harmful content at the beginning, and is more reliable for explicit harm than implicit harm. This reveals systematic biases and dilution effects, providing actionable insights for improving safety in long-context LLM applications.

## Method Summary
The study evaluates three large language models (LLaMA-3-8B, Qwen2.5-7B, Mistral-7B) on their ability to identify and localize harmful sentences within artificially constructed long prompts. Using three harm datasets (IHC, OffensEval, JigsawToxic), prompts are created by sampling sentences with controlled variables: prompt length (600-6000 tokens), harm ratio (0.05-0.5), harm region (beginning/middle/end/all), and harm type (implicit/explicit/both). Models receive instruction-tuned prompts requesting sentence indices of harmful content. Evaluation uses macro-F1, precision, recall, and Predicted Prevalence Value (PPV) across 128 trials per setting with temperature=0.0 and top-p=1.0.

## Key Results
- Model performance peaks at moderate harmful prevalence (~0.25) but declines when content is very sparse or dominant
- Recall decreases with increasing context length due to attention dilution
- Harmful content at the beginning is detected most reliably, followed by middle and end positions
- Models maintain calibration (PPV ≈ true prevalence) but struggle with precise localization (lower recall/F1)
- Explicit harm detection is more reliable than implicit harm detection across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs maintain calibration of harmful content prevalence but fail at precise localization.
- Mechanism: When presented with mixed harmful/non-harmful sentences, models develop an implicit prior about expected harmfulness frequency. Predicted Prevalence Value (PPV) closely tracks true harm ratio, yet recall and F1 remain lower—indicating models estimate "how many" but struggle with "which ones."
- Core assumption: The calibration-localization gap stems from distributed attention across long sequences rather than a fundamental misunderstanding of harm definitions.
- Evidence anchors:
  - [abstract] "performance peaks at moderate harmful prevalence (~0.25) but declines when content is very sparse or dominant"
  - [section 5.2] "PPV for the long-context evaluation is close to the actual #harm/#total, which suggests that the model can estimate the amount of harmful content in the context"
  - [corpus] LongSafety (arXiv:2502.16971) corroborates safety degradation in extended contexts but focuses on compliance scores rather than localization

### Mechanism 2
- Claim: Positional attention bias causes earlier harmful content to be detected more reliably than later content.
- Mechanism: Transformer attention mechanisms exhibit primacy effects—tokens at the beginning receive higher effective attention weight across layers. When harmful sentences appear early, they benefit from this positional advantage.
- Core assumption: The position effect is architectural (attention pattern) rather than task-specific (harm detection uniquely sensitive to position).
- Evidence anchors:
  - [abstract] "harmful content at the beginning is detected most reliably"
  - [section 5.4] "models achieve higher macro-F1 and harmful recall when harmful sentences appear at the beginning, followed by the middle, and lowest at the end"
  - [corpus] No direct corpus evidence on positional bias in safety contexts; related work on long-context retrieval (Sense and Sensitivity, arXiv:2505.13353) examines semantic recall but not positional effects on harmful content

### Mechanism 3
- Claim: Dilution reduces harmful content salience proportionally to non-harmful token expansion.
- Mechanism: With fixed harmful sentence count, adding non-harmful sentences spreads attention more thinly. The signal-to-noise ratio decreases, and harmful content receives less cumulative attention.
- Core assumption: The dilution effect is primarily about attention distribution, not confusion from semantic interference (prompts were artificially constructed without discourse coherence).
- Evidence anchors:
  - [abstract] "recall decreases with increasing context length"
  - [section 5.3] "By increasing the number of non-harmful sentences, the model's ability to extract harmful ones decreases. Both harmful recall and harmful precision drop"
  - [corpus] Jailbreaking in the Haystack (arXiv:2511.04707) examines needle-in-haystack jailbreak attacks, conceptually related but focuses on adversarial injection rather than natural harmful content dilution

## Foundational Learning

- Concept: **Macro-F1 vs. Micro-F1 for imbalanced classification**
  - Why needed here: Harmful content is typically sparse (5–50% in experiments). Macro-F1 averages per-class performance equally, preventing the majority (non-harmful) class from dominating the metric.
  - Quick check question: If a model labels 95% of sentences as non-harmful in a 10% harmful dataset and achieves 90% accuracy, is it performing well? (Answer: No—accuracy is misleading; Macro-F1 would reveal poor harmful-class recall.)

- Concept: **Predicted Prevalence Value (PPV) as calibration metric**
  - Why needed here: PPV measures whether models' output distribution matches input distribution. A model that always predicts 25% harmful regardless of actual prevalence is miscalibrated.
  - Quick check question: If true harm ratio is 0.10 and PPV is 0.40, what does this indicate? (Answer: Over-prediction bias; model labels too many sentences as harmful.)

- Concept: **Dilution vs. prevalence as distinct experimental factors**
  - Why needed here: Dilution fixes harmful count and varies total; prevalence varies the ratio directly. Conflating them obscures whether performance drops due to sparsity or absolute context length.
  - Quick check question: In a dilution experiment with 10 harmful sentences, why does increasing total sentences from 50 to 200 degrade recall even though harm definition hasn't changed? (Answer: Attention spreads across more tokens; harmful signal becomes proportionally weaker.)

## Architecture Onboarding

- Component map:
  Input layer -> Tokenized numbered sentences -> Prompt template -> Model backbone -> Output parser -> Evaluation

- Critical path:
  1. Sample harmful sentences from dataset (IHC/OffensEval/JigsawToxic) based on harm ratio r
  2. Fill remaining tokens with non-harmful sentences
  3. Position harmful content in specified region (beginning/middle/end/all)
  4. Number sentences and construct prompt
  5. Parse model output as integer list
  6. Compare against ground-truth indices

- Design tradeoffs:
  - Artificial prompts (controlled variables) vs. coherent documents (ecological validity): Paper chooses control; real-world performance may differ with discourse structure
  - Sentence-level vs. long-context prompting: Sentence-level shows high recall (~90%) but low precision (~30%); long-context improves calibration but reduces recall
  - Explicit vs. implicit harm detection: Explicit relies on lexical cues (easier); implicit requires semantic/pragmatic inference (harder, underexplored in long-context)

- Failure signatures:
  - Over-prediction at sentence level: PPV >> true prevalence (models biased toward "harmful")
  - Under-localization at long context: PPV ≈ true prevalence but recall < sentence-level baseline
  - End-position blindness: Harmful content in final third detected least reliably
  - Implicit harm blindness: Macro-F1 drops 5–15 points for implicit vs. explicit across all models

- First 3 experiments:
  1. **Reproduce prevalence curve at 600–6000 tokens** on a held-out harm category (e.g., self-harm content) to validate whether the 0.25 peak generalizes beyond toxic/offensive/hate speech.
  2. **Ablate position with controlled semantic context** by embedding harmful sentences in coherent paragraphs rather than random sentence lists to test whether discourse structure attenuates or amplifies positional bias.
  3. **Compare Mistral sliding-window vs. LLaMA full attention on 15k+ tokens** to isolate whether the middle-region advantage observed in Mistral (Appendix F) is architecture-specific.

## Open Questions the Paper Calls Out

- **Question**: Do the observed dilution and regional bias patterns persist when harmful content is embedded within coherent, naturally flowing long-form documents rather than artificially constructed random sentences?
  - Basis in paper: [explicit] The authors state in the Limitations that their prompts "lack natural coherence" and suggest that "constructing coherent long texts—potentially using synthetic data generation—would allow evaluation under more realistic conditions."
  - Why unresolved: The current study isolates variables by using random sentence combinations, but this removes the semantic context of real documents which might aid or hinder detection.
  - What evidence would resolve it: Replicating the evaluation framework using synthetically generated or human-written coherent documents with embedded harmful spans.

- **Question**: Do the findings regarding prevalence and position sensitivity generalize to other safety categories such as self-harm encouragement or dangerous instructions?
  - Basis in paper: [explicit] The Limitations section notes, "Beyond hate speech, offensive, and toxic language, one could examine encouragement of self-harm, spam... we focused on three well-studied categories."
  - Why unresolved: Different harm types rely on different semantic markers; implicit hate speech may present different detection challenges compared to implicit self-harm or phishing.
  - What evidence would resolve it: Applying the diagnostic framework to datasets containing these additional categories of harm.

- **Question**: Can specific prompting strategies or retrieval policies close the performance gap between accurately estimating harm prevalence (calibration) and correctly identifying specific harmful sentences (localization)?
  - Basis in paper: [inferred] The Discussion highlights a mismatch where "LLMs partially cover harmful content; they recognize the presence of harm but fail to extract all harmful sentences," and calls for "safer prompting strategies, calibration methods, and retrieval policies."
  - Why unresolved: The paper identifies the failure mode (models know harm is present but miss specific instances) but does not test interventions to mitigate this specific localization deficit.
  - What evidence would resolve it: Experiments testing specific interventions (e.g., retrieval-augmented localization or self-consistency checks) within this framework to measure improvements in recall.

## Limitations

- The study uses artificially constructed prompts without natural discourse coherence, which may not reflect real-world document detection challenges
- Results represent instruction-tuned base models without safety fine-tuning, limiting applicability to deployed safety systems
- The sentence-level evaluation achieves high recall but low precision, suggesting the artificial setup may not reflect practical harm detection where false positives are costly

## Confidence

- **High confidence**: The dilution effect (performance declines with increasing context length while maintaining calibration) and prevalence-performance relationship (peak at ~0.25 harm ratio) are directly supported by experimental data across all three models. The position-based detection bias (beginning > middle > end) shows consistent patterns across models.

- **Medium confidence**: The mechanism explanations (attention-based primacy effects, distributed attention dilution) are plausible given the architecture but lack direct causal evidence. The claim that models estimate "how many" but struggle with "which ones" (calibration-localization gap) is supported by PPV-recall divergence but could have alternative explanations.

- **Low confidence**: The implicit harm detection challenges are identified but not deeply analyzed mechanistically. The claim that implicit harm detection is "more underexplored" lacks systematic comparison of why explicit vs. implicit harm differs beyond lexical vs. semantic processing.

## Next Checks

1. **Generalization across harm categories**: Reproduce the prevalence curve (600-6000 tokens) on self-harm or misinformation content to test whether the 0.25 peak generalizes beyond toxic/offensive/hate speech.

2. **Coherent context ablation**: Embed harmful sentences in semantically coherent paragraphs rather than random lists to test whether discourse structure attenuates or amplifies positional bias.

3. **Architecture-specific attention patterns**: Compare Mistral's sliding-window vs. LLaMA's full attention on 15k+ tokens to isolate whether the middle-region advantage observed in Mistral (Appendix F) is architecture-specific or generalizable.