---
ver: rpa2
title: 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM'
arxiv_id: '2511.19496'
source_url: https://arxiv.org/abs/2511.19496
tags:
- uni00000011
- uni00000008
- uni00000013
- uni00000044
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Xmodel-2.5 introduces a 1.3-billion-parameter small language model\
  \ that improves complex reasoning efficiency using a three-phase Warmup\u2013Stable\u2013\
  Decay curriculum and maximal-update parameterization (\xB5P). A key innovation is\
  \ switching from AdamW to Muon optimizer during the decay phase, which raises the\
  \ 13-task reasoning average by 4.58% without altering other hyperparameters."
---

# Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM

## Quick Facts
- arXiv ID: 2511.19496
- Source URL: https://arxiv.org/abs/2511.19496
- Reference count: 7
- Primary result: 1.3B model achieves 52.49% average on 13 reasoning benchmarks using 25.7× fewer training tokens than Qwen3

## Executive Summary
Xmodel-2.5 is a 1.3-billion-parameter small language model designed for efficient complex reasoning. It introduces a three-phase Warmup–Stable–Decay curriculum combined with maximal-update parameterization (µP) to enable zero-shot hyperparameter transfer from small proxies. A key innovation is switching from AdamW to Muon optimizer during the decay phase, yielding a 4.58% reasoning improvement. The model achieves competitive performance on 13 reasoning benchmarks while using only 1.4T training tokens—demonstrating strong data efficiency for edge and cost-sensitive applications.

## Method Summary
The model uses a 48-layer decoder-only transformer (1536 hidden, 3840 intermediate) with µP-compliant attention scaling and RoPE position encoding. Training follows a 560k-step WSD schedule: Warmup (2k steps), Stable-S1 (270k steps), Stable-S2 (260k steps), Decay (20k steps), and Long-Context Refinement (10k steps). Data mixing includes 66.9% high-quality SFT during Decay. The optimizer switches from AdamW to Muon at Decay onset. FP8 mixed-precision training (E4M3 forward, E5M2 backward) provides ~30% throughput gain. Hyperparameters are tuned on a 20M-parameter proxy and transfer directly to the 1.3B model under µP.

## Key Results
- Achieves 52.49% average on 13 reasoning benchmarks (2nd among 1–2B models)
- Uses 25.7× fewer training tokens than Qwen3 (1.4T vs 36T) while remaining within 4.47% of Qwen3 performance
- Muon optimizer switch during decay improves reasoning average by 4.58% over AdamW-only baseline
- FP8 training increases throughput by ~30% while preserving accuracy
- Long-context adaptation to 16K tokens improves downstream reasoning despite modest perplexity increase

## Why This Works (Mechanism)

### Mechanism 1: Optimizer Switching (AdamW → Muon During Decay)
Switching from AdamW to Muon during the decay phase improves downstream reasoning without modifying other hyperparameters. AdamW provides early-training stability via adaptive moment estimation, while Muon applies sharper, more directed updates in the final training stage. The paper posits that early stability + late sharpening yields better feature refinement. Evidence shows a 4.58% improvement in the 13-task reasoning average under this single-variable change.

### Mechanism 2: Maximal-Update Parameterization (µP) for Hyperparameter Transfer
µP allows hyperparameters tuned on a 20M-parameter proxy model to transfer directly to the 1.3B full model, even with tied word embeddings. µP rescales initialization and learning rates proportionally to width, preserving activation magnitudes across model scales. This enables extensive hyperparameter search on cheap proxies without re-tuning at scale. The transfer is validated even under the parameter-tied tie-word-embedding architecture.

### Mechanism 3: WSD Curriculum with Decay-Phase SFT Blending
A three-phase Warmup–Stable–Decay schedule, with 66.9% high-quality SFT data blended only during decay, improves reasoning efficiency per token. Broad pre-training data in the Stable phase builds general representations, while decay-phase SFT blending sharpens task-relevant capabilities just before convergence when the model is most plastic to high-quality signals.

## Foundational Learning

- **Maximal-Update Parameterization (µP)**
  - Why needed: Enables zero-shot hyperparameter transfer from small proxies to large models; critical for avoiding expensive HP sweeps at 1B+ scale
  - Quick check: If you double the hidden dimension, does your current training recipe automatically adjust LR and initialization to preserve activation scale?

- **Muon Optimizer (Matrix Orthogonalization)**
  - Why needed: The paper's key ablation switches to Muon during decay; understanding its orthogonalization mechanism explains why late-phase sharpening helps reasoning
  - Quick check: Can you explain why orthogonalizing gradient updates might improve conditioning compared to Adam's moment-based adaptation?

- **Learning-Rate Schedules (WSD vs. Cosine Decay)**
  - Why needed: WSD decouples annealing from total token count, allowing flexible data mixing during decay; contrasts with fixed cosine schedules
  - Quick check: In a WSD schedule, what happens to training dynamics if you extend the Stable phase without adjusting the Decay phase duration?

## Architecture Onboarding

- **Component map**: DeepSeek-v3 tokenizer (129K vocab) → 48-layer transformer (1536 hidden, 3840 intermediate) → µP attention with 1/d_head scaling → RoPE position encoding (base 500K) → FP8 mixed-precision training

- **Critical path**: 
  1. Set up µP-scaled proxy (20M params) and run HP search
  2. Transfer HPs to full 1.3B model
  3. Execute WSD phases with correct data blends (SFT only in decay)
  4. Switch optimizer from AdamW to Muon at decay start
  5. Optional: 10K-step long-context adaptation (16K)

- **Design tradeoffs**:
  - Tie-word-embeddings reduces parameters but complicates µP transfer—validated here but not guaranteed for other architectures
  - FP8 vs. BF16: +30% throughput, but requires careful amax-history-len tuning; may degrade on outliers
  - Muon switch: +4.58% reasoning, but optimizer-state incompatibility requires warm restart (no momentum carryover)

- **Failure signatures**:
  - Training loss spikes immediately after AdamW→Muon switch (check: LR scaling, weight decay compatibility)
  - Proxy-to-full HP transfer yields divergent training curves (check: µP attention scaling, embedding LR scaling)
  - FP8 underflow causing NaN gradients (check: master-weight dtype, amax-history-len ≥128)

- **First 3 experiments**:
  1. **Proxy validation**: Train 20M proxy with µP, verify loss curve matches expected dynamics before scaling up
  2. **Optimizer ablation**: Run decay-phase-only comparison (AdamW vs. Muon) on a mid-training checkpoint to confirm +4-5% reasoning gain reproduces
  3. **FP8 stability test**: Compare BF16 vs. FP8 loss curves on 1B tokens; confirm <0.5% perplexity gap before full training run

## Open Questions the Paper Calls Out

- **Multi-step agentic behaviors**: Systematic evaluation of multi-step agentic behaviors remains under-explored despite SLMs being positioned as agent cores. Current benchmarks focus on single-turn QA; no standardized protocol exists for sequential decision-making in resource-constrained settings.

- **FP8 + µP interaction**: FP8 mixed-precision and maximal-update parameterization have not been jointly studied. µP theory assumes full-precision computations; reduced precision could perturb the hyperparameter transfer mechanism.

- **Muon optimizer mechanism**: The 4.58% reasoning gain from switching AdamW to Muon during decay is empirically demonstrated but the causal mechanism remains unclear. Is it loss landscape geometry, momentum handling, or schedule interaction?

- **Perplexity-reasoning trade-off**: Long-context adaptation increases perplexity while simultaneously improving downstream reasoning accuracy, challenging the conventional assumption that perplexity correlates positively with downstream task performance.

## Limitations

- µP + tie-word-embedding transfer is theoretically sound but lacks broad empirical validation beyond this paper's 20M→1.3B case; architectural deviations could break the transfer
- WSD curriculum with decay-phase SFT blending is novel and shows positive internal results, but lacks external ablation studies or comparative schedules in the corpus
- Muon optimizer benefits are supported by recent literature but the specific AdamW→Muon switch during decay is only validated here; edge cases like gradient instability are not fully characterized

## Confidence

- **High confidence**: FP8 + WSD + long-context adaptation results and comparisons (WikiText-2 perplexity, 13-task reasoning, token efficiency metrics are well specified)
- **Medium confidence**: Muon optimizer contribution (+4.58% reasoning) due to strong internal evidence but limited external ablation and the known µP alignment caveat from recent papers
- **Medium confidence**: µP transfer claim; theoretically justified and internally validated, but direct empirical support for tie-word-embedding + µP scaling remains sparse in the corpus

## Next Checks

1. **Muon optimizer ablation**: Run a controlled decay-phase-only experiment comparing AdamW vs. Muon (on a mid-training checkpoint) to confirm the +4-5% reasoning gain reproduces independently

2. **µP transfer stress test**: Intentionally train a slightly different proxy (e.g., with modified attention scaling) and measure proxy-to-full transfer fidelity to expose µP limits

3. **WSD schedule ablation**: Compare reasoning performance when SFT data is introduced uniformly throughout training vs. only in the decay phase, holding total SFT token count constant