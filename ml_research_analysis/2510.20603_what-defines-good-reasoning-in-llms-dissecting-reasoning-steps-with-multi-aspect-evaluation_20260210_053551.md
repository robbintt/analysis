---
ver: rpa2
title: What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect
  Evaluation
arxiv_id: '2510.20603'
source_url: https://arxiv.org/abs/2510.20603
tags:
- reasoning
- case
- evaluation
- step
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a multi-aspect evaluation framework for large
  language model reasoning, decomposing reasoning quality into relevance and coherence.
  Relevance assesses if a step is grounded in and addresses the problem, while coherence
  measures if it logically follows from prior steps.
---

# What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation

## Quick Facts
- **arXiv ID:** 2510.20603
- **Source URL:** https://arxiv.org/abs/2510.20603
- **Reference count:** 9
- **Primary result:** Introduces CaSE evaluation and demonstrates that optimizing for relevance/coherence beyond correctness improves LLM reasoning performance.

## Executive Summary
This paper addresses the fundamental question of what constitutes good reasoning in large language models by decomposing reasoning quality into two orthogonal aspects: relevance (whether a step addresses the problem) and coherence (whether it logically follows from prior steps). The authors propose Causal Stepwise Evaluation (CaSE) to evaluate each reasoning step using only its preceding context, avoiding hindsight bias that plagues whole-trace evaluation. Through experiments on new expert-annotated benchmarks, they show CaSE achieves stronger agreement with human judgments than baselines, and that training data curated using CaSE-evaluated aspects or inference-time guidance emphasizing these aspects directly improves downstream problem-solving accuracy.

## Method Summary
The authors introduce a multi-aspect evaluation framework for LLM reasoning that decomposes step quality into relevance and coherence. CaSE evaluates each step using only preceding context to avoid hindsight bias. They construct two expert-annotated benchmarks (MRa-GSM8K, MRa-MATH) to validate this approach. Experiments demonstrate CaSE outperforms whole-trace baselines in agreement with human judgments, and that optimizing for these aspects through data curation or inference guidance improves final task performance. The framework includes component maps showing evaluator selection, prompt design, and application to SFT filtering and inference-time guidance.

## Key Results
- CaSE achieves stronger agreement with human judgments than whole-trace baselines, with larger gains in smaller models (Phi-3.5-mini: 0.612 vs 0.480 avg F1; GPT-4o: 0.766 vs 0.746)
- Among incorrect solution traces, those satisfying both relevance and coherence are 2× more likely to yield correct final answers (52% vs 24%)
- Aspect-guided inference improves AIME accuracy by +1.1 average across models, and CaSE-curated SFT data outperforms heuristic-filtered baselines on MATH/GPQA/AIME24 across 4B-32B model scales

## Why This Works (Mechanism)

### Mechanism 1: Causal Stepwise Evaluation (CaSE) Prevents Hindsight Bias
CaSE evaluates each reasoning step using only its preceding context, preventing evaluators from retroactively justifying flawed steps based on correct outcomes. This enforces temporal grounding that mirrors autoregressive generation.

### Mechanism 2: Relevance and Coherence Provide Complementary Quality Signals Beyond Correctness
Relevance filters out steps that don't contribute to solving the problem, while coherence filters out logical discontinuities. Together they capture process quality orthogonal to local step correctness, with solutions maintaining both aspects being 2× more likely to reach correct answers even when containing incorrect steps.

### Mechanism 3: Aspect-Guided Inference and Data Curation Transfer Evaluation Signals to Generation
Explicitly emphasizing relevance and coherence during inference or in training data selection improves downstream reasoning accuracy. Sample-level filtering (requiring all steps to satisfy aspects) outperforms step-level pruning, suggesting complete coherent trajectories provide stronger training signal.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: CaSE operates on decomposed reasoning steps; understanding step-by-step generation is prerequisite
  - Quick check question: Can you explain why "Let's think step by step" prompts improve multi-step problem solving?

- **Concept: LLM-as-a-Judge Evaluation Paradigm**
  - Why needed here: CaSE uses LLM evaluators; understanding biases and limitations of LLM judgment is critical
  - Quick check question: What is "hindsight bias" in evaluation and why might showing a full solution trace affect judgment quality?

- **Concept: Process Reward Models (PRMs) vs Outcome Reward Models (ORMs)**
  - Why needed here: Paper positions relevance/coherence as process-level signals beyond outcome correctness
  - Quick check question: Why might optimizing only for final-answer correctness fail to improve reasoning quality?

## Architecture Onboarding

- **Component map:**
  [Input: Question + Reasoning Trace] → [Step Segmentation] → [CaSE Evaluator Loop] → [Aggregation] → [Application]
  For each step k: Context: (Question, Steps 1 to k-1) → Relevance Assessment → Coherence Assessment → Correctness Assessment

- **Critical path:** Evaluator model selection → Prompt design for aspect assessment → Filtering threshold calibration → Downstream task validation

- **Design tradeoffs:**
  - Evaluator size vs cost: Smaller evaluators benefit more from CaSE but may need ensembles
  - Step-level vs sample-level filtering: Sample-level yields larger gains but discards more data
  - Aspect strictness: Over-filtering for relevance may reject valid exploratory reasoning

- **Failure signatures:**
  - Low agreement with human judgments on complex tasks
  - No improvement on saturated models (already at ceiling)
  - Excessive data filtering (>90% discarded)

- **First 3 experiments:**
  1. Validate CaSE on your domain: Replicate Table 1 methodology with 50-100 traces and human expert labeling
  2. Calibrate filtering thresholds: Test step-level vs sample-level filtering at different CaSE score thresholds
  3. Ablate aspect contributions: Train SFT models with relevance-only, coherence-only, and both aspects filtering

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies on small expert-annotated datasets without full statistical power analysis reported
- Effect sizes are modest for larger models, suggesting diminishing returns for well-calibrated evaluators
- SFT experiments don't isolate whether relevance, coherence, or their combination drives the gains

## Confidence

- **High confidence:** CaSE prevents hindsight bias (Table 1 validation, clear mechanism)
- **Medium confidence:** Relevance and coherence provide complementary signals beyond correctness (Figure 3 correlation, but weak corpus support)
- **Medium confidence:** Aspect-guided inference and data curation improve performance (Figure 5, 9 show consistent gains but lack ablations)

## Next Checks

1. **Replicate CaSE validation on your domain:** Sample 50-100 reasoning traces from your target domain, have human experts label relevance/coherence, and measure CaSE vs Best-of-N agreement using GPT-4o or Qwen2.5-72B as evaluator.

2. **Calibrate filtering thresholds:** Test step-level vs sample-level filtering at different CaSE score thresholds (0.5, 0.7, 0.9) on a held-out validation set to optimize the data retention vs quality tradeoff for your specific use case.

3. **Ablate aspect contributions:** Train three separate SFT models with (a) relevance-only filtering, (b) coherence-only filtering, and (c) both aspects—measure downstream accuracy to verify whether both dimensions contribute independently or if one dominates the improvement.