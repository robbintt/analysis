---
ver: rpa2
title: Sparse Autoencoders Make Audio Foundation Models more Explainable
arxiv_id: '2509.24793'
source_url: https://arxiv.org/abs/2509.24793
tags:
- layer
- sparsity
- audio
- representations
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies sparse autoencoders (SAEs) to analyze representations\
  \ from four audio foundation models\u2014AST, WavLM, HuBERT, and MERT\u2014in the\
  \ context of singing technique classification. SAEs with sparsity constraints project\
  \ dense audio representations into sparse codes, aiming to reveal interpretable\
  \ acoustic factors."
---

# Sparse Autoencoders Make Audio Foundation Models more Explainable

## Quick Facts
- **arXiv ID**: 2509.24793
- **Source URL**: https://arxiv.org/abs/2509.24793
- **Reference count**: 0
- **Primary result**: SAEs significantly improve completeness—fewer dimensions are needed to predict generative factors—while maintaining informativeness in audio foundation models.

## Executive Summary
This paper applies sparse autoencoders (SAEs) to analyze representations from four audio foundation models—AST, WavLM, HuBERT, and MERT—in the context of singing technique classification. SAEs with sparsity constraints project dense audio representations into sparse codes, aiming to reveal interpretable acoustic factors. The authors first select informative layers via linear probing and then train SAEs at sparsity levels from 75% to 99%. While reconstruction quality decreases with higher sparsity, classification accuracy remains robust until very high sparsity levels, indicating preserved task-relevant information. SAEs significantly improve completeness—fewer dimensions are needed to predict generative factors—while maintaining informativeness. Pitch-related information emerges in early layers, while formant and phoneme-related information concentrates in deeper layers. Overall, SAEs provide a promising approach for improving the interpretability of audio foundation models by disentangling underlying acoustic factors.

## Method Summary
The paper uses SAEs to project dense audio representations into sparse codes for interpretability analysis. The authors first identify informative layers via linear probing on four audio foundation models (AST, WavLM, HuBERT, MERT) using the VocalSet dataset for singing technique classification. They then train TopK SAEs (N=2048 dimensions) at varying sparsity levels (75-99%) on these selected layers. To evaluate interpretability, they extract acoustic factors using OpenSMILE's eGeMAPS features and train Lasso regressors to predict these factors from the SAE latent codes. The disentanglement metrics—informativeness (R² via Lasso) and completeness (dimensions required to predict a factor)—are computed to assess how well SAEs disentangle underlying acoustic factors.

## Key Results
- SAEs significantly improve completeness while maintaining informativeness across all tested models and sparsity levels
- Classification accuracy remains robust until very high sparsity levels (>95%), indicating preserved task-relevant information
- Pitch information is encoded in early layers (WavLM layer 1, HuBERT layer 3) while formant and phoneme information concentrates in deeper layers (layer 12)
- Reconstruction MSE increases with sparsity but task performance degrades only at extreme sparsity levels (>95%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TopK sparsity constraints force dense representations into interpretable sparse codes where fewer active dimensions encode task-relevant information.
- Mechanism: The TopK operator retains only the k largest activations from the ReLU-activated encoder output, creating a hard sparsity constraint. This forces the model to distribute information across specialized dimensions rather than spreading it densely.
- Core assumption: Interpretable features correspond to sparsely-activating dimensions in an overcomplete basis (N=2048 > D=768).
- Evidence anchors:
  - [abstract] "SAEs with sparsity constraints project dense audio representations into sparse codes, aiming to reveal interpretable acoustic factors."
  - [section 3.2] "The operator TopK(·) retains only the k largest activations, with k acting as a sparsity-control hyperparameter."
  - [corpus] Related work on LLMs (Cunningham et al., Gao et al.) shows similar TopK SAE mechanisms extract interpretable features from language models.
- Break condition: If reconstruction error grows faster than completeness improves, the sparsity level has exceeded the intrinsic dimensionality of the task-relevant information.

### Mechanism 2
- Claim: Completeness (fewer dimensions needed to predict a factor) increases with sparsity while informativeness remains stable, indicating concentrated rather than dispersed feature encoding.
- Mechanism: Lasso regression from SAE dimensions to acoustic factors reveals that high sparsity forces predictive information into fewer units. The L1 penalty on the regression identifies which sparse dimensions are most predictive.
- Core assumption: Acoustic factors from eGeMAPS (pitch, formants, MFCCs) are valid ground-truth descriptors for singing voice characteristics.
- Evidence anchors:
  - [section 4.3.1] "Completeness increases with sparsity... when informativeness remains constant, but fewer active dimensions are available, the representation must concentrate the predictive information into fewer units."
  - [figure 2] Shows R² (informativeness) stable across sparsity levels while completeness increases.
  - [corpus] DCI framework (Eastwood & Williams) established completeness/informativeness as disentanglement metrics; this paper applies them to audio SSL.
- Break condition: If completeness increases but informativeness drops sharply, the SAE has discarded factor-relevant information rather than concentrated it.

### Mechanism 3
- Claim: Audio foundation models encode acoustic features hierarchically—pitch in early layers, formants/phonemes in deeper layers—and SAEs preserve this structure while making it more accessible.
- Mechanism: Linear probing across layers identifies where task information peaks (AST: layers 6-12, WavLM/HuBERT: early layers, MERT: layers 4-7). SAEs trained on these layers then reveal which acoustic factors are best predicted at each depth.
- Core assumption: Layer-wise probing accuracy reflects the linear accessibility of task-relevant features at each processing stage.
- Evidence anchors:
  - [section 4.3.2] "Pitch information is accurately predicted by the first layers (1 and 3, respectively) while formants information emerges in the final layer (12)."
  - [table 1] Shows different models peak at different layers for singing technique classification.
  - [corpus] Pasad et al. (2023) and Zhou et al. (2025) previously established hierarchical encoding in speech/music SSL models; this work confirms SAEs preserve it.
- Break condition: If probing accuracy is uniform across layers, the model lacks hierarchical feature organization and layer selection becomes arbitrary.

## Foundational Learning

- Concept: **Sparse Autoencoders (SAEs)**
  - Why needed here: SAEs are the core intervention—without understanding how TopK constraints create sparse codes, you cannot interpret the disentanglement results.
  - Quick check question: Given a 768-dim input and 2048-dim hidden layer with 95% sparsity, how many dimensions are active?

- Concept: **Linear Probing**
  - Why needed here: Layer selection depends on probing accuracy; this determines where SAEs are trained and how feature hierarchy is inferred.
  - Quick check question: If a linear probe on layer 3 achieves 73% accuracy and layer 12 achieves 60%, which layer should you prioritize for SAE analysis if you hypothesize phonetic information is task-relevant?

- Concept: **Disentanglement Metrics (Informativeness vs. Completeness)**
  - Why needed here: The paper's key claim is that SAEs improve completeness without sacrificing informativeness—understanding these metrics is essential to interpret the results.
  - Quick check question: If a factor has R²=0.95 but completeness=0.3, what does this tell you about how it is encoded in the representation?

## Architecture Onboarding

- Component map:
  ```
  Audio Input → Foundation Model (AST/WavLM/HuBERT/MERT)
           → Layer Representations (D=768, 13 layers)
           → Temporal Mean Pooling
           → TopK SAE Encoder (N=2048, sparsity 75-99%)
           → Sparse Code z
           → Linear Decoder (reconstruction) OR Linear Probe (classification) OR Lasso Regression (factor prediction)
  ```

- Critical path:
  1. Run linear probing on all 13 layers to identify task-informative layers (target: >70% accuracy).
  2. Train TopK SAEs at multiple sparsity levels (75%, 80%, 85%, 90%, 95%, 99%) on selected layers.
  3. Extract acoustic factors using OpenSMILE eGeMAPS on the same audio.
  4. Fit Lasso regressions from sparse codes to each factor; compute R² and completeness.
  5. Compare SAE vs. original representations on both metrics.

- Design tradeoffs:
  - Higher sparsity → better completeness but higher reconstruction MSE; task accuracy drops sharply beyond 95%.
  - Layer selection: Early layers capture pitch (WavLM layer 1, HuBERT layer 3); deeper layers capture formants/phonemes but may lose task accuracy for some models.
  - Expansion factor (N/D = 2048/768 ≈ 2.67): Larger expansion may improve interpretability but increases compute.

- Failure signatures:
  - Reconstruction MSE spikes (>10× baseline) before 90% sparsity: encoder/decoder capacity insufficient.
  - Probing accuracy on SAE codes drops >10% from original representations: task-relevant information was suppressed.
  - Completeness does not improve with sparsity: factors may be inherently high-entropy (requires more dimensions).

- First 3 experiments:
  1. **Layer sweep probe**: Train linear classifiers on each layer's pooled representations for your target task; identify the 2 layers with highest accuracy (early and late) for comparison.
  2. **Sparsity calibration**: Train SAEs at 85%, 90%, 95% sparsity on one selected layer; plot reconstruction MSE vs. probing accuracy to find the knee point where task information begins to degrade.
  3. **Factor discovery**: Extract 10-20 interpretable acoustic features relevant to your task (e.g., pitch, energy, spectral centroid); fit Lasso from SAE codes to each feature and report which factors achieve R² > 0.7 and completeness > 0.8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of temporal structure analysis, rather than average pooling, impact the disentanglement and completeness of factors learned by SAEs in audio models?
- Basis in paper: [explicit] The authors state in the conclusion, "we plan to... integrate the analysis of the temporal structure of the representations, which was discarded in this work via average pooling."
- Why unresolved: Current methodology flattens representations across time (average pooling), removing the ability to analyze how acoustic factors evolve or are disentangled over time in sequential audio.
- What evidence would resolve it: An architecture that processes temporal sequences of hidden states and reports disentanglement metrics for time-varying features like vibrato rate or rhythmic patterns.

### Open Question 2
- Question: Do the improvements in completeness and robustness to sparsity generalize to audio domains outside of singing voice, such as environmental sounds or standard speech?
- Basis in paper: [inferred] The experimental scope is restricted to the VocalSet dataset (singing techniques), despite the authors introducing the models as general "audio foundation models" for speech and sound events.
- Why unresolved: It is unclear if the specific acoustic properties of singing (e.g., sustained vowels, distinct formants) drive the observed disentanglement success, or if the method is robust to noisier, out-of-domain data.
- What evidence would resolve it: Replicating the probing and SAE training protocol on datasets like AudioSet (sound events) or LibriSpeech (speech) to compare completeness scores.

### Open Question 3
- Question: Does the reliance on linear probing and the eGeMAPS feature set cause the model to miss interpretable factors that are non-linearly encoded or absent from standard acoustic descriptors?
- Basis in paper: [inferred] The paper evaluates disentanglement solely by predicting eGeMAPS features using Lasso regression (linear), which assumes the relevant factors are both linearly accessible and within the eGeMAPS definition.
- Why unresolved: Foundation models may encode high-level abstractions (e.g., singer identity, room acoustics) that are not linearly decodable or represented in the 88 eGeMAPS variables, limiting the scope of interpretability.
- What evidence would resolve it: An analysis using non-linear probes or unsupervised clustering on SAE activations to identify features that lack a correlation with the pre-defined eGeMAPS factors.

## Limitations
- The exact computation of the completeness metric remains unclear despite referencing the DCI framework
- Layer selection methodology assumes linear probe accuracy directly reflects feature accessibility, which may not capture nonlinear relationships
- The fixed expansion ratio (N=2048 vs D=768) and sparsity schedule may not generalize across different model architectures or tasks

## Confidence
- **High confidence**: SAEs improve completeness while maintaining informativeness (directly measured in experiments)
- **Medium confidence**: Hierarchical organization of acoustic features (consistent with prior speech SSL literature but not extensively validated here)
- **Medium confidence**: Task-relevant information preserved up to 95% sparsity (but sharp drop at 99% needs more analysis)

## Next Checks
1. **Recompute completeness** using both DCI's original formula and an alternative normalization to verify the reported improvements are robust to metric specification
2. **Layer hierarchy validation** by training nonlinear probes (small MLPs) on the same layers to check if SAEs capture the same feature organization
3. **Sparsity sensitivity analysis** with finer-grained sparsity levels (88%, 91%, 93%, 96%) to better characterize the sharp accuracy drop between 95-99%