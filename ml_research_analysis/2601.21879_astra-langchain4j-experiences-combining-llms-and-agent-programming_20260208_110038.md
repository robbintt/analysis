---
ver: rpa2
title: 'astra-langchain4j: Experiences Combining LLMs and Agent Programming'
arxiv_id: '2601.21879'
source_url: https://arxiv.org/abs/2601.21879
tags:
- agent
- llms
- example
- listing
- astra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a prototype library, astra-langchain4j, for
  integrating large language models (LLMs) with the ASTRA agent programming language.
  The authors explore how traditional agent toolkits can benefit from LLMs and vice
  versa through three example implementations: a travel planner using FIPA Request
  Protocol, Tic-Tac-Toe players with varying LLM strategies, and Towerworld block
  building.'
---

# astra-langchain4j: Experiences Combining LLMs and Agent Programming

## Quick Facts
- arXiv ID: 2601.21879
- Source URL: https://arxiv.org/abs/2601.21879
- Authors: Rem Collier; Katharine Beaumont; Andrei Ciortea
- Reference count: 19
- Primary result: astra-langchain4j enables LLM integration with ASTRA agent programming language, but reveals LLMs struggle with contextual decision-making and complex reasoning tasks

## Executive Summary
This paper presents a prototype library, astra-langchain4j, for integrating large language models with the ASTRA agent programming language. Through three example implementations—a travel planner using FIPA Request Protocol, Tic-Tac-Toe players with varying LLM strategies, and Towerworld block building—the authors explore how traditional agent toolkits can benefit from LLMs and vice versa. Their experiences reveal that while LLMs can be easily integrated into agent toolkits and some Agentic AI workflows can be realized through existing multi-agent system technologies, LLMs show poor performance in contextual decision-making and complex reasoning tasks. The authors found that prompting remains challenging and that LLM-generated plans were inconsistent and often inferior to simple non-LLM approaches.

## Method Summary
The paper describes a proof-of-concept library integrating LLMs (OpenAI, Google Gemini) with the ASTRA agent programming language. The method involves extending base agent classes (OpenAIAgent, GeminiAgent), using templater modules for prompt and response templates, and implementing BeliefRAG for context injection from agent beliefs. Three examples were implemented: a travel planner using FIPA Request Protocol for multi-agent orchestration, Tic-Tac-Toe players with various LLM strategies, and Towerworld block building tasks. The evaluation methodology is described as exploratory rather than statistically rigorous, with limited details on exact prompts, evaluation counts, and statistical methods.

## Key Results
- LLMs can be easily integrated into agent toolkits through templated prompts and response parsing
- Some Agentic AI workflows (evaluator-optimizer patterns) can be realized within single agents using existing multi-agent system technologies
- LLMs show poor performance in contextual decision-making and complex reasoning tasks, with inconsistent and often inferior results compared to simple non-LLM approaches

## Why This Works (Mechanism)

### Mechanism 1: BeliefRAG for Context Injection
Agent belief bases serve as structured knowledge sources for retrieval-augmented generation without external vector stores. The BelRAG module queries agent beliefs at prompt generation time, maps predicate arguments to template placeholders, and concatenates generated sentences into the prompt context.

### Mechanism 2: Template-Based Response Parsing with Binding Inference
Structured response templates enable reliable extraction of LLM outputs into agent-consumable variables without brittle regex parsing. ResponseTemplate defines expected output patterns; templater.inferBindings matches LLM reply against the pattern and extracts variable bindings.

### Mechanism 3: FIPA Request Protocol for Multi-Agent Orchestration
Standardized agent communication protocols (FIPA Request) can implement agentic workflow patterns like round-robin group chat without custom coordination infrastructure. Orchestrator agent sends sequential request messages to specialist agents using request/agree/inform performatives.

## Foundational Learning

- **AgentSpeak(L) / BDI Architecture**
  - Why needed here: ASTRA is an AgentSpeak(L) derivative; understanding beliefs, desires, intentions, plans, and event queues is essential for reading agent code and extending modules
  - Quick check question: Can you explain how a plan rule `+!goal : condition { body }` is triggered and executed in the BDI reasoning cycle?

- **FIPA ACL Communication Primitives**
  - Why needed here: The travel planner example uses `request`, `agree`, `inform` performatives; incorrect protocol usage breaks multi-agent coordination
  - Quick check question: What is the difference between a FIPA `request` and `query` performative, and when would you use each?

- **Prompt Template Variable Binding**
  - Why needed here: The library's `PromptTemplate` and `ResponseTemplate` classes use `${variable}` syntax; understanding binding scope and reset behavior is critical for correct prompt construction
  - Quick check question: If you reuse a `PromptTemplate` without calling `reset()`, what happens to previous variable bindings?

## Architecture Onboarding

- **Component map:** OpenAIAgent/GeminiAgent -> Templater module (PromptTemplate, ResponseTemplate, CompositeTemplate) -> BelRAG module -> Domain-specific agent classes (RoundRobin, Assistant, AbstractPlayer)

- **Critical path:** 1) Extend OpenAIAgent or GeminiAgent for LLM access, 2) Create PromptTemplate with `${variable}` placeholders, 3) Bind variables via `templater.addBinding(template, key, value)`, 4) Call `model.chat(template, string reply)`, 5) Parse reply using ResponseTemplate and `templater.inferBindings()`, 6) Use extracted values in agent plans/actions

- **Design tradeoffs:** Single-agent vs. multi-agent workflows (many patterns can be implemented within one agent via plans); BeliefRAG vs. external RAG (simpler but limited to structured beliefs); Temperature 0 vs. higher values (deterministic vs. creative tasks)

- **Failure signatures:** LLM suggests invalid moves (prompt fails to convey constraints); Response template binding returns null (LLM output format diverges from template); Inconsistent performance across runs (LLM non-determinism or model version changes); Towerworld generates wrong tower plan (LLM fails to generalize beyond training-like configurations)

- **First 3 experiments:** 1) Implement minimal agent extending OpenAIAgent that sends simple prompt and prints response, 2) Build BeliefRAG-powered agent with 5-10 beliefs and query LLM for task prioritization, 3) Reproduce Tic-Tac-Toe linear player and verify it consistently beats naive LLM player

## Open Questions the Paper Calls Out

### Open Question 1
Can improved prompting strategies or architectural modifications enable LLMs to make reliable, consistent decisions in game-playing and planning contexts? The authors found LLMs are not good at contextual decision making and prompting remains a dark art with small changes having big impacts. This remains unresolved because multiple prompting strategies were tested but all produced inconsistent results; LLMs failed to detect winning/losing positions and could not reliably force draws against simple linear players.

### Open Question 2
Under what conditions do multi-agent architectures provide meaningful benefits over single-agent implementations for Agentic AI workflows? The authors observe that not all workflows require multiple agents as most can be implemented through simple plans within a single agent. This remains unresolved because the Travel Planner successfully used multi-agent orchestration while the Reflective Player implemented an expert-assessor workflow within a single agent, raising questions about when distribution is genuinely advantageous.

### Open Question 3
How can traditional agent-oriented software engineering principles be leveraged to address LLM limitations in reasoning and planning? The paper demonstrates integration of LLMs with BDI-style agent architectures but reveals that LLMs struggle with complex reasoning, suggesting untapped potential for hybrid symbolic-neural approaches. This remains unresolved because while the library enables LLM calls within ASTRA agents, the reasoning limitations persist and the authors note limited existing work on how Generative AI can benefit from those toolkits and their rich experience base.

## Limitations
- Code listings are described as "sketches" with potential omissions requiring substantial reconstruction
- Evaluation methodology lacks statistical rigor with unknown number of game episodes and statistical methods
- No systematic exploration of prompt engineering variations that might improve LLM performance

## Confidence
- Confidence is Medium in the core claim that LLMs can be integrated into agent toolkits
- Confidence is Low in the performance claims about LLM inadequacy due to lack of statistical rigor and controlled ablation studies
- Major uncertainties include incomplete code listings, unknown statistical methodology, potential selection bias, and no systematic exploration of prompt engineering variations

## Next Checks
1. Implement complete reproduction of all three examples (Travel Planner, Tic-Tac-Toe, Towerworld) from sketch listings and run controlled experiments comparing LLM-based agents against non-LLM baselines with minimum 30 games per comparison

2. Conduct systematic prompt engineering ablation by varying prompt templates, context injection methods, and temperature settings for each example to determine whether poor LLM performance is inherent or due to suboptimal prompting

3. Perform BeliefRAG scalability test by measuring performance degradation as belief base size increases from 10 to 1000 predicates, documenting when pagination or filtering becomes necessary and how this affects LLM response quality