---
ver: rpa2
title: 'Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs
  for Knowledge-Based Causal Discovery'
arxiv_id: '2506.08771'
source_url: https://arxiv.org/abs/2506.08771
tags:
- subgraph
- causal
- subgraphs
- ranker
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel approach for integrating Knowledge
  Graphs (KGs) with Large Language Models (LLMs) to enhance knowledge-based causal
  discovery. The core method identifies informative metapath-based subgraphs within
  KGs and refines their selection using Learning-to-Rank-based models, which are then
  incorporated into zero-shot prompts to improve LLMs' effectiveness in inferring
  causal relationships.
---

# Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery

## Quick Facts
- arXiv ID: 2506.08771
- Source URL: https://arxiv.org/abs/2506.08771
- Reference count: 40
- Introduces novel method combining KGs with LLMs for causal discovery

## Executive Summary
This paper presents a novel approach that integrates Knowledge Graphs with Large Language Models to enhance knowledge-based causal discovery. The method identifies informative metapath-based subgraphs within KGs and refines their selection using Learning-to-Rank models, which are then incorporated into zero-shot prompts to improve LLMs' effectiveness in inferring causal relationships. Experiments on biomedical and open-domain datasets demonstrate significant performance improvements, with F1 scores increasing by up to 44.4 points compared to baselines.

## Method Summary
The approach combines Knowledge Graphs (KGs) with Large Language Models (LLMs) for causal discovery by first identifying informative metapath-based subgraphs within KGs. These subgraphs are then refined using Learning-to-Rank-based models to select the most relevant paths. The refined subgraphs are incorporated into zero-shot prompts designed to enhance the LLM's ability to infer causal relationships from the structured knowledge. This integration aims to leverage the structured reasoning capabilities of KGs with the generative power of LLMs for improved causal inference.

## Key Results
- Achieved up to 44.4 F1 point improvement over baseline methods
- Consistently outperformed most baselines across diverse LLMs and KGs
- Demonstrated effectiveness on both biomedical and open-domain datasets

## Why This Works (Mechanism)
The method works by leveraging the structured information in Knowledge Graphs through metapath-based subgraph identification, which captures meaningful relationships between entities. The Learning-to-Rank component ensures that only the most informative paths are selected, reducing noise and focusing the LLM's attention on relevant causal evidence. By incorporating these refined subgraphs into zero-shot prompts, the LLM can access structured knowledge in a format that enhances its causal reasoning capabilities, bridging the gap between symbolic knowledge representation and neural reasoning.

## Foundational Learning

**Knowledge Graphs**: Structured representations of entities and their relationships, essential for organizing domain knowledge in a machine-readable format. Quick check: Verify that the KG contains relevant causal relationships and entities for the target domain.

**Metapaths**: Sequences of relations that define specific types of paths between entities in a KG, crucial for identifying meaningful patterns of relationships. Quick check: Ensure metapaths capture relevant causal pathways rather than arbitrary connections.

**Learning-to-Rank**: Machine learning approach for ranking items based on their relevance, necessary for selecting the most informative subgraphs from potentially thousands of candidates. Quick check: Validate that the ranking model effectively distinguishes between causally relevant and irrelevant subgraphs.

**Zero-shot prompting**: Technique for guiding LLM behavior without fine-tuning, important for leveraging pre-trained models while incorporating domain-specific knowledge. Quick check: Confirm that prompts effectively communicate the causal discovery task and available evidence.

## Architecture Onboarding

**Component Map**: Knowledge Graph -> Metapath-based Subgraph Extraction -> Learning-to-Rank Model -> Zero-shot Prompt Engineering -> LLM Causal Inference

**Critical Path**: The most critical sequence is the flow from metapath-based subgraph extraction through Learning-to-Rank refinement to prompt engineering, as errors at any stage directly impact the quality of causal inference.

**Design Tradeoffs**: The approach trades computational complexity (multiple processing stages) for improved causal discovery accuracy. An alternative would be direct KG-to-LLM transfer without metapath refinement, but this would likely result in lower precision due to noise.

**Failure Signatures**: Poor performance may indicate: 1) inadequate metapath definitions that miss causal pathways, 2) suboptimal Learning-to-Rank models that select irrelevant subgraphs, or 3) ineffective prompt engineering that fails to communicate causal relationships to the LLM.

**First Experiments**:
1. Validate metapath extraction on a small KG subset to ensure causal pathways are captured
2. Test Learning-to-Rank model independently on pre-extracted subgraphs
3. Evaluate prompt effectiveness with a simple LLM before full pipeline integration

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation primarily focused on biomedical and open-domain datasets, potentially limiting generalizability to other domains
- Lacks comprehensive ablation studies to isolate the specific contribution of Learning-to-Rank versus metapath selection
- Claims of cross-LLM and cross-KG generalizability need more rigorous validation on structurally diverse KGs

## Confidence
- Performance claims: Medium
- Generalizability claims: Medium
- Domain transfer claims: Medium

## Next Checks
1. **Domain Transferability Test**: Evaluate the approach on specialized domains (e.g., financial, legal, or social science causal discovery) that differ significantly from the biomedical and open-domain datasets used in the paper.

2. **Ablation Study**: Conduct a systematic ablation study to quantify the individual contributions of metapath-based subgraph selection, Learning-to-Rank refinement, and zero-shot prompting to the overall performance improvement.

3. **Scalability Assessment**: Test the approach on large-scale KGs (e.g., Wikidata, Freebase) with millions of nodes to evaluate computational efficiency and performance stability at scale.