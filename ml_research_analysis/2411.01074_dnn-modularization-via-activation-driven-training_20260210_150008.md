---
ver: rpa2
title: DNN Modularization via Activation-Driven Training
arxiv_id: '2411.01074'
source_url: https://arxiv.org/abs/2411.01074
tags:
- moda
- class
- training
- modules
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MODA is a novel DNN modularization approach that promotes modularity\
  \ during training by directly regulating activation outputs rather than using external\
  \ masks. It applies three modularization objectives\u2014intra-class affinity, inter-class\
  \ dispersion, and compactness\u2014to refine activation patterns in each layer,\
  \ yielding compact, accuracy-preserving modules."
---

# DNN Modularization via Activation-Driven Training

## Quick Facts
- arXiv ID: 2411.01074
- Source URL: https://arxiv.org/abs/2411.01074
- Reference count: 40
- Key outcome: MODA achieves modules with up to 24x fewer weights and 37x less overlap while preserving original accuracy without retraining

## Executive Summary
MODA is a novel DNN modularization approach that promotes modularity during training by directly regulating activation outputs rather than using external masks. It applies three modularization objectives—intra-class affinity, inter-class dispersion, and compactness—to refine activation patterns in each layer, yielding compact, accuracy-preserving modules. Compared to state-of-the-art methods, MODA achieves modules with up to 24x fewer weights and 37x less overlap while preserving original accuracy without retraining. It requires 22% less training time than mask-based approaches and improves target class accuracy by 12% on average in replacement scenarios with minimal impact on other classes.

## Method Summary
MODA trains DNNs with three additional modularization objectives: intra-class affinity (maximizing cosine similarity of same-class activations), inter-class dispersion (minimizing cosine similarity between different classes), and compactness (L1 regularization on activations). After training, modules are extracted by thresholding neuron activation frequencies per class (τ=0.9). The approach requires no external masks, works with standard DNN architectures, and achieves module decomposition through activation pattern manipulation rather than architectural modifications.

## Key Results
- Modules with up to 24x fewer weights and 37x less overlap compared to state-of-the-art
- Preserved original accuracy without requiring retraining for module extraction
- 22% faster training time than mask-based approaches
- 12% average improvement in target class accuracy during replacement scenarios

## Why This Works (Mechanism)

### Mechanism 1: Intra-Class Affinity Regularization
Aligning activation patterns within the same class creates consistent sub-networks that can be extracted as modules. MODA minimizes affinity loss by maximizing cosine similarity between activation vectors of samples from the same class, encouraging the model to recruit similar neurons for predicting any sample of a given class.

### Mechanism 2: Inter-Class Dispersion via Orthogonality Pressure
Pushing activation patterns of different classes toward orthogonality reduces weight sharing across modules. MODA minimizes dispersion loss, computed as 1 minus average cosine similarity between activation vectors of different classes. Near-orthogonal activations mean neurons specialized for one class have near-zero activation for others.

### Mechanism 3: Compactness via L1-Induced Activation Sparsity
L1 regularization on activations suppresses marginally-contributing neurons, yielding smaller, less overlapping modules. Compactness loss applies L1-norm to activation vectors, pushing near-zero activations to exactly zero. Unlike pruning, this is class-conditional—a neuron suppressed for class A may remain active for class B.

## Foundational Learning

- **Concept: Cosine Similarity for Activation Vectors**
  - Why needed here: MODA's affinity and dispersion losses rely on measuring directional similarity between activation patterns
  - Quick check question: Given two activation vectors [3, 0, 1] and [6, 0, 2], what is their cosine similarity? (Answer: 1.0—same direction, different magnitude)

- **Concept: L1 vs. L2 Regularization Effects**
  - Why needed here: Compactness uses L1-norm for sparsity. L1 drives coefficients to exact zero; L2 shrinks but rarely zeros
  - Quick check question: Why would L2-regularized activations not yield sparse, modularizable networks? (Answer: L2 produces many small but non-zero activations, obscuring neuron-class assignments)

- **Concept: Threshold-Based Decomposition (τ)**
  - Why needed here: After training, MODA uses activation frequency threshold τ (default 0.9) to decide which neurons belong to which module
  - Quick check question: If τ=0.95 yields smaller modules but τ=0.8 yields higher accuracy, which should you choose for a safety-critical application? (Answer: τ=0.8 for accuracy; size matters less than reliability)

## Architecture Onboarding

- **Component map:** Standard DNN + three modular loss terms (Laff, Ldis, Lcom) added to cross-entropy, computed per-layer from activation vectors → Post-training module extraction using activation frequency over training samples → Modules composed into sub-task models or integrated with calibration layer

- **Critical path:** Hyperparameter selection (α, β, γ, τ) → Modular training (200 epochs) → Decomposition with τ → Composed model evaluation

- **Design tradeoffs:**
  - Higher τ → smaller, more disjoint modules but risk of accuracy loss
  - Higher γ (compactness weight) → more sparsity but may over-suppress critical neurons
  - MODA avoids mask parameters (vs. MwT) but requires careful loss balancing

- **Failure signatures:**
  - Reuse accuracy drops significantly (>5%) from full model: likely τ too high or γ too aggressive
  - Modules still have high overlap (>20%): inter-class dispersion not effective; check β and class separability
  - Training diverges: modular losses dominating cross-entropy; reduce α, β, γ

- **First 3 experiments:**
  1. Replicate VGG16-CIFAR10 baseline with default hyperparameters (α=1.0, β=1.0, γ=0.3, τ=0.9); verify reuse accuracy within 1% of ST (Section 5.1 reports 94.5% vs. 95.2%)
  2. Ablate compactness (set γ=0) on same setup; compare module size and overlap to Table 3 to confirm Lcom contribution
  3. Test module replacement: train weak LeNet5 on 10% data, replace one class module from VGG16; verify accuracy improvement per Table 1 methodology (expect ~10% gain for target class)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can MODA be generalized to sequential architectures like RNNs and Transformers? The authors state they have "evaluated MODA primarily on CNN models" and "leave" the application to RNNs and Transformers, which process sequential data across timesteps, "for future work."

- **Open Question 2:** How does MODA perform with non-ReLU activation functions, such as Leaky ReLU, GELU, or Sigmoid? The paper notes that to date, the approach has focused on the commonly-used ReLU, but "Our future work will extend to other activation functions."

- **Open Question 3:** Can MODA modules be effectively reused to add new functionalities to a different model without full retraining? The authors identify that modules could be "reused to add new functionalities to a different model" as an "exciting potential application," but acknowledge it "may require further adaptation strategies."

## Limitations
- Scalability concerns with pairwise similarity computations across large batch sizes
- Orthogonality pressure may conflict with necessary feature reuse in fine-grained classification
- L1 compactness may suppress neurons with small but critical cross-class contributions

## Confidence

- **High Confidence**: Claims about reduced module overlap (37x less than MwT) and preserved accuracy without retraining are supported by direct experimental comparisons in Tables 2 and 3
- **Medium Confidence**: The mechanism of using cosine similarity for activation alignment is theoretically sound, but the practical implementation details for large-scale training remain unclear
- **Medium Confidence**: The claim of 22% faster training time compared to mask-based approaches is based on ablation studies, but the exact comparison methodology is not fully specified

## Next Checks

1. **Scalability Test**: Implement the pairwise similarity calculation on a small batch (32 samples) and measure memory usage and computation time. Scale up to batch size 128 to verify if the approach remains practical.

2. **Fine-Grained Classification Test**: Apply MODA to a dataset with visually similar classes (e.g., Stanford Dogs) to evaluate whether the orthogonality pressure conflicts with necessary feature reuse and causes accuracy degradation.

3. **Critical Neuron Suppression Test**: Systematically ablate neurons identified as "suppressed" by the compactness loss and measure their individual contribution to accuracy across different classes to quantify the risk of over-suppression.