---
ver: rpa2
title: Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing
  Its OCR Proficiency
arxiv_id: '2507.08309'
source_url: https://arxiv.org/abs/2507.08309
tags:
- text
- dimt
- image
- mllm
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving document image
  machine translation (DIMT) performance in multimodal large language models (MLLMs)
  while preventing catastrophic forgetting of their optical character recognition
  (OCR) capabilities. The proposed method, Synchronously Self-Reviewing (SSR), fine-tunes
  MLLMs by first generating OCR text in the source language before producing translation
  text in the target language.
---

# Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency

## Quick Facts
- arXiv ID: 2507.08309
- Source URL: https://arxiv.org/abs/2507.08309
- Reference count: 35
- Qwen2-VL achieves BLEU scores of 57.23 on in-domain tests and 41.91 on cross-domain tests

## Executive Summary
This paper introduces Synchronously Self-Reviewing (SSR), a novel fine-tuning method for multimodal large language models (MLLMs) that improves document image machine translation (DIMT) while preventing catastrophic forgetting of optical character recognition (OCR) capabilities. The approach leverages the model's strong monolingual OCR proficiency by generating OCR text in the source language before producing translation text in the target language. Experiments on DoTA and DITrans datasets demonstrate significant improvements in DIMT performance while maintaining OCR accuracy and enabling cross-lingual visual question answering capabilities.

## Method Summary
The proposed Synchronously Self-Reviewing (SSR) method fine-tunes MLLMs through a two-stage generation process. First, the model generates OCR text in the source language from document images, leveraging its existing OCR proficiency. Then, it produces translation text in the target language based on the generated OCR text. This synchronous approach prevents catastrophic forgetting by maintaining the model's OCR capabilities while learning the cross-lingual translation task. The method uses a 7B Qwen2-VL model as the base, fine-tuned with carefully designed training data that includes diverse document images with bilingual annotations.

## Key Results
- SSR achieves BLEU scores of 57.23 on in-domain DoTA tests and 41.91 on cross-domain tests
- Maintains OCR accuracy at 85.18 CA while improving translation performance
- Enables cross-lingual VQA capabilities without additional fine-tuning
- Demonstrates superior generalization across domains and languages compared to traditional supervised fine-tuning

## Why This Works (Mechanism)
The mechanism works by exploiting the MLLM's existing strong monolingual OCR proficiency before introducing translation tasks. By generating OCR text in the source language first, the model can focus on its well-established visual-language understanding capabilities without interference from cross-lingual complexities. The subsequent translation step then builds on this reliable OCR output, allowing the model to learn translation in a more controlled manner. This synchronous approach prevents the interference that typically occurs when directly training for translation, which can degrade OCR performance.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): Why needed - form the foundation for document understanding; Quick check - must handle both visual and textual inputs
- Optical Character Recognition (OCR): Why needed - extracts text from document images; Quick check - accuracy measured by Character Accuracy (CA)
- BLEU Score: Why needed - standard metric for translation quality evaluation; Quick check - higher scores indicate better translation accuracy
- Document Image Machine Translation (DIMT): Why needed - enables translation of text within document images; Quick check - requires both OCR and translation capabilities
- Catastrophic Forgetting: Why needed - understanding how models lose previously learned capabilities; Quick check - must preserve OCR while learning translation
- Cross-lingual Visual Question Answering: Why needed - demonstrates broader multimodal understanding; Quick check - ability to answer questions about images in different languages

## Architecture Onboarding

Component Map:
Document Image -> OCR Generation -> Translation Generation -> Final Output

Critical Path:
Image input → Visual encoder → OCR text generation (source language) → Translation text generation (target language) → Output

Design Tradeoffs:
- OCR first vs. direct translation: OCR-first approach preserves existing capabilities but adds computational overhead
- Synchronous vs. sequential training: Synchronous training prevents forgetting but requires more complex training data preparation
- Model size vs. performance: 7B model balances capability with computational efficiency

Failure Signatures:
- OCR accuracy degradation indicates catastrophic forgetting
- Translation quality drops when OCR generation fails
- Cross-lingual VQA performance degradation suggests interference between tasks

First 3 Experiments:
1. Compare SSR against traditional supervised fine-tuning on DoTA benchmark
2. Evaluate OCR accuracy preservation before and after fine-tuning
3. Test cross-lingual VQA capabilities on multilingual document images

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to only two datasets (DoTA and DITrans)
- No evaluation on established benchmarks like OCRBench or public document datasets
- Cross-lingual VQA results lack detailed performance analysis across different language pairs
- Limited ablation studies on varying OCR quality thresholds and self-reviewing strategies

## Confidence
High: BLEU score improvements on DoTA benchmark (57.23 in-domain)
Medium: OCR accuracy preservation claims (85.18 CA)
Medium: Cross-lingual VQA capability demonstration
Low: Generalization claims to unseen domains and languages

## Next Checks
1. Test model performance on established document understanding benchmarks like OCRBench and real-world document collections to validate generalization claims
2. Conduct systematic ablation study varying OCR quality threshold and self-reviewing frequency to understand impact on translation accuracy
3. Evaluate cross-lingual VQA capabilities with detailed error analysis across multiple language pairs to identify failure modes and performance bottlenecks