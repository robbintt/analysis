---
ver: rpa2
title: 'Thought of Search: Planning with Language Models Through The Lens of Efficiency'
arxiv_id: '2404.11833'
source_url: https://arxiv.org/abs/2404.11833
tags:
- state
- block
- word
- states
- successor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the gap in analyzing soundness, completeness,
  and efficiency of LLM-based planning methods. It proposes using LLMs to generate
  code for search components (successor function and goal test) rather than calling
  LLMs at every search step.
---

# Thought of Search: Planning with Language Models Through The Lens of Efficiency

## Quick Facts
- arXiv ID: 2404.11833
- Source URL: https://arxiv.org/abs/2404.11833
- Reference count: 40
- Primary result: LLM-based planning method achieving 100% accuracy with dramatically improved efficiency (seconds vs days, $1000+ vs minimal cost)

## Executive Summary
This paper addresses a critical gap in the evaluation of LLM-based planning methods by analyzing soundness, completeness, and efficiency through a novel approach. Rather than calling LLMs at every search step, the method uses LLMs to generate code for search components (successor function and goal test), which are then executed by standard search algorithms. The approach is evaluated across four diverse domains including 24 Game, Mini Crosswords, BlocksWorld, and PrOntoQA, demonstrating both theoretical guarantees and practical efficiency gains.

The results show the method can solve entire datasets in seconds with minimal cost, compared to existing approaches that take days and incur costs exceeding $1000. The paper provides a comprehensive framework for evaluating LLM-based planners, filling a significant gap in the literature where most existing work focuses solely on accuracy metrics without considering fundamental computational properties.

## Method Summary
The proposed method transforms LLM-based planning by generating executable code for search components rather than using LLMs as direct planners. The process involves using LLMs to create successor functions and goal test predicates, which are then integrated into standard search algorithms. This architecture allows the method to inherit soundness and completeness guarantees from established search algorithms while dramatically reducing the number of LLM calls required. The approach is evaluated across four domains with varying complexity, demonstrating both theoretical rigor and practical efficiency improvements over existing LLM-based planning methods.

## Key Results
- Achieved 100% accuracy across all four evaluated domains (24 Game, Mini Crosswords, BlocksWorld, PrOntoQA)
- Solved entire datasets in seconds versus days required by existing methods
- Reduced costs from over $1000 to minimal levels through dramatically fewer LLM calls
- Maintained soundness and completeness guarantees inherited from standard search algorithms

## Why This Works (Mechanism)
The efficiency gains stem from fundamentally changing how LLMs interact with the planning process. By generating code for search components once rather than calling LLMs at every search step, the method reduces computational overhead while maintaining the benefits of LLM reasoning. The approach leverages the strength of LLMs in understanding and generating structured code while using efficient, well-understood search algorithms for execution. This separation of reasoning (LLM) from execution (standard algorithms) creates a more efficient workflow that scales better than direct LLM planning approaches.

## Foundational Learning
- **Search algorithm soundness**: Why needed - to ensure the method provides correct solutions; Quick check - verify solutions match expected outputs
- **Completeness in planning**: Why needed - to guarantee the method can find solutions when they exist; Quick check - test on problems with known solutions
- **LLM code generation**: Why needed - core mechanism for creating executable search components; Quick check - validate generated code produces expected behavior
- **Cost analysis in LLM applications**: Why needed - to evaluate practical viability beyond accuracy; Quick check - compare token usage and API costs across methods
- **Domain representation**: Why needed - to properly structure problems for LLM understanding; Quick check - test with varying levels of domain specificity
- **Success predicate formulation**: Why needed - critical for defining problem goals; Quick check - verify goal tests correctly identify valid solutions

## Architecture Onboarding

**Component Map**: Problem Specification -> LLM Code Generation -> Search Algorithm -> Solution

**Critical Path**: The critical execution path involves generating the successor function and goal test via LLM, then running a standard search algorithm (BFS, DFS, or A*) using these components to find solutions.

**Design Tradeoffs**: The method trades off the flexibility of direct LLM planning for the efficiency and guarantees of standard search algorithms. This creates better scalability but requires accurate code generation. The approach assumes LLMs can reliably generate correct code for search components, which may not hold for all problem types.

**Failure Signatures**: Potential failures include incorrect code generation leading to unsound results, search algorithm limitations in complex spaces, and domain specification errors that confuse the LLM. The method may also struggle with problems requiring significant exploration where the initial code generation becomes a bottleneck.

**First Experiments**: 
1. Run the method on a simple BlocksWorld problem to verify basic functionality
2. Test code generation accuracy by validating the successor function on known states
3. Compare execution time and cost against direct LLM planning on a small 24 Game instance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four specific domains, unclear how well approach generalizes to complex real-world planning problems
- Doesn't address potential brittleness in code generation step where errors could compromise soundness guarantees
- Claims of 100% accuracy based on relatively small and specialized problem sets
- Efficiency improvements may not scale linearly to larger problems with exponentially growing search spaces

## Confidence
- Efficiency improvements: Medium confidence - impressive but may not scale to larger problems
- 100% accuracy: High confidence - but needs context of limited problem set
- Soundness and completeness guarantees: High confidence - inherited from standard algorithms
- Code generation reliability: Medium confidence - not thoroughly analyzed for failure modes

## Next Checks
1. Test the approach on larger, more complex planning domains to verify if efficiency gains hold as problem complexity increases
2. Implement systematic error checking for generated code to validate the soundness guarantees under imperfect code generation
3. Compare against a broader range of existing LLM-based planners, including those that might use different prompting strategies or architectural approaches