---
ver: rpa2
title: 'OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open
  Knowledge Benchmarking'
arxiv_id: '2511.08598'
source_url: https://arxiv.org/abs/2511.08598
tags:
- question
- article
- knowledge
- questions
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OKBench, a fully automated framework for generating
  high-quality, dynamic knowledge benchmarks on demand, addressing the problem of
  static benchmarks becoming outdated and contaminated with pretraining data. The
  core method involves an agentic pipeline that automates news extraction, QA generation,
  validation, and dataset versioning, producing daily benchmarks from fresh news articles.
---

# OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking

## Quick Facts
- arXiv ID: 2511.08598
- Source URL: https://arxiv.org/abs/2511.08598
- Reference count: 40
- Primary result: Fully automated pipeline generates daily QA benchmarks from fresh news, showing retrieval augmentation narrows performance gaps between small and large models

## Executive Summary
OKBench introduces a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand from fresh news articles. The system addresses the problem of static benchmarks becoming outdated and contaminated with pretraining data by creating daily benchmarks that exploit the temporal gap between pretraining cutoffs and current events. Human validation confirms high quality, with 92% clarity and 100% correctness on sampled questions. Evaluation on diverse LLMs shows retrieval augmentation significantly improves performance and narrows the gap between model sizes, highlighting the importance of robust retrieval pipelines over model scale for evolving knowledge.

## Method Summary
OKBench is an agentic pipeline that automates the sourcing, creation, validation, and distribution of benchmarks from raw news articles. The four-stage process includes: (1) RSS-based news extraction from 22 sources within a 24-hour window, (2) LLM-based QA generation targeting factual, time-sensitive questions, (3) validation agent filtering ambiguous or article-referencing questions, and (4) cryptographic versioning for reproducibility. The system costs approximately $4.21 per day to generate ~2,350 questions, using GPT-4.1 for both generation and validation. Benchmarks are evaluated under No-Context (parametric knowledge only), Oracle-Context (gold article provided), and Retrieval settings using BM25, DPR, and ColBERT v2 retrievers.

## Key Results
- Human validation shows 92% clarity and 100% correctness on sampled questions
- Retrieval augmentation narrows performance gap between small and large models from 20-25% to near-parity
- BM25 outperforms dense retrievers (DPR, ColBERT v2) on news due to strong lexical cues
- 3-4B parameter models achieve 90-95% accuracy with oracle context, indicating reading comprehension saturation at moderate scale

## Why This Works (Mechanism)

### Mechanism 1
An agentic pipeline can automate the entire benchmark creation process from raw news to validated QA pairs at low cost (~$4.21/day). Four-stage orchestration: (1) RSS-based news extraction with metadata preservation, (2) LLM-based QA generation targeting factual, time-sensitive questions, (3) validation agent filtering ambiguous or article-referencing questions, (4) cryptographic versioning (MD5 hash + model signature + timestamp) for reproducibility. The validation agent specifically enforces standalone questions without "according to the article" phrasing. Core assumption: LLMs can reliably generate and validate factual QA pairs with minimal human oversight; quality is sufficient for evaluation even with ~8% question rejection rate. Evidence: Agentic framework automation, pipeline details, $4.21 daily cost, YourBench similarity. Break condition: If validation agent's pass rate drops significantly or generation costs spike due to API changes.

### Mechanism 2
Fresh news articles (within 24 hours) provide contamination-free evaluation by exploiting the temporal gap between pretraining cutoffs and current events. Questions are generated from articles published within the last 24 hours, targeting entity-specific facts unlikely to exist in pretraining corpora. This creates a "no-context" setting where models must rely purely on parametric knowledge, exposing true knowledge cutoffs rather than memorized benchmark answers. Core assumption: Models cannot extrapolate or reason about truly novel facts without external context; any above-random performance reflects partial knowledge from ongoing stories covered before the cutoff. Evidence: Contamination reduction claim, no-context performance observation, motivation about evolving knowledge, no direct corpus validation. Break condition: If models develop strong temporal reasoning or if news cycles slow significantly.

### Mechanism 3
Retrieval augmentation compresses the performance gap between small and large models from 20-25% (no-context) to near-parity on fresh knowledge. When provided relevant context (oracle article or retrieved passages), models above ~3-4B parameters achieve 90-95% accuracy regardless of size. This suggests reading comprehension saturates at moderate scale, while parametric knowledge continues scaling. BM25 outperforms dense retrievers on news due to strong lexical cues. Core assumption: Reading comprehension and parametric knowledge are partially decoupled capabilities; dense retrievers trained on MS MARCO/NQ suffer domain shift on news. Evidence: Model size threshold observation, improvement gap comparison, BM25 superiority finding, no direct corpus corroboration. Break condition: If dense retrievers are fine-tuned on news domains, or if model architectures improve context integration for sub-3B models.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The paper's core evaluation distinguishes parametric (no-context) vs. retrieval-augmented performance, showing they reveal different model capabilities.
  - Quick check question: If a model scores 95% with oracle context but 30% without, what does this imply about its knowledge vs. reasoning?

- **Concept: Benchmark Contamination**
  - Why needed here: OKBench's primary motivation is avoiding contamination where models memorize benchmark answers during pretraining, inflating performance estimates.
  - Quick check question: Why does temporal freshness reduce contamination risk compared to static Wikipedia-based benchmarks?

- **Concept: Lexical vs. Dense Retrieval**
  - Why needed here: The paper finds BM25 outperforms DPR/ColBERT v2 on news, contrary to trends on static benchmarks—domain matters for retriever selection.
  - Quick check question: What document features might make BM25 stronger on breaking news compared to academic QA corpora?

## Architecture Onboarding

- **Component map:** RSS Feeds (22 sources) → Article Parser → [QA Generation Agent (GPT-4.1)] → [Validation Agent (GPT-4.1)] → Versioning (MD5 + timestamp + model sig) → Evaluation Harness (No-Context / Oracle / Retrieval) → Retrievers: BM25, DPR (FAISS), ColBERT v2

- **Critical path:** QA Generation → Validation. The validation agent is the quality gate; if it passes ambiguous questions, downstream evaluation validity degrades. Human validation showed 92% clarity pass rate but only 66% on combined clarity + freshness.

- **Design tradeoffs:**
  - **Proprietary LLM dependence** vs. reproducibility: Version signatures help, but API drift or access changes could break regeneration.
  - **Automation** vs. quality: Fully automated enables daily updates and democratization, but accepts ~8-34% question quality issues per human checks.
  - **News-only domain** vs. breadth: Focuses temporal evaluation but excludes multilingual, paywalled, and specialized sources.

- **Failure signatures:**
  - Questions referencing "the article" directly (validation agent failed)
  - Ambiguous timing ("recently" without dates)
  - Questions answerable from prior knowledge (freshness violation)
  -