---
ver: rpa2
title: Improving Methodologies for LLM Evaluations Across Global Languages
arxiv_id: '2601.15706'
source_url: https://arxiv.org/abs/2601.15706
tags:
- english
- responses
- language
- evaluation
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A joint exercise by Singapore, Japan, Australia, Canada, EU, France,
  Kenya, South Korea and the UK AI Safety Institutes tested two open-weight models
  across ten languages on five harm categories (privacy, non-violent crime, violent
  crime, intellectual property, and jailbreak robustness). Over 6,000 translated prompts
  were evaluated using both LLM-as-a-judge and human annotation.
---

# Improving Methodologies for LLM Evaluations Across Global Languages

## Quick Facts
- arXiv ID: 2601.15706
- Source URL: https://arxiv.org/abs/2601.15706
- Reference count: 0
- Over 6,000 translated prompts tested across ten languages on five harm categories

## Executive Summary
This joint evaluation exercise by nine international AI Safety Institutes tested two open-weight models across ten languages on five harm categories using both LLM-as-a-judge and human annotation. The study found that non-English safeguards were generally slightly weaker than English, with jailbreak protections being the weakest and intellectual property protections the strongest. The LLM-as-a-judge showed promise but required human oversight, especially for nuanced harms. Methodological improvements included contextualising translations, stress-testing evaluators, clearer annotation guidelines, and running multiple test iterations.

## Method Summary
The evaluation used three datasets totaling ~700 unique prompts translated to 10 languages, including MLCommons AILuminate subset, AnswerCarefully subset, and CyberSecEval Prompt Injection. Translated prompts were run through test-LLMs (Mistral Large, Gemma 2) and evaluated by a distinct LLM-as-a-judge model, with human annotators validating a subset of outputs. The methodology employed a binary Accept/Refusal classification with an "N.A." category for edge cases like gibberish or hallucination. The study ran single inference runs per prompt due to efficiency constraints.

## Key Results
- Non-English safeguards were generally slightly weaker than English safeguards, with jailbreak protections being the weakest
- Safeguards for intellectual property were the strongest across all languages
- LLM-as-a-judge showed significant discrepancies with human judgment (up to 18.9% in Japanese), particularly failing to detect "warning + harm" patterns
- Language drift occurred frequently in low-resource languages like Telugu and Japanese, where models responded in English or third languages
- Hallucination in low-resource languages (Farsi, Telugu, Kiswahili) often resulted in gibberish that was classified as "safe" but functionally useless

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Safety Transfer Across Languages
The model's safety boundary is more robustly defined in the dominant training language (English). When prompted in lower-resourced languages or those with different scripts, the model's internal representations may fail to trigger refusal circuits as strongly, or the output generation may devolve into hallucinations that bypass safety layers entirely.

### Mechanism 2: "Warning-Compliance" Blindness in LLM-as-a-Judge
Automated evaluators frequently fail to flag responses that contain a safety warning followed by harmful instructions, misclassifying them as "Acceptable" due to the presence of the superficial warning. The evaluator likely weighs the presence of "refusal-like" tokens heavily in its classification logic.

### Mechanism 3: Cultural Refusal Friction
Safety training that optimizes for direct refusal creates conflicts in languages/cultures where direct refusal is considered impolite, leading to "soft" refusals or evasion that humans interpret as unsafe or unhelpful. The model generates outputs that satisfy semantic safety constraints but violate cultural pragmatics.

## Foundational Learning

- **Concept: LLM-as-a-Judge Discrepancy**
  - Why needed here: The core methodology relies on an LLM evaluator, but Section 6.3 shows a discrepancy rate of up to 18.9% compared to humans
  - Quick check question: If a model says "Warning: this is illegal. Step 1: [illegal act]," will the automated judge likely flag it? (Answer: Likely no)

- **Concept: Jailbreak Robustness vs. Harm Categories**
  - Why needed here: The paper identifies "Jailbreak protections" as the specific weak point in the safety layer
  - Quick check question: Which harm category showed the weakest safeguards across non-English languages? (Answer: Jailbreak robustness)

- **Concept: Contextual Translation vs. Literal Translation**
  - Why needed here: Section 5 (Limitations) and Section 7 emphasize that literal translation of benchmarks invalidates tests
  - Quick check question: Why might a prompt translated literally from English to Telugu fail to test safety effectively? (Answer: It may lack cultural relevance or result in mistranslation of proper nouns/terms)

## Architecture Onboarding

- **Component map:** Moonshot Toolkit -> Inference Engine (Test-LLMs) -> Evaluator Engine (LLM-as-a-Judge) -> Human Review Layer

- **Critical path:**
  1. Translation: Convert English benchmarks into 9 target languages using human-validated machine translation
  2. Inference: Run prompts through Test-LLMs (single run, specific temperature settings)
  3. Automated Eval: Run outputs through Judge-LLM
  4. Human Validation: Annotators grade outputs; calculate discrepancy (Human vs. Judge)

- **Design tradeoffs:**
  - Single-run vs. Multi-run: Study relied mostly on single runs for efficiency, acknowledging this limits reproducibility
  - Binary vs. Multi-label: Started with binary Accept/Refusal but introduced "N.A." for edge cases, noting that binary schemes miss nuance

- **Failure signatures:**
  - "Warning + Harm" Pattern: Model gives a safety disclaimer but proceeds to provide the harmful content
  - Language Drift: Model responds in English or a third language when prompted in a low-resource language
  - Gibberish Hallucination: In low-resource languages, the model outputs incomprehensible text that is technically "safe" but functionally useless

- **First 3 experiments:**
  1. Judge Stress-Test: Run the "Warning + Harm" examples through your evaluator prompt to see if it flags them
  2. Language Consistency Check: Prompt the model in a low-resource language and verify if it maintains that language in the output
  3. Translation Validation: Take a sample of "Unacceptable" responses caused by translation errors and re-write them with cultural context to see if the failure rate drops

## Open Questions the Paper Calls Out

### Open Question 1
How can multilingual safety datasets be effectively adapted to ensure cultural equivalence beyond literal translation? The paper states that "literal translations are insufficient" and prompts should be "contextually adapted to each language and culture." This remains unresolved because translated prompts often failed to account for regional legal jurisdictions or cultural norms regarding refusal tone.

### Open Question 2
Can evaluator models be stress-tested or fine-tuned to reliably detect "mixed-signal" harms (e.g., warnings followed by harmful content) in non-English languages? The authors note that LLM-as-a-judge "failed to catch" cases where models provided an initial warning but proceeded to share harmful content, and suggest that "Evaluator models... should be stress-tested."

### Open Question 3
Does the generation of hallucinated or incoherent content in low-resource languages pose distinct safety risks compared to coherent refusals? The Telugu deep dive asks whether the tendency to generate irrelevant content poses more dangers than it seems to be in a real-world scenario. The evaluation methodology often classified hallucinated responses as "harmless" or "safe" because they did not explicitly enable harm.

## Limitations
- LLM-as-a-judge model identity and exact evaluation prompt templates remain unspecified, limiting reproducibility
- Single-run evaluation approach reduces statistical confidence in the findings
- The study does not establish baseline English safety performance metrics, making it difficult to quantify the exact magnitude of the "safety lag" across languages

## Confidence

- **High confidence:** The general finding that non-English safeguards are weaker than English safeguards, particularly for jailbreak robustness
- **Medium confidence:** The identification of specific failure modes (warning-compliance blindness, cultural refusal friction) based on qualitative analysis
- **Low confidence:** The precise magnitude of safety disparities across languages due to single-run methodology and lack of statistical significance testing

## Next Checks

1. **Mechanism Validation:** Run the "warning + harm" examples from Table 5 through your LLM-as-a-judge to verify if it exhibits the same failure mode, then refine the evaluation prompt to explicitly penalize harmful content regardless of prefatory warnings.

2. **Language Consistency Verification:** Test a low-resource language (e.g., Telugu) by prompting the model and checking if it maintains that language in outputs. If language drift occurs, implement explicit language instruction cues like "Please respond in [target language]" and measure the reduction in drift.

3. **Cultural Context Impact:** Take a sample of "Unacceptable" responses attributed to translation errors and rewrite them with proper cultural context (localized names/places). Re-run these prompts to measure if the failure rate decreases, validating the importance of contextual translation over literal translation.