---
ver: rpa2
title: 'COLE: a Comprehensive Benchmark for French Language Understanding Evaluation'
arxiv_id: '2510.05046'
source_url: https://arxiv.org/abs/2510.05046
tags:
- task
- french
- sentence
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The COLE benchmark addresses the need for a comprehensive evaluation
  of French language understanding by introducing 23 diverse tasks covering sentiment
  analysis, paraphrase detection, grammatical judgment, and reasoning. It benchmarks
  95 large language models using a zero-shot evaluation setup and identifies significant
  performance gaps between closed- and open-weights models.
---

# COLE: a Comprehensive Benchmark for French Language Understanding Evaluation

## Quick Facts
- **arXiv ID:** 2510.05046
- **Source URL:** https://arxiv.org/abs/2510.05046
- **Reference count:** 40
- **Primary result:** COLE evaluates 95 LLMs on 23 French NLU tasks, revealing a 21-point gap between closed- and open-weights models

## Executive Summary
COLE introduces a comprehensive benchmark for French language understanding evaluation, featuring 23 diverse tasks across sentiment analysis, paraphrase detection, grammatical judgment, and reasoning. The benchmark evaluates 95 large language models in a zero-shot setting, providing a systematic comparison between closed- and open-weights models. Results show significant performance disparities, with closed-weights models dominating the rankings while highlighting challenging frontiers in zero-shot French NLU.

## Method Summary
The benchmark employs a zero-shot evaluation approach where models are prompted with task-specific templates without any fine-tuning. Each of the 23 tasks uses standardized metrics (Accuracy, Exact Match, or F1) with scores normalized to [0,1]. The Composite Score aggregates performance across all tasks by computing an unweighted mean and scaling to [0,100]. Tasks cover sentiment analysis, paraphrase detection, grammatical acceptability, natural language inference, extractive question-answering, word sense disambiguation, and regional language understanding. Models must either select from predefined labels or generate text responses according to the task format.

## Key Results
- Closed-weights models achieved a maximum Composite Score of 70.12% versus 49.14% for the best open-weights model
- Zero-shot extractive question-answering and fine-grained word sense disambiguation emerged as particularly challenging tasks
- Models demonstrated strong performance on grammatical judgment tasks and coarse-grained semantic understanding
- Regional language understanding tasks (QFrCoRE/QFrCoRT) proved especially difficult for open-weights models, often near random baseline

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of French NLU capabilities through diverse task types and its standardized zero-shot evaluation protocol. By using consistent prompt templates and metrics across all tasks, COLE enables fair comparisons between different model families and weights accessibility. The zero-shot approach reveals models' inherent language understanding capabilities without the confounding factor of task-specific fine-tuning, providing insights into their general reasoning abilities in French.

## Foundational Learning
- **Zero-shot prompting**: Why needed - Enables evaluation without task-specific training; Quick check - Verify prompt templates produce expected output format
- **Composite Score calculation**: Why needed - Provides single metric for overall performance; Quick check - Confirm unweighted averaging across 23 tasks
- **Metric selection (Accuracy/EM/F1)**: Why needed - Task-appropriate evaluation measures; Quick check - Validate metric implementation matches task requirements
- **Prompt engineering**: Why needed - Critical for zero-shot performance; Quick check - Test prompts with sample inputs to ensure correct label generation
- **Cross-lingual evaluation**: Why needed - Addresses gap in French NLU assessment; Quick check - Confirm French language handling across all tasks
- **Model API integration**: Why needed - Enables evaluation of both closed and open models; Quick check - Test API calls with simple inference

## Architecture Onboarding
**Component map:** Data loading -> Prompt formatting -> Model inference -> Output post-processing -> Metric calculation -> Composite aggregation

**Critical path:** Prompt template → Model API call → Output parsing → Metric computation

**Design tradeoffs:** Zero-shot evaluation trades task-specific optimization for generalizability assessment; comprehensive task coverage trades depth for breadth

**Failure signatures:** 
- Near-random performance indicates prompt formatting issues or model incapability
- Task-specific failures (e.g., 0% EM in QA) suggest output generation problems
- Consistent underperformance in open-weights models indicates data/compute limitations

**3 first experiments:**
1. Validate prompt templates with one task using sample inputs
2. Test Composite Score calculation with synthetic per-task scores
3. Run baseline random selection to confirm expected performance ranges

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot evaluation may underestimate model capabilities compared to fine-tuned approaches
- Performance gaps between closed- and open-weights models may reflect access disparities rather than inherent quality differences
- The benchmark focuses on French, limiting generalizability to other languages

## Confidence
- **Benchmark construction**: High - follows established multilingual NLU evaluation practices
- **Exact numerical results**: Medium - pending verification of API access and decoding parameters
- **Performance gap claims**: High - consistent with literature on closed-vs-open model disparities

## Next Checks
1. Implement and test the exact prompt templates from Figure 2 with at least one open-weights model to verify task formatting and output consistency
2. Run a small-scale validation on one or two tasks (e.g., grammatical judgment) to confirm metric calculations and Composite Score aggregation logic
3. Verify the zero-shot baseline performance (random selection) yields approximately 1/3 score on 3-way classification tasks and 1/2 on binary tasks