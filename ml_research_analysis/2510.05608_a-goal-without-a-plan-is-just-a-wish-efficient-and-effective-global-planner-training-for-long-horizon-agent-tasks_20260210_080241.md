---
ver: rpa2
title: 'A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner
  Training for Long-Horizon Agent Tasks'
arxiv_id: '2510.05608'
source_url: https://arxiv.org/abs/2510.05608
tags:
- task
- plan
- executor
- global
- cabinet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EAGLET, a framework for improving global
  planning in large language model-based agents. It addresses the challenge of brainless
  trial-and-error and hallucinatory actions in long-horizon tasks by training a plug-and-play
  global planner.
---

# A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks

## Quick Facts
- arXiv ID: 2510.05608
- Source URL: https://arxiv.org/abs/2510.05608
- Authors: Shuzheng Si; Haozhe Zhao; Kangyang Luo; Gang Chen; Fanchao Qi; Minjia Zhang; Baobao Chang; Maosong Sun
- Reference count: 40
- This paper introduces EAGLET, a framework for improving global planning in large language model-based agents, achieving state-of-the-art performance while reducing training costs by 8x compared to RL-based baselines.

## Executive Summary
This paper addresses the challenge of brainless trial-and-error and hallucinatory actions in long-horizon tasks by training a plug-and-play global planner for LLM-based agents. The authors propose EAGLET, a framework that synthesizes high-quality plans using homologous consensus filtering from an advanced LLM, followed by supervised fine-tuning and rule-based reinforcement learning. The method achieves superior performance on three long-horizon tasks while dramatically reducing training costs compared to traditional RL approaches, without requiring manual effort or additional training data.

## Method Summary
EAGLET introduces a novel approach to global planning for LLM-based agents by leveraging homologous consensus filtering to synthesize high-quality plans from an advanced LLM. The framework then employs supervised fine-tuning on these synthesized plans, followed by rule-based reinforcement learning with an executor capability gain reward. This approach enables the training of a plug-and-play global planner that can effectively guide agents through long-horizon tasks without the computational overhead of traditional RL methods. The method is designed to address the common issues of trial-and-error exploration and hallucinatory actions that plague existing LLM-based agents in complex task environments.

## Key Results
- EAGLET outperforms existing methods on three long-horizon tasks (API usage, Minecraft, and BabyAI)
- Achieves state-of-the-art performance while reducing training costs by 8x compared to RL-based baselines
- Demonstrates effectiveness without requiring manual effort or extra training data

## Why This Works (Mechanism)
The framework's success stems from its ability to leverage the planning capabilities of advanced LLMs while avoiding their computational costs during deployment. By synthesizing high-quality plans through homologous consensus filtering, EAGLET creates a distilled representation of expert knowledge that can be efficiently learned by a smaller, specialized planner. The combination of supervised fine-tuning and rule-based reinforcement learning allows the system to adapt these plans to the specific capabilities of the executor agent, resulting in more effective task completion without the extensive trial-and-error exploration required by traditional RL approaches.

## Foundational Learning
- **Homologous Consensus Filtering**: A method for synthesizing high-quality plans by finding agreement across multiple plan variations. Why needed: To extract reliable plan structures from advanced LLMs while filtering out inconsistencies. Quick check: Verify that consensus filtering consistently produces more coherent plans than individual LLM outputs.
- **Supervised Fine-Tuning (SFT)**: Training a model on labeled examples using cross-entropy loss. Why needed: To efficiently learn the distilled plan knowledge from synthesized examples. Quick check: Compare SFT performance against few-shot prompting on the same plan data.
- **Rule-Based Reinforcement Learning**: A RL variant using predefined rules rather than learned rewards. Why needed: To guide planner learning without expensive reward engineering. Quick check: Measure improvement in executor capability before and after RL fine-tuning.

## Architecture Onboarding

### Component Map
EAGLET -> Consensus Filtering -> SFT -> Rule-Based RL -> Executor Agent

### Critical Path
1. Advanced LLM generates multiple plan variations
2. Homologous consensus filtering synthesizes high-quality plans
3. Supervised fine-tuning trains initial planner
4. Rule-based RL fine-tunes planner with executor capability rewards
5. Trained planner guides executor agent through long-horizon tasks

### Design Tradeoffs
The framework trades the generality of end-to-end RL for the efficiency of plan distillation and supervised learning. This design choice significantly reduces training costs but may limit adaptability to highly dynamic environments where plans need frequent updating. The reliance on an advanced LLM for plan synthesis introduces a dependency on external models but enables zero-manual-effort training.

### Failure Signatures
- Consensus filtering produces incoherent plans due to LLM inconsistencies
- SFT fails to capture nuanced plan variations, leading to brittle execution
- Rule-based RL rewards don't align with actual executor capabilities
- Trained planner overfits to specific plan structures, failing on task variations
- Executor agent cannot execute synthesized plans due to capability gaps

### Exactly 3 First Experiments
1. Test consensus filtering quality by comparing plan coherence scores between filtered plans and individual LLM outputs
2. Evaluate SFT efficiency by measuring planner performance against few-shot prompting baselines
3. Assess rule-based RL effectiveness by comparing executor capability gains with and without the RL fine-tuning phase

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to three specific long-horizon tasks, raising questions about generalizability
- Reliance on advanced LLM for plan synthesis introduces potential single point of failure
- Claims of zero manual effort may underestimate engineering complexity in implementation

## Confidence

**High confidence:**
- The core methodology of using homologous consensus filtering for plan synthesis is technically sound and well-explained

**Medium confidence:**
- The 8x cost reduction claim is based on controlled experiments but may not fully account for engineering overhead
- Performance improvements are demonstrated but limited to specific benchmark tasks

**Low confidence:**
- Claims about zero manual effort and no extra training data requirements may underestimate implementation complexity

## Next Checks
1. Test EAGLET's performance on a broader range of long-horizon tasks beyond the three presented, including domains with different characteristics and API complexities
2. Conduct a detailed ablation study isolating the contributions of each component (consensus filtering, supervised fine-tuning, and rule-based RL) to verify their individual impacts
3. Perform extensive error analysis on failure cases to understand the framework's limitations and failure modes in complex planning scenarios