---
ver: rpa2
title: Integration Matters for Learning PDEs with Backward SDEs
arxiv_id: '2505.01078'
source_url: https://arxiv.org/abs/2505.01078
tags:
- bsde
- loss
- pinns
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies the primary cause of the performance gap
  between BSDE and PINN methods for solving high-dimensional PDEs. The authors demonstrate
  that the standard Euler-Maruyama (EM) integration scheme introduces a discretization
  bias in one-step self-consistency BSDE losses, shifting the optimization landscape
  away from the true solution.
---

# Integration Matters for Learning PDEs with Backward SDEs

## Quick Facts
- arXiv ID: 2505.01078
- Source URL: https://arxiv.org/abs/2505.01078
- Reference count: 40
- Primary result: Heun-BSDE achieves RL2 errors of 0.0493±0.0109 for 100D HJB vs 0.3626±0.0113 for EM-BSDE

## Executive Summary
This paper identifies a fundamental discretization bias in standard Euler-Maruyama (EM) integration schemes when used with one-step self-consistency losses for BSDE-based PDE solvers. The bias, which persists even as step-size approaches zero, causes the optimization landscape to shift away from the true solution, creating a performance gap between BSDE and PINN methods. The authors propose using Stratonovich-based BSDEs with stochastic Heun integration, which completely eliminates this bias. Empirical results show the proposed method consistently outperforms EM-based variants across multiple high-dimensional benchmarks.

## Method Summary
The method implements BSDE-based PDE solving using Stratonovich calculus with stochastic Heun integration. The forward SDE is integrated using a predictor-corrector approach, while the backward SDE is updated using a corrector step that averages drift and diffusion terms. The loss function is a one-step self-consistency residual comparing the neural network prediction with the integrated backward value. The approach requires float64 precision and uses an 8-layer MLP with Fourier embeddings and skip connections. Training employs Adam optimization with a three-stage learning rate schedule.

## Key Results
- Heun-BSDE achieves RL2 errors of 0.0493±0.0109 for 100D HJB vs 0.3626±0.0113 for EM-BSDE
- Heun-BSDE achieves RL2 errors of 0.0535±0.0113 for 100D BSB vs 0.3735±0.0470 for EM-BSDE
- EM-BSDE error plateaus at high levels regardless of step-size reduction or longer training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The performance gap in standard BSDE solvers is caused by a discretization bias inherent to the Euler-Maruyama (EM) scheme when applied to one-step self-consistency losses.
- Mechanism: EM integration introduces a non-vanishing bias term of order $O(1)$ proportional to $\frac{1}{2}\text{tr}((H \cdot \nabla^2 u)^2)$ into the loss landscape. Because this bias term is the same order as the PDE residual error, optimizing the EM loss shifts the solution away from the true PDE solution, a problem that persists even as step-size $\tau \to 0$.
- Core assumption: The PDE functions $f, g, h$ satisfy regularity conditions ($C^{0,1}$ or $C^{1,1}$), and the neural network ansatz $u_\theta$ is sufficiently smooth ($C^{2,1}$ or $C^{3,1}$).
- Evidence anchors:
  - [Abstract]: "We identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme... shifting the optimization landscape off target."
  - [Section 4.1, Theorem 4.2]: Proves $L_{EM,\tau}(\theta)$ includes a bias term $\frac{1}{2T}\int \text{tr}((H \cdot \nabla^2 u)^2) dt$ that does not vanish as $\tau \to 0$.
  - [Corpus]: Weak direct evidence; related papers focus on acceleration or different formulations rather than the specific EM bias mechanism.
- Break condition: If the PDE solution $u$ is linear (Hessian is zero), the bias term vanishes, and EM may perform adequately.

### Mechanism 2
- Claim: Stratonovich-based BSDEs integrated with the stochastic Heun method eliminate the optimization bias found in EM-based approaches.
- Mechanism: By interpreting the forward and backward SDEs as Stratonovich SDEs, the chain rule avoids the Itô correction term. The stochastic Heun scheme (a predictor-corrector method) converges to the Stratonovich solution. This combination ensures that any additional bias terms only enter at $O(\tau^{1/2})$, which is higher order than the leading residual term, effectively removing the barrier to convergence.
- Core assumption: Assumption of sufficient smoothness ($C^{1,1}$ or $C^{3,1}$) for the drift, diffusion, and network components.
- Evidence anchors:
  - [Abstract]: "We propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration... [which] completely eliminates the bias issues faced by EM integration."
  - [Section 4.2, Lemma 4.3]: Shows $\tau^{-2} \cdot \ell_{Heun,\tau}(\theta, x, t)$ equals the residual squared plus $O(\tau^{1/2})$, lacking the $O(1)$ bias term found in Lemma 4.1.
  - [Corpus]: No direct corpus support for this specific Stratonovich/Heun fix in the context of learning PDEs.
- Break condition: If the computational graph does not support the higher-order derivatives or computational cost of Heun (approx. 2x EM), the mechanism is impractical.

### Mechanism 3
- Claim: Multi-step self-consistency losses (interpolating between one-step and full-horizon) fail to robustly resolve the EM bias because of a trade-off between bias reduction and loss landscape quality.
- Mechanism: While full-horizon ($k=N$) EM losses can approximate the true BSDE loss without the specific bias of one-step losses, the full-horizon loss itself is dominated by the single-step loss (via Jensen's inequality). Thus, reducing bias by increasing horizon length degrades the supervision signal, creating a "pick your poison" scenario that requires careful hyperparameter tuning.
- Core assumption: The optimization dynamics are sensitive to the dominance hierarchy of loss components.
- Evidence anchors:
  - [Section 5]: "Neither of the EM-BSDE losses for k=1 nor k=N provides a completely satisfactory solution... [due to] trade-offs for Multi-Step BSDE Losses."
  - [Section 6.2, Figure 5]: Shows EM-BSDE performance degrades or fluctuates with skip-length, whereas Heun remains stable.
  - [Corpus]: Mentions "interpolating loss" in the abstract but focuses on the proposed Heun solution rather than refining interpolation.
- Break condition: If an optimal intermediate horizon length can be analytically determined for a specific problem, this trade-off might be managed, but the paper suggests this is brittle.

## Foundational Learning

- Concept: **Itô vs. Stratonovich Calculus**
  - Why needed here: The core mechanism relies on switching the interpretation of the SDE. Itô calculus (standard in finance) adds a correction term involving the Hessian during chain rule expansion, which causes the EM bias. Stratonovich calculus uses the standard chain rule, aligning better with Heun integration.
  - Quick check question: Does the standard chain rule apply to the SDE integration, or must I account for the quadratic variation of Brownian motion (Itô correction)?

- Concept: **Strong vs. Weak Convergence of SDE Integrators**
  - Why needed here: The analysis relies on "strong order 1/2 convergence" to prove that the discretization error terms vanish relative to the loss. Understanding that Heun preserves the Stratonovich solution pathwise is crucial.
  - Quick check question: Does the integration scheme converge to the specific path distribution (strong) or just the distribution moments (weak)?

- Concept: **Self-Consistency in BSDEs**
  - Why needed here: The loss function is not a simple residual but a "self-consistency" check along a trajectory. The method trains a network to satisfy the relationship between time points along the simulated SDE path.
  - Quick check question: Is the network learning to match a target value, or is it learning to satisfy a consistency constraint between two time states?

## Architecture Onboarding

- Component map: Neural Network -> Forward SDE Simulator -> Backward SDE Integrator -> Loss Module
- Critical path:
  1. Sample initial state $x_0$ and Brownian path
  2. **Forward Rollout**: Integrate Forward SDE using Heun to get $\hat{X}_{n+1}$
  3. **Backward Integration**: Compute $\hat{Y}_{n+1}$ using the Heun corrector step (requires evaluating network at current and predicted future states)
  4. **Loss Calculation**: Compare network prediction $u_\theta(\hat{X}_{n+1})$ with the integrated backward value $\hat{Y}_{n+1}$

- Design tradeoffs:
  - **Accuracy vs. Speed**: Heun-BSDE is roughly 2x slower than EM-BSDE per step due to the predictor-corrector structure and potential Hessian requirements for some PDE forms (though Stratonovich often avoids Hessian for the diffusion term, the paper notes computational overhead)
  - **Precision vs. Stability**: Heun requires `float64` to avoid accumulation of floating-point errors, whereas EM is more stable in `float32`
  - **Bias vs. Variance**: EM introduces bias; Heun introduces negligible bias but suffers from standard variance which is manageable

- Failure signatures:
  - **EM Stagnation**: Validation error plateaus at a high level ($\sim 0.3$ RL2) regardless of step-size reduction or longer training
  - **Heun NaNs/Instability**: Training diverges if using `float32` or if the PDE dynamics are stiff without adaptive time-stepping
  - **Coupled System Failure**: For fully-coupled FBSDEs (like the BZ problem), standard methods may fail to converge if optimization landscapes are too complex (as seen in 100D BZ results)

- First 3 experiments:
  1. **Ablation on Integrators**: Train on a 10D HJB equation using EM-BSDE vs. Heun-BSDE with identical hyperparameters. Verify that EM loss converges to a non-zero floor while Heun converges to near-zero
  2. **Step-Size Sensitivity**: Train EM-BSDE with varying step sizes ($\tau \in \{10^{-1}, 10^{-2}, 10^{-3}\}$). Confirm that the RL2 error remains roughly constant (proving the bias is step-size independent)
  3. **Skip-Length Analysis**: Plot RL2 error vs. skip-length $k$ for EM-BSDE. Reproduce the "U-shaped" curve where intermediate $k$ performs better than $k=1$ or $k=N$, demonstrating the trade-off identified in Section 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational overhead of Heun-BSDE be reduced while maintaining its bias-elimination properties through Hutchinson trace estimation, reversible Heun integration, or adaptive time-stepping?
- Basis in paper: [explicit] "In future works, we aim to reduce Heun-BSDE's computational costs through methods such as Hutchinson trace estimation, reversible Heun, and adaptive time-stepping."
- Why unresolved: The paper demonstrates Heun-BSDE's effectiveness but notes it is approximately 6x slower than EM-BSDE; these proposed efficiency improvements remain untested.
- What evidence would resolve it: Empirical comparison showing reduced training time with comparable RL2 errors to current Heun-BSDE on the same benchmark problems.

### Open Question 2
- Question: Can numerically stable float32 implementations of Heun-BSDE be developed to eliminate the need for expensive float64 precision?
- Basis in paper: [explicit] "We leave more numerically stable implementations of the Heun solver in float32, such as PDE non-dimensionalization and the use of the reversible Heun solver, to future work."
- Why unresolved: Table 3 shows Heun-BSDE performance degrades significantly in float32 (RL2 0.4587 vs 0.0535 in float64 for 100D BSB), indicating floating point sensitivity.
- What evidence would resolve it: Demonstration of Heun-BSDE achieving comparable RL2 errors in float32 to float64 on high-dimensional benchmarks.

### Open Question 3
- Question: Can the theoretical analysis of Heun-BSDE be extended to fully-coupled FBSDEs where the forward SDE depends on $Y_t$?
- Basis in paper: [explicit] "Another limitation is that we do not consider fully-coupled FS-BSDEs in our analysis (e.g., the Bender & Zhang (BZ) PDE)."
- Why unresolved: All methods failed to converge on the 100D BZ problem, and the theoretical framework excludes coupled dynamics despite their practical importance.
- What evidence would resolve it: Theoretical analysis proving bias-elimination properties for coupled FBSDEs, plus empirical demonstration of improved convergence on coupled problems.

## Limitations
- The analysis focuses on one-step self-consistency losses and doesn't explore alternative loss formulations
- Computational overhead of Heun integration (approximately 2x EM) may limit scalability to higher dimensions
- The requirement for float64 precision introduces memory constraints for large-scale applications

## Confidence
- **High Confidence**: The theoretical analysis of EM bias is rigorous and well-supported by both proof and empirical evidence
- **Medium Confidence**: The claim that multi-step self-consistency losses fail to resolve the bias is supported by experimental evidence but relies on empirical observations
- **Low Confidence**: The assertion that this is the "primary cause" of performance gaps in the literature assumes no other significant factors contribute substantially

## Next Checks
1. **Alternative Loss Formulations**: Test whether modified self-consistency losses (e.g., with weighted terms or regularization) can reduce EM bias without switching to Stratonovich/Heun, isolating the integration scheme as the critical factor

2. **Computational Scaling Study**: Evaluate Heun-BSDE performance on 200D or 500D versions of the benchmark problems to quantify the practical limits of the computational overhead and memory requirements

3. **Adaptive Step-Size Integration**: Implement an adaptive time-stepping version of both EM and Heun to test whether local step-size adaptation can mitigate discretization bias while maintaining computational efficiency