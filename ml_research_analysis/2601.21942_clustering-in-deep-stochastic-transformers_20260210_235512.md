---
ver: rpa2
title: Clustering in Deep Stochastic Transformers
arxiv_id: '2601.21942'
source_url: https://arxiv.org/abs/2601.21942
tags:
- clustering
- dynamics
- deep
- stochastic
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes deep Transformers where randomness arises intrinsically
  from random initialization of value matrices. Under diffusion scaling with RMS normalization,
  the discrete token dynamics converge to an interacting-particle system on the sphere
  driven by common matrix-valued Brownian noise.
---

# Clustering in Deep Stochastic Transformers

## Quick Facts
- arXiv ID: 2601.21942
- Source URL: https://arxiv.org/abs/2601.21942
- Reference count: 40
- Two-token systems show phase transition in clustering behavior depending on interaction strength and token dimension

## Executive Summary
This paper analyzes deep Transformers where randomness arises intrinsically from random initialization of value matrices. Under diffusion scaling with RMS normalization, the discrete token dynamics converge to an interacting-particle system on the sphere driven by common matrix-valued Brownian noise. The analysis shows that initialization noise qualitatively alters clustering behavior: for two tokens, a phase transition occurs depending on the interaction strength and token dimension, making antipodal configurations attracting with positive probability.

Numerical experiments confirm the predicted transition, show antipodal formations persist beyond two tokens, and demonstrate that suppressing the intrinsic noise degrades accuracy. The work provides a theoretical foundation for understanding how initialization noise influences Transformer behavior through stochastic differential equation analysis.

## Method Summary
The authors analyze deep Transformers by modeling the token dynamics under diffusion scaling limits. They show that with RMS normalization, the discrete dynamics converge to an interacting-particle system on the sphere driven by common matrix-valued Brownian noise. The theoretical framework uses stochastic analysis techniques to prove convergence and analyze the limiting behavior, particularly focusing on two-token systems where phase transitions can be rigorously characterized.

## Key Results
- Convergence of discrete token dynamics to interacting-particle system on sphere under diffusion scaling with RMS normalization
- Phase transition in two-token systems: antipodal configurations become attracting with positive probability depending on interaction strength and token dimension
- Numerical experiments confirm predicted transition and show antipodal formations persist beyond two tokens
- Suppressing intrinsic noise degrades accuracy in empirical Transformers

## Why This Works (Mechanism)
The mechanism relies on the interaction between initialization noise and token dynamics. Random initialization of value matrices introduces stochasticity that, under RMS normalization and diffusion scaling, creates an effective Brownian motion on the sphere. This noise interacts with the deterministic attention dynamics to create stable clustering patterns. The phase transition occurs when the interaction strength and token dimension reach a critical ratio, causing antipodal configurations to become stable attractors rather than unstable equilibria.

## Foundational Learning

**Stochastic Differential Equations**
- Why needed: The convergence analysis requires tools from stochastic calculus to handle the random initialization effects
- Quick check: Verify understanding of Ito calculus and diffusion limits

**Interacting Particle Systems**
- Why needed: The limiting behavior is characterized as particles interacting on a sphere
- Quick check: Understand mean-field limits and their application to multi-agent systems

**Diffusion Scaling**
- Why needed: This scaling regime reveals the continuous-time behavior from discrete updates
- Quick check: Can you explain how discrete dynamics converge to continuous SDEs

**RMS Normalization**
- Why needed: This specific normalization is crucial for the theoretical convergence
- Quick check: Compare RMS normalization behavior to other schemes like LayerNorm

## Architecture Onboarding

**Component Map**
Input tokens → Value matrix initialization (random) → Token dynamics with attention → Diffusion scaling → Interacting particle system on sphere → Clustering behavior

**Critical Path**
The critical path is: random initialization → stochastic dynamics → clustering behavior. The random initialization of value matrices is the source of intrinsic noise, which then propagates through the attention mechanism to create the observed clustering patterns.

**Design Tradeoffs**
The paper's analysis assumes RMS normalization and specific scaling regimes. Other normalization schemes or parameter choices might not exhibit the same behavior, representing a tradeoff between theoretical tractability and practical applicability.

**Failure Signatures**
If the assumptions of diffusion scaling or RMS normalization are violated, the theoretical guarantees may not hold. Models using different normalization or operating outside the scaling regime may show different clustering behavior.

**First Experiments**
1. Verify the phase transition in two-token systems across different initialization scales
2. Test clustering behavior under alternative normalization schemes (LayerNorm vs RMS)
3. Measure the correlation between clustering patterns and downstream accuracy across different tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis assumes spherical dynamics and RMS normalization, but empirical Transformers may use different normalization schemes
- The diffusion scaling limit relies on specific parameter regimes that may not hold in practical implementations with finite depth and width
- The connection between clustering behavior and downstream accuracy remains correlational rather than mechanistic

## Confidence

**Major Claim Confidence Labels:**
- Convergence to interacting-particle system: **High** - The mathematical derivation follows established stochastic analysis techniques
- Phase transition in two-token systems: **Medium** - Proven mathematically but sensitive to parameter choices and scaling assumptions
- Antipodal formations persist beyond two tokens: **Medium** - Numerical evidence supports this but lacks theoretical proof
- Suppressing intrinsic noise degrades accuracy: **Low-Medium** - Correlation observed but causation not established

## Next Checks

1. Test the phase transition predictions across different normalization schemes (LayerNorm, BatchNorm) and scaling regimes to assess robustness
2. Conduct ablation studies on full-scale models where initialization noise is systematically controlled while measuring both clustering metrics and task performance
3. Develop a mechanistic understanding of how clustering patterns specifically influence attention weights and gradient flows during training