---
ver: rpa2
title: Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations
  and Models for Table Question Answering
arxiv_id: '2505.14131'
source_url: https://arxiv.org/abs/2505.14131
tags:
- table
- question
- tables
- reasoning
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether table images or text representations
  are more effective for table question answering (TQA) when using large language
  models (LLMs) versus multimodal large language models (MLLMs). The authors create
  a controlled benchmark from six datasets, categorizing instances by question complexity
  (retrieval vs.
---

# Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering

## Quick Facts
- arXiv ID: 2505.14131
- Source URL: https://arxiv.org/abs/2505.14131
- Reference count: 25
- Primary result: FRES achieves 10% exact match improvement over baseline approaches for table question answering

## Executive Summary
This paper investigates whether table images or text representations are more effective for table question answering (TQA) when using large language models (LLMs) versus multimodal large language models (MLLMs). The authors create a controlled benchmark from six datasets, categorizing instances by question complexity (retrieval vs. reasoning) and table size (small vs. big). They evaluate seven open-weight model pairs and find that model size significantly affects the best representation choice: large models perform better with table images, while small models' performance varies by setting. Based on these findings, the authors propose FRES, a feature-based table representation selection method that dynamically chooses the optimal representation.

## Method Summary
The authors evaluate seven MLLM/LLM pairs across 1,600 instances from six datasets, categorizing each instance by table size (small < 2M pixels and 288 tokens; big otherwise) and question complexity (retrieval vs. reasoning). They implement a feature-based selection method (FRES) that chooses between text-only and combined text+image representations based on table size and question type. FRES applies a simple rule: use text-only for big tables or retrieval questions, and use both representations for small tables with reasoning questions. The method is evaluated against baselines using exact match accuracy with statistical significance testing via Wilcoxon signed-rank test.

## Key Results
- Model size is the primary factor determining optimal input representation: large models (72B) consistently perform better with table images, while small models show setting-dependent effectiveness
- Combining both text and image representations works best for small tables with reasoning questions, while text alone suffices for retrieval questions
- FRES achieves a 10% average exact match improvement over baseline approaches while reducing computational efficiency by up to 66% through fewer input tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale determines optimal input representation for TQA tasks.
- Mechanism: Large models (72B parameters) consistently perform better with table images across all settings, while small models (≤12B) require context-dependent representation selection based on table size and question complexity.
- Core assumption: Larger models develop more robust visual reasoning capabilities during training, though the paper does not isolate the specific training factors.
- Evidence anchors:
  - Table 3 shows Qwen-2-72B achieves higher exact match with MLLM(i) than LLM(t) across all four settings (e.g., 46.5 vs 29.2 for Big+Reasoning).
  - Page 4 states "model size indeed plays a role in determining the most effective model-representation combinations."
  - TableReasoner (arxiv 2507.08046) notes LLM-powered approaches face challenges with large tables, supporting the scale-dependency observation.
- Break condition: Models below 12B parameters show no consistent pattern; representation effectiveness becomes highly setting-dependent.

### Mechanism 2
- Claim: Visual encoding robustness degrades with increasing table image resolution.
- Mechanism: MLLMs process images through fixed-resolution visual encoders, causing information loss or fragmentation when table images exceed optimal pixel counts (threshold ~2M pixels based on MMTab averages).
- Core assumption: Visual encoder architecture limitations cause the degradation, though the paper does not ablate encoder components.
- Evidence anchors:
  - Figure 2 shows declining EM scores for MLLM(i) as table pixel count increases across six bins.
  - Page 4 explicitly states "MLLMs struggle with processing large images" and cites Li et al. (2023) on high-resolution challenges.
  - TALENT (arxiv 2510.07098) corroborates that VLMs "often miss fine-grained details unless scaled to very large sizes."
- Break condition: Text representations show stable performance across table sizes; text token count has minimal impact on LLM(t) performance (Figure 5).

### Mechanism 3
- Claim: Question complexity interacts with representation modality through task-aligned training exposure.
- Mechanism: MLLMs leverage dual representations (text+image) more effectively for reasoning questions, potentially because multi-modal training data emphasizes visual reasoning patterns. Retrieval tasks align better with text-only input, matching typical instruction-tuning formats.
- Core assumption: Training data composition drives the modality-task alignment; the paper infers this from performance patterns but does not analyze training corpora.
- Evidence anchors:
  - Page 4 reports combining both representations is most effective for small tables with reasoning questions (p < .05), while text alone suffices for retrieval.
  - Page 4 hypothesizes "MLLMs' reasoning advantages come from training on massive image reasoning data."
  - CRAFT (arxiv 2505.14984) discusses retrieval costs but does not address modality-reasoning interactions, indicating this mechanism requires further validation.
- Break condition: TabFact (simple binary classification) shows no significant representation differences, suggesting complexity thresholds matter.

## Foundational Learning

- Concept: Multi-modal alignment in transformer architectures
  - Why needed here: Understanding how vision encoders project visual features into LLM embedding space explains why image representation effectiveness varies with model scale and table resolution.
  - Quick check question: Can you explain why a fixed-resolution visual encoder might struggle with variable-sized table images?

- Concept: Instruction tuning and task-specific fine-tuning
  - Why needed here: The paper's hypothesis about retrieval tasks favoring text input relies on understanding how instruction-tuning data formats bias model behavior.
  - Quick check question: Why might a model trained primarily on text-based QA tasks perform better with text input for retrieval questions?

- Concept: Exact match evaluation and statistical significance testing (Wilcoxon signed-rank)
  - Why needed here: The paper reports 10% EM improvements and p-values; understanding these metrics is essential for interpreting whether observed differences are meaningful.
  - Quick check question: What does a statistically significant difference (p < .05) in exact match scores tell you about representation effectiveness?

## Architecture Onboarding

- Component map:
  - Input layer: Table serializer (text: Markdown/HTML-like formats) + Table renderer (image: HTML-to-image pipeline with 4 templates)
  - Model pairs: 7 MLLM/LLM pairs sharing decoder weights (e.g., Qwen-2-VL / Qwen-2, Pixtral / Mistral-Nemo)
  - Feature extraction: Question classifier (Qwen-2-72B, 93% accuracy) + Table size thresholds (2M pixels, 288 tokens)
  - Selection logic: FRES rule-based switch based on (table_size, question_complexity) features
  - Evaluation: Exact match scoring with JSON-formatted output parsing

- Critical path:
  1. Ingest table in both formats (text serialization + image rendering)
  2. Extract features (question type via classifier, table size via pixel/token counts)
  3. Apply FRES decision rules:
     - Big table → text-only to MLLM
     - Small table + retrieval → text-only to MLLM
     - Small table + reasoning → text+image to MLLM
  4. Execute inference with selected representation
  5. Parse JSON response for exact match evaluation

- Design tradeoffs:
  - Accuracy vs. efficiency: FRES achieves 10% EM gain but requires pre-classification overhead; alternative is indiscriminate dual-modality input (higher token cost, lower accuracy)
  - Rule-based vs. learned selection: FRES uses hand-crafted thresholds; a learned selector could adapt to edge cases but requires labeled training data
  - Zero-shot vs. fine-tuned: Paper evaluates zero-shot; TableLlaVA (fine-tuned) shows different baseline performance patterns

- Failure signatures:
  - Question classifier errors (19% of FRES failures): Misclassified retrieval as reasoning leads to unnecessary dual-modality processing
  - Table size threshold limitations (7% of failures): Tables near 2M pixel / 288 token boundaries may be misclassified
  - Model capability ceiling (65% of failures): Wrong predictions persist across all representations, indicating fundamental model limitations

- First 3 experiments:
  1. Reproduce FRES decision boundaries: Test the 2M pixel / 288 token thresholds on your target table distribution; adjust if your tables cluster near boundaries.
  2. Validate question classifier on domain-specific queries: The 93% accuracy was measured on WTQ; test on your question types before deployment.
  3. Ablate image rendering templates: The paper uses 4 HTML templates (Figure 3); test whether template choice affects MLLM performance on your table structures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the observed effectiveness of text versus image representations for TQA be generalized to other table-centric tasks, such as table summarization?
- Basis in paper: [explicit] The authors state in the Limitations section that it "could be pertinent to investigate whether our findings are applicable to other table-centric tasks, such as table summarization."
- Why unresolved: The study exclusively focused on Table Question Answering due to the availability of extensive datasets, leaving other tasks unexplored.
- What evidence would resolve it: Evaluating the performance of FRES or similar representation selection strategies on table-to-text generation and summarization benchmarks.

### Open Question 2
- Question: Can more intricate methods leveraging multimodal findings outperform the zero-shot prompting approach used in this study?
- Basis in paper: [explicit] The authors note they "only explore the application of our findings in a simple setting where MLLMs are prompted in a zero-shot manner" and suggest future work "develop more intricate methods."
- Why unresolved: The current work establishes a baseline using zero-shot inference but does not explore fine-tuning or complex architectures that might better exploit the identified modality preferences.
- What evidence would resolve it: Comparing the zero-shot FRES method against fine-tuned models or architectures specifically designed to switch or merge modalities based on table size and question complexity.

### Open Question 3
- Question: How does the potential loss of information during representation conversion (e.g., OCR errors or rendering issues) impact the practical applicability of dynamic representation selection?
- Basis in paper: [explicit] The authors assume the presence of high-quality parallel data (text and images) but note that "in practical applications, the conversion between these representations may result in the loss of information."
- Why unresolved: The benchmark used existing or high-quality rendered images, ignoring the noise introduced by real-world OCR or suboptimal text-to-image rendering.
- What evidence would resolve it: Evaluating FRES on a dataset where the "secondary" representation (e.g., the image in a text-heavy workflow) is generated via noisy real-world conversion tools.

## Limitations

- The paper's findings are based on zero-shot evaluation across six datasets, which may not generalize to domain-specific or fine-tuned scenarios
- The 93% accuracy of the question classifier introduces uncertainty in FRES's decision-making process, as misclassification errors account for 19% of FRES failures
- The paper does not analyze the specific training data composition of the evaluated models, leaving the hypothesized mechanism linking training exposure to representation effectiveness unconfirmed

## Confidence

- **High confidence**: Model scale determining optimal input representation (supported by consistent patterns across 7 model pairs and statistical significance)
- **Medium confidence**: Visual encoding degradation with table size (supported by Figure 2 but not ablated to isolate encoder-specific effects)
- **Low confidence**: Training data composition driving modality-task alignment (inferred from performance patterns, but not empirically validated)

## Next Checks

1. Test FRES decision boundaries on your target table distribution to ensure the 2M pixel / 288 token thresholds align with your data's clustering patterns
2. Validate the question classifier on domain-specific queries before deployment, as the 93% accuracy was measured on WTQ and may not transfer to specialized question types
3. Ablate image rendering templates to assess whether template choice affects MLLM performance on your table structures, as the paper uses 4 fixed HTML templates without exploring alternatives