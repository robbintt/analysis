---
ver: rpa2
title: Evaluating Steering Techniques using Human Similarity Judgments
arxiv_id: '2505.19333'
source_url: https://arxiv.org/abs/2505.19333
tags:
- size
- kind
- prompt
- human
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Large Language Model (LLM) steering techniques
  using a human-inspired triadic similarity judgment task to assess both steering
  accuracy and alignment with human cognition. Researchers tested prompt-based, task
  vector, DiffMean, and sparse autoencoder (SAE) steering methods on Gemma2-27b and
  Gemma2-9b models, measuring accuracy against ground truth and alignment via Procrustes
  correlation with human embeddings.
---

# Evaluating Steering Techniques using Human Similarity Judgments

## Quick Facts
- arXiv ID: 2505.19333
- Source URL: https://arxiv.org/abs/2505.19333
- Reference count: 32
- Key result: Prompt-based steering methods outperformed task vectors, DiffMean, and SAEs on both steering accuracy and human alignment

## Executive Summary
This study evaluates four steering techniques—prompting, task vectors, DiffMean, and sparse autoencoders—using a human-inspired triadic similarity judgment task. The authors assess both steering accuracy and alignment with human semantic representations on the Round Things Dataset using Gemma2-27b and Gemma2-9b models. Prompt-based methods consistently outperformed intervention-based approaches in both accuracy and alignment, revealing a persistent "kind" bias in neutral prompts. Critically, no steering method achieved strong alignment with human representations, especially for size judgments, suggesting fundamental differences in how humans and LLMs process semantic similarity.

## Method Summary
The study uses the Round Things Dataset with 46 spherical concepts varying by kind (artifacts/plants) and continuous size. Triplets are constructed where x1 is ground-truth closer to xref than x2 along a specified dimension (size, kind, or neutral). Four steering methods are tested: prompting (zero-shot and 15-shot ICL), task vectors (residual stream patching at best layer), DiffMean (activation difference averaging), and SAEs (feature direction addition, Gemma2-9b only). Models generate ≥2,500 triplet judgments per condition. 2D embeddings are estimated via crowd-kernel triplet loss and compared to human embeddings using Procrustes correlation. Accuracy vs ground truth and human alignment (r²) are measured across dimensions.

## Key Results
- Prompt-based steering methods outperformed task vectors, DiffMean, and SAEs on both steering accuracy and human alignment
- Neutral prompts showed a persistent bias toward 'kind' similarity over 'size' similarity (β = 0.187, p < 0.001)
- No steering method achieved strong alignment with human representations, particularly for size judgments
- SAE results were limited to Gemma2-9b due to computational constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt-based steering outperforms intervention-based methods because it leverages the model's native context-sensitivity more effectively than activation-level manipulation.
- **Mechanism:** Prompts engage learned attention patterns over the full forward pass, allowing distributed representation adjustments. Intervention methods (Task Vectors, DiffMean, SAE) modify activations at specific layers/tokens, which may not propagate effectively through remaining layers.
- **Core assumption:** The model's learned context-sensitivity from pre-training is better-suited for semantic steering than post-hoc activation manipulation.
- **Evidence anchors:**
  - [abstract] "We found that prompt-based steering methods outperformed other methods both in terms of steering accuracy and model-to-human alignment."
  - [section 4.2] "Intervention methods (SAEs, task vectors, and DiffMean) consistently performed worse than prompting (βTV = −0.29, βDM = −0.30, βSAE = −0.29, p < 0.001)"
  - [corpus] Related work on contrastive activation engineering (arXiv:2505.03189) reports similar patterns where steering success varies by task and layer selection.

### Mechanism 2
- **Claim:** LLMs exhibit a pre-existing "kind" bias in their representational geometry that persists across steering methods.
- **Mechanism:** Pre-training on text corpora emphasizes categorical/taxonomic relationships (kind) over continuous physical properties (size), creating privileged representational axes that are easier to access via steering.
- **Core assumption:** Text-only pre-training underspecifies physical magnitude information relative to categorical relationships.
- **Evidence anchors:**
  - [abstract] "This evaluation approach...reveals privileged representational axes in LLMs prior to steering."
  - [section 4.1] "Neutral prompts that simply asked LLMs to indicate which of the two options was most similar to the target without additional context were better aligned to 'kind' judgments than to size judgments (β = 0.187, p < 0.001)"
  - [corpus] Weak corpus support—neighbor papers focus on social perception and confidence steering, not physical magnitude representation.

### Mechanism 3
- **Claim:** High task accuracy does not imply human-aligned representations—LLMs can isolate task-relevant dimensions more sharply than humans.
- **Mechanism:** Humans naturally integrate multiple semantic dimensions (size and kind "leak" into each other). LLMs under prompting can achieve sharper isolation, producing accurate but cognitively implausible representational geometries.
- **Core assumption:** Human semantic similarity inherently involves dimensional integration, not pure isolation.
- **Evidence anchors:**
  - [section 4.3] "When humans evaluate size, they remain influenced by kind information (and vice versa). In contrast, LLMs appear capable, under some prompting methods, of isolating task-specific information with remarkable precision."
  - [section 4.3] "In the size comparisons, where the models are highly successful in predicting the ground truth (performance), they are poorly aligned (competence) with human judgments"
  - [corpus] Linsley et al. (2023), cited in section 2, found higher object recognition accuracy led to poorer alignment with human visual cortex—consistent with accuracy/alignment tradeoff.

## Foundational Learning

- **Concept: Triadic Similarity Judgment Task**
  - Why needed here: Core evaluation method. Presents (reference, option1, option2) and asks which option is more similar to reference along a specified dimension.
  - Quick check question: Given triplet (orange, baseball, walnut) with dimension "size," which option should be selected and why?

- **Concept: Procrustes Correlation**
  - Why needed here: Quantifies alignment between embedding spaces after allowing for affine transformations (rotation, scaling, translation). Measures whether pairwise distance relationships are preserved.
  - Quick check question: If two embedding spaces have Procrustes r² = 0.50, what proportion of variance in human distance judgments is explained by model distance judgments?

- **Concept: Sparse Autoencoder (SAE) Steering**
  - Why needed here: One of four steering methods tested. SAEs decompose activations into sparse feature directions; steering adds decoder vectors for target features.
  - Quick check question: Why might SAE steering underperform prompting even when the SAE identifies relevant features?

## Architecture Onboarding

- **Component map:**
  - Triplet prompt construction -> Four parallel steering paths (Prompt/ICL, Task Vector, DiffMean, SAE) -> Final token '+' extraction -> Binary choice logits/2D embeddings -> Accuracy and Procrustes correlation computation

- **Critical path:**
  1. Generate ≥2,500 triplet judgments per steering method
  2. Fit 2D embeddings via crowd-kernel triplet loss
  3. Compute Procrustes correlation between model and human embedding spaces
  4. Compare accuracy vs. alignment across methods

- **Design tradeoffs:**
  - Layer selection: Optimal layer differs by steering method and dimension (paper selected l* per condition based on held-out accuracy)
  - Sample size: More triplets → more reliable embeddings but higher compute
  - Embedding dimensionality: 2D chosen for interpretability; higher dimensions may capture more structure

- **Failure signatures:**
  - Random-chance accuracy → steering vector not extracting relevant direction
  - High accuracy + near-zero alignment → model using non-human-like representations
  - Kind accuracy >> size accuracy → unsteered model relying on default kind bias
  - 27b worse alignment than 9b despite higher accuracy → representational divergence with scale

- **First 3 experiments:**
  1. Replicate neutral prompt baseline to confirm kind-bias (expected: kind accuracy > size accuracy, r²neutral,kind > r²neutral,size)
  2. Test prompt steering with varied instruction phrasing to measure sensitivity to prompt engineering
  3. Apply DiffMean steering at multiple layers simultaneously to test whether single-layer limitation explains underperformance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the finding that prompting outperforms internal interventions generalize to more complex, naturalistic semantic tasks?
- Basis in paper: [explicit] The authors note that "in practice, there are more complex and naturalistic behaviors" than triadic judgments, and future work must "incorporporate such tasks."
- Why unresolved: The current study relies on a constrained dataset (Round Things) focusing only on size and kind dimensions.
- What evidence would resolve it: Applying the same alignment evaluation framework to diverse, open-ended tasks requiring flexible semantic control.

### Open Question 2
- Question: What are the mechanistic reasons for the superior alignment of prompt-based steering compared to SAEs or task vectors?
- Basis in paper: [explicit] The conclusion calls for future work to "uncover the basis of prompting’s success in guiding LLM's learned representations."
- Why unresolved: The study establishes the empirical performance gap but does not investigate the internal representational dynamics causing it.
- What evidence would resolve it: Causal tracing or layer-wise analysis comparing how different steering methods alter the model's existing representational geometry.

### Open Question 3
- Question: Do the alignment results hold for larger frontier models and other untested steering methods like LoRA or supervised fine-tuning?
- Basis in paper: [explicit] The authors acknowledge they "did not exhaustively test all possible forms of interventions" (e.g., LoRA, ReFT) and were limited to models runnable on consumer GPUs.
- Why unresolved: SAE results were missing for the 27b model, and the method suite was restricted by compute constraints.
- What evidence would resolve it: Replicating the triadic judgment evaluation on larger models (e.g., 70B+ parameters) and alternative fine-tuning techniques.

## Limitations

- Focus on spherical objects limits generalizability to broader semantic domains where size-kind relationships may differ
- Computational constraints prevented SAE testing on the larger Gemma2-27b model, potentially missing performance differences at scale
- 2D embedding dimensionality was chosen for interpretability but may underrepresent the full complexity of semantic similarity judgments

## Confidence

- **High Confidence**: Prompt-based steering superiority (accuracy and alignment), kind bias in neutral prompts, accuracy/alignment tradeoff pattern
- **Medium Confidence**: Mechanism explanations for prompt superiority and kind bias, SAE performance relative to other intervention methods
- **Low Confidence**: Generalizability to non-spherical domains, 27b model alignment results, specific layer selection choices

## Next Checks

1. **Cross-domain replication**: Test steering methods on semantic domains beyond spherical objects (e.g., animals, tools) to assess domain generality.
2. **Multimodal training impact**: Evaluate whether models trained on image-text data show reduced kind/size asymmetry in their representational geometry.
3. **Multi-layer intervention**: Test DiffMean and task vector methods applied simultaneously at multiple layers to determine if single-layer limitations explain underperformance.