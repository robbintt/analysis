---
ver: rpa2
title: Source-Free Cross-Domain Continual Learning
arxiv_id: '2510.01649'
source_url: https://arxiv.org/abs/2510.01649
tags:
- domain
- learning
- problem
- source
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the source-free cross-domain continual learning
  (SFCDCL) problem, where labeled source domain samples are prohibited, and the goal
  is to perform continual domain adaptations using only a pre-trained source model
  and unlabeled target domain samples. The key challenges include over-dependence
  on the pre-trained source model, noisy pseudo-labels, and double catastrophic forgetting
  (DCF).
---

# Source-Free Cross-Domain Continual Learning

## Quick Facts
- **arXiv ID:** 2510.01649
- **Source URL:** https://arxiv.org/abs/2510.01649
- **Reference count:** 40
- **Primary result:** REFEREE outperforms prior arts on VisDA, OfficeHome, and DomainNet without using labeled source-domain samples.

## Executive Summary
This paper tackles the challenging source-free cross-domain continual learning (SFCDCL) problem, where labeled source data is prohibited and only a pre-trained source model and unlabeled target data are available. The key challenges are over-reliance on the pre-trained source model, noisy pseudo-labels due to domain shift, and double catastrophic forgetting. To address these, the authors propose REFEREE, a rehearsal-free method combining a source-pretrained model with a large-scale vision-language model (CLIP) to extract complementary domain-specific and domain-invariant knowledge. REFEREE uses frequency-aware prompting to suppress noise, uncertainty-aware weighting to mitigate pseudo-label errors, and kernel linear discriminant analysis (KLDA) with random Fourier features to prevent forgetting by freezing the backbone.

## Method Summary
REFEREE operates in a source-free setting using a dual-branch network: a learnable source-pretrained ViT branch and a frozen CLIP branch. The method generates pseudo-labels by fusing predictions from both branches, applies frequency-aware augmentation to suppress high-frequency noise, and updates a KLDA classifier using uncertainty-weighted statistics. The backbone remains frozen throughout to prevent catastrophic forgetting. Training is gradient-free, updating only class means and covariance matrices in the KLDA space.

## Key Results
- REFEREE significantly outperforms prior arts on complex benchmarks (VisDA, OfficeHome, DomainNet).
- The dual-branch architecture provides substantial gains over single-branch methods.
- Frequency-aware prompting and uncertainty weighting effectively improve pseudo-label quality.
- KLDA with frozen backbone successfully mitigates double catastrophic forgetting.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** REFEREE's dual-branch architecture improves generalization by combining domain-specific and domain-invariant knowledge.
- **Mechanism:** A source-pretrained ViT branch captures domain-specific features, while a frozen CLIP branch provides domain-invariant, zero-shot capabilities. Their predictions are aggregated using a weighted mixture.
- **Core assumption:** The domain shift is a primary failure mode; combining models with different inductive biases provides complementary robustness.
- **Evidence anchors:** [abstract] Synergy between source-pretrained model and VLM; [section IV-B] weighted mixture of two network outputs.
- **Break condition:** The claim relies on the effectiveness of CLIP as a zero-shot learner for the target domain.

### Mechanism 2
- **Claim:** Frequency-aware prompting and uncertainty weighting reduce the impact of noisy pseudo-labels caused by domain shift.
- **Mechanism:** Target domain samples undergo Discrete Wavelet Transform. High-frequency components are suppressed/modified to create augmented samples. Additionally, Shannon entropy of the fused prediction is used to calculate uncertainty weights, which down-weight unreliable pseudo-labels.
- **Core assumption:** High-frequency image components correlate with noise or domain-specific artifacts that harm generalization.
- **Evidence anchors:** [abstract] Domain shift handled by frequency-aware prompting; [section IV-C] DWT-based augmentation.
- **Break condition:** If the task relies critically on high-frequency details which are destroyed by the frequency suppression.

### Mechanism 3
- **Claim:** KLDA with RFF mitigates Double Catastrophic Forgetting (DCF) by freezing the backbone and enabling non-linear classification.
- **Mechanism:** The feature extractors are frozen completely. Only a linear classifier (LDA) is trained on top. To handle non-linear separability, features are projected into a higher-dimensional space using Random Fourier Features before LDA.
- **Core assumption:** The frozen backbone features are sufficiently rich and transferable to separate classes in all seen domains.
- **Evidence anchors:** [abstract] KLDA where backbone network is frozen; [section III-C] RFF approximation for LDA.
- **Break condition:** If the frozen backbone features are insufficient for a new domain, the model cannot adapt its feature extraction.

## Foundational Learning

- **Concept: Source-Free Domain Adaptation (SFDA)**
  - **Why needed here:** This is the core problem setting. REFEREE cannot be understood without grasping that labeled source data is prohibited during adaptation.
  - **Quick check question:** Can you explain why traditional fine-tuning is impossible in this setting?

- **Concept: Pseudo-Labeling and Confirmation Bias**
  - **Why needed here:** Without target labels, the model must generate its own supervision (pseudo-labels). The quality of these labels dictates success. A key risk is confirmation bias.
  - **Quick check question:** How might the entropy-based weighting in REFEREE specifically combat confirmation bias?

- **Concept: Catastrophic Forgetting (CF) in Continual Learning**
  - **Why needed here:** The "Continual" part of SFCDCL means tasks arrive sequentially. CF is the loss of performance on previous tasks when learning new ones.
  - **Quick check question:** Why does REFEREE's KLDA approach inherently prevent forgetting in the backbone, and what are the trade-offs?

## Architecture Onboarding

- **Component map:**
  Input -> Branch 1 (ViT: Frozen Backbone → Feature Extractor → RFF → KLDA Classifier) + Branch 2 (VLM/CLIP: Frozen Encoder → Zero-shot Prediction) -> Fusion & Pseudo-Labeling -> Frequency-Aware Augmentation -> Update KLDA Statistics

- **Critical path:**
  1. Load pre-trained ViT and CLIP models.
  2. Initialize KLDA parameters for source classes.
  3. For each target task:
     a. Generate pseudo-labels using dual-branch fusion.
     b. Apply frequency augmentation.
     c. Calculate uncertainty weights.
     d. Update KLDA statistics (mean/covariance) for new classes only. Do not backpropagate through backbone.

- **Design tradeoffs:**
  - Frozen Backbone vs. Plasticity: Sacrifices ability to learn new low-level features for perfect stability (no forgetting).
  - Dual-Branch Complexity vs. Robustness: Adds computational cost for improved pseudo-label quality over a single-branch approach.
  - RFF Approximation vs. Exact Kernel: RFF makes KLDA scalable for continual learning but is an approximation of a true RBF kernel.

- **Failure signatures:**
  - Low Accuracy on Early Target Tasks: Suggests the source pre-trained model is poor or CLIP zero-shot capability is weak for the target domain.
  - Degraded Performance Over Tasks (Forgetting): Check if the backbone is truly frozen.
  - Instability in KLDA: Covariance matrix becoming singular or ill-conditioned.

- **First 3 experiments:**
  1. Baseline Comparison: Reproduce the "w/o CLIP" ablation on VisDA to verify the contribution of the dual-branch design.
  2. Component Ablation: Run REFEREE on a single domain pair disabling the Frequency-Aware Prompting to isolate its effect.
  3. Hyperparameter Sensitivity: Vary the RFF dimension and kernel width on a smaller dataset to confirm robustness claims.

## Open Questions the Paper Calls Out
- How can REFEREE be adapted to handle non-identical label spaces, such as open-set or partial cross-domain continual learning problems?
- Does the suppression of high-frequency components result in the loss of discriminative information for texture-sensitive tasks?
- Is the method robust to target domains where the frozen CLIP model provides poor zero-shot guidance?

## Limitations
- Performance claims rely heavily on specific hyperparameter choices without extensive ablation or sensitivity analysis.
- Frequency-aware prompting lacks theoretical grounding for why high-frequency suppression generalizes well across diverse domains.
- The KLDA classifier with RFF is evaluated only in comparison to existing methods rather than against simpler baselines.

## Confidence
- **High confidence:** The dual-branch architecture combining source-pretrained ViT and CLIP provides robustness against domain shift.
- **Medium confidence:** Frequency-aware prompting and uncertainty weighting effectively reduce noisy pseudo-label impact.
- **Medium confidence:** KLDA with RFF successfully mitigates double catastrophic forgetting by freezing the backbone.

## Next Checks
1. Ablation Study: Run REFEREE on VisDA with frequency-aware prompting disabled to quantify its isolated contribution.
2. Hyperparameter Sensitivity: Systematically vary RFF dimension and kernel width on Office-31 to verify robustness claims.
3. Computational Overhead Analysis: Measure inference time and memory usage when running both ViT and CLIP branches, comparing against single-branch alternatives.