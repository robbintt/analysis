---
ver: rpa2
title: 'EquiPy: Sequential Fairness using Optimal Transport in Python'
arxiv_id: '2503.09866'
source_url: https://arxiv.org/abs/2503.09866
tags:
- fairness
- predictions
- sensitive
- fair
- unfairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EquiPy, a Python package for achieving algorithmic
  fairness across multiple sensitive variables using optimal transport theory. The
  package addresses the challenge of maintaining predictive accuracy while mitigating
  biases in machine learning models, particularly when dealing with complex scenarios
  involving multiple sensitive attributes like gender and race.
---

# EquiPy: Sequential Fairness using Optimal Transport in Python

## Quick Facts
- arXiv ID: 2503.09866
- Source URL: https://arxiv.org/abs/2503.09866
- Reference count: 5
- Primary result: Achieves algorithmic fairness across multiple sensitive variables using Wasserstein barycenters with minimal predictive accuracy loss

## Executive Summary
EquiPy is a Python package that addresses algorithmic fairness in machine learning by leveraging optimal transport theory. The package transforms model predictions to achieve Demographic Parity across multiple sensitive attributes while minimizing accuracy degradation. By using Wasserstein barycenters, EquiPy provides a post-processing approach that is model-agnostic and compatible with various ML frameworks. The package offers both exact and approximate fairness adjustments, comprehensive visualization tools, and supports both classification and regression tasks through a user-friendly scikit-learn-like interface.

## Method Summary
The core method utilizes Wasserstein barycenters to sequentially transform model predictions, ensuring Demographic Parity across sensitive groups while minimizing accuracy loss. The approach decomposes multi-attribute fairness into manageable single-attribute sub-problems, exploiting the associativity property of barycenter compositions. For each sensitive attribute, the method computes group-wise conditional distributions of predictions and finds a "middle-ground" distribution (barycenter) that satisfies DP by construction. Each prediction is mapped through quantile-quantile transformation to this fair distribution at minimal Wasserstein cost. The package also offers geodesic interpolation for controlled trade-offs between fairness and performance.

## Key Results
- Successfully reduced unfairness metrics from 0.437 to 0.067 for ethnicity-based predictions
- Effectively handled multiple sensitive attributes with minimal performance degradation
- Demonstrated comprehensive visualization tools for analyzing fairness-performance trade-offs
- Achieved model-agnostic implementation compatible with various ML frameworks

## Why This Works (Mechanism)

### Mechanism 1: Wasserstein Barycenter Projection
Transforming model predictions to their Wasserstein barycenter achieves Demographic Parity while minimizing accuracy loss. The approach computes group-wise conditional distributions of predictions, then finds a "middle-ground" distribution (barycenter) that satisfies DP by construction. Each prediction is mapped through the quantile-quantile transformation: `Q_barycenter ∘ F_group(original_score)`, transporting scores to the fair distribution at minimal Wasserstein cost. Core assumption: At least one group's prediction distribution has a density with respect to Lebesgue measure (ensures unique barycenter).

### Mechanism 2: Sequential Decomposition for Multiple Sensitive Attributes
Multi-attribute fairness can be achieved by sequentially composing single-attribute fair predictors, with order-independent final results. For r sensitive attributes, the global fair predictor equals the composition of marginal fair predictors: `fB = fB1 ∘ fB2 ∘ ... ∘ fBr`. This exploits the associativity property proved in Hu et al. (2024), allowing decomposition into tractable sub-problems without exponentially many intersectional subgroups. Core assumption: Unfairness is measured additively across attributes.

### Mechanism 3: Geodesic Interpolation for Controlled Trade-offs
Approximate fairness with parameter ε interpolates between optimal accuracy and full fairness, enabling practitioners to navigate the fairness-performance frontier. Uses geodesic parameterization in Wasserstein space: `f^ε_Bi = (1-εi)·fBi + εi·f*`, where εi ∈ [0,1] controls how much unfairness is tolerated relative to the original model. Core assumption: The fairness-accuracy frontier can be meaningfully parameterized by a single scalar per attribute.

## Foundational Learning

- **Concept: Wasserstein Distance (1-D)**
  - Why needed here: Core mathematical object used to measure unfairness and define the fair projection.
  - Quick check question: Can you explain why `W1(ν, ν|ai) = 0` implies Demographic Parity holds for attribute Ai?

- **Concept: Quantile Functions and CDFs**
  - Why needed here: The fair transformation is implemented via quantile-quantile mapping; understanding `F` and `Q` is essential for debugging.
  - Quick check question: Given two groups with CDFs F0 and F1, what does `Q1 ∘ F0` compute?

- **Concept: Demographic Parity (Independence)**
  - Why needed here: EquiPy enforces DP specifically; other fairness criteria (Equalized Odds, Calibration) are not supported for regression.
  - Quick check question: Why might DP be inappropriate if your use case requires predictions that reflect group-level base rate differences?

## Architecture Onboarding

- **Component map**:
  ```
  fairness/
    ├── FairWasserstein    → Single sensitive attribute correction
    └── MultiWasserstein   → Sequential multi-attribute correction
  metrics/
    ├── unfairness()       → W1-based additive unfairness measure
    └── performance()      → Wrapper around sklearn metrics
  graphs/
    ├── fair_density_plot     → Pre/post distribution comparison
    ├── fair_arrow_plot       → Fairness-performance trajectory
    └── fair_waterfall_plot   → Sequential contribution breakdown
  ```

- **Critical path**:
  1. Train any ML model → obtain scores (probabilities for classification, continuous outputs for regression)
  2. Split unused data into calibration and test sets
  3. `MultiWasserstein.fit(predictions_calib, sensitive_features_calib)` — learns quantile functions
  4. `MultiWasserstein.transform(predictions_test, sensitive_features_test, epsilon=...)` — applies fair mapping
  5. Evaluate with `metrics.unfairness()` and `metrics.performance()`

- **Design tradeoffs**:
  - Post-processing vs. in-processing: Model-agnostic and deployable on existing systems, but cannot improve model internals; limited to score manipulation.
  - Exact vs. approximate fairness: ε parameterization enables nuanced trade-offs but requires manual tuning and justification.
  - Default quantile grid vs. POT linear program: Grid approximation (`approximate=True`) is faster for large N; exact OT (`approximate=False`) needed for small samples but is O(N²).

- **Failure signatures**:
  - Unfairness doesn't decrease: Check that sensitive_features match between fit and transform; verify predictions are continuous scores, not hard labels.
  - Performance degrades severely: Large distribution shifts between calibration and test sets; consider recalibrating or using training data for calibration.
  - Numerical instability: Predictions may have atoms; increase `sigma` parameter for jittering (default 0.0001).

- **First 3 experiments**:
  1. **Single-attribute baseline**: Apply `FairWasserstein` to one sensitive attribute on held-out test data; verify `unfairness()` drops near zero while `performance()` changes minimally.
  2. **Sequential vs. intersectional**: For two binary attributes, compare sequential correction against creating a 4-class intersectional attribute; observe unfairness and performance differences on a fixed sample size.
  3. **Epsilon sweep**: Using `fair_multiple_arrow_plot` with varying `epsilon` vectors, map the fairness-performance frontier to identify acceptable operating points for your application context.

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal ordering of sensitive attributes be determined a priori to minimize the performance degradation (the "price of fairness") during sequential correction? While the paper proves the commutativity of the final fair predictor, it does not provide a theoretical or heuristic method for selecting the sequence that maximizes preservation of accuracy at intermediate steps.

### Open Question 2
Can the sequential Wasserstein barycenter approach be extended to enforce conditional fairness metrics, such as Equalized Odds, without losing the closed-form solution? The current mathematical formulation relies on mapping distributions to a barycenter based on marginal distributions; satisfying conditional independence requires a different transport mapping that is not currently implemented or derived.

### Open Question 3
What are the theoretical error bounds for the default quantile-based approximation compared to the exact linear programming solution when calculating unfairness? The authors acknowledge the approximation is for computational efficiency, but the impact of this discretization on the robustness of the fairness guarantee in small samples is not quantified.

## Limitations

- Sequential decomposition may not fully capture intersectional effects when sensitive attributes are correlated
- Package focuses exclusively on Demographic Parity, limiting applicability for other fairness criteria
- Requires continuous prediction distributions and may struggle with discrete predictions without jittering

## Confidence

- **High Confidence**: Wasserstein barycenter projection mechanism and its implementation via quantile mapping
- **Medium Confidence**: Sequential decomposition for multiple sensitive attributes
- **Medium Confidence**: Geodesic interpolation for controlled trade-offs

## Next Checks

1. **Intersectional vs. Sequential Comparison**: Implement direct comparison between sequential correction and intersectional treatment of correlated sensitive attributes on real-world datasets to quantify potential masking effects.

2. **Discrete Prediction Robustness**: Systematically test the package's performance when applied to models with discrete output distributions across varying levels of jittering to establish stability boundaries.

3. **Cross-Criteria Fairness Extension**: Evaluate whether the Wasserstein barycenter framework can be extended beyond Demographic Parity to support Equalized Odds or calibration, identifying mathematical and implementation barriers.