---
ver: rpa2
title: 'Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from
  Tables and Charts'
arxiv_id: '2511.10075'
source_url: https://arxiv.org/abs/2511.10075
tags:
- chart
- charts
- table
- evidence
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the robustness of multimodal large language\
  \ models (LLMs) in verifying scientific claims when evidence is presented in different\
  \ formats\u2014specifically tables and charts. While prior datasets focus on a single\
  \ format, this work extends two datasets (SciTabAlign and ChartMimic) to create\
  \ aligned table-chart pairs representing the same information."
---

# Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts

## Quick Facts
- **arXiv ID**: 2511.10075
- **Source URL**: https://arxiv.org/abs/2511.10075
- **Reference count**: 14
- **Primary result**: Multimodal LLMs perform significantly better with table-based evidence than chart-based evidence, despite identical information content.

## Executive Summary
This paper systematically investigates the robustness of multimodal large language models (LLMs) in scientific claim verification when evidence is presented as tables versus charts. While prior datasets focused on single formats, this work extends two datasets (SciTabAlign and ChartMimic) to create aligned table-chart pairs representing identical information. Evaluating 12 multimodal LLMs under table-only, chart-only, and combined input settings reveals a consistent performance gap favoring tables. Human annotators maintain consistent performance across formats, highlighting a model limitation rather than task ambiguity. The study also reveals that smaller models (<8B parameters) show weak correlation between table and chart performance, indicating limited cross-modal generalization. These findings underscore the need for improved multimodal reasoning, particularly in chart understanding, to support reliable scientific claim verification.

## Method Summary
The authors extend two existing datasets—SciTabAlign (70 tables, 162 claims) and ChartMimic (76 charts, 152 claims)—to create aligned table-chart pairs by generating four chart types per table and claims for chart-based evidence. They evaluate 12 open-source multimodal LLMs (ranging from 1B to 72B parameters) using zero-shot Chain-of-Thought prompting in three input settings: table-only, chart-only, and combined table-chart. The primary metric is macro-F1 score, with inference performed on NVIDIA A100 80 GB GPUs. The evaluation protocol uses instruct-tuned model versions and extracts final answers from `<ans>` tags in the model output.

## Key Results
- Models perform significantly better with table-based evidence than chart-based evidence (11 of 12 models), despite both formats containing identical information
- Smaller models (<8B parameters) show weak correlation between table-only and chart-only performance, indicating limited cross-modal generalization
- Human annotators achieve consistent performance across both formats (94-96% F1), highlighting the gap as a model limitation
- Combined table-chart inputs often yield lower performance than table-only inputs, suggesting visual noise interferes with reasoning

## Why This Works (Mechanism)

### Mechanism 1
Multimodal LLMs process structured text (tables) more reliably than visual encodings (charts) for claim verification because text-based inputs bypass the error-prone visual decoding step required for charts. Tables provided in text-based formats (e.g., JSON) are tokenized directly, allowing the LLM backbone to access exact numerical values immediately. In contrast, charts require the vision encoder to infer values from pixels, introducing "derendering" noise before reasoning can occur. The performance gap is driven by input modality processing constraints rather than the complexity of the reasoning chain itself.

### Mechanism 2
Parameter count constrains cross-modal generalization; smaller models (<8B parameters) struggle to align reasoning strategies between visual and textual representations of the same data. Smaller models possess limited capacity in their latent space to form a unified "data concept" that persists whether the input is a pixel pattern or a text token. This results in a weak correlation between their performance on table-only vs. chart-only tasks. High correlation between modalities indicates a robust internal representation of the data, whereas low correlation indicates modality-specific "memorization" or brittle processing.

### Mechanism 3
Current multimodal fusion mechanisms are susceptible to "visual noise," where the inclusion of a confusing chart actively degrades the reasoning capability enabled by a clear table. The attention mechanism distributes focus across both modalities. When the model attends to the chart (which it processes poorly), it generates weak or conflicting attention signals that interfere with the stronger signals from the text table, leading to performance drops in the combined setting compared to table-only. The model cannot dynamically "ignore" the low-quality signal from the chart in favor of the high-quality table signal.

## Foundational Learning

- **Concept**: Multimodal Alignment
  - **Why needed here**: To understand why models fail to treat a chart and a table as the same object. The paper reveals a misalignment between the visual embedding space (charts) and the textual embedding space (tables), preventing consistent reasoning.
  - **Quick check question**: Does the model's performance on a specific claim correlate when the evidence is swapped from a table to a chart?

- **Concept**: Zero-Shot Chain-of-Thought (CoT)
  - **Why needed here**: This is the evaluation protocol used in the paper ("Think step by step"). Understanding CoT is necessary to replicate the prompting strategy that forces the model to verbalize its logic before making a binary (Supported/Refuted) prediction.
  - **Quick check question**: Did the paper fine-tune models on the dataset, or did it rely on prompting pre-trained models?

- **Concept**: Macro-F1 Score
  - **Why needed here**: This is the primary metric used to account for potential class imbalance (Supported vs. Refuted). It treats both classes equally, preventing a model from inflating its score by just guessing the majority class.
  - **Quick check question**: If a model predicts "Supported" for every single claim, would Accuracy or Macro-F1 better expose this failure?

## Architecture Onboarding

- **Component map**: Input Layer -> Vision Encoder -> Projection Layer -> LLM Backbone -> Output Head
- **Critical path**: The integrity of the Vision Encoder → Projection Layer path is the bottleneck. If the encoder fails to precisely transcribe pixel-values into semantic concepts (e.g., reading "0.85" as "0.8"), the LLM backbone cannot recover the logic, resulting in the performance drops observed in the paper.
- **Design tradeoffs**: Relying solely on text-tables is faster and more accurate (per the paper) but fails in real-world scenarios where only PDF/image charts exist. Larger models (32B+) show better cross-modal correlation but incur higher inference costs. Smaller models (<8B) are cheaper but effectively "blind" to the equivalence between charts and tables.
- **Failure signatures**: Negative Correlation (a model succeeds on a table but fails on the identical chart data, specific to small models like InternVL3-1B); Combinatorial Degradation (performance drops when adding a chart to a table input, indicating the model is confused by the visual modality rather than aided by it); Format Bias (consistently higher Macro-F1 scores on Table-Only inputs compared to Chart-Only inputs, observed in 11 out of 12 models).
- **First 3 experiments**:
  1. Sanity Check (Table-Only): Evaluate the baseline MLLM on the text-table dataset (SciTabAlign+) to establish the "text reasoning" ceiling without visual noise.
  2. Modality Gap (Chart-Only): Run the same model on the generated chart images (Basic Bar/Symbol) to quantify the "visual penalty" (Gap = Table-F1 - Chart-F1).
  3. Interference Test (Combined): Feed both the table and the chart simultaneously. If F1 scores drop below the Table-Only baseline, the visual integration module is introducing critical noise.

## Open Questions the Paper Calls Out

- How does multimodal LLM performance on claim verification generalize to multi-chart and multi-table evidence scenarios commonly found in scientific papers? The authors state "We decided to exclude sub-charts, as our focus is on single-chart analysis. Multi-chart scenarios are left for future work" (Section 3.2). Current experiments only evaluate single charts/tables, but real scientific papers often present results across multiple figures and tables that must be jointly reasoned about.
- What specific visual reasoning components cause the persistent performance gap between table-based and chart-based evidence processing? The paper demonstrates consistent performance gaps but does not identify whether failures stem from data extraction, numerical reasoning, or visual encoding issues. Error analysis is limited to aggregate metrics; root causes of chart-specific failures remain unexplored despite humans achieving 94-96% F1 on both formats.
- Can targeted chart-to-table pretraining or cross-modal alignment objectives improve smaller models' (<8B parameters) weak cross-modal generalization? The authors find "smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization" (Abstract, Section 6). The correlation analysis reveals the problem but offers no interventions; current training regimes may not sufficiently align visual and tabular representations in smaller models.

## Limitations

- The vision encoder's fidelity is not directly measured. Performance drops could stem from tokenization errors or insufficient chart resolution rather than pure visual understanding.
- Human evaluation only confirms format consistency, not absolute reasoning quality. Human accuracy across formats is not reported, limiting comparison to model baselines.
- The study focuses on simple chart types (bar, line). Complex scientific charts with multiple series or annotations may exacerbate the observed gaps.

## Confidence

- **High Confidence**: Models perform significantly better with table-based evidence than chart-based evidence when both formats contain identical information. This is supported by consistent Macro-F1 differences across 11 of 12 models.
- **Medium Confidence**: Smaller models (<8B parameters) show weak or negative correlation between table-only and chart-only performance, indicating limited cross-modal generalization. While observed, the causal link between parameter count and latent space alignment remains theoretical.
- **Low Confidence**: The exact mechanism by which visual noise degrades combined input performance. While models show lower F1 with combined inputs versus table-only, the paper does not definitively prove attention interference versus other factors like prompt confusion.

## Next Checks

1. **Vision Encoder Benchmark**: Test the same models on a synthetic chart dataset where ground-truth pixel-to-value mappings are known. Measure derendering accuracy before claim verification.
2. **Format-Fidelity Control**: Convert tables to image format (screenshots) and charts to structured text (JSON) to isolate whether the gap is modality-driven or format-driven.
3. **Human Performance Gap**: Collect human accuracy scores on both table-only and chart-only claim verification tasks to establish whether the format gap exists for humans, which would invalidate the model limitation hypothesis.