---
ver: rpa2
title: Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large
  Reasoning Models
arxiv_id: '2508.10751'
source_url: https://arxiv.org/abs/2508.10751
tags:
- pass
- training
- group
- advantage
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pass@k Training improves exploration in reinforcement learning
  with verifiable rewards (RLVR) by using Pass@k as the reward metric. This approach
  enhances the model's ability to generate diverse solutions without compromising
  Pass@1 performance.
---

# Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models

## Quick Facts
- arXiv ID: 2508.10751
- Source URL: https://arxiv.org/abs/2508.10751
- Reference count: 40
- Key outcome: Pass@k Training improves exploration in RLVR by rewarding groups of responses, enhancing diversity without compromising Pass@1 performance.

## Executive Summary
Pass@k Training introduces a novel reinforcement learning approach that improves exploration in large reasoning models by using Pass@k as the reward metric. The method rewards groups of responses rather than individual trajectories, preserving exploration capability while maintaining strong Pass@1 performance. Through analytical derivation of the advantage function and bootstrap sampling techniques, Pass@k Training achieves higher efficiency and effectiveness compared to traditional Pass@1 Training methods.

## Method Summary
Pass@k Training modifies reinforcement learning with verifiable rewards (RLVR) by assigning rewards at the group level rather than individual responses. For each prompt, the method generates multiple responses, groups them, and assigns the maximum reward from any response in the group to all responses in that group. This can be implemented through three variants: full sampling, bootstrap sampling, and analytical derivation. The analytical approach directly computes advantages using combinatorial formulas based on counts of correct and incorrect responses, avoiding the computational overhead of explicit sampling. The method uses DAPO (a GRPO variant) with clip-higher-only regularization and trains on reasoning tasks using models like Qwen2.5-7B-Instruct.

## Key Results
- Pass@k Training maintains higher policy entropy longer than Pass@1 Training, preventing premature convergence to local optima
- Smaller models (7B) trained with Pass@k can surpass powerful LLMs (GPT-4o) in reasoning tasks when continued training on Pass@1 is applied
- The method achieves higher efficiency through analytical derivation compared to bootstrap sampling while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
Rewarding groups of responses rather than individual trajectories preserves exploration capability. In standard Pass@1 Training, a correct reasoning process that yields an incorrect answer receives a negative reward, penalizing the exploration path. Pass@k Training assigns the group reward to all responses in a group if any single response is correct, offering "partial credit" to incorrect responses that co-occur with successful ones.

### Mechanism 2
Analytical derivation of the advantage function shifts optimization focus from "medium" to "hard" problems. The analytical Pass@k advantage peaks at lower accuracy levels (harder problems) compared to Pass@1 Training which peaks at 50% accuracy. This gradient scaling naturally focuses model capacity on unsolved or difficult instances.

### Mechanism 3
Exploration and exploitation are not conflicting in this regime; enhanced exploration transfers to better exploitation. Pass@k Training maintains higher policy entropy longer than Pass@1 Training. By preventing the policy from collapsing to a local optimum too early, the model discovers superior reasoning paths that transfer to improved Pass@1 performance.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed: Pass@k Training modifies the advantage estimation within the GRPO framework. You must understand that standard GRPO calculates advantage based on group mean/std deviation of rewards.
  - Quick check: How does normalizing rewards by group standard deviation differ from raw reward scaling?

- **Concept: Pass@k Metric**
  - Why needed: This is the objective function. It measures the probability of solving a problem given k attempts.
  - Quick check: Why does maximizing Pass@1 often hurt Pass@k? (Hint: Variance reduction vs. mean shift).

- **Concept: Bootstrap Sampling**
  - Why needed: The paper uses bootstrap sampling to construct response groups efficiently before deriving the analytical solution.
  - Quick check: How does sampling with replacement allow us to simulate more groups than we have raw rollouts?

## Architecture Onboarding

- **Component map:** Rollout Engine -> Verifier -> Advantage Calculator -> Optimizer
- **Critical path:** Prompt batch → Rollout N_rollout responses → Verify all responses → Calculate group rewards analytically → Compute advantages → Update policy
- **Design tradeoffs:** Full Sampling vs. Analytical (computational wastefulness vs. statistical assumptions); Choice of k (small k approximates Pass@1, large k increases exploration but reduces advantage magnitude)
- **Failure signatures:** Flat Loss (if k is too large and LR is too small); Entropy Collapse (if bootstrap variance is too high)
- **First 3 experiments:** 1) Sanity Check: Train Qwen-7B with Pass@1 vs. Pass@k on Maze, plot Pass@1 and Pass@8 over steps; 2) Efficiency Validation: Compare Bootstrap vs. Analytical on wall-clock time and variance; 3) Transfer Run: Train with Pass@k to convergence, then switch to Pass@1 and verify performance jump

## Open Questions the Paper Calls Out

### Open Question 1
How can the advantage function be adaptively adjusted during RLVR based on the model's real-time state (e.g., policy entropy) rather than relying on static definitions? Current variants use fixed formulas, but a dynamic mechanism responding to training feedback is not established.

### Open Question 2
What are the general theoretical principles for "implicit reward design" that ensure stability and effectiveness across diverse reasoning tasks? The paper's exploration is preliminary, relying on empirical curve fitting rather than a unified theoretical framework.

### Open Question 3
Can the "Exceeding Pass@k Training" method be refined to improve Pass@1 performance without sacrificing the rapid Pass@k gains observed in early training stages? Currently there's a trade-off where focusing heavily on exploration delays consolidation of correct answers.

## Limitations
- Sample complexity remains substantial due to multiple responses per prompt requirement
- Generalizability beyond synthetic tasks (Maze, Enigmata) needs broader validation on complex real-world reasoning benchmarks
- Choice of k and learning rate tuning requires careful hyperparameter optimization, with limited sensitivity analysis

## Confidence

- **High Confidence**: Core mechanism of using group rewards to preserve exploration is well-supported by theory and empirical results
- **Medium Confidence**: Analytical derivation of advantage functions and transfer of exploration gains to exploitation are logically sound but rely on statistical assumptions
- **Low Confidence**: Scalability to very large models and behavior in low-data regimes are not fully characterized

## Next Checks

1. **Scalability Test**: Implement Pass@k Training on Qwen2.5-32B and evaluate whether exploration benefits scale proportionally; compare wall-clock time and memory usage against Pass@1 Training

2. **Robustness to Verifier Noise**: Introduce controlled noise (5-10% label flips) into the verifier and measure impact on Pass@k Training stability; compare against Pass@1 Training under same conditions

3. **Transfer to Open-Ended Tasks**: Apply Pass@k Training to mathematical proof generation or code synthesis datasets; evaluate whether exploration gains translate to improved Pass@k and Pass@1 performance in these domains