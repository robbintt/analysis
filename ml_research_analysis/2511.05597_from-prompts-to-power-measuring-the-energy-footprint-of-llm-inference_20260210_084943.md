---
ver: rpa2
title: 'From Prompts to Power: Measuring the Energy Footprint of LLM Inference'
arxiv_id: '2511.05597'
source_url: https://arxiv.org/abs/2511.05597
tags:
- energy
- consumption
- nvidia
- arxiv
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of systematic analysis of inference
  energy consumption in large language models (LLMs), despite its dominant role in
  total lifecycle energy usage. It presents over 32,500 measurements across 155 model
  architectures and 21 GPU configurations, using the vLLM inference engine to quantify
  energy usage at the prompt level.
---

# From Prompts to Power: Measuring the Energy Footprint of LLM Inference

## Quick Facts
- arXiv ID: 2511.05597
- Source URL: https://arxiv.org/abs/2511.05597
- Reference count: 40
- Primary result: Over 32,500 measurements across 155 model architectures and 21 GPU configurations quantify inference energy consumption at the prompt level

## Executive Summary
This study systematically analyzes the energy consumption of LLM inference, addressing a critical gap given that inference dominates total lifecycle energy usage. The research measures energy consumption across 155 model architectures and 21 GPU configurations using the vLLM inference engine, identifying key factors that shape energy demand. The findings reveal that output token length has a far greater impact on energy use than input length, and that batching significantly reduces per-prompt energy consumption. The authors develop a predictive model that accurately estimates inference energy consumption across unseen architectures and hardware configurations.

## Method Summary
The research collected over 32,500 measurements across 155 model architectures and 21 GPU configurations using the vLLM inference engine. Energy consumption was measured using CodeCarbon with NVML on Google Cloud VMs, tracking GPU power draw during inference. The study varied prompt configurations including input/output token lengths and batch sizes, then trained Random Forest and XGB Linear regressors using 5-fold, 5-repetition cross-validation. The predictive model uses features including token counts, batch size, model architecture specifications (layers, dimensions), and hardware specifications (memory bandwidth, TDP) to estimate energy consumption.

## Key Results
- Output token length has a far greater impact on LLM inference energy consumption than input token length
- Batching multiple prompts significantly reduces per-prompt energy consumption by amortizing fixed overheads
- A predictive model achieves strong generalization performance, with RMSE as low as 0.0057 Wh and MAPE as low as 10.48%
- The model is implemented as a browser extension to raise awareness of the environmental impact of generative AI usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Output token length has a far greater impact on LLM inference energy consumption than input token length.
- Mechanism: Each output token requires an autoregressive pass through the entire decoder stack, repeating for every generated token with attention over a growing KV cache. Input tokens are processed in a single parallel pass during prefill.
- Core assumption: Decode phase computational and energy costs scale linearly with generated tokens, dominating total cost over typical prompt lengths.
- Evidence anchors: A prompt with 900 output tokens and 100 input tokens consumes more than three times the energy of a prompt with 100 output tokens and 900 input tokens; output length identified as most influential factor in corpus citations.

### Mechanism 2
- Claim: Batching multiple prompts significantly reduces per-prompt energy consumption by amortizing fixed overheads and improving hardware utilization.
- Mechanism: Modern GPUs are massively parallel, allowing computation to be distributed across more cores when processing batches, increasing throughput and lowering energy per prompt.
- Core assumption: Inference engine can efficiently manage memory and KV cache for concurrent requests without latency constraints forcing small batch sizes.
- Evidence anchors: A batch of 100 prompts takes 29.4s vs 15.4s for one; total inference time remains nearly the same as processing single prompts.

### Mechanism 3
- Claim: Architectural dimensionality (hidden/intermediate size) influences energy consumption quadratically, while number of layers influences it linearly.
- Mechanism: Core transformer operations like self-attention and feed-forward projections involve matrix multiplications with computational complexity scaling quadratically with internal dimensions.
- Core assumption: Compute operations are the primary driver of energy consumption rather than data movement.
- Evidence anchors: Dimensionality has a quadratic effect identified in findings; linearity with layers discussed in methodology.

## Foundational Learning

- Concept: **LLM Inference Phases (Prefill vs. Decode)**
  - Why needed here: The core finding that output tokens cost more energy stems from the iterative, sequential nature of the decode phase versus the parallel prefill phase.
  - Quick check question: Why is generating one output token typically more energy-intensive than processing one input token?

- Concept: **GPU Utilization and Batching**
  - Why needed here: The paper's main optimization insight is that batching improves efficiency, requiring understanding that GPU power draw is not zero when idle and parallelism increases useful work per joule.
  - Quick check question: If a GPU consumes 300W, does it use less total energy to process 10 prompts in a batch or one at a time? Explain.

- Concept: **Model Architecture Components (Layers, Dimensions)**
  - Why needed here: The paper deconstructs "model size" to show how specific architectural choices drive energy costs differently (linear vs. quadratic).
  - Quick check question: According to the findings, would halving the number of layers or halving the hidden dimension have a greater impact on reducing energy? Why?

## Architecture Onboarding

- Component map:
  - vLLM inference engine -> GPU accelerators (A100, H100) -> Model architectures (155 models) -> Predictive model

- Critical path:
  1. Deploy vLLM on target GPUs
  2. Load a model architecture
  3. Process prompts, batching them automatically
  4. Measure GPU power draw and calculate total energy per inference run
  5. Use the predictive model to estimate energy for new configurations

- Design tradeoffs:
  - Batch Size vs. Latency: Larger batches improve energy efficiency but increase per-prompt latency
  - GPU Provisioning: Over-provisioning GPUs for small models reduces efficiency due to overhead
  - Precision (Quantization): Lower precision saves energy and memory but can reduce model quality

- Failure signatures:
  - KV Cache Thrashing: Erratic, high energy per prompt caused by insufficient memory for concurrent requests
  - GPU Under-utilization: Consistently high energy per prompt for small models on large clusters
  - Prediction Failure: Large errors on unseen architectures indicating missing features in predictive model

- First 3 experiments:
  1. Baseline Token Scaling: Measure energy for a standard model across input/output token lengths (100-900) on a single GPU
  2. Batch Scaling Analysis: Vary concurrent prompts (batch size 1-200) for a fixed prompt configuration to find efficiency sweet spot
  3. Architecture Ablation: Create modified versions of a base model by changing layer count and hidden dimension independently to validate linear vs. quadratic scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating full system-level energy profiling (CPU, RAM, disk) alter the relative energy efficiency rankings of different LLM architectures compared to GPU-only measurements?
- Basis in paper: Authors state in conclusion that future work should move beyond GPU-only measurements to encompass full system-level energy profiling.
- Why unresolved: Current study isolated GPU consumption because cloud-based virtualization prevented access to physical hardware metrics for other components.
- What evidence would resolve it: Measurement study on bare-metal hardware or cloud instances with RAPL access, aggregating CPU and RAM consumption alongside GPU data.

### Open Question 2
- Question: How do performance-energy trade-offs vary systematically across a wider range of specific tasks and real-world deployment conditions?
- Basis in paper: Authors note in Section VI that future work should investigate these trade-offs more systematically across a wider range of tasks and deployment conditions.
- Why unresolved: Current analysis relies on three specific benchmarks (Chatbot Arena, MMLU Pro, WinoGrande) and does not capture diversity of specialized real-world applications.
- What evidence would resolve it: Evaluation of models across comprehensive suite of domain-specific benchmarks correlated with per-task energy measurements.

### Open Question 3
- Question: Can a generalizable predictive model accurately estimate inference energy while accounting for specific impacts of diverse quantization techniques?
- Basis in paper: Section V states predictive model does not explicitly consider quantization due to lack of sufficient data, despite quantization being a major efficiency factor identified in Section IV-D.
- Why unresolved: Current predictive model excludes quantization parameters to avoid overfitting and inaccuracy due to insufficient training data covering methods like AWQ and GPTQ.
- What evidence would resolve it: Predictive model trained on dataset with sufficient coverage of different quantization schemes and bit-depths, demonstrating high accuracy against held-out quantized models.

## Limitations

- Energy Model Generalization: Predictive model's ability to handle entirely novel model designs (e.g., Mixture-of-Experts, new attention mechanisms) remains untested
- Real-world Operational Factors: Measurements don't account for network latency, model loading, serving infrastructure, or concurrent workloads on shared cloud hardware
- Hardware Dependency: Study limited to NVIDIA GPUs; relationship between architectural features and energy on other hardware (AMD GPUs, specialized AI accelerators) is unknown

## Confidence

- **High Confidence:**
  - Output token length is the dominant factor in inference energy consumption
  - Architectural dimensionality has a quadratic effect on energy while layer count has a linear effect
  - Predictive model achieves stated performance metrics on held-out data

- **Medium Confidence:**
  - Batching significantly reduces per-prompt energy consumption
  - vLLM engine with continuous batching and PagedAttention is optimal for minimizing energy

## Next Checks

1. Validate Generalization to Novel Architectures: Test trained predictive model on Mixture-of-Experts architectures and models with different attention mechanisms to quantify generalization ability

2. Measure Real-world Serving Energy: Deploy models in production-like environment including network data transfer, model loading, and serving infrastructure to assess practical impact of findings

3. Replicate on Alternative Hardware: Repeat measurement and modeling process on AMD Instinct accelerators or cloud TPUs to understand hardware dependency of findings