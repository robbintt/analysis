---
ver: rpa2
title: 'InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback'
arxiv_id: '2505.23950'
source_url: https://arxiv.org/abs/2505.23950
tags:
- multi-turn
- human
- multimodal
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INTERMT, the first human preference dataset
  for multi-turn multimodal interaction, addressing the lack of long-horizon, interleaved
  multimodal understanding and generation data. It contains 15.6k prompts, 52.6k multi-turn
  QA instances, and 32.4k human-labeled preference pairs across nine dimensions at
  both local and global levels.
---

# InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback

## Quick Facts
- **arXiv ID:** 2505.23950
- **Source URL:** https://arxiv.org/abs/2505.23950
- **Reference count:** 40
- **Primary result:** Introduces the first human preference dataset for multi-turn multimodal interaction (INTERMT) with 15.6k prompts, 52.6k QA instances, and 32.4k preference pairs across 9 dimensions.

## Executive Summary
This paper addresses the critical gap in multi-turn multimodal preference alignment by introducing INTERMT, the first human preference dataset specifically designed for long-horizon, interleaved multimodal understanding and generation. The dataset contains 15.6k prompts generating 52.6k multi-turn QA instances, with 32.4k human-labeled preference pairs across nine dimensions at both local and global levels. To construct this dataset, the authors develop an agentic workflow using tool-augmented MLLMs (including image generation, editing, and retrieval tools) to simulate high-quality multi-turn interactions. The paper also introduces INTERMT-Bench to evaluate MLLMs' ability to judge multi-turn multimodal tasks, revealing that current models struggle with long-horizon alignment while excelling at recognizing crucial steps.

## Method Summary
The INTERMT dataset was constructed through a four-stage agentic workflow: (1) Question Collection using seed questions from open-source VL datasets, web-scraping, and human-written prompts; (2) Agentic Workflow employing tool-augmented MLLMs (Qwen2-VL, GPT-4o, Gemini) with three tools (FLUX.1-Schnell, Stable-Diffusion, Gemini-2.0-flash editing, web retrieval) to generate multi-turn QA instances in a tree structure; (3) Preference Annotation where human annotators evaluated responses across 9 dimensions (4 local, 5 global) and labeled pairwise preferences; (4) Tree Reconstruction to create the final dataset. Judge models were trained using Qwen2.5-VL-3B/7B with prefix-preference objectives on local annotations, evaluated on held-out splits without seed question overlap.

## Key Results
- Modeling fine-grained local (turn-level) preferences is more effective than directly modeling global (conversation-level) preferences for capturing human values in multi-turn multimodal interactions
- Judge moderation exhibits a multi-turn scaling law: accuracy improves with more training turns but generalizes less well to longer evaluation horizons
- Current MLLMs struggle with long-horizon preference alignment but excel at recognizing crucial steps in multi-turn multimodal tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling fine-grained local (turn-level) preferences is more effective than directly modeling global (conversation-level) preferences for aligning multi-turn multimodal interactions.
- **Mechanism:** Local preferences decompose complex, long-horizon human intent into tractable, per-turn signals. This allows a reward or judge model to learn robust, fine-grained alignment criteria (e.g., image-text consistency, contextual coherence) that aggregate better than trying to learn an end-to-end conversation-level reward from sparse global labels.
- **Core assumption:** User intent and quality can be meaningfully assessed at each turn and these local assessments are predictive of overall conversation quality.
- **Evidence anchors:** [abstract] "Experiments reveal that modeling fine-grained local preferences is more effective than direct global modeling"; [section 4.1] "Our findings... suggest that modeling fine-grained local (turn-level) preferences is more effective in capturing human values."

### Mechanism 2
- **Claim:** A "multi-turn scaling law" exists for judge moderation: accuracy improves with more training turns, but generalization to longer evaluation horizons diminishes.
- **Mechanism:** Training a judge model on more turns of conversation exposes it to a richer context, enabling better prediction of subsequent preferences. However, this benefit comes with reduced generalization because the model overfits to the specific turn-length distribution and context drift patterns seen during training.
- **Core assumption:** The patterns and preference dynamics in the first k turns of a conversation are indicative of, and can predict, preferences in later turns.
- **Evidence anchors:** [abstract] "...demonstrate a multi-turn scaling law in judge moderation—accuracy improves with more training turns but generalizes less well to longer evaluations"; [section 4.1, Figure 7] "As training turns increase, model's ability to predict future preferences improves (left), while generalization diminishes as evaluation turns increases (right)."

### Mechanism 3
- **Claim:** An agentic workflow using tool-augmented MLLMs is a viable method for constructing high-quality, multi-turn, interleaved multimodal QA data.
- **Mechanism:** Since no single model excels at long-horizon interleaved understanding and generation, the workflow orchestrates multiple specialized components: (1) LLMs for question generation and response drafting, (2) external tools to produce visual content, and (3) validation filters for quality and coherence. This simulates the branching, tool-using behavior of an ideal assistant, creating a diverse "tree" of interaction paths.
- **Core assumption:** The outputs of this synthetic workflow are sufficiently realistic and high-quality to serve as a substrate for human preference annotation.
- **Evidence anchors:** [abstract] "To compensate for the lack of capability for multi-modal understanding and generation, we introduce an agentic workflow that leverages tool-augmented MLLMs to construct multi-turn QA instances"; [section 2.2, Figure 3] Details the four-stage pipeline.

## Foundational Learning

- **Concept: Bradley-Terry (BT) Preference Model**
  - **Why needed here:** It is the foundational probabilistic model used to derive a scalar reward or preference score from pairwise human comparisons (the core data type in InterMT). Understanding it is essential for implementing the preference modeling experiments.
  - **Quick check question:** Given a BT model where response A has a reward of 2 and response B has a reward of 1, what is the probability a human prefers A over B? (Answer: $\frac{e^2}{e^2 + e^1} \approx 0.73$)

- **Concept: Multi-turn Dialogue Context Management**
  - **Why needed here:** The core challenge is modeling preferences that evolve over many turns. An engineer must understand how to feed conversation history into a model and how reward signals might be accumulated or discounted across turns.
  - **Quick check question:** In a multi-turn setup, why might applying a single reward at the end of the conversation be problematic? (Answer: Credit assignment is difficult; it's unclear which turns contributed positively/negatively to the final outcome.)

- **Concept: Interleaved Multimodal Representation**
  - **Why needed here:** InterMT's data consists of text and images interspersed in both user inputs and model outputs. Systems must handle a sequence where the modality can change at any token (e.g., `<text>, <image>, <text>`).
  - **Quick check question:** How does representing an image in a conversation differ from simply including its caption? (Answer: Direct image tokens preserve visual information and alignment cues that text captions may lose, which is critical for tasks like visual consistency checking.)

## Architecture Onboarding

- **Component map:** Seed questions → Agentic Workflow (MLLMs + Tools) → Preference Annotation Platform → Preference Modeling Suite → INTERMT-Bench
- **Critical path:** The fidelity of the entire pipeline hinges on the **Human Annotation Platform**. The synthetic agentic workflow generates candidates, but only human preference annotation injects the ground-truth alignment signal. Poor annotation guidelines or annotator fatigue directly corrupt the dataset.
- **Design tradeoffs:**
  - **Synthetic vs. Human-Human Dialogues:** Chose AI-synthesized dialogues annotated by humans, citing 7-10x cost increase for pure human-human data collection. Tradeoff: potential model bias vs. prohibitive cost.
  - **Local vs. Global Annotation:** Annotating both levels provides rich data but increases complexity and cost. Tradeoff: computational simplicity vs. data richness/nuance.
  - **Tool Diversity vs. Pipeline Complexity:** Using many image generation/editing tools increases output diversity but complicates the agentic workflow and quality control filtering.
- **Failure signatures:**
  - **Low Inter-Annotator Agreement:** Indicates ambiguous guidelines or overly subjective tasks. Check agreement rates per dimension (Table 6).
  - **Position/High-Score Bias in Judge Models:** If a judge model always prefers the first response or assigns uniformly high scores, it indicates it has not learned the nuanced preference dimensions.
  - **Collapsing Diversity in Generated Dialogues:** If follow-up questions or image styles become repetitive across the dataset, the agentic workflow is failing to explore the space.
- **First 3 experiments:**
  1. **Validate Local vs. Global Preference Transfer:** Train two reward models—one on local preference pairs, one on global. Evaluate both on held-out local and global test sets to confirm local-to-general transfer is stronger.
  2. **Probe the Scaling Law:** Train a series of judge models on 1, 2, 3, and 4 turns of conversation history. Evaluate each on fixed test sets of varying lengths (e.g., 2, 4, 6 turns) to reproduce the scaling and generalization curves from Figure 7.
  3. **Benchmark a Judge Model with InterMT-Bench:** Use an off-the-shelf MLLM (e.g., GPT-4o or a fine-tuned Qwen-VL) and run it through the three tasks in InterMT-Bench (Scoring Evaluation, Pair Comparison, Crucial Step Recognition). Compare its performance to the baselines in Table 1.

## Open Questions the Paper Calls Out
1. **How can we effectively model long-horizon preferences in multi-turn multimodal interactions using the local and global human annotations provided in InterMT?** [explicit] Section 4 explicitly lists this as a primary direction for future research. While the paper provides baseline preference modeling strategies, it notes that current models struggle to align with long-horizon human values and that directly modeling global preferences often fails to capture nuanced dynamics.

2. **How can the InterMT framework be extended to encompass video and audio modalities to provide a more holistic representation of communication dynamics?** [inferred] Section 5.1 (Limitations) states that human communication extends beyond vision and language, noting that "Future work is essential to extend INTERMT to encompass these additional modalities." The current dataset and agentic workflow are specifically tailored for vision-language tasks, whereas real-world interaction involves temporal dynamics and prosodic cues found in audio and video.

3. **What advanced alignment algorithms can most effectively incorporate the structured language feedback provided in InterMT to enhance MLLM performance?** [explicit] Section 4 asks, "How can we design algorithms that effectively incorporate real human feedback from INTERMT to assess and enhance the performance of MLLMs?" The paper introduces the dataset and benchmark, but the application section focuses on basic preference modeling and supervised fine-tuning, leaving the integration of complex, structured natural language critiques into the training loop largely unexplored.

## Limitations
- The synthetic nature of the training data may introduce model biases that don't fully capture genuine human-human multi-turn interactions
- Human annotation across nine dimensions introduces inter-annotator variability that may not be fully characterized
- The scaling law's generalization limitation suggests the dataset may be insufficiently diverse to capture long-horizon preference dynamics

## Confidence

- **High confidence:** The dataset construction methodology and the empirical observation that local preference modeling outperforms global modeling
- **Medium confidence:** The multi-turn scaling law and its generalization implications, as these are based on controlled experiments but may not fully capture real-world deployment scenarios
- **Medium confidence:** The agentic workflow's ability to generate high-quality synthetic dialogues, as this depends on the capabilities and biases of the underlying MLLMs and tools used

## Next Checks
1. **External validity check:** Test the trained judge models on a small set of human-human multi-turn dialogues (even if collected post-hoc) to assess whether the synthetic preference patterns generalize to genuine interactions
2. **Bias analysis:** Conduct a systematic analysis of the synthetic dialogues for systematic artifacts or biases introduced by the agentic workflow (e.g., repetitive patterns, unrealistic tool usage) that could skew the preference annotations
3. **Annotation robustness test:** Perform an independent annotation of a held-out subset of the dataset by a new group of annotators to measure inter-annotator agreement and identify dimensions with high subjectivity that might undermine the dataset's reliability