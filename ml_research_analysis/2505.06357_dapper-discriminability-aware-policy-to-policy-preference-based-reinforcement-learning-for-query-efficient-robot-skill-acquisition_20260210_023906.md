---
ver: rpa2
title: 'DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement
  Learning for Query-Efficient Robot Skill Acquisition'
arxiv_id: '2505.06357'
source_url: https://arxiv.org/abs/2505.06357
tags:
- policy
- learning
- query
- queries
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low query efficiency in preference-based
  reinforcement learning (PbRL) for legged robot skill acquisition. The core issue
  stems from policy bias in traditional PbRL, where a single policy limits trajectory
  diversity, reducing the number of discriminable queries available for learning human
  preferences.
---

# DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition

## Quick Facts
- arXiv ID: 2505.06357
- Source URL: https://arxiv.org/abs/2505.06357
- Reference count: 20
- This paper addresses the problem of low query efficiency in preference-based reinforcement learning (PbRL) for legged robot skill acquisition.

## Executive Summary
This paper addresses the problem of low query efficiency in preference-based reinforcement learning (PbRL) for legged robot skill acquisition. The core issue stems from policy bias in traditional PbRL, where a single policy limits trajectory diversity, reducing the number of discriminable queries available for learning human preferences. The proposed method, DAPPER, improves query efficiency by integrating preference discriminability with trajectory diversification. It generates queries by comparing trajectories from multiple policies trained from scratch, avoiding policy bias. A discriminator estimates preference discriminability, enabling prioritized sampling of more discriminable queries. The framework jointly maximizes preference reward and discriminability score during training. Experiments in simulated and real-world legged robot environments demonstrate that DAPPER outperforms previous methods in query efficiency, particularly under challenging preference discriminability conditions.

## Method Summary
DAPPER is a preference-based reinforcement learning framework that addresses policy bias by training multiple diverse policies from scratch and using a discriminator to estimate preference discriminability. The method maintains a policy dataset and uses discriminability-weighted sampling to select informative queries. During policy training, a combined reward signal incorporates both preference reward (learned from human labels) and discriminability reward (from the discriminator). The framework iteratively trains policies, collects human preferences, updates the reward models, and uses the learned discriminability estimates to prioritize future queries. The approach is implemented using Lagrangian PPO with constraints to prevent unrealistic behaviors that would hinder query labeling.

## Key Results
- DAPPER reduces the number of queries needed to reach target feature error from 1,300-1,500 queries (baseline) to 300-400 queries for posture control tasks
- The discriminator successfully filters indistinguishable queries, reducing indiscriminable rates from 47% to 23% in vision-language model labeling experiments
- Joint optimization with discriminability reward (β=0.6) maintains exploration while converging to target features within 500 queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training multiple policies from scratch reduces policy bias and increases trajectory diversity for more discriminable queries.
- Mechanism: Each time the reward model is updated, DAPPER retrains a new policy from scratch rather than incrementally updating the previous policy. This breaks the correlation between successive policies that causes behavioral similarity in standard PbRL. The diverse policies populate a policy dataset Π used for cross-policy trajectory comparisons.
- Core assumption: Policy bias is the primary limiter of behavioral diversity in PbRL, and training from scratch yields sufficiently different behaviors without requiring explicit diversity objectives.
- Evidence anchors: [abstract] "train multiple diverse policies from scratch and generate queries by comparing trajectories from different policies"; [Section IV-A] "Multiple policies with distinct parameters θ_i are trained for use in query comparisons"

### Mechanism 2
- Claim: A learned discriminator enables prioritized sampling of queries that humans can reliably label.
- Mechanism: The discriminator D takes trajectory pairs as input and estimates the probability they are discriminable. It is trained on historical query data where labels y ∈ {0.5, other} indicate indistinguishable queries. During query collection, policies are sampled proportionally to their predicted discriminability (Eq. 6), filtering out pairs that would yield uninformative responses.
- Core assumption: Past indistinguishability patterns generalize to future query pairs, and discriminability can be approximated as a learnable function of trajectory features.
- Evidence anchors: [abstract] "employs a discriminator that learns to estimate preference discriminability, enabling the prioritized sampling of more discriminable queries"; [Section IV-E] "The discriminator D is trained on the query dataset D, where the label y can take inseparable values y ∈ {0.5, other}"

### Mechanism 3
- Claim: Jointly optimizing for preference reward and discriminability produces policies that are both high-performing and easy to compare.
- Mechanism: The policy reward r combines preference reward R_H (learned from human labels) with discriminability reward R_D (from the discriminator) via r = (1-β)R_H + βR_D (Eq. 4). R_D encourages the current policy to remain distinguishable from past policies in Π, creating an exploration pressure toward novel, discriminable behaviors.
- Core assumption: The trade-off parameter β can be tuned to balance convergence to preferred behaviors while maintaining sufficient diversity for informative queries.
- Evidence anchors: [abstract] "jointly maximizes the preference reward and preference discriminability score"; [Section IV-B] "r = (1-β)R_H + βR_D" with β = 0.6 in experiments

## Foundational Learning

- Concept: Preference-based Reinforcement Learning (PbRL)
  - Why needed here: DAPPER builds directly on PbRL's query-response loop; understanding how reward models are learned from trajectory comparisons is prerequisite.
  - Quick check question: Can you explain how binary preferences over trajectory pairs are converted into a reward function via the Bradley-Terry model (Eq. 1)?

- Concept: Policy Bias in RL
  - Why needed here: The core problem DAPPER addresses—understanding why incremental policy updates produce correlated behaviors helps motivate the from-scratch training approach.
  - Quick check question: Why does updating a single policy iteratively with a changing reward function tend to produce similar behaviors across iterations?

- Concept: Proximal Policy Optimization (PPO) with Constraints
  - Why needed here: DAPPER uses Lagrangian PPO (LPPO) for policy learning with constraint terms that prevent unrealistic behaviors that would hinder query labeling.
  - Quick check question: How does LPPO differ from standard PPO in handling hard constraints on policy behavior?

## Architecture Onboarding

- Component map: Policy π_i -> Policy Dataset Π -> Trajectory Generator G(π) -> Preference Reward Model R_H; Discriminator D -> Feature Extractor f

- Critical path: 1. Train policy π_i using combined reward r = (1-β)R_H + βR_D until convergence; 2. Store π_i in policy dataset Π; 3. Sample policy π_x from Π using discriminability-weighted sampling (Eq. 6); 4. Generate trajectories τ_x, τ_i and collect human label y ∈ {0, 0.5, 1}; 5. Update R_H from preference labels and D from discriminability labels; 6. Repeat from step 1 with new reward models

- Design tradeoffs:
  - Computational cost vs. diversity: Retraining from scratch each iteration costs ~12-13 minutes but ensures diversity (Section V-F)
  - β parameter: Higher β improves exploration but risks convergence issues (Section V-G recommends β=0.6)
  - Temperature α: Controls sampling sharpness; α=10^-3 balances discriminability focus with query diversity

- Failure signatures:
  - Low discrimination rate (<50%): Discriminator not filtering effectively; check training data balance
  - Policy convergence to similar features despite from-scratch training: R_D signal may be weak or β too low
  - Query exhaustion: All policy pairs marked indistinguishable; discriminability threshold d_disc may be too high for current feature space

- First 3 experiments:
  1. Reproduce the "Posture" task with simulated annotator (d_disc="Medium") to validate query efficiency gains over Baseline; target feature error d_pref < 0.02 within 500 queries
  2. Ablate R_D component to measure its contribution; expect ~3x more queries required (Section V-D, Fig. 4a)
  3. Test VLM-based labeling on the same task; expect ~23% indiscriminable rate with DAPPER vs. ~47% with Baseline (Table IV)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational cost of retraining policies from scratch be reduced via warm-starting or meta-learning without sacrificing the diversity required to avoid policy bias?
- Basis in paper: [Explicit] The authors identify the trade-off between computational cost and policy diversity, suggesting "potential directions include warm-starting policy initialization... or leveraging meta-learning techniques."
- Why unresolved: The current DAPPER framework retrains policies from scratch in every iteration to ensure diversity, which is computationally expensive compared to incremental updates.
- What evidence would resolve it: A study demonstrating that a warm-started or meta-learned initialization achieves equivalent query efficiency and trajectory diversity with significantly lower wall-clock time per iteration.

### Open Question 2
- Question: How does the framework perform as the dimensionality of the feature space scales significantly beyond the six features tested?
- Basis in paper: [Explicit] In Section V-I, the authors note that "investigating this potential limitation remains an important direction for future work" regarding the increase in required queries as feature dimensions rise.
- Why unresolved: The experiments were limited to two, four, and six feature dimensions; higher dimensions may broaden the human "indistinguishability" threshold, potentially causing the discriminator to fail.
- What evidence would resolve it: Successful convergence of the policy in a complex manipulation or locomotion task requiring significantly higher feature dimensions (e.g., >10) without a prohibitive increase in query count.

### Open Question 3
- Question: Can the dependency on hand-crafted feature extractors be removed by integrating automated feature extraction from high-dimensional sensor data?
- Basis in paper: [Explicit] The Discussion states that the "overall performance is sensitive to the quality of this feature selection" and lists "integrating automated feature extraction" as a promising direction.
- Why unresolved: The current method relies on specific, manually defined features (e.g., body height, gait cycle), making it difficult to apply to tasks where optimal features are not known in advance.
- What evidence would resolve it: An implementation using learned embeddings from vision or sensor data that matches or exceeds the performance of the hand-crafted features.

## Limitations

- The approach relies heavily on the assumption that training policies from scratch generates sufficient behavioral diversity to overcome policy bias, with no direct measurement of policy diversity metrics.
- The computational cost of retraining policies from scratch each iteration (12-13 minutes per policy) may limit scalability to more complex tasks or higher-dimensional state spaces.
- The effectiveness of the discriminator for query prioritization depends on its ability to generalize discriminability estimates across diverse behavioral regimes, which may degrade as the policy dataset grows large.

## Confidence

- **High confidence**: The core mechanism of combining preference reward with discriminability reward, and the experimental demonstration of improved query efficiency in the simulated tasks. The mathematical formulation (Eqs. 1-8) is clearly specified and internally consistent.
- **Medium confidence**: The claim that training from scratch sufficiently reduces policy bias. While the experimental results support this, the analysis doesn't directly measure policy diversity metrics or compare against alternative diversity-inducing methods.
- **Medium confidence**: The effectiveness of the discriminator for query prioritization. The discriminator improves indiscriminable rates, but the analysis doesn't explore failure modes where the discriminator might overfit or become unstable.

## Next Checks

1. **Diversity Analysis**: Measure pairwise behavioral distance (e.g., feature space distance) between policies trained from scratch to quantify the actual diversity gained. Compare against baseline where policies are incrementally updated.

2. **Discriminator Robustness Test**: Evaluate discriminator performance on out-of-distribution policy pairs not seen during training. Test whether discriminability estimates degrade as Π grows or when policies explore new behavioral regions.

3. **Scalability Assessment**: Run DAPPER on a more complex locomotion task (e.g., with obstacles or terrain variation) to evaluate whether the from-scratch training approach remains computationally tractable and whether discriminability-based sampling maintains its advantages in higher-dimensional feature spaces.