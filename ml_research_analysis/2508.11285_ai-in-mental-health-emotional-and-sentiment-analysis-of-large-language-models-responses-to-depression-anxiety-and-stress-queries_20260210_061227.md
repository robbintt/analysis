---
ver: rpa2
title: 'AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models''
  Responses to Depression, Anxiety, and Stress Queries'
arxiv_id: '2508.11285'
source_url: https://arxiv.org/abs/2508.11285
tags:
- health
- mental
- emotional
- sentiment
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed emotional and sentiment patterns in responses
  from eight large language models (LLMs) to mental health queries about depression,
  anxiety, and stress. A total of 2,880 responses were collected and scored using
  advanced NLP tools for sentiment and emotions.
---

# AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries

## Quick Facts
- arXiv ID: 2508.11285
- Source URL: https://arxiv.org/abs/2508.11285
- Reference count: 40
- Primary result: LLM emotional profiles vary widely; model selection is crucial for mental health applications

## Executive Summary
This study analyzes emotional and sentiment patterns in responses from eight large language models (LLMs) to mental health queries about depression, anxiety, and stress. A total of 2,880 responses were collected and scored using advanced NLP tools for sentiment and emotions. The analysis revealed that optimism, fear, and sadness were the dominant emotional expressions, with neutral sentiment consistently high. LLM choice significantly influenced emotional tone—Llama was most optimistic and joyful, while Mixtral expressed the most negative emotions like disapproval and annoyance. Anxiety prompts triggered the highest fear scores (0.974), depression the highest sadness (0.686) and negative sentiment, and stress the highest optimism (0.755). Demographic framing had only marginal effects. The results emphasize that LLM emotional profiles vary widely and that model selection is crucial in mental health applications to ensure supportive, appropriate user interactions.

## Method Summary
The study generated 20 mental health questions across depression, anxiety, and stress conditions, each framed with six demographic variations (baseline, woman, man, young, old, university student), yielding 360 unique prompts. These prompts were submitted to eight LLMs (Claude 3.5-Sonnet, Copilot, Gemini 1.5 Pro, GPT-4o, GPT-4o mini, Llama 3.1-405B, Mixtral 8x7b, Perplexity) via OpenRouter API or official interfaces, collecting 2,880 responses. Responses were analyzed using Twitter-roBERTa-base for sentiment (positive/negative/neutral) and cardiffnlp/twitter-roberta-large-emotion-latest plus GoEmotions-based models for emotions (11 and 27 categories respectively). Statistical comparisons were conducted using Mann-Whitney U tests due to non-normal distributions.

## Key Results
- LLM choice significantly influences emotional tone: Llama showed highest optimism (0.716) and joy (0.236), while Mixtral displayed highest disapproval (0.346), annoyance (0.212), and sadness (0.493)
- Condition-dependent emotional mirroring: Anxiety prompts elicited highest fear scores (0.974), depression prompts generated most sadness (0.686), and stress prompts produced most optimism (0.755)
- Demographic framing had minimal impact on emotional tone, with only marginal variations across categories
- Neutral sentiment remained dominant across all models (0.599 overall), with GPT-4o showing highest neutral sentiment at 0.662

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Affective Mirroring
- Claim: LLMs exhibit emergent emotional adaptation where they mirror the dominant affective tone associated with specific mental health conditions.
- Mechanism: The model retrieves and amplifies emotionally congruent language patterns from training data containing mental health discussions—fear language for anxiety, sadness language for depression, solution-oriented language for stress.
- Core assumption: Training corpora contain sufficient condition-specific emotional language for models to learn associative patterns between mental health terms and corresponding emotional expressions.
- Evidence anchors:
  - [abstract] "Anxiety prompts elicited the highest fear scores (0.974), depression prompts generated the most sadness (0.686), and stress prompts produced the most optimistic responses (0.755)"
  - [section 4.2] Depression queries suppressed fear to 0.324 while anxiety generated fear at 0.974; stress queries produced lowest pessimism at 0.075
  - [corpus] Related paper "Detecting Emotion Drift in Mental Health Text" confirms nuanced emotional shifts in mental health language, supporting the premise that condition-specific emotional patterns exist in training data
- Break condition: If models were tested on novel or ambiguous mental health conditions not well-represented in training data, emotional mirroring would likely weaken or produce inconsistent patterns.

### Mechanism 2: Architecture-Training Emotional Entrenchment
- Claim: Each LLM exhibits a stable "emotional signature" determined by its architecture, training objectives, and safety alignment, which persists across query types.
- Mechanism: RLHF (Reinforcement Learning from Human Feedback) and safety tuning create persistent response patterns—Claude's helpfulness focus, Llama's community-oriented positivity, Mixtral's less constrained output space—that bias emotional expression independently of prompt content.
- Core assumption: Safety and helpfulness training procedures create systematic emotional biases that remain stable across diverse mental health contexts.
- Evidence anchors:
  - [abstract] "Llama displaying the most optimism and joy, while Mixtral showed the highest levels of negative emotions"
  - [section 4.1] Llama achieved highest optimism (0.716) and joy (0.236); Mixtral showed highest disapproval (0.346), annoyance (0.212), and sadness (0.493)
  - [corpus] Weak corpus evidence—related papers focus on single-model evaluations rather than comparative emotional signatures across architectures
- Break condition: If models undergo significant retraining or fine-tuning for mental health applications, these baseline signatures would shift.

### Mechanism 3: Demographic Prompt Dilution
- Claim: Surface-level demographic framing ("I am a woman suffering from depression") produces negligible emotional variation because LLMs do not maintain coherent user models across the context window.
- Mechanism: Demographic descriptors in prompts receive minimal attention weight compared to clinical content; models process condition terms (depression, anxiety) as primary semantic anchors while demographic modifiers are treated as low-salience context.
- Core assumption: Current LLM attention mechanisms and tokenization strategies do not systematically weight demographic identity markers as emotionally salient.
- Evidence anchors:
  - [abstract] "Demographic framing had minimal impact on emotional tone"
  - [section 4.2] Only marginal variations observed: "Old" demographic showed highest positive sentiment (0.189) but differences were statistically small; most demographic effects reached significance only at p < 0.05 with small effect sizes
  - [corpus] No direct corpus support for this mechanism; related papers do not examine demographic framing effects on LLM emotional responses
- Break condition: If prompts included extended demographic context (e.g., detailed personal narratives) rather than single-phrase modifiers, emotional variation might increase.

## Foundational Learning

- Concept: **Sentiment vs. Emotion Analysis**
  - Why needed here: The paper distinguishes sentiment (positive/negative/neutral polarity) from emotion (fear, sadness, optimism, etc.). Understanding this distinction is essential for interpreting why neutral sentiment remained high (0.599) while specific emotions like fear (0.616) and sadness (0.437) showed condition-dependent variation.
  - Quick check question: If a response scores high on "neutral sentiment" but also high on "fear emotion," what does this indicate about the response's language?

- Concept: **RoBERTa-based Emotion Classification**
  - Why needed here: The study uses transformer-based models (Twitter-roBERTa-base, cardiffnlp/twitter-roberta-large-emotion-latest) trained on social media data. These models output probability distributions over emotion categories rather than binary classifications, enabling the fine-grained scoring (0.974 for fear) reported in results.
  - Quick check question: Why might a model trained on Twitter data be appropriate for analyzing LLM responses to mental health queries?

- Concept: **Affective Mirroring in Therapeutic Contexts**
  - Why needed here: The paper interprets condition-dependent emotional responses (fear for anxiety, sadness for depression) as a form of "affective mirroring." Understanding this therapeutic communication concept is necessary to evaluate whether the observed patterns represent appropriate empathic response or problematic emotional reinforcement.
  - Quick check question: In a therapeutic context, what are the potential risks of an AI system mirroring a user's negative emotional state?

## Architecture Onboarding

- Component map:
  Query Generation Module (20 questions × 3 conditions × 6 demographics) -> LLM Response Collection (8 models × 360 prompts) -> Sentiment Analyzer (Twitter-roBERTa-base) -> Emotion Analyzer 1 (cardiffnlp/twitter-roberta-large-emotion-latest) -> Emotion Analyzer 2 (GoEmotions-based model) -> Statistical Testing (Mann-Whitney U tests)

- Critical path:
  1. Generate all query variants with demographic framing
  2. Collect responses from each LLM with default parameters
  3. Run all three analysis models on each response
  4. Aggregate scores by LLM, condition, and demographic
  5. Apply nonparametric statistical tests

- Design tradeoffs:
  - **Automated vs. human evaluation**: Study uses automated emotion detection for scalability (2,880 responses) but acknowledges this may miss therapeutic nuance; human evaluation would provide clinical validity but reduce scope
  - **Twitter-trained vs. clinical models**: RoBERTa models trained on social media may capture colloquial emotional expression but may not align with clinical definitions of emotional states
  - **Single time-point vs. longitudinal**: Snapshot evaluation cannot detect emotional signature drift as models are updated

- Failure signatures:
  - **High variance in dominant emotions** (optimism range: 0.592-0.716 across models) indicates model selection is critical for consistent user experience
  - **Negative correlation between positive emotions** (approval negatively correlated with optimism) suggests models make implicit trade-offs between different supportive communication styles
  - **Neutral sentiment dominance** (GPT-4o at 0.662) may indicate emotional detachment perceived as clinical coldness

- First 3 experiments:
  1. **Replicate with extended demographic context**: Replace single-phrase framing ("I am a woman") with paragraph-length user profiles to test whether richer demographic context increases emotional variation.
  2. **Longitudinal emotional signature tracking**: Collect responses from the same 8 models monthly over 6 months to quantify emotional drift and stability of identified signatures.
  3. **Human evaluation of therapeutic appropriateness**: Have mental health professionals rate a subset of responses (e.g., top/bottom quartile by fear and sadness scores) to validate whether automated emotion scores correlate with clinical judgments of appropriateness.

## Open Questions the Paper Calls Out

- **Question**: How do repeated interactions with LLMs exhibiting specific emotional signatures (e.g., optimistic vs. negative) impact long-term user mental health outcomes?
  - Basis in paper: [explicit] The authors state in the Discussion that "longitudinal studies examining how repeated interactions with different emotional profiles impact user mental health outcomes would provide essential evidence for clinical deployment decisions."
  - Why unresolved: The current study provides a cross-sectional analysis of single-turn responses rather than tracking user health over time.
  - What evidence would resolve it: Longitudinal clinical trials monitoring user well-being and engagement metrics over extended periods of interaction with specific models.

- **Question**: Does intersectional demographic framing (e.g., combining age, gender, and occupation) reveal emotional biases that were masked by the single-attribute framing used in this study?
  - Basis in paper: [explicit] The authors explicitly suggest future research should "deepen the investigation of demographic factors to encompass intersectional identities."
  - Why unresolved: This study found minimal demographic impact but tested categories (Man, Woman, Young, etc.) in isolation rather than in combination.
  - What evidence would resolve it: Experiments utilizing multi-label demographic prompts (e.g., "I am an older male student") to detect compound biases in emotional tone.

- **Question**: What is the relationship between an LLM's displayed emotional congruence and the clinical safety or accuracy of its mental health advice?
  - Basis in paper: [inferred] The authors acknowledge a limitation in Section 5, noting the study focused on emotional content "rather than clinical accuracy or therapeutic appropriateness, which remain critical areas for investigation."
  - Why unresolved: A model might successfully mirror a user's sadness (high emotional congruence) while still providing clinically unsafe or inaccurate suggestions.
  - What evidence would resolve it: Evaluation frameworks that simultaneously score LLM responses for both sentiment/emotion alignment and clinical safety validity.

## Limitations

- Automated emotion detection models trained on social media data may not accurately capture therapeutic nuances of mental health conversations
- Minimal demographic impact finding may reflect simplicity of single-phrase descriptors rather than true absence of demographic influence
- Cross-sectional design cannot capture temporal variations in model behavior or emotional signature evolution

## Confidence

- **High Confidence**: LLM choice significantly influences emotional tone (e.g., Llama's consistent optimism vs. Mixtral's negative emotions) with large effect sizes and clear between-model patterns
- **Medium Confidence**: Condition-dependent emotional mirroring (fear for anxiety, sadness for depression) is supported by data but requires validation for therapeutic appropriateness
- **Low Confidence**: Minimal impact of demographic framing is based on limited descriptors and may not generalize to richer contextual information

## Next Checks

1. Conduct human evaluation study with mental health professionals to validate whether automated emotion scores correlate with clinical assessments of therapeutic appropriateness and safety
2. Replicate the analysis with extended demographic context (detailed user narratives rather than single phrases) to determine if richer user information produces meaningful emotional variation
3. Perform longitudinal tracking of the same 8 models over 6+ months to quantify emotional signature stability and detect drift that may affect mental health applications