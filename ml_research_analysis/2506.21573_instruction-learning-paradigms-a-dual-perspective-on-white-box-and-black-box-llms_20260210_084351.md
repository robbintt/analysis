---
ver: rpa2
title: 'Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box
  LLMs'
arxiv_id: '2506.21573'
source_url: https://arxiv.org/abs/2506.21573
tags:
- instruction
- tasks
- performance
- across
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dual-perspective instruction learning framework
  that combines black-box initialization with white-box refinement to optimize instructions
  for large language models. The approach leverages ChatGPT for high-quality initial
  instructions and a white-box model for fine-grained representation learning through
  hidden states and output features.
---

# Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs

## Quick Facts
- arXiv ID: 2506.21573
- Source URL: https://arxiv.org/abs/2506.21573
- Reference count: 37
- Primary result: Dual-perspective framework achieves 0.6955 mean score across 30 tasks, outperforming InstructZero (0.6733) and EvoPrompt (0.6800)

## Executive Summary
This paper introduces a dual-perspective instruction learning framework that combines black-box initialization with white-box refinement to optimize instructions for large language models. The approach leverages ChatGPT for high-quality initial instructions and a white-box model for fine-grained representation learning through hidden states and output features. By incorporating semantic similarity constraints and iterative optimization, the method significantly improves instruction quality across 30 diverse tasks.

The framework addresses the challenge of automated instruction optimization by exploiting the complementary strengths of both paradigms: black-box models provide diverse, high-quality initializations while white-box models enable interpretable, fine-grained analysis through hidden representations. This hybrid approach achieves state-of-the-art performance with particular success on reasoning-intensive tasks like cause_and_effect (0.9467) and word_unscrambling (0.5900).

## Method Summary
The framework employs a hybrid pipeline that begins with black-box initialization using ChatGPT-4o to generate 40 diverse instructions per task. These instructions are then refined through white-box hidden-state analysis using Vicuna-13B, where hidden representations and output features are fused via concatenation and average pooling. A neural network trained with combined regression and similarity regularization predicts instruction quality scores, enabling iterative optimization. The process iterates between prediction, generation, evaluation, and retraining until convergence, ultimately producing optimized instructions that outperform existing baselines.

## Key Results
- Achieves mean score of 0.6955 across 30 tasks, outperforming InstructZero (0.6733) and EvoPrompt (0.6800)
- Black-box initialization with ChatGPT-4o produces superior initial instructions (0.6713 mean) compared to ChatGPT-3.5 (0.5645)
- Similarity regularization significantly improves performance on reasoning tasks (cause_and_effect: 0.9467 → 0.8133 without regularization)
- Hidden-output representation fusion provides consistent gains across task categories, particularly in object_counting (0.3433 with fusion vs 0.2600 without)

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Initialization-Refinement Pipeline
- Claim: Combining black-box initialization with white-box refinement appears to improve instruction quality more effectively than either paradigm alone, particularly for complex reasoning tasks.
- Mechanism: A black-box LLM (ChatGPT-4o) generates 40 diverse initial instructions using a crafted prompt template and 5 example input-output pairs. These instructions are then refined through white-box hidden-state analysis rather than relying solely on external evaluation signals.
- Core assumption: Black-box models produce semantically richer initial instructions than white-box models can generate independently, especially for tasks requiring complex reasoning.
- Evidence anchors:
  - [abstract] "Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features."
  - [section 3.1] "employing black-box models for initialization appears to be a more viable and effective solution... mitigates the risk of catastrophic failure during the instruction learning process."
  - [corpus] PRESTO (arXiv:2510.25808) similarly addresses black-box instruction optimization but does not integrate white-box hidden representations, suggesting this fusion is a differentiating factor.
- Break condition: If black-box initialization quality degrades (e.g., API changes, model version drift) or costs become prohibitive, the initialization phase may fail to provide sufficient diversity for effective white-box learning.

### Mechanism 2: Hidden-Output Representation Fusion
- Claim: Concatenating hidden representations with output representations through average pooling may provide more stable and informative features for instruction quality prediction than either representation alone.
- Mechanism: Hidden representation h is extracted from intermediate white-box layers (capturing semantic structure). Output representation o is derived from processing the black-box's output through the white-box. These are concatenated [h; o] and dimensionally reduced via average pooling to create fused representation c_ho.
- Core assumption: Hidden states encode instruction quality signals that are complementary to behavioral output features, and simple concatenation-plus-pooling preserves sufficient information for effective learning.
- Evidence anchors:
  - [section 3.2] "Empirical results demonstrate that the fused representation substantially boosts the model's prediction accuracy and lays a more stable foundation for subsequent iterative optimization processes."
  - [section 4.4, Table 2] Removing output representation drops mean performance from 0.6955 to 0.6717, with notable declines in object_counting (0.3433 → 0.2600).
  - [corpus] No direct corpus comparison for this specific fusion strategy was found, limiting external validation.
- Break condition: If the white-box model architecture changes significantly (affecting hidden state semantics) or if the pooling operation discards task-critical features, the fusion may lose discriminative power.

### Mechanism 3: Similarity-Regularized Neural Network Optimization
- Claim: Training a neural network to predict instruction scores with a similarity constraint may produce more consistent optimization trajectories than pure regression-based approaches.
- Mechanism: An MLP (N) predicts reward scores for instruction embeddings. The loss L combines regression loss (L_Reg) against true evaluation scores with similarity loss (L_Sim) that aligns embeddings with the best-performing instruction via cosine similarity. Dynamic weight adjustment (λ = max(a, min(b, L_Reg))) balances these components during training.
- Core assumption: Instructions semantically similar to high-performing examples are more likely to be effective, and this inductive bias helps guide exploration during iterative refinement.
- Evidence anchors:
  - [section 3.3] "Using cosine similarity, we can ensure that the generated instructions align in semantic direction with the effective instructions generated by ChatGPT."
  - [section 4.4, Table 2] Removing similarity regularization drops mean score from 0.6955 to 0.6840, with larger drops in cause_and_effect (0.9467 → 0.8133) and informal_to_formal (0.5791 → 0.4816).
  - [corpus] Related work on instruction optimization (InstructZero, INSTINCT) uses bandit algorithms or evolutionary approaches but does not explicitly incorporate similarity-constrained neural networks, suggesting this is a methodological contribution.
- Break condition: If the best-performing instruction embedding is itself suboptimal or if λ tuning fails to balance losses appropriately, similarity constraints could anchor learning to local optima.

## Foundational Learning

- Concept: **Hidden State Extraction from Transformer Layers**
  - Why needed here: The framework relies on extracting intermediate representations from a white-box LLM (Vicuna-13B). Understanding which layers capture semantic vs. syntactic information affects fusion quality.
  - Quick check question: Can you explain why average pooling across hidden states might lose positional or token-level information critical for certain tasks?

- Concept: **Cosine Similarity for Semantic Alignment**
  - Why needed here: The similarity loss constrains learned embeddings to remain semantically close to effective instructions. Understanding embedding space geometry is essential for debugging convergence issues.
  - Quick check question: If two instruction embeddings have cosine similarity of 0.95 but produce very different evaluation scores, what might this indicate about the embedding space or evaluation metric?

- Concept: **Iterative Optimization with Neural Surrogates**
  - Why needed here: The MLP serves as a surrogate model predicting instruction quality, enabling gradient-based updates without repeated black-box queries. Understanding surrogate accuracy vs. query efficiency tradeoffs is critical.
  - Quick check question: What failure mode might occur if the surrogate network overfits to early instruction samples during iterative refinement?

## Architecture Onboarding

- Component map:
  Initialization Module -> Representation Extractor -> Fusion Layer -> Scoring Network -> Evaluation Oracle -> Iterative Loop

- Critical path:
  1. Initialize 40 instructions via black-box API.
  2. Extract fused representations for each instruction.
  3. Evaluate instructions on task dataset to obtain scores.
  4. Train scoring network on (embedding, score) pairs.
  5. Iteratively: predict latent soft prompt → generate instruction → evaluate → retrain → update best.
  6. Terminate after T iterations or when performance threshold reached.

- Design tradeoffs:
  - **Black-box initialization cost vs. quality**: ChatGPT-4o produces better initial instructions (0.6713 mean) than ChatGPT-3.5 (0.5645) but at higher API cost.
  - **Network depth vs. overfitting**: Depth 5 achieves highest mean (0.6955) but may overfit on limited instruction samples per task.
  - **Similarity weight (λ) sensitivity**: Very low weights (10⁻¹⁰) degrade performance; dynamic adjustment based on L_Reg is critical.

- Failure signatures:
  - **Catastrophic initialization failure**: White-box-only initialization produces near-random instructions for complex tasks (e.g., second_word_letter drops to 0.4330 with InstructZero).
  - **Representation collapse**: If hidden and output representations are highly correlated, fusion provides no additional signal.
  - **Surrogate drift**: If the scoring network diverges from true evaluation scores, iterative updates may optimize for proxy rather than actual performance.

- First 3 experiments:
  1. **Ablate initialization source**: Run framework with ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, and DeepSeek V3 initialization on 5 diverse tasks. Measure mean score and variance to validate Table 2 claims.
  2. **Vary similarity weight λ**: Test λ ∈ {10⁻¹⁰, 0.01, dynamic} across reasoning-heavy tasks (cause_and_effect, word_unscrambling). Track convergence speed and final performance.
  3. **Cross-task transfer test**: Train scoring network on one task (e.g., sentiment), evaluate on related task (e.g., translation_en_es). Assess whether learned representations generalize or remain task-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework maintain high performance while significantly reducing the reliance on expensive black-box API calls for initialization and evaluation?
- Basis in paper: [explicit] The Conclusion states, "Looking ahead, we plan to reduce black-box dependencies..."
- Why unresolved: The current implementation relies on ChatGPT-4o for initialization and ChatGPT-3.5-Turbo for evaluation, which incurs the financial costs the paper aims to mitigate.
- Evidence: Experiments substituting the black-box components with strong open-source models (e.g., Llama 3) to compare performance degradation versus cost savings.

### Open Question 2
- Question: How can the dual-perspective mechanism be effectively integrated into reinforcement learning (RL) frameworks?
- Basis in paper: [explicit] The Conclusion proposes extending the framework to "broader instruction-generation scenarios, including reinforcement learning."
- Why unresolved: The current method uses a neural network to predict scores via regression, rather than optimizing a policy through reward signals as is standard in RL.
- Evidence: An implementation of the hidden-state fusion strategy within an RLHF (Reinforcement Learning from Human Feedback) or PPO (Proximal Policy Optimization) loop.

### Open Question 3
- Question: Why does the proposed semantic similarity constraint fail to improve performance on specific semantic tasks like `sentence_similarity` and `synonyms`?
- Basis in paper: [inferred] Table 1 shows the method scores 0.0 on `sentence_similarity` and 0.1567 on `synonyms`, significantly underperforming compared to baselines.
- Why unresolved: The paper does not analyze why the similarity constraint, which theoretically aligns semantics, fails or degrades performance in these specific linguistic categories.
- Evidence: An ablation study analyzing the learned representations for these specific tasks to determine if the fused embeddings lose critical semantic nuance.

## Limitations
- The framework's heavy reliance on external APIs (ChatGPT-4o, ChatGPT-3.5-Turbo) introduces variability and cost concerns that limit reproducibility and scalability.
- Several critical implementation details remain unspecified, including neural network architecture parameters, specific Vicuna-13B layers for hidden state extraction, and exact hyperparameter settings.
- The method shows inconsistent performance across task categories, with notable failures on semantic tasks like sentence_similarity and synonyms despite theoretical advantages from similarity regularization.

## Confidence
- **High Confidence**: The hybrid initialization-refinement pipeline demonstrates measurable improvements over single-paradigm approaches, with black-box initialization providing better starting points than white-box alone.
- **Medium Confidence**: The hidden-output representation fusion mechanism provides consistent performance gains across tasks, though specific architectural choices could be optimized further.
- **Low Confidence**: The generalizability of learned representations across task domains remains unclear, and the sensitivity to black-box model selection and potential API-induced variability are not fully characterized.

## Next Checks
1. **Architecture Ablation Study**: Implement and compare multiple neural network architectures (depth 3, 5, 7; width 256, 512, 1024) on a subset of 5 tasks to identify optimal scoring network configuration. Track training stability and generalization across iterations.

2. **Cross-Paradigm Initialization Robustness**: Test initialization quality consistency across different black-box model versions (ChatGPT-3.5, ChatGPT-4, ChatGPT-4o, DeepSeek V3) on 10 diverse tasks. Measure instruction diversity metrics and initial performance variance to assess API dependency.

3. **Temporal Stability Evaluation**: Run the complete framework across three separate time periods with 1-week intervals on the full 30-task benchmark. Compare mean scores, convergence patterns, and best instruction stability to quantify reproducibility under model/API variations.