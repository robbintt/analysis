---
ver: rpa2
title: Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing
arxiv_id: '2512.23684'
source_url: https://arxiv.org/abs/2512.23684
tags:
- injection
- prompt
- llms
- review
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the susceptibility of LLM-based academic reviewing
  to multilingual hidden prompt injection attacks. The authors construct a dataset
  of 484 real ICML-accepted papers and inject semantically equivalent adversarial
  prompts in English, Japanese, Chinese, and Arabic.
---

# Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing

## Quick Facts
- **arXiv ID**: 2512.23684
- **Source URL**: https://arxiv.org/abs/2512.23684
- **Reference count**: 16
- **Primary result**: LLM-based academic reviewing systems are highly vulnerable to multilingual hidden prompt injection attacks, with over 98% of papers experiencing harsher reviews and more than 50% shifting from accept to non-accept decisions for English, Japanese, and Chinese injections.

## Executive Summary
This paper evaluates the susceptibility of LLM-based academic reviewing to multilingual hidden prompt injection attacks. The authors construct a dataset of 484 real ICML-accepted papers and inject semantically equivalent adversarial prompts in English, Japanese, Chinese, and Arabic. Each paper is reviewed using a local LLM under baseline and injected conditions. Results show substantial negative score drift and high injection success rates for English, Japanese, and Chinese, with over 98% of papers experiencing harsher reviews and more than 50% shifting from accept to non-accept decisions. In contrast, Arabic injection exhibits negligible effects, suggesting uneven multilingual robustness. These findings highlight significant vulnerabilities in document-based LLM workflows and underscore the need for improved defences against multilingual prompt injection attacks.

## Method Summary
The study constructs a dataset of 484 real ICML-accepted papers and injects semantically equivalent adversarial prompts in four languages (English, Japanese, Chinese, Arabic). Each paper undergoes review using a local LLM under baseline conditions and with injected prompts. The evaluation measures changes in review scores and decision outcomes, comparing acceptance rates and score distributions between baseline and injected conditions to quantify attack effectiveness across different languages.

## Key Results
- Over 98% of papers experienced harsher reviews when injected with prompts in English, Japanese, and Chinese
- More than 50% of papers shifted from accept to non-accept decisions under these injections
- Arabic injection showed negligible effects compared to other languages, indicating uneven multilingual robustness

## Why This Works (Mechanism)
The paper does not provide a detailed mechanism explanation for why multilingual prompt injection attacks succeed or fail.

## Foundational Learning
- **Multilingual adversarial prompt injection**: Understanding how semantic equivalence across languages affects attack success
  - Why needed: To assess vulnerability patterns across different linguistic contexts
  - Quick check: Test injection effectiveness across language families
- **Document-based LLM vulnerability**: Recognition that static document review is susceptible to hidden prompt attacks
  - Why needed: To understand attack surface in real-world academic workflows
  - Quick check: Compare vulnerability between document review and interactive review scenarios
- **Model training bias**: Awareness that training data composition affects multilingual robustness
  - Why needed: To explain why certain languages (Arabic) show resistance
  - Quick check: Analyze training data distribution across target languages

## Architecture Onboarding
- **Component map**: Paper dataset -> LLM reviewer -> Score assessment -> Decision classification
- **Critical path**: Document injection -> LLM processing -> Score drift measurement -> Decision shift analysis
- **Design tradeoffs**: Single model evaluation vs. cross-architecture validation; static document review vs. interactive scenarios
- **Failure signatures**: Score drift magnitude, decision reversal rates, language-specific vulnerability patterns
- **First experiments**: 1) Test injection prompts across multiple LLM architectures, 2) Expand language coverage to include Romance and Slavic languages, 3) Implement and evaluate defensive mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation uses only four languages despite broader multilingual claims
- Single local LLM model limits external validity across different architectures
- Focus on static document review rather than dynamic or interactive review scenarios
- Arabic injection ineffectiveness may reflect specific model training biases

## Confidence
- **High**: Demonstrated vulnerability of LLM-based reviewing systems to multilingual prompt injection
- **Medium**: Claim of uneven multilingual robustness due to language-specific effects
- **Low**: Generalizability of attack effectiveness across different LLM architectures

## Next Checks
1. Test the same injection prompts across multiple LLM architectures (GPT, Claude, LLaMA variants) to assess cross-model vulnerability patterns
2. Expand language coverage to include Romance and Slavic languages to better characterize the relationship between language family and injection susceptibility
3. Implement and evaluate defensive mechanisms (input sanitization, adversarial training, or prompt filtering) to measure practical mitigation effectiveness against these attacks