---
ver: rpa2
title: 'Bubble2Heat: Optical to Thermal Inference in Pool Boiling Using Physics-encoded
  Generative AI'
arxiv_id: '2505.00823'
source_url: https://arxiv.org/abs/2505.00823
tags:
- temperature
- data
- simulation
- heat
- boiling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents Bubble2Heat, a deep learning framework for
  inferring 2D temperature fields in pool boiling systems using only high-speed visual
  data and pointwise thermocouple measurements. The approach employs a physics-encoded
  conditional generative adversarial network (PECGAN) trained on simulation data generated
  via a hybrid lattice Boltzmann-finite difference method, producing paired temperature
  and phase contour maps.
---

# Bubble2Heat: Optical to Thermal Inference in Pool Boiling Using Physics-encoded Generative AI

## Quick Facts
- **arXiv ID**: 2505.00823
- **Source URL**: https://arxiv.org/abs/2505.00823
- **Reference count**: 40
- **Primary result**: Mean absolute error below 0.06 in normalized temperature units across all simulation datasets

## Executive Summary
Bubble2Heat presents a deep learning framework for inferring 2D temperature fields in pool boiling systems using only high-speed visual data and pointwise thermocouple measurements. The approach employs a physics-encoded conditional generative adversarial network (PECGAN) trained on simulation data generated via a hybrid lattice Boltzmann-finite difference method, producing paired temperature and phase contour maps. Experimental data is processed through instance segmentation using a pre-trained Mask R-CNN to extract phase contours, enabling direct model application without advanced measurement techniques.

The framework achieves strong performance on simulation datasets with MAE below 0.06 normalized units, with the best model incorporating three past phase contours and a reference temperature map. Data augmentation through horizontal flipping improves physical plausibility and generalization. Qualitative trends in temperature-vapor fraction relationships are preserved from simulation to experimental datasets, despite thermodynamic and regime mismatches. Results demonstrate generative models' potential to bridge observable multiphase phenomena with underlying thermal transport, offering a practical tool for augmenting experimental measurements in complex two-phase systems.

## Method Summary
The Bubble2Heat framework uses a physics-encoded conditional GAN (PECGAN) to map phase contour stacks to temperature fields. Training data consists of 9 simulation datasets (256×256, 200 frames each) generated using a hybrid lattice Boltzmann-finite difference method with varying heater configurations. Phase contours are extracted from density fields using a threshold (ρ_th=3.79552), and temperature is normalized using Jacob number scaling. The U-Net generator takes stacked phase contours (current + p past frames) and optional reference temperature as input, while the discriminator classifies real vs generated temperature fields. The model is trained with adversarial and reconstruction losses, with horizontal flipping augmentation to enforce physical plausibility. The best-performing model uses p=3 temporal frames, includes reference temperature, and employs data augmentation.

## Key Results
- PECGAN achieves MAE below 0.06 in normalized temperature units across all simulation datasets
- Model with three past phase contours and reference temperature shows optimal performance
- Data augmentation improves generalization to experimental data despite slight training error increase
- Qualitative temperature-vapor fraction trends transfer from simulation to experimental datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A conditional GAN trained on simulation data can encode physics that transfers to experimental inference without requiring explicit physical constraints
- Mechanism: The generator learns a mapping from phase contour stacks → temperature fields through adversarial training on LB-FD simulation pairs. The discriminator implicitly learns joint structure of inputs/outputs, encouraging physically consistent outputs without hard PDE constraints
- Core assumption: The simulation's phase-temperature relationships approximate experimental physics sufficiently for transfer learning
- Evidence anchors:
  - [abstract] "mean absolute error below 0.06 in normalized temperature units across all simulation datasets"
  - [section 4.1] Table 3 shows G_{p=3,Tref}+ achieving lowest vapor temperature error (0.0395)
  - [corpus] Weak corpus support—related works address similar problems but with different architectures
- Break condition: When experimental void fraction (0.1+) significantly exceeds simulation range (0.015–0.04), trend recreation fails

### Mechanism 2
- Claim: Providing temporal context (multiple past phase contours) improves inference accuracy by enabling trajectory-aware prediction
- Mechanism: Stacking p previous phase contours with current frame gives the model implicit velocity/history information. When quantization causes bubble contours to disappear, the model can still infer from trajectory history
- Core assumption: Bubble dynamics exhibit temporal continuity that correlates with thermal evolution
- Evidence anchors:
  - [section 4.1] "generator G_{p=3} achieves lower error in all metrics but two compared to generators with lower p values"
  - [section 4.1, Fig. 3g] Shows missing inference regions in p=0 model that are recovered in p=3 model
- Break condition: Increased p raises gradient error due to local noise amplification

### Mechanism 3
- Claim: Data augmentation (horizontal flipping) compensates for unavailable physical constraints by enforcing perspective invariance
- Mechanism: Flipping during training prevents the model from overfitting to directional biases in simulation data, improving generalization to experimental images with different bubble trajectories
- Core assumption: Pool boiling physics has approximate horizontal symmetry, or at minimum the model should not rely on fixed viewing orientation
- Evidence anchors:
  - [section 3.3] "We aim to encourage the model to produce consistent inference regardless of viewing orientation"
  - [section 4.1, Figs. 3a–b] Shows decreased inference error on test datasets with augmentation
- Break condition: Augmentation increases gradient error since the model cannot exploit default trajectory direction

## Foundational Learning

- **Conditional GANs (cGAN / Pix2Pix)**: Why needed here: The PECGAN uses a U-Net generator and patch discriminator trained with conditional adversarial loss; understanding this architecture is essential for reproducing or modifying the model. Quick check: Can you explain why the discriminator receives both the phase contour input and temperature output, rather than just the output?

- **Lattice Boltzmann Method for Multiphase Flow**: Why needed here: Training data comes from LB-FD simulations; understanding density fields, pseudo-potential formulation, and the coupling to thermal transport explains the ground-truth characteristics and limitations. Quick check: How does the density threshold (ρ_th) relate to phase contour extraction, and why might this introduce quantization errors?

- **Instance Segmentation with Mask R-CNN**: Why needed here: Experimental preprocessing relies on a pre-trained Mask R-CNN to extract bubble masks from high-speed images; pipeline failure here propagates to all downstream inference. Quick check: What is the effect of segmentation errors on the phase contour maps φ, and how does the model handle missing or spurious bubble contours?

## Architecture Onboarding

- **Component map**: Input stack Φ(x,y,t) → U-Net generator G → normalized temperature T* → patch discriminator D → binary real/fake classification

- **Critical path**: 1. Generate simulation pairs (LB-FD) → (density, temperature) fields 2. Quantize density → phase contours φ; normalize T → T* 3. Train PECGAN on simulation; validate on held-out sets 4. Process experimental images: Mask R-CNN → bubble masks → φ; thermocouple → T_ref 5. Apply trained G to experimental φ-stack for temperature inference

- **Design tradeoffs**: p (temporal frames): Higher p reduces temperature error but increases gradient noise; Data augmentation: Improves test generalization but degrades gradient accuracy; T_ref inclusion: Stabilizes gradient error with minimal impact on temperature error; Zoom factor z: Larger z causes diffusive temperature predictions

- **Failure signatures**: Void fraction mismatch: Sim range 0.015–0.04 vs experimental 0.1+ → trends diverge at high void fraction; Quantization instability: Bubble contours can disappear mid-trajectory in single-frame (p=0) models; Regime mismatch: Simulation shows isolated nucleation; experiments show continuous nucleation/departure → higher inference error during rapid bubble behavior changes

- **First 3 experiments**: 1. Reproduce baseline: Train G_{p=3,T_ref} on provided simulation splits, evaluate MAE on held-out sets. Target: MAE < 0.06 normalized units. 2. Ablate temporal depth: Compare p ∈ {0,1,2,3} on vapor temperature error and gradient error. Expect tradeoff between temperature accuracy and gradient stability. 3. Stress-test domain shift: Apply trained model to experimental data with varying zoom factors z ∈ {1, 2, 3, 5}. Document where trends break down; identify max tolerable void fraction gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to accurately infer temperature fields in high heat flux regimes where bubble coalescence and departure dynamics differ significantly from the isolated nucleate boiling behavior used in training?
- Basis in paper: [explicit] The authors state the model produces "invalid inferences for pool boiling at high heat fluxes" because the training datasets only exhibit isolated bubble nucleation
- Why unresolved: The geometric features of bubbles at high heat fluxes lie outside the distribution of the simulation training data, causing the model to fail on the "continued bubble nucleation" seen in experiments
- What evidence would resolve it: Successful inference on experimental datasets with heat fluxes significantly higher than 22.92 W/cm² without retraining on those specific regimes

### Open Question 2
- Question: How can the quantitative mismatch in void fraction ranges between simulation (0.015–0.04) and experimental datasets (>0.1) be bridged to prevent the "drastic difference" that renders learned physical trends unobservable?
- Basis in paper: [explicit] The paper notes that the significant difference in void fraction ranges is a primary cause of error, making learned trends unobservable in experimental tests
- Why unresolved: The simulation data covers a limited void fraction range, whereas experimental data starts at a much higher baseline, causing the model to extrapolate poorly
- What evidence would resolve it: A model trained on a broader simulation regime that quantitatively matches experimental void fraction trends without manual resizing interventions

### Open Question 3
- Question: Can rigorous physical constraints (e.g., conservation of energy) be integrated into the loss function to replace the current reliance on data augmentation (flipping) as a proxy for enforcing physical plausibility?
- Basis in paper: [inferred] The authors use horizontal flipping "as a substitute to hard physical constraints that are not available," implying the integration of actual physics constraints remains a methodological gap
- Why unresolved: The GAN architecture prioritizes data-driven mapping, and the authors note that "precise physical constraints are not applicable" for the current use case
- What evidence would resolve it: A modified loss function incorporating thermodynamic residuals that reduces the need for augmentation while maintaining or improving the MAE below 0.06

## Limitations
- Void fraction mismatch between simulation (0.015-0.04) and experimental (>0.1) regimes causes trend degradation at higher void fractions
- Reliance on simulation training data introduces uncertainty about model behavior in regimes not represented during training
- Density quantization to binary phase contours may introduce information loss, particularly during bubble dynamics

## Confidence

- **High**: Model performance metrics on simulation data (MAE < 0.06), basic architecture implementation, data preprocessing pipeline
- **Medium**: Transfer learning effectiveness from simulation to experiment, impact of data augmentation on generalization, temporal context benefits
- **Low**: Long-term extrapolation beyond training void fraction ranges, robustness to different boiling regimes, generalization to different fluids or surface conditions

## Next Checks

1. **Domain gap quantification**: Systematically vary void fraction in simulation test sets to identify the exact threshold where inference error degrades, then compare against experimental void fraction distribution

2. **Ablation study on temporal depth**: Compare p=0, p=1, p=2, p=3 models on both temperature and gradient accuracy to precisely quantify the tradeoff between temporal information and gradient noise

3. **Cross-regime validation**: Apply the trained model to datasets with different heater arrangements or flow conditions (if available) to test robustness beyond the isolated nucleation regime used in training