---
ver: rpa2
title: 'On Group Relative Policy Optimization Collapse in Agent Search: The Lazy Likelihood-Displacement'
arxiv_id: '2512.04220'
source_url: https://arxiv.org/abs/2512.04220
tags:
- likelihood
- search
- training
- grpo
- llds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies Lazy Likelihood Displacement (LLD) as the
  primary cause of training collapse in tool-integrated reinforcement learning, where
  the likelihood of correct responses systematically decreases even as rewards increase.
  This phenomenon arises due to structural factors in GRPO-based systems: incorrect
  responses often share high embedding similarity with correct ones, and low-likelihood
  incorrect responses generate disproportionately large negative gradients that suppress
  correct action likelihoods.'
---

# On Group Relative Policy Optimization Collapse in Agent Search: The Lazy Likelihood-Displacement

## Quick Facts
- arXiv ID: 2512.04220
- Source URL: https://arxiv.org/abs/2512.04220
- Authors: Wenlong Deng; Yushu Li; Boying Gong; Yi Ren; Christos Thrampoulidis; Xiaoxiao Li
- Reference count: 29
- Primary result: LLDS prevents training collapse by selectively penalizing likelihood decreases, improving GRPO performance by 37-45% on Qwen2.5 models

## Executive Summary
This paper identifies Lazy Likelihood Displacement (LLD) as a fundamental cause of training collapse in tool-integrated reinforcement learning systems using Group Relative Policy Optimization (GRPO). The authors demonstrate that incorrect responses often share high embedding similarity with correct ones, leading to large negative gradients that suppress correct action likelihoods even as rewards increase. They propose a likelihood-preserving regularization method (LLDS) that selectively penalizes only likelihood reductions on response actions with non-negative advantages, activating only when likelihood decreases. This approach stabilizes training, prevents gradient explosion, and yields substantial performance improvements across model scales.

## Method Summary
The authors introduce a likelihood-preserving regularization method (LLDS) that addresses training instability in GRPO-based systems by selectively penalizing likelihood decreases on response actions with non-negative advantages. The method activates only when likelihood decreases, creating a targeted intervention that preserves successful probability distributions while preventing the suppression of correct responses. LLDS operates through a simple regularization term added to the GRPO objective, requiring no additional supervision or reward engineering beyond standard sparse rewards.

## Key Results
- +45.2% performance improvement on Qwen2.5-3B over vanilla GRPO
- +37.1% performance improvement on Qwen2.5-7B over vanilla GRPO
- Outperforms dense-reward baselines that require additional supervision

## Why This Works (Mechanism)
The paper identifies that in tool-integrated reasoning tasks, incorrect responses often share high embedding similarity with correct ones, creating large negative gradients that suppress correct action likelihoods. This "lazy likelihood displacement" occurs because the model becomes increasingly certain about incorrect responses while simultaneously becoming less likely to produce correct ones. The LLDS method addresses this by selectively penalizing only those likelihood decreases that occur on actions with non-negative advantages, effectively preventing the model from becoming too confident in incorrect responses while preserving the learning signal from correct ones.

## Foundational Learning

**Tool-Integrated Reinforcement Learning**: Combining external tools with RL agents for complex reasoning tasks; needed to understand the specific failure modes in practical agent systems; quick check: verify the agent can call and use external APIs correctly.

**Group Relative Policy Optimization (GRPO)**: A policy gradient method that compares rewards within groups rather than against a fixed baseline; needed to understand the optimization dynamics being stabilized; quick check: confirm GRPO implementation matches standard formulations.

**Embedding Similarity Analysis**: Using cosine distance to measure semantic similarity between response embeddings; needed to identify structural factors causing gradient conflicts; quick check: verify embedding similarity correlates with response correctness.

## Architecture Onboarding

**Component Map**: Model → Tool API Calls → Response Generation → Reward Evaluation → GRPO Update → (LLDS Regularization)

**Critical Path**: Response generation → Reward evaluation → Policy gradient update → Model parameter update

**Design Tradeoffs**: Sparse rewards vs. dense rewards (simplicity vs. supervision), selective regularization vs. global constraints (precision vs. coverage)

**Failure Signatures**: Likelihood decrease despite reward increase, gradient explosion on similar incorrect responses, training instability after initial improvements

**First Experiments**: 1) Test LLDS activation conditions with varying advantage thresholds, 2) Compare embedding similarity distributions between correct and incorrect responses, 3) Evaluate training stability across different sequence lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to Qwen2.5 model family and modest parameter scales
- Analysis restricted to tool-integrated reasoning tasks, unclear generalizability to other RLHF applications
- Embedding similarity analysis relies on cosine distance without exploring alternative metrics

## Confidence
**High Confidence**: The identification of LLD as a distinct failure mode is well-supported by empirical evidence showing systematic likelihood decrease despite reward increase.

**Medium Confidence**: The claim that LLD is the "primary cause" of training collapse requires more nuanced interpretation, as other factors like reward hacking may coexist.

**Medium Confidence**: Comparative advantage over dense-reward baselines is demonstrated but may be context-dependent and require further validation across task types.

## Next Checks
1. Test LLDS on non-Qwen models (Llama, Mistral) across different scales (1B, 13B, 70B) to verify generalizability
2. Systematically vary LLDS activation conditions to identify most critical regularization aspects
3. Extend validation to tasks requiring extended reasoning chains (>50 steps) to test scalability