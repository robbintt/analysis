---
ver: rpa2
title: Uncovering Utility Functions from Observed Outcomes
arxiv_id: '2503.13432'
source_url: https://arxiv.org/abs/2503.13432
tags:
- utility
- function
- demand
- pearl
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEARL, an algorithm that recovers a representation
  of the underlying utility function of a utility-maximising consumer under a budget
  constraint, which generated observed consumption data as a result of the utility-maximising
  process. It uses economic theory from revealed preferences to obtain conditions
  for ensuring that the data set is consistent with a rational utility-maximising
  consumer.
---

# Uncovering Utility Functions from Observed Outcomes

## Quick Facts
- **arXiv ID**: 2503.13432
- **Source URL**: https://arxiv.org/abs/2503.13432
- **Reference count**: 40
- **Primary result**: PEARL recovers utility function parameters from observed consumer choices by enforcing GARP consistency and minimizing expenditure prediction error using an Input-Concave Neural Network.

## Executive Summary
This paper introduces PEARL, an algorithm that recovers a representation of the underlying utility function from observed consumer consumption data under budget constraints. The method uses economic theory from revealed preferences to ensure data consistency with rational utility-maximizing behavior. Under GARP consistency conditions, a rationalizing utility function is guaranteed to exist, and PEARL obtains its representation through an iterative process. The authors also introduce the Input-Concave Neural Network (ICNN) architecture that maintains concavity with respect to inputs through specific activation functions and weight constraints.

## Method Summary
PEARL works in two iterative stages: (1) finding the expenditure-minimizing bundle that achieves the observed utility level, and (2) updating utility function parameters to minimize the difference between observed and estimated expenditures. The algorithm first checks GARP consistency using transitive closure and adjusts data via Afriat's efficiency index if needed. The ICNN architecture enforces concavity through non-negative weights on lateral layers and concave activation functions (concave-tanh, concave-sigmoid, concave-log). Training uses projected gradient descent for the inner loop and Adam optimization for outer loop parameter updates.

## Key Results
- PEARL recovers true utility function parameters when the functional form is known and data is consistent with rational preferences
- ICNN with concave activation functions achieves small errors even on small sample sizes
- The algorithm successfully handles both synthetic data (with and without noise) and real-world datasets
- Performance is demonstrated on small sample sizes (N=160-1600) and small numbers of goods (k=2-10)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: If observed consumer data satisfies GARP, a rationalizing utility function is guaranteed to exist
- **Mechanism**: Constructs preference graph, computes transitive closure, checks for cycles. Adjusts observations via Afriat's efficiency index until acyclic
- **Core assumption**: Consumer acts with consistent preferences (completeness, transitivity) and is a utility maximizer with efficiency error ε
- **Evidence anchors**: Abstract mentions revealed preferences conditions; section 4.1 details Afriat's index adjustment
- **Break condition**: If required efficiency index ε approaches 0, data becomes effectively irrational

### Mechanism 2
- **Claim**: Minimizing gap between observed expenditure and theoretical minimum expenditure recovers utility function parameters
- **Mechanism**: Iteratively solves dual problem (expenditure minimization). Finds cheapest bundle achieving utility uᵢ, minimizes L₁ loss between cost of this bundle and observed expenditure
- **Core assumption**: Utility function is strictly concave, ensuring unique expenditure-minimizing bundle
- **Evidence anchors**: Abstract describes recovery of utility function from maximizing process; section 4.2 details iterative estimation
- **Break condition**: Non-convex loss landscape or vanishing gradients can stall parameter updates

### Mechanism 3
- **Claim**: Neural network can represent valid utility function if architecture enforces concavity and monotonicity
- **Mechanism**: ICNN restricts weights on lateral layers to be non-negative and uses specific concave activation functions
- **Core assumption**: True utility function is smooth, monotonically increasing, and concave
- **Evidence anchors**: Abstract introduces ICNN as concave with respect to input; section 4.4.2 details mathematical conditions
- **Break condition**: Standard activation functions or negative weights violate concavity property

## Foundational Learning

- **Concept: Revealed Preference Theory (GARP)**
  - **Why needed here**: Theoretical filter that determines which data points are used for training
  - **Quick check question**: If a consumer chooses bundle A over B, and B over C, but then chooses C over A, what does GARP say about this data?

- **Concept: Duality in Consumer Theory (Hicksian vs. Marshallian Demand)**
  - **Why needed here**: PEARL trains on dual problem (minimizing cost for given utility) rather than primal
  - **Quick check question**: Does Hicksian demand treat utility as constraint and income as outcome, or vice versa?

- **Concept: Input-Convex/Concave Architectures**
  - **Why needed here**: Standard neural networks can learn non-convex shapes; architectural constraints enforce economic properties
  - **Quick check question**: Why is concavity of utility function essential for predicting consumer demand?

## Architecture Onboarding

- **Component map**: Pre-processor (GARP check) -> Inner Loop (Expenditure Minimization) -> Outer Loop (Parameter Update) -> ICNN Core
- **Critical path**: Inner loop execution is bottleneck, running to convergence for every observation every parameter update
- **Design tradeoffs**:
  - Cobb-Douglas vs. ICNN: Fast parameter recovery vs. flexibility with higher computational cost
  - Strict GARP vs. ε-adjustment: Theoretical purity vs. retaining real-world noisy data
- **Failure signatures**:
  - Loss plateau: Inner loop fails to converge due to flat or non-concave utility estimate
  - Negative weights: Violates input-concavity, invalidating economic guarantees
  - Oscillation: Learning rate too high relative to inner loop convergence stability
- **First 3 experiments**:
  1. Sanity Check: Generate synthetic Cobb-Douglas data, train PEARL, verify recovered parameters match ground truth
  2. Architecture Ablation: Replace concave-log with ReLU, compare demand prediction error
  3. Noise Sensitivity: Inject noise into synthetic data, tune ε parameter to observe GARP breakdown

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does PEARL performance scale with high-dimensional data involving large numbers of goods and observations?
- **Basis in paper**: Experiments limited to small sample sizes (N=160, 1600) and small numbers of goods (k=2, 5, 10)
- **Why unresolved**: Performance on larger dimensions remains untested
- **What evidence would resolve it**: Benchmarking on datasets with k > 50 and larger N demonstrating convergence and error rates

### Open Question 2
- **Question**: Can the algorithm be adapted to explicitly model and track changing preferences or utility parameters over time?
- **Basis in paper**: Explicitly acknowledges PEARL does not account for changing preferences over time
- **Why unresolved**: Current methodology assumes static preferences and cannot isolate preference drift from noise
- **What evidence would resolve it**: Modified framework successfully differentiating demand shocks from structural utility changes in longitudinal data

### Open Question 3
- **Question**: How can the framework be extended to account for heterogeneity across individuals rather than assuming a representative agent?
- **Basis in paper**: Assumes observations come from same deterministic utility maximization objective
- **Why unresolved**: Current model assumes single utility function or homogeneous agents
- **What evidence would resolve it**: Demonstration of algorithm fitting distinct utility functions for different consumer segments

## Limitations
- Does not account for changing preferences or parameters over time
- Limited experimental validation to small sample sizes and small numbers of goods
- Computational cost of inner loop optimization for each observation and parameter update

## Confidence

- **High confidence**: Theoretical foundation linking GARP consistency to existence of rationalizing utility functions; mathematical framework for ICNN ensuring concavity
- **Medium confidence**: Recovery accuracy on synthetic data with known ground truth, particularly for Cobb-Douglas baseline
- **Low confidence**: Generalization to real-world noisy data and scalability to high-dimensional settings

## Next Checks
1. **Scalability Test**: Evaluate PEARL on synthetic datasets with k=20-50 goods and N=10,000+ observations to measure computational scaling and prediction accuracy degradation
2. **Architecture Ablation**: Replace ICNN with standard feed-forward network (no concavity constraints) and compare performance on same tasks to quantify value of economic constraints
3. **Temporal Stability**: Simulate preference drift over time by gradually changing utility function parameters and measure PEARL's ability to track or adapt to these changes