---
ver: rpa2
title: 'Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing'
arxiv_id: '2602.02386'
source_url: https://arxiv.org/abs/2602.02386
tags:
- skill
- selection
- skills
- performance
- capability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BELLA addresses the challenge of selecting cost-effective large
  language models (LLMs) for tasks by introducing a skill-based profiling and routing
  framework. The method extracts interpretable skill profiles from LLM outputs via
  a critic model, clusters these into structured capability matrices, and performs
  multi-objective optimization to recommend models that maximize performance within
  budget constraints.
---

# Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing

## Quick Facts
- arXiv ID: 2602.02386
- Source URL: https://arxiv.org/abs/2602.02386
- Authors: Mika Okamoto; Ansel Kaplan Erol; Glenn Matlin
- Reference count: 11
- Primary result: Introduces BELLA, a skill-based profiling and routing framework that enables cost-effective LLM selection with interpretable rationale for financial reasoning tasks.

## Executive Summary
BELLA addresses the challenge of selecting cost-effective large language models (LLMs) for tasks by introducing a skill-based profiling and routing framework. The method extracts interpretable skill profiles from LLM outputs via a critic model, clusters these into structured capability matrices, and performs multi-objective optimization to recommend models that maximize performance within budget constraints. BELLA provides natural-language rationale for recommendations, distinguishing it from black-box routing systems. The framework was evaluated on financial reasoning tasks, demonstrating its ability to make principled cost-performance trade-offs while offering transparency in model selection decisions.

## Method Summary
BELLA is a four-stage framework for cost-aware LLM routing. First, multiple LLMs are benchmarked on a task dataset to collect outputs, accuracy, and costs. Second, a critic LLM analyzes each instance's ground truth and model output to extract demonstrated and missing skills in natural language. Third, skill phrases are embedded and clustered to build a canonical skill taxonomy, from which capability matrices are constructed. Fourth, these matrices are used in a constrained optimization to select models that maximize estimated performance under a budget, providing interpretable rationales for each recommendation.

## Key Results
- BELLA enables principled cost-performance trade-offs for LLM selection with transparent skill-based rationale
- Framework demonstrated effectiveness on financial reasoning benchmarks using structured skill profiles
- Distinguishes itself from black-box routing by providing interpretable model capability explanations

## Why This Works (Mechanism)
The approach works by creating a bridge between raw model outputs and interpretable capability representations. By having a critic LLM analyze what skills were demonstrated versus missing in each response, BELLA transforms opaque model behaviors into structured, comparable profiles. These profiles enable both human understanding of why a model succeeds or fails on specific instances, and algorithmic routing decisions that can balance cost against capability requirements.

## Foundational Learning
- **Critic LLM skill extraction** - A secondary LLM analyzes outputs to identify demonstrated and missing skills - needed to transform raw outputs into structured capability data; quick check: verify extracted skills align with human judgment of output quality
- **Skill phrase embedding and clustering** - OpenAI embeddings + K-Means creates canonical skill taxonomy - needed to normalize diverse skill descriptions into comparable dimensions; quick check: assess cluster coherence and label interpretability
- **Capability matrix construction** - Models are represented as vectors over skills (0-1 scale) - needed to enable quantitative comparison across models; quick check: ensure matrix isn't too sparse to distinguish models
- **Budget-constrained optimization** - Selection maximizes performance within cost limits - needed to operationalize cost-performance trade-offs; quick check: validate selected models meet both budget and capability thresholds

## Architecture Onboarding

**Component map:** LLM benchmarking -> Critic skill extraction -> Skill embedding/clustering -> Capability matrix construction -> Budget-constrained selection

**Critical path:** The bottleneck is the critic LLM's ability to consistently extract meaningful, comparable skills across all model outputs. Poor skill extraction propagates through embedding, clustering, and ultimately degrades routing quality.

**Design tradeoffs:** The framework trades off the overhead of skill profiling (including critic analysis and clustering) against the benefits of more informed routing. This is justified when the cost savings from better model selection exceed the profiling overhead, particularly in high-stakes domains with significant cost variation between models.

**Failure signatures:** If the capability matrix becomes too sparse (most entries near 0), the system cannot differentiate between models. If skill extraction is inconsistent, the routing will be unstable across runs. If the performance estimator p̂(m,t) is poorly calibrated, recommendations will systematically underperform.

**3 first experiments:**
1. Implement the complete four-stage pipeline on a small financial QA benchmark (5 models, 50 instances) and verify that capability matrices can distinguish between models
2. Perform ablation: compare skill-based routing vs. cost-only threshold filtering on the same benchmark to quantify skill-value contribution
3. Test critic consistency by running the skill extraction step twice on the same dataset and measuring vocabulary overlap

## Open Questions the Paper Calls Out
None

## Limitations
- Core approach heavily dependent on quality of critic LLM's skill extraction, which lacks detailed specification
- Manual skill taxonomy construction may not scale well to domains with richer or more nuanced skill sets
- Performance estimator p̂(m,t) is left unspecified, which is critical for routing accuracy
- Evaluation limited to financial reasoning tasks, constraining generalizability claims

## Confidence
**High confidence:** The framework's four-stage architecture is clearly specified and methodologically sound. The use of structured skill profiles for transparent routing is a novel and valuable contribution. The cost-performance trade-off formulation is well-defined.

**Medium confidence:** The effectiveness of skill-based routing over simpler cost-based approaches, given the lack of detailed p̂(m,t) specification. The scalability of manual skill taxonomy construction to other domains.

**Low confidence:** Claims about generalization to arbitrary domains without further empirical validation beyond financial reasoning.

## Next Checks
1. Implement and test multiple candidate performance estimators (e.g., logistic regression, gradient boosting, or small neural networks trained on historical benchmark data) to determine which yields the best selection accuracy in cross-validation.
2. Conduct a domain transfer experiment: apply the complete BELLA pipeline to a non-financial domain (e.g., medical QA or code generation) and evaluate whether the skill profiles and routing decisions remain meaningful and effective.
3. Perform an ablation study comparing BELLA's skill-based routing against a simple cost-based threshold filter on the same benchmark to quantify the added value of skill profiling for selection quality.