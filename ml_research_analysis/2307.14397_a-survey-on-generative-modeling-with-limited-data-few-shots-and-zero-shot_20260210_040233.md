---
ver: rpa2
title: A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot
arxiv_id: '2307.14397'
source_url: https://arxiv.org/abs/2307.14397
tags:
- learning
- image
- data
- generation
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews Generative Modeling under Data
  Constraint (GM-DC), focusing on limited-data, few-shot, and zero-shot settings.
  It introduces novel taxonomies for GM-DC tasks (e.g., unconditional/conditional
  generation, cross-domain adaptation, subject-driven modeling) and methodological
  approaches (e.g., transfer learning, data augmentation, meta-learning, frequency-aware
  modeling).
---

# A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot

## Quick Facts
- arXiv ID: 2307.14397
- Source URL: https://arxiv.org/abs/2307.14397
- Authors: Milad Abdollahzadeh; Guimeng Liu; Touba Malekzadeh; Christopher T. H. Teo; Keshigeyan Chandrasegaran; Ngai-Man Cheung
- Reference count: 40
- This survey comprehensively reviews Generative Modeling under Data Constraint (GM-DC), focusing on limited-data, few-shot, and zero-shot settings. It introduces novel taxonomies for GM-DC tasks and methodological approaches, analyzing over 230 papers to highlight key challenges and promising future directions.

## Executive Summary
This survey systematically organizes the rapidly growing field of generative modeling under data constraints. It presents a novel taxonomy categorizing GM-DC tasks (unconditional/conditional generation, cross-domain adaptation, subject-driven modeling) and methodological approaches (transfer learning, data augmentation, meta-learning, frequency-aware modeling). The authors analyze over 230 papers to identify core challenges including overfitting, frequency bias, and incompatible knowledge transfer, while providing practical insights for researchers entering this field.

## Method Summary
The survey employs a systematic methodology to analyze GM-DC literature, creating taxonomies for both tasks and approaches. The core analysis revolves around the Sankey diagram visualization that maps relationships between tasks, approaches, and specific methods. For reproduction purposes, the survey specifies using StyleGAN2 pre-trained on FFHQ as the source model, with AFHQ-Cat or Flowers as target datasets. The AdAM adaptation method is used as a representative technique, with FID (Fréchet Inception Distance) as the primary evaluation metric, computed using the clean-FID library. The minimum viable reproduction plan involves adapting StyleGAN2 to three different 10-shot subsets of AFHQ-Cat and measuring FID variance to demonstrate sample selection sensitivity.

## Key Results
- Transfer learning is the most popular approach but faces challenges with incompatible knowledge transfer when domain proximity is low
- Data augmentation methods like ADA effectively prevent discriminator overfitting but can cause augmentation leakage artifacts
- Single-image generation methods exploit internal patch statistics but struggle with coherent global structure for complex scenes
- Sample selection significantly impacts performance, with FID scores varying dramatically (49.9 vs 90.0) across different 10-shot subsets

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Preservation via Regularized Transfer
Transfer learning can mitigate overfitting in data-constrained generative models when regularization strategies prevent source knowledge forgetting. Methods like Cross-Domain Correspondence (CDC) or Elastic Weight Consolidation (EWC) add loss terms that constrain adaptation to preserve structural priors learned from large source datasets while adapting style. This mechanism assumes source and target domains share latent structural similarities; otherwise, preserved knowledge becomes incompatible. The mechanism fails when domain gaps are large (e.g., Faces to Flowers), causing negative transfer where source features appear as artifacts in target domain outputs.

### Mechanism 2: Adaptive Augmentation for Discriminator Balancing
Adaptive data augmentation prevents discriminator overfitting by dynamically adjusting augmentation probability based on discriminator accuracy. ADA increases augmentation when the discriminator becomes too accurate, forcing it to learn robust features rather than memorizing pixel noise. This mechanism assumes augmentations are differentiable, invertible, and semantically preserving. It fails through "augmentation leakage" when transformations are poorly selected, causing generators to produce images with visible noise, blur, or color distortion artifacts inherited from the augmentation pipeline.

### Mechanism 3: Internal Patch Statistics for Single-Image Generation
Single-image generative models exploit the internal distribution of patches within that image. Methods like SinGAN train pyramid generators that learn to generate variations maintaining global structure while varying local textures, bypassing the need for large external datasets. This mechanism assumes the single input image possesses sufficient self-similarity to define a distribution. It fails to capture high-level semantics or coherent global structure for complex scenes, resulting in incoherent arrangements despite maintaining local texture fidelity.

## Foundational Learning

- **Overfitting vs. Mode Collapse**: The survey distinguishes these as distinct failure modes in GM-DC. Overfitting implies the generator memorizes training samples (reproducing them exactly), while mode collapse implies the generator produces diverse outputs but covers only a subset of the true distribution. Quick check: If a model generates the exact same background for 100 different generated faces, is this overfitting or mode collapse? (Answer: Mode collapse).

- **Spectral Bias (Frequency Bias)**: GM-DC models exhibit frequency bias, prioritizing low-frequency components (broad shapes) while ignoring high-frequency details (texture), a problem exacerbated by limited data. This causes medical imaging GANs to produce blurry lesions when trained with limited data. Quick check: Why might a medical imaging GAN trained with limited data produce blurry lesions? (Answer: Spectral bias causes the model to fail at learning high-frequency details).

- **Domain Proximity**: Transfer learning effectiveness depends heavily on semantic distance between source and target domains. Understanding this helps in selecting appropriate pre-trained models. Quick check: Is adapting a "Human Face" model to "Baby Faces" easier than adapting it to "Churches"? (Answer: Yes, due to closer domain proximity).

## Architecture Onboarding

- **Component map**: Source Model -> Adaptation Module -> Target Generator
- **Critical path**:
  1. Define Constraint: Categorize as Limited Data (50-5k samples), Few-Shot (1-50), or Zero-Shot
  2. Select Approach: Use Sankey Diagram to map task to approach; Transfer Learning dominates Few-Shot
  3. Check Proximity: Assess domain similarity; if "Remote" (e.g., medical), consider Data Augmentation or specific architectures instead of Transfer Learning
- **Design tradeoffs**:
  - Regularization vs. Flexibility: Strong regularizers prevent overfitting but may fail to adapt if target domain is distinct
  - Efficiency vs. Fidelity: Parameter-efficient tuning (LoRA, Modulation) is faster but may underfit complex domain shifts compared to full fine-tuning
- **Failure signatures**:
  - Incompatible Transfer: Generating artifacts from source domain (e.g., "hats" on "flowers")
  - Augmentation Leakage: Generated images contain visible noise, blur, or color distortion artifacts
  - Semantic Drift: In subject-driven generation, the generated subject loses identity while adhering too strictly to text prompt
- **First 3 experiments**:
  1. Baseline Stability: Fine-tune pre-trained StyleGAN2 on 10-shot "Sketch" dataset without regularization to observe overfitting speed
  2. Augmentation Ablation: Implement ADA on baseline to measure FID improvement and check for leakage
  3. Proximity Test: Transfer from "FFHQ" to "AFHQ-Cat" (Distant) vs. "FFHQ-Baby" (Close) using AdAM to quantify performance drop from domain gap

## Open Questions the Paper Calls Out

### Open Question 1
How can we enable effective knowledge transfer for target domains that are semantically distant or remote from the source domain? [explicit] Section 7.3.3 identifies this as largely unexplored, noting that current methods fail when domain gap is large (e.g., adapting from Human Faces to Flowers). Why unresolved: Existing transfer learning approaches prioritize preserving source knowledge, which hinders adaptation to remote domains, often resulting in low synthesis quality and incompatible knowledge transfer (Figure 6). What evidence would resolve it: Development of adaptation methods that achieve high-fidelity generation on benchmarks with significant domain shifts without introducing source domain artifacts.

### Open Question 2
How can we develop holistic evaluation frameworks that remain robust under extreme data scarcity? [explicit] Section 7.3.4 highlights that standard metrics like FID and KID lose statistical significance in few-shot and zero-shot settings. Why unresolved: Current metrics rely heavily on features from large-scale pre-trained models which may not capture out-of-domain properties, and there is no unified framework for human evaluation. What evidence would resolve it: A standardized evaluation protocol or metric that provides statistically significant results on datasets with fewer than 50 samples and correlates strongly with human judgment.

### Open Question 3
What data-centric strategies can be developed to understand and mitigate the impact of sample selection on model performance? [explicit] Section 7.3.5 notes that sample selection is often overlooked, while Figure 7 shows different random 10-shot sets can cause drastically different FID scores (49.9 vs 90.0). Why unresolved: Research has focused primarily on training procedures rather than data curation, despite evidence that specific characteristics of limited training samples significantly influence generalization. What evidence would resolve it: Algorithms or heuristics that can quantify the "quality" of a specific sample set for GM-DC tasks, leading to more stable performance regardless of random sampling variations.

## Limitations
- Limited quantitative comparisons across the 230+ papers analyzed, relying more on qualitative aggregation
- Absence of standardized benchmarks for GM-DC tasks makes cross-method comparisons difficult
- Insufficient coverage of medical and scientific imaging applications and real-world deployment challenges

## Confidence
- **High**: Taxonomic framework robustness and identification of key challenges (overfitting, frequency bias, incompatible transfer)
- **Medium**: Claims about specific mechanism effectiveness due to limited quantitative comparisons across papers
- **Medium**: Claims about method performance hierarchies without standardized benchmarking

## Next Checks
1. **Benchmark Validation**: Implement standardized evaluation protocol comparing 3-5 leading approaches (ADA, AdAM, LoRA) on common few-shot dataset to verify claimed performance hierarchies.

2. **Domain Gap Quantification**: Systematically measure adaptation performance across controlled domain proximity levels (FFHQ→AFHQ-Dog vs FFHQ→AFHQ-Wildlife) to validate domain proximity claims.

3. **Frequency Bias Measurement**: Develop quantitative metrics to measure high-frequency information preservation in limited-data settings, moving beyond qualitative assessments of "blurriness."