---
ver: rpa2
title: 'ANNIE: Be Careful of Your Robots'
arxiv_id: '2509.03383'
source_url: https://arxiv.org/abs/2509.03383
tags:
- attack
- safety
- action
- robot
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of adversarial safety
  attacks on embodied AI (EAI) systems, addressing the critical gap in defining and
  measuring safety in physically grounded robotics. The authors formalize a principled
  taxonomy of safety violations (critical, dangerous, risky) based on ISO standards
  for human-robot interactions, and introduce ANNIEBench, a benchmark of nine safety-critical
  scenarios with 2,400 video-action sequences.
---

# ANNIE: Be Careful of Your Robots

## Quick Facts
- arXiv ID: 2509.03383
- Source URL: https://arxiv.org/abs/2509.03383
- Reference count: 40
- Introduces the first systematic study of adversarial safety attacks on embodied AI systems, achieving >50% attack success rates across safety-critical scenarios.

## Executive Summary
This paper presents the first systematic study of adversarial safety attacks on embodied AI (EAI) systems, addressing the critical gap in defining and measuring safety in physically grounded robotics. The authors formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on ISO standards for human-robot interactions, and introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences. They also propose ANNIE-Attack, a task-aware adversarial framework that uses an attack leader model to decompose long-horizon goals into frame-level perturbations. Evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories, demonstrating the effectiveness of the attack framework. Physical robot experiments validate the real-world impact of these attacks. The results highlight the urgent need for security-driven defenses in EAI systems as they become more integrated into human-centric environments.

## Method Summary
The authors propose ANNIE-Attack, a task-aware adversarial framework for embodied AI systems. The framework uses an attack leader model that takes natural language instructions, visual observations, and attack type as input to predict action deltas. These deltas guide PGD-based perturbation optimization on input frames to induce safety violations. The safety taxonomy is grounded in ISO/TS 15066 standards, categorizing violations into critical (distance breaches), dangerous (velocity violations), and risky (collisions). The ANNIEBench benchmark includes nine safety-critical scenarios with 2,400 video-action sequences for evaluation.

## Key Results
- Attack success rates exceeding 50% across all safety categories (critical, dangerous, risky) on representative EAI models
- ANNIE-Attack framework demonstrates effectiveness through systematic evaluation on nine safety-critical scenarios
- Physical robot experiments validate real-world impact with 40% attack success rate in 4 out of 10 trials
- Adaptive sparse attack strategies achieve comparable ASR to dense attacks with lower attack frequency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-aware adversarial perturbations can systematically induce safety violations in VLA-controlled robots by decomposing long-horizon goals into frame-level attack targets.
- Mechanism: The Attack Leader Module conditions on visual observations and attack type to predict an action delta, which is added to the VLA's original action output to form an adversarial target action. PGD perturbation optimization is then applied to manipulate the input frame, shifting the robot's trajectory toward unsafe states defined by ISO constraints.
- Core assumption: The VLA model's action space is differentiable with respect to visual input, enabling gradient-based perturbation transfer.
- Evidence anchors: [abstract], [section 5.2], and corpus papers confirming VLA vulnerability to targeted attacks.
- Break condition: If VLA action normalization uses distribution-aware methods, perturbation impact is dampened, reducing ASR.

### Mechanism 2
- Claim: Safety violations can be objectively quantified through ISO/TS 15066-derived physical constraints rather than task failure metrics.
- Mechanism: The taxonomy maps ISO/TS 15066 collaborative robot standards to three violation levels based on distance, velocity, and collision boundaries, evaluated automatically in simulation via sensor data.
- Core assumption: The simulation environment accurately captures real-world physics sufficient for safety constraint evaluation.
- Evidence anchors: [abstract], [section 3.1], and weak direct evidence from neighbor papers discussing EAI vulnerabilities.
- Break condition: If sensor tracking fails or human position is ambiguous, safety constraint evaluation becomes unreliable.

### Mechanism 3
- Claim: Adaptive sparse attacks achieve comparable ASR to dense attacks with lower attack frequency by dynamically prioritizing early-sequence frames.
- Mechanism: The attack scale output reflects action phase—large scale indicates early execution where perturbations have greater trajectory influence. The adaptive strategy applies high-frequency perturbations when attack scale exceeds a threshold, then aggressively skips frames as scale decreases.
- Core assumption: Early frames in a vision-action sequence disproportionately determine final robot state.
- Evidence anchors: [abstract], [section 5.3], and no corpus papers specifically addressing adaptive sparse attack timing for VLAs.
- Break condition: If the task requires precise late-stage corrections, sparse early-focused attacks may fail to induce safety violations.

## Foundational Learning

- Concept: Vision-Language-Action (VLA) Models
  - Why needed here: VLA models are the attack target—they map visual observations + language instructions to robot actions.
  - Quick check question: Given an observation O_t and instruction l, what does a VLA model output? (Answer: action a_t, typically end-effector delta + gripper state)

- Concept: Projected Gradient Descent (PGD) Adversarial Attacks
  - Why needed here: PGD is the underlying optimization method for generating perturbations once the attack leader provides a target action.
  - Quick check question: What constraint does PGD enforce on perturbations, and how? (Answer: L-infinity or L-2 bound via projection step after each gradient update)

- Concept: ISO/TS 15066 Safety Modes
  - Why needed here: The safety taxonomy is grounded in this standard. Understanding SRMS, SSM, PFL modes clarifies why distance/velocity/collision constraints define violation levels.
  - Quick check question: Which ISO/TS 15066 mode permits human-robot workspace sharing with dynamic speed adjustment based on proximity? (Answer: Speed and Separation Monitoring—SSM)

## Architecture Onboarding

- Component map:
  Attack Leader Module (ResNet encoders + attack type embedding) -> VLA Model (observation, instruction) -> PGD Loop (perturbation optimization) -> Evaluation Layer (ManiSkill simulator + safety constraint checks)

- Critical path:
  1. Collect observation O_t from simulation/robot
  2. VLA forward pass → original action a_original
  3. Attack Leader forward pass → delta ∆a → target action ã = a_original + ∆a
  4. PGD optimization → perturbed observation Ō_t
  5. Execute action in environment → check safety constraint violations
  6. Compute metrics: ASR, AC, AD, TSRC

- Design tradeoffs:
  - Dense vs. sparse attack: Dense maximizes ASR but requires continuous access; sparse reduces detectability with moderate ASR loss
  - Model choice: Baku shows higher ASR but larger trajectory deviation; ACT shows lower ASR but smoother attacks
  - Attack leader training: Per-scenario training improves ASR but limits generalization to unseen tasks

- Failure signatures:
  - Low ASR with high AD: Perturbations are detected/filtered; consider reducing perturbation magnitude
  - High ASR but task still succeeds: Safety violation may be marginal; tighten constraint thresholds
  - Sim-to-real gap: Real-world ASR significantly lower than simulation; check camera calibration, lighting, and physics fidelity

- First 3 experiments:
  1. Baseline validation: Run ANNIE-Dense on ACT and Baku across all 9 scenarios; verify ASR >50% reproduces.
  2. Ablation on attack leader: Compare random vs. fixed vs. attack leader guidance; verify 0.1 → 0.3 → 0.5 ASR progression.
  3. Sparse vs. adaptive comparison: Run ANNIE-2, ANNIE-3, and ANNIE-ADAP on a single scenario; confirm ANNIE-ADAP achieves ~1.0 ASR with lowest frequency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the attack leader model generalize to unseen tasks and environments without requiring task-specific retraining?
- Basis in paper: [Explicit] Section 8 states that the attack framework works well in seen scenarios but struggles to generalize to unseen tasks.
- Why unresolved: The current model architecture and training data appear to overfit to specific scenarios.
- What evidence would resolve it: Successful induction of safety violations in scenarios not present in the training set.

### Open Question 2
- Question: What specific defense mechanisms can effectively mitigate task-aware adversarial perturbations in the EAI action chain without causing excessive task failure?
- Basis in paper: [Explicit] The abstract highlights the urgent need for security-driven defenses, and Section 2.3 notes the lack of effective defense strategies for vision-language-action models.
- Why unresolved: The paper focuses entirely on defining safety and demonstrating attacks; it does not propose or evaluate defensive algorithms.
- What evidence would resolve it: Introduction of a defense method that significantly lowers ASR while maintaining high TSRC.

### Open Question 3
- Question: How can the transferability of adversarial perturbations be improved to achieve higher success rates in black-box attack settings?
- Basis in paper: [Inferred] Section 6.2 reports transfer-based black-box attacks on ACT achieved only 0.1 success rate.
- Why unresolved: Low transferability suggests perturbations optimized for a substitute model do not generalize well to the target model.
- What evidence would resolve it: A black-box attack strategy achieving ASR comparable to white-box methods without access to target model gradients.

## Limitations
- Sim-to-real gap: Physical robot experiments showed significantly lower attack success rates (40%) compared to simulation (50-60% range)
- Scenario-specific attack leader: Limited generalization to novel tasks without retraining for each new environment
- Safety metric validity: Relies on accurate tracking of human positions and tool states, which may fail under occlusion or sensor noise

## Confidence
- High Confidence: The attack mechanism (attack leader + PGD perturbation) is technically sound and reproducible
- Medium Confidence: The ISO-grounded safety taxonomy is principled but relies on simulation accuracy
- Medium Confidence: Adaptive sparse attack timing is theoretically justified but requires empirical validation across diverse task types

## Next Checks
1. Conduct systematic ablation on factors affecting real-world ASR (lighting, camera calibration, physics fidelity) to identify primary contributors to the performance gap.
2. Evaluate the attack leader's performance on out-of-distribution scenarios to measure ASR drop and analyze zero-shot adaptation potential.
3. Test safety violation detection under sensor noise, occlusion, and ambiguous configurations to quantify false positive/negative rates.