---
ver: rpa2
title: 'Multilingual Tokenization through the Lens of Indian Languages: Challenges
  and Insights'
arxiv_id: '2506.17789'
source_url: https://arxiv.org/abs/2506.17789
tags:
- languages
- tokenizers
- multilingual
- language
- tokenizer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive intrinsic evaluation of tokenization
  strategies across 17 Indian languages, addressing the challenge of building fair
  and effective tokenizers for morphologically rich and script-diverse languages underrepresented
  in existing NLP tools. The authors evaluate Byte Pair Encoding (BPE) and Unigram
  Language Model (ULM) algorithms across vocabulary sizes from 32K to 256K, comparing
  joint and cluster-based multilingual vocabulary construction approaches.
---

# Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights

## Quick Facts
- arXiv ID: 2506.17789
- Source URL: https://arxiv.org/abs/2506.17789
- Reference count: 18
- 17 Indian languages evaluated for tokenization quality using BPE vs ULM algorithms

## Executive Summary
This paper presents a comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages, addressing the challenge of building fair and effective tokenizers for morphologically rich and script-diverse languages underrepresented in existing NLP tools. The authors evaluate Byte Pair Encoding (BPE) and Unigram Language Model (ULM) algorithms across vocabulary sizes from 32K to 256K, comparing joint and cluster-based multilingual vocabulary construction approaches. Key findings include: normalization significantly improves tokenization quality, with language-specific rules like anusvāra conversion reducing fertility scores; ULM outperforms BPE in morphological alignment (IndicMorphScore); cluster-based training achieves lower word fragmentation rates (WFR) for morphologically rich languages compared to joint training; and multilingual tokenizers can effectively transfer to extremely low-resource languages when trained on related high-resource languages.

## Method Summary
The study evaluates multilingual tokenization using the Sangraha corpus (10% sampled, filtered for >10k rows/language, excluding Roman-script heavy text) and FLORES-200 dev set for evaluation. SentencePiece library is used to train BPE and ULM tokenizers with vocabulary sizes of 32k, 64k, 128k, and 256k. Two training strategies are compared: joint training (all languages together) and cluster-based training (languages grouped by script/morphology, then vocabularies merged). Normalization includes IndicNLP Unicode standardization plus custom anusvāra-to-nasal-consonant conversion. Temperature sampling (α=0.3) balances high-resource and low-resource language representation. Evaluation metrics include fertility (lower=better), character-per-token (CPT, higher=better), word fragmentation rate (WFR, lower=better), parity ratio, and IndicMorphScore for morphological alignment.

## Key Results
- Normalization reduces fertility scores (128k BPE improves from 1.717 to 1.701)
- ULM outperforms BPE in morphological alignment (IndicMorphScore)
- Cluster-based training reduces WFR for morphologically rich languages (Malayalam improves from 62.83 to 55.43)
- Multilingual tokenizers achieve fertility scores of 1.307-1.758 and CPT values of 2.820-3.880 in zero-shot settings for extremely low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Script-Level Normalization Reduces Tokenizer Entropy
Language-specific normalization rules, particularly anusvāra-to-nasal-consonant conversion, lower fertility scores by reducing Unicode variation. Standardizing character representations collapses semantically equivalent surface forms into shared subword units, allowing statistical algorithms to allocate merge operations more efficiently across actual morphological patterns rather than orthographic noise.

### Mechanism 2: ULM's Probabilistic Segmentation Aligns with Morphological Boundaries
Unigram Language Model produces tokens better aligned with morphological segmentation than BPE for Indian languages. ULM operates top-down by initializing with a large vocabulary and pruning based on likelihood maximization, allowing it to maintain multiple segmentation hypotheses and converge toward units that optimize the overall language model probability for morphologically rich languages.

### Mechanism 3: Cluster-Based Vocabulary Construction Mitigates High-Resource Dominance
Grouping typologically or script-wise similar languages into clusters before vocabulary construction reduces word fragmentation rates for morphologically rich, lower-resource languages. This approach partitions languages by script/morphology, trains independent tokenizers per cluster, then merges vocabularies, guaranteeing each cluster receives full vocabulary capacity and preserving language-specific subword units.

## Foundational Learning

- **Subword Tokenization Algorithms (BPE vs ULM)**
  - Why needed here: The paper's core comparison hinges on understanding how BPE (bottom-up, frequency-based merging) differs from ULM (top-down, probabilistic pruning) in handling morphological complexity.
  - Quick check question: Given a vocabulary budget of 128k, which algorithm would you expect to produce more morphologically coherent tokens for an agglutinative language like Turkish, and why?

- **Intrinsic Evaluation Metrics for Tokenizers**
  - Why needed here: The study relies entirely on intrinsic metrics (fertility, CPT, WFR, IndicMorphScore, parity ratio) to quantify tokenization quality without training downstream models.
  - Quick check question: If a tokenizer achieves fertility 1.3 for Language A and 2.1 for Language B on parallel sentences, what does this indicate about fairness, and what architectural change might address it?

- **Morphological Typology and Tokenization**
  - Why needed here: Indian languages span two families with different morphological profiles (Indo-Aryan: fusional; Dravidian: agglutinative), affecting how tokenizers segment words.
  - Quick check question: Why might a tokenizer optimized for English (analytical, limited inflection) produce excessive word fragmentation when applied to Kannada (agglutinative, complex suffix chains)?

## Architecture Onboarding

- **Component map:**
  Sangraha corpus → filtering (>10k rows, <10 Roman-script words) → 10% subsampling → IndicNLP normalization (Unicode standardization + anusvāra conversion) → temperature-based balancing (α=0.3) → SentencePiece tokenizer training → evaluation with FLORES-200 devset

- **Critical path:**
  1. Apply script-level normalization before any tokenizer training (non-negotiable per Section 3.1 findings)
  2. Use temperature sampling (α=0.3) to prevent high-resource language dominance in corpus composition
  3. Prefer ULM over BPE for morphologically rich Indian languages (evidence: IndicMorphScore)
  4. Use cluster-based training when WFR is high for specific low-resource languages (Table 4)
  5. Target 128k–256k vocabulary for best quality-compute tradeoff (Table 2)

- **Design tradeoffs:**
  - Joint training: Simpler pipeline, but vocabulary skewed toward high-resource languages
  - Cluster training: Fairer WFR for low-resource languages, but adds complexity (clustering step, vocabulary merging)
  - Larger vocabulary (256k): Better fertility/CPT/WFR, but increases embedding matrix size and training cost
  - Normalization: Improves intrinsic scores, but requires language-specific expertise and may obscure dialectal variation

- **Failure signatures:**
  - WFR > 50% for a specific language → vocabulary under-allocation; consider cluster-based training or increase vocab size
  - IndicMorphScore < 0.2 → BPE likely inappropriate; switch to ULM
  - High variance in token counts (Gini > 0.04, Table 10) across parallel sentences → unfair vocabulary distribution; rebalance corpus or adjust temperature
  - Zero-shot fertility > 2.0 on related low-resource languages → insufficient linguistic similarity or vocabulary coverage gap

- **First 3 experiments:**
  1. **Baseline validation**: Train joint ULM tokenizer with 128k vocabulary on normalized corpus; evaluate fertility and WFR across all 17 languages; confirm results align with Table 2 averages (F=1.680, WFR=42.067)
  2. **Cluster ablation**: Implement cluster-based training using Table 6 groupings; compare WFR reduction for Assamese, Bengali, Kannada, Malayalam, Tamil, Telugu against joint baseline; expect 5–8% WFR improvement per Table 4
  3. **Zero-shot transfer test**: Apply trained multilingual tokenizer to Awadhi, Bhojpuri, Chhattisgarhi, Magahi (no training data); verify fertility within 1.307–1.758 and CPT within 2.820–3.880 ranges as reported in Table 13

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do the observed differences in intrinsic metrics (such as fertility and Word Fragmentation Rate) between joint and cluster-based tokenizers correlate with actual performance on downstream multilingual tasks? The study is limited to intrinsic evaluation and does not train language models to measure the impact of tokenization strategies on end-tasks.

### Open Question 2
What is the optimal vocabulary size that balances the gains in tokenization quality against the increased computational costs and embedding matrix sizes for morphologically rich Indic languages? While the paper tests sizes from 32k to 256k and notes that larger sizes improve intrinsic scores, it acknowledges that larger vocabularies incur added cost without defining the point of diminishing returns.

### Open Question 3
How does the choice of tokenizer training strategy (specifically joint vs. cluster-based) influence the efficiency and effectiveness of cross-lingual transfer between high-resource and low-resource languages? The paper shows that high-resource tokenizers can segment low-resource languages (zero-shot), but it does not measure if cluster-based methods improve the transfer of linguistic knowledge or model performance compared to joint training.

### Open Question 4
Does the superior morphological alignment of the Unigram Language Model (ULM) over Byte Pair Encoding (BPE) persist across a wider variety of Indic languages beyond Hindi and Marathi? The morphological evaluation was constrained by the availability of high-quality morphological datasets, limiting the generalizability of the ULM superiority claim to the entire dataset of 17 languages.

## Limitations

- The study's evaluation is entirely intrinsic, with no assessment of downstream task performance to validate whether improved tokenization scores translate to better model quality
- Normalization rules, particularly anusvāra conversion, were implemented using language-specific heuristics that may not fully capture the phonological and morphological diversity across all 17 languages
- The cluster formation process uses K-means on binary vocabulary vectors, which assumes that linguistic similarity correlates with shared subword units—an assumption that may not hold for all typologically distinct languages

## Confidence

**High Confidence**: The observation that normalization reduces fertility scores (1.717→1.701 for 128k BPE) is directly supported by quantitative evidence from Table 1. The superiority of ULM over BPE for morphological alignment is consistently observed across multiple metrics (IndicMorphScore) and aligns with prior research by Bostrom and Durrett (2020).

**Medium Confidence**: The claim that cluster-based training improves WFR for morphologically rich languages (Malayalam: 62.83→55.43) is supported by Table 4, but the clustering methodology's sensitivity to K-means initialization and the vocabulary merging procedure introduce uncertainty. The effectiveness of multilingual tokenizers in zero-shot transfer to extremely low-resource languages (fertility 1.307-1.758) is demonstrated but limited to a small set of languages.

**Low Confidence**: The assertion that temperature sampling (α=0.3) effectively prevents high-resource language dominance relies on the assumption that the sampling formula adequately represents linguistic diversity, but this is not empirically validated within the study.

## Next Checks

1. **Downstream Task Validation**: Train multilingual language models using the evaluated tokenizers and assess performance on representative NLP tasks (POS tagging, NER, translation) for both high-resource and low-resource languages to verify that improved intrinsic metrics translate to better model quality.

2. **Normalization Ablation Study**: Systematically vary the anusvāra conversion rules across languages and measure the impact on fertility scores, WFR, and downstream task performance to identify the optimal balance between normalization and morphological preservation.

3. **Cluster Robustness Analysis**: Evaluate the sensitivity of cluster-based training to different clustering algorithms (hierarchical, spectral) and K values, measuring the variance in WFR and vocabulary overlap across multiple runs to establish the stability of the approach.