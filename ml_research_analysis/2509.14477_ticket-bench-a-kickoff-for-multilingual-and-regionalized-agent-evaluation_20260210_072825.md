---
ver: rpa2
title: 'Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation'
arxiv_id: '2509.14477'
source_url: https://arxiv.org/abs/2509.14477
tags:
- game
- team
- languages
- across
- ticket
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ticket-Bench is a multilingual benchmark for evaluating large language
  models in task-oriented function calling, using soccer ticket purchasing as a domain.
  It includes over 1000 scenarios in six languages (Portuguese, English, Spanish,
  German, Italian, and French) with localized entities, schedules, and user profiles.
---

# Ticket-Bench: A Kickoff for Multilingual and Regionalized Agent Evaluation

## Quick Facts
- arXiv ID: 2509.14477
- Source URL: https://arxiv.org/abs/2509.14477
- Authors: Thales Sales Almeida; João Guilherme Alves Santos; Thiago Laitz; Giovana Kerche Bonás
- Reference count: 40
- Models show multilingual function-calling performance disparities, with reasoning models achieving up to 0.91 pass^3 accuracy but significant cross-lingual gaps persisting

## Executive Summary
Ticket-Bench is a multilingual benchmark for evaluating large language models in task-oriented function calling, using soccer ticket purchasing as a domain. It includes over 1000 scenarios in six languages (Portuguese, English, Spanish, German, Italian, and French) with localized entities, schedules, and user profiles. Models interact with a fixed set of functions to book tickets under various constraints, and success is measured by whether the final environment state matches the expected outcome across multiple runs. Results show that reasoning models like GPT-5 and Qwen3-235B perform best, with pass^3 accuracy up to 0.91, but notable cross-lingual disparities persist, indicating uneven multilingual performance.

## Method Summary
The benchmark evaluates models across six languages using 1,020 queries (17 templates × 10 instantiations × 6 languages) involving soccer ticket purchasing. Each query requires models to call five functions (Get_User_Info, List_Games, Buy_Game_Ticket, Get_Leaderboard, Get_Weekday_From_Date) to navigate constraints like budget, team preference, date, and price. Success is measured using pass^k metrics (M=3 runs, k=3) where the environment state after execution must match expected bookings. The benchmark includes localized teams, cities, user profiles, and schedules for authentic cross-lingual evaluation.

## Key Results
- Reasoning models (GPT-5, Qwen3-235B) achieve highest pass^3 accuracy up to 0.91, while specialized function-calling fine-tuned models underperform their base counterparts
- Cross-lingual performance disparities persist even among top models, with no language being uniformly "easy" or "hard"
- Scaling improves accuracy and consistency but does not eliminate cross-lingual robustness challenges

## Why This Works (Mechanism)

### Mechanism 1: Localization-Driven Cross-Lingual Gap Detection
Using region-specific entities (teams, cities, culturally appropriate names) exposes language performance disparities that naive translation benchmarks miss. Localized queries require models to ground reasoning in authentic cultural contexts rather than performing surface-level translation. This reveals training data imbalances—models perform better on languages/regions with more representation in their training corpora.

### Mechanism 2: Pass^k Consistency Metric for Reliability
Multi-run consistency (pass^3) captures agent reliability better than single-attempt accuracy by rewarding stable execution. The metric computes p_i = (c_i/M)^k where c_i is correct runs out of M total attempts. A query solved correctly 3/3 times contributes 1.0; solved 2/3 times contributes only 0.296. This filters out lucky guesses and exposes stochastic failure modes.

### Mechanism 3: Reasoning-Architecture Compensation for Linguistic Uncertainty
Models optimized for extended reasoning cycles systematically outperform standard models on multilingual function-calling by allocating more compute to constraint decomposition and validation. Reasoning models expend additional inference compute to parse multi-constraint queries, validate function arguments against user profiles, and maintain state. This overhead compensates for language-specific uncertainties that would otherwise cause errors in standard models.

## Foundational Learning

- **Concept: Pass@k metrics (adapted from code generation)**
  - Why needed here: Understanding how consistency scoring works; the paper adapts pass@k from HumanEval-style benchmarks to agent evaluation.
  - Quick check question: If a model succeeds 2 out of 3 runs on a query, what's its contribution to pass^3? (Answer: (2/3)³ ≈ 0.296)

- **Concept: Function-calling agent paradigm**
  - Why needed here: The benchmark assumes models act as agents that call functions to manipulate environment state, not just generate text responses.
  - Quick check question: What's the critical difference between turn-level function-calling evaluation and end-state environment evaluation?

- **Concept: Cross-lingual transfer and training data bias**
  - Why needed here: Performance variation across languages directly reflects training corpus composition; understanding this helps interpret family-specific asymmetries.
  - Quick check question: Why might a model trained predominantly on English data struggle with Portuguese despite shared Romance language features?

## Architecture Onboarding

- **Component map:**
Environment Layer (per language):
├── Users (20 each): culturally appropriate names, balance, preferred team
├── Game Schedules (380 matches): localized teams/cities/stadiums, prices, dates
└── Leaderboards: historical league stats (points, goals, wins/losses)

Function Interface (5 functions, fully translated):
├── Get_User_Info → profile + constraints
├── List_Games → paginated search with filters (location, team, ordering)
├── Buy_Game_Ticket → state mutation (balance--, booking++)
├── Get_Leaderboard → historical query by year
└── Get_Weekday_From_Date → utility for day-of-week constraints

Query Templates: 17 templates × 10 instantiations × 6 languages = 1,020 total
└── 15% intentionally unsolvable (tests negative-case handling)

- **Critical path:**
1. Parse localized user query
2. Call Get_User_Info → extract budget + team preference constraints
3. Call List_Games with filters → retrieve candidate matches
4. Apply multi-constraint logic (semester, weekday, price, location, leaderboard)
5. Call Get_Weekday_From_Date if weekday constraint present
6. Call Buy_Game_Ticket with correct game_id
7. Verify final state: expected bookings present, no unexpected bookings

- **Design tradeoffs:**
- Breadth vs. depth: 6 major languages provides coverage but excludes underrepresented languages entirely
- LLM-free evaluation: Deterministic state checks ensure reproducibility but cannot evaluate dialogue quality or user experience
- Fixed function set: Controlled environment isolates language effects but doesn't test real-world tool discovery

- **Failure signatures:**
- Wrong game booked → constraint parsing or filtering error
- Multiple bookings → state tracking failure
- No booking when solution exists → over-constraining or search bug
- Booking when no valid solution → failure to detect unsatisfiable constraints

- **First 3 experiments:**
1. **Language asymmetry profiling:** Run one model family across all 6 languages; plot per-language pass^3 deviation from mean to identify systematic biases.
2. **Constraint complexity scaling:** Group templates by constraint count (1–5 constraints); analyze whether cross-lingual gaps widen with complexity.
3. **Error mode classification:** Sample 50 failures per language; categorize as (a) function selection, (b) argument extraction, (c) multi-constraint reasoning, or (d) state management to identify weak links.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cross-lingual robustness in agentic function-calling be achieved without relying solely on model scaling?
- Basis in paper: [explicit] The conclusion states that "scaling... is not sufficient" and "cross-lingual robustness remains an open challenge," noting that even the best models exhibit performance gaps of at least 5 points between languages.
- Why unresolved: While reasoning models and larger parameters improve overall accuracy (pass^3), they do not eliminate the variance in performance across different languages (e.g., Portuguese vs. English).
- Evidence would resolve it: The development of a training regime or architecture that yields statistically equivalent pass-rates across all six languages tested, decoupling performance from the specific dominant language of the training set.

### Open Question 2
- Question: Why does specialized function-calling fine-tuning (e.g., xLAM) appear to degrade multilingual generalization compared to the base instruct models?
- Basis in paper: [explicit] The results section observes that xLAM models "consistently worse than their base Qwen2.5 models," suggesting the specialized fine-tuning may have "negatively impacted generalization across languages."
- Why unresolved: The paper identifies the performance drop (e.g., xLAM-2-32b-fc-r scoring 0.26 vs. base 0.33) but does not isolate the cause, such as whether the fine-tuning data was English-centric or if it caused catastrophic forgetting of multilingual reasoning.
- Evidence would resolve it: An ablation study varying the language composition of function-calling fine-tuning datasets to measure the impact on multilingual agent benchmarks.

### Open Question 3
- Question: To what extent do specific pre-training corpora imbalances drive the family-specific language asymmetries observed in agentic tasks?
- Basis in paper: [explicit] The paper notes "family-specific asymmetries" (e.g., Gemini excelling in English but struggling in Italian, while Qwen excels in Portuguese) and states these patterns are "likely a reflection of the training data distribution."
- Why unresolved: This remains a hypothesis in the paper; the specific correlation between the volume of language-specific pre-training data and the resulting "delta from mean" performance in agentic scenarios is not quantified.
- Evidence would resolve it: A controlled analysis correlating the percentage of low-resource language data in pre-training with the reduction in standard deviation of pass^3 scores across languages.

## Limitations
- Performance variation across languages may reflect not just training data bias but also prompt quality and function interface clarity in different languages
- Cross-lingual evaluation focuses on six major languages, excluding entire language families and underrepresented linguistic contexts
- Pass^k metric assumes identical execution conditions across runs; any environmental randomness could inflate consistency scores

## Confidence
- **High confidence**: The core finding that multilingual function-calling remains challenging and shows systematic cross-lingual disparities, supported by consistent patterns across multiple model families
- **Medium confidence**: Attributing performance gaps specifically to training data distribution rather than prompt translation quality or interface clarity differences
- **Medium confidence**: The reasoning-architecture advantage claim; while GPT-5 and Qwen3-235B show superior results, direct comparison with equivalent non-reasoning models under identical conditions is not provided

## Next Checks
1. Conduct ablation studies testing whether cross-lingual performance gaps persist when using professionally translated prompts versus machine-translated prompts for all languages
2. Measure correlation between language performance and specific training data statistics (token counts, document diversity) across model families to verify training data bias hypothesis
3. Test whether simplifying the function interface or reducing the number of required function calls reduces cross-lingual disparities, isolating whether complexity or language representation drives the gap