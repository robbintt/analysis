---
ver: rpa2
title: Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal
  Solution Explanation
arxiv_id: '2504.03197'
source_url: https://arxiv.org/abs/2504.03197
tags:
- visual
- explanation
- solution
- keypoints
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the multimodal solution explanation task\
  \ and the ME2 benchmark, which aim to evaluate models\u2019 ability to identify\
  \ visual keypoints (e.g., lines, points, angles) and generate explanations that\
  \ explicitly reference these elements for educational purposes. The ME2 dataset\
  \ includes 1,000 math problems with annotated visual keypoints and keypoint-based\
  \ explanations."
---

# Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation

## Quick Facts
- arXiv ID: 2504.03197
- Source URL: https://arxiv.org/abs/2504.03197
- Authors: Jaewoo Park; Jungyang Park; Dongju Jang; Jiwan Chung; Byungwoo Yoo; Jaewoo Shin; Seonjoon Park; Taehyeong Kim; Youngjae Yu
- Reference count: 18
- Key outcome: Current models struggle with visual keypoint identification and generating visually-grounded explanations for math problems; only 23% of models succeed on both tasks.

## Executive Summary
This paper introduces the multimodal solution explanation task and the ME2 benchmark to evaluate models' ability to identify visual keypoints (lines, points, angles) and generate explanations that explicitly reference these elements for educational purposes. The ME2 dataset includes 1,000 math problems with annotated visual keypoints and keypoint-based explanations. Experiments show that current models struggle with both visual keypoint identification and generating coherent, visually-grounded explanations, especially open-source models. Proprietary models like Gemini 2.0 Flash perform better, but overall, there remains a significant gap in models' mathematical visual grounding and reasoning abilities for educational use.

## Method Summary
The ME2 benchmark evaluates models on two tasks: (1) Visual Keypoint Identification—selecting the correct set of visual keypoints from 5 options given a problem image, text, answer, and solution summary; (2) Keypoint-based Explanation Generation—producing an educational explanation that references provided visual keypoints. The benchmark contains 1,000 instances (763 geometry, 237 graph problems) with an average of 3.73 visual keypoints per problem. Evaluation uses LLM-based scoring (GPT-4o) on three criteria—Correctness, Fidelity, and Referencing—plus automatic metrics (BLEU, ROUGE, METEOR, BERTScore). Models are evaluated via inference-only on the tasks without additional training.

## Key Results
- Task 1 (Keypoint Identification): Proprietary models achieve 57.6% accuracy while open-source models range from 2.9% to 53.6%, with most performing near chance level (≈20%)
- Task 2 (Explanation Generation): Generated explanations score 1-2/5 on Referencing criteria, indicating weak integration of visual keypoints
- Dual-task success: Only 23% of proprietary models and 4-10% of open-source models succeed on both tasks
- Attention analysis: Models fail to focus on relevant visual keypoints despite having global visual attention capacity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit visual keypoints anchor model explanations to pedagogically meaningful diagram regions, reducing hallucination.
- Mechanism: By requiring models to identify and reference specific visual elements before generating explanations, the task forces alignment between visual perception and textual output.
- Core assumption: Models that attend to pedagogically relevant visual regions will generate more faithful explanations.
- Evidence anchors:
  - [abstract] "models can identify visual keypoints... and generate explanations that incorporate these key elements essential for understanding"
  - [section 6.2] Attention maps show models "fail to focus on the keypoints that are most relevant for explanation"
  - [corpus] Weak direct evidence; related work on visual grounding exists but not specifically for mathematical keypoints
- Break condition: If models can game the multiple-choice format without genuine visual understanding (e.g., through textual shortcuts), the mechanism fails.

### Mechanism 2
- Claim: Solution summary anchoring constrains the explanation space, enabling consistent keypoint-reasoning alignment.
- Mechanism: Providing T^tldr_s (solution summary) narrows valid reasoning paths, making keypoint identification more deterministic and evaluation more reliable.
- Core assumption: A single anchored solution path is sufficient for educational value, even when multiple valid solutions exist.
- Evidence anchors:
  - [section 3.1] "this summary anchors a single solution direction during model explanation generation, ensuring an unambiguous consensus set of visual keypoints"
  - [appendix C] Without anchoring, "the model's explanations often drift toward alternative reasoning paths or focus on irrelevant keypoints"
  - [corpus] No direct corpus evidence for this specific anchoring mechanism
- Break condition: If the anchored summary is incorrect or suboptimal, all downstream explanations inherit the error.

### Mechanism 3
- Claim: Two-stage task decomposition isolates perceptual failures from reasoning failures.
- Mechanism: Separating Visual Keypoint Identification from Keypoint-based Explanation Generation allows attribution of errors to either visual grounding or reasoning gaps.
- Core assumption: The stages are meaningfully decomposable; performance on one predicts performance on the other.
- Evidence anchors:
  - [section 4] "we structured the tasks to isolate perceptual and reasoning subskills"
  - [table 4] Only 23% (proprietary), 10% (generalist), 4% (specialized) succeed on both tasks—suggesting the stages are coupled
  - [corpus] No direct corpus evidence for this decomposition in mathematical education
- Break condition: If the multiple-choice format for keypoint identification introduces artifacts that don't reflect real open-ended visual grounding ability.

## Foundational Learning

- Concept: **Dual Coding Theory (DCT)**
  - Why needed here: The paper explicitly grounds its motivation in DCT—combining verbal and visual information enhances comprehension more than either alone.
  - Quick check question: Can you explain why a student might understand a geometry proof better when both a diagram highlight AND a textual reference to that highlight are provided simultaneously?

- Concept: **Visual Grounding in MLLMs**
  - Why needed here: The core limitation identified is that current MLLMs lack "mathematical visual grounding"—the ability to connect textual reasoning to specific visual elements.
  - Quick check question: Given an image of a triangle with a new auxiliary line added, can a model correctly identify that line and explain its purpose without hallucinating?

- Concept: **Attention Mechanisms in Vision-Language Models**
  - Why needed here: Section 6.2 analyzes attention maps to diagnose why models fail to focus on keypoint regions despite having the capacity to attend globally.
  - Quick check question: If a model's attention map shows uniform distribution across an image, what does that suggest about its ability to perform fine-grained visual reasoning?

## Architecture Onboarding

- Component map:
  - Problem text + problem image + solution summary → Visual Keypoint Identification → Correct VK set
  - Problem text + problem image + VKs → Keypoint-based Explanation Generation → Explanation text
  - Explanation + reference → LLM/Human evaluation → Scores on 3 criteria

- Critical path:
  1. Problem + image + summary → Visual Keypoint Identification → Correct VK set
  2. Problem + image + VKs → Keypoint-based Explanation Generation → Explanation text
  3. Explanation + reference → LLM/Human evaluation → Scores on 3 criteria

- Design tradeoffs:
  - Multiple-choice vs. open-ended for Task 1: Authors chose multiple-choice for "robust evaluation" since "open-ended scoring is ambiguous"—trades ecological validity for measurement reliability.
  - Anchoring with T^tldr_s: Constrains solution space but may exclude valid alternative explanations.
  - Korean-to-English translation: Increases accessibility but may introduce translation artifacts.

- Failure signatures:
  - Chance-level accuracy (≈20%) on Task 1: Indicates no meaningful visual grounding (observed in Math-PUMA, URSA).
  - High Correctness but low Fidelity/Referencing on Task 2: Model reasons correctly but doesn't follow the intended path or reference keypoints.
  - Attention maps showing global but not keypoint-specific focus: Visual processing without pedagogical grounding.

- First 3 experiments:
  1. Baseline probing: Run all 10 listed models on Task 1 to establish the gap between proprietary (Gemini 57.6%) and open-source (best Qwen2.5-VL 72B: 53.6%, worst URSA: 2.9%).
  2. Ablation on solution summary: Compare Task 2 performance with and without T^tldr_s to quantify anchoring effect (see Appendix C for qualitative examples).
  3. Attention visualization: Extract and analyze attention maps for Task 1 on a 10% subset to diagnose whether failures stem from perception (not looking at keypoints) or reasoning (looking but not understanding).

## Open Questions the Paper Calls Out
- Can architectural modifications to visual attention mechanisms close the performance gap between open-source and proprietary models in mathematical visual grounding?
- Does optimizing MLLMs for mathematical reasoning (specialization) inherently degrade the instruction-following capabilities required for multimodal explanation?
- How can the task of visual keypoint identification be transitioned from multiple-choice to robust open-ended generation?

## Limitations
- Dataset accessibility and representativeness: The ME2 benchmark was curated in-house, raising questions about potential domain bias and whether translation preserved original visual-verbal alignment.
- Evaluation reliability: Reliance on LLM-based evaluation introduces potential circularity, and the three evaluation criteria may not cleanly separate visual grounding from reasoning failures.
- Model selection: The evaluation includes diverse model families without controlling for architectural differences that could confound interpretation.

## Confidence
- High confidence: Models struggle with mathematical visual grounding—empirical results showing chance-level performance on keypoint identification and weak referencing in explanations are robust across multiple model families.
- Medium confidence: The two-stage task decomposition effectively isolates perceptual from reasoning failures—while the task design is sound, the coupling between stages suggests the decomposition may not be as clean as claimed.
- Low confidence: Anchoring with solution summaries meaningfully improves educational value—the paper provides qualitative examples but lacks rigorous comparison of learning outcomes between anchored and unanchored explanations.

## Next Checks
1. Benchmark replication with alternative evaluation: Recreate the ME2 tasks using a subset of problems and evaluate using both LLM-based scoring and human expert evaluation on a held-out sample to quantify agreement and identify potential evaluation artifacts.
2. Cross-linguistic validation: Translate a sample of problems back to Korean and evaluate whether performance changes significantly, testing whether translation artifacts or cultural/educational differences affect visual grounding capabilities.
3. Alternative anchoring mechanisms: Test performance when providing either no solution summary or multiple alternative solution paths to determine whether single-solution anchoring artificially constrains the explanation space and whether models can handle pedagogical ambiguity.