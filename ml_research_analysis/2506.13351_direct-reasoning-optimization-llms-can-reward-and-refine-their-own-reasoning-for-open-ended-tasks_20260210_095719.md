---
ver: rpa2
title: 'Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning
  for Open-Ended Tasks'
arxiv_id: '2506.13351'
source_url: https://arxiv.org/abs/2506.13351
tags:
- reward
- arxiv
- reasoning
- tokens
- rubric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Direct Reasoning Optimization (DRO) addresses the challenge of\
  \ applying reinforcement learning to open-ended, long-form reasoning tasks where\
  \ traditional verifiable reward signals are unavailable. DRO introduces a novel\
  \ token-level dense reward called Reasoning Reflection Reward (R3) that selectively\
  \ emphasizes reasoning-reflective tokens\u2014those whose likelihoods show high\
  \ variability under different chain-of-thought prefixes."
---

# Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks

## Quick Facts
- arXiv ID: 2506.13351
- Source URL: https://arxiv.org/abs/2506.13351
- Reference count: 40
- DRO achieves 63.7% win rate on ParaRev, 57.6% on RaR-Medicine, 84.5 macro-F1 on ContractNLI, and 68.4% accuracy on FinQA

## Executive Summary
Direct Reasoning Optimization (DRO) addresses the challenge of applying reinforcement learning to open-ended, long-form reasoning tasks where traditional verifiable reward signals are unavailable. DRO introduces a novel token-level dense reward called Reasoning Reflection Reward (R3) that selectively emphasizes reasoning-reflective tokens—those whose likelihoods show high variability under different chain-of-thought prefixes. This design captures the consistency between reasoning and reference outcomes at a fine-grained level while avoiding dilution from uninformative tokens.

The framework combines R3 with rubric-gating, which enforces essential task constraints through hard acceptance/rejection checks at the rollout group level. This ensures that fundamental lexical and semantic requirements are met, preventing reward hacking and mode collapse. Additionally, DRO employs variance-based filtering to improve training stability and sample efficiency by rejecting low-variance rollout groups. Evaluated across four diverse datasets, DRO consistently outperforms strong baselines and reaches target performance 2–3× faster than competing methods.

## Method Summary
DRO builds on Group Relative Policy Optimization (GRPO) with three key innovations: (1) R3 reward that computes token-level self-certainty across sampled CoT traces, identifies high-variance tokens, and applies softmax-weighted aggregation to emphasize reasoning-reflective tokens while suppressing uninformative ones; (2) rubric-gating that enforces essential task criteria through hard acceptance/rejection of rollout groups based on coverage and consistency gates, separating feasibility determination from reward ranking; (3) variance-based filtering that improves training stability by rejecting low-variance rollout groups where token-level discriminativeness is weak. The method is evaluated on four diverse datasets (ParaRev, RaR-Medicine, ContractNLI, FinQA) using Qwen3-14B and DeepSeek-R1-Distill-Qwen-7B base models, with pairwise win rates and accuracy metrics as primary evaluation measures.

## Key Results
- DRO achieves up to 63.7% win rate on ParaRev paragraph revision task
- DRO reaches target performance 2–3× faster than competing methods
- DRO demonstrates improved rubric compliance without explicit rubric scoring

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Reflective Token Emphasis via Variance-Weighted Self-Certainty
Long-form reference outputs contain few tokens whose self-certainty varies meaningfully across different CoT traces; emphasizing these improves reward discriminability. R3 computes per-token self-certainty across sampled CoTs, identifies high-variance tokens, and applies softmax-weighted aggregation with probability clipping. This amplifies informative tokens while suppressing dilution from stylistic/formatting tokens that dominate naive averaging. Core assumption: Tokens with high cross-CoT variance in self-certainty causally reflect reasoning quality differences. Break condition: If reasoning-reflective tokens are absent (<1% of reference), or if variance correlates with noise rather than reasoning quality, R3 may amplify spurious signal.

### Mechanism 2: Rubric-Gating as Hard Constraints Separate from Dense Rewards
Separating feasibility (rubric pass/fail) from optimization (dense reward ranking) prevents reward hacking without requiring reliable rubric-derived scores. Query-specific yes/no rubrics are generated and validated against reference answers. Rollout groups are rejected (not scored) if coverage gate (µ rollouts satisfy each rubric) or consistency gate (top ρ% by R3 satisfy ≥ν% rubrics) fail. Rejected groups don't contribute gradients. Core assumption: Rubrics can be generated reliably and capture essential task constraints; hard rejection doesn't excessively prune exploration. Break condition: If rubrics are poorly specified, over-constrain, or drift from task definition, gating may reject valid rollouts or fail to catch degenerate outputs.

### Mechanism 3: Variance-Based Filtering for Gradient Signal Quality
Low within-group reward variance leads to spurious z-score amplification in GRPO; filtering high-variance groups improves sample efficiency and stability. Compute per-token standard deviation σG,j across rollouts; identify top 10% high-variance tokens; if their mean σG < threshold τt, reject the group. Threshold adapts dynamically or via static pre-filtering. Core assumption: High token-level variance indicates meaningful discriminative signal; low variance indicates uniform low-quality rollouts. Break condition: If variance is driven by noise rather than signal, or if filtering is too aggressive, the effective training set may become too small or biased.

## Foundational Learning

- Concept: **Self-certainty / token-level conditional probability**
  - Why needed here: R3 relies on computing π(yj | q, ĉi, y<j) for each reference token conditioned on CoT; understanding how LLMs assign probability to tokens given context is essential.
  - Quick check question: Can you explain why log-probability aggregation is more sensitive to outliers than probability aggregation for long sequences?

- Concept: **Group Relative Policy Optimization (GRPO) and advantage computation**
  - Why needed here: DRO builds on GRPO; the z-score advantage formula (Eq. 1) directly affects how rewards translate to policy updates and why dilution matters.
  - Quick check question: In GRPO, what happens to advantages when all rollouts have similar rewards but the standard deviation is very small?

- Concept: **Reward hacking in RL for language models**
  - Why needed here: The paper's motivation centers on dense rewards enabling degenerate behaviors (e.g., no-revision mode collapse); rubric-gating specifically addresses this.
  - Quick check question: Why might a model learn to generate fluent but content-free outputs that still receive high self-certainty rewards?

## Architecture Onboarding

- Component map:
  Query -> Rollout Generator -> G traces -> R3 Calculator -> rewards {r_i} -> Rubric Module -> gate decisions -> Variance Filter -> GRPO Optimizer -> policy update

- Critical path:
  1. Query → Rollout Generator → G traces
  2. Traces + Reference → R3 Calculator → rewards {r_i}
  3. Traces → Rubric Module → gate decisions
  4. Rewards + Gates + Variance Filter → GRPO Optimizer → policy update

- Design tradeoffs:
  - **ω (emphasis factor)**: Higher ω (>4) accelerates early learning but over-sharpens weights, concentrating on too few tokens and magnifying noise. Paper recommends ω∈[1,2].
  - **Gate strictness (µ, ρ, ν)**: Stricter gates reduce reward hacking but may over-prune. Paper uses µ=1, ρ=25%, ν=60% as defaults.
  - **Filtering threshold (q-percentage)**: Aggressive filtering (~10% retention) improves quality but reduces effective data. Paper retains ~10-18% depending on dataset size.

- Failure signatures:
  - **Mode collapse to no-revision**: Entropy drops near zero, outputs become uniform, self-certainty stays high (Figure 5). Fix: Enable rubric-gating.
  - **Late-stage performance collapse**: R3 alone may destabilize as comparative weighting amplifies low-variance noise. Fix: Enable variance filtering.
  - **Rubric drift**: Generated rubrics become inconsistent or over-specific. Fix: Validate rubrics against reference answers; limit to 10 atomic criteria.

- First 3 experiments:
  1. **Ablate R3 components**: Compare plain Avg Prob vs. R3 with clipping only (ω=0) vs. full R3 (ω>0, clipping) on a held-out validation set; measure correlation between advantages and manually-labeled reasoning quality.
  2. **Gate sensitivity analysis**: Vary (µ, ρ, ν) systematically; track rejection rates, rubric compliance over training, and downstream metrics. Identify settings where rejection rate flattens (Figure 4) vs. stays linear.
  3. **Variance filter calibration**: Run with static pre-filtering at different q-percentiles (5%, 10%, 25%) and dynamic filtering; compare training curves (Figure 6) and final performance. Target: minimum retention that matches unfiltered peak performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The core assumption that high-variance token self-certainty reflects reasoning quality differences remains empirically unverified
- Rubric generation reliability and coverage completeness across diverse domains lack thorough analysis
- Variance filtering threshold calibration and optimal retention rates lack precise guidance

## Confidence
**High Confidence**:
- DRO consistently outperforms baselines across four diverse datasets
- The framework's three-component architecture (R3, rubric-gating, variance filtering) is technically sound
- Empirical results show 2-3× faster convergence than competing methods

**Medium Confidence**:
- The mechanism by which R3 emphasizes reasoning-reflective tokens (via softmax-weighted variance)
- The separation of feasibility (rubric-gating) from optimization (dense rewards) prevents reward hacking
- Variance filtering improves sample efficiency and stability

**Low Confidence**:
- Causal relationship between token-level variance patterns and reasoning quality
- Rubric generation reliability and coverage completeness across diverse domains
- Optimal calibration of filtering thresholds and gating parameters

## Next Checks
1. **Ablation Study on R3 Components**: Implement controlled experiments comparing plain average probability rewards vs. R3 with only clipping vs. full R3 with variance weighting. Measure correlation between computed advantages and manually-labeled reasoning quality on a held-out validation set. This directly tests whether the variance-weighted component adds meaningful signal beyond simpler alternatives.

2. **Rubric Quality Analysis**: Systematically analyze generated rubrics for coverage completeness, consistency, and potential bias. Compare rubric satisfaction rates between reference answers and model-generated answers to detect systematic gaps or drift. Additionally, test rubric robustness by intentionally perturbing reference answers and measuring rubric response stability.

3. **Variance Filter Parameter Sensitivity**: Conduct grid search over filtering thresholds (q-percentiles from 5% to 50%) and retention rates (5% to 50%) across all four datasets. Track not just final performance but training stability metrics (gradient variance, KL divergence) and effective dataset utilization. Identify the minimum retention rate that matches peak performance to establish practical bounds.