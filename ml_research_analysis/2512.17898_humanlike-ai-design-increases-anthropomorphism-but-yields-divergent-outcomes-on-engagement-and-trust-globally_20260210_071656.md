---
ver: rpa2
title: Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes
  on Engagement and Trust Globally
arxiv_id: '2512.17898'
source_url: https://arxiv.org/abs/2512.17898
tags:
- anthropomorphism
- trust
- design
- humanlike
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the causal relationship between humanlike
  AI design and user engagement/trust across 10 diverse countries (N=3,500). Using
  two large-scale experiments with real-time AI interactions, researchers found that
  while humanlike design levers reliably increase anthropomorphism, the downstream
  effects on trust and engagement are culturally contingent rather than universal.
---

# Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally

## Quick Facts
- arXiv ID: 2512.17898
- Source URL: https://arxiv.org/abs/2512.17898
- Reference count: 40
- Primary result: Humanlike AI design reliably increases anthropomorphism, but effects on trust and engagement vary significantly across cultures.

## Executive Summary
This study challenges universal assumptions about humanlike AI design through two large-scale experiments across 10 countries (N=3,500). While design characteristics and conversational sociability consistently increased anthropomorphism, their effects on trust and engagement proved culturally contingent rather than universal. The research reveals that users evaluate humanlikeness through applied interactional cues rather than abstract attributes, and that design choices fostering trust in some populations (e.g., Brazil) triggered opposite effects in others (e.g., Japan). These findings call for culturally adaptive governance frameworks rather than one-size-fits-all policies.

## Method Summary
The study employed a 2x2 factorial design manipulating Design Characteristics (DC) and Conversational Sociability (CS) using GPT-4o with temperature=1. Participants engaged in open-ended conversations with AI agents that varied in response speed, tone, emoji use, and social interaction patterns. Measures included 10-item anthropomorphism scales, behavioral engagement metrics (message count/length), and an incentivized Trust Game (100-point endowment). Qualitative analysis used LLM-in-the-loop labeling with 38 categories. The experiment was conducted across 10 countries with local language validation.

## Key Results
- Humanlike design levers (DC and CS) reliably increased anthropomorphism across all tested cultures
- Cultural background moderated the relationship between anthropomorphism and trust/engagement
- Design choices that increased trust in Brazil decreased it in Japan, demonstrating cultural contingency
- Users focus on interactional cues (conversation flow, response speed) rather than theoretical attributes when evaluating humanlikeness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Design Characteristics (DC) causally increase anthropomorphism by manipulating surface-level interactional cues.
- Mechanism: Users infer humanlikeness from pragmatic features—response speed variability, informal tone, emoji use, response length fluctuation, and human-seeming avatar/name—rather than from abstract attributes like consciousness.
- Core assumption: Users attend to conversational dynamics over philosophical attributes when forming anthropomorphic perceptions.
- Evidence anchors:
  - [abstract] "users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective"
  - [section] Figure 4 shows conversation flow (32.1%), understanding perspective (24.4%), response speed (22.5%) as top user-identified aspects; consciousness and soul each <0.5%
  - [corpus] Limited direct corpus support; related work on personality expression in LLM agents shows similar user perception effects but not this specific mechanism
- Break condition: If users in a target population prioritize theoretical attributes over interactional cues, DC manipulations may fail to increase anthropomorphism.

### Mechanism 2
- Claim: Conversational Sociability (CS) causally increases anthropomorphism through interpersonal behavior patterns.
- Mechanism: High-CS conditions—empathy, warmth, relationship-building questions, personality adaptation—activate social response schemas, making users perceive the agent as more humanlike.
- Core assumption: Humans have evolved social cognition that responds to warmth and relationship-building cues even from non-human agents.
- Evidence anchors:
  - [abstract] "humanlike design levers (Design Characteristics and Conversational Sociability) reliably increased anthropomorphism"
  - [section] High-DC/high-CS vs. low-DC/low-CS: β = 0.386, t(2396) = 5.590, p < 0.001; CS-only also significant (β = 0.18, p = 0.009)
  - [corpus] "Humanizing Machines" paper describes anthropomorphism operating across outlook, language, behavior, and reasoning—aligns with CS dimension but does not test causation
- Break condition: If CS cues are perceived as inauthentic or manipulative, backlash effects may reduce anthropomorphism and trust.

### Mechanism 3
- Claim: Culture moderates the relationship between anthropomorphism and trust/engagement, fracturing any universal effect.
- Mechanism: Cultural backgrounds shape baseline anthropomorphism tendencies and the interpretation of humanlike cues; what signals trustworthiness in one culture may signal deception in another.
- Core assumption: Cultural frameworks (e.g., animist traditions vs. human-machine distinction traditions) systematically influence how humanlike AI is evaluated.
- Evidence anchors:
  - [abstract] "design choices that increased trust in Brazil reduced it in Japan, demonstrating that effects are culturally contingent"
  - [section] Two country clusters emerged: Indonesia/Mexico/India/Nigeria/Egypt/Brazil (M=3.98) vs. US/Germany/Japan/South Korea (M=3.29); Japanese participants in high-DC/low-CS showed decreased trust and engagement
  - [corpus] Corpus does not provide direct cross-cultural anthropomorphism evidence; this is a gap the paper addresses
- Break condition: If cultural differences are reduced (e.g., through globalization of AI interaction patterns), moderator effects may weaken over time.

## Foundational Learning

- Concept: Anthropomorphism vs. perceived competence
  - Why needed here: The paper shows anthropomorphism can increase without changing perceived intelligence/competence; governance frameworks that conflate these will mispredict risks.
  - Quick check question: Can an AI be rated as more humanlike but equally competent? (Yes—this is what the DC/CS manipulations achieved.)

- Concept: WEIRD sampling bias
  - Why needed here: Existing safety frameworks assume Western user psychology generalizes globally; this paper demonstrates it does not.
  - Quick check question: Why might a design that increases trust in US users fail in Japan? (Cultural frameworks differ; no universal "AI experience.")

- Concept: Behavioral vs. self-reported measures
  - Why needed here: The paper uses both (Trust Game for behavioral trust; Likert scales for self-report); they can diverge.
  - Quick check question: If self-reported trust increases but Trust Game behavior does not, which is more consequential for governance? (Open question—depends on what outcomes governance targets.)

## Architecture Onboarding

- Component map:
  - Foundation model (GPT-4o) → System prompt (DC/CS manipulations) → User interface (chat) → Conversation logging → Post-interaction measures (Likert scales + Trust Game)
  - DC manipulation: response speed, tone, emoji, avatar, name (via prompt instructions)
  - CS manipulation: empathy, warmth, follow-up questions, personality adaptation (via prompt instructions)

- Critical path:
  1. Define target cultures and validate manipulations in each language
  2. Implement DC/CS prompt variations with localization (e.g., culturally appropriate names)
  3. Deploy open-ended conversation with guardrails (no deception, no self-attribution of human traits)
  4. Collect behavioral metrics (message count, token length) and self-report measures
  5. Run incentivized Trust Game for behavioral trust validation

- Design tradeoffs:
  - Stringency vs. generalizability: Using same foundation model (GPT-4o) for all conditions provides clean test but limits generalization to other architectures
  - Ecological validity vs. control: Open-ended conversation is realistic but introduces variability; structured tasks would reduce noise
  - Mundane vs. high-stakes contexts: Paper tests everyday interactions; results may not generalize to medical, financial, or crisis contexts

- Failure signatures:
  - Manipulation fails to increase anthropomorphism: Check whether DC/CS cues are culturally appropriate; what works in English may not translate
  - Anthropomorphism increases but trust/engagement do not: Expected for some populations—this is the heterogeneity finding, not a failure
  - Behavioral and self-report measures diverge: Expected; consider which metric is decision-relevant

- First 3 experiments:
  1. Replicate DC/CS manipulation in a new cultural group not in original study to validate cultural contingency pattern
  2. Test voice-based interface to assess whether modality amplifies anthropomorphism effects
  3. Extend to high-stakes context (e.g., financial advice scenario) to identify boundary conditions where trust effects may emerge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do humanlike AI design effects on trust and engagement differ in high-stakes or sensitive contexts (e.g., medical advice, financial guidance, crisis support)?
- Basis in paper: [explicit] Authors state: "Testing the effects of humanlike design in sensitive, high-stakes contexts, for example when it comes to persuasion or fraud attempts, would help delineate the boundary conditions of our findings."
- Why unresolved: This study deliberately limited interactions to mundane, non-sensitive topics; the relationship between anthropomorphism and trust may differ substantially in contexts where users are vulnerable or seeking critical guidance.
- What evidence would resolve it: Experimental designs matching Study 2's methodology but with high-stakes scenarios (medical diagnosis, financial advice, crisis counseling) across diverse cultural populations.

### Open Question 2
- Question: Do voice-based or embodied AI systems trigger stronger or qualitatively different anthropomorphic responses than text-based interfaces?
- Basis in paper: [explicit] Authors note: "Voice-based or embodied AI systems may trigger different, and potentially stronger, anthropomorphic responses due to additional sensory cues."
- Why unresolved: The study used only text-based chatbot interactions; voice prosody, facial expressions, or physical embodiment may amplify anthropomorphic effects or change their downstream consequences on trust and engagement.
- What evidence would resolve it: Cross-modal experiments comparing text, voice-only, and embodied AI agents with matched conversational content, measuring both self-reported anthropomorphism and behavioral trust outcomes.

### Open Question 3
- Question: Which specific cultural or psychological mechanisms explain the divergent effects of humanlike design across countries (e.g., positive trust effects in Brazil vs. negative effects in Japan)?
- Basis in paper: [inferred] The paper documents significant cross-cultural heterogeneity but acknowledges the post-hoc, exploratory nature of these findings and the limited sample of countries.
- Why unresolved: Cultural distance from the US showed only a non-significant correlation with anthropomorphism; deeper mechanisms (e.g., religious traditions, attitudes toward technology, individualism-collectivism) remain untested.
- What evidence would resolve it: Studies incorporating validated cultural dimension measures, qualitative interviews, and larger country samples to identify mediating factors between cultural background and anthropomorphism outcomes.

### Open Question 4
- Question: How do the effects of humanlike AI design unfold over time with repeated interactions—do they amplify, attenuate, or stabilize?
- Basis in paper: [explicit] "Longitudinal designs would illuminate how human-AI relationships evolve with repeated interaction and whether the effects we observed are transient or durable."
- Why unresolved: The cross-sectional design captured only immediate responses; trust, engagement, and emotional attachment may develop differently as users build familiarity with AI systems over weeks or months.
- What evidence would resolve it: Longitudinal panel studies tracking users across multiple interaction sessions over extended periods, with repeated measures of anthropomorphism, trust, and behavioral engagement.

## Limitations
- Cultural sampling may underrepresent global diversity, missing regions with unique human-AI interaction traditions
- Results limited to mundane conversational contexts; may not generalize to high-stakes domains like healthcare or finance
- Foundation model dependence (GPT-4o) limits generalization to alternative architectures or smaller models

## Confidence
- **High confidence**: The causal relationship between humanlike design levers and increased anthropomorphism is well-supported through randomized experimental design and replicated across multiple cultural groups.
- **Medium confidence**: The cultural moderation effects, while statistically significant and theoretically compelling, rely on post-hoc clustering of countries and could benefit from more granular cultural dimension analysis.
- **Medium confidence**: The behavioral vs. self-report trust measurement divergence is methodologically sound, but the paper does not fully explore why these measures dissociate or which is more consequential for real-world outcomes.

## Next Checks
1. **Architecture Generalization Test**: Replicate the core DC/CS manipulations using a different foundation model (e.g., Claude 3.5 or open-weight model) to assess whether cultural moderation effects persist across architectures, particularly testing the Brazil-Japan trust divergence pattern.

2. **High-Stakes Context Extension**: Design a follow-up experiment testing the same cultural groups in a consequential decision-making scenario (e.g., financial advice or medical triage) to identify boundary conditions where trust effects may converge or diverge further.

3. **Granular Cultural Dimension Analysis**: Apply established cultural frameworks (e.g., Hofstede dimensions, Schwartz values) to the existing dataset to test whether specific cultural orientations predict the direction and magnitude of trust/engagement effects, moving beyond country-level clustering to identify underlying mechanisms.