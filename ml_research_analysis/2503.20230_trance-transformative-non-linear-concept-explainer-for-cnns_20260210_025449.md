---
ver: rpa2
title: 'TraNCE: Transformative Non-linear Concept Explainer for CNNs'
arxiv_id: '2503.20230'
source_url: https://arxiv.org/abs/2503.20230
tags:
- concept
- faith
- explanations
- concepts
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TraNCE, a novel concept-based explainability
  framework for CNNs that automatically discovers accurate and consistent concepts
  by leveraging variational autoencoders (VAEs) for non-linear decomposition of image
  activations. Unlike existing methods that assume linear reconstructability of image
  activations, TraNCE captures intricate relationships within the activations through
  its VAE-based approach, enabling more meaningful concept discovery.
---

# TraNCE: Transformative Non-linear Concept Explainer for CNNs

## Quick Facts
- arXiv ID: 2503.20230
- Source URL: https://arxiv.org/abs/2503.20230
- Reference count: 40
- Primary result: TraNCE achieves Faith scores exceeding 98% on fine-grained visual categorization tasks, outperforming baseline methods through VAE-based non-linear concept discovery.

## Executive Summary
This paper introduces TraNCE, a novel concept-based explainability framework for CNNs that automatically discovers accurate and consistent concepts by leveraging variational autoencoders (VAEs) for non-linear decomposition of image activations. Unlike existing methods that assume linear reconstructability of image activations, TraNCE captures intricate relationships within the activations through its VAE-based approach, enabling more meaningful concept discovery. The framework introduces a new evaluation metric, the Faith score, which combines both Coherence and Fidelity to provide a comprehensive assessment of explainer faithfulness and consistency. Experiments on fine-grained visual categorization tasks demonstrate that TraNCE outperforms baseline methods, achieving Faith scores exceeding 98% across multiple CNN architectures and image classes, while addressing issues of concept duplication and ambiguity through its Bessel function-based visualization module.

## Method Summary
TraNCE employs a VAE to non-linearly decompose high-dimensional CNN activations into latent concepts. The framework extracts activations from intermediate CNN layers, reshapes and normalizes them, then trains a VAE to reconstruct these activations. The latent vector represents discovered concepts, which are evaluated using a new Faith score metric combining Fidelity (prediction agreement) and Coherence (concept consistency). The method includes a Bessel function-based visualization module that creates smooth transitions between concept activations, reducing ambiguity. The framework is validated on fine-grained visual categorization tasks using ImageNet validation images and pre-trained CNN architectures like ResNet50 and InceptionV3.

## Key Results
- TraNCE achieves Faith scores exceeding 98% on FGVC tasks, outperforming baseline methods (ICE, CRAFT) that use linear decomposition
- The Faith score provides more comprehensive evaluation than Fidelity alone by incorporating both accuracy and consistency measures
- Bessel function visualization successfully reduces concept duplication and ambiguity compared to threshold-based methods

## Why This Works (Mechanism)

### Mechanism 1
Non-linear decomposition via VAE captures intricate relationships in CNN activations that linear methods (PCA, NMF) cannot, leading to more valid latent projections. The VAE encoder maps high-dimensional activations to a lower-dimensional latent space through a probabilistic bottleneck, producing valid non-negative latent projections by fitting the intrinsic geometric structure of the data. Core assumption: CNN activations contain complex, non-linear feature relationships that cannot be isometrically embedded as a Euclidean subspace—particularly relevant for FGVC tasks with small inter-class variance.

### Mechanism 2
The Faith score (combining Fidelity and Coherence) provides a more comprehensive evaluation of explanation faithfulness than Fidelity alone. Fidelity measures prediction agreement between CNN and explainer via MAPE-like computation, while Coherence measures concept consistency through normalized cross-spectral density between CNN predictions and explainer predictions using Fourier transforms. The Faith score averages both, balancing accuracy with consistency. Core assumption: Concept explanations can be imperfect in accuracy but still trustworthy if consistent; conversely, inconsistent explanations cannot be trusted regardless of Fidelity.

### Mechanism 3
Bessel function-based visualization creates smooth transitions between high and low concept activations, reducing concept duplication and ambiguity compared to threshold-based methods. The Bessel function J_ν(x) of the first kind is applied to concept heatmaps, producing sinusoidal color mapping with smooth transitions: red for strong activations, blue for moderate, grey for low. Unlike hard thresholds, this interpolation reveals what the CNN saw and avoided without abrupt edges. Core assumption: Smooth transitions in visualization better reflect the underlying data distribution and human perceptual preferences for understanding concept boundaries.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs)**
  - Why needed here: Core to TraNCE's non-linear decomposition; understanding the encoder-decoder architecture, reparameterization trick, and ELBO objective is essential for grasping how concepts are extracted from activations.
  - Quick check question: Can you explain why a VAE's probabilistic bottleneck enables smoother latent representations than a deterministic autoencoder?

- **Concept: Concept Activation Vectors (CAVs)**
  - Why needed here: TraNCE builds on CAV methodology to represent high-level concepts as vectors in activation space; understanding directional derivatives and concept sensitivity is required for interpreting the Contribution scores.
  - Quick check question: How does a CAV quantify the influence of a semantic concept on a model's prediction?

- **Concept: Non-negative Matrix Factorization (NMF)**
  - Why needed here: TraNCE positions itself against NMF-based explainers (ICE, CRAFT); understanding NMF's linear decomposition assumption helps contextualize why non-linear methods are proposed.
  - Quick check question: What constraints does NMF impose on factorized matrices, and why might these be insufficient for FGVC tasks?

## Architecture Onboarding

- **Component map:** CNN Split (E(x) | C(A_l)) → VAE Explainer (f̂_θ(x,z) → z, W) → Visualization (Bessel enhancement) → Prototype Selector (MMD-critic) → Faith Evaluator (Fidelity + Coherence)
- **Critical path:** Extract activations A_l → Reshape and normalize → Train VAE on G_l → Extract z and W → Compute concept Contributions → Generate visualizations and Faith scores
- **Design tradeoffs:** Latent dimension c' (higher values increase granularity but computational cost); Layer selection (deeper layers produce more discriminative concepts); VAE vs. linear reducers (VAE offers superior non-linear modeling but requires hyperparameter tuning)
- **Failure signatures:** Duplicate/ambiguous concepts for classes with high inter-class similarity; Low Faith scores indicating CNN classification inefficiencies; Sanity check degradation with high Gaussian noise or extreme warping
- **First 3 experiments:** 1) Run TraNCE on ResNet50 with Australian Kelpie image, verify Contribution scores align with visualizations; 2) Compare TraNCE against ICE and CRAFT using Faith, Fidelity, and Coherence metrics; 3) Add Gaussian noise (σ = 0.3) to input images, confirm Faith scores decrease and prototype similarity drops

## Open Questions the Paper Calls Out

### Open Question 1
Can the TraNCE framework be effectively adapted for Vision Transformers (ViT), given the architectural differences in capturing global dependencies? The paper states, "The proposed TraNCE's suitability for transformers... remains uncertain. Exploring this path could be... a research direction." This is unresolved because TraNCE currently exploits the sequential data processing and local feature capture of CNNs, whereas transformers operate on global dependencies among input tokens in parallel.

### Open Question 2
How can a "contrasting paradigm" be formally integrated into TraNCE to prioritize distinctive concepts in classes with high inter-class similarity? The authors note a limitation where TraNCE discovers identical concepts (e.g., "feathers") for different classes and suggest, "A future line of work should prioritize the most important (contrasting) concepts." This is unresolved because the current unsupervised discovery process treats feature maps holistically, leading to duplicate or ambiguous prototypes when classes share visual features.

### Open Question 3
How can TraNCE be extended to video data to capture non-visual, temporal features such as behavior or temperament? The authors state, "Such limitations necessitate video categorization and expanding TraNCE's capabilities for video classifiers." This is unresolved because the current framework relies on static image analysis, which cannot capture temporal dynamics or behaviors that are often necessary for fine-grained categorization.

## Limitations
- Bessel function visualization's practical contribution to reducing concept ambiguity lacks quantitative validation
- Faith score may mask poor explanations when Fidelity is low but Coherence is high
- VAE training procedure is underspecified, particularly reconstruction loss formulation and β parameter

## Confidence

- **High Confidence:** VAE-based non-linear decomposition provides superior concept extraction compared to linear methods for CNN activations
- **Medium Confidence:** The Faith score comprehensively evaluates explanation quality by balancing Fidelity and Coherence
- **Medium Confidence:** Bessel function visualization improves interpretability by reducing threshold artifacts

## Next Checks

1. **Faith Score Edge Cases:** Test explanations with high Coherence but critically low Fidelity (<50%) to verify the metric doesn't misleadingly suggest acceptability
2. **Bessel Visualization Impact:** Quantitatively measure concept ambiguity reduction by comparing duplicate concept rates with and without Bessel enhancement across multiple classes
3. **VAE Hyperparameter Sensitivity:** Systematically vary the β parameter in the VAE loss and latent dimension c' to determine their impact on Faith scores and concept quality