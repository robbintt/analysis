---
ver: rpa2
title: 'ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual
  Question Answering'
arxiv_id: '2508.07321'
source_url: https://arxiv.org/abs/2508.07321
tags:
- answer
- question
- base
- indirection
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ObfusQAte, a framework to evaluate LLM robustness
  against obfuscated factual questions. ObfusQAte systematically generates complex,
  multi-layered question variants (Named-Entity Indirection, Distractor Indirection,
  and Contextual Overload) from base factual questions, increasing cognitive demand
  while preserving semantic intent.
---

# ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering

## Quick Facts
- arXiv ID: 2508.07321
- Source URL: https://arxiv.org/abs/2508.07321
- Reference count: 40
- Key outcome: LLM accuracy drops significantly on obfuscated factual QA, especially for Distractor Indirection and Contextual Overload variants.

## Executive Summary
ObfusQAte introduces a systematic framework to evaluate LLM robustness against obfuscated factual questions by generating three types of complex variants—Named-Entity Indirection, Distractor Indirection, and Contextual Overload—from base questions. Evaluated on 1024 questions across 7 state-of-the-art LLMs, the framework reveals substantial performance degradation on obfuscated inputs, with GPT-4o accuracy falling from 67.97% on base questions to 25.78% on Distractor Indirection variants. The study highlights a key vulnerability in LLMs: difficulty handling nuanced or contextually complex queries that require deeper semantic reasoning rather than pattern matching.

## Method Summary
The framework generates three types of obfuscated factual questions from 256 base questions sourced from TriviaQA and GKToday. Named-Entity Indirection replaces entities with synonyms or descriptions, Distractor Indirection adds plausible but incorrect answers, and Contextual Overload increases contextual complexity. Gemini 2.0 Flash generates variants at temperature 0.75 with human-in-the-loop annotation ensuring ground truth preservation (86.2% Cohen's κ). Seven LLMs are evaluated using zero-shot, few-shot, and chain-of-thought prompting strategies, with Exact Match accuracy comparing normalized model outputs to ground truth answers.

## Key Results
- GPT-4o accuracy drops from 67.97% on base questions to 25.78% on Distractor Indirection variants under zero-shot prompting.
- Contextual Overload variants show significant performance degradation across all models, indicating difficulty processing heavily loaded contexts.
- Intrinsic analysis reveals reduced model confidence and earlier hidden-state compression when processing obfuscated queries compared to base questions.

## Why This Works (Mechanism)
The framework works by systematically increasing the cognitive demand required to answer factual questions while preserving their semantic intent. Named-Entity Indirection forces models to perform multi-step inferential reasoning rather than pattern matching on explicit entity names. Distractor Indirection challenges models to ignore plausible but incorrect answers embedded in the question. Contextual Overload tests whether models can extract relevant information from heavily loaded contexts. This multi-faceted approach reveals whether models rely on surface-level patterns or genuine semantic understanding when answering factual questions.

## Foundational Learning
- **Named-Entity Indirection**: Replacing explicit entities with synonyms, pronouns, or descriptions to test semantic grounding. Needed because models often rely on explicit entity co-occurrence patterns. Quick check: Verify that NEI variants maintain ground truth while increasing inference steps.
- **Distractor Indirection**: Embedding plausible but incorrect answers within questions to test distractor filtering. Needed because real-world queries often contain irrelevant information. Quick check: Ensure DI variants include distractors that are contextually relevant but semantically incorrect.
- **Contextual Overload**: Increasing contextual complexity to test information extraction capabilities. Needed because user queries frequently contain excessive context. Quick check: Confirm CO variants have increased token count while preserving core question intent.

## Architecture Onboarding
- **Component Map**: Base Questions -> Obfuscation Generation (NEI/DI/CO) -> Human Verification -> LLM Evaluation (Zero/Few/CoT) -> Performance Analysis
- **Critical Path**: Question generation → Human verification → LLM evaluation → Accuracy computation
- **Design Tradeoffs**: Balance between obfuscation complexity and ground truth preservation; human verification ensures quality but adds cost and time.
- **Failure Signatures**: Reduced accuracy on obfuscated variants, lower model confidence scores, and earlier hidden-state compression in intrinsic analysis.
- **First Experiments**:
  1. Generate 50 base questions and evaluate on original LLMs to establish baseline performance
  2. Apply NEI obfuscation to 10 base questions and compare performance drop to baseline
  3. Test model confidence scores on obfuscated vs. base questions to verify intrinsic analysis findings

## Open Questions the Paper Calls Out
- Does the observed performance drop in factual QA generalize to tasks requiring logical or mathematical derivation, such as mathematical reasoning or translation?
- How does linguistic obfuscation impact LLM robustness in low-resource languages compared to English?
- Can white-box analysis of internal model states identify specific layers or features responsible for the failure to process obfuscated inputs?
- Does the "early representational compression" observed in smaller models (8B parameters) also occur in state-of-the-art models when processing obfuscated queries?

## Limitations
- Evaluation restricted to factual QA, leaving uncertainty about performance on reasoning, generation, or multi-turn conversational tasks.
- Dataset size (1024 questions) may not capture the full spectrum of real-world query complexity or domain diversity.
- Performance drops may be architecture-specific, as only 7 LLMs were tested without systematic ablation across model families.

## Confidence
- High Confidence: Significant accuracy drops on obfuscated inputs are well-supported by reported data and methodology.
- Medium Confidence: Claim that obfuscation reveals LLM reliance on pattern matching rather than genuine reasoning is plausible but not definitively proven.
- Low Confidence: Assertion that models do not memorize obfuscated queries lacks strong evidence.

## Next Checks
1. Evaluate ObfusQAte on non-factual QA tasks (e.g., open-ended reasoning or multi-hop inference) to assess framework applicability beyond factual questions.
2. Test ObfusQAte across diverse model architectures (e.g., transformer variants, retrieval-augmented models) to determine if performance drops are architecture-agnostic or model-specific.
3. Conduct a secondary human review of a stratified sample of obfuscated questions to verify ground truth consistency and detect potential annotation drift.