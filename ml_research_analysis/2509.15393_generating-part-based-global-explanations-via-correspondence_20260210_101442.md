---
ver: rpa2
title: Generating Part-Based Global Explanations Via Correspondence
arxiv_id: '2509.15393'
source_url: https://arxiv.org/abs/2509.15393
tags:
- explanations
- image
- global
- images
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEPC (Global Explanations via Part Correspondence),
  a system that generates part-based global explanations for deep neural network image
  classifiers. The approach addresses the challenge of producing interpretable global
  insights from black-box models by leveraging user-defined part labels from a small
  annotated subset of images.
---

# Generating Part-Based Global Explanations Via Correspondence

## Quick Facts
- arXiv ID: 2509.15393
- Source URL: https://arxiv.org/abs/2509.15393
- Reference count: 10
- Primary result: GEPC generates interpretable global part-based explanations for deep CNN image classifiers via correspondence-based label transfer and greedy set cover aggregation

## Executive Summary
This paper introduces GEPC (Global Explanations via Part Correspondence), a system that generates part-based global explanations for deep neural network image classifiers. The approach addresses the challenge of producing interpretable global insights from black-box models by leveraging user-defined part labels from a small annotated subset of images. GEPC uses Hyperpixel Flow to transfer these part labels to a larger dataset via visual correspondence, then generates local Minimal Sufficient Explanations (MSXs) using beam search. Finally, it aggregates these local explanations into global symbolic explanations using a greedy set cover algorithm.

## Method Summary
The GEPC pipeline operates in five steps: (1) annotate a small subset of images per class with human-defined part labels (e.g., {head, wing, eye}), (2) use Hyperpixel Flow to transfer these labels to unlabeled images by finding visual correspondences, (3) segment images into superpixels using SLIC, (4) generate local MSX explanations via beam search perturbation, identifying minimal superpixel sets that maintain 90% classification confidence, and (5) aggregate MSXs into global decision lists using greedy set cover to maximize coverage while minimizing rule count. The method was evaluated on Stanford Cars, CUB-200, and PartImageNet datasets using a 70:30 train-test split.

## Key Results
- Label transfer accuracy averaged 84.28% across 158 categories on PartImageNet dataset
- Global explanations achieved substantial coverage of model predictions via five-fold cross-validation
- Relational explanations (e.g., "A above B") covered more images than part-based ones in CUB200 and Stanford Cars datasets

## Why This Works (Mechanism)

### Mechanism 1: Semantic Correspondence via Hyperpixel Flow
- **Claim:** Transferring part labels from a small annotated set to a large unlabeled set is feasible because deep network features encode semantic correspondence across images.
- **Mechanism:** The system uses Hyperpixel Flow (HPF) to match "hyperpixels" (multi-layer feature vectors at a spatial location) between a query image and a visually similar "gallery" image. It enforces geometric consistency via Hough space voting, allowing it to map high-level concepts (like "wheel" or "eye") even if pixel colors differ, provided the visual angle is similar.
- **Core assumption:** The pre-trained backbone (e.g., ResNet) has already learned a feature space where object parts cluster semantically and maintain geometric relationships across instances.
- **Evidence anchors:**
  - [abstract] "transfers them to a larger dataset using Hyperpixel Flow (HPF)... achieves 84.28% average accuracy"
  - [section 3.1] "key idea of matching is to re-weight appearance similarity by Hough space voting to enforce geometric consistency."
  - [corpus] Contextual relevance found in "Local-to-Global Logical Explanations for Deep Vision Models" regarding utilizing primitive concepts, though specific validation of HPF mechanics is absent from the corpus neighbors.

### Mechanism 2: Sufficiency via Beam Search (MSX)
- **Claim:** A classifier's decision can be localized to a minimal set of image regions (superpixels) that maintain high prediction confidence.
- **Mechanism:** Instead of gradients, the method uses a perturbation-based beam search. It iteratively blurs regions to find the smallest set of superpixels (Minimal Sufficient Explanations - MSX) that keeps the classification score above 90% of the original.
- **Core assumption:** The model relies on distinct local features rather than global context or "background" pixels for classification.
- **Evidence anchors:**
  - [section 3.2] "MSX of an image... is defined as a minimal set of superpixels that achieves a high prediction confidence... where we set Ph = 0.9."
  - [abstract] "Local explanations are generated for each image using beam search."
  - [corpus] Weak direct evidence; "Training Feature Attribution for Vision Models" discusses attribution generally but does not validate this specific beam search sufficiency.

### Mechanism 3: Global Aggregation via Greedy Set Cover
- **Claim:** Local part-based explanations can be aggregated into a concise global decision list by solving a coverage optimization problem.
- **Mechanism:** Local MSXs are converted into symbolic rules (e.g., {Head, Wing}). A greedy set cover algorithm selects the most frequent rules that explain the largest number of images, resulting in a disjunctive normal form (DNF) explanation for the class.
- **Core assumption:** There exists a finite, small set of part-combinations that consistently explains the majority of class predictions.
- **Evidence anchors:**
  - [section 3.3] "We employ a greedy set cover algorithm... which iteratively selects the symbolic MSX that covers most of the remaining images."
  - [abstract] "aggregated into global explanations via a greedy set cover algorithm."

## Foundational Learning

- **Concept:** **Hypercolumns / Multi-scale Feature Fusion**
  - **Why needed here:** HPF relies on extracting features from multiple CNN layers (conv1 to conv5) to capture both texture (low-level) and semantics (high-level). Without understanding how layers hierarchically represent images, the correspondence mechanism is opaque.
  - **Quick check question:** Can you explain why matching only the final layer of a ResNet might fail to align object parts compared to using a hypercolumn?

- **Concept:** **Beam Search**
  - **Why needed here:** Finding the absolute minimal set of superpixels is combinatorially explosive. Beam search is the heuristic used to make MSX generation tractable.
  - **Quick check question:** If the beam width is set to 1, does the algorithm guarantee the minimal explanation? (Answer: No, it is a greedy heuristic).

- **Concept:** **Set Cover Problem (NP-Hardness)**
  - **Why needed here:** The transition from local to global explanations is framed as an optimization problem. Understanding this helps justify why the authors use a *greedy* approximation rather than an exact solver.
  - **Quick check question:** Why does the greedy algorithm provide an approximation ratio of O(log n) rather than the optimal solution?

## Architecture Onboarding

- **Component map:** Backbone -> SLIC Segmentation -> Nearest-Neighbor Search -> Hyperpixel Flow (HPF) -> Beam Search (MSX) -> Greedy Set Cover
- **Critical path:** The **Labeler**. If HPF transfers incorrect labels (e.g., labeling a "tire" as a "head"), the global rules will be hallucinated nonsense. The 84% accuracy cited is the ceiling for downstream explanation correctness.
- **Design tradeoffs:**
  - **Precision vs. Cost:** The system requires only 15% manual annotation (Gallery set), but sacrifices accuracy compared to 100% supervision.
  - **Stability vs. Diversity:** MSX finds *multiple* explanations per image. Aggregating them creates robust global rules, but may obscure rare, critical features in favor of common ones.
- **Failure signatures:**
  - **Rule Explosion:** If the greedy set cover returns hundreds of rules for a single class, the MSX threshold ($P_h$) may be too high, or the model is overfitting to noise.
  - **Visual Mismatch:** If global explanations highlight parts that visually don't correlate with the class, check the Nearest-Neighbor retrieval before HPF (are the gallery images actually similar to the queries?).
- **First 3 experiments:**
  1. **Ablation on Layer Depth:** Run HPF using only the final layer features vs. the full hypercolumn stack to quantify the value of multi-scale matching.
  2. **MSX Sensitivity Analysis:** Vary the confidence threshold $P_h$ (e.g., 0.8 vs 0.9 vs 0.95) to observe how the "minimality" of explanations changes vs. coverage.
  3. **Relational vs. Part-based:** Replicate the comparison in Figure 11 on a new dataset to verify if relational rules (e.g., "A above B") consistently cover more images than simple part presence.

## Open Questions the Paper Calls Out
- How can the GEPC methodology be adapted to derive global explanations for non-spatial or sequential data modalities, such as gene expression analysis or question answering in text?
- Can the dependency on strict visual similarity be relaxed to improve label transfer accuracy for query images with high pose variance or occlusion compared to the gallery set?
- How can the greedy set cover algorithm be modified to generate global explanations that are uniquely discriminative for specific classes rather than generally applicable?

## Limitations
- The HPF correspondence mechanism works well only on visually similar images, requiring nearest-neighbor search to find gallery images before label transfer
- The greedy set cover algorithm may generate global explanations that are not discriminative enough, as the same explanation might hold for multiple classes
- The method relies on spatial features and superpixels, making it unsuitable for non-spatial or sequential data modalities without significant modification

## Confidence
- **High Confidence:** The overall methodology (label transfer → MSX → aggregation) is logically coherent and addresses a genuine problem in XAI.
- **Medium Confidence:** The HPF correspondence mechanism works as described for similar images, but the geometric consistency enforcement and Hough space voting details are underspecified.
- **Low Confidence:** The beam search implementation details and the sensitivity of MSX quality to hyperparameters like Ph are not sufficiently described to reproduce or validate the results.

## Next Checks
1. **Error Analysis on Label Transfer:** Segment the 84.28% accuracy into per-class breakdowns to identify which categories HPF struggles with (e.g., cars vs. birds) and whether specific part types are consistently mislabeled.
2. **Beam Search Parameter Sweep:** Systematically vary beam width and Ph threshold to determine how these parameters affect the minimality and coverage of MSX explanations, identifying the stability region.
3. **Rule Complexity vs. Interpretability:** For each dataset, measure the number of rules required to cover 90%, 95%, and 99% of test images to empirically determine the "rule explosion" point where explanations become uninterpretable.