---
ver: rpa2
title: Retrieval-augmented GUI Agents with Generative Guidelines
arxiv_id: '2509.24183'
source_url: https://arxiv.org/abs/2509.24183
tags:
- arxiv
- guidance
- tutorials
- tutorial
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes RAG-GUI, a lightweight vision-language model
  that enhances GUI agents by generating task-aware guidance from web tutorials at
  inference time. It addresses the challenge of scarce training data and complex tasks
  requiring long-tailed knowledge by leveraging a two-stage training approach: supervised
  fine-tuning with synthetic labels followed by self-guided rejection sampling finetuning.'
---

# Retrieval-augmented GUI Agents with Generative Guidelines

## Quick Facts
- arXiv ID: 2509.24183
- Source URL: https://arxiv.org/abs/2509.24183
- Reference count: 20
- Primary result: Lightweight vision-language model that generates task-aware guidance from web tutorials to enhance GUI agents

## Executive Summary
This paper introduces RAG-GUI, a novel approach to improve GUI agents by generating task-aware guidance from web tutorials at inference time. The method addresses the challenges of scarce training data and complex tasks requiring long-tailed knowledge by leveraging a two-stage training approach: supervised fine-tuning with synthetic labels followed by self-guided rejection sampling finetuning. The approach is model-agnostic and functions as a plug-and-play module, achieving significant performance improvements across three distinct tasks with two model sizes (7B and 72B).

## Method Summary
RAG-GUI enhances GUI agents by retrieving and generating task-aware guidance from web tutorials during inference. The method employs a two-stage training approach: first, supervised fine-tuning with synthetic labels, then self-guided rejection sampling finetuning. This lightweight vision-language model is designed to be model-agnostic, functioning as a plug-and-play module that can be integrated with existing GUI agents. The approach addresses the challenge of scarce training data and complex tasks requiring long-tailed knowledge by leveraging web-based information at inference time.

## Key Results
- Achieves 13.3% and 10.7% absolute gains on the online AndroidWorld benchmark
- Consistently outperforms baseline agents across three distinct tasks with both 7B and 72B model sizes
- Narrows or surpasses the gap with training-based methods in real-world settings

## Why This Works (Mechanism)
The mechanism leverages retrieval-augmented generation to access relevant web-based knowledge during inference, effectively addressing the long-tail problem in GUI tasks. By generating task-aware guidance from tutorials, the model can handle complex tasks that require specialized knowledge not available in standard training data. The two-stage training approach (supervised fine-tuning followed by self-guided rejection sampling) allows the model to learn from synthetic data while maintaining quality through iterative refinement.

## Foundational Learning
- **Vision-language models**: Why needed - to process both visual GUI elements and natural language instructions; Quick check - model correctly identifies UI components and understands text commands
- **Retrieval-augmented generation**: Why needed - to access external knowledge beyond training data; Quick check - model retrieves relevant tutorial information for given tasks
- **Synthetic data generation**: Why needed - to overcome scarcity of labeled GUI interaction data; Quick check - synthetic examples maintain task diversity and complexity
- **Rejection sampling finetuning**: Why needed - to iteratively improve model quality while filtering poor examples; Quick check - performance improves monotonically across finetuning iterations

## Architecture Onboarding

**Component map**: GUI Agent -> RAG-GUI Module -> Web Tutorial Retriever -> Vision-Language Model -> Task Guidance Generator

**Critical path**: User task input → GUI Agent action → RAG-GUI module activation → Web tutorial retrieval → Guidance generation → Agent task execution

**Design tradeoffs**: The lightweight design sacrifices some potential performance for flexibility and broad compatibility, choosing model-agnosticism over specialized optimization. The reliance on web tutorials enables access to vast knowledge but introduces latency and dependency on internet connectivity.

**Failure signatures**: Performance degradation when web connectivity is unavailable, guidance quality issues when tutorial content is outdated or irrelevant, and potential model hallucination when retrieval fails but generation continues.

**First experiments**:
1. Test RAG-GUI module integration with a simple GUI agent on a single task type
2. Evaluate retrieval accuracy using known tutorial content for common tasks
3. Measure guidance generation quality by comparing agent performance with and without RAG-GUI assistance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Scalability concerns for synthetic data generation pipeline with larger, more diverse GUI tasks
- Performance on non-Android platforms remains unverified
- Reliance on synthetic labels may introduce distribution shifts between synthetic and real user interactions

## Confidence

| Claim | Confidence |
|-------|------------|
| RAG-GUI consistently outperforms baseline agents | High |
| RAG-GUI narrows or surpasses gap with training-based methods | Medium |
| Approach is model-agnostic and plug-and-play | Medium |

## Next Checks
1. Evaluate RAG-GUI's performance on cross-platform GUI tasks (e.g., iOS, web-based interfaces) to test generalizability beyond Android
2. Conduct a robustness analysis comparing synthetic and real user interaction data to quantify distribution shift impacts
3. Benchmark RAG-GUI against state-of-the-art end-to-end trained models on a wider range of complex, long-tailed GUI tasks to further substantiate scalability claims