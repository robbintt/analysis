---
ver: rpa2
title: 'Planning with affordances: Integrating learned affordance models and symbolic
  planning'
arxiv_id: '2502.02768'
source_url: https://arxiv.org/abs/2502.02768
tags:
- object
- tasks
- environment
- world
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a planning approach that combines learned affordance
  models with symbolic planning to perform complex multi-step tasks in hybrid discrete-continuous
  environments. The method extends PDDLStream by replacing hand-designed samplers
  with learned generative models for continuous variables.
---

# Planning with affordances: Integrating learned affordance models and symbolic planning

## Quick Facts
- arXiv ID: 2502.02768
- Source URL: https://arxiv.org/abs/2502.02768
- Reference count: 5
- Primary result: Learned affordance models significantly improve task success rates compared to hand-designed samplers in AI2-Thor simulation

## Executive Summary
This paper presents a novel approach to task and motion planning that integrates learned affordance models with symbolic planning. The method extends PDDLStream by replacing hand-designed samplers for continuous variables with learned generative models, specifically Kernel Density Estimation (KDE) models trained on successful object-action interactions. Experiments in AI2-Thor demonstrate that this approach significantly improves success rates for multi-step tasks, particularly when environmental complexity increases with additional objects.

## Method Summary
The approach learns object-action-specific affordance models using successful interaction data, then integrates these models into symbolic planning via the PDDLStream framework. For each object-action pair, a KDE model is trained on successful interaction positions. These learned models replace hand-designed samplers in PDDLStream streams, allowing the planner to generate valid continuous parameter values during planning. The method uses Fast Downward as the symbolic planner and AI2-Thor for simulation and execution, demonstrating improved task success rates compared to baseline approaches.

## Key Results
- Success rates improve from 70% to 100% for simple tasks (complexity 4)
- Medium complexity tasks (complexity 6) show improvement from 70% to 80%
- The method demonstrates better robustness when extra objects are added to the environment
- Learned models enable effective adaptation to new environments for tasks like moving obstacles

## Why This Works (Mechanism)

### Mechanism 1
Learning object-action-specific distributions for continuous parameters improves sampling efficiency compared to hand-designed heuristics. For each (object, action) pair, successful interaction examples are collected and fitted with a KDE model. When the planner needs a continuous parameter (e.g., a position from which to pick up an object), the KDE samples from the learned distribution rather than uniform or rule-based sampling. This biases samples toward regions where past interactions succeeded. The core assumption is that the distribution of successful continuous parameters is consistent across episodes and can be approximated from limited interaction data.

### Mechanism 2
Integrating learned samplers into PDDLStream's stream abstraction preserves symbolic planning guarantees while improving continuous parameter quality. PDDLStream's incremental algorithm alternates between applying streams to generate certified facts about continuous variables and running a discrete PDDL solver. By replacing hand-coded streams with learned affordance models, the declarative stream interface remains unchanged—the planner still receives certified predicates like `PickablePos(?o, ?q, ?pos)`—but the procedural component now samples from data-driven distributions. The core assumption is that the stream abstraction correctly encodes the relationship between continuous parameters and their certified predicates.

### Mechanism 3
Per-object affordance learning enables task-agnostic skill composition for multi-step planning. Affordance models are learned independently for each object-action pair without task-specific rewards. When a new task is given (e.g., "move obstacle to reach goal"), the symbolic planner composes these models by sequencing actions whose preconditions are satisfied by certified facts from prior steps. This decouples learning from planning. The core assumption is that tasks can be decomposed into object-action primitives whose affordances transfer across task contexts.

## Foundational Learning

- **Concept**: PDDL (Planning Domain Definition Language)
  - Why needed here: The entire framework builds on PDDL's representation of states as literals, actions with preconditions/effects, and goal specifications. Understanding PDDL syntax is required to read domain.pddl and instance.pddl files.
  - Quick check question: Can you write a PDDL action schema for `PickupObject` with parameters, preconditions, and effects?

- **Concept**: PDDLStream streams as conditional generators
  - Why needed here: Streams bridge discrete planning and continuous variables. You must understand how streams declare domains (input predicates), outputs (sampled values), and certified facts (guarantees about outputs).
  - Quick check question: Given a stream that generates pickable positions, what would its domain and certified facts look like?

- **Concept**: Kernel Density Estimation (KDE)
  - Why needed here: The paper uses KDE to learn affordance distributions. Understanding bandwidth selection and how KDE produces generative samples is necessary to debug or extend the learning component.
  - Quick check question: How does KDE differ from histogram-based density estimation, and what happens if bandwidth is too small?

## Architecture Onboarding

- **Component map**: Environment -> Affordance Learner -> Stream Layer -> Symbolic Planner -> Executor

- **Critical path**:
  1. Collect interaction data: For each object, attempt all actions; log successful (input, continuous_param) tuples
  2. Train KDE models: Fit one KDE per (object, action) pair on successful parameter values
  3. Define streams: For each action type (pick, drop, open), create stream schemas linking object/pose inputs to sampled position outputs
  4. Run planner: Incremental algorithm calls streams, adds certified facts, invokes Fast Downward until plan found or timeout
  5. Execute plan: Step through action sequence in AI2-Thor

- **Design tradeoffs**:
  - Per-object vs. generalized models: Current approach learns separate KDEs per object-action pair (high fidelity, poor generalization). Future work suggests learning over object/action classes
  - Hand-designed vs. learned samplers: Learned models improve success rates but require interaction data; hand-designed samplers need no training but perform worse (70% → 100% on simple tasks)
  - Full observability assumption: Current system assumes perfect perception; partial observability would require belief-state planning (noted in future work)

- **Failure signatures**:
  - Drop action failures: Most common failure mode; indicates KDE for droppable positions underfits or environment collision geometry differs from training
  - Extra objects near goal: Learned samplers must produce near-perfect values when obstacles clutter the goal region; performance degrades but less than baseline
  - Stream instance explosion: Algorithm exhaustively generates instances; many are irrelevant, increasing planning time (noted in future work)

- **First 3 experiments**:
  1. Replicate Task 1 (complexity 4) baseline comparison: Implement both hand-designed uniform sampler and KDE-based sampler for pick/drop positions; measure success rate over 10 runs with 0 and 20 extra objects
  2. Ablate training data size: Train affordance KDEs with 10, 50, 100, 200 successful interactions per object-action; plot success rate vs. training samples to identify minimum viable dataset
  3. Test cross-pose generalization: Train KDEs on objects in canonical pose; evaluate on randomized poses to assess robustness to the orientation variance the paper claims to handle

## Open Questions the Paper Calls Out

### Open Question 1
Can a single generic affordance model be learned across object categories or action types, rather than requiring separate models for each object-action pair? The current approach uses KDE to learn separate density functions for each object-action pair, which does not share statistical strength across similar objects. Experiments showing successful task completion using category-level or action-level shared models on unseen objects from known categories would resolve this.

### Open Question 2
How can the framework be extended to handle partially observable environments where planning over belief states is required? The current formulation assumes full observability and perfect perception of object bounding boxes and poses. A modified PDDLStream formulation with belief-state planning that maintains distributions over possible world states and achieves comparable success rates would resolve this.

### Open Question 3
Can heuristics for ranking stream instances improve planning efficiency by prioritizing useful samples? Currently, stream instances are exhaustively generated for all facts satisfying stream domains, creating many irrelevant instances that increase search space. Demonstration of reduced planning time with ranked stream instances while maintaining or improving task success rates would resolve this.

## Limitations
- Assumes full observability and known object states, limiting real-world applicability
- Performance on complex tasks (stacking, complexity 8) shows near-zero success rates
- Object-specific models rather than class-based learning limit generalization to novel objects

## Confidence
- **High confidence**: The mechanism showing learned KDE samplers improve over hand-designed baselines (70%→100% for simple tasks) is well-supported by experimental results
- **Medium confidence**: The claim that per-object affordance learning enables task-agnostic skill composition is plausible but only demonstrated on relatively simple task compositions
- **Low confidence**: The assertion that the method effectively handles significant pose variations is weakly supported, as the paper doesn't thoroughly test generalization across diverse object configurations

## Next Checks
1. Test generalization to novel object types not seen during training to evaluate class-based learning potential
2. Evaluate performance under partial observability conditions by occluding objects or adding sensor noise
3. Measure planning time scalability with increasing numbers of objects and actions to assess computational efficiency limits