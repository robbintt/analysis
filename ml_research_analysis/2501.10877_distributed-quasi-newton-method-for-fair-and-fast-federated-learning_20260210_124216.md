---
ver: rpa2
title: Distributed Quasi-Newton Method for Fair and Fast Federated Learning
arxiv_id: '2501.10877'
source_url: https://arxiv.org/abs/2501.10877
tags:
- learning
- local
- dqn-fed
- methods
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DQN-Fed, a novel federated learning framework
  designed to ensure fairness while leveraging the fast convergence properties of
  quasi-Newton methods. The method addresses the issue of unfairness in federated
  learning, where models often perform poorly on certain clients' local datasets despite
  high average accuracy.
---

# Distributed Quasi-Newton Method for Fair and Fast Federated Learning

## Quick Facts
- arXiv ID: 2501.10877
- Source URL: https://arxiv.org/abs/2501.10877
- Reference count: 40
- Primary result: DQN-Fed achieves up to 2% higher average accuracy and significantly lower standard deviation across clients compared to state-of-the-art fair federated learning methods.

## Executive Summary
This paper introduces DQN-Fed, a federated learning framework designed to address the fairness problem in FL, where models often perform poorly on certain clients' local datasets despite high average accuracy. DQN-Fed leverages the fast convergence properties of quasi-Newton methods while ensuring fairness by assisting the server in updating the global model such that all local loss functions decrease. The authors prove the convergence of DQN-Fed and demonstrate its linear-quadratic convergence rate through experiments on seven datasets.

## Method Summary
DQN-Fed addresses fairness in federated learning by ensuring all local loss functions decrease during global model updates. The method works by having each client compute their local gradient and approximate a quasi-Newton direction using BFGS. The server then performs gradient orthogonalization to find a common descent direction that reduces all local losses, scales these orthogonalized gradients by the quasi-Newton derived terms, and updates the global model. The framework is theoretically proven to converge with a linear-quadratic rate under strong convexity assumptions.

## Key Results
- Achieves up to 2% higher average accuracy compared to state-of-the-art fair federated learning methods
- Demonstrates significantly lower standard deviation across clients, indicating better fairness
- Shows faster convergence requiring fewer communication rounds
- Outperforms baselines on seven different datasets including CIFAR-10/100 and Shakespeare

## Why This Works (Mechanism)

### Mechanism 1: Fairness via Guaranteed Common Descent
The server computes a minimal-norm vector within the convex hull of orthogonalized client gradients. This direction ensures positive directional derivatives for all clients, guaranteeing no client's loss increases after an update. This relies on client gradient vectors being linearly independent and loss functions being L-Lipschitz smooth.

### Mechanism 2: Fast Convergence via Quasi-Newton Alignment
The orthogonalized gradients are scaled by a factor derived from local quasi-Newton directions. This forces the global update's impact per client to match the local Newton step's rate of change, leveraging curvature information. This requires the inverse Hessian to be reasonably approximated by the BFGS algorithm on each client.

### Mechanism 3: Linear-Quadratic Convergence Rate
The proof shows the error recursion depends on the Hessian approximation error. Under specific conditions (small initial error, accurate Hessian approximation), this leads to super-linear (quadratic) convergence; otherwise, it is linear. This assumes the global loss function is λ-strongly convex and L-smooth.

## Foundational Learning
- **Quasi-Newton Methods (e.g., BFGS):** Provide acceleration by approximating second-order information (curvature) without computing full Hessians. Quick check: Why does approximating the Hessian help in optimization compared to using just the gradient?
- **Multi-Objective Optimization (MOO) & Pareto Optimality:** Fairness is formalized as an MOO problem where the goal is to simultaneously minimize all local losses. Quick check: What characterizes a Pareto-stationary point, and why is it a weaker guarantee than Pareto-optimality?
- **Federated Learning Fundamentals (Non-IID Data):** The core problem is data heterogeneity (non-IID) across clients, which causes standard averaging to produce unfair models. Quick check: How does the distribution of data across clients (e.g., Non-IID vs. IID) affect the fairness of a global model trained with FedAvg?

## Architecture Onboarding
- **Component map:** Client -> Local training, gradient computation, BFGS update -> Send to Server; Server -> Gradient orthogonalization -> Compute optimal weights -> Update global model
- **Critical path:** The server-side gradient orthogonalization process is sequential and must be completed for all participating clients before the global model can be updated
- **Design tradeoffs:** Computation vs. Communication (adds client-side computation to reduce rounds), Fairness vs. Average Accuracy (forcing descent can limit peak performance), Hyperparameters (critical role of local epochs E and learning rate η)
- **Failure signatures:** Non-convergence (linearly dependent gradients or learning rate violations), Divergence (poor Hessian approximation)
- **First 3 experiments:** 1) Baseline Reproduction: Re-run CIFAR-10/100 experiments to validate implementation pipeline. 2) Participation Ablation: Test performance with varying client participation rates (10%, 50%, 100%). 3) Convergence Speed Test: Plot validation accuracy vs. communication rounds against FedMGDA+.

## Open Questions the Paper Calls Out
The paper mentions in Appendix I that fair FL algorithms are "not inherently robust against label noise" and currently relies on FedCorr integration to handle noisy datasets, leaving the question of whether DQN-Fed can be modified to provide inherent robustness to label noise unresolved.

## Limitations
- Assumes strong convexity and smoothness conditions rarely met in deep learning
- BFGS approximation quality and its impact on convergence is not extensively analyzed empirically
- Performance under highly heterogeneous architectures (different client models) is not addressed

## Confidence
- **High:** Guaranteed common descent through gradient orthogonalization is theoretically sound under stated assumptions
- **Medium:** Faster convergence due to quasi-Newton alignment is plausible but relies on Hessian approximation quality
- **Low:** Linear-quadratic convergence rate claim is theoretical; conditions for quadratic convergence may not hold in practice

## Next Checks
1. **Robustness to gradient dependence:** Systematically test DQN-Fed with varying levels of data heterogeneity, including cases where client gradients become linearly dependent
2. **BFGS approximation analysis:** Quantify the error in the BFGS Hessian approximation across different client datasets and epochs, correlating it with observed convergence speed
3. **Scalability benchmark:** Evaluate DQN-Fed's performance and communication efficiency on larger datasets (e.g., ImageNet) and with a larger number of clients (e.g., 100+) to assess real-world applicability