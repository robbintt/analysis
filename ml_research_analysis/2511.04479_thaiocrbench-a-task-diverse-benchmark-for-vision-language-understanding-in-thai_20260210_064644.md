---
ver: rpa2
title: 'ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in
  Thai'
arxiv_id: '2511.04479'
source_url: https://arxiv.org/abs/2511.04479
tags:
- tasks
- question
- answer
- document
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ThaiOCRBench, the first comprehensive benchmark
  for evaluating vision-language models on Thai text-rich visual tasks. The dataset
  includes 2,808 human-annotated samples across 13 diverse task categories such as
  chart parsing, table parsing, document classification, and handwritten content extraction.
---

# ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai

## Quick Facts
- **arXiv ID**: 2511.04479
- **Source URL**: https://arxiv.org/abs/2511.04479
- **Reference count**: 40
- **Primary result**: First comprehensive benchmark for Thai text-rich visual understanding tasks

## Executive Summary
ThaiOCRBench introduces a novel benchmark for evaluating vision-language models on Thai text-rich visual tasks. The dataset comprises 2,808 human-annotated samples across 13 diverse task categories including chart parsing, table parsing, document classification, and handwritten content extraction. The benchmark reveals significant performance gaps between proprietary and open-source VLMs, with proprietary models like Gemini 2.5 Pro outperforming open-source counterparts. Among open-source models, Qwen2.5-VL 72B performs best, though all struggle notably on fine-grained text recognition and handwritten content tasks.

## Method Summary
The benchmark employs zero-shot evaluation across 13 task categories using specific metrics per task group: Tree Edit Distance for structural tasks, BMFL (BLEU+METEOR+F1+NLS average) for generation/recognition, F1 for extraction, and ANLS for VQA tasks. Models are evaluated using vLLM inference engine with greedy decoding. The dataset includes chart parsing, table parsing, document parsing, fine-grained text recognition, full-page text recognition, key information extraction, key information mapping, document classification, diagram cognition, infographics VQA, and handwritten content extraction tasks.

## Key Results
- Proprietary models (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) significantly outperform open-source VLMs in zero-shot evaluation
- Among open-source models, Qwen2.5-VL 72B shows the best performance across most tasks
- Open-source models exhibit three main failure modes: language bias/code-switching, structural mismatch, and hallucinated content
- Performance degrades notably on fine-grained text recognition and handwritten content extraction tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic evaluation of VLMs across diverse Thai text-rich visual tasks using task-specific metrics. By employing zero-shot evaluation with standardized metrics like TED for structural tasks and BMFL for generation tasks, it provides a fair comparison framework. The identification of specific failure modes through error analysis enables targeted improvements in model architectures and training approaches for Thai-language document understanding.

## Foundational Learning

- **Tree Edit Distance (TED)**
  - Why needed here: Used as the primary metric for structural tasks like table, chart, and document parsing to compare the predicted hierarchical structure against the ground truth
  - Quick check question: How does TED differ from a simple string-based metric like BLEU when evaluating a table parsed into an HTML tree?

- **Code-Switching in VLMs**
  - Why needed here: Identified as a major failure mode; refers to a model's tendency to inappropriately switch from Thai to another language (e.g., English) during generation
  - Quick check question: A model given a Thai document outputs the summary in English. Is this an example of the "Language Bias and Code-Switching" error described in the paper?

- **Character Error Rate (CER) Decomposition**
  - Why needed here: The paper uses CER, broken down into substitutions, deletions, and insertions, to analyze the "Incorrect Content" failure mode in transcription tasks
  - Quick check question: If a model consistently outputs more characters than the ground truth, which component of the CER decomposition will be abnormally high?

## Architecture Onboarding

- **Component map**: Data Sourcing -> Data Annotation -> Q-A Generation -> Final Quality Check
- **Critical path**: Q-A Generation and Validation is the most critical step, requiring substantial human revision to ensure accuracy
- **Design tradeoffs**: The composite BMFL metric is an unweighted average of BLEU, METEOR, F1, and NLS, trading nuanced metric weighting for simplicity and reproducibility
- **Failure signatures**:
  - Language Bias/Code-Switching: Model outputs English or mixed Thai-English when prompted in Thai
  - Structural Mismatch: Keys and values misaligned or output in incorrect format in parsing tasks
  - Incorrect Content: High insertions (hallucinations) or deletions (omissions) in CER analysis
- **First 3 experiments**:
  1. Establish Baseline: Evaluate Qwen2.5-VL 7B on all 13 tasks to validate performance gaps
  2. Isolate Failure Mode: Test code-switching detection on Chart Parsing examples
  3. Metric Sensitivity Analysis: Compare TED vs LLM-as-Judge rankings for Document Parsing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM evaluator impact the alignment of benchmark scores with human judgment compared to traditional metrics?
- Basis in paper: [explicit] The authors state in Section 5.3 that "the choice of which LLM to serve as the 'judge' remains an open question requiring further systematic analysis"
- Why unresolved: The study only utilized GPT-4o-mini for LLM-as-Judge evaluation, finding a moderate Pearson correlation of 0.651 with traditional metrics, but did not test other potential judge models
- What evidence would resolve it: A comparative study evaluating VLM outputs using multiple LLM judges against human annotations

### Open Question 2
- Question: Can fine-tuning or instruction tuning on Thai-specific data significantly close the performance gap between open-source and proprietary models?
- Basis in paper: [explicit] The Limitations section notes that the exclusive use of zero-shot settings "does not account for performance gains achievable through fine-tuning"
- Why unresolved: The current benchmark only assesses out-of-the-box generalization, leaving the potential for domain adaptation unmeasured
- What evidence would resolve it: Evaluation of open-source models after fine-tuning on Thai corpora

### Open Question 3
- Question: Would a weighted or multi-objective composite metric better capture the heterogeneous difficulty across task types than the unweighted BMFL score?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section that the unweighted averaging of the BMFL metric "may not fully capture the heterogeneous difficulty across task types"
- Why unresolved: The current composite metric treats BLEU, METEOR, F1, and NLS equally, potentially obscuring specific model weaknesses
- What evidence would resolve it: A sensitivity analysis testing various weighted formulations of the composite metric

## Limitations
- Zero-shot evaluation only, not accounting for fine-tuning performance gains
- Modest dataset size (2,808 samples) compared to larger vision-language benchmarks
- Proprietary models subject to potential data contamination concerns from Thai document training data

## Confidence
- **High confidence**: Identification of three distinct failure modes in open-source VLMs
- **Medium confidence**: Qwen2.5-VL 72B performs best among open-source models based on limited comparisons
- **Medium confidence**: Proprietary models significantly outperform open-source counterparts, potentially influenced by data contamination

## Next Checks
1. **Data Contamination Audit**: Verify whether proprietary models were trained on Thai document data by analyzing performance on held-out samples created after training cutoffs
2. **Fine-tuning Impact Study**: Replicate evaluation with fine-tuned open-source models on ThaiOCRBench tasks to quantify performance gap reduction
3. **Prompt Template Robustness Test**: Systematically vary prompt templates across 13 task categories to determine if failure modes persist across different prompting strategies