---
ver: rpa2
title: 'VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness
  Validation'
arxiv_id: '2504.15659'
source_url: https://arxiv.org/abs/2504.15659
tags:
- design
- dataset
- code
- test
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating Register Transfer
  Level (RTL) code from natural language specifications using Large Language Models
  (LLMs). While existing datasets ensure syntactic correctness, they lack functional
  validation, leading to models that produce compilable but potentially incorrect
  code.
---

# VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation

## Quick Facts
- arXiv ID: 2504.15659
- Source URL: https://arxiv.org/abs/2504.15659
- Authors: Anjiang Wei, Huanmi Tan, Tarun Suresh, Daniel Mendoza, Thiago S. F. X. Teixeira, Ke Wang, Caroline Trippel, Alex Aiken
- Reference count: 40
- Primary result: VeriCoder achieves state-of-the-art RTL generation with up to 71.7% relative gain in functional correctness by training on functionally validated datasets

## Executive Summary
This paper addresses the challenge of generating Register Transfer Level (RTL) code from natural language specifications using Large Language Models (LLMs). While existing datasets ensure syntactic correctness, they lack functional validation, leading to models that produce compilable but potentially incorrect code. The authors introduce VeriCoder, a model fine-tuned on a dataset validated for functional correctness using a novel pipeline that combines unit test generation with feedback-directed refinement guided by a teacher LLM (GPT-4o-mini). This process iteratively refines RTL designs until they pass simulation tests, resulting in a dataset of 125,777 validated examples. VeriCoder achieves state-of-the-art performance on two RTL benchmarks, VerilogEval and RTLLM, with relative gains of up to 71.7% and 27.4% in functional correctness metrics.

## Method Summary
The authors construct a functionally validated RTL dataset by generating unit tests with a teacher LLM (GPT-4o-mini) and iteratively refining designs through feedback-directed simulation. Starting from the OriGen dataset of 217,462 (spec, design) pairs, they generate synthetic unit tests and simulate each design. Failed designs are refined up to 5 times using error messages from the simulator, with both the design and test updated as needed. This process yields 125,777 validated (spec, design, test) triples. A Qwen2.5-14B-Instruct base model is then fine-tuned using LoRA (rank=16, alpha=32) on this validated dataset. The resulting VeriCoder model is evaluated on VerilogEval and RTLLM benchmarks using Pass@k metrics.

## Key Results
- VeriCoder achieves 55.8% Pass@5 on VerilogEval, outperforming existing models by up to 71.7% relative gain
- On RTLLM, VeriCoder achieves 48.3% functional correctness (Pass@5), up from 44.8% for non-validated data
- Ablation study confirms that models trained on functionally validated data outperform those trained on syntactically-validated-only data
- Dataset construction success rate: 58% of OriGen examples pass functional validation within 5 refinement attempts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on functionally validated RTL examples improves downstream generation quality compared to syntactically-validated-only data.
- **Mechanism:** Unit tests provide explicit functional specifications that constrain the learning space. When a model trains on examples guaranteed to pass simulation, it learns the correspondence between natural language specifications and correct hardware behavior, not just valid syntax patterns.
- **Core assumption:** The synthetic unit tests generated by the teacher LLM accurately capture the intended functionality described in natural language specifications.
- **Evidence anchors:** [abstract] "An ablation study further shows that models trained on our functionally validated dataset outperform those trained on functionally non-validated datasets"; [Section V-B, Table IV] Pass@5 on VerilogEval improves from 53.5% (unvalidated data) to 55.8% (validated data); [corpus] Weak external validation—neighboring papers focus on reinforcement learning and reasoning enhancements rather than dataset validation methodology.

### Mechanism 2
- **Claim:** Feedback-directed refinement with simulation error messages enables correction of semantically buggy but syntactically valid RTL.
- **Mechanism:** Simulation provides ground-truth behavioral signals. When a design fails, error messages (timeouts, assertion failures, incorrect outputs) expose the mismatch between implementation and specification. The teacher LLM uses this signal to make targeted corrections rather than random modifications.
- **Core assumption:** The teacher LLM (GPT-4o-mini) can correctly diagnose root causes from error messages and generate appropriate fixes.
- **Evidence anchors:** [Section III-B, Figure 2-3] The buggy and3 module times out due to combinational loop; teacher model corrects by replacing multiple non-blocking assignments with single blocking assignment; [Section III-C, Algorithm 1] Iterative loop continues until design passes or max attempts (T=5) reached; [corpus] VeriReason [arxiv:2505.11849] similarly uses testbench feedback for RTL generation, suggesting simulation-guided refinement is a viable pattern.

### Mechanism 3
- **Claim:** Co-validation of tests alongside designs reduces specification drift during dataset construction.
- **Mechanism:** Tests can be incorrect or incomplete. By allowing the teacher model to update tests when they don't align with specifications, the pipeline maintains consistency across the triple (specification, design, test). This prevents designs from passing tests that don't actually validate intended behavior.
- **Core assumption:** The teacher model can distinguish between test bugs and design bugs when simulation fails.
- **Evidence anchors:** [Section III-A] "When needed, the unit test is also updated to better align with the natural language specification"; [Section III-C, Figure 4b] Refinement prompt instructs model to "make changes to either the design or the test (or both)"; [corpus] No direct external validation of this specific mechanism found in neighbors.

## Foundational Learning

- **Concept: RTL vs. Software Semantics**
  - **Why needed here:** RTL has hardware-specific failure modes invisible to syntax checkers—combinational loops, blocking vs. non-blocking assignment semantics, timing hazards. Understanding these is prerequisite to diagnosing simulation failures.
  - **Quick check question:** Given an `always @*` block that both reads and writes variable `y`, what happens during simulation?

- **Concept: LoRA Fine-Tuning**
  - **Why needed here:** The paper uses Low-Rank Adaptation to fine-tune a 14B parameter model efficiently. Understanding LoRA's rank and scaling factor hyperparameters is necessary to reproduce or modify the training setup.
  - **Quick check question:** If LoRA rank is 16 and the original weight matrix is 4096×4096, how many trainable parameters does LoRA add per layer?

- **Concept: Pass@k Metric**
  - **Why needed here:** RTL generation is evaluated via Pass@k, which estimates probability that at least one of k samples passes all tests. Understanding this metric is essential for interpreting benchmark comparisons.
  - **Quick check question:** If a model generates n=10 samples and c=6 pass, is Pass@1 equal to 0.6?

## Architecture Onboarding

- **Component map:** [Original Dataset D] → (spec, design pairs, syntax-validated only) → [Teacher LLM: GPT-4o-mini] ←── Test Generation Prompt → [Generated Unit Test] → [Compiler + Simulator] → pass/fail + error message → (if fail) [Teacher LLM] ←── Refinement Prompt (spec, design, test, error) → [Updated Design/Test] → loop back to simulation → (if pass, within T=5 attempts) [Validated Dataset D'] → (spec, design, test triples) → [LoRA Fine-Tuning] on Qwen2.5-14B-Instruct → [VeriCoder Model]

- **Critical path:** The refinement loop (Algorithm 1, lines 4-10) determines dataset quality. If too many examples fail to pass within T=5 attempts, dataset size shrinks and coverage suffers. The paper reports 125,777 validated examples from the original OriGen dataset of 217,462—approximately 58% success rate.

- **Design tradeoffs:**
  - **Teacher model choice:** GPT-4o-mini selected for cost efficiency over stronger models (GPT-4o, o3-mini). Tradeoff: lower per-example quality vs. dataset scale.
  - **Max attempts T=5:** Limits API costs and prevents infinite loops on unfixable examples. Tradeoff: some fixable examples may be abandoned.
  - **LoRA rank 16:** Reduces memory/compute vs. full fine-tuning. Tradeoff: may underfit compared to full parameter updates.

- **Failure signatures:**
  - **Combinational loops:** Simulation timeout (as in Figure 2b)—detectable via simulation but not compilation.
  - **Test-quality failures:** Design passes synthetic test but fails benchmark evaluation—suggests test insufficiently covers specification.
  - **Specification ambiguity:** Teacher model generates test that misinterprets natural language—manual review found 8% mismatch rate.

- **First 3 experiments:**
  1. **Reproduce ablation:** Fine-tune the same base model (Qwen2.5-14B-Instruct) on both the original OriGen dataset and the validated VeriCoder dataset using identical hyperparameters. Verify that validated data yields higher Pass@5 on VerilogEval and RTLLM.
  2. **Test generation quality audit:** Randomly sample 50 examples from the validated dataset. For each, manually verify that the generated test cases cover the key behaviors described in the specification. Quantify coverage gaps.
  3. **Refinement iteration analysis:** Log the number of refinement iterations needed per example. Analyze whether examples requiring more iterations have lower quality (measured by downstream benchmark performance when excluded from training).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can formal verification techniques be integrated into the dataset construction pipeline to rigorously ensure functional correctness beyond the coverage of synthetic unit tests?
- **Basis in paper:** [explicit] The authors acknowledge that synthetic test cases may fail to capture all edge cases and explicitly call for future work to explore integrating formal verification techniques.
- **Why unresolved:** Current validation relies on simulation-based testing, which cannot guarantee the absence of logic errors or corner cases in the generated RTL.
- **What evidence would resolve it:** A dataset construction pipeline that incorporates model checking or theorem proving to formally prove design correctness alongside simulation.

### Open Question 2
- **Question:** Can the feedback-directed refinement and unit test generation methodology be effectively scaled to repository-level RTL generation involving complex cross-file dependencies?
- **Basis in paper:** [explicit] The paper notes that existing approaches focus on small-scale generation and identifies extending the methodology to repository-level codebases with long-range context as a key area for future work.
- **Why unresolved:** The current dataset and pipeline focus on individual modules, whereas real-world hardware development requires handling intricate dependencies across multiple files.
- **What evidence would resolve it:** A system capable of generating and validating functionally correct, multi-file RTL projects rather than isolated module snippets.

### Open Question 3
- **Question:** To what extent can Reinforcement Learning (RL) utilizing test-case feedback further optimize LLM performance compared to the current supervised fine-tuning approach?
- **Basis in paper:** [explicit] The authors suggest that RL could be a powerful framework for optimization and propose applying RL techniques to the VeriCoder dataset using test outcomes as feedback.
- **Why unresolved:** The current model is trained solely via supervised fine-tuning (LoRA), leaving the potential performance gains from an RL-based feedback loop unquantified.
- **What evidence would resolve it:** Comparative experiments showing performance metrics (e.g., pass@k) for an RL-optimized model against the supervised baseline.

## Limitations

- The quality of the validated dataset depends heavily on the teacher model's ability to generate comprehensive unit tests and make correct refinements based on simulation feedback
- The synthetic test generation process may miss edge cases or misinterpret specifications, with manual review revealing an 8% mismatch rate
- The methodology focuses on small-scale module generation rather than repository-level RTL development with complex cross-file dependencies

## Confidence

- **High confidence:** The ablation study showing improved performance on validated vs. non-validated data (55.8% vs 53.5% Pass@5 on VerilogEval)
- **Medium confidence:** The 71.7% relative gain on VerilogEval, as this depends heavily on the quality of the OriGen dataset and evaluation setup
- **Medium confidence:** The claim that feedback-directed refinement with simulation error messages enables semantic corrections, as this assumes the teacher model can correctly diagnose root causes

## Next Checks

1. **Test Coverage Audit**: Systematically sample 100 examples from the validated dataset and manually verify that generated test cases exercise all major functional requirements stated in the specifications. Calculate the fraction of specifications with complete test coverage.

2. **Refinement Convergence Analysis**: For a stratified sample of 50 examples, log the number of refinement iterations required to pass. Correlate this with downstream performance to determine if examples requiring many iterations are lower quality.

3. **Cross-Validation with Independent Tests**: Take 20 examples from the validated dataset and generate an independent set of test cases using a different teacher model (e.g., Claude 3.5 Sonnet). Measure the agreement rate between the original tests and the independent tests on the same designs.