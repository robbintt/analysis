---
ver: rpa2
title: LLMs are Bayesian, in Expectation, not in Realization
arxiv_id: '2507.11768'
source_url: https://arxiv.org/abs/2507.11768
tags:
- length
- theoretical
- martingale
- bayesian
- log2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper resolves a theoretical paradox in large language models:\
  \ transformers violate martingale properties (a requirement for Bayesian inference\
  \ on exchangeable data) yet achieve Bayesian-level compression efficiency. The authors\
  \ show that positional encodings fundamentally alter the learning problem\u2014\
  transformers minimize expected conditional Kolmogorov complexity over permutations\
  \ rather than permutation-invariant complexity."
---

# LLMs are Bayesian, in Expectation, not in Realization

## Quick Facts
- arXiv ID: 2507.11768
- Source URL: https://arxiv.org/abs/2507.11768
- Authors: Leon Chlon; Sarah Rashidi; Zein Khamis; MarcAntonio M. Awada
- Reference count: 40
- Primary result: Transformers violate martingale properties yet achieve Bayesian-level compression efficiency by minimizing expected conditional Kolmogorov complexity over permutations

## Executive Summary
This paper resolves a theoretical paradox in large language models: transformers violate martingale properties (a requirement for Bayesian inference on exchangeable data) yet achieve Bayesian-level compression efficiency. The authors show that positional encodings fundamentally alter the learning problem—transformers minimize expected conditional Kolmogorov complexity over permutations rather than permutation-invariant complexity. This explains why transformers can simultaneously violate exchangeability while achieving near-optimal MDL optimality with excess risk O(n^(-1/2)). The framework enables practical methods for extracting calibrated uncertainty estimates and optimizing computational efficiency in deployment.

## Method Summary
The paper combines theoretical analysis with empirical validation on GPT-3. Theoretical results establish martingale violation bounds, MDL optimality proofs, and implicit posterior representation guarantees. Empirically, the authors generate balanced binary sequences (100 per length for n=10,12,...,198), query GPT-3 for next-token log probabilities, compute martingale gaps, and fit scaling models. They use debiasing techniques to remove RoPE periodicity artifacts and compare log(n)/n vs 1/n scaling models with weighted least squares. Permutation averaging experiments validate variance reduction scaling, and compression efficiency tests measure convergence to theoretical entropy limits.

## Key Results
- Transformers violate the martingale property at rate Θ(log n/n) rather than Θ(1/n)
- Achieves MDL optimality with excess risk O(n^(-1/2)) when performance is averaged over random input orderings
- Implicit posterior representation enables 99% of theoretical entropy limits within 20 examples on binary sequences

## Why This Works (Mechanism)

### Mechanism 1: Positional Encoding Induces Martingale Violations
- Claim: Transformers with positional encodings systematically violate the martingale property at a rate of Θ(log n/n).
- Mechanism: Positional encodings explicitly break exchangeability, meaning predictions are not invariant to input ordering. The log n/n scaling arises from the expected distance between random permutations of the sequence, creating a predictable, quantifiable deviation from ideal Bayesian updating.
- Core assumption: The transformer's core function is Lipschitz continuous and positional encoding variance is bounded.
- Evidence anchors:
  - [abstract] "positional encodings induce martingale violations of order Θ(log n/n)"
  - [section 3.2, Theorem 3.4] Formal bound: $\Delta_n \leq \frac{L_f^2 \sigma_{PE}^2}{2} \cdot \frac{\log n}{n} + O(n^{-3/2})$
  - [section 5.2.1] Empirical validation on GPT-3: adjusted R² = 0.759 for log n/n model vs. R² < 0.3 for 1/n model.
  - [corpus] Weak direct corpus evidence on this specific Θ(log n/n) scaling.

### Mechanism 2: Bayesian Optimality in Expectation via Conditional Complexity Minimization
- Claim: Transformers achieve Bayesian-level compression (MDL optimality) when performance is averaged over random input orderings, despite being non-Bayesian in any single realization.
- Mechanism: The model minimizes expected conditional Kolmogorov complexity E_π[K(X|π)], which equals the permutation-invariant complexity K(X) plus an informational term I(X;π). Averaging over orderings neutralizes the position-specific bias, recovering optimal compression.
- Core assumption: Input data is exchangeable; the permutation distribution is uniform over permutations consistent with sufficient statistics.
- Evidence anchors:
  - [abstract] "transformers achieve information-theoretic optimality with excess risk O(n^{-1/2}) in expectation over orderings"
  - [section 3.3, Theorem 3.7] EX,π[MDLn(Tθ∗, Xπ(1:n))] = nH(p) + O(√(n log n))
  - [corpus] Related work (ArXiv:2509.11208) uses similar expected conditional description length framework.

### Mechanism 3: Implicit Posterior Representation in Sufficient Statistics
- Claim: Hidden states implicitly encode moments of the Bayesian posterior, enabling approximate Bayesian prediction.
- Mechanism: Attention mechanisms learn to compute sufficient statistics (e.g., token counts), which MLP layers then transform into approximations of posterior moments (e.g., mean, variance of a Beta distribution).
- Core assumption: The architecture and training allow for formation of "counting heads" and moment-approximating MLPs.
- Evidence anchors:
  - [section 3.4, Theorem 3.8] Predictive distribution approximation error is O(t⁻¹) for learned pseudo-counts.
  - [section 5.3] GPT-3 achieves 99% of theoretical entropy limits within 20 examples.
  - [corpus] Related work (ArXiv:2512.24780) describes neural networks exhibiting Bayesian uncertainty tracking.

## Foundational Learning

- Concept: Martingale Property
  - Why needed here: It is the core theoretical property that transformers are shown to violate, central to the paper's "paradox."
  - Quick check question: If a sequence of random variables is a martingale, what is E[Xn+1 | X1,...,Xn]?

- Concept: Kolmogorov Complexity
  - Why needed here: It is the theoretical tool used to formalize the difference between learning on exchangeable vs. position-conditioned data.
  - Quick check question: What does K(X) represent and why is it generally uncomputable?

- Concept: Sufficient Statistic
  - Why needed here: The paper argues transformers operate in the space of sufficient statistics, which is key to their compression efficiency.
  - Quick check question: For a sequence of coin flips, what is a sufficient statistic for estimating the probability of heads?

## Architecture Onboarding

- Component map: Input -> [Positional Encoding + Embedding] -> [Attention: Counting Head (computes sufficient stats)] -> [MLP: Moment Approximation (estimates posterior)] -> [Softmax: Predictive Distribution]
- Critical path: The martingale violation originates from the Positional Encoder; the Bayesian behavior emerges from the Attention-MLP pathway's ability to process sufficient statistics.
- Design tradeoffs: Position-awareness (required for sequence tasks) vs. exchangeability (required for strict Bayesian updating). High capacity enables implicit inference but increases computational cost.
- Failure signatures: (1) Increasing martingale gap with n suggests flawed positional encoding. (2) Poor compression efficiency (far from entropy limits) indicates counting mechanisms did not form.
- First 3 experiments:
  1. Martingale Gap Scaling: Measure Δn across n ∈ {10,...,200} on balanced binary sequences. Fit log n/n vs 1/n models. Expect R² > 0.7 for log n/n.
  2. Permutation Averaging: For a fixed prompt, average predictions over k ∈ {1,...,50} random shuffles. Plot variance vs k. Expect σ ∝ k⁻⁰·⁵ scaling.
  3. Compression Efficiency: Measure empirical cross-entropy on Bernoulli sequences and compare to theoretical limit nH(p). Expect convergence to >95% efficiency within n=20.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the theoretically derived optimal chain-of-thought length $k^* = \Theta(\sqrt{n} \log(1/\varepsilon))$ empirically generalize to natural language tasks?
- **Basis in paper:** [explicit] The authors explicitly state that "empirical validation of the optimal chain-of-thought bounds... is deferred to follow-up work" due to the extensive computational resources required.
- **Why unresolved:** The formula is derived mathematically and tested only implicitly via binary sequence experiments; it lacks direct validation on production-scale language tasks where CoT is most critical.
- **What evidence would resolve it:** Experiments measuring performance retention and cost savings on complex reasoning benchmarks (e.g., GSM8K) when applying the derived $k^*$ formula compared to unbounded chains.

### Open Question 2
- **Question:** How do the $\Theta(\log n/n)$ martingale violation scaling laws change when applied to natural language data with hierarchical, non-exchangeable dependencies?
- **Basis in paper:** [explicit] The authors note their experiments focused on binary sequences to isolate effects, but "natural language exhibits complex dependencies that may modulate the scaling."
- **Why unresolved:** Linguistic structure (syntax, semantics) introduces correlations that violate the simple exchangeability assumptions used to derive the theoretical bounds in Section 3.
- **What evidence would resolve it:** Replicating the martingale violation analysis on natural text corpora (e.g., Wikitext) to determine if the $\log n/n$ scaling holds or if new terms are required.

### Open Question 3
- **Question:** Does increasing model scale mitigate or amplify the positional encoding-induced martingale violations?
- **Basis in paper:** [explicit] Section 6.3 asks whether "larger models may better approximate exchangeable behavior... or they may exhibit stronger positional biases due to their ability to memorize fine-grained patterns."
- **Why unresolved:** It is theoretically ambiguous whether the increased capacity of larger models allows them to "average out" positional biases or simply overfit to them.
- **What evidence would resolve it:** Scaling law experiments measuring the martingale gap $\Delta_n$ across a family of models (e.g., 1B to 70B parameters) trained on identical data distributions.

## Limitations

- Empirical validation is limited to GPT-3 on synthetic binary sequences, which may not generalize to other domains or model families
- Theoretical framework assumes exchangeability of input data, but real-world pretraining data often exhibits strong non-exchangeability
- Implicit Bayesian representation mechanism relies on sufficient statistics formation, which may not occur in models trained on distributions that don't require counting-based inference

## Confidence

**High Confidence**: The core theoretical framework connecting positional encodings to martingale violations is mathematically rigorous and well-supported by formal bounds and empirical scaling analysis.

**Medium Confidence**: The implicit posterior representation mechanism is plausible given empirical compression results, but the specific architectural requirements for forming counting heads are not fully characterized.

**Low Confidence**: The practical implications for uncertainty quantification and chain-of-thought optimization require extensive validation on real-world tasks beyond synthetic sequences.

## Next Checks

1. **Architecture Generalization**: Test martingale gap scaling on multiple transformer variants (GPT-2, LLaMA, Mistral) across diverse data types (text, code, tabular). Verify that Θ(log n/n) scaling holds when positional encoding schemes are varied (learned vs sinusoidal) or removed entirely.

2. **Real-World Data Exchangeability**: Apply the permutation averaging framework to naturally occurring text sequences with known hierarchical structure (e.g., news articles, technical documentation). Measure whether variance reduction still follows k^(-1/2) scaling when data violates exchangeability assumptions.

3. **Cross-Domain Compression Efficiency**: Evaluate compression performance on non-binary, structured prediction tasks (e.g., machine translation, code completion) to verify that transformers consistently achieve near-optimal description length across diverse domains, not just synthetic Bernoulli sequences.