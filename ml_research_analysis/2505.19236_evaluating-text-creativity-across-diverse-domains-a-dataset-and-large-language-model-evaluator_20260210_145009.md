---
ver: rpa2
title: 'Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language
  Model Evaluator'
arxiv_id: '2505.19236'
source_url: https://arxiv.org/abs/2505.19236
tags:
- creativity
- creative
- responses
- data
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating textual creativity
  in large language models across diverse domains. The authors propose a novel pairwise-comparison
  framework that leverages shared contextual instructions to improve evaluation consistency.
---

# Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator

## Quick Facts
- arXiv ID: 2505.19236
- Source URL: https://arxiv.org/abs/2505.19236
- Reference count: 40
- One-line primary result: Novel pairwise-comparison framework and CreataSet dataset enable CrEval to outperform GPT-4o by 18.7% in agreement with human creativity judgments

## Executive Summary
This paper addresses the challenge of evaluating textual creativity in large language models across diverse domains. The authors propose a novel pairwise-comparison framework that leverages shared contextual instructions to improve evaluation consistency. They introduce CreataSet, a large-scale dataset with over 100K human-level and 1M+ synthetic creative instruction-response pairs spanning 87 domains. Using this dataset, they train CrEval, an LLM-based creativity evaluator that demonstrates remarkable superiority over existing methods, achieving 18.7% higher agreement with human judgments compared to GPT-4o. The work highlights the importance of integrating both human and synthetic data for training robust evaluators and showcases CrEval's practical utility in enhancing LLM creativity.

## Method Summary
The authors construct CreataSet by collecting diverse creative text sources (humor, poetry, instruction-response pairs), generating synthetic responses using multiple LLMs with ordinary and creative prompts, and applying weak supervision to create pairwise comparisons. They train CrEval by fine-tuning Qwen2.5-7B/14B-Instruct using LoRA on the pairwise dataset, optimizing for classification of which response is more creative. The evaluation uses human-annotated test sets with pairwise comparisons to measure agreement metrics.

## Key Results
- CrEval achieves 18.7% higher agreement with human judgments compared to GPT-4o
- CrEval-14B reaches F1 score of 0.735 vs GPT-4o's 0.703 on human-labeled test set
- Human inter-rater agreement improves from ICC=0.59 to ICC=0.75 with shared contextual instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-aware pairwise comparison improves evaluation consistency by anchoring creativity judgments to a shared instruction.
- Mechanism: Instead of rating creativity in isolation, the framework presents two responses to the same instruction, constraining evaluation to relative differences within fixed context and reducing subjective variance.
- Core assumption: Annotators can more reliably agree on which of two responses is more creative relative to the same prompt than on an absolute creativity score for decontextualized text.
- Evidence anchors: Human agreement improved from ICC=0.59 to 0.75 when shared instruction was provided.
- Break condition: May fail if creativity is inherently non-comparative or if provided instruction fails to constrain creative space meaningfully.

### Mechanism 2
- Claim: Training a specialized LLM evaluator on a large-scale, weakly-supervised dataset allows it to surpass general-purpose frontier models in aligning with human creativity judgments.
- Mechanism: CrEval is fine-tuned on over 1 million synthetic and human-labeled examples specifically formatted for pairwise creativity comparison, teaching it to recognize subtle creativity signals that general-purpose models may miss.
- Core assumption: A model trained on creativity-specific pairwise examples develops more robust internal representation of creative attributes than general-purpose models.
- Evidence anchors: CrEval achieves 18.7% higher agreement with human judgments compared to GPT-4o; CrEval-14B achieves F1 of 0.735 vs GPT-4o's 0.703.
- Break condition: If weak supervision contains systematic biases, evaluator could learn flawed heuristics failing on truly novel creative forms.

### Mechanism 3
- Claim: Integrating human-generated data with synthetically generated data creates a more robust evaluator than using either alone.
- Mechanism: Human data provides gold standard for peak creativity and helps align model with human values, while synthetic data teaches model to distinguish creativity levels across vast array of styles and domains.
- Core assumption: Synthetic data contains learnable signal that correlates with creativity gradient and complements sparse human signal.
- Evidence anchors: Ablation study shows removing synthetic data or using only synthetic data degrades performance compared to full model.
- Break condition: If synthetic data is too low-quality or systematically biased, it could overwhelm or distort learning signal from human data.

## Foundational Learning

- Concept: **Pairwise Comparison for Subjective Tasks**
  - Why needed here: Creativity is subjective; absolute scoring leads to high inter-annotator disagreement. Forced-choice simplifies task and improves consistency.
  - Quick check question: If you ask three people to rate a joke's funniness from 1-10, will you get the same three numbers? If you ask which of two jokes is funnier, will you get more agreement?

- Concept: **Weak Supervision & Pseudo-labeling**
  - Why needed here: Annotating 1 million creative pairs is prohibitively expensive. Paper uses assumptions (stronger models/prompting = more creative) to automatically generate labels.
  - Quick check question: Can you use a heuristic or trusted proxy to generate millions of labels that are "good enough" to train a smaller, specialized model?

- Concept: **LLM-as-a-Judge & Specialization**
  - Why needed here: General-purpose LLMs are versatile but not necessarily optimal for specific subjective evaluation tasks. Specialized model can outperform much larger general-purpose model.
  - Quick check question: Why might a general practitioner be worse at diagnosing a rare skin condition than a specialist dermatologist, even if the GP has more total medical knowledge?

## Architecture Onboarding

- Component map: Data sources (humor, poetry, instruction-response pairs) -> Instruction Generator (trained on reversed pairs) -> Response Augmenter (multiple LLMs with ordinary/creative prompts) -> Labeling Engine (weak supervision heuristics) -> CrEval (Qwen2.5 backbone fine-tuned with LoRA)

- Critical path: Quality of pseudo-labeling in data construction pipeline. Assumption that stronger model always generates more creative responses is heuristic that, if flawed, will propagate bias through entire system. Verifying this assumption's validity is key step.

- Design tradeoffs:
  - Scale vs. Label Quality: Massive scale of weakly supervised data (1M+ pairs) vs. perfect accuracy of human labels. Authors chose scale, hypothesizing quantity compensates for label noise.
  - Pairwise vs. Absolute Scoring: Pairwise better for consistency and aligns with preference learning, but makes output less interpretable as single "creativity score."

- Failure signatures:
  - Mode Collapse: Evaluator might learn superficial features of generating models instead of genuine creative attributes.
  - Domain Brittleness: Despite 87 domains, may fail on truly novel domains or styles not well-represented in initial dataset sources.
  - Over-optimization: Models trained with this evaluator could learn to produce "creativity-slop" that scores high on metric but feels artificial to humans.

- First 3 experiments:
  1. Baseline Validation: Re-run pairwise comparison using ablated model trained only on synthetic or only on human data to quantify individual contribution of each data source.
  2. Positional Bias Test: Run full test set twice (I, R1, R2) and (I, R2, R1), measure consistency rate. Significant drop would indicate model hasn't fully mitigated positional bias.
  3. Downstream DPO Integration: Use CrEval to filter and rank separate dataset, then train LLM using DPO with CrEval-curated data. Evaluate resulting model's perceived creativity via human evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does using CrEval as a dense reward signal in Proximal Policy Optimization (PPO) compare to Direct Preference Optimization (DPO) for enhancing LLM creativity?
- Basis in paper: The authors state in limitations that there are more ways of using CrEval as a plug-in, e.g., using it as a reward model and refining LLMs via PPO, leaving this for future work.
- Why unresolved: Paper only demonstrates utility through DPO, leaving efficacy of online reinforcement learning approaches untested.
- What evidence would resolve it: Comparative study training LLMs with CrEval-based PPO versus DPO, measuring creativity gains and training stability.

### Open Question 2
- Question: What specific linguistic mechanisms does CrEval internalize to distinguish creativity, and do these align with human cognitive processes?
- Basis in paper: Limitations acknowledge that understanding and analyzing the core of text creativity remains a challenge and that true mechanisms behind this process remain unknown.
- Why unresolved: While paper identifies attributes like "unique imagery" post-hoc, it doesn't confirm if model is learning faithful proxy of human cognitive creativity or exploiting superficial statistical patterns.
- What evidence would resolve it: Mechanistic interpretability analysis on CrEval's hidden states to map learned representations to established linguistic creativity theories.

### Open Question 3
- Question: Does aggressive optimization for CrEval scores lead to "algorithmic stereotyping" that stifles unconventional forms of human creativity?
- Basis in paper: Societal impact statement warns of risk of over-optimization for machine-friendly metrics, potentially stifling genuine human creativity.
- Why unresolved: Paper demonstrates alignment with current human judgments but doesn't evaluate long-term feedback loops where models might converge on narrow definition of creativity defined by evaluator.
- What evidence would resolve it: Analyzing diversity of "creative" outputs generated by models fine-tuned solely to maximize CrEval scores over multiple iterations.

### Open Question 4
- Question: Can the CrEval framework, trained predominantly on Simplified Chinese data, generalize to low-resource languages without significant performance degradation?
- Basis in paper: Paper notes dataset is "predominantly in Simplified Chinese" and claims framework is "language-agnostic," but provides no cross-lingual experimental evidence.
- Why unresolved: Creativity is often culturally and linguistically bound; unclear if evaluator captures universal concept of creativity or relies on language-specific features.
- What evidence would resolve it: Zero-shot or few-shot evaluation of CrEval on human-labeled creative pairs in typologically distinct languages.

## Limitations
- Generalizability Across Domains: Despite 87 domains, initial data sources may create domain-specific biases; pairwise framework assumes creativity is meaningfully comparable within shared contexts.
- Weak Supervision Quality: Labeling mechanism relies on heuristics (stronger model = more creative) without rigorous validation of these assumptions; systematic biases could emerge.
- Positional Bias Mitigation: Despite swap augmentation during training, model may retain positional preferences; paper reports consistency metrics but lacks detailed ablation studies.

## Confidence

**High Confidence**: Core methodology of pairwise comparison for subjective tasks is well-established and reported improvements in human agreement are directly measured. Architectural approach and training procedure are clearly specified and reproducible.

**Medium Confidence**: Claim that CrEval outperforms GPT-4o by 18.7% in agreement with human judgments is supported by test set results, but evaluation is circular (CrEval trained on data partially derived from GPT-4o-mini's judgments). Ablation studies are suggestive but not definitive without testing on truly held-out domains.

**Low Confidence**: Assertion that CrEval can meaningfully enhance LLM creativity through DPO training is presented as brief application note without systematic validation. Societal impact discussion acknowledges risks but doesn't provide empirical evidence of failure modes or mitigation strategies.

## Next Checks

1. **Domain Transfer Test**: Evaluate CrEval on held-out set of domains completely absent from CreataSet (technical patent descriptions, culinary innovation) to measure true generalization capability and identify domain-specific failure patterns.

2. **Human Preference Validation**: Conduct preference study where human annotators rank responses from models trained with and without CrEval guidance to validate whether CrEval's judgments align with human perceptions of genuine creativity.

3. **Bias Characterization Study**: Systematically analyze CrEval's predictions across different response lengths, writing styles, and cultural contexts to identify and quantify potential systematic biases introduced through weak supervision process.