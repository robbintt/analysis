---
ver: rpa2
title: On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made
  by AI Systems
arxiv_id: '2509.25662'
source_url: https://arxiv.org/abs/2509.25662
tags:
- decision
- proxy
- abductive
- explanations
- discrimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of explaining proxy discrimination\
  \ and unfairness in AI decision-making systems, particularly when bias is hidden\
  \ through structural pathways rather than explicit protected attributes. The authors\
  \ propose a formal abductive reasoning framework that leverages background knowledge\
  \ to identify proxy variables\u2014features that act as stand-ins for protected\
  \ attributes."
---

# On Explaining Proxy Discrimination and Unfairness in Individual Decisions Made by AI Systems

## Quick Facts
- arXiv ID: 2509.25662
- Source URL: https://arxiv.org/abs/2509.25662
- Authors: Belona Sonna; Alban Grastien
- Reference count: 40
- This paper addresses the challenge of explaining proxy discrimination and unfairness in AI decision-making systems, particularly when bias is hidden through structural pathways rather than explicit protected attributes.

## Executive Summary
This paper introduces a formal abductive reasoning framework to explain proxy discrimination and unfairness in AI decision-making systems. The authors address the challenge of hidden bias that occurs when protected attributes are not explicitly used but their effects are encoded through proxy variables in the data. By introducing the concept of "aptitude" as a task-relevant property independent of group membership, the framework provides interpretable explanations for individual decisions. The approach leverages background knowledge to identify proxy variables and assess fairness by comparing treatment of individuals with equivalent aptitudes across groups. The German credit dataset serves as a validation case, demonstrating the framework's ability to uncover structural biases in high-stakes AI systems.

## Method Summary
The proposed framework combines abductive reasoning with background knowledge to identify and explain proxy discrimination in AI systems. The core innovation is the introduction of "aptitude" - a task-relevant property that is independent of group membership. The framework defines a mapping function that aligns equivalent aptitudes across different groups and establishes criteria for assessing fairness. When an individual receives an unfair decision, the system uses background knowledge to explain why this occurred by identifying proxy variables and structural pathways that led to the biased outcome. The approach is implemented using probabilistic reasoning over the background knowledge base, allowing the system to generate human-understandable explanations for why certain individuals were treated unfairly despite having equivalent aptitudes to those who received fair treatment.

## Key Results
- The framework successfully identifies proxy discrimination in individual decisions where protected attributes are not explicitly used but their effects are encoded through proxy variables.
- Using the German credit dataset, the method demonstrates its ability to uncover structural biases and provide interpretable explanations for unfair decisions.
- The concept of "aptitude" provides a novel way to assess fairness that is independent of group membership while remaining task-relevant.

## Why This Works (Mechanism)
The framework works by formalizing fairness assessment through abductive reasoning over background knowledge. By defining aptitude as a group-independent task-relevant property, the system can compare individuals across protected groups on an equal footing. The mapping function that aligns equivalent aptitudes across groups serves as the foundation for fairness criteria - if two individuals have the same aptitude but receive different treatments, this indicates unfairness. The abductive reasoning component then works backward from unfair outcomes to identify the proxy variables and structural pathways that caused the discrimination. This approach bridges the gap between technical fairness metrics and actionable explanations by providing interpretable reasoning chains that explain why specific individuals were treated unfairly.

## Foundational Learning
- Abductive reasoning: Why needed - to explain unfair outcomes by working backward from effects to causes; Quick check - can the system generate plausible explanations for observed unfair decisions?
- Aptitude concept: Why needed - to provide a group-independent basis for fairness comparison; Quick check - does the aptitude measure align with domain experts' understanding of task relevance?
- Background knowledge representation: Why needed - to encode structural relationships between variables and protected attributes; Quick check - can the knowledge base capture the key causal pathways in the domain?
- Group-conditional probability estimation: Why needed - to assess how different groups are treated given equivalent aptitudes; Quick check - are probability estimates stable across different data samples?
- Fairness criteria definition: Why needed - to establish objective standards for detecting unfairness; Quick check - do the criteria align with established fairness definitions in the literature?

## Architecture Onboarding

**Component Map:**
Background Knowledge Base -> Abductive Reasoner -> Aptitude Mapping Function -> Fairness Evaluator -> Explanation Generator

**Critical Path:**
1. Background knowledge is encoded and loaded
2. Individual decision and protected attributes are input
3. Aptitude is calculated for the individual
4. Fairness criteria are applied using group-conditional probabilities
5. If unfairness is detected, abductive reasoning identifies proxy variables
6. Explanation is generated for the unfair outcome

**Design Tradeoffs:**
The framework prioritizes interpretability over computational efficiency, as generating human-understandable explanations requires more complex reasoning than simple statistical checks. The use of background knowledge enables more accurate detection of proxy discrimination but requires domain expertise to construct and maintain. The aptitude concept provides a principled basis for fairness assessment but may be challenging to define precisely in some domains.

**Failure Signatures:**
- Inability to detect proxy discrimination when background knowledge is incomplete or inaccurate
- False positives when aptitude calculation is noisy or when group-conditional probabilities are poorly estimated
- Scalability issues when background knowledge becomes too complex for efficient abductive reasoning
- Explanations that are too technical or abstract for non-expert stakeholders to understand

**3 First Experiments:**
1. Apply the framework to a synthetic dataset where ground truth proxy discrimination is known to verify detection accuracy
2. Test the framework on a multi-class classification problem to assess its performance beyond binary decisions
3. Evaluate explanation quality through a user study with domain experts and non-experts to assess interpretability and actionability

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability of abductive reasoning with complex background knowledge remains untested beyond the German credit case
- Reliance on accurate group-conditional probability estimates may be challenging in real-world, high-dimensional settings
- The assumption that aptitude can be precisely mapped across groups may not hold in all fairness contexts
- The framework does not address potential noise or bias in background knowledge itself

## Confidence
- Conceptual clarity and methodological rigor: High
- Framework's ability to explain individual unfairness decisions: Medium (well-supported in studied case, but broader applicability requires validation)
- Scalability to complex real-world scenarios: Low (not extensively tested)
- Robustness to noisy or incomplete background knowledge: Low (not addressed in the paper)

## Next Checks
1. Apply the framework to a high-dimensional dataset (e.g., healthcare or hiring) to test scalability and robustness
2. Conduct a user study to evaluate the interpretability and actionability of explanations across diverse stakeholder groups
3. Simulate adversarial settings where proxy discrimination is intentionally masked to assess the framework's detection limits