---
ver: rpa2
title: 'DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context
  Reasoning Capabilities'
arxiv_id: '2502.17807'
source_url: https://arxiv.org/abs/2502.17807
tags:
- reasoning
- answer
- evaluation
- arxiv
- long-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocPuzzle, a new benchmark for evaluating
  long-context reasoning capabilities in large language models. The benchmark consists
  of 100 expert-level question-answering problems that require multi-step reasoning
  over long real-world documents from diverse domains.
---

# DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic Long-Context Reasoning Capabilities

## Quick Facts
- arXiv ID: 2502.17807
- Source URL: https://arxiv.org/abs/2502.17807
- Reference count: 6
- Advanced slow-thinking models (o1-preview, DeepSeek-R1) outperform instruct models (Claude 3.5 Sonnet) by 12-25% on expert-level long-context reasoning tasks

## Executive Summary
DocPuzzle introduces a new benchmark for evaluating long-context reasoning capabilities in large language models. The benchmark consists of 100 expert-level question-answering problems requiring multi-step reasoning over long real-world documents from diverse domains. The authors employ a human-AI collaborative annotation-validation pipeline to ensure quality and complexity. A process-aware evaluation framework uses checklists to assess reasoning steps rather than just final answers, reducing guessing bias. Evaluation shows that advanced slow-thinking reasoning models like o1-preview (69.7%) and DeepSeek-R1 (66.3%) significantly outperform general instruct models like Claude 3.5 Sonnet (57.7%). Distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B (41.3%) fall far behind their teacher models, indicating challenges in maintaining reasoning generalization through distillation alone.

## Method Summary
DocPuzzle consists of 100 expert-level Chinese question-answering problems requiring multi-step reasoning over long documents (median 10,641 tokens) across five domains: literature, news, policy, financial reports, and scientific papers. Each sample includes a document, question, answer, checklist, and reasoning chain. Questions must involve at least two reasoning operations from categories including temporal reasoning, arithmetic reasoning, bridging, comparative analysis, and causal inference. The benchmark employs a human-AI collaborative annotation pipeline with iterative validation and debate-based consensus. Evaluation uses a judge model (GPT-4o) to assess both answer accuracy and checklist compliance through majority voting across three prompt variants, requiring both criteria to pass.

## Key Results
- Advanced slow-thinking models (o1-preview at 69.7%, DeepSeek-R1 at 66.3%) significantly outperform general instruct models (Claude 3.5 Sonnet at 57.7%)
- Distilled reasoning models (DeepSeek-R1-Distill-Qwen-32B at 41.3%) show 25-point performance drop versus teacher models
- Chain-of-Thought prompting helps Claude 3.5 Sonnet (+9.4%) but harms smaller models (Qwen2.5-7B -3%, moonshot-v1 -3.2%)
- Arithmetic reasoning deficits identified as key failure mode, with models often using heuristic estimation rather than logical derivation

## Why This Works (Mechanism)

### Mechanism 1: Checklist-Guided Process Evaluation
- Claim: Process-aware evaluation reduces guessing bias by validating intermediate reasoning steps rather than final answers alone
- Mechanism: Each sample includes a checklist specifying mandatory reasoning milestones. A judge model (GPT-4o) evaluates both answer accuracy and checklist compliance via majority voting across three prompt variants, requiring both criteria to pass
- Core assumption: Judge models can reliably assess reasoning validity without being fooled by plausible-but-wrong chains; checklist items capture necessary intermediate conclusions
- Evidence anchors: [abstract] "mitigates guessing bias through checklist-guided process analysis"; [section 3.2] "we take not only the final answer but also the reasoning chain into account by utilizing a checklist for each sample"

### Mechanism 2: Multi-Operation Reasoning Complexity Constraints
- Claim: Enforcing minimum reasoning complexity (≥2 operations from defined categories) ensures tasks require genuine slow-thinking rather than single-hop retrieval
- Mechanism: Questions must combine operations from: temporal reasoning, arithmetic reasoning, bridging, comparative analysis, causal inference. Annotators explicitly design cognitive traps and implicit reasoning paths without shortcuts. Single-fragment retrieval is prohibited
- Core assumption: Models that excel at shallow retrieval will fail when forced to chain multiple reasoning types over scattered document evidence
- Evidence anchors: [section 3.1] "Each question must involve at least two reasoning operations from the following categories"; [Table 1] Example shows 9-step reasoning chain integrating document retrieval, arithmetic, and temporal inference

### Mechanism 3: Iterative Human-AI Validation with Controversy Resolution
- Claim: Multi-stage validation with debate-based consensus reduces annotation ambiguity and eliminates low-discrimination samples
- Mechanism: Stage 1 filters samples where all baseline LLMs succeed; Stage 2 requires independent reviewer validation with debate for inconsistencies. Persistent disagreements lead to sample rejection. Only consensus samples survive
- Core assumption: Expert debate converges on objective ground truth; samples with persistent controversy are flawed rather than legitimately ambiguous
- Evidence anchors: [section 3.1] "Contested items require consensus through debate. Samples with persistent disagreements are discarded"; [Figure 1] Five-stage pipeline: LLM answering → annotation/revision → automatic evaluation → review/rebuttal → final version

## Foundational Learning

- **Slow-thinking vs. fast-thinking models**:
  - Why needed here: Results distinguish between models optimized for deliberative reasoning (o1-preview, DeepSeek-R1) versus standard instruction-tuned models; understanding this taxonomy is essential for interpreting the 12-25% performance gaps
  - Quick check question: Can you explain why o1-preview (69.7%) outperforms Claude 3.5 Sonnet (57.7%) despite both being frontier models?

- **Knowledge distillation limitations for reasoning**:
  - Why needed here: DeepSeek-R1-Distill-Qwen-32B achieves 41.3% versus teacher's 66.3%—a 25-point gap suggesting distillation transfers surface patterns but not generalizable reasoning; critical for understanding model selection tradeoffs
  - Quick check question: Why does supervised fine-tuning on distilled reasoning traces fail to transfer long-context reasoning capabilities?

- **Chain-of-Thought prompting thresholds**:
  - Why needed here: CoT helps Claude 3.5 Sonnet (+9.4%) but harms Qwen2.5-7B (-3%) and moonshot-v1 (-3.2%); models need minimum baseline capacity (~32.7%) to benefit from reasoning path prompting
  - Quick check question: What performance threshold must a model exceed before CoT prompting provides positive returns?

## Architecture Onboarding

- **Component map**: Document collection and aggregation → Question design with ≥2 reasoning operations → Checklist creation defining mandatory intermediate conclusions → LLM baseline filtering (discard if all solve) → Independent human review with debate → Judge-based process evaluation → Score aggregation

- **Critical path**: 1. Document collection and aggregation → 2. Question design with ≥2 reasoning operations → 3. Checklist creation defining mandatory intermediate conclusions → 4. LLM baseline filtering (discard if all solve) → 5. Independent human review with debate → 6. Judge-based process evaluation → 7. Score aggregation

- **Design tradeoffs**: Free-form answers vs. multiple-choice (chose free-form to eliminate random guessing bias but requires judge model evaluation); Process evaluation vs. answer-only (process-aware increases evaluation complexity but captures reasoning validity; allows numerical tolerance); Human-AI collaboration vs. pure human (AI accelerates filtering but may miss nuanced errors; debate stage compensates)

- **Failure signatures**: Arithmetic reasoning (models fail to extract formulas from tabular data, use heuristic estimation instead); Common sense gaps (models don't recognize "gross floor area > usable area," leading to incorrect threshold judgments); Distillation collapse (distilled models match teacher on math/code but drop 25+ points on long-context reasoning)

- **First 3 experiments**:
  1. Baseline evaluation sweep: Run o1-preview, DeepSeek-R1, Claude 3.5 Sonnet, and at least one distilled model on all 100 samples with default settings; compare accuracy and response length correlations
  2. CoT prompting ablation: Test "Let's think step-by-step" on instruct models across size spectrum (7B to 72B); identify threshold where CoT transitions from harmful to beneficial
  3. Pass@k exploration analysis: Generate 3 responses per model with controlled sampling (temperature=0.3, top_p=0.2); compute pass@3 to measure exploration capability independent of single-shot accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) or self-improvement methods effectively bridge the generalization gap in long-context reasoning where supervised finetuning (SFT) via distillation fails?
- Basis in paper: [explicit] Section 5.2 concludes that the poor performance of distilled models "challenges the universal applicability of knowledge distillation techniques and calls for other techniques, such as reinforcement learning and self-improvement, to improve reasoning generalization"
- Why unresolved: The paper empirically demonstrates the failure of distillation for this specific domain but does not experimentally verify if RL succeeds in transferring these complex reasoning capabilities
- What evidence would resolve it: A comparative evaluation showing RL-trained models achieving significantly higher accuracy on DocPuzzle compared to their distilled counterparts

### Open Question 2
- Question: How can model architectures be improved to reliably perform arithmetic reasoning and formula extraction from tabular data within long contexts?
- Basis in paper: [explicit] Section 5.4 (Error Analysis) identifies "Arithmetic Reasoning Deficits," noting that state-of-the-art models like o1-preview "fail to extract the price relationship formula from tabular data, instead generating plausible but incorrect responses"
- Why unresolved: The authors identify this as a specific failure mode where models rely on heuristic estimation rather than logical derivation, but they do not propose a solution
- What evidence would resolve it: Ablation studies or new model variants that demonstrate superior performance on the arithmetic subsets of the benchmark without hallucinating formulas

### Open Question 3
- Question: Why does exploration potential (measured by pass@3) show no significant correlation with pairwise accuracy in long-context reasoning tasks?
- Basis in paper: [explicit] Section 5.3 states that the analysis "reveals no significant correlation between LLM exploration potential... and pairwise accuracy scores," suggesting that simply sampling more diverse reasoning paths may not help solve these specific puzzles
- Why unresolved: This counter-intuitive finding implies that current exploration strategies might not be suited for the structured, deterministic nature of document-based reasoning
- What evidence would resolve it: A study identifying whether the lack of correlation is due to temperature settings or a fundamental misalignment between exploration metrics and context-dependent logical chaining

## Limitations
- Dataset accessibility prevents independent verification of sample quality and annotation consistency
- Judge model reliability for process-aware evaluation lacks direct validation against human expert consensus
- Small sample size (100 examples) may not capture full distribution of long-context reasoning failure modes
- Cross-linguistic limitations restrict applicability to Chinese-language documents and reasoning patterns

## Confidence
- High: Core benchmark construction methodology and performance comparisons between model families
- Medium: Specific mechanisms of checklist-guided evaluation given limited direct validation of judge model reliability
- Low: Generalization claims due to small sample size and lack of cross-linguistic validation

## Next Checks
1. **Judge model reliability validation**: Conduct human expert review of 20 randomly selected samples to assess agreement rates between GPT-4o process evaluation and human checklist compliance judgments
2. **Distillation transfer analysis**: Compare performance across reasoning operation types (temporal, arithmetic, bridging, comparative, causal) to identify which specific reasoning capabilities are lost during distillation
3. **Sample size sensitivity analysis**: Perform statistical power analysis to determine minimum sample size needed to detect 5% performance differences with 95% confidence across model families