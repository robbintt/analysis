---
ver: rpa2
title: Normative Evaluation of Large Language Models with Everyday Moral Dilemmas
arxiv_id: '2501.18081'
source_url: https://arxiv.org/abs/2501.18081
tags:
- moral
- llms
- arxiv
- each
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated seven large language models (LLMs) on over
  10,000 real-world moral dilemmas from Reddit's AITA community, comparing their moral
  judgments and reasoning to human responses. LLMs showed significant variation in
  their verdict distributions and low inter-model agreement, though individual models
  demonstrated moderate to high self-consistency.
---

# Normative Evaluation of Large Language Models with Everyday Moral Dilemmas

## Quick Facts
- arXiv ID: 2501.18081
- Source URL: https://arxiv.org/abs/2501.18081
- Reference count: 40
- The study evaluated seven large language models on over 10,000 real-world moral dilemmas from Reddit's AITA community, finding significant variation in moral judgments and low inter-model agreement despite individual model consistency.

## Executive Summary
This study evaluates seven large language models (LLMs) on over 10,000 real-world moral dilemmas from Reddit's AITA community, comparing their moral judgments and reasoning to human responses. The research finds that while individual models show distinct moral biases and low agreement with each other, an ensemble of models collectively approximates human consensus. The analysis reveals that models invoke moral principles differently, with some showing greater sensitivity to specific themes like fairness or harm. These findings highlight the challenges of implementing consistent ethical judgment in AI systems and the importance of careful evaluation as LLMs are increasingly used in roles requiring ethical decision-making.

## Method Summary
The researchers collected 10,826 AITA posts from Reddit (October 2022-March 2023) with scores â‰¥25, filtering for quality and removing duplicates. They prompted seven LLMs (GPT-3.5, GPT-4, Claude Haiku, PaLM 2 Bison, Llama 2, Mistral, Gemma) with system messages defining verdict labels and post text, running three replicates per submission. Model outputs were processed using regex to extract verdicts and RoBERTa-Large classifiers (fine-tuned on Yudkin et al.'s dataset) to identify six moral themes in reasoning. The study employed Krippendorff's alpha for agreement metrics, TF-IDF cosine similarity for semantic analysis, and prevalence difference calculations to examine theme-verdict correlations.

## Key Results
- Individual LLMs showed significant variation in verdict distributions, with some models exhibiting strong biases (Llama 2 biased toward YTA, Mistral toward NAH/ESH)
- Ensemble voting across models collectively approximated human consensus despite individual inconsistencies
- Analysis of model explanations revealed distinct patterns in invoking moral themes, with some models showing greater sensitivity to specific themes like fairness or harm
- Models demonstrated moderate to high self-consistency but low inter-model agreement, suggesting systematic differences in moral priors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating judgments from an ensemble of diverse LLMs may approximate human consensus better than individual models by averaging out idiosyncratic moral biases.
- **Mechanism:** Individual models exhibit systematic "moral priors" driven by their specific alignment objectives. When ensembled, these opposing systematic errors cancel out, revealing a signal closer to the human distribution.
- **Core assumption:** The errors in moral reasoning across different model families are sufficiently uncorrelated that they cancel out in aggregation, rather than reinforcing specific biases.
- **Evidence anchors:**
  - "An ensemble of LLMs collectively approximated human consensus despite individual inconsistencies."
  - "For the NTA and YTA labels, the average label rate increases as more models vote for the corresponding label... suggesting that LLM agreement correlates with Redditor consensus."
  - "The Pluralistic Moral Gap" supports the premise that significant judgment differences exist between models and humans, necessitating aggregation.
- **Break condition:** If model training data and alignment techniques converge, their errors will correlate, and the ensemble will fail to approximate human consensus, instead amplifying the shared bias.

### Mechanism 2
- **Claim:** LLMs weight moral principles differently when assigning blame, suggesting that "moral reasoning" in these systems is a function of specific feature sensitivity rather than a unified ethical framework.
- **Mechanism:** Models act as feature detectors for specific moral themes. If a model has learned a strong correlation between "Harm" and "YTA" during training, the presence of harm-related tokens disproportionately increases the probability of a blame verdict.
- **Core assumption:** The RoBERTa classifiers accurately capture the semantic concepts the LLMs are attending to, and the correlations observed are not spurious artifacts of linguistic styles.
- **Evidence anchors:**
  - "Analysis of model explanations revealed distinct patterns in invoking moral themes... distinct patterns in how models invoke various moral principles."
  - "Fairness had a significant impact on model verdict... GPT-4 used the NTA verdict 15% more often when it invoked Fairness... Feelings saw the largest prevalence differences."
  - "Are Language Models Consequentialist or Deontological Moral Reasoners?" investigates the underlying reasoning processes, relevant to the claim that models rely on specific feature sensitivities.
- **Break condition:** If the model's reasoning explanations are post-hoc rationalizations rather than faithful representations of internal processing, the correlation between "invoked themes" and "verdicts" may not reflect actual causal mechanisms.

### Mechanism 3
- **Claim:** Verdict distributions are heavily influenced by a model's "base rate" bias (priors) regarding the definition of the output labels, which can override scenario details.
- **Mechanism:** Models form a semantic understanding of output tokens based on training data. If a model interprets the "YTA" token as the default response for conflict or is over-aligned to identify fault, it assigns that label irrespective of nuance in the prompt.
- **Core assumption:** The observed verdict skews are not purely due to the input data but are artifacts of the model's internal alignment to the label definitions provided in the system prompt.
- **Evidence anchors:**
  - "The consistent labeling of posts as YTA by Llama symbolizes a broader issue... Llama 2's disproportionate labeling of users as YTA."
  - "Mistral placed a stronger emphasis on the term 'asshole' and whether it was truly an appropriate descriptor... This contrasts with the AITA community's implicit norm."
  - "From Stability to Inconsistency: A Study of Moral Preferences in LLMs" confirms that moral preferences in LLMs can be unstable or skewed, supporting the existence of internal biases.
- **Break condition:** If prompt engineering significantly shifts these base rates (e.g., explicitly defining "YTA" as a rare outcome in the system prompt eliminates the bias), then the mechanism is not a fixed model trait but a prompt sensitivity issue.

## Foundational Learning

- **Concept:** **Inter-Annotator Agreement (Krippendorff's Alpha)**
  - **Why needed here:** The paper relies on this metric to quantify "self-consistency" and "inter-model agreement." Without understanding that $\alpha < 0$ implies *systematic disagreement*, you cannot interpret Figure 2 or the claim that models have distinct "moral priors."
  - **Quick check question:** If Model A assigns "YTA" 90% of the time and Model B assigns "NTA" 90% of the time on the same dataset, would their Krippendorff's Alpha be positive, zero, or negative? (Answer: Likely negative, indicating systematic disagreement).

- **Concept:** **Moral Foundations Theory (Yudkin et al. categories)**
  - **Why needed here:** The paper moves beyond simple "good/bad" labels by classifying reasoning into six themes. Understanding this taxonomy is required to interpret the "Prevalence Difference" results, which show *why* models differ.
  - **Quick check question:** Which moral theme is most likely to trigger a "blame" (YTA) response across most models according to the paper's findings? (Answer: "Feelings" and "Harm" often showed negative prevalence differences for NTA, implying higher blame).

- **Concept:** **Ensemble Learning (Voting)**
  - **Why needed here:** A key finding is that an ensemble of models approximates human consensus. This relies on the principle that aggregating diverse classifiers reduces variance and specific biases.
  - **Quick check question:** Why does the ensemble approach fail if all models in the ensemble share the same training data bias? (Answer: The errors correlate, and the bias is amplified rather than canceled out).

## Architecture Onboarding

- **Component map:** Dataset (Reddit AITA) -> Preprocessing (filtering, regex extraction) -> Prompting (system + user messages) -> Generation (7 LLMs, 3 replicates) -> Analysis (RoBERTa theme classifiers, Krippendorff's alpha, ensemble voting)

- **Critical path:**
  1. **Preprocessing:** Filter submissions for score > 25 and ensure no training data leakage
  2. **Generation:** Prompt each model 3 times with low but non-zero temperature
  3. **Classification:** Run reasoning outputs through RoBERTa classifiers to tag moral themes
  4. **Correlation:** Calculate "Prevalence Difference" between theme invocation and verdicts

- **Design tradeoffs:**
  - **Prompt Adherence vs. Native Behavior:** Low temperature stabilizes results but may hide full variance of model "opinions"
  - **Classifier Accuracy:** Using RoBERTa for theme classification is scalable but assumes classifier aligns with human semantic understanding
  - **Ensemble Size:** Using 7 models is computationally expensive but necessary to prove the "diversity helps" hypothesis

- **Failure signatures:**
  - **Verdict Dumping:** Models ignoring input context and defaulting to "YTA" for >79% of posts
  - **Literal Interpretation:** Models interpreting "Asshole" literally rather than socially, leading to excessive "NAH" or "ESH" verdicts
  - **Refusal:** Models refusing to judge moral content (though noted as <0.1%)

- **First 3 experiments:**
  1. **Perspective Reversal:** Rewrite 50 submissions from the "Other Party's" perspective and rerun models to test for OP-bias vs. objective reasoning
  2. **System Prompt Ablation:** Remove explicit definitions of NTA/YTA from the system prompt for a subset of models to measure how much "alignment" comes from the prompt vs. pre-training
  3. **Threshold Sensitivity:** Vary the threshold for the RoBERTa theme classifier to test the robustness of the "Moral Reasoning" mechanism

## Open Questions the Paper Calls Out

- **Question:** How does re-framing moral dilemmas from the opposing party's perspective affect the stability and bias of LLM moral judgments?
  - **Basis in paper:** The authors note that the AITA framework is inherently asymmetric because the Original Poster (OP) controls the narrative. They suggest evaluating dilemmas from both perspectives could facilitate more robust examination.
  - **Why unresolved:** The current study only evaluated scenarios as written by the OP, creating an asymmetry where the privilege of posting might bias the ground truth. It is unknown if models would exhibit similar consistency or agreement if the narrative stance were switched.
  - **What evidence would resolve it:** A study where LLMs evaluate the same core dilemma rewritten from both the protagonist's and antagonist's perspectives to measure consistency and perspective-taking bias.

- **Question:** Does the observed alignment between LLMs and Reddit users reflect a general human moral consensus or merely an over-representation of WEIRD values?
  - **Basis in paper:** The authors explicitly caution that their findings showing agreement with Redditors may serve as further evidence of LLM alignment with Western, Educated, Industrialized, Rich, and Democratic (WEIRD) subpopulations rather than universal moral reasoning.
  - **Why unresolved:** The "human" ground truth was derived from Redditors, a demographic known to be predominantly young, male, and liberal. High alignment scores may indicate the model has simply learned these specific subcultural norms.
  - **What evidence would resolve it:** Comparative evaluation using moral dilemmas sourced from demographically diverse populations or cross-cultural moral benchmarks to see if alignment persists outside of Reddit demographics.

- **Question:** Do new, distinct moral norms emerge when LLMs are organized into deliberative ensembles that evaluate each other's assessments?
  - **Basis in paper:** The authors suggest exploring whether new norms emerge when LLMs evaluate each other's assessments of moral dilemmas.
  - **Why unresolved:** The paper found that while individual models have low inter-model agreement, an ensemble vote approximates human consensus. It is unclear if this is merely averaging errors or if the interaction itself generates a distinct, consistent moral framework.
  - **What evidence would resolve it:** Experiments involving multi-agent frameworks where LLMs critique and refine each other's moral reasoning, followed by analysis of whether the resulting "ensemble norm" differs systematically from any single model's logic.

## Limitations
- The study relies on model-generated explanations for moral reasoning analysis, which may not faithfully represent actual reasoning processes
- The prompt engineering significantly influences model behavior, with system prompt definitions potentially overriding scenario content
- The ensemble approach assumes diverse model architectures will produce uncorrelated errors, but the extent of this diversity is not empirically validated

## Confidence
- **High confidence:** Verdict distribution patterns (Llama 2 bias toward YTA, Mistral toward NAH/ESH) are directly observable from the data and show strong statistical significance
- **Medium confidence:** The correlation between moral theme invocation and verdict assignment, as this depends on the accuracy of the RoBERTa theme classifiers
- **Medium confidence:** The ensemble approach approximating human consensus, though this is supported by quantitative evidence showing increasing label rate with agreement

## Next Checks
1. Validate RoBERTa theme classifier accuracy by having human annotators code a subset of LLM explanations and comparing to classifier outputs to establish alignment rates
2. Test prompt sensitivity by systematically varying the system prompt definitions of verdict labels and measuring shifts in verdict distributions to quantify prompt influence vs. scenario content
3. Conduct cross-validation by splitting the AITA dataset by date and training/testing on temporal partitions to ensure models aren't simply memorizing common verdict patterns from their training data