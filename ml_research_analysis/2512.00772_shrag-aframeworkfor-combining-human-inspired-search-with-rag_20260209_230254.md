---
ver: rpa2
title: 'SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG'
arxiv_id: '2512.00772'
source_url: https://arxiv.org/abs/2512.00772
tags:
- search
- query
- retrieval
- queries
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHRAG is a framework that combines human-inspired search strategies
  with retrieval-augmented generation (RAG) to improve cross-lingual academic information
  retrieval. It uses a large language model as a query strategist to automatically
  generate structured search queries from natural language inputs, employing multilingual
  keyword extraction and Boolean retrieval.
---

# SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG

## Quick Facts
- arXiv ID: 2512.00772
- Source URL: https://arxiv.org/abs/2512.00772
- Reference count: 28
- Primary result: SHRAG combines human-inspired search strategies with RAG to improve cross-lingual academic information retrieval, achieving 94% success rate on MIRACL dataset

## Executive Summary
SHRAG is a framework that combines human-inspired search strategies with retrieval-augmented generation (RAG) to improve cross-lingual academic information retrieval. It uses a large language model as a query strategist to automatically generate structured search queries from natural language inputs, employing multilingual keyword extraction and Boolean retrieval. The framework integrates multilingual query expansion and embedding models to handle Korean-English mixed datasets effectively. Experiments show that using only OR operators in search queries yields higher document retrieval success rates compared to AND-based queries, with 94% success rate on the MIRACL dataset.

## Method Summary
SHRAG is a 5-stage pipeline that combines Boolean search with semantic re-ranking for cross-lingual academic information retrieval. The system extracts multilingual keywords from queries using an LLM, generates disjunctive (OR-only) Boolean queries with progressive keyword reduction, retrieves candidate documents via search engine, applies multilingual embedding-based re-ranking to select top-5 documents, and uses an LLM to generate structured answers from the retrieved context. The framework uses Gemini 2.5 Flash for query strategy and answer generation, and mGTE embeddings for semantic similarity computation.

## Key Results
- Using only OR operators in search queries yields higher document retrieval success rates (94%) compared to AND-based queries
- Achieved first place in the ScienceON AI Challenge by optimizing retrieval performance and evidence coverage
- Demonstrated effectiveness as a scalable, plug-and-play RAG solution for Korean-English mixed datasets

## Why This Works (Mechanism)

### Mechanism 1: Disjunctive Query Strategy for High Recall
- **Claim:** Using LLM-generated search queries composed exclusively of OR operators yields higher document retrieval success rates than AND-based queries in RAG pipelines.
- **Mechanism:** The LLM extracts keywords and constructs disjunctive queries ($k_1 \lor k_2 \lor ...$). This broadens the search surface area, prioritizing high recall. The system retrieves any document matching at least one key concept, mitigating the risk of "vocabulary mismatch" where specific terms in the query differ from the corpus.
- **Core assumption:** The target corpus contains the answer, and the initial "noisy" retrieval set is sufficiently small to be processed efficiently by the re-ranker without overwhelming the context window.
- **Evidence anchors:** [section 5.1.1] shows OR-only queries achieved 94% success rate vs. lower rates for AND-based strategies; [figure 3] demonstrates pure OR results in higher success rates and fewer total documents to process.

### Mechanism 2: Multilingual Keyword Bridging
- **Claim:** Generating keywords in both the query language and the target document language is necessary for effective cross-lingual retrieval.
- **Mechanism:** The framework utilizes an LLM to extract keywords in English and Korean simultaneously from a single query. By injecting these translated/transliterated keywords into the Boolean query, the system matches documents that use specialized terminology in their native script.
- **Core assumption:** The LLM possesses sufficient cross-lingual knowledge to generate accurate domain-specific translations or transliterations without hallucination.
- **Evidence anchors:** [section 3.1] states SHRAG composed and executed prompts tailored to each respective language to extract keywords.

### Mechanism 3: Two-Stage Retrieval (Keyword → Semantic)
- **Claim:** Decoupling the initial retrieval (Sparse/Boolean) from the final selection (Dense/Semantic) optimizes the trade-off between exact term matching and contextual understanding.
- **Mechanism:** Step 3 performs a brute-force Boolean search to gather a candidate pool based on lexical overlap. Step 4 then applies a multilingual embedding model (mGTE) to compute semantic similarity and re-rank this pool.
- **Core assumption:** The embedding model (mGTE) is sufficiently robust to align semantic meaning across the mixed-language candidate set.
- **Evidence anchors:** [section 3.4] describes the multilingual embedding model's goal to improve cross-language retrieval performance.

## Foundational Learning

- **Concept: Boolean Logic in IR (AND vs OR)**
  - **Why needed here:** The core finding of SHRAG challenges the typical search intuition of "narrowing down" with AND operators. Understanding that RAG benefits from "recall-first" (OR) strategies is counter-intuitive but central to this architecture.
  - **Quick check question:** Why does a search query using only OR operators typically return *more* results but potentially lower precision than an AND query?

- **Concept: Cross-Lingual Information Retrieval (CLIR)**
  - **Why needed here:** The system is designed for mixed-language environments (Korean/English). You must understand that vector spaces can align concepts across languages, but keyword extraction often requires explicit translation steps.
  - **Quick check question:** How does a multilingual embedding model (like mGTE) represent the concept "Artificial Intelligence" differently in a vector space compared to a monolingual model processing a translated keyword?

- **Concept: Sparse vs. Dense Retrieval**
  - **Why needed here:** SHRAG is a hybrid system. It uses Sparse (Boolean/Keywords) for the initial cast and Dense (Embeddings) for the final selection.
  - **Quick check question:** In a RAG pipeline, which stage is typically faster computationally: sparse keyword matching or dense vector similarity search?

## Architecture Onboarding

- **Component map:**
  1. Query Strategist (LLM): User Query → Multilingual Keyword Lists
  2. Query Constructor: Keywords → 10 Iterative OR-Queries (10 keywords down to 1)
  3. Retriever (Search Engine): OR-Queries → Raw Document Set (D)
  4. Re-ranker (mGTE): Query + Raw Docs → Top-5 Similar Docs (D_top5)
  5. Generator (LLM): Query + Top-5 Docs → Structured Answer

- **Critical path:** The most fragile step is Multilingual Keyword Extraction. If the LLM extracts "compound keywords" rather than single tokens, the search engine may return zero results.

- **Design tradeoffs:**
  - **Latency vs. Coverage:** Generating 10 sequential queries per user question increases API calls and search latency significantly compared to a single-shot vector search, but maximizes the chance of finding the answer.
  - **Token Usage:** The system retrieves potentially 100 docs (10 queries × 10 docs) to select 5, requiring memory for embeddings not typically found in simple RAG apps.

- **Failure signatures:**
  - **Zero Retrieval:** If the LLM hallucinates non-existent acronyms
  - **Context Drift:** If the re-ranker selects a document that shares semantic similarity but is factually contradictory to the user's specific intent

- **First 3 experiments:**
  1. **Keyword Granularity Test:** Compare retrieval success rates when using raw compound keywords vs. space-split single keywords
  2. **Boolean Ablation:** Run the same query set using AND operators vs. OR operators to reproduce the "94% success rate" drop-off
  3. **Embedding Model Swap:** Test mGTE vs. Snowflake to verify which better handles Korean-English cross-lingual alignment

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Boolean NOT operators be effectively integrated into the query strategy to improve precision without requiring excessive contextual reasoning?
- **Basis in paper:** [explicit] The authors state in Section 3.2 that queries with NOT operators were excluded because their generation requires deep contextual reasoning "beyond the scope of current keyword based extraction," deferring this to future work.
- **Why unresolved:** The current framework relies exclusively on OR operators to maximize recall; the challenge remains in prompting an LLM to reliably identify exclusionary terms without overspecifying the query.
- **What evidence would resolve it:** Ablation studies on the ScienceON dataset comparing recall and precision metrics between OR-only queries and those augmented with LLM-generated NOT clauses.

### Open Question 2
- **Question:** How can the framework be modified to overcome the "lexical gap" inherent in keyword-based extraction?
- **Basis in paper:** [explicit] Section 6 lists the "lexical gap problem," where retrieval fails if exact keywords are absent from the document text, as a primary limitation of the current rigid architecture.
- **Why unresolved:** The current pipeline depends on exact term matching between extracted keywords and the document corpus, making it brittle when terminology diverges.
- **What evidence would resolve it:** Experiments incorporating semantic expansion or dense retrieval hybridization that successfully retrieves relevant documents lacking the exact keywords extracted by the LLM.

### Open Question 3
- **Question:** To what extent does the query decomposition module improve performance on complex multi-hop questions compared to single-hop retrieval?
- **Basis in paper:** [inferred] Although Appendix B outlines a Query Decomposer for multi-hop queries, Section 5.1.2 notes the competition data was primarily single-hop, meaning the multi-hop architecture remains theoretically proposed but empirically unverified.
- **Why unresolved:** The paper validates performance on single-hop queries; the efficacy of the decomposition logic for synthesizing answers across multiple documents is currently unknown.
- **What evidence would resolve it:** Benchmarking the SHRAG pipeline with the Query Decomposer enabled on a standard multi-hop dataset against a single-hop baseline.

## Limitations
- Framework's effectiveness primarily validated on a single domain (academic scientific literature) and specific competition dataset, limiting generalizability
- Performance gain from OR-only queries needs independent replication across different datasets and domains
- Paper does not provide detailed ablation studies on how individual components contribute to overall performance

## Confidence
- **High Confidence:** The two-stage retrieval architecture (Boolean → semantic) is technically sound and aligns with established IR principles
- **Medium Confidence:** The multilingual keyword extraction approach is effective for Korean-English retrieval, though performance may vary with different language pairs
- **Medium Confidence:** The finding that OR-only queries outperform AND queries in RAG pipelines, though this may be specific to the academic domain context

## Next Checks
1. **Cross-Domain Replication:** Test SHRAG on non-academic datasets (e.g., news, legal documents) to verify the generalizability of the OR-only query strategy
2. **Component Ablation Study:** Systematically disable each component (multilingual extraction, disjunctive queries, re-ranking) to quantify individual contributions to the 94% success rate
3. **Real-Time Performance Analysis:** Measure and report the end-to-end latency of the 10-query iterative approach compared to single-shot semantic search methods in production environments