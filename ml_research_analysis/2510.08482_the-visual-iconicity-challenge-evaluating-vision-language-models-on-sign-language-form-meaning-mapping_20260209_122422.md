---
ver: rpa2
title: 'The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign
  Language Form-Meaning Mapping'
arxiv_id: '2510.08482'
source_url: https://arxiv.org/abs/2510.08482
tags:
- iconicity
- sign
- form
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce the Visual Iconicity Challenge, a novel video-based
  benchmark for evaluating vision-language models on sign language form-meaning mapping.
  Using Sign Language of the Netherlands, we test 13 state-of-the-art VLMs on phonological
  form prediction (handshape, location, movement), transparency (inferring meaning
  from form), and iconicity ratings.
---

# The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping

## Quick Facts
- arXiv ID: 2510.08482
- Source URL: https://arxiv.org/abs/2510.08482
- Authors: Onur Keleş; Aslı Özyürek; Gerardo Ortega; Kadir Gökgöz; Esam Ghaleb
- Reference count: 13
- Primary result: VLMs show partial success in phonological form prediction but struggle significantly on transparency and iconicity tasks, with smaller models near random chance

## Executive Summary
This paper introduces the Visual Iconicity Challenge, a novel benchmark for evaluating vision-language models on sign language form-meaning mapping using Sign Language of the Netherlands. The study tests 13 state-of-the-art VLMs across three tasks: phonological form prediction (handshape, location, movement), transparency (inferring meaning from form), and iconicity ratings. Models show varied performance, with location and handedness easier than handshape and path shape, and performance well below human baselines. Notably, models with stronger phonological form prediction correlate better with human iconicity judgments, suggesting shared sensitivity to visually grounded structure.

## Method Summary
The Visual Iconicity Challenge uses 96 NGT sign videos with annotations for five phonological features (handshape, location, path shape, path repetition, handedness), iconicity type (object-based vs action-based), and 1-7 iconicity ratings. Three tasks are evaluated: phonological form prediction via zero-shot/few-shot classification, transparency via open-set and multiple-choice meaning identification, and iconicity rating prediction via Spearman correlation. The study tests 13 VLMs ranging from 7B to 5T parameters using both native video and frame-based inputs, with results compared against human baselines.

## Key Results
- Models show partial success in phonological form prediction, with accuracy varying across parameters (location and handedness easier than handshape and path shape)
- On transparency tasks, models struggle significantly, especially in open-set identification (near random chance)
- For iconicity ratings, top models correlate moderately with human judgments (ρ up to 0.365), with larger models generally performing better
- Models systematically over-value object-based iconicity and under-value action-based iconicity, opposite to human preferences
- Models with stronger phonological form prediction show better alignment with human iconicity ratings

## Why This Works (Mechanism)

### Mechanism 1: Phonological Form Competence Supports Iconicity Sensitivity
- Claim: Models that better predict phonological features show stronger alignment with human iconicity ratings
- Mechanism: Both tasks require grounding in structured bodily properties; models attending to articulatory features develop representations that transfer to form-meaning mapping
- Evidence: Correlation between phonological accuracy and iconicity alignment across models (ρ=0.365, d=0.9); top models show both high phonological accuracy and high iconicity correlation
- Break condition: If phonological and iconicity tasks relied on independent representations, no correlation would emerge

### Mechanism 2: Task Difficulty Mirrors Human Acquisition Asymmetries
- Claim: VLM performance patterns parallel well-established acquisition asymmetries in human sign language learners
- Mechanism: Location and handedness are visually salient and globally perceivable; handshape and path shape require fine-grained local attention
- Evidence: Location accuracy (0.135-0.865) higher than handshape (0.083-0.677) across models; mirrors human acquisition patterns
- Break condition: If model architectures with specialized pose encodings were used, the difficulty hierarchy might shift

### Mechanism 3: Static Visual Bias Over Dynamic Action Mapping
- Claim: Models systematically over-value object-based iconicity and under-value action-based iconicity, unlike humans
- Mechanism: VLMs trained on image-text pairs lack embodied experience; they depend on surface correlations rather than motoric simulation
- Evidence: Humans show consistent action bias; most open-source models display reverse pattern favoring object-based signs
- Break condition: If models were trained on embodied simulation data or pose-structured inputs, action bias might emerge

## Foundational Learning

- **Concept: Sign Language Phonology Parameters**
  - Why needed here: The benchmark evaluates five phonological features; understanding these is required to interpret model outputs and error patterns
  - Quick check question: Can you explain why handshape is harder than location for both humans and models?

- **Concept: Iconicity Types (Object-based vs. Action-based)**
  - Why needed here: The dataset distinguishes iconicity types, and models show systematic bias; understanding this distinction is necessary for analyzing failure modes
  - Quick check question: Why would an action-based sign like TO-SMS be harder for a model without embodied grounding?

- **Concept: Zero-shot vs. Few-shot Evaluation**
  - Why needed here: The paper uses both protocols; few-shot provides only marginal gains, indicating the bottleneck is visual grounding, not task understanding
  - Quick check question: What does the lack of few-shot improvement on transparency suggest about model limitations?

## Architecture Onboarding

- **Component map:** NGT sign videos → frame extraction/native video → annotation layer (5 phonological features + iconicity type + 1-7 rating + gloss) → prompt templates → model inference → accuracy/Spearman ρ evaluation

- **Critical path:** Verify video loading works for target VLM → standardize prompt format → run zero-shot baseline → compare against random and human baselines

- **Design tradeoffs:** Video vs. frame extraction (native video preserves temporal dynamics); open-set vs. multiple-choice transparency (open-set is harder but more diagnostic); proprietary vs. open models (closed models perform best but lack transparency)

- **Failure signatures:** Scale compression (models cluster ratings around midpoint); object bias (higher ratings for object-based than action-based iconic signs); handshape blindness (accuracy near random for handshape on smaller models)

- **First 3 experiments:** 1) Replicate zero-shot phonology task on Qwen2.5-VL-7B; 2) Test pose-enhanced input on transparency task; 3) Fine-tune on phonological descriptors and evaluate transfer to iconicity rating

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating structured pose encodings (e.g., MediaPipe) mitigate VLMs' bias toward static object-based iconicity over dynamic action-based mappings?
- Basis in paper: Section 7 suggests integrating structured pose information to provide geometric grounding that raw video lacks
- Why unresolved: Current models lack "bodily grounding," relying on surface visual correlations which causes them to systematically undervalue dynamic agency in signs
- Evidence to resolve it: Comparative evaluation of pose-enhanced VLMs on the action-based subset showing preference shift toward action-based signs

### Open Question 2
- Question: To what extent does fine-tuning VLMs with auto-generated phonological descriptors improve their ability to infer meaning from sign form?
- Basis in paper: Section 7 proposes fine-tuning with auto-generated phonological descriptors to bridge the gap between form prediction and meaning
- Why unresolved: While models show some phonological sensitivity, they fail to translate this structural knowledge into semantic transparency in zero-shot settings
- Evidence to resolve it: Performance delta on transparency task before and after fine-tuning with synthetic data pairing video features with text descriptions

### Open Question 3
- Question: Do the identified visual grounding limitations persist across different sign languages and continuous, naturalistic discourse?
- Basis in paper: Section 8 notes the study is limited to 96 isolated NGT signs, potentially limiting generalizability
- Why unresolved: It remains unclear if the models' failure to infer meaning and their object-bias are specific to NGT or represent fundamental deficits in processing dynamic sign language
- Evidence to resolve it: Evaluating failure modes of these VLMs on continuous signing dataset from a distinct language family (e.g., ASL)

## Limitations
- Dataset size limited to 96 signs (64 iconic, 32 arbitrary), which may affect statistical power for detecting subtle model preferences
- Study focuses on isolated signs rather than continuous, naturalistic discourse, limiting generalizability
- Proprietary models perform best but lack transparency, making it difficult to understand architectural or training differences responsible for performance gaps

## Confidence

- **High confidence**: Phonological feature difficulty hierarchy (location > handedness > handshape > path shape) mirrors human acquisition patterns; robust across multiple models with strong theoretical grounding
- **Medium confidence**: Correlation between phonological form prediction and iconicity alignment; statistically significant but moderate effect sizes and potential confounding factors limit strong causal claims
- **Low confidence**: Claims about action bias inversion and its interpretation as evidence of static visual bias; limited dataset size and potential confounds in sign distribution make this finding less certain

## Next Checks

1. **Dataset Size Sensitivity Analysis**: Systematically vary the number of training examples (1-16 shots) for each phonological feature to determine whether the difficulty hierarchy persists under different sample sizes, helping isolate whether current patterns reflect genuine learning constraints versus dataset artifacts.

2. **Pose-Enhanced Input Experiment**: Re-run all tasks using MediaPipe keypoints concatenated to video prompts, testing whether action-based sign performance improves specifically for the phonological and transparency tasks where current models show systematic weaknesses.

3. **Architecture Ablation Study**: Compare performance across VLMs with different visual encoders (CNN-based vs transformer-based) and different training regimes (image-text vs video-text) to determine which architectural components most strongly predict success on phonological form prediction versus iconicity judgment.