---
ver: rpa2
title: 'Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual
  Foresight'
arxiv_id: '2511.16175'
source_url: https://arxiv.org/abs/2511.16175
tags:
- action
- arxiv
- mantis
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mantis introduces Disentangled Visual Foresight (DVF) to enhance
  Vision-Language-Action (VLA) models by decoupling visual state prediction from the
  backbone using meta queries and a diffusion Transformer (DiT) head. This design
  enables the model to capture latent actions that guide explicit action prediction
  while preserving comprehension and reasoning capabilities through language supervision.
---

# Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight

## Quick Facts
- arXiv ID: 2511.16175
- Source URL: https://arxiv.org/abs/2511.16175
- Authors: Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng
- Reference count: 40
- Primary result: 96.7% success rate on LIBERO benchmark

## Executive Summary
Mantis introduces Disentangled Visual Foresight (DVF) to enhance Vision-Language-Action (VLA) models by decoupling visual state prediction from the backbone using meta queries and a diffusion Transformer (DiT) head. This design enables the model to capture latent actions that guide explicit action prediction while preserving comprehension and reasoning capabilities through language supervision. Pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on the LIBERO benchmark, surpassing strong baselines and exhibiting faster convergence. Real-world evaluations on an Agilex platform show that Mantis outperforms π0.5 in instruction-following, generalization to unseen instructions, and reasoning ability. The Adaptive Temporal Ensemble (ATE) variant reduces inference counts by 50% while maintaining performance.

## Method Summary
Mantis employs a three-stage progressive training approach: (1) vision-only pretraining on human videos with frozen backbone to learn visual dynamics via DVF, (2) action prediction training on robot demonstrations with frozen backbone, and (3) mixed training with language supervision to integrate all modalities. The model uses Qwen2.5-VL as backbone with a connector and Sana DiT head for visual foresight prediction, plus a separate DiT-based action head. The DVF architecture uses meta queries to predict future frames while residual connections enable latent action capture. The model is trained on SSV2, DROID, and multimodal datasets, then finetuned on LIBERO benchmark with 5.8B total parameters.

## Key Results
- Achieves 96.7% success rate on LIBERO benchmark, surpassing π0.5 and other baselines
- Exhibits faster convergence than entangled visual foresight approaches, reaching high SR within 5 epochs
- ATE variant reduces inference counts by ~50% while maintaining performance
- Outperforms baselines in real-world instruction-following, generalization to unseen instructions, and reasoning ability on Agilex platform

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Visual Foresight (DVF) Decouples Representation Learning
Separating visual foresight prediction from the VLA backbone preserves representational capacity for language understanding while providing dense auxiliary supervision for action learning. The backbone produces latent representations via meta queries, which are then processed by a separate DiT head for future frame prediction. A residual connection feeds the current visual state directly to the DiT, enabling meta queries to focus on inter-frame dynamics rather than full reconstruction. The residual connection forces meta queries to encode only the delta between frames, not redundant static information.

### Mechanism 2: Latent-Action Queries Bridge Visual Foresight to Action Prediction
Meta queries trained on next-state prediction automatically capture action-relevant latent representations that improve explicit action generation. The DVF training objective forces [LAT] queries to predict visual trajectories. These queries then provide contextual signals to action queries [ACT] via causal attention, effectively transferring visual foresight knowledge to action prediction without direct supervision. The inter-frame visual dynamics are sufficiently correlated with physical robot actions for the learned representations to transfer.

### Mechanism 3: Progressive Training Prevents Modality Competition
Staged introduction of vision, action, and language modalities yields more stable optimization than simultaneous joint training. Stage 1 trains visual foresight on human videos (backbone frozen). Stage 2 adds action supervision on robot data. Stage 3 unfreezes backbone for language-supervised mixed training. This prevents early overfitting to easier action signals or dominant language modality. The model can transfer visual dynamics learned from human manipulation to robot action prediction.

## Foundational Learning

- **Diffusion Transformers (DiT)**
  - Why needed here: Both the DVF head (Sana) and action head use DiT architectures for iterative denoising of future frames and action trajectories.
  - Quick check question: Can you explain how a DiT differs from a standard transformer decoder in terms of iterative refinement?

- **Vision-Language Models (VLMs)**
  - Why needed here: Mantis builds on Qwen2.5-VL, leveraging its pretrained visual-linguistic alignments; understanding VLM tokenization and attention patterns is essential.
  - Quick check question: How does a VLM jointly process image patches and text tokens in a single sequence?

- **Query-based Decoding (Meta Queries)**
  - Why needed here: [LAT], [ACT], and [GAP] queries extract specific information from backbone outputs; understanding attention-based query mechanisms is critical.
  - Quick check question: How do learnable query tokens aggregate information from input embeddings via cross-attention?

## Architecture Onboarding

- **Component map:** (o_t, l, [LAT]) -> Backbone -> h_t -> Connector -> DVF Head -> o_{t+n} (training only); (o_t, l, [LAT], [ACT]) -> Backbone -> Action Head -> a_{t:t+n}
- **Critical path:** 1. Forward pass: (o_t, l, [LAT]) → Backbone → h_t → Connector → DVF → o_{t+n} (training only); 2. Action inference: (o_t, l, [LAT], [ACT]) → Backbone → Action Head → a_{t:t+n}; 3. At inference, DVF head is omitted; only action path is executed.
- **Design tradeoffs:** DVF disentanglement vs. entanglement: Disentanglement preserves language capabilities but requires careful connector design. Entanglement (UnifiedVLA) simplifies architecture but shows slower convergence. Multi-gap training vs. fixed-gap: Multi-gap enables flexible temporal prediction but increases training complexity. ATE vs. standard TE: ATE reduces inference by ~50% but may miss stability needs in complex manipulations if overlap detection fails.
- **Failure signatures:** Poor convergence in first 10 epochs: May indicate residual connection not functioning (check flawed-DVF configuration). Degraded instruction-following after fine-tuning: Likely insufficient language supervision (verify Stage 3 training with β > 0). ATE instability during grasping: Check τ_target and τ_dynamic thresholds; overlap detection may be too conservative.
- **First 3 experiments:** 1. Validate DVF residual connection: Train vanilla-DVF vs. flawed-DVF (no residual) on a single LIBERO task for 10 epochs; expect >2% SR gap confirming mechanism 1. 2. Ablate progressive training: Compare staged training vs. simultaneous joint training on DROID subset; measure convergence speed and final SR. 3. Test ATE threshold sensitivity: Sweep τ_target ∈ {0.5, 1, 2} and τ_dynamic ∈ {8, 12, 16} on LIBERO Spatial; identify settings that maintain >95% SR while minimizing inference count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the incorporation of explicit robot state inputs (proprioception) resolve the "minor motion rollbacks" observed in real-world scenarios, and how does this affect the policy's reactive latency?
- Basis in paper: The conclusion identifies "minor motion rollbacks in real-world scenarios due to missing robot state inputs" as a primary limitation.
- Why unresolved: The current model relies solely on visual feedback, which may lack the high-frequency precision required to arrest momentum or correct overshoots in physical interactions.
- What evidence would resolve it: A comparative study on the Agilex platform measuring rollback frequency and task completion time with and without joint state inputs.

### Open Question 2
- Question: To what extent does integrating 3D point cloud data into the Mantis backbone enhance performance on manipulation tasks involving heavy occlusion or depth-critical alignment?
- Basis in paper: The authors list "integrat[ing] richer inputs (e.g., 3D point clouds)" as a specific direction for future work.
- Why unresolved: The current reliance on 2D RGB images limits the model's ability to reason about object geometry and spatial relations in unstructured, 3D environments.
- What evidence would resolve it: Benchmarking Mantis against a variant with depth/point-cloud encoders on tasks like precise insertion or object retrieval in cluttered bins.

### Open Question 3
- Question: Do the latent-action queries learned via the DVF head converge to physical motion primitives, or do they retain correlations with non-causal visual artifacts (e.g., shadows, textures)?
- Basis in paper: The paper argues that DVF captures inter-frame dynamics to guide action, but prior works (cited in Related Work) note that visual foresight often hallucinates non-physical changes.
- Why unresolved: While empirical success is shown, the semantic content of the latent space remains unverified; it is unclear if the model learns physics or merely visual interpolation.
- What evidence would resolve it: Linear probing of the [LAT] embeddings to test for linear correlation with ground-truth robot proprioceptive states versus lighting/texture changes.

## Limitations

- **Scalability concerns**: The DVF head (1.4B parameters) adds significant computational overhead, and the architecture's generalization to more complex, long-horizon tasks with longer action trajectories remains untested.
- **Transfer assumption**: The three-stage training approach assumes that visual dynamics learned from human videos transfer effectively to robot action prediction, which may not hold for tasks requiring precise force control or contact-rich manipulation.
- **ATE reliability**: The Adaptive Temporal Ensemble reduces inference by 50% but depends on overlap detection between target and dynamic patches, which may fail in scenarios with ambiguous visual cues or similar-looking objects.

## Confidence

- **High confidence**: The core claim that disentangled visual foresight improves convergence speed compared to entangled approaches (Mechanism 1) is well-supported by ablation studies showing 96.2% vs 91.3% SR and faster convergence in Figure 5.
- **Medium confidence**: The assertion that latent-action queries automatically capture action-relevant representations (Mechanism 2) is plausible but relies on the assumption that visual dynamics correlate with physical actions, which may not hold in all environments.
- **Low confidence**: The claim of superior real-world generalization to unseen instructions (96.7% success rate) lacks detailed ablation analysis, making it difficult to assess whether Mantis would maintain this performance on substantially different task distributions.

## Next Checks

1. **Stress-test DVF residual connection**: Train Mantis with the residual connection disabled on a complex LIBERO task requiring precise spatial reasoning (e.g., Goal Place-8). Measure whether the SR gap between vanilla-DVF and flawed-DVF increases beyond the reported 1.3%, indicating that the residual becomes more critical for complex tasks.

2. **Evaluate transfer across embodiment gaps**: Fine-tune Mantis on DROID demonstrations from a different robot platform (e.g., Panda vs Agilex) and test on LIBERO. Compare SR drop to a baseline VLA model without DVF pretraining to quantify how much the visual foresight representation generalizes across physical differences.

3. **Test ATE failure modes**: Create adversarial test cases where target and dynamic patches have high visual similarity (e.g., placing a red cube on a red plate). Measure whether ATE triggers correctly or produces unstable predictions, and quantify the performance gap between ATE and standard TE in these edge cases.