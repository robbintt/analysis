---
ver: rpa2
title: 'DELM: a Python toolkit for Data Extraction with Language Models'
arxiv_id: '2509.20617'
source_url: https://arxiv.org/abs/2509.20617
tags:
- delm
- extraction
- prompt
- data
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DELM addresses the engineering challenges of building reliable,
  reproducible LLM-based data extraction pipelines at scale. The toolkit introduces
  a configuration-driven framework that decouples scientific inquiry from engineering
  overhead through structured workflows, deterministic caching, batch execution, and
  comprehensive provenance tracking.
---

# DELM: a Python toolkit for Data Extraction with Language Models

## Quick Facts
- arXiv ID: 2509.20617
- Source URL: https://arxiv.org/abs/2509.20617
- Reference count: 4
- Primary result: DELM provides a configuration-driven framework for reproducible, cost-aware LLM-based data extraction with deterministic caching and structured validation

## Executive Summary
DELM addresses the engineering challenges of building reliable, reproducible LLM-based data extraction pipelines at scale. The toolkit introduces a configuration-driven framework that decouples scientific inquiry from engineering overhead through structured workflows, deterministic caching, batch execution, and comprehensive provenance tracking. Two case studies demonstrate its utility: first, a cost-recall tradeoff analysis using keyword filtering on commodity extraction from investor call transcripts, revealing how recall degrades as costs are constrained; second, an LLM-in-the-loop prompt optimization experiment showing iterative improvement in extraction precision through automated prompt refinement.

## Method Summary
DELM implements a configuration-driven data extraction pipeline where all parameters are persisted as data rather than code. The system loads data from multiple formats (text, CSV, Parquet, PDF, Word, HTML), preprocesses and chunks it, optionally scores relevance, filters based on keywords, and executes batch LLM calls using Instructor with caching. All outputs are validated against user-defined Pydantic schemas and results are persisted with comprehensive cost tracking. The framework supports two main use cases: commodity extraction from investor call transcripts with cost-recall tradeoff analysis using greedy keyword selection, and prompt optimization via the LILPRO algorithm for iterative refinement of extraction prompts.

## Key Results
- Keyword filtering on commodity extraction reveals a concave Pareto frontier where recall degrades as costs are constrained
- LILPRO algorithm demonstrates iterative improvement in extraction precision through automated prompt refinement
- Deterministic caching prevents redundant API calls and ensures identical configurations produce identical results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic caching prevents redundant API calls and ensures identical configurations produce identical results.
- Mechanism: Cache keys are computed from the fully materialized prompt, system prompt, model identifier, and generation parameters. Before each API call, the executor queries the cache; on miss, it persists validated results. SQLite WAL is the default backend with LMDB and filesystem alternatives.
- Core assumption: Inputs that hash identically should produce semantically equivalent outputs (temperature=0 or deterministic sampling required for full reproducibility).
- Evidence anchors:
  - [abstract] "deterministic caching" as a core feature
  - [section 4.2] "Semantic Cache using SQLite WAL by default with LMDB and file system alternatives"
  - [corpus] Weak direct evidence; OnPrem.LLM addresses privacy, not caching strategies
- Break condition: Non-deterministic sampling (temperature > 0) combined with cache hits will return stale results that don't reflect the sampling distribution.

### Mechanism 2
- Claim: Configuration-as-data decouples scientific inquiry from engineering overhead, enabling reproducible experiment comparison.
- Mechanism: Every run persists its complete configuration (data sources, sampling filters, prompt text, model/temperature settings, evaluation parameters) to disk. This enables exact reproduction and post-hoc performance analysis across parameter settings.
- Core assumption: Researchers will reference persisted configurations rather than modifying them in-place; no implicit state affects outcomes.
- Evidence anchors:
  - [section 3.3] "We therefore treat configuration as data: every run records data sources, sampling filters, prompt text, model and temperature settings, and evaluation parameters"
  - [section 4.1] "maintains comprehensive provenance records and cost tracking while supporting interruption-safe resumption"
  - [corpus] QueryGym similarly targets reproducibility for LLM query reformulation but focuses on retrieval rather than extraction pipelines
- Break condition: If external state (API version changes, model weight updates) is not captured in configuration, reproducibility fails.

### Mechanism 3
- Claim: Pydantic schema validation enforces structured outputs, enabling systematic evaluation with classification/regression metrics.
- Mechanism: Schema Manager loads YAML specifications, constructs Pydantic models, renders prompts dynamically, and validates all LLM responses against these schemas. This permits interchangeable scoring modules (accuracy, precision, recall, F1, R², MAE).
- Core assumption: LLM outputs can be constrained to valid JSON matching the schema; Instructor library handles provider-specific implementations.
- Evidence anchors:
  - [abstract] "structured outputs, built-in validation, flexible data-loading and scoring strategies"
  - [section 4.1] "validates returned objects against the user-defined schema before persisting results"
  - [corpus] No direct corpus validation; TinyTroupe uses LLM agents but for simulation, not structured extraction
- Break condition: Schema violations that pass validation (e.g., semantically incorrect but syntactically valid JSON) will corrupt downstream metrics.

## Foundational Learning

- Concept: Pydantic models and YAML schema specification
  - Why needed here: Core to DELM's validation pipeline; all extraction outputs are validated against user-defined Pydantic schemas generated from YAML specs.
  - Quick check question: Can you write a Pydantic model with optional fields and nested structures, then validate a JSON response against it?

- Concept: Exponential backoff and retry logic for API rate limits
  - Why needed here: DELM wraps all model calls with automatic retries; understanding backoff prevents thundering-herd failures at scale.
  - Quick check question: Given rate limit R requests/minute, what backoff schedule ensures recovery without overloading the API on retry storms?

- Concept: Batch processing with checkpointing and resumability
  - Why needed here: Large extraction jobs require interruption-safe execution; checkpointing protects against lost progress.
  - Quick check question: If a 10,000-document batch fails at document 7,342, what state must be persisted to resume correctly?

## Architecture Onboarding

- Component map:
  core/ -> Data Processor (loading, chunking, scoring, filtering) -> Extraction Manager (batched Instructor calls, concurrency, cache integration) -> Experiment Manager (run directories, checkpointing, output consolidation)
  schemas/ -> Schema Manager (YAML → Pydantic, prompt rendering, validation)
  strategies/ -> Loaders (text, CSV, Parquet, PDF via marker OCR, Word, HTML) -> Scorers (keyword, fuzzy) -> Splitters (chunking rules)
  utils/ -> Retry Handler, Semantic Cache, Cost Tracker, logging, concurrency helpers

- Critical path: Load data → Preprocess/chunk → Score relevance (optional) → Filter → Batch LLM calls via Instructor → Cache lookup/store → Schema validation → Persist results + cost logs

- Design tradeoffs:
  - SQLite WAL vs. LMDB vs. filesystem cache: WAL is default; LMDB offers higher throughput for very large caches; filesystem is simplest but slowest.
  - Keyword filtering reduces cost but sacrifices recall (Figure 1 shows concave Pareto frontier—diminishing recall returns as cost increases).
  - Checkpointing frequency vs. I/O overhead: more frequent checkpoints reduce lost work but increase latency.

- Failure signatures:
  - Empty extraction results with no errors: likely cache hit on stale configuration; clear cache or verify configuration drift.
  - Unexpectedly high costs: scoring/filtering not applied, or cache misses due to minor prompt variations; check cost tracker logs.
  - Validation failures on valid-looking JSON: schema mismatch (e.g., field name typo in YAML); compare schema to model output directly.

- First 3 experiments:
  1. Run the commodity extraction example from Section 5.1 with k=5 keywords vs. k=20 keywords; compare normalized cost and recall to verify Figure 1 trends.
  2. Implement a minimal Pydantic schema for a new extraction task (e.g., extracting dates and entity names); run with temperature=0 and verify cache hit on re-run.
  3. Run LILPRO (Algorithm 1) for 3 batches on price_expectation with batch size B=50; log precision per batch and confirm improvement trajectory matches Figure 2 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework maintain deterministic provenance and reproducibility when integrating non-deterministic, multi-step agentic behaviors?
- Basis in paper: [explicit] The authors list "agentic AI integrations" as a primary future direction for development.
- Why unresolved: Agentic workflows involve dynamic tool use and reasoning chains that conflict with the current static, configuration-driven architecture.
- What evidence would resolve it: A case study demonstrating an agentic workflow within DELM that preserves comprehensive logging and caching across stochastic steps.

### Open Question 2
- Question: How does the LILPRO algorithm compare to compiler-based optimization frameworks (e.g., DSPy) regarding convergence speed and stability?
- Basis in paper: [inferred] The paper introduces LILPRO but does not benchmark it against existing declarative optimizers like DSPy mentioned in related work.
- Why unresolved: Without comparative benchmarks, it is unclear if LILPRO offers advantages over established gradient-free or compiler-based prompt tuning methods.
- What evidence would resolve it: Ablation studies measuring precision gains and token costs of LILPRO against DSPy on identical extraction tasks.

### Open Question 3
- Question: Does the concave cost-recall frontier observed in commodity extraction generalize to other domains or languages?
- Basis in paper: [inferred] The cost-recall analysis is restricted to a single dataset of investor call transcripts.
- Why unresolved: The shape of the Pareto frontier depends on keyword density and document structure, which likely vary significantly across different text corpora.
- What evidence would resolve it: Repeating the keyword filtering experiment on diverse datasets (e.g., clinical notes, legal briefs) to observe if the frontier shape persists.

## Limitations
- Evaluation focuses on two specific use cases from a single dataset of investor call transcripts, limiting generalizability
- Does not report statistical significance testing for performance differences across configurations
- Cost estimates depend on specific LLM API provider and pricing model, which may change over time

## Confidence
- **High confidence**: Configuration-as-data mechanism for reproducibility, Pydantic schema validation for structured outputs, batch processing with checkpointing
- **Medium confidence**: Cost-recall tradeoff patterns from keyword filtering, LILPRO algorithm effectiveness for prompt optimization
- **Low confidence**: Generalizability across diverse extraction tasks, performance relative to alternative frameworks, long-term reliability of caching across API provider changes

## Next Checks
1. **Cross-dataset validation**: Apply DELM to at least two additional extraction tasks (e.g., legal document entity extraction, medical record information extraction) to assess generalizability beyond investor call transcripts.

2. **Baseline comparison**: Implement a minimal alternative pipeline using OpenAI's SDK directly (without DELM's caching, validation, and configuration management) and compare extraction quality, cost, and execution time on identical tasks.

3. **Statistical analysis**: For the commodity extraction experiment, perform statistical significance testing (e.g., paired t-tests) on recall differences across keyword filtering configurations to determine whether observed tradeoffs are meaningful beyond random variation.