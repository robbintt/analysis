---
ver: rpa2
title: Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language
  Model Capabilities
arxiv_id: '2510.19892'
source_url: https://arxiv.org/abs/2510.19892
tags:
- dixit
- card
- players
- player
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dixit, a game-based evaluation framework
  for multimodal large language models (MLMs), addressing limitations of existing
  benchmarks that rely on subjective pairwise comparisons or isolated task assessments.
  The Dixit game requires players to generate creative captions for fantasy cards
  that trick some but not all other players into selecting the correct card, testing
  multiple MLM capabilities including image comprehension, reasoning, classification,
  and captioning within a single unified task.
---

# Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities

## Quick Facts
- arXiv ID: 2510.19892
- Source URL: https://arxiv.org/abs/2510.19892
- Reference count: 12
- Primary result: Game-based framework that evaluates multiple MLM capabilities simultaneously without subjective pairwise comparisons

## Executive Summary
This paper introduces Dixit, a game-based evaluation framework for multimodal large language models (MLMs) that addresses limitations of existing benchmarks. Unlike subjective pairwise comparisons or isolated task assessments, Dixit uses the popular card game's mechanics to test image comprehension, reasoning, classification, and captioning within a single unified task. The framework requires players to generate creative captions for fantasy cards that trick some but not all other players into selecting the correct card, creating an engaging evaluation method that provides objective scoring without external judges.

Experiments with five MLMs (GPT-4o, Claude-3.5 Sonnet, InternVL2, Qwen2VL, and Molmo) demonstrate above-random performance across all models, with Dixit rankings perfectly correlating with established benchmarks like ChatbotArena and OpenVLM Leaderboard. Human versus MLM gameplay reveals that even relatively weak models like GPT-4o Mini can outperform some human players, while analysis shows MLMs tend to generate more literal, descriptive captions compared to humans who reference external knowledge. The framework provides an efficient, objective evaluation method that tests multiple MLM capabilities simultaneously.

## Method Summary
The Dixit framework adapts the popular card game into a multimodal evaluation tool by having MLMs play as virtual players. The game uses 100 open-source fantasy cards and requires storyteller players to select a card and generate a caption that allows some but not all opponents to identify the correct card. MLMs act as both storytellers (generating captions) and guessers (selecting cards based on captions). The framework employs zero-shot prompting with chain-of-thought reasoning, using models' APIs wrapped in prompt templates for card selection, caption generation, and voting. Games are played until one player reaches 30 points or the deck is exhausted, with scores computed based on whether the correct card was identified by some, all, or none of the opponents.

## Key Results
- All five tested MLMs (GPT-4o, Claude-3.5 Sonnet, InternVL2, Qwen2VL, Molmo) achieved above-random performance on Dixit
- Dixit game rankings perfectly correlated with established MLM benchmarks (ChatbotArena and OpenVLM Leaderboard)
- Even weaker models like GPT-4o Mini could outperform some human players in head-to-head gameplay
- MLMs generated more literal, descriptive captions (CLIPScore 15-30) compared to humans who referenced external knowledge (CLIPScore ~5-10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Game-based evaluation with fixed rules eliminates subjective external judges while jointly testing multiple MLM capabilities.
- Mechanism: Competitive games inherently require diverse abilities to win, and fixed scoring rules provide objective evaluation without human or model-based pairwise comparisons that suffer from verbosity/position biases.
- Core assumption: Capabilities tested in games transfer meaningfully to real-world MLM performance.
- Evidence anchors:
  - [abstract] "games require multiple abilities for players to win, are inherently competitive, and are governed by fixed, objective rules"
  - [section 1] Games "eliminating the need for external judges and ensuring objective, efficient evaluations"
  - [corpus] Related work on game-based LLM evaluation (e.g., "Can Large Language Models Master Complex Card Games?") shows similar transfer, though FMR scores are moderate (~0.49 average).

### Mechanism 2
- Claim: The Dixit storyteller scoring constraint forces calibrated caption generation, testing creativity and theory-of-mind simultaneously.
- Mechanism: Storytellers score 0 points if no one or everyone guesses correctly; they must generate captions sufficiently relevant to guide some players but not all—requiring estimation of other players' interpretation patterns.
- Core assumption: Models can internally simulate other agents' understanding without explicit theory-of-mind training.
- Evidence anchors:
  - [section 3] "the caption t should allow some of the other players to identify the correct card c, but not all of them"
  - [section 5.3.2] MLMs produce literal descriptions (CLIPScore 15-30) vs. humans' ambiguous, external-knowledge-referencing captions (CLIPScore ~5-10)
  - [corpus] DixitWorld paper (FMR 0.55) confirms Dixit evaluates multimodal abductive reasoning.

### Mechanism 3
- Claim: MLM rankings on Dixit correlate with established benchmarks because the game compresses multiple isolated tasks into one unified evaluation.
- Mechanism: Dixit requires: (1) image comprehension (selecting matching cards), (2) creative captioning, (3) strategic reasoning (predicting what others will select), and (4) classification accuracy—all tested through gameplay rather than separate benchmarks.
- Core assumption: Performance on these game-embedded tasks reflects the same underlying capabilities measured by individual benchmarks.
- Evidence anchors:
  - [section 5.1] "Dixit game rankings are perfectly correlated with rankings on OpenVLM and ChatbotArena"
  - [section 1] "Dixit captures diverse model abilities in a single, cohesive task"
  - [corpus] Corpus shows moderate FMR (~0.49), suggesting correlation but not definitive causation; more benchmarks needed.

## Foundational Learning

- Concept: Multimodal grounding (linking visual features to semantic text)
  - Why needed here: Models must interpret fantasy card images and generate/evaluate captions that match visual content.
  - Quick check question: Can the model describe an image's key elements accurately before attempting strategic captioning?

- Concept: Theory-of-mind / intentional ambiguity
  - Why needed here: Winning Dixit requires predicting how others will interpret a caption—not just matching caption to image.
  - Quick check question: Given two captions for one image, can the model predict which would be harder for others to guess?

- Concept: Calibration in generation
  - Why needed here: The storyteller must balance specificity—too vague yields 0 points (no guesses), too specific yields 0 points (all guesses).
  - Quick check question: Can the model adjust caption specificity based on how many players it wants to fool?

## Architecture Onboarding

- Component map: Game state manager -> MLM player agent -> Evaluation logger -> Human interface (optional)

- Critical path:
  1. Initialize deck (100 open-source Dixit-style cards)
  2. Storyteller agent receives hand → selects card → generates caption via prompted LLM call
  3. Other agents receive caption + their hand → select matching card
  4. Pool all selected cards → each agent votes on storyteller's card
  5. Apply scoring rules → repeat until 30 points or deck exhausted

- Design tradeoffs:
  - Zero-shot prompting vs. fine-tuning: Paper uses zero-shot; fine-tuning could improve but risks overfitting to game
  - Closed-source vs. open-weight models: Closed-source models showed better chain-of-thought reasoning; open-weights exhibited more hallucinations
  - Literal vs. abstract captions: Literal captions (current MLM behavior) are easier to evaluate but weaker for fooling calibrated opponents

- Failure signatures:
  - Hallucination in card selection rationale (model references visual elements not present)
  - No reasoning output (model repeats caption without justification)
  - Implausible matching (model selects cards with tenuous connections)
  - Over-literal captions with high CLIPScore (>25) that are too easy for opponents

- First 3 experiments:
  1. Baseline replication: Run 20 games with the 5 tested MLMs to verify ranking correlation with ChatbotArena/OpenVLM.
  2. Ablation on caption strategy: Prompt models to generate abstract/external-knowledge captions; measure impact on win-rate and CLIPScore.
  3. Human vs. MLM scale-up: Expand human-MLM games beyond 3 players/3 games to quantify variance and identify consistent skill gaps.

## Open Questions the Paper Calls Out

None

## Limitations

- The game's fantasy art style may not generalize to real-world visual reasoning tasks
- The framework's reliance on zero-shot prompting leaves performance gains from fine-tuning unexplored
- Current evaluation uses only 100 open-source cards, limiting the diversity of visual concepts tested

## Confidence

**High confidence** in the framework's ability to provide objective, game-based evaluation without pairwise comparisons. The mechanism of fixed rules and scoring is sound, and the elimination of subjective judges is well-supported by the experimental design.

**Medium confidence** in the claim that Dixit captures "diverse model abilities in a single, cohesive task." While the game does require multiple capabilities, the degree to which these map to real-world MLM performance remains to be proven beyond the current correlation with ChatbotArena and OpenVLM.

**Low confidence** in the assertion that MLMs' literal captioning represents a fundamental limitation versus human external-knowledge referencing. The current prompting strategy may be constraining model behavior rather than revealing true capability gaps.

## Next Checks

1. **Correlation robustness test**: Expand the evaluation to 15+ models including frontier models (GPT-4o, Gemini, Claude-3.5) and open-weight models across different scales. Verify whether the perfect correlation with ChatbotArena/OpenVLM persists and quantify any deviations.

2. **Caption strategy ablation**: Systematically vary prompting strategies to elicit abstract versus literal captions. Compare CLIPScore distributions, win-rates, and human evaluation of caption quality to determine whether MLMs can match human-level abstract reasoning when properly prompted.

3. **Real-world generalization benchmark**: Design a parallel evaluation using real photographs instead of fantasy art, maintaining the same game mechanics. Measure whether models that excel at Dixit fantasy cards show comparable performance on realistic visual reasoning tasks, establishing ecological validity.