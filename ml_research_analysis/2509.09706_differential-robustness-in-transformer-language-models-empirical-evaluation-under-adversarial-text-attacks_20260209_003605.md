---
ver: rpa2
title: 'Differential Robustness in Transformer Language Models: Empirical Evaluation
  Under Adversarial Text Attacks'
arxiv_id: '2509.09706'
source_url: https://arxiv.org/abs/2509.09706
tags:
- attack
- adversarial
- attacks
- success
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the robustness of large language models against
  adversarial text attacks using TextFooler and BERTAttack. BERT-Base showed significant
  vulnerability with a 93.75% attack success rate, while RoBERTa-Base and Flan-T5
  demonstrated complete resilience with 0% attack success rates.
---

# Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks

## Quick Facts
- **arXiv ID:** 2509.09706
- **Source URL:** https://arxiv.org/abs/2509.09706
- **Reference count:** 6
- **Primary result:** BERT-Base showed 93.75% vulnerability to TextFooler attacks while RoBERTa-Base and Flan-T5 demonstrated complete resilience with 0% attack success rates.

## Executive Summary
This study systematically evaluated the robustness of large language models against adversarial text attacks using TextFooler and BERTAttack on SST-2 and SQuAD v2 datasets. The research revealed dramatic variations in model resilience, with BERT-Base showing significant vulnerability (93.75% attack success rate) while RoBERTa-Base and Flan-T5 demonstrated complete robustness (0% attack success rates). The findings highlight that model robustness is not uniformly distributed across architectures and that effective defensive mechanisms often require substantial computational resources. The work contributes to understanding LLM security by revealing strengths and weaknesses in current safeguarding approaches, with implications for developing more efficient and effective defensive strategies.

## Method Summary
The study evaluated BERT-Base, RoBERTa-Base, and Flan-T5 against TextFooler and BERTAttack using PromptBench library on Google Colab Pro A100 with <50 GPU hours. The methodology involved running attacks on 100 test instances per evaluation from SST-2 (sentiment classification) and SQuAD v2 (question answering) datasets. Attacks only targeted correctly classified samples, with Algorithm 4.2 iterating through the dataset to apply attacks, track success/fail/skip counts, and calculate metrics including Attack Success Rate (ASR), Original Accuracy, Accuracy Under Attack, Average Perturbed Word Percentage, Average Queries, and Robustness Score (R = 1 - ASR).

## Key Results
- BERT-Base demonstrated 93.75% vulnerability to TextFooler attacks with significant accuracy degradation
- RoBERTa-Base and Flan-T5 showed complete resilience with 0% attack success rates
- Flan-T5 maintained 0% ASR while requiring higher computational resources (query counts) than BERT
- Robust models exhibited substantial variation in computational efficiency during attack defense

## Why This Works (Mechanism)

### Mechanism 1: Importance-Based Word Substitution
Adversarial success relies on identifying and replacing high-importance tokens to flip model predictions while preserving semantic similarity. The attack calculates word importance scores by measuring prediction confidence changes when tokens are removed, then iteratively replaces top-ranking words with synonyms derived from embedding space using cosine similarity. This mechanism succeeds when models' decision boundaries are highly sensitive to specific input tokens, but fails when decision boundaries are smooth or rely on global context.

### Mechanism 2: Architectural and Training Robustness
Transformer robustness varies based on specific pre-training objectives and data diversity rather than parameter count alone. RoBERTa's dynamic masking and Flan-T5's instruction tuning create representations where single-token perturbations don't significantly shift output distributions. This emergent robustness property stems from training methodology rather than model size, as evidenced by smaller models (Flan-T5 250M params, RoBERTa 125M params) outperforming larger BERT (110M params).

### Mechanism 3: Computational Cost of Defense
Higher robustness imposes computational costs measurable by query counts required to find successful perturbations. Robust models force attackers to search larger spaces, with the study defining efficiency metrics showing RoBERTa required 239.71 queries per sample to confirm invulnerability. This computational burden acts as a practical deterrent, though improved attack algorithms could reduce this defensive barrier.

## Foundational Learning

- **Concept:** Word Embedding Space & Cosine Similarity
  - **Why needed here:** TextFooler operates by finding replacement words close in vector space, but geometric closeness doesn't guarantee semantic equivalence
  - **Quick check question:** If two words have a cosine similarity of 0.9, are they guaranteed to be interchangeable in a sentence without changing the sentiment?

- **Concept:** Token Importance (Saliency)
  - **Why needed here:** The core attack strategy is greedy replacement based on importance scores; understanding how models assign weight to specific tokens is crucial
  - **Quick check question:** In the equation $I(w_i) = f_\theta(x)_y - f_\theta(x \setminus w_i)_y$, what does a negative result imply about the word $w_i$?

- **Concept:** Encoder-Decoder vs. Encoder-Only Architectures
  - **Why needed here:** The paper compares BERT/RoBERTa (Encoders) with Flan-T5 (Encoder-Decoder), where structural differences influence susceptibility to perturbations
  - **Quick check question:** Why might an Encoder-Decoder model (Flan-T5) be more resilient to input perturbations than a pure Encoder (BERT) in a classification task?

## Architecture Onboarding

- **Component map:** Victim Models (BERT-Base, RoBERTa-Base, Flan-T5) -> Attack Engines (TextFooler, BERTAttack) -> Evaluation Datasets (SST-2, SQuAD v2) -> Framework (PromptBench)
- **Critical path:** Setup models and datasets → Screen correctly classified samples → Attack with TextFooler/BERTAttack → Verify prediction flips → Calculate ASR and Robustness Score
- **Design tradeoffs:** Robustness vs. Compute (robust models demand more compute); Black-Box vs. White-Box (TextFooler is black-box, transferable but less efficient)
- **Failure signatures:** High Perturbation % (>30%) indicates semantically useless attacks; Query Exhaustion (>200 queries) indicates practical robustness
- **First 3 experiments:**
  1. Baseline Vulnerability Test: Replicate BERT-Base + TextFooler on SST-2 subset to confirm 90%+ failure rate
  2. Architectural Comparison: Run identical attack on RoBERTa-Base to observe importance score distribution differences
  3. Perturbation Budget Analysis: Modify TextFooler constraint ε to test Flan-T5's tolerance threshold compared to BERT's

## Open Questions the Paper Calls Out

- What specific internal architectural or training characteristics enable complete resilience of RoBERTa-Base and Flan-T5 compared to BERT-Base?
- Does the high robustness observed on standardized benchmarks generalize to complex, unstructured real-world environments?
- Can the proposed multi-objective optimization framework effectively reduce high computational costs required for robust defense?

## Limitations

- Limited methodological transparency regarding TextFooler and BERTAttack hyperparameters (similarity thresholds, maximum perturbation percentages, query limits)
- Unclear whether models were fine-tuned on target datasets or used zero-shot, creating ambiguity around baseline accuracies
- Sampling strategy for selecting 100 test instances remains unspecified, raising questions about representativeness
- Generalizability of robustness findings beyond synonym-based attacks to other adversarial vectors remains uncertain

## Confidence

**High Confidence:** The differential vulnerability pattern between BERT and more robust architectures is well-supported by quantitative metrics (ASR: 93.75% vs 0%) and aligns with established findings about architectural influences on robustness.

**Medium Confidence:** The computational cost-defense trade-off claim requires cautious interpretation due to limited comparative data across attack types and defensive mechanisms.

**Low Confidence:** The generalizability of robustness findings beyond synonym-based attacks remains uncertain, as the paper doesn't test alternative attack vectors or empirically isolate mechanisms behind Flan-T5's complete resilience.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary TextFooler's similarity threshold τ (0.7, 0.8, 0.9) and maximum perturbation percentage across all three models to determine whether 0% ASR for RoBERTa/Flan-T5 persists under more aggressive attack settings.

2. **Architectural Ablation Study:** Test a fine-tuned BERT variant with RoBERTa's dynamic masking or Flan-T5's instruction tuning capabilities to isolate whether robustness stems from architectural differences versus training methodology.

3. **Cross-Dataset Generalization Test:** Evaluate the same attack-defense dynamics on MNLI and CoNLL-2003 datasets to determine if observed robustness patterns are task-specific or represent general architectural properties.