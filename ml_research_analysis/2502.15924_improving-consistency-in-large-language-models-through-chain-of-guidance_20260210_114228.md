---
ver: rpa2
title: Improving Consistency in Large Language Models through Chain of Guidance
arxiv_id: '2502.15924'
source_url: https://arxiv.org/abs/2502.15924
tags:
- consistency
- answer
- answers
- lora
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Chain of Guidance (CoG), a multi-step prompting
  technique to improve semantic consistency in LLM outputs for closed-book question
  answering tasks. The approach uses guided paraphrasing and answer selection to generate
  consistent question-answer pairs, which are then used to fine-tune smaller LLMs.
---

# Improving Consistency in Large Language Models through Chain of Guidance

## Quick Facts
- **arXiv ID:** 2502.15924
- **Source URL:** https://arxiv.org/abs/2502.15924
- **Reference count:** 26
- **Primary result:** CoG fine-tuning improves semantic consistency in closed-book QA models, with >2x gains measured via entailment, paraphrase detection, and ROUGE-L.

## Executive Summary
This paper introduces Chain of Guidance (CoG), a multi-step prompting technique designed to improve semantic consistency in LLM outputs for closed-book question answering. By using guided paraphrasing and answer selection, CoG generates synthetic question-answer pairs that are then used to fine-tune smaller LLMs. The approach yields more than twice the consistency of base models, as measured by entailment, paraphrase detection, and ROUGE-L metrics, while retaining performance on general tasks. CoG demonstrates that guided synthetic data generation can effectively enhance LLM reliability and trustworthiness.

## Method Summary
CoG leverages a guiding LLM to generate multiple paraphrases and candidate answers for a given question. These are filtered and ranked to select the most semantically consistent pairs, which are then used to fine-tune smaller, task-specific LLMs. The method relies on a multi-step prompting pipeline, where the guiding LLM iteratively refines outputs to maximize consistency. The synthetic data is evaluated and filtered using both automated metrics and human oversight to ensure high-quality training pairs. This process results in fine-tuned models that exhibit significantly improved consistency on unseen datasets.

## Key Results
- CoG fine-tuning produces more than twice the semantic consistency compared to base models, as measured by entailment, paraphrase detection, and ROUGE-L.
- Consistency improvements generalize to unseen datasets, indicating robust gains.
- General task performance is largely retained after fine-tuning with CoG-generated data.

## Why This Works (Mechanism)
CoG improves consistency by explicitly guiding the LLM to produce semantically aligned paraphrases and answers, then using these as training data for fine-tuning. The multi-step prompting process ensures that only high-quality, consistent pairs are retained, reducing the noise and variability typical of standard training data. By focusing on semantic alignment at the data generation stage, CoG instills more reliable and trustworthy behavior in the fine-tuned models.

## Foundational Learning
- **Semantic consistency:** The degree to which different outputs for the same or related inputs convey the same meaning. Needed to ensure trustworthiness and reliability in LLM outputs; quick check: compare entailment and paraphrase scores across model generations.
- **Synthetic data generation:** Creation of artificial training examples via LLM prompting, used to supplement or replace real data. Needed to scale training data and target specific behaviors; quick check: evaluate quality and diversity of generated pairs.
- **Fine-tuning with curated datasets:** Adapting a pre-trained LLM to a specific task or behavior using carefully selected data. Needed to transfer consistency improvements from the guiding LLM to the target model; quick check: measure performance and consistency post-fine-tuning.
- **Paraphrasing and answer selection:** Generating multiple rephrasings and candidate answers, then selecting the most semantically consistent. Needed to surface and reinforce reliable outputs; quick check: verify selection criteria and ranking logic.

## Architecture Onboarding

**Component map:** Guiding LLM -> Paraphrasing/Answer Generation -> Consistency Filtering -> Synthetic Data -> Fine-tuning Target LLM

**Critical path:** The guiding LLM generates paraphrases and answers, which are filtered for consistency and used to fine-tune the target LLM. This path directly determines the quality and impact of the consistency improvements.

**Design tradeoffs:** Using a single guiding LLM introduces potential bias and limits generalization; however, it simplifies the pipeline and reduces computational overhead. Multi-step prompting increases reliability but adds latency and complexity.

**Failure signatures:** Inconsistent or low-quality synthetic data leads to minimal or no consistency gains. Over-reliance on automated metrics may miss nuanced semantic drift. Fine-tuning on biased data can propagate undesirable behaviors.

**3 first experiments:**
1. Compare consistency metrics (entailment, paraphrase detection, ROUGE-L) before and after CoG fine-tuning on a held-out QA dataset.
2. Evaluate the impact of varying the number of paraphrases and candidate answers on consistency gains.
3. Test generalization by applying CoG-tuned models to unseen QA datasets and measuring consistency retention.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to closed-book QA; performance on open-domain or multi-step reasoning tasks is untested.
- Synthetic data generation depends on a single guiding LLM, which may introduce bias.
- Consistency improvements are measured using automated metrics, which may not fully capture nuanced semantic drift or hallucination.
- Computational overhead of multi-step prompting and fine-tuning may limit scalability for very large models or latency-sensitive applications.

## Confidence
- **Consistency improvement via CoG:** High
- **Generalization to unseen datasets:** Medium
- **Retention of general task performance:** Medium

## Next Checks
1. Evaluate CoG-tuned models on open-domain QA and multi-step reasoning benchmarks to assess cross-task consistency.
2. Conduct human evaluations to verify that semantic consistency improvements align with perceived reliability and trustworthiness.
3. Test robustness by varying the guiding LLM and introducing adversarial or ambiguous inputs to probe for bias or failure modes.