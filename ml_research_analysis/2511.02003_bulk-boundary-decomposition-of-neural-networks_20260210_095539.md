---
ver: rpa2
title: Bulk-boundary decomposition of neural networks
arxiv_id: '2511.02003'
source_url: https://arxiv.org/abs/2511.02003
tags:
- neural
- networks
- deep
- network
- boundary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a bulk-boundary decomposition (BBD) framework
  for analyzing the training dynamics of deep neural networks. The key insight is
  to reorganize the Lagrangian of stochastic gradient descent into two parts: a data-independent
  bulk term, governed by network architecture and activation functions, and a data-dependent
  boundary term, encoding interactions from training samples at input and output layers.'
---

# Bulk-boundary decomposition of neural networks

## Quick Facts
- arXiv ID: 2511.02003
- Source URL: https://arxiv.org/abs/2511.02003
- Reference count: 0
- Primary result: Introduces a theoretical framework separating architectural (bulk) from data-driven (boundary) dynamics in neural network training

## Executive Summary
This paper presents a bulk-boundary decomposition (BBD) framework that reorganizes the Lagrangian of stochastic gradient descent into data-independent bulk terms (driven by architecture) and data-dependent boundary terms (encoding sample interactions). This separation makes explicit the local and homogeneous structure of deep networks, bridging deep learning dynamics with statistical physics and field theory. The authors develop a field-theoretic formulation treating network depth as an effective spatial coordinate, deriving actions that capture both bulk and boundary contributions for locally-connected architectures.

## Method Summary
The framework reorganizes SGD dynamics through a variable substitution promoting pre-activations to fundamental degrees of freedom, yielding a Lagrangian decomposition into bulk (architecture-driven) and boundary (data-driven) terms. A field-theoretic formulation treats depth as an effective spatial coordinate, deriving continuum limit actions for locally-connected networks. The bulk term captures inherent dynamics from network architecture and activation functions, while the boundary term encodes interactions from training samples at input/output layers.

## Key Results
- Successfully separates data-independent bulk dynamics from data-dependent boundary contributions in neural network training
- Derives field-theoretic actions for locally-connected architectures that capture both bulk and boundary effects
- Demonstrates how local interactions along depth can lead to long-range order phenomena

## Why This Works (Mechanism)
The decomposition works by reorganizing the Lagrangian to expose the intrinsic structure of deep networks. The bulk term captures the homogeneous, architecture-driven dynamics that exist regardless of training data, while the boundary term encodes all data-dependent interactions. This separation makes explicit how network depth creates an effective spatial dimension where local connectivity can generate long-range correlations.

## Foundational Learning
- Lagrangian mechanics in neural network training: Needed to understand the variational formulation of SGD dynamics; Quick check: Verify action extremization recovers standard backpropagation equations
- Variable substitution in neural networks: Required for promoting pre-activations to fundamental variables; Quick check: Confirm reconstructed biases satisfy the substitution relation within numerical tolerance
- Field-theoretic continuum limit: Essential for deriving effective actions from discrete networks; Quick check: Compare correlation functions from microscopic dynamics vs. field theory predictions

## Architecture Onboarding

**Component map:** SGD dynamics → Lagrangian formulation → Variable substitution → Bulk-boundary decomposition → Field-theoretic action

**Critical path:** The core computation involves applying the variable substitution (b^(m)_i = z^(m+1)_i - Σ_j W^(m)_ij σ(z^(m)_j)) to reorganize the Lagrangian, followed by continuum limit derivation for locally-connected architectures.

**Design tradeoffs:** Local connectivity enables field-theoretic treatment but excludes fully-connected or attention-based networks; continuum approximation simplifies analysis but may lose finite-width effects.

**Failure signatures:** Non-locality in width direction violates field theory assumptions; incorrect Jacobian handling leads to wrong action formulation; continuum limit fails for finite-width networks.

**First experiments:**
1. Verify Lagrangian decomposition algebra for simple two-layer network
2. Compare bulk vs boundary contributions in SGD trajectories for small CNN
3. Test continuum limit convergence for increasing network widths

## Open Questions the Paper Calls Out

**Open Question 1:** Does emergent long-range order in the bulk Lagrangian correlate with successful training convergence and serve as a predictive diagnostic for generalization? The field-theoretic formulation is derived but not applied to analyze actual training trajectories or establish quantitative correlations with task performance.

**Open Question 2:** Can statistical-mechanical treatment of boundary stochasticity reproduce observed generalization behavior through effective thermal ensembles? The boundary term encodes data-driven stochasticity, but the connection to generalization via statistical mechanics is proposed but not developed.

**Open Question 3:** How does the BBD framework extend to architectures without local width-direction connectivity, such as fully connected or attention-based networks? The field theory derivation explicitly assumes local connectivity, noting that fully connected architectures render the system nonlocal in width.

## Limitations
- Assumes small width limits and continuum approximations that may not hold for finite-width architectures
- Field-theoretic formulation requires careful numerical discretization with unproven convergence properties
- Scale parameter γ = 1/η appears ad hoc without rigorous physical justification

## Confidence
- High confidence in algebraic correctness of Lagrangian decomposition (Eqs. 5-7)
- Medium confidence in theoretical framework's applicability to local architectures
- Low confidence in practical utility of field-theoretic formulation without empirical validation

## Next Checks
1. Verify Jacobian determinant computation for z-for-b variable substitution and its impact on action formulation
2. Implement finite-width corrections to test validity of continuum limit assumptions across different network scales
3. Compare predicted training trajectories from bulk-boundary decomposition against full backpropagation on standard architectures (MLP, CNN) for simple classification tasks