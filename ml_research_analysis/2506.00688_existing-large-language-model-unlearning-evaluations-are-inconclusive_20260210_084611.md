---
ver: rpa2
title: Existing Large Language Model Unlearning Evaluations Are Inconclusive
arxiv_id: '2506.00688'
source_url: https://arxiv.org/abs/2506.00688
tags:
- unlearning
- arxiv
- evaluations
- information
- unlearned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies critical shortcomings in existing adversarial
  unlearning evaluation methods for large language models, showing they often inject
  new information or depend on task-specific formats, leading to inconclusive results.
  The authors propose two key principles for future evaluations: minimal information
  injection and downstream task awareness.'
---

# Existing Large Language Model Unlearning Evaluations Are Inconclusive

## Quick Facts
- arXiv ID: 2506.00688
- Source URL: https://arxiv.org/abs/2506.00688
- Reference count: 40
- Primary result: Standard unlearning evaluations can inject information or depend on task-specific formats, making their results unreliable.

## Executive Summary
This paper identifies critical flaws in existing adversarial evaluation methods for LLM unlearning. The authors demonstrate that finetuning attacks and input-space attacks (like Enhanced GCG) can inject information comparable to the knowledge being tested for removal, while memorization detectors yield inconsistent results across different output formats. Through systematic experiments on benchmarks like TOFU and WMDP-Bio, they show that unlearning conclusions vary drastically depending on whether evaluations use multiple-choice letter probability, vocabulary probability, or open-ended generation tasks. The paper proposes two key principles for future evaluations: minimal information injection and downstream task awareness, and recommends using cross-format leakage matrices and memorization detectors as stricter benchmarks.

## Method Summary
The authors evaluate three types of adversarial unlearning attacks (finetuning, Enhanced GCG input-space, and ACR memorization detection) across multiple LLM architectures (Zephyr-7B, Phi-1.5, Llama-3.2-1B-Instruct) and datasets (TOFU, WMDP-Bio, WMDP-Chem, WMDP-Cyber). They systematically test for information injection by comparing the bits available in optimization variables to the bits needed to encode target knowledge, and examine spurious correlations by testing whether finetuning on retain sets improves performance on held-out forget sets. Cross-format consistency is evaluated by running the same unlearned models on identical knowledge using different task formats (CHOOSE, OPTION, GENERATE), and memorization is quantified using Adversarial Compression Ratio (ACR) optimization.

## Key Results
- Finetuning attacks on unlearned models can recover base model performance when using maximum letter probability generation, but not maximum text probability generation, showing format-dependent effectiveness
- Enhanced GCG prefix optimization can encode sufficient information to represent answers for hundreds of MCQ questions, potentially injecting knowledge rather than eliciting it
- ACR memorization detection shows inconsistent results across task formats, with success rates varying from 11.1% to 90% for the same unlearned model depending on whether using CHOOSE, OPTION, or GENERATE tasks

## Why This Works (Mechanism)

### Mechanism 1
Optimization-based attacks (finetuning or prompt optimization) allocate bits within model weights or prompt tokens that encode target knowledge. Enhanced GCG with 100 tokens and vocabulary ~32,000 can encode ~1500 bits, sufficient to represent answers for ~1300 MCQ questions at 55% accuracy (~1430 bits). This mechanism shows how adversarial evaluations can reintroduce the very knowledge they're supposed to test for removal.

### Mechanism 2
Maximum letter probability evaluation only requires identifying which of four tokens has highest probability, while maximum text probability requires generating correct answer text. These probe different internal representations and are differentially affected by unlearning methods, causing conclusions to vary drastically across task formats. For example, RMU decreases success rate while NPO increases it for GENERATE tasks.

### Mechanism 3
Finetuning on retain data improves accuracy on held-out forget set even when models never saw forget data, because benchmark datasets contain implicit patterns (e.g., TOFU-MCQ formatting conventions) that transfer across fictional author entries. This spurious correlation invalidates evaluations that assume retain and forget sets are independent.

## Foundational Learning

- Concept: **Machine Unlearning Paradigms** (exact vs approximate vs heuristic)
  - Why needed here: The paper critiques evaluations primarily for heuristic unlearning, where success is defined by refusal to generate rather than provable removal. Understanding which paradigm an evaluation targets is essential for interpreting results.
  - Quick check question: Does this evaluation claim to prove information is mathematically removed (exact/approximate) or merely that the model refuses to generate it (heuristic)?

- Concept: **Adversarial Compression Ratio (ACR)**
  - Why needed here: ACR quantifies memorization by finding minimum prompt length needed to generate target text (|y|/|x*|). Used as memorization detector that provides stricter criterion than generation tests.
  - Quick check question: If ACR remains unchanged after unlearning, what does that imply about whether information was removed from model weights?

- Concept: **Information-Theoretic Capacity Bounds**
  - Why needed here: Core to quantifying injection budgets—determining whether optimization variables (tokens, weights) have sufficient bits to encode target knowledge.
  - Quick check question: Given a 50-token prefix and vocabulary of 50k, what's the maximum bits of information that could theoretically be encoded?

## Architecture Onboarding

- Component map:
  Unlearning algorithms (RMU, NPO) -> Evaluation attacks (Finetuning, Enhanced GCG, ACR) -> Data formats (MCQ letter/text probability, open-ended generation) -> Metrics (Accuracy, success rate, ACR)

- Critical path:
  1. Identify forget set (WMDP-Bio, TOFU) and retain set
  2. Apply unlearning algorithm to produce M_U
  3. Select evaluation attack type based on threat model
  4. Choose task format(s) for evaluation
  5. Compute injection budget (bits available vs bits needed)
  6. Run cross-format leakage matrix (minimum 3 task types)
  7. Compare against memorization detector (ACR) as strict baseline

- Design tradeoffs:
  - Finetuning attacks: Strongest upper bound on recoverability but highest injection risk and requires forget set knowledge for format matching
  - Input-space attacks: No weight modification but prompt can encode substantial information; format-dependent success
  - ACR memorization detector: Strictest criterion (positive result = definite failure) but negative result is inconclusive

- Failure signatures:
  - MCQ accuracy recovers but open-ended generation doesn't → format-dependent unlearning, likely shallow
  - Retain-set-only finetuning recovers forget-set accuracy → spurious correlation in dataset
  - ACR unchanged after unlearning → information still memorized in weights despite generation refusal
  - Enhanced GCG prefix length approaches target length → prefix is encoding answer, not eliciting knowledge

- First 3 experiments:
  1. Baseline injection test: Finetune base model (never saw forget set) on retain set, measure forget-set accuracy. If accuracy increases, dataset has spurious correlations invalidating evaluation.
  2. Cross-format consistency check: Run same unlearned model on identical knowledge using CHOOSE (letter probability), OPTION (vocabulary probability), and GENERATE (open-ended). Report cross-format leakage matrix; if conclusions differ, evaluation is inconclusive.
  3. Injection budget disclosure: For Enhanced GCG, compute (prefix tokens × log vocab size) vs (test set size × log answer space × accuracy). If ratio approaches 1.0, evaluation is confounded by injection.

## Open Questions the Paper Calls Out

### Open Question 1
How can the field develop rigorous, non-heuristic metrics to quantify the "injection budget" of information introduced during adversarial evaluations?
The authors state current measures are "relatively heuristic" and call for future work to "seek to measure of injected information, such as... tools in the information theoretic literature." They demonstrate evaluations inject info but rely on coarse estimates rather than precise quantification.

### Open Question 2
Do the spurious correlations identified in the TOFU benchmark exist in other standard unlearning datasets?
The paper acknowledges a "limited experimental scope" focusing on "academic style benchmark datasets," while Section 4.1 provides evidence of spurious generalization in TOFU. It remains unknown if this is a systemic issue across the field's other benchmarks.

### Open Question 3
Can we design an adversarial evaluation method that satisfies the "Minimal Information Injection" principle while still serving as a strong upper bound for knowledge recovery?
The authors propose "Minimal Information Injection" as a key principle but note current strong attacks (finetuning, GCG) likely inject information. They do not propose a specific new attack that resolves this trade-off.

### Open Question 4
How can memorization metrics like Adversarial Compression Ratio (ACR) be standardized to provide consistent results across different output formats?
Section 5.1 recommends using memorization detectors as "yard-sticks," but Section 4.3 shows ACR results vary drastically depending on whether the task is "CHOOSE," "OPTION," or "GENERATE," undermining its utility as a standard benchmark.

## Limitations

- The paper's conclusions are limited to specific benchmark datasets (TOFU, WMDP) and may not generalize to all unlearning scenarios or real-world applications
- Quantification of "minimal injection" remains heuristic rather than rigorously bounded, with bit-counting analysis providing useful framework but no definitive thresholds
- Memorization detector (ACR) sensitivity to different unlearning algorithms and knowledge types is not fully characterized, potentially trading false positives for false negatives

## Confidence

**High Confidence**:
- Information injection is possible in optimization-based attacks when the injection budget approaches or exceeds the knowledge capacity needed
- Cross-format evaluation reveals significant discrepancies in unlearning effectiveness measurements
- Benchmark datasets contain spurious correlations that enable generalization during finetuning

**Medium Confidence**:
- Current adversarial evaluations are generally inconclusive rather than uniformly flawed
- The proposed principles (minimal injection, downstream task awareness) will improve evaluation reliability
- Memorization detectors provide more rigorous evaluation criteria than generation-based tests

**Low Confidence**:
- The specific bit-counting thresholds proposed will apply universally across different model sizes and architectures
- The identified limitations affect all existing unlearning evaluations equally
- The cross-format leakage matrix approach will resolve all format sensitivity issues

## Next Checks

1. **Injection Budget Verification**: For each attack type (finetuning, Enhanced GCG), compute the ratio of actual injection (measured via information-theoretic bounds) to target knowledge size across multiple knowledge types and model scales. This would establish whether the paper's bit-counting analysis accurately predicts when injection becomes problematic.

2. **Cross-Dataset Spurious Correlation Analysis**: Apply the spurious correlation test (retain-set-only finetuning on held-out forget data) to a diverse set of unlearning benchmarks beyond TOFU and WMDP, including both synthetic and real-world datasets. This would determine whether the identified problem is dataset-specific or represents a fundamental limitation of current evaluation protocols.

3. **Memorization Detector Sensitivity Study**: Systematically vary ACR optimization parameters (token budget, learning rate, optimization steps) across different knowledge types and unlearning algorithms to characterize the detector's sensitivity and false negative rates. This would establish whether ACR provides a truly stricter criterion or simply trades false positives for false negatives.