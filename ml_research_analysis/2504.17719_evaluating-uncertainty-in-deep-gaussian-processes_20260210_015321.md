---
ver: rpa2
title: Evaluating Uncertainty in Deep Gaussian Processes
arxiv_id: '2504.17719'
source_url: https://arxiv.org/abs/2504.17719
tags:
- deep
- calibration
- uncertainty
- learning
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluated uncertainty quantification methods\u2014Deep\
  \ Gaussian Processes (DGPs), Deep Sigma Point Processes (DSPPs), and Deep Ensembles\u2014\
  on regression (CASP) and classification (ESR) tasks. Models were assessed using\
  \ MAE, accuracy, Negative Log-Likelihood (NLL), and Expected Calibration Error (ECE),\
  \ both in-distribution and under synthetic feature-level distribution shifts."
---

# Evaluating Uncertainty in Deep Gaussian Processes
## Quick Facts
- arXiv ID: 2504.17719
- Source URL: https://arxiv.org/abs/2504.17719
- Reference count: 37
- Primary result: DSPPs showed superior in-distribution calibration; Deep Ensembles achieved best accuracy and robustness under distribution shift

## Executive Summary
This work evaluates uncertainty quantification methods—Deep Gaussian Processes (DGPs), Deep Sigma Point Processes (DSPPs), and Deep Ensembles—on regression (CASP) and classification (ESR) tasks. Models are assessed using MAE, accuracy, Negative Log-Likelihood (NLL), and Expected Calibration Error (ECE), both in-distribution and under synthetic feature-level distribution shifts. DSPPs demonstrated superior in-distribution calibration with the lowest ECE, leveraging deterministic sigma point approximations for uncertainty propagation. Deep Ensembles achieved the best overall accuracy and robustness under distribution shift, maintaining stable performance and calibration across increasing perturbation severities. DGPs showed particular sensitivity to shifts, especially in regression tasks.

## Method Summary
The study compares three uncertainty quantification approaches: DGPs, DSPPs, and Deep Ensembles across regression and classification benchmarks. Evaluation metrics include MAE, accuracy, NLL, and ECE. Synthetic feature-level distribution shifts are applied to test robustness. DSPPs use deterministic sigma point approximations for uncertainty propagation, while Deep Ensembles average predictions from multiple independently trained models. DGPs rely on GP-based uncertainty propagation through deep architectures.

## Key Results
- DSPPs achieved lowest Expected Calibration Error (ECE) in in-distribution settings
- Deep Ensembles maintained best accuracy and calibration stability under synthetic distribution shifts
- DGPs showed high sensitivity to feature-level distribution shifts, particularly in regression tasks

## Why This Works (Mechanism)
The effectiveness of each method stems from their distinct uncertainty propagation mechanisms. DSPPs use deterministic sigma point approximations that capture uncertainty deterministically through the network, enabling better calibration in-distribution. Deep Ensembles leverage model diversity through independent training, providing robust predictions that maintain calibration under distributional shifts. DGPs propagate uncertainty through GP layers but are more sensitive to shifts due to their parametric assumptions.

## Foundational Learning
1. **Uncertainty quantification in deep learning** - Why needed: Essential for reliable predictions in safety-critical applications; Quick check: Can the model provide both predictions and uncertainty estimates?
2. **Distributional shift detection** - Why needed: Real-world data often deviates from training distribution; Quick check: How does model performance degrade as data shifts increase?
3. **Calibration metrics (ECE, NLL)** - Why needed: Measures whether predicted uncertainties match empirical accuracy; Quick check: Does predicted uncertainty correlate with actual error rates?

## Architecture Onboarding
**Component Map**: Input -> DGP/DSPP/Ensemble -> Uncertainty Propagation -> Output
**Critical Path**: Data → Model Architecture → Uncertainty Propagation Method → Evaluation Metrics → Results
**Design Tradeoffs**: Calibration vs robustness (DSPPs excel in calibration but ensembles are more robust), computational efficiency (ensembles require multiple model trainings), parametric assumptions (DGPs rely on GP assumptions)
**Failure Signatures**: DGPs: Degraded performance under shifts; DSPPs: Potential overconfidence in shifted regions; Ensembles: Computational overhead, potential underconfidence
**First Experiments**: 1) Baseline in-distribution performance comparison; 2) Single-step distribution shift application; 3) Calibration metric analysis on perturbed data

## Open Questions the Paper Calls Out
None

## Limitations
- Limited number of datasets tested may affect generalizability
- Synthetic distribution shifts may not capture real-world drift patterns
- Computational efficiency differences between methods not explored

## Confidence
- DSPPs' calibration superiority: High
- Deep Ensembles' robustness: High
- DGP sensitivity findings: Medium

## Next Checks
1. Test across additional real-world datasets with naturally occurring distribution shifts
2. Evaluate computational trade-offs including training time, inference latency, and memory requirements
3. Investigate impact of different DGP architectures and hyperparameters on shift sensitivity