---
ver: rpa2
title: 'AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities
  of Large Language Models'
arxiv_id: '2502.16906'
source_url: https://arxiv.org/abs/2502.16906
tags:
- inputs
- constraint
- return
- constraints
- arrangement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoLogi introduces an automated method for generating open-ended
  logic puzzles to evaluate Large Language Models' reasoning abilities. The approach
  uses program-based verification and controllable difficulty levels to create a bilingual
  benchmark that mitigates random guessing and provides more reliable assessment than
  traditional multiple-choice formats.
---

# AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models

## Quick Facts
- arXiv ID: 2502.16906
- Source URL: https://arxiv.org/abs/2502.16906
- Reference count: 40
- Primary result: AutoLogi generates open-ended logic puzzles with program-based verification, achieving 35-73% model performance spread vs 21-37% for source multiple-choice datasets

## Executive Summary
AutoLogi introduces an automated method for generating open-ended logic puzzles to evaluate Large Language Models' reasoning abilities. The approach uses program-based verification and controllable difficulty levels to create a bilingual benchmark that mitigates random guessing and provides more reliable assessment than traditional multiple-choice formats. Experiments with eight modern LLMs show AutoLogi achieves a wider performance range (35% to 73%) compared to source datasets (21% to 37%), better reflecting true model capabilities. The method also generates high-quality training data through rejection sampling with verifiers, improving model performance on independent reasoning benchmarks by 4-7%.

## Method Summary
AutoLogi is a three-stage pipeline that transforms multiple-choice reasoning datasets into open-ended logic puzzles with program-based verification. Stage 1 extracts puzzle structure from source corpora using LLM prompting. Stage 2 generates JSON format requirements, Python verifiers, and traversal functions with cross-validation to ensure solvability. Stage 3 augments puzzles through constraint reduction (easier variants) and expansion (harder variants) while maintaining non-empty solution spaces. The pipeline produces bilingual benchmarks and training data via rejection sampling with verifiers filtering correct/incorrect model responses.

## Key Results
- Performance spread of 35-73% across eight LLMs on AutoLogi vs 21-37% on source multiple-choice datasets
- Program-based verification achieves F1 = 0.96 vs 0.76 for LLM-as-judge evaluation
- Training data from AutoLogi improves Qwen2.5-72b performance by 7.05% on AR-LSAT and 6.13% on LiveBench
- 30% of LLM-generated constraints create unsolvable puzzles without traversal validation
- 3% verifier error rate even with cross-validation framework

## Why This Works (Mechanism)

### Mechanism 1: Open-Ended Generation Forces Genuine Reasoning
Open-ended format eliminates shortcut strategies by forcing models to construct complete solutions from scratch, significantly mitigating performance inflation from random guessing strategies like elimination or positional bias.

### Mechanism 2: Program-Based Verification Provides Objective Ground Truth
Deterministic verifier functions check constraint satisfaction objectively, while traversal functions enumerate all valid solutions. Cross-validation between verifiers and traversal catches generation errors and ensures valid puzzles have non-empty solution spaces.

### Mechanism 3: Constraint Density Controls Difficulty Scaling
Systematic constraint addition/removal enables controllable difficulty gradients. More constraints reduce solution space size exponentially, increasing search complexity. Reduction creates easier variants while expansion creates harder variants validated by traversal.

### Mechanism 4: Rejection Sampling with Verifiers Generates Clean Training Signals
Program-based verification enables higher-quality training data than multiple-choice datasets that may reward flawed reasoning paths. Verifiers filter model responses into verified-correct (for SFT) and correct-incorrect pairs (for DPO), ensuring only genuinely valid solutions receive positive training signal.

## Foundational Learning

### Concept: Constraint Satisfaction Problems (CSP)
Logic puzzles are CSPs with finite domains, constraints defining valid assignments, and potentially multiple solutions. Understanding backtracking, constraint propagation, and solution space structure is essential for interpreting difficulty metrics and traversal functions.

### Concept: Program Verification and Symbolic Execution
Verifier functions are assertion-based checks that must correctly encode puzzle constraints. Understanding how to write, test, and validate verification code—and why cross-validation between verifiers and traversal is necessary—prevents subtle bugs that corrupt evaluation/training data.

### Concept: Rejection Sampling for RLHF/DPO
The training data generation uses rejection sampling to create preference pairs. Understanding how verifier-filtered data differs from human-annotated preferences—and the tradeoffs between SFT data (correct examples) and DPO data (preference pairs)—is critical for effective use.

## Architecture Onboarding

### Component Map
Stage 1: Puzzle Formulation -> Input: Source corpus (e.g., LogiQA, AR-LSAT) -> LLM: GPT-4 with one-shot prompting -> Output: {background, logical_constraints}
Stage 2: Verification & Format Generation -> Format Requirement: JSON schema + example -> Verifiers: format_verifier + constraint_verifier (Python functions) -> Traversal Function: Enumerates solution space -> Cross-validation: Traversal <-> Verifier mutual check
Stage 3: Data Augmentation -> Reduction: Remove constraints -> easier puzzles -> Expansion: LLM generates new constraints -> harder puzzles -> Validation: Traversal verifies non-empty solution space
Training Data Pipeline -> Rejection Sampling: N responses per prompt (paper uses N=8) -> Verifier Filter: Correct vs incorrect classification -> Output: D_sft (correct only) + D_dpo (preference pairs)

### Critical Path
1. Puzzle quality hinges on Stage 2 cross-validation—without it, 23% of generated puzzles are unsolvable
2. Training data quality hinges on verifier accuracy—3% error rate introduces noise
3. Difficulty scaling hinges on expansion validation—30% of LLM-generated constraints create unsolvable puzzles

### Design Tradeoffs
- Traversal complexity vs. verification accuracy: Traversal exhaustively searches solution space but is computationally expensive for large domains
- LLM generation vs. rule-based templates: LLM provides linguistic diversity but introduces errors
- Strict vs. lenient verification: Paper uses strict (binary correct/incorrect). Alternative: partial credit for satisfying subset of constraints (not explored)

### Failure Signatures
- Empty solution space after augmentation: Constraint added by LLM is too restrictive → regenerate or reduce constraint count
- Verifier accepts malformed inputs: Format verifier has bugs → add more unit tests
- Model generates correct arrangement in wrong format: Format specification unclear → improve format_example clarity
- Performance plateau despite training: Verifier is incorrect, teaching wrong patterns → audit verifier logic manually

### First 3 Experiments
1. Baseline verification accuracy test: Take 50-100 puzzles with known ground-truth solutions, run verifier on model outputs, manually verify correctness. Target: F1 > 0.95
2. Difficulty scaling validation: Generate 20 puzzles, apply 5 rounds of expansion, measure solution space size per round and model accuracy degradation
3. Training data ablation: Train two models—(a) with verifier-filtered SFT data, (b) with unfiltered SFT data. Evaluate on held-out AutoLogi + independent benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Can the verification function accuracy be improved beyond the current ~97% accuracy, particularly for puzzles with complex or interacting constraints? The authors note that cross-validation doesn't guarantee perfect verification functions, with about 3% of validation results containing errors.

### Open Question 2
How sensitive is the puzzle generation quality to the capability level of the LLM used for generating verifiers and traversal functions? The authors identify this as a limitation, noting that limited-capacity LLMs may cause high failure rates in synthesizing questions.

### Open Question 3
To what extent do the cross-lingual performance differences (particularly LLaMA's poor Chinese performance) reflect reasoning capabilities versus language-specific processing limitations? The paper observes significant performance gaps but doesn't control for comprehension differences across languages.

### Open Question 4
Do the training improvements generalize to model architectures and families beyond Qwen? All training experiments use Qwen models, leaving open whether synthetic data produces similar improvements for LLaMA, GPT, or Claude models.

## Limitations
- Heavy dependence on specific LLM APIs (GPT-4, GPT-4o) creates reproducibility challenges and potential quality degradation with model updates
- 3% verifier error rate and 30% failure rate for LLM-generated constraints represent significant quality concerns
- Training data generation assumes verifier correctness correlates with reasoning quality, but verifiers may accept valid solutions derived through flawed reasoning patterns

## Confidence

**High Confidence (Corpus-Supported):**
- Open-ended format reduces guessing inflation compared to multiple-choice (supported by performance spread data)
- Program-based verification outperforms LLM-as-judge for objective constraint checking (F1 = 0.96 vs 0.76)
- Rejection sampling with verifiers produces higher-quality training data (4-7% improvement on independent benchmarks)

**Medium Confidence (Limited Direct Evidence):**
- Constraint density reliably controls difficulty scaling (based on 20-puzzle sample)
- Solution space size correlates with human-perceived difficulty (assumed but not directly validated)
- Training signal quality improvement translates to genuine reasoning capability gains (correlation assumed)

**Low Confidence (Speculative or Untested):**
- The approach generalizes to non-grid logic puzzle formats (only grid puzzles tested)
- Verifier strictness doesn't reject valid alternative solutions (cross-validation suggests this but not exhaustively tested)
- Performance improvements reflect reasoning capability rather than pattern matching (possible given template-based generation)

## Next Checks

1. **Verifier Robustness Test**: Run 100 puzzles through the complete pipeline with 3 different LLM providers (GPT-4, Claude, Gemini) and compare verifier accuracy rates. Target: F1 > 0.95 across all providers.

2. **Cross-Format Generalization**: Adapt the pipeline to generate non-grid logic puzzles (e.g., syllogisms, sequence completion) and test whether the same verification accuracy and difficulty scaling hold. Success requires maintaining F1 > 0.90 and monotonic difficulty progression across at least 2 new puzzle types.

3. **Reasoning vs. Memorization Analysis**: Train two models on AutoLogi data—(a) with standard training, (b) with puzzle templates randomized (variable names, constraint order, domain elements). If model (b) shows significantly worse performance (<80% of model a), the approach may be teaching pattern matching rather than genuine reasoning.