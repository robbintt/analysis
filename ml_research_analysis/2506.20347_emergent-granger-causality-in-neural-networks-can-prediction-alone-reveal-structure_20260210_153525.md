---
ver: rpa2
title: 'Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal
  Structure?'
arxiv_id: '2506.20347'
source_url: https://arxiv.org/abs/2506.20347
tags:
- time
- series
- granger
- data
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether deep neural networks can learn
  Granger causality (GC) from multivariate time series data by prediction alone, without
  explicit sparse regression. The authors propose a novel paradigm where GC structure
  is uncovered by comparing the distribution of model residuals under full versus
  reduced models (with/without specific time series components).
---

# Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal Structure?

## Quick Facts
- **arXiv ID**: 2506.20347
- **Source URL**: https://arxiv.org/abs/2506.20347
- **Reference count**: 5
- **Primary result**: Neural networks trained jointly on prediction tasks can discover Granger causality structure by comparing residual distributions, with Transformers showing strongest performance

## Executive Summary
This paper investigates whether deep neural networks can learn Granger causality from multivariate time series data by prediction alone, without explicit sparse regression. The authors propose a novel paradigm where GC structure is uncovered by comparing the distribution of model residuals under full versus reduced models (with/without specific time series components). Using Monte Carlo dropout for uncertainty quantification, they compare the ability of CNN, LSTM, and Transformer architectures to discover GC across several simulation studies and real EEG data from Alzheimer's patients. Their results show that well-regularized models trained on prediction tasks can effectively learn the true GC structure, performing comparably to or better than sparse regression methods.

## Method Summary
The method trains a joint neural network (Transformer, CNN, or LSTM) to predict future time series values from past lags using all available series. Granger causality is discovered post-hoc by comparing residual distributions when each candidate cause is masked at inference. Monte Carlo dropout enables distributional comparison by generating stochastic prediction ensembles. A binary classifier determines whether full and reduced model residuals come from different distributions, with the overlap probability quantifying GC strength. Three training regimes are tested: No ILD (recommended), DP ILD (dual pass), and ILD (input layer dropout, degrades performance).

## Key Results
- Well-regularized models trained on prediction tasks can effectively learn true GC structure without explicit sparse regression priors
- Transformer architectures show most consistent and robust GC discovery performance across varying conditions
- Input layer dropout during training degrades GC learning capability by encouraging spurious correlation learning
- The approach performs comparably to or better than sparse regression methods across VAR, Lorenz-96, Netsim, chain/fork/collider structures, and real EEG data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural networks trained jointly on all time series components can encode Granger causal structure in their prediction residuals, which is recovered post-hoc by comparing error distributions.
- Mechanism: A model trained on full inputs learns to rely on causal parents for prediction. When a true cause is masked at inference (reduced model), prediction error increases. The divergence between full and reduced model residual distributions quantifies causal influence via a trained binary classifier.
- Core assumption: The network learns to depend on genuine causal inputs rather than spurious correlations when properly regularized and trained without input masking.
- Evidence anchors:
  - [abstract] "We propose to uncover the learned GC structure by comparing the model uncertainty or distribution of the residuals when the past of everything is used as compared to the one where a specific time series component is dropped from the model."
  - [section 3.3] "If the distributions do not overlap and if the mean of E_j,i,r is greater then E_j,f then it is an indication that removing the past of time series i degrades the ability of the model to predict the future dynamics of time series j"
  - [corpus] Related work on KAN-based GC inference (KANGCI) similarly extracts structure from learned representations, suggesting convergent validity of post-hoc extraction approaches.
- Break condition: If the model learns redundant representations or compensatory pathways, removing a true cause may not increase error, yielding false negatives.

### Mechanism 2
- Claim: Monte Carlo dropout enables distributional comparison of residuals by generating stochastic prediction ensembles from a single trained model.
- Mechanism: Dropout remains active during inference. Multiple forward passes with fixed input produce varied predictions, yielding a distribution of squared errors. This allows statistical comparison between full and reduced model error distributions rather than relying on point estimates.
- Core assumption: MC dropout approximates Bayesian uncertainty; the variance across passes reflects genuine model uncertainty about predictions.
- Evidence anchors:
  - [abstract] "Using Monte Carlo dropout for uncertainty quantification, they compare the ability of CNN, LSTM, and Transformer architectures..."
  - [section 3.1] "MC Dropout proposed in Gal and Ghahramani (2016) provides a computationally economical way for model uncertainty quantification in neural networks."
  - [corpus] Corpus lacks direct validation of MC dropout specifically for GC; related papers focus on architecture variants rather than uncertainty methods.
- Break condition: If dropout rate is poorly tuned or model is under-regularized, MC samples may not meaningfully reflect uncertainty, producing overlapping distributions regardless of causal structure.

### Mechanism 3
- Claim: Input layer dropout during training degrades Granger causality learning by encouraging the model to encode spurious correlations.
- Mechanism: When true causal inputs are randomly dropped during training, the model learns compensatory mappings from correlated but non-causal series to maintain prediction accuracy. These shortcuts persist at inference, reducing the error gap when true causes are masked.
- Core assumption: The model optimizes for prediction accuracy and will exploit any available correlations; input dropout creates pressure to learn redundant pathways.
- Evidence anchors:
  - [abstract] "Input layer dropout during training degrades GC learning capability."
  - [section 4.1, Table 1] ILD regime shows substantially lower AUCROC (e.g., 0.60 for chain structure) compared to No ILD (1.00).
  - [corpus] Related papers do not investigate input dropout effects; this appears to be a novel finding specific to this work.
- Break condition: If data has no spurious correlations or redundant signals, input dropout may not harm GC learning; however, this scenario is rare in practice.

## Foundational Learning

- Concept: **Granger Causality Definition**
  - Why needed here: The entire method hinges on understanding GC as prediction improvement—if excluding series i degrades prediction of series j, then i Granger-causes j. Without this, the residual comparison logic is opaque.
  - Quick check question: Given two time series A and B, if a model predicts B equally well with or without A's history, does A Granger-cause B?

- Concept: **Monte Carlo Dropout as Bayesian Approximation**
  - Why needed here: The method relies on MC dropout to generate residual distributions. Understanding why dropout at inference produces meaningful uncertainty estimates is essential for debugging and tuning.
  - Quick check question: Why must dropout remain active during inference for uncertainty quantification, rather than being disabled as in standard prediction?

- Concept: **Distribution Comparison via Binary Classifier**
  - Why needed here: The paper uses a classifier to assess whether full and reduced model residuals come from different distributions. This is non-standard; understanding why overlap probability quantifies GC strength is key.
  - Quick check question: If a binary classifier achieves 50% accuracy distinguishing full vs. reduced residuals, what does this imply about Granger causality?

## Architecture Onboarding

- Component map: Input layer (with/without dropout) -> Encoder backbone (Transformer/CNN/LSTM) -> Output layer -> Residual extraction (Q MC passes) -> GC classifier
- Critical path: Train joint model (No ILD) -> Extract residuals via MC dropout (Q passes, fixed seeds) -> For each (i,j) pair, compare full vs. i-masked residuals -> Train classifier to compute overlap probability -> Threshold to obtain binary adjacency matrix
- Design tradeoffs:
  - **Transformer vs. LSTM vs. CNN**: Transformers show most consistent performance across conditions; CNNs may suffer under input dropout due to parameter sharing; LSTMs perform well but variance increases in harder settings.
  - **No ILD vs. DP ILD vs. ILD**: No ILD is recommended—best GC recovery, simplest training. DP ILD adds computational cost with minimal benefit. ILD degrades performance significantly.
  - **Q (number of MC passes)**: Higher Q yields more stable distributions but increases inference cost. Paper does not specify optimal Q.
- Failure signatures:
  - **Near-random AUCROC (~0.5)**: Model may be underfitting or dropout rate is too high/low
  - **High variance across random seeds**: Insufficient training data (cf. Netsim results with T=200)
  - **False positives in dense graphs**: Model learning correlations rather than causation; check input dropout settings
  - **All-zero adjacency matrix**: Threshold too aggressive; examine raw P(i→j) scores before thresholding
- First 3 experiments:
  1. **Reproduce VAR(2) simulation with Transformer, No ILD**: Train on synthetic data with known sparse ground truth; verify AUCROC ≈ 1.0. This validates the pipeline end-to-end.
  2. **Ablate dropout rate in hidden layers**: Test α ∈ {0.1, 0.2, 0.3, 0.5} on VAR data; identify rate that maximizes GC recovery. Assumption: paper used cross-validation but does not report specific values.
  3. **Compare Q ∈ {10, 50, 100} MC passes**: Measure stability of P(i→j) estimates and inference time. Determine minimum Q for stable GC estimates on your data scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical conditions under which a prediction-trained joint neural network is guaranteed to learn the true Granger causal structure?
- Basis in paper: [explicit] The authors state their approach works "under specific settings" and "when there is sufficient training data" but do not characterize these conditions formally.
- Why unresolved: The paper demonstrates empirical success across simulation studies but provides no theoretical guarantees or formal characterization of when the approach succeeds versus fails.
- What evidence would resolve it: A theoretical analysis establishing conditions (e.g., on data generating process, network capacity, regularization strength, sample size) that ensure consistent GC recovery; or systematic failure mode analysis identifying boundary conditions.

### Open Question 2
- Question: Why does input layer dropout during training degrade GC discovery capability, and can this be mitigated?
- Basis in paper: [explicit] The paper observes that "dropout in the input layer hinders the models ability to learn the true Granger causation" and suggests "in the absence of a true Granger causal time series the model may learn a mapping from correlated time series," but this explanation remains speculative.
- Why unresolved: The mechanism is not empirically validated, and the trade-off between regularization benefits and GC learning degradation is not characterized.
- What evidence would resolve it: Controlled experiments isolating the mechanism (e.g., analyzing learned representations, probing spurious correlation learning) or modified training procedures that preserve regularization while maintaining GC discovery.

### Open Question 3
- Question: How does the approach scale to high-dimensional systems (e.g., hundreds of time series)?
- Basis in paper: [inferred] The simulation studies test at most 20 dimensions (Lorenz-96), and the EEG case study uses 19 channels. The authors note component-wise sparse methods "become quickly computationally expensive if the number of time series grows," but do not evaluate joint model scalability.
- Why unresolved: Computational complexity and GC discovery performance in high-dimensional settings remain untested.
- What evidence would resolve it: Experiments on systems with 50-500+ time series components, measuring both computational cost and GC recovery accuracy relative to baseline methods.

## Limitations

- Hyperparameter sensitivity is not thoroughly explored; specific dropout rates, learning rates, and MC pass counts are not reported, limiting reproducibility
- Binary classifier implementation details are underspecified (type, training procedure, sample size)
- Input layer dropout's negative effect is demonstrated but theoretical justification for why it specifically degrades GC learning is limited
- The method assumes MC dropout provides meaningful uncertainty estimates, but this is not directly validated for the GC discovery task

## Confidence

- **High**: Granger causality can be learned from prediction tasks without explicit sparse regression (demonstrated across multiple simulation and real datasets)
- **Medium**: Transformer architectures show superior and more robust GC discovery compared to CNN/LSTM across varying conditions
- **Low**: Input layer dropout consistently degrades GC learning due to spurious correlation encoding (mechanism proposed but not rigorously proven)

## Next Checks

1. Replicate the VAR(2) simulation with Transformer, No-ILD regime and verify AUCROC ≈ 1.0 on known sparse ground truth
2. Perform dropout rate ablation study on VAR data to identify optimal hidden layer dropout α
3. Compare MC pass counts (Q=10, 50, 100) on VAR data to determine minimum stable Q for GC estimation