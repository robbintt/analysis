---
ver: rpa2
title: Stable diffusion models reveal a persisting human and AI gap in visual creativity
arxiv_id: '2511.16814'
source_url: https://arxiv.org/abs/2511.16814
tags:
- human
- creativity
- genai
- creative
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study compared visual creativity between humans (artists and\
  \ non-artists) and AI (Stable Diffusion) in generating images from abstract stimuli.\
  \ Using the Test of Creative Imagery Abilities (TCIA), researchers found a clear\
  \ creativity gradient: visual artists non-artists \u2265 human-guided AI self-guided\
  \ AI."
---

# Stable diffusion models reveal a persisting human and AI gap in visual creativity

## Quick Facts
- arXiv ID: 2511.16814
- Source URL: https://arxiv.org/abs/2511.16814
- Reference count: 0
- Primary result: Visual artists outperformed both non-artists and AI, with human-guided AI reaching near-human levels but still falling short of expert performance

## Executive Summary
This study systematically compared visual creativity between humans (visual artists and non-artists) and AI (Stable Diffusion XL) using the Test of Creative Imagery Abilities. Researchers found a clear hierarchy: visual artists generated the most creative images, followed by non-artists, human-guided AI, and finally self-guided AI. Human raters consistently scored human-generated images as more creative than AI-generated ones, though human guidance significantly improved AI performance to approach non-expert human levels. GPT-4o evaluations showed different patterns, rating AI images more favorably than human raters did, suggesting fundamental differences in how AI and humans evaluate visual creativity.

## Method Summary
The study used the Test of Creative Imagery Abilities (TCIA) with 12 abstract stimuli to generate 1,000 images across four groups: visual artists (n=27), non-artists (n=26), human-guided AI (n=230), and self-guided AI (n=222). SDXL Base 1.0 was fine-tuned with LoRA on human drawings, using ControlNet with TCIA stimuli as structural guidance. Human-guided AI received elaborated prompts based on human responses while self-guided AI used only the abstract stimuli. Images were rated by 255 human participants on five dimensions (Liking, Vividness, Originality, Aesthetics, Curiosity) and by GPT-4o via API, with creativity scores derived through factor analysis.

## Key Results
- Visual artists significantly outperformed all other groups in creativity scores
- Non-artists and human-guided AI showed no significant difference in creativity
- Self-guided AI performed significantly worse than both human groups
- GPT-4o ratings showed less discrimination and favored AI outputs compared to human ratings
- Multi-group CFA confirmed human and GPT-4o raters use different evaluation constructs

## Why This Works (Mechanism)

### Mechanism 1
Human guidance via prompt elaboration elevates GenAI creative output to non-expert human levels. Abstract visual stimuli alone provide insufficient framing for diffusion models; adding concrete human-generated ideas to prompts supplies contextual anchors that simulate the associative process humans perform spontaneously. Core assumption: The TCIA abstract stimulus triggers spontaneous imagery generation in humans through embodied perception and memory, which GenAI lacks without explicit semantic injection. Evidence: HI-GenAI matched Non-Artists (p = .198), but if HI-GenAI matched Visual Artists, the mechanism would suggest guidance can substitute for expertise.

### Mechanism 2
Visual creativity imposes perceptual-contextual demands that language-model training does not transfer. Visual Mental Imagery engages neural networks from early visual areas to episodic memory; diffusion models trained on pixel-statistics lack embodied perceptual grounding and real-world contextual sensitivity. Core assumption: The gap stems from fundamental architectural differences—GenAI operates on probabilistic associations from training data without interactive environmental experience. Evidence: GenAI faces unique challenges in visual domains where creativity depends on perceptual nuance and contextual sensitivity—distinctly human capacities not readily transferable from language models. Break condition: If multimodal models with embodied training close the gap, the transfer limitation hypothesis weakens.

### Mechanism 3
GPT-4o evaluates visual creativity using different latent constructs than humans, systematically favoring GenAI outputs. GPT-4o's ratings show higher variance, less category discrimination, and inflated scores for GenAI images—suggesting it optimizes for computable surface features rather than contextual-perceptual creativity dimensions humans weight. Core assumption: The factor loadings and latent means differ fundamentally between human and GPT-4o raters. Evidence: Neither overall perceived creativity scores nor image category discrimination matched between human and GenAI ratings. Multi-group CFA strongly suggests factor loadings and latent means are different. Break condition: If GPT-4o ratings correlated strongly with human ratings (r > 0.8) and showed matching discrimination patterns, mechanism fails.

## Foundational Learning

- **Divergent Thinking vs. Creative Mental Imagery (CMI)**: The paper contrasts language-based DT tasks (where LLMs excel) with visual CMI tasks (where they struggle); conflating these leads to overclaiming GenAI creativity. Quick check: Can you explain why high fluency in DT tasks doesn't guarantee strong performance on open-ended visual imagery tasks?

- **Stable Diffusion with ControlNet conditioning**: The methodology uses ControlNet to inject TCIA abstract stimuli as structural guidance while prompts provide semantic content; understanding this separation is critical for interpreting the HI vs. SG conditions. Quick check: What happens if you provide ControlNet input with an empty prompt vs. a detailed prompt?

- **Factor Analysis for latent construct validation**: The paper derives "Creativity" scores from five dimensions via CFA; understanding multi-group invariance testing explains how they proved human and GPT-4o raters use different evaluation constructs. Quick check: Why does failing strong invariance mean you cannot compare latent means across groups directly?

## Architecture Onboarding

- **Component map**: TCIA stimuli → Human/SDXL fine-tuning → Image generation (HI/SG) → Human rating task → GPT-4o rating → Mixed-effects Beta regression + Multi-group CFA + k-means clustering

- **Critical path**: TCIA stimuli quality controls both human and GenAI image generation; Prompt engineering (HI vs SG) is the key intervention variable; Rating methodology (blind human raters vs GPT-4o) determines comparability; Factor-analytic derivation of Creativity score determines statistical validity

- **Design tradeoffs**: Using monochromatic children's drawing style limits ecological validity for professional art contexts but controls for technical complexity; GPT-4o rating via API with single-prompt simulation may not capture its full multimodal reasoning capabilities; 30 ratings per image balances statistical power against rater fatigue

- **Failure signatures**: HI-GenAI and SG-GenAI showing no significant difference → prompt engineering failed as manipulation; Human raters unable to distinguish human from GenAI images → task insufficiently sensitive; CFA showing poor fit indices → creativity dimensions not measuring unified construct

- **First 3 experiments**:
  1. Replicate with newer multimodal models (e.g., GPT-4o image generation instead of SDXL) to test whether architecture matters
  2. Vary prompt elaboration continuously (not binary HI/SG) to identify the guidance threshold where GenAI matches Non-Artists
  3. Have GPT-4o rate images alongside its generation process to test whether co-generation improves evaluation alignment with humans

## Open Questions the Paper Calls Out

### Open Question 1
Can multimodal AI models be trained or calibrated to evaluate visual creativity in a manner that aligns with human aesthetic judgment patterns? Basis: GPT-4o failed to mirror humans in their creativity appraisal and showed less discrimination between image categories, in some cases even favoring GenAI images. Unresolved because the study demonstrated the discrepancy but did not test interventions to align AI judgment with human perception. Evidence needed: Experiments showing AI raters trained on human-rated visual creativity datasets achieving high correlation and similar discrimination patterns with human raters across image categories.

### Open Question 2
What specific perceptual or contextual features drive the performance gap between humans and GenAI in open-ended visual imagination tasks? Basis: GenAI models face unique challenges in visual domains where creativity depends on perceptual nuance and contextual sensitivity—distinctly human capacities that may not be readily transferable from language models. Unresolved because while the paper identifies the gap, it does not conduct controlled ablation or feature-attribution analyses to pinpoint responsible factors. Evidence needed: Targeted experiments manipulating specific perceptual or contextual elements to identify which factors most affect AI vs. human performance.

### Open Question 3
Does the observed human–AI creativity gap generalize across different generative architectures and cultural contexts? Basis: The study uses only Stable Diffusion XL and predominantly Western participant samples. Unresolved because it remains unclear whether findings are architecture-specific and whether cultural differences affect both human and AI creativity ratings. Evidence needed: Replication across multiple generative model classes and diverse cultural populations, analyzing whether the creativity gradient persists or shifts.

## Limitations
- TCIA task uses abstract stimuli and monochromatic children's drawing style, limiting ecological validity for professional creative contexts
- Fine-tuning dataset (N=550 human drawings) may be insufficient for stable LoRA adaptation
- GPT-4o ratings conducted via API rather than multimodal co-generation, potentially underestimating capabilities
- Human rating sample from Prolific may introduce demographic and motivation biases

## Confidence

- **High Confidence**: The creativity gradient (Visual Artists > Non-Artists ≥ HI-GenAI > SG-GenAI) is robust across multiple analyses with strong statistical evidence (p < .001 for most comparisons). Multi-group CFA confirming different evaluation constructs between humans and GPT-4o is methodologically sound.

- **Medium Confidence**: The claim that human guidance elevates GenAI to non-expert human levels is well-supported within this experimental paradigm, but may not generalize to open-ended creative tasks. The mechanism explaining architectural limitations is plausible but not definitively proven.

- **Low Confidence**: The broader claim about "unique challenges" facing AI in visual creativity domains requires more ecological validation. Controlled conditions may overstate the gap by using simplified stimuli and restricted output formats.

## Next Checks

1. **Ecological Generalization Test**: Replicate the experiment using professional artists' briefs (complex, multi-element prompts) and full-color output requirements to test whether the creativity gap persists in real-world creative contexts.

2. **Model Architecture Comparison**: Test whether newer multimodal models (GPT-4o image generation, DALL-E 3) show different performance patterns than Stable Diffusion, particularly in HI vs SG conditions, to isolate whether the gap stems from architecture or training approach.

3. **Embodied Training Intervention**: Train diffusion models on physically-grounded image-text pairs (real-world photographs with contextual descriptions) rather than abstract artistic drawings to test whether adding perceptual grounding reduces the creativity gap.