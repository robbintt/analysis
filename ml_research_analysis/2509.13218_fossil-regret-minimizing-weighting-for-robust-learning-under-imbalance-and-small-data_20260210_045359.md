---
ver: rpa2
title: 'FOSSIL: Regret-minimizing weighting for robust learning under imbalance and
  small data'
arxiv_id: '2509.13218'
source_url: https://arxiv.org/abs/2509.13218
tags:
- learning
- loss
- regret
- augmentation
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOSSIL introduces a regret-minimizing weighting framework that
  addresses class imbalance and small-data challenges in high-stakes domains. The
  method unifies class-prior correction, difficulty-based curriculum, augmentation
  penalties, and warmup scheduling into a single interpretable formula.
---

# FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data

## Quick Facts
- **arXiv ID**: 2509.13218
- **Source URL**: https://arxiv.org/abs/2509.13218
- **Reference count**: 31
- **Primary result**: FOSSIL achieves AUC 0.89, balanced accuracy 0.83 on synthetic data with 9:1 imbalance, outperforming baselines with lowest dynamic regret (0.16).

## Executive Summary
FOSSIL introduces a regret-minimizing weighting framework that addresses class imbalance and small-data challenges in high-stakes domains. The method unifies class-prior correction, difficulty-based curriculum, augmentation penalties, and warmup scheduling into a single interpretable formula. Theoretically, FOSSIL provides regret guarantees and stability bounds, while subsuming existing schemes such as class-balanced loss and focal loss as special cases. Empirically, on synthetic data with 9:1 imbalance, it achieves AUC 0.89, balanced accuracy 0.83, and lowest dynamic regret (0.16). On real medical imaging data (PAD-UFES-20), it maintains AUC 0.82, balanced accuracy 0.73, and improves recall by 26 percentage points versus ERM. Wilcoxon tests confirm statistical significance (p < 0.05). FOSSIL consistently outperforms baselines without architectural changes, delivering both robustness and improved generalization under imbalance and data scarcity.

## Method Summary
FOSSIL is a bilevel optimization framework that learns sample weights to minimize validation regret in imbalanced and small-data settings. The core weight formula w_i(t) = (1/Kp(y_i)) · exp(-d_i/T_t) · (1-γ_t·1{i∈A}) · min(1, t/t_warm) integrates four signals: class balance correction (inverse class priors), difficulty-aware curriculum (temperature-scaled difficulty), augmentation penalties (reducing synthetic sample influence), and warmup scheduling (gradual curriculum onset). The method uses conjugate gradient to approximate hypergradients without materializing Hessians, enabling efficient updates to weights and temperature parameters. FOSSIL theoretically guarantees sublinear static/dynamic regret and generalization bounds via effective sample size analysis.

## Key Results
- On synthetic data with 9:1 imbalance, FOSSIL achieves AUC 0.89, balanced accuracy 0.83, and lowest dynamic regret (0.16).
- On PAD-UFES-20 medical imaging data, FOSSIL maintains AUC 0.82, balanced accuracy 0.73, improving recall by 26 percentage points versus ERM.
- Wilcoxon signed-rank and permutation tests confirm statistical significance (p < 0.05) across all real-data metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiplicative unification of class balance, difficulty, augmentation, and warmup yields stable, interpretable sample weights.
- Mechanism: The weight formula `w_i(t) = (1/Kp(y_i)) · exp(-d_i/T_t) · (1-γ_t·1{i∈A}) · min(1, t/t_warm)` factorizes four signals. Boundedness (Lemma 3.1) prevents explosion; monotonic curriculum (Lemma 3.2) ensures difficulty progressively upweights harder samples as temperature T_t decays.
- Core assumption: Class priors are known or estimated; difficulty proxy d_i (e.g., 1 − max softmax confidence) correlates with true sample hardness.
- Evidence anchors:
  - [abstract] "unified weighting framework that seamlessly integrates class imbalance correction, difficulty-aware curricula, augmentation penalties, and warmup dynamics into a single interpretable formula"
  - [section 3.1-3.2] Lemma 3.1 (boundedness) and Lemma 3.2 (monotonic curriculum progression) formalize stability
  - [corpus] Limited direct corpus evidence for this specific multiplicative formulation; related work (MIDAS, difficulty-aware methods) addresses subproblems in isolation
- Break condition: If priors are misspecified (e.g., label noise) or difficulty proxy is adversarially inverted, weights may misallocate emphasis.

### Mechanism 2
- Claim: Bilevel optimization with validation-driven hypergradients learns weights that minimize generalization regret without architectural changes.
- Mechanism: Lower level optimizes model θ on weighted training loss; upper level tunes (w, λ) via hypergradients `∇_w,λ F = −∇²_{θ,(w,λ)} L_D · (∇²_{θθ} L_D)^{-1} ∇_θ L_val`. Conjugate gradient approximation (Proposition 4.4) reduces Hessian inversion from O(d²) to O(d) per iteration.
- Core assumption: Validation set is clean and representative; inner optimization reaches approximate stationarity for implicit differentiation validity.
- Evidence anchors:
  - [section 4.3] Proposition 4.4 provides Hessian-vector identity for efficient hypergradients
  - [section 5] Synthetic experiments show FOSSIL achieves lowest dynamic regret (0.16 at IR=9:1)
  - [corpus] Meta-Weight-Net (cited) uses bilevel reweighting but lacks unified augmentation/warmup terms; FOSSIL extends this line
- Break condition: If validation set is small or noisy, hypergradient signal becomes unreliable; warmup term mitigates early instability but cannot fully compensate.

### Mechanism 3
- Claim: Regret minimization framework guarantees sublinear static/dynamic regret under distributional drift.
- Mechanism: Online convex optimization analysis (Theorem 4.3) yields `Regret_stat(T) = O(√T)` and `Regret_dyn(T) = O(√T + P_T)` where P_T is comparator path-length. Effective sample size `N_eff = (∑w_i)² / ∑w_i²` controls generalization (Theorem 4.2).
- Core assumption: Losses are convex or locally approximated as convex; gradients are bounded (G-Lipschitz).
- Evidence anchors:
  - [section 4.2] Theorem 4.3 establishes regret bounds
  - [section 4.1] Theorem 4.2 bounds validation gap by `O(1/√N_eff)`
  - [corpus] No corpus papers directly evaluate regret in this bilevel weighting context; this is a distinct contribution
- Break condition: Non-convex deep networks violate formal assumptions; empirical regret still measured but guarantees become heuristic.

## Foundational Learning

- **Curriculum learning and difficulty pacing**: Why needed here: FOSSIL's temperature schedule T_t and difficulty term `exp(-d_i/T_t)` require understanding how easy→hard progression affects convergence. Quick check: Can you explain why decreasing temperature increases the influence of hard samples over time?

- **Bilevel optimization and implicit differentiation**: Why needed here: Upper-level weight updates depend on differentiating through the lower-level optimizer via the implicit function theorem. Quick check: Why does implicit differentiation avoid storing the full unrolled optimization trajectory?

- **Online convex optimization and regret**: Why needed here: Theoretical guarantees are framed in regret terms; understanding static vs. dynamic regret clarifies what FOSSIL provably achieves. Quick check: What does sublinear regret imply about long-run average performance?

## Architecture Onboarding

- **Component map**: Data → Difficulty scores → Weight computation (Eq. 1) → Weighted training loss → Model update → Validation loss → Hypergradient estimation (CG) → Weight/parameter updates

- **Critical path**: 1) Forward pass computes per-sample losses and difficulty scores (softmax confidence complement). 2) Eq. (1) yields weights w_i(t). 3) Lower-level update: θ ← θ − η_θ ∇_θ L_train(θ; w, λ). 4) CG solves H·v = ∇_θ L_val for implicit gradient direction. 5) Upper-level update: w, λ ← momentum_step(∇_w,λ F) with projection. 6) Repeat per epoch; log regret metrics.

- **Design tradeoffs**: CG iterations vs. accuracy: More iterations improve hypergradient fidelity but increase compute; 10–20 is typically sufficient. Warmup length t_warm: Longer warmup stabilizes early training but delays curriculum; paper uses 5–10 epochs. Difficulty proxy choice: Softmax confidence (default) most stable; entropy and loss-based proxies showed higher variance (Appendix C.3).

- **Failure signatures**: Weight collapse: If N_eff → 1, single sample dominates; check weight distribution histogram. Regret non-decreasing: Suggests schedules (T_t, γ_t) are misconfigured or validation set is corrupted. Minority-class AUC drops: Augmentation penalty γ_t may be too aggressive, suppressing useful synthetic samples.

- **First 3 experiments**: 1) Synthetic validation at IR=9:1: Replicate Table 2 with MLP backbone; confirm balanced accuracy ≥0.82 and dynamic regret ≤0.18 vs. ERM baseline. 2) Ablation of multiplicative terms: Remove each factor (class, difficulty, augmentation, warmup) individually; quantify contribution to G-mean and regret. 3) Difficulty proxy robustness: Compare softmax vs. entropy vs. loss-based d_i on synthetic data; verify softmax yields lowest variance (Table A9 pattern).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FOSSIL be extended to use model-agnostic uncertainty measures rather than model-dependent difficulty scores such as softmax confidence?
- Basis in paper: [explicit] The authors state: "Remaining limitations include reliance on model-dependent difficulty scores... Future work should explore model-agnostic uncertainty."
- Why unresolved: Current difficulty proxies (softmax, entropy, loss) all depend on the current model's predictions, which may be unreliable early in training or under severe class imbalance.
- What evidence would resolve it: Demonstrating that ensemble-based, Bayesian, or distributional uncertainty measures can replace softmax confidence while maintaining or improving regret bounds and balanced accuracy.

### Open Question 2
- Question: How does the FOSSIL weighting formula generalize to multi-class settings beyond binary classification?
- Basis in paper: [explicit] The authors acknowledge "a focus on binary tasks" as a limitation and call for extension to "multi-class... settings."
- Why unresolved: The class term 1/(K·p(yi)) and difficulty interactions may behave differently when K > 2, and pairwise vs. one-vs-rest formulations need theoretical validation.
- What evidence would resolve it: Empirical evaluation on multi-class imbalanced benchmarks (e.g., CIFAR-100-LT, ImageNet-LT) with updated regret analysis showing whether O(√T + PT) bounds hold.

### Open Question 3
- Question: How robust is FOSSIL to the choice of difficulty proxy when statistical significance was not achieved across alternatives (p > 0.1)?
- Basis in paper: [inferred] Appendix C.3 shows softmax outperformed entropy and loss-based difficulty, but "differences were not statistically significant," with loss-based proxies showing unstable G-mean (0.17 ± 0.30).
- Why unresolved: The lack of significant differences suggests the observed softmax advantage may be dataset-specific, and the instability of loss-based difficulty indicates potential failure modes.
- What evidence would resolve it: Systematic ablation across diverse datasets with statistical power analysis, or development of adaptive difficulty measures that automatically select or combine proxies based on data characteristics.

## Limitations
- Theoretical regret guarantees rely on convexity assumptions that do not strictly hold for deep neural networks, making empirical regret bounds heuristic.
- Effectiveness of difficulty proxies is sensitive to proxy choice, with softmax confidence outperforming alternatives but no guarantee this holds across domains.
- Hypergradient computation via conjugate gradient introduces approximation error that is not characterized in the bounds.

## Confidence
- **High confidence**: Synthetic data experiments showing FOSSIL's superior balanced accuracy and dynamic regret at IR=9:1; Wilcoxon significance tests (p<0.05) on real data; multiplicative weight formula unification claim.
- **Medium confidence**: Theoretical regret bounds (Theorem 4.3) and generalization bounds (Theorem 4.2) given convex approximation; difficulty proxy ablation results in Appendix C.3.
- **Low confidence**: Generalization to datasets with different data modalities (e.g., tabular vs. image) without hyperparameter tuning; stability under extreme label noise.

## Next Checks
1. **Ablation on difficulty proxy choice**: Replicate Table A9 on synthetic data to verify softmax confidence consistently yields lower variance and higher balanced accuracy than entropy or loss-based proxies.
2. **Hyperparameter sensitivity**: Systematically vary temperature decay, warmup length, and γ_scale on synthetic IR=9:1 data; quantify impact on regret and minority-class recall.
3. **External validation robustness**: Apply trained ConvNeXt model to MSLD v2.0 (Monkeypox dataset) and measure drop in AUC/recall versus internal cross-validation performance.