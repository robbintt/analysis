---
ver: rpa2
title: 'GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework'
arxiv_id: '2504.17471'
source_url: https://arxiv.org/abs/2504.17471
tags:
- byzantine
- nodes
- learning
- granite
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRANITE introduces a Byzantine-resilient framework for dynamic
  gossip learning that addresses the vulnerability of existing solutions to adversarial
  manipulation of peer sampling protocols. The method combines a History-aware Peer
  Sampling (HaPS) protocol that exponentially reduces Byzantine presence in local
  views using accumulated historical node identifiers, and an Adaptive Probabilistic
  Threshold (APT) mechanism that dynamically adjusts aggregation thresholds based
  on estimated Byzantine ratios using Chernoff bounds.
---

# GRANITE : a Byzantine-Resilient Dynamic Gossip Learning Framework

## Quick Facts
- arXiv ID: 2504.17471
- Source URL: https://arxiv.org/abs/2504.17471
- Authors: Yacine Belal; Mohamed Maouche; Sonia Ben Mokhtar; Anthony Simonet-Boulogne
- Reference count: 40
- Primary result: Maintains convergence with up to 30% Byzantine nodes on 9× sparser graphs than theoretical bounds

## Executive Summary
GRANITE introduces a Byzantine-resilient framework for dynamic gossip learning that addresses the vulnerability of existing solutions to adversarial manipulation of peer sampling protocols. The method combines a History-aware Peer Sampling (HaPS) protocol that exponentially reduces Byzantine presence in local views using accumulated historical node identifiers, and an Adaptive Probabilistic Threshold (APT) mechanism that dynamically adjusts aggregation thresholds based on estimated Byzantine ratios using Chernoff bounds. Empirical results demonstrate that GRANITE maintains convergence with up to 30% Byzantine nodes, achieves 9× sparser graph requirements compared to theoretical bounds, and improves convergence speed through adaptive filtering. The framework successfully defends against strong model poisoning attacks (FOE, ALIE) and flooding attacks while preserving the communication efficiency and scalability benefits of dynamic gossip learning.

## Method Summary
GRANITE integrates three core mechanisms: (1) HaPS maintains an expanding history of node identifiers from push/pull exchanges and uses min-wise independent permutations with slot-specific hash functions to rank candidates for local views, (2) APT computes dynamic filtering thresholds via Chernoff bounds using the estimated Byzantine ratio from HaPS, and (3) these components work with robust aggregators like Clipped Summation and Geometric Trimmed Summation. The system initializes with bootstrap sets containing honest nodes, accumulates history over time, and adaptively adjusts filtering thresholds to ensure Byzantine-resilient learning on sparser graphs than static theory requires. Implementation requires gossip learning infrastructure, dynamic graph support, and proper attack parameterization.

## Key Results
- Maintains convergence with up to 30% Byzantine nodes in the network
- Achieves 9× sparser graph requirements compared to theoretical bounds for static graphs
- Successfully defends against strong model poisoning attacks (FOE, ALIE) and flooding attacks
- Improves convergence speed through adaptive filtering that becomes less conservative over time

## Why This Works (Mechanism)

### Mechanism 1: History-Aware Peer Sampling (HaPS) for Byzantine Dilution
- Claim: Accumulating a history of node identifiers and selecting neighbors via local, hash-based ranking functions exponentially reduces the proportion of Byzantine nodes in a node's local view over time.
- Mechanism: Each node maintains a monotonically expanding set $h_i(t)$ of encountered identifiers (from push/pull exchanges) and uses a per-slot, seeded ranking function $g_{seed(i,v)}(p)$ to greedily fill its view $N^{out}_i(t)$ with the best-ranked candidates from this history. Seeds are refreshed periodically to keep the graph dynamic. By probabilistically sampling from a growing pool of honest identifiers, the expected local Byzantine ratio $\mathcal{B}(t)$ decays exponentially toward the global Byzantine fraction $f$.
- Core assumption: Nodes have sufficient memory to store a history of identifiers and the system exhibits lower churn than typical gossip applications, as participants are incentivized to remain for improved later-stage models. Initial bootstrap set $I$ contains at least one honest node. The theoretical upper bound is derived under a worst-case model where Byzantine nodes have unlimited communication power ($F=\infty$) and may pre-position their IDs.
- Evidence anchors:
  - [abstract] "History-aware Peer Sampling (HaPS) protocol that exponentially reduces Byzantine presence in local views using accumulated historical node identifiers."
  - [section V.B, Theorem V.1 & Corollary V.2] Provides derivation of exponential convergence for the known-honest count $C(t)$ and the resulting upper bound for the Byzantine ratio $\mathcal{B}(t)$.
  - [corpus] Weak direct evidence in provided corpus, which focuses more on Federated Learning. The mechanism is presented as a novel contribution of this paper.
- Break condition: The mechanism's exponential decay guarantee may fail if honest nodes are completely isolated (Eclipse attack) or if the initial bootstrap set contains only Byzantine nodes. Assumption of unlimited adversary capability in the analysis provides a theoretical upper bound, but practical performance depends on the actual adversarial strategy.

### Mechanism 2: Adaptive Probabilistic Threshold (APT) for Robust Aggregation
- Claim: By using the estimated time-dependent Byzantine ratio $\mathcal{B}(t)$ from HaPS within a Chernoff bound, the filtering threshold $b(t)$ for robust aggregators can be dynamically set to ensure correct aggregation with a user-defined high probability $1-\kappa$.
- Mechanism: At each round $t$, APT computes an expected number of Byzantine nodes in the local view, $\mu = v \cdot \mathcal{B}(t)$. It then finds the smallest $\delta > 0$ such that the Chernoff bound guarantees $P(X_t \ge (1+\delta)\mu) \le \kappa$, where $X_t$ is the actual count. The adaptive threshold is set to $b(t) = (1+\delta)\mu$. This $b(t)$ is then used as the filtering parameter for the underlying robust aggregator (e.g., CS or GTS).
- Core assumption: The local view slots are occupied by Byzantine nodes independently with probability at most $\mathcal{B}(t)$. This is a mean-field approximation of the sampling process, which may not hold perfectly in practice.
- Evidence anchors:
  - [abstract] "Adaptive Probabilistic Threshold (APT) mechanism that dynamically adjusts aggregation thresholds based on estimated Byzantine ratios using Chernoff bounds."
  - [section V.C, Lemma V.3] Formally states the guarantee provided by APT based on the Chernoff bound.
  - [corpus] Not directly addressed in corpus. This is a core algorithmic contribution linking the RPS state to the learning layer.
- Break condition: The probabilistic guarantee is void if the independence assumption for Byzantine node placement is significantly violated. The bound may be loose in early rounds due to the conservative nature of $\mathcal{B}(t)$, potentially leading to overly aggressive filtering and slower initial convergence. If $b(t) \ge v$, the fallback of keeping only one model is a heuristic without the same theoretical backing.

### Mechanism 3: Integration with Robust Aggregators for Sparse Dynamic Graphs
- Claim: Combining HaPS and APT with robust aggregators like Clipped Summation (CS) and Geometric Trimmed Summation (GTS) enables Byzantine-resilient learning on graphs up to 9× sparser than required by static theory.
- Mechanism: Existing robust GL aggregators require a bounded number of Byzantine nodes per view ($b$) to guarantee convergence, which is difficult in dynamic settings with adversarial RPS. GRANITE addresses this by having HaPS bound $\mathcal{B}(t)$, which APT translates into a safe, time-varying $b(t)$. This effectively transforms the dynamic graph setting into an equivalent static setting with a bounded adversarial presence, allowing the provable guarantees of CS and GTS for sparse graphs to hold.
- Core assumption: The underlying robust aggregators (CS, GTS) are correctly parameterized and their theoretical requirements for convergence (e.g., minimum connectivity) are otherwise met. The system knows an upper bound on the total number of Byzantine nodes $B$.
- Evidence anchors:
  - [abstract] "...achieves 9× sparser graph requirements compared to theoretical bounds..."
  - [section VII.B, Table III] Provides a direct comparison of theoretical filtering thresholds (e.g., 180 for CS with f=0.3) versus GRANITE's empirical threshold (b(t) ≤19) and calculates the communication gain.
  - [corpus] Related work in corpus (e.g., "Byzantine-Resilient Zero-Order Optimization") focuses on Federated Learning, not this specific Dynamic Gossip Learning integration.
- Break condition: The claimed sparsity advantage is based on empirical performance relative to theoretical bounds derived for static graphs. If the assumptions of the robust aggregators are not met (e.g., insufficient underlying connectivity, extreme non-IID data), convergence may fail regardless of GRANITE's components.

## Foundational Learning

- Concept: **Gossip Learning (GL) and Dynamic Graphs**
  - Why needed here: GRANITE is built for Dynamic GL, where nodes communicate over a continuously changing sparse graph, unlike static or centralized setups.
  - Quick check question: Can you explain the key difference between a static and dynamic communication graph for decentralized learning, and why Dynamic GL can converge faster with sparser connections?

- Concept: **Random Peer Sampling (RPS) Protocols**
  - Why needed here: The core vulnerability addressed is the manipulation of the RPS protocol, which is responsible for maintaining the dynamic graph.
  - Quick check question: What is the primary function of a Random Peer Sampling protocol, and what are Hub and Eclipse attacks in this context?

- Concept: **Robust Aggregation (Clipped Summation, Trimmed Mean)**
  - Why needed here: GRANITE's APT is designed to dynamically provide the filtering threshold $b$ required by these robust aggregators.
  - Quick check question: How does a robust aggregator like Clipped Summation differ from a simple average, and what role does the filtering threshold $b$ play in its operation?

## Architecture Onboarding

- Component Map:
  1.  **HaPS Module**: Manages node history $h_i(t)$ and performs push/pull exchanges with neighbors to discover identifiers. Uses ranking functions to select its outgoing view $N^{out}_i(t)$.
  2.  **APT Calculator**: Takes global parameters ($B$, $|H|$, $v$, $\kappa$) and the current time $t$ to compute the adaptive filtering threshold $b(t)$. Depends on the HaPS module's state (implicitly via $t$ and the analytical model of $\mathcal{B}(t)$).
  3.  **Learning Loop**: Performs local training, receives models from incoming neighbors, and uses the robust aggregator (parameterized by $b(t)$ from APT) to update its local model.
  4.  **Communication Interface**: Handles sending models to the outgoing view (from HaPS) and receiving models/push-pull messages from the network.

- Critical Path: The integrity of the entire system hinges on the HaPS module's ability to accumulate a sufficient history of honest identifiers. The exponential decay of the Byzantine ratio is time-dependent, meaning the first few rounds are the most critical and most vulnerable. The APT's calculation of $b(t)$ must therefore be conservative in early rounds. For the learning to succeed, the robust aggregator must receive a sufficient number of honest models, which is guaranteed with high probability by APT's correct operation.

- Design Tradeoffs:
  - **Memory vs. Security**: HaPS requires storing a history of identifiers, trading memory for security against RPS manipulation attacks.
  - **Convergence Speed vs. Robustness**: A more conservative failure probability $\kappa$ in APT will yield a larger $b(t)$, increasing robustness but potentially slowing convergence by filtering out more honest models in early rounds. The adaptive nature aims to mitigate this over time.
  - **Connectivity vs. Sparsity**: The system operates on sparser graphs than static theory requires, which improves communication efficiency but may be more fragile to node churn or unexpected network partitions.

- Failure Signatures:
  - **Early Divergence**: If the model diverges within the first ~20 rounds, it indicates that the initial Byzantine spike in the local view exceeded even the conservative $b(t)$, possibly due to an extremely strong flooding attack or an issue with the analytical bound's assumptions.
  - **Stagnation**: If the model shows a flat learning curve for an extended period, it may suggest that $b(t)$ is set too high, causing the aggregator to filter out too many honest models.
  - **Late-stage Collapse**: A sudden drop in performance after a period of learning could indicate a failure in the HaPS seed refresh mechanism, leading to graph ossification or an unexpected surge in Byzantine presence.

- First 3 experiments:
  1.  **Baseline HaPS & APT Validation**: Implement the analytical model for $\mathcal{B}(t)$ and the Chernoff-based $b(t)$ calculation. Run a simulation of the HaPS protocol under various flooding forces ($F=1, 2, \infty$) without any learning. Plot the empirical $f_{in}$ against the theoretical upper bound to verify the exponential decay and the bound's conservatism.
  2.  **Ablation Study on APT**: Integrate the full GRANITE framework with a chosen robust aggregator (e.g., CS). Compare the convergence of (a) GRANITE with adaptive $b(t)$, (b) GRANITE with a fixed, non-adaptive $b$, and (c) a baseline without HaPS/APT, under a standard attack (e.g., FOE with $f=0.1$). This isolates the contribution of the APT mechanism.
  3.  **Sparsity and Attack Scaling**: Evaluate GRANITE's performance under increasing Byzantine fractions ($f=0.1, 0.2, 0.3$) and varying graph sparsity (e.g., view sizes $v=10, 20, 30$) against strong attacks (FOE, ALIE). This will test the claim of maintaining convergence under 30% Byzantine nodes and on graphs 9× sparser than theoretical bounds.

## Open Questions the Paper Calls Out

- **Question**: Can formal theoretical convergence guarantees be derived for GRANITE, specifically explaining the empirically observed 9× reduction in graph density requirements compared to existing robust aggregator theory?
  - **Basis in paper**: [explicit] Section VII-B states, "Extending theoretical convergence guarantees for GRANITE is left as future work."
  - **Why unresolved**: The authors provide a formal analysis of the Byzantine ratio decay ($B(t)$) but rely on empirical validation to demonstrate convergence on sparser graphs than current theoretical bounds dictate.
  - **What evidence would resolve it**: A formal proof showing convergence bounds under the HaPS protocol that match the empirical sparsity levels (view size $v=20$ for $n=300$).

- **Question**: How does GRANITE perform under Sybil attacks where adversaries can craft an arbitrary number of identifiers, given the framework's reliance on history-aware identifier tracking?
  - **Basis in paper**: [explicit] Section IV.A states, "Sybil attacks (where adversaries can craft an arbitrary number of identifiers) are out of the scope of this work."
  - **Why unresolved**: The HaPS mechanism tracks *previously encountered identifiers* to limit Byzantine presence. If an adversary can generate infinite new IDs, the history could potentially be flooded, bypassing the ranking-based filtering.
  - **What evidence would resolve it**: Empirical or theoretical analysis of GRANITE’s stability when Byzantine nodes continuously generate new, unique identifiers to penetrate the history sets of honest nodes.

- **Question**: Can the theoretical upper bound $B(t)$ be tightened to reduce the gap between the conservative estimate and the empirical Byzantine ratio, improving early-round convergence speed?
  - **Basis in paper**: [inferred] Section VII-D notes that the bound $B(t)$ is "almost 2 times larger than $f_{in}$" in early rounds due to worst-case derivation assumptions.
  - **Why unresolved**: The current derivation assumes a worst-case history population scenario to guarantee safety. This conservatism forces the Adaptive Probabilistic Threshold (APT) to filter aggressively early on, potentially slowing initial learning.
  - **What evidence would resolve it**: A modified derivation of $B(t)$ that incorporates network topology or time-dependent constraints to provide a less conservative, yet still safe, threshold for the first $t$ rounds.

## Limitations

- The analytical bound for B(t) is conservative by design (2× larger than empirical values), which may lead to overly aggressive filtering in early rounds and slower initial convergence
- Claims of 9× sparsity improvement are based on theoretical bounds for static graphs, not direct comparison with dynamic Byzantine-resilient methods
- The framework assumes lower churn than typical gossip applications and sufficient memory for history accumulation, which may not hold in all practical settings

## Confidence

- **High confidence** in the core mechanism integration (HaPS + APT + robust aggregators) based on theoretical proofs provided for Byzantine ratio decay and Chernoff-bound filtering
- **Medium confidence** in empirical claims regarding sparsity improvements and convergence under attacks, as these depend on proper attack parameterization and simulator implementation details not fully specified
- **Low confidence** in reproducibility without clarification on hash function implementation, attack parameters, and exact α calculation formulas

## Next Checks

1. Implement HaPS protocol simulation with varying flooding forces (F=1, 2, ∞) to empirically verify exponential decay of Byzantine ratio and validate analytical upper bound conservatism
2. Conduct ablation study comparing GRANITE with adaptive b(t) versus fixed threshold to isolate APT contribution to convergence speed and robustness
3. Test GRANITE's performance under extreme conditions (f=0.3 Byzantine, minimal connectivity) to verify claims of maintaining convergence on 9× sparser graphs while defending against strong model poisoning attacks