---
ver: rpa2
title: A Graph Signal Processing Framework for Hallucination Detection in Large Language
  Models
arxiv_id: '2510.19117'
source_url: https://arxiv.org/abs/2510.19117
tags:
- spectral
- gpt-2
- hallucinations
- semantic
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a graph signal processing framework for detecting
  hallucinations in large language models by modeling transformer layers as dynamic
  graphs induced by attention mechanisms, with token embeddings as signals on these
  graphs. The approach defines spectral diagnostics including Dirichlet energy, spectral
  entropy, and high-frequency energy ratios, with theoretical connections to computational
  stability.
---

# A Graph Signal Processing Framework for Hallucination Detection in Large Language Models

## Quick Facts
- arXiv ID: 2510.19117
- Source URL: https://arxiv.org/abs/2510.19117
- Authors: Valentin Noël
- Reference count: 32
- Primary result: Graph signal processing framework achieves 88.75% hallucination detection accuracy using spectral signatures

## Executive Summary
This paper introduces a novel graph signal processing framework for detecting hallucinations in large language models by modeling transformer layers as dynamic graphs induced by attention mechanisms. The approach defines spectral diagnostics including Dirichlet energy, spectral entropy, and high-frequency energy ratios, with theoretical connections to computational stability. Experiments across GPT architectures reveal universal spectral patterns: factual statements exhibit consistent "energy mountain" behavior with low-frequency convergence, while different hallucination types show distinct signatures. The framework achieves 88.75% accuracy versus 75% for perplexity-based baselines, demonstrating practical utility for hallucination detection.

## Method Summary
The framework constructs undirected weighted graphs from symmetrized attention matrices (W = (A + A^T)/2) and treats token embeddings as graph signals. It computes spectral diagnostics including Dirichlet energy (measuring signal smoothness), spectral entropy (frequency concentration), high-frequency energy ratio (proportion of energy in high frequencies), and Fiedler value (algebraic connectivity). The Spectral Hallucination Detector (SHD) compares final-layer spectral signatures against baseline statistics using z-score thresholds. The approach is theoretically grounded with propositions linking Dirichlet energy to local consistency and HFER to global connectivity.

## Key Results
- Universal "energy mountain" pattern across GPT architectures: factual statements show consistent energy buildup and low-frequency convergence
- Different hallucination types exhibit distinct spectral signatures: logical contradictions destabilize spectra (g > 1.0), semantic errors show connectivity drift, substitutions are intermediate
- SHD detector achieves 88.75% accuracy versus 75% for perplexity baselines
- Computational overhead is 10-60 seconds per sequence using randomized Lanczos for eigendecomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention matrices can be interpreted as weighted adjacency matrices of dynamic token graphs, enabling spectral analysis of reasoning trajectories
- Mechanism: Post-softmax attention weights A(ℓ,h) are symmetrized (W = (A + A^T)/2) to form undirected graphs. Token embeddings become graph signals. The graph Laplacian L = D - W then captures how representations flow across the attention-induced topology
- Core assumption: The geometric structure of attention graphs encodes computational stability and reasoning quality
- Evidence anchors:
  - [abstract] "We propose a spectral analysis framework that models transformer layers as dynamic graphs induced by attention, with token embeddings as signals on these graphs"
  - [section 2] "We build an undirected weighted graph by symmetrization, W(ℓ,h) = 1/2(A(ℓ,h) + (A(ℓ,h))^⊤)"
  - [corpus] Related work "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning" and "Hallucination Detection in LLMs Using Spectral Features of Attention Maps" apply similar spectral approaches to attention, suggesting convergent validation
- Break condition: If attention heads capture task-irregular relationships, graph topology may not reflect reasoning quality

### Mechanism 2
- Claim: Factual reasoning exhibits a universal "energy mountain" trajectory with low-frequency convergence, while hallucinations diverge predictably
- Mechanism: Energy builds through middle layers (2–9M peak), then dissipates (~0.1M final). HFER drops to 0.1–0.3 in final layers. Logical hallucinations destabilize this pattern (high run-to-run variance); semantic hallucinations maintain stability but show late-layer connectivity drift
- Core assumption: Spectral smoothness correlates with reliable computation
- Evidence anchors:
  - [abstract] "factual statements exhibit consistent 'energy mountain' behavior with low-frequency convergence, while different hallucination types show distinct signatures"
  - [section 5.1] "All architectures follow the energy mountain: initial low energy (~10K), sharp buildup (2.0M–9.0M peak), and dissipation to ~0.1M at output. Reduction ratios (50–60×) are invariant to model size"
  - [corpus] Weak direct evidence for the specific "energy mountain" pattern—it appears novel; corpus papers focus on detection accuracy rather than layerwise dynamics
- Break condition: Tasks requiring precise position-sensitive reasoning may need high-frequency components; convergence may not indicate correctness

### Mechanism 3
- Claim: Hallucination types leave distinguishable spectral fingerprints: logical errors destabilize spectra (g > 1.0), semantic errors show connectivity drift, substitutions are intermediate
- Mechanism: Logical contradictions cause entropy spikes and HFER oscillations across runs (variance-based detection). Semantic errors remain spectrally stable but show elevated Fiedler values in late layers (over-connectivity). Substitution hallucinations show moderate entropy/smoothness perturbations
- Core assumption: Hallucination types reflect fundamentally different computational failures
- Evidence anchors:
  - [abstract] "Logical contradictions destabilize spectra with large effect sizes (g > 1.0), semantic errors remain stable but show connectivity drift"
  - [section 5.5] Tables 3–5 show Hedges' g effect sizes: logical hallucinations g > 1.0 for HFER; semantic hallucinations g = 0.34–0.56 for Fiedler drift; substitutions g = 0.40–0.51 for entropy
  - [corpus] "EigenTrack" and "HSAD" papers also report spectral signatures for hallucination detection, supporting general approach but with different metrics
- Break condition: Novel or hybrid hallucination types may have unmapped signatures; the three-type taxonomy may be incomplete

## Foundational Learning

- Concept: **Graph Laplacian and spectral decomposition**
  - Why needed here: The framework depends on understanding L = D - W, eigenvalue decomposition L = UΛU^T, and how eigenvalues encode connectivity (Fiedler value = algebraic connectivity)
  - Quick check question: For a connected graph, why is the smallest eigenvalue of L always zero, and what does the second eigenvalue (Fiedler value) measure?

- Concept: **Dirichlet energy and graph signal smoothness**
  - Why needed here: Energy E(ℓ) = Tr((X^T)LX) quantifies how smooth token embeddings are relative to the attention graph; zero energy means constant signal on connected components
  - Quick check question: If Dirichlet energy is high, what does this indicate about local consistency of neighboring token representations?

- Concept: **Spectral entropy and frequency-domain concentration**
  - Why needed here: SE(ℓ) = -Σ p_m log p_m measures how concentrated signal energy is across frequency bands; low entropy indicates focused, coherent representations
  - Quick check question: High HFER means energy concentrates in high frequencies—what does Proposition 2 say this implies about local token consistency?

## Architecture Onboarding

- Component map:
  - Input extraction -> Graph construction -> Spectral diagnostics -> Detection layer

- Critical path:
  1. Extract post-softmax attention from all heads during inference
  2. Aggregate and symmetrize per-layer attention to form Laplacians
  3. Track layerwise spectral trajectory (energy, entropy, HFER, Fiedler)
  4. Flag outputs whose final-layer Fiedler value exceeds domain-calibrated thresholds

- Design tradeoffs:
  - Head aggregation: Uniform vs. learned α_h weights—paper uses simple averaging (transparent but may discard head-specific signal)
  - Frequency cutoff K for HFER: Arbitrary choice affects sensitivity—paper doesn't specify optimal K
  - Computation: Randomized Lanczos for partial eigendecomposition is O(nnz(W)) but adds 10–60s overhead per sequence

- Failure signatures:
  - Semantic hallucinations: Stay within baseline variance for HFER/entropy (Table 2); require Fiedler drift detection
  - Model-specific anomalies: Qwen2.5-7B shows late-layer connectivity collapse (Table 8: Fiedler 0.80 → 0.20), unlike GPT patterns
  - High variance: Logical hallucinations require multiple runs—single-sample detection unreliable

- First 3 experiments:
  1. **Replicate energy mountain**: Run 10+ factual statements through GPT-2 family; plot E(ℓ), SE(ℓ), HFER(ℓ), Fiedler(ℓ) to verify universal convergence pattern
  2. **Calibrate baseline statistics**: Compute mean ± SD for final-layer Fiedler values, entropy, and HFER across factual runs; establish detection thresholds
  3. **Test hallucination detection**: Implement SHD detector (Eq. 8) on held-out factual/hallucination pairs; compare accuracy vs. perplexity baseline (target: 88.75% vs. 75%)

## Open Questions the Paper Calls Out

None

## Limitations

- **Cross-architecture generalization**: The framework shows promise within GPT family but Qwen2.5-7B exhibits late-layer connectivity collapse, suggesting potential model-specific calibration needs
- **Three-type taxonomy constraint**: The framework assumes hallucinations fall into logical contradictions, semantic errors, and substitutions, which may not capture all real-world hallucination patterns
- **Single-sequence reliability**: Logical hallucination detection requires multiple runs due to high variance, limiting practical deployment where single inference is standard

## Confidence

**High confidence** in mathematical framework: The graph signal processing formulation is rigorously defined with clear connections to computational stability through Dirichlet energy and spectral entropy

**Medium confidence** in empirical patterns: The "energy mountain" and spectral signatures are consistently observed within the GPT family, but the Qwen2.5-7B anomaly and limited cross-architecture testing suggest these may not be universal

**Low confidence** in practical deployment readiness: The requirement for multiple runs for logical hallucination detection, potential model-specific calibration needs, and the three-type assumption limit immediate real-world applicability

## Next Checks

1. **Cross-architecture spectral universality test**: Apply the framework to diverse architectures including decoder-only (LLaMA, Mistral), encoder-decoder (T5, BART), and multimodal models. Compare whether the "energy mountain" pattern and hallucination signatures persist across architectural families, particularly examining whether non-GPT models show similar or divergent spectral trajectories

2. **Hybrid hallucination detection benchmark**: Create a benchmark dataset with mixed or novel hallucination types (e.g., combining logical and semantic errors, or introducing new failure modes like context collapse). Evaluate whether the current three-type spectral signatures remain discriminative or if the framework requires expansion to handle complex reasoning failures

3. **Single-shot detection optimization**: Develop and test methods to improve single-sequence reliability for logical hallucination detection. This could include ensemble methods using different K values for HFER, attention head-specific diagnostics, or lightweight consistency checks that don't require full re-sampling, aiming to maintain accuracy while reducing computational overhead