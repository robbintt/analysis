---
ver: rpa2
title: Online Statistical Inference of Constrained Stochastic Optimization via Random
  Scaling
arxiv_id: '2505.18327'
source_url: https://arxiv.org/abs/2505.18327
tags:
- methods
- aveplugin
- avers
- inference
- avebm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops an online statistical inference procedure for
  constrained stochastic optimization problems using a method called Adaptive Inexact
  Stochastic Sequential Quadratic Programming (AI-SSQP). The key innovation is a "random
  scaling" technique that constructs a pivotal test statistic by studentizing averaged
  AI-SSQP iterates using a random scaling matrix, enabling asymptotically valid confidence
  intervals without requiring covariance matrix estimation.
---

# Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling

## Quick Facts
- **arXiv ID:** 2505.18327
- **Source URL:** https://arxiv.org/abs/2505.18327
- **Reference count:** 40
- **Primary result:** Matrix-free online inference for constrained stochastic optimization using random scaling achieves O((d+m)²) cost vs O((d+m)³) for plug-in estimators

## Executive Summary
This paper introduces a novel online statistical inference method for constrained stochastic optimization using Adaptive Inexact Stochastic Sequential Quadratic Programming (AI-SSQP) with a random scaling technique. The key innovation is constructing a pivotal test statistic by studentizing averaged AI-SSQP iterates using a random scaling matrix, which enables asymptotically valid confidence intervals without requiring explicit covariance matrix estimation. This approach achieves computational efficiency matching first-order methods (O((d+m)²) per iteration) while providing faster convergence through second-order updates. The method is validated through numerical experiments on constrained linear and logistic regression problems, demonstrating superior performance over existing inference procedures with coverage rates close to the nominal 95% level.

## Method Summary
The paper develops an online inference procedure for constrained stochastic optimization problems with equality constraints. The method uses AI-SSQP to solve the optimization problem, where stochastic gradients and Hessians are computed at each iteration, and a sketched Newton system is solved using randomized Kaczmarz iterations. The key innovation is the random scaling technique: after averaging the AI-SSQP iterates, a random scaling matrix is constructed recursively from the iterate differences. This scaling matrix enables construction of a pivotal test statistic that yields asymptotically valid confidence intervals without requiring covariance matrix estimation. The approach is matrix-free, requiring only O((d+m)²) operations per iteration compared to O((d+m)³) for plug-in covariance estimators, while achieving faster convergence through second-order updates.

## Key Results
- Random scaling technique achieves O((d+m)²) computational cost per iteration vs O((d+m)³) for plug-in covariance estimators
- Coverage rates close to nominal 95% level across various problem dimensions (d ∈ {5, 20, 40}) and constraint configurations
- Faster convergence compared to first-order methods while maintaining matrix-free implementation
- Numerical experiments show superior performance over existing inference procedures in both coverage accuracy and computational efficiency

## Why This Works (Mechanism)
The random scaling technique constructs a pivotal test statistic by studentizing averaged AI-SSQP iterates using a random scaling matrix that captures the variability of the iterates. This approach avoids the need for explicit covariance matrix estimation, which is computationally expensive for large-scale problems. The method leverages the structure of the stochastic optimization problem and the properties of the AI-SSQP algorithm to ensure that the scaled statistic converges to a known distribution, enabling valid inference without storing historical data.

## Foundational Learning
- **AI-SSQP algorithm**: Stochastic Sequential Quadratic Programming with adaptive stepsizes and sketching for solving constrained optimization problems
  - Why needed: Provides second-order convergence while handling constraints in stochastic settings
  - Quick check: Verify convergence of iterates to true solution under different problem settings
- **Random scaling matrix**: Matrix constructed from averaged iterate differences that enables pivotal test statistic construction
  - Why needed: Allows inference without explicit covariance estimation
  - Quick check: Verify positive definiteness and convergence properties of V_t
- **Sketched Newton system**: Approximation of Newton system using randomized Kaczmarz iterations
  - Why needed: Reduces computational cost while maintaining convergence properties
  - Quick check: Compare convergence with exact vs. sketched Newton systems

## Architecture Onboarding

**Component Map:** ξ_t -> F(x_t, ξ_t) -> ∇F(x_t, ξ_t), H̄_t -> (8) -> Δ̄x_t, Δ̄λ_t -> (x_{t+1}, λ_{t+1}) -> V_t -> CI

**Critical Path:** Stochastic sampling → AI-SSQP iteration → Averaged iterates → Random scaling matrix → Confidence intervals

**Design Tradeoffs:** The method trades off exact second-order information for computational efficiency through sketching (τ < ∞) while maintaining statistical validity. The random scaling technique avoids expensive covariance estimation but requires careful construction of the scaling matrix to ensure pivotality.

**Failure Signatures:** Undercoverage (<90%) when using inexact SSQP with plug-in covariance estimators; non-convergence when τ ≲ d; singular V_t when w aligns with constraint normal direction.

**Three First Experiments:**
1. Implement linear regression with only linear constraints to verify basic functionality
2. Compare coverage rates for τ = ∞ (exact) vs τ = 40 (sketched) to validate sketching benefits
3. Test computational cost scaling with dimension d to verify O((d+m)²) complexity

## Open Questions the Paper Calls Out

**Open Question 1:** Can the random scaling inference procedure be extended to inequality-constrained stochastic optimization problems? The current framework handles only equality constraints; extending to inequality constraints introduces active-set identification challenges.

**Open Question 2:** Does the random scaling procedure remain valid under Markovian (non-i.i.d.) data sampling? Current proofs rely on i.i.d. assumptions; dependency structures in Markovian data require different technical tools.

**Open Question 3:** How does the random scaling method scale to high-dimensional settings where dimension d grows with sample size t? The O((d+m)²) computational cost and asymptotic normality proofs assume fixed dimension; rates and validity under d → ∞ are unknown.

**Open Question 4:** What causes the overcoverage phenomenon observed in certain design configurations (e.g., d=40, Equi-correlation design)? The paper acknowledges this anomaly but does not explain its mechanism or predict when overcoverage occurs.

## Limitations
- Hessian regularization scheme details not fully specified (δ_t threshold for positive definiteness)
- Adaptive stepsize selection beyond safeguard condition relies on external references without algorithmic details
- Handling of nonlinear constraint ∥x∥₂ = R² within SQP framework requires additional implementation considerations
- Overcoverage observed in certain configurations without clear explanation of mechanism

## Confidence

**High confidence:** Computational complexity claims (O((d+m)²) vs O((d+m)³)), basic convergence properties of AI-SSQP algorithm, matrix-free implementation

**Medium confidence:** Theoretical guarantees of random scaling technique, coverage rate claims across all configurations, robustness to distributional assumptions

**Low confidence:** Exact implementation details for Hessian regularization and stepsize selection, behavior under non-standard problem configurations, high-dimensional scaling properties

## Next Checks

1. Verify coverage rates across different constraint configurations (linear-only, nonlinear-only, both) and problem dimensions to assess robustness beyond the reported d ∈