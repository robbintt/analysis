---
ver: rpa2
title: First-Order Representation Languages for Goal-Conditioned RL
arxiv_id: '2512.19355'
source_url: https://arxiv.org/abs/2512.19355
tags:
- goal
- state
- learning
- lifted
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces three variants of Hindsight Experience Replay
  (HER) for generalized planning: state HER, propositional HER, and lifted HER. The
  core idea is to relabel unsuccessful trajectories with alternative goals to enable
  more data-efficient learning in sparse reward environments.'
---

# First-Order Representation Languages for Goal-Conditioned RL

## Quick Facts
- arXiv ID: 2512.19355
- Source URL: https://arxiv.org/abs/2512.19355
- Reference count: 18
- Propositional HER solved 82.4% of instances vs 51.0% for state HER

## Executive Summary
This work introduces three variants of Hindsight Experience Replay (HER) for generalized planning: state HER, propositional HER, and lifted HER. The method exploits first-order relational representations to generate curriculum-like learning sequences by progressively identifying harder goals from unsuccessful trajectories. The primary results show that propositional and lifted HER significantly outperform state HER, with learned policies often producing shorter solutions than the baseline LAMA planner while solving more instances overall.

## Method Summary
The paper proposes three variants of Hindsight Experience Replay that leverage first-order relational representations for generalized planning. State HER relabels unsuccessful trajectories with alternative goals at the state level, while propositional HER operates at the propositional level and lifted HER uses lifted representations. The approach generates an automatic curriculum by progressively identifying harder goals from unsuccessful trajectories, enabling learning in domains too large for prior methods that required precomputing optimal values.

## Key Results
- Propositional HER solved 82.4% of instances versus 51.0% for state HER
- Lifted HER achieved 67.2% success rate, showing domain-dependent performance
- Learned policies often produced shorter solutions than LAMA planner while solving more instances (20.2% vs 82.4%)

## Why This Works (Mechanism)
The method works by exploiting first-order relational representations to relabel unsuccessful trajectories with alternative goals, creating a curriculum-like learning sequence. By identifying harder goals from unsuccessful trajectories, the approach enables more data-efficient learning in sparse reward environments. The first-order representations allow for generalization across similar states and goals, improving sample efficiency compared to state-based approaches.

## Foundational Learning
- **Hindsight Experience Replay (HER)**: Relabels unsuccessful trajectories with alternative goals to enable learning from failed attempts. Needed because sparse rewards make learning difficult; quick check: verify relabeling strategy preserves valid goal-state relationships.
- **First-order relational representations**: Express states and goals using logical predicates that generalize across similar situations. Needed for transfer and generalization; quick check: confirm lifted representations maintain semantic equivalence.
- **Curriculum learning through goal difficulty**: Progressively learns from easier to harder goals by selecting from unsuccessful trajectories. Needed for efficient exploration; quick check: verify curriculum ordering improves convergence.

## Architecture Onboarding
- **Component map**: Agent -> Experience Replay Buffer -> Goal Selector -> Policy Network -> Environment
- **Critical path**: Policy execution → Environment step → Store transition → Goal relabeling → Update policy
- **Design tradeoffs**: First-order representations enable generalization but require symbolic preprocessing; relabeling strategy affects curriculum quality but may introduce bias.
- **Failure signatures**: Poor curriculum selection leads to learning plateaus; incorrect goal relabeling causes policy confusion; symbolic preprocessing errors break generalization.
- **First experiments**: 1) Verify relabeling preserves valid goal relationships, 2) Test curriculum ordering impact on learning curves, 3) Compare generalization across structurally similar goals.

## Open Questions the Paper Calls Out
None

## Limitations
- Results confined to synthetic planning domains, unclear if advantages generalize to real-world problems with noise and partial observability
- Substantial performance variation across domains (67.2% vs 82.4%) suggests benefits depend heavily on domain structure
- Reliance on pre-specified action models and complete state information limits applicability to domains with unknown dynamics

## Confidence
- **Data efficiency claims**: Medium - methodology sound but results domain-specific
- **Curriculum generation effectiveness**: Medium - promising but lacks formal guarantees about quality or convergence
- **Generality of first-order benefits**: Low - narrow experimental scope limits conclusions

## Next Checks
1. Evaluate on continuous control domains with partial observability to test robustness beyond symbolic planning
2. Compare against alternative curriculum learning methods that don't require hindsight relabeling to isolate the contribution
3. Conduct ablation studies varying the goal sampling strategy during relabeling to quantify its impact on learning efficiency and solution quality