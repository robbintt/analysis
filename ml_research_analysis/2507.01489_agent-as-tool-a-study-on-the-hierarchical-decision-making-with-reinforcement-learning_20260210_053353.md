---
ver: rpa2
title: 'Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement
  Learning'
arxiv_id: '2507.01489'
source_url: https://arxiv.org/abs/2507.01489
tags:
- search
- tool
- reasoning
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agent-as-Tool, a hierarchical reinforcement
  learning framework that decouples reasoning and tool usage in multi-hop question
  answering. By separating responsibilities between a Planner (for reasoning) and
  a Toolcaller (for tool invocation), the approach addresses challenges in existing
  methods that jointly handle both processes.
---

# Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.01489
- Source URL: https://arxiv.org/abs/2507.01489
- Authors: Yanfei Zhang
- Reference count: 7
- Surpasses Search-R1 by 4.8% exact match and 3.2% cover exact match on Bamboogle benchmark

## Executive Summary
Agent-as-Tool introduces a hierarchical reinforcement learning framework that decouples reasoning from tool usage in multi-hop question answering. By separating responsibilities between a Planner (for reasoning) and a Toolcaller (for tool invocation), the approach addresses challenges in existing methods that jointly handle both processes. The framework uses GRPO to fine-tune the Planner with structured observations from the Toolcaller, improving reasoning clarity and efficiency. Experiments on datasets like Bamboogle show state-of-the-art performance with 63.2% exact match and 75.2% cover exact match.

## Method Summary
The framework trains a Planner using GRPO on 180 samples from HotpotQA and 2WikiMultiHopQA, with the Planner emitting reasoning and tool-calling queries wrapped in `<tool calling>` tags. A Toolcaller agent (GPT-4o-mini based) executes these queries, retrieves web search results, and returns structured summaries as `<obs>` blocks. The Planner operates on these cleaner inputs rather than raw tool outputs. During training, observation masking replaces `<obs>` content with `<fim_pad>` tokens to prevent reward leakage. The approach achieves convergence after ~30 training steps with minimal data requirements.

## Key Results
- 63.2% exact match and 75.2% cover exact match on Bamboogle benchmark
- Surpasses Search-R1 by 4.8% and 3.2% respectively
- Achieves results with only 180 training samples
- Training converges after approximately 30 steps

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decoupling of Reasoning and Tool Execution
Separating reasoning (Planner) from tool invocation (Toolcaller) reduces training complexity and improves reasoning accuracy. The Planner emits natural language sub-queries wrapped in `<tool calling>` tags; the Toolcaller executes them and returns structured summaries as `<obs>` blocks. This allows the Planner to operate on cleaner inputs rather than raw tool outputs. If tool outputs are already structured/low-noise, decoupling may yield diminishing returns.

### Mechanism 2: Structured Observation Processing
Pre-processing raw tool outputs through a dedicated Toolcaller reduces reasoning noise and improves multi-hop decomposition. The Toolcaller (GPT-4o-mini based) retrieves top-k results and returns structured summaries, filtering redundant symbols and unrelated details before passing to Planner. Raw search results contain task-irrelevant tokens that degrade chain-of-thought reasoning. If downstream tasks require full raw context (e.g., fact verification), summarization may lose critical signals.

### Mechanism 3: GRPO Fine-tuning with Observation Masking
Lightweight RL fine-tuning (180 samples) improves Planner's question decomposition and tool-calling decisions. GRPO optimizes Planner policy; `<obs>` blocks are masked with `<fim pad>` during training to prevent reward leakage from Toolcaller outputs. Credit assignment is cleaner when Toolcaller outputs don't influence reward gradients. If Toolcaller quality varies significantly, masking may hide useful learning signals.

## Foundational Learning

- **Reinforcement Learning from Human Feedback (RLHF) / GRPO**
  - Why needed here: The Planner is fine-tuned using GRPO, a PPO variant with KL regularization. Understanding advantage estimation and clipping is essential for debugging training instability.
  - Quick check question: Can you explain why the KL penalty term (β) matters for preventing policy collapse?

- **ReAct-style Tool Calling**
  - Why needed here: The framework extends ReAct patterns (reason → act → observe) but delegates "act" to a subordinate agent. Prior familiarity with ReAct helps trace the execution flow.
  - Quick check question: How does Agent-as-Tool's separation differ from standard ReAct where one model handles all steps?

- **Multi-hop Question Decomposition**
  - Why needed here: The benchmarks (HotpotQA, Bamboogle, etc.) require decomposing questions into sub-questions. The Planner must learn this structure.
  - Quick check question: Given "Who was the mother of the husband of Queen Victoria?", what are the two hops?

## Architecture Onboarding

- **Component map**: Question → Planner (Qwen-2.5-7B-Instruct) → `<tool calling>` → Toolcaller (GPT-4o-mini) → `<obs>` → Planner → Answer
- **Critical path**: 1) Planner receives question + conversation history 2) Planner generates `<thinkquiry>` reasoning and `<tool calling>` query 3) Toolcaller executes search, returns structured `<obs>` 4) Loop continues until Planner emits final answer 5) During training: `<obs>` masked, reward computed (F1 if correct, -2 if malformed)
- **Design tradeoffs**: Using GPT-4o-mini for Toolcaller adds API dependency but provides high-quality summarization; 180-sample training is efficient but may not generalize to out-of-distribution tool types; observation masking prevents reward leakage but hides Toolcaller quality signals
- **Failure signatures**: Planner fails to decompose multi-hop questions → returns incomplete answers; training loss unstable for first 30 steps → expected with small data; Toolcaller returns irrelevant summaries → Planner hallucinates or loops
- **First 3 experiments**: 1) Baseline sanity check: Run Direct IO and CAMEL + Web Search on Bamboogle to reproduce Table 1 metrics within ±2%; 2) Ablation on observation masking: Train Planner with and without `<obs>` masking; compare EM/CEM on held-out set to validate Section 3.2.2 claim; 3) Toolcaller swap: Replace GPT-4o-mini Toolcaller with a smaller model (e.g., Qwen-2.5-3B) and measure performance drop to quantify Toolcaller quality impact

## Open Questions the Paper Calls Out
- **Question**: Can the hierarchical Agent-as-Tool framework scale effectively when the Planner must dynamically orchestrate multiple heterogeneous tools (e.g., calculators, code interpreters, web search) rather than a single search tool?
- **Question**: Does training stability and performance improve significantly with larger training datasets, or does the 180-sample efficiency generalize?
- **Question**: Would applying reinforcement learning to the Toolcaller, rather than keeping it fixed as a GPT-4o-mini CAMEL agent, further improve end-to-end performance?

## Limitations