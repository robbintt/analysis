---
ver: rpa2
title: A RAG-Based Institutional Assistant
arxiv_id: '2501.13880'
source_url: https://arxiv.org/abs/2501.13880
tags:
- documents
- chunk
- chunks
- questions
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a retrieval-augmented generation (RAG) system\
  \ for answering questions about the University of S\xE3o Paulo. The authors developed\
  \ a modular architecture combining a retriever and a generative model, testing different\
  \ components and chunk sizes."
---

# A RAG-Based Institutional Assistant

## Quick Facts
- arXiv ID: 2501.13880
- Source URL: https://arxiv.org/abs/2501.13880
- Reference count: 0
- Primary result: RAG system achieves 30% Top-5 retrieval accuracy and 22.04% generative score, improving to 54.02% when correct chunks are provided

## Executive Summary
This paper presents a retrieval-augmented generation system designed to answer questions about the University of SÃ£o Paulo. The authors developed a modular architecture combining a retriever and generative model, testing various configurations including different chunk sizes and retrieval components. The system was evaluated on institutional knowledge, demonstrating both the potential and current limitations of RAG approaches for domain-specific applications.

## Method Summary
The authors implemented a modular RAG architecture consisting of a document retrieval component and a generative model. They experimented with different chunk sizes for document segmentation and tested various retrieval strategies, including the Jina AI retriever. The system processes queries by first retrieving relevant document chunks from an institutional corpus, then generating answers using these retrieved documents as context. The evaluation framework included both retrieval accuracy metrics and generative model performance against ground truth answers.

## Key Results
- Top-5 retrieval accuracy of 30% on institutional corpus
- Generative model score of 22.04% against ground truth answers
- Significant improvement to 54.02% accuracy when correct document chunks are provided

## Why This Works (Mechanism)
The RAG architecture leverages the strengths of both retrieval and generation systems. The retriever identifies relevant institutional documents that provide factual grounding for the generative model. This combination allows the system to access up-to-date institutional knowledge while maintaining the natural language generation capabilities of large language models. The modular design enables independent optimization of retrieval and generation components.

## Foundational Learning
- **Document Chunking**: Breaking large documents into smaller, semantically coherent pieces is crucial for efficient retrieval. Why needed: Enables targeted retrieval and reduces computational overhead. Quick check: Verify that chunks maintain topic coherence and are appropriately sized for embedding models.

- **Vector Embeddings**: Converting text to numerical representations for similarity comparison. Why needed: Enables semantic search beyond keyword matching. Quick check: Test embedding quality by measuring similarity scores between semantically related documents.

- **Retrieval-Augmentation**: Combining retrieved context with generative capabilities. Why needed: Grounds LLM responses in factual information. Quick check: Measure performance improvement when providing ground truth chunks.

## Architecture Onboarding

**Component Map**: Query -> Retriever -> Document Chunks -> Generative Model -> Answer

**Critical Path**: Query processing through retrieval to generation represents the core workflow. Document retrieval quality directly impacts generative model performance, making it the most critical component to optimize.

**Design Tradeoffs**: 
- Chunk size vs. retrieval precision: Smaller chunks improve precision but may lose context
- Embedding model choice affects semantic understanding
- Retrieval strategy (sparse vs dense) impacts both speed and accuracy

**Failure Signatures**: 
- Poor retrieval manifests as low Top-5 accuracy and poor generative scores
- Generation failures occur when retrieved chunks lack relevant information
- Chunking issues cause loss of semantic coherence across document segments

**3 First Experiments**:
1. Test retrieval accuracy across different chunk sizes (100, 200, 500 tokens)
2. Compare performance with ground truth chunks at 25%, 50%, 75%, 100% rates
3. Evaluate different embedding models on the same retrieval task

## Open Questions the Paper Calls Out
None

## Limitations
- Low retrieval accuracy (30% Top-5) indicates fundamental challenges in institutional corpus search
- Generative model performs poorly without perfect retrieval, suggesting high dependency
- Paper lacks detailed analysis of why retrieval fails and which components need improvement

## Confidence
- Retrieval System: Low - Poor baseline performance (30% Top-5) with no diagnostic analysis
- Generative Model: Medium - Performance significantly improves (22.04% to 54.02%) with correct chunks, suggesting generation quality is adequate when given good input
- Evaluation Metrics: Low - Limited depth in metrics and lack of comprehensive error analysis

## Next Checks
1. Conduct detailed error analysis of retrieval failures, categorizing issues by chunking strategy, embedding quality, or semantic mismatch
2. Test alternative retrieval architectures including hybrid sparse-dense approaches and cross-encoder reranking
3. Perform controlled experiments with ground truth chunks at varying rates (0%, 25%, 50%, 75%, 100%) to establish retrieval-generations performance relationship