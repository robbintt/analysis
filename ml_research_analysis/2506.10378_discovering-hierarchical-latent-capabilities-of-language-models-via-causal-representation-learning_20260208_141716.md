---
ver: rpa2
title: Discovering Hierarchical Latent Capabilities of Language Models via Causal
  Representation Learning
arxiv_id: '2506.10378'
source_url: https://arxiv.org/abs/2506.10378
tags:
- causal
- mmlu
- figure
- matrix
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causal representation learning framework
  to uncover hierarchical latent capabilities in language models. The method models
  observed benchmark performance as a linear transformation of latent capability factors,
  with a three-node causal structure (general reasoning, instruction-following, and
  mathematical reasoning).
---

# Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning

## Quick Facts
- **arXiv ID:** 2506.10378
- **Source URL:** https://arxiv.org/abs/2506.10378
- **Reference count:** 40
- **Key outcome:** This paper introduces a causal representation learning framework to uncover hierarchical latent capabilities in language models.

## Executive Summary
This work introduces a causal representation learning framework to uncover hierarchical latent capabilities in language models. The method models observed benchmark performance as a linear transformation of latent capability factors, with a three-node causal structure (general reasoning, instruction-following, and mathematical reasoning). Applied to over 1500 models from the Open LLM Leaderboard, the approach identifies interpretable capability hierarchies and demonstrates that controlling for base models is essential for accurate causal analysis. Key findings include a clear causal progression from foundational general capabilities to specialized skills, and empirical validation that instruction-following enhances mathematical reasoning ability. The method achieves a maximum inexactness coefficient of 0.04 and provides actionable insights for targeted capability development.

## Method Summary
The Hierarchical Component Analysis (HCA) framework recovers a linear Structural Causal Model (SCM) representing hierarchical latent capabilities from observed benchmark performance. The method first runs Independent Component Analysis (ICA) independently on each base model's domain to obtain unmixing matrices. A row-residual extraction step then identifies shared latent structure across domains, recovering a mixing matrix and domain-specific weight matrices that encode a directed acyclic graph among capabilities. The framework requires that the latent capability factors follow a linear SCM invariant across a subset of base models, with non-Gaussian noise variables.

## Key Results
- HCA framework achieves maximum inexactness coefficient of 0.04 when recovering latent capability factors
- Identified three-factor causal hierarchy: general reasoning → instruction-following → mathematical reasoning
- Empirical validation shows instruction-following fine-tuning increases mathematical reasoning performance
- Controlling for base model as confounder is essential for valid causal inference about capability relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hierarchical Component Analysis (HCA) can recover interpretable latent capability factors and their causal structure by exploiting heterogeneity across base models, provided certain identifiability conditions hold.
- **Mechanism:** The framework assumes observed benchmark performance is a linear transformation of a few latent capability factors. By applying ICA independently to each base model's domain, it obtains unmixing matrices. A row-residual extraction step then identifies shared latent structure across domains, recovering a mixing matrix and domain-specific weight matrices that encode a directed acyclic graph (DAG) among capabilities.
- **Core assumption:** The latent capability factors follow a linear (approximately exact) structural causal model (SCM) that is invariant across a subset of base models, and the noise variables are non-Gaussian.
- **Evidence anchors:** [abstract] "observed benchmark performance is modeled as a linear transformation of a few latent capability factors... identified as causally interrelated after appropriately controlling for the base model"; [section 3.1] Describes the ICA-based unmixing, row-residual extraction, and permutation alignment steps of HCA.

### Mechanism 2
- **Claim:** Controlling for the base model as a confounder is essential for valid causal inference about capability relationships.
- **Mechanism:** The base model (e.g., Llama-3-8B, Qwen2.5-7B) influences all downstream capabilities via pretraining. Without conditioning on it, correlations between capabilities (e.g., instruction-following and math reasoning) could be spurious. The method explicitly models the base model as a shared latent parent in the SCM.
- **Core assumption:** The base model is a primary source of variation (confounder) that affects multiple capabilities simultaneously; its effect can be isolated by analyzing models within the same base model family.
- **Evidence anchors:** [abstract] "underscore the essential role of carefully controlling base model variations during evaluation"; [section 2] PCA analysis shows distinct heterogeneity in PC subspaces across different base model families.

### Mechanism 3
- **Claim:** The three identified latent factors (z1: general reasoning, z2: instruction-following, z3: math reasoning) may form a causal chain where z1 → z2 → z3, implying instruction-tuning can indirectly improve math performance.
- **Mechanism:** After HCA recovers latent factors, their causal graph weights and correlations with specific benchmarks (e.g., IFEval, MATH Lvl 5) are used to assign semantic interpretations. Interventional proxies (e.g., IFEval SFT) are then analyzed for alignment with predicted causal effects.
- **Core assumption:** The linear mapping from latent factors to benchmarks is stable enough that factor-benchmark correlations reflect underlying capabilities, and fine-tuning interventions approximate do-operations on latent factors.
- **Evidence anchors:** [section 4.2] Interprets z1 via BBH/MMLU-Pro scaling laws, z2 via IFEval correlations, and z3 via MATH Lvl 5 correlations. Table 2 shows IFEval SFT increases MATH scores for some models.

## Foundational Learning

**Concept: Structural Causal Models (SCMs)**
- **Why needed here:** The entire framework formalizes capability relationships as a linear SCM. Understanding DAGs, exogenous noise, and causal edges is critical.
- **Quick check question:** Can you explain what an edge A → B represents in a causal DAG within the SCM framework?

**Concept: Independent Component Analysis (ICA)**
- **Why needed here:** HCA uses ICA as a first step to unmix observed benchmarks into approximately independent source variables per domain.
- **Quick check question:** What assumption does ICA make about the source signals to achieve identifiability?

**Concept: Causal Representation Learning (CRL)**
- **Why needed here:** This work is an instance of CRL—recovering latent causal variables from multi-domain observational data.
- **Quick check question:** How does CRL differ from standard representation learning methods like PCA or autoencoders?

## Architecture Onboarding

**Component map:**
1. Data Preprocessing: Group leaderboard models by base model, extract benchmark accuracy vectors
2. Domain PCA/ICA: Run PCA/ICA per base model to check low-rank structure and obtain unmixing matrices
3. HCA Core: Row-residual extraction across domains → permutation search → best DAG fit (minimize MIC)
4. Interpretation: Regress latent factors on benchmarks, analyze fine-tuning interventions as proxies

**Critical path:** Validating the invariant domain set (`S_inv`) → running HCA → interpreting factors and causal edges → checking generalizability to other base models

**Design tradeoffs:**
- MIC vs. interpretability: Lower MIC indicates better model fit but may require excluding some base models, reducing generality
- Latent dimension (d0): Choosing d0=3 balances explained variance and simplicity; higher d0 may fit better but loses interpretability
- Linear SCM assumption: Enables identifiability but may oversimplify non-linear capability interactions

**Failure signatures:**
- High MIC (>0.1) suggests the linear causal model poorly explains the data
- Recovered factors correlate weakly with known benchmarks or fail out-of-sample tests on other base models
- Interventions (e.g., SFT) produce effects inconsistent with predicted causal edges

**First 3 experiments:**
1. Reproduce the HCA analysis on the provided leaderboard data for the four base models and verify the recovered causal graph matches Figure 7
2. Add/remove one base model and observe changes in MIC and causal graph structure
3. Fine-tune a new model on IFEval data and measure performance changes across all six benchmarks to compare with predicted effects

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can fine-tuning strategies be developed that selectively enhance mathematical reasoning capability (z3) without causing catastrophic forgetting or negatively impacting foundational general capabilities (z1) and instruction-following (z2)?
- Basis in paper: [explicit] The authors state in Section 4.2 that "Identifying fine-tuning strategies that selectively enhance mathematical reasoning (z3) without negatively impacting other core capabilities (z1, z2) remains a critical open question."
- Why unresolved: Targeted mathematical fine-tuning often leads to catastrophic forgetting, degrading performance on the general tasks the paper identifies as causal parents to math reasoning.
- What evidence would resolve it: A fine-tuning method that results in statistically significant gains on the MATH benchmark while showing no degradation (or gains) on BBH and IFEval benchmarks within the same model.

**Open Question 2**
- Question: How can causal inference methodologies be adapted to handle selection bias when limited to observational benchmark data, specifically addressing violations of conditional ignorability?
- Basis in paper: [explicit] Remark 3 asks: "how might we circumvent this methodological challenge [selection bias] when limited to observational performance data? Developing robust approaches that account for such selection biases represents a significant opportunity for future work."
- Why unresolved: The paper notes that conditional ignorability is likely violated because researchers may preferentially select specific base models for specific tasks, confounding the estimation of true average treatment effects.
- What evidence would resolve it: A causal estimation technique that successfully adjusts for this selection bias, validated by its ability to predict fine-tuning results on out-of-distribution model-task pairings.

**Open Question 3**
- Question: How can latent capability factors be identified when the underlying structure involves correlated factors that do not possess a direct causal relationship (e.g., two distinct parent nodes)?
- Basis in paper: [explicit] Appendix G.2 notes the restriction of the current model: "It will be an interesting future direction to investigate how to identify the latent factors in these cases."
- Why unresolved: The current HCA algorithm assumes a specific linear SCM structure where capabilities are arranged in a strict hierarchy; it cannot disentangle factors that are merely correlated but independent causes of a third factor.
- What evidence would resolve it: An extension of the HCA algorithm that successfully recovers a "V-structure" (two parents causing one child) without false positive causal edges between the parents.

## Limitations
- Linear SCM assumption may oversimplify complex capability interactions, particularly for larger capability sets
- Method's generalizability to base models outside the studied set (Llama-3 and Qwen2.5 families) remains unproven
- Intervention proxies provide indirect evidence of causal relationships but do not directly manipulate latent factors

## Confidence

**High Confidence:** The HCA framework's ability to recover interpretable latent factors when identifiability conditions hold (MIC ≈ 0.04)

**Medium Confidence:** The proposed causal hierarchy (general → instruction-following → math reasoning) based on benchmark correlations and intervention proxies

**Low Confidence:** Generalizability of findings to base models not included in the invariant domain set and to capability structures with more than three latent factors

## Next Checks
1. **Generalizability Test:** Apply HCA to a new base model family (e.g., Mistral or Gemma) to verify if the three-factor structure and causal hierarchy replicate
2. **Nonlinear Extension:** Implement a nonlinear variant of HCA (e.g., using kernel ICA or autoencoders) to test whether the causal structure persists beyond linear assumptions
3. **Direct Intervention Experiment:** Design a fine-tuning study that explicitly manipulates instruction-following and general reasoning capabilities separately, measuring downstream effects on mathematical reasoning to directly test the proposed causal chain