---
ver: rpa2
title: 'CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences'
arxiv_id: '2511.07691'
source_url: https://arxiv.org/abs/2511.07691
tags:
- capo
- reward
- preference
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAPO, a multilingual preference optimization
  method that improves upon DPO by incorporating a relative reward margin into the
  loss function. The approach dynamically scales the learning signal based on confidence
  in preference pairs, addressing the limitation of treating all preference margins
  equally.
---

# CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences

## Quick Facts
- arXiv ID: 2511.07691
- Source URL: https://arxiv.org/abs/2511.07691
- Reference count: 16
- Key outcome: CAPO improves multilingual preference alignment by 16-33% over DPO baselines through dynamic loss scaling based on relative reward margins.

## Executive Summary
CAPO introduces a confidence-aware approach to multilingual preference optimization that dynamically scales the learning signal based on the relative reward margin between preferred and dispreferred responses. By incorporating a log-ratio term into the DPO loss function, CAPO addresses the limitation of treating all preference margins equally, instead emphasizing confident preference pairs while softening gradients on ambiguous cases. The method demonstrates particular effectiveness in languages with lower BLEU scores between preferred and dispreferred samples, improving alignment by widening the gap between response quality levels.

## Method Summary
CAPO modifies the standard DPO objective by adding a Relative Reward Margin (RRM) term that uses the log-likelihood ratio of preferred to dispreferred responses. This creates a confidence-aware loss that scales based on the ratio of log-probabilities rather than absolute differences, compensating for tokenization rate disparities across languages. The approach uses Llama-3.1-8B-Instruct with LoRA adapters, trained on MTPE data from DIVEMT and MLQE-PE datasets across 8 translation directions. Training employs a fixed α=2.0 hyperparameter and reference-free optimization, with evaluation on multilingual benchmarks including MT-Bench, XLSum, and M-IFEval.

## Key Results
- 16-33% higher reward accuracy compared to DPO and SimPO baselines
- Improved alignment by widening the gap between preferred and dispreferred responses
- Particularly effective in languages with lower BLEU scores between preference pairs
- Inverse correlation (r=-0.47) between BLEU scores and CAPO-vs-DPO reward shift

## Why This Works (Mechanism)

### Mechanism 1
Dynamic loss scaling via Relative Reward Margin (RRM) improves optimization by emphasizing confident preference pairs and softening gradients on ambiguous cases. The log-ratio term α·logπ(yw|x)/logπ(yl|x) increases loss when margins are small (model uncertain) and decreases it when margins are large (model already aligned). Core assumption: log-likelihood ratios capture meaningful confidence information. Evidence: modulation according to confidence pairs, RRM as adaptive loss modulator, related work on difficulty-aware weighting. Break condition: ratio instability when log-probabilities approach zero; overfitting when α > 2.0.

### Mechanism 2
RRM compensates for tokenization rate disparities across languages that cause unfair reward signals in standard DPO. Languages with longer tokenizations get smaller per-token log-probs, leading to smaller Δr in DPO and larger penalties. RRM normalizes via ratio rather than absolute difference, reducing tokenization-induced bias. Core assumption: tokenization rate differences are a meaningful source of cross-linguistic reward variance. Evidence: theoretical justification about reward differences, RRM adjusting on per-example basis. Break condition: if preference quality varies systematically with language, RRM may overcorrect.

### Mechanism 3
CAPO increases reward separation (Δr) between preferred and dispreferred responses, particularly for languages where preference pairs are more distinguishable. The confidence-aware objective encourages clearer separation by amplifying gradients on decisive preferences. Languages with lower BLEU between preferred/dispreferred benefit most. Core assumption: larger reward separation correlates with better alignment outcomes. Evidence: inverse correlation with BLEU scores, distribution shift visual showing broader spread, related CRPO findings. Break condition: if increased separation reflects reward hacking rather than genuine preference alignment.

## Foundational Learning

- **Bradley-Terry preference modeling**: Why needed: CAPO builds on DPO, which derives from Bradley-Terry objective treating preference probability as sigmoid of reward difference. Quick check: Can you explain why L_BT = -logσ(r(yw) - r(yl)) is the standard preference loss?

- **Log-likelihood ratios vs. differences**: Why needed: CAPO's key innovation is adding a log-ratio term alongside the log-difference term; understanding their distinct properties is essential. Quick check: Given logπ(yw|x) = -2.0 and logπ(yl|x) = -4.0, what are the difference, ratio, and log-ratio?

- **Cross-linguistic tokenization variance**: Why needed: The paper's motivation rests on tokenization rates affecting reward signals differently across languages. Quick check: Why might a sentence in Nepali produce different per-token log-probabilities than its English translation in the same model?

## Architecture Onboarding

- **Component map**: Input triplets (x, yw, yl) → Policy model πθ (Llama-3.1-8B-Instruct with LoRA) → Loss computation (DPO + RRM terms) → Fine-tuned policy

- **Critical path**: 1) Format preference pairs using chat template 2) Compute logπ(yw|x) and logπ(yl|x) from policy 3) Calculate RRM = logπ(yw|x) / logπ(yl|x) 4) Combine: L_CAPO = -logσ(β(Δlogπ) + α·RRM) 5) Backprop through LoRA parameters only

- **Design tradeoffs**: α tuning: Higher α (up to 2.0) reduces training loss but risks overfitting; no length normalization to avoid unfair tokenization effects; no reference model for simpler deployment; small training set (800 samples) for efficiency

- **Failure signatures**: α > 2.0 causes evaluation accuracy to drop sharply; English performance worse than DPO due to EN→other-languages MTPE data; negative Δr persisting indicates model failing to learn preference

- **First 3 experiments**: 1) Validate RRM effect by running DPO vs CAPO on held-out pairs and plotting reward difference distributions per language 2) α sensitivity sweep on validation set to identify optimal point before accuracy drop 3) Cross-lingual transfer test by training on subset of languages and evaluating on held-out languages

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed decrease in CAPO's reward accuracy for English (`en`) stem specifically from the use of English-to-other-language MTPE training data, and would native English preference data resolve this discrepancy? The authors identify the anomaly but do not conduct an ablation study with native English data to confirm the hypothesis.

### Open Question 2
Can the reweighting coefficient (α) be adapted dynamically or per-language to optimize the trade-off between training loss minimization and evaluation accuracy? The paper uses a single static value across all languages without exploring if different α values might be optimal for different languages or dataset sizes.

### Open Question 3
Do CAPO's improvements in training stability and alignment persist when using full parameter fine-tuning instead of LoRA? The findings are restricted to parameter-efficient fine-tuning, and it's unclear if the confidence-aware optimization signal scales effectively when the model's full capacity is updated.

### Open Question 4
Can high-quality Machine Translated Post Edited (MTPE) data be generated automatically to expand language coverage without manual annotation? The current study relies on existing, limited-scope MTPE datasets, and the authors highlight the need to scale this data source to "many languages that could not be studied in this work."

## Limitations

- The effectiveness of log-likelihood ratios as confidence measures depends on distributional properties of rewards across languages being amenable to RRM normalization
- Evaluation relies on self-evaluation (same model being fine-tuned), which may overestimate actual alignment quality
- Training dataset is relatively small (800 samples across 8 translation directions), raising questions about scalability and generalization
- The specific mechanism of RRM compensating for tokenization disparities lacks direct empirical validation

## Confidence

- **High Confidence**: CAPO improves multilingual preference alignment compared to DPO baselines (supported by multiple independent evaluation metrics)
- **Medium Confidence**: RRM specifically addresses tokenization rate disparities across languages (theoretical justification present but direct evidence lacking)
- **Low Confidence**: Optimal hyperparameter α=2.0 is determined from limited experimentation with sharp performance drop beyond this value

## Next Checks

1. **Cross-lingual tokenization validation**: Measure per-token log-probability distributions for translation pairs across languages to quantify actual tokenization rate disparities CAPO claims to address

2. **Ablation study on RRM components**: Train variants of CAPO with RRM disabled, with only the ratio term, and with only the difference term to determine which component drives improvements

3. **Out-of-distribution language evaluation**: Test CAPO on languages completely unseen during training (e.g., languages from different families) to assess true multilingual generalization versus transfer from related languages