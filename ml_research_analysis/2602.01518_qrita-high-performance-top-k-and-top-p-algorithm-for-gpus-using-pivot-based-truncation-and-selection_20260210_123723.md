---
ver: rpa2
title: 'Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based
  Truncation and Selection'
arxiv_id: '2602.01518'
source_url: https://arxiv.org/abs/2602.01518
tags:
- size
- larger
- block
- mask
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qrita addresses the challenge of efficient Top-k and Top-p sampling
  in large language models by introducing a pivot-based selection algorithm that avoids
  full-vocabulary sorting. It uses Gaussian-based sigma-truncation to reduce the search
  space and quaternary pivot search with duplication handling to ensure deterministic
  output while halving search iterations.
---

# Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection

## Quick Facts
- arXiv ID: 2602.01518
- Source URL: https://arxiv.org/abs/2602.01518
- Reference count: 40
- Primary result: Achieves up to 2× higher throughput and half the memory usage compared to vLLM, SGLang, and FlashInfer

## Executive Summary
Qrita addresses the challenge of efficient Top-k and Top-p sampling in large language models by introducing a pivot-based selection algorithm that avoids full-vocabulary sorting. It uses Gaussian-based sigma-truncation to reduce the search space and quaternary pivot search with duplication handling to ensure deterministic output while halving search iterations. Implemented in Triton, Qrita achieves significant performance gains while maintaining identical outputs to sorting-based algorithms.

## Method Summary
Qrita is a high-performance GPU algorithm for Top-k and Top-p sampling that combines Gaussian-based sigma-truncation with quaternary pivot search and deterministic duplicate handling. The algorithm first applies sigma-truncation to filter out low-probability tokens based on the assumption that LLM logits follow a quasi-Gaussian distribution. It then uses quaternary pivot search (selecting three pivots per iteration) to find the k-th largest element more efficiently than binary search. Finally, it handles duplicate values deterministically using cumulative masking to ensure reproducible outputs. The entire pipeline is implemented in Triton with a persistent kernel design optimized for GPU memory coalescing.

## Key Results
- Achieves up to 2× higher throughput compared to state-of-the-art methods
- Reduces memory usage by approximately 50% compared to vLLM, SGLang, and FlashInfer
- Maintains identical deterministic outputs to sorting-based algorithms across all test cases
- Demonstrates consistent performance gains across multiple LLM architectures (R1-Distill-Llama-8B, Qwen3-8B, GPT-OSS-20B, Gemma3-4B)

## Why This Works (Mechanism)

### Mechanism 1: Gaussian-based σ-truncation
Qrita assumes logit values follow a quasi-Gaussian distribution and computes mean (μ) and standard deviation (σ) to establish a lower-bound cutoff threshold t = μ + δ · σ. Only logits exceeding this threshold are passed to the pivot search, greatly reducing the search space. The core assumption is that target Top-k/Top-p tokens exist in the upper outlier tail. Break condition: If distribution is multi-modal or flat, truncation may miss targets, forcing fallback to full-vocabulary search.

### Mechanism 2: Quaternary Pivot Search
Instead of binary search (1 pivot per iteration), Qrita selects three pivots (25th, 50th, 75th percentiles) and counts elements above them simultaneously. This narrows the search range by roughly 75% per iteration, halving the number of required iterations. Core assumption: The overhead of checking 3 pivots is offset by reduced global memory passes. Break condition: On hardware with scarce registers, 3-pivot overhead may exceed iteration savings, especially for small k.

### Mechanism 3: Deterministic Duplicate Handling via Cumulative Masking
The algorithm tracks minimum values at the pivot boundary and counts duplicates to calculate exactly how many to keep. It generates a removal mask using cumulative sum to discard excess duplicates based on position index, ensuring strict ordering. Core assumption: Users require full candidate sets with strict determinism. Break condition: Floating-point precision issues in parallel reduction may cause inconsistent equality checks, leading to incorrect Top-k counts.

## Foundational Learning

- **Concept: Top-p (Nucleus) Sampling**
  - Why needed: Qrita unifies Top-k and Top-p; understanding that Top-p requires finding dynamic cutoff where cumulative probability mass ≥ p is essential
  - Quick check: How does Qrita handle the difference between searching for count k versus cumulative probability p?

- **Concept: Pivot-based Selection (Quickselect)**
  - Why needed: This is the core alternative to sorting; understanding iterative pivot guessing and counting is crucial
  - Quick check: Why does standard Quickselect fail to be deterministic with duplicate values?

- **Concept: GPU Memory Coalescing & Vectorization**
  - Why needed: Understanding how GPUs load memory in blocks explains why reducing search space size directly improves throughput
  - Quick check: Why does Qrita use tiered block sizes (BLOCK_SIZE vs BLOCK_SIZE_2) in its Triton implementation?

## Architecture Onboarding

- **Component map:** Input Logits → Truncation Layer → Search Layer → Masking Layer → Output
- **Critical path:** The Quaternary Search Loop is where iterative memory reads occur; performance relies on Gaussian Truncation reducing loop iterations
- **Design tradeoffs:** Trades memory (allocating buffer for truncated outliers) for speed (searching smaller array); uses conservative safety margin to ensure correctness at some performance cost
- **Failure signatures:** Infinite loop from precision errors, incorrect output count from duplicate handling bugs, performance cliff from invalid Gaussian assumption
- **First 3 experiments:**
  1. Verify output equivalence between Qrita and sorting-based Top-k on identical logits with duplicates
  2. Measure truncation hit rate across different model outputs to validate Gaussian assumption
  3. Compare binary vs. quaternary search latency on fallback path to quantify iteration reduction benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Qrita be optimized to handle multiple batches within a single Triton program to overcome scalability limits when batch sizes exceed available SMs?
- Basis: Current implementation allocates one program per SM, creating ceiling for throughput scaling with large batches
- Resolution evidence: Benchmarks showing linear scaling for batch sizes >256 with multi-batch kernel implementation

### Open Question 2
- Question: Can computational overhead of Gaussian σ-truncation be reduced to improve latency without compromising hit-rate?
- Basis: Truncation contributes 44% of total runtime (0.4ms of 0.9ms); authors suggest reducing this overhead as future direction
- Resolution evidence: Modified algorithm demonstrating reduced "Zeroth pass" time or comparative analysis against in-place implementation

### Open Question 3
- Question: Does adaptive strategy selecting between binary and quaternary pivot search based on hardware provide significant performance advantage?
- Basis: Quaternary search is slower than binary in fast-success cases due to register pressure; optimal trade-off varies by architecture
- Resolution evidence: Heuristic-based implementation showing consistent improvements across different hardware targets

### Open Question 4
- Question: How can logit distribution modeling be refined for Top-p sampling to reduce variance in hit-rates across diverse LLM architectures?
- Basis: Top-p truncation experiences larger variances in probability and hit-rate across models compared to Top-k
- Resolution evidence: Adaptive profiling mechanism maintaining high, consistent hit-rate for Top-p across evaluated model set

## Limitations
- Gaussian truncation assumption validity is not rigorously validated; lookup table thresholds are empirically tuned with safety margins suggesting anticipated edge cases
- Numerical precision concerns from floating-point equality checks and parallel reduction non-associativity may cause inconsistent results
- Implementation complexity from 450+ line Triton kernel requires manual transcription without official repository, introducing error risk

## Confidence
- **High confidence**: Core algorithmic innovations (quaternary pivot search, duplicate handling) are well-defined and mathematically sound; deterministic output guarantee is testable
- **Medium confidence**: Performance claims based on specific baseline versions and hardware configurations; reproduction may yield different absolute numbers
- **Low confidence**: Gaussian truncation hit-rate and universal applicability of lookup table thresholds across model families remain weakly supported

## Next Checks
1. **Output determinism verification**: Run Qrita and sorting-based Top-k on identical logits with duplicates across multiple seeds; verify identical candidate sets for all k and p values
2. **Truncation hit-rate profiling**: Instrument Qrita to log truncation hit frequency across diverse model outputs; compare actual hit rates against claimed 90%+ efficiency; test safety margin adjustments
3. **Numerical stability stress test**: Create adversarial logit distributions with large dynamic ranges, threshold-clustered values, and many duplicates; verify correct Top-k results and proper termination without infinite loops