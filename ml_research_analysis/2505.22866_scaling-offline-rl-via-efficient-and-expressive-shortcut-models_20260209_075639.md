---
ver: rpa2
title: Scaling Offline RL via Efficient and Expressive Shortcut Models
arxiv_id: '2505.22866'
source_url: https://arxiv.org/abs/2505.22866
tags:
- sorl
- steps
- arxiv
- offline
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of scaling offline reinforcement\
  \ learning (RL) by improving both training efficiency and inference-time flexibility.\
  \ The key contribution is SORL, an algorithm that leverages shortcut models\u2014\
  a novel class of generative models\u2014to achieve expressive policy learning with\
  \ a simple, one-stage training procedure."
---

# Scaling Offline RL via Efficient and Expressive Shortcut Models

## Quick Facts
- arXiv ID: 2505.22866
- Source URL: https://arxiv.org/abs/2505.22866
- Reference count: 40
- Primary result: SORL achieves best performance on 5/8 evaluated environments and demonstrates strong inference-time scaling

## Executive Summary
This paper introduces SORL, an offline RL algorithm that improves both training efficiency and inference-time flexibility through shortcut models. The key innovation is incorporating self-consistency into training, enabling a single-stage training procedure that maintains high performance across varying inference budgets, including one-step inference. SORL theoretically regularizes the policy to the behavior of the offline data in Wasserstein distance and empirically outperforms prior methods on 5 out of 8 evaluated environments while demonstrating strong scaling behavior with inference-time compute.

## Method Summary
SORL uses a shortcut model s_θ(a_t, t, h | x) that conditions on action, timestep, step size, and state to output velocity directions. The method combines three losses: Q-loss for policy optimization, flow-matching loss for behavioral regularization, and self-consistency loss to ensure multi-step fidelity. Training uses M_BTT=8 backpropagation steps, M_disc=8 discretization steps, and M_inf=4 inference steps by default. The algorithm samples inference steps m from Unif{1,...,M_BTT} for Q-loss computation, applies flow-matching at h=1/M_disc, and enforces self-consistency by comparing single large jumps versus two small jumps. Evaluation uses 50 episodes every 100K steps, reporting mean±std over 8 seeds for final 3 evaluations (800K, 900K, 900K steps).

## Key Results
- SORL achieves best performance on 5 out of 8 evaluated environments (40 tasks total)
- Performance improves with increased inference-time compute, enabling reduced training compute through better inference
- SORL generalizes to more inference steps at test time than used during training
- One-stage training procedure eliminates the need for slow iterative sampling or complex two-stage distillation

## Why This Works (Mechanism)

### Mechanism 1: Self-Consistency Enables Variable Inference Budgets
Self-consistency training allows the policy to generate high-quality actions under any inference step budget, including one-step. The self-consistency loss enforces that one large jump of size 2h produces equivalent output to two combined jumps of size h, creating a unified model where multi-step inference is learned by a single network in one training run. This works when self-consistency error ε_SC is small, but degrades if the loss doesn't converge.

### Mechanism 2: Wasserstein Behavioral Regularization via Flow Matching
The combined flow-matching and self-consistency losses implicitly regularize the learned policy to stay close to the behavior policy in 2-Wasserstein distance. Flow matching at the smallest step size (h=1/M) ensures the model tracks the true drift function, while self-consistency propagates this fidelity across larger step sizes. This requires Lipschitz continuity assumptions and small ε_FM/ε_SC errors to maintain distributional fidelity.

### Mechanism 3: Inference-Time Scaling via Q-Function Verification
Additional inference compute (more steps or best-of-N sampling) improves performance even when training compute is reduced. Sequential scaling uses more Euler steps at inference, while parallel scaling samples N actions and selects the highest-Q action, treating the learned Q-function as a verifier for post-hoc policy improvement. This assumes the learned Q-function is sufficiently accurate to rank candidate actions.

## Foundational Learning

- **Flow Matching and ODE-based Generative Models**: SORL builds on flow matching to define drift functions v_t that transport noise to data. Understanding Equation 3 (d z_t / dt = v_t(z_t)) and Euler discretization is essential to grasp how shortcut models modify this process. *Quick check*: Can you explain why smaller step sizes in Euler integration reduce discretization error but increase compute?

- **Offline RL and Distributional Shift**: The core challenge SORL addresses is learning from fixed datasets without exploration. Behavioral regularization prevents the policy from querying out-of-distribution actions where Q-estimates are unreliable. *Quick check*: Why does offline RL require explicit regularization to the behavior policy, unlike online RL?

- **Backpropagation Through Time (BPTT) in Generative Policies**: SORL's efficiency claim rests on reducing M_BTT (steps backpropagated during training). Understanding the cost of BPTT in multi-step diffusion/flow sampling clarifies why this matters. *Quick check*: If a diffusion policy requires 100 denoising steps, what is the memory implication of backpropagating through all steps?

## Architecture Onboarding

- **Component map**: Dataset -> Shortcut network s_θ -> Q-network Q_φ -> Target networks -> Combined losses (L_QL + L_FM + L_SC) -> Policy update
- **Critical path**: 1) Sample batch from offline dataset D. 2) Compute flow-matching target (a_1 - a_0) and self-consistency target via two small-step predictions. 3) Sample inference steps m ~ Unif{1,...,M_BTT} for Q-loss; backpropagate through m steps only. 4) Update actor with combined losses; update critic via TD error.
- **Design tradeoffs**: M_BTT vs. M_disc vs. M_inf: Fewer BTT steps speeds training but may reduce policy quality; more inference steps improves performance but increases latency. Q-loss coefficient: Higher values prioritize return maximization over behavioral fidelity. Distillation vs. unified model: SORL avoids two-stage distillation but requires self-consistency loss to maintain multi-step fidelity.
- **Failure signatures**: Performance degrades at M_inf > M_BTT without best-of-N: Self-consistency may not generalize far beyond trained step counts. Best-of-N fails to improve: Q-function is miscalibrated or policy is already greedy w.r.t. Q. Training instability: Self-consistency loss diverges if step-size sampling or target network construction is incorrect.
- **First 3 experiments**: 1) Ablation over M_BTT (1, 2, 4, 8) with fixed M_inf: Verify training compute vs. performance tradeoff. 2) Sequential scaling sweep: Fix M_BTT=4, vary M_inf ∈ {1, 2, 4, 8}; confirm performance improves with more inference steps. 3) Parallel scaling test: For M_BTT=2, apply best-of-N (N=8) at M_inf=8; check if reduced training compute is recovered.

## Open Questions the Paper Calls Out

- **Can SORL effectively incorporate adaptive test-time scaling mechanisms, such as selecting the number of inference steps dynamically based on Q-function gradients?**: The paper explicitly identifies investigating adaptive test-time scaling, specifically selecting steps based on Q-function gradients, as an avenue for future work. The current implementation uses fixed inference budgets or standard best-of-N sampling.

- **How robust is SORL's parallel scaling capability when the learned Q-function verifier is significantly inaccurate or out-of-distribution?**: While empirically beneficial, the theoretical guarantee of improvement is absent because the verifier is a learned value estimator rather than a ground-truth reward. The paper notes that gains are not guaranteed if the Q-function is imperfect.

- **Can the training runtime of SORL be optimized to match one-step methods like FQL without sacrificing the expressiveness gained from backpropagation through time?**: Appendix A states that SORL generally has a longer training runtime than FQL due to the computational cost of backpropagation through time. The paper prioritizes inference flexibility over training speed.

## Limitations

- Self-consistency may not generalize effectively to inference step counts much larger than those used during training
- Parallel scaling benefits depend on the accuracy of the learned Q-function verifier, which may be miscalibrated
- Theoretical Wasserstein bounds rely on strong Lipschitz assumptions that may not hold in high-dimensional action spaces
- Empirical results are limited to 40 tasks across 8 environments, with no ablation on dataset size or quality

## Confidence

- **Training efficiency gains**: High - verified via M_BTT ablation showing clear tradeoff between training compute and performance
- **Inference-time scaling**: Medium - results show improvement but limited to M_inf ≤ 8, generalization beyond M_BTT untested
- **Theoretical contribution**: Medium - strong regularity assumptions required for Wasserstein bounds may not hold in practice
- **Empirical validation**: Medium - strong results on 40 tasks but limited environment diversity and no dataset size analysis

## Next Checks

1. Test self-consistency generalization by training with M_BTT=2 and evaluating M_inf ∈ {4,8,16}; measure degradation vs. training with M_BTT=M_inf
2. Perform Q-function calibration analysis: compute TD-error and off-policy action-value statistics to explain best-of-N variance
3. Scale dataset size (e.g., 2×, 4× samples) and retrain SORL to verify performance scaling with offline data quantity