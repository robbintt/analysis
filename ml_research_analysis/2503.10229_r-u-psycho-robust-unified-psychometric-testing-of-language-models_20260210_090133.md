---
ver: rpa2
title: R.U.Psycho? Robust Unified Psychometric Testing of Language Models
arxiv_id: '2503.10229'
source_url: https://arxiv.org/abs/2503.10229
tags:
- answer
- question
- option
- options
- statement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R.U.Psycho is a framework for robust, reproducible psychometric
  testing of generative language models using configurable JSON-based experiments.
  It supports multiple models, prompt templates, and postprocessing pipelines including
  cleaners, validators, and judges for mapping responses to questionnaire answer options.
---

# R.U.Psycho? Robust Unified Psychometric Testing of Language Models

## Quick Facts
- arXiv ID: 2503.10229
- Source URL: https://arxiv.org/abs/2503.10229
- Reference count: 40
- One-line primary result: R.U.Psycho enables reproducible LLM psychometric testing via JSON-based experiments with three-stage postprocessing, showing model traits vary by size, wording, and prompt order with minimal persona bias.

## Executive Summary
R.U.Psycho is a framework for designing and running robust, reproducible psychometric experiments on generative language models with limited coding expertise. It uses JSON-based experiment definitions to capture all parameters, ensuring reproducibility across studies. The framework addresses the challenge of mapping verbose, inconsistent LLM outputs to discrete psychometric answer options through a three-stage postprocessing pipeline of cleaners, validators, and judges.

The framework demonstrates that LLM personality traits vary significantly by model size, prompt wording, and answer option order, while persona simulation has minimal impact on measured traits. Experiments with diverse questionnaires (BFI, RFQ, BDI, GSDB, Trolley problems) show that structured JSON prompts substantially reduce invalid responses compared to natural language prompts. The framework is available as a Python package and lowers the bar for rigorous machine psychology research.

## Method Summary
R.U.Psycho employs a four-stage pipeline: experiment definition (JSON configuration), response generation (iterating over questions × models × personas × seeds), postprocessing (cleaners → validators → judges), and export. The postprocessing stage maps verbose LLM outputs to psychometric answer options using either rule-based token overlap or model-based classifiers. Experiments use 4-bit quantized models with temperature=1.0, top_k=50, top_p=0.95, max_tokens=64. The framework supports multiple models, prompt templates, personas from Aher et al. (2023) name lists, and questionnaires with comma-separated answer options.

## Key Results
- Rule-based judge (token overlap) outperforms model-based judge with higher F1 scores in mapping LLM responses to answer options
- JSON-formatted prompts eliminate refusals and reduce invalid responses (1.17% vs 1.99% for natural prompts)
- Model traits vary significantly by size, prompt wording, and answer option order, with minimal persona-induced bias
- BDI contamination effects observed across models, while BFI shows consistent agreeableness/conscientiousness correlations

## Why This Works (Mechanism)

### Mechanism 1: Configuration-Driven Reproducibility
JSON-based experiment definitions ensure reproducible psychometric experiments by capturing all parameters in version-controlled files. This eliminates undocumented variations in prompt design, parameter settings, and model versions that have plagued prior research. Core assumption: researchers adopt standardized configurations; Break condition: configurations become too complex or fail to capture nuanced designs.

### Mechanism 2: Three-Stage Postprocessing Pipeline for Noisy Outputs
Sequential cleaners, validators, and judges reliably map verbose LLM responses to discrete answer options. Cleaners remove noise and parse formats, validators flag refusals, and judges use token-overlap or classifier-based methods. Core assumption: token-overlap adequately captures semantic equivalence; Break condition: responses become too ambiguous for token matching.

### Mechanism 3: Structured JSON-Format Prompting
JSON-formatted output instructions substantially reduce invalid responses and refusals by constraining model generation. This leverages instruction-tuned models' ability to follow structured output constraints. Core assumption: models reliably follow JSON instructions; Break condition: models generate malformed JSON or constraints bias response distributions.

## Foundational Learning

- **Psychometric questionnaires (Likert scales, multiple-choice formats)**: Human-designed instruments with scoring and answer option formats; why needed: understanding validity concerns for LLM administration. Quick check: Can you explain why inverting BDI answer order tests prompt sensitivity?
- **LLM output instability and prompt sensitivity**: LLMs produce inconsistent outputs sensitive to design and parameters; why needed: explains framework's necessity. Quick check: Why use free text generation instead of token probabilities?
- **Persona simulation in LLMs**: Framework supports demographic personas; why needed: understanding persona effects on LLM outputs. Quick check: What did Experiment 3 find about persona bias, and why might this be limited?

## Architecture Onboarding

- **Component map**:
```
┌─────────────────────────────────────────────────────────────┐
│ EXPERIMENT DEFINITION (JSON)                                │
│ ├── Models (list of LLMs)                                   │
│ ├── Prompt Template (system + user prompts)                 │
│ └── Questionnaire (questions + answer options)              │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│ RESPONSE GENERATION                                         │
│ └── Iterate over questions × models × personas × seeds      │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│ POSTPROCESSING                                              │
│ ├── Cleaners → Remove noise, parse JSON                     │
│ ├── Validators → Flag refusals/apologies                    │
│ └── Judges → Map to answer options (rule-based OR model-based) │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│ EXPORT                                                      │
│ └── DataFrame / JSONL / CSV → Analysis tools                │
└─────────────────────────────────────────────────────────────┘
```

- **Critical path**: Configuration → Response generation → Judge selection (rule-based recommended) → Score aggregation. Judge is bottleneck: high rejection rates indicate design issues.
- **Design tradeoffs**:
  - Rule-based vs. model-based judges: Rule-based outperforms with higher F1 scores; use rule-based by default
  - Natural vs. JSON prompts: JSON eliminates refusals but may constrain expressiveness; paper adopts JSON
  - Persona complexity vs. interpretability: Simple personas minimize confounds but may miss nuanced effects
- **Failure signatures**:
  - High %na or "inconclusive" rates → Judge entropy threshold too strict or prompt needs refinement
  - Significant persona score differences → Possible bias (paper found minimal effects)
  - Inconsistent responses to semantically equivalent questions → Suggests contamination or shallow reasoning
- **First 3 experiments**:
  1. Validate judge performance: Run RFQ with both judges on small sample; manually verify mappings
  2. Establish baseline traits: Run BFI on Qwen 2.5 across sizes to reproduce agreeableness/conscientiousness finding
  3. Test prompt robustness: Run BDI with standard and inverted answer orders to measure sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating chat history alter LLM consistency or psychometric scores compared to isolated prompts?
- Basis in paper: Authors state they simulate questionnaires as "disconnected individual prompts" and extending to chat history "requires further research"
- Why unresolved: Unknown if remembering previous answers reduces output instabilities or changes measured personality traits
- What evidence would resolve it: Comparative experiments using R.U.Psycho in chat-mode vs. isolated prompt mode

### Open Question 2
- Question: Do detailed persona backstories induce stronger social biases than minimal persona definitions used?
- Basis in paper: Authors acknowledge their persona simulation (title + surname) is "rather straightforward" and minimal bias may not generalize to detailed demographic backgrounds
- Why unresolved: Study found minimal bias, but unclear if richer persona descriptions would trigger intersectional biases
- What evidence would resolve it: Re-running bias experiments using personas with extensive biographical descriptions

### Open Question 3
- Question: Can model-based judges be optimized to outperform rule-based methods in mapping noisy outputs?
- Basis in paper: Authors found fine-tuned model-based judge was "substantially" outperformed by simple rule-based judge despite optimization
- Why unresolved: Model-based approach should theoretically handle negations and synonyms better, but fails to provide practical advantage
- What evidence would resolve it: Classifier achieving statistically significant higher F1 score than rule-based baseline

## Limitations

- Rule-based judge relies on exact token matching which may fail for paraphrased or semantically equivalent responses
- Pre-aligned questionnaires represent idealized scenario that may not generalize to subjective scoring criteria
- Persona experiments used simplified design (title + surname) that may not capture nuanced demographic effects

## Confidence

- **High Confidence**: JSON-based reproducibility mechanism and structured prompt formatting (strong empirical support from response rejection rates)
- **Medium Confidence**: Postprocessing pipeline effectiveness (limited external validation, novel architecture)
- **Medium Confidence**: Minimal persona-induced bias findings (specific to simplified persona design, may not generalize)

## Next Checks

1. **Judge Robustness Test**: Apply rule-based judge to questionnaire with ambiguous answer options and measure false positive/negative rates against human annotations

2. **Contamination Sensitivity Analysis**: Run BDI with contaminated and non-contaminated versions across multiple model families to quantify contamination effect size

3. **Prompt Order Sensitivity Replication**: Replicate inverted BDI answer order test across broader model range including frontier models to validate robustness finding