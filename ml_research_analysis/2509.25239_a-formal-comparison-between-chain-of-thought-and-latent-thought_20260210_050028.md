---
ver: rpa2
title: A Formal Comparison Between Chain of Thought and Latent Thought
arxiv_id: '2509.25239'
source_url: https://arxiv.org/abs/2509.25239
tags:
- thought
- latent
- each
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formally compares the computational power of chain-of-thought
  (CoT) and latent thought reasoning in large language models. The authors analyze
  their expressive power through complexity-theoretic analysis, showing that latent
  thought enables efficient parallel computation while CoT enables approximate counting
  through stochastic decoding.
---

# A Formal Comparison Between Chain of Thought and Latent Thought

## Quick Facts
- arXiv ID: 2509.25239
- Source URL: https://arxiv.org/abs/2509.25239
- Reference count: 40
- Primary result: Latent thought enables efficient parallel computation (capturing TC^k) while CoT enables approximate counting (FPRAS) through stochastic decoding

## Executive Summary
This paper formally compares the computational power of chain-of-thought (CoT) and latent thought reasoning in large language models through complexity-theoretic analysis. The authors prove that latent thought with log^k n iterations exactly captures TC^k (polylogarithmic parallel computation) by evaluating computation graphs in O(depth(G_n)) iterations, while CoT with the same iterations cannot achieve this power due to sequentiality, being limited to TC^{k-1}. Conversely, CoT can implement fully polynomial-time randomized approximation schemes (FPRAS) for counting problems that are intractable for deterministic latent thought. Empirical validation on parallelizable tasks (word problems, graph connectivity, arithmetic evaluation, edit distance) shows latent thought requires fewer iterations than CoT to reach comparable performance, while on approximate counting tasks (DNF counting, graph coloring sampling), CoT outperforms latent thought as predicted by theory.

## Method Summary
The paper analyzes two reasoning paradigms: CoT (decoder-only Transformer with autoregressive token generation and KV caching) and latent thought (hidden state h_k replaces token embeddings). Theoretical analysis establishes complexity class separations through circuit complexity (TC^k, AC^k, NC^k) and self-reducibility in counting problems. Empirical validation uses synthetic algorithmic data including Erdős–Rényi graphs G(n, p=1.7/n) for connectivity, DNF formulas with m clauses and w literals, modular arithmetic (modulus r=3), and S5 group composition. Training uses AdamW optimizer (lr=1e-4, weight decay=0.01, batch size=256) with embedding dim=256 and 4 attention heads. Curriculum learning and stepwise internalization strategies are employed for training efficiency.

## Key Results
- Latent thought with log^k(n) iterations exactly captures TC^k (polylogarithmic parallel computation) while CoT with log^k(n) steps is limited to TC^{k-1}
- CoT with stochastic decoding can implement FPRAS for self-reducible counting problems, a capability absent in deterministic latent thought
- On parallelizable tasks, latent thought requires O(depth(G_n)) iterations versus O(size) steps for CoT, with empirical validation showing accuracy scales with log(input_size)
- On approximate counting tasks, CoT achieves low relative error and TV distance to uniform distribution, while deterministic latent thought fails to provide accurate approximations

## Why This Works (Mechanism)

### Mechanism 1: Latent Thought Enables Layer-wise Parallel Computation
Latent thought evaluates computation graphs in O(depth(G_n)) iterations by computing all nodes at the same depth simultaneously in continuous hidden states. The attention layer aggregates all inputs uniformly across positions; the feedforward layer encodes the entire computation graph and computes all node functions in parallel within a single forward pass. Each loop iteration advances one "layer" of the DAG. Core assumption: The embedding dimension scales with input size (non-uniform model), providing sufficient capacity to encode multiple node states simultaneously.

### Mechanism 2: CoT Enables Approximate Counting via Stochastic Decoding
CoT with stochastic decoding can implement FPRAS for self-reducible counting problems; deterministic latent thought cannot achieve the same approximation guarantees. CoT explicitly samples intermediate tokens, inducing stochastic computation paths. For self-reducible relations, approximating conditional probabilities p(y_i|x, y_{<i}) reduces to approximate counting subproblems. CoT simulates Monte Carlo sampling through its autoregressive token sampling. Core assumption: FPTAS ⊊ FPRAS for self-reducible relations (standard complexity assumption).

### Mechanism 3: Complexity-Class Separation Determines Task Suitability
Under standard assumptions (TC^{k-1} ⊊ TC^k), latent thought with log^k(n) iterations strictly exceeds CoT with log^k(n) steps for decision problems in TC^k. CoT[log^k(n)] ⊆ TC^{k-1} because log^k(n) steps partition into log^{k-1}(n) blocks of log(n) steps, each simulable in TC^0. Latent thought[log^k(n)] = TC^k exactly by composing TC^0 blocks for log^k(n) iterations. Core assumption: Standard circuit complexity hierarchy (TC^{k-1} ⊊ TC^k).

## Foundational Learning

- **Concept: Circuit Complexity Classes (TC^k, AC^k, NC^k)**
  - Why needed here: The paper's core theoretical contribution maps model expressivity to these classes. Without this, Theorem 3.12 is opaque.
  - Quick check question: Can you explain why TC^k captures polylogarithmic-depth threshold circuits?

- **Concept: Self-Reducibility in Counting Problems**
  - Why needed here: The approximate counting separation (Section 4) relies entirely on self-reducible relations where global counting reduces to prefix-wise conditional estimation.
  - Quick check question: Given a SAT formula, how would you define the extension counting function Ext_R(x, w) for partial assignment w?

- **Concept: FPRAS vs. FPTAS**
  - Why needed here: The paper's separation hinges on randomized (FPRAS) vs. deterministic (FPTAS) approximation—understanding their runtime/error tradeoffs is essential.
  - Quick check question: What are the runtime dependencies for FPRAS vs. FPTAS, and why does stochasticity matter?

## Architecture Onboarding

- **Component map:**
  - CoT: Decoder-only Transformer -> autoregressive token generation -> KV-cache amortizes sequential access -> intermediate tokens serve as explicit scratchpad
  - Coconut: Decoder-only Transformer -> hidden state h_k replaces token embeddings -> continuous latent vectors carry compressed reasoning state
  - Looped TF: Non-causal Transformer block -> full sequence re-computed each iteration -> output hidden states fed back as input embeddings

- **Critical path:** For parallelizable tasks (arithmetic evaluation, graph connectivity), use looped TF with iteration count ≈ log(input_size). For approximate counting (DNF counting, sampling), use CoT with stochastic decoding and step count scaled to desired FPRAS confidence.

- **Design tradeoffs:**
  - CoT: Lower per-step compute (KV-cache), but sequential token dependency limits parallelism; benefits from stochasticity
  - Latent thought: Higher per-iteration compute (full recompute), but parallel layer-wise evaluation; deterministic only
  - Latent thought requires larger embedding dimension to encode parallel node states; CoT trades space for sequential steps

- **Failure signatures:**
  - Looped TF on counting tasks: Deterministic computation cannot approximate #P problems efficiently—expect high error or slow convergence
  - CoT on TC^k tasks with few steps: Sequentiality limits depth—theorem predicts O(size) steps needed, not O(depth)
  - Insufficient iterations: Figure 6 shows accuracy drops sharply below threshold iteration count

- **First 3 experiments:**
  1. Validate depth scaling: Train looped TF on arithmetic evaluation; vary loop count from 2–8; confirm accuracy scales with log(input_size), not input_size directly (replicate Figure 6)
  2. Test FPRAS separation: Compare CoT vs. looped TF on DNF counting; measure relative error vs. (iterations × trials). Expect CoT to converge to low error with fewer total compute steps
  3. Probe stochasticity requirement: Run CoT with greedy vs. stochastic decoding on graph coloring sampling; measure TV distance to uniform distribution. Expect greedy to fail (replicate Figure 7 bottom)

## Open Questions the Paper Calls Out

- Can diffusion language models combine the parallelizability of latent thought with the stochasticity of CoT to achieve the strengths of both paradigms?
- Can distillation techniques reduce the number of reasoning iterations required by CoT or latent thought without compromising their computational power?
- Do the theoretical separations between CoT and latent thought hold for realistic downstream tasks beyond the synthetic algorithmic benchmarks studied?
- Can hybrid architectures combining explicit token sampling with latent computation achieve strictly greater computational power than either paradigm alone?

## Limitations
- Analysis limited to synthetic algorithmic benchmarks rather than realistic downstream tasks
- Theoretical separation assumes standard complexity assumptions that may not hold in practice
- Unclear how practical constraints (finite precision, finite memory) affect theoretical guarantees

## Confidence
- Theoretical proofs establishing complexity class separations: High
- Empirical validation on synthetic benchmarks: Medium
- Generalization to real-world tasks: Low
- Practical implications for model selection: Medium

## Next Checks
1. Reproduce Figure 6: Train looped TF on arithmetic evaluation with varying iteration counts (2-8) and verify accuracy scales with log(input_size) rather than input_size
2. Validate FPRAS separation: Implement DNF counting with CoT and looped TF; measure relative error convergence and compare total compute steps required
3. Test stochasticity requirement: Run graph coloring sampling with CoT using greedy vs. stochastic decoding; measure TV distance to uniform distribution and confirm greedy fails to achieve low error