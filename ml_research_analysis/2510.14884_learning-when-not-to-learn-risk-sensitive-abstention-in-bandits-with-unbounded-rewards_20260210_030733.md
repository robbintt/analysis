---
ver: rpa2
title: 'Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded
  Rewards'
arxiv_id: '2510.14884'
source_url: https://arxiv.org/abs/2510.14884
tags:
- regret
- learning
- reward
- algorithm
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies safe learning in high-stakes settings where
  individual actions can cause irreparable harm, formalizing this as a contextual
  bandit with an abstain option and unbounded negative rewards. The core challenge
  is determining when to abstain to avoid catastrophic outcomes without a mentor.
---

# Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards

## Quick Facts
- arXiv ID: 2510.14884
- Source URL: https://arxiv.org/abs/2510.14884
- Reference count: 31
- Primary result: Achieves sublinear expected regret O((L+σ²)T^(n+1)/(n+2)(lnT)^(n+1) + Tν(lnT)) in contextual bandits with abstain option and unbounded negative rewards.

## Executive Summary
This paper addresses the challenge of safe learning in high-stakes settings where individual actions can cause irreparable harm, formalized as a contextual bandit with an abstain option and unbounded negative rewards. The core insight is that standard optimistic learning algorithms fail catastrophically in this setting because a single error can incur infinite regret. The authors establish impossibility results showing that any algorithm committing without caution can incur infinite regret, and that sublinear regret is impossible when all inputs are uniformly far out-of-distribution. They propose a caution-based algorithm that only commits when evidence doesn't certify harm, using a trusted region around the origin and per-bin confidence bounds. Under i.i.d. inputs, the algorithm achieves sublinear expected regret that degrades gracefully with the frequency of out-of-distribution inputs.

## Method Summary
The method uses a trusted region around the origin (assumed safe) with radius m(T)=ln(T), partitioning this region into n-cubes of side length w(T)=T^{-1/(n+2)}. For each bin, the algorithm maintains an empirical mean reward and confidence radius based on subgaussian noise assumptions. The agent commits only if the upper confidence bound (μ̂_B + γ(k_B) + L√n w(T)) is non-negative, otherwise abstains. Outside the trusted region, the agent always abstains. This creates a "certification" mechanism where statistical evidence must suggest non-negative returns before committing, operationalizing pessimism under uncertainty. The method requires knowledge of Lipschitz constant L and noise variance σ².

## Key Results
- Any algorithm that commits without caution can incur infinite expected regret when rewards have unbounded negative tails
- When all inputs are uniformly far from the origin, no algorithm can achieve sublinear regret
- Under i.i.d. inputs, the cautious algorithm achieves sublinear expected regret of O((L+σ²)T^(n+1)/(n+2)(lnT)^(n+1) + Tν(lnT))
- The regret bound gracefully degrades with the frequency of out-of-distribution inputs while maintaining sublinear regret for any fixed input distribution

## Why This Works (Mechanism)

### Mechanism 1: Trusted Region Confinement
Restricting exploration to a "trusted ball" around the origin prevents infinite expected regret from out-of-distribution inputs. The origin is assumed safe with positive reward, and Lipschitz continuity ensures nearby points inherit safety properties. Inputs outside radius m(T)=ln(T) trigger automatic abstention.

### Mechanism 2: Spatial Discretization (Binning)
Partitioning the trusted region into finite bins converts an unbounded continuous problem into bounded estimation problems. Lipschitz continuity bounds within-bin reward variation by L√n w(T), allowing sample-efficient estimation of bin means.

### Mechanism 3: Pessimistic Certification
A certification rule based on confidence bounds ensures the agent only commits when statistical evidence suggests non-negative returns. The agent computes μ̂_B + γ(k_B) + L√n w(T) and abstains if negative, operationalizing "pessimism under uncertainty."

## Foundational Learning

- **Lipschitz Continuity**: Provides structural bridge between origin safety and nearby point safety. Without it, points arbitrarily close to origin could have unbounded negative reward. Quick check: If I move ε distance, does reward change by at most Lε?

- **Subgaussian Noise**: Essential for concentration inequalities ensuring empirical means converge sufficiently fast for valid confidence bounds. Quick check: Do noise tails decay at least as fast as Gaussian?

- **Catastrophe Asymmetry**: Models negative rewards as unbounded while positive rewards are capped, invalidating standard OFU algorithms. Quick check: Can a single wrong action force cumulative regret to -∞?

## Architecture Onboarding

- **Component map**: Input Layer -> Radius Filter -> Bin Locator -> Certification Check -> Executor -> Updater

- **Critical path**: The Certification Check is safety-critical, calculating confidence radius γ(k) and Lipschitz penalty before allowing commit.

- **Design tradeoffs**: Bin width w(T) balances Lipschitz error (smaller safer) vs. number of bins (larger slower learning). Radius m(T) trades exploration potential vs. OOD penalty.

- **Failure signatures**: Infinite regret if m(T) scales too fast or L underestimated; paralysis if confidence bounds too conservative or L overestimated.

- **First 3 experiments**:
  1. Origin Sanity Check: Verify algorithm commits and achieves optimal reward when inputs are x=0
  2. Tail Resilience Test: Generate inputs with polynomial tails, verify regret scales with Tν̄(lnT)
  3. Bin Scaling: In dimension n=2, sweep w(T) and confirm minimum near theoretical T^{-1/4} scaling

## Open Questions the Paper Calls Out

- Can cautious learning guarantees extend to non-i.i.d. input processes like drifting distributions or adversarial sequences?
- Is it possible to develop parameter-free algorithms that adapt to unknown Lipschitz constants and noise variances?
- Can regret bounds improve by incorporating richer structural assumptions like low dimensionality or margin conditions?
- How does the unbounded negative reward model compare to alternatives like inescapable trap states in MDPs?

## Limitations
- Algorithm requires knowledge of Lipschitz constant L and noise variance σ², which must be estimated in practice
- Impossibility result shows fundamental limitations when input distribution has heavy polynomial tails
- Curse of dimensionality becomes severe as ambient dimension n increases, with regret exponent approaching 1

## Confidence
- **High Confidence**: Trusted region confinement mechanism is sound given stated assumptions; impossibility results are mathematically rigorous
- **Medium Confidence**: Sublinear regret bound holds under stated conditions but practical performance varies with unknown constants
- **Low Confidence**: Algorithm's robustness to misestimated L or σ² is not fully characterized

## Next Checks
1. Implement algorithm with estimated (rather than known) L and σ² values, quantify regret degradation
2. Generate synthetic data with polynomial tails of varying degrees, empirically verify regret scales with Tν̄(lnT)
3. In synthetic experiments, vary dimension n from 2 to 10, measure whether regret/T ratio remains sublinear or degrades toward 1