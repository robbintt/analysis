---
ver: rpa2
title: 'Towards a rigorous evaluation of RAG systems: the challenge of due diligence'
arxiv_id: '2507.21753'
source_url: https://arxiv.org/abs/2507.21753
tags:
- pour
- llm-juge
- dans
- syst
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating RAG systems in
  high-stakes industrial contexts like due diligence, focusing on issues such as hallucinations,
  off-topic responses, and citation failures. The authors propose a hybrid evaluation
  protocol combining human annotations with LLM-Judge annotations, inspired by the
  Prediction-Powered Inference (PPI) method, to achieve statistically robust performance
  measurements.
---

# Towards a rigorous evaluation of RAG systems: the challenge of due diligence

## Quick Facts
- arXiv ID: 2507.21753
- Source URL: https://arxiv.org/abs/2507.21753
- Authors: Grégoire Martinon; Alexandra Lorenzo de Brionne; Jérôme Bohard; Antoine Lojou; Damien Hervault; Nicolas J-B. Brunel
- Reference count: 0
- Key outcome: This paper addresses the challenge of evaluating RAG systems in high-stakes industrial contexts like due diligence, focusing on issues such as hallucinations, off-topic responses, and citation failures. The authors propose a hybrid evaluation protocol combining human annotations with LLM-Judge annotations, inspired by the Prediction-Powered Inference (PPI) method, to achieve statistically robust performance measurements. Using a comprehensive dataset from an investment fund's RAG system (Alban), the study measures performance metrics such as language correctness, response relevance, and factuality at both response and sentence levels. Human annotations showed strong variability across themes, with lower relevance in IT (32%) but relatively high factuality (80–88%). The LLM-Judge tended to overestimate relevance, and PPI provided limited additional gain due to low human-LLM agreement. The findings highlight the critical need for high-quality LLM-Judge performance and human oversight to ensure reliable RAG evaluation in industrial settings.

## Executive Summary
This paper tackles the critical challenge of evaluating RAG (Retrieval-Augmented Generation) systems in high-stakes industrial contexts, specifically due diligence processes. The authors identify three key risks: hallucinations (false information presented as fact), off-topic responses, and citation failures. They propose a hybrid evaluation protocol that combines human expert annotations with LLM-based automated evaluation, drawing inspiration from Prediction-Powered Inference (PPI) methods. The study provides practical insights from evaluating an investment fund's RAG system (Alban), demonstrating the complexities of achieving reliable performance measurements when dealing with complex, domain-specific queries.

## Method Summary
The authors developed a hybrid evaluation protocol that combines human annotations with LLM-Judge annotations using a methodology inspired by Prediction-Powered Inference. They evaluated responses and sentences from a real-world RAG system (Alban) used in an investment fund's due diligence process. The evaluation covered four themes: Management, IT, Products, and Legal, with each query annotated by multiple human experts and compared against LLM-Judge assessments. Performance metrics included language correctness, response relevance, and factuality, measured at both response and sentence levels. The human annotators were recruited from the same fund using a skills-based approach, and their annotations were used to assess the reliability of LLM-Judge evaluations.

## Key Results
- Human annotations showed strong variability across themes, with lower relevance in IT (32%) but relatively high factuality (80–88%)
- LLM-Judge tended to overestimate response relevance compared to human judgments
- PPI provided limited additional gain due to low human-LLM agreement, suggesting the need for higher-quality LLM evaluators
- Human annotations revealed the system's tendency to generate off-topic responses and occasionally hallucinate facts

## Why This Works (Mechanism)
The hybrid evaluation approach works by leveraging human expertise to establish ground truth while using LLM-Judges for scalable assessment. The Prediction-Powered Inference framework allows for statistical inference about system performance based on a stratified sample of human annotations. This combination addresses the scalability limitations of pure human evaluation while maintaining the accuracy needed for high-stakes applications where incorrect outputs could have significant financial or legal consequences.

## Foundational Learning
- **Prediction-Powered Inference (PPI)**: A statistical method that combines human and LLM judgments to provide confidence intervals for performance metrics. Why needed: Pure human annotation is too expensive for continuous evaluation, while pure LLM evaluation lacks reliability. Quick check: Verify that human-LLM agreement exceeds the 93% threshold where PPI becomes cost-effective.

- **Stratified sampling**: Dividing the evaluation space into meaningful themes (Management, IT, Products, Legal) to ensure representative coverage. Why needed: Different themes have different characteristics and failure modes that require separate analysis. Quick check: Ensure each stratum has sufficient samples for statistical significance.

- **Atomic fact extraction**: The ideal unit for hallucination detection, though too laborious for humans. Why needed: Sentence-level analysis can miss fine-grained hallucinations within valid statements. Quick check: Compare omission rates between atomic and sentence-level analysis.

- **Human-LLM agreement thresholds**: The 93% agreement level where PPI becomes more efficient than simple polling. Why needed: Determines when automated evaluation can replace human judgment cost-effectively. Quick check: Calculate agreement rates for each evaluation dimension.

- **Response vs. sentence-level evaluation**: Measuring performance at different granularities to balance precision and feasibility. Why needed: Responses are too coarse for fine-grained failure analysis, while sentences are more manageable. Quick check: Compare correlation between response and sentence-level metrics.

- **Theme-specific evaluation**: Recognizing that different domains (IT vs. Legal) require different evaluation approaches. Why needed: Domain complexity affects both generation quality and evaluation difficulty. Quick check: Analyze performance variance across themes.

## Architecture Onboarding
- **Component map**: Human annotators -> Annotation platform -> Performance metrics -> LLM-Judge evaluation -> PPI statistical analysis -> Confidence intervals
- **Critical path**: Query generation → Human annotation → LLM-Judge assessment → Statistical aggregation → Performance reporting
- **Design tradeoffs**: Precision vs. scalability (human vs. LLM evaluation), granularity vs. feasibility (atomic vs. sentence-level), statistical rigor vs. practical constraints (PPI implementation)
- **Failure signatures**: Low human-LLM agreement indicates LLM-Judge inadequacy; high theme variance suggests domain-specific challenges; PPI limited gain signals insufficient annotation quality
- **Three first experiments**: 1) Measure inter-annotator agreement within each theme to establish baseline reliability, 2) Test alternative LLM-Judge configurations to improve agreement rates, 3) Implement Active Statistical Inference as an alternative to PPI for stratified evaluation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does Active Statistical Inference (ASI) outperform Prediction-Powered Inference (PPI) in reducing uncertainty when evaluating stratified RAG outputs?
- Basis in paper: [explicit] Section 9 states the protocol could be extended to methodological variants like ASI, which are "better adapted to stratified surveys" than PPI.
- Why unresolved: The current study found PPI provided limited gain due to low human-LLM agreement; it is unknown if ASI's adaptive sampling would handle this noise better.
- What evidence would resolve it: A comparative benchmark of PPI versus ASI on the Alban dataset measuring confidence interval width and annotation efficiency.

### Open Question 2
- Question: Can domain-specific prompt engineering or fine-tuning consistently raise human-LLM agreement above the 93% threshold required for cost-effective PPI?
- Basis in paper: [inferred] The simulation in Section 10.5 shows PPI only becomes advantageous over simple polling at ~93% agreement, but the authors observed relevance agreement as low as 50% (IT theme).
- Why unresolved: The authors note that designing prompts to achieve the necessary agreement rate can be more time-consuming than simply performing more human annotations.
- What evidence would resolve it: A study demonstrating a specific LLM-Judge configuration that achieves >93% agreement with human annotators on "relevance" in the IT domain without prohibitive engineering costs.

### Open Question 3
- Question: Is it possible to automate atomic fact extraction for hallucination detection without suffering the high omission rates observed in human annotation?
- Basis in paper: [explicit] Section 1 notes that working at the atomic fact scale is "extremely laborious" for humans and difficult for LLMs, with omission rates around 50%, forcing the authors to use sentence-level granularity.
- Why unresolved: Sentence-level analysis lacks the precision to localize specific hallucinations within a valid statement, limiting fine-grained system improvement.
- What evidence would resolve it: An automated extraction protocol that captures atomic facts with high recall (>90%) and correlates strongly with human fact-verification.

## Limitations
- The observed discrepancies between human and LLM-Judge annotations suggest potential limitations in the automated evaluation pipeline
- The dataset, while comprehensive for the investment fund context, may not generalize to other high-stakes domains or RAG applications
- Strong variability in human annotations across different themes raises questions about inter-annotator reliability and the stability of the evaluation framework

## Confidence
- High confidence: Human annotations showed substantial variability across themes and metrics
- High confidence: LLM-Judge tended to overestimate response relevance compared to human judgments
- Medium confidence: PPI method's limited additional benefit, as this may depend on the specific dataset characteristics and agreement levels

## Next Checks
1. Conduct inter-annotator agreement analysis to quantify human annotation reliability and identify systematic biases across themes
2. Test the evaluation framework across multiple high-stakes domains beyond investment due diligence to assess generalizability
3. Implement and validate alternative statistical methods for combining human and LLM judgments beyond PPI to determine if improved performance measurement is possible with higher-quality LLM evaluators