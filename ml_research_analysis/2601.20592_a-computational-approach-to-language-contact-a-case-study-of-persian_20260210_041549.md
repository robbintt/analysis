---
ver: rpa2
title: A Computational Approach to Language Contact -- A Case Study of Persian
arxiv_id: '2601.20592'
source_url: https://arxiv.org/abs/2601.20592
tags:
- language
- persian
- languages
- contact
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates structural traces of language contact in
  the internal representations of a monolingual Persian language model, ParsBERT.
  Using probing methods, the research measures how linguistic features from contact
  languages (Arabic, English, French, German, Hindi, Japanese, Russian, Turkish) are
  encoded in ParsBERT's intermediate layers.
---

# A Computational Approach to Language Contact -- A Case Study of Persian

## Quick Facts
- **arXiv ID**: 2601.20592
- **Source URL**: https://arxiv.org/abs/2601.20592
- **Reference count**: 24
- **Primary result**: Monolingual Persian language models encode selective traces of contact languages, with structural encoding constrained by Persian's morphosyntactic properties.

## Executive Summary
This study investigates how structural traces of language contact are encoded in the internal representations of ParsBERT, a monolingual Persian language model. Using probing methods and attribution analysis, the research measures how linguistic features from eight contact languages (Arabic, English, French, German, Hindi, Japanese, Russian, Turkish) are represented in ParsBERT's intermediate layers. The findings reveal that while universal syntactic categories like UPOS tags show little contact sensitivity, morphological features like CASE and GENDER are strongly shaped by the structural properties of the contact languages and their alignment with Persian's analytic morphosyntax.

The results demonstrate that contact effects in monolingual language models are selective and structurally constrained. ParsBERT poorly captures rich inflectional case systems but performs better on analytic case marking, reflecting Persian's own morphosyntactic patterns. The model also encodes gender information more effectively for gender-neutral languages and two-gender systems than for three-gender languages. Attribution analysis reveals that contact-related information is distributed across representations rather than localized in dedicated neurons, with notable selectivity for Japanese due to orthographic differences.

## Method Summary
The study employs ParsBERT, a 12-layer monolingual Persian BERT model, to probe representations of eight contact languages from the Parallel Universal Dependencies (PUD) treebanks. For each language, sentences are passed through ParsBERT and token-level representations are extracted from each layer. Variational usable information (ÎV) quantifies how much linguistic information (language identity, UPOS, CASE, GENDER) can be recovered from these representations. LAPE (Language Activation Probability Entropy) attribution identifies condition-selective neurons by computing activation entropy across conditions and thresholding at the lowest percentile.

## Key Results
- Universal POS tags show minimal contact sensitivity, while morphological features like CASE and GENDER are strongly shaped by contact language structure
- ParsBERT encodes case-related information asymmetrically, favoring languages with minimal or residual case marking while failing to capture rich inflectional case systems
- Contact-related information is distributed across representations rather than localized in dedicated neurons, except for Japanese which shows high selectivity due to script mismatch
- The model encodes gender information more effectively for gender-neutral languages and two-gender systems than for three-gender languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual language models encode structural traces of contact languages, but this encoding is constrained by the morphosyntactic properties of the training language.
- Mechanism: ParsBERT, trained exclusively on Persian, develops internal representations shaped by Persian's analytic morphosyntax. When processing contact languages, the model's ability to recover linguistic features depends on structural alignment: features realized analytically or configurationally in the contact language (like residual case marking) are more recoverable than features realized through rich inflectional paradigms absent from Persian.
- Core assumption: The model's representations reflect statistical patterns in the training data, including loanwords, calques, and contact-induced constructions that preserve structural traces of source languages.
- Evidence anchors:
  - [abstract] "contact effects in monolingual language models are selective and structurally constrained"
  - [section 5.3] "ParsBERT encodes case-related information in a highly asymmetric manner, favoring languages with minimal or residual case marking while failing to capture rich inflectional case systems absent in Persian"
  - [corpus] Weak direct evidence; corpus papers focus on Persian corpora and cross-lingual adaptation rather than representational contact effects.
- Break condition: If the training data contained no contact-induced lexical or structural patterns (no loanwords, calques, or borrowed constructions), the model would show no systematic affinities to contact languages beyond chance.

### Mechanism 2
- Claim: Contact-related linguistic information is distributed across neural representations rather than localized in dedicated, language-specific neurons.
- Mechanism: Attribution analysis using LAPE reveals that most languages have few or no highly selective neurons assigned to them. For languages with historical contact (Arabic, Hindi, Russian), language identity information is encoded in shared, distributed features rather than compact, condition-specific units. This suggests the model captures surface-level or semantic alignment through overlapping representational patterns.
- Core assumption: LAPE's selectivity criterion correctly identifies localization; low LAPE counts do not imply absence of information but rather distributed encoding.
- Evidence anchors:
  - [section 5.1] "no neurons are assigned to English, French, German, or Turkish under the LAPE criterion"
  - [section 6] "contact-related alignment is primarily reflected through distributed cues rather than compact, category-specific features"
  - [corpus] No corpus papers directly address neuron-level attribution or localization in language models.
- Break condition: If contact-induced features were encoded in a small set of highly specialized neurons (like Japanese), LAPE would show high counts for all contact languages rather than the observed skew.

### Mechanism 3
- Claim: Strong orthographic and typological divergence creates more localized, condition-selective representations even in monolingual models.
- Mechanism: Japanese, despite having minimal historical contact with Persian, shows the highest number of language-specific neurons under LAPE. This arises from extreme script mismatch (logographic + syllabic vs. Persian consonantal script) and near-zero subword overlap under a Persian-trained WordPiece tokenizer. The model must allocate distinct representational pathways for tokens it cannot process through its standard vocabulary.
- Core assumption: Localization under LAPE reflects processing difficulty or novelty rather than linguistic similarity or contact intensity.
- Evidence anchors:
  - [section 5.1] "The prominence of Japanese may be related to its strong script/orthographic mismatch with Persian and the resulting low subword overlap"
  - [figure 2] Japanese accounts for the largest LAPE score by far among all test languages
  - [corpus] Weak evidence; corpus papers do not address cross-script representation or orthographic effects on localization.
- Break condition: If the tokenizer could segment Japanese text into shared subword units with Persian, Japanese would show lower LAPE scores and more distributed encoding.

## Foundational Learning

- Concept: **Variational Usable Information (I_V)**
  - Why needed here: This is the core probing metric used to quantify how much linguistic information (Y) can be recovered from model representations (X). Understanding this metric is essential for interpreting all results.
  - Quick check question: If normalized usable information Î_V = 0.8 for a feature, what proportion of that feature's uncertainty can be recovered from the representation?

- Concept: **Language Contact Typology (matter vs. pattern borrowing)**
  - Why needed here: The paper distinguishes between borrowing morphological material (matter) and borrowing organizational patterns (pattern). Persian exhibits both, which shapes what structural traces appear in the model.
  - Quick check question: If Persian borrowed only vocabulary from Turkish but no grammatical patterns, would you expect ParsBERT to recover Turkish case morphology?

- Concept: **Analytic vs. Fusional vs. Agglutinative Morphology**
  - Why needed here: Persian is analytic with light-verb constructions; contact languages span all three types. The model's ability to encode morphological features depends on typological alignment.
  - Quick check question: Why would an analytic-language model struggle to recover features from an agglutinative language's rich inflectional paradigm?

## Architecture Onboarding

- Component map:
  - Input layer: WordPiece tokenizer trained on Persian script → token embeddings (768-dim)
  - Encoder stack: 12 transformer encoder layers, each producing intermediate representations
  - Probing interface: Extract token-level representations from each layer for downstream analysis
  - Attribution module: LAPE computation across all 768 dimensions × 12 layers (9,216 total elements)

- Critical path:
  1. Load ParsBERT checkpoint (Persian-monolingual weights)
  2. For each test language, load PUD treebank sentences
  3. Tokenize with Persian WordPiece (expect high OOV for non-Persian scripts)
  4. Forward pass → extract intermediate representations at each layer
  5. Compute Î_V for each (language, layer, feature) combination
  6. Run LAPE attribution to identify selective elements

- Design tradeoffs:
  - Using a monolingual model ensures any cross-lingual signal must arise from training-data contact traces, not multilingual pretraining
  - PUD's sentence-level alignment controls for domain variation but limits test set size (~1,000 sentences per language)
  - Normalizing by marginal entropy H(Y) enables cross-feature comparison but may inflate scores for features with low entropy

- Failure signatures:
  - Near-zero Î_V across all layers for a feature → model cannot encode that feature (expected for rich case systems)
  - Flat Î_V across layers → feature encoded at embedding level, not transformed by encoder
  - High Î_V but zero LAPE neurons → distributed encoding without localized selectivity (common for contact languages)
  - Unstable Î_V across random seeds → probing classifier may be overfitting; increase regularization

- First 3 experiments:
  1. Replicate Figure 1 (language identification Î_V heatmap) to validate probing pipeline; verify Arabic and Hindi show higher scores than Turkish.
  2. Run LAPE attribution for language identification on Layer 2 and Layer 12; confirm Japanese dominance and Turkish/European language scarcity.
  3. Test a control feature (e.g., sentence length or token frequency) to establish baseline Î_V; ensure contact-related features show distinct patterns from this non-linguistic control.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does ParsBERT exhibit strong language identification and high neuron selectivity for Japanese, given the minimal historical contact between Persian and Japanese?
- Basis in paper: [explicit] Authors state "The model's identification of Japanese remains unclear; further investigation is needed to clarify this" (p. 5).
- Why unresolved: The observed pattern contradicts contact-based expectations; the authors hypothesize orthographic mismatch as a factor but do not test it directly.
- What evidence would resolve it: Controlled experiments with transliterated Japanese (Persian script) or ablation studies isolating script effects from structural features.

### Open Question 2
- Question: Would finer-grained syntactic phenomena beyond UPOS tags reveal contact-related effects that remain hidden at the level of universal syntactic categories?
- Basis in paper: [explicit] "It remains unclear whether this reflects a genuine robustness of universal syntax to language contact, or finer-grained syntactic phenomena beyond UPOS would reveal contact-related effects" (p. 6).
- Why unresolved: UPOS is too coarse-grained; the study does not probe dependency relations, clause structure, or construction-level patterns.
- What evidence would resolve it: Probing experiments targeting fine-grained syntactic relations, word order patterns, and construction types across contact languages.

### Open Question 3
- Question: How would the findings generalize to key contact languages absent from PUD, such as Azerbaijani, Turkmen, Urdu, Armenian, and Kurdish?
- Basis in paper: [explicit] The Limitations section identifies this as a central constraint and calls for "future work incorporating UD treebanks or comparable resources for these languages" (p. 9).
- Why unresolved: PUD lacks several languages with the most intense and sustained contact with Persian, limiting the validity of conclusions about structural convergence.
- What evidence would resolve it: Extending the probing methodology to newly available UD treebanks or custom-annotated corpora for these historically critical contact languages.

### Open Question 4
- Question: Does the low usable information for Turkish despite extensive historical contact reflect deeper morphosyntactic divergence, or is it an artifact of dataset limitations?
- Basis in paper: [inferred] Authors note Turkish yields surprisingly low usable information "given the extensive historical contact" and suggest morphosyntactic divergence as an explanation (p. 5), but do not disentangle this from data-related confounds.
- Why unresolved: Turkish is underrepresented in PUD (17K tokens, the smallest), making it unclear whether structural or sample-size factors drive the result.
- What evidence would resolve it: Replication with larger, balanced Turkish datasets and controlled comparison with other agglutinative languages.

## Limitations
- Corpus coverage limited to PUD treebanks (~17-28K tokens per language), potentially missing fine-grained contact patterns
- Feature set (UPOS, CASE, GENDER) may not capture all contact-induced phenomena like word order changes or light-verb constructions
- Probe architecture and training details not fully specified, making it difficult to assess probe reliability
- Attribution method (LAPE) may be too conservative, potentially underestimating localized encoding

## Confidence
- **High Confidence**: The core finding that contact-related information is distributed rather than localized is well-supported by LAPE results showing minimal selective neurons for most contact languages.
- **Medium Confidence**: The interpretation that Japanese's high LAPE scores reflect script mismatch rather than linguistic similarity is plausible but not definitively proven.
- **Low Confidence**: Claims about specific contact-induced constructions in the training corpus are largely inferential without direct analysis of training data.

## Next Checks
1. **Probe Capacity Validation**: Replicate the probing experiments using progressively larger probe architectures (from logistic regression to multi-layer perceptrons). If ÎV scores increase with probe capacity, this would indicate current probes are underfitting rather than features being absent.

2. **Cross-Domain Verification**: Test whether the representational patterns observed in PUD treebanks generalize to Persian text containing loanwords and calques. If ParsBERT shows similar feature encoding patterns when processing Persian sentences with Arabic/Hindi-origin vocabulary, this would strengthen the contact-tracing interpretation.

3. **Tokenizer Overlap Analysis**: Quantify subword overlap between Persian and each test language before and after tokenization. Correlate these overlap scores with LAPE counts and ÎV patterns to formally test whether orthographic/typological distance predicts representational localization.