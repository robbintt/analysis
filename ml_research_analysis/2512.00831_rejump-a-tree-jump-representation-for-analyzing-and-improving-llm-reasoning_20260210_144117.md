---
ver: rpa2
title: 'ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning'
arxiv_id: '2512.00831'
source_url: https://arxiv.org/abs/2512.00831
tags:
- reasoning
- problem
- solution
- answer
- jump
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReJump is a tree-jump representation for analyzing and comparing
  LLM reasoning. It encodes reasoning traces as visitation orders over nodes in a
  tree of intermediate problem-solving steps, with transitions (jumps) labeled by
  action types (calculation, verification, backtracking).
---

# ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning

## Quick Facts
- arXiv ID: 2512.00831
- Source URL: https://arxiv.org/abs/2512.00831
- Reference count: 40
- Primary result: ReJump representation enables analysis and improvement of LLM reasoning, achieving over 0.9 similarity with human annotations and +6.8% to +9.1% performance gains through Best-of-N selection.

## Executive Summary
ReJump is a structured representation for analyzing and comparing LLM reasoning traces. It encodes reasoning as visitation orders over nodes in a tree of intermediate problem-solving steps, with transitions (jumps) labeled by action types (calculation, verification, backtracking). This enables quantification of reasoning behaviors such as exploration-exploitation balance, overthinking, forgetting, and verification. Using ReJump-Extractor, an LLM agent, reasoning traces are automatically converted into this format. Evaluations show ReJump-Extractor achieves over 0.9 tree and jump similarity with human annotations on Game of 24, and over 80% accuracy on MATH-500. ReJump reveals that models with similar accuracy can exhibit distinct reasoning behaviors, and that different tasks favor different strategies (e.g., exploration vs. exploitation). It also shows that RL encourages task-preferred reasoning dynamics and that distilled models inherit reasoning behaviors from teachers. Finally, ReJump improves reasoning quality at test time through Best-of-N selection and prompt selection, yielding +6.8% to +9.1% performance gains on Game of 24.

## Method Summary
ReJump converts unstructured LLM reasoning traces into a structured format with two layers: a tree layer capturing the hierarchical structure of partial solutions, and a jump layer capturing transitions between nodes labeled as calculation, verification, or backtracking. The ReJump-Extractor, an LLM agent (Gemini 2.5 Pro), uses two-step prompting to extract tree and jump layers from raw text. The system computes six metrics: solution count, jump distance, success rate, verification rate, overthinking rate, and forgetting rate. Two comparison metrics are used: tree similarity via Zhang-Shasha tree edit distance and jump similarity via Jensen-Shannon divergence on transition probability matrices. The representation is evaluated on Game of 24 and MATH-500 datasets using various models including base models, RL-trained models, and distilled variants.

## Key Results
- ReJump-Extractor achieves over 0.9 tree and jump similarity with human annotations on Game of 24
- Over 80% accuracy on MATH-500 using ReJump-Extractor
- +6.8% to +9.1% performance gains on Game of 24 through Best-of-N selection using ReJump metrics
- Distilled models show higher tree and jump similarity to teacher models than base models
- RL-trained models exhibit task-preferred reasoning dynamics (higher exploration on Game of 24, lower overthinking on MATH-500)

## Why This Works (Mechanism)

### Mechanism 1
Decomposing a linear reasoning trace into a static logical dependency tree and a dynamic execution sequence (jumps) isolates specific cognitive behaviors such as backtracking or verification that are invisible in raw text. The representation separates what steps are logically connected (Tree Layer: parent-child prerequisites) from how the model visited them (Jump Layer: transitions labeled as calc, verify, backtrack). By measuring the distance and type of transitions (e.g., returning to an ancestor node), the system infers exploration vs. exploitation balance.

### Mechanism 2
Selecting reasoning outputs based on specific structural metrics (e.g., jump distance) without ground-truth labels can improve pass@1 accuracy on tasks requiring exploration. In search-heavy tasks (e.g., Game of 24), reasoning quality correlates with exploration width. ReJump quantifies this via $d_{jump}$ (tree distance between derived solution steps). Selecting the candidate with the highest $d_{jump}$ prioritizes diverse search paths over linear, potentially stuck reasoning.

### Mechanism 3
Distillation transfers the structural behavior (how to reason) from a teacher to a student model, distinct from merely transferring accuracy. By comparing the Tree and Jump Similarity ($Sim_T, Sim_J$) of base vs. distilled models, the authors observe that distillation increases the similarity of the student's reasoning "shape" to the teacher's. This implies the student learns the policy of when to backtrack or verify.

## Foundational Learning

**Concept**: **Tree-of-Thought (ToT) vs. Chain-of-Thought (CoT)**
- **Why needed here**: ReJump treats linear CoT as a traversal over an implicit ToT structure. You must understand that reasoning isn't just sequential derivation but potentially non-linear exploration (backtracking) to interpret the "Jump" layer.
- **Quick check question**: Can you distinguish between a "calculation" move (parent to child) and a "backtracking" move (child to ancestor) in a search tree?

**Concept**: **Exploration vs. Exploitation**
- **Why needed here**: The paper uses $d_{jump}$ to quantify this trade-off. High jump distance implies global exploration (jumping across the tree), while low distance implies local exploitation (staying in one branch).
- **Quick check question**: If a model finds a correct solution but continues to generate 10 more failed attempts, is this high exploration or high overthinking?

**Concept**: **LLM-as-a-Judge/Parsing**
- **Why needed here**: The system relies on "ReJump-Extractor," an LLM agent, to convert unstructured text into structured JSON. Understanding prompt engineering for extraction is critical.
- **Quick check question**: Why might an LLM fail to identify a "verification" step if the text simply re-uses a formula without re-stating the problem definition?

## Architecture Onboarding

**Component map**: Input (raw reasoning trace) -> ReJump-Extractor (LLM agent) -> Tree JSON + Jump JSON -> Metric Engine -> Output metrics

**Critical path**: The Prompt Engineering for the Extractor (Listing 1 & 2). The distinction between "Calculation" (derivation), "Backtracking" (abandoning a path), and "Verification" (re-doing work) is strictly defined in the prompt instructions. If the prompt fails to enforce the "Mandatory Backtracking Rule" (Listing 2), the structure collapses.

**Design tradeoffs**:
- **Cost vs. Interpretability**: Running a large LLM (Gemini 2.5 Pro) as the Extractor is expensive (Sec 7 mentions cost as a limitation) compared to regex, but necessary for semantic parsing.
- **Structure vs. Semantics**: The current metrics ignore node semantics (content) for structure. Two trees are similar if the logic flow is identical, even if the math topics differ.

**Failure signatures**:
- **High Forgetting Rate**: The model revisits a leaf node via calculation rather than verification. This indicates poor state tracking (it forgot it already solved that sub-problem).
- **Linear Chains**: A very low $d_{jump}$ and low $r_{verify}$ often indicate the model is hallucinating a linear path without self-correction, typical of weaker base models.

**First 3 experiments**:
1. **Validation**: Run the Extractor on 5 synthetic Game of 24 traces with known ground-truth structures. Verify if the JSON output matches the manually constructed tree/jump.
2. **Ablation**: Compare pass@1 on a small dataset using (a) standard Majority Vote vs. (b) ReJump Best-of-N (select max $d_{jump}$). Check if gains are positive only on search-heavy tasks.
3. **Behavioral Analysis**: Compare the $r_{verify}$ and $r_{overthinking}$ of a Base LLM (e.g., Qwen2.5) vs. its LRM counterpart (e.g., QwQ-32B). Confirm if the LRM shows higher verification rates.

## Open Questions the Paper Calls Out

**Open Question 1**: How can ReJump metrics be extended to incorporate temporal dynamics and semantic content to differentiate reasoning traces with identical transition matrices?
- **Basis in paper**: [Explicit] The authors note in Section 7 that their current similarity metrics "can mask important differences," such as distinct temporal behaviors (e.g., batching derivations vs. interleaving them) and semantic content, suggesting this as "promising direction of future work."
- **Why unresolved**: Current metrics only capture logical structure and transition probability distributions ($P_{a,b}$), ignoring the order of actions over time and the meaning of specific steps.
- **What evidence would resolve it**: A new metric formulation that weights transition sequences based on order and semantic embedding similarity, validated against human judgments of reasoning quality.

**Open Question 2**: Can ReJump extraction be optimized for efficiency to enable real-time feedback during reinforcement learning?
- **Basis in paper**: [Explicit] The paper states in Section 7 that the reliance on a capable LLM (ReJump-Extractor) makes the process "computationally expensive and slow, limiting large-scale use such as real-time feedback during training."
- **Why unresolved**: The current method relies on prompting a large model (Gemini 2.5 Pro) for every reasoning trace, which is too slow for online training loops.
- **What evidence would resolve it**: The development of a lightweight, fine-tuned encoder model capable of extracting tree-jump representations with high speed and comparable accuracy to the LLM agent.

**Open Question 3**: Can the definition of "partial solutions" be automated to generalize ReJump across arbitrary tasks without manual engineering?
- **Basis in paper**: [Explicit] The authors list "task-specific prompting" as a limitation in Section 7, noting that the method "still requires defining partial solutions for each task" and that "automating this adaptation would greatly improve usability."
- **Why unresolved**: The extraction process currently relies on distinct prompts and schema definitions (e.g., arithmetic expressions for Game of 24 vs. sub-problems for MATH).
- **What evidence would resolve it**: A generalized ReJump-Extractor that infers the hierarchical state space of a reasoning trace without prior domain-specific instructions.

## Limitations
- The ReJump-Extractor relies on a commercial LLM (Gemini 2.5 Pro), creating dependencies on API availability and cost.
- The synthetic ground-truth dataset (82 Game of 24 traces) is not publicly released, making full reproducibility difficult.
- Limited ablation studies on how different extraction prompts affect the metrics.
- RL experiments show 100% accuracy on Game of 24, but comparison baseline and exact DAPO hyperparameters are not fully specified.

## Confidence

**High confidence**: The ReJump representation's validity for comparing reasoning behaviors (supported by strong human annotation correlations).

**Medium confidence**: Best-of-N selection improvements (+6.8% to +9.1% gains are task-specific and not demonstrated across diverse domains).

**Low confidence**: The distillation mechanism claims without ablation on whether the similarity metrics capture meaningful "reasoning style" versus superficial structural patterns.

## Next Checks

1. **Reproduce extraction accuracy**: Generate 10 new Game of 24 traces with known solutions, run ReJump-Extractor, and compare tree/jump similarity against manual annotation to verify the claimed >0.9 similarity rates.

2. **Cross-task generalization test**: Apply ReJump to a different search-heavy task (e.g., Sokoban or maze navigation) and test if Best-of-N selection by jump distance consistently improves pass@1 as claimed for Game of 24.

3. **Ablation of prompt instructions**: Modify the ReJump-Extractor prompts to relax the "Mandatory Backtracking Rule" and measure the impact on metric reliability and downstream task performance.