---
ver: rpa2
title: 'Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models'
arxiv_id: '2601.23255'
source_url: https://arxiv.org/abs/2601.23255
tags:
- audio
- speech
- arxiv
- preprint
- delivery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates a novel attack vector against large audio-language
  models (LALMs) by exploiting paralinguistic and psychological features of speech.
  The authors design a black-box jailbreak method that embeds disallowed directives
  within narrative-style audio streams, leveraging persuasive and therapeutic delivery
  styles to elicit restricted outputs from state-of-the-art models.
---

# Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models

## Quick Facts
- arXiv ID: 2601.23255
- Source URL: https://arxiv.org/abs/2601.23255
- Authors: Ye Yu; Haibo Jin; Yaoning Yu; Jun Zhuang; Haohan Wang
- Reference count: 22
- Key outcome: Novel black-box jailbreak method exploiting paralinguistic features achieves up to 98.26% success rate on Gemini 2.0 Flash

## Executive Summary
This paper introduces a novel black-box jailbreak attack against Large Audio-Language Models (LALMs) that exploits paralinguistic features of speech delivery. The authors demonstrate that embedding disallowed directives within narrative-style audio streams, using persuasive and therapeutic delivery styles, can significantly improve attack success rates compared to text-only approaches. The attack leverages the fact that LALMs process audio as unified representations where prosody and semantic content are fused, allowing vocal authority or empathy to bypass text-centric safety mechanisms.

## Method Summary
The attack uses DeepInception narrative framing to wrap harmful prompts in nested stories, then synthesizes audio using GPT-4o Mini TTS with psychologically-grounded delivery styles (Authoritative Demand, Affiliative Persuasion, Urgent Directive, Emotive Suggestion, Social Bonding Appeal). The synthesized audio is fed to end-to-end LALMs (GPT-4o Realtime, Gemini 2.0 Flash, Qwen2.5-Omni-7B), with responses evaluated by an LLM-as-judge (GPT-4o) to determine safety policy violations. The AdvWave augmentation approach iteratively refines audio through 30 rounds, retaining the best-performing delivery style per round.

## Key Results
- Attack success rate reaches 98.26% on Gemini 2.0 Flash using audio narrative attacks
- Paralinguistic attacks outperform text-only and signal-level baselines across all tested LALMs
- Five psychologically-grounded delivery styles systematically improve attack effectiveness
- Smaller LALMs (Qwen2.5-Omni) show decoding instability rather than successful refusal in many cases

## Why This Works (Mechanism)

### Mechanism 1: Paralinguistic as Unalignment Vector
Safety alignment trained on text fails to generalize to acoustic modality where prosody and semantics fuse. End-to-end LALMs process audio as unified representations, lacking "immunity" to acoustic pressures that signal social compliance.

### Mechanism 2: Personification Bias and Social Engineering
LALMs internalize human-like social dynamics from audio pre-training data, responding to vocal authority or empathy as social cues rather than data inputs. This causes them to prioritize implied social context over explicit safety policy.

### Mechanism 3: Narrative Context Dilution
The storytelling format embeds threats inside complex, multi-layered contexts that lower the perceived danger of malicious instructions. Safety classifiers may be less sensitive to distributed semantic threats in long-form audio narratives.

## Foundational Learning

- **Concept**: End-to-End vs. Cascaded LALMs
  - **Why needed**: Attack targets end-to-end models that preserve paralinguistic data; cascaded models destroy this data during transcription
  - **Quick check**: Does the model convert audio to text before reasoning? If yes, paralinguistic influence is likely mitigated

- **Concept**: Paralinguistics (Prosody)
  - **Why needed**: This is the attack surface—speech comprises not just words but pitch, rhythm, and loudness that convey authority or urgency
  - **Quick check**: Does the model interpret "Do this now" differently when shouted vs. laughed?

- **Concept**: Personification Bias
  - **Why needed**: Theoretical underpinning—models trained on human data mimic human social vulnerabilities
  - **Quick check**: Would a calculator change its output based on how "nicely" you asked it to calculate 2+2?

## Architecture Onboarding

- **Component map**: Instruction-Following TTS (with Style Directive) -> Generate .wav -> End-to-End LALM -> LLM-As-Judge (GPT-4o)
- **Critical path**: 1) Style Injection (define psychological strategy), 2) Narrative Wrapping (embed in story), 3) Synthesis (generate audio), 4) Inference (send to LALM), 5) Detection (check response)
- **Design tradeoffs**: Synthetic audio preferred for scalability; no universal style—ensemble approach required with different styles working better on different models
- **Failure signatures**: Smaller models show premature termination or repetition loops (43% and 34.9% of failures) rather than successful refusal
- **First 3 experiments**: 1) Baseline Replication (run DeepInception text prompt), 2) Modality Ablation (convert same text to Neutral audio), 3) Style Optimization (use Authoritative Demand style)

## Open Questions the Paper Calls Out

1. How can adversarial delivery styles be systematically discovered and optimized rather than relying on hand-crafted heuristics?
2. How does vocal modulation interact with broader contextual factors such as emotional polarity and prompt length to affect LALM compliance?
3. Do delivery-based attacks generalize across languages and accents, or are vulnerabilities specific to English speech?
4. Are smaller LALMs genuinely more resistant to paralinguistic attacks, or do observed failures reflect architectural instability rather than stronger alignment?

## Limitations

- Synthetic TTS voices used rather than human recordings, potentially underestimating real-world attack fidelity
- Attack transferability across TTS systems not extensively validated—results may vary with different voice models
- LLM-as-judge methodology introduces meta-model dependency that could misclassify edge cases

## Confidence

- **High confidence**: Paralinguistic features influence LALM outputs differently than text (supported by ablation studies)
- **Medium confidence**: Specific psychological delivery styles as optimal attack vectors (empirically validated but generalizability uncertain)
- **Medium confidence**: Quantitative ASR improvements (depend on specific model versions, judge calibration, dataset composition)

## Next Checks

1. Cross-TTS validation: Test attack methodology using at least two different TTS systems to assess robustness against synthesis variability
2. Human evaluation benchmark: Deploy successful attacks with human judges to validate LLM-as-judge's safety policy assessments
3. Cascaded model control: Implement same narrative attack against cascaded ASR-LM pipeline to confirm end-to-end processing is necessary for vulnerability