---
ver: rpa2
title: 'CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary
  of Visual Perception and Reasoning'
arxiv_id: '2507.00045'
source_url: https://arxiv.org/abs/2507.00045
tags:
- reasoning
- arxiv
- mllms
- clues
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CaughtCheating, a benchmark designed to evaluate
  the detective-level visual perception and reasoning capabilities of multimodal large
  language models (MLLMs). The benchmark consists of 100 real-world images collected
  from social media, where users request detection of subtle, context-dependent suspicious
  clues that contradict claims in shared photos.
---

# CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning

## Quick Facts
- **arXiv ID**: 2507.00045
- **Source URL**: https://arxiv.org/abs/2507.00045
- **Reference count**: 40
- **Primary result**: Even the best MLLM (GPT-o3) achieves only 26.0% accuracy on detecting subtle visual clues in real-world images, highlighting severe limitations in detective-level visual perception and reasoning.

## Executive Summary
This paper introduces CaughtCheating, a novel benchmark designed to evaluate multimodal large language models (MLLMs) on detective-level visual perception and reasoning tasks. The benchmark presents 100 real-world images collected from social media where users request detection of subtle, context-dependent clues that contradict claims in shared photos (e.g., "My boyfriend said he's alone at a hotel, do you notice anything suspicious?"). The images are split into Clued (containing clues) and Unclued (innocent) categories, with detailed annotations including primary questions, deterministic/non-deterministic clues, and decomposed questions for analyzing perception versus reasoning capabilities.

Extensive experiments reveal that current MLLMs struggle significantly with this task. Even the strongest model, GPT-o3, achieves only 26.0% accuracy on Clued images, 8.0% on Unclued images, and an overall F1 score of 23.9%. The study demonstrates that models can identify clues when explicitly pointed out (62.0% perception accuracy) but fail to connect them to social context independently. The results expose fundamental limitations in MLLMs' visual search strategies and social reasoning capabilities, suggesting these models are far from matching human detective-level visual intelligence.

## Method Summary
The evaluation pipeline involves querying MLLMs with standardized question templates based on real-world cheating-detection scenarios. For each image, models receive a claim-based question asking if they notice anything suspicious. The 100-image dataset (50 Clued, 50 Unclued) was collected from social media and annotated with primary questions, deterministic and non-deterministic ground-truth clues, and decomposed perception/reasoning questions. Model responses are processed through an automated evaluation pipeline using GPT-4.1 with specific prompts to detect clue presence/absence and calculate metrics including Clued Accuracy, Unclued Accuracy, Intersection over Union (IoU) for non-deterministic clues, and overall F1 score.

## Key Results
- GPT-o3 achieves only 26.0% Clued Accuracy, 8.0% Unclued Accuracy, and 17.2% IoU for detecting non-deterministic clues
- Models show large performance gaps: 62.0% perception accuracy (when clues are pointed out) vs. 26.0% reasoning accuracy (understanding social implications independently)
- Overall F1 score of 23.9% indicates poor balance between detection capability and evidential restraint
- Advanced reasoning models (GPT-o3, Gemini-2.5-pro) exhibit higher false positive rates, hallucinating clues in innocent images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task difficulty is governed by the alignment between MLLMs' search strategy and task search requirements.
- Mechanism: Advanced MLLMs like GPT-o3 rely on exhaustive grid search, which fails when clues have low visual salience and aren't predefined objects requiring social context inference.
- Core assumption: Guided Search theory accurately describes MLLM visual reasoning constraints.
- Evidence anchors: Section 2.1-2.2 states o3 uses "methodical, exhaustive grid search" and analyzes failures via Guided Search theory noting "extremely low bottom-up salience" and lack of "top-down feature guidance."

### Mechanism 2
- Claim: Performance decomposes into separable perception and reasoning failures.
- Mechanism: High perception accuracy (when cued) but low reasoning accuracy indicates models can see details but fail to connect them to social narratives without guidance.
- Core assumption: Visual perception and social reasoning components are partially modular in current MLLM architectures.
- Evidence anchors: Section 4.2 Table 2 shows GPT-o3 Dec. P Acc of 62.0% vs. Clued Acc of 26.0%, with text noting models "can identify key subtle items... if explicitly mentioned."

### Mechanism 3
- Claim: High false positive rates stem from misalignment between reasoning objective and evidentiary standards.
- Mechanism: Strong reasoning models primed to generate explanations produce numerous speculative clues from ambiguous elements rather than correctly identifying absence of evidence.
- Core assumption: Hallucination tendency is a side effect of strong generative reasoning capabilities and safety alignment.
- Evidence anchors: Section 4.1 Table 1 shows GPT-o3 8.0% Unclued Acc, with text noting models "tend to hallucinate and accuse everything by generating lots of so-called suspicious clues."

## Foundational Learning

- Concept: **Guided Search Theory Factors**
  - Why needed here: This cognitive science framework is the primary lens for analyzing why detective tasks are hard for MLLMs.
  - Quick check question: For a task like "find a reflection in a spoon," which Guided Search factor is most deficient: bottom-up salience or top-down feature guidance?

- Concept: **Perception vs. Reasoning Decomposition in MLLMs**
  - Why needed here: The decomposition pinpoints whether models fail at seeing clues or understanding implications.
  - Quick check question: If a model has high Decomposed Perception Accuracy but low Decomposed Reasoning Accuracy, what component is the bottleneck?

- Concept: **Precision/Recall Tradeoff in Evidentiary Tasks**
  - Why needed here: The benchmark uniquely penalizes false positives via Unclued Accuracy and F1 score.
  - Quick check question: Why is Unclued Accuracy crucial, and how does it relate to the model's tendency to hallucinate?

## Architecture Onboarding

- Component map: Vision Encoder -> Implicit Search/Attention Deployment -> Language Reasoning -> Textual Answer
- Critical path: Image Input -> Vision Encoding -> Implicit Search/Attention Deployment (fails if clue has low salience/no top-down guidance) -> Language Reasoning (fails if clue's social implication is not inferred) -> Textual Answer
- Design tradeoffs: Models with powerful reasoning achieve higher recall (Clued Acc) but suffer from very low precision (Unclued Acc), while weaker models have near-zero recall but better precision by failing to generate clues at all.
- Failure signatures:
  - Exhaustive Search Failure: Model examines relevant areas but reports no clue due to missing social significance inference.
  - Reasoning Hallucination: Model lists numerous ambiguous elements as suspicious in innocent images.
  - Context Misalignment: Model identifies a deterministic clue but fails to connect it to the specific social claim.
- First 3 experiments:
  1. Implement hypothesis-driven search module that first generates potential clue categories based on the claim, then directs attention.
  2. Fine-tune or prompt-engineer model to distinguish between "possible" and "suspicious" clues, explicitly penalizing speculation.
  3. Systematically vary prompt specificity to quantify the perception-reasoning gap and identify minimal context needed.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three proprietary MLLM APIs without access to internal configurations or training details
- Real-world image sourcing from social media introduces potential bias in image selection and annotation quality
- Automated evaluation pipeline using GPT-4.1 for scoring introduces dependency on another black-box model, creating potential evaluation bias

## Confidence

- **High confidence** in the finding that current MLLMs struggle with detective-level visual reasoning tasks requiring subtle clue detection and social context integration.
- **Medium confidence** in the mechanism explaining failures via Guided Search theory and search strategy limitations.
- **Medium confidence** in the claim that advanced reasoning models exhibit higher false positive rates.

## Next Checks

1. Re-score a subset of model responses using human annotators rather than the GPT-4.1 evaluation pipeline to verify accuracy of classifications.
2. Implement and test a hypothesis-driven search module that generates potential clue categories before examining images, then measure impact on Clued Accuracy.
3. Systematically vary prompt specificity across all models to quantify the perception-reasoning gap and identify minimal context required for successful detection.