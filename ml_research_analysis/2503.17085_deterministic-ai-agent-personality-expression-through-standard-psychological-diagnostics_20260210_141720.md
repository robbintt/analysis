---
ver: rpa2
title: Deterministic AI Agent Personality Expression through Standard Psychological
  Diagnostics
arxiv_id: '2503.17085'
source_url: https://arxiv.org/abs/2503.17085
tags:
- personality
- agent
- agents
- rmse
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that AI agents can express consistent, deterministic
  personalities when instructed using established psychological frameworks, with advanced
  models like GPT-4o and o1 showing the highest accuracy in expressing specified personalities
  across both Big Five and Myers-Briggs assessments. The results reveal that personality
  expression operates through holistic reasoning rather than question-by-question
  optimization, with response-scale metrics showing higher variance than test-scale
  metrics, and that model fine-tuning affects communication style independently of
  personality expression accuracy.
---

# Deterministic AI Agent Personality Expression through Standard Psychological Diagnostics

## Quick Facts
- arXiv ID: 2503.17085
- Source URL: https://arxiv.org/abs/2503.17085
- Reference count: 40
- Key outcome: AI agents can express consistent, deterministic personalities using established psychological frameworks, with advanced models showing highest accuracy

## Executive Summary
This paper demonstrates that AI agents can reliably express consistent personalities when instructed using established psychological frameworks like Big Five and Myers-Briggs. The research establishes that personality expression operates through holistic reasoning rather than question-by-question optimization, with advanced models like GPT-4o and o1 showing the highest accuracy in expressing specified personalities. The findings reveal that model fine-tuning affects communication style independently of personality expression accuracy, and that response-scale metrics show higher variance than test-scale metrics, suggesting human-like personality expression rather than test gaming.

## Method Summary
The study uses a character builder agent (GPT-4o) to generate personality templates combining Big Five and MBTI assessments, which are then provided to target AI models via system prompts. Agents take standardized personality tests (50-question Big Five and 70-question MBTI) and their responses are scored using established psychological formulas. The evaluation measures both test-scale accuracy (overall personality match) and response-scale variance (consistency of individual responses). The research tests multiple model conditions including base models (4o-mini, 4o, o1, o3-mini) and fine-tuned variants, with some experiments requiring agents to provide motivations for their answers.

## Key Results
- Advanced models (GPT-4o, o1) achieve highest personality expression accuracy across both Big Five and MBTI assessments
- Personality expression operates through holistic reasoning rather than question-by-question optimization, with high response-scale variance but accurate test-scale results
- Model fine-tuning affects communication style independently of personality expression accuracy, demonstrating orthogonal control vectors
- Systematic bias toward high Openness scores persists across models, representing a key limitation
- Requiring motivations for answers degrades performance in standard models but stabilizes high-reasoning models like o1

## Why This Works (Mechanism)

### Mechanism 1
AI agents express specified personalities through holistic reasoning rather than question-by-question optimization. The agent maintains a latent representation of the target personality (the "template") and generates responses based on that global context. Accuracy is achieved by aggregating noisy individual responses (high response-scale variance) into a consistent mean (high test-scale accuracy), mirroring human testing behaviors rather than algorithmic "gaming" of individual questions.

### Mechanism 2
Accuracy of personality expression is a function of the linear combination of model intelligence and reasoning capability. High intelligence allows the model to understand and encode complex personality templates. Explicit reasoning capabilities (e.g., "motivation" requirements or o1's internal chain-of-thought) stabilize this expression. For low-intelligence models, forcing a motivation acts as a crutch to improve accuracy; for high-intelligence/low-reasoning models, forcing motivation can introduce noise.

### Mechanism 3
Communication style (tone/slang) and personality traits (Big Five/MBTI) operate as orthogonal control vectors. Personality traits are encoded in the system prompt (semantic instruction), while style is modulated via fine-tuning (statistical weight adjustment). The paper suggests that fine-tuning shifts the "surface form" of the text without altering the "deep structure" of the decision-making logic that determines test answers.

## Foundational Learning

- **Test-Scale vs. Response-Scale Variance**: Distinguishes between an agent "gaming" a test (perfect answers per question) and genuinely "expressing" a personality (noisy answers that average to the correct trait). Quick check: If an agent has Pearson correlation of 0.99 at test-scale but high variance at response-scale, is it failing the personality test? (Answer: No, this indicates human-like consistency).

- **Big Five Inventory (BFI) Scoring Logic**: Understanding the "hard" vs. "easy" dimensions and the Openness bias. Quick check: Why does the paper suggest Openness is a "hard" dimension to control? (Answer: Models may have a training bias toward high intellectual curiosity/Openness, making low-Openness profiles difficult to express).

- **System Prompt Primacy**: The entire architecture relies on the System Prompt overriding the model's inherent "helpful assistant" priors. Quick check: What happens if user input contradicts the personality definition in the system prompt? (Answer: The paper assumes the system prompt dominates, but strong user prompting is a known vector for persona shedding).

## Architecture Onboarding

- **Component map**: Character Builder Agent (GPT-4o) → Target Agent (Base or Fine-tuned Model) → Evaluation Engine (administers tests, calculates metrics)

- **Critical path**: The generation of the JSON Personality Template is the critical dependency. If the Big Five and MBTI types generated here are statistically inconsistent, the downstream evaluation metrics will be inherently noisy regardless of model capability.

- **Design tradeoffs**: Reasoning Models (o1) vs. Standard Models (4o): o1 provides the most "human-like" variance and stability but is likely slower/more expensive. 4o is accurate but may be less robust under "motivation" loads. Motivation Requirement: Requiring the agent to explain answers adds a "reasoning tax" that may degrade performance in non-reasoning-optimized models.

- **Failure signatures**: The "Openness Floor" (agents consistently fail to score below ~3.0 on Openness), Diagonal Confusion Matrices (if perfectly diagonal, agent is likely "gaming" the test), Style-Value Decoupling Failure (if fine-tuning for "edgy" style causes Agreeableness score to drop mechanically).

- **First 3 experiments**: 1) Baseline Consistency Check: Run 10 agents on base GPT-4o without motivation to establish variance baseline. 2) Reasoning Stress Test: Re-run requiring "motivation" for each answer to verify hypothesis about model degradation/stabilization. 3) Orthogonality Verification: Take high-performing agent, apply "edgy" fine-tuning, and re-test to confirm style changes while metrics remain stable.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does deterministic personality expression persist in multimodal contexts or complex agent-to-agent interactions? Experiments were limited to text-based questionnaires rather than video, audio, or dynamic task-oriented environments.

- **Open Question 2**: Can the systematic bias toward high Openness scores be corrected through system prompts or fine-tuning? Current fine-tuning efforts had only a minor impact on reducing this specific trait bias.

- **Open Question 3**: How does deterministic personality expression influence user trust and interaction outcomes in applied settings? The study focused on technical generation rather than human reception or efficacy of these personalities.

## Limitations

- The inability to express low Openness scores appears to be a model training artifact rather than a theoretical limitation, suggesting personality expression may be bounded by base model priors.

- The orthogonal relationship between style and personality traits, while supported by data, relies on assumptions about fine-tuning dataset composition that aren't fully validated.

- Response-scale variance metrics, while theoretically important, may be influenced by stochastic generation settings that weren't systematically controlled.

## Confidence

- **High Confidence**: Models can express consistent personalities when instructed (supported by multiple metrics across all tested models)
- **Medium Confidence**: Personality expression operates through holistic reasoning rather than question-by-question optimization (supported by variance patterns but indirect evidence)
- **Medium Confidence**: Orthogonal control of style and personality traits (qualitative evidence supports but fine-tuning details are limited)
- **Low Confidence**: Openness bias is primarily a training artifact (inferred from data but not experimentally validated)

## Next Checks

1. **Training Bias Validation**: Systematically test low-Openness personality expression across multiple base models and fine-tuning datasets to isolate whether the bias is model-specific or task-specific

2. **Orthogonality Stress Test**: Create fine-tuning datasets with strong semantic conflicts (e.g., aggressive style paired with high agreeableness prompt) and measure breakdown points in the style-personality separation

3. **Context Window Boundary**: Test personality expression stability across varying prompt lengths and conversation contexts to identify minimum context requirements for maintaining persona consistency