---
ver: rpa2
title: 'Conda: Column-Normalized Adam for Training Large Language Models Faster'
arxiv_id: '2509.24218'
source_url: https://arxiv.org/abs/2509.24218
tags:
- conda
- muon
- training
- arxiv
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Conda, a novel optimizer that addresses spectral
  pathologies in Adam-based training of transformer architectures. The key idea is
  to integrate column-wise spectral normalization into Adam's update mechanism by
  projecting gradients into an orthogonal subspace and applying second-moment normalization
  to projected gradients.
---

# Conda: Column-Normalized Adam for Training Large Language Models Faster

## Quick Facts
- arXiv ID: 2509.24218
- Source URL: https://arxiv.org/abs/2509.24218
- Authors: Junjie Wang; Pan Zhou; Yiming Dong; Huan Li; Jia Li; Xun Zhou; Qicheng Lao; Cong Fang; Zhouchen Lin
- Reference count: 40
- Key outcome: Conda achieves 2-2.5× faster convergence than AdamW on LLaMA models in both training steps and wall-clock time

## Executive Summary
This paper introduces Conda, a novel optimizer that addresses spectral pathologies in Adam-based training of transformer architectures. The key innovation is integrating column-wise spectral normalization into Adam's update mechanism by projecting gradients into an orthogonal subspace and applying second-moment normalization to projected gradients. This approach preserves Adam's coordinate-wise adaptivity while achieving better spectral conditioning. Extensive experiments demonstrate that Conda consistently outperforms AdamW, Muon, and other baselines across multiple benchmarks.

## Method Summary
Conda modifies the Adam optimizer by incorporating spectral normalization at the column level of weight matrices. The algorithm projects gradients into an orthogonal subspace and applies second-moment normalization to these projected gradients, creating an update rule that maintains Adam's adaptivity while improving spectral conditioning. This design specifically targets the spectral pathologies that can slow convergence in large transformer models. The implementation requires only modest memory overhead compared to standard AdamW, making it practical for large-scale deployments.

## Key Results
- Conda achieves 2-2.5× faster convergence than AdamW on LLaMA models
- Consistent performance improvements across GPT-2 and LLaMA series
- Shows robustness across different model scales, sequence lengths, and hyperparameters
- Modest memory overhead compared to AdamW

## Why This Works (Mechanism)
Conda works by addressing the spectral conditioning problems inherent in Adam-based optimization for transformers. The optimizer projects gradients into an orthogonal subspace, which helps decouple different frequency components of the gradient signal. By applying second-moment normalization to these projected gradients, Conda maintains adaptive learning rates while improving the spectral properties of the updates. This approach preserves the coordinate-wise adaptivity that makes Adam effective while adding a layer of spectral control that prevents pathological scaling issues common in large transformer training.

## Foundational Learning
- **Spectral normalization**: Technique for controlling the spectral norm of weight matrices to improve training stability
  - Why needed: Large language models often suffer from exploding/vanishing gradients due to poor spectral conditioning
  - Quick check: Verify that spectral norms remain bounded during training

- **Gradient projection**: Mathematical operation that maps gradients into specific subspaces
  - Why needed: Allows selective modification of gradient components while preserving others
  - Quick check: Confirm projection preserves gradient direction in the target subspace

- **Second-moment normalization**: Adaptive scaling based on historical gradient variance
  - Why needed: Enables per-parameter learning rate adaptation without manual tuning
  - Quick check: Monitor that adaptive learning rates respond appropriately to gradient statistics

## Architecture Onboarding

Component Map:
Input gradients -> Gradient projection -> Second-moment normalization -> Parameter update -> Loss computation

Critical Path:
The critical path involves computing gradients, projecting them into the orthogonal subspace, applying second-moment normalization, and updating parameters. The projection step is the key innovation that distinguishes Conda from standard Adam.

Design Tradeoffs:
- Preserves Adam's adaptivity while adding spectral control
- Introduces orthogonality constraint that may affect gradient flow
- Modest memory overhead but additional computational complexity

Failure Signatures:
- Convergence issues if projection is too aggressive
- Memory bottlenecks if implementation is not optimized
- Potential loss of adaptivity if normalization parameters are poorly tuned

First Experiments:
1. Compare training curves of Conda vs AdamW on small GPT-2 variants
2. Measure spectral norms of weight matrices during training with both optimizers
3. Profile memory usage and computational overhead relative to AdamW

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily demonstrated on transformer architectures, generalizability to other architectures untested
- Focus on language modeling tasks leaves effectiveness for other domains unverified
- Theoretical analysis provides justification but practical implications of orthogonality constraint need more investigation
- Memory overhead comparisons presented but absolute requirements for large-scale deployments not fully characterized

## Confidence
- Convergence speed improvements (2-2.5× faster): High confidence based on multiple benchmarks
- Robustness across scales and hyperparameters: Medium confidence, limited parameter exploration
- Spectral conditioning benefits: Medium confidence, theoretical but needs more empirical validation
- Memory overhead claims: High confidence for stated comparisons, but incomplete characterization

## Next Checks
1. Test Conda on non-transformer architectures (CNNs, RNNs, GNNs) to verify architectural generalizability
2. Conduct ablation studies varying the orthogonality constraint strength to understand its impact on convergence
3. Perform large-scale memory profiling on production-sized models to quantify absolute memory overhead in real-world deployments