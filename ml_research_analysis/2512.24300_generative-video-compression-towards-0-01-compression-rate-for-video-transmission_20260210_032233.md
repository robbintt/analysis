---
ver: rpa2
title: 'Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission'
arxiv_id: '2512.24300'
source_url: https://arxiv.org/abs/2512.24300
tags:
- video
- compression
- generative
- communication
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Video Compression (GVC), a new
  video compression framework that achieves extreme compression rates (as low as 0.02%)
  by leveraging generative video models. Instead of traditional pixel-level compression,
  GVC encodes video into compact latent representations and reconstructs it using
  generative priors, trading computation for compression.
---

# Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission

## Quick Facts
- **arXiv ID**: 2512.24300
- **Source URL**: https://arxiv.org/abs/2512.24300
- **Reference count**: 4
- **Primary result**: Achieves 0.02% compression rate using generative priors, requiring up to 6x less bandwidth than HEVC

## Executive Summary
This paper introduces Generative Video Compression (GVC), a new video compression framework that achieves extreme compression rates (as low as 0.02%) by leveraging generative video models. Instead of traditional pixel-level compression, GVC encodes video into compact latent representations and reconstructs it using generative priors, trading computation for compression. The framework is built on the AI Flow paradigm and prioritizes task-oriented communication over fidelity. Experimental results on the MCL-JCV dataset show that GVC achieves competitive perceptual quality (LPIPS=0.180) at just 0.008 bpp, requiring up to 6x less bandwidth than HEVC. It also maintains strong downstream task performance (VOS J&F=79.28%) at 0.0175 bpp. To ensure practicality, the authors propose a compression-computation trade-off strategy enabling inference on consumer-grade GPUs with around 2-second latency, demonstrating GVC's potential for real-world deployment in bandwidth-constrained environments.

## Method Summary
GVC consists of two main components: a Neural Encoder that extracts compressed tokens (keyframes, high-level descriptors, low-level features) from input video, and a Generative Video Decoder that reconstructs video from these tokens using a diffusion-based model. The framework shifts the reconstruction burden from transmission to the receiver, where powerful generative priors synthesize high-quality video from minimal transmitted information. A compression-computation trade-off strategy allows practical deployment on consumer GPUs by adjusting the balance between transmitted information density and decoder model complexity.

## Key Results
- Achieves extreme compression rates of 0.008 bpp (0.02% compression rate) with LPIPS=0.180
- Requires up to 6x less bandwidth than HEVC while maintaining perceptual quality
- Maintains strong downstream task performance (VOS J&F=79.28%) at 0.0175 bpp
- Enables practical deployment on consumer-grade GPUs with approximately 2-second inference latency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Generative priors enable reconstruction of perceptually high-quality video from extremely compact latent representations.
- **Mechanism**: The decoder's pre-trained generative model synthesizes frames using learned priors rather than relying on transmitted pixel data. The encoder transmits only task-relevant descriptors, reducing the information burden on the channel.
- **Core assumption**: The generative model's learned priors are sufficiently general to reconstruct semantically and perceptually faithful video across diverse input distributions.
- **Evidence anchors**: [abstract] "encodes video into extremely compact representations and delegates content reconstruction to the receiver"; [section 2.2] "Modern generative models are capable of producing high-quality video given only latent representations"; [corpus] Related work supports diffusion models for semantic reconstruction.

### Mechanism 2
- **Claim**: Conditional diffusion synthesis reconstructs video guided by both discrete and continuous compressed tokens.
- **Mechanism**: Compressed tokens serve dual roles—some as direct inputs to the denoising process, others as conditioning signals. The reconstruction becomes a conditional generation task where the model iteratively denoises toward outputs that are visually faithful to the original.
- **Core assumption**: The token design preserves sufficient semantic and motion information for the diffusion model to reconstruct temporally coherent video.
- **Evidence anchors**: [section 2.1] "Some of these tokens serve as direct inputs to the denoising process, while others function as the conditions"; [abstract] "encoding videos into compact latent representations and reconstructing them using powerful generative priors at the decoder"; [corpus] Conditional Video Generation for High-Efficiency Video Compression corroborates conditional diffusion.

### Mechanism 3
- **Claim**: Enriching latent representations allows smaller, faster decoder models while maintaining perceptual quality.
- **Mechanism**: When decoder computational capacity is constrained, the framework increases the information density of transmitted latents, reducing reliance on heavy generative inference. Model compression techniques further trade compression for practicality.
- **Core assumption**: The quality loss from model simplification can be adequately compensated by richer latents without negating compression gains.
- **Evidence anchors**: [section 2.3] "One such strategy is to increase the richness of the compressed latent representations, thereby reducing the reliance on large generative models"; [abstract] "model compression techniques enable practical deployment on consumer-grade GPUs with inference latency around 2 seconds"; [corpus] Weak evidence on compression-computation trade-offs in related work.

## Foundational Learning

- **Concept: Diffusion Models for Conditional Generation**
  - **Why needed here**: The decoder is a diffusion-based video generator; understanding denoising schedules, classifier-free guidance, and conditioning mechanisms is essential for debugging reconstruction quality.
  - **Quick check question**: Can you explain how a diffusion model uses a conditioning signal (e.g., text or latent) to guide the denoising trajectory?

- **Concept: Rate-Distortion Theory and Perceptual Metrics**
  - **Why needed here**: GVC optimizes for perceptual quality (LPIPS) rather than pixel-level fidelity (PSNR); understanding this shift is critical for interpreting results.
  - **Quick check question**: Why does LPIPS better correlate with human perception than MSE/PSNR for generative reconstruction?

- **Concept: Task-Oriented Communication (Shannon-Weaver Level C)**
  - **Why needed here**: GVC explicitly targets task-effectiveness rather than signal fidelity; this paradigm shift frames design decisions.
  - **Quick check question**: In the Shannon-Weaver model, what distinguishes Level C (effectiveness) from Level A (technical/accuracy)?

## Architecture Onboarding

- **Component map**: Input video -> Neural Encoder -> compressed tokens -> Residual Coding -> bitstream -> decoding -> tokens -> Generative Decoder (diffusion denoising) -> reconstructed video

- **Critical path**:
  1. Input video → Neural Encoder → compressed tokens
  2. Compressed tokens → Residual Coding → bitstream
  3. Bitstream → decoding → tokens
  4. Tokens → Generative Decoder (diffusion denoising) → reconstructed video

- **Design tradeoffs**:
  - Higher latent richness → lower decoder compute but higher bitrate
  - Larger generative model → better quality at extreme compression but higher latency
  - Task-specific token design → better downstream performance but potentially worse generic perception

- **Failure signatures**:
  - Temporal flickering or incoherent motion: insufficient motion dynamics in tokens
  - Semantic drift (wrong objects/actions): latent representation discards critical semantics
  - Excessive latency on target hardware: model too large; revisit compression-computation tradeoff

- **First 3 experiments**:
  1. Baseline compression test: Run GVC on MCL-JCV sequences, measure LPIPS at 0.008 bpp, compare against HEVC to reproduce paper claims
  2. Ablation on latent richness: Reduce token dimensionality by 50%, measure quality drop and bitrate reduction to quantify the compression-computation tradeoff
  3. Downstream task validation: Pass reconstructed video through a pre-trained XMEM model on DAVIS2017, verify J&F scores approximate reported 75.22%

## Open Questions the Paper Calls Out
None

## Limitations
- Unspecified encoder architecture and compression scheme details prevent exact reproduction
- Limited comparison against strong traditional baselines like VVC/H.266 on standard benchmarks
- Compression-computation tradeoff lacks quantitative mapping between model size, latent dimensionality, and latency

## Confidence

- **High confidence**: The core generative video compression paradigm is sound and well-supported by related work on diffusion models for conditional generation and semantic communication.
- **Medium confidence**: The specific compression rates and perceptual quality are likely achievable given the reported methodology, but exact reproduction requires resolving unspecified encoder details.
- **Low confidence**: The practical deployment claims (2-second latency on consumer GPUs) lack sufficient detail about hardware specifications and actual computational breakdown.

## Next Checks

1. **Compression fidelity validation**: Run GVC on the MCL-JCV dataset at 0.008 bpp and measure LPIPS against HEVC at equivalent rates using standard HEVC configurations (e.g., x265 with appropriate settings).

2. **Downstream task verification**: Take reconstructed videos from step 1 and run them through a pre-trained XMEM model on the DAVIS2017 dataset to verify the reported VOS performance metrics (J&F=79.28%, J=75.22%).

3. **Latency profiling**: Implement the compression-computation tradeoff strategy by progressively simplifying the decoder model while measuring actual inference time on a consumer GPU (e.g., RTX 4060) to verify the 2-second latency claim.