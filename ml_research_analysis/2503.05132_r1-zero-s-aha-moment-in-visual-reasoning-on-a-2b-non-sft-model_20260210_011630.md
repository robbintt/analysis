---
ver: rpa2
title: R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model
arxiv_id: '2503.05132'
source_url: https://arxiv.org/abs/2503.05132
tags:
- reasoning
- training
- length
- response
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper successfully replicates the "aha moment" phenomenon
  from DeepSeek R1 for multimodal reasoning by applying reinforcement learning directly
  to a non-SFT 2B model (Qwen2-VL-2B). Starting from the base model and training on
  the SAT dataset, their VisualThinker R1 Zero achieved 59.47% accuracy on CVBench,
  outperforming the base model by ~30% and instruction-tuned SFT models by ~2%.
---

# R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model

## Quick Facts
- arXiv ID: 2503.05132
- Source URL: https://arxiv.org/abs/2503.05132
- Authors: Hengguang Zhou; Xirui Li; Ruochen Wang; Minhao Cheng; Tianyi Zhou; Cho-Jui Hsieh
- Reference count: 20
- Primary result: RL on non-SFT 2B model achieves 59.47% CVBench accuracy, outperforming base by ~30% and SFT models by ~2%

## Executive Summary
This paper successfully replicates the "aha moment" phenomenon from DeepSeek R1 for multimodal reasoning by applying reinforcement learning directly to a non-SFT 2B model (Qwen2-VL-2B). Starting from the base model and training on the SAT dataset, their VisualThinker R1 Zero achieved 59.47% accuracy on CVBench, outperforming the base model by ~30% and instruction-tuned SFT models by ~2%. The key innovation is demonstrating that RL on non-SFT models induces autonomous reasoning capabilities, while applying RL to instruction-tuned models leads to trivial reasoning patterns rather than genuine problem-solving. The model exhibited increased response length and self-reflection during training, key characteristics of R1-like reasoning.

## Method Summary
The authors applied reinforcement learning directly to a non-SFT 2B Qwen2-VL model, using the SAT dataset as the primary training corpus. They employed a combination of supervised fine-tuning and reinforcement learning phases, with specific reward mechanisms designed to encourage reasoning behaviors. The training pipeline included careful curriculum design to balance reasoning and non-reasoning examples, along with mechanisms to prevent reward hacking and ensure genuine reasoning development.

## Key Results
- VisualThinker R1 Zero achieved 59.47% accuracy on CVBench, outperforming base model by ~30%
- RL on non-SFT models induced autonomous reasoning capabilities while RL on SFT models produced trivial patterns
- Model exhibited increased response length and self-reflection during training
- Demonstrated ~2% improvement over instruction-tuned SFT models

## Why This Works (Mechanism)
The "aha moment" emerges when RL is applied to non-SFT models because these models retain their original reasoning capabilities while being exposed to structured reasoning tasks. The base model's inherent reasoning abilities interact with RL's reward signals to develop sophisticated problem-solving strategies. In contrast, instruction-tuned models have their reasoning capabilities suppressed through alignment training, making them unable to develop genuine reasoning patterns when exposed to RL.

## Foundational Learning
- **Reinforcement Learning**: Why needed - optimizes behavior through reward signals rather than direct supervision. Quick check - verify reward functions properly incentivize reasoning length and correctness.
- **Non-SFT vs SFT models**: Why needed - base models retain original reasoning capabilities while SFT models are aligned for instruction following. Quick check - compare response patterns between base and SFT models under same RL conditions.
- **Curriculum Learning**: Why needed - gradually introduces complexity to build reasoning capabilities systematically. Quick check - measure performance improvements across different curriculum stages.
- **Visual-Text Reasoning**: Why needed - combines visual perception with logical reasoning for multimodal problem-solving. Quick check - validate reasoning chains on increasingly complex visual problems.
- **Reward Shaping**: Why needed - guides model behavior without over-constraining solution space. Quick check - ensure rewards encourage reasoning length without promoting verbosity.
- **Base Model Properties**: Why needed - understanding inherent capabilities before RL application. Quick check - benchmark base model reasoning abilities before RL training.

## Architecture Onboarding
- **Component Map**: Base Model -> SAT Dataset -> RL Training Loop -> CVBench Evaluation -> Analysis
- **Critical Path**: Base model reasoning capabilities → SAT dataset exposure → RL reward optimization → reasoning pattern emergence → CVBench validation
- **Design Tradeoffs**: Non-SFT models retain reasoning but may be less aligned; SFT models are aligned but lose reasoning capabilities. The choice between them depends on whether the goal is instruction following or autonomous reasoning.
- **Failure Signatures**: Trivial reasoning patterns when applying RL to SFT models; reward hacking when length rewards are too strong; loss of general capabilities when training data is too narrow.
- **First Experiments**: 1) Test RL on different non-SFT model sizes to establish scalability limits, 2) Apply RL to SFT models to confirm trivial pattern emergence, 3) Vary SAT dataset composition to find optimal curriculum design.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to 2B parameter model size, scalability to larger models remains untested
- Single SAT dataset may introduce domain-specific biases limiting generalization
- No analysis of reasoning capabilities across different base model architectures or training histories

## Confidence
- High confidence: RL on non-SFT models produces autonomous reasoning capabilities
- Medium confidence: RL on SFT models leads to trivial reasoning patterns
- Medium confidence: "aha moment" phenomenon transfers from text to multimodal reasoning

## Next Checks
1. Test the approach on larger model architectures (7B-70B parameters) to assess scalability limits
2. Conduct ablation studies varying the proportion of reasoning vs. non-reasoning examples in the training dataset
3. Evaluate transfer performance across diverse visual reasoning benchmarks beyond CVBench to establish generalizability