---
ver: rpa2
title: Fully-Decentralized MADDPG with Networked Agents
arxiv_id: '2503.06747'
source_url: https://arxiv.org/abs/2503.06747
tags:
- agents
- maddpg
- agent
- consensus
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fully decentralized multi-agent reinforcement
  learning algorithms based on MADDPG, addressing scalability and communication limitations
  in centralized training approaches. The core method involves replacing centralized
  critics with local ones trained on individual replay buffers, using surrogate policies
  to approximate other agents' behaviors from local observations.
---

# Fully-Decentralized MADDPG with Networked Agents

## Quick Facts
- arXiv ID: 2503.06747
- Source URL: https://arxiv.org/abs/2503.06747
- Reference count: 40
- One-line primary result: Fully-decentralized MADDPG achieves comparable performance to centralized MADDPG for 2-5 agents with faster convergence and improved stability for larger populations (10 agents)

## Executive Summary
This paper introduces fully decentralized multi-agent reinforcement learning algorithms based on MADDPG, addressing scalability and communication limitations in centralized training approaches. The core method replaces centralized critics with local ones trained on individual replay buffers, using surrogate policies to approximate other agents' behaviors from local observations. Two communication strategies are proposed: hard consensus updates that average critic parameters across neighboring agents, and soft consensus updates that add a penalty term to encourage parameter similarity. Experiments in the multi-particle environment show that decentralized MADDPG achieves comparable performance to the original algorithm for 2-5 agents, with faster convergence and improved stability for larger agent populations (10 agents). The algorithms also generalize to adversarial settings by treating opponents' actions as fixed during policy optimization.

## Method Summary
The paper proposes three decentralized MADDPG variants: fully-decentralized with surrogate policies, hard consensus (parameter averaging across neighbors), and soft consensus (penalty-based parameter alignment). Each agent maintains local replay buffers storing true joint actions and observations, learns a joint policy network outputting all agents' actions from local observations, and updates critics using local data. Consensus mechanisms coordinate learning across the communication network. The approach enables training without centralized state access while preserving multi-agent credit assignment through surrogate policy approximations.

## Key Results
- Fully-decentralized MADDPG achieves comparable performance to original MADDPG for 2-5 agents
- Faster convergence and improved stability for larger agent populations (10 agents)
- Networked algorithms (hard and soft consensus) converge equally fast as fully-decentralized variant
- Soft consensus shows better stability with aggressive hyperparameters compared to hard consensus
- Algorithms generalize to adversarial settings by treating opponents' actions as fixed during policy optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surrogate policies enable decentralized training by approximating teammates' behaviors from local observations alone.
- Mechanism: Each agent learns a joint policy network μθᵢ(oᵢ) that outputs all N agents' actions, using only its own observations. During critic updates, this surrogate serves as a plug-in estimator for target y values. The paper describes this as "imagined, intrinsic representation" akin to how humans anticipate teammates' actions when learning cooperative tasks.
- Core assumption: Agents can infer others' policies from their own local observation stream and true joint actions recorded in replay buffers. This assumes sufficient correlation between local observations and global state.
- Evidence anchors:
  - [abstract] "We introduce surrogate policies in order to decentralize the training while allowing for local communication during training."
  - [section 3.2] Describes how surrogate policies model other agents "as one centrally controlled actor" and increase output dimension to match joint action space.
  - [corpus] Moderate support from "Distributed Value Decomposition Networks with Networked Agents" (FMR=0.62) which addresses similar distributed training under partial observability.
- Break condition: If local observations provide insufficient signal about teammate behavior (e.g., teammates rarely enter observation range), surrogate policies may diverge from true policies, causing unstable learning.

### Mechanism 2
- Claim: Local replay buffers with true joint actions enable critic learning without centralized state access.
- Mechanism: Each agent i stores transitions (oᵢ, a, rᵢ, o'ᵢ) where a represents the true joint action taken by all agents. Critics learn Qᵢ(oᵢ, a) using these buffers. This decouples training across agents while preserving the joint action information needed for credit assignment in multi-agent settings.
- Core assumption: Agents can observe or be informed of joint actions taken during environment interaction, even if they cannot access centralized state or other agents' policies during training.
- Evidence anchors:
  - [section 3.2] "The local critic Qᵢ is then updated using solely the local replay buffer of agent i" and notes that "true joint actions are used, meaning that we assume that each agent's critic has access to the true joint actions."
  - [section 4.3.2] Shows that fully-decentralized MADDPG achieves comparable performance to original MADDPG for 2-5 agents with faster convergence.
  - [corpus] Limited direct evidence; corpus papers focus on value decomposition rather than replay buffer architectures.
- Break condition: If joint action observation is unavailable or expensive to implement (e.g., requires communication during execution), this mechanism fails. Scalability may degrade if joint action dimension grows too large.

### Mechanism 3
- Claim: Consensus updates coordinate decentralized critics through parameter averaging or penalty-based alignment.
- Mechanism: Hard consensus replaces each agent's critic parameters with a weighted average of neighbors' parameters after each learning interval: μᵢ ← Σⱼ Cᵗ(i,j)μⱼ. Soft consensus adds a regularization term to the critic loss: ζ · Σⱼ Cᵗ(i,j)∥μᵢ - μⱼ∥² / ∥μⱼ∥². Both approaches encourage parameter similarity across the communication network.
- Core assumption: Neighboring agents in the communication graph share compatible learning objectives (cooperative setting) and parameter similarity correlates with coordinated behavior.
- Evidence anchors:
  - [section 3.3.1-3.3.2] Formal definitions of hard and soft consensus updates with communication matrix Cᵗ.
  - [section 4.3.2] "Networked algorithms...train equally fast as the fully decentralized algorithm and especially for N=5 agents considerably faster than the (centralized) MADDPG." Soft consensus shows better stability with aggressive hyperparameters.
  - [corpus] "Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization" provides external validation of consensus mechanisms, though in different domain.
- Break condition: Hard consensus can cause instability if averaging disrupts individual learning progress (paper notes this). Soft consensus may fail if penalty weight ζ is too high (over-regularization) or too low (no coordination benefit).

## Foundational Learning

- Concept: **Actor-Critic Methods with Deterministic Policies (DDPG)**
  - Why needed here: The paper builds directly on MADDPG, which extends DDPG to multi-agent settings. Understanding off-policy learning, replay buffers, target networks, and the deterministic policy gradient is essential before tackling the decentralized modifications.
  - Quick check question: Can you explain why DDPG uses target networks and how the deterministic policy gradient differs from the stochastic policy gradient?

- Concept: **Partially Observable Stochastic Games (POSG)**
  - Why needed here: The paper formalizes the problem as POSG with networked agents, where each agent has individual observations and reward functions. Distinguishing POSG from MDPs and Dec-POMDPs clarifies why centralized training is challenging.
  - Quick check question: What information does an agent in a POSG lack compared to a centralized controller, and how does this affect credit assignment?

- Concept: **Consensus Algorithms in Distributed Optimization**
  - Why needed here: The hard and soft consensus mechanisms draw from distributed optimization literature. Understanding why parameter averaging can converge to shared solutions helps diagnose when consensus helps versus harms learning.
  - Quick check question: Under what conditions does averaging parameters across agents converge to a shared optimum?

## Architecture Onboarding

- Component map:
  - Actor network (MLP) -> Surrogate joint policy (outputs all N agents' actions) -> Local observations
  - Critic network (MLP) -> Q-value estimation -> Local observations + true joint actions
  - Local replay buffer -> Stores (oᵢ, a_joint, rᵢ, o'ᵢ) tuples -> Environment interaction
  - Communication module -> Parameter averaging (hard) or penalty (soft) -> Consensus matrix Cᵗ
  - Target networks -> Soft update with τ -> Actor/critic parameters

- Critical path:
  1. Environment step → each agent selects action from surrogate joint policy using only local obs
  2. Execute joint action → observe rewards and next observations
  3. Store transitions in local buffers (with true joint actions)
  4. Sample minibatch → compute critic targets using surrogate policies for predicted actions
  5. Update critic via TD loss; update actor via deterministic policy gradient
  6. Apply consensus (hard: override params; soft: add penalty during loss computation)
  7. Soft update target networks

- Design tradeoffs:
  - **Hard vs. soft consensus**: Hard is simpler but can destabilize learning; soft is more stable but introduces hyperparameter ζ.
  - **Communication frequency**: Lower η (0.001) is more stable; higher η (0.05) causes instability for hard consensus (section B.2).
  - **Network connectivity**: Sparse vs. fully-connected communication graphs show minimal performance difference (section B.3), so use sparse for computational efficiency.
  - **Surrogate vs. true policies**: Surrogates reduce dependency on centralized information but may introduce approximation error.

- Failure signatures:
  - **Hard consensus instability**: Oscillating or diverging scores, especially with larger agent counts or aggressive communication. Switch to soft consensus.
  - **Surrogate policy drift**: Agents converge to incompatible policies despite shared reward. Indicates local observations insufficient for inferring teammates.
  - **Scalability collapse**: Performance degrades sharply beyond ~10 agents. Paper notes even original MADDPG struggles here.
  - **No learning in adversarial settings**: Paper reports failure to learn effective policies even after hyperparameter search—inherent limitation of base MADDPG in these environments.

- First 3 experiments:
  1. **Cooperative 3-agent simple-spread**: Compare fully-decentralized vs. hard consensus vs. soft consensus vs. original MADDPG. Use η=0.001, train 3×10⁵ steps. Expected: similar final scores, decentralized variants converge faster.
  2. **Ablation on communication sparsity**: 10-agent simple-spread with circle topology vs. fully-connected. Use soft consensus with η=0.001. Expected: minimal performance difference, lower compute for circle.
  3. **Hyperparameter sensitivity test**: Vary ζ for soft consensus (0.001, 0.01, 0.1) and η for hard consensus (0.001, 0.01, 0.05) in 5-agent setting. Expected: hard consensus unstable at high η; soft consensus robust across ζ range.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed decentralization techniques (surrogate policies, networked consensus) improve performance when applied to algorithms like MAPPO that outperform MADDPG in Multi-Particle Environments?
- Basis in paper: [explicit] Section 5 states the authors wish to "apply the decentralization techniques... to other algorithms such as MAPPO, which perform considerably better in MPE environments."
- Why unresolved: The current study is restricted to MADDPG, which the authors note suffers from poor cooperation and performance issues in the tested settings, potentially masking the benefits of the decentralization methods.
- What evidence would resolve it: Empirical results comparing centralized MAPPO against the decentralized, networked variants in the same cooperative tasks.

### Open Question 2
- Question: Is the decentralized algorithm viable if the assumption that critics have access to the true joint actions is removed?
- Basis in paper: [inferred] Section 3.2 explicitly states, "we assume that each agent's critic has access to the true joint actions," despite the goal of fully decentralized training.
- Why unresolved: This assumption simplifies the problem significantly by bypassing the need to infer teammate actions for the critic update. It is unknown if the method collapses if the critic must rely only on local observations or estimated joint actions.
- What evidence would resolve it: Ablation studies where the critic input is restricted to local action-observation histories rather than ground-truth joint actions.

### Open Question 3
- Question: Do the proposed algorithms converge in adversarial settings when initialized with stable hyperparameters?
- Basis in paper: [inferred] Section 4.4.2 notes that in adversarial settings, "agents do not learn effective policies at all," but attributes this to the "shortcoming of the underlying MADDPG algorithm" rather than the decentralized architecture.
- Why unresolved: It remains unclear if the fully-decentralized modifications exacerbate the instability in adversarial zero-sum games or if the failure is purely inherited from the base MADDPG implementation.
- What evidence would resolve it: Experiments in "Simple Adversary" environments showing learning stability for the decentralized variants where standard MADDPG fails.

## Limitations

- Missing core hyperparameters (learning rate, batch size, replay buffer size, discount factor, target update rate) that could significantly impact performance comparisons
- No ablation study isolating the effect of surrogate policies versus consensus mechanisms on performance gains
- Limited scalability testing beyond 10 agents, with performance degradation noted but not thoroughly characterized

## Confidence

- **High**: The theoretical foundation of using surrogate policies to approximate teammate behaviors from local observations is well-established in multi-agent RL literature
- **Medium**: The experimental results showing comparable performance to centralized MADDPG for 2-5 agents and faster convergence are supported by the paper's evidence, but exact hyperparameters are missing
- **Low**: The claim of scalability to larger agent populations (10 agents) is demonstrated but lacks comparison against alternative decentralized approaches or state-of-the-art methods

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary learning rate, batch size, and communication penalty weight ζ to identify robust settings and understand performance variability
2. **Scalability stress test**: Evaluate the algorithms with 15-20 agents in the simple-spread environment to determine where performance degradation begins and compare against other decentralized methods
3. **Generalization to different domains**: Test the algorithms in heterogeneous agent environments (varying observation spaces, action dimensions) to assess robustness beyond the homogeneous MPE setting