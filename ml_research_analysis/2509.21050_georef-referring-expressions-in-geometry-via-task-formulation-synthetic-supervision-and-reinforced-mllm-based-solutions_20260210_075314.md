---
ver: rpa2
title: 'GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic
  Supervision, and Reinforced MLLM-based Solutions'
arxiv_id: '2509.21050'
source_url: https://arxiv.org/abs/2509.21050
tags:
- geometric
- qwen2
- reasoning
- answer
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Referring Expression Comprehension
  (REC) for geometric problems, designed to assess models' ability to localize geometric
  elements based on natural language queries. To support this task, the authors present
  GeoRef, a benchmark dataset built from existing geometric problem corpora, featuring
  diverse, high-quality annotations and queries.
---

# GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions

## Quick Facts
- **arXiv ID**: 2509.21050
- **Source URL**: https://arxiv.org/abs/2509.21050
- **Reference count**: 40
- **Key outcome**: Introduces GeoRef benchmark for Referring Expression Comprehension in geometry, combining synthetic data generation with reinforced fine-tuning to achieve state-of-the-art results.

## Executive Summary
This paper introduces a novel task for Referring Expression Comprehension (REC) in geometric problems, designed to evaluate models' ability to localize geometric elements based on natural language queries. The authors present GeoRef, a benchmark dataset constructed from existing geometric problem corpora, featuring diverse high-quality annotations. To address the lack of training data, they develop a large-scale synthetic dataset using structured geometric formal language. The paper explores both supervised fine-tuning and reinforcement learning approaches, introducing a verify-and-regenerate mechanism that improves accuracy by refining predictions using reasoning history. Experimental results demonstrate significant performance gains over existing methods.

## Method Summary
The authors tackle the challenge of geometric REC by first formulating the task and creating the GeoRef benchmark dataset from existing geometric problem sources. Due to limited annotated training data, they generate a synthetic dataset using a structured geometric formal language that ensures mathematical consistency while covering diverse scenarios. Two fine-tuning approaches are explored: Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO). The key innovation is a verify-and-regenerate mechanism that allows the model to iteratively refine incorrect predictions by leveraging contextual reasoning history. The enhanced model achieves state-of-the-art results on GeoRef and demonstrates improved performance on downstream geometric reasoning tasks.

## Key Results
- GRPO significantly outperforms SFT on the GeoRef benchmark
- The verify-and-regenerate mechanism improves accuracy through iterative refinement
- Models trained on GeoRef show measurable improvements on downstream geometric reasoning tasks
- Synthetic data generation enables large-scale training despite limited real annotations

## Why This Works (Mechanism)
The approach succeeds by combining synthetic data generation with reinforcement learning and iterative refinement. The synthetic dataset provides broad coverage of geometric scenarios while maintaining mathematical consistency. GRPO enables the model to learn from rewards rather than just labeled examples, allowing it to develop better reasoning strategies. The verify-and-regenerate mechanism leverages the model's own reasoning history to catch and correct errors, effectively implementing a self-correction loop that improves final accuracy.

## Foundational Learning
- **Geometric formal language**: Needed to generate mathematically consistent synthetic data; quick check: verify generated problems are solvable and correctly annotated
- **Group Relative Policy Optimization**: Required for reinforcement learning in multimodal settings; quick check: compare reward signals before and after GRPO training
- **Referring expression comprehension**: Core task of localizing elements from natural language; quick check: measure precision/recall on localization tasks
- **Multimodal reasoning**: Integration of text and geometric understanding; quick check: test cross-modal consistency in predictions
- **Synthetic data generation**: Creates training data when real annotations are scarce; quick check: evaluate diversity coverage across geometric problem types
- **Iterative refinement mechanisms**: Enables self-correction through reasoning history; quick check: measure improvement rate across refinement iterations

## Architecture Onboarding
- **Component map**: Geometric formal language generator -> Synthetic dataset -> MLLM fine-tuning (SFT/GRPO) -> Verify-and-regenerate module -> Final predictions
- **Critical path**: Input query → Geometric element localization → Verification → (if failed) Regenerate → Output
- **Design tradeoffs**: Synthetic vs real data quality, computational cost of iterative refinement vs accuracy gains, GRPO complexity vs supervised learning simplicity
- **Failure signatures**: Incorrect initial localization, verification module false positives/negatives, iterative refinement getting stuck in loops, geometric inconsistency in synthetic data
- **First experiment**: Compare SFT vs GRPO performance on simple geometric problems
- **Second experiment**: Test verify-and-regenerate mechanism on problems with clear error patterns
- **Third experiment**: Evaluate synthetic data coverage by testing on real geometric problems not represented in synthetic set

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Synthetic dataset generation may introduce biases from structured formal language templates
- Verify-and-regenerate mechanism's effectiveness depends on reasoning history quality but lacks thorough failure mode analysis
- Benchmark dataset construction may inherit biases from source corpora without independent annotation validation

## Confidence
- Task formulation and benchmark creation: High
- Synthetic dataset generation effectiveness: Medium
- Enhanced model performance claims: High

## Next Checks
1. Conduct ablation studies on synthetic data quality by systematically varying template diversity and measuring impact on model performance across different geometric problem types
2. Implement a human evaluation study to assess the quality and consistency of annotations in both the benchmark and synthetic datasets, focusing on edge cases and ambiguous scenarios
3. Perform stress testing of the verify-and-regenerate mechanism by introducing controlled errors in the reasoning history and measuring the system's robustness to such perturbations