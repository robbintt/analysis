---
ver: rpa2
title: 'AI-Driven Generation of Old English: A Framework for Low-Resource Languages'
arxiv_id: '2507.20111'
source_url: https://arxiv.org/abs/2507.20111
tags:
- english
- translation
- data
- texts
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a framework for generating high-quality Old
  English texts using large language models, addressing the challenge of low-resource
  languages. The approach combines parameter-efficient fine-tuning, backtranslation,
  and a dual-agent pipeline for content generation and translation.
---

# AI-Driven Generation of Old English: A Framework for Low-Resource Languages

## Quick Facts
- **arXiv ID:** 2507.20111
- **Source URL:** https://arxiv.org/abs/2507.20111
- **Reference count:** 19
- **Primary result:** BLEU scores increase from 26 to over 65 for English-to-Old English translation using a dual-agent pipeline with parameter-efficient fine-tuning and backtranslation.

## Executive Summary
This study presents a framework for generating high-quality Old English texts using large language models, addressing the challenge of low-resource languages. The approach combines parameter-efficient fine-tuning, backtranslation, and a dual-agent pipeline for content generation and translation. Evaluation using automated metrics (BLEU, METEOR, CHRF) and expert human assessment demonstrates significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. The generated texts show high grammatical accuracy and stylistic fidelity, providing a scalable solution for expanding the Old English corpus and offering a blueprint for revitalizing other endangered languages through AI-driven synthetic data generation.

## Method Summary
The framework uses a dual-phase training approach with Llama 3.1 8B as the base model. Phase 1 applies parameter-efficient fine-tuning (LoRA) with four tasks (Text Completion, Forward/Back Translation, Crossed Definition) to create OldEnglishBase. Phase 2 employs backtranslation to generate synthetic parallel data, which is combined with human-annotated data to create OldEnglishRefined. A dual-agent pipeline separates content generation (using GPT-4o-mini) from translation (using OldEnglishRefined), allowing specialized handling of each task. The approach leverages Modern English as a high-resource anchor language to bootstrap Old English generation.

## Key Results
- BLEU scores improved from ~26 (baseline) to ~65 for English-to-Old English translation
- Expert evaluation showed high scores for Lexical Choice (9.1) and Inflection (9.0) out of 10
- Backtranslation augmentation improved forward translation performance from BLEU 60.0 to 65.4
- The model successfully avoided repetitive generation patterns common in low-resource scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Efficient domain adaptation via parameter-efficient fine-tuning (LoRA) appears sufficient to transfer competence from a high-resource language (Modern English) to a low-resource relative (Old English).
- **Mechanism:** By freezing the pre-trained model weights and injecting trainable low-rank decomposition matrices, the model adjusts its latent space to accommodate the morphological and syntactic specificities of Old English without catastrophic forgetting of the source language.
- **Core assumption:** The genetic proximity and shared vocabulary between Modern and Old English allow for efficient "task-similar" transfer learning.
- **Evidence anchors:** [Page 3] Describes "Efficient Task-Similar Domain-Adaptive Continual-Pretraining" using four tasks; [Page 5] Table 4 shows OldEnglishBase improving BLEU scores from ~29 to ~60 after 1–3 epochs.

### Mechanism 2
- **Claim:** Data augmentation via backtranslation correlates with improved forward translation fidelity by expanding the effective training corpus.
- **Mechanism:** The model translates monolingual Old English text into Modern English to generate synthetic parallel pairs. Training on this synthetic data regularizes the model, exposing it to broader lexical diversity than the limited gold-standard corpus allows.
- **Core assumption:** The model's competence in reverse translation (Old English → Modern English) is sufficiently high to generate "clean" synthetic source sentences for forward training.
- **Evidence anchors:** [Page 5] Table 5 indicates OldEnglishRefined (trained with backtranslation) outperforms OldEnglishBase in forward translation (BLEU 65.4 vs 60.0).

### Mechanism 3
- **Claim:** Decoupling content generation from translation in a dual-agent pipeline improves the stylistic and grammatical quality of synthetic low-resource data.
- **Mechanism:** A strong general-purpose agent ensures semantic coherence in the high-resource language, while a specialized agent handles the difficult morphological rendering into Old English.
- **Core assumption:** Generating coherent content directly in a low-resource language is harder than translating coherent content into it.
- **Evidence anchors:** [Page 4] Figure 2 illustrates the separation of Fragment Generator and Translation Agent; [Page 8] Expert evaluation shows high scores for Lexical Choice (9.1) and Inflection (9.0).

## Foundational Learning

- **Concept:** **Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The study relies on LoRA to fine-tune an 8B parameter model efficiently. Without understanding LoRA, one cannot replicate the "Efficient Task-Similar" training phase.
  - **Quick check question:** How does freezing the base model weights and injecting rank-decomposition matrices prevent catastrophic forgetting compared to full fine-tuning?

- **Concept:** **Backtranslation (in NMT)**
  - **Why needed here:** This is the core data augmentation strategy for the OldEnglishRefined model. Understanding the directionality (Target → Source → Target) is crucial for the data preparation phase.
  - **Quick check question:** Why is backtranslation particularly effective for low-resource languages where parallel data is scarce but monolingual target data exists?

- **Concept:** **Catastrophic Forgetting**
  - **Why needed here:** The paper explicitly monitors this during training. Table 4 shows performance dropping at 5 epochs, indicating the model is forgetting its original domain knowledge.
  - **Quick check question:** In the context of Table 4, why does the reverse translation score (ANG → EN) drop significantly as epochs increase, while forward translation remains more stable?

## Architecture Onboarding

- **Component map:** DOEC Corpus + Bosworth-Toller Dictionary -> Llama 3.1 8B (Transformer) -> LoRA Adapter -> OldEnglishBase/Refined -> Dual-Agent Pipeline (FragmentGen + OldEnglishTranslator)

- **Critical path:**
  1. **Data Curation:** Standardize DOEC using Classical Language Toolkit
  2. **Phase 1 (Base Adaptation):** Train LoRA adapter on 4 tasks (Completion, Fwd/Bck Translation, Definitions) -> Produces OldEnglishBase
  3. **Phase 2 (Refinement):** Generate synthetic pairs via backtranslation; retrain -> Produces OldEnglishRefined
  4. **Inference:** Run Dual-Agent pipeline for synthetic corpus expansion

- **Design tradeoffs:**
  - **Generalizability vs. Fidelity:** The paper notes the model avoids "structurally marked configurations" (like V2 word order), suggesting the model prioritizes standard fluency over complex historical accuracy
  - **Epoch count:** Training beyond 3 epochs induced catastrophic forgetting (Table 4). New engineers must strictly validate early stopping criteria

- **Failure signatures:**
  - **Looped Generation:** Output contains repetitive phrases (e.g., "fræolice fræolice...")
  - **Vocabulary Hallucination:** Inventing words or failing to translate specific terms (Table 3)
  - **Anachronisms:** Semantic context breaks when translating modern concepts (e.g., "data training") into Old English

- **First 3 experiments:**
  1. **Baseline Validation:** Run the base Llama 3.1 8B on the test set to replicate the low BLEU scores (~26) and identify specific "looped generation" errors
  2. **Overfitting Check:** Train the OldEnglishBase model and evaluate at 1, 3, and 5 epochs to confirm the performance degradation on reverse translation reported in Table 4
  3. **Backtranslation Ablation:** Compare OldEnglishBase vs. OldEnglishRefined on a held-out set to isolate the specific contribution of the backtranslation augmentation data

## Open Questions the Paper Calls Out

- **Question:** Can Retrieval-Augmented Generation (RAG) integration substantially improve semantic coherence scores (currently 7.8/10) and reduce anachronistic hallucinations in Old English text generation?
- **Basis in paper:** [explicit] Authors identify RAG as "a promising avenue" to mitigate hallucinations, repetitive text, and stylistic drift, and to improve semantic coherence.
- **Why unresolved:** The paper proposes RAG but does not implement or evaluate it; technical challenges include determining optimal chunk sizes and managing computational overhead.
- **What evidence would resolve it:** A comparative study measuring semantic coherence, factual accuracy, and hallucination rates between the current dual-agent pipeline and a RAG-enhanced version, evaluated by the same expert linguistic criteria.

## Limitations

- The framework relies heavily on Modern English as a high-resource anchor, limiting generalizability to truly isolated low-resource languages without such linguistic relatives
- Expert evaluation shows strong performance on lexical choice and inflection but semantic coherence scores lag behind, indicating potential semantic drift in complex contexts
- The model systematically avoids complex historical syntactic features like V2 word order, suggesting a trade-off between fluency and historical accuracy

## Confidence

- **High Confidence:** The dual-agent pipeline architecture and its effectiveness for generating grammatically accurate Old English texts
- **Medium Confidence:** The specific BLEU score improvements and their replicability across different low-resource language pairs
- **Medium Confidence:** The backtranslation augmentation strategy's contribution to model performance, given the alignment between reported results and established NMT literature

## Next Checks

1. Test the framework on a non-Indo-European low-resource language pair to assess cross-linguistic generalizability beyond English-related languages
2. Evaluate semantic coherence scores on longer generated texts to determine if the current evaluation methodology captures longer-range consistency issues
3. Implement controlled experiments varying the ratio of synthetic to human-annotated training data to quantify the optimal backtranslation contribution