---
ver: rpa2
title: 'Beyond Gaussian Initializations: Signal Preserving Weight Initialization for
  Odd-Sigmoid Activations'
arxiv_id: '2509.23085'
source_url: https://arxiv.org/abs/2509.23085
tags:
- initialization
- activation
- proposed
- networks
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses signal and gradient collapse in deep, narrow
  networks with sigmoidal activations. The authors define an "odd-sigmoid" function
  class and propose an activation-aware initialization based on controlling the effective
  gain distribution at each layer.
---

# Beyond Gaussian Initializations: Signal Preserving Weight Initialization for Odd-Sigmoid Activations

## Quick Facts
- arXiv ID: 2509.23085
- Source URL: https://arxiv.org/abs/2509.23085
- Authors: Hyunwoo Lee; Hayoung Choi; Hyunju Kim
- Reference count: 40
- Primary result: Preserves forward activation variance and backpropagated gradient norms in deep, narrow networks with sigmoidal activations through bifurcation-based initialization.

## Executive Summary
This paper addresses signal and gradient collapse in deep, narrow networks with sigmoidal activations. The authors define an "odd-sigmoid" function class and propose an activation-aware initialization based on controlling the effective gain distribution at each layer. This approach preserves forward activation variance and backpropagated gradient norms over a wide range of noise scales and network widths, unlike standard Gaussian initializations which require precise variance tuning. Empirical results show the method is more data-efficient and robust to depth, width, and activation scale, with stable training on standard image benchmarks and lower losses in physics-informed neural networks. The key advantage is maintaining trainable signals without batch normalization or fine-tuned learning rates.

## Method Summary
The proposed initialization constructs weight matrices as diagonal-plus-noise: $W^\ell = D^\ell + Z^\ell$ where diagonal entries equal the critical gain $\omega = 1/f'(0)$ and noise entries are Gaussian with variance calibrated to target a specific negative rate. The noise scale $\sigma_z$ is determined through a closed-form calibration that balances information retention with expressivity, using a surrogate scalar model to set the probability of sign flips at depth L. The method preserves signals by positioning the network at the bifurcation threshold between collapse and saturation, while gradient amplification remains stable due to a baseline $\omega^2$ term that buffers against variance misspecification.

## Key Results
- Preserves forward activation variance and backward gradient norms across 1000+ layers without batch normalization
- More data-efficient and robust to depth, width, and activation scale compared to standard Gaussian initializations
- Stable training on MNIST, Fashion-MNIST, CIFAR-10/100 with lower losses in physics-informed neural networks
- Maintains trainable signals through bifurcation-based initialization rather than fine-tuned learning rates

## Why This Works (Mechanism)

### Mechanism 1: Pitchfork Bifurcation-Based Signal Preservation
Structuring diagonal weights at critical gain $\omega = 1/f'(0)$ creates a bifurcation point determining whether signals converge to zero or saturate. For odd-sigmoid functions $f \in F$, the scalar iteration $x_{n+1} = f(ax_n)$ exhibits pitchfork bifurcation: when gain $a \leq \omega$, trajectories collapse to 0; when $a > \omega$, trajectories converge to nonzero fixed points $\pm\xi_a$. By setting diagonal entries to $\omega$, initialization keeps signals at the boundary between collapse and saturation.

### Mechanism 2: Noise Calibration via Negative Rate Control
Controlling probability of sign flips at depth L provides principled calibration for noise scale $\sigma_z$. The scalar surrogate model $a \sim N(\omega, \sigma_z^2)$ yields closed-form relationship between $\sigma_z$ and negative rate $\pi_L(\sigma)$ at depth L. Setting $p_{\text{real}} = 0.4$ (targeting ~60% sign preservation, ~40% flips) balances information retention with expressivity. Calibration formula is: $\sigma^*(p, L, \omega) = -\omega / \Phi^{-1}((1-(1-2p)^{1/L})/2)$.

### Mechanism 3: Gradient Amplification Stability via $\omega^2$ Term
Diagonal structure provides baseline $\omega^2$ term in gradient amplification that buffers against variance misspecification. Under mean-field analysis, gradient amplification factor $\chi^{\ell+1} \approx (\omega^2 + \sigma_z^2)E[f'(h^{\ell+1})^2]$. As $\sigma_z$ increases, $E[f'^2]$ decreases (more saturation), so product self-stabilizes near 1. In contrast, Gaussian i.i.d. has $\chi^* \approx \sigma_w^2 E[f'^2]$ with no $\omega^2$ buffer, making it fragile to variance choice.

## Foundational Learning

- **Fixed-point analysis of iterative maps**
  - Why needed here: The paper analyzes $x_{n+1} = f(ax_n)$ as dynamical system; understanding convergence to fixed points requires comfort with iterative function composition
  - Quick check question: For $f(x) = \tanh(x)$, what happens to $x_{n+1} = f(0.9·x_n)$ vs. $x_{n+1} = f(1.5·x_n)$?

- **Mean-field theory for neural networks**
  - Why needed here: Gradient amplification analysis relies on assuming preactivations are Gaussian with layerwise variance $q^\ell$, enabling tractable $\chi$ calculations
  - Quick check question: Under mean-field assumptions, if $\chi > 1$ at all layers, what happens to gradients as depth increases?

- **Signal propagation in deep networks (edge of chaos)**
  - Why needed here: Paper positions itself as improving upon EOC initialization by providing variance robustness while maintaining $\chi \approx 1$
  - Quick check question: Why does "edge of chaos" ($\chi \approx 1$) matter for trainability in very deep networks?

## Architecture Onboarding

- **Component map:**
  Input $x^0$ → [Layer 1: $W^1=D^1+Z^1$, $f(\cdot)$] → $x^1$ → ... → [Layer L: $W^L=D^L+Z^L$, $f(\cdot)$] → $x^L$ → Output

  Where:
  - $D^\ell$: Diagonal matrix with entries = $\omega$ (critical gain)
  - $Z^\ell$: Noise matrix with $(Z^\ell)_{ij} \sim N(0, \sigma_z^2/N_{\ell-1})$
  - $f$: Any odd-sigmoid function (tanh, erf, arctan, softsign, etc.)
  - $\omega = 1/f'(0)$ (activation-dependent)
  - $\sigma_z = \sigma^*(p(L), L, \omega)$ from closed-form calibration

- **Critical path:**
  1. Identify activation function $f$ and compute $\omega = 1/f'(0)$
  2. Determine network depth $L$ and choose target negative rate $p(L)$ using $p_{\text{real}}=0.4$ for shallow, exponential decay for deep
  3. Compute $\sigma_z = -\omega / \Phi^{-1}((1-(1-2p(L))^{1/L})/2)$
  4. Initialize $W^\ell = \omega·I + \sigma_z/\sqrt{N} · G^\ell$ where $G^\ell_{ij} \sim N(0,1)$
  5. Set learning rate $\eta \in [10^{-5}\omega, 10^{-3}\omega]$

- **Design tradeoffs:**
  - **Higher $p(L)$**: More sign flips → more expressivity but potential information loss
  - **Lower $p(L)$**: Better sign preservation → may reduce feature diversity
  - **Diagonal magnitude > $\omega$**: Faster convergence to nonzero fixed points but risk of premature saturation
  - **Noise scale $\sigma_z$**: Must balance gradient stability (requires some noise) with signal preservation

- **Failure signatures:**
  - Training collapses immediately: Likely $\sigma_z$ too small or learning rate outside $\omega$-scaled band
  - Activations saturate at $\pm1$ by mid-network: $\sigma_z$ too large
  - Gradients vanish in deep layers: Check that diagonal entries are exactly $\omega$, not rounded
  - Width-dependent performance: If narrow networks fail, verify $\sigma_z$ scales as $1/\sqrt{N}$ (not $1/N$)

- **First 3 experiments:**
  1. **Verify signal propagation**: Initialize 100-layer tanh network (width 64) with proposed vs. EOC; plot activation histograms at layers [1, 10, 50, 100]. Expect: proposed maintains spread, EOC collapses toward zero.
  2. **Gradient norm preservation**: Same network; compute $||\partial L/\partial h^\ell||_2$ at initialization for L=10,100,1000. Expect: proposed preserves norm within factor ~2, EOC decays exponentially.
  3. **Negative rate calibration check**: For depth L=50, test $p_{\text{target}} \in \{0.01, 0.1, 0.3, 0.4, 0.49\}$; plot validation accuracy. Expect: peak near $p \approx 0.01-0.05$ for deep networks (after exponential decay correction), not at $p=0.4$.

## Open Questions the Paper Calls Out

- **Can the diagonal-plus-noise initialization scheme be effectively adapted for modern architectures like CNNs or Transformers?**
  - The paper explicitly focuses on "feedforward neural networks" and mentions the "elementwise formulation" relies on weight matrices $W^\ell$
  - The proposed method relies on a specific matrix structure (diagonal plus noise) and scalar dynamics, which may not directly translate to convolutional filters or attention heads
  - Successful application of a modified initialization rule to Convolutional Networks or Transformers that matches or exceeds standard He/Xavier performance without warm-up would resolve this

- **Is there a theoretical justification for the empirical target negative rate $p_{\text{real}} = 0.4$?**
  - Appendix B.7 states: "We do not claim that $p_{\text{real}} = 0.4$ is an information-theoretically optimal value. Rather, it is an empirically grounded target"
  - The value was determined via empirical validation across benchmarks, but no formal proof connects this specific probability to optimal information preservation or trainability
  - A theoretical derivation linking the negative rate to the Fisher information or a proof showing the optimal rate varies with depth-to-width ratios would resolve this

- **Can the gain calibration method be generalized to non-odd or non-sigmoid activations (e.g., ReLU, GELU)?**
  - The paper restricts the method to "odd-sigmoid activations" and notes in Section 4.1 that the analysis relies on unique fixed points and boundedness
  - The theoretical underpinnings (pitchfork bifurcation, fixed point convergence) rely on the symmetry and saturation properties of odd-sigmoid functions, which unbounded or non-odd functions like ReLU do not possess
  - A derivation of a similar noise scale calibration for ReLU networks that maintains gradient norms without relying on the boundedness assumption would resolve this

## Limitations
- Elementwise scalar approximation may break down in extremely narrow networks (N < 32) where weight sharing across channels becomes significant
- Mean-field gradient analysis assumes preactivations remain approximately Gaussian, which may not hold with high noise scales (σ_z ≫ ω)
- Calibration formula's exponential decay correction for deep networks is empirically derived but lacks theoretical justification for why p(L) ≈ 2.05e^{-0.133L} specifically

## Confidence

- **High Confidence**: Signal preservation mechanism (pitchfork bifurcation analysis), gradient amplification stability, basic experimental demonstrations on standard datasets
- **Medium Confidence**: Noise calibration formula validity across diverse architectures, PINN applications, scaling behavior to very deep networks (L > 1000)
- **Low Confidence**: Behavior with non-odd-sigmoid activations, extremely narrow widths (N < 16), and in architectures with residual connections

## Next Checks

1. Test calibration formula robustness: For a fixed architecture, sweep $p_{\text{target}} \in [0.01, 0.4]$ and verify the claimed exponential decay correction for $L \in [10, 100, 1000]$
2. Verify gradient amplification stability: Compute $\chi^\ell$ across layers for proposed vs. EOC initialization; confirm $\chi \approx 1$ over wide $(L, \sigma_z)$ region
3. Test failure modes: Intentionally initialize with wrong $\omega$ values (e.g., $\omega \pm 0.1$) and observe activation collapse patterns to confirm bifurcation sensitivity