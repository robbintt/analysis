---
ver: rpa2
title: 'YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large
  Language Models'
arxiv_id: '2509.08997'
source_url: https://arxiv.org/abs/2509.08997
tags:
- risk
- safety
- youth
- data
- youthsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YouthSafe, a youth-centric safety benchmark
  and safeguard model for large language models. The authors developed YAIR, a dataset
  of 12,449 annotated conversation snippets spanning 78 fine-grained risk types relevant
  to youth-GenAI interactions.
---

# YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models

## Quick Facts
- arXiv ID: 2509.08997
- Source URL: https://arxiv.org/abs/2509.08997
- Reference count: 40
- Primary result: Youth-specific safety detection model achieves AUPRC 0.94, F1 0.88, precision 0.88, recall 0.89

## Executive Summary
YouthSafe addresses the critical gap in AI safety for teenagers and young adults by introducing a youth-centric benchmark and safeguard model. The system features YAIR, a dataset of 12,449 annotated conversation snippets across 78 fine-grained risk types, and YouthSafe, a real-time risk detection model fine-tuned on this specialized data. The work demonstrates that existing moderation models substantially underperform in detecting youth-specific risks, while YouthSafe significantly outperforms prior systems across all evaluation metrics.

## Method Summary
The method involves creating a youth-specific risk taxonomy with 91 low-level risk types, curating a balanced dataset through hybrid real-synthetic data collection, and fine-tuning the Aegis-Guard-Defensive model using LoRA instruction-tuning. The training uses LLaMA-Factory with 6 epochs, batch size 2, learning rate 5e-5, and max token length 4096. The model outputs both binary risk detection and multi-label classification across 11 medium-level risk categories.

## Key Results
- YouthSafe achieves AUPRC 0.94, F1 score 0.88, precision 0.88, and recall 0.89 on youth-specific risk detection
- The model outperforms baseline moderation systems by substantial margins across all metrics
- Synthetic data augmentation proves essential, with ablation showing recall dropping from 0.89 to 0.67 without synthetic data

## Why This Works (Mechanism)

### Mechanism 1: Granular Risk Taxonomy
Youth-specific safety detection requires granular, developmentally-grounded risk taxonomies rather than general "toxicity" labels. The system replaces broad categories with a three-tier taxonomy containing 91 low-level risk types, allowing the model to distinguish contextually subtle harms that standard models misclassify as safe.

### Mechanism 2: Hybrid Data Curation
Hybrid data curation combining limited real logs with targeted synthetic generation resolves label imbalance and improves coverage of rare, high-risk scenarios. Real-world data was imbalanced (~25% unsafe), so synthetic dialogues were generated using LLMs and validated via human/LLM consensus to learn "rare" risks without massive real-world exposure.

### Mechanism 3: Instruction-Tuned Fine-Tuning
Fine-tuning an existing safety-aligned model (Aegis) using instruction-tuning is more effective for domain adaptation than training from scratch. The model inherits robust general safety capabilities while specializing its output layer to the new YAIR taxonomy, handling "On-policy" vs. "Off-policy" distinctions better than general-purpose APIs.

## Foundational Learning

- **Concept: Area Under the Precision-Recall Curve (AUPRC)**
  - Why needed here: The paper emphasizes AUPRC over accuracy because youth safety data is inherently imbalanced. AUPRC focuses on the model's ability to find risky snippets without being skewed by the majority "safe" class.
  - Quick check question: If a model predicts "Safe" for 100% of inputs in a dataset that is 90% safe, what is the accuracy, and why is AUPRC better here?

- **Concept: False Negative Rate vs. False Positive Rate**
  - Why needed here: The paper explicitly argues that in youth safety, False Negatives (missing a harm) are far more critical than False Positives (over-flagging). Understanding this trade-off is necessary to interpret the "Recall" metrics prioritized in the results.
  - Quick check question: In a safety system, lowering the classification threshold increases Recall but lowers Precision. What does this imply for the user experience of a teenager using the AI?

- **Concept: Instruction Tuning**
  - Why needed here: YouthSafe follows a prompt structure to output both a safe/unsafe label and a specific risk category. Understanding how prompts structure model behavior is key to implementing the system.
  - Quick check question: How does the training input format (Table 2) differ from standard supervised fine-tuning for text classification?

## Architecture Onboarding

- **Component map:** Taxonomy Layer (3-Tier hierarchy) -> Data Layer (YAIR-LOG + YAIR-SYN) -> Validation Layer (Human/LLM majority voting) -> Model Layer (LoRA fine-tuning on Aegis) -> Inference (Prompt-response structure)
- **Critical path:** Define risk taxonomy -> Generate synthetic scenarios/dialogues to balance dataset -> Validate synthetic data -> Fine-tune Aegis using LoRA on YAIR-TRAINING -> Evaluate on YAIR-HUMANVAL
- **Design tradeoffs:** Specificity vs. Coverage (targets 91 risks but synthesizes only 78 due to safety filters) vs. Context vs. Speed (uses snippets rather than full conversation history)
- **Failure signatures:** Role-Play False Positives (fictional violence flagged as real aggression), Subtle Manipulation Misses (high False Negative rates on "Undue Influence"), Refusal Loops (synthetic generation failed for 13 risk types)
- **First 3 experiments:** Baseline Taxonomy Mismatch (OpenAI Moderation API on YAIR-HUMANVAL to confirm high Precision but low Recall), Ablation on Data Source (retrain using only YAIR-LOG to observe recall drop), False Negative Heatmap (generate heatmap to visualize which risk categories are most frequently missed)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can youth safety benchmarks incorporate developmental severity to differentiate between critical harms (e.g., grooming) and minor infractions?
- Basis in paper: The Limitations section notes the current taxonomy treats all 91 risk types equally, despite developmental consequences varying significantly.
- Why unresolved: The YAIR dataset lacks severity annotations, preventing models from prioritizing high-stakes interventions over less harmful interactions.
- What evidence would resolve it: A re-annotation of the dataset with severity weights and a model evaluation showing prioritized recall for high-severity risks.

### Open Question 2
- Question: To what extent do youth-specific risks emerge across multiple conversation turns rather than within single-turn snippets?
- Basis in paper: The authors state the current snippet-based evaluation "does not consider previous dialogue context," which may overlook risks unfolding over time.
- Why unresolved: The YouthSafe model currently evaluates isolated exchanges, failing to capture longitudinal manipulation or dependency patterns.
- What evidence would resolve it: A comparative evaluation of model performance on full dialogue histories versus isolated snippets.

### Open Question 3
- Question: How can high-severity risk scenarios (e.g., simulated child abuse) be ethically synthesized to ensure complete benchmark coverage?
- Basis in paper: The authors acknowledge they "opted not to pursue manual synthesis" for 13 risk types because LLMs refused to generate them due to safety filters.
- Why unresolved: Current safety-aligned generative models block the creation of data necessary to train detectors for the most severe youth risks.
- What evidence would resolve it: A validated data generation pipeline (adversarial or manual) that successfully produces ethical training data for these red-teamed categories.

## Limitations
- The taxonomy may miss emerging youth-specific harms as GenAI capabilities evolve
- Synthetic data generation failed for 13 risk types (including GAI Normalizing or Escalating Sexual Undertone), creating blind spots
- Validation methodology relying on LLM consensus introduces potential bias through shared blind spots

## Confidence

**High Confidence:** The taxonomic framework's empirical grounding and the demonstrated performance gap between YouthSafe and general moderation models (AUPRC 0.94 vs baseline ~0.5-0.6)

**Medium Confidence:** The synthetic data augmentation strategy's effectiveness, as it relies on the assumption that LLM-generated scenarios capture real youth interaction patterns

**Low Confidence:** The long-term stability of the taxonomy and whether the 91 risk types will remain comprehensive as GenAI capabilities and youth interaction patterns evolve

## Next Checks
1. **Taxonomy Evolution Test:** Re-run the taxonomy refinement process with new data collected 12 months after initial annotation to measure drift in risk categories and identify newly emerged harm patterns
2. **Real-World Deployment Audit:** Deploy YouthSafe in a controlled environment with actual teenage users for 30 days, measuring false positive/negative rates in authentic interaction contexts rather than curated snippets
3. **Cross-Cultural Validation:** Test YouthSafe's performance on youth-AI interaction data from different cultural contexts (e.g., non-Western countries) to assess whether the taxonomy captures universal vs. culturally-specific risk patterns