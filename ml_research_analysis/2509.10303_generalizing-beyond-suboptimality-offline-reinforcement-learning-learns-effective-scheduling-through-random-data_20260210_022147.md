---
ver: rpa2
title: 'Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective
  Scheduling through Random Data'
arxiv_id: '2509.10303'
source_url: https://arxiv.org/abs/2509.10303
tags:
- cdqac
- instances
- training
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conservative Discrete Quantile Actor-Critic
  (CDQAC), an offline reinforcement learning algorithm for job-shop scheduling problems
  that learns effective policies directly from historical data without requiring online
  interaction. CDQAC combines a quantile-based critic with delayed policy updates
  and conservative Q-learning to accurately estimate action values while avoiding
  overestimation of out-of-distribution actions.
---

# Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data

## Quick Facts
- **arXiv ID:** 2509.10303
- **Source URL:** https://arxiv.org/abs/2509.10303
- **Reference count:** 40
- **Primary result:** Offline RL algorithm achieves state-of-the-art scheduling performance, performing best when trained on random data rather than high-quality expert solutions

## Executive Summary
This paper introduces Conservative Discrete Quantile Actor-Critic (CDQAC), an offline reinforcement learning algorithm that learns effective job-shop scheduling policies directly from historical data without requiring online interaction. CDQAC combines a quantile-based critic with delayed policy updates and conservative Q-learning to accurately estimate action values while avoiding overestimation of out-of-distribution actions. Remarkably, the method performs best when trained on random data rather than higher-quality data from genetic algorithms or priority rules, achieving average gaps of 5.86% compared to 6.43% and 12.34% respectively. The approach is highly sample-efficient, requiring only 10-20 training instances to match performance achieved with full datasets, and demonstrates strong generalization from small to larger problem instances.

## Method Summary
CDQAC uses a dual attention network to encode scheduling features (operation and machine attributes) and learns a distributional critic using quantile regression. The critic estimates 64 quantiles of the return distribution and incorporates CQL regularization to prevent overestimation of unseen actions. Policy updates are delayed by 4 steps to improve stability. The method trains entirely offline from fixed datasets generated by various scheduling heuristics, achieving state-of-the-art performance without simulator access.

## Key Results
- CDQAC achieves 5.86% average gap to optimal on JSP benchmark (10×5) when trained on random data, outperforming online RL baselines and competing offline methods
- Random data outperforms high-quality data (GA: 6.43% gap, PDR: 12.34% gap) due to superior state-action coverage despite lower solution quality
- Only 10-20 training instances needed for performance matching full dataset training, demonstrating exceptional sample efficiency
- Strong generalization from 10×5 to larger instances (10×10, 10×20) without retraining

## Why This Works (Mechanism)

### Mechanism 1: Quantile Critic for Distributional Value Estimation
The critic learns 64 quantiles of the return distribution rather than expected values, capturing variance and multimodality that scalar Q-values miss. This provides more accurate state-action value representations in scheduling problems where action outcomes have meaningful variance.

### Mechanism 2: Conservative Q-learning Prevents OOD Overestimation
CQL regularization adds a penalty term that pushes Q-values of out-of-distribution actions below in-distribution actions. This is critical because offline RL cannot correct overestimation through exploration, and prevents the critic from assigning inflated values to unseen machine-operation pairs.

### Mechanism 3: Random Data Outperforms High-Quality Data via Coverage
Random data provides broader state-action coverage (7.7× vs 3.0× for GA), exposing the critic to diverse transitions. Higher-quality datasets concentrate on similar solutions, lacking counterfactual examples of what not to do. This coverage allows CDQAC to "stitch together" good decisions from diverse trajectories.

## Foundational Learning

- **Concept: Offline RL vs Online RL**
  - **Why needed here:** Offline RL learns from fixed datasets without environment interaction, eliminating simulator requirements but introducing distribution shift.
  - **Quick check question:** Can you explain why a Q-function trained on dataset D might overestimate actions that never appear in D?

- **Concept: Distributional RL / Quantile Regression**
  - **Why needed here:** CDQAC uses quantile critics to model full return distributions; understanding why distributions help requires grasping that expectations hide risk/variance.
  - **Quick check question:** Given two actions with equal expected return but different variance, why might a quantile critic distinguish them while a DQN critic cannot?

- **Concept: Dueling Network Architecture**
  - **Why needed here:** The critic separates V(s) (state value) from A(s,a) (advantage), allowing V to update at every step while A updates per action.
  - **Quick check question:** Why is the mean-advantage subtraction (Eq. 6) necessary to make V and A identifiable?

## Architecture Onboarding

- **Component map:** Input Features (Table 6) → Dual Attention Network (DAN) → Operation Attention Blocks → Machine Attention Blocks → Embeddings (h_G, h_Oi,j, h_Mk, h_pair) → Dueling Quantile Network → V_θ(h_G) and A_θ(all embeddings) → Z_θ (return distribution, 64 quantiles) → CQL penalty → Critic loss → Q^Z_θ (scalar via mean) → Policy π_ψ (updated every η=4 steps)

- **Critical path:** Dataset quality (coverage > solution quality) → Critic accuracy (quantile + dueling) → Policy stability (delayed updates). Failure at any stage propagates.

- **Design tradeoffs:** Quantile N=64 (more quantiles = finer distribution but slower training), η=4 delay (higher stability vs slower policy improvement), α_CQL=0.05 (stronger conservatism reduces OOD overestimation but may underutilize good OOD actions), Random vs expert data (Random provides coverage but contains more noise; expert provides quality but lacks diversity)

- **Failure signatures:** High variance across seeds (insufficient data diversity, try PDR-GA instead of pure GA), policy matches/worsens behavior policy (α_CQL too high or dataset lacks counterfactuals), poor generalization to larger instances (training instances too small or action space explosion)

- **First 3 experiments:** 1) Train CDQAC on Random dataset (10×5, 100 instances × 100 solutions), evaluate on generated 10×5, confirm greedy gap ≈11%. 2) Reduce Random dataset to 10 instances, measure performance drop (expected: minimal drop <1% gap increase). 3) Disable quantile critic (use DQN), disable dueling, set η=1, run ablation per Appendix G.1 to quantify each component's contribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Distributional assumption validity may not hold in low-variance deterministic environments where quantile overhead provides no benefit
- CQL hyperparameter sensitivity could degrade performance with different dataset qualities or problem scales
- Coverage vs quality tradeoff generality may not extend to domains where suboptimal actions are catastrophically bad rather than merely suboptimal

## Confidence
- **High confidence:** Dueling network architecture contribution, delayed policy updates improving stability, CDQAC outperforming online RL baselines
- **Medium confidence:** Quantile critic superiority over DQN (supported by ablation but limited cross-domain validation), CQL preventing OOD overestimation (theoretical justification with empirical support)
- **Medium confidence:** Random data outperforming expert data (robust result but contradicts prior literature, suggesting domain-specific mechanisms)

## Next Checks
1. **Variance analysis:** Measure return distribution variance for different action types in scheduling to validate quantile critic's core assumption about informative variance structure.
2. **Coverage sensitivity:** Systematically vary dataset diversity (pure GA vs hybrid PDR-GA vs random) while holding dataset size constant to isolate coverage effects from noise.
3. **Scalability stress test:** Evaluate CDQAC on FJSP instances with 100+ operations to quantify action-space explosion effects and determine practical scaling limits.