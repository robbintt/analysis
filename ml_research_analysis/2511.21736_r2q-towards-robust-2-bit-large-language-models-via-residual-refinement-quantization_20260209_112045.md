---
ver: rpa2
title: 'R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization'
arxiv_id: '2511.21736'
source_url: https://arxiv.org/abs/2511.21736
tags:
- quantization
- bitdistiller
- performance
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces R2Q, a novel 2-bit quantization framework
  for large language models that decomposes the quantization process into two sequential
  1-bit sub-quantizations, forming an adaptive quantization lattice. The method addresses
  the challenge of severe accuracy degradation in existing 2-bit quantization methods
  by refining quantization through a residual learning mechanism.
---

# R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization

## Quick Facts
- arXiv ID: 2511.21736
- Source URL: https://arxiv.org/abs/2511.21736
- Reference count: 40
- One-line primary result: R2Q improves 2-bit LLM quantization accuracy significantly by decomposing quantization into two sequential 1-bit steps with residual refinement

## Executive Summary
This paper introduces R2Q, a novel 2-bit quantization framework for large language models that addresses the severe accuracy degradation problem in existing 2-bit methods. R2Q decomposes the 2-bit quantization process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice that better fits non-uniform weight distributions. The method uses a residual learning mechanism where the second quantization step corrects the error from the first approximation, leading to improved performance, training stability, and faster convergence. Experimental results demonstrate that R2Q consistently outperforms existing 2-bit quantization methods across multiple benchmarks and model sizes.

## Method Summary
R2Q implements 2-bit quantization by decomposing it into two sequential 1-bit quantization steps. For each weight group, the first step computes a binary approximation using sign(W) with scaling factor α₁ = ||W||₁/G. The residual r = W - α₁q₁ is then quantized in the second step to (q₂, α₂). The final reconstruction is ŵ = α₁q₁ + α₂q₂. This creates an adaptive codebook {−α₁−α₂, −α₁+α₂, α₁−α₂, α₁+α₂} that shifts based on the actual weight distribution rather than using fixed uniformly-spaced values. The method integrates seamlessly into quantization-aware training frameworks using straight-through estimator for gradient backpropagation.

## Key Results
- On Llama-7B, R2Q improves ARC-e accuracy from 26.81 to 56.82 and reduces WikiText-2 perplexity from 1776.98 to 17.13
- R2Q consistently outperforms existing 2-bit quantization methods across multiple benchmarks (ARC-c/e, BoolQ, HellaSwag, PIQA, Winogrande, MMLU, WikiText-2)
- The method shows superior stability across different group sizes and faster convergence compared to baseline approaches
- R2Q can be integrated as a plug-and-play module into existing quantization-aware training frameworks without architectural modifications

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Quantization Lattice via 1-bit Decomposition
R2Q decomposes 2-bit quantization into two sequential 1-bit subproblems, creating an adaptive lattice {−α₁−α₂, −α₁+α₂, α₁−α₂, α₁+α₂} where scaling factors (α₁, α₂) are learned per weight group. This allows the four quantization points to shift non-uniformly based on actual weight distribution, unlike static RTN approaches with fixed uniformly-spaced values.

### Mechanism 2: Residual Refinement Error Correction
After computing the first coarse approximation (q₁, α₁), the residual r = W - α₁q₁ is quantized to (q₂, α₂). This hierarchical approach ensures the second bit addresses the approximation error rather than duplicating information from the first bit, reducing mean squared error and stabilizing training.

### Mechanism 3: Gradient Stabilization via Smoother Quantization Landscape
R2Q's adaptive lattice produces lower quantization error before training begins, leading to more stable gradients and faster convergence during QAT. Lower initial quantization error means the loss landscape has fewer sharp discontinuities, reducing oscillation and gradient explosion risks during straight-through estimator backpropagation.

## Foundational Learning

- **Quantization Fundamentals (Group-wise, RTN)**: Understanding baseline failure modes is essential as R2Q builds directly on group-wise quantization and contrasts against RTN baselines. Quick check: Why does uniform RTN struggle specifically at 2-bit precision but works reasonably at 4-bit?
- **Residual Learning**: The core innovation applies residual learning to quantization error, not just network layers. Quick check: In R2Q, what does the residual r represent and how is it used differently from a standard ResNet skip connection?
- **Straight-Through Estimator (STE)**: The sign function is non-differentiable; understanding STE is critical for debugging gradient flow issues. Quick check: What approximation does STE make for ∂ŵ/∂W, and when might this approximation fail catastrophically?

## Architecture Onboarding

- **Component map**: Weight grouping module -> 1-bit quantizer -> Residual computer -> 1-bit quantizer (second) -> Reconstruction layer -> STE backward pass -> QAT integration wrapper
- **Critical path**: Initialize with pre-trained BF16 weights → Apply R2Q to each linear layer → Forward pass uses reconstructed ŵ → Backward pass: STE identity for quantization ops, update full-precision W → Re-quantize after each weight update
- **Design tradeoffs**: Group size (G): smaller G (128) → finer granularity, better accuracy but higher memory overhead; larger G (-1 per-channel) → fewer params, harder optimization but more compression. Parameter matching: R2Q uses 2× scaling params, so use G=256 to match baseline G=128 param budgets.
- **Failure signatures**: Gradient explosion in RTN baselines (PPL >40,000 on Qwen models); no improvement over 1-bit (check q₂ computed from residual r, not original weights); high WikiText-2 PPL (>100) suggests group-size mismatch or incorrect scaling factor computation.
- **First 3 experiments**: 1) Sanity check on single layer of small model (OPT-125M), measure MSE vs. RTN before training; 2) Coarse-grained baseline on Llama-7B, replicate Table 1 with group size=-1, compare LLM-QAT vs. BitDistiller vs. R2Q focusing on WikiText-2 PPL; 3) Integration test with BitDistiller on Qwen3-4B (coarse setting), verify training completes without gradient explosion and PPL <500.

## Open Questions the Paper Calls Out

### Open Question 1
Can a formal theoretical justification be established for the heuristic assumption that combining two 1-bit kernels yields an optimal, distribution-independent 2-bit representation? The authors state this remains an open problem despite empirical support, noting that while the 1-bit subproblem has an optimal solution, the mathematical proof that sequentially combining two such solutions results in a globally optimal 2-bit configuration is currently missing.

### Open Question 2
How can the R2Q framework be extended to jointly quantize both weights and activations to maximize compression ratios and inference speedup? The paper notes R2Q currently focuses on static weight distributions and doesn't extend to activation quantization, which limits overall compression ratio. Extending the residual refinement mechanism to dynamic, input-dependent activations presents a stability and optimization challenge.

### Open Question 3
What are the practical efficiency gains when implementing specialized R2Q operators using 1-bit arithmetic logic compared to standard integer kernels? The authors identify developing high-performance R2Q operators with 1-bit arithmetic logic as future work to ensure inference efficiency, noting that while theoretical multiplication complexity is reduced, actual speedup depends on optimized hardware/software kernels not yet implemented.

## Limitations

- The stability of L1-norm scaling approach under extreme outlier distributions or very small group sizes remains unclear
- The paper doesn't fully explore the tradeoff space between memory overhead and accuracy across different group sizes
- The straight-through estimator approximation may fail when quantization error becomes too large, potentially explaining gradient explosion in baseline approaches

## Confidence

- **High Confidence**: The core mechanism of decomposing 2-bit quantization into two sequential 1-bit subproblems with residual refinement is well-defined and experimentally validated, with strong empirical support from significant improvements in WikiText-2 perplexity and ARC-e accuracy
- **Medium Confidence**: The claim about improved training stability and faster convergence is supported by gradient norm analysis and MSE comparisons, but needs more systematic validation across architectures
- **Low Confidence**: The plug-and-play integration claim is promising but lacks detailed ablation studies showing R2Q's performance when integrated into different existing quantization frameworks

## Next Checks

1. **Distribution sensitivity analysis**: Test R2Q on weight distributions with varying degrees of skewness and kurtosis (using synthetic weight matrices with controlled statistics) to identify precise conditions under which the adaptive lattice provides most benefit over static RTN approaches

2. **Group size optimization sweep**: Systematically vary group sizes (G=64, 128, 256, 512, -1) across multiple model sizes (Llama-7B, Qwen3-8B) to identify optimal tradeoff between parameter overhead and accuracy, and verify claimed stability holds across full range

3. **STE approximation validation**: Implement and compare alternative gradient estimators (biased STE variants, smoothed sign functions) during QAT process to quantify impact of STE approximation on R2Q's convergence and final accuracy, particularly in coarse-grained settings where gradient explosion was observed