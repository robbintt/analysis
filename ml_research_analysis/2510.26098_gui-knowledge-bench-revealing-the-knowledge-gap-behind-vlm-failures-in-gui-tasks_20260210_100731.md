---
ver: rpa2
title: 'GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI
  Tasks'
arxiv_id: '2510.26098'
source_url: https://arxiv.org/abs/2510.26098
tags:
- knowledge
- task
- type
- question
- interface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GUI Knowledge Bench, a benchmark designed
  to systematically evaluate the GUI knowledge encoded in vision-language models (VLMs).
  The authors categorize GUI knowledge into three dimensions: interface knowledge
  (recognizing widget functions, layout semantics, and system states), interaction
  knowledge (understanding GUI interaction conventions and effects), and procedure
  knowledge (knowing task objectives and workflow sequences).'
---

# GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks

## Quick Facts
- **arXiv ID:** 2510.26098
- **Source URL:** https://arxiv.org/abs/2510.26098
- **Reference count:** 40
- **Primary result:** Current VLMs understand widget functions but struggle with system states, interaction conventions, and task completion assessment.

## Executive Summary
This paper introduces GUI Knowledge Bench, a benchmark designed to systematically evaluate the GUI knowledge encoded in vision-language models (VLMs). The authors categorize GUI knowledge into three dimensions: interface knowledge (recognizing widget functions, layout semantics, and system states), interaction knowledge (understanding GUI interaction conventions and effects), and procedure knowledge (knowing task objectives and workflow sequences). The benchmark consists of 3,483 multiple-choice and yes-no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Evaluation results show that current VLMs generally understand widget functions but struggle with perceiving system states, predicting GUI interactions, and verifying task completion progress.

## Method Summary
The benchmark uses 40,000+ screenshots from multiple GUI datasets, transformed into multiple-choice and yes/no questions through human annotation and GPT-5 expansion. Visual markers (bounding boxes, red dots) eliminate grounding requirements, while constrained answer formats isolate knowledge assessment from generation variability. Questions span three knowledge dimensions across six platforms and 292 applications. Evaluation uses zero-shot prompting with specific templates, requiring JSON-formatted responses containing "thought" and "answer" fields. The methodology enables focused assessment of whether models possess GUI-specific knowledge rather than visual grounding, language generation, or abstract reasoning abilities.

## Key Results
- Current VLMs achieve high accuracy on widget function recognition but show significant weaknesses in system state perception, interaction prediction, and task completion verification
- Interface knowledge performance: 61.36% accuracy; Interaction knowledge: 45.42%; Procedure knowledge: 48.38%
- Knowledge absence guarantees task failure (P(S₂×|S₁×) = 100% across tested models)
- Procedure knowledge injection via operation plans improves task pass@1 from 24.81% to 30.79%

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Isolation from Execution Confounds
The benchmark isolates GUI knowledge assessment by using visual markers that eliminate grounding requirements and multiple-choice/yes-no formats that reduce generation variability. This creates a direct mapping between question performance and knowledge possession rather than execution capability. Performance remains nearly unchanged across all models regardless of answer ordering, indicating the benchmark is not sensitive to test-taking heuristics.

### Mechanism 2: Knowledge as Necessary but Insufficient Condition
Knowledge provides the correct action selection space, but execution requires additional capabilities (coordinate prediction, timing, context handling). Lacking knowledge forces the model into incorrect action spaces from which recovery is impossible. Experiments show that lacking knowledge guarantees execution failure (P(S₂×|S₁×) = 100%), validating that knowledge is the strict lower bound for control.

### Mechanism 3: Training Method Knowledge Blindness
Standard SFT and RL approaches improve execution patterns but do not systematically inject domain-specific GUI knowledge. Knowledge may degrade during task-focused fine-tuning (e.g., UITARS-1.5-7B shows interface knowledge decline). The evidence shows knowledge degradation after SFT and modest gains from procedure injection, suggesting current training objectives don't target knowledge gaps.

## Foundational Learning

- **Concept: State-Action Transition in GUI Environments**
  - Why needed here: The benchmark's interaction knowledge dimension formalizes GUI dynamics as S + a → S′, requiring understanding that actions are discrete operations with predictable state changes.
  - Quick check question: Given a toggle switch in "off" state, what action type and expected state transition should a VLM predict?

- **Concept: Visual Grounding vs. Semantic Knowledge Decoupling**
  - Why needed here: The benchmark's core contribution is separating "where to click" (grounding) from "what this widget does" (knowledge). Without this distinction, benchmark results conflate multiple failure modes.
  - Quick check question: If a VLM correctly identifies a button's function but clicks 50 pixels off-target, which capability failed?

- **Concept: Hierarchical GUI Knowledge Organization**
  - Why needed here: The three-dimension taxonomy (interface, interaction, procedure) reflects a knowledge hierarchy from perception to action to task-level planning. Understanding this structure is essential for interpreting model weaknesses and targeting improvements.
  - Quick check question: A model correctly identifies a "Save" button (interface knowledge) but tries to single-click when the application requires double-click (interaction knowledge). Which layer failed?

## Architecture Onboarding

- **Component map:** GUI-Odyssey, VideoGUI, OSWorld trajectories, ScreenSpot v2, YouTube tutorials → unified screenshot pool (40,000+ images) → human seed questions → GPT-5 expansion → Qwen-2.5-VL-7B filtering → Manual verification → 3,483 multiple-choice and yes-no questions across six platforms and 292 applications

- **Critical path:**
  1. Select target VLM and platform scope (desktop: 7 actions; mobile: 4 actions)
  2. Run all three knowledge modules with consistent prompt templates (see section 6.4)
  3. Compare against baseline knowledge levels from Table 2
  4. If knowledge gaps identified, inject procedure knowledge via operation plans before attempting task execution

- **Design tradeoffs:**
  - Multiple-choice format → reduces generation variance but may allow test-taking heuristics (mitigated by option randomization validation)
  - Visual markers → eliminate grounding confound but unrealistically simplify real-world conditions
  - Single-step questions → isolate knowledge but miss multi-hop reasoning requirements present in actual tasks
  - 292 applications → broad coverage but limited depth per application (average ~12 questions/app)

- **Failure signatures:**
  - System state blindness: Models mistake non-blocking notifications for modal dialogs
  - Click-action overprediction: Confusion matrix shows models default to "click" when uncertain about double-click, right-click, or drag
  - Task completion hallucination: Models claim "Yes, task complete" when wrong element was modified
  - Knowledge degradation after SFT: UITARS-1.5-7B shows lower interface knowledge than base Qwen2.5VL-7B

- **First 3 experiments:**
  1. **Baseline diagnostic:** Run your target VLM on all three knowledge modules to identify which dimensions are weakest (use Table 2 as reference). If interface knowledge <60%, focus on widget function and state recognition before attempting interaction improvements.
  2. **Knowledge injection test:** For procedure knowledge gaps, follow Table 3 methodology—generate operation plans with GPT-4o or o3, append to task descriptions, measure pass@1 improvement. Expect 3-6 percentage point gains if procedure knowledge is the bottleneck.
  3. **Correlation validation:** Transform 10-20 benchmark questions into executable GUI tasks (following section 4.4.2 protocol), compute P(S₂✓|S₁✓) and P(S₂×|S₁×). If P(S₂×|S₁×) < 100%, knowledge questions may not capture task-critical knowledge.

## Open Questions the Paper Calls Out
None

## Limitations
- Visual markers unrealistically simplify real-world conditions by eliminating grounding requirements
- Single-step questions miss multi-hop reasoning requirements present in actual tasks
- The 39 transformed questions may not capture all task-critical knowledge dimensions
- Knowledge degradation after SFT may reflect training corpus limitations rather than fundamental capability constraints

## Confidence

- **High:** The benchmark construction methodology (visual markers, multiple-choice format) effectively isolates GUI knowledge from grounding/execution abilities.
- **Medium:** GUI knowledge is a necessary condition for task success, supported by P(S₂×|S₁×) = 100% results.
- **Low:** Current training methods cannot systematically inject GUI knowledge, though this may reflect corpus limitations rather than fundamental constraints.

## Next Checks

1. **Knowledge Generalization Test:** Select 20 questions from the benchmark and transform them into executable GUI tasks across different applications. Measure whether P(S₂✓|S₁✓) > 95% across all transformations.

2. **Alternative Training Impact:** Implement a training protocol that explicitly targets GUI knowledge gaps (e.g., inverse dynamics learning for interaction prediction, contrastive learning for system state discrimination). Measure whether this closes knowledge gaps more effectively than SFT alone.

3. **Knowledge Hierarchy Validation:** Conduct ablation studies removing each knowledge dimension and measure task failure rates. Verify that interface knowledge failures are most frequent, followed by interaction, then procedure.