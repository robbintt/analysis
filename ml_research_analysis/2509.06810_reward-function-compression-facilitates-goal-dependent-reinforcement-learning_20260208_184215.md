---
ver: rpa2
title: Reward function compression facilitates goal-dependent reinforcement learning
arxiv_id: '2509.06810'
source_url: https://arxiv.org/abs/2509.06810
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000048
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Humans can assign value to novel outcomes based on goals, but this
  flexibility is cognitively costly and makes learning less efficient. This study
  proposes that goal-dependent learning initially relies on working memory (WM) to
  maintain and evaluate goals, which limits learning efficiency.
---

# Reward function compression facilitates goal-dependent reinforcement learning

## Quick Facts
- **arXiv ID:** 2509.06810
- **Source URL:** https://arxiv.org/abs/2509.06810
- **Reference count:** 40
- **Primary result:** Goal-dependent learning initially relies on working memory but becomes efficient when goals are compressed into simple rules offloaded to long-term memory.

## Executive Summary
This study investigates how humans learn to assign value to outcomes based on abstract goals. The authors propose that goal-dependent learning initially consumes working memory (WM) resources, creating a bottleneck that impairs learning efficiency. With repeated exposure, learners create compressed reward functions—simplified rules defining the goal—that are transferred to long-term memory, freeing up WM resources and improving learning. The study demonstrates that learning performance is parametrically impaired by goal space size but improves when the goal space structure allows for compression. Faster reward processing correlates with better learning performance, supporting the idea that as goal valuation becomes more automatic, more resources are available for learning.

## Method Summary
The study uses a 6-armed bandit task where participants map 6 stimuli to 3 key presses, with feedback in the form of either numeric points or abstract goal images. Experiments vary the number of goal pairs (1-4) to test WM capacity limits and manipulate goal space structure to test compressibility (separable by one dimension vs. requiring conjunction search). The authors collect behavioral choice data and reward collection reaction times, then fit computational models that include Q-learning with forgetting, stickiness, habit systems, and learning rate modulation by reaction times. The winning model uses a negative weight on median log RT to scale the learning rate.

## Key Results
- Learning performance degrades parametrically as goal space size increases (from 1 to 4 goal pairs)
- Performance improves significantly in compressible goal structures compared to incompressible ones, despite identical goal frequencies
- Faster reward collection reaction times correlate with better learning performance
- Computational models support the interpretation that efficient goal-directed learning relies on compressing complex goal information into stable reward functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Goal-dependent learning initially relies on working memory (WM), creating a resource trade-off that impairs learning efficiency.
- **Mechanism:** The system uses WM to actively maintain current goal representations and match them to outcomes. This consumes limited executive resources that would otherwise support the reinforcement learning (RL) update process.
- **Core assumption:** Cognitive resources are finite; maintaining abstract goals in WM competes with the computational resources needed for value updating.
- **Evidence anchors:**
  - [abstract] "goal-dependent learning is initially supported by a capacity-limited working memory system."
  - [section 2.2] Experiment 1 showed "learning is parametrically impaired by the size of the goal space."
  - [corpus] Corpus evidence is weak or unrelated (focuses on LLM compression/automated driving rather than cognitive resource limits).
- **Break condition:** If the goal space exceeds WM capacity (e.g., >4 unique goal/nongoal pairs), performance degrades parametrically.

### Mechanism 2
- **Claim:** Learning efficiency improves only when the goal space allows for "compression" into a simplified rule, not merely through repetition.
- **Mechanism:** The cognitive system attempts to extract a low-dimensional rule (e.g., "all goal images are purple") from high-dimensional inputs. If successful, this compressed function replaces the need to store individual exemplars.
- **Core assumption:** The brain seeks minimum sufficient information to classify outcomes, shedding redundant features.
- **Evidence anchors:**
  - [abstract] "learning... improves when the goal space structure allows for compression."
  - [section 2.3] Experiment 2 showed significantly higher accuracy in "Compressible" blocks vs. "Incompressible" blocks, despite identical goal frequencies.
  - [corpus] [2511.21726] Mentions "Goal-Agnostic Memory Compression," but the target paper emphasizes *goal-dependent* structural compression.
- **Break condition:** If the goal classification logic is non-linear or requires attending to all features simultaneously (Type VI classification), compression fails, and the system remains stuck in an inefficient WM-dependent state.

### Mechanism 3
- **Claim:** Compressed reward functions are offloaded to long-term memory (LTM), automating outcome valuation and freeing WM.
- **Mechanism:** Once a rule is learned, valuation becomes automatic (faster reaction times), effectively bypassing the WM bottleneck. This allows the RL algorithm to utilize a higher effective learning rate ($\alpha$).
- **Core assumption:** Reaction times (RTs) for reward collection serve as a proxy for the cognitive cost of valuation.
- **Evidence anchors:**
  - [abstract] "faster reward processing [correlates] with better learning performance."
  - [section 2.5] Computational models linking learning rates to reward collection RTs provided a superior fit, suggesting "the bottleneck... occurs at the value-updating stage."
  - [corpus] N/A (Specific cognitive offloading mechanism not present in neighbor papers).
- **Break condition:** Conflicting labels for the same visual outcome (Exp 6) prevent the formation of a stable rule, blocking the transfer to LTM.

## Foundational Learning

- **Concept:** **Rescorla-Wagner / Delta Rule**
  - **Why needed here:** The paper models human learning using this standard RL algorithm. Understanding that value updates are proportional to prediction errors ($\delta$) is required to interpret why a reduced "cognitive load" effectively increases the realized learning rate.
  - **Quick check question:** If a reward is fully predicted, does the value update? (Answer: No, $\delta = 0$).

- **Concept:** **Working Memory Capacity Limits (e.g., Miller's Law / Cowan)**
  - **Why needed here:** Experiment 1 relies on the assumption that WM holds ~4 items. You must understand why adding a 4th goal pair causes parametric degradation to grasp the "resource competition" argument.
  - **Quick check question:** Why does performance drop when moving from 1 to 4 goal pairs if only one pair is relevant per trial? (Answer: The system attempts to maintain the *entire* goal space context).

- **Concept:** **Automaticity (Controlled vs. Automatic Processing)**
  - **Why needed here:** The core transition described is from "controlled" (effortful, WM-based) to "automatic" (effortless, LTM-based) valuation. This explains the behavioral shift from slow to fast reward collection RTs.
  - **Quick check question:** What behavioral signature indicates a process has become automatized in this study? (Answer: Faster reward collection reaction times).

## Architecture Onboarding

- **Component map:** Outcome -> Goal Valuator (WM/LTM) -> Reward Signal -> RL Agent (Q-table) -> Policy (Softmax) -> Action

- **Critical path:**
  1. **Encoding:** Stimulus and Goal Info enter WM.
  2. **Evaluation (Bottleneck):** If uncompressed, WM actively compares Outcome to stored Goal (High Cost). If compressed, LTM Rule applies automatically (Low Cost).
  3. **Update:** RL Agent receives scalar reward ($r$) and updates Values.

- **Design tradeoffs:**
  - **Flexibility vs. Efficiency:** High WM engagement allows learning from *any* novel goal (flexible) but is slow and capacity-limited. Compressed rules are fast (efficient) but brittle if goal definitions change (Exp 6).

- **Failure signatures:**
  - **High Load:** Accuracy degradation in >4 goal conditions (Exp 1).
  - **Incompressibility:** Performance stagnation in Type VI classification tasks (Exp 2).
  - **Interference:** Performance collapse when outcome images have inconsistent labels (Exp 6).
  - **Latency:** Slow reward collection RTs indicate failure to transition to LTM/offload from WM.

- **First 3 experiments:**
  1.  **Replicate Exp 1 (Load Test):** Vary the number of goal pairs (1-4) to verify the parametric WM capacity limit in your agent.
  2.  **Replicate Exp 2 (Structure Test):** Implement two goal sets—one separable by one dimension (Compressible), one requiring conjunction search (Incompressible)—to test if the compression mechanism engages.
  3.  **Latency Analysis:** Correlate reward collection RTs with learning curves to confirm that faster valuation (proxy for compression) predicts higher learning rates.

## Open Questions the Paper Calls Out
None

## Limitations
- The precise behavioral dataset (trial-by-trial choices and RTs) for all experiments is not publicly available in the paper, limiting direct model validation
- The specific outlier removal criteria for RT analysis (beyond the stated <300ms exclusion) are not fully specified
- The computational model shows good fit but cannot definitively prove the WM-to-LTM transition mechanism

## Confidence

- **High confidence**: The parametric load effect (WM capacity limits) and the compressibility effect (structural facilitation) - these are well-supported by direct experimental evidence
- **Medium confidence**: The RT-learning rate correlation as a proxy for compression efficiency - while statistically supported, alternative explanations exist
- **Medium confidence**: The computational model architecture - it fits the data well but alternative model structures might also explain the patterns

## Next Checks

1. **Direct RT-α correlation test**: Collect new data explicitly testing whether individual differences in reward collection RTs predict individual differences in learning rates (parameter α) across conditions
2. **WM load manipulation**: Conduct a targeted experiment varying concurrent WM load during learning to test whether additional WM burden disrupts the hypothesized WM-to-LTM transition
3. **Neuroimaging validation**: Use fMRI or EEG to measure neural signatures of WM engagement during early vs. late learning phases, testing for reduced PFC activation as compression occurs