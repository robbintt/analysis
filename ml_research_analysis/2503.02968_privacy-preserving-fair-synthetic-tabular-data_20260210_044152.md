---
ver: rpa2
title: Privacy-Preserving Fair Synthetic Tabular Data
arxiv_id: '2503.02968'
source_url: https://arxiv.org/abs/2503.02968
tags:
- data
- privacy
- synthetic
- fairness
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating synthetic tabular
  data that preserves both privacy and fairness while maintaining utility. The proposed
  model, PF-WGAN, is a privacy-preserving and fair synthetic tabular data generator
  based on the WGAN-GP model.
---

# Privacy-Preserving Fair Synthetic Tabular Data

## Quick Facts
- **arXiv ID:** 2503.02968
- **Source URL:** https://arxiv.org/abs/2503.02968
- **Reference count:** 40
- **Primary result:** PF-WGAN demonstrates balanced trade-offs among utility, privacy, and fairness compared to state-of-the-art synthetic data generators

## Executive Summary
This paper addresses the challenge of generating synthetic tabular data that simultaneously preserves individual privacy, maintains fairness across demographic groups, and retains data utility. The proposed model, PF-WGAN, extends the Wasserstein GAN with Gradient Penalty (WGAN-GP) architecture by incorporating identifiability-based privacy and demographic parity-based fairness as components of the loss function during training. The approach aims to enable publication of datasets that protect individual privacy while remaining unbiased toward any particular group. Results demonstrate that PF-WGAN outperforms competing models in fairness metrics on three of four datasets while providing better privacy than WGAN and TabFairGAN, though it is less protective than the privacy-focused ADS-GAN.

## Method Summary
PF-WGAN modifies the standard WGAN-GP architecture by adding two auxiliary losses: one for privacy preservation (identifiability score) and one for fairness (demographic parity difference). The model trains in two phases: an initial warm-up period using only the standard WGAN-GP loss to stabilize utility, followed by the full loss function incorporating privacy and fairness terms. The generator uses linear layers with batch normalization and ReLU activation for numerical features, while categorical features are handled through Gumbel Softmax activation. The critic uses linear layers with LeakyReLU activation. Privacy is measured through mean squared error between real and synthetic batch embeddings, while fairness is measured through the demographic parity difference between privileged and unprivileged groups.

## Key Results
- PF-WGAN achieves better fairness performance than competing models on three of four datasets (Adult, ProPublica, Bank Marketing)
- The model provides improved privacy protection compared to WGAN and TabFairGAN, though ADS-GAN offers stronger privacy guarantees
- PF-WGAN demonstrates a more balanced trade-off among utility, privacy, and fairness compared to state-of-the-art synthetic data generators
- Experimental results show competitive accuracy, F1, and AUC-ROC scores while meeting privacy and fairness constraints

## Why This Works (Mechanism)
PF-WGAN works by integrating privacy and fairness constraints directly into the adversarial training process through modified loss functions. The model first establishes a baseline utility through standard WGAN-GP training, then progressively introduces privacy and fairness penalties. The privacy loss minimizes the identifiability of synthetic samples by reducing the distance between real and synthetic batch embeddings, while the fairness loss enforces demographic parity by minimizing the difference in positive prediction rates across privileged and unprivileged groups. This joint optimization ensures that the generator learns to produce data that is simultaneously useful, private, and fair.

## Foundational Learning
- **WGAN-GP Architecture**: Why needed: Provides stable training and improved gradient flow for GANs. Quick check: Verify critic provides meaningful gradients without mode collapse.
- **Identifiability Score**: Why needed: Quantifies privacy by measuring how distinguishable synthetic samples are from real ones. Quick check: Confirm privacy loss decreases during training without degrading utility.
- **Demographic Parity**: Why needed: Ensures equal positive prediction rates across protected groups for fairness. Quick check: Verify fairness loss converges to near-zero values.
- **Gumbel Softmax**: Why needed: Enables differentiable sampling from categorical distributions in the generator. Quick check: Confirm categorical outputs match training data distribution.
- **Two-Phase Training**: Why needed: Prevents early constraint imposition from destroying utility. Quick check: Monitor utility metrics during warm-up phase.
- **Gradient Penalty**: Why needed: Enforces Lipschitz continuity for stable Wasserstein distance estimation. Quick check: Verify gradient penalty term remains stable during training.

## Architecture Onboarding

**Component Map:**
Quantile Transform/One-Hot Encoding -> Generator (WGAN-GP + Privacy + Fairness) -> Critic -> Loss Calculation

**Critical Path:**
Data preprocessing → Generator (with Gumbel Softmax for categoricals) → Critic evaluation → Combined loss computation → Backpropagation → Synthetic data output

**Design Tradeoffs:**
- **Privacy vs. Utility**: Higher privacy weights (λp) reduce identifiability but may decrease classification accuracy
- **Fairness vs. Utility**: Stronger fairness constraints (λf) improve demographic parity but can reduce overall model performance
- **Two-Phase Training**: Initial warm-up improves stability but increases training time
- **Identifiability Metric**: MSE-based distance is computationally efficient but may not capture all privacy risks

**Failure Signatures:**
- **NaN Loss Values**: Indicates numerical instability, often from division by zero in fairness calculations
- **Catastrophic Utility Drop**: Suggests privacy/fairness constraints are too aggressive too early
- **Mode Collapse**: Generator produces limited variety of samples, failing to capture data distribution
- **Training Divergence**: Critic loss becomes unstable, preventing meaningful generator updates

**First Experiments:**
1. **Sanity Check**: Train standard WGAN-GP on Adult dataset for 50 epochs, verify utility metrics match baseline
2. **Ablation Test**: Train PF-WGAN without privacy loss (λp=0), verify fairness improvements over baseline
3. **Constraint Sensitivity**: Train with varying λf values (0.5, 1.0, 2.0), measure fairness-utility trade-off curve

## Open Questions the Paper Calls Out
- How can differential privacy be effectively integrated into the PF-WGAN framework while minimizing the utility loss associated with noise injection?
- How does the PF-WGAN model perform when optimizing for alternative fairness metrics, such as equalized odds or disparate impact, rather than demographic parity?
- Is PF-WGAN robust against specific adversarial attacks such as membership inference, attribute inference, and linkage attacks?

## Limitations
- Missing architectural details (hidden layer dimensions) prevent exact reproduction
- Unspecified regularization schedule (PFstart/PFend epochs) affects training protocol
- Privacy evaluation relies on distance-based identifiability score rather than formal differential privacy guarantees
- Limited evaluation of robustness against active adversarial attacks

## Confidence

**High Confidence:**
- Conceptual framework combining privacy and fairness with WGAN-GP is technically sound
- Integration of auxiliary losses for privacy and fairness is well-articulated

**Medium Confidence:**
- Experimental results showing balanced trade-offs are plausible given methodology
- Performance comparisons to baseline models are reasonable but not independently verifiable

**Low Confidence:**
- Direct numerical comparisons to competing models cannot be verified without complete implementation details
- Exact hyperparameter configurations and architectural specifications are missing

## Next Checks
1. **Architectural Sensitivity Analysis**: Implement multiple versions with varying hidden layer widths (128, 256, 512 units) to determine performance sensitivity
2. **Regularization Schedule Calibration**: Systematically test different PFstart/PFend epoch ranges (10-50, 50-100, 100-150) to identify optimal constraint timing
3. **Gradient Clipping Robustness**: Experiment with different gradient clipping bounds (±1.0, ±5.0, ±10.0) to assess impact on training stability and final performance