---
ver: rpa2
title: 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects
  in Large Language Models'
arxiv_id: '2502.11028'
source_url: https://arxiv.org/abs/2502.11028
tags:
- calibration
- answer
- accuracy
- confidence
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the calibration of large language models
  (LLMs) in Question-Answering (QA) tasks, focusing on the problem of overconfidence
  where predicted confidence is misaligned with true correctness. The study introduces
  a novel evaluation paradigm using structured distractor-augmented prompts, where
  models select answers from one correct and multiple plausible incorrect options.
---

# Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models

## Quick Facts
- arXiv ID: 2502.11028
- Source URL: https://arxiv.org/abs/2502.11028
- Authors: Prateek Chhikara
- Reference count: 40
- Key outcome: Structured distractors improve LLM calibration on QA tasks, reducing Expected Calibration Error (ECE) by up to 90% and increasing accuracy by up to 460% relative to free-generation settings.

## Executive Summary
This paper investigates calibration—the alignment between predicted confidence and actual correctness—in large language models across Question-Answering tasks. The study introduces a novel evaluation paradigm using structured distractor-augmented prompts, where models select answers from one correct and multiple plausible incorrect options. Across nine state-of-the-art LLMs spanning various model sizes, architectures, and fine-tuning regimes, the results demonstrate that incorporating distractors substantially improves calibration and accuracy. The findings reveal that smaller models benefit disproportionately from distractors but remain significantly miscalibrated, while larger RLHF-tuned models show better inherent calibration but can paradoxically suffer increased miscalibration on easier queries.

## Method Summary
The evaluation compares two prompting regimes: N (free-generation) where models generate answers with confidence scores, and D (distractor-augmented) where models select from one correct answer plus three plausible distractors. Distractors are generated by GPT-4o-mini with type-matching and plausibility validation. Responses are judged by GPT-4o-mini as CORRECT/INCORRECT/NOT_ATTEMPTED, with ECE computed using 10-bin empirical calibration. The study uses three datasets: SimpleQA (4326 QA pairs), FaVIQ (2922 "supports" test examples), and first 1000 TriviaQA validation points.

## Key Results
- Distractors reduce ECE by up to 90% (relative) and increase accuracy by up to 460% across models
- Smaller models benefit more from distractors but remain poorly calibrated even with improved accuracy
- Larger RLHF-tuned models show better inherent calibration but paradoxically worsen on easy queries with distractors
- Person-based queries remain persistently miscalibrated even with structured choices
- Model size and training quality impact calibration more than RLHF alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured distractors reduce overconfidence by forcing comparative evaluation before response selection.
- Mechanism: Inspired by cognitive psychology's "consider-the-opposite" strategy, presenting plausible incorrect options alongside the correct answer compels models to evaluate relative merit rather than generating from unanchored probability distributions.
- Core assumption: Models can meaningfully compare options when explicitly prompted; distractors are sufficiently plausible to trigger genuine discrimination.
- Evidence anchors: Distractors halve ECE (up to ∆ECE≈0.4) and more than double accuracy for smaller variants; self-generated distractor methods show similar effects.

### Mechanism 2
- Claim: Larger models learn confidence quantification as an emergent capability from scale.
- Mechanism: Scaling increases capacity to represent uncertainty in activations and distinguish between high-confidence and low-confidence knowledge retrieval.
- Core assumption: Confidence quantification requires representational capacity beyond factual storage; this capacity scales non-linearly with model size.
- Evidence anchors: GPT-4o compresses ECE by 92% (to 0.037) with distractors, while GPT-4o-mini remains at 0.32 despite larger relative accuracy gains; smaller models acquire knowledge faster than self-assessment ability.

### Mechanism 3
- Claim: RLHF improves calibration only when combined with sufficient pretraining scale and data quality.
- Mechanism: RLHF aligns outputs with human preferences, which can include appropriate hedging and uncertainty expression, but without sufficient underlying knowledge representations, RLHF may produce superficial verbal calibration patterns.
- Core assumption: Human feedback captures uncertainty signals; annotators penalize overconfident wrong answers.
- Evidence anchors: Qwen-qwq-32B demonstrates better accuracy and calibration despite lacking RLHF; "Don't Think Twice!" suggests over-reasoning can impair calibration.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Primary metric for quantifying miscalibration; understanding binning is essential for interpreting results. Quick check: If ECE is 0.45 on SimpleQA but 0.07 on TriviaQA, what does this tell you about task difficulty and calibration?
- **Verbalized/Elicited Confidence**: Paper uses self-reported 0-100 confidence scores rather than token log-probabilities; this methodological choice affects interpretability. Quick check: Why might elicited confidence be more reliable than log-probabilities for RLHF-tuned models?
- **Distractor Generation and Plausibility**: Quality of calibration improvement depends on distractors being plausible but incorrect. Quick check: What three criteria did the paper use to validate distractor quality?

## Architecture Onboarding

- **Component map**: Evaluation Pipeline (Question dataset → Prompt template → LLM response → GPT-4o-mini judge → ECE computation) → Distractor Module (GPT-4o-mini generates 3 type-matched plausible wrong answers) → Metrics Layer (Accuracy, ECE, D_helped/D_harmed counts)
- **Critical path**: 1. Define evaluation dataset (SimpleQA recommended) 2. Generate distractors with type/format matching validation 3. Run N and D settings 4. Use independent LLM judge 5. Compute ECE with empirical binning
- **Design tradeoffs**: Same vs. independent judge (risk of self-bias), 3 distractors vs. more (accuracy vs. overwhelm), elicited vs. log-prob confidence (black-box vs. true uncertainty)
- **Failure signatures**: High D_harmed rate (15-32% for small models on FaVIQ/TriviaQA), paradoxical ECE increase for large models on easy queries, persistent person-query miscalibration
- **First 3 experiments**: 1. Baseline calibration audit on SimpleQA measuring accuracy, ECE, NOT_ATTEMPTED rate 2. Distractor ablation computing D_helped vs. D_harmed 3. Question-type breakdown stratifying by person/date/place/number

## Open Questions the Paper Calls Out

1. **Distractor Source Generalization**: Does calibration improvement generalize when distractors are generated by alternative models or human curation rather than GPT-4o-mini? The paper notes this is important future work but hasn't been tested.

2. **RLHF Paradox Mechanism**: What mechanisms cause large RLHF-tuned models to exhibit degraded calibration on easier queries despite accuracy improvements? The authors hypothesize "confidence inflation" but don't empirically validate this.

3. **Person-Query Failure**: Why do person-based queries exhibit persistent calibration failures across model families? The analysis shows the problem exists but doesn't identify whether causes are name ambiguity, role overlap, or training data gaps.

## Limitations
- Evaluation limited to factual QA with binary correctness judgments, potentially limiting generalizability to open-ended tasks
- GPT-4o-mini judge introduces single point of bias and may systematically misclassify NOT_ATTEMPTED responses from smaller models
- Study doesn't explore distractor generation quality variations beyond initial plausibility checks
- No cross-model distractor generation comparison to isolate distractor quality effects

## Confidence
- **High**: Core finding that distractors substantially improve calibration (90% ECE reduction) and accuracy (460% relative gains) across diverse LLMs
- **Medium**: Claims about RLHF's mixed effects and person-query failures are data-supported but sensitive to dataset composition
- **Low**: Assertion that "training volume and quality significantly impact performance more than sheer parameter count" based on limited model comparisons

## Next Checks
1. **Judge Independence Validation**: Run evaluation using multiple independent judges (GPT-4o, Claude-3, LLaMA-3) to quantify systematic bias in GPT-4o-mini judging protocol
2. **Distractor Generation Ablation**: Systematically vary distractor generation strategies (different base models, varying quantity, self-generated vs externally generated) to isolate distractor quality vs quantity contributions
3. **Task Diversity Expansion**: Apply N vs D evaluation to non-factual QA tasks (commonsense reasoning, creative writing, multi-step reasoning) to test generalizability beyond fact retrieval