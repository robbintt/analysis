---
ver: rpa2
title: 'Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for
  LLMs'
arxiv_id: '2511.01202'
source_url: https://arxiv.org/abs/2511.01202
tags:
- information
- semantic
- theory
- llms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a semantic information-theoretic framework
  for large language models (LLMs), replacing Shannon's bit-based approach with a
  token-centric view. The core idea is to treat LLMs as generative models that optimize
  directed information flows rather than focus on exact reconstruction.
---

# Forget BIT, It is All about TOKEN: Towards Semantic Information Theory for LLMs

## Quick Facts
- arXiv ID: 2511.01202
- Source URL: https://arxiv.org/abs/2511.01202
- Reference count: 40
- This paper establishes a semantic information-theoretic framework for large language models (LLMs), replacing Shannon's bit-based approach with a token-centric view.

## Executive Summary
This paper introduces a semantic information-theoretic framework for understanding and analyzing large language models, moving beyond traditional Shannon information theory to focus on token-based semantic information flows. The framework treats LLMs as generative models that optimize directed information rather than exact reconstruction, providing new theoretical foundations for pre-training, post-training, and inference phases. By establishing concepts like directed rate-distortion functions and semantic information flow, the paper offers a unified theoretical approach that can analyze various LLM architectures including Transformers, Mamba, and LLaDA.

## Method Summary
The paper develops a comprehensive information-theoretic framework centered on directed information flows between tokens rather than bit-based reconstruction. It introduces the directed rate-distortion function for pre-training optimization, the directed rate-reward function for post-training alignment, and semantic information flow for inference analysis. The framework provides an information-theoretically optimal semantic embedding method and establishes a general definition of autoregressive LLMs. The theoretical approach shows how Transformer architecture can be derived from these information-theoretic principles, while also being applicable to other architectures like Mamba and LLaDA.

## Key Results
- Establishes a token-centric semantic information theory framework that replaces Shannon's bit-based approach for LLM analysis
- Derives the Transformer architecture from information-theoretic principles, demonstrating theoretical foundations for its design
- Provides new metrics for LLM analysis including ELBO, generalization bounds, and memory capacity based on semantic information theory
- Demonstrates applicability across multiple LLM architectures (Transformer, Mamba, LLaDA) through unified theoretical framework

## Why This Works (Mechanism)
The framework works by shifting from traditional reconstruction-based objectives to directed information flow optimization, which better captures the semantic nature of language modeling. Instead of minimizing reconstruction error between input and output bits, the approach optimizes the directed information flow between tokens, which naturally handles the autoregressive and generative nature of LLMs. This semantic focus allows the framework to provide more meaningful metrics for LLM performance and generalization, as it captures the actual information content being processed rather than just bit-level accuracy.

## Foundational Learning
- Directed Information Theory: Understanding how information flows directionally between variables rather than bidirectionally; needed to capture autoregressive generation in LLMs; quick check: verify that directed information properly captures causal relationships in sequential data.
- Rate-Distortion Theory: Framework for optimizing the tradeoff between compression rate and reconstruction quality; needed to formalize the pre-training optimization objective; quick check: confirm that rate-distortion bounds match observed training tradeoffs.
- Variational Inference: Statistical framework for approximating complex probability distributions; needed to derive ELBO-based metrics; quick check: validate that ELBO calculations align with actual training objectives.
- Information Bottleneck: Principle for compressing input while preserving relevant information; needed to understand generalization and memory capacity; quick check: measure actual information bottlenecks during training.
- Semantic Embedding Theory: Methods for representing meaning rather than surface form; needed for the proposed optimal embedding approach; quick check: verify semantic embeddings preserve meaning across contexts.

## Architecture Onboarding

Critical Path:
Semantic Embedding -> Directed Rate-Distortion (Pre-training) -> Directed Rate-Reward (Post-training) -> Semantic Information Flow (Inference)

Component Map:
Semantic Embedding -> Directed Rate-Distortion Function -> Directed Rate-Reward Function -> Semantic Information Flow

Design Tradeoffs:
- Bit-based vs. token-based information: Trade precision for semantic relevance
- Directed vs. undirected information: Sacrifices some mathematical convenience for causal accuracy
- Rate vs. distortion: Balancing compression efficiency against semantic preservation
- Model complexity vs. theoretical tractability: More complex models may better capture semantics but are harder to analyze theoretically

Failure Signatures:
- Over-optimization of directed information leading to loss of reconstruction ability
- Theoretical metrics not aligning with practical performance measures
- Framework not capturing attention mechanisms effectively
- Computational intractability in high-dimensional token spaces

First Experiments:
1. Implement semantic embedding method on standard benchmark and measure semantic preservation vs. baseline
2. Apply directed rate-distortion framework to small-scale pre-training and compare optimization dynamics
3. Test semantic information flow analysis on inference tasks to validate theoretical predictions

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of empirical validation for theoretical claims about information flow optimization
- Heavy reliance on information-theoretic concepts that may not translate cleanly to practical training scenarios
- Theoretical derivation of Transformer architecture remains largely abstract without demonstrated implementation
- Applicability to non-Transformer architectures remains theoretical without empirical results

## Confidence
High: Mathematical formalism for directed rate-distortion functions and semantic information flow appears internally consistent and builds on established information theory concepts.
Medium: Framework provides new metrics for analyzing LLM performance (ELBO, generalization bounds, memory capacity) that are plausible but require empirical validation.
Low: Claims that framework fundamentally reinterprets LLM training as optimizing directed information flows and that Transformer architecture can be derived from this theory remain largely theoretical without demonstrated practical implementation or empirical validation.

## Next Checks
1. Implement the proposed semantic information-theoretic framework on a standard LLM training task (e.g., language modeling on Wikitext-103) and compare performance metrics against conventional training approaches.

2. Test the framework's predictions about information bottlenecks and semantic embeddings by measuring actual information flow during training and inference, comparing against the theoretical expectations.

3. Apply the framework to analyze existing LLM architectures beyond Transformers, specifically Mamba and LLaDA, to verify the claimed applicability and identify any discrepancies between theoretical predictions and observed behavior.