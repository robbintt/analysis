---
ver: rpa2
title: Neurosymbolic Reasoning Shortcuts under the Independence Assumption
arxiv_id: '2507.11357'
source_url: https://arxiv.org/abs/2507.11357
tags:
- concept
- independence
- assumption
- nesy
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that the independence assumption in neurosymbolic
  models prevents them from representing uncertainty over concept combinations in
  the presence of reasoning shortcuts. The authors prove that models with conditionally
  independent concepts can only be "reasoning shortcut aware" (RS-aware) in extremely
  rare cases, specifically when confusion sets are either singletons or contain exactly
  two concepts differing in one variable.
---

# Neurosymbolic Reasoning Shortcuts under the Independence Assumption

## Quick Facts
- arXiv ID: 2507.11357
- Source URL: https://arxiv.org/abs/2507.11357
- Reference count: 22
- One-line primary result: Independence assumption in neurosymbolic models prevents calibrated uncertainty over reasoning shortcuts except in trivial cases

## Executive Summary
This paper establishes that the independence assumption in neurosymbolic models prevents them from representing uncertainty over concept combinations in the presence of reasoning shortcuts. The authors prove that models with conditionally independent concepts can only be "reasoning shortcut aware" (RS-aware) in extremely rare cases, specifically when confusion sets are either singletons or contain exactly two concepts differing in one variable. This limitation means such models either learn the reasoning shortcut with high confidence or fail to solve the task, but cannot express calibrated uncertainty. In contrast, expressive models without the independence assumption can be RS-aware using appropriate architectures and loss functions like KL-divergence to uniform distributions over consistent concepts.

## Method Summary
The paper analyzes reasoning shortcuts in neurosymbolic learning by comparing models with independent concept distributions versus expressive models that can represent joint distributions. Three architectures are evaluated: independent (factorized Bernoulli), joint (4-way softmax), and autoregressive (sequential Bernoulli with restricted conditioning). Two losses are used: semantic loss (maximum likelihood) and uniform-KL (KL-divergence to uniform over consistent concepts). Experiments use XOR MNIST (pairs of MNIST 0/1 digits, predict XOR label) and Traffic Lights MNIST (logical function β(c) = ¬c1 ∨ ¬c2). Models are trained for 5 epochs using Adam optimizer with learning rate 0.0001 and batch size 64.

## Key Results
- Independent models become highly confident in reasoning shortcuts (up to 99.8% label accuracy but only 45% concept accuracy)
- Expressive autoregressive models achieve both high label accuracy (99.9%) and better concept calibration (10.2% expected calibration error)
- The independence assumption creates expressivity gap that prevents RS-awareness except in trivial cases (Theorem 7)
- KL-divergence to uniform distributions enables RS-awareness in expressive models when combined with appropriate architecture

## Why This Works (Mechanism)

### Mechanism 1: Independence Assumption Creates Expressivity Gap
- Claim: Conditionally independent concept distributions cannot represent mixtures over reasoning shortcuts except in trivial cases.
- Mechanism: The independence assumption factorizes p(c|x) = ∏p(cᵢ|x), which restricts the support of learnable distributions to covers of implicants. When confusion sets contain multiple concepts that differ in more than one variable, independent models cannot assign calibrated probability mass across them while maintaining correct label predictions.
- Core assumption: Assumptions A1 (oracle f exists s.t. f(c,s)=x) and A2 (unique label per concept) hold in the generative process.
- Evidence anchors: [abstract]: "The authors prove that models with conditionally independent concepts can only be 'reasoning shortcut aware' (RS-aware) in extremely rare cases"; [section 4, Theorem 7]: "if the UCI model class is (weakly) RS-aware over A, then for all ground-truth worlds c*, the confusion set Vc* is equal to the cover of an implicant"
- Break condition: If concepts have full supervision (e.g., φ₀ = c₁ ∧ c₂ in Traffic Lights MNIST provides complete information), independence suffices because no ambiguity exists.

### Mechanism 2: KL-Divergence to Uniform Enables RS-Awareness in Expressive Models
- Claim: Minimizing KL-divergence to uniform distribution over consistent concepts enables expressive models to learn calibrated uncertainty over reasoning shortcuts.
- Mechanism: The loss KL[q(c|y) || pθ(c|x)] with q(c|y) = φᵧ(c)/|Cᵧ| forces the model to distribute probability mass uniformly across all concept configurations consistent with the label, preventing collapse to a single deterministic mapping.
- Core assumption: The model architecture can represent joint distributions over concepts (violating independence).
- Evidence anchors: [abstract]: "expressive autoregressive models achieve both high label accuracy (99.9%) and better concept calibration (10.2% expected calibration error)"; [section 3.2, Eq. 7]: KL-divergence formulation explicitly shown; [section 5, Table 1]: AR + Uniform-KL achieves 99.88% label accuracy with 10.24% ECE on XOR MNIST
- Break condition: If applied to independent models, this loss fails to learn anything beyond initialization because the KL minimizer under independence is μc₁ = μc₂ = 0.5 regardless of input.

### Mechanism 3: Autoregressive Architecture Provides Necessary Inductive Bias
- Claim: Autoregressive factorization p(c₁|x)·p(c₂|x,c₁) combined with restricted conditioning enables both RS-awareness and correct concept learning.
- Mechanism: By conditioning c₂ on a 1-dimensional embedding of c₁ rather than full joint information, the model can express dependencies needed for RS-awareness while preventing overfitting to single valid concepts in non-RS settings.
- Core assumption: Task structure has sequential concept dependencies that align with autoregressive ordering.
- Evidence anchors: [section 5]: "a well-designed autoregressive model can solve Traffic Lights MNIST with high concept accuracy, while also being RS-aware on XOR MNIST"; [Appendix C]: Explicit architecture description showing restricted conditioning mechanism
- Break condition: If task requires simultaneous reasoning over all concepts with no natural ordering, autoregressive factorization may introduce bias.

## Foundational Learning

- Concept: Conditional Independence in Probabilistic Models
  - Why needed here: The independence assumption p⊥⊥(c|x) = ∏p(cᵢ|x) is the core modeling choice being analyzed; understanding its expressivity limitations is essential.
  - Quick check question: Can a product of Bernoulli distributions represent a distribution that assigns 0.5 probability to (0,1) and (1,0) while assigning 0 to (0,0) and (1,1)?

- Concept: Reasoning Shortcuts as Concept Remappings
  - Why needed here: RSs are formalized as functions α: C→C that preserve label correctness while changing concepts; understanding this is key to the paper's theoretical results.
  - Quick check question: For XOR with β(c) = c₁⊕c₂, is the mapping α(0,0)=(1,1), α(1,1)=(0,0), α(0,1)=(1,0), α(1,0)=(0,1) a reasoning shortcut?

- Concept: Implicants and Covers in Boolean Logic
  - Why needed here: Theorem 7 connects RS-awareness to implicant covers; this terminology from Boolean logic is essential for understanding the theoretical contribution.
  - Quick check question: For constraint φᵧ(c) = c₁∨c₂, is the incomplete concept c_D = (1, _) an implicant?

## Architecture Onboarding

- Component map:
  - Neural encoder (LeNet CNN): Maps input x to embeddings
  - Concept distribution head: Outputs p(c|x)—either factorized (independent), joint (4-way softmax), or autoregressive (sequential Bernoulli)
  - Symbolic program β: Hand-specified logical function mapping concepts to labels
  - Loss function: Either semantic loss (Eq. 1) or uniform-KL (Eq. 7)

- Critical path:
  1. Identify whether task has reasoning shortcuts (check if multiple concept configurations yield same label)
  2. If RS exist and independence assumption is currently used, model will be overconfident
  3. Switch to expressive architecture (joint or autoregressive) with uniform-KL loss
  4. Validate concept calibration using ECE, not just label accuracy

- Design tradeoffs:
  - Independent models: Fast inference, tractable reasoning, but cannot be RS-aware
  - Joint models: Can be RS-aware, but may overfit to single concepts in non-RS settings
  - Autoregressive models: Can handle both RS and non-RS settings, but require careful architecture design for conditioning

- Failure signatures:
  - Independent model: 99%+ label accuracy with <50% concept accuracy → RS learned
  - Independent model with uniform-KL: Model fails to learn (stays at initialization)
  - Joint model on non-RS task: High label accuracy, poor concept accuracy (69% in Table 1)
  - Autoregressive with full joint conditioning: Overconfidence in single valid concept

- First 3 experiments:
  1. Replicate XOR MNIST with independent model: Expect ~50% runs to learn RS (11/20 in paper), verify with concept accuracy
  2. Add uniform-KL loss to independent model: Confirm failure to learn (Figure 4 behavior)
  3. Implement autoregressive model with restricted conditioning: Should achieve >99% label accuracy with ECE <15% on both XOR and Traffic Lights tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically design neural architectures that generally facilitate reasoning shortcut awareness (RS-awareness) without requiring specific task-level inductive biases?
- Basis in paper: [explicit] The authors state that "neural network architecture design is a crucial factor" and explicitly call for "studying what expressive architecture design suits NeSy predictors" as a primary direction for future research.
- Why unresolved: The experiments showed that a joint model failed to learn the correct concepts while a specifically designed autoregressive model succeeded, suggesting the solution relies on finding the right inductive bias, which is currently done by trial-and-error.
- Evidence: A set of guiding principles or a theoretical framework that links specific architectural properties (e.g., connectivity, autoregression) to the ability to represent RS mixtures across diverse logical programs.

### Open Question 2
- Question: How does the performance gap between independent and expressive NeSy models change when evaluated on benchmarks that violate the full observability assumption (Assumption A1)?
- Basis in paper: [explicit] The conclusion suggests that "studying benchmarks that go beyond the naive assumption of full observability" would help further distinguish the behaviors of independent and expressive predictors.
- Why unresolved: The theoretical proofs and experimental validation in the paper rely on the assumption that ground-truth concepts can be fully derived from the input ($c^* = f^{-1}(x)$), a condition not always met in complex environments like autonomous driving.
- Evidence: Empirical results on datasets with partial observability showing the comparative degradation of concept accuracy and calibration between independent and expressive models.

### Open Question 3
- Question: Is the KL-to-uniform loss generally sufficient for inducing RS-awareness in all expressive models, or does its effectiveness depend critically on the underlying architecture?
- Basis in paper: [inferred] While the authors use KL-divergence to successfully train an autoregressive model, they note that a standard joint model failed to learn the correct concept distribution on the Traffic Lights task despite using the same loss function.
- Why unresolved: This discrepancy implies that expressiveness alone is insufficient; the paper leaves open the question of exactly how the loss function interacts with the model's parameterization to avoid reasoning shortcuts.
- Evidence: A comparative study showing that the KL-to-uniform loss fails to calibrate certain expressive architectures (e.g., standard joint networks) while succeeding on others (e.g., autoregressive networks).

## Limitations

- The theoretical results assume full observability (A1) and unique label mapping (A2), which may not hold in real-world scenarios with partial observability or multi-label concepts
- The autoregressive architecture design appears to require task-specific inductive biases and careful tuning of the conditioning mechanism
- The effectiveness of KL-divergence to uniform distributions depends critically on the underlying architecture, as demonstrated by the joint model's failure on Traffic Lights MNIST

## Confidence

- Theoretical claims about expressivity limitations: High confidence (well-defined mathematical theorems with clear proofs)
- Experimental results demonstrating failure modes: Medium confidence (potential implementation variations in autoregressive architecture details)
- KL-divergence effectiveness claims: High confidence for expressive models, Medium confidence for independent models (depends on precise loss implementation)

## Next Checks

1. **Theoretical edge case validation**: Verify Theorem 7's claim that RS-awareness under independence requires confusion sets to be covers of implicants by testing with specific XOR confusion sets (e.g., {(0,1), (1,0)} and {(0,0), (1,1)}).

2. **Implementation fidelity check**: Replicate Figure 4 behavior showing independent models with uniform-KL loss failing to learn beyond initialization, confirming the loss indeed drives all concept probabilities to 0.5 regardless of input.

3. **Architectural sensitivity test**: Systematically vary the autoregressive conditioning dimensionality (e.g., try 2-dim or full joint conditioning) to identify the precise architectural constraints that enable successful RS-awareness without overfitting to single concepts.