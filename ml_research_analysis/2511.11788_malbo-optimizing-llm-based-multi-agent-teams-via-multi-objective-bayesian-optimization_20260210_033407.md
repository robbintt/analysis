---
ver: rpa2
title: 'MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian
  Optimization'
arxiv_id: '2511.11788'
source_url: https://arxiv.org/abs/2511.11788
tags:
- optimization
- cost
- performance
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces MALBO, a Bayesian optimization framework
  for optimizing the assignment of LLMs to specialized roles in multi-agent teams.
  The key problem is a vast combinatorial search space with expensive black-box evaluations
  and conflicting objectives of performance and cost.
---

# MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization

## Quick Facts
- arXiv ID: 2511.11788
- Source URL: https://arxiv.org/abs/2511.11788
- Reference count: 0
- One-line primary result: MALBO reduces average cost of high-performing LLM team configurations by over 45% versus random search.

## Executive Summary
MALBO is a Bayesian optimization framework that addresses the challenge of assigning LLMs to specialized roles in multi-agent teams, balancing performance and cost in a vast, expensive, and conflicting objective space. It represents LLMs as continuous feature vectors and uses multi-objective Bayesian optimization with Gaussian Processes and the q-Expected Hypervolume Improvement acquisition function to efficiently explore the space. The framework discovers a Pareto front of optimal configurations, enabling cost reductions of over 45% versus random search and up to 65.8% versus homogeneous baselines, while maintaining maximum performance.

## Method Summary
MALBO optimizes the assignment of LLMs to roles in a 5-agent team (Manager, Search Agent, Text Inspector, Visual QA, Reformulator), where 2 roles are fixed and 3 are optimized from a pool of 10 models. Each model is represented by a 5D feature vector (performance on MMLU-Pro, LiveCodeBench, GPQA Diamond, input/output token costs). The framework uses independent Gaussian Process surrogates for accuracy and cost, and the q-LogEHVI acquisition function to balance exploration and exploitation. Continuous relaxation via feature vectors and Euclidean nearest-neighbor projection to discrete assignments enables efficient search. The process starts with 15 random evaluations, followed by 15 BO iterations on a 10-task GAIA benchmark subset (5 easy, 5 medium).

## Key Results
- MALBO achieved over 45% reduction in average cost of high-performing configurations versus random search.
- Up to 65.8% cost reduction versus homogeneous baselines while maintaining maximum performance.
- Provided insights into which agent roles and model features most influence system performance and cost.

## Why This Works (Mechanism)
MALBO works by transforming the discrete, combinatorial problem of LLM assignment into a continuous optimization over model feature vectors. This allows Bayesian optimization to efficiently navigate the vast search space using surrogate models (GPs) that balance exploration and exploitation via the q-Expected Hypervolume Improvement acquisition function. The approach leverages the correlation structure in the feature space to predict the performance and cost of unseen configurations, guiding the search toward the Pareto-optimal frontier. By jointly optimizing for accuracy and cost, MALBO finds configurations that would be missed by single-objective methods.

## Foundational Learning
- **Multi-Objective Bayesian Optimization (MOBO)**: Needed to balance conflicting objectives (accuracy vs. cost) in a structured way; quick check: verify the surrogate models and acquisition function properly handle multi-objective trade-offs.
- **Gaussian Processes (GPs)**: Provide uncertainty-aware predictions for expensive black-box evaluations; quick check: ensure GP kernels and hyperparameters are well-tuned for the feature space.
- **Hypervolume Indicator**: Quantifies the quality of the Pareto front by measuring the dominated objective space; quick check: confirm hypervolume calculations are correct and stable across iterations.
- **Continuous Relaxation & Nearest-Neighbor Projection**: Enables BO to operate in a continuous space while respecting the discrete nature of model assignments; quick check: verify projection function correctly maps continuous points to real models.
- **q-LogEHVI Acquisition**: Balances batch exploration and exploitation in multi-objective settings; quick check: ensure acquisition function outputs are reasonable and diverse.
- **Feature Normalization**: Critical for meaningful distance calculations in the continuous space; quick check: confirm features are properly scaled (min-max normalization in this case).

## Architecture Onboarding
- **Component Map**: SmolAgents (5-role architecture) -> GAIA Benchmark -> Multi-Objective BO (GPs, q-LogEHVI) -> Pareto Front
- **Critical Path**: Define feature space and model pool → Initialize GPs and BO → Run initial random evaluations → Iterate BO-guided evaluations → Extract Pareto front
- **Design Tradeoffs**: Continuous relaxation simplifies BO but may introduce suboptimality; independent GPs are simpler but may miss correlations; fixed roles reduce search space but limit flexibility.
- **Failure Signatures**: Stagnant hypervolume early (acquisition function or GP issues), no cost reduction (token accounting or objective mis-specification), poor Pareto front quality (feature space inadequacy).
- **3 First Experiments**: 1) Validate single-objective BO on accuracy alone to confirm surrogate model behavior. 2) Run multi-objective BO with synthetic objectives to test Pareto front extraction. 3) Test continuous relaxation and projection on a small model pool to check for suboptimality.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can MALBO leverage low-fidelity evaluations (e.g., reduced task subsets) to accelerate convergence without compromising the quality of the final Pareto front? [explicit] Section 7.3 proposes integrating Multiple Information Source Optimization (MISO) to use cheaper, auxiliary information sources like smaller benchmark subsets.
- **Open Question 2**: How does the inclusion of latency or hallucination rate as a third objective alter the geometry of the Pareto front and the selection of optimal agent teams? [explicit] Section 7.3 identifies extending the framework to co-optimize for latency, uncertainty, and hallucination rate as a critical future direction.
- **Open Question 3**: Do the structural insights regarding the Manager's dominance hold true for decentralized multi-agent architectures or different benchmark domains? [inferred] Section 7.2 notes that a larger-scale study is required to confirm generalizability, and Section 6.3.1 establishes the Manager as the primary performance driver in the specific hierarchical setup used.

## Limitations
- Performance claims rely on a single benchmark (GAIA) and a fixed agent architecture, limiting generalizability to other multi-agent systems or tasks.
- Continuous relaxation and nearest-neighbor projection may introduce suboptimality when the feature space does not well-represent the discrete assignment problem.
- Exact GAIA task IDs for the 5 easy and 5 medium subsets, and specific GP kernel settings and random seeds used, are not specified, affecting reproducibility.

## Confidence
- **High** confidence in the methodological framework (MOBO with q-LogEHVI, independent GPs, and continuous relaxation) as it is well-established in BO literature.
- **Medium** confidence in the empirical cost reduction results (~45% vs. random, up to 65.8% vs. homogeneous baselines) due to potential unreported variance, seed sensitivity, and limited benchmark diversity.
- **Low** confidence in the robustness of insights into agent-role importance, as these are tied to the specific GAIA subset and architecture.

## Next Checks
1. Re-run the full BO process with different random seeds and compare hypervolume trajectories and Pareto front stability.
2. Validate cost savings on an additional multi-agent benchmark (e.g., HotpotQA or a custom multi-agent task suite) with the same 5-agent architecture.
3. Test sensitivity of the continuous relaxation approach by varying the feature space dimensionality or using alternative projection methods (e.g., learned embeddings or constrained discrete optimization).