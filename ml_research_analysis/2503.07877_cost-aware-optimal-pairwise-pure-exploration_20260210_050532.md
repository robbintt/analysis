---
ver: rpa2
title: Cost-Aware Optimal Pairwise Pure Exploration
arxiv_id: '2503.07877'
source_url: https://arxiv.org/abs/2503.07877
tags:
- exploration
- pure
- which
- have
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a general framework for pure exploration in
  multi-armed bandits, focusing on identifying pairwise relationships between arms
  while optimizing cumulative costs. A novel algorithm, CAET, is proposed, leveraging
  track-and-stop principles with a novel forced exploration method to handle zero-cost
  arms.
---

# Cost-Aware Optimal Pairwise Pure Exploration

## Quick Facts
- **arXiv ID:** 2503.07877
- **Source URL:** https://arxiv.org/abs/2503.07877
- **Reference count:** 40
- **Primary result:** CAET algorithm achieves asymptotically optimal cumulative cost in pure exploration with potentially zero-cost arms.

## Executive Summary
This paper introduces a novel framework for cost-aware pure exploration in multi-armed bandits, focusing on identifying pairwise relationships between arms while minimizing cumulative sampling costs. The key challenge addressed is the presence of zero-cost arms, which create a tension between needing to sample them sufficiently for accurate reward estimates while avoiding excessive cost accumulation. The proposed CAET algorithm builds on the track-and-stop principle with a novel forced exploration mechanism that allocates vanishing proportions to zero-cost arms as confidence increases. Theoretical analysis proves CAET asymptotically approaches the fundamental performance lower bound, with extensions to regret minimization via explore-then-commit schemes.

## Method Summary
The method employs a track-and-stop approach where optimal sampling proportions are computed via constrained optimization over an alternative set, then transformed to actual pulling proportions. A key innovation is the forced exploration mechanism using parameter α = 1 - log^{-r}(1/δ) that allocates vanishing proportion to zero-cost arms while ensuring accurate reward estimates. The algorithm uses a truncation function D_δ to detect zero-cost arms and employs a generalized likelihood ratio test for stopping, checking whether pairwise statistics exceed time-dependent thresholds for some valid hypothesis.

## Key Results
- CAET achieves asymptotic optimality, approaching the fundamental lower bound on cumulative cost
- The algorithm handles the challenging case of zero-cost arms through forced exploration with parameter α
- Experimental results demonstrate effectiveness across various settings including Bernoulli and Gaussian rewards
- Extensions to regret minimization via explore-then-commit achieve optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Cost-Aware Optimal Proportion Tracking
- **Claim:** Minimizing cumulative cost requires sampling arms in proportion to a cost-weighted optimal allocation.
- **Mechanism:** CAET computes ω*(c, μ) by solving constrained optimization over alternative set, transforms via G_c to get pulling proportions u*. C-tracking pulls the arm most lagging behind target cumulative proportion. Zero-cost arms detected via truncation function D_δ on empirical costs.
- **Core assumption:** Reward distributions belong to canonical one-parameter exponential family; costs are non-negative and may be zero.
- **Evidence anchors:** [abstract] "CAET builds on the track-and-stop principle with a novel design to handle the arm-specific costs"; [Section 5.1] "u*(c_δ(t), μ̂(t)) = G_{c_δ(t)}(ω*(c_δ(t), μ̂(t)))"
- **Break condition:** If costs not bounded away from zero or distributions are heavy-tailed outside exponential families, KL-based lower bound derivations and convergence guarantees may not hold.

### Mechanism 2: Forced Exploration for Zero-Cost Arms
- **Claim:** Zero-cost arms must be sampled sufficiently to provide accurate reward estimates for guiding non-zero-cost arm allocation, but sampling rate must decay asymptotically.
- **Mechanism:** Introduce α = 1 - log^{-r}(1/δ) (0 < r < 1/2). Sampling distribution becomes u_α = α·uniform(N(c)) + (1-α)·u*(c, μ). This allocates vanishing proportion to zero-cost arms as δ→0, ensuring cost-optimality while guaranteeing finite stopping time.
- **Core assumption:** Zero-cost arms exist in N(c) and their rewards are needed to support comparisons in exploration task I_m.
- **Evidence anchors:** [abstract] "costs, which can potentially be zero and thus represent a very challenging case"; [Section 5.1] "u_α(c_δ(t), μ̂(t)) = 1_α + (1-α)u*"; "α = 1 - log^{-r}(1/δ)"
- **Break condition:** If α decays too fast (r ≥ 1/2), zero-cost arms may be under-sampled, preventing convergence of μ̂ for comparisons; if α decays too slowly, cumulative cost diverges from lower bound.

### Mechanism 3: Pairwise Generalized Likelihood Ratio Stopping
- **Claim:** Any pure exploration task expressible as pairwise comparisons can be solved by tracking pairwise GLR statistics against time-dependent threshold.
- **Mechanism:** For each pair (a,b) in task's index set I_m, compute Z_{a,b}(t) using KL divergences. Stop when ∃G_m such that ∀(a,b)∈I_m, Z_{a,b}(t) > β(t,δ).
- **Core assumption:** Task is a "pairwise exploration task" per Definition 1 (intersections of pairwise order constraints).
- **Evidence anchors:** [abstract] "focus on identifying the pairwise relationships between targeted arm pairs"; [Section 5.2] "τ_δ = inf{t ∈ ℕ : ∃G_m, ∀(a,b)∈I_m, Z_{a,b}(t) > β(t,δ)}"
- **Break condition:** If task cannot be expressed as pairwise comparisons, framework does not apply.

## Foundational Learning

- **Concept: Multi-Armed Bandits (MAB) with Pure Exploration**
  - Why needed here: CAET operates in fixed-confidence pure exploration setting, distinct from regret minimization. Understanding δ-PAC correctness and stopping time is essential.
  - Quick check question: Can you explain why sample complexity ~ O(log(1/δ)) rather than O(T) for fixed-horizon regret?

- **Concept: KL Divergence and Exponential Families**
  - Why needed here: Lower bound and GLR statistics rely on d(μ, λ) = KL(π_μ, π_λ) for exponential family distributions. Gaussian and Bernoulli are key special cases.
  - Quick check question: Compute d(x, y) for two Bernoulli distributions with means x and y.

- **Concept: Track-and-Stop Algorithms**
  - Why needed here: CAET inherits C-tracking principle from Garivier & Kaufmann (2016)—maintaining empirical sampling proportions close to evolving target.
  - Quick check question: Why does C-tracking guarantee convergence of Na(t)/t to target proportion even when target changes with estimates?

## Architecture Onboarding

- **Component map:** Input layer (K arms, δ, costs, rewards) -> Estimation layer (empirical means, truncation) -> Optimization layer (solve for ω*) -> Sampling layer (C-tracking + forced exploration) -> Stopping layer (GLR statistics) -> Decision layer (output G_m)
- **Critical path:** Per-round bottleneck is solving optimization for ω* (line 4 in Algorithm 1), requiring computing inf_{λ∈Alt(c,μ)} over all alternative models.
- **Design tradeoffs:** Larger r → lower cumulative cost but longer stopping time O(log^{1+r}(1/δ)); θ > 1 in threshold → looser upper bound; θ = 1 is optimal but requires specific thresholds.
- **Failure signatures:** Infinite loop if zero-cost arms never get sufficient samples; cost explosion if r is too small; incorrect identification if threshold β(t,δ) too loose.
- **First 3 experiments:**
  1. Bernoulli 3-arm BAI with heterogeneous costs: μ = (0.5, 0.4, 0.3), c = (1.0, 0.5, 0). Compare cumulative cost of CAET vs uniform sampling; plot vs log(1/δ).
  2. Zero-cost arm sensitivity: Vary r ∈ (0, 0.5) on ranking identification task. Plot stopping time and cumulative cost vs r.
  3. Regret minimization via ETC: Use c = Δ (suboptimality gaps), set δ = 1/T, run CAET + commit. Compare cumulative regret to asymptotic lower bound.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a batched version of CAET be designed to improve computational efficiency while maintaining theoretical guarantees? (The current algorithm requires solving complex optimization in every round, creating computational bottleneck.)
- **Open Question 2:** Can this framework be generalized to pure exploration tasks that do not rely on pairwise comparisons? (Current theory is restricted to pairwise tasks defined by intersections of binary comparisons.)
- **Open Question 3:** What are performance bounds and optimal strategies for cost-aware pure exploration in the fixed-budget setting? (This work exclusively addresses fixed-confidence setting, leaving fixed-budget counterpart unexplored.)

## Limitations

- Computational complexity of solving optimization problem for ω* at each timestep remains unclear and may require approximations
- Framework assumes rewards belong to exponential families and tasks can be expressed as pairwise comparisons, limiting broader applicability
- Forced exploration parameter α requires careful tuning; optimal choices may be problem-dependent

## Confidence

- **High confidence:** Asymptotic optimality claim and correctness of GLRT stopping rule for pairwise tasks, supported by established track-and-stop theory
- **Medium confidence:** Forced exploration mechanism for zero-cost arms and truncation function D_δ, as these introduce novel elements with limited empirical validation
- **Medium confidence:** Extension to regret minimization via explore-then-commit, as transformation from pure exploration to regret guarantees involves additional assumptions

## Next Checks

1. **Computational tractability:** Implement and benchmark optimization oracle for ω* on problems with K=10+ arms. Measure runtime per iteration and assess whether approximations significantly impact cumulative cost.
2. **Zero-cost arm sensitivity:** Systematically vary r ∈ {0.1, 0.3, 0.4, 0.49} on 5-arm ranking task with heterogeneous costs. Plot stopping time and cumulative cost; identify r value that minimizes cost while ensuring termination.
3. **Robustness to distribution misspecification:** Test CAET on heavy-tailed reward distributions (e.g., Student's t with 3 degrees of freedom). Measure empirical δ-PAC performance and cumulative cost relative to theoretical lower bound.