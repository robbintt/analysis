---
ver: rpa2
title: Learning To Explore With Predictive World Model Via Self-Supervised Learning
arxiv_id: '2502.13200'
source_url: https://arxiv.org/abs/2502.13200
tags:
- agent
- modules
- learning
- state
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to intrinsic motivation in
  reinforcement learning by employing a predictive world model with modular, hierarchical,
  and attentional structures inspired by cognitive elements of the human neocortex.
  The proposed method combines sparsity, modularity, independence, hierarchy, and
  attention to generate intrinsic rewards through prediction errors between expected
  and actual future states.
---

# Learning To Explore With Predictive World Model Via Self-Supervised Learning

## Quick Facts
- **arXiv ID:** 2502.13200
- **Source URL:** https://arxiv.org/abs/2502.13200
- **Reference count:** 9
- **One-line primary result:** Predictive world model with modular structure achieves superior performance on 18 Atari games compared to state-of-the-art intrinsic motivation methods.

## Executive Summary
This paper introduces a novel approach to intrinsic motivation in reinforcement learning by employing a predictive world model with modular, hierarchical, and attentional structures inspired by cognitive elements of the human neocortex. The method generates intrinsic rewards through prediction errors between expected and actual future states, enabling agents to explore environments without external rewards. Experiments on 18 Atari games demonstrate superior performance compared to state-of-the-art methods, achieving significant improvements in many test cases with both dense and sparse rewards.

## Method Summary
The approach uses a predictive world model consisting of bidirectional recurrent models with competitive independent modules that generate representations of current and possible future states. At each timestep, modules in the "expected state" generate predictions before the agent acts. After executing an action, the intrinsic reward is computed as the L2 distance between predicted and actual latent representations. The model employs key-value attention to selectively activate a subset of modules per layer, creating specialization and reducing computational overhead. The entire system is trained using PPO with normalized advantages, where the policy network takes the current state representation as input and the world model is trained to minimize prediction errors.

## Key Results
- Superior performance on 18 Atari games compared to state-of-the-art intrinsic motivation methods
- Achieved mean human-normalized score of 2.41 vs 1.14 for baseline ICM method
- Particularly effective in dynamic environments requiring reactive behaviors
- Demonstrated more efficient exploration strategies leading to structured cognitive behaviors without external rewards

## Why This Works (Mechanism)

### Mechanism 1: Prediction Error as Intrinsic Reward Signal
Prediction errors between expected and actual future states generate useful exploration signals without external rewards. At each timestep, modules generate predictions before the agent acts, and the intrinsic reward is computed as the L2 distance between predicted and actual latent representations. High prediction error leads to high reward, driving exploration of novel state-action regions.

### Mechanism 2: Modular Sparse Activation with Competitive Selection
The world model decomposes into competitively activated sparse modules that improve representation quality and exploration efficiency. The hidden state partitions into n modules per layer, with key-value attention determining which m modules activate at each timestep. This specialization allows each module to become expert on certain environmental aspects.

### Mechanism 3: Bidirectional Attention for Reactive-Deliberative Balance
Bottom-up and top-down attention signals enable the agent to switch between reactive and planning behaviors. Bottom-up attention routes sensory input to activate relevant modules quickly for reactive responses, while top-down attention queries higher-layer modules for contextual predictions useful for planning.

## Foundational Learning

- **Concept: Intrinsic Motivation in RL**
  - **Why needed here:** The entire method replaces extrinsic rewards with internally generated signals. Understanding why prediction error drives exploration is foundational.
  - **Quick check question:** Can you explain why maximizing prediction error might cause an agent to seek novel states rather than repeat known ones?

- **Concept: Key-Value Attention (Scaled Dot Product)**
  - **Why needed here:** Module activation, inter-module communication, and cross-layer communication all use this attention mechanism.
  - **Quick check question:** Given queries Q, keys K, and values V, what does Softmax(QK^T/√d)V compute and why does the √d scaling matter?

- **Concept: Recurrent Independent Mechanisms (RIMs)**
  - **Why needed here:** BRIMs extend RIMs to multi-layer hierarchical settings. Understanding single-layer RIMs is a prerequisite.
  - **Quick check question:** In a RIM with n=6 modules and m=2 active per timestep, how are inactive modules updated?

## Architecture Onboarding

- **Component map:** Encoder (θ_E) → BRIM Predictive World Model (θ_F) → Policy Network (θ_P) → Action → Environment → Encoder (θ_E)

- **Critical path:**
  1. Observe s_t → encode to x_t
  2. Attention determines which modules activate for current vs. expected states
  3. Active modules compute h^p_t (current representation) and ĥ^f_t (predicted next state)
  4. Sample action a ~ π(h^p_t), execute in environment
  5. Observe s_{t+1}, encode to x_{t+1}
  6. Compute intrinsic reward r^int_t = ||h^p_{t+1} - ĥ^f_t||² / n
  7. Update θ_E, θ_P to maximize reward (PPO); update θ_E, θ_F to minimize prediction loss (MSE)

- **Design tradeoffs:**
  - **m (active modules):** Higher m = more capacity but less sparsity/specialization. Paper uses m = n/2.
  - **Rollout length:** 128 works for reactive games but underperforms in sparse environments (Pitfall). Longer rollouts may help sparse tasks.
  - **State normalization:** Simple /255 normalization used; paper notes standard observation normalization caused policy collapse in baselines.

- **Failure signatures:**
  - **Policy collapse:** Agent stops exploring; scores plateau near zero. Monitor intrinsic reward variance.
  - **Stochasticity trap:** Highly random environments cause persistent high prediction error → agent stuck exploring noise rather than structure.
  - **Module underutilization:** If attention scores concentrate on same modules, check learning rate or increase m.

- **First 3 experiments:**
  1. **Reactive environment sanity check:** Train on Asterix or Centipede for 10M steps. Verify intrinsic reward correlates with extrinsic score early in training.
  2. **Ablation on module count:** Compare n=8/m=4 vs. n=4/m=2 on MsPacman. Expect degraded performance with fewer modules.
  3. **Sparse reward stress test:** Train on Freeway with rollout=128 vs. rollout=512. Confirm longer rollouts improve crossing count.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do cognitive behaviors emerge when applying this predictive world model to complex humanoid object manipulation tasks in highly sparse, realistic environments?
- **Basis in paper:** [explicit] The Conclusion states: "Future work will address how cognitive behaviours emerge in complex humanoids to object manipulation tasks in highly sparse and realistic environments."
- **Why unresolved:** The current study evaluates the agent exclusively on Atari games, which are visually simpler and less physically complex than realistic 3D environments or robotics simulations.
- **What evidence would resolve it:** Demonstration of the agent learning manipulation skills in a physics simulator (e.g., MuJoCo) without external rewards.

### Open Question 2
- **Question:** Does increasing the rollout size beyond 128 steps significantly improve performance in extremely sparse environments like Pitfall where the current model fails?
- **Basis in paper:** [inferred] The authors attribute the negative results in Pitfall to the rollout size: "The 128 rollout is relatively small for sparse environments."
- **Why unresolved:** The authors identify the hyperparameter as a likely cause for failure but do not provide experimental results using longer rollouts to validate the hypothesis.
- **What evidence would resolve it:** Ablation studies showing score improvements in Pitfall and similar sparse games when using extended rollout lengths (e.g., 256 or 512).

### Open Question 3
- **Question:** Does increasing the number of hierarchical layers in the Bidirectional Recurrent Models improve performance on tasks requiring long-term planning?
- **Basis in paper:** [inferred] The paper proposes a "hierarchical" predictive world model, but the Experiment Setup specifies: "We use one layer of Recurrent Independent Mechanisms... with eight modules."
- **Why unresolved:** While the architecture theoretically supports multiple layers to handle planning, the experiments restrict the implementation to a single layer, leaving the benefits of the hierarchical depth untested.
- **What evidence would resolve it:** Comparative results on deliberative games (e.g., Solaris) showing the performance impact of adding additional recurrent layers.

## Limitations

- Several architectural details remain underspecified, including exact encoder kernel sizes and policy network dimensions
- Limited ablation study on module count and attention hyperparameters makes it unclear which components drive performance gains
- Claims about generating "structured cognitive behaviors" lack mechanistic detail and empirical validation beyond Atari games

## Confidence

- **High confidence** in the core prediction-error mechanism for intrinsic motivation (well-established in literature, direct derivation in paper)
- **Medium confidence** in modular RIMs improving exploration efficiency (supported by experiments but limited ablation)
- **Low confidence** in claims about generating "structured cognitive behaviors" and achieving "human-like cognitive elements" - these appear aspirational rather than empirically demonstrated

## Next Checks

1. **Module ablation study**: Systematically vary n and m across multiple games to identify whether sparse activation (m < n) is essential or if dense processing with more modules achieves similar results.

2. **Intrinsic reward ablation**: Train identical agents without intrinsic rewards (pure random exploration) to quantify the marginal contribution of the predictive world model beyond simple curiosity.

3. **Hierarchical behavior analysis**: Apply unsupervised skill discovery (e.g., DIAYN) to analyze whether the modular structure produces distinguishable, reusable skills versus monolithic exploration patterns.