---
ver: rpa2
title: 'ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models'
arxiv_id: '2509.21826'
source_url: https://arxiv.org/abs/2509.21826
tags:
- wang
- policy
- arxiv
- rest
- grpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training large language models
  (LLMs) as tool-use agents using reinforcement learning (RL). The core problem is
  that existing RL methods for tool-use tasks rely on sparse, outcome-only rewards,
  which lead to high-variance policy gradients and inefficient training, especially
  in multi-turn scenarios.
---

# ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models

## Quick Facts
- **arXiv ID**: 2509.21826
- **Source URL**: https://arxiv.org/abs/2509.21826
- **Reference count**: 40
- **Primary result**: ResT achieves SOTA results on BFCL and API-Bank, outperforming prior methods by up to 8.76%

## Executive Summary
This paper tackles the challenge of training large language models (LLMs) as tool-use agents using reinforcement learning (RL). The core problem is that existing RL methods for tool-use tasks rely on sparse, outcome-only rewards, which lead to high-variance policy gradients and inefficient training, especially in multi-turn scenarios. The authors establish a theoretical link between policy entropy and training stability, showing that structured, low-entropy tokens (like tool names and parameters) are the primary determinants of rewards. Based on this insight, they propose Reshaped Token-level policy gradients (ResT), which reshapes the policy gradient by entropy-informed token reweighting. This approach progressively upweights reasoning tokens as training proceeds, enabling a smooth shift from structural correctness to semantic reasoning and stabilizing convergence. ResT achieves state-of-the-art results on BFCL and API-Bank benchmarks, outperforming prior methods by up to 8.76%. When fine-tuned on a 4B base LLM, ResT surpasses GPT-4o by 4.11% on single-turn tasks and 1.50% on multi-turn base tasks.

## Method Summary
ResT applies entropy-informed token weights to 4 regions (format tags, tool names, parameters, chain-of-thought) during RL training. The method uses a curriculum that anneals format weights and increases parameter/reasoning weights as training progresses. It integrates with GRPO-style training, applying token-weighted PPO objectives to stabilize convergence and improve semantic accuracy. The approach uses a rule-based reward system that computes exact matches on format, Jaccard similarity on names, and exact matches on parameter values.

## Key Results
- Achieves SOTA on BFCL and API-Bank benchmarks, outperforming prior methods by up to 8.76%
- When fine-tuned on a 4B base LLM, surpasses GPT-4o by 4.11% on single-turn tasks and 1.50% on multi-turn base tasks
- Ablation studies show removing curriculum reduces accuracy by 4.36%-4.86% depending on model size

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Based Variance Reduction
- **Claim**: Lower token-level entropy reduces policy-gradient variance, stabilizing training for tool-use tasks.
- **Mechanism**: The paper proves (Theorem 1) that gradient variance scales with β_t = E[‖J_t‖²_F (1 − e^{−H_t})], where H_t is Shannon entropy. Low-entropy tokens (deterministic choices like tool names) contribute less variance than high-entropy tokens (open-ended reasoning). By inversely weighting tokens by their variance contribution, the estimator remains unbiased while tightening variance bounds.
- **Core assumption**: Advantage estimates Â_i are independent of (J_t, H_t) or bounded via Hölder's inequality; this is stated in Appendix A.2 remarks.
- **Evidence anchors**:
  - [abstract]: "establish a theoretical link between policy entropy and training stability... structured, low-entropy tokens are primary determinants of rewards"
  - [Section 3.1]: Theorem 2 derives optimal weights ω̃*_t ∝ 1/β_t, minimizing variance bound
  - [corpus]: ST-PPO identifies token-level importance sampling instability in multi-turn settings, supporting the need for variance control mechanisms
- **Break condition**: If token entropy does not correlate with reward-critical positions (e.g., reasoning tokens become low-entropy in your domain), the reweighting will incorrectly suppress learning signals.

### Mechanism 2: Structured Token Dominance in Reward Signal
- **Claim**: In tool-use tasks, rewards concentrate on a small set of structured tokens (tool names, parameters, format markers), while reasoning tokens contribute little to early reward signals.
- **Mechanism**: Rule-based rewards (Eq. 10-13) compute exact matches on format, Jaccard similarity on names, and exact matches on parameter values—all tied to specific token positions. Uniform token weighting dilutes this sparse signal, especially for GRPO which lacks token-level critics.
- **Core assumption**: The reward formulation accurately reflects task success; noisy or misspecified rewards will misdirect reweighting.
- **Evidence anchors**:
  - [Section 3.2.1]: "rule-based rewards are inherently concentrated on format tags, tool names, and key parameters"
  - [Figure 4]: Shows entropy distributions across token regions—reasoning tokens have higher entropy than tool-invocation tokens
  - [corpus]: TTPA similarly identifies fine-grained token-level optimization needs for tool-use alignment
- **Break condition**: If your task requires rewarding reasoning quality rather than format compliance (e.g., creative recommendations), the reward structure must change before reweighting helps.

### Mechanism 3: Curriculum-Driven Weight Annealing
- **Claim**: Progressively shifting weight from format tokens to parameter and reasoning tokens stabilizes convergence and improves semantic accuracy.
- **Mechanism**: Equations 16-17 define schedules: format weights decay as ω̃_{fmt}(ν) = max(w_min, ω̃_{fmt} − α_f·ν), while parameter/reasoning weights grow. This implements a curriculum: structural correctness first, then semantic refinement. The curriculum is synchronized with training progress ν ∈ [0,1].
- **Core assumption**: The optimal curriculum pace (α_f, α_p, α_t) transfers across model scales and task distributions; ablation shows ~4.86% drop without curriculum on Qwen3-8B.
- **Evidence anchors**:
  - [Section 3.2.2]: Describes the curriculum schedule and synchronization of CoT and parameter weights
  - [Table 3]: Ablation shows removing curriculum reduces accuracy by 4.36%-4.86% depending on model size
  - [corpus]: Weak direct evidence; related work (CE-GPPO) addresses entropy coordination but not curriculum annealing specifically
- **Break condition**: If training steps are insufficient for curriculum to complete (ν reaches 1), format weights may remain too high, limiting semantic learning.

## Foundational Learning

- **Concept: Policy Gradient Variance**
  - **Why needed here**: ResT's core contribution is a variance reduction theorem; understanding why high variance harms RL (noisy updates, unstable convergence) is prerequisite.
  - **Quick check question**: Given a mini-batch of G trajectories, how does variance scale with batch size, and what does Lemma 1 say about coordinate-wise variance?

- **Concept: Shannon Entropy vs. Determinism**
  - **Why needed here**: The mechanism hinges on entropy as a proxy for token "importance"—low entropy ≈ deterministic choice ≈ reward-critical.
  - **Quick check question**: For a token distribution p_t, why does E[‖s_t‖²] ≤ 1 − e^{−H_t} hold, and what does this imply about uniform vs. peaked distributions?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here**: ResT builds on GRPO as its base RL algorithm; understanding how GRPO constructs advantages without a value network clarifies why token-level reshaping is needed.
  - **Quick check question**: How does GRPO compute advantages from a group of G outputs, and why does it apply the same advantage to all tokens in a sequence?

## Architecture Onboarding

- **Component map**: Tokenizer + Region Partitioner -> Entropy Calculator -> Reward Scorer -> Curriculum Scheduler -> Gradient Reshaper
- **Critical path**: Rollout -> Reward computation -> Token partitioning -> Entropy calculation -> Curriculum reweighting -> Normalized gradient update. The reward scorer is the bottleneck for correctness; errors here propagate through all downstream weighting.
- **Design tradeoffs**:
  - **Entropy-only approximation (Eq. 9) vs. full β_t**: Computing exact Jacobian norms is expensive; entropy-only is cheaper but may misweight if ‖J_t‖²_F varies significantly across positions.
  - **Curriculum pace (α values)**: Faster annealing reaches semantic learning sooner but risks unstable early training; ablation suggests tuning per model scale.
  - **Region granularity**: 4 regions is task-specific; extending to new domains requires defining new regions and entropy baselines.
- **Failure signatures**:
  1. **Policy collapse to short responses**: If format weights remain too high, model may output minimal valid tool calls without reasoning (see GRPO baseline in Figure 3 collapsing to ~125 tokens).
  2. **Reward hacking**: Model exploits format-matching rewards without semantic correctness (high format score, low accuracy score).
  3. **Entropy explosion**: If curriculum anneals too fast, reasoning tokens may not receive sufficient gradient signal, leading to high-entropy, low-quality outputs.
- **First 3 experiments**:
  1. **Entropy profiling**: Sample 50 tool-use responses from your base model; compute H_avg per region to verify reasoning tokens have higher entropy than tool-invocation tokens (replicate Figure 4 for your domain).
  2. **Ablation on curriculum**: Train with static weights (no curriculum) vs. full ResT on a held-out validation set; expect 3-5% gap as per Table 3.
  3. **Variance monitoring**: Log per-batch gradient variance during training; compare ResT vs. GRPO to verify the claimed ~2× reduction in policy entropy (Figure 3, left panel).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can verifiable structural rewards be effectively hybridized with semantic LLM-as-a-judge scores within the ResT framework?
- **Basis in paper**: [explicit] The conclusion states future work will "integrate verifiable structural rewards with judge-based semantic scores."
- **Why unresolved**: ResT currently relies on rule-based matching for low variance, but this may fail on tasks requiring semantic nuance where rules are brittle.
- **What evidence would resolve it**: An ablation study combining ResT's structural weighting with a calibrated semantic reward model, showing improved performance on open-ended tasks.

### Open Question 2
- **Question**: Can the curriculum for entropy-aware mixing be learned dynamically rather than fixed?
- **Basis in paper**: [explicit] The conclusion proposes to "learn schedules for entropy-aware mixing."
- **Why unresolved**: ResT currently uses hand-crafted curriculum functions (Eq. 16) to adjust weights, which may not be optimal for all model scales or tasks.
- **What evidence would resolve it**: Implementation of a meta-learning algorithm to optimize the reweighting schedule ($\alpha, \nu$) dynamically during training.

### Open Question 3
- **Question**: Does computing exact token-level variance contributions ($\beta_t$) yield significant gains over the entropy-only approximation?
- **Basis in paper**: [inferred] Section 3.1 notes that computing the optimal weights (Eq. 7) is expensive and uses entropy-only statistics (Eq. 9) as a surrogate.
- **Why unresolved**: It is unclear if the computational savings of the approximation come at the cost of theoretical optimality or convergence speed.
- **What evidence would resolve it**: A comparative analysis of convergence rates and policy performance when using exact Jacobian norms vs. the proposed entropy approximation.

## Limitations

- The paper's theoretical contribution hinges on the assumption that token-level entropy correlates with gradient variance and reward importance, but the practical assumption that Jacobian norm ∥J_t∥²_F is approximately uniform across tokens is not empirically validated.
- The curriculum design uses fixed pace parameters (α_f, α_p, α_t) that appear tuned for Qwen3-8B/4B, with no exploration of whether these hyperparameters generalize to other model scales or domains.
- The region partitioning into 4 fixed categories (format, name, param, CoT) is task-specific to the tool-use benchmarks used, limiting direct applicability to domains with different token criticality patterns.

## Confidence

- **High Confidence**: The empirical results showing ResT outperforms GRPO and other baselines on BFCL and API-Bank benchmarks (up to 8.76% improvement). The ablation studies demonstrating curriculum importance are well-supported with specific metrics.
- **Medium Confidence**: The theoretical link between entropy and variance (Theorem 1) and the practical approximation using entropy-only weighting. While the proof structure is sound, the uniformity assumption for ∥J_t‖²_F is not validated on the actual training data.
- **Low Confidence**: The claim that ResT would generalize to non-tool-use domains without modification. The methodology is tightly coupled to the reward structure and token partitioning scheme of the BFCL/API-Bank benchmarks.

## Next Checks

1. **Variance Profile Validation**: For a sample of training trajectories, compute the actual gradient variance per token and compare it to the entropy-based variance proxy (β_t ≈ 1−e^{−H_t}). Quantify the correlation and identify positions where the approximation fails.

2. **Cross-Domain Transfer Test**: Apply ResT to a non-tool-use RLHF task (e.g., dialogue summarization with sentiment reward) using the same curriculum parameters. Measure whether the fixed α_f, α_p, α_t values maintain performance, or whether they require domain-specific tuning.

3. **Jacobian Norm Analysis**: For the actual training data, estimate ∥J_t‖²_F across different token positions (format vs. reasoning) and compare to entropy values. Determine if the uniformity assumption holds, or if a position-dependent weighting would be more accurate than the current entropy-only approach.