---
ver: rpa2
title: Using a single actor to output personalized policy for different intersections
arxiv_id: '2503.07678'
source_url: https://arxiv.org/abs/2503.07678
tags:
- traffic
- network
- intersections
- each
- intersection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently scaling multi-agent
  reinforcement learning for adaptive traffic signal control across large networks
  of intersections. It identifies that while parameter sharing accelerates training,
  it fails to capture the personalized needs of intersections with different traffic
  patterns, leading to suboptimal policies.
---

# Using a single actor to output personalized policy for different intersections

## Quick Facts
- arXiv ID: 2503.07678
- Source URL: https://arxiv.org/abs/2503.07678
- Reference count: 38
- Primary result: Achieves up to 29.8% reduction in average travel time using a shared actor with hyper-action mechanism for multi-intersection adaptive traffic signal control

## Executive Summary
This paper addresses the challenge of scaling multi-agent reinforcement learning for adaptive traffic signal control across large networks of intersections. The key insight is that while parameter sharing accelerates training, it fails to capture the personalized needs of intersections with different traffic patterns. The authors propose HAMH-PPO, which uses a shared actor-critic but enables personalization through multi-head value estimation and a hyper-action mechanism that weights these values based on local observations.

## Method Summary
HAMH-PPO employs a centralized training, decentralized execution (CTDE) architecture with a shared actor-critic network. The critic uses a Graph Attention Network (GAT) to capture spatial dependencies between intersections, outputting k-dimensional value vectors per intersection. The actor produces hyper-action weights that combine these value vectors into personalized policy gradients. Training uses PPO with entropy regularization on hyper-action distributions to maintain diversity. The method is evaluated on synthetic and real-world traffic datasets, showing significant improvements over parameter-sharing and non-sharing baselines.

## Key Results
- HAMH-PPO achieves up to 29.8% reduction in average travel time compared to state-of-the-art methods
- The method maintains efficient training time comparable to parameter sharing while delivering personalization performance
- Ablation studies confirm that both the hyper-action mechanism and entropy regularization are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Personalized policies can emerge from a shared actor when guided by intersection-specific weighted combinations of multiple value functions
- Mechanism: The MH-Critic outputs a k-dimensional value vector for each intersection. The HA-Actor produces a hyper-action (probability distribution over k heads). The joint value provides personalized gradient signals for policy updates
- Core assumption: Intersections have heterogeneous traffic flow distributions that benefit from different value estimation strategies, yet share enough structure to benefit from parameter sharing
- Evidence anchors: Abstract statement about hyper-action and multi-head values enabling personalization; formal definition in Section 4.2, Eq. 7

### Mechanism 2
- Claim: Graph attention networks enable the centralized critic to capture spatial dependencies between intersections for more accurate joint value estimation
- Mechanism: Local observations are embedded and passed through multi-layer GAT, aggregating neighbor information with learned attention weights
- Core assumption: Traffic state at one intersection is influenced by neighboring intersections
- Evidence anchors: Section 4.2 description of GAT for graph representations; validation from related work like Colight

### Mechanism 3
- Claim: Entropy regularization on hyper-action distributions prevents premature collapse and maintains diversity across value function heads during training
- Mechanism: An entropy bonus is added to the PPO loss with coefficient λ, encouraging exploration across value heads
- Core assumption: Without entropy regularization, the hyper-action network may quickly converge to a single dominant head
- Evidence anchors: Loss function includes entropy term (Section 4.1, Eq. 6); ablation shows PPO+hyper+Entropy outperforms PPO+hyper alone

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: HAMH-PPO builds directly on PPO's clipped objective; understanding the clipping mechanism is essential before modifying the loss
  - Quick check question: Can you explain why PPO clips the importance sampling ratio and how this affects policy update stability?

- Concept: Parameter Sharing in MARL
  - Why needed here: The core tension is balancing parameter sharing efficiency against intersection personalization
  - Quick check question: What is the tradeoff between shared vs. independent parameters when agents face non-iid observation distributions?

- Concept: Graph Attention Networks (GAT)
  - Why needed here: The critic uses GAT to aggregate spatial information
  - Quick check question: How does GAT differ from GCN in aggregating neighbor node information?

## Architecture Onboarding

- Component map:
  - HA-Actor: Local observation → MLP embedding → GRU (temporal features) → two heads: (1) action distribution over phases, (2) hyper-action distribution over k value heads
  - MH-Critic: All local observations → shared MLP embedding → multi-layer GAT (spatial aggregation) → MLP with k output heads per intersection
  - Shared components: Embedding layer shared between actor and critic; actor and critic parameters shared across all intersections

- Critical path:
  1. Collect trajectories from all intersections using current policy
  2. Compute multi-head values via MH-Critic (requires global observation)
  3. Compute hyper-action weights via HA-Actor (local observation + intersection index)
  4. Compute joint value as dot product; derive advantage via GAE
  5. Update actor using PPO-clip loss + entropy regularization; update critic using TD error

- Design tradeoffs:
  - Hyper-action dimension k: Higher k increases personalization capacity but risks overfitting. Paper finds k≈32 optimal
  - Entropy coefficient λ: Too low allows collapse; too high prevents specialization. Default λ=0.01
  - GAT layers: More layers increase receptive field but add computation; paper uses multi-layer GAT without specifying exact depth

- Failure signatures:
  - Hyper-action collapse: If >90% of weight concentrates on one head across all intersections, personalization is failing
  - No convergence improvement over PPO-share: May indicate k too small, GAT not learning meaningful attention, or learning rate issues
  - Instability on large networks: If performance degrades on 100-intersection grids, verify GAT is scaling properly

- First 3 experiments:
  1. Replicate Figure 1c comparison on a 1×3 network: PPO-share vs. PPO-non-share vs. HAMH-PPO
  2. Ablate hyper-action: Run HAMH-PPO with k=1 (equivalent to standard shared critic)
  3. Visualize hyper-action distributions: For two intersections with different traffic flow, plot hyper-action weights over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the information content of hyper-actions be expanded to include explicit inter-intersection communication or global traffic state predictions?
- Basis in paper: The conclusion states future work will study "how to further enrich the information encompassed in hyper-action"
- Why unresolved: Current architecture limits hyper-actions to local observation history and intersection subscripts
- What evidence would resolve it: Comparative study integrating communication channel into hyper-action vector

### Open Question 2
- Question: Can HAMH-PPO maintain performance if the graph connectivity matrix G is dynamic rather than fixed?
- Basis in paper: The paper defines the connectivity matrix G as a "fixed constant matrix"
- Why unresolved: GAT relies on static adjacency structure; robustness to topological changes is unverified
- What evidence would resolve it: Experiments on networks with stochastically disabled edges

### Open Question 3
- Question: Is there a mechanism to adaptively determine the optimal dimension k for the hyper-action space based on network heterogeneity?
- Basis in paper: Ablation studies show performance is sensitive to hyper-action dimension
- Why unresolved: Fixed dimension imposes manual tuning burden
- What evidence would resolve it: Algorithm variant that dynamically adjusts k based on observed variance

## Limitations

- The optimal hyper-action dimension k is determined through manual search rather than an adaptive mechanism
- The exact GAT architecture (number of layers, attention heads) is underspecified, affecting reproducibility
- Entropy regularization coefficient sensitivity is not thoroughly analyzed across different network scales

## Confidence

- High confidence: Core mechanism (shared actor with hyper-action + multi-head critic) is clearly defined and mathematically specified
- Medium confidence: Personalization claim is well-supported within tested scenarios but generalization remains unverified
- Low confidence: External validation of the specific hyper-action mechanism is absent from the corpus

## Next Checks

1. Run HAMH-PPO with k=1 on Grid4×4 to confirm performance matches PPO-share baseline
2. Perform sensitivity analysis on entropy coefficient λ across [0.001, 0.01, 0.1] on the same scenario
3. Test on a larger network (e.g., Grid10×10) with varying traffic density to verify scalability