---
ver: rpa2
title: Context-Efficient Retrieval with Factual Decomposition
arxiv_id: '2503.19574'
source_url: https://arxiv.org/abs/2503.19574
tags:
- question
- questions
- what
- retrieval
- chunk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces FADER, a retrieval-augmented generation system
  that improves efficiency by decomposing external corpora into concise, semi-structured
  entity-description pairs (EDPs) before retrieval. Unlike traditional RAG that uses
  fixed-size text chunks, FADER generates speculative questions to guide the extraction
  of atomic facts and augments the knowledge base through multiple sampling runs.
---

# Context-Efficient Retrieval with Factual Decomposition
## Quick Facts
- arXiv ID: 2503.19574
- Source URL: https://arxiv.org/abs/2503.19574
- Reference count: 40
- Key outcome: FADER improves retrieval efficiency by decomposing corpora into concise, semi-structured entity-description pairs (EDPs) before retrieval, achieving higher accuracy with fewer retrieved tokens than traditional RAG baselines.

## Executive Summary
This work introduces FADER, a retrieval-augmented generation system that improves efficiency by decomposing external corpora into concise, semi-structured entity-description pairs (EDPs) before retrieval. Unlike traditional RAG that uses fixed-size text chunks, FADER generates speculative questions to guide the extraction of atomic facts and augments the knowledge base through multiple sampling runs. Experiments on NarrativeQA, Qasper, and QuALITY show that FADER achieves higher accuracy with fewer retrieved tokens across varying context budgets, outperforming baselines including standard retrieval, proposition decomposition, and retrieve-then-summarize methods. The method is especially effective in short-context regimes, reducing LLM inference costs while maintaining strong performance.

## Method Summary
FADER improves retrieval-augmented generation by transforming external corpora into concise, semi-structured entity-description pairs (EDPs) prior to retrieval. Instead of using fixed-size text chunks as in traditional RAG, FADER generates speculative questions to guide the extraction of atomic facts from documents. This process creates a more structured and efficient knowledge base, which is further augmented through multiple sampling runs. During retrieval, FADER leverages these EDPs to return more targeted, relevant information, leading to improved accuracy with fewer retrieved tokens. The approach is evaluated across multiple benchmarks, showing strong performance particularly in short-context regimes where inference costs are reduced.

## Key Results
- FADER achieves higher accuracy than standard retrieval, proposition decomposition, and retrieve-then-summarize methods on NarrativeQA, Qasper, and QuALITY benchmarks.
- FADER consistently retrieves fewer tokens while maintaining or improving performance, especially in short-context regimes.
- Ablation studies confirm that both speculative question generation and knowledge base augmentation are critical to FADER's gains.

## Why This Works (Mechanism)
FADER improves efficiency by decomposing knowledge into atomic, semi-structured facts (EDPs), enabling targeted retrieval and reducing token usage. Speculative question generation helps extract the most relevant facts, while knowledge base augmentation increases coverage. The approach shifts retrieval from unstructured chunking to a more structured, fact-based representation, which is particularly advantageous in short-context settings.

## Foundational Learning
- **Entity-Description Pairs (EDPs):** Semi-structured representation linking atomic facts to concise descriptions. Needed to enable targeted retrieval and reduce noise in retrieved contexts. Quick check: Ensure each EDP captures a single, self-contained fact relevant to the task.
- **Speculative Question Generation:** Method to generate questions that guide fact extraction from documents. Needed to identify and extract the most salient atomic facts. Quick check: Verify generated questions align closely with the most important facts in the source text.
- **Knowledge Base Augmentation:** Process of expanding the EDP knowledge base through multiple sampling runs. Needed to improve coverage and robustness of retrieved information. Quick check: Compare coverage and diversity of EDPs before and after augmentation.
- **Retrieval-augmented Generation (RAG):** Framework for augmenting LLM generation with retrieved external knowledge. Needed as the baseline paradigm FADER improves upon. Quick check: Confirm that retrieved contexts are used effectively to inform LLM generation.
- **Context Efficiency:** Measure of how well retrieved information supports LLM performance relative to the number of tokens used. Needed to quantify FADER's efficiency gains. Quick check: Compare accuracy versus retrieved token count across methods.
- **Factual Decomposition:** Process of breaking down documents into atomic facts. Needed to create structured knowledge for efficient retrieval. Quick check: Ensure decomposition preserves all essential information without introducing redundancy.

## Architecture Onboarding
- **Component Map:** Document -> Speculative Questions -> Fact Extraction -> EDPs -> Knowledge Base Augmentation -> Retrieval Engine -> LLM
- **Critical Path:** Speculative question generation → fact extraction → EDP creation → retrieval → LLM generation
- **Design Tradeoffs:** Structured EDPs improve retrieval precision but require upfront decomposition cost; speculative questions add extraction overhead but improve fact relevance.
- **Failure Signatures:** Poor question generation leads to missing key facts; overly aggressive decomposition loses contextual nuance; retrieval engine may miss relevant EDPs due to representation mismatch.
- **First Experiments:** (1) Test EDP creation quality on a small document set and verify fact completeness; (2) Evaluate speculative question generation against human annotations for relevance; (3) Measure retrieval accuracy using only EDPs versus traditional chunking on a sample corpus.

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements depend heavily on speculative question generation quality, which may vary across domains and tasks, limiting generalizability.
- Experimental validation is limited to three benchmarks; performance on more diverse, real-world datasets is uncertain.
- The assumption that atomic facts extracted via speculation are always the most useful for downstream tasks may not hold in cases requiring broader contextual understanding.

## Confidence
- Retrieval efficiency gains: High
- Sustained accuracy with larger corpora: Medium
- Effectiveness of multi-run augmentation under resource constraints: Medium

## Next Checks
- Test FADER on a broader set of domains, including non-narrative QA and domain-specific corpora, to assess generalizability.
- Measure retrieval and inference time per token across different context budgets to quantify efficiency gains in practice.
- Conduct ablation studies removing speculative questions to quantify their exact contribution to performance improvements.