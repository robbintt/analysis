---
ver: rpa2
title: A Translation of Probabilistic Event Calculus into Markov Decision Processes
arxiv_id: '2507.12989'
source_url: https://arxiv.org/abs/2507.12989
tags:
- state
- time
- fluent
- actions
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a formal translation of Probabilistic Event
  Calculus (PEC) into Markov Decision Processes (MDPs), creating the PEC-MDP framework.
  The key contribution is enabling PEC's interpretable narrative reasoning capabilities
  to leverage MDP's extensive reinforcement learning tools while preserving PEC's
  flexible action semantics through "action-taking situations." The translation includes
  numerical encoding schemes, transition functions, and non-stationary policies that
  map PEC's probabilistic narratives to MDP dynamics.
---

# A Translation of Probabilistic Event Calculus into Markov Decision Processes

## Quick Facts
- arXiv ID: 2507.12989
- Source URL: https://arxiv.org/abs/2507.12989
- Authors: Lyris Xu; Fabio Aurelio D'Asaro; Luke Dickens
- Reference count: 19
- One-line primary result: PEC-MDP framework enables PEC's narrative reasoning to leverage MDP reinforcement learning tools while preserving interpretability

## Executive Summary
This paper introduces PEC-MDP, a formal framework that translates Probabilistic Event Calculus (PEC) into Markov Decision Processes (MDPs). By bridging these two paradigms, the authors enable PEC's human-readable narrative reasoning capabilities to access MDP's extensive reinforcement learning toolkit. The translation preserves PEC's flexible action semantics through "action-taking situations" while converting probabilistic narratives into MDP dynamics. A Python implementation demonstrates successful application to a logistics domain, showing how PEC can now support both narrative reasoning and goal-directed behavior optimization.

## Method Summary
The PEC-MDP framework establishes a systematic translation between PEC's probabilistic event calculus and MDP formalism. The authors develop numerical encoding schemes to represent PEC fluents and events as MDP states and actions, construct transition functions that capture PEC's probabilistic dynamics, and design non-stationary policies that map PEC's narrative structure to MDP decision-making. The framework supports temporal projection queries and objective-directed planning, with bidirectional conversion capabilities allowing policies learned through MDP solvers to be translated back into human-readable PEC narratives. The approach maintains PEC's interpretability while leveraging MDP's computational advantages for planning and learning.

## Key Results
- Formal translation framework enabling PEC narratives to be processed by MDP reinforcement learning algorithms
- Preservation of PEC's interpretable action semantics through action-taking situation encoding
- Successful demonstration in logistics domain with bidirectional conversion between learned policies and PEC representations
- Integration of temporal projection and goal-directed planning capabilities within unified framework

## Why This Works (Mechanism)
The framework works by creating a precise mathematical mapping between PEC's narrative reasoning constructs and MDP's state-action-reward structure. PEC's probabilistic event calculus, which models actions and their effects through situations and causal relationships, is encoded into MDP states representing fluent valuations and actions representing events. The transition function captures PEC's probabilistic dynamics by computing state transitions based on event preconditions and effects. Non-stationary policies encode the temporal structure of PEC narratives, allowing MDP solvers to optimize behavior sequences that correspond to meaningful stories in PEC. This translation preserves semantic meaning while enabling computational optimization.

## Foundational Learning

**Markov Decision Processes**: Why needed - Provides computational framework for sequential decision making under uncertainty. Quick check - States, actions, transition probabilities, reward function, and policy definition.

**Probabilistic Event Calculus**: Why needed - Offers interpretable framework for reasoning about actions and their effects in narratives. Quick check - Situations, fluents, events, and causal relationships.

**Action-Taking Situations**: Why needed - Enables flexible representation of actions without fixed preconditions in PEC. Quick check - Mapping between situations and action execution contexts.

**Non-Stationary Policies**: Why needed - Captures temporal evolution of PEC narratives in MDP framework. Quick check - Policy dependency on time step and current state.

**Fluent Encoding**: Why needed - Converts PEC's logical fluents into MDP state representations. Quick check - Binary encoding of fluent truth values.

**Policy Conversion**: Why needed - Enables translation of learned MDP policies back to interpretable PEC narratives. Quick check - Reverse mapping from state-action sequences to PEC situations and events.

## Architecture Onboarding

**Component Map**: PEC Narratives -> Encoding Module -> MDP Solver -> Policy Learning -> Conversion Module -> Interpretable PEC Actions

**Critical Path**: The critical path flows from PEC narrative specification through encoding, MDP solution, and policy conversion. The encoding module must correctly represent all relevant fluents and events, the MDP solver must find optimal policies within computational constraints, and the conversion module must preserve semantic meaning when translating back to PEC.

**Design Tradeoffs**: The framework balances interpretability against computational efficiency. Richer PEC representations improve narrative expressiveness but increase MDP state space complexity. The choice of encoding scheme affects both solution quality and conversion accuracy. Non-stationary policies better capture PEC's temporal structure but require more computational resources than stationary alternatives.

**Failure Signatures**: Common failure modes include state space explosion from complex PEC narratives, loss of semantic meaning during encoding, suboptimal policies from incomplete PEC specification, and conversion errors that produce uninterpretable PEC narratives. Debugging typically requires examining each translation stage separately.

**First Experiments**:
1. Translate simple PEC narrative (single fluent, two actions) to MDP and verify correct policy behavior
2. Apply framework to logistics domain with multiple fluents and actions, comparing PEC and MDP solutions
3. Test bidirectional conversion by learning MDP policy then converting back to PEC and verifying semantic preservation

## Open Questions the Paper Calls Out

None

## Limitations

- Computational scalability challenges due to MDP state space explosion in complex domains
- Assumption of complete knowledge of initial conditions and action preconditions
- Limited to discrete time steps and finite action spaces, restricting continuous-time applications
- Performance dependency on quality of underlying MDP solver algorithms

## Confidence

**High Confidence**: Formal correctness of PEC-to-MDP translation and preservation of probabilistic semantics
**Medium Confidence**: Practical effectiveness of learned policies when converted back to PEC narratives
**Low Confidence**: Performance guarantees in highly dynamic environments with incomplete information

## Next Checks

1. **Scalability Testing**: Implement framework on larger, more complex domains (e.g., multi-agent logistics with hundreds of variables) to identify performance bottlenecks and evaluate state space reduction techniques.

2. **Robustness Evaluation**: Test framework under partial observability and noisy observations to assess how well learned policies generalize when complete state information assumption is violated.

3. **Human Interpretability Assessment**: Conduct user studies comparing interpretability of PEC narratives generated from learned policies versus traditional planning approaches, measuring comprehension time and accuracy.