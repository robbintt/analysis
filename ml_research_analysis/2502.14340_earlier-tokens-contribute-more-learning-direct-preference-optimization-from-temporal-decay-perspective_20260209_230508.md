---
ver: rpa2
title: 'Earlier Tokens Contribute More: Learning Direct Preference Optimization From
  Temporal Decay Perspective'
arxiv_id: '2502.14340'
source_url: https://arxiv.org/abs/2502.14340
tags:
- d2po
- decay
- preference
- optimization
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the length bias problem in Direct Preference\
  \ Optimization (DPO), where models tend to generate longer responses than reference\
  \ models. The authors propose D2PO, which introduces a temporal decay factor \u03B3\
  \ that assigns higher weights to earlier tokens in the sequence during training."
---

# Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective

## Quick Facts
- arXiv ID: 2502.14340
- Source URL: https://arxiv.org/abs/2502.14340
- Authors: Ruichen Shao; Bei Li; Gangao Liu; Yang Chen; Xiang Zhou; Jingang Wang; Xunliang Cai; Peng Li
- Reference count: 37
- Primary result: D2PO addresses length bias in DPO by introducing temporal decay, achieving 5.9-8.8 points improvement on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard

## Executive Summary
This paper addresses the length bias problem in Direct Preference Optimization (DPO), where models tend to generate longer responses than reference models. The authors propose D2PO, which introduces a temporal decay factor γ that assigns higher weights to earlier tokens in the sequence during training. This mechanism prioritizes tokens that contribute more to alignment while mitigating overfitting to less relevant data. Experiments on multiple benchmarks show D2PO consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures. The method also maintains performance on mathematical reasoning tasks and can be extended to reference-free settings, achieving competitive results.

## Method Summary
The proposed D2PO method introduces a temporal decay factor γ that reweights the importance of tokens during training. This decay mechanism assigns higher weights to earlier tokens in the sequence, based on the intuition that initial tokens have greater influence on the overall response quality. The decay factor is applied to the likelihood scores of tokens, creating a weighted sum where tokens closer to the beginning of the sequence contribute more to the optimization objective. This approach aims to address the length bias in standard DPO while maintaining the model's ability to generate coherent and helpful responses. The method can be applied to both pairwise and reference-free preference optimization settings.

## Key Results
- D2PO outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 benchmark
- D2PO achieves 3.3-9.7 points improvement on Arena-Hard benchmark
- Method maintains performance on mathematical reasoning tasks (GSM8K, MATH)
- Successfully extends to reference-free DPO settings on StackOverflow dataset

## Why This Works (Mechanism)

D2PO works by addressing the fundamental length bias in DPO through temporal weighting. In standard DPO, all tokens contribute equally to the loss function regardless of their position in the sequence. This creates an incentive for models to generate longer responses, as more tokens mean more opportunities to accumulate positive likelihood scores. By introducing temporal decay, D2PO effectively reduces the influence of later tokens that may contribute less to the overall response quality or alignment. The decay factor γ controls the rate at which token importance decreases over the sequence, allowing for fine-tuned control over the optimization process.

## Foundational Learning

1. **Direct Preference Optimization (DPO)**: A preference learning method that optimizes language models based on pairwise comparisons without explicit reward modeling. Needed to understand the baseline method being improved. Quick check: Verify DPO loss formulation and pairwise comparison mechanism.

2. **Temporal decay in sequence modeling**: The concept of weighting sequence elements based on their position, commonly used in various NLP applications. Needed to understand how position-based weighting affects model training. Quick check: Review decay functions and their impact on sequence processing.

3. **Length bias in language models**: The tendency of models to generate longer responses due to optimization incentives. Needed to understand the specific problem D2PO addresses. Quick check: Analyze how token count affects loss in standard training.

4. **Preference optimization objectives**: The mathematical formulation of how models learn from preference data. Needed to understand how D2PO modifies the standard objective. Quick check: Compare standard and decayed preference objectives.

## Architecture Onboarding

**Component map**: Input sequences -> Temporal decay weighting -> Modified likelihood scores -> Preference optimization loss

**Critical path**: The temporal decay mechanism directly modifies the likelihood scores of tokens before they are used in the preference optimization loss calculation. This modification occurs at each training step and affects how the model updates its parameters based on preference data.

**Design tradeoffs**: The decay factor γ represents the primary design tradeoff - higher values give more weight to later tokens but may reintroduce length bias, while lower values strongly favor earlier tokens but may lose important information from later parts of responses. The method trades off between addressing length bias and preserving complete information from preference pairs.

**Failure signatures**: 
- Too high γ values: Model reverts to length bias behavior
- Too low γ values: Model may ignore important context from later tokens
- Improper γ tuning: Suboptimal performance on alignment metrics

**First experiments to run**:
1. Ablation study with γ values ranging from 0.1 to 0.9 to find optimal decay rate
2. Comparison of training/validation loss curves between D2PO and standard DPO
3. Analysis of generated response lengths across different γ settings

## Open Questions the Paper Calls Out

None

## Limitations

- The root cause of length bias (training data distribution vs. optimization dynamics) is not definitively established
- Limited ablation studies for γ values make optimal decay factor unclear for different tasks
- Extension to reference-free DPO evaluated only on single dataset (StackOverflow)
- Focus primarily on instruction following and helpfulness metrics, with limited evaluation on other alignment dimensions

## Confidence

- **High confidence**: D2PO outperforms vanilla DPO on AlpacaEval 2 and Arena-Hard benchmarks (measured improvements of 5.9-8.8 and 3.3-9.7 points respectively)
- **Medium confidence**: D2PO maintains performance on mathematical reasoning tasks (based on GSM8K and MATH evaluations)
- **Medium confidence**: D2PO's effectiveness in reference-free settings (limited to single dataset validation)

## Next Checks

1. **Decay factor sensitivity analysis**: Conduct systematic ablation studies across a wider range of γ values (0.1 to 0.9) and model scales to determine optimal settings and verify that improvements aren't specific to the chosen value

2. **Overfitting validation**: Measure validation loss trajectories and compute overfitting metrics (e.g., gap between training and validation performance) to directly verify the claim that temporal decay mitigates overfitting

3. **Generalization to other alignment objectives**: Evaluate D2PO on safety alignment tasks, factuality benchmarks, and different model architectures (beyond Llama and Mistral families) to assess broader applicability beyond instruction following