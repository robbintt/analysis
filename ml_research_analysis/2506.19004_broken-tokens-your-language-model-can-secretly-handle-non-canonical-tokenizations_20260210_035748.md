---
ver: rpa2
title: Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations
arxiv_id: '2506.19004'
source_url: https://arxiv.org/abs/2506.19004
tags:
- tokenization
- answer
- should
- language
- non-canonical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of language models (LMs)
  to non-canonical tokenizations of text that were unseen during training. Surprisingly,
  instruction-tuned models retain up to 93.4% performance under random tokenization
  and 90.8% under character-level tokenization across 20 benchmarks.
---

# Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations

## Quick Facts
- **arXiv ID:** 2506.19004
- **Source URL:** https://arxiv.org/abs/2506.19004
- **Authors:** Brian Siyuan Zheng; Alisa Liu; Orevaoghene Ahia; Jonathan Hayase; Yejin Choi; Noah A. Smith
- **Reference count:** 40
- **Primary result:** Instruction-tuned models retain up to 93.4% performance under random tokenization and 90.8% under character-level tokenization across 20 benchmarks.

## Executive Summary
This paper investigates the robustness of language models to non-canonical tokenizations that were unseen during training. Surprisingly, instruction-tuned models retain most of their performance under random and character-level tokenizations, with stronger models showing greater robustness. The authors identify that this capability emerges during the instruction-tuning phase rather than pretraining, where models learn to produce fluent responses regardless of input formatting. Base models, while understanding non-canonical forms, mimic perceived misspellings and produce nonsensical output. The findings suggest LMs are less tied to their tokenizer than previously believed and demonstrate the potential of dynamically optimizing tokenization at inference time.

## Method Summary
The authors evaluate robustness to non-canonical tokenizations by testing instruction-tuned models (Llama-3.1-8B, Qwen-2.5-7B, OLMo-2-7B) on 20 benchmarks using both canonical and non-canonical tokenizations. Random tokenizations are generated via recursive splitting with weighted sampling, while character-level tokenization uses single-character tokens. They also design four tasks to evaluate domain-specific benefits: Counting Characters (1001 samples), Acronyms (3594 samples), Code Description (4800 samples), and Arithmetic (1000 samples). Performance retention is measured as the percentage of canonical accuracy retained, with additional metrics including spelling score, grammaticality, and win rate via AlpacaEval GPT-4 judge.

## Key Results
- Instruction-tuned models retain up to 93.4% performance under random tokenization and 90.8% under character-level tokenization
- Stronger base models (Qwen-2.5 > Llama-3.1 > OLMo-2) show greater robustness to non-canonical tokenizations
- Character-level tokenization improves string manipulation and code understanding by up to +14%, and right-aligned digit grouping enhances arithmetic by +33%
- Robustness emerges during instruction-tuning; base models degenerate into nonsensical output when given non-canonical tokenization

## Why This Works (Mechanism)

### Mechanism 1: Implicit Character-Level Encoding in Token Embeddings
- **Claim:** Larger instruction-tuned models develop robust character-level representations within their token embeddings, enabling composition of arbitrary token sequences including non-canonical ones.
- **Mechanism:** During pretraining, word variants that don't share tokens create pressure for the model to learn spelling relationships as a general solution. This builds an "implicit vocabulary" that can compose unfamiliar token sequences into meaningful representations.
- **Core assumption:** Token embeddings in sufficiently capable models encode sub-token character information, allowing semantic understanding even when token boundaries are altered.
- **Evidence anchors:**
  - [abstract] "instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization"
  - [section 5] "token embeddings have been found to robustly encode character-level information, especially in larger models... word variants that do not share tokens in common incentivize the model to learn spelling as a general solution"
  - [corpus] Weak direct evidence—corpus neighbors focus on tokenization effects but don't address character encoding mechanisms
- **Break condition:** Smaller models or those with limited capacity may not develop sufficient character-level encoding; performance retention correlates with base model capability (Qwen-2.5: 93.4% > Llama-3.1: 87.7% > OLMo-2: 73.1%)

### Mechanism 2: Instruction-Tuning Decouples Understanding from Mimicry
- **Claim:** Robustness to non-canonical tokenization emerges specifically during supervised fine-tuning (SFT), not pretraining, because SFT trains models to provide fluent responses independent of input formatting.
- **Mechanism:** Base models perceive non-canonical tokenizations as containing "misspellings" and attempt to continue in the same corrupted style, producing degenerate output. Instruction-tuning with separated conversation turns teaches models that their role is to respond helpfully regardless of input surface form. The separation of instruction and response as distinct turns (with special delimiter tokens) is the critical training structure.
- **Core assumption:** Both base and instruct models semantically understand non-canonical forms equally well—the difference is behavioral (mimicry vs. helpful response generation).
- **Evidence anchors:**
  - [abstract] "robustness arises during the instruction-tuning phase... base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses"
  - [section 4.1] Base models score ≤0.317 on spelling and ≤0.260 on grammaticality with character-level input; post-trained models show "much of the improvement coming from the SFT stage alone"
  - [section 4.2] Ablation: removing chat template drops spelling score from 0.786 to 0.0698; "the separation of the context and expected continuation—as different turns of dialogue demarcated with a special token—is key to robustness"
  - [corpus] Not directly addressed in corpus neighbors
- **Break condition:** Training without turn separation or special tokens fails to produce robustness even with identical data content; directly concatenating instruction-response pairs breaks the effect

### Mechanism 3: Granularity-Performance Tradeoff with Domain-Specific Optimization
- **Claim:** Performance degrades smoothly as tokenization granularity increases, but strategically chosen non-canonical schemes can outperform canonical tokenization for specific task types by better aligning token boundaries with task-relevant structure.
- **Mechanism:** Character-level tokenization exposes orthographic information obscured by subword merging, benefiting tasks requiring character-level reasoning (string manipulation, code understanding). Right-aligned digit grouping aligns number tokens with place-value structure (thousands, millions), improving arithmetic. However, excessive granularity increases sequence length and moves further from training distribution, causing general performance decline.
- **Core assumption:** Models can dynamically adapt to different tokenization schemes at inference time without weight updates, and the optimal scheme varies by task domain.
- **Evidence anchors:**
  - [abstract] "character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%"
  - [section 2.4] "finer-grained tokenization generally leading to worse performance... negative correlation is statistically significant under Kendall's τ with p = 0.003"
  - [section 3.2] Alternative schemes: Counting Characters +6.99%, Acronyms +7.74%, Code Description +14.3%, Arithmetic +33.7%
  - [corpus] "Broken Words, Broken Performance" neighbor paper discusses tokenization effects on performance but from different angle
- **Break condition:** Character-level tokenization on Chinese text (3 bytes per character under UTF-8) shows only ~3% drop, suggesting the mechanism may not fully transfer to non-ASCII scripts; extreme granularity (>5x canonical length) likely causes severe degradation

## Foundational Learning

- **Concept: Byte-Pair Encoding (BPE) Tokenization and Non-Canonical Variants**
  - **Why needed here:** Understanding that BPE deterministically produces one "canonical" tokenization via ordered merge rules, but the same vocabulary can represent text in exponentially many alternative segmentations
  - **Quick check question:** Given vocabulary {a, b, c, ab, bc, abc}, what are three valid tokenizations of "abc"? (Answer: [abc], [ab, c], [a, bc], [a, b, c])

- **Concept: The Base → SFT → DPO → Instruct Training Pipeline**
  - **Why needed here:** Robustness emerges at the SFT stage specifically, not during pretraining or later DPO—understanding this pipeline is essential for reproducing or modifying the effect
  - **Quick check question:** At which training stage does the paper identify robustness to non-canonical tokenization as emerging? (Answer: Supervised Fine-Tuning / SFT)

- **Concept: Conversational Turn Structure with Special Tokens**
  - **Why needed here:** The paper's ablations show that turn separation with special delimiter tokens is necessary for robustness—removing this structure causes base-model-style failures
  - **Quick check question:** What training data format change eliminates robustness: (a) using QA template instead of chat template, or (b) concatenating instruction and response without delimiters? (Answer: b)

## Architecture Onboarding

- **Component map:**
  Input Text -> [Tokenizer - BPE with merge rules] -> Canonical Tokenization (deterministic) <-> Non-Canonical Variants (random/character-level/domain-specific) -> [Token Embeddings] -> [Transformer Layers] -> [Output Generation] (Base: mimicry vs Instruct: fluent response)

- **Critical path:**
  1. Generate non-canonical tokenization using vocabulary-valid segmentation (random via recursive splitting with weighted sampling, or deterministic character-level, or domain-specific like right-aligned digits)
  2. Pass to instruction-tuned model without any weight modification
  3. Model processes via implicit character composition in embeddings
  4. Generates fluent output regardless of input tokenization style

- **Design tradeoffs:**
  - **Granularity vs. General Performance:** Higher length ratios (finer granularity) correlate with lower retention—average retention drops as tokenization length ratio increases
  - **Domain Optimization vs. Cross-Task Robustness:** Right-aligned digits help arithmetic but may hurt other tasks; character-level helps code/string tasks but reduces efficiency
  - **Random vs. Deterministic Schemes:** Random sampling provides test of general robustness; domain-specific schemes require task knowledge but yield larger gains
  - **Model Size vs. Robustness:** Stronger models (Qwen-2.5 > Llama-3.1 > OLMo-2) show higher retention, suggesting capacity requirements

- **Failure signatures:**
  - **Base model with non-canonical input:** Generates text with repeated characters, odd substitutions ("Yoou", "haviin"), sometimes relevant but incoherent—spelling score ≤0.317, grammaticality ≤0.260
  - **Instruct model without chat template:** Same failure mode as base model—spelling drops to 0.0698
  - **Excessive granularity:** Performance degrades smoothly; length ratio >3x canonical shows measurably lower retention
  - **Model perception of misspellings:** Both base and instruct models perform at chance (~48-56%) when asked to distinguish non-canonical tokenization from actual misspellings—they interpret unusual tokenization as typos

- **First 3 experiments:**
  1. **Baseline robustness test:** Take an instruction-tuned model (e.g., Llama-3.1-8B-Instruct), evaluate on 5-10 benchmarks with canonical vs. character-level tokenization, compute retention percentage—expect ~80-95% retention depending on model strength
  2. **Base vs. SFT comparison:** Take a model family with released checkpoints (e.g., OLMo-2 or TULU-3), evaluate base and SFT stages on AlpacaEval with character-level tokenization, measure spelling/grammaticality/win-rate metrics—expect base to fail (spelling <0.35), SFT to succeed (spelling >0.7)
  3. **Domain-specific tokenization gain:** On arithmetic task with 10-digit numbers, compare canonical (left-aligned: ["100", "000", "0"]) vs. right-aligned (["1", "000", "000"]) digit grouping—expect +20-35% accuracy improvement without any fine-tuning

## Open Questions the Paper Calls Out
- **Question:** How can we develop algorithms to automatically identify the optimal tokenization scheme for a given task or input at inference time?
  - **Basis in paper:** [explicit] Section 3.2 states, "We leave automatically identifying the optimal tokenization as a promising direction for future work."
  - **Why unresolved:** The authors manually selected schemes based on intuition (e.g., right-aligned digits for arithmetic) rather than an automated search process.
  - **What evidence would resolve it:** An algorithm capable of dynamically selecting tokenizations that maximizes downstream task accuracy compared to canonical baselines.

- **Question:** Does the robustness to non-canonical tokenization scale to significantly larger models (e.g., 70B+) or different architectures?
  - **Basis in paper:** [inferred] The experimental scope is limited to 7B-8B parameter models (Llama, Qwen, OLMo).
  - **Why unresolved:** The paper posits that stronger models are more robust, but it is unclear if this trend continues linearly or if the mechanisms change at the frontier scale.
  - **What evidence would resolve it:** Evaluation of 70B+ parameter models or non-Transformer architectures on the non-canonical setups described in the paper.

- **Question:** Can models be explicitly trained to utilize non-canonical tokenization for performance gains, rather than just acquiring robustness as a byproduct of instruction tuning?
  - **Basis in paper:** [inferred] The paper analyzes robustness as an emergent property of standard SFT, but does not explore targeted training interventions.
  - **Why unresolved:** Current models tolerate non-canonical forms; it is unknown if specialized training could teach them to actively leverage these representations for superior reasoning.
  - **What evidence would resolve it:** Fine-tuning experiments using tokenization augmentation (e.g., BPE-dropout) showing significant performance boosts over standard instruction-tuned models.

## Limitations
- The claim that robustness emerges specifically from instruction-tuning is supported by ablation studies but lacks systematic comparison across different model families and sizes at intermediate training stages.
- While strong performance retention is demonstrated, these results come from evaluating on English text only, with limited testing on Chinese text showing only ~3% drop.
- The assertion that models can handle arbitrary non-canonical tokenizations at inference time is compelling but untested for extreme cases where performance might collapse entirely.

## Confidence
- **High Confidence** - Performance retention numbers and benchmark results are well-supported by the experimental methodology and multiple model comparisons. The statistical significance (p=0.003 for Kendall's τ correlation) is properly established.
- **Medium Confidence** - The instruction-tuning mechanism explanation and base model vs. instruct model behavioral differences are well-demonstrated through controlled ablations, though the exact training dynamics remain somewhat unclear.
- **Low Confidence** - Claims about sub-token character encoding and the generality of robustness across all tokenization schemes are based on indirect evidence and related work rather than direct measurements in this study.

## Next Checks
1. **Intermediate Training Stage Analysis:** Evaluate base, SFT, DPO, and instruct checkpoints of the same model family on character-level tokenization tasks to definitively establish when robustness emerges in the training pipeline. This would confirm whether SFT alone is sufficient or if DPO contributes to the effect.

2. **Extreme Granularity Stress Test:** Systematically evaluate performance as tokenization length ratio increases beyond 3x canonical (e.g., 5x, 10x) to identify the breaking point where robustness fails completely. This would validate the claim that performance degrades smoothly versus abruptly.

3. **Cross-Lingustic Generalization Test:** Extend the non-canonical tokenization evaluation to multiple language families (Arabic, Japanese, Russian) with varying character encodings and tokenization challenges to verify the universality of the robustness mechanism beyond English and Chinese.