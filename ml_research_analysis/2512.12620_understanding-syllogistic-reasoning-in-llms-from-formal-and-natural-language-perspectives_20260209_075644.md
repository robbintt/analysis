---
ver: rpa2
title: Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language
  Perspectives
arxiv_id: '2512.12620'
source_url: https://arxiv.org/abs/2512.12620
tags:
- reasoning
- bias
- language
- accuracy
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates syllogistic reasoning capabilities across
  14 large language models, introducing a dual ground truth framework assessing both
  logical validity and natural language believability. We find that top-tier models
  achieve near-perfect syntactic accuracy (99.6%) while struggling with semantic plausibility
  judgments (52% NLU accuracy), exhibiting an accuracy gap of 25.5 percentage points.
---

# Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives

## Quick Facts
- arXiv ID: 2512.12620
- Source URL: https://arxiv.org/abs/2512.12620
- Reference count: 7
- Key outcome: Top-tier models achieve near-perfect syntactic accuracy (99.6%) while struggling with semantic plausibility judgments (52% NLU accuracy), exhibiting a 25.5 percentage point accuracy gap.

## Executive Summary
This study evaluates syllogistic reasoning capabilities across 14 large language models using a dual ground truth framework that assesses both logical validity and natural language believability. The research reveals that while top-tier models excel at formal syntactic reasoning (achieving 99.6% accuracy), they struggle with semantic plausibility judgments (52% NLU accuracy), creating an accuracy gap of 25.5 percentage points. Counterintuitively, few-shot prompting degrades performance compared to zero-shot approaches, and consistency across content variants strongly correlates with reasoning accuracy (ρ=0.890). The findings suggest LLMs are evolving toward formal reasoning engines rather than human-like reasoners susceptible to natural language biases.

## Method Summary
The study constructs 160 syllogisms (40 base + 3 variants: nonsense, order-switched, combined) with dual ground truth annotations for validity and believability. It evaluates 14 LLMs across 4 prompting strategies (Zero-Shot, One-Shot, Few-Shot with 4 examples, Zero-Shot CoT) and 3 temperatures (0.0, 0.5, 1.0). For τ>0, self-consistency aggregation with up to 10 samples and early stopping at 5 unanimous responses is used. Primary metrics include syntax accuracy, NLU accuracy, belief bias (Δ_bias = Acc_congruent - Acc_incongruent), and consistency across variants. Code is available at https://github.com/XAheli/Logic-in-LLMs.

## Key Results
- Top-tier models achieve near-perfect syntactic accuracy (99.6%) while struggling with semantic plausibility judgments (52% NLU accuracy)
- Few-shot prompting yields lower mean accuracy (79.1%) compared to zero-shot (82.7%), with significant error redistribution (McNemar χ²=42.88, p<0.0001)
- Majority of models show positive belief bias (+10.81 pp), performing better when logic aligns with intuition
- Strong correlation (ρ=0.890) between accuracy and cross-variant consistency, indicating abstract reasoning usage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs develop formal syntactic reasoning capabilities that operate independently from natural language semantic processing, creating a capability gap opposite to human reasoning patterns.
- **Mechanism:** Models trained on diverse corpora may internalize logical form as an abstract pattern-matching substrate separate from world knowledge representation. When presented with syllogisms, high-performing models apply syntactic validity rules to premise-conclusion structures while treating semantic content as orthogonal signal.
- **Core assumption:** Syntactic reasoning circuits and semantic plausibility circuits develop as partially separable computational pathways during pre-training, with instruction tuning potentially strengthening the former at the expense of human-like integration.
- **Evidence anchors:**
  - [abstract]: "Top-tier models achieve near-perfect syntactic accuracy (99.6%) while struggling with semantic plausibility judgments (52% NLU accuracy)"
  - [Section 4.1]: "Syntax accuracy (81.7%) substantially exceeds NLU accuracy (56.2%)... Top-tier models show large syntax-NLU gaps: Gemini 2.5 Flash (47.9 pp)"
  - [corpus]: PEIRCE paper explicitly addresses "the effective integration of material and formal inference" as a "persistent challenge"

### Mechanism 2
- **Claim:** Belief bias susceptibility decreases systematically as formal reasoning capability improves, suggesting higher-performing models learn to prioritize abstract logical rules over semantic heuristics.
- **Mechanism:** Lower-capability models lack robust formal reasoning circuits and default to semantic plausibility as a heuristic shortcut when logical evaluation fails. Higher-capability models develop stronger pattern-internalized logical schemas that override semantic interference on incongruent problems.
- **Core assumption:** Belief bias in LLMs reflects computational resource allocation rather than fundamental architectural limitation; as models acquire more reliable logical pattern-matching, they allocate less computation to semantic shortcut strategies.
- **Evidence anchors:**
  - [Section 4.3]: "Twelve of 14 models exhibit positive belief bias... mean bias effect is +10.81pp (SD=16.32), statistically significant"
  - [Section 4.5]: "Negative correlation (ρ=−0.565, p=0.0353)... indicates that higher reasoning ability reduces reliance on content-based heuristics"
  - [corpus]: "Mitigating Content Effects on Reasoning" paper (arXiv:2505.12189) addresses how LLMs "conflating content plausibility with logical validity" can be mitigated

### Mechanism 3
- **Claim:** Few-shot prompting degrades formal reasoning performance because demonstration examples introduce task-irrelevant variance that interferes with abstract rule application.
- **Mechanism:** Zero-shot prompts elicit direct application of internalized logical patterns. Few-shot examples add demonstration instances that may prime content-specific reasoning rather than abstract pattern-matching, introduce noise through example selection bias, or cause models to match surface features of examples rather than underlying logical structure.
- **Core assumption:** Formal reasoning tasks benefit from direct rule invocation rather than analogical reasoning from examples; the "less is more" pattern suggests demonstrations may compete with abstract pattern retrieval.
- **Evidence anchors:**
  - [Section 4.2]: "FS prompting yields the lowest mean accuracy (79.1%), while ZS achieves 82.7%... FS significantly underperforms ZS (Δ=−3.57pp, p=0.0165)"
  - [Section 4.2]: "ZS solves 786 instances that FS fails, while FS solves only 546 that ZS fails"
  - [corpus]: Related work on CoT improvements (arXiv:2502.12616) addresses content biases in reasoning explanations

## Foundational Learning

- **Concept: Syllogistic Validity vs. Believability**
  - **Why needed here:** The paper's central methodological innovation is dual ground truth annotation. Without distinguishing "does the conclusion logically follow?" from "is the conclusion true?", the observed performance patterns would be uninterpretable.
  - **Quick check question:** Given "All birds can fly; Penguins are birds; Therefore penguins can fly"—is this valid? Is it believable? (Answer: Valid=yes, Believable=no)

- **Concept: Belief Bias in Reasoning**
  - **Why needed here:** The paper positions itself relative to cognitive psychology literature on human reasoning fallibility. Understanding that humans reliably accept invalid-but-plausible conclusions provides the baseline against which LLM behavior is measured as "opposite to human reasoning tendencies."
  - **Quick check question:** Why might a reasoner accept "All flowers need water; Roses need water; Therefore roses are flowers" as valid? (Answer: The conclusion is believable, creating interference with logical evaluation—this is affirming the consequent, which is formally invalid)

- **Concept: Consistency as Capability Indicator**
  - **Why needed here:** The strong correlation (ρ=0.890) between accuracy and cross-variant consistency is presented as evidence that high performers use abstract reasoning rather than surface features. This interpretive frame is essential for understanding why nonsense variants matter.
  - **Quick check question:** If a model answers correctly on "All calculators are machines; All computers are calculators; Therefore some machines are not computers" but incorrectly on the logically equivalent "All blargs are zimons; All glorps are blargs; Therefore some zimons are not glorps," what does this suggest about its reasoning process? (Answer: It's likely using content-specific heuristics rather than abstract logical patterns)

## Architecture Onboarding

- **Component map:**
  - Dual Ground Truth Framework -> Syllogism Variant Generator -> Adaptive Inference Procedure -> Belief Bias Quantification
  - Dataset (160 syllogisms) -> Model evaluation (14 LLMs × 4 strategies × 3 temperatures) -> Dual evaluation (validity + believability) -> Performance metrics

- **Critical path:**
  1. Construct syllogism variants preserving logical structure while manipulating content (normal→nonsense→order-switched→combined)
  2. Annotate each instance with dual ground truth (validity + believability labels)
  3. Run inference across model×strategy×temperature configurations
  4. Evaluate predictions against both ground truths independently
  5. Compute belief bias as Acc_congruent - Acc_incongruent
  6. Measure consistency as agreement rate across logically equivalent variants

- **Design tradeoffs:**
  - Binary response format ensures comparability across prompting strategies but forfeits confidence calibration analysis
  - Majority-vote aggregation at τ>0 reduces variance but may mask systematic reasoning path differences
  - Nonsense variant inclusion tests pure logical reasoning but creates NLU evaluation asymmetry (76.2% unbelievable/neutral conclusions)
  - Categorical syllogism focus provides controlled evaluation but may not generalize to modal, conditional, or nested quantifier structures

- **Failure signatures:**
  - High precision, low recall (e.g., Qwen3-Next Thinking: 99.2%/42.8%): Model has learned to default to "incorrect" as conservative strategy
  - High recall, low precision (e.g., Gemma 3 27B: 93.1%/61.0%): Model over-accepts conclusions, possibly conflating believability with validity
  - Large belief bias (>20pp): Model lacks robust formal reasoning circuits and relies heavily on semantic plausibility heuristics
  - Low cross-variant consistency (<80%): Model reasoning is content-dependent rather than structure-abstracted

- **First 3 experiments:**
  1. Reproduce the dual evaluation framework on a held-out set of 20 syllogisms across 3 model tiers (top: Gemini 2.5 Flash, mid: Llama 3.3 70B, low: Llama 3.2 1B) with zero-shot prompting at τ=0
  2. Test the few-shot degradation hypothesis by comparing zero-shot vs. few-shot performance on the 40 base syllogisms, tracking instance-level error redistribution using McNemar's test
  3. Validate consistency-accuracy correlation by computing C_all for a subset of 10 base syllogisms and correlating with overall syntax accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do high-performing LLMs rely on learned explicit logical rules or statistical approximations when solving syllogisms?
- **Basis in paper:** [explicit] The authors explicitly call for "mechanistic interpretability studies" to determine if models learn "explicit logical rules, statistical approximations, or hybrid representations."
- **Why unresolved:** The current study evaluates performance outcomes rather than internal computational mechanisms, leaving the specific reasoning process opaque.
- **What evidence would resolve it:** Causal tracing or probing experiments that identify specific circuit subgraphs responsible for logical operations versus statistical pattern matching.

### Open Question 2
- **Question:** Why does few-shot prompting degrade performance in formal reasoning tasks compared to zero-shot approaches?
- **Basis in paper:** [explicit] The authors note that the observed performance decline "challenges conventional wisdom" and explicitly state that this "warrants systematic exploration of when and why demonstration examples help versus hinder reasoning."
- **Why unresolved:** While the paper quantifies the degradation, it does not investigate the underlying attention mechanisms or distraction effects caused by the added context.
- **What evidence would resolve it:** Ablation studies analyzing attention entropy and distraction in few-shot contexts compared to zero-shot baselines.

### Open Question 3
- **Question:** Does logical training reduce belief bias, or does reduced bias enable better reasoning?
- **Basis in paper:** [explicit] The authors identify the "causal relationship between reasoning capability and bias resistance" as an open question that remains unanswered by their observational data.
- **Why unresolved:** The study establishes a correlation (ρ=−0.565) between high accuracy and low bias but cannot determine the direction of causality.
- **What evidence would resolve it:** Controlled fine-tuning experiments that isolate reasoning instruction from bias mitigation to observe the directional impact on model performance.

### Open Question 4
- **Question:** Do the observed patterns of high syntactic accuracy and low semantic plausibility generalize to complex logical systems?
- **Basis in paper:** [explicit] The authors state that future work must extend evaluation to "richer logical systems such as modal logics, transitive closure logics... to test whether the observed patterns generalize beyond categorical syllogisms."
- **Why unresolved:** The current study is limited to categorical syllogisms, which may not capture the difficulties of nested quantifiers or modal operators.
- **What evidence would resolve it:** Benchmarking the same models on modal logic datasets to see if the syntax-NLU accuracy gap persists.

## Limitations
- The dual ground truth framework introduces annotation complexity that could affect results, as believability annotations may reflect annotator cultural/educational biases rather than universal plausibility judgments.
- The few-shot degradation finding contradicts much prior literature on few-shot benefits, suggesting either a genuine phenomenon specific to formal reasoning or an artifact of the particular demonstration examples used.
- The claim that LLMs are "evolving toward formal reasoning engines rather than human-like reasoners" extrapolates from current performance to future trends, which may not hold as architectures and training approaches continue to evolve.

## Confidence
- **High Confidence:** Syntax accuracy findings (99.6% for top models) and the overall trend of belief bias being positively correlated with lower reasoning ability.
- **Medium Confidence:** The specific few-shot degradation mechanism and the interpretation that this reflects interference with abstract rule application rather than example quality issues.
- **Low Confidence:** The claim that LLMs are "evolving toward formal reasoning engines rather than human-like reasoners" as an evolutionary trend.

## Next Checks
1. **Cross-cultural annotation validation:** Re-annotate a subset of 20 syllogisms with diverse annotator pools to assess whether believability judgments vary systematically across cultural/educational backgrounds.
2. **Few-shot example sensitivity analysis:** Systematically vary the demonstration examples in few-shot prompting to determine whether the degradation is consistent across example sets or specific to the original demonstration selection.
3. **Nested syllogism generalization:** Extend the dual evaluation framework to nested or conditional syllogisms to test whether the observed capability gap and belief bias patterns generalize beyond categorical syllogisms.