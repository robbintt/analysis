---
ver: rpa2
title: 'Talk to Right Specialists: Routing and Planning in Multi-agent System for
  Question Answering'
arxiv_id: '2501.07813'
source_url: https://arxiv.org/abs/2501.07813
tags:
- agents
- knowledge
- answer
- question
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RopMura, a multi-agent system for question
  answering that incorporates routing and planning mechanisms to address knowledge
  sovereignty issues. The system includes a router that intelligently selects the
  most relevant agents based on knowledge boundaries, and a planner that decomposes
  complex multi-hop queries into manageable steps.
---

# Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering

## Quick Facts
- **arXiv ID:** 2501.07813
- **Source URL:** https://arxiv.org/abs/2501.07813
- **Reference count:** 29
- **Primary result:** RopMura achieves 74.93% GPT Evaluation score on HotpotQA and 85.03% on Multi-hop RAG tasks

## Executive Summary
This paper introduces RopMura, a multi-agent question answering system that combines routing and planning mechanisms to handle both single-hop and multi-hop queries while preserving knowledge sovereignty between agents. The system features a router that selects relevant agents based on knowledge boundaries encoded as cluster centroids, and a planner that decomposes complex queries into manageable steps. Experimental results demonstrate that RopMura effectively addresses the knowledge overlap and retrieval challenges in multi-agent QA systems, achieving superior performance on both single-hop and multi-hop benchmarks.

## Method Summary
RopMura operates through a router that selects agents based on centroid representations of their knowledge boundaries, and a planner that iteratively decomposes and processes multi-hop queries. Agents are RAG-based specialists that partition their knowledge into clusters (n = ⌊√m⌋), compute centroids, and share only these with the router. For multi-hop queries, the planner uses a greedy strategy with four modules: Splitter (generates subquestions), Selector (chooses one), Judger (checks completion), and Defender (finalizes answer). The system filters unreliable agent responses before synthesizing the final answer.

## Key Results
- RopMura achieves 74.93% GPT Evaluation score on HotpotQA multi-hop task
- The system reaches 85.03% GPT Evaluation score on Multi-hop RAG dataset
- RopMura outperforms presplit planning and raw knowledge approaches, with "Selected Agents" variant scoring 78.26% vs 65.97% for raw knowledge

## Why This Works (Mechanism)

### Mechanism 1: Centroid-Based Knowledge Boundary Encoding
Routing accuracy depends on compact representation of each agent's knowledge coverage. Each agent partitions its knowledge into ⌊√m⌋ clusters via hierarchical clustering, computes centroids, and shares only these with the router. The router selects agents by comparing query embeddings against centroids using top-k similarity. Core assumption: cluster centroids sufficiently approximate semantic coverage; embedding similarity correlates with retrieval relevance.

### Mechanism 2: Iterative Greedy Planning with Progress Judgment
Multi-hop query decomposition benefits from dynamic plan refinement rather than upfront splitting. The planner iteratively uses Splitter, Selector, Router, and Judger modules to process queries until resolution. Core assumption: subquestions can be reliably answered in sequence; the judger accurately assesses partial progress.

### Mechanism 3: Response Quality Filtering at Router Level
Final answer quality improves by filtering agent responses before synthesis. The router evaluates each response as "Good and reliable," "Conjecture," or "Unhelpful" based on analysis soundness, then synthesizes only reliable responses. Core assumption: LLM-based evaluation can reliably distinguish sound analysis from conjecture.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG) pipeline**
  - Why needed: The entire system assumes familiarity with how retrieval, reranking, and generation compose; each agent is a RAG-based specialist.
  - Quick check: Can you explain why sparse + dense retrieval hybrid (Mixture) outperforms either alone in Table 1a?

- **Concept: Hierarchical clustering on embeddings**
  - Why needed: Router depends on understanding how agents compress knowledge into centroids; incorrect clustering granularity directly harms routing.
  - Quick check: Why does the paper set n = ⌊√m⌋ rather than a fixed cluster count?

- **Concept: Multi-hop reasoning decomposition**
  - Why needed: The planner's value hinges on recognizing when a query requires sequential reasoning vs. single retrieval.
  - Quick check: Given "Who is the CEO of the company that acquired GitHub?", can you identify the two subquestions?

## Architecture Onboarding

- **Component map:** Query → Router (centroid matching) → Agents (retrieval + generation) → Router (response filtering) → Planner (for multi-hop) → Splitter → Selector → Router → Judger → Defender → Final Answer

- **Critical path:** 1) Query arrives → Router matches to agents via centroid similarity 2) (For multi-hop) Planner decomposes query → Router routes each subquestion 3) Selected agents retrieve → generate responses with analysis 4) Router filters unreliable responses → synthesizes final answer

- **Design tradeoffs:** Token cost vs. accuracy (RopMura achieves highest GPT Eval but at 3-5x token cost); cluster granularity (more clusters improve precision but increase overhead); planning rigidity (Presplit is cheaper but cannot adapt; greedy adapts but risks compounding errors)

- **Failure signatures:** High "Conjecture" rate in agent responses → router falls back to internal knowledge (potential hallucination); low answerable rate despite high agent count → centroid representation may be inadequate; Judger loops without progress → subquestions may not be decomposable

- **First 3 experiments:**
  1. Single-hop routing validation on Natural Questions subset with 64 Wikipedia-category agents; measure agent-level answerable rate and useful rate
  2. Planning strategy comparison on HotpotQA; compare One-shot, Presplit, Greedy, and RopMura to isolate iterative planning contribution
  3. Token efficiency profiling on Multi-hop RAG task; log tokens per component to identify bottlenecks

## Open Questions the Paper Calls Out

- **Open Question 1:** How can routing and planning mechanisms be adapted to support agents with versatile modalities, such as images or audio? The paper states future research should focus on versatile modalities and investigate encoding multimodal information for effective agent selection.

- **Open Question 2:** Can the system generalize to agents that rely on fine-tuned models rather than RAG-based knowledge retrieval? The paper notes it cannot generalize to systems where agents rely on well fine-tuned models to specific tasks (e.g., coding).

- **Open Question 3:** How does the router's accuracy degrade when knowledge domains overlap significantly or embeddings are less distinct? The paper highlights that routing heavily relies on knowledge boundary quality and may fail if domains overlap significantly.

## Limitations

- Router dependency on centroid quality for knowledge boundary representation
- Planner's greedy strategy potentially introducing misleading intermediate answers
- Inability to handle queries requiring external or dynamic knowledge
- Experiments lack exploration of cluster linkage algorithms and alternative planning strategies

## Confidence

- **High confidence:** Centroid-based routing mechanism's theoretical soundness and empirical superiority of combined routing+planning
- **Medium confidence:** Claim that centroid-based representation is more efficient than full-knowledge sharing
- **Medium confidence:** Robustness of greedy planner for multi-hop reasoning

## Next Checks

1. **Cluster granularity sensitivity analysis:** Systematically vary n (cluster count) as a function of m (knowledge pieces) beyond the √m heuristic to determine optimal balance between routing precision and computational overhead

2. **Planner robustness testing:** Design adversarial multi-hop queries where early subquestion answers are misleading to assess whether the greedy planner can recover or whether errors compound irreversibly

3. **Cross-domain generalization:** Evaluate RopMura on datasets from domains outside Wikipedia/news (e.g., scientific literature, social media discourse) to test whether centroid-based routing generalizes when knowledge distributions differ significantly from training domains