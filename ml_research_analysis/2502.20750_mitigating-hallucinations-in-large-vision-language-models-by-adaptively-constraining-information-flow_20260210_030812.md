---
ver: rpa2
title: Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining
  Information Flow
arxiv_id: '2502.20750'
source_url: https://arxiv.org/abs/2502.20750
tags:
- visual
- object
- information
- hallucinations
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object hallucination in large
  vision-language models, where generated image descriptions contain objects not present
  in the input image. The authors propose AdaVIB, a method based on Variational Information
  Bottleneck (VIB), to mitigate this issue by constraining information flow and reducing
  overconfidence in irrelevant visual features.
---

# Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow

## Quick Facts
- **arXiv ID:** 2502.20750
- **Source URL:** https://arxiv.org/abs/2502.20750
- **Reference count:** 9
- **Primary result:** AdaVIB reduces object hallucinations in LVLMs by 14.5% vs LURE on MSCOCO using entropy-based adaptive noise injection

## Executive Summary
This paper addresses object hallucination in large vision-language models (LVLMs), where generated descriptions include objects not present in input images. The authors propose AdaVIB, a method that mitigates this issue by constraining information flow through Variational Information Bottleneck (VIB) applied to the vision-language projector. By adaptively controlling injected noise based on the entropy of similarity distributions between visual tokens and LLM embeddings, AdaVIB achieves consistent improvements across different model architectures and evaluation benchmarks.

## Method Summary
AdaVIB fine-tunes only the vision-language projector in frozen LVLM architectures, applying VIB with adaptive noise control. The method introduces stochastic noise via reparameterization to constrain irrelevant information flow, while an entropy-based mechanism dynamically adjusts the regularization strength per sample. The approach requires no architectural changes beyond adding mean and variance heads to the projector, making it broadly applicable to existing LVLMs.

## Key Results
- Achieves 14.5% improvement over LURE under CHAIR S and CHAIR I metrics on MSCOCO
- Consistently reduces object hallucinations across both MiniGPT4 and LLaVa-1.5 architectures
- Maintains performance while requiring only projector fine-tuning (no LLM modification)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VIB reduces overconfidence in irrelevant visual features during projection
- Mechanism: Introduces stochastic noise via KL divergence to constrain information flow, preventing overfitting to spurious correlations
- Core assumption: Overconfidence in irrelevant features causes hallucinations
- Evidence anchors: [abstract] "overconfidence in irrelevant visual features when soft visual tokens map to the LLM's word embedding space"
- Break condition: Excessive noise may discard genuinely predictive features

### Mechanism 2
- Claim: Entropy-based adaptive noise control dynamically adjusts regularization
- Mechanism: Uses similarity distribution entropy to determine overconfidence; low entropy triggers stronger noise injection
- Core assumption: Entropy reliably proxies overconfidence risk per sample
- Evidence anchors: [abstract] "entropy-based noise-controlling strategy to enable the injected noise to be adaptively constrained"
- Break condition: Entropy may be decoupled from hallucination risk due to dataset artifacts

### Mechanism 3
- Claim: Projector-only optimization is sufficient to reduce hallucinations
- Mechanism: Fine-tunes only the vision-language projector while freezing LLM and visual encoder
- Core assumption: Projector is the primary bottleneck for hallucinations
- Evidence anchors: [Methodology] "we focus on optimizing the vision-language projector with other modules frozen"
- Break condition: LLM priors or visual encoder limitations may dominate hallucination sources

## Foundational Learning

- **Concept:** Variational Information Bottleneck (VIB)
  - Why needed: Core technique for constraining irrelevant information
  - Quick check: Can you explain the trade-off controlled by β in compression vs prediction?

- **Concept:** Entropy as confidence measure
  - Why needed: Drives adaptive β for noise injection
  - Quick check: Why would low entropy trigger stronger regularization?

- **Concept:** Vision-language projector role
  - Why needed: AdaVIB targets this component specifically
  - Quick check: What are the inputs/outputs and how might overfitting cause hallucinations?

## Architecture Onboarding

- **Component map:** Visual encoder (frozen) → connector → vision-language projector (AdaVIB-modified MLP) → soft visual tokens → LLM (frozen)
- **Critical path:** Average-pooled visual tokens → similarity entropy calculation → adaptive β → z = μ + Σ⊙ε → LLM generation
- **Design tradeoffs:** Higher β reduces hallucinations but risks losing relevant detail; adaptive β mitigates this per-sample but adds computation
- **Failure signatures:** CHAIR/POPE metrics plateau; entropy remains low; KL loss doesn't decrease
- **First 3 experiments:**
  1. Replicate small-scale projector-only fine-tuning with/without VIB on COCO-5k
  2. Ablate adaptive β with fixed values [1e-9, 1e-7, 1e-1] to test CHAIR sensitivity
  3. Visualize entropy and similarity scores for hallucinated vs non-hallucinated samples

## Open Questions the Paper Calls Out
None

## Limitations
- Entropy may not reliably proxy overconfidence across diverse samples
- CHAIR metrics rely on COCO annotations which may not capture all hallucination nuances
- Results limited to two model architectures and two datasets

## Confidence
- **High Confidence:** AdaVIB reduces hallucinations as evidenced by improved metrics
- **Medium Confidence:** Entropy-based β adjustment reliably controls regularization
- **Low Confidence:** Method generalizes effectively to architectures/datasets not tested

## Next Checks
1. Ablate adaptive β by fixing to different values and compare CHAIR scores
2. Visualize similarity distribution entropy for hallucinated vs non-hallucinated samples
3. Apply AdaVIB to a different LVLM architecture (e.g., BLIP-2) and evaluate on MSCOCO/POPE