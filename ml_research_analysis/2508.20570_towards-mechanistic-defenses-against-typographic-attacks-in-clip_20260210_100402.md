---
ver: rpa2
title: Towards Mechanistic Defenses Against Typographic Attacks in CLIP
arxiv_id: '2508.20570'
source_url: https://arxiv.org/abs/2508.20570
tags:
- typographic
- attention
- attacks
- heads
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free defense against typographic
  attacks in CLIP models by identifying and ablating specialized attention heads that
  transmit typographic information to the cls token. Using a Typographic Attention
  Score, the authors locate attention heads in later layers that disproportionately
  attend to text regions.
---

# Towards Mechanistic Defenses Against Typographic Attacks in CLIP

## Quick Facts
- **arXiv ID:** 2508.20570
- **Source URL:** https://arxiv.org/abs/2508.20570
- **Reference count:** 40
- **Primary result:** Training-free defense against typographic attacks in CLIP models via attention head ablation, improving robustness by up to 19.6% while preserving accuracy

## Executive Summary
This paper introduces a training-free defense against typographic attacks in CLIP models by identifying and ablating specialized attention heads that transmit typographic information to the CLS token. Using a Typographic Attention Score, the authors locate attention heads in later layers that disproportionately attend to text regions. These heads are then selectively suppressed via circuit ablation, preserving general image classification performance while improving robustness to typographic attacks by up to 19.6% on ImageNet-100-Typo. The method generalizes across CLIP model sizes and remains competitive with fine-tuned defenses.

## Method Summary
The authors compute a Typographic Attention Score ($T_{i,\ell}$) for each attention head by measuring how much attention it allocates to text regions in images. They then iteratively ablate heads with the highest scores, stopping when accuracy degradation on clean data exceeds 1%. This creates a circuit of heads that can be suppressed at inference time, effectively blocking typographic information from reaching the CLS token while preserving standard classification capabilities.

## Key Results
- Improved robustness to typographic attacks by up to 19.6% on ImageNet-100-Typo
- Preserved general image classification accuracy with <1% degradation
- Demonstrated effectiveness across five different CLIP model sizes (ViT-B, L, H, G, Big-G)
- Outperformed fine-tuned defenses like Defense-Prefix while requiring no training

## Why This Works (Mechanism)

### Mechanism 1: Specialized Attention Head Ablation
A sparse set of attention heads in CLIP's later layers causally transmits typographic information to the CLS token. These heads show disproportionately high attention weights to image regions containing text. When ablated, their contribution to the CLS token's residual stream is zeroed, preventing typographic features from influencing the final representation. This works because the identified heads are specialized for typography and not critical for general visual processing.

### Mechanism 2: Attention Pattern Control
Controlling the attention pattern of typographic heads directly modulates the model's typographic robustness. By manipulating the attention pattern $A_{i,\ell}$ with a parameter $\alpha$, the paper demonstrates that forcing these heads to attend more to their CLS token (acting as an attention sink) rather than spatial tokens reduces the model's sensitivity to typographic attacks. This shows the causal link between head attention patterns and typographic vulnerability.

### Mechanism 3: Temporal Separation of Processing
Typographic information in CLIP is processed in a distinct, sharply demarcated phase in the latter half of the network. Linear probes trained on CLS token embeddings show a sharp increase in typographic label prediction accuracy in later layers, whereas object classification accuracy improves gradually. This temporal separation allows for targeted intervention without disrupting general visual processing.

## Foundational Learning

**Attention Head Ablation**
- *Why needed here:* This is the core intervention method. Understanding how to selectively zero-out the contribution of a specific head to the residual stream is essential.
- *Quick check question:* What is the effect of setting a head's output to the CLS token to zero while preserving its spatial output?

**Linear Probing**
- *Why needed here:* The paper uses linear probes to map where typographic and object information become linearly decodable in the network, which is the primary localization technique.
- *Quick check question:* If a linear probe achieves high accuracy at layer $\ell$, what does that imply about the information contained in that layer's representation?

**Typographic Attack**
- *Why needed here:* Understanding the threat model is crucial. The paper defends against attacks where adversarial text is overlaid on an image.
- *Quick check question:* How does a typographic attack exploit a multimodal model like CLIP?

## Architecture Onboarding

**Component map:**
- Vision Encoder (ViT) -> Attention Heads -> CLS Token -> Zero-shot classification
- Spatial Tokens (image patches) -> preserved computations
- Typographic circuit $\mathcal{C}$ -> subset of attention heads identified for ablation

**Critical path:**
1. Compute Typographic Attention Score ($T_{i,\ell}$) for each head using a dataset with typography
2. Rank heads by $T_{i,\ell}$
3. Iteratively add top-ranked heads to a candidate circuit $\mathcal{C}$ and evaluate accuracy drop on a clean dataset ($\Delta Acc$)
4. Stop when $\Delta Acc \ge \epsilon$. The resulting set is the final circuit
5. At inference, ablate all heads in $\mathcal{C}$ by zeroing their contribution to the CLS token

**Design tradeoffs:**
- **Robustness vs. Capability:** The $\epsilon$ parameter directly controls this trade-off. A lower $\epsilon$ preserves more standard accuracy but may allow more typographic vulnerability
- **Training-Free vs. Fine-Tuning:** This method requires no gradient updates, making it fast and scalable. Fine-tuned defenses may achieve higher robustness but are computationally expensive and less flexible
- **CLS Token vs. Spatial Tokens:** This defense only protects the CLS token. Applications using spatial tokens may remain vulnerable

**Failure signatures:**
- Accuracy Drop: Standard classification accuracy falls by more than 1% on non-typographic benchmarks
- Ineffective Defense: Accuracy on typographic benchmarks does not improve, indicating the wrong heads were ablated
- Attention Sink Failure: The heads in $\mathcal{C}$ do not use CLS-to-CLS attention as a sink, and manipulating attention patterns does not control robustness

**First 3 experiments:**
1. Replicate Circuit Discovery: Calculate $T_{i,\ell}$ on Unsplash-Typo for a ViT-B model and identify heads with scores $\ge 3\times\bar{T}$. Verify the identified heads are in the later layers
2. Ablation Impact: Perform a controlled ablation of the top-ranked heads on ImageNet-100-Typo. Plot the trade-off curve between the number of ablated heads and accuracy on both typographic and clean ImageNet-100
3. Causal Manipulation: For the highest-scoring head, implement the attention pattern manipulation using the $\alpha$ parameter. Sweep $\alpha$ from 0.0 to 1.0 and plot the resulting probability of the typographic label $p(y_{typo})$ to confirm the causal relationship

## Open Questions the Paper Calls Out

**Open Question 1:** Does ablating the typographic circuit in CLIP vision encoders effectively defend downstream Vision-Language Models (VLMs) that rely on spatial tokens rather than just the CLS token? The authors note that applications like LLaVA "leverage... spatial tokens," explicitly calling for "investigation into its generalizability to VLM setups" since their method only modifies the CLS token residual stream.

**Open Question 2:** Can adversarial inputs be optimized to bypass the defense by forcing non-ablated attention heads to take over the transmission of typographic information? Under "Misuse Potential," the authors suggest "adversarial inputs might be crafted to increase the spatial attention of heads in C, making typographic attacks even more effective."

**Open Question 3:** To what extent does the "dyslexic" ablation degrade the model's ability to perform legitimate Optical Character Recognition (OCR) or text-referencing tasks? While the paper confirms preservation of general image classification, it does not explicitly measure the performance drop on tasks where reading text is the intended utility.

## Limitations

- **Dataset generation uncertainty:** Exact text rendering parameters (font, size, color, background box usage) for synthetic datasets are unspecified, which could affect which heads are identified
- **Implementation specificity:** The circuit ablation mechanism is described abstractly, and implementation details of which residual components to zero may significantly impact effectiveness
- **Architecture variability:** While claimed to generalize across model sizes, the specific circuits identified may vary substantially between architectures, making the $\epsilon=0.01$ threshold less appropriate across scales

## Confidence

**High Confidence:** The core mechanism of using attention head ablation to defend against typographic attacks is sound and well-supported. The causal ablation experiments demonstrate that identified heads transmit typographic information to the CLS token.

**Medium Confidence:** The claim that this defense generalizes across different CLIP model sizes without retraining is supported by experiments on five model variants, but the specific circuits identified vary by model.

**Low Confidence:** The release of dyslexic CLIP models for safety-critical applications is presented as a direct outcome, but the paper provides limited validation of these models in actual safety-critical contexts.

## Next Checks

**Check 1: Ablation Granularity Verification**
Implement a fine-grained ablation study where you systematically zero different components of each attention head's output (query-key interaction, value projection, full output) to the CLS token. This will verify whether the paper's assumption about the specific residual component being responsible for typographic transmission is correct.

**Check 2: Cross-Architecture Circuit Transfer**
Apply the typographic attention scoring method from a large CLIP model (e.g., ViT-G/14) to a smaller model (e.g., ViT-B/32) without retraining. Compare the identified circuits - do the same heads or head positions emerge as typographic-sensitive? This tests the claim of architecture-agnostic circuit discovery.

**Check 3: Robustness to Text Rendering Variations**
Create multiple versions of ImageNet-100-Typo with systematically varied text rendering parameters (font families, sizes from 12px to 48px, colors, background boxes with varying opacities). Apply the defense to each variant and measure both robustness gains and accuracy degradation. This will reveal whether the defense is brittle to realistic variations in how adversarial text might be presented.