---
ver: rpa2
title: 'ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models
  with Bayesian Optimization'
arxiv_id: '2508.11551'
source_url: https://arxiv.org/abs/2508.11551
tags:
- data
- mixture
- optimization
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ADMIRE-BayesOpt accelerates optimal data mixture selection for
  large language models through Bayesian Optimization. It models validation performance
  as a Gaussian Process over data mixtures and model sizes, using acquisition functions
  to balance exploration and exploitation.
---

# ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization

## Quick Facts
- arXiv ID: 2508.11551
- Source URL: https://arxiv.org/abs/2508.11551
- Reference count: 22
- Primary result: Achieves 195-500% speed-ups in finding optimal data mixtures for LLMs through Bayesian Optimization

## Executive Summary
ADMIRE-BayesOpt addresses the computational challenge of optimizing data mixtures for large language model training. The method uses Bayesian Optimization to efficiently search the space of data mixture weights, modeling validation performance as a Gaussian Process that captures uncertainty. By balancing exploration and exploitation through acquisition functions, it dramatically accelerates the search process compared to traditional methods. The approach successfully transfers mixtures learned on small proxy models to larger target models, enabling practical application to billion-parameter systems.

## Method Summary
The approach models validation performance as a Gaussian Process over data mixture weights and model sizes, enabling uncertainty-aware optimization. Bayesian Optimization is used to select new mixture candidates by balancing exploration of uncertain regions with exploitation of promising areas. The method includes a multi-fidelity extension that uses subsampled data batches to reduce computational cost, with the GP model aggregating results across fidelity levels. A key innovation is the successful transfer of optimal mixtures from small proxy models (1M parameters) to large target models (7B parameters), validated across both pre-training and instruction finetuning scenarios.

## Key Results
- Achieves 195-500% speed-ups in finding optimal data mixtures compared to baseline methods
- Reduces computational cost by 67-82% using multi-fidelity optimization versus single-fidelity approaches
- Successfully transfers learned mixtures from 1M parameter proxy models to 7B parameter target models
- Demonstrates effectiveness across both pre-training and instruction finetuning tasks

## Why This Works (Mechanism)
Bayesian Optimization provides an efficient framework for expensive black-box optimization by maintaining a probabilistic model of the objective function. The Gaussian Process surrogate captures both the predicted performance and uncertainty at each point in the mixture space. The acquisition function uses this uncertainty to guide the search toward regions that are either promising (exploitation) or poorly understood (exploration). Multi-fidelity optimization further accelerates the process by using cheaper approximations at lower fidelities to inform the search before committing to full evaluations.

## Foundational Learning
- **Gaussian Processes**: Probabilistic models that provide both predictions and uncertainty estimates for continuous functions, essential for Bayesian Optimization's exploration-exploitation tradeoff
- **Bayesian Optimization**: Sequential optimization framework that uses a surrogate model to guide the search for optimal hyperparameters, critical for expensive-to-evaluate objectives like LLM training
- **Multi-fidelity Optimization**: Technique that leverages cheaper approximations at lower fidelity levels to accelerate the overall optimization process, reducing computational burden
- **Data Mixture Optimization**: The problem of finding optimal weights for combining multiple training datasets to maximize model performance, fundamental to curriculum learning and dataset curation
- **Transfer Learning**: The ability to apply knowledge gained from solving one problem (small proxy models) to a related problem (large target models), enabling practical application to expensive models

## Architecture Onboarding

**Component Map**: Data Mixtures -> Validation Performance -> Gaussian Process Model -> Acquisition Function -> New Mixtures -> (Multi-fidelity: Subsampled Batches)

**Critical Path**: Data Mixture Definition → Model Training/Evaluation → Validation Performance → GP Update → Acquisition Function → Next Mixture Selection

**Design Tradeoffs**: GP kernel choice affects model expressiveness vs. computational cost; acquisition function parameters balance exploration vs. exploitation; fidelity levels trade evaluation accuracy vs. speed

**Failure Signatures**: Poor kernel choice leads to over/under-confident predictions; acquisition function issues cause premature convergence or excessive exploration; multi-fidelity mismatch between subsampled and full-batch performance

**First Experiments**:
1. Verify GP predictions match observed validation performance on held-out mixture points
2. Test acquisition function's ability to balance exploration vs. exploitation on synthetic test functions
3. Validate multi-fidelity GP aggregation by comparing predictions to actual full-fidelity evaluations

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Relies on validation performance as optimization target, which may not perfectly correlate with final task performance
- Assumes proxy model behavior generalizes to larger models, though evidence supports this across 1M to 7B parameters
- Effectiveness depends on GP kernel and acquisition function choices with limited exploration of alternatives
- Multi-fidelity extension assumes subsampled batches provide reliable performance signals for all task types

## Confidence
- **High Confidence**: Speed-up measurements (195-500%) and multi-fidelity cost reductions (67-82%) based on controlled experiments with clear metrics
- **Medium Confidence**: Generalization claims across model sizes supported by experiments but limited to specific architectures and tasks
- **Medium Confidence**: ADMIRE IFT Runs dataset enables reproducibility, though representativeness for diverse use cases remains to be established

## Next Checks
1. Test method's robustness to noisy or unstable validation metrics by introducing controlled perturbations in validation pipeline
2. Evaluate whether learned mixtures transfer across different model families (e.g., from decoder-only to encoder-decoder architectures)
3. Compare Bayesian Optimization performance against gradient-based data mixture optimization approaches for continuous mixture weight spaces