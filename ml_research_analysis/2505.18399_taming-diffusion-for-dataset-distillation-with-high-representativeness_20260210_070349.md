---
ver: rpa2
title: Taming Diffusion for Dataset Distillation with High Representativeness
arxiv_id: '2505.18399'
source_url: https://arxiv.org/abs/2505.18399
tags:
- dataset
- distribution
- distillation
- latents
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of generating compact, representative\
  \ distilled datasets for deep learning by improving diffusion-based methods. It\
  \ identifies key issues in prior approaches\u2014inaccurate distribution matching\
  \ in the VAE latent space, structural information loss due to random noise injection,\
  \ and lack of consideration for overall distribution alignment\u2014and proposes\
  \ D3HR to address them."
---

# Taming Diffusion for Dataset Distillation with High Representativeness

## Quick Facts
- **arXiv ID**: 2505.18399
- **Source URL**: https://arxiv.org/abs/2505.18399
- **Reference count**: 37
- **Primary result**: D3HR achieves up to 12.5% higher accuracy on CIFAR-10 (ResNet-18, 50 IPC) and 27.5% improvement in robustness across seeds compared to state-of-the-art dataset distillation methods

## Executive Summary
This paper addresses the challenge of generating compact, representative distilled datasets for deep learning by improving diffusion-based methods. The authors identify three key limitations in prior approaches: inaccurate distribution matching in VAE latent space, structural information loss from random noise injection, and lack of overall distribution alignment. To address these issues, they propose D3HR, which uses DDIM inversion to map latents from complex VAE latent space to a high-normality Gaussian domain for more accurate distribution matching, followed by group sampling to select the most representative subset. Experiments across multiple datasets and architectures demonstrate significant improvements over state-of-the-art methods, with D3HR achieving up to 12.5% higher accuracy and 27.5% better robustness.

## Method Summary
D3HR improves dataset distillation by first using DDIM inversion to transform latents from the complex, low-normality VAE latent space into a high-normality Gaussian domain, enabling more accurate distribution matching. The method then employs a group sampling scheme that selects representative subsets based on statistical metrics, ensuring both diversity and structural consistency in the distilled dataset. This two-stage approach addresses key limitations in prior diffusion-based methods, including inaccurate distribution alignment and loss of structural information during noise injection. The method is evaluated across CIFAR-10/100, Tiny-ImageNet, and ImageNet-1K using various architectures, demonstrating significant performance improvements over existing state-of-the-art approaches.

## Key Results
- D3HR achieves up to 12.5% higher accuracy on CIFAR-10 (ResNet-18, 50 IPC) compared to state-of-the-art methods
- Shows 27.5% improvement in robustness across different random seeds
- Demonstrates superior cross-architecture generalization and reduced storage requirements
- Outperforms existing methods on multiple datasets including CIFAR-10/100, Tiny-ImageNet, and ImageNet-1K

## Why This Works (Mechanism)
The method works by addressing fundamental limitations in diffusion-based dataset distillation. By using DDIM inversion to map from complex VAE latent space to a high-normality Gaussian domain, D3HR achieves more accurate distribution matching. The group sampling scheme then ensures that the selected subset captures both diversity and structural consistency, which is critical for maintaining representativeness while reducing dataset size. This combination allows for better preservation of important data characteristics while achieving significant compression.

## Foundational Learning
- **Diffusion Models**: Generative models that learn to denoise data through a reverse diffusion process; needed to understand how the method leverages diffusion for dataset generation; quick check: can generate realistic samples from random noise
- **VAE Latent Space**: The compressed representation space learned by Variational Autoencoders; important because the method operates on and transforms these latents; quick check: latents should capture essential data characteristics
- **DDIM Inversion**: A technique for mapping between latent spaces while preserving structure; critical for the distribution matching component; quick check: can accurately transform latents between spaces
- **Distribution Matching**: Aligning statistical properties between different data distributions; fundamental to ensuring the distilled dataset represents the original; quick check: matched distributions should have similar statistical metrics
- **Group Sampling**: A strategy for selecting representative subsets based on diversity and similarity metrics; enables efficient dataset compression; quick check: selected groups should cover the data distribution well

## Architecture Onboarding

**Component Map**: VAE Encoder -> DDIM Inversion -> Distribution Matching -> Group Sampling -> Distilled Dataset

**Critical Path**: The method flows from encoding images through a VAE, applying DDIM inversion to transform latents to a Gaussian domain, performing distribution matching to align statistical properties, then using group sampling to select the most representative subset for the final distilled dataset.

**Design Tradeoffs**: The approach trades computational complexity of DDIM inversion against improved distribution matching accuracy. Using group sampling instead of random selection increases computational overhead but ensures better representativeness and diversity in the distilled dataset.

**Failure Signatures**: Poor performance may manifest as: (1) distilled datasets that don't generalize well to new architectures, (2) significant loss of diversity in the selected subset, (3) failure to maintain distribution alignment between original and distilled data, or (4) computational infeasibility for large-scale datasets due to DDIM inversion costs.

**3 First Experiments**:
1. Generate distilled datasets using D3HR on CIFAR-10 with varying numbers of image per class (IPC) and evaluate training accuracy with ResNet-18
2. Compare distribution alignment metrics between original and distilled datasets using statistical tests
3. Test cross-architecture generalization by training on D3HR-distilled CIFAR-10 using ResNet-18 and evaluating on ResNet-50

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on standard benchmark datasets with relatively clean image distributions; performance on real-world, noisy, or domain-specific data remains untested
- Computational costs of DDIM inversion at scale are not addressed, which could be prohibitive for larger datasets or higher-resolution images
- Results are not averaged over multiple training runs, limiting confidence in claims about stability despite demonstrated robustness across random seeds

## Confidence
- **High**: Claims about distribution alignment and structural preservation are directly supported by quantitative metrics and qualitative visualizations
- **Medium to High**: Claims about improvements over baselines are well-supported by consistent outperformance across multiple architectures and datasets, though some baselines appear relatively weak
- **Medium**: Contribution of group sampling approach is less certain due to lack of ablation studies isolating its effect

## Next Checks
1. Evaluate D3HR on out-of-distribution datasets or real-world noisy data to test generalization beyond curated benchmarks
2. Perform an ablation study isolating the contribution of DDIM inversion versus group sampling to quantify each component's impact on performance
3. Measure and report wall-clock training time and memory usage for the DDIM inversion step across different dataset scales to assess practical scalability