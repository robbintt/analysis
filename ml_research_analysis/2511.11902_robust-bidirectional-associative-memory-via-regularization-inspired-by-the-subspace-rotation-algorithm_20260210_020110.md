---
ver: rpa2
title: Robust Bidirectional Associative Memory via Regularization Inspired by the
  Subspace Rotation Algorithm
arxiv_id: '2511.11902'
source_url: https://arxiv.org/abs/2511.11902
tags:
- same
- robustness
- trained
- patterns
- b-bp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of Bidirectional Associative
  Memory (BAM) trained with Bidirectional Backpropagation (B-BP) to noise and adversarial
  attacks. To improve robustness, the authors propose a novel gradient-free training
  algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which optimizes
  weight matrices through orthogonal subspace rotation.
---

# Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm

## Quick Facts
- **arXiv ID**: 2511.11902
- **Source URL**: https://arxiv.org/abs/2511.11902
- **Reference count**: 36
- **Primary result**: B-SRA and SAME configurations achieve significantly higher robustness than B-BP across multiple attack types and memory capacities

## Executive Summary
This paper addresses the vulnerability of Bidirectional Associative Memory (BAM) to noise and adversarial attacks when trained with standard Bidirectional Backpropagation (B-BP). The authors propose two complementary approaches: a novel gradient-free training algorithm called Bidirectional Subspace Rotation Algorithm (B-SRA) that optimizes weight matrices through orthogonal subspace rotation, and two regularization strategies (Orthogonal Weight Matrix and Gradient-Pattern Alignment) that can be integrated into B-BP. Extensive experiments demonstrate that B-SRA outperforms B-BP in robustness, successfully retrieving patterns under strong noise and adversarial perturbations. The SAME configuration, combining both regularizations, achieves the highest resilience across different memory capacities and attack types, with significantly lower error rates than other methods.

## Method Summary
The paper proposes two approaches to enhance BAM robustness. First, B-SRA is a gradient-free algorithm that performs alternating SVD updates to rotate weight matrices, initialized with orthogonal weights and alternating forward/backward passes to update W_Y and W_X respectively. Second, the SAME configuration augments B-BP with two regularizations: OWM loss to maintain orthogonality of weight matrices and GPA loss to align gradients with pattern directions. The experiments use MNIST and character script datasets converted to bipolar form, testing memory capacities of 50, 100, and 200 pattern pairs with 3-layer BAM for smaller capacities and 5-layer BAM for larger ones. The primary metric is Mean Squared Error of retrieved patterns, with additional robustness indicators including OWM deviation and GPA alignment.

## Key Results
- B-SRA outperforms B-BP in robustness across all tested scenarios
- SAME configuration achieves the highest resilience against both noise and adversarial attacks
- B-SRA successfully retrieves patterns under strong perturbations including FGSM, FFGSM, and PGD attacks
- Robustness improvements are consistent across different memory capacities (50, 100, 200 pairs)

## Why This Works (Mechanism)
The subspace rotation approach works by maintaining orthogonal weight matrices that preserve the geometric structure of stored patterns, making them more resistant to perturbations. The OWM regularization enforces weight matrix orthogonality during training, preventing degradation of the pattern space structure. The GPA regularization ensures that gradient updates align with the direction of pattern changes, improving the model's ability to recover from corruptions. Together, these mechanisms create a BAM that maintains better pattern separability and retrieval accuracy under adverse conditions.

## Foundational Learning
- **Bidirectional Associative Memory (BAM)**: A neural network that stores bidirectional associations between two pattern sets, needed to understand the core architecture being improved
- **Orthogonal Subspace Rotation**: Mathematical operation that preserves distances and angles in vector spaces, crucial for maintaining pattern integrity during training
- **Gradient-Free Optimization**: Training methods that don't rely on backpropagation, important for understanding B-SRA's alternative approach
- **Regularization in Neural Networks**: Techniques to prevent overfitting and improve generalization, fundamental to understanding the proposed robustness improvements
- **Adversarial Attacks (FGSM, PGD)**: Methods to generate perturbations that fool neural networks, essential context for evaluating robustness claims

## Architecture Onboarding

**Component Map**
B-BP -> Weight Updates -> Pattern Retrieval -> Error Calculation -> Regularization (if SAME)

**Critical Path**
Training: Input Patterns → Forward Pass → Backward Pass → Weight Update → Validation
Inference: Input Pattern → Forward Pass → Sign Function → Retrieved Pattern

**Design Tradeoffs**
- B-SRA: Gradient-free but potentially slower convergence vs B-BP's faster but less robust training
- OWM Regularization: Improved robustness but may restrict weight space exploration
- GPA Regularization: Better alignment with pattern directions but adds computational overhead

**Failure Signatures**
- B-BP: High MSE under adversarial attacks, unstable training with poor initialization
- B-SRA: Incorrect SVD application leading to non-orthogonal updates
- SAME: Over-regularization causing underfitting or slow convergence

**3 First Experiments**
1. Implement B-SRA with MNIST (50 pairs) and verify orthogonal weight maintenance
2. Add OWM regularization to B-BP and measure impact on Gaussian noise robustness
3. Combine both regularizations in SAME configuration and test against FGSM attacks

## Open Questions the Paper Calls Out
None

## Limitations
- Missing specific hyperparameter values for regularization weights (λ_ortho, λ_align) in the SAME configuration
- Incomplete specification of attack hyperparameters for some adversarial methods
- Lack of source information for the script datasets used in experiments

## Confidence

**High Confidence Claims**
- Robustness improvement claims: Extensive experimental evidence across multiple attack types and memory capacities

**Medium Confidence Claims**
- Algorithm effectiveness: Clear specification but missing complete hyperparameter details
- Theoretical justification: Established foundations but primarily validated empirically

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary λ_ortho and λ_align in the SAME configuration to identify their impact on robustness and determine optimal values

2. **Cross-Dataset Generalization Test**: Evaluate the trained models on additional datasets beyond MNIST and the script datasets to assess generalization of robustness claims

3. **Ablation Study of Regularization Components**: Conduct controlled experiments isolating OWM and GPA regularizations to quantify their individual contributions to the overall robustness improvement