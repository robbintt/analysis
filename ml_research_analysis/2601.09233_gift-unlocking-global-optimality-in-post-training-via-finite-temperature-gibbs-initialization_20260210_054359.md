---
ver: rpa2
title: 'GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature
  Gibbs Initialization'
arxiv_id: '2601.09233'
source_url: https://arxiv.org/abs/2601.09233
tags:
- base
- gift
- arxiv
- preprint
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the optimization mismatch between Supervised
  Fine-Tuning (SFT) and Reinforcement Learning (RL) in post-training Large Reasoning
  Models, where SFT's rigid supervision causes distributional collapse that limits
  subsequent RL exploration. The authors propose GIFT (Gibbs Initialization with Finite
  Temperature), which reformulates SFT as a finite-temperature energy potential rather
  than zero-temperature collapse, creating a distributional bridge between stages.
---

# GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization

## Quick Facts
- arXiv ID: 2601.09233
- Source URL: https://arxiv.org/abs/2601.09233
- Reference count: 37
- Primary result: Achieves 52.43% average pass@1 on mathematical reasoning benchmarks, outperforming standard SFT by 3.85%

## Executive Summary
This paper addresses the optimization mismatch between Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in post-training Large Reasoning Models, where SFT's rigid supervision causes distributional collapse that limits subsequent RL exploration. The authors propose GIFT (Gibbs Initialization with Finite Temperature), which reformulates SFT as a finite-temperature energy potential rather than zero-temperature collapse, creating a distributional bridge between stages. GIFT achieves 52.43% average pass@1 on mathematical reasoning benchmarks with Qwen2.5-7B, outperforming standard SFT (48.58%) and other variants by 3.85%. It also shows superior generalization on out-of-distribution tasks (64.10% vs 59.78% for standard SFT) and maintains better geometric and distributional consistency throughout post-training, providing a mathematically principled pathway to unlock global optimality in the SFT-then-RL paradigm.

## Method Summary
GIFT reformulates SFT as a finite-temperature Gibbs initialization to preserve base model priors for subsequent RL. For each token, GIFT computes soft targets by taking the base model's logits and adding β to the ground-truth token's logit, then applying softmax to get a target distribution. This creates a distributional bridge between the base model and expert data. The method is trained for 6 epochs using cross-entropy against these soft targets, then the resulting checkpoint initializes GRPO for RL fine-tuning. The key innovation is treating SFT as a finite-temperature policy rather than zero-temperature collapse, preserving exploration capacity for RL while maintaining expert alignment.

## Key Results
- Achieves 52.43% average pass@1 on mathematical reasoning benchmarks, outperforming standard SFT (48.58%) by 3.85%
- Shows superior generalization on out-of-distribution tasks (64.10% vs 59.78% for standard SFT)
- Maintains better geometric and distributional consistency throughout post-training (higher cosine similarity, lower KL divergence, higher top-K token overlap)

## Why This Works (Mechanism)

### Mechanism 1: Finite-Temperature Policy Preservation
Standard SFT causes distributional collapse by forcing policies toward zero-temperature limits, destroying exploration capacity needed for RL. GIFT preserves base model priors through finite-temperature Gibbs initialization. Standard SFT minimizes cross-entropy with one-hot targets, which "aggressively suppresses the probability mass of non-target tokens, thereby forcing the policy distribution to collapse onto deterministic data points." GIFT uses finite temperature β to define the optimal initialization as: π*_sft(y|x) ∝ π_base(y|x) · e^(βR(x,y)), where β < ∞ allows base priors to persist rather than being overwritten.

### Mechanism 2: Token-Level Soft Target Decomposition
The sequence-level Gibbs distribution can be approximated autoregressively via token-level soft targets, making training tractable for LLMs. Under sparse reward assumptions and "oracle path optimality" (expert tokens lie on optimal trajectories), the soft advantage function A*(y_t, y_<t) simplifies to β·I(y_t = y*_t). This yields: π*_sft(y_t|y_<t, x) ∝ π_base(y_t|y_<t, x) · e^(β·I(y_t=y*_t)), adding β to the logit of the expert token while preserving relative probabilities of non-target tokens.

### Mechanism 3: Objective Consistency Through Distributional Bridging
Maintaining finite temperature creates distributional continuity between SFT initialization and RL convergence, improving sample efficiency and asymptotic performance. GIFT initialization ensures the RL convergence point π*_stage2 matches the global optimum π*_global. This preserves geometric and distributional consistency measured via cosine similarity, L2 distance, KL divergence, and top-K token overlap.

## Foundational Learning

- **Gibbs Distribution / Boltzmann Distribution:**
  - Why needed here: Core mathematical object—GIFT reformulates policy as π ∝ base × e^(βR), a Gibbs distribution where "energy" is negative reward.
  - Quick check question: Given rewards R1=2, R2=1 and β=1 with equal base probabilities, what is P(state1)/P(state2)?

- **KL Divergence and KL-Regularized Optimization:**
  - Why needed here: The global objective maximizes reward while penalizing deviation from base model; understanding this trade-off is essential.
  - Quick check question: Why does KL(π||π_base) ≥ 0, and when does it equal zero?

- **Temperature in Softmax Distributions:**
  - Why needed here: Inverse temperature β controls policy sharpness; β→∞ collapses to one-hot, β→0 flattens toward base model.
  - Quick check question: If β is doubled, does the policy become more or less concentrated on high-reward actions?

## Architecture Onboarding

- **Component map:**
  Base Model (π_base, frozen) -> Forward pass -> Logits z_base for all tokens -> Advantage: β·I(k=y*_t) -> Softmax(z_base + advantage) -> Soft targets π*_sft -> Trainable Model (π_θ) -> Cross-entropy against soft targets -> Checkpoint @ Epoch 6 -> Initialize RL (GRPO)

- **Critical path:** Quality depends on (1) accurate base model forward pass for priors, (2) computing log-probabilities for all tokens (not just expert), (3) selecting appropriate β for your model architecture.

- **Design tradeoffs:**
  - Higher β -> stronger expert alignment but reduced exploration; optimal β varies by model (~20 for Qwen2.5, ~10 for Llama-3.1)
  - Uniform smoothing (λ=0.01) stabilizes weak-prior models but adds noise for strong-prior models

- **Failure signatures:**
  - Numerical underflow: When π_base(y*_t) ≈ 0 for weak base models; fix with uniform smoothing λ=0.01
  - Over-constrained (β too high): Pass@k scaling plateaus early, mode collapse evident
  - Under-constrained (β too low): Good validation loss but RL fails to improve
  - Late checkpointing: Using checkpoints after overfitting (e.g., >epoch 6 for Llama) degrades RL

- **First 3 experiments:**
  1. β sweep on validation: Train GIFT with β ∈ {5, 10, 15, 20, 25}, evaluate pass@1 and pass@8 to identify optimal temperature for your base model.
  2. Distributional shift diagnosis: Compare top-K token overlap between (base→GIFT→RL) vs. (base→SFT→RL); expect GIFT to show higher overlap across both transitions.
  3. Smoothing ablation: For weak-prior models, test GIFT with/without λ=0.01 smoothing; expect significant stability differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive mechanism be developed to dynamically adjust the inverse temperature β during training based on sample difficulty or evolving policy confidence, eliminating the need for manual hyperparameter selection?
- Basis in paper: Section 6 (Limitations) states: "Currently, the inverse temperature β is treated as a fixed hyperparameter... Developing an adaptive approach to implicitly adjust β during training—potentially eliminating the need for pre-defined configuration—remains a promising direction to further enhance the framework's robustness and ease of use."
- Why unresolved: The current formulation treats β as static, requiring separate tuning for different backbone architectures, which increases deployment friction.
- What evidence would resolve it: An adaptive β schedule that achieves comparable or superior performance to optimally-tuned fixed β across multiple architectures without architecture-specific configuration.

### Open Question 2
- Question: How does GIFT perform on non-mathematical reasoning domains such as code generation, open-ended dialogue, or multimodal tasks where reward structures and reasoning patterns differ substantially?
- Basis in paper: All experiments are conducted exclusively on mathematical reasoning benchmarks (GSM8K, MATH500, OlympiadBench, AIME) and a limited set of general benchmarks (GPQA, MMLU).
- Why unresolved: Mathematical reasoning has verifiable, sparse rewards; other domains may involve dense, subjective, or multi-objective rewards that challenge the sparse recovery assumption.
- What evidence would resolve it: Systematic evaluation on code benchmarks (e.g., HumanEval, MBPP), long-form generation tasks, or multimodal reasoning with appropriate reward functions.

### Open Question 3
- Question: What are the failure modes of the binary advantage approximation (Eq. 20) when multiple valid reasoning paths exist for a single problem, and can a softer advantage formulation improve performance?
- Basis in paper: Appendix B.2 assumes "oracle path optimality" (unique manifold maximizing reward) and "sparse recovery" (sharp decay for off-policy deviations).
- Why unresolved: The current approximation assigns equal advantage β to all expert tokens regardless of their relative importance or alternative valid completions, potentially over-constraining problems with multiple solution paths.
- What evidence would resolve it: Analysis on tasks with ground-truth diversity metrics comparing binary vs. graded advantage functions, or ablations measuring sensitivity to the sparsity assumption.

## Limitations

- The empirical validation is confined to mathematical reasoning tasks where expert solutions are assumed to be unique and optimal, conditions that may not generalize to other reasoning domains.
- The temperature hyperparameter β emerges as a critical sensitivity point requiring empirical grid search for optimal selection, with no clear guidance for arbitrary base models.
- The binary advantage assumption is derived under specific conditions of sparse rewards and oracle optimality, yet the paper does not extensively test whether this approximation holds under realistic conditions with imperfect expert data or multiple valid solution paths.

## Confidence

**High Confidence:** The experimental results demonstrating improved pass@1 accuracy (52.43% vs 48.58% for standard SFT) on mathematical reasoning benchmarks, and the consistent superiority across multiple out-of-distribution tasks, are well-supported by the presented data.

**Medium Confidence:** The theoretical derivation of the Gibbs distribution reformulation and the token-level soft target decomposition are mathematically sound within their stated assumptions, but the practical applicability of these assumptions beyond mathematical reasoning is uncertain.

**Low Confidence:** The claims about distributional bridging creating meaningful advantages for RL exploration are supported by geometric metrics but lack direct evidence linking these preserved distributions to improved reasoning capabilities or more efficient exploration during RL training.

## Next Checks

1. **Cross-domain generalization test:** Evaluate GIFT on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference, or open-ended generation) to determine whether the distributional preservation mechanism provides benefits beyond structured mathematical problems.

2. **β hyperparameter transfer study:** Systematically test whether the β values optimized for one model architecture transfer to other architectures with similar priors, or whether a universal selection method based on base model properties can be established.

3. **Sparse reward relaxation experiment:** Modify the DeepMath dataset to include multiple valid solution paths or noisy expert annotations, then evaluate whether GIFT maintains its advantages under these more realistic conditions.