---
ver: rpa2
title: Infrequent Exploration in Linear Bandits
arxiv_id: '2510.26000'
source_url: https://arxiv.org/abs/2510.26000
tags:
- regret
- exploration
- time
- infex
- logt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INFEX, a framework for infrequent exploration
  in linear bandits that combines a base exploratory algorithm with greedy actions
  according to a predefined schedule. The method addresses the gap between fully adaptive
  exploration (which explores every step) and purely greedy approaches (which require
  strong assumptions).
---

# Infrequent Exploration in Linear Bandits

## Quick Facts
- arXiv ID: 2510.26000
- Source URL: https://arxiv.org/abs/2510.26000
- Reference count: 40
- Primary result: INFEX achieves instance-dependent regret matching standard algorithms when exploration frequency exceeds ω(log t) threshold.

## Executive Summary
This paper introduces INFEX, a framework for infrequent exploration in linear bandits that combines a base exploratory algorithm with greedy actions according to a predefined schedule. The method addresses the gap between fully adaptive exploration (which explores every step) and purely greedy approaches (which require strong assumptions). The authors prove that INFEX achieves instance-dependent regret matching standard algorithms like LinUCB or LinTS, provided exploration frequency exceeds a logarithmic threshold. Specifically, the regret bound is O(1/Δ(log T + d log log T + d log d)²), matching existing results up to constants independent of T. They also show this logarithmic threshold is necessary.

## Method Summary
INFEX executes a base exploratory policy (e.g., LinUCB or LinTS) at scheduled time steps T_e, otherwise selects actions greedily using a ridge estimator. The framework maintains V_t = V_{t-1} + X_t X_t^T and θ̂_{t-1} = V^{-1}_{t-1} Σ_{i<t} X_i Y_i, updated at every step. At non-exploration steps, the algorithm chooses X_t = argmax_{x∈X} x^T θ̂_{t-1}. The key insight is that sparse exploratory updates ensure the optimal arm is selected sufficiently often to prevent greedy selections from incurring linear regret, while computational cost is reduced proportionally to the sparsity of the exploration schedule.

## Key Results
- INFEX achieves instance-dependent regret O(1/Δ(log T + d log log T + d log d)²) matching standard algorithms when exploration frequency f(t) = ω(log t).
- The ω(log t) exploration threshold is both sufficient and necessary for polylogarithmic regret.
- Computational cost is reduced proportionally to exploration sparsity without asymptotically degrading regret, with empirical speedups of 1.5×-3× for 95% greedy actions.
- INFEX outperforms purely greedy and ε-greedy baselines while maintaining comparable or better regret than full LinUCB/LinTS.

## Why This Works (Mechanism)

### Mechanism 1
Exploration frequency of ω(log t) is sufficient—and necessary—to achieve polylogarithmic instance-dependent regret while predominantly selecting greedy actions. A base exploratory algorithm is invoked at scheduled time steps T_e, and between these steps INFEX selects actions greedily via the ridge estimator. The sparse exploratory updates ensure the optimal arm x* is selected at least Ω(f(t)) times, causing the estimation error of x*^T θ* to decay proportionally to 1/√N_opt(t). This prevents greedy selections from incurring linear regret. The analysis breaks down if f(t) = O(log t) or if Δ = 0.

### Mechanism 2
The regret from greedy selections is bounded by the inverse of the number of optimal arm selections accumulated over time. Greedy regret is decomposed using ridge estimator concentration, with the key insight that ∥x*∥²_{V^{-1}_{t−1}} ≤ 1/(1 + N_opt(t−1)). This means the more often the optimal arm is chosen, the tighter the bound on regret from subsequent greedy steps. The proof shows that once N_opt(t) grows linearly with t, the cumulative greedy regret becomes O(α_T β_T²/Δ + β_T² log T/Δ). The mechanism fails if the optimal arm is not selected sufficiently often.

### Mechanism 3
Computational cost is reduced proportionally to the sparsity of the exploration schedule without asymptotically degrading regret. Expensive operations (confidence radius computation for LinUCB: O(d²K), posterior sampling for LinTS: O(d³)) are restricted to |T_e| = O(T/m) or O(log^r T) steps. Greedy selections require only O(d² + dK) via Sherman–Morrison updates to maintain V_t^{-1}. Since the regret bound's asymptotic term does not depend on the schedule, runtime improves with minimal regret trade-off. The mechanism fails when the schedule is so sparse that finite-time performance degrades severely.

## Foundational Learning

- **Linear Bandits and Instance-Dependent Regret**: Why needed: The entire INFEX framework is built on the stochastic linear bandit model where rewards are Y_t = X_t^T θ* + η_t, and the analysis hinges on instance-dependent quantities like Δ (minimum gap) and α_T (log determinant of the design matrix). Quick check: Can you explain why the minimum gap Δ appears in the denominator of the regret bounds, and what happens theoretically as Δ → 0?

- **Ridge Regression and Self-Normalized Bounds**: Why needed: The greedy action uses the ridge estimator θ̂_{t−1} = V^{-1}_{t−1} Σ_{i<t} X_i Y_i, and the confidence bound β_t(δ) is central to the error analysis. Quick check: What is the role of the regularization matrix V_0 = I_d in ensuring the estimator is well-defined, and how does α_t = log det(V_t)/det(V_0) measure information accumulation?

- **Asymptotic Notation for Schedules (ω, O, Ω)**: Why needed: The core theorem requires f(t) = ω(log t), meaning f(t)/log t → ∞ as t → ∞. Understanding this threshold is critical for choosing valid schedules. Quick check: Given a schedule where the base algorithm runs at steps {⌊e^{√n}⌋ : n ∈ ℕ}, does this satisfy f(t) = ω(log t)? Why or why not?

## Architecture Onboarding

- Component map: Scheduler -> Base Algorithm -> Ridge Estimator -> Greedy Selector
- Critical path:
  1. Initialize V_0 = I_d.
  2. At each t, check if t ∈ T_e. If yes, invoke Alg to choose X_t; otherwise, compute θ̂_{t−1} and select greedily.
  3. Observe reward Y_t and update V_t ← V_t + X_t X_t^T (Sherman–Morrison for V_t^{-1} update).
  4. The regret analysis relies on N_opt(t) growing sufficiently; monitor that the base algorithm correctly identifies x* during exploratory steps.
- Design tradeoffs:
  - Schedule sparsity vs. finite-time constant: Sparse schedules reduce computation but increase G_const. For unknown Δ, conservative periodic schedules (m = 5–100) are recommended.
  - Base algorithm choice: LinUCB provides tighter confidence bounds; LinTS is more computationally expensive per invocation but may have better empirical regret.
  - Known vs. unknown Δ: The optimal schedule depends on Δ, but Δ is typically unknown. The analysis guarantees asymptotic polylog regret even without this knowledge.
- Failure signatures:
  - Linear or near-linear regret: Indicates f(t) is not ω(log t), or the base algorithm fails to reach τ_Alg.
  - Exploding constant term: Observed when f(t) grows too slowly (e.g., (log t)^r with r > 1), causing G_const to scale exponentially in d/Δ².
  - Persistent suboptimal greedy selections: Suggests the ridge estimator is not converging toward θ*—check for bugs in V_t updates or insufficient exploration diversity.
- First 3 experiments:
  1. Implement INFEX(LinUCB, mN) and INFEX(LinTS, mN) for m ∈ {5, 20, 100} on randomly generated instances (d=10, K∈{10, 100, 1000}, T=10000). Compare cumulative regret against full LinUCB, LinTS, and purely greedy.
  2. Test schedules from Table 1 (periodic, t/(log t)^r, t^r, (log t)^r). Measure both regret and G_const-approximation behavior. Identify which schedules maintain bounded G_const for given (d, Δ).
  3. Instrument code to measure time spent in exploratory vs. greedy steps. Verify that speedup scales as expected (e.g., 95% greedy → ~5× theoretical reduction in expensive operations).

## Open Questions the Paper Calls Out

- **Optimal infrequent exploration for minimax regret**: Finding the optimal infrequent exploration strategy and trade-offs for the minimax regret bound would be an interesting open problem. The authors conjecture that the ω(log t) threshold would be too small to achieve the optimal O(√T) minimax guarantees.

- **Extension to dynamically varying optimal arms**: An important and open challenge remains: extending the performance guarantees of INFEX to scenarios involving dynamically varying optimal arms. Current analysis relies on the property that estimation error of the optimal arm's reward diminishes when that arm is selected frequently.

- **Adaptive exploration schedules**: The authors note that the ω(log t) threshold might not be necessary when the exploration schedule is adaptive to the observations. This result applies to predetermined exploration schedules, and the threshold could be lower for data-dependent scheduling.

## Limitations

- The ω(log t) exploration threshold is both sufficient and necessary for polylogarithmic regret, but the construction of a hard instance is theoretical. Practical schedules with f(t) close to log t may still perform well.
- While asymptotic regret matches standard algorithms, the constant G_const(τ_Alg, f) can grow exponentially with d/Δ² for poorly chosen schedules (e.g., f(t) = (log t)^r with r > 1).
- The claim about computational efficiency is supported by a single figure with limited m values. No rigorous runtime complexity analysis is provided.

## Confidence

- **High**: The asymptotic regret bound O(1/Δ(log T + d log log T + d log d)²) is correctly derived, assuming the stated conditions hold.
- **Medium**: The necessity of ω(log t) exploration frequency is proven, but the practical implications for schedule design are not fully explored.
- **Low**: The claim about computational efficiency is supported by limited empirical data with no rigorous runtime analysis.

## Next Checks

1. **Empirical G_const scaling**: For a fixed instance (d, K, T, Δ), measure G_const-approximation behavior across different schedule families (periodic, polynomial, logarithmic) to validate theoretical bounds.

2. **Exploration frequency sensitivity**: Systematically vary m (e.g., m ∈ {2, 5, 10, 20, 50, 100}) and f(t) = t^r for r ∈ {0.1, 0.5, 0.9, 1.1, 2} to map the transition from polylogarithmic to polynomial regret regimes.

3. **Computational profiling**: Instrument INFEX implementations to measure per-step time for base algorithm vs. greedy selection across d ∈ {10, 50, 100} and K ∈ {100, 1000, 10000}. Verify claimed O(d² + dK) complexity for greedy steps.