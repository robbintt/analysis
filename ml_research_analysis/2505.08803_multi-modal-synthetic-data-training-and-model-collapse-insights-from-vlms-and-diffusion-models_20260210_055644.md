---
ver: rpa2
title: 'Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs
  and Diffusion Models'
arxiv_id: '2505.08803'
source_url: https://arxiv.org/abs/2505.08803
tags:
- data
- collapse
- synthetic
- wang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates model collapse in multi-modal vision-language
  generative systems, specifically text-to-image diffusion models and vision-language
  models (VLMs), under recursive training on self-generated synthetic data. Unlike
  prior studies focused on single-modality models, this work examines realistic scenarios
  where multiple models interact autonomously through synthetic data.
---

# Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models

## Quick Facts
- arXiv ID: 2505.08803
- Source URL: https://arxiv.org/abs/2505.08803
- Reference count: 40
- Primary result: Investigates model collapse in multi-modal vision-language systems under recursive synthetic data training

## Executive Summary
This paper investigates model collapse in multi-modal vision-language generative systems, specifically text-to-image diffusion models and vision-language models (VLMs), under recursive training on self-generated synthetic data. Unlike prior studies focused on single-modality models, this work examines realistic scenarios where multiple models interact autonomously through synthetic data. The authors systematically evaluate how model collapse manifests differently in diffusion models versus VLMs, revealing that variance does not always collapse during model collapse, particularly in VLM image captioning tasks.

The study identifies key factors that influence model collapse severity, including decoding budgets, model diversity, and training strategies. Notably, the authors find that relabeling with frozen models significantly mitigates model collapse, while joint fine-tuning without frozen models accelerates it. They also demonstrate that commonly used quality measures like FID and BLEU-4 correlate well with synthetic data robustness against model collapse, providing practical metrics for monitoring system health during recursive training.

## Method Summary
The authors conducted controlled experiments using Stable Diffusion 1.5 (SD 1.5) and BLIP-2 (VQA) models in a recursive training setup. They created synthetic datasets by having models generate or label images iteratively, then fine-tuned the models on their own generated data for up to 30 iterations. Multiple experimental conditions were tested, including variations in decoding budgets, model architectures, and training strategies (frozen vs. joint fine-tuning). Quality metrics such as FID and BLEU-4 were used to assess synthetic data quality, while image diversity was measured through standard deviation of pixel values. The study compared model collapse behavior between text-to-image diffusion models and vision-language models to identify modality-specific characteristics.

## Key Results
- Variance does not always collapse during model collapse, particularly in VLM image captioning tasks
- Increased decoding budgets generate more robust synthetic datasets that resist model collapse
- FID and BLEU-4 quality measures correlate well with synthetic data robustness against model collapse
- Model diversity is crucial for maintaining stability in recursive fine-tuning scenarios
- Relabeling with frozen models significantly mitigates model collapse, while joint fine-tuning accelerates it

## Why This Works (Mechanism)
Model collapse in multi-modal systems occurs through different mechanisms depending on the model architecture and task. In diffusion models, recursive training on synthetic data leads to gradual loss of diversity and content richness, with images becoming increasingly homogeneous over iterations. For VLMs, the collapse manifests differently, often preserving variance in certain dimensions while losing others, particularly affecting the semantic richness of generated captions. The key insight is that multi-modal systems exhibit more complex collapse dynamics than single-modality models due to the interaction between different generative processes.

The effectiveness of frozen model relabeling as a mitigation strategy stems from breaking the recursive feedback loop - by keeping one model frozen while using another for generation, the system avoids the self-reinforcing degradation that occurs when both models are jointly fine-tuned on their own outputs. Similarly, higher decoding budgets produce more diverse initial synthetic data, providing a stronger foundation that resists collapse for more iterations.

## Foundational Learning
- Recursive training dynamics: Understanding how models degrade when trained on their own outputs is essential for recognizing early warning signs of collapse and designing appropriate monitoring systems
- Multi-modal interaction effects: VLMs and diffusion models influence each other's collapse trajectories differently than single-modality systems, requiring specialized intervention strategies
- Quality metric correlation: FID and BLEU-4 serve as early indicators of synthetic data robustness, enabling proactive detection of potential collapse before catastrophic failure

Quick checks: Monitor FID/BLEU-4 trends during recursive training; compare variance preservation across different model architectures; test frozen model relabeling as a mitigation strategy

## Architecture Onboarding

**Component Map**: Diffusion model -> Image generation -> VLM captioning -> Synthetic dataset creation -> Recursive fine-tuning loop

**Critical Path**: Synthetic data generation (diffusion model) → Quality assessment (FID/BLEU-4) → Recursive fine-tuning (VLMs and diffusion models) → Collapse detection → Mitigation implementation

**Design Tradeoffs**: Higher decoding budgets improve synthetic data robustness but increase computational cost; frozen model relabeling reduces collapse risk but limits adaptation; model diversity improves stability but requires more resources

**Failure Signatures**: Gradual increase in FID scores; decreasing BLEU-4 scores; loss of image diversity (reduced pixel variance); semantic impoverishment in generated captions; convergence to limited output patterns

**First 3 Experiments**:
1. Measure FID and BLEU-4 scores across 30 recursive training iterations with varying decoding budgets
2. Compare variance preservation in diffusion model outputs versus VLM caption diversity during collapse
3. Test frozen model relabeling versus joint fine-tuning across different model architecture combinations

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to specific model architectures (SD 1.5 and BLIP-2) and relatively short recursive training chains (≤30 iterations)
- Controlled laboratory conditions may not fully represent real-world deployment scenarios with data distribution shifts
- Focus on synthetic data from single initial dataset limits generalizability to scenarios with multiple data sources or continuous data streams

## Confidence

**High Confidence:**
- Model collapse occurs in multi-modal systems under recursive training conditions
- Increased decoding budgets produce more robust synthetic datasets
- Model diversity is crucial for maintaining stability in recursive fine-tuning

**Medium Confidence:**
- Variance preservation in VLM image captioning during collapse
- Correlation between FID/BLEU-4 scores and synthetic data robustness
- Relabeling with frozen models effectively mitigates collapse

**Low Confidence:**
- Generalizability of findings to larger model architectures and longer training chains
- Extrapolation of results to production-scale systems with millions of iterations
- Performance of proposed mitigation strategies in dynamic, real-world environments

## Next Checks

1. **Extended Chain Validation**: Replicate experiments with 100+ recursive training iterations using larger foundation models (e.g., SDXL, GPT-4V) to assess long-term collapse dynamics and test the scalability of proposed mitigation strategies.

2. **Real-World Distribution Testing**: Evaluate model performance on out-of-distribution datasets and continuously shifting data distributions to assess robustness beyond the controlled MSCOCO dataset used in this study.

3. **Alternative Metric Validation**: Implement and validate additional quality metrics including CLIP score, aesthetic quality metrics, and semantic preservation measures to comprehensively assess model behavior during recursive training and verify the correlation findings with FID and BLEU-4.