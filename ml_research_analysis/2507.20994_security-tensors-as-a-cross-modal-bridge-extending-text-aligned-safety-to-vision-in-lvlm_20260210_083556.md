---
ver: rpa2
title: 'Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to
  Vision in LVLM'
arxiv_id: '2507.20994'
source_url: https://arxiv.org/abs/2507.20994
tags:
- safety
- visual
- security
- tensors
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Security tensors are trainable input perturbations applied to either
  textual or visual modalities of large visual-language models (LVLMs) to extend text-aligned
  safety mechanisms to visual inputs. The method uses a curated training dataset containing
  malicious image-text pairs (for safety activation), benign pairs with structurally
  similar text (to prevent text-pattern overfitting), and general benign samples (to
  preserve normal functionality).
---

# Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM

## Quick Facts
- **arXiv ID**: 2507.20994
- **Source URL**: https://arxiv.org/abs/2507.20994
- **Reference count**: 40
- **Primary result**: Security tensors significantly improve harmful image rejection in LVLMs without modifying model weights.

## Executive Summary
This paper introduces security tensors, trainable input perturbations that extend text-aligned safety mechanisms to visual inputs in large visual-language models (LVLMs). The method bridges the cross-modal safety gap by optimizing imperceptible perturbations applied to either image or text embeddings during preprocessing. By training on a curated dataset containing malicious image-text pairs, benign pairs with similar text structure, and general benign samples, security tensors enable LVLMs to reject harmful visual content while maintaining performance on benign tasks. Experiments demonstrate significant improvements in Harmless Rate (HR) on harmful inputs across multiple LVLM architectures without requiring model parameter modifications.

## Method Summary
The approach uses trainable security tensors (δ) as input perturbations to bridge safety alignment from text to vision in LVLMs. Two types of tensors are introduced: visual tensors (δv) added to preprocessed image representations, and textual tensors (δt) inserted in the embedding space between image and text tokens. The training dataset consists of 1,000 samples divided into Safety Activation (SA) sets with malicious images and benign text, Text Contrast Benign (TCB) sets with benign images and structurally similar text to prevent overfitting, and General Benign (GB) sets. Optimization minimizes a dual-objective loss combining Cross-Entropy for SA samples and KL divergence for GB/TCB samples, with model weights frozen and tensors constrained for imperceptibility.

## Key Results
- Visual security tensors achieved 86.43% Harmless Rate on seen-category harmful images for LLaMA-3.2-11B-Vision with minimal impact on benign tasks.
- Textual security tensors showed consistent improvements across all tested LVLM architectures (LLaMA-3.2-11B-Vision, Qwen-VL-Chat, LLaVA-1.5).
- Security tensors demonstrated generalization to unseen harmful categories while maintaining low false rejection rates on benign inputs.
- Internal analysis revealed that security tensors activate language modules' safety layers when processing harmful visual inputs.

## Why This Works (Mechanism)
Security tensors work by providing a cross-modal bridge that activates the language module's existing safety mechanisms when processing harmful visual content. The tensors are optimized to create perturbations that trigger safety responses specifically for malicious image-text pairs while remaining imperceptible on benign inputs. The dual training objective ensures the tensors learn to distinguish harmful content based on visual features rather than text patterns, with the TCB set preventing overfitting to textual heuristics.

## Foundational Learning
- **LVLM Architecture**: Understanding how visual and language modules interact is crucial for implementing tensor injection points. Quick check: Verify tensor dimensions match model-specific preprocessed sizes.
- **Safety Alignment Mechanisms**: Knowledge of how text-based safety works in LVLMs helps explain why security tensors can bridge to vision. Quick check: Confirm base model has functional safety layers before applying tensors.
- **Input Perturbation Optimization**: Understanding how small perturbations can significantly affect model behavior is essential. Quick check: Monitor tensor norms to ensure they remain imperceptible.

## Architecture Onboarding

**Component Map**: Image Preprocessing -> Security Tensor Injection -> LVLM Backbone -> Safety Layer Activation

**Critical Path**: The security tensor must be injected at the correct point in the preprocessing pipeline before the vision encoder processes the image representation. For visual tensors, this occurs immediately after image preprocessing; for textual tensors, this occurs between image and text embeddings.

**Design Tradeoffs**: The method trades computational overhead at inference time for improved safety without model modification. Visual tensors require more careful constraint tuning due to larger dimensionality and slower convergence compared to textual tensors.

**Failure Signatures**: High False Rejection Rate (FRR) on benign inputs indicates overfitting to text patterns rather than visual features. Loss divergence during training suggests issues with gradient flow or constraint violations.

**Three First Experiments**:
1. Implement visual tensor injection and verify imperceptible perturbations on benign images through norm analysis.
2. Train security tensors on a small dataset and measure HR improvements on a held-out harmful test set.
3. Compare FRR on TCB test queries versus general benign queries to diagnose text-pattern overfitting.

## Open Questions the Paper Calls Out
- Can additional strategies beyond the Text Contrast Benign (TCB) set be developed to further mitigate text-pattern overfitting? The paper notes high FRR on TCB-style test queries despite using the TCB set, suggesting the need for novel regularization techniques.
- How robust are security tensors against adaptive adversarial attacks specifically designed to neutralize input-level perturbations? The paper does not assess resilience against attackers who might craft inputs to bypass the fixed δv or δt.
- To what extent does the efficacy of security tensors depend on the pre-existing strength of the language module's textual safety alignment? The method relies on activating "intrinsic capacity," but its effectiveness for poorly aligned models remains untested.

## Limitations
- The approach depends heavily on the quality and representativeness of the curated training dataset, with specific refusal templates and TCB text generation methodology not fully disclosed.
- Security tensor effectiveness is highly sensitive to the preprocessing pipeline and constraint threshold λ, requiring careful tuning.
- The method adds computational overhead at inference time due to additional tensor operations, potentially impacting real-time applications.

## Confidence
**High Confidence**: The core methodology of using trainable input perturbations to extend text-aligned safety to visual inputs is clearly defined and supported by experimental results showing improved Harmless Rate on harmful inputs while maintaining performance on benign tasks.

**Medium Confidence**: Implementation details of the loss function and optimization procedure are clear, but the precise impact of hyperparameters on final performance requires further validation. Reported improvements are plausible but rely on evaluation dataset quality.

**Low Confidence**: Long-term generalization capability to entirely novel types of harmful visual content beyond tested categories is uncertain. Potential for adaptive attacks targeting the security tensor mechanism is not addressed.

## Next Checks
1. **Dataset Fidelity Check**: Verify the constructed training dataset accurately reflects the described distribution (400 SA, 400 TCB, 200 GB samples) and that text prompts maintain required syntactic similarity while having divergent semantics to prevent text-pattern overfitting.

2. **Tensor Constraint Verification**: Implement and test the λ=1 constraint on visual tensor to ensure it produces truly imperceptible perturbations on benign inputs, quantifying L2 norms on both benign and harmful images.

3. **Unseen Category Generalization Test**: Extend evaluation to include broader harmful categories not present in training data (e.g., weapons, hate symbols) to rigorously test generalization claims and compare against baseline text-only safety mechanisms.