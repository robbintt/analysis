---
ver: rpa2
title: Existing Large Language Model Unlearning Evaluations Are Inconclusive
arxiv_id: '2506.00688'
source_url: https://arxiv.org/abs/2506.00688
tags:
- unlearning
- arxiv
- evaluations
- information
- unlearned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies critical shortcomings in existing adversarial
  unlearning evaluation methods for large language models, showing they often inject
  new information or depend on task-specific formats, leading to inconclusive results.
  The authors propose two key principles for future evaluations: minimal information
  injection and downstream task awareness.'
---

# Existing Large Language Model Unlearning Unlearning Evaluations Are Inconclusive

## Quick Facts
- arXiv ID: 2506.00688
- Source URL: https://arxiv.org/abs/2506.00688
- Reference count: 40
- The paper identifies critical shortcomings in existing adversarial unlearning evaluation methods for large language models, showing they often inject new information or depend on task-specific formats, leading to inconclusive results.

## Executive Summary
This paper systematically evaluates existing adversarial methods for assessing large language model unlearning and finds them fundamentally flawed. The authors demonstrate that common evaluation approaches—including finetuning attacks, input-space attacks like Enhanced GCG, and memorization detectors like ACR—either inject substantial new information into models or produce inconsistent results across different task formats. Through experiments with models like Zephyr-7B, Phi-1.5, and Llama-3.2-1B-Instruct, they show that these evaluation failures mask true unlearning performance by either re-teaching models during testing or depending on spurious dataset correlations. The paper concludes that current unlearning evaluation practices are inconclusive and proposes two key principles for future evaluations: minimizing information injection and maintaining downstream task awareness.

## Method Summary
The study evaluates unlearning methods (RMU and NPO) on multiple models using three evaluation approaches: finetuning attacks with LoRA rank 128, Enhanced GCG input-space attacks with 100-token prefixes, and ACR memorization detectors. Models are tested on MCQ benchmarks (TOFU-MCQ with 2000 questions, WMDP-Bio) using three formats: CHOOSE (max letter probability), OPTION (max vocabulary probability), and GENERATE (greedy decoding match). Finetuning uses specified hyperparameters (lr=2e-4, 3-5 epochs) while ACR optimization employs lr=1e-2 for 200-350 steps. The evaluation examines whether adversarial methods recover forget-set knowledge and whether results vary across task formats.

## Key Results
- Finetuning attacks and input-space attacks can inadvertently reintroduce forget-set knowledge through information injection
- Memorization detectors yield inconsistent results across different output formats (CHOOSE vs OPTION vs GENERATE)
- Unlearning evaluation outcomes vary significantly across tasks, undermining generalizability
- Standard evaluations can mask true unlearning performance by re-teaching models during testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial evaluation methods can inadvertently inject new information into unlearned models, confounding measurement of true unlearning.
- Mechanism: Optimized prompts (Enhanced GCG) or finetuning trajectories have sufficient information capacity to encode forget-set knowledge directly. The 100-token adversarial prefix carries ~1500 bits, enough to encode correct answers for the WMDP-Bio MCQ test set (~1430 bits required). The evaluation process thus re-teaches rather than reveals.
- Core assumption: LLMs function as universal sequence approximators capable of storing and retrieving information injected via optimized inputs or weight updates.
- Evidence anchors:
  - [abstract] "some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing"
  - [section 4.2] Enhanced GCG optimization on 7 samples boosts base model accuracy from 39.9% to 53.8% on WMDP-Bio
  - [corpus] Related work (Leak@$k$) confirms unlearning methods fail under probabilistic decoding, suggesting evaluation methodology critically affects conclusions
- Break condition: When the information budget (tokens × log(vocabulary_size)) of the adversarial intervention exceeds the bits required to encode the forget-set answers, the evaluation is confounded.

### Mechanism 2
- Claim: Unlearning evaluation outcomes are highly sensitive to downstream task format, making single-metric conclusions unreliable.
- Mechanism: MCQ evaluation via maximum letter probability (Equation 4) probes only single-token predictions, while maximum text probability (Equation 5) and open-ended generation require coherent multi-token reasoning. Unlearning methods like NPO may suppress letter-selection behavior while leaving generative capabilities intact—or vice versa.
- Core assumption: Different output formats engage different model capabilities and memorization pathways.
- Evidence anchors:
  - [abstract] "evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines"
  - [section 4.3, Figure 6] NPO models recover base performance on letter probability but not text probability; RMU shows opposite pattern on generation tasks
  - [corpus] BLUR benchmark notes forget-retain overlap complicates evaluation; related work confirms format sensitivity
- Break condition: When evaluation conclusions (which method is "better") reverse depending on whether you use CHOOSE, OPTION, or GENERATE tasks.

### Mechanism 3
- Claim: Benchmark datasets contain spurious correlations that enable illegitimate generalization from retain to forget sets.
- Mechanism: Finetuning on retain-set samples improves performance on held-out forget-set data because the retain and forget sets share underlying patterns or formatting artifacts, not because knowledge was recovered. This violates the assumed independence between sets.
- Core assumption: Retain and forget sets should be informationally disjoint for valid evaluation.
- Evidence anchors:
  - [abstract] "many evaluations rely on spurious correlations, making their results difficult to trust"
  - [section 4.1, Figure 2] Finetuning Phi-1.5 on TOFU-MCQ subsets improves accuracy on held-out data about "distinct" fictitious authors
  - [corpus] Related paper "Machine Unlearning Fails to Remove Data Poisoning Attacks" shows similar generalization issues
- Break condition: When a model trained only on retain data achieves above-baseline accuracy on forget-set questions it has never seen.

## Foundational Learning

- Concept: **Adversarial Compression Ratio (ACR)**
  - Why needed here: Provides a memorization metric that quantifies how efficiently a prompt can elicit target text; used as a "yardstick" for unlearning evaluation
  - Quick check question: If ACR remains unchanged after unlearning, what does this suggest about the unlearning method's effectiveness?

- Concept: **Finetuning attacks as upper bounds**
  - Why needed here: Establishes the theoretical maximum recovery rate for any adversarial evaluation; if finetuning can't recover knowledge, weaker attacks won't either
  - Quick check question: Why does finetuning on retain-set samples (rather than forget-set samples) provide stronger evidence of unlearning failure?

- Concept: **Information-theoretic capacity of adversarial interventions**
  - Why needed here: Essential for implementing the "minimal information injection" principle—quantifying how many bits an evaluation can leak
  - Quick check question: How would you calculate whether a 100-token adversarial prefix has sufficient capacity to encode answers for a 1000-question MCQ benchmark?

## Architecture Onboarding

- Component map:
  - Base model → Unlearning algorithm (RMU/NPO) → Unlearned model → Evaluation pipeline (finetuning attacks / input-space attacks / memorization detectors)
  - Evaluation modes: CHOOSE (letter probability), OPTION (vocabulary-wide probability), GENERATE (open-ended)
  - Information budget tracking: tokens × log₂(vocabulary_size)

- Critical path:
  1. Select unlearning method and forget/retain split
  2. Apply multiple evaluation formats (cross-modality leakage matrix)
  3. Quantify information injection for each evaluation
  4. Check for spurious generalization by testing retain-only finetuning

- Design tradeoffs:
  - Stronger evaluations (more finetuning steps, longer prompts) increase information injection risk
  - MCQ formats are efficient but probe superficial behavior; generation is comprehensive but noisy
  - ACR provides strict memorization test but may miss non-memorized yet recoverable knowledge

- Failure signatures:
  - Contradictory rankings across task formats (RMU beats NPO on generation, NPO beats RMU on MCQ)
  - Retain-only finetuning improves forget-set accuracy
  - Adversarial prefix length approaches bits-needed threshold for forget-set encoding

- First 3 experiments:
  1. Replicate Figure 6 experiment: Evaluate same unlearned model using all three MCQ formats (CHOOSE/OPTION/GENERATE) on WMDP-Bio to verify format sensitivity
  2. Information budget audit: Calculate bits required to encode your forget-set answers vs. bits available in your adversarial evaluation method
  3. Spurious correlation test: Finetune a fresh base model on retain-set only and measure forget-set accuracy; any improvement above baseline indicates dataset contamination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can rigorous, non-heuristic metrics be developed to quantify the "information injection" budget in adversarial unlearning evaluations?
- Basis in paper: [explicit] Recommendation 1 states future work should "seek to measure of injected information" beyond the heuristic bit-counting or spurious generalization measures currently used.
- Why unresolved: The paper demonstrates that current estimates (e.g., counting tokens or bits) are insufficient because LLMs process information densely, making it hard to decouple evaluation artifacts from model knowledge.
- What evidence would resolve it: A formal metric, grounded in information theory, that successfully distinguishes between knowledge recovered from the model's weights versus knowledge introduced by the evaluation prompt or finetuning process.

### Open Question 2
- Question: How can the proposed "cross-modality leakage matrix" be standardized to prevent evaluations from relying on narrow task formats like multiple-choice questions?
- Basis in paper: [explicit] Recommendation 2 proposes reporting a cross-format leakage matrix but notes that defining a standard for "cross-format" robustness is necessary for future safety claims.
- Why unresolved: The authors show that unlearning success rates vary drastically between "CHOOSE," "OPTION," and "GENERATE" tasks, but a unified framework to aggregate these into a single reliability score does not exist.
- What evidence would resolve it: A benchmark suite that validates unlearning across diverse modalities (e.g., MCQ, open-ended generation, log-likelihood) and demonstrates that a model passing the suite cannot be exploited by switching formats.

### Open Question 3
- Question: To what extent do spurious correlations within benchmark datasets (like TOFU) invalidate the assumptions of independence between retain and forget sets?
- Basis in paper: [inferred] From the "Evidence of spurious generalization" in Section 4.1, where finetuning on retain data paradoxically improves accuracy on held-out forget data.
- Why unresolved: If benchmarks contain implicit links between retain and forget data, finetuning attacks may appear to "recover" knowledge that was never truly unlearned, rather simply re-learning spurious associations.
- What evidence would resolve it: The creation of a dataset where rigorous statistical tests confirm the independence of retain and forget sets, or an algorithmic method to sanitize existing benchmarks of these correlations.

## Limitations
- Analysis focuses primarily on MCQ-style benchmarks, which may not represent the full spectrum of unlearning evaluation scenarios
- Examines only two unlearning methods (RMU and NPO) and limited model sizes, raising questions about generalizability
- Doesn't fully explore whether format sensitivity reflects genuine differences in recoverable knowledge versus evaluation artifacts

## Confidence

- **High confidence**: The identification of format sensitivity across CHOOSE/OPTION/GENERATE tasks is well-supported by experimental evidence (Figure 6) showing contradictory rankings of unlearning methods depending on evaluation format. The information-theoretic argument about adversarial prefixes having sufficient capacity to encode forget-set answers is mathematically sound and supported by the Enhanced GCG results showing dramatic accuracy improvements.

- **Medium confidence**: The spurious correlation analysis (finetuning on retain-only data improving forget-set accuracy) demonstrates a real phenomenon, but the paper doesn't fully quantify how prevalent this issue is across different dataset splits or whether it represents a fundamental flaw versus a controllable experimental artifact.

- **Low confidence**: The broader claim that "existing evaluations are inconclusive" may overstate the case, as the paper doesn't demonstrate that all current evaluation approaches suffer from these issues, nor does it provide a comprehensive survey of the evaluation landscape beyond the specific methods tested.

## Next Checks

1. **Dataset independence validation**: Systematically test multiple random splits of TOFU and WMDP datasets to quantify how often retain-only finetuning produces spurious forget-set improvements, establishing the prevalence and severity of this issue across different dataset configurations.

2. **Cross-format consistency matrix**: Evaluate a diverse set of unlearning methods (including Exact Neural Erasure and SISA) across all three MCQ formats plus open-ended generation tasks on multiple benchmarks, creating a comprehensive matrix to identify which methods show consistent versus format-dependent performance.

3. **Information budget calibration**: For each unlearning method and evaluation approach, calculate the actual information content of adversarial interventions versus the theoretical capacity needed to encode forget-set answers, then empirically test whether removing or truncating information from adversarial prompts reduces their effectiveness in a predictable manner.