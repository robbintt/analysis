---
ver: rpa2
title: 'Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations
  in Healthcare and Beyond'
arxiv_id: '2502.03945'
source_url: https://arxiv.org/abs/2502.03945
tags:
- medical
- speech
- arxiv
- summary
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Afrispeech-Dialog, a benchmark dataset of
  50 simulated African-accented English conversations in healthcare and general domains.
  The dataset is used to evaluate state-of-the-art speaker diarization, ASR, and LLM-based
  summarization models.
---

# Afrispeech-Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond

## Quick Facts
- **arXiv ID:** 2502.03945
- **Source URL:** https://arxiv.org/abs/2502.03945
- **Reference count:** 40
- **Primary result:** Benchmark of 50 African-accented English healthcare and general conversations reveals 10%+ WER degradation for accented speech and 2-5 point drop in LLM summarization quality when using ASR transcripts

## Executive Summary
This paper introduces Afrispeech-Dialog, a benchmark dataset of 50 simulated African-accented English conversations in healthcare and general domains. The dataset is used to evaluate state-of-the-art speaker diarization, ASR, and LLM-based summarization models. Speaker diarization models achieved DERs ranging from 16.27% to 26.87%, with performance degrading by 10%+ compared to native accents. ASR models showed WERs of 20.38%-86.34%, with open-source models performing better than wav2vec2. LLM summarization models achieved BERTScores of 85.48%-91.34% when using human transcripts, with a 2-5 point drop when using ASR transcripts. The results highlight challenges in accented speech recognition and the impact of ASR errors on downstream tasks.

## Method Summary
The Afrispeech-Dialog dataset contains 50 simulated African-accented English conversations (~7 hours total) in medical and general domains, featuring 11 different African accents. The dataset includes manually transcribed audio with timestamps and speaker labels. The benchmark evaluates three tasks: speaker diarization using Pyannote, Reverb diarization v2, and Titanet-L; ASR using multiple models including Whisper variants, Canary, and wav2vec2; and LLM-based summarization using models like GPT-4o, Claude-3-Sonnet, and Med42. Evaluation metrics include Diarization Error Rate (DER), Word Error Rate (WER), BERTScore for semantic similarity, and LLM-as-Judge scores based on six criteria.

## Key Results
- Speaker diarization DER ranges from 16.27% to 26.87% overall, degrading by 10%+ compared to native accents
- ASR WER ranges from 20.38% to 86.34%, with open-source models outperforming wav2vec2
- Medical conversations show ~5% higher WER than general conversations
- LLM summarization achieves BERTScores of 85.48%-91.34% with human transcripts, dropping 2-5 points with ASR transcripts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** African-accented English causes 5-20% absolute WER degradation in state-of-the-art ASR systems compared to native-accented conversational benchmarks.
- **Mechanism:** Pre-trained ASR models (Whisper, Wav2Vec2, etc.) are optimized on speech patterns from high-resource, predominantly native-accented training corpora. African accents introduce phonological variations (vowel quality, stress patterns, intonation) that fall outside the learned acoustic-phonetic distributions, increasing substitution and deletion errors.
- **Core assumption:** The degradation stems primarily from acoustic mismatch rather than vocabulary/domain mismatch.
- **Evidence anchors:**
  - [abstract]: "10%+ performance degradation" on accented speech vs native accents
  - [section 5.2, Table 5]: Whisper-large-v3 achieves 20.38% WER on Afrispeech-Dialog vs 16.01% on AMI, 11.3% on Earnings22
  - [corpus]: Related work (AfriSpeech-MultiBench, arXiv:2511.14255) corroborates accented English ASR gaps; no direct counter-evidence found.
- **Break condition:** If accent-specific fine-tuning data >100 hours is added, expect degradation to narrow by 30-50% (assumption based on Olatunji et al., 2023b).

### Mechanism 2
- **Claim:** Medical conversations incur ~5% higher WER than general-domain conversations for the same accented speech.
- **Mechanism:** Medical dialogue contains denser numerical entities (dosages, vital signs), specialized terminology (medication names, diagnoses), and more structured turn-taking. Accent-affected phoneme confusion compounds with domain vocabulary not well-represented in general-purpose ASR training data.
- **Core assumption:** Medical terminology error rates are accent-sensitive, not just domain-sensitive.
- **Evidence anchors:**
  - [section 5.2]: "models exhibit superior performance on non-medical audio... roughly 5% better WER"
  - [section 3.1.2]: Medical conversations have higher turn counts (78.6 avg) vs general (30.55 avg)
  - [corpus]: Greek medical dictation ASR (arXiv:2509.23550) shows domain-specific challenges, but lacks accent comparison.
- **Break condition:** If medical-specific vocabulary adaptation is applied without accent adaptation, the 5% gap may persist.

### Mechanism 3
- **Claim:** ASR transcription errors propagate to downstream LLM summarization, causing 2-5 point drops in clinical information extraction accuracy.
- **Mechanism:** The cascading pipeline (Diarization → ASR → LLM) is lossy. ASR errors (especially deletions of clinical keywords like medication names or symptom descriptors) reduce the signal available to the summarization model. BERTScore remains stable because it captures semantic similarity at a coarse level, but fine-grained clinical detail recall degrades.
- **Core assumption:** The summarization model does not robustly infer missing clinical information from context.
- **Evidence anchors:**
  - [section 5.5]: "LLM-Eval-M scores are on average 2 to 5 points lower than LLM-Eval-H"
  - [Table 6]: BERTScore-H vs BERTScore-M shows minimal change, but LLM-Eval-H vs LLM-Eval-M shows notable drops for most models
  - [corpus]: No corpus papers directly address ASR-to-summarization error propagation; evidence is dataset-specific.
- **Break condition:** If LLMs are fine-tuned on noisy ASR transcripts with correction objectives, propagation may reduce (untested).

## Foundational Learning

- **Word Error Rate (WER):**
  - Why needed here: Primary metric for ASR evaluation; measures edit distance (substitutions, insertions, deletions) normalized by reference word count.
  - Quick check question: If a transcript has 100 words, 5 substitutions, 2 deletions, and 3 insertions, what is the WER? (Answer: 10%)

- **Diarization Error Rate (DER):**
  - Why needed here: Measures speaker attribution errors (missed speech, false alarm, speaker confusion) as percentage of total speech time.
  - Quick check question: If DER is 16.27% on general conversations but 34.64% on medical, what does this suggest about speaker turn structure? (Answer: Medical conversations have more complex/rapid turn-taking.)

- **Cascading Pipeline Error Propagation:**
  - Why needed here: Understanding how upstream errors (diarization, ASR) compound in downstream tasks (summarization).
  - Quick check question: If ASR WER is 20%, would you expect downstream summarization BERTScore to drop by ~20%? Why or why not? (Answer: No—BERTScore is semantic and robust to minor word-level errors; finer-grained metrics reveal the true loss.)

## Architecture Onboarding

- **Component map:**
  Audio Input → Speaker Diarization → ASR (per segment) → Transcript → LLM Summarizer → Summary
  (Titanet/Pyannote)      (Whisper/Canary)                         (GPT-4o/Claude)

- **Critical path:** ASR accuracy is the bottleneck. A 10% WER improvement yields measurable downstream summarization gains (per LLM-Eval-H vs LLM-Eval-M). Diarization errors compound but are less impactful for 2-speaker conversations.

- **Design tradeoffs:**
  - Larger ASR models (Whisper-large-v3) vs faster inference (distil-whisper): 5% WER penalty for 2x speedup.
  - General LLMs (GPT-4o) vs biomedical LLMs (Med42): General models score higher on clinical detail recall in this benchmark; biomedical models may over-constrain.
  - Simulated vs real-world data: Simulated conversations are cleaner; real-world deployment will show higher DER/WER.

- **Failure signatures:**
  - Medical conversations: DER spikes (30-58%) due to rapid turn-taking and interruptions.
  - Numerical entities: Dosages and vital signs prone to substitution errors.
  - Code-switching: Brief interjections in non-English languages may be dropped entirely.

- **First 3 experiments:**
  1. Baseline replication: Run Whisper-large-v3 on Afrispeech-Dialog, compute WER and compare to Table 5 values.
  2. Accent-specific evaluation: Subset analysis by accent type (Hausa, Yoruba, Swahili, etc.) to identify highest-error phoneme patterns.
  3. Error propagation trace: For a single medical conversation, manually annotate ASR errors and measure their impact on LLM summary clinical detail recall (using LLM-Eval criteria).

## Open Questions the Paper Calls Out

- **Question:** To what extent does the performance of diarization and ASR models on simulated African-accented conversations generalize to real-world, spontaneous clinical dialogues?
- **Basis in paper:** [Explicit] The authors state in the Limitations section that simulated conversations "may not fully reflect the complexity of real-world natural dialogues" where interruptions and background noise are typical.
- **Why unresolved:** The dataset relies on actors following "patient cards" and "topic cards," which creates a structured environment that differs from the chaotic nature of actual hospital workflows or casual conversation.
- **What evidence would resolve it:** A comparative study benchmarking these models on both the Afrispeech-Dialog dataset and a newly collected corpus of authentic, in-clinic African conversations.

- **Question:** Can Large Language Models (LLMs) be specifically fine-tuned or prompted to maintain summary accuracy when processing the distinct error patterns found in African-accented ASR outputs?
- **Basis in paper:** [Inferred] The results in Section 5.5 demonstrate that summary quality drops when using machine transcripts (LLM-Eval-M) compared to human transcripts (LLM-Eval-H), indicating that models struggle to handle the noise introduced by accented ASR.
- **Why unresolved:** The paper establishes that error propagation occurs (a 2-5 point drop in evaluation scores), but does not test methods to mitigate this specific type of signal degradation in the summarization pipeline.
- **What evidence would resolve it:** Experiments testing domain-specific robustness techniques (e.g., noisy channel modeling or accent-specific data augmentation) on the summarization task using the provided dataset.

- **Question:** What specific acoustic or structural features of the medical conversations cause the significant performance gap in speaker diarization compared to general domain conversations?
- **Basis in paper:** [Inferred] Table 4 and Section 5.1 show that diarization error rates (DER) are significantly higher for medical conversations (e.g., 58.04% for Reverb) compared to general conversations (20.10%), despite general conversations having longer speaking turns.
- **Why unresolved:** The paper speculates that the "structured" nature of medical exams might be a factor but does not perform a fine-grained error analysis to determine if the issue stems from shorter turn durations, specific frequency bands in accented speech, or speaker embedding confusion.
- **What evidence would resolve it:** A breakdown of DER components (false alarm, missed detection, speaker confusion) correlated with turn-taking metrics and signal-to-noise ratios across the medical subset.

## Limitations

- The dataset size (50 conversations, ~7 hours total) limits statistical power for drawing broad conclusions about African-accented English ASR performance.
- The simulated nature of the conversations may not capture the full variability of real-world spontaneous speech including disfluencies and background noise.
- The benchmark focuses on two-speaker conversations, which may not generalize to multi-party medical consultations common in healthcare settings.

## Confidence

**High Confidence** (Multiple corroborating sources, well-established methodology):
- The 10%+ WER degradation for accented speech compared to native accents is consistent with broader ASR literature on accented speech challenges
- The 5% WER difference between medical and general domains aligns with domain adaptation research
- The error propagation from ASR to LLM summarization (2-5 point drop) is mechanistically sound

**Medium Confidence** (Dataset-specific findings, limited external validation):
- Specific DER ranges (16.27%-26.87%) for the three diarization models are only validated on this dataset
- The ranking of LLM summarization models is based on this particular medical/general conversation style
- The particular accent groups represented may not reflect all African English varieties

**Low Confidence** (Limited external validation, simulation constraints):
- The absolute performance numbers may differ significantly in real-world clinical settings
- The impact of specific accent features on error patterns would require phonetic analysis beyond this benchmark
- The generalizability to other healthcare tasks beyond summarization is untested

## Next Checks

1. **External Dataset Validation**: Test the best-performing models (Whisper-large-v3, Pyannote) on AfriSpeech-MultiBench or other African-accented English datasets to verify whether the ~10% degradation generalizes beyond this specific dataset.

2. **Accent-Specific Error Analysis**: Perform phoneme-level error analysis on the ASR transcripts to identify which specific accent features (vowel quality, consonant realization, prosody) contribute most to the degradation, validating the acoustic-mismatch mechanism.

3. **Real-World Deployment Study**: Conduct a small pilot study using these models on actual recorded healthcare conversations (with appropriate consent) to measure the gap between simulated and real-world performance, particularly for medical terminology recognition.