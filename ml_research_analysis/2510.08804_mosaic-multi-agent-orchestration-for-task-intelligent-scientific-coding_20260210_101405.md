---
ver: rpa2
title: 'MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding'
arxiv_id: '2510.08804'
source_url: https://arxiv.org/abs/2510.08804
tags:
- arxiv
- code
- mosaic
- scientific
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MOSAIC, a multi-agent LLM framework designed
  to solve challenging scientific coding tasks. Scientific workflows require precise
  algorithms, domain-specific reasoning, and often involve interdependent subproblems,
  making existing code-generation methods inadequate.
---

# MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding

## Quick Facts
- arXiv ID: 2510.08804
- Source URL: https://arxiv.org/abs/2510.08804
- Reference count: 40
- Primary result: MOSAIC achieves 20.01% main problem-solving rate and 41.69% subproblem-solving rate with GPT-4o, an 8.5% improvement over baselines

## Executive Summary
MOSAIC is a training-free multi-agent LLM framework designed to solve complex scientific coding tasks that require precise algorithms, domain-specific reasoning, and handling of interdependent subproblems. The framework orchestrates four specialized agents (Self-Reflection, Rationale, Coding, and Debugging) within a student-teacher paradigm, using few-shot examples from validation data to guide algorithm generation. MOSAIC employs a Consolidated Context Window to mitigate hallucinations and iteratively refines code without requiring I/O test cases. It outperforms existing approaches across three LLM backbones, demonstrating the effectiveness of orchestrated agent specialization for scientific coding tasks.

## Method Summary
MOSAIC implements a multi-agent architecture where problems are first routed to scientific domains, then processed through a teacher module that retrieves few-shot examples from validation data. The self-reflection agent generates domain-specific pseudocode from ground-truth rationales, while the rationale agent uses a Consolidated Context Window to produce step-by-step plans for current subproblems. The coding agent converts these plans to executable code, and the debugger agent performs iterative syntactic error correction. The framework uses domain-specific memory to prevent cross-domain interference and employs whole-rationale self-reflection rather than stepwise processing to maintain problem context across chained subproblems.

## Key Results
- Achieves 20.01% main problem-solving rate and 41.69% subproblem-solving rate with GPT-4o
- Demonstrates 8.5% improvement over baseline approaches
- Reduces syntactic errors and improves numerical precision in scientific code generation
- Shows domain-specific performance variations (Physics: 145 subproblems, Biology: 0/7 problems with GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1: Consolidated Context Window
The CCW mitigates hallucinations by constraining context to essential information across chained subproblems. It provides only prior function signatures and one-sentence summaries rather than full code history, reducing noise while preserving inter-task dependencies. Core assumption: Full code history introduces irrelevant fragments that cause models to replicate code inappropriately; concise abstractions retain critical structure without the noise. Evidence: Table III shows unrestricted CCW drops performance to 4/65 main problems vs. restricted CCW achieving 12/65. Break condition: Problems with >10 subproblems show context maintenance failures even with CCW.

### Mechanism 2: Whole-Rationale Self-Reflection
Whole-rationale self-reflection preserves problem context better than stepwise reflection for scientific domains. Processing the entire rationale at once maintains holistic understanding of interdependencies, whereas stepwise processing fragments context and prevents knowledge transfer across related tasks. Core assumption: Scientific problems have tightly coupled subproblems; losing the global view during reflection breaks the reasoning chain. Evidence: Table III ablation shows stepwise self-reflection solves 6/65 main problems vs. 12/65 for whole-rationale reflection. Break condition: Highly modular problems with minimal interdependencies may not benefit from whole-rationale reflection.

### Mechanism 3: Domain-Specific Memory with Agent Orchestration
Domain-specific memory with specialized agent orchestration prevents cross-domain interference and enables complementary problem-solving. Each scientific domain maintains dedicated memory, while four specialized agents solve different problem aspects. Core assumption: Scientific domains have distinct reasoning patterns; mixing them degrades performance. Evidence: Table I shows 8.5-24% improvement over baselines across three LLM backbones. Break condition: Isolated agent addition reduced performance in ablation studies (Table III line 3: 4/65 vs. baseline 7/65).

## Foundational Learning

- **Knowledge Distillation (Teacher-Student)**: Why needed: MOSAIC uses few-shot examples from validation data to teach domain-specific patterns without fine-tuning. Quick check: Can you explain how ground-truth rationales guide the student agent to generalize to test problems?
- **Chain-of-Thought with Self-Reflection**: Why needed: Scientific problems require explicit reasoning steps that must be critically evaluated before coding. Quick check: How does self-reflection differ from simple chain-of-thought in terms of error detection?
- **Context Window Management**: Why needed: Chained subproblems grow context; unmanaged accumulation causes hallucinations and irrelevant code replication. Quick check: What information should be preserved vs. discarded when managing context across 10+ subproblems?

## Architecture Onboarding

- **Component map**: Problem → Bucketing → Domain assignment → Teacher module → Self-Reflection Agent → Rationale Agent (with CCW) → Coding Agent → Debugger Agent → Output
- **Critical path**: 1) Problem routed to domain, 2) Teacher retrieves few-shot examples, 3) Self-Reflection generates pseudocode, 4) Rationale produces detailed plan, 5) Coding implements code, 6) Debugger fixes errors iteratively, 7) Output fed back to CCW for next subproblem
- **Design tradeoffs**: CCW scope: function signatures only (chosen) vs. full code (43% performance drop); Self-reflection granularity: whole-rationale (chosen) vs. stepwise (14% drop); Memory architecture: domain-specific (chosen) vs. shared (10-12% drop); LLM backbone: homogeneous (chosen) vs. heterogeneous (future work)
- **Failure signatures**: Biology domain consistently lowest performance (0/7 problems) due to limited validation exemplars; long chains (>10 subproblems) show context loss even with CCW; isolated agent addition reduces performance below baseline; LLM instruction following sensitive to prompt changes
- **First 3 experiments**: 1) Reproduce Table III ablation incrementally to validate orchestration as causal factor; 2) CCW ablation on single domain comparing no CCW, full-code CCW, and signature-only CCW; 3) Cross-domain interference test measuring degradation from physics-only to mixed-domain memory

## Open Questions the Paper Calls Out

None

## Limitations
- Domain transfer claims lack empirical validation beyond the five studied scientific domains
- Biology domain shows consistently poor performance (0/7 problems), suggesting fundamental limitations in handling conceptual reasoning
- The framework's reliance on ground-truth rationales for self-reflection raises questions about real-world applicability
- Claims about "numerical precision" improvements lack quantitative validation metrics

## Confidence
- **Low** for domain transfer claims: Limited to five domains with single LLM evaluation
- **Medium** for CCW mechanism: Ablation study supports claims but mechanism details remain underspecified
- **High** for orchestration hypothesis: Incremental ablation demonstrates clear performance progression and validates agent synergy

## Next Checks
1. **Cross-Domain Contamination Test**: Run identical physics problems with physics-only memory vs. mixed-domain memory containing chemistry/biology examples vs. adversarial domains to quantify cross-domain interference and validate domain-specific memory design.
2. **CCW Granularity Experiment**: For Physics domain with 50+ subproblems, systematically vary CCW content (no CCW, function signatures only, signatures + summaries, full code history) to measure accuracy and hallucination frequency.
3. **Agent Isolation Validation**: Replicate ablation showing isolated agent addition drops performance below baseline, and test each agent in isolation with and without CCW to determine whether orchestration benefit comes from agent synergy or CCW alone.