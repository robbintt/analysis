---
ver: rpa2
title: 'Min-p, Max Exaggeration: A Critical Analysis of Min-p Sampling in Language
  Models'
arxiv_id: '2506.13681'
source_url: https://arxiv.org/abs/2506.13681
tags:
- min-p
- sampling
- sampler
- diversity
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper critically examines claims that the "min-p" sampling
  method improves quality and diversity of language model outputs. The authors identify
  four key issues with the supporting evidence: 1) Human evaluation data was incomplete,
  with one baseline sampler''s scores omitted without explanation; 2) Statistical
  tests failed to support claims of consistent superiority; 3) Qualitative feedback
  actually favored basic sampling over min-p; 4) New human evaluations using different
  implementations still showed no advantage for min-p.'
---

# Min-p, Max Exaggeration: A Critical Analysis of Min-p Sampling in Language Models

## Quick Facts
- arXiv ID: 2506.13681
- Source URL: https://arxiv.org/abs/2506.13681
- Authors: Rylan Schaeffer; Joshua Kazdan; Yegor Denisov-Blanch
- Reference count: 40
- Key outcome: The paper finds no evidence that min-p sampling consistently improves quality or diversity of language model outputs compared to established methods.

## Executive Summary
This paper critically examines claims that the "min-p" sampling method improves the quality and diversity of language model outputs. The authors identify four key issues with the supporting evidence: incomplete human evaluation data, failed statistical tests, qualitative feedback favoring basic sampling over min-p, and new evaluations showing no advantage for min-p. Additional analyses reveal that min-p does not outperform other samplers on NLP benchmarks when controlling for hyperparameter volume, and LLM-as-a-judge evaluations showed inconsistent reporting and methodological issues. The claimed community adoption metrics were found to be unsubstantiated and later retracted. The authors conclude that evidence fails to support claims that min-p improves quality, diversity, or their trade-off compared to established methods.

## Method Summary
The authors conducted a comprehensive re-evaluation of min-p sampling through multiple approaches. They performed new human evaluations using different implementations of min-p and compared it against standard sampling methods. They analyzed performance on NLP benchmarks while controlling for hyperparameter volume to ensure fair comparisons. The team also examined LLM-as-a-judge evaluations, identifying methodological issues and inconsistent reporting. Additionally, they investigated community adoption claims and found them to be unsubstantiated, with the authors later retracting these claims.

## Key Results
- Human evaluation data was incomplete, with one baseline sampler's scores omitted without explanation
- Statistical tests failed to support claims of consistent superiority for min-p sampling
- Qualitative feedback from human evaluators actually favored basic sampling over min-p
- New human evaluations using different implementations still showed no advantage for min-p

## Why This Works (Mechanism)
The paper does not present a mechanism for why min-p sampling would work, as it is a critical analysis of existing claims rather than a presentation of a new method.

## Foundational Learning

Temperature Sampling: A hyperparameter that controls randomness in text generation by scaling logits before softmax. Needed to understand the baseline methods being compared to min-p. Quick check: Higher temperature produces more diverse but potentially less coherent outputs.

Top-k Sampling: A method that selects from only the k most probable next tokens. Needed to understand one of the standard baselines. Quick check: Limits the candidate pool to the most likely tokens, trading diversity for quality.

Nucleus Sampling: A dynamic version of top-k that adjusts the candidate pool size based on probability mass. Needed to understand another standard baseline. Quick check: Selects tokens from the smallest set whose cumulative probability exceeds a threshold p.

Perplexity: A measure of how well a probability model predicts a sample. Needed to evaluate model performance on benchmarks. Quick check: Lower perplexity indicates better predictive performance on test data.

## Architecture Onboarding

Component Map: Language Model (LLM) -> Sampling Method (min-p, temperature, top-k, nucleus) -> Generated Text Output -> Evaluation (Human, LLM-as-a-judge, Benchmarks)

Critical Path: Input prompt → LLM tokenization → Probability distribution over next tokens → Sampling method selection → Token selection → Output generation → Quality/diversity evaluation

Design Tradeoffs: The paper examines the fundamental tradeoff between generation quality and diversity, which is central to all sampling methods. Min-p was claimed to optimize this tradeoff better than existing methods.

Failure Signatures: Inconsistent results across different implementations of min-p, failure to show consistent improvement over baseline methods, methodological issues in supporting studies, and unsubstantiated community adoption claims.

First Experiments:
1. Re-run human evaluations with complete data reporting for all baseline methods
2. Conduct hyperparameter volume-matched comparisons across all sampling methods on standard benchmarks
3. Perform ablation studies on LLM-as-a-judge evaluation protocols to identify sources of inconsistency

## Open Questions the Paper Calls Out
None

## Limitations

- Performance may depend heavily on specific parameter settings not fully explored
- LLM-as-a-judge evaluations show inconsistent results, suggesting reliability issues with automated assessment
- Lack of standardized definitions for "quality" versus "diversity" introduces subjectivity
- Focus on English language generation limits generalizability to other languages or specialized domains

## Confidence

High Confidence: The claim that original supporting evidence was incomplete and methodologically flawed, with clear documentation of missing data, failed statistical tests, and inconsistent reporting.

Medium Confidence: The conclusion that min-p does not consistently outperform established sampling methods when controlling for hyperparameter volume, though some configurations might still favor min-p in specific use cases.

Low Confidence: The assertion that min-p sampling has no practical value whatsoever, as the analysis does not rule out niche applications where min-p might be beneficial under specific conditions.

## Next Checks

1. Conduct a systematic hyperparameter sweep across temperature, top-k, and nucleus sampling parameters to identify conditions under which min-p might outperform other methods, ensuring fair comparisons by matching effective hyperparameter volumes.

2. Expand human evaluation studies to include multilingual and domain-specific text generation tasks, assessing whether min-p shows advantages in specialized contexts not captured in general-purpose evaluations.

3. Perform ablation studies on the LLM-as-a-judge evaluation methodology to identify sources of inconsistency and develop standardized protocols for automated quality assessment that align more closely with human judgment.