---
ver: rpa2
title: Investigating Literary Motifs in Ancient and Medieval Novels with Large Language
  Models
arxiv_id: '2504.21742'
source_url: https://arxiv.org/abs/2504.21742
tags:
- occurrences
- similarity
- motifs
- love
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study used fine-tuned large language models to extract and
  analyze literary motifs from a corpus of ancient and medieval Greek novels. The
  method involved chunking texts, fine-tuning GPT-4o on annotated examples, clustering
  motif sentences, and summarizing results.
---

# Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models

## Quick Facts
- arXiv ID: 2504.21742
- Source URL: https://arxiv.org/abs/2504.21742
- Reference count: 0
- Key outcome: Fine-tuned LLMs extracted 350 motifs from 17 Greek novels, revealing period-specific patterns and text similarities (max 0.81 cosine score)

## Executive Summary
This study investigates literary motifs in ancient and medieval Greek novels using fine-tuned large language models. The approach combines LLM-based motif extraction, semantic clustering, and quantitative similarity analysis to examine motif evolution across three historical periods. Results show both persistent themes (fate, leadership) and period-specific variations, with Komnenian novels exhibiting distinctive features like maiden beauty descriptions. The method successfully identifies recurrent narrative patterns and enables quantitative comparison of textual relationships, offering a scalable approach for studying genre evolution.

## Method Summary
The study chunks 17 Greek novels into 13,000+ segments (max 1000 tokens each) with 2-chunk context. GPT-4o is fine-tuned on 74 manually annotated examples to extract motif sentences. Motif sentences are embedded using all-mpnet-base-v2, clustered via UMAP and HDBSCAN, and summarized by Llama-3.1-8B-Instruct. Analysis includes frequency counting, standard deviation for fluctuation, mean frequency for persistence, and cosine similarity between novel vectors. The corpus spans Imperial (6,105 chunks), Komnenian (2,100), and Palaiologan (2,214) periods.

## Key Results
- 350 motifs identified across 17 Greek novels
- Motif similarity scores ranged from 0.13 to 0.81 (Aithiopika and Leucippe and Clitophon most similar)
- Komnenian novels showed increased focus on maidens' beauty and day-to-night transitions
- Some motifs (fate, leadership) remained stable across periods while others fluctuated
- ~25-27% of chunks classified as outliers and excluded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned GPT-4o can extract literary motifs from ancient Greek texts despite limited pre-training data in classical languages
- Mechanism: The model uses few-shot transfer learning from 74 manually annotated examples to learn the pattern of identifying recurring units of meaning within text chunks. Context windows (previous 2 chunks) provide narrative continuity for motif recognition.
- Core assumption: The definition of "motif" as "units of meaning, extracted from a limited textual segment" is sufficiently operationalizable through natural language prompts and examples.
- Evidence anchors:
  - [abstract] "By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common"
  - [section] "GPT-4o was fine-tuned on 74 manually annotated examples using the OpenAI:s API (3 batches, batch size 1, Learning Rate Multiplier 2)"
  - [corpus] No direct corpus evidence on transfer efficiency; neighboring papers suggest LLMs encode literary style but don't specifically address low-resource classical languages
- Break condition: If annotated examples contain systematic annotation bias, or if the definition of "motif" varies inconsistently across training examples, the model will extract inconsistent patterns.

### Mechanism 2
- Claim: Semantic clustering of motif sentences enables discovery of motif families without pre-defined taxonomies
- Mechanism: Sentence embeddings (all-mpnet-base-v2) map motif descriptions to high-dimensional vectors; UMAP reduces dimensionality while preserving local structure; HDBSCAN identifies dense clusters as motif families; Llama-3.1-8B-Instruct generates human-readable cluster labels.
- Core assumption: Semantically similar motifs will cluster together in embedding space regardless of surface-level wording differences.
- Evidence anchors:
  - [section] "The sentences representing the motifs were subsequently embedded using a sentence-transformer (all-mpnet-base-v2), and clustered with BERTopic, UMAP and HDBSCAN"
  - [section] "In the next step, each list of clustered motif labels were summarized by another LLM"
  - [corpus] Neighboring work on "prompt embeddings encoding literary style" (FMR=0.57) supports embedding-based literary analysis, but no direct validation of clustering quality
- Break condition: If clusters are too granular (over-segmentation) or too broad (conflating distinct motifs), the summary labels will be misleadingly complex or vacuous.

### Mechanism 3
- Claim: Vector representations of motif frequency distributions enable quantitative similarity comparison between texts
- Mechanism: Each novel is represented as a frequency vector over the 350 identified motifs; cosine similarity measures motif overlap between text pairs; network visualization reveals textual relationships.
- Core assumption: Motif frequency captures meaningful signal about textual similarity, and differences in corpus size can be normalized appropriately.
- Evidence anchors:
  - [section] "similarity score is calculated by first representing each novel as a vector (a numerical summary of all the motifs the novel contains); then comparing how closely aligned these vectors are to each other using cosine similarity"
  - [abstract] "Similarity analysis showed novels shared motifs to varying degrees, with Aithiopica and Leucippe and Clitophon most similar (score 0.81)"
  - [corpus] Weak corpus evidence for this specific metric choice; related work on computational character analysis uses similar vector representations but with different features
- Break condition: If corpus size imbalances are not properly normalized, larger corpora will artificially appear more similar to all other texts.

## Foundational Learning

- Concept: Fine-tuning vs. in-context learning for extraction tasks
  - Why needed here: The study uses fine-tuning (74 examples) rather than pure prompting; understanding the tradeoff is critical for replication
  - Quick check question: Can you explain why the authors chose fine-tuning over few-shot prompting with GPT-4o?

- Concept: Sentence embeddings and semantic clustering
  - Why needed here: The clustering pipeline (sentence-transformers → UMAP → HDBSCAN) is the core analytical engine
  - Quick check question: What does UMAP's `n_neighbors=5` parameter control, and how would changing it affect cluster granularity?

- Concept: Cosine similarity for document comparison
  - Why needed here: All textual similarity claims depend on this metric's appropriateness for motif frequency vectors
  - Quick check question: Why might cosine similarity be preferred over Euclidean distance for comparing texts of different lengths?

## Architecture Onboarding

- Component map: TikToken chunking (1000 tokens, 2-chunk context) -> Fine-tuned GPT-4o motif extraction -> all-mpnet-base-v2 embeddings -> UMAP dimensionality reduction -> HDBSCAN clustering -> BERTopic wrapper -> Llama-3.1-8B-Instruct summarization -> Frequency counting and similarity analysis
- Critical path: Fine-tuning quality -> motif extraction quality -> embedding quality -> clustering quality. Manual annotation is the single point of failure; bad training data propagates through entire pipeline.
- Design tradeoffs:
  - Chunk size (1000 tokens): Larger chunks capture more context but may conflate multiple motifs; smaller chunks risk motif fragmentation
  - Clustering parameters (HDBSCAN min_cluster_size=10): Smaller values find rare motifs; larger values merge distinct patterns
  - Excluding outliers: ~25-27% of chunks classified as outliers and excluded—potential signal loss
- Failure signatures:
  - Cluster labels that are overly complex ("the model tries to capture every aspect of the cluster") indicate over-segmentation
  - Low similarity scores between known related texts suggest extraction inconsistency
  - Motifs that should cluster together appearing separately indicates embedding model limitation
- First 3 experiments:
  1. **Annotation variance test**: Have 2-3 domain experts independently annotate the same 20 chunks; measure inter-annotator agreement and model sensitivity to annotation differences
  2. **Clustering parameter sweep**: Vary HDBSCAN `min_cluster_size` (5, 10, 15, 20) and measure cluster stability; check whether top-10 motifs remain consistent
  3. **Baseline comparison**: Compare fine-tuned GPT-4o extraction against (a) zero-shot GPT-4o and (b) traditional topic modeling (LDA/BERTopic without LLM extraction) on the same corpus to quantify fine-tuning benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are LLMs at extracting literary motifs from low-resource historical languages with limited representation in pre-training data, such as Ancient and Medieval Greek?
- Basis in paper: [explicit] The authors state on page 6: "However, their effectiveness in dealing specifically with literary motifs and in texts written in various low-resource languages, with only partial representation in their pre-training data — such as ancient Greek – remains an open research question."
- Why unresolved: This study demonstrates feasibility on one corpus but does not systematically benchmark performance against alternative methods or validate extraction quality across different model configurations.
- What evidence would resolve it: Comparative evaluation metrics (precision, recall, F1) against manually annotated ground truth across multiple low-resource historical languages, plus ablation studies testing different pre-training data compositions.

### Open Question 2
- Question: What is the relationship between motif cluster labels generated by LLMs and the underlying textual content they represent?
- Basis in paper: [explicit] The author states on page 13: "Returning to the definition of motifs... it is probable that, to get a fair representation of what these motifs really consist of, it would be necessary to examine them thoroughly, not just look at the summarizing label. That, however, is the subject for another study."
- Why unresolved: The study relies on LLM-generated cluster summaries without systematic validation of whether these summaries accurately capture the semantic content of clustered passages.
- What evidence would resolve it: Close reading analysis comparing cluster labels against representative passage samples, inter-annotator agreement studies on motif label accuracy, and qualitative assessment by domain experts.

### Open Question 3
- Question: How does the imbalance in corpus sizes across time periods (Imperial: 6,105 chunks; Komnenian: 2,100; Palaiologan: 2,214) affect the reliability of fluctuation and persistence measurements?
- Basis in paper: [inferred] The author notes on page 9 that size differences "makes it complicated to look at the absolute number of frequencies for the motifs" and attempted to use relative rather than absolute measures, but did not systematically test whether size imbalance biases standard deviation calculations.
- Why unresolved: Standard deviation and mean frequency metrics may be sensitive to sample size differences in ways not accounted for in the current analysis.
- What evidence would resolve it: Bootstrap resampling experiments normalizing corpus sizes, sensitivity analysis testing whether motif rankings change under different sampling regimes, and statistical tests explicitly accounting for unequal group sizes.

### Open Question 4
- Question: How sensitive are the identified motif networks and similarity scores to the specific clustering hyperparameters used (UMAP n_neighbors=5, HDBSCAN min_cluster_size=10)?
- Basis in paper: [inferred] The author acknowledges on pages 12-13 that clustering "is the part of the process that is most difficult to control" with adjustable parameters affecting cluster size, structure, and quantity, and that "clustering is doubtless the part of the work process where more time could be spent."
- Why unresolved: No sensitivity analysis or hyperparameter robustness checks were reported to determine whether results are stable across different reasonable parameter choices.
- What evidence would resolve it: Systematic hyperparameter sweeps testing whether text similarity rankings and motif frequency distributions remain stable across a range of clustering configurations, with quantification of result variance.

## Limitations
- Manual motif annotations are not released, making it impossible to assess annotation quality or replicate the fine-tuning process
- ~25-27% of chunks excluded as outliers represents significant signal loss without clear criteria for exclusion
- Cross-linguistic extraction claims assume GPT-4o can reliably handle Greek despite limited classical language training data
- Clustering labels are "relatively complex" and may not accurately represent underlying motif content

## Confidence
- **High confidence**: The methodological pipeline (fine-tuning → extraction → embedding → clustering → analysis) is technically sound and follows established NLP practices. The observed similarity patterns between texts (e.g., Aithiopika and Leucippe and Clitophon at 0.81) are internally consistent with the vector representation approach.
- **Medium confidence**: The identified motif frequencies and their temporal fluctuations reflect genuine patterns in the corpus, though the specific motif labels and their cultural interpretations require domain expertise validation. The claim that Komnenian novels show increased focus on "maidens' beauty" and "day-to-night transitions" is plausible but not independently verified.
- **Low confidence**: Cross-linguistic motif extraction claims and broader literary-historical interpretations exceed what the methodology can rigorously support. The study cannot definitively prove that identified motifs are culturally significant rather than artifacts of the annotation scheme or model biases.

## Next Checks
1. **Inter-annotator agreement study**: Have 2-3 domain experts independently annotate 20 randomly selected chunks using the same annotation guidelines; measure Cohen's kappa and test whether the fine-tuned model's outputs align more closely with expert consensus than random agreement.
2. **Parameter sensitivity analysis**: Systematically vary HDBSCAN's `min_cluster_size` (5, 10, 15, 20) and UMAP's `n_neighbors` (3, 5, 7, 10) to determine cluster stability; verify whether the top-10 most frequent motifs remain consistent across parameter settings.
3. **Cross-linguistic extraction test**: Apply the fine-tuned model to a bilingual corpus (Greek texts with English translations) and compare motif extraction consistency; alternatively, test on a known ancient Greek text with established motif scholarship to verify extraction quality against ground truth.