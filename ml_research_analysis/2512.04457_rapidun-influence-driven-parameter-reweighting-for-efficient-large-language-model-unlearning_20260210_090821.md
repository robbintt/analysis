---
ver: rpa2
title: 'RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language
  Model Unlearning'
arxiv_id: '2512.04457'
source_url: https://arxiv.org/abs/2512.04457
tags:
- unlearning
- forget
- clean
- rapidun
- retain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently removing specific
  data influences from large language models without full retraining. The authors
  propose RapidUn, an influence-guided unlearning framework that uses fast token-wise
  influence estimation (RapidIn) to derive per-sample weights for targeted parameter
  updates.
---

# RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning

## Quick Facts
- **arXiv ID:** 2512.04457
- **Source URL:** https://arxiv.org/abs/2512.04457
- **Reference count:** 39
- **Key result:** Achieves up to 100× speedup over full retraining while reducing ASR by up to 29 percentage points on seen triggers and 21 on OOD triggers.

## Executive Summary
This paper addresses the challenge of efficiently removing specific data influences from large language models without full retraining. The authors propose RapidUn, an influence-guided unlearning framework that uses fast token-wise influence estimation (RapidIn) to derive per-sample weights for targeted parameter updates. These weights modulate LoRA-based gradient ascent on forget data and descent on retain data, enabling stable forgetting while preserving general model behavior. Evaluated on Llama-3-8B and Mistral-7B across Dolly-15k and Alpaca-57k, RapidUn achieves significant speedup and outperforms baselines like GA, Fisher, and LoReUn on both seen and out-of-distribution forgetting tasks.

## Method Summary
RapidUn uses influence-guided parameter reweighting for efficient LLM unlearning. It computes token-wise gradient alignment to approximate per-sample influence, then derives per-sample weights via robust scaling (median/MAD) with temperature smoothing and clipping. These weights are used to modulate LoRA-based optimization: gradient ascent on forget data (weighted by S_f) and descent on retain data (weighted by S_r). The framework leverages four directional influence types (FF, FR, RF, RR) to distinguish harmful from beneficial patterns, combining them with tuned coefficients. The approach achieves 100× speedup over full retraining while maintaining competitive clean perplexity.

## Key Results
- Achieves up to 100× speedup over full retraining while maintaining competitive clean perplexity
- Reduces ASR by up to 29 percentage points on seen triggers and 21 on OOD triggers
- Consistently outperforms GA, Fisher, and LoReUn baselines across multiple model sizes and datasets

## Why This Works (Mechanism)

### Mechanism 1: Token-wise Gradient Alignment as Influence Proxy
RapidUn approximates per-sample influence by measuring token-level gradient alignment between training examples. It computes normalized token gradients for each position, then aggregates pairwise inner products across all token pairs between two examples. High alignment indicates shared influence pathways in parameter space. This serves as a computationally tractable proxy for expensive exact influence functions.

### Mechanism 2: Cross-Set Influence Fusion for Selective Forgetting
The framework combines four directional influences (FF, FR, RF, RR) to discriminate between harmful and beneficial patterns. Forget-set scores emphasize self-reinforcing harmful patterns (FF) while penalizing harmful influence on retain behavior (FR). Retain-set scores emphasize self-reinforcing clean patterns (RR) while penalizing sensitivity to harmful contamination (RF).

### Mechanism 3: Robust Scaling Prevents Gradient Explosion
RapidUn uses median-based normalization with temperature smoothing and log-space clipping to yield stable per-sample weights even with tiny, imbalanced forget sets. This approach uses MAD (median absolute deviation) scaling, which resists outliers better than mean/std, preventing gradient explosion while ensuring E[w_A(i)] = 1.

## Foundational Learning

- **Influence Functions (Koh & Liang 2017)**: Classical method to measure how much each training sample influences model predictions, typically requiring expensive Hessian computations. Needed to understand what RapidIn approximates.
- **LoRA / Low-Rank Adaptation**: Technique that freezes original weights and learns low-rank adapter matrices for efficient fine-tuning. Critical for RapidUn's parameter efficiency.
- **Robust Statistics (Median, MAD)**: Statistical methods resistant to outliers. Used in RapidUn's weight mapping instead of mean/std to handle heavy-tailed forget-set distributions.

## Architecture Onboarding

- **Component map:** RapidIn -> DirectionalAggregator -> ScoreFusion -> WeightMapper -> RapidUnTrainer
- **Critical path:** Compute gradients for all samples -> Build influence matrices -> Map to weights -> Cache weights -> Run LoRA training with fixed weights
- **Design tradeoffs:** Pre-computed weights (static, cheaper) vs. dynamic reweighting; LoRA-only (efficient, reversible) vs. full-model; MAD-based scaling (robust) vs. simpler min-max
- **Failure signatures:** ASR stuck near baseline (check influence matrices, weight normalization); Clean PPL exploding (check ascent coefficient, retain buffer); OOD ASR not improving (check trigger coverage)
- **First 3 experiments:** 1) Run RapidUn with uniform weights (all w=1) as sanity check against standard GA; 2) Replace RobustScale with z-score normalization to validate robustness claim; 3) Reduce forget-set size systematically to find practical lower bound

## Open Questions the Paper Calls Out

1. **Dynamic weighting:** How would dynamically updating influence weights during unlearning improve stability or performance compared to RapidUn's static pre-computed weights? The paper acknowledges this could better capture model dynamics but hasn't been explored.

2. **Incomplete forget sets:** How robust is RapidUn when the provided forget set is incomplete or fails to fully represent the harmful behavior distribution? The method assumes access to a representative forget set, which may not always be available in privacy-sensitive domains.

3. **Multimodal extensions:** Can RapidUn's token-wise influence estimation be adapted for multimodal models where data interactions span different modalities? The paper notes experiments are restricted to text-based instruction data.

## Limitations

- Lack of implementation details for RapidIn influence estimator, requiring significant reverse-engineering
- Reliance on influence functions as gradient alignment proxies without validation against exact methods
- Assumes access to a small but representative forget set, which may not always be available

## Confidence

- **High confidence** in overall framework design and empirical results
- **Medium confidence** in specific influence approximation method due to missing implementation details
- **Low confidence** in practical reproducibility without substantial additional work

## Next Checks

1. Implement exact influence function computation for a small subset of samples and compare against RapidIn's gradient alignment approximation to quantify approximation error.

2. Systematically vary the four fusion coefficients and temperature parameters across plausible ranges to document how ASR and clean perplexity vary with each parameter.

3. Compare RapidIn's gradient alignment approach against simpler influence proxies such as gradient norm magnitude or cosine similarity between sample embeddings to determine if the complex computation provides measurable benefits.