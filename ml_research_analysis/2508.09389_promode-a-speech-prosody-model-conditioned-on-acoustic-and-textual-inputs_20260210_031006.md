---
ver: rpa2
title: 'ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs'
arxiv_id: '2508.09389'
source_url: https://arxiv.org/abs/2508.09389
tags:
- prosody
- speech
- promode
- energy
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMode, a zero-shot, stand-alone prosody
  model built on the Perceiver IO architecture for speech synthesis. ProMode takes
  masked acoustic and textual features as input and predicts F0 and energy for masked
  regions, leveraging contextual information from unmasked segments.
---

# ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs

## Quick Facts
- arXiv ID: 2508.09389
- Source URL: https://arxiv.org/abs/2508.09389
- Reference count: 0
- Zero-shot prosody model for speech synthesis with F0 and energy prediction.

## Executive Summary
This paper introduces ProMode, a zero-shot, stand-alone prosody model built on the Perceiver IO architecture for speech synthesis. ProMode takes masked acoustic and textual features as input and predicts F0 and energy for masked regions, leveraging contextual information from unmasked segments. Unlike many existing prosody models tightly coupled with specific downstream tasks, ProMode is designed to be general-purpose and can be used in various applications like TTS.

The model employs a dual-decoder approach in the Perceiver-based architecture, combining both conditional and unconditional decoders along with an auxiliary acoustic-only loss to enhance its ability to capture prosodic variations. Experimental results on GigaSpeech demonstrate ProMode's superior performance compared to several baselines in predicting both F0 and energy. The model shows consistent improvements at different levels of granularity, with RPA of 43.9%, RCA of 43.6%, and RMSE of 21.5% for F0 prediction. When integrated into a TTS system, ProMode leads to significantly better prosody perception of the synthesized speech, as evidenced by subjective listening tests where listeners showed a strong preference for ProMode over the baselines.

## Method Summary
ProMode is a zero-shot prosody model that predicts F0 and energy for masked regions in speech using both acoustic and textual inputs. The model is built on the Perceiver IO architecture with a dual-decoder approach: a conditional decoder that uses both text and acoustic context, and an unconditional decoder that prevents collapse by predicting prosody from latent embeddings alone. The model is trained on GigaSpeech with 60% phoneme-boundary-aligned masking, using a combination of L1, MSE, and BCE losses. The dual-decoder design allows ProMode to generalize across different tasks and improve prosody prediction accuracy.

## Key Results
- ProMode achieves RPA of 43.9%, RCA of 43.6%, and RMSE of 21.5% for F0 prediction on GigaSpeech.
- Integration into TTS systems leads to significantly better prosody perception in subjective listening tests.
- ProMode outperforms baselines in both objective metrics and downstream TTS quality.

## Why This Works (Mechanism)
ProMode's dual-decoder architecture with an auxiliary acoustic-only loss prevents the model from collapsing to text-only predictions, ensuring it captures acoustic context. The modified adaLN-zero mechanism in the conditional decoder allows for fine-grained, time-dependent prosodic adjustments by predicting scale and shift parameters for each time step. The phoneme-boundary-aligned masking ensures that the model learns to predict prosody based on surrounding context, improving generalization.

## Foundational Learning
- **Perceiver IO Architecture:** A transformer-based model that handles variable-length inputs by encoding them into a fixed-size latent space. Why needed: Enables efficient processing of long sequences like speech and text. Quick check: Verify the encoder uses 8 heads and 18 layers.
- **Dual-Decoder Design:** Combines a conditional decoder (using both text and acoustic context) and an unconditional decoder (preventing collapse). Why needed: Ensures the model doesn't ignore acoustic context. Quick check: Confirm AOL loss is active during training.
- **Phoneme-Boundary-Aligned Masking:** Masks 60% of input features aligned to phoneme boundaries. Why needed: Forces the model to predict masked regions based on context. Quick check: Verify masking ratio and alignment in training.
- **Modified adaLN-zero:** Predicts time-dependent scale and shift parameters via cross-attention. Why needed: Allows fine-grained prosodic adjustments. Quick check: Ensure the module generates TÃ—6 parameters per time step.
- **Auxiliary Acoustic-Only Loss (AOL):** Prevents decoder collapse by training the unconditional decoder separately. Why needed: Ensures the model uses acoustic context. Quick check: Confirm AOL loss is included in the total loss.
- **Feature Processing Pipeline:** Extracts F0, energy, and Mel-spectrogram at 11.6 ms frames, processes with ConvNeXt V2. Why needed: Standardizes input for the model. Quick check: Verify frame rate and feature extraction steps.

## Architecture Onboarding

**Component Map:**
Text + Acoustic Features -> Perceiver IO Encoder -> Latent Embeddings -> Dual Decoders (PD1, PD2) -> F0 + Energy Predictions

**Critical Path:**
Text and acoustic features are encoded into latent embeddings, which are then processed by both decoders. PD1 (unconditional) prevents collapse, while PD2 (conditional) makes the final predictions using modified adaLN-zero.

**Design Tradeoffs:**
- **Dual Decoders:** Adds complexity but improves robustness and prevents collapse.
- **Phoneme-Aligned Masking:** Ensures context-based predictions but may limit fine-grained temporal modeling.
- **Modified adaLN-zero:** Enables fine-grained adjustments but increases model complexity.

**Failure Signatures:**
- High RMSE, low variance in predictions: Decoder collapse (ignores acoustic context).
- Temporal smoothing in predictions: Incorrect implementation of modified adaLN-zero.
- Poor generalization: Inadequate masking or AOL loss weighting.

**First Experiments:**
1. Train with only PD1 (unconditional) to test AOL effectiveness.
2. Train with only PD2 (conditional) to measure marginal benefit of dual-decoder design.
3. Vary masking ratio (e.g., 40%, 60%, 80%) to assess impact on prosody prediction.

## Open Questions the Paper Calls Out
None

## Limitations
- Learning rate schedule and optimizer configuration are unspecified, which may affect convergence.
- Internal details of the modified adaLN-zero implementation are not fully detailed.
- Masking strategy application (audio frames vs. phonemes) is unclear.

## Confidence
- **High Confidence:** Core methodology (Perceiver IO, dual-decoder design, GigaSpeech dataset, experimental setup) is clearly described.
- **Medium Confidence:** Performance improvements are credible but depend on unreported hyperparameters.
- **Low Confidence:** Exact contribution of modified adaLN-zero cannot be fully validated without internal details.

## Next Checks
1. Systematically vary the learning rate and warmup schedule to assess their impact on final prosody prediction accuracy.
2. Train variants with only PD1 (unconditional) or only PD2 (conditional) to quantify the marginal benefit of the dual-decoder design and AOL.
3. Evaluate ProMode on a held-out or external dataset (e.g., LJSpeech or VCTK) to test robustness beyond GigaSpeech.