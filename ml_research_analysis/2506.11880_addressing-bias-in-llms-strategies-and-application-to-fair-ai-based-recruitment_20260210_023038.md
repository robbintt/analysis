---
ver: rpa2
title: 'Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment'
arxiv_id: '2506.11880'
source_url: https://arxiv.org/abs/2506.11880
tags:
- gender
- bias
- information
- systems
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates gender bias in Transformer-based language
  models (BERT, RoBERTa) applied to AI-based recruitment tools. When trained with
  gender-biased scores, both models learned to discriminate against women, producing
  male-dominated shortlists (69% male, 31% female) and violating fairness metrics
  (demographic ratio 0.441-0.493, below the 0.8 threshold).
---

# Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment

## Quick Facts
- **arXiv ID:** 2506.11880
- **Source URL:** https://arxiv.org/abs/2506.11880
- **Authors:** Alejandro Peña; Julian Fierrez; Aythami Morales; Gonzalo Mancera; Miguel Lopez; Ruben Tolosana
- **Reference count:** 22
- **Primary result:** Transformer models trained on gender-biased scores learn to discriminate, producing 69% male shortlists; two privacy-enhancing methods successfully reduce bias to near-equal representation.

## Executive Summary
This paper investigates gender bias in Transformer-based language models (BERT, RoBERTa) when applied to AI-based recruitment tools. The study demonstrates that when trained on gender-biased scores, these models learn to discriminate against women, producing male-dominated shortlists and violating fairness metrics. The authors propose a bias mitigation framework using two privacy-enhancing methods: Integrated Gradients-based token removal and adversarial learning to remove gender information from latent representations. Both approaches successfully reduced bias while maintaining predictive performance, achieving near-equal gender representation and demographic ratios above the 4/5 rule threshold.

## Method Summary
The study uses FairCVdb, a dataset of 24,000 synthetic profiles with candidate competencies and biographies. The authors implement a scoring tool that combines frozen BERT/RoBERTa embeddings with structured competencies through a 3-layer MLP. Two bias mitigation approaches are evaluated: (1) Integrated Gradients-based token removal that identifies and masks gender-proxy tokens from biographies, and (2) adversarial learning that removes gender information from latent representations through min-max optimization. Both methods are trained on gender-biased scores and evaluated against demographic ratio, Kullback-Leibler divergence, and recall metrics.

## Key Results
- Baseline models trained on gender-biased scores produced male-dominated shortlists (69% male, 31% female) with demographic ratios of 0.441-0.493, violating fairness thresholds.
- Both mitigation methods achieved near-equal gender representation (49-51%) with demographic ratios above 0.84 and DKL below 0.05.
- Method 1 (IG-based removal) showed slightly better bias reduction (DKL 0.0267-0.0168) while Method 2 (adversarial) achieved higher overall recall (83-80%).

## Why This Works (Mechanism)

### Mechanism 1: Token Removal via Integrated Gradients
- Claim: Removing gender-proxy tokens from input text prevents models from learning biased correlations when trained on biased labels.
- Mechanism: Integrated Gradients identifies tokens with high attribution for predictions that correlate with gender but not job-relevant competence. By iteratively detecting and masking these tokens, the model cannot establish a predictive path from gender-proxy features to biased scores during training.
- Core assumption: Gender bias manifests primarily through explicit lexical markers identifiable via token-level attribution.
- Evidence anchors: Abstract shows successful bias reduction achieving near-equal gender representation; Approach 1 section demonstrates DKL reduction to 0.0267.

### Mechanism 2: Adversarial Learning for Latent Debiasing
- Claim: Adversarial learning removes gender information from latent representations by forcing the model to learn gender-invariant features.
- Mechanism: An auxiliary gender classifier attaches to intermediate layers. During training, the main network minimizes prediction loss while maximizing the classifier's uncertainty about gender (min-max optimization). This reduces mutual information between latent representations and gender.
- Core assumption: Gender information can be isolated and removed from latent representations without degrading task-relevant features.
- Evidence anchors: Abstract shows achieving higher overall recall (83-80%); Approach 2 section demonstrates DKL value under 0.05.

### Mechanism 3: Gender Information Exploitation in Frozen Transformers
- Claim: Transformer models exploit demographic information encoded in text representations when trained on biased labels.
- Mechanism: When human annotators introduce gender bias into scores, the model extracts gender signals from biography embeddings and uses them as predictive features, creating an allocational harm pathway independent of competence.
- Core assumption: Frozen BERT/RoBERTa encodings contain sufficient exploitable gender information.
- Evidence anchors: Abstract shows models learned to discriminate producing male-dominated shortlists; Gender Information Analysis section demonstrates systems extracting gender information from deep representations.

## Foundational Learning

- Concept: **Statistical Parity and the 4/5 Rule**
  - Why needed here: Baseline models produced demographic ratios of 0.441-0.493, well below the 0.8 threshold indicating disparate impact. Understanding this metric is essential for interpreting bias severity.
  - Quick check question: If a model's shortlist is 60% male and 40% female, does it pass the 4/5 rule?

- Concept: **Mutual Information in Adversarial Learning**
  - Why needed here: Approach 2 minimizes I(z; f(x,t)) to remove gender information from latent representations. Without this concept, the loss function and auxiliary classifier role are opaque.
  - Quick check question: Why does reducing mutual information between latent features and gender help prevent biased predictions?

- Concept: **Attribution Methods for Bias Detection**
  - Why needed here: Integrated Gradients identifies gender-proxy tokens by measuring feature relevance. This differs from manual removal of gendered pronouns.
  - Quick check question: What is the difference between a token being "relevant to the task" vs. "relevant to predicting gender"?

## Architecture Onboarding

- Component map:
  - Text Understanding Module (W_T): Frozen BERT/RoBERTa processes tokenized biographies → outputs 768-dim embedding (mean of output embeddings)
  - Fusion Module (W_F): 3-layer MLP (768→300→20, concatenate with 7-dim competencies x, → output score ŷ)
  - Adversarial Head (Approach 2 only): Auxiliary gender classifier attached to 300-dim intermediate layer
  - Attribution Pipeline (Approach 1 only): IG computed on validation set → top-20 tokens per resume → frequency analysis → mask tokens → retrain

- Critical path:
  1. Load frozen BERT/RoBERTa (base: 12 layers, 12 heads, 768 hidden)
  2. Tokenize biographies with model-specific tokenizer
  3. Forward pass through frozen transformer → mean-pool to f_t
  4. Concatenate f_t with competencies x → MLP → sigmoid output
  5. Train with RMSE loss (batch=32, lr=1e-3, AdamW, 10 epochs)

- Design tradeoffs:
  - **Approach 1 (IG-based removal)**: Better bias reduction (DKL 0.017-0.027) but requires iterative retraining and may over-remove task-relevant tokens
  - **Approach 2 (Adversarial)**: Higher recall (80-83%) and single-pass training, but residual bias remains (DKL 0.042-0.049) and gender still partially recoverable in latent space
  - **Frozen vs. fine-tuned transformers**: Paper freezes W_T; fine-tuning might improve performance but risks overfitting to bias

- Failure signatures:
  - Demographic ratio < 0.8 indicates model learned gender bias
  - DKL > 0.3 between gender score distributions indicates severe bias
  - Recall gap > 10 points between genders violates Equality of Opportunity
  - t-SNE clustering by gender (not labor sector) indicates latent representations encode gender

- First 3 experiments:
  1. Reproduce baseline bias: Train W_F on biased scores (yG) without mitigation → expect 67-69% male shortlist, DKL > 0.3
  2. Apply IG-based removal: Identify and mask top gender-proxy tokens → retrain → verify DKL < 0.03, demographic ratio > 0.96
  3. Apply adversarial learning: Attach gender classifier with λ=0.1 → verify recall > 80%, demographic ratio > 0.84

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the proposed privacy-enhancing framework be adapted to mitigate bias for intersectional demographics or non-binary gender definitions?
  - Basis in paper: The authors explicitly acknowledge they rely on a binary definition of gender due to existing data limitations, while noting that "a shift to new, inclusive definitions is needed."
  - Why unresolved: The current experimental design and mutual information optimization targets (z ∈ {0, 1}) are structurally limited to binary classification.

- **Open Question 2:** How does the removal of proxy tokens via Integrated Gradients affect the retention of legitimate, task-relevant semantic information?
  - Basis in paper: The authors identify words like "engineering" and "psychology" as high-relevance tokens for male and female sets respectively, and remove them as bias proxies, despite these words being directly relevant to job competencies.
  - Why unresolved: While removing these tokens reduces bias, it may inadvertently "blind" the model to valid qualifications.

- **Open Question 3:** Do the bias mitigation strategies generalize to uncontrolled, real-world resume datasets outside of the synthetic FairCVdb environment?
  - Basis in paper: The entire experimental validation relies on FairCVdb, which comprises 24,000 "synthetic profiles" with controlled, injected bias.
  - Why unresolved: Synthetic data allows for controlled variables but lacks the noise, complex linguistic variations, and intersectional correlations found in natural language.

## Limitations

- The gender-biased scores (y_G) generation mechanism is not explicitly specified, making exact baseline reproduction challenging.
- The evaluation focuses on binary gender fairness and does not address intersectional biases or non-binary gender identities.
- Results are validated only on synthetic FairCVdb data, limiting generalizability to real-world recruitment scenarios with naturally occurring gender bias.

## Confidence

**High Confidence:** The fundamental finding that Transformer-based models trained on biased labels learn to discriminate against women, producing male-dominated shortlists and violating fairness metrics. This is supported by experimental results showing demographic ratios of 0.441-0.493 for baseline models.

**Medium Confidence:** The effectiveness of both bias mitigation approaches in achieving near-equal gender representation (49-51%) and demographic ratios above 0.84. While results are promising, the exact token removal lists for Approach 1 and the complete adversarial training procedure for Approach 2 are not fully specified.

**Low Confidence:** The generalizability of these results to real-world recruitment scenarios with naturally occurring gender bias, rather than synthetically introduced bias in FairCVdb. The paper does not validate the approaches on production recruitment data or test for potential degradation in predictive performance when applied to live hiring decisions.

## Next Checks

1. **Baseline Bias Reproduction:** Implement the MLP fusion architecture with frozen BERT/RoBERTa and train on the gender-biased scores (y_G) from FairCVdb. Verify that the model produces demographic ratios below 0.5 and DKL values above 0.3, confirming the presence of learned gender discrimination.

2. **Token Removal Coverage Analysis:** After implementing Approach 1, analyze the overlap between automatically identified gender-proxy tokens and domain-specific job-relevant terms. Measure the percentage of occupation-related tokens being masked and assess whether this impacts model performance on unbiased scores (y).

3. **Latent Space Gender Separation:** For Approach 2, train the adversarial model and use t-SNE or UMAP to visualize the 300-dimensional intermediate representations. Quantify the separability of gender clusters before and after adversarial training to verify that gender information has been successfully removed from latent representations.