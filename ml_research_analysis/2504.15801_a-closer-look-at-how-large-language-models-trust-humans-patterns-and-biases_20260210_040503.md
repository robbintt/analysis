---
ver: rpa2
title: 'A closer look at how large language models trust humans: patterns and biases'
arxiv_id: '2504.15801'
source_url: https://arxiv.org/abs/2504.15801
tags:
- trust
- integrity
- scenario
- benevolence
- trustworthiness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) develop
  implicit trust in humans across five decision-making scenarios, using established
  psychological theories of trustworthiness (competence, benevolence, and integrity)
  and demographic factors (gender, age, and religion). Across 43,200 simulations using
  five popular LLM models, the research finds that LLM trust in humans generally aligns
  with human trust mechanisms, with strong correlations between trustworthiness and
  trust in most scenarios.
---

# A closer look at how large language models trust humans: patterns and biases

## Quick Facts
- arXiv ID: 2504.15801
- Source URL: https://arxiv.org/abs/2504.15801
- Authors: Valeria Lerman; Yaniv Dover
- Reference count: 40
- This study investigates how large language models (LLMs) develop implicit trust in humans across five decision-making scenarios, using established psychological theories of trustworthiness (competence, benevolence, and integrity) and demographic factors (gender, age, and religion).

## Executive Summary
This study investigates how large language models (LLMs) develop implicit trust in humans across five decision-making scenarios, using established psychological theories of trustworthiness (competence, benevolence, and integrity) and demographic factors (gender, age, and religion). Across 43,200 simulations using five popular LLM models, the research finds that LLM trust in humans generally aligns with human trust mechanisms, with strong correlations between trustworthiness and trust in most scenarios. However, trust expression varies significantly across models and contexts, and demographic biases—especially in financial scenarios—were observed, with religion-based bias being particularly consistent. Notably, newer and larger models tended to better mimic human-like trust behavior. These findings highlight the need to monitor and mitigate AI-to-human trust biases, as unchecked, such patterns could embed harmful biases into high-stakes decision-making systems.

## Method Summary
The study used a factorial design with 144 unique prompts per scenario (combining 2³ trustworthiness levels × 2 genders × 3 ages × 3 religions), repeated 12 times across five LLMs and five scenarios. The three-stage prompting process involved: (1) scenario description with manipulated trustee characteristics, (2) trust quantification prompt eliciting concrete decisions (e.g., loan amount, babysitting hours), and (3) Mayer & Davis (1999) trust questionnaire to compute competence/benevolence/integrity scores. The analysis examined correlations between trustworthiness scores and trust outputs, OLS regression coefficients predicting trust from trustworthiness and demographics, and t-tests for manipulation checks.

## Key Results
- LLM trust is strongly predicted by trustworthiness dimensions (competence, benevolence, integrity), with correlations ranging from 0.45-0.77 (p<0.001) across scenarios
- Demographic biases emerged consistently in financial scenarios, particularly religion-based bias favoring Jewish subjects with coefficients ranging from +5,940 to +14,471 in loan scenarios
- Newer and larger models showed better alignment with human-like trust patterns, though scale alone did not guarantee consistent performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs develop implicit trust in humans that correlates with perceived trustworthiness dimensions (competence, benevolence, integrity).
- Mechanism: When presented with trustee descriptions, LLMs assign quantitative trust measures (loan amounts, hours, ratings) that systematically increase with higher trustworthiness scores, as validated through post-hoc questionnaire responses.
- Core assumption: The paper assumes that LLM responses to trust questionnaires after decision-making reflect genuine internal trust states rather than pattern-matching on questionnaire structure alone.
- Evidence anchors:
  - [abstract] "LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender"
  - [section: Results - Trust in Organizations] Table 1 shows correlations between 0.45-0.77 (p<0.001) between trustworthiness dimensions and trust scores across all five models
  - [corpus] Related work on "deferred trust" (arxiv 2511.16769) suggests trust dynamics emerge from comparative assessments, supporting the measurement approach
- Break condition: When correlations drop below ~0.20 (e.g., donation scenario with older models), the mechanism fails—LLM trust becomes decoupled from trustworthiness cues.

### Mechanism 2
- Claim: Demographic variables introduce systematic bias into LLM trust decisions, particularly in financial contexts.
- Mechanism: Controlling for trustworthiness, demographic factors independently shift trust outputs—e.g., male subjects receive ~$2,000 higher loan recommendations; Jewish subjects receive positive trust boosts across money-related scenarios.
- Core assumption: Assumes the controlled experimental design (holding trustworthiness constant while varying demographics) isolates bias rather than capturing spurious correlations from training data distributions.
- Evidence anchors:
  - [abstract] "demographic biases—especially in financial scenarios—were observed, with religion-based bias being particularly consistent"
  - [section: Results - Demographic Biases] Table 4 shows significant religion effects: Jewish coefficient = +5,940 to +14,471 across models in loan scenario
  - [corpus] Weak direct corpus evidence on religion bias mechanisms; related work (arxiv 2511.16769) addresses trust formation but not demographic bias pathways
- Break condition: Bias magnitude varies substantially across models and scenarios—e.g., babysitter scenario shows near-zero demographic effects, suggesting context modulates bias expression.

### Mechanism 3
- Claim: Newer and larger models exhibit stronger alignment with human-like trust patterns.
- Mechanism: Model scale/capability improvements appear to enhance sensitivity to trustworthiness cues, producing higher correlations and more consistent behavioral patterns across scenarios.
- Core assumption: The paper infers but does not prove that model size/generation causes better human alignment—alternative explanations include training data composition or architecture changes.
- Evidence anchors:
  - [abstract] "newer and larger models tended to better mimic human-like trust behavior"
  - [section: Results - Trust Beyond Organizational Context] Authors note: "the two models with the strongest correlations per scenario per dimension... are mostly exhibited for the newer better models (4o mini, 3o mini and Flash 2)"
  - [corpus] No direct corpus validation of scale-trust alignment hypothesis
- Break condition: Even newer models show scenario-specific failures (e.g., Gemini Flash 2 has weak benevolence correlation r=0.06 in loan scenario), indicating scale alone is insufficient.

## Foundational Learning

- **Concept: Trustworthiness Triad (Competence, Benevolence, Integrity)**
  - Why needed here: The entire experimental framework manipulates these three dimensions to predict trust outcomes; without understanding how they differ (skills vs. goodwill vs. principles), you cannot interpret regression coefficients or design valid scenarios.
  - Quick check question: If a loan applicant has high competence but low integrity, would you expect higher or lower trust than someone with moderate scores on all three?

- **Concept: OLS Regression with Categorical Predictors**
  - Why needed here: All results tables use OLS to decompose trust into trustworthiness and demographic components; you need to understand how dummy variables (Male=1, Age60=1) represent category effects relative to baselines.
  - Quick check question: In Table 2, why does "Age 40" have coefficient 0.02 for ChatGPT 3.5 but 0.05 for Gemini Flash 2—what does this difference mean?

- **Concept: Correlation vs. Causation in LLM Behavior**
  - Why needed here: The paper shows correlations between manipulated inputs and outputs, but cannot prove LLMs "have" trust; distinguishing measurement from attribution is critical for deployment decisions.
  - Quick check question: If an LLM assigns higher loans to high-integrity applicants, is this evidence of trust reasoning or pattern matching on the word "integrity"?

## Architecture Onboarding

- **Component map:**
  Prompt design → Manipulation check (t-tests on high vs. low conditions) → Trust extraction → Questionnaire validation → Regression analysis

- **Critical path:**
  Prompt design → Manipulation check (t-tests on high vs. low conditions) → Trust extraction → Questionnaire validation → Regression analysis
  *Failure at manipulation check invalidates all downstream inferences.*

- **Design tradeoffs:**
  - Using established scenarios (senior manager) risks data leakage from training corpus; novel scenarios reduce leakage but lack literature grounding
  - Large N (1,728 per scenario-model) increases statistical power but inflates API costs
  - Multiple models improve generalizability but complicate interpretation when models disagree

- **Failure signatures:**
  - Low questionnaire differentiation (high vs. low conditions not significantly different) → manipulation failed
  - Near-zero correlations (r<0.10) between trustworthiness and trust → model not engaging trust mechanism for this context
  - High Adjusted R² with large demographic coefficients → potential fairness risk in deployment

- **First 3 experiments:**
  1. **Replication check**: Run senior manager scenario with a new model not in original study (e.g., Claude or Llama); verify correlation pattern matches Table 1
  2. **Boundary test**: Create a scenario where trustworthiness cues are subtle (implicit) rather than explicit; assess whether correlations degrade
  3. **Bias probe**: Run loan scenario with demographic labels removed; compare mean trust levels to identify how much bias is label-driven vs. inference-driven

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed alignment between LLM trust and human trust mechanisms reflect genuine cognitive-like processes, or is it an artifact of training data containing psychological literature on trust?
- Basis in paper: [explicit] The authors explicitly raise concerns about "data leakage" and state: "what seems to be human-like behavior could be a result of the language models using what they 'learned' directly from the research literature and not how it will respond to other, 'out of sample' trust scenarios."
- Why unresolved: The study used scenarios similar to established literature; no methodology was employed to distinguish genuine trust formation from pattern matching on training data.
- What evidence would resolve it: Testing LLM trust behavior on novel scenarios with no precedent in psychological literature, or using anti-contamination techniques to identify memorization vs. reasoning.

### Open Question 2
- Question: What mechanisms explain why newer and larger models better approximate human-like trust patterns?
- Basis in paper: [inferred] The authors note that "the two models with the strongest correlations per scenario per dimension... are mostly exhibited for the newer better models" and state "a broader analysis across more models is needed for a definitive conclusion."
- Why unresolved: The study observed this pattern across five models but did not systematically vary model characteristics to isolate whether size, architecture, training data, or other factors drive improved human-likeness.
- What evidence would resolve it: Controlled experiments across model families systematically varying parameter count, training data composition, and architecture while holding other factors constant.

### Open Question 3
- Question: Why do demographic biases emerge strongly in financial scenarios but remain minimal in organizational trust scenarios?
- Basis in paper: [explicit] The authors observe that "most biases occur for the money-related scenarios: the donation and loan request scenarios" while noting "very little evidence for an effect, or bias of demographic variables" in the organizational scenario, calling this "curious and somewhat surprising."
- Why unresolved: The study documented the pattern but did not identify whether context framing, outcome stakes, or scenario-specific linguistic cues trigger biased reasoning.
- What evidence would resolve it: Systematic manipulation of scenario framing (e.g., financial vs. non-financial outcomes in identical relational contexts) to isolate which contextual elements activate demographic bias.

### Open Question 4
- Question: Can non-linear models better capture the relationship between trustworthiness dimensions and LLM trust expression than the OLS framework used?
- Basis in paper: [explicit] The authors state: "one can argue that the effective trust calculation can be equivalent to a non-linear and more complex model which could also better predict trust from trustworthiness and other variables. This could also be studied in future work."
- Why unresolved: Only linear regression was employed; potential interaction effects between trustworthiness dimensions (e.g., high competence amplifying integrity effects) were not modeled.
- What evidence would resolve it: Comparing OLS predictions against machine learning models with interaction terms and non-linear transformations on held-out trust scenarios.

## Limitations

- The study cannot establish causation between model scale/generation and trust alignment—only correlation—leaving open alternative explanations like training data composition shifts.
- Religion-based bias effects are consistently reported but the underlying mechanism (direct encoding vs. indirect cultural correlations) remains unclear without deeper probe into model representations.
- Cross-model consistency varies significantly; newer models show better human-like trust patterns on average but still exhibit scenario-specific failures, suggesting scale is not deterministic.

## Confidence

- **High Confidence**: Trustworthiness-trust correlations across scenarios (supported by consistent p<0.001 findings and manipulation checks)
- **Medium Confidence**: Demographic bias patterns (particularly religion effects), though mechanisms are unclear
- **Low Confidence**: Claims about newer/larger models being "better" at human-like trust (correlation observed but causation unproven)

## Next Checks

1. **Bias Mechanism Probe**: Run controlled experiments removing demographic labels to distinguish label-driven from inference-driven bias, and test whether religion effects persist across neutral contexts.
2. **Scale Attribution Test**: Compare trust alignment between models of similar size but different architectures/trainings to isolate whether observed patterns stem from scale or other factors.
3. **Contextual Boundary Test**: Create scenarios where trustworthiness cues are implicit rather than explicit to assess whether trust correlations degrade, revealing reliance on pattern matching vs. reasoning.