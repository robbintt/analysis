---
ver: rpa2
title: Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World
arxiv_id: '2507.06256'
source_url: https://arxiv.org/abs/2507.06256
tags:
- audio
- adversarial
- noise
- attack
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that audio-based large language models
  (ALLMs) like Qwen2-Audio are vulnerable to adversarial audio attacks in real-world
  scenarios. The authors show that an attacker can craft stealthy audio perturbations
  to manipulate ALLMs into exhibiting specific targeted behaviors, such as waking
  up with "Hey Qwen" or executing harmful commands like deleting calendar events.
---

# Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World

## Quick Facts
- **arXiv ID**: 2507.06256
- **Source URL**: https://arxiv.org/abs/2507.06256
- **Authors**: Vinu Sankar Sadasivan; Soheil Feizi; Rajiv Mathews; Lun Wang
- **Reference count**: 14
- **Primary result**: Audio-based LLMs can be manipulated through stealthy adversarial audio perturbations, with 100% success in targeted attacks and significant degradation in untargeted attacks when played over the air to target devices.

## Executive Summary
This paper demonstrates that audio-based large language models (ALLMs) like Qwen2-Audio are vulnerable to adversarial audio attacks in real-world scenarios. The authors show that an attacker can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors, such as waking up with "Hey Qwen" or executing harmful commands like deleting calendar events. They also demonstrate untargeted attacks where adversarial background noise degrades the model's speech recognition performance. Using audio augmentation techniques including SpecAugment, they successfully transferred these attacks to real-world settings, achieving 100% success in targeted attacks and significant degradation in untargeted attacks when playing adversarial audio through the air to target innocent users' devices.

## Method Summary
The authors develop two types of attacks on ALLMs: targeted attacks that force specific outputs and untargeted attacks that degrade overall performance. Targeted attacks use gradient-based optimization to craft audio perturbations that maximize the probability of generating target outputs, employing perplexity-based loss and projected gradient descent with constraint clipping. Untargeted attacks optimize only the audio tower (feature extractor) to maximize feature space distance, making them faster and more efficient. Both attacks employ audio augmentation techniques including SpecAugment, translation, and additive noise during optimization to enable real-world transfer through over-the-air transmission.

## Key Results
- 100% success rate for targeted attacks (waking up model with "Hey Qwen" and executing harmful commands) in real-world settings using SpecAugment augmentation
- 62.75% ASR@99% for untargeted attacks with adversarial noise versus 17.15% for random noise, demonstrating significant degradation in speech recognition
- SpecAugment alone increased real-world attack success from 0% to 70%, with full augmentation pipeline achieving 100% success
- EnCodec neural compression (≤12 kbps) completely eliminated attack success, providing an effective defense mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based optimization can craft audio perturbations that maximize the probability of target outputs from ALLMs.
- Mechanism: The attack computes gradients of a perplexity-based loss with respect to the audio input, iteratively updating via projected gradient descent: `x_l = clip(x_{l-1} - α∇L(x), -ε, ε)`. This directly optimizes the conditional probability `p(t_{1:n}|x, s)` of generating target tokens.
- Core assumption: White-box access to model parameters and differentiable architecture.
- Evidence anchors:
  - [abstract]: "adversary can craft stealthy audio perturbations to manipulate ALLMs into exhibiting specific targeted behaviors"
  - [section 3.1]: Equations 1-3 define the perplexity loss and PGD update; 100% attack success achieved across all hyperparameter settings (Table 1)
  - [corpus]: Weak direct evidence—AudioTrust (Li et al.) benchmarks ALLM trustworthiness but does not evaluate gradient-based attacks.
- Break condition: Black-box deployment removes gradient access; adaptive defenses (e.g., EnCodec compression) disrupt perturbation structure.

### Mechanism 2
- Claim: Targeting only the audio tower (feature extractor) is sufficient to degrade ALLM utility without optimizing the full LLM.
- Mechanism: Untargeted attacks maximize `||M_F(x + δ) - M_F(x)||_2` where `M_F` is the Whisper-based audio encoder. Randomized coordinate masking prevents optimization from focusing on subset of feature dimensions. This disconnects user speech from reliable tokenization.
- Core assumption: The audio tower's feature space is consequential for downstream LLM behavior; perturbations transfer from features to outputs.
- Evidence anchors:
  - [section 3.2]: "This lets our attack only target the audio tower of the model, making it much faster and more efficient"; adversarial noise (ε=0.1) achieves 62.75% ASR@99% vs. 17.15% for random noise (Table 2)
  - [corpus]: No direct corpus evidence on audio-tower-specific attacks; related work (AdvWave, AudioTrust) focuses on full-model or speech-specific settings.
- Break condition: Audio encoders with built-in denoising or robust feature quantization; system prompts instructing noise-robust behavior (partial mitigation observed in §5.1).

### Mechanism 3
- Claim: SpecAugment-based optimization enables adversarial audio to survive over-the-air transmission and recording.
- Mechanism: During optimization, audio is augmented with (1) translation (temporal shifts), (2) additive noise, and (3) SpecAugment (frequency band masking). The gradient incorporates these: `∇L(A(x))`. SpecAugment is critical—it simulates spectral distortions from acoustic propagation and microphone response.
- Core assumption: The augmentation distribution approximates real-world channel effects; the model does not adaptively counter these augmentations at inference.
- Evidence anchors:
  - [section 4, Table 3]: No augmentations → 0% success; SpecAugment alone → 70%; SpecAugment + additive noise → 100% real-world attack success
  - [abstract]: "Using audio augmentation techniques including SpecAugment, they successfully transferred these attacks to real-world scenarios"
  - [corpus]: Weak—no corpus papers evaluate SpecAugment for adversarial robustness in ALLMs.
- Break condition: EnCodec neural compression (1.5–12 kbps) reduces attack success to 0% (Table 6); sample-rate modification breaks recorded audio transfer (Table 4).

## Foundational Learning

- Concept: **Projected Gradient Descent (PGD) for constrained optimization**
  - Why needed here: The attack iteratively updates audio perturbations while constraining `||δ||_∞ ≤ ε` to maintain stealthiness. Without understanding clipping and projection, the optimization will produce invalid or audible perturbations.
  - Quick check question: Given a loss gradient `g` and constraint `ε=0.1`, what is the update rule that ensures the perturbation stays within bounds?

- Concept: **Spectrogram-based audio representations and SpecAugment**
  - Why needed here: The paper's critical real-world transfer mechanism operates in the frequency domain. Understanding `S = Spec(x)`, frequency masking `M(S)`, and reconstruction `InvSpec(x)(S')` is essential to grasp why spectral augmentation enables acoustic robustness.
  - Quick check question: Why might frequency-band masking improve robustness to over-the-air transmission more than time-domain noise addition?

- Concept: **Perplexity as a proxy for model confidence**
  - Why needed here: Attack success is measured via perplexity increases (ASR@99% threshold). PPL quantifies how "surprised" the model is by its own output given perturbed input—high PPL indicates the audio is pushing the model off its training distribution.
  - Quick check question: If clean audio yields PPL=1.16 and adversarial audio yields PPL=10.26, what does this ratio suggest about the model's uncertainty?

## Architecture Onboarding

- Component map: [Adversary Audio] → [Augmentation Pipeline: Translation + AdditiveNoise + SpecAugment] → [Whisper-large-v3 Audio Tower M_F] → [Audio Tokens] → [Qwen-7B LLM M_LLM] → [Text Output] → [Gradient Flow: ∇L w.r.t. x or δ]
- Critical path: SpecAugment integration in the forward pass is the single most important factor for real-world transfer (70% → 100% success). The augmentation must be differentiable or approximated via straight-through estimators.
- Design tradeoffs:
  - Targeted vs. untargeted: Targeted requires full-model gradients (slower, 5000 iterations); untargeted only needs audio tower (faster, batches of 100).
  - Stealthiness (ε) vs. success rate: ε=0.01 achieves 100% digital success but lower real-world transfer; ε=0.1 required for over-the-air robustness.
  - Defense-aware optimization: Adding augmentations improves transfer but may require more iterations; adaptive attackers can include defenses (e.g., EnCodec) in optimization loop.
- Failure signatures:
  - 0% real-world success with no augmentations → channel effects destroy perturbation structure.
  - High PPL but low WER → perturbation affects LLM confidence but not transcription accuracy (untargeted attack may be ineffective).
  - Defense via EnCodec (0% success at ≤12 kbps) → compression removes high-frequency perturbation components.
- First 3 experiments:
  1. Reproduce Table 1 with a single target ("Hey Qwen") on Qwen2-Audio; verify 100% digital success with ε=0.1, 4s duration.
  2. Ablate augmentations: run real-world attack with (a) no augmentation, (b) SpecAugment only, (c) full pipeline; confirm Table 3 trajectory (0% → 70% → 100%).
  3. Test EnCodec defense: apply 6 kbps compression to optimized adversarial audio before model input; verify attack success drops to 0% (Table 6).

## Open Questions the Paper Calls Out
The paper identifies several limitations and open questions in its discussion section, including the need for black-box transfer validation, testing against other ALLM architectures, and exploring more robust defense mechanisms beyond EnCodec compression.

## Limitations
- The white-box assumption for targeted attacks is highly unrealistic in deployment scenarios; the paper acknowledges but does not extensively validate black-box transfer
- Real-world validation uses a single audio tower (Whisper-large-v3) and LLM (Qwen-7B); generalization to other ALLM architectures remains untested
- The defense evaluation focuses on EnCodec compression but does not test other practical defenses like input denoising, audio preprocessing, or adaptive model architectures

## Confidence
- **High confidence**: The core mechanism of gradient-based audio perturbation optimization (Mechanism 1) - well-established adversarial ML technique with strong empirical validation
- **Medium confidence**: SpecAugment's critical role in real-world transfer (Mechanism 3) - strong empirical support in this paper but limited external validation
- **Medium confidence**: Audio-tower-only optimization sufficiency (Mechanism 2) - demonstrated effectiveness but lacks comparison with full-model optimization in untargeted setting

## Next Checks
1. **Black-box transfer validation**: Test whether adversarial audio optimized against Qwen2-Audio can successfully attack other ALLMs (e.g., GPT-4o Audio, Gemini Live) without model access
2. **Adaptive defense evaluation**: Implement and test EnCodec compression during the attack optimization loop (not just post-hoc) to assess whether adaptive defenses can eliminate successful attacks
3. **Real-world robustness validation**: Conduct over-the-air attacks in varying acoustic environments (different room acoustics, background noise levels, distances) to establish the operational limits of successful attack transfer