---
ver: rpa2
title: Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)
arxiv_id: '2504.04520'
source_url: https://arxiv.org/abs/2504.04520
tags:
- hessian
- matrix
- function
- diagonal
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a comprehensive guide on computing the Hessian
  matrix for large language models (LLMs) using PyTorch's autograd library, addressing
  the infeasibility of computing the full Hessian due to memory constraints. The core
  method involves computing exact Hessian for small parameter subsets using automatic
  differentiation and estimating the full diagonal using Hutchinson's trick with multiple
  vector-Hessian products.
---

# Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)

## Quick Facts
- arXiv ID: 2504.04520
- Source URL: https://arxiv.org/abs/2504.04520
- Authors: Ivan Ilin
- Reference count: 4
- Provides practical tools for computing Hessian matrices of perplexity in LLMs using PyTorch autograd

## Executive Summary
This paper addresses the computational challenge of obtaining Hessian information for large language models by combining exact computation for small parameter subsets with Hutchinson's trick for diagonal estimation. The approach enables researchers to analyze second-order optimization landscapes of perplexity without requiring full Hessian computation, which is infeasible for models with billions of parameters. Through experiments with OPT-125M, the work demonstrates that accurate Hessian estimates can be obtained using relatively modest computational resources, making second-order analysis accessible for LLM research.

## Method Summary
The method combines exact Hessian computation for small parameter groups using PyTorch's automatic differentiation with stochastic estimation of the full diagonal via Hutchinson's trick. The core innovation is an additive perplexity function that allows computing Hessians over large batches by summing contributions from smaller batches, circumventing memory limitations. Multiple vector-Hessian products are computed to estimate the diagonal, with empirical validation showing that the diagonal structure assumption holds for the tested model. The implementation provides practical tooling for researchers to analyze Hessian structure in LLMs, with source code available on GitHub.

## Key Results
- Accurate Hessian estimates achievable with batch sizes around 60 for OPT-125M model
- Diagonal structure assumption empirically validated for small-scale models
- Computational feasibility demonstrated despite quadratic memory scaling constraints
- Additive perplexity modification enables batch processing for large datasets

## Why This Works (Mechanism)
The approach leverages the fact that second-order optimization information can be partially recovered without full Hessian computation. By computing exact Hessians for small parameter subsets, the method captures local curvature information precisely where needed. Hutchinson's trick provides unbiased diagonal estimates using random vector projections, trading some statistical uncertainty for massive computational savings. The additive perplexity modification cleverly decomposes the batch-wise computation, allowing the algorithm to scale to larger datasets while maintaining mathematical correctness.

## Foundational Learning
1. **Automatic Differentiation**: PyTorch's autograd computes exact gradients and Hessians through computational graphs - needed for precise local curvature information; quick check: verify `torch.autograd.functional.hessian` works on simple functions.
2. **Hutchinson's Trick**: Stochastic method for estimating matrix diagonals using random vectors - needed to avoid full Hessian computation; quick check: confirm diagonal estimate converges with increasing vector count.
3. **Second-Order Optimization**: Hessian captures curvature information for optimization - needed to understand why Hessian analysis matters for LLM training; quick check: compare gradient descent vs Newton's method on simple convex problem.
4. **Perplexity as Loss Function**: Measures language model quality using cross-entropy - needed as the specific objective whose Hessian is computed; quick check: verify perplexity decreases during training.

## Architecture Onboarding

**Component Map:**
Perplexity Function -> Additive Decomposition -> Exact Hessian (small subsets) -> Hutchinson's Trick (diagonal estimation) -> PyTorch Autograd

**Critical Path:**
1. Define additive perplexity modification to enable batch processing
2. Compute exact Hessian for small parameter groups using autograd
3. Apply Hutchinson's trick with multiple vector-Hessian products
4. Aggregate diagonal estimates to form complete Hessian approximation

**Design Tradeoffs:**
- Memory vs Accuracy: Full Hessian impossible for large models, diagonal estimation sacrifices some information for feasibility
- Computational Cost vs Statistical Uncertainty: More Hutchinson vectors improve accuracy but increase runtime
- Batch Size vs Approximation Error: Larger batches improve estimation but strain memory limits

**Failure Signatures:**
- Memory overflow during exact Hessian computation indicates subset size too large
- High variance in diagonal estimates suggests insufficient Hutchinson vectors
- Negative perplexity values indicate implementation errors in additive modification

**First Experiments:**
1. Verify exact Hessian computation on 2-parameter toy model
2. Test Hutchinson's trick diagonal estimation on diagonal matrix
3. Validate additive perplexity modification with known batch-wise sums

## Open Questions the Paper Calls Out
None

## Limitations
- Computational expense remains prohibitive for models beyond 125M parameters
- Hutchinson's trick introduces statistical uncertainty that compounds with parameter count
- Additive perplexity modification may introduce approximation errors across different batches
- Empirical validation limited to single small-scale model and moderate batch sizes

## Confidence

**High confidence:** PyTorch autograd implementation correctness and basic Hutchinson's trick methodology
**Medium confidence:** Diagonal structure assumption validity and additive perplexity modification effectiveness
**Low confidence:** Scalability claims to larger models and robustness across different model architectures

## Next Checks
1. **Scalability validation**: Test the approach on models of increasing size (500M, 1B, 3B parameters) to determine the practical upper limit where full diagonal computation becomes infeasible.

2. **Architecture generalization**: Apply the method to different model architectures (GPT-2 variants, LLaMA-style models) to verify whether the diagonal structure assumption consistently holds across architectures.

3. **Batch size sensitivity**: Conduct systematic experiments varying batch sizes from 16 to 1024 to quantify how the additive perplexity approximation error scales with batch size and affects Hessian estimation quality.