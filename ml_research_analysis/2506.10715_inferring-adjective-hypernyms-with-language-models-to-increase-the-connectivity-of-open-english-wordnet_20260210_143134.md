---
ver: rpa2
title: Inferring Adjective Hypernyms with Language Models to Increase the Connectivity
  of Open English Wordnet
arxiv_id: '2506.10715'
source_url: https://arxiv.org/abs/2506.10715
tags:
- adjectives
- hypernymy
- dataset
- wordnet
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores hypernymy among adjectives, a relation largely
  overlooked in existing lexical resources. It constructs a gold-standard dataset
  of 302 English adjective pairs by adapting data from Polish and Dutch wordnets and
  performing human annotation.
---

# Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet

## Quick Facts
- arXiv ID: 2506.10715
- Source URL: https://arxiv.org/abs/2506.10715
- Reference count: 10
- Primary result: Language models improve adjective hypernymy prediction after fine-tuning on a gold-standard dataset, especially with definitions and synonym-augmented training data.

## Executive Summary
This study addresses the understudied task of hypernymy among adjectives, constructing the first English gold-standard dataset of 302 adjective pairs. The dataset, derived from Polish and Dutch wordnets and validated via human annotation, is used to evaluate and fine-tune large language models (TaxoLLaMa and SmolLM-360M-Instruct) on adjective hypernymy discovery. Results show that models initially struggle due to polysemy and part-of-speech confusion, but their performance improves significantly after fine-tuning, particularly when provided with definitions and synonym-augmented datasets. The work advances both theoretical understanding and computational modeling of adjectival semantics.

## Method Summary
The authors constructed a gold-standard dataset of 302 English adjective hyponym-hypernym pairs by extracting and translating data from Polish and Dutch wordnets, then validating against Open English WordNet (OEWN) and performing human annotation. They created two training versions: a single-hypernym dataset and a synonym-augmented multiple-hypernym dataset. TaxoLLaMa and SmolLM-360M-Instruct were fine-tuned using LoRA (60 steps, lr=2e-4, batch=8) on 211 training pairs, then evaluated on 91 test pairs with and without definitions. Metrics included MRR, precision, recall, F1, and POS prediction accuracy.

## Key Results
- Zero-shot models default to noun predictions (TaxoLLaMa: 58% nouns without definitions, 21% with definitions)
- Fine-tuning significantly improves POS accuracy (TaxoLLaMa: 100% adjective accuracy post-ft vs 21-44% pre-ft)
- Definition inclusion boosts MRR (TaxoLLaMa: 9.4→25.8 zero-shot; 23.6→33.3 post-ft)
- Synonym-augmented training (ft-multi) outperforms single-hypernym training (ft-single) across metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing explicit word definitions alongside input hyponyms improves hypernym prediction accuracy by reducing semantic ambiguity.
- Mechanism: Adjectives are highly polysemous (e.g., "cold" → temperature vs. personality). Definitions constrain the semantic field, enabling the model to map to the correct sense-specific hypernym rather than averaging across senses.
- Core assumption: The definition accurately represents the intended sense in the gold standard.
- Evidence anchors:
  - [abstract]: "performance improves significantly after fine-tuning, especially when provided with definitions"
  - [section 7]: Zero-shot TaxoLLaMa MRR increased from 9.4 (no definition) to 25.8 (with definition); after fine-tuning: 23.6 → 33.3
  - [corpus]: Weak direct corpus evidence on definition-based disambiguation for adjectives specifically; related work focuses on noun/verb entailment.
- Break condition: If definitions are noisy, circular, or mismatched to the target sense, disambiguation fails and may introduce bias (e.g., lexical overlap artifacts noted in TaxoLLaMa-ft-single).

### Mechanism 2
- Claim: Synonym-augmented training data (multiple hypernyms per hyponym) improves model performance by aligning training with the inherently graded, multi-valid nature of adjective hypernymy.
- Mechanism: Single-hypernym labels penalize semantically correct predictions that differ from the annotated instance. Synset-based augmentation (adding OEWN synonyms) expands the acceptance set, reducing false negatives and encouraging the model to capture the broader semantic space.
- Core assumption: Synset membership in OEWN reliably indicates semantic equivalence for hypernymy purposes.
- Evidence anchors:
  - [section 5]: "a second version of the dataset was later developed... synonyms of each hypernym were added based on synset membership in OEWN"
  - [section 7, Table 5]: TaxoLLaMa-ft-multi outperforms ft-single across metrics; F1 improves from 0.14/0.16 (ft-single) to 0.17/0.26 (ft-multi)
  - [corpus]: No direct corpus validation; neighbor papers address lexical semantics broadly but not adjective hypernymy augmentation.
- Break condition: If synsets conflate senses or include near-synonyms that are not true hypernyms, augmentation introduces label noise.

### Mechanism 3
- Claim: Fine-tuning with LoRA on a small, high-quality domain-specific dataset can correct systemic part-of-speech (POS) confusion and induce task-specific lexical competence, even in smaller models.
- Mechanism: Pre-trained models default to noun predictions for hypernymy (TaxoLLaMa trained only on nouns/verbs). Fine-tuning on adjective-specific pairs re-biases output distribution toward adjectives and calibrates the hypernymy relation for adjectival semantics.
- Core assumption: The 302-pair gold standard is sufficiently representative to shift model behavior without catastrophic forgetting.
- Evidence anchors:
  - [section 7, Table 3]: TaxoLLaMa zero-shot POS accuracy for adjectives: 21% (no def), 44% (with def); after ft-multi: 100% (both settings); SmolLM-360M shows similar gains
  - [section 6.1]: LoRA fine-tuning (60 steps, lr=2e-4, batch=8 simulated) on 211 training pairs
  - [corpus]: Transformers can learn transitive/connectivity relations in some graphs (arXiv:2509.22343), but success depends on structure; adjective hypernymy appears learnable given appropriate supervision.
- Break condition: If training data is too small, biased, or contains annotation errors, fine-tuning may overfit to spurious patterns rather than generalize.

## Foundational Learning

- Concept: **Hypernymy vs. synonymy for adjectives**
  - Why needed here: Unlike nouns, adjective hypernymy lacks clear hierarchical structure and is easily conflated with synonymy. The substitution test (hypernym must be substitutable in context without contradiction) is the operational boundary.
  - Quick check question: Given "lucid" → "aware," can "aware" replace "lucid" in "a lucid explanation" without changing core meaning? If not, is this hypernymy or another relation?

- Concept: **Polysemy and sense disambiguation in adjectives**
  - Why needed here: Adjectives like "hard" (physical solidity vs. difficulty) or "cold" (temperature vs. personality) require context to resolve sense. Models must use definitions or context to map correctly.
  - Quick check question: For "sharp" in "a sharp knife" vs. "a sharp mind," which semantic field is activated, and how would the hypernym differ?

- Concept: **WordNet synset structure (OntoLex-Lemon model)**
  - Why needed here: The gold standard uses OEWN synsets to validate pairs, add synonyms, and retrieve definitions. Understanding LexicalEntry, LexicalSense, and LexicalConcept is essential for data construction and evaluation.
  - Quick check question: If "difficult" and "hard" appear in the same OEWN synset, why were they excluded from the gold standard despite being hypernym-hyponym in plWN?

## Architecture Onboarding

- Component map:
  Polish/Dutch WordNet -> wn library extraction -> English translation -> OEWN validation -> human annotation -> definition attachment -> synonym augmentation -> single/multiple datasets

- Critical path:
  1. Gold standard construction (human annotation + synset validation + definition inclusion)
  2. Single vs. multiple dataset creation
  3. Zero-shot baseline evaluation (with/without definition)
  4. LoRA fine-tuning (single, then multiple)
  5. Evaluation against both test sets (91 pairs, 30% holdout)

- Design tradeoffs:
  - **Single vs. multiple hypernyms**: Single enforces precision but penalizes valid alternatives; multiple is more forgiving but requires synset quality
  - **Definition inclusion**: Improves disambiguation but risks lexical-overlap bias (model predicts words from definition)
  - **Model size**: TaxoLLaMa (7B) outperforms SmolLM (360M), but smaller model still shows substantial POS correction—suggests data quality may matter more than scale for this task

- Failure signatures:
  - **POS confusion**: Model outputs nouns instead of adjectives (zero-shot TaxoLLaMa: 58% noun-only outputs without definition)
  - **Lexical overlap bias**: Model predicts words appearing in definition rather than true hypernym (e.g., "existent" from "still in existence")
  - **Overfitting to small data**: High training performance but poor generalization if test pairs diverge from training distribution
  - **Annotation subjectivity**: Inter-annotator agreement moderate (Cohen's κ=0.61–0.65; Fleiss' κ=0.48); edge cases may be inconsistently labeled

- First 3 experiments:
  1. **Baseline POS audit**: Run zero-shot TaxoLLaMa and SmolLM on the 91-pair test set; record POS distribution of outputs with/without definitions. Confirm the 58% noun vs. 21% adjective split before proceeding.
  2. **Ablation on definition inclusion**: Fine-tune TaxoLLaMa-ft-multi with definition-prompting vs. lemma-only prompting. Measure MRR delta and identify cases where definition hurts (e.g., overlap bias).
  3. **Cross-dataset generalization test**: Train on plWN-derived pairs only, test on ODWN-derived pairs (and vice versa) to assess whether the model learns language-general adjective hypernymy or resource-specific artifacts.

## Open Questions the Paper Calls Out

- **Question**: How does the performance of language models on adjective hypernymy transfer to or interfere with other semantic relations specific to adjectives, such as antonymy or similarity?
  - Basis: The authors state they aim to "extend language models evaluation to other semantic relations between adjectives to explore how they differ theoretically and how well they are captured computationally."
  - Why unresolved: The current study focused exclusively on hypernymy, leaving the distinct computational modeling of other adjectival relations unexplored.
  - What evidence would resolve it: A comprehensive benchmark evaluating fine-tuned models on tasks involving antonymy, similarity, and pertainymy alongside hypernymy discovery.

- **Question**: Can the proposed fine-tuning methodology successfully identify and integrate hypernymy relations for the entire set of adjectives in the Open English Wordnet (OEWN) to fully connect the graph?
  - Basis: The authors list "model adjective hypernymy in OEWN, identifying and representing hypernymy relations for all adjectives in the resource" as a primary goal for future work.
  - Why unresolved: The current experiments were limited to a small gold-standard dataset (302 pairs), and the methodology has not yet been applied to the full resource.
  - What evidence would resolve it: The successful application of the fine-tuned model to the OEWN adjective set, resulting in a significantly higher connectivity score for the lexical graph.

- **Question**: To what extent do the observed results generalize to larger or architecturally distinct language models beyond TaxoLLaMa and SmolLM-360M-Instruct?
  - Basis: The authors explicitly limit the scope of their conclusions by noting that "Only two language models were used," which restricts generalizability regarding model capabilities.
  - Why unresolved: The paper did not test state-of-the-art architectures of different sizes or training paradigms, leaving the performance ceiling for this task unknown.
  - What evidence would resolve it: A comparative evaluation of diverse models (e.g., GPT-4, LLaMA-3, specialized encoder models) on the adjective hypernymy gold standard.

## Limitations

- The gold standard is small (302 pairs) and derived from translated Polish/Dutch data, raising cross-linguistic generalizability concerns.
- Moderate inter-annotator agreement (κ=0.61–0.65) indicates subjective boundaries in adjective hypernymy.
- Zero-shot performance remains low, suggesting substantial reliance on fine-tuning for task success.
- Synonym-augmentation depends on OEWN synset quality, which is not independently validated.
- Lexical-overlap bias from definition inclusion remains a concern in evaluation.

## Confidence

- **High**: Models initially default to noun predictions for adjective hypernymy (supported by POS accuracy metrics before and after fine-tuning).
- **Medium**: Definition inclusion improves disambiguation by constraining sense (mechanism plausible but not directly tested for adjectives in prior work).
- **Medium**: Synonym-augmented datasets improve performance by aligning training with graded adjective hypernymy (mechanism reasonable but synset quality unverified).
- **Medium**: LoRA fine-tuning on 211 pairs effectively shifts model behavior (training procedure specified, but small sample size limits generalizability).

## Next Checks

1. Cross-linguistic generalization: Train on plWN-derived pairs, test on ODWN-derived pairs (and vice versa) to assess whether the model learns language-general adjective hypernymy or resource-specific artifacts.
2. Definition ablation study: Fine-tune TaxoLLaMa-ft-multi with definition-prompting vs. lemma-only prompting. Measure MRR delta and identify cases where definition inclusion hurts due to lexical-overlap bias.
3. Human evaluation of synset quality: Independently validate a sample of OEWN synsets used for synonym augmentation to confirm that synset membership reliably indicates semantic equivalence for hypernymy purposes.