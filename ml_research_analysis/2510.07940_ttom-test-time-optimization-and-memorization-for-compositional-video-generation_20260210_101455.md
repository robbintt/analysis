---
ver: rpa2
title: 'TTOM: Test-Time Optimization and Memorization for Compositional Video Generation'
arxiv_id: '2510.07940'
source_url: https://arxiv.org/abs/2510.07940
tags:
- generation
- optimization
- memory
- video
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compositional text-to-video
  generation, where current models struggle with complex scenes involving multiple
  objects, attributes, and spatial-temporal relationships. The proposed Test-Time
  Optimization and Memorization (TTOM) framework introduces a training-free method
  that aligns video outputs with spatiotemporal layouts during inference.
---

# TTOM: Test-Time Optimization and Memorization for Compositional Video Generation

## Quick Facts
- arXiv ID: 2510.07940
- Source URL: https://arxiv.org/abs/2510.07940
- Reference count: 21
- The paper introduces a training-free test-time optimization framework that significantly improves compositional text-to-video generation through layout-aligned attention optimization and parametric memory.

## Executive Summary
The paper addresses the challenge of compositional text-to-video generation where current models struggle with complex scenes involving multiple objects, attributes, and spatial-temporal relationships. The proposed Test-Time Optimization and Memorization (TTOM) framework introduces a training-free method that aligns video outputs with spatiotemporal layouts during inference. Instead of directly modifying latents or attention maps, TTOM optimizes new parameters guided by layout-attention objectives and maintains historical optimization contexts using a parametric memory mechanism with flexible operations like insert, read, update, and delete.

## Method Summary
TTOM operates on cross-attention maps in diffusion transformers to align video generation with layouts. The method extracts attention maps from specific DiT layers, computes JSD loss between attention maps and Gaussian-smoothed layout masks, and optimizes lightweight LoRA parameters (rank=32) injected into cross-attention blocks during early denoising steps (1-5). A parametric memory stores optimized parameters keyed by scene abstraction, enabling transfer of compositional patterns across prompts through retrieval and initialization.

## Key Results
- Achieves 34% and 14% relative gains over CogVideoX-5B and Wan2.1-14B on T2V-CompBench benchmark
- Memory integration adds +13.91% improvement over test-time optimization alone (0.4321 → 0.4922 on motion)
- Optimal performance at 5 denoising steps with 8 iterations; degradation occurs with more steps/iterations
- Parametric memory enables efficient inference when parameters can be loaded directly without optimization

## Why This Works (Mechanism)

### Mechanism 1
Cross-attention maps in specific DiT layers exhibit measurable correlation with spatial layouts of generated video content. The probe experiment extracts cross-attention maps A_i from each layer, generates corresponding videos, and computes mIoU overlap between attention maps and segmentation masks from GroundingDINO+SAM2. Layer-wise relevance varies significantly (Fig. 3 shows mIoU ranging ~0.02–0.10 across layers 0–30).

### Mechanism 2
Optimizing lightweight parameters (LoRA) on attention-to-layout alignment loss during early denoising steps improves compositional adherence without degrading visual quality. Insert LoRA (rank=32) into Q/K/V/O projections of cross-attention blocks. Compute JSD loss L_align = (1/N) Σ JSD(Ā_i || B̄_i) between attention maps and Gaussian-smoothed layout masks. Update only during first 5 denoising steps (Tab. 7 shows 5 steps optimal; 7 steps degrades).

### Mechanism 3
Parametric memory enables transfer of compositional patterns across prompts through scene abstraction and parameter retrieval. Memory M = {g(C) : φ*_C | C ∈ C_H} stores optimized LoRA weights keyed by scene abstraction. For "A vibrant red balloon drifts right to left above a grand statue," abstraction is "<object A> drifts right to left above <object B>." Retrieval uses text embedding similarity; matched parameters initialize or replace optimization.

## Foundational Learning

- Concept: Cross-attention in diffusion transformers
  - Why needed here: TTOM operates on cross-attention maps to align video generation with layouts. Understanding where text-vision interaction occurs is essential for layer selection.
  - Quick check question: In a DiT, which layers would you inspect to find where object phrases influence spatial generation?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: TTOM inserts LoRA weights (rank=32) into attention projections as the optimizable parameters. Without this, you'd modify either latents (risking collapse) or full model weights (too expensive).
  - Quick check question: Why is optimizing LoRA parameters safer than directly modifying latents z_t during denoising?

- Concept: Diffusion denoising trajectory (early vs. late steps)
  - Why needed here: TTOM constrains optimization to the first 5 steps. This relies on the principle that early steps set structure while late steps refine details.
  - Quick check question: What artifact would you expect if you applied layout alignment optimization at all 50 denoising steps instead of just the first 5?

## Architecture Onboarding

- Component map:
  LLM Layout Planner (GPT-4o) → Spatiotemporal layout {Y_0, ..., Y_N} → VFM Backbone (CogVideoX-5B / Wan2.1-14B) with DiT architecture → Cross-attention extractor → Attention maps A_i per layer → TTO module: JSD loss + AdamW optimizer on LoRA parameters (rank=32, lr=1e-4) → Parametric memory: Key-value store with scene abstraction, text embedding, LFU eviction → Memory controller: insert/read/update/delete operations

- Critical path:
  1. Prompt → LLM → Layout (bbox sequences per object)
  2. Layout → Gaussian smoothing → Soft masks B̄_i
  3. During denoising (steps 1-5): Extract A_i from high-relevance layers → Compute JSD loss → Backprop to LoRA params → Update
  4. After generation: Unload LoRA params → Abstract scene → Store in memory with key
  5. Next prompt: Retrieve matched params → Initialize LoRA → Optionally continue optimization

- Design tradeoffs:
  - Memory size vs. retrieval noise: More entries (200 vs. 50) improve performance (Tab. 6) but increase retrieval complexity and potential for irrelevant matches
  - Optimization steps vs. quality: 5 steps optimal (0.4321); 7 steps degrades (0.4296). More optimization is not better
  - Iterations per step: 8 iterations optimal (0.4321); 16 iterations severely degrades (0.3547)
  - Direct load vs. continued optimization: Memory-only initialization achieves 0.4754 (Tab. 4); adding TTO reaches 0.4846 but adds 200s latency

- Failure signatures:
  - Flickering/inconsistency: Excessive optimization iterations (>12) or steps (>5)
  - Poor layout adherence: Wrong layer selection (low attention-layout relevance)
  - Irrelevant retrieval: Scene abstraction too generic; matches wrong compositional patterns
  - Object disappearance: Layout planning failure; LLM generates inconsistent bbox sequences

- First 3 experiments:
  1. Reproduce the probe experiment (Fig. 3): Generate 10 videos with single-object prompts, extract attention from all layers, compute mIoU against SAM2 segmentations. Identify which layers show highest correlation for your target VFM
  2. Ablate optimization steps: Test 1, 3, 5, 7 denoising steps with 8 iterations each on 20 compositional prompts. Confirm 5 steps is the saturation point
  3. Test memory transfer: Optimize on "A car moves left," then directly load params for "A dog moves left" without optimization. Measure whether motion direction transfers (qualitative + quantitative on motion category)

## Open Questions the Paper Calls Out

### Open Question 1
How can fusion strategies be refined to maximize the benefit of retrieved historical contexts while minimizing noise injection when initializing parameters from the parametric memory? The paper currently utilizes a simple average fusion of matched entries, which limits performance gains when retrieving multiple context items.

### Open Question 2
How can the computational overhead of test-time optimization be minimized to support efficient, on-the-fly video generation in streaming scenarios? While the memory mechanism allows for "read-only" efficient inference in ideal cases, the standard operational mode requires gradient descent steps that currently double the inference time in some configurations.

### Open Question 3
To what extent does the accuracy of the LLM-generated spatiotemporal layout act as a bottleneck for the attention-to-layout alignment process? The methodology relies entirely on an LLM (GPT-4o) to generate ground-truth bounding boxes, but does not analyze the system's robustness to common LLM hallucinations or spatial reasoning errors in the layout phase.

## Limitations
- Layer selection mechanism lacks specificity about exact layers used for optimization
- Memory transfer mechanism relies on manual template matching without clear criteria for abstraction granularity
- Memory controller retrieval mechanism (thresholds, fusion strategy) is underspecified

## Confidence

**High confidence**: The core hypothesis that early denoising steps (1-5) are optimal for layout alignment (Mechanism 2) is well-supported by ablation studies (Tab. 7, Tab. 8). The performance degradation when exceeding 5 steps or 8 iterations is consistent and quantifiable.

**Medium confidence**: The attention-layout correlation probe (Mechanism 1) shows clear variance across layers, but the generalization to unseen compositional prompts is not directly validated. The layer selection for optimization depends on this generalization, which remains an assumption.

**Low confidence**: The parametric memory's contribution to performance (Tab. 3 shows +13.91%) is reported but lacks detailed analysis of retrieval accuracy, abstraction quality, or failure cases. The memory-only initialization (0.4754) vs. continued optimization (0.4846) comparison suggests additional gains, but the conditions for optimal memory usage are unclear.

## Next Checks

1. **Probe reproducibility test**: Replicate the attention-layout correlation experiment with 20 compositional prompts not in the original probe set. Measure mIoU across all layers and verify that the high-relevance layers identified match those used in the main experiments.

2. **Memory retrieval fidelity analysis**: For 50 memory entries, manually inspect the scene abstraction keys and their corresponding LoRA parameters. Test retrieval accuracy by querying with prompts sharing abstract structures and measuring whether retrieved parameters actually improve compositional alignment compared to random initialization.

3. **Optimization step sensitivity on diverse compositions**: Test the 1-3-5-7 step ablation on prompts spanning all seven T2V-CompBench categories (Motion, Numeracy, Spatial, Con-attr, Dyn-attr, Action, Interact). Confirm that 5 steps remains optimal across all compositional types and identify any categories where more or fewer steps might be preferable.