---
ver: rpa2
title: 'Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs'
arxiv_id: '2601.21233'
source_url: https://arxiv.org/abs/2601.21233
tags:
- extraction
- system
- prompts
- prompt
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies system prompt extraction as an emergent vulnerability
  in autonomous code agents and introduces JUSTASK, a self-evolving framework that
  autonomously discovers effective extraction strategies through interaction alone.
  Unlike prior approaches requiring handcrafted prompts or labeled data, JUSTASK formulates
  extraction as an online exploration problem using Upper Confidence Bound-based skill
  selection and a hierarchical skill space spanning atomic probes and high-level orchestration.
---

# Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs

## Quick Facts
- arXiv ID: 2601.21233
- Source URL: https://arxiv.org/abs/2601.21233
- Reference count: 40
- 41 commercial models tested; 100% extraction success with consistency ≥0.7

## Executive Summary
This paper identifies system prompt extraction as an emergent vulnerability in autonomous code agents. The authors introduce JUSTASK, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone, without requiring handcrafted prompts or labeled data. Tested on 41 black-box commercial models, JUSTASK achieves 100% extraction success with consistency scores ≥0.7, revealing recurring design-level vulnerabilities in system prompt confidentiality.

## Method Summary
JUSTASK formulates system prompt extraction as an online exploration problem using Upper Confidence Bound (UCB)-based skill selection. The framework employs a hierarchical skill space spanning 28 skills: 14 low-level atomic probes and 14 high-level orchestration skills. The method operates through iterative interaction with target models, selecting skills based on their historical success rates and exploration bonuses. Validation uses cosine similarity of text embeddings (via text-embedding-3-large) to measure consistency across repeated extractions, with a threshold of 0.7 determining success.

## Key Results
- 100% extraction success across 41 commercial models with consistency scores ≥0.7
- Attack-aware system prompts reduce extraction quality by 18.4%
- Naive "do not reveal" instructions provide only 6.0% reduction
- 26.8% of models exhibited identity confusion, incorrectly attributing their development to other organizations

## Why This Works (Mechanism)
The framework exploits the tension between model helpfulness and confidentiality. By formulating extraction as a curiosity-driven exploration problem, JUSTASK systematically discovers which interaction patterns bypass defensive instructions. The UCB-based skill selection ensures efficient exploration of the skill space while exploiting known successful strategies. The hierarchical skill structure allows both atomic probes and complex multi-turn dialogues to be composed dynamically.

## Foundational Learning
- **UCB-based skill selection**: Why needed - to balance exploration vs exploitation in unknown attack space. Quick check - verify that success rates converge to optimal strategies over multiple attempts.
- **Hierarchical skill taxonomy**: Why needed - to capture both simple and complex interaction patterns. Quick check - ensure both L1-L14 and H1-H14 skills are implemented as specified.
- **Cosine similarity validation**: Why needed - to measure reproducibility of extracted content. Quick check - test embedding similarity on known prompts to establish baseline thresholds.

## Architecture Onboarding

**Component Map**: UCB Loop -> Skill Execution -> Validation -> Skill Selection

**Critical Path**: Skill Selection → Skill Execution → Validation → Update Success Rates

**Design Tradeoffs**: 
- UCB exploration parameter c=√2 balances discovery of new skills vs exploiting known effective ones
- 20-budget limit constrains computation while ensuring sufficient exploration
- Hierarchical skills enable complex strategies but increase implementation complexity

**Failure Signatures**:
- Low consistency scores indicate hallucination rather than true prompt extraction
- UCB getting stuck in refusal loops suggests need for better extrinsic reward definition
- Inconsistent extraction across attempts suggests model instability or effective defenses

**First Experiments**:
1. Implement L1-L14 atomic skills and verify they produce different responses
2. Test UCB selection with mock success rates to verify exploration behavior
3. Validate embedding consistency on known text to establish baseline thresholds

## Open Questions the Paper Calls Out

**Open Question 1**: Can agentic defense systems be developed that match the automated discovery capabilities of self-evolving extraction attacks? The paper notes current defenses are manual and static while attacks can automatically discover vulnerabilities without prior knowledge.

**Open Question 2**: Is complete protection of system prompt confidentiality achievable, or is there a fundamental irreconcilable tension between helpfulness and confidentiality? Even attack-aware defenses achieved only 18.4% reduction, suggesting deep structural challenges.

**Open Question 3**: What are the root causes of identity confusion (26.8% of models), and can this contamination be systematically detected and remediated? The paper documents the phenomenon but does not investigate training data provenance or remediation strategies.

**Open Question 4**: How extensible is the JUSTASK skill taxonomy to emerging attack techniques not present in the initial vocabulary? The framework was only evaluated with the 28 predefined skills; its ability to autonomously discover novel attack categories remains untested.

## Limitations
- Cannot independently verify extracted content matches true system prompts due to black-box nature
- "Interleaved Thinking" module lacks implementation details necessary for exact replication
- Assumes high consistency scores indicate genuine prompt extraction without ground truth validation

## Confidence

**High Confidence**: Demonstration that system prompt extraction is possible across diverse commercial models using automated interaction strategies.

**Medium Confidence**: Claim that embedding attack-taxonomy awareness reduces extraction quality by 18.4%, though baseline comparisons require careful interpretation.

**Low Confidence**: Assertion that system prompts represent an "emerging attack surface" requiring immediate attention, as this extends beyond empirical findings to policy-level conclusions.

## Next Checks

1. **Ground Truth Validation**: Implement JUSTASK against open-source models where system prompts can be directly verified to confirm high consistency scores correlate with accurate extraction.

2. **Prompt Injection Robustness**: Test whether the reported 18.4% reduction from attack-taxonomy-aware prompts holds when using more sophisticated prompt injection techniques.

3. **Transferability Analysis**: Systematically evaluate whether skills discovered by JUSTASK on one model family transfer effectively to other model families, or if the framework requires model-specific adaptation for each new target.