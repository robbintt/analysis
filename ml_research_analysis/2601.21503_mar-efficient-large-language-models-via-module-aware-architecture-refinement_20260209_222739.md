---
ver: rpa2
title: 'MAR: Efficient Large Language Models via Module-aware Architecture Refinement'
arxiv_id: '2601.21503'
source_url: https://arxiv.org/abs/2601.21503
tags:
- spiking
- energy
- loss
- sequence
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MAR, a two-stage framework for efficient large
  language models that jointly addresses the computational bottlenecks of quadratic
  attention and dense feed-forward networks. MAR first replaces attention with State
  Space Models (SSMs) for linear-time sequence modeling, then applies spiking neurons
  to sparsify activations and reduce FFN energy consumption.
---

# MAR: Efficient Large Language Models via Module-aware Architecture Refinement

## Quick Facts
- **arXiv ID:** 2601.21503
- **Source URL:** https://arxiv.org/abs/2601.21503
- **Reference count:** 0
- **One-line primary result:** MAR achieves near-teacher performance (57.93% vs 61.88% accuracy) while significantly reducing inference energy consumption.

## Executive Summary
MAR proposes a two-stage framework for efficient large language models that addresses the computational bottlenecks of quadratic attention and dense feed-forward networks. The approach first replaces attention with State Space Models (SSMs) for linear-time sequence modeling, then applies spiking neurons to sparsify activations and reduce FFN energy consumption. To integrate Spiking Neural Networks (SNNs) with SSMs, MAR introduces Adaptive Ternary Multi-step Neuron (ATMN) for enhanced information capacity and Spike-aware Bidirectional Distillation Strategy (SBDS) to mitigate temporal mismatch. Experimental results show MAR outperforms both efficient models of comparable scale and larger spiking LLMs.

## Method Summary
MAR operates in two stages: first, it replaces attention mechanisms with Discrete Mamba-2 SSMs for linear-time sequence processing; second, it inserts Adaptive Ternary Multi-step Neurons (ATMNs) at four positions per decoder layer and trains using Spike-aware Bidirectional Distillation Strategy (SBDS). The ATMN uses ternary {-1, 0, +1} spikes with adaptive thresholds to increase information density while maintaining sparsity. SBDS combines forward and reverse KL divergence with pre-normalization feature alignment to transfer knowledge from dense ANNs to sparse SNNs. The framework is trained on 7B tokens from GenQA, OpenHermes 2.5, and InfinityInstruct for 1 epoch.

## Key Results
- Restores teacher model performance from 61.88% to 57.93% average accuracy on six benchmarks
- Achieves 8.92 percentage point improvement over binary spiking neurons with ATMN (46.28% → 55.20%)
- Pre-normalization alignment (57.20%) outperforms post-normalization (56.75%) in SBDS

## Why This Works (Mechanism)

### Mechanism 1: SSM-based Linear-Time Sequence Modeling
Replacing quadratic attention with State Space Models (SSMs) converts O(L²) complexity to O(L) while preserving representational capacity through knowledge distillation. The sequential inductive bias of SSMs can approximate token-mixing functions of attention for language tasks when properly distilled.

### Mechanism 2: Adaptive Ternary Multi-step Neuron (ATMN)
ATMN increases information density per spike event by emitting {-1, 0, +1} values instead of binary {0, 1}, mitigating representational loss from sparsification. This triples information capacity per timestep while maintaining sparsity through adaptive thresholds.

### Mechanism 3: Spike-aware Bidirectional Distillation Strategy (SBDS)
SBDS combines forward KL, reverse KL, and pre-normalization feature alignment to transfer knowledge from dense ANN to sparse SNN while compensating for temporal mismatch. The asymmetric distillation objectives capture different distributional characteristics of sparse spike outputs.

## Foundational Learning

- **State Space Models (Mamba architecture)**: Why needed—understanding how SSMs propagate information through continuous hidden states vs attention's discrete key-value access. Quick check—Can you explain why Mamba's selective state space mechanism allows content-dependent reasoning without explicit attention?

- **Spiking Neural Networks and membrane dynamics**: Why needed—ATMN extends leaky integrate-and-fire neurons with ternary outputs and adaptive thresholds. Quick check—What is the functional difference between hard reset (V → 0 after spike) vs soft reset (V → V - threshold), and which does ATMN use?

- **Knowledge Distillation objectives (KL divergence variants)**: Why needed—SBDS combines forward and reverse KL asymmetrically. Quick check—Why does reverse KL(q||p) encourage the student to focus on high-probability regions of the teacher, while forward KL(p||q) penalizes missing mass?

## Architecture Onboarding

- **Component map**: Input → Embedding → [Layer × N] → RMSNorm → LM Head → Output
  - Each layer contains: RMSNorm → ATMN → Discrete Mamba-2 (SSM) → ATMN → Residual
  - RMSNorm → ATMN → FFN (Gate/Up/Down Proj) → ATMN → Residual

- **Critical path**: Stage 1 converts pre-trained LLaMA attention to Llamba (Discrete Mamba-2) via distillation. Stage 2 inserts ATMN modules at all 4 positions per layer. Initialize V_adaptive = e^a with small random a. Train using SBDS loss with α≈0.2, β≈0.7. Validate on zero-shot benchmarks.

- **Design tradeoffs**: Ternary vs binary spikes yields +8.9% accuracy gain but 3× energy per spike event. Pre-norm alignment (57.20%) outperforms post-norm (56.75%). FFN dominates energy below L≈2500-9000 sequence length.

- **Failure signatures**: Firing rate collapse (<5%) indicates underfitting; firing rate explosion (>50%) negates energy savings; temporal lag suggests SSM hidden state decay misconfiguration; distillation divergence indicates β weight may be too aggressive.

- **First 3 experiments**: 1) Train binary neurons baseline on 1B-token subset (~46% accuracy). 2) Enable ATMN only (~55% accuracy). 3) Sweep α∈{0.1, 0.2, 0.3} × β∈{0.5, 0.7, 0.9} with full SBDS.

## Open Questions the Paper Calls Out

### Open Question 1
Can MAR's two-stage spiking approach maintain effectiveness when scaled to models significantly larger than 1.4B parameters? All experiments use Llamba-1B; scaling behavior to 7B+ is unknown.

### Open Question 2
What is the theoretical explanation for why pre-normalization alignment outperforms post-normalization alignment in SBDS? The paper states pre-norm features are "more stable and transferable" but offers no mechanistic explanation.

### Open Question 3
Do theoretical energy estimates for spiking operations (MAC vs AC) translate to realized savings on actual hardware accelerators? Energy savings are estimated using published per-operation costs rather than measured on hardware.

### Open Question 4
Can MAR's performance gap to the teacher model (61.88% → 57.93%) be closed further without increasing model scale or sacrificing sparsity? It's unclear whether this gap stems from fundamental SNN capacity limits or suboptimal distillation.

## Limitations

- All experiments conducted on 1B parameter model; scaling behavior to larger models remains unexplored
- Training protocol specifies only 1 epoch on 7B tokens without detailed hyperparameter specification
- Energy measurements based on theoretical per-operation costs rather than actual hardware measurements

## Confidence

**High Confidence (>80%)**: Core technical contributions (ATMN architecture and SBDS distillation) are mathematically sound with strong ablation support.

**Medium Confidence (50-80%)**: Overall performance claims depend on specific implementation details and hardware platform that are not fully disclosed.

**Low Confidence (<50%)**: Scaling hypothesis that efficiency gains will remain proportional as model size increases is speculative with no supporting evidence.

## Next Checks

1. **Cross-Model Scaling Validation**: Implement MAR on LLaMA-3.2-8B and LLaMA-3.2-70B, measuring accuracy retention and energy efficiency at each scale.

2. **Hardware-Informed Energy Benchmarking**: Implement MAR on both GPU (dense baseline) and neuromorphic hardware (spiking implementation) to measure actual energy consumption under identical workload conditions.

3. **Extended Training Protocol Analysis**: Train MAR for multiple epochs (3-5) with comprehensive hyperparameter sweeps to establish whether 1-epoch results represent convergence or underfitting.