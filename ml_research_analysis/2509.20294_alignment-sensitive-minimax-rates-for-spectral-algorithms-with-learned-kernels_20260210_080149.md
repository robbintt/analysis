---
ver: rpa2
title: Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels
arxiv_id: '2509.20294'
source_url: https://arxiv.org/abs/2509.20294
tags:
- theorem
- equation
- have
- risk
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Effective Span Dimension (ESD) as a novel
  complexity measure to characterize signal-kernel alignment in adaptive learning
  settings. Unlike traditional measures, the ESD depends jointly on the signal, spectrum,
  and noise level, making it suitable for analyzing learned kernels where spectral
  assumptions may not hold.
---

# Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels

## Quick Facts
- **arXiv ID:** 2509.20294
- **Source URL:** https://arxiv.org/abs/2509.20294
- **Reference count:** 40
- **Primary result:** Introduces Effective Span Dimension (ESD) as a complexity measure for signal-kernel alignment, proving minimax optimal rates for spectral algorithms and showing OP-GF can provably reduce ESD.

## Executive Summary
This paper introduces the Effective Span Dimension (ESD) as a novel complexity measure for adaptive learning with learned kernels. Unlike traditional measures that assume fixed kernels, ESD jointly depends on signal structure, spectrum, and noise level, making it suitable for analyzing adaptive feature learning. The authors prove that spectral algorithms achieve minimax optimal rates of $\sigma^2 K$ where $K$ is the ESD, and demonstrate that over-parameterized gradient flow can provably reduce ESD by improving signal-kernel alignment during training.

## Method Summary
The paper develops a theoretical framework for analyzing spectral algorithms with learned kernels. The core method involves computing the ESD by sorting eigenvalues and finding where the tail signal energy drops below cumulative noise variance. For over-parameterized gradient flow, parameters are updated through gradient descent on a deep linear network parameterization, which modifies the effective spectrum. The PC estimator is then applied using the learned spectrum truncated at the updated ESD value.

## Key Results
- Proves minimax excess risk scales as $\sigma^2 K$ where $K$ is the ESD for sequence models
- Shows over-parameterized gradient flow can provably reduce ESD by reordering eigenvalues to align with signal structure
- Demonstrates the span profile framework overcomes limitations of classical fixed-kernel theories by characterizing problem hardness through the trade-off function $H_{\theta^*,\lambda}(k)$
- Provides numerical experiments showing ESD evolution during training and its relationship to estimation error

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Effective Span Dimension (ESD) provides a minimax optimal rate for spectral algorithms by quantifying the interaction between signal structure, kernel spectrum, and noise.
- **Mechanism:** The ESD $d^\dagger$ identifies the smallest number of leading eigencoordinates such that the energy of the remaining signal tail drops below the cumulative variance $k\sigma^2$. This point defines the optimal bias-variance trade-off for the Principal Component (PC) estimator. The paper proves that for a class of signals with ESD at most $K$, the minimax risk scales as $\sigma^2 K$.
- **Core assumption:** The estimator is a spectral algorithm (specifically PC in the theoretical bounds), and the noise is uncorrelated with known variance $\sigma^2$.
- **Evidence anchors:** [abstract]: "We prove that for sequence models whose ESD is at most $K$, the minimax excess risk scales as $\sigma^2 K$." [section 3]: Theorem 3.3 establishes the minimax risk scaling $\inf \sup R(\hat{\theta}, \theta^*) \asymp K\sigma^2$ for the class $\mathcal{F}_{K,\lambda}^{(n)}$.
- **Break condition:** If the signal has significant mass on eigenfunctions with small eigenvalues (high misalignment), the ESD grows large, potentially making the bound $\sigma^2 d^\dagger$ vacuous.

### Mechanism 2
- **Claim:** Over-parameterized gradient flow (OP-GF) can reduce the ESD by adaptively modifying the kernel eigenvalues to better align with the signal.
- **Mechanism:** The OP-GF dynamics update parameters such that the learned eigenvalues $\tilde{\lambda}_j(t)$ associated with strong signals grow faster than those with weak/noisy signals. This reordering of the spectrum concentrates the signal energy into the leading principal components, reducing the tail bias term $H_{\theta^*,\lambda}(k)$ and thus lowering the ESD.
- **Core assumption:** Strong signals must be distinguishable from noise ($\eta_{i,j} \ge C_\eta \epsilon$), and the initialization $b_0$ must be properly scaled relative to the noise level.
- **Evidence anchors:** [abstract]: "...over-parameterized gradient flow can reduce the ESD." [section 6]: Theorem 6.2 proves that under specific conditions, $d^\dagger(t_2) \le d^\dagger(t_1)$, confirming the reduction of ESD during training.
- **Break condition:** If the noise is too high relative to the signal separation, or if initialization is poor, the gradient flow may fail to reorder eigenvalues correctly.

### Mechanism 3
- **Claim:** The "span profile" framework overcomes limitations of classical fixed-kernel theories (which rely on source conditions) by characterizing the hardness of a problem directly through the trade-off function $H_{\theta^*,\lambda}(k)$.
- **Mechanism:** Instead of assuming specific decay rates for eigenvalues (polynomial) or signal smoothness (source conditions), the span profile $D_{\theta^*,\lambda}(\tau)$ measures how quickly the average tail signal $H(k)$ decays relative to noise. This allows for sharp risk bounds even for learned or adapted kernels where classical assumptions may not hold.
- **Core assumption:** The spectrum is $(K,n)$-regular for the RKHS extension (Assumption 9.4), ensuring the problem is well-posed for the sample size.
- **Evidence anchors:** [section 4]: Theorem 4.3 provides lower bounds on minimax risk using the quota sequence $K_n$, bypassing classical source conditions. [section 5]: Demonstrates that a "ridge-guided" complexity measure fails to capture intrinsic difficulty (saturation effect), unlike the ESD.
- **Break condition:** If the kernel has finite rank $d < \infty$, a systematic bias is unavoidable, requiring a modification to the ESD definition (Remark 9.8) to account for the projection error.

## Foundational Learning

- **Concept: Spectral Decomposition in RKHS**
  - **Why needed here:** The paper relies entirely on the Mercer decomposition of kernels into eigenvalues $\lambda_j$ and eigenfunctions $\psi_j$. Understanding how estimators filter these components (Eq. 5-7) is prerequisite to understanding alignment.
  - **Quick check question:** How does the Ridge filter $\psi^R_\nu(\lambda) = \frac{\nu}{\lambda+\nu}$ differ from the Principal Component (PC) filter $\psi^{PC}_\nu(\lambda) = \mathbb{1}\{\lambda < \nu\}$ in terms of shrinking coefficients?

- **Concept: Bias-Variance Trade-off in Sequence Models**
  - **Why needed here:** The ESD is defined explicitly at the crossing point where the squared bias (tail sum) equals the estimation variance ($k\sigma^2$).
  - **Quick check question:** In a sequence model with noise variance $\sigma^2$, if you include $k$ components in your estimator, what happens to the bias and variance as $k$ increases?

- **Concept: Minimax Optimality**
  - **Why needed here:** The paper claims the ESD characterizes the *minimax* rate, meaning no estimator can perform significantly better than the ESD-bound over the worst-case signal in the defined class.
  - **Quick check question:** What does it mean for an estimator to be "minimax optimal" over a class of parameters $\mathcal{F}$?

## Architecture Onboarding

- **Component map:** Data $(y_i, x_i)$ or sequence observations $z_j$ -> Spectral Layer (Mercer decomposition) -> Eigenvalues $\lambda$, Eigenfunctions $\psi$ -> ESD Calculator (sorts $\lambda$, computes tail sums) -> Adaptation Module (OP-GF updates parameters) -> Estimator (PC using learned spectrum truncated at updated ESD)
- **Critical path:** The sorting/permutation $\pi$ of eigenvalues is critical. The definition of ESD assumes $\lambda_{\pi_1} > \lambda_{\pi_2} > \dots$. If the permutation is incorrect or unstable, the ESD calculation will be flawed.
- **Design tradeoffs:**
  - **Depth $D$:** Deeper models ($D > 0$) in OP-GF can achieve lower ESD values (better alignment) but may require more iterations to converge (Fig 2).
  - **Ridge vs. PC:** Ridge is more stable but suffers from "saturation" (Sec 5), failing to adapt to smooth signals; PC is optimal for the ESD framework but sensitive to tuning.
- **Failure signatures:**
  - **Saturation:** If using Ridge, the risk lower bound $d_\Delta$ remains high even if the signal is smooth, and the ESD/Ridge ratio diverges (Sec 5.5).
  - **Non-regular Spectrum:** If the spectrum decays too fast or is rank-deficient relative to $n$, Assumption 9.4 breaks, and the minimax bounds may not hold.
- **First 3 experiments:**
  1. **Verify ESD-Risk Coupling:** Generate synthetic sequence data with known $\theta^*$ and $\lambda$. Calculate the "Oracle" PC risk and compare it to the theoretical bound $\sigma^2 d^\dagger$ across varying noise levels $\sigma^2$. (Replicate Sec 7 / Fig 2 logic).
  2. **Alignment Dynamics:** Implement OP-GF (Eq 14) on a misaligned dataset ($q > 1$). Plot the Span Profile $D_{\theta^*,\tilde{\lambda}(t)}$ over training epochs. Confirm it shifts downwards (Fig 1).
  3. **Ridge vs. ESD:** Reproduce Example 5.7. Plot the ratio $d^\dagger / d_\Delta$ for a smooth signal. Verify that for Ridge, this ratio tends to 0 (indicating Ridge fails to generalize well relative to the intrinsic complexity), while PC tracks $d^\dagger$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Effective Span Dimension (ESD) framework be extended to analyze settings where the kernel's eigenfunctions evolve during training, rather than remaining fixed?
- **Basis in paper:** [explicit] The authors state in Section 6 that "Analyzing eigenfunction evolution technically more difficult than eigenvalues evolution and is left for future work."
- **Why unresolved:** The current theoretical analysis focuses on the tractable case of eigenvalue adaptation via Over-Parameterized Gradient Flow (OP-GF) while keeping the eigenbasis fixed.
- **What evidence would resolve it:** A theoretical derivation of ESD dynamics for algorithms that modify the kernel's basis, or bounds showing how eigenfunction alignment affects ESD reduction.

### Open Question 2
- **Question:** How does the depth of an over-parameterized model quantitatively affect the reduction of ESD and the resulting generalization performance?
- **Basis in paper:** [explicit] Section 7 notes that deeper models ($D=3$) achieved lower ESD values than shallow ones ($D=0$), but concludes that "a comprehensive study for general models is left for future research."
- **Why unresolved:** The paper provides empirical observations regarding depth but lacks a general theoretical explanation for why increased depth facilitates better spectral adaptation.
- **What evidence would resolve it:** Theoretical bounds linking the number of layers ($D$) to the rate of ESD decay, or proofs characterizing the representational benefits of depth in this specific spectral context.

### Open Question 3
- **Question:** Can the ESD be estimated accurately from finite samples to serve as a practical, data-dependent complexity measure for model selection or regularization?
- **Basis in paper:** [inferred] The paper repeatedly defines ESD as a "population complexity measure" dependent on the true signal $\theta^*$ and noise $\sigma^2$, noting it is "not a training input."
- **Why unresolved:** Using ESD as a theoretical tool requires oracle knowledge of the signal structure; applying it in practice requires solving the inverse problem of estimating this alignment from noisy, finite data.
- **What evidence would resolve it:** An algorithm to consistently estimate the span profile or ESD from a training set, along with convergence guarantees showing the estimate approaches the population ESD.

### Open Question 4
- **Question:** To what extent does the ESD framework explain the generalization gap between standard deep neural networks and their Neural Tangent Kernel (NTK) approximations?
- **Basis in paper:** [explicit] Section 11 states: "We expect to relate this framework to learned representations in neural networks to explain their superior generalization performance."
- **Why unresolved:** The paper validates the framework on sequence models and linear networks, but the connection to the complex, non-linear feature learning dynamics of full neural networks remains conceptual.
- **What evidence would resolve it:** Empirical or theoretical results demonstrating that standard neural training reduces ESD relative to the static NTK, correlating with the performance gap observed in practice.

## Limitations

- The framework assumes Mercer decomposition is well-defined and that kernel is learned through idealized continuous-time gradient flow dynamics
- Results rely heavily on assumptions about eigenvalue decay and signal separability from noise
- ESD measure requires careful interpretation in finite-sample settings where observations may be comparable to intrinsic dimensionality
- Practical implications of discretization and early stopping in over-parameterized gradient flow remain unclear

## Confidence

- **High confidence:** The theoretical framework connecting ESD to minimax rates (Mechanisms 1-3) is mathematically rigorous and well-supported by proofs in the paper
- **Medium confidence:** The claim that OP-GF provably reduces ESD (Mechanism 2) is supported by Theorem 6.2, but practical conditions may be difficult to verify in real applications
- **Medium confidence:** The span profile framework's advantage over classical source conditions (Mechanism 3) is demonstrated theoretically, but practical impact needs further empirical validation

## Next Checks

1. **Verify ESD computation:** Implement Definition 3.1 to calculate ESD for synthetic data with known signal-kernel misalignment. Compare the computed ESD against theoretical prediction and validate that PC estimators using ESD truncation achieve predicted $\sigma^2 d^\dagger$ risk.

2. **Test OP-GF dynamics:** Implement the over-parameterized gradient flow dynamics (Eq 14) with varying depths D and initializations. Track ESD evolution during training and verify the reduction predicted by Theorem 6.2. Test sensitivity to the separation condition $\eta_{i,j} \ge C_\eta \epsilon$.

3. **Compare to ridge saturation:** Reproduce Example 5.7 by computing $d^\dagger / d_\Delta$ ratios for smooth signals under ridge regularization. Verify the saturation effect where ridge complexity $d_\Delta$ fails to capture intrinsic signal complexity, while ESD correctly identifies problem difficulty.