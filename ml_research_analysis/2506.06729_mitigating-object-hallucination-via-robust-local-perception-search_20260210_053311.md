---
ver: rpa2
title: Mitigating Object Hallucination via Robust Local Perception Search
arxiv_id: '2506.06729'
source_url: https://arxiv.org/abs/2506.06729
tags:
- arxiv
- visual
- image
- hallucination
- mllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses object hallucination in multimodal large language
  models (MLLMs), where models generate text that does not align with visual content.
  The proposed Local Perception Search (LPS) method leverages the model's inherent
  local visual attention capabilities to generate priors that guide decoding, mitigating
  hallucinations without external models or additional training.
---

# Mitigating Object Hallucination via Robust Local Perception Search

## Quick Facts
- arXiv ID: 2506.06729
- Source URL: https://arxiv.org/abs/2506.06729
- Reference count: 14
- Primary result: Reduces object hallucination rates by over 1% on Qwen 2.5 VL and demonstrates robustness under adversarial attacks

## Executive Summary
This paper addresses object hallucination in multimodal large language models (MLLMs), where models generate text that does not align with visual content. The proposed Local Perception Search (LPS) method leverages the model's inherent local visual attention capabilities to generate priors that guide decoding, mitigating hallucinations without external models or additional training. LPS achieves significant improvements over baselines on established hallucination benchmarks and demonstrates robustness under adversarial conditions.

## Method Summary
LPS is a plug-and-play decoding strategy that enhances MLLM text generation by utilizing the model's own local visual attention mechanisms. Rather than relying on external models or fine-tuning, LPS generates visual priors from the MLLM's internal attention patterns and uses these to guide the decoding process. This approach specifically targets object-level hallucinations by ensuring generated text remains aligned with actual visual content. The method is designed to be compatible with various MLLM architectures and can be applied without architectural modifications.

## Key Results
- LPS reduces object hallucination rates by over 1% on Qwen 2.5 VL using POPE and CHAIR benchmarks
- Demonstrates nearly 3% improvement on Phi 3.5 Vision under severe adversarial perturbations
- Achieves robust performance without requiring external models or additional training

## Why This Works (Mechanism)
LPS exploits the inherent local visual attention capabilities of MLLMs to generate visual priors that guide text generation. By leveraging the model's own attention mechanisms rather than external components, LPS maintains coherence between generated text and visual content. The method effectively constrains the decoding process to stay grounded in actual visual information, preventing the model from generating text that references non-existent objects or details.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: AI systems that process both visual and textual inputs to generate coherent responses. Why needed: LPS builds directly on MLLM architectures to enhance their hallucination resistance. Quick check: Understanding how MLLMs fuse visual and textual representations.

**Visual Attention Mechanisms**: The process by which MLLMs focus on specific regions of visual input during processing. Why needed: LPS relies on these mechanisms to generate visual priors. Quick check: Familiarity with self-attention and cross-attention in vision-language models.

**Object Hallucination**: Generation of text describing objects or details not present in the visual input. Why needed: This is the primary problem LPS addresses. Quick check: Understanding hallucination types and detection benchmarks like POPE and CHAIR.

**Plug-and-Play Methods**: Techniques that can be applied to existing models without architectural modifications or retraining. Why needed: LPS claims this property for easy deployment. Quick check: Experience with decoding strategies that modify generation without changing model weights.

## Architecture Onboarding

**Component Map**: Input Image -> MLLM Encoder -> Visual Attention Maps -> LPS Module -> Guided Decoding -> Output Text

**Critical Path**: Visual input flows through the MLLM encoder, where LPS intercepts the attention maps to generate priors that guide the decoding process. The critical path is during text generation, where LPS provides real-time guidance.

**Design Tradeoffs**: LPS trades minimal additional computation during decoding for significant hallucination reduction. The method prioritizes compatibility and ease of deployment over maximum performance gains, avoiding the need for model retraining.

**Failure Signatures**: LPS may be less effective when MLLM attention mechanisms are unreliable or when visual content is ambiguous. Performance could degrade if the attention maps do not accurately reflect object boundaries or if the model's visual understanding is fundamentally flawed.

**First Experiments**:
1. Apply LPS to a simple image captioning task and compare hallucination rates with baseline decoding
2. Test LPS across different decoding temperatures to assess temperature sensitivity
3. Evaluate LPS performance on varied image types (natural scenes, synthetic images, text-heavy images) to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to object-level hallucinations, with untested effectiveness on attribute or relational hallucinations
- Claims of plug-and-play compatibility not validated across diverse MLLM architectures beyond two specific models
- Computational overhead during inference not quantified, limiting practical deployment assessment

## Confidence

**Medium confidence**: Core claim that LPS reduces object hallucination rates on tested benchmarks
**Medium confidence**: Claim of robustness under adversarial attacks with limited attack methodology details
**Low confidence**: Plug-and-play and architecture-agnostic claims due to limited cross-model testing

## Next Checks
1. Evaluate LPS performance on additional hallucination types beyond object-level errors, including attribute hallucinations, relational inconsistencies, and context-level hallucinations using established benchmarks like HINTS or SEED-Bench.

2. Test LPS across a broader range of MLLM architectures (including GPT-4V, Gemini, LLaVA, and other open-weight models) to validate the claimed architecture-agnostic nature and scalability across different model families and sizes.

3. Characterize the computational overhead and latency impact of LPS during inference across different hardware configurations, measuring token generation time, memory usage, and throughput compared to baseline decoding approaches.