---
ver: rpa2
title: Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition
arxiv_id: '2601.08094'
source_url: https://arxiv.org/abs/2601.08094
tags:
- global
- features
- local
- emotion
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a local-global feature fusion framework for
  subject-independent EEG emotion recognition. The approach integrates channel-wise
  EEG features (differential entropy + graph-theoretic connectivity) with trial-level
  global descriptors (time-domain, spectral, and multifractal features) within a dual-branch
  transformer architecture.
---

# Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition

## Quick Facts
- arXiv ID: 2601.08094
- Source URL: https://arxiv.org/abs/2601.08094
- Reference count: 26
- Mean accuracy of ~40% for 7-class EEG emotion recognition under LOSO protocol

## Executive Summary
This paper introduces a dual-branch transformer architecture that fuses local channel-wise EEG features with global trial-level descriptors for subject-independent emotion recognition. The approach combines differential entropy and graph-theoretic connectivity features with aggregated time, spectral, and multifractal characteristics, achieving approximately 40% accuracy on the 7-class SEED-VII dataset. The method demonstrates that integrating fine-grained neural dynamics with global coordination patterns improves cross-subject generalization compared to single-view baselines.

## Method Summary
The framework extracts local features per channel (differential entropy in 5 frequency bands plus 4 graph-theoretic measures from functional connectivity) and global features at the trial level (time-domain, spectral, and multifractal descriptors). These are projected into shared embeddings and fused within a dual-branch MAET transformer architecture using attention mechanisms. Domain-adversarial regularization via gradient reversal encourages subject-invariant representations. The model is evaluated under leave-one-subject-out protocol on SEED-VII with 20 subjects and 7 emotion classes.

## Key Results
- Achieves approximately 40% mean accuracy and 38.7% macro-F1 for 7-class emotion recognition on SEED-VII
- Dual-branch fusion outperforms single-view baselines (local-only: 34.6%, global-only: 36.8%)
- MAET transformer architecture outperforms classical ML approaches (SVM/RF: 21-25% accuracy)
- Ablation shows spectral and multifractal global features contribute more than time-domain features

## Why This Works (Mechanism)

### Mechanism 1
Fusing channel-wise local features with trial-level global descriptors improves subject-independent emotion recognition over either view alone. Local features capture fine-grained electrode activity but are sensitive to inter-subject variability, while global features smooth over channel-specific noise and encode coordination patterns. Their combination via attention enables the model to leverage complementary information scales. Break condition: If emotion signatures are predominantly local or predominantly global for a given dataset/task, fusion may add noise without benefit.

### Mechanism 2
Graph-theoretic connectivity descriptors encode inter-channel coordination patterns that improve cross-subject robustness. Correlation-based functional connectivity matrices are computed per trial, and four node-level measures (weighted degree, clustering coefficient, betweenness centrality, PageRank) quantify each channel's role in the network. These capture network-level coordination that is less sensitive to absolute signal amplitude variations across subjects. Break condition: If connectivity patterns themselves exhibit high inter-subject variability or are dominated by noise/artifact correlations.

### Mechanism 3
Domain-adversarial regularization reduces subject-specific distribution shifts during training. A gradient reversal layer is applied to shared representations, encouraging the model to learn features that cannot predict subject identity. The training objective combines classification loss with domain loss. Break condition: If subject-invariant features sacrifice too much discriminative emotion information, accuracy may drop despite improved generalization.

## Foundational Learning

- **Concept: Differential Entropy (DE) for EEG spectral features**
  - Why needed here: DE is the primary local feature, computed per channel per frequency band. Understanding DE = log₂(100 × PSD) helps interpret the 62×5 local feature tensor.
  - Quick check question: Given a PSD value of 1.0, what would the DE value be approximately? (Answer: log₂(100) ≈ 6.64)

- **Concept: Transformer self-attention for multi-modal fusion**
  - Why needed here: The MAET backbone uses self-attention to integrate local (558-dim) and global (25-dim) feature tokens, enabling adaptive weighting across feature types.
  - Quick check question: In the dual-branch setup, where does feature fusion actually occur—before or within the transformer blocks?

- **Concept: Leave-One-Subject-Out (LOSO) evaluation**
  - Why needed here: LOSO is the evaluation protocol; each fold uses 19 subjects for training, 1 for testing. This ensures reported metrics reflect true cross-subject generalization.
  - Quick check question: Why is subject-wise normalization computed on training subjects only, not on test subjects?

## Architecture Onboarding

- **Component map:**
  - Input: Preprocessed EEG → Local features (62 channels × 9 features = 558-dim) + Global features (25-dim trial-level)
  - Embedding: Two modality-specific projection layers (f_mv^eeg, f_mv^aux)
  - Backbone: MAET dual-branch transformer with L stacked blocks
  - Heads: Emotion classifier (7-class softmax) + Domain classifier (via GRL for adversarial training)

- **Critical path:**
  1. Feature extraction (DE + graph + time/spectral/multifractal) is the most computationally intensive preprocessing step
  2. Subject-wise normalization must use training statistics only (fallback to average if needed)
  3. Dual embeddings must share dimensionality for concatenation
  4. Classification token (h_cls) feeds emotion head

- **Design tradeoffs:**
  - Early fusion (concatenate all 583 features into single branch) achieves 38.5% vs. dual-branch achieving 40.1% — representation-level integration matters
  - Global spectral features alone (38.6%) outperform time-domain (36.8%) or fractal (37.2%) alone — spectral captures most emotion-relevant global information
  - Classical ML baselines (SVM, RF) achieve only 21–25% even with fused features — transformer backbone is critical

- **Failure signatures:**
  - Low accuracy on disgust/sadness (see confusion matrix): these emotions have overlapping neural patterns
  - High subject-to-subject variance (>10% accuracy spread): inter-subject variability not fully mitigated
  - Macro-F1 substantially lower than accuracy with single-view baselines: indicates class imbalance issues

- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run MAET with DE-only (310-dim) features to verify ~36.4% accuracy, then add graph + global features to confirm improvement to ~40%
  2. **Ablate global feature groups:** Test local + spectral-only, local + multifractal-only, local + time-only to validate that spectral and multifractal contribute most
  3. **Domain-adversarial ablation:** Train with and without GRL (λ=0 vs. λ>0) on a single LOSO fold to quantify regularization impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would graph-aware or spatiotemporal transformer architectures operating directly on structured EEG representations improve upon the flattened-vector approach used here?
- Basis in paper: The authors note that the "MAET backbone operates on flattened feature vectors without explicitly preserving spatial relationships among EEG channels," and suggest incorporating graph-aware architectures.
- Why unresolved: The current method flattens 62-channel data into a 558-dim vector, potentially discarding topological relationships that structured architectures could exploit.
- What evidence would resolve it: Comparative experiments where a graph-transformer replaces the MAET backbone on the same SEED-VII features, showing significant accuracy gains.

### Open Question 2
- Question: Can lightweight subject-adaptive calibration or more advanced domain-invariant learning techniques further reduce the persistent performance gap in subject-independent settings?
- Basis in paper: The discussion states that "learning fully subject-invariant representations from EEG signals remains an open problem" and suggests exploring "lightweight subject-adaptive calibration mechanisms" to address the remaining variability.
- Why unresolved: Despite domain-adversarial training, the results show noticeable inter-subject variability, with some subjects performing significantly worse than others.
- What evidence would resolve it: A study showing that adding a calibration phase or advanced domain generalization (e.g., contrastive learning on source domains) significantly reduces the variance in subject-wise accuracy.

### Open Question 3
- Question: Does extending the framework to model continuous affective dimensions or integrating multimodal data (e.g., eye tracking) enhance robustness in real-world scenarios?
- Basis in paper: The authors explicitly propose future work involving "jointly modeling discrete emotion categories and continuous affective dimensions" and "integrating additional modalities."
- Why unresolved: The current study is limited to discrete 7-class classification on a single dataset, which may obscure latent affective gradients or fail to leverage complementary physiological signals available in the SEED-VII dataset.
- What evidence would resolve it: Results from a multi-task learning setup that simultaneously predicts valence/arousal and emotion categories, or a multimodal fusion experiment, outperforming the unimodal EEG baseline.

## Limitations
- Several critical implementation details are underspecified, including MAET transformer hyperparameters and multifractal analysis parameters
- SEED-VII dataset contains relatively homogeneous Chinese participants, limiting generalization to more diverse populations
- The reported 40% accuracy, while outperforming baselines, still represents a challenging recognition task with 7 emotion classes

## Confidence

**High Confidence**: The general framework of combining local (channel-wise) and global (trial-level) features is well-established in the literature, supported by both the proposed method and the neighboring "Learning from Brain Topography" paper. The use of domain-adversarial regularization to improve cross-subject generalization is a recognized technique with theoretical justification.

**Medium Confidence**: The specific architectural choices (dual-branch MAET, attention-based fusion, the exact feature composition) are described but not fully detailed. The reported performance improvements over baselines are consistent across different experimental conditions, but absolute values depend on unspecified hyperparameters and implementation details.

**Low Confidence**: The comparative claims against classical ML baselines (SVM, RF) at 21-25% accuracy may be implementation-dependent, as these results can vary significantly with feature scaling, kernel selection, and hyperparameter tuning.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the MAET transformer architecture (L=1,2,3 layers; hidden_dim=128,256,512; attention_heads=4,8,16) and training parameters (learning_rate=1e-3,1e-4; batch_size=16,32; λ=0.1,0.5,1.0) to identify optimal configurations and measure performance stability across the hyperparameter space.

2. **Cross-Dataset Generalization Test**: Evaluate the trained model on an independent EEG emotion dataset (e.g., DEAP or DREAMER) using the same 7-class mapping to assess whether the local-global fusion approach generalizes beyond SEED-VII, particularly testing the robustness of the domain-adversarial regularization across different recording conditions and participant demographics.

3. **Statistical Significance Testing**: Conduct paired t-tests or Wilcoxon signed-rank tests comparing the MAET dual-branch approach against single-view baselines and classical ML methods across all LOSO folds to confirm that the reported performance improvements are statistically significant rather than due to random variation in the training process.