---
ver: rpa2
title: Automating Structural Engineering Workflows with Large Language Model Agents
arxiv_id: '2510.11004'
source_url: https://arxiv.org/abs/2510.11004
tags:
- structural
- engineering
- system
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MASSE automates end-to-end structural engineering tasks using\
  \ a multi-agent LLM framework with specialized Analyst, Engineer, and Management\
  \ teams. Each agent handles a distinct workflow stage\u2014data extraction, analysis,\
  \ design, and safety verification\u2014communicating via structured JSON to ensure\
  \ clarity and traceability."
---

# Automating Structural Engineering Workflows with Large Language Model Agents

## Quick Facts
- **arXiv ID:** 2510.11004
- **Source URL:** https://arxiv.org/abs/2510.11004
- **Reference count:** 40
- **Primary result:** Multi-agent LLM framework reduces structural engineering workload from ~2 hours to ~2 minutes with >85% accuracy on real-world racking systems

## Executive Summary
MASSE is a training-free, multi-agent LLM framework that automates end-to-end structural engineering workflows for racking system design. The system decomposes complex tasks into specialized teams—Analysts extract data, Engineers perform calculations, and Managers verify safety—communicating via structured JSON to ensure clarity and traceability. Evaluated on 100 real-world racking system cases, MASSE achieved accuracy scores above 85% across multiple benchmarks while reducing expert workload from approximately 2 hours to 2 minutes per case.

## Method Summary
The framework uses AutoGen to orchestrate three specialized teams: Analyst Team (Loading, Seismic, Dynamic, Structural analysts) extracts parameters from natural language inputs; Engineer Team (Design, Model, Verification engineers) runs calculations using OpenSeesPy for FEM analysis; Management Team (Project, Safety managers) coordinates workflow and final verification. The system employs structured JSON communication protocols, a centralized Structural Memory Manager for persistent state, and RAG with FAISS for building code retrieval. GPT-4o, Claude 3.5 Sonnet, and o4-mini models operate at temperature 0 (1 for o4-mini) without fine-tuning.

## Key Results
- Achieved SAAB score of 96.6, SDAB of 91.4, LAB of 93.8, and MASEB of 94.7 across multiple benchmarks
- Reduced expert workload from ~2 hours to ~2 minutes per racking system case
- Reasoning models (o4-mini) achieved the most consistent performance across all evaluation metrics
- Structured communication protocol and memory management were critical for success

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing complex engineering workflows into specialized, hierarchical roles reduces error propagation compared to monolithic single-agent approaches
- **Mechanism:** The system separates concerns: Analysts extract data, Engineers run calculations, and Managers verify safety. This isolates failures and allows localized retries
- **Core assumption:** LLMs can maintain strict role adherence and distinct mental models for subtasks better than they can manage a single, long-context end-to-end reasoning chain
- **Evidence anchors:** Multi-agent framework shows success where single-agent approaches fail (30% logic errors, 50% dependency errors); ablation studies confirm decomposition benefits
- **Break condition:** Underspecified inter-agent dependencies can cause conversation loops without resolution

### Mechanism 2
- **Claim:** Enforcing structured communication (JSON) over verbose natural language stabilizes tool-mediated interactions
- **Mechanism:** Agents output strictly formatted JSON schemas for inputs/outputs, reducing ambiguity that causes LLMs to hallucinate function parameters
- **Core assumption:** LLM instruction-following capability is sufficient to consistently generate valid syntax for required structured artifacts
- **Evidence anchors:** Structured protocols prevent context window overflow; ablation study shows JSON format component significantly boosts performance
- **Break condition:** Deeply nested or complex JSON schemas may cause LLM generation failures

### Mechanism 3
- **Claim:** A centralized, persistent memory store enables long-horizon coherence by decoupling data storage from agent conversations
- **Mechanism:** Structural Memory Manager holds intermediate state (seismic parameters, load calculations), allowing downstream agents to use exact upstream values
- **Core assumption:** Memory state remains uncorrupted and agents reliably query it rather than relying on internal context
- **Evidence anchors:** Memory management is core integration; ablation study shows adding Memory improves LAB from 81.6 to 89.6
- **Break condition:** Early agent errors written to memory propagate deterministically to all subsequent agents

## Foundational Learning

- **Concept: Finite Element Analysis (FEA) Basics (Nodes/Elements/Materials)**
  - **Why needed here:** To understand what the Engineer Team is actually simulating—why geometry, supports, and loads are distinct inputs for valid structural models
  - **Quick check question:** Can you distinguish between defining a node (a point in space) and an element (the connection between points with material properties)?

- **Concept: JSON Schema & Structured Data**
  - **Why needed here:** The system relies on agents outputting valid JSON to pass data—you need to read schemas to debug why an agent failed to pass data
  - **Quick check question:** Given a text description of a beam, can you manually construct the JSON object defining its ID, node connections, and section properties?

- **Concept: LLM Context Window vs. Persistent State**
  - **Why needed here:** To understand why "Memory" is a separate component—differentiate between what the LLM remembers in chat history vs. what is stored externally
  - **Quick check question:** If an agent restarts, does it remember seismic parameters from the previous turn if they are not in immediate chat history but are in memory?

## Architecture Onboarding

- **Component map:**
  - User Input -> Project Manager (Decomposition) -> Seismic/Loading Analysts (Data Extraction) -> Structural Memory -> Structural Analyst (Model Generation) -> Model Engineer (OpenSees Execution) -> Safety Manager (Verification)

- **Critical path:** User Input → Project Manager → Seismic/Loading Analysts → Structural Memory → Structural Analyst → Model Engineer → Safety Manager

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** o4-mini achieves top scores but with highest token cost/runtime; GPT-3.5 is cheap but inaccurate
  - **Rounds vs. Latency:** Increasing communication rounds improves quality (40→90 score) but linearly increases runtime (20s→70s)

- **Failure signatures:**
  - **Single-Agent Logic Failure:** 30% logic errors, 50% dependency errors in Figure 2—agent loses track of variable dependencies in long chains
  - **Formatting Collapse:** Without JSON constraint, agents wrap code in markdown or conversational filler, breaking tool parsers

- **First 3 experiments:**
  1. **Replicate Single-Agent Failure:** Run a standard structural prompt on base GPT-4o without multi-agent framework to observe dependency/formatting errors shown in Figure 2
  2. **Ablation on Structure:** Disable JSON enforcement (set mode to natural language) and measure drop in SAAB scores to quantify structured communication value
  3. **Runtime Scaling:** Increase racking system complexity (add more bays) and plot "Agent to Agent Iterations" needed to maintain high MASEB score to find latency ceiling

## Open Questions the Paper Calls Out

- **Question:** Can MASSE generalize to other knowledge-intensive domains beyond structural engineering without domain-specific modifications?
  - **Basis in paper:** Explicit hypothesis that agent-driven workflows can generalize to domains where tasks are verbalizable, procedural, and tool-mediated
  - **Why unresolved:** Validation limited to racking system design; no experiments in other domains presented
  - **What evidence would resolve it:** Successful deployment and evaluation in at least one other professional domain (e.g., architecture, finance) using same framework

- **Question:** How can MASSE be extended to support adaptive self-improvement and real-time feedback integration?
  - **Basis in paper:** Explicit mention of future work emphasizing real-time feedback for adaptive self-improvement
  - **Why unresolved:** Current system operates without learning from execution outcomes or adapting agent behaviors over time
  - **What evidence would resolve it:** Modified MASSE version demonstrating improved performance or efficiency over successive iterations on similar problems

- **Question:** How does MASSE performance scale to more complex structural engineering tasks beyond racking systems?
  - **Basis in paper:** Evaluation limited to 100 racking system cases representing standardized geometries and workflows
  - **What evidence would resolve it:** Evaluation on diverse structural types (multi-story buildings, bridges) with varied complexity, comparing performance to expert benchmarks

## Limitations

- **Schema dependence:** Performance heavily relies on quality of JSON schemas and agents' ability to strictly follow them; edge cases with schema violations are not fully addressed
- **Jurisdictional specificity:** Evaluation focuses on racking systems in BC, Canada, raising questions about generalizability to other domains with different regulatory requirements
- **Cost accounting:** Economic claims (2 minutes vs 2 hours) only measure runtime, not total operational costs including model API fees and computational resources

## Confidence

- **High confidence:** Multi-agent decomposition strategy and structured JSON communication demonstrably improve accuracy over single-agent approaches (controlled comparison in Figure 2, ablation studies in Table 2)
- **Medium confidence:** System generalizes to other engineering domains beyond racking systems (inferred from methodology but not empirically validated on other domains)
- **Low confidence:** Cost-effectiveness claim accounts for all operational costs (only runtime is measured, not total cost)

## Next Checks

1. Test schema robustness by introducing malformed JSON inputs and measuring error recovery rates
2. Evaluate on structural engineering problems from different jurisdictions to assess domain transferability
3. Benchmark total operational costs (API fees + compute) against human expert time to validate economic claims