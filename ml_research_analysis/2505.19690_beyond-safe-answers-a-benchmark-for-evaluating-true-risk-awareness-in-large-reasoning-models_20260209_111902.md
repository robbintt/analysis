---
ver: rpa2
title: 'Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large
  Reasoning Models'
arxiv_id: '2505.19690'
source_url: https://arxiv.org/abs/2505.19690
tags:
- reasoning
- risk
- safety
- safe
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Beyond Safe Answers (BSA) benchmark to
  evaluate true risk awareness in Large Reasoning Models (LRMs). BSA reveals the Superficial
  Safety Alignment (SSA) phenomenon, where models produce seemingly safe outputs but
  fail to genuinely detect and mitigate underlying risks during reasoning.
---

# Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models

## Quick Facts
- **arXiv ID:** 2505.19690
- **Source URL:** https://arxiv.org/abs/2505.19690
- **Reference count:** 40
- **Key outcome:** Introduces BSA benchmark revealing Superficial Safety Alignment in LRMs, where models produce safe outputs without genuine risk detection; top models achieve only 38.0% accuracy in identifying risk rationales.

## Executive Summary
This paper introduces the Beyond Safe Answers (BSA) benchmark to evaluate true risk awareness in Large Reasoning Models (LRMs). BSA reveals the Superficial Safety Alignment (SSA) phenomenon, where models produce seemingly safe outputs but fail to genuinely detect and mitigate underlying risks during reasoning. Evaluations across 19 state-of-the-art LRMs on 2,000 instances spanning nine risk categories show that top-performing models achieve only 38.0% accuracy in correctly identifying risk rationales. The study finds a strong correlation between reasoning accuracy and response safety, highlighting that effective risk detection during reasoning is crucial for producing safe outputs. Additionally, the paper explores mitigation strategies including safety rules, fine-tuning on safety reasoning data, and decoding strategies, with safety rules showing the most consistent improvements.

## Method Summary
The BSA benchmark evaluates LRMs on 2,000 challenging instances across 9 risk categories and 3 SSA scenarios. The evaluation uses an LLM-as-a-Judge (GPT-4o) to score Query-Thinking-Response (QTR) triples on Response Safety and Reasoning Correctness, computing Safe@1/k and Think@1/k metrics. Fine-tuning experiments use STAR-1 dataset with DeepSpeed ZeRO-2 on 16Ã—A100 GPUs, 5 epochs, batch size 128, lr=1e-5, and AdamW optimizer. Sampling parameters are specified per model (k=5 per query with model-specific temperature, top-p, and top-k values). Safety rules are implemented as system prompts following the structure in Appendix F.3.

## Key Results
- Top-performing LRMs achieve only 38.0% accuracy in correctly identifying risk rationales on BSA benchmark
- Strong correlation exists between reasoning accuracy and response safety, indicating reasoning correctness drives safety
- Safety rules mitigation shows statistically significant improvements in both response safety and reasoning accuracy, particularly effective for Risk Omission scenarios
- Fine-tuning on safety reasoning data improves reasoning but simultaneously increases over-sensitivity (safety alignment tax)

## Why This Works (Mechanism)

### Mechanism 1: Superficial Safety Alignment (SSA) from Pattern Memorization
LRMs can produce safe final responses without genuinely identifying or reasoning about underlying risks, due to memorization of safety-related patterns from alignment data. During safety alignment training, models may rote-memorize surface characteristics of risky queries and learn to reject them without developing an internal model of risk concepts.

### Mechanism 2: Reasoning Accuracy-Safety Correlation
Accurate identification of risks in the reasoning chain causally improves the consistency and reliability of safe outputs. When LRMs correctly identify and articulate risk rationales during their Chain-of-Thought process, they anchor their final response in a structured safety analysis, reducing reliance on heuristics.

### Mechanism 3: Safety Rules as External Reasoning Scaffolds
Integrating explicit safety rules into prompts mitigates SSA by forcing systematic, rule-based risk analysis during inference. Safety rules act as a cognitive scaffold, directing the model's attention to conduct a step-by-step risk assessment before response generation, compensating for incomplete internal risk models.

## Foundational Learning

**Concept: Chain-of-Thought (CoT) Reasoning in LRMs**
*Why needed:* BSA evaluates the safety of the reasoning trace itself, not just the final answer. Understanding that LRMs generate intermediate reasoning steps is essential to grasp what SSA is and why it matters.
*Quick check:* Can you explain why a model might produce a safe final answer while its CoT trace contains flawed or dangerous reasoning?

**Concept: Safety Alignment Tax & Over-Sensitivity**
*Why needed:* Mitigation strategies like fine-tuning or safety rules can inadvertently cause models to refuse benign requests (over-sensitivity), representing a trade-off known as the "safety alignment tax."
*Quick check:* In the paper's fine-tuning experiments, which SSA scenario worsened as reasoning improved, illustrating this tax?

**Concept: Evaluation Metrics (Safe@k vs. Think@k)**
*Why needed:* The paper introduces specific metrics to quantify SSA. `Safe@k` measures response safety across multiple samples, while `Think@k` measures consistent correctness of risk reasoning. The gap between `@1` and `@k` metrics is used to detect latent vulnerability.
*Quick check:* What does a high `Safe@1` but low `Safe@k` suggest about a model's safety profile?

## Architecture Onboarding

**Component Map:** BSA Benchmark (2,000 instances, 9 categories, 3 SSA scenarios) -> Evaluation Pipeline (LLM-as-Judge, QTR scoring) -> Mitigation Modules (Safety Rules, Fine-tuning, Decoding Adjustments)

**Critical Path:**
1. **Detection:** Run target LRM on BSA bench subset. Compute `Safe@1`, `Think@1`, and gap to establish SSA profile.
2. **Diagnosis:** Examine per-category performance (RO@1, CS@1, OS@1) to identify dominant SSA failure mode.
3. **Intervention:** Apply most suitable mitigation based on diagnosis, then re-evaluate.

**Design Tradeoffs:**
- Safety vs. Utility: Aggressive safety rules reduce SSA but increase over-sensitivity (refusing benign queries)
- Evaluation Cost: Human evaluation is gold-standard but costly; LLM-as-Judge is efficient but has ~6% error rate
- Model Scale: Larger models show better SSA resistance but at higher computational cost with diminishing returns from fine-tuning

**Failure Signatures:**
1. **High Safe@1, Low Think@1:** Classic SSA signature indicating superficial safety
2. **High Variance in Safe@k:** Indicates inconsistent safety, a byproduct of superficial alignment
3. **Disproportionate Failure on "Cognitive Shortcut":** Suggests model defaults to identifying only most obvious risk in multi-risk queries

**First 3 Experiments:**
1. **Baseline SSA Profiling:** Run target LRM on BSA subset (100 samples per scenario). Compute `Safe@1`, `Think@1`, and gap to establish profile.
2. **Safety Rule Ablation:** Implement safety rules as system prompt. Re-evaluate on same subset. Quantify Think@1 improvement and over-sensitivity increase (refusal rate).
3. **Per-Category Error Analysis:** Manually review 10-20 failure cases from worst-performing SSA category. Code failure reasons to inform targeted mitigation.

## Open Questions the Paper Calls Out

**Open Question 1:** How does SSA manifest and evolve during complex, multi-turn conversational interactions?
*Basis:* Appendix B notes current experiments are single-turn; extending to multi-turn interactions is necessary but uncertain whether SSA intensifies in continuous exchanges.

**Open Question 2:** Can safety reasoning be improved without simultaneously increasing the "Safety Alignment Tax" (over-sensitivity to benign inputs)?
*Basis:* Sections 5.4-5.5 show safety rules and fine-tuning mitigate SSA but increase over-sensitivity, presenting an unresolved trade-off.

**Open Question 3:** To what extent does SSA exist in rare, long-tail security scenarios not covered by current nine risk categories?
*Basis:* Appendix A-B note BSA cannot encompass all security risks; generalizability to edge cases where internal reasoning fidelity fails differently remains untested.

## Limitations
- SSA evaluation relies on correlational evidence between reasoning accuracy and safety outputs, lacking direct causal proof
- LLM-as-a-Judge (GPT-4o) introduces potential noise with reported 6% error rate in safety assessments
- STAR-1 fine-tuning data is not fully characterized, making it difficult to assess whether improvements generalize beyond training distribution

## Confidence

**High Confidence:** Existence of SSA as measurable phenomenon (large gaps between Safe@1 and Think@1 scores across multiple LRMs); benchmark construction and methodology are well-specified and reproducible.

**Medium Confidence:** Effectiveness of safety rules mitigation (statistical significance but noted side effects of increased over-sensitivity); correlation between reasoning accuracy and safety outputs (consistently observed but not causally proven).

**Low Confidence:** Memorization hypothesis for SSA emergence (lacks direct empirical validation); claim that safety rules are "most consistent" improvements (based on limited comparisons, may not generalize to all risk categories).

## Next Checks

1. **Causal Validation of Reasoning-Safety Link:** Design intervention study where models are fine-tuned specifically to improve reasoning accuracy on BSA instances, then measure whether safety improvements follow proportionally.

2. **Human Evaluation of LLM-as-Judge Reliability:** Conduct small-scale human evaluation (n=100) comparing GPT-4o judgments against human annotators on BSA safety assessments to quantify actual error rate and bias patterns.

3. **Cross-Domain Generalization Test:** Apply safety rules mitigation to LRMs on completely different safety benchmark (e.g., AdvBench or ToxiGen) not overlapping with BSA's construction to test generalizability.