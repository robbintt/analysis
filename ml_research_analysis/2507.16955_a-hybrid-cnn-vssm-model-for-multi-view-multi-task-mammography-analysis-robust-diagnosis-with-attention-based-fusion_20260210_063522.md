---
ver: rpa2
title: 'A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust
  Diagnosis with Attention-Based Fusion'
arxiv_id: '2507.16955'
source_url: https://arxiv.org/abs/2507.16955
tags:
- bi-rads
- hybrid
- learning
- multi-task
- vssm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early and accurate breast
  cancer detection from mammography images, which remains difficult due to subtle
  imaging findings and high inter-observer variability. The proposed method introduces
  a hybrid CNN-VSSM architecture that combines convolutional neural networks for local
  feature extraction with Visual State Space Models (VSSMs) for global context modeling,
  integrated within a multi-view, multi-task framework that processes all four standard
  mammography views and jointly predicts diagnostic labels and BI-RADS scores.
---

# A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion

## Quick Facts
- **arXiv ID:** 2507.16955
- **Source URL:** https://arxiv.org/abs/2507.16955
- **Reference count:** 35
- **Primary result:** Hybrid CNN-VSSM architecture achieves AUC of 0.9967 and F1 of 0.9830 on binary BI-RADS classification task

## Executive Summary
This study addresses the challenge of early and accurate breast cancer detection from mammography images, which remains difficult due to subtle imaging findings and high inter-observer variability. The proposed method introduces a hybrid CNN-VSSM architecture that combines convolutional neural networks for local feature extraction with Visual State Space Models (VSSMs) for global context modeling, integrated within a multi-view, multi-task framework that processes all four standard mammography views and jointly predicts diagnostic labels and BI-RADS scores. A gated attention-based fusion module dynamically weights information across views to handle missing data and improve robustness. The model is evaluated on the TOMPEI-CMMD dataset across three diagnostic tasks of increasing complexity.

## Method Summary
The proposed approach combines CNN and VSSM architectures within a multi-view, multi-task framework. Each of the four standard mammography views (CC left/right, MLO left/right) is processed by a shared CNN-VSSM backbone consisting of CNN layers for local feature extraction followed by VSSM layers for global context modeling. A gated attention fusion module dynamically weights and combines features from all available views before passing them to task-specific heads for BI-RADS classification and diagnostic prediction. The model is trained jointly on three tasks of increasing complexity: binary classification (BI-RADS 1 vs 5), ternary classification (BI-RADS 1 vs 3 vs 5), and five-class classification (all BI-RADS categories).

## Key Results
- Binary BI-RADS 1 vs. 5 classification: AUC of 0.9967 and F1 score of 0.9830
- Ternary classification (BI-RADS 1 vs. 3 vs. 5): F1 score of 0.7790
- Five-class BI-RADS classification: Best F1 score of 0.4904

## Why This Works (Mechanism)
The hybrid architecture leverages CNNs for precise local feature extraction from individual mammography views while VSSMs capture long-range dependencies and global context across the entire image. The multi-view framework processes all four standard views simultaneously, with the attention-based fusion module dynamically weighting information from each view based on its relevance and handling cases where some views may be missing. The multi-task learning approach enables knowledge transfer between related diagnostic objectives, improving overall performance.

## Foundational Learning
- **Convolutional Neural Networks (CNNs):** Essential for extracting local spatial features from mammography images, capturing fine-grained details like microcalcifications and architectural distortions that are critical for cancer detection
- **Visual State Space Models (VSSMs):** Provide global context modeling capabilities that complement CNNs, capturing long-range dependencies and temporal-like patterns across the image that may be missed by local feature extraction alone
- **Multi-View Processing:** Critical in mammography as each view provides complementary information; processing all four views (CC/MLO, left/right) captures complete anatomical context and improves diagnostic accuracy
- **Attention-Based Fusion:** Enables dynamic weighting of information from different views based on their relevance and handles missing data scenarios that commonly occur in clinical practice
- **Multi-Task Learning:** Allows simultaneous optimization for multiple related objectives (BI-RADS scoring and diagnosis), promoting knowledge sharing and improving generalization across different classification tasks

## Architecture Onboarding

Component Map:
CNN Backbone -> VSSM Layer -> Gated Attention Fusion -> Task-Specific Heads

Critical Path:
Each mammography view → CNN feature extraction → VSSM global context modeling → Gated attention fusion → Joint classification heads

Design Tradeoffs:
- Complexity vs. interpretability: Hybrid CNN-VSSM architecture provides strong performance but increases model complexity
- Multi-view processing requires handling missing data scenarios common in clinical practice
- Multi-task learning improves overall performance but may introduce conflicting optimization objectives across tasks

Failure Signatures:
- Poor performance on intermediate BI-RADS categories suggests limitations in distinguishing subtle differences
- Single-dataset evaluation limits generalizability across different populations and imaging systems
- Attention fusion may over-rely on certain views, potentially missing important information from underrepresented views

First Experiments:
1. Ablation study removing VSSM components to quantify their contribution to performance
2. Single-task training comparison to evaluate multi-task learning benefits
3. Cross-validation across different patient demographics to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on five-class BI-RADS classification (F1 = 0.4904) indicates limitations in distinguishing intermediate risk categories
- Single-dataset evaluation on TOMPEI-CMMD may limit generalizability across different populations and imaging systems
- Attention-based fusion mechanism increases complexity, potentially affecting interpretability and clinical adoption

## Confidence
- **High:** Binary classification results (AUC 0.9967) given strong performance metrics and clear methodology
- **Medium:** Multi-task learning framework's effectiveness, as improvements over single-task baselines are not explicitly compared
- **Low:** Claims about clinical applicability, as study lacks radiologist validation and real-world implementation assessment

## Next Checks
1. External validation on multiple independent mammography datasets from different institutions and imaging systems to assess generalizability
2. Comparison against established clinical workflows and radiologist performance on the same cases
3. Ablation studies to quantify the specific contributions of the CNN, VSSM, and attention fusion components to overall performance