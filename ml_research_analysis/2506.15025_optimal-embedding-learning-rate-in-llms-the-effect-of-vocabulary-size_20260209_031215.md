---
ver: rpa2
title: 'Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size'
arxiv_id: '2506.15025'
source_url: https://arxiv.org/abs/2506.15025
tags:
- learning
- size
- embedding
- vocabulary
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses a gap in the theory of maximal update parametrization\
  \ (\u03BCP) for large language models (LLMs) by considering the effect of vocabulary\
  \ size, which is typically much larger than model width in practice. While \u03BC\
  P provides scaling rules for hyperparameters with respect to width, it assumes fixed\
  \ vocabulary size, leading to suboptimal embedding layer learning rates in LLMs."
---

# Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size

## Quick Facts
- arXiv ID: 2506.15025
- Source URL: https://arxiv.org/abs/2506.15025
- Authors: Soufiane Hayou; Liyuan Liu
- Reference count: 40
- Primary result: The optimal embedding layer learning rate in LLMs should scale as Θ(√width) rather than Θ(1) when vocabulary size is much larger than model width.

## Executive Summary
This paper addresses a critical gap in maximal update parametrization (μP) theory for large language models by incorporating vocabulary size effects. While μP provides scaling rules for hyperparameters with respect to model width, it assumes fixed vocabulary size, leading to suboptimal embedding layer learning rates in practice. The authors develop a theoretical framework showing that as vocabulary size increases, training dynamics transition to a "Large Vocabulary (LV) Regime" where the optimal ratio of embedding to hidden layer learning rates scales as Θ(√width). This finding is validated through extensive experiments including training a 1B parameter transformer from scratch, demonstrating improved training perplexity and convergence compared to conventional μP approaches.

## Method Summary
The authors propose Large Vocabulary Parametrization (LVP) as an extension to μP that accounts for large vocabulary effects. LVP uses the same initialization variance d⁻¹/² for all layers as μP, but scales the embedding layer learning rate as η·d⁻¹/² while hidden layers use η·d⁻¹. The theoretical analysis tracks feature learning dynamics δᴱ and δᵂ, showing that large vocabulary size reduces the embedding layer's effective gradient signal strength from Θ(d) to Θ(√d) due to token frequency sparsity. This is validated through small-scale experiments on Wikitext2 across different model widths and vocabulary sizes, followed by large-scale validation on a 1B parameter transformer trained on 1.75T tokens.

## Key Results
- The optimal embedding layer learning rate ratio to hidden layers scales as Θ(√width) in the large vocabulary regime, contrasting with μP's Θ(1) prediction
- Training a 1B parameter transformer with LVP yields improved training perplexity compared to baseline μP parametrization
- The √d scaling rule demonstrates better hyperparameter transfer properties across different model widths and vocabulary sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large vocabulary size reduces the embedding layer's effective gradient signal strength from Θ(d) to Θ(√d) due to token frequency sparsity.
- Mechanism: Under Zipf-Mandelbrot token distributions, the average squared frequency ᾱ² scales as Θ(m⁻¹). This causes the cross-term d(d−1)/m in the feature learning component δᴱ to become comparable to d when m ≫ d, yielding δᴱ = Θ(ηᴱσᵦ√d) instead of Θ(ηᴱσᵦd) in the large-vocabulary limit.
- Core assumption: Token frequencies follow Zipf-Mandelbrot law with exponent a ∈ [ā, ā] where 1/2 < ā ≤ ā.
- Evidence anchors:
  - [abstract] "as vocabulary size increases, the training dynamics interpolate between the μP regime and another regime that we call Large Vocab (LV) Regime"
  - [section 3, Theorem 2] Shows δ̄ᴱ = Θ(ηᴱσᵦ√d) for high-frequency tokens when m is large
  - [corpus] Related work on μP transfer (arXiv:2511.01734) provides context but does not address vocabulary effects

### Mechanism 2
- Claim: Adam-like optimizers' element-wise normalization fundamentally alters the width-vocabulary interaction by standardizing gradient magnitudes across dimensions.
- Mechanism: SignSGD (a simplification of Adam without momentum) replaces gradient coordinates with their signs. This normalization makes gradient magnitudes Θ(1) regardless of scale, causing the vocabulary size effect to manifest through covariance structure rather than raw magnitude. The sign operation's correlation with Gaussian weights produces the √d scaling via Stein's lemma.
- Core assumption: The optimizer uses element-wise normalization (Adam, SignSGD, or similar); plain SGD would exhibit different dynamics.
- Evidence anchors:
  - [abstract] References Adam optimizer specifically
  - [section 3.1] "normalization is particularly significant because it fundamentally changes the magnitude of gradients, which becomes Θ(1) across virtually all scalable dimensions"
  - [corpus] No direct corpus evidence on optimizer-vocabulary interaction

### Mechanism 3
- Claim: The residual stream in transformers preserves the embedding-projection interaction that drives the LV regime dynamics.
- Mechanism: Even with hidden layers between embedding and projection, the residual connection creates a direct pathway. The theoretical analysis uses a linear model (embedding → projection) to capture this essential interaction. The covariance structure of updates depends on this direct connection.
- Core assumption: Residual connections preserve the core embedding-projection dynamics; the analysis assumes this extends to full transformers despite using a simplified model.
- Evidence anchors:
  - [section 3, "Extension to Transformer Architectures"] "the residual stream, which creates a direct connection between the embedding and projection layers"
  - [section 4.2] 1B transformer experiments validate the √d rule in production-scale models
  - [corpus] Weak evidence—no corpus papers explicitly test this architectural assumption

## Foundational Learning

- Concept: **Maximal Update Parametrization (μP)**
  - Why needed here: The paper positions its findings as a correction/extension to μP. Understanding μP's goals (HP transfer, maximal feature learning) and its scaling rules (hidden LR ∝ d⁻¹, embedding LR ∝ 1) is essential to grasp why the √d rule matters.
  - Quick check question: If μP predicts embedding LR = Θ(1) and hidden LR = Θ(d⁻¹), what ratio does this imply for embedding/hidden LR?

- Concept: **Feature Learning Dynamics in Infinite-Width Limits**
  - Why needed here: The paper defines "maximal feature learning" as δY = Θ(1) (feature change after one step). The analysis tracks how δᴱ and δᵂ scale with d and m to derive optimal learning rates.
  - Quick check question: Why is "lazy training" (δY → 0 as d → ∞) considered undesirable?

- Concept: **Zipf-Mandelbrot Distribution**
  - Why needed here: The theoretical results depend on token frequencies following a power law. This assumption drives the ᾱ² = Θ(m⁻¹) result that enables the LV regime.
  - Quick check question: If token frequencies were uniform rather than power-law distributed, how would ᾱ² scale with m?

## Architecture Onboarding

- Component map:
  - **Embedding layer** (E ∈ ℝᵐˣᵈ): Lookup table; rows updated proportionally to token frequency αᵢ
  - **Hidden layers** (θₗ): Attention + MLP blocks; use standard μP LR scaling (ηd⁻¹)
  - **Projection/output layer** (W ∈ ℝᵈˣᵐ): Maps hidden states back to vocabulary; use same LR as hidden layers per LVP
  - **Residual stream**: Direct path from embedding to projection that preserves LV dynamics

- Critical path: Embedding initialization σᴱ = d⁻¹/² → Embedding LR ηᴱ = ηd⁻¹/² → Hidden LR ηᵂ = ηd⁻¹ → Monitor δᴱ ≈ δᵂ ≈ Θ(1) for high-frequency tokens

- Design tradeoffs:
  - LVP vs μP: LVP uses √d ratio for embedding/hidden LR; μP uses d ratio. LVP better for m ≫ d but may be suboptimal for small vocabularies.
  - LVP vs SP: Both use same initialization (d⁻¹/²), but LVP scales hidden LR with d⁻¹ while SP uses constant LR. LVP designed for HP transfer.
  - High vs low frequency tokens: √d rule derived for high-frequency tokens; low-frequency tokens may benefit from different scaling (paper notes this limitation).

- Failure signatures:
  - Embedding LR too high (≫ √d × hidden LR): Training instability, exploding early losses
  - Embedding LR too low (≪ √d × hidden LR): Slow convergence, undertrained embeddings on rare tokens
  - Using μP embedding LR (constant) with large vocab: Suboptimal perplexity, poor HP transfer

- First 3 experiments:
  1. **Small-scale validation**: Train 2-layer transformer on Wikitext2 with d ∈ {256, 512, 1024, 2048}, fixed m=32768. Sweep ηᴱ and plot optimal ηᴱ vs d. Verify slope ≈ -1/2 on log-log plot (Figure 3 replication).
  2. **Ratio ablation**: Fix d=2048, train with ηᴱ/ηᵂ ∈ {1, √d, d, 2√d}. Compare training curves to confirm √d ≈ 45 is near-optimal (Figure 4b replication).
  3. **HP transfer test**: Tune η on small model (d=256), then apply LVP scaling to larger model (d=2048) without retuning. Compare against baseline that retunes ηᴱ independently.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the √d scaling rule truly optimal for feature learning in the large vocabulary regime, or merely an approximation that improves upon existing methods?
- Basis in paper: [explicit] The authors state: "It is unclear whether the limit with this scaling is optimal, and further theoretical and empirical studies are needed to investigate this."
- Why unresolved: The theoretical analysis derives the √d rule from feature learning dynamics, but does not prove optimality. Empirical results show variance around the √d line, suggesting potential limitations.
- What evidence would resolve it: A theoretical proof of optimality bounds, or empirical studies systematically comparing √d to alternative scaling exponents across diverse architectures and vocabularies.

### Open Question 2
- Question: How should optimal parametrization incorporate token frequency information, and would frequency-aware learning rates further improve training?
- Basis in paper: [explicit] "Optimal parametrization should incorporate information about token frequency... further theoretical and empirical studies are needed."
- Why unresolved: The analysis shows feature learning differs for frequent vs. infrequent tokens, but the proposed √d rule uses a single embedding learning rate rather than token-specific adaptation.
- What evidence would resolve it: Experiments comparing uniform embedding LR to frequency-weighted schemes, or theoretical analysis deriving frequency-dependent optimal learning rates.

### Open Question 3
- Question: Do the scaling rules generalize to multi-step training with practical optimizers like Adam, beyond the single-step SignSGD analysis?
- Basis in paper: [explicit] "The more general case where t is free is more challenging" and results were "derived in a simplified setting where we consider a simple model trained with SignSGD with 1 step."
- Why unresolved: The theoretical proof relies on single-step analysis with a simplified optimizer; extending to multiple steps introduces interaction effects not captured in the current framework.
- What evidence would resolve it: Theoretical analysis of multi-step dynamics, or empirical validation showing √d scaling remains optimal across varying training durations and optimizer choices.

## Limitations
- The theoretical framework relies on simplifying assumptions (Zipf-Mandelbrot distribution, SignSGD analysis, linear model architecture) that may not fully capture practical LLM training
- The √d scaling rule is derived for high-frequency tokens and may leave low-frequency tokens undertrained
- The precise boundary conditions between μP and LV regimes and exact m/d threshold remain unclear

## Confidence

- **High confidence**: The empirical validation on the 1B parameter transformer demonstrating improved perplexity with √d embedding learning rate scaling. The small-scale experiments showing optimal ηᴱ ∝ d⁻¹/² across multiple vocabulary sizes. The theoretical identification of vocabulary size as a previously overlooked dimension in μP scaling rules.

- **Medium confidence**: The mechanism explaining why element-wise normalization (Adam/SignSGD) is essential for the √d scaling to emerge. The extension of theoretical results from simple linear models to full transformer architectures with residual connections. The claim that this approach enables better hyperparameter transfer across scales.

- **Low confidence**: The precise boundary conditions between μP and LV regimes (exact m/d threshold). The robustness of √d scaling across different tokenization methods, data distributions, and optimization algorithms beyond Adam. The optimal scaling behavior for low-frequency tokens, which the paper acknowledges but does not fully resolve.

## Next Checks
1. **Cross-domain frequency validation**: Train models on datasets with different tokenization schemes (BPE, SentencePiece, token-free) and verify whether Zipf-Mandelbrot assumptions hold and whether √d scaling remains optimal. This would test the robustness of the theoretical frequency assumptions.

2. **Optimizer ablation study**: Compare √d scaling performance across different optimizers (SGD, AdamW, Lion) with varying degrees of normalization. This would validate whether the normalization mechanism is truly essential or if the scaling emerges through other pathways.

3. **Low-frequency token analysis**: Design experiments that isolate and measure training dynamics for low-frequency tokens under different embedding learning rate scalings. This would quantify the underexposure issue mentioned in the paper and potentially inform hybrid scaling rules.