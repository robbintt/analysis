---
ver: rpa2
title: 'MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models
  with Real-World MCP Servers'
arxiv_id: '2512.15163'
source_url: https://arxiv.org/abs/2512.15163
tags:
- attack
- tool
- attacks
- task
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCP-SafetyBench, a comprehensive benchmark
  for evaluating the safety of large language models (LLMs) operating in real-world
  Model Context Protocol (MCP) environments. Unlike prior work that focuses on isolated
  attacks or lacks integration with actual MCP servers, MCP-SafetyBench provides realistic
  multi-step evaluation across five domains (browser automation, financial analysis,
  location navigation, repository management, and web search) using real MCP servers.
---

# MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers

## Quick Facts
- **arXiv ID**: 2512.15163
- **Source URL**: https://arxiv.org/abs/2512.15163
- **Reference count**: 40
- **Primary result**: No LLM achieves both strong task performance and robust defense against MCP attacks, with attack success rates ranging from 29.8% to 48.2% across 13 models

## Executive Summary
This paper introduces MCP-SafetyBench, a comprehensive benchmark for evaluating the safety of large language models (LLMs) operating in real-world Model Context Protocol (MCP) environments. Unlike prior work that focuses on isolated attacks or lacks integration with actual MCP servers, MCP-SafetyBench provides realistic multi-step evaluation across five domains (browser automation, financial analysis, location navigation, repository management, and web search) using real MCP servers. The benchmark includes 20 distinct attack types spanning server, host, and user sides, and employs execution-based evaluation that measures both task success and attack success. Systematic evaluation of leading open- and closed-source LLMs reveals substantial safety gaps, with no model achieving both strong task performance and robust defense, highlighting the urgent need for stronger defenses and establishing MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.

## Method Summary
The paper introduces MCP-SafetyBench as the first execution-based benchmark for safety evaluation of LLMs in real-world MCP environments. The benchmark systematically evaluates 13 models across five domains using real MCP servers, with 20 attack types spanning server, host, and user sides. Evaluation measures both task success (TSR) and attack success (ASR) through execution-based testing rather than simulation. The methodology includes multi-step task scenarios that reflect real-world usage patterns, comprehensive attack taxonomies covering indirect injections, tool poisoning, and privilege escalation, and a balanced dataset of 120 tasks (8 per domain). The approach emphasizes realistic evaluation by testing actual MCP server interactions rather than simulated environments, providing a more accurate assessment of safety risks in production settings.

## Key Results
- No model achieves both high task success and strong defense; safety-utility trade-off observed (r=-0.57, p=0.041)
- Attack success rates range from 29.8% (GPT-4o-mini) to 48.2% (Qwen2.5-14B-Instruct), with average ASR of 40.6%
- Host-side attacks are most successful (81.94% average ASR), with Identity Injection achieving 100% success across all models
- Financial Analysis domain shows highest vulnerability (46.6% ASR) while Web Search shows lowest (30.3% ASR)
- Safety prompts alone reduce ASR by only 1.22% (not statistically significant, p=0.2908)

## Why This Works (Mechanism)
MCP-SafetyBench works by providing realistic, execution-based evaluation of LLM safety in MCP environments rather than simulated scenarios. The benchmark's effectiveness stems from its integration with real MCP servers, multi-step task scenarios that reflect actual usage patterns, and comprehensive attack taxonomies that cover the full spectrum of potential vulnerabilities. By measuring both task success and attack success, the benchmark captures the critical safety-utility trade-off that exists in real-world deployments. The execution-based approach ensures that safety evaluations reflect actual model behavior when interacting with real tools and services, providing more reliable insights than simulation-based methods that may miss subtle vulnerabilities or over-estimate model robustness.

## Foundational Learning
- **MCP Server Architecture**: Understanding how MCP servers mediate tool access between LLMs and external services is crucial for identifying attack vectors at the server-host interface.
- **Multi-Step Task Execution**: Real-world tasks often require multiple tool invocations, creating opportunities for attack amplification across sequential steps.
- **Contextual State Management**: LLMs must maintain coherent state across tool interactions, but this creates vulnerabilities to context poisoning and state confusion attacks.
- **Privilege Escalation Paths**: Attackers can exploit overly permissive tool access to escalate privileges beyond intended boundaries.
- **Tool Injection Mechanisms**: Understanding how malicious tools can be introduced through both server-side and host-side channels is essential for defense design.
- **Safety-Utility Trade-offs**: There exists an inherent tension between maximizing task completion and maintaining robust security boundaries.

Quick check: Can the model maintain task context while rejecting malicious tool invocations across multiple steps?

## Architecture Onboarding

Component map: User -> LLM -> MCP Host -> MCP Server -> External Services

Critical path: User query → LLM intent parsing → MCP host coordination → Tool selection → MCP server execution → Result return → LLM response

Design tradeoffs:
- Execution-based vs simulation-based evaluation: Execution provides realism but higher cost
- Task complexity vs evaluation scalability: More complex tasks better reflect real usage but require more resources
- Attack diversity vs benchmark comprehensiveness: Broader attack coverage increases evaluation thoroughness but extends testing time

Failure signatures:
- Unexpected tool invocations outside task scope
- Inconsistent state management across multi-step tasks
- Privilege escalation through tool chaining
- Context confusion leading to tool misdirection

First experiments:
1. Test single-step tasks to establish baseline task success rates
2. Evaluate simple tool injection attacks to identify immediate vulnerabilities
3. Measure context coherence across basic multi-step tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What multi-layered defense strategies beyond prompt-level safeguards can effectively mitigate MCP attacks without degrading task performance?
- Basis in paper: [explicit] The conclusion states: "To address these challenges, future work will explore multi-layered defense strategies that go beyond prompt-level safeguards" and shows safety prompts alone reduce ASR by only 1.22% (not statistically significant, p=0.2908).
- Why unresolved: Current defenses show a safety-utility trade-off (r=-0.57, p=0.041), with no model achieving both high TSR and strong defense.
- What evidence would resolve it: Demonstration of a defense mechanism that achieves both TSR >40% and ASR <20% on MCP-SafetyBench.

### Open Question 2
- Question: How can dynamic tool vetting validate tool invocations in real time using contextual and behavioral signals to block suspicious actions?
- Basis in paper: [explicit] The authors state: "A central direction is dynamic tool vetting, which validates tool invocations in real time using contextual and behavioral signals, blocking or downgrading suspicious actions."
- Why unresolved: Current approaches lack mechanisms to detect malicious tool behavior during execution, especially for subtle attacks like Tool Redirection (70.63% ASR).
- What evidence would resolve it: Implementation of a real-time vetting system that reduces Tool Redirection and Tool Poisoning ASRs below 20%.

### Open Question 3
- Question: How can architectural changes address the universal vulnerability to host-side attacks, particularly Identity Injection which achieves 100% success across all 13 models?
- Basis in paper: [explicit] Results show host-side attacks yield 81.94% average success, with Identity Injection at 100% across all models, exposing "critical flaws in intent parsing and state management."
- Why unresolved: The paper identifies the vulnerability but provides no defense mechanism for host-side coordination logic attacks.
- What evidence would resolve it: An architecture that reduces host-side attack ASR below 30% while maintaining task performance.

### Open Question 4
- Question: How can the "contextual least privilege principle" be formalized through privilege narrowing, context coherence checking, and risk-tiered responses?
- Basis in paper: [explicit] The conclusion proposes to "formalize 'safe' MCP behavior through the contextual least privilege principle, supported by system mechanisms such as privilege narrowing, context coherence checking, and risk-tiered responses."
- Why unresolved: Excessive Privileges Misuse remains an attack vector, and no implementation of contextual privilege constraints exists.
- What evidence would resolve it: A formalized framework with measurable reductions in privilege escalation attacks.

## Limitations
- Benchmark covers only five domains and 20 attack types, potentially missing other safety risks
- Evaluation relies on fixed MCP servers and tools, which may not represent all real-world implementations
- Execution-based methodology may not capture all edge cases or rare failure modes in production
- Safety-utility trade-off may be influenced by specific benchmark design and evaluation criteria

## Confidence
- **High**: Core finding of substantial safety gaps in current LLMs across multiple models and domains
- **Medium**: Safety-utility trade-off observation, as correlation may be influenced by benchmark design
- **Medium**: Domain-specific vulnerability findings, as attack success rates may vary with different task compositions

## Next Checks
1. Replicate the benchmark evaluation with additional MCP servers and tools to verify whether identified safety gaps persist across different implementations
2. Conduct adversarial testing with expert red teams to identify potential attack vectors not covered by current 20 attack types
3. Perform longitudinal studies to assess whether model safety improves over time with new releases and safety training approaches, focusing on safety-utility trade-off dynamics