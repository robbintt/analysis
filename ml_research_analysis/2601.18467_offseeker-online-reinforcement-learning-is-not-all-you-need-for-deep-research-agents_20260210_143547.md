---
ver: rpa2
title: 'OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research
  Agents'
arxiv_id: '2601.18467'
source_url: https://arxiv.org/abs/2601.18467
tags:
- honey
- training
- density
- search
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that expensive online reinforcement learning
  is not necessary for building powerful deep research agents. Instead, the authors
  introduce DeepForge, a fully open-source task synthesis framework that generates
  large-scale research queries without heavy preprocessing, and release the first
  comprehensive dataset of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs.
---

# OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents

## Quick Facts
- arXiv ID: 2601.18467
- Source URL: https://arxiv.org/abs/2601.18467
- Authors: Yuhang Zhou, Kai Zheng, Qiguang Chen, Mengkang Hu, Qingfeng Sun, Can Xu, Jingjing Chen
- Reference count: 40
- Primary result: OffSeeker-8B achieves 26.6 on BrowseComp-zh, competitive with 30B-parameter online RL models

## Executive Summary
This paper challenges the necessity of expensive online reinforcement learning for building powerful deep research agents. The authors introduce DeepForge, a fully open-source task synthesis framework that generates large-scale research queries without heavy preprocessing, and release the first comprehensive dataset of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Using these resources, they train OffSeeker (8B), a model developed entirely offline through supervised fine-tuning and DPO. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL, achieving 26.6 on BrowseComp-zh compared to 25.5 for WebSailor-32B, while significantly reducing training costs to near zero.

## Method Summary
OffSeeker is trained using a two-stage offline approach: first, supervised fine-tuning (SFT) on 33k filtered trajectories with 128k context windows for 3 epochs, then Direct Preference Optimization (DPO) on 21k preference pairs generated by an LLM evaluator. The DeepForge pipeline synthesizes training data by expanding seed entities through web search, constructing entity graphs, generating multi-hop questions with DeepSeek-v3.1, and pruning surface-level cues. The agent uses a ReAct-style framework with four tools (search, visit_urls, search_wiki, execute_code) and is trained on Qwen3-8B using 360-LLaMA-Factory with sequence parallelism across 64× NVIDIA H20 GPUs.

## Key Results
- OffSeeker-8B achieves 26.6 on BrowseComp-zh, outperforming WebSailor-32B (25.5) despite being 4× smaller
- Maintains competitive performance across GAIA, HLE, XBench-DeepSearch, and WebWalkerQA benchmarks
- Eliminates online RL training costs (GRPO training: ~$350) while achieving comparable results
- 128k context window provides significant advantage over 32k (35% performance gap on BrowseComp-zh)

## Why This Works (Mechanism)

### Mechanism 1: Web-based entity expansion yields more diverse training tasks
LLM generates seed nouns → search API retrieves URLs → html2text extracts content → LLM identifies long-tail entities → deduplication creates entity pool. This bypasses static Wikipedia dumps, capturing more diverse and current entities than curated encyclopedias.

### Mechanism 2: Multi-hop question synthesis with clue pruning creates tasks requiring genuine reasoning
Entity graph traversal → DeepSeek-v3.1 generates multi-hop questions with gold answers → pruning removes specific identifiers (dates, unique superlatives) while preserving solvability → tasks demand iterative search and cross-entity reasoning.

### Mechanism 3: Offline SFT + DPO can match online RL performance when data quality is sufficient
SFT on 33k filtered trajectories → DPO on 21k preference pairs (top-2 vs. bottom-2 trajectories by LLM evaluator) → no environment interaction during training. DPO objective maximizes log-ratio between preferred and rejected trajectories.

## Foundational Learning

- **ReAct-style agent frameworks**: Why needed here: OffSeeker uses interleaved reasoning (``) and action (``) tags; understanding this pattern is essential for trajectory interpretation. Quick check: Can you trace how a ReAct agent processes: search → observation → reasoning → next action?

- **Direct Preference Optimization (DPO)**: Why needed here: Core training method replacing online RL; requires understanding how preference pairs substitute for reward modeling. Quick check: Explain why DPO avoids learning a separate reward model compared to RLHF.

- **Multi-hop reasoning evaluation**: Why needed here: Benchmarks (GAIA, BrowseComp, HLE) measure multi-step information integration; task design targets this capability. Quick check: How does evaluating on BrowseComp differ from standard QA benchmarks like HotpotQA?

## Architecture Onboarding

- **Component map**: Entity expansion (LLM + search API) → Graph construction → Question generation (DeepSeek-v3.1) → Pruning → SFT (128k context, 3 epochs) → DPO (48k context, 1 epoch)

- **Critical path**: 1) Set up search API (Serper) and content extraction (Jina) 2) Run DeepForge entity expansion on small seed set (verify entity diversity) 3) Generate QA pairs, validate multi-hop property via tool call counts 4) Train SFT model on filtered trajectories 5) Generate preference pairs using SFT model + LLM evaluator 6) Run DPO training

- **Design tradeoffs**: Context window: 128k enables complex reasoning but increases compute; paper shows 32k achieves only ~35% of 128k performance on BrowseComp-zh. Dataset scale vs. quality: Figure 4 shows 2k high-quality samples outperform larger low-quality datasets (DeepDive-9B with 858 samples: 15.7% vs. 19.7%). Offline vs. online: Zero API cost but potentially lower ceiling than online RL for edge cases.

- **Failure signatures**: Low tool call counts (<8 per task) indicate insufficient task complexity. SFT loss plateauing early suggests data quality issues in filtering pipeline. DPO accuracy not improving indicates preference pair quality problems (preferred/rejected not discriminative).

- **First 3 experiments**: 1) Sanity check: Reproduce DeepForge entity expansion on 100 seed nouns, verify long-tail entity extraction rate (>60% entities should be low-frequency) 2) Ablation: Train SFT model on 2k vs. 10k vs. 33k samples, measure BrowseComp-zh accuracy to validate data scaling claims 3) Baseline comparison: Run SFT-only vs. SFT+DPO on held-out subset, verify DPO gains (+2-4 points as reported in Table 3)

## Open Questions the Paper Calls Out

1. Can the DeepForge synthesis framework and offline training paradigm effectively generalize to complex domains beyond web search, such as scientific literature or multimodal tasks?

2. To what extent would integrating external memory or hierarchical summarization mechanisms improve performance on queries that currently exceed the 128k token context window?

3. Does the competitiveness of the offline training approach persist when scaling model parameters significantly (e.g., >70B) compared to state-of-the-art online RL agents?

## Limitations

- Data Quality Dependency: The entire offline training approach depends heavily on the DeepForge dataset quality, with no independent validation beyond reported benchmark performance.

- Scalability Ceiling: Authors acknowledge potential capability ceilings compared to online RL for edge cases but don't empirically test this limit or provide error analysis.

- Resource Requirements: Despite claiming "near-zero" training costs, infrastructure requirements (64× H20 GPUs, 128k context windows) remain substantial.

## Confidence

- **High Confidence**: Claims about cost reduction (GRPO training: ~$350 vs. offline training: near-zero) and benchmark results on BrowseComp-zh (26.6 vs. WebSailor-32B: 25.5) are well-supported by experimental data.

- **Medium Confidence**: Mechanism claims about DeepForge's entity expansion and pruning approaches are supported by method descriptions and comparisons to Wikipedia-only methods, but lack direct ablation studies isolating each component's contribution.

- **Low Confidence**: The claim that their approach "solves" the need for expensive online RL is overstated - they demonstrate competitive performance but acknowledge potential capability ceilings without characterizing them.

## Next Checks

1. **Dataset Quality Audit**: Perform independent quality assessment of the DeepForge dataset by sampling trajectories and verifying the 5-stage filtering criteria are consistently applied.

2. **Capability Ceiling Analysis**: Design targeted edge-case scenarios that online RL would discover through exploration (rare query types, complex reasoning chains) and test whether OffSeeker-8B fails on these while RL-trained models succeed.

3. **Cost-Benefit Scaling Study**: Measure performance degradation when reducing context window from 128k to 32k and when training on progressively smaller subsets of the DeepForge dataset to quantify the practical limits of their approach.