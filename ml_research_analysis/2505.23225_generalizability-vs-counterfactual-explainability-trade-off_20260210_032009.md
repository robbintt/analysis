---
ver: rpa2
title: Generalizability vs. Counterfactual Explainability Trade-Off
arxiv_id: '2505.23225'
source_url: https://arxiv.org/abs/2505.23225
tags:
- counterfactual
- training
- decision
- boundary
- valid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a novel theoretical and empirical connection\
  \ between model generalization and counterfactual explainability. The authors introduce\
  \ \u03B5-valid counterfactual probability (\u03B5-VCP) - the probability of finding\
  \ perturbations within an \u03B5-neighborhood that change a model's prediction."
---

# Generalizability vs. Counterfactual Explainability Trade-Off

## Quick Facts
- arXiv ID: 2505.23225
- Source URL: https://arxiv.org/abs/2505.23225
- Reference count: 40
- Primary result: Models that overfit have higher ε-valid counterfactual probability (ε-VCP), establishing a trade-off between generalization and counterfactual explainability.

## Executive Summary
This paper establishes a novel theoretical and empirical connection between model generalization and counterfactual explainability. The authors introduce ε-valid counterfactual probability (ε-VCP) - the probability of finding perturbations within an ε-neighborhood that change a model's prediction. Through theoretical analysis, they prove that ε-VCP increases as the margin between data points and the decision boundary decreases, which occurs during overfitting. This creates a fundamental trade-off: as models become more complex and fit training data more tightly, they become easier to explain via counterfactuals but generalize worse. Empirical results on Water Potability and Air Quality datasets validate this theory, showing that both linear and non-linear models exhibit increasing ε-VCP as training accuracy improves. Regularization techniques like dropout partially mitigate this effect by smoothing decision boundaries. The work provides both theoretical insights and a practical metric (average ε-VCP) for quantifying model overfitting through counterfactual explainability.

## Method Summary
The authors introduce ε-valid counterfactual probability (ε-VCP) as a metric to quantify the probability of finding valid counterfactual explanations within an ε-neighborhood of training points. For linear models, they derive closed-form expressions for ε-VCP based on the geometric margin. For non-linear models, they use local linear approximation via tangent hyperplanes. The method involves computing geometric margins for training samples, generating random perturbations within the ε-neighborhood, and measuring the proportion that flip the model's prediction. They validate their theory on two tabular datasets (Water Potability and Air Quality) using logistic regression with polynomial expansion and multi-layer perceptrons with and without dropout regularization.

## Key Results
- Theoretical proof that ε-VCP increases as model overfitting increases due to reduced margins
- Empirical validation showing regularized MLPs achieve lower peak ε-VCP than unregularized counterparts
- Linear model experiments confirm ε-VCP increases monotonically with training epochs while average margin decreases
- Both linear and non-linear models exhibit the generalizability-explainability trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The probability of finding valid counterfactuals within a local ε-neighborhood (ε-VCP) increases as model overfitting increases.
- Mechanism: Overfitting produces complex, convoluted decision boundaries that tightly conform to training data. This reduces the average geometric margin (distance from data points to decision boundary), meaning a larger proportion of random perturbations within ε distance will cross the boundary and produce label changes.
- Core assumption: Perturbation distribution φ is uniform within the counterfactual shell S(xi, γi, ε); data geometry is approximately Euclidean in the local neighborhood.
- Evidence anchors:
  - [abstract] "We provide a theoretical analysis of ε-VCP in relation to the geometry of the model's decision boundary, showing that ε-VCP tends to increase with model overfitting."
  - [section 4.1] "In the presence of a strong degree of model overfitting, the decision boundary learned becomes a highly convoluted surface... Therefore, each data point, on average, is 'closer' to the decision boundary."
  - [corpus] "Countering Overfitting with Counterfactual Examples" (FMR=0.598) directly supports the overfitting-counterfactual relationship; other corpus papers show weaker direct alignment.
- Break condition: If decision boundaries become smoother through regularization while maintaining low margins (e.g., specific architectural constraints), ε-VCP may not correlate reliably with overfitting.

### Mechanism 2
- Claim: ε-VCP is analytically derivable from the geometric margin for linear models and locally approximable for non-linear models.
- Mechanism: For linear classifiers, the decision boundary is a hyperplane; the probability of crossing it via random perturbation depends on the spherical cap volume ratio, which is a function of margin γi and perturbation radius ε (Theorem 4.2). For non-linear models, local linear approximation via tangent hyperplanes yields analogous bounds (Theorem 4.3).
- Core assumption: Non-linear boundaries can be locally approximated by tangent hyperplanes in sufficiently small neighborhoods (ε → γi limit).
- Evidence anchors:
  - [section 4.3.1] Theorem 4.2 provides closed-form ε-VCP using regularized incomplete Beta functions: p^ε_i = (1/2)(ε^n/(ε^n - γ^n_i))I(1 - (γ_i/ε)²; (n+1)/2, 1/2)
  - [section 4.3.2] Theorem 4.3 shows local approximation: p^ε_i ≈ C × γ_i^((n-1)/2) × δ^((n-1)/2) where δ = ε - γ_i
  - [corpus] No direct corpus validation for the specific mathematical derivation; mechanism relies primarily on paper's internal proofs.
- Break condition: For highly irregular boundaries (e.g., fractal-like decision surfaces in severely overfitted deep networks), tangent plane approximations may fail, making ε-VCP estimation unreliable.

### Mechanism 3
- Claim: Average ε-VCP across training data serves as a quantitative proxy for detecting overfitting.
- Mechanism: By Jensen's inequality applied to the convex function g(γ_i), average ε-VCP is lower-bounded by g(γ̄). Since g is monotonically decreasing in margin and margins decrease with overfitting (per margin theory literature), higher average ε-VCP indicates poorer generalization.
- Core assumption: The relationship between margin and generalization from margin theory (Mason et al., Neyshabur et al., Wu & Yu) holds for the model class under consideration.
- Evidence anchors:
  - [section 4.4] "Relevant literature on margin theory has shown that overfitting generally results in reduced margins around training samples on average... Therefore, the average ε-VCP can indeed serve as a quantitative indicator of model overfitting."
  - [section 5.2] Empirical validation shows regularized MLPs have lower ε-VCP than unregularized counterparts across both Water Potability and Air Quality datasets.
  - [corpus] "Decoupling Generalizability and Membership Privacy Risks" (FMR=0.545) discusses related trade-offs but doesn't directly validate ε-VCP as proxy.
- Break condition: If regularization techniques smooth boundaries without significantly increasing margins (e.g., certain dropout configurations), ε-VCP may understate overfitting risk.

## Foundational Learning

- **Concept: Geometric Margin**
  - Why needed here: The entire theoretical framework relates ε-VCP to the distance from data points to decision boundaries. Without understanding margin as the minimum distance to the boundary, the inverse relationship with counterfactual probability is opaque.
  - Quick check question: Given a binary classifier with decision boundary h(x) = w·x + b = 0, what is the margin for a point x₀, and how does it change if ||w|| doubles while keeping the prediction unchanged?

- **Concept: Counterfactual Explanations**
  - Why needed here: The paper redefines counterfactual generation as a probabilistic process rather than an optimization problem. Understanding that counterfactuals answer "what input change flips the prediction?" is prerequisite to grasping ε-VCP as "probability of finding such a change randomly."
  - Quick check question: For a loan applicant predicted "reject," what information does a counterfactual explanation provide, and how does ε-VCP quantify the "ease" of finding one?

- **Concept: Monte Carlo Integration**
  - Why needed here: The empirical estimation of ε-VCP uses Monte Carlo sampling to approximate the integral over the counterfactual shell. Without this, the connection between theoretical derivations and practical implementation is unclear.
  - Quick check question: Why does sampling 1,000 random perturbations from the ε-ball around each training point provide a reasonable estimate of p^ε_i, and when would this approach fail?

## Architecture Onboarding

- **Component map:**
  Margin Estimator -> Perturbation Sampler -> ε-VCP Calculator -> Training Monitor

- **Critical path:**
  Margin estimation → Perturbation sampling → Label change detection → ε-VCP aggregation. The margin estimation step is the computational bottleneck for non-linear models, as it requires finding the closest boundary point or using proxy methods.

- **Design tradeoffs:**
  - **Exact vs. approximate margins**: Exact computation for deep networks is intractable; gradient-based approximations (Wu & Yu method) trade accuracy for speed
  - **Uniform vs. manifold-aware perturbations**: Paper assumes uniform; real data lies on manifolds, but manifold sampling (e.g., via autoencoders) adds complexity
  - **ε selection**: Too small yields empty shells (ε < γ_i); too large produces unrealistic perturbations. Paper suggests dataset-dependent empirical calibration

- **Failure signatures:**
  - ε-VCP ≈ 0.5 for all models regardless of regularization → ε too large relative to data scale
  - ε-VCP ≈ 0 for all models → ε too small; most margins exceed ε
  - Non-monotonic ε-VCP trajectory during training → potential bug in margin estimation or perturbation sampling

- **First 3 experiments:**
  1. **Reproduce linear model validation**: Train logistic regression on Water Potability with polynomial feature expansion (d=6); verify ε-VCP increases monotonically with training epochs while average margin decreases. Compare against Theorem 4.2 predictions.
  2. **Regularization ablation**: Train identical MLP architectures (5 hidden layers) with and without dropout (p=0.5) on both datasets; confirm regularized models achieve lower peak ε-VCP at comparable training accuracy. Plot ε-VCP vs. accuracy curves as in Figure 4.
  3. **ε sensitivity analysis**: For a fixed trained model, compute ε-VCP across a range of ε values (e.g., [0.1, 0.5, 1.0, 2.0] normalized to feature scale); verify ε-VCP converges as ε increases and identify the ε range where overfitting signal is strongest (steepest separation between regularized/unregularized models).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a regularization term derived from ε-VCP be effectively incorporated into training objectives to simultaneously reduce overfitting and preserve counterfactual explainability?
  - Basis in paper: [explicit] The authors state in Section 6, "in future work, we plan to design a regularization term based on ε-VCP and incorporate it into standard training objectives to mitigate overfitting while preserving explainability."
  - Why unresolved: While the paper establishes ε-VCP as a proxy for overfitting, it does not demonstrate how to minimize this metric during training to improve generalization.
  - What evidence would resolve it: A modified loss function that penalizes high ε-VCP values, resulting in improved test set accuracy and smoother decision boundaries compared to unregularized models.

- **Open Question 2**: How does the generalization-explainability trade-off shift when using manifold-based or data-driven perturbations instead of uniform distributions?
  - Basis in paper: [explicit] The authors note that assuming uniform perturbations "may not reflect real-world data geometry" and suggest that "future work could incorporate manifold-based or data-driven perturbations... to obtain sharper insights."
  - Why unresolved: The current theoretical analysis and Monte Carlo estimation rely on uniform sampling within an ε-ball, which ignores the underlying data manifold and may misestimate the true probability of finding valid counterfactuals.
  - What evidence would resolve it: A theoretical derivation and empirical validation of ε-VCP using generative models to sample perturbations, showing a modified correlation with the generalization gap.

- **Open Question 3**: Can K-Lipschitz assumptions provide more accurate geometric margin estimates for calculating ε-VCP in complex deep neural networks?
  - Basis in paper: [explicit] Section 6 highlights that the geometric margin is analytically computable only for linear models, suggesting "a potential direction is to estimate the margin using a K-Lipschitz assumption on h_θ" for complex models.
  - Why unresolved: The current method for non-linear models uses a local linear approximation (tangent hyperplane), which may fail to capture the complex curvature of deep network decision boundaries.
  - What evidence would resolve it: Experiments comparing local linear approximation margins against Lipschitz-constrained margin bounds in deep networks to see which better predicts empirical ε-VCP.

## Limitations

- The theoretical framework assumes Euclidean geometry and uniform perturbation distributions, which may not hold for real-world data lying on low-dimensional manifolds.
- The paper acknowledges that exact margin computation for deep networks is intractable and relies on approximations, potentially weakening the ε-VCP-overfitting relationship for highly non-linear models.
- The choice of ε appears empirically driven without a principled method, raising questions about result sensitivity to this hyperparameter.

## Confidence

- **High confidence**: The inverse relationship between margin and ε-VCP, and the linear model analysis with closed-form derivations.
- **Medium confidence**: The non-linear case due to reliance on local linear approximations, and ε-VCP as a general overfitting proxy given sensitivity to ε selection and data geometry.
- **Medium confidence**: The empirical validation across different datasets and model architectures, though the specific ε values and margin estimation methods introduce uncertainty.

## Next Checks

1. **Perturbation distribution sensitivity**: Repeat ε-VCP experiments using Gaussian perturbations instead of uniform sampling to assess robustness to the assumed perturbation distribution.

2. **Manifold-aware counterfactuals**: Implement and compare ε-VCP using perturbations restricted to the data manifold (via autoencoder decoding) versus uniform Euclidean perturbations to test the Euclidean geometry assumption.

3. **Transferability across datasets**: Validate the ε-VCP-overfitting relationship on image datasets (e.g., MNIST/CIFAR) where margin theory applicability and perturbation realism can be more rigorously assessed.