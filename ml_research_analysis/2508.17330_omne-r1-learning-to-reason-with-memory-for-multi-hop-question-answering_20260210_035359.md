---
ver: rpa2
title: 'Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering'
arxiv_id: '2508.17330'
source_url: https://arxiv.org/abs/2508.17330
tags:
- reasoning
- question
- tool
- knowledge
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Omne-R1, a multi-stage training framework for
  multi-hop question answering on schema-free knowledge graphs. The approach combines
  rule-based reinforcement learning, supervised fine-tuning with weighted loss masking,
  and post-SFT reinforcement learning to improve reasoning performance.
---

# Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering

## Quick Facts
- arXiv ID: 2508.17330
- Source URL: https://arxiv.org/abs/2508.17330
- Reference count: 40
- Key outcome: Multi-stage training framework achieves 94.1% accuracy on 1/2 hop questions and 69.3% on 3+ hop questions

## Executive Summary
This paper presents Omne-R1, a multi-stage training framework for multi-hop question answering on schema-free knowledge graphs. The approach combines rule-based reinforcement learning, supervised fine-tuning with weighted loss masking, and post-SFT reinforcement learning to improve reasoning performance. Experiments show that the model achieves 94.1% accuracy on 1/2 hop questions and 69.3% on 3+ hop questions after training, significantly outperforming baselines. The method demonstrates strong generalization across diverse knowledge domains and improves reasoning trajectory standardization through reinforcement learning.

## Method Summary
The three-stage training workflow begins with rule-based reinforcement learning on Qwen2.5-14B to establish basic instruction following and format compliance. This is followed by supervised fine-tuning on Qwen3-14B using hybrid data (model-generated reasoning trajectories plus template-synthesized data) with a weighted loss mask that de-emphasizes thinking tokens (weight=0.001). The final stage applies post-SFT reinforcement learning with a simplified reward structure and repetition penalty. The system uses schema-free knowledge graphs constructed via LightRAG, with tools for semantic entity matching and node information retrieval.

## Key Results
- Multi-stage training achieves 94.1% accuracy on 1/2 hop questions and 69.3% on 3+ hop questions
- Weighted loss masking with 0.001 weight on thinking tokens improves accuracy by 1.3% on 1/2-hop and 7.7% on 3+ hop questions compared to equal weighting
- Progressive capability stacking shows Stage 1 RL provides 22.6% improvement on 1/2-hop questions, Stage 2 SFT adds 36.6% additional gain

## Why This Works (Mechanism)

### Mechanism 1: Progressive Capability Stacking via Multi-Stage Training
Sequential training through RL → SFT → RL produces higher multi-hop accuracy than any single training phase alone. Stage 1 RL establishes basic instruction following and format compliance; Stage 2 SFT imitates complete reasoning trajectories with tool calls; Stage 3 RL explores improved trajectories beyond imitation. Core assumption: Capabilities learned in earlier stages transfer positively to later stages without catastrophic forgetting. Evidence: Accuracy progression from 34.9% to 94.1% on 1/2-hop questions through the training pipeline.

### Mechanism 2: Weighted Loss Masking De-emphasizes Verbiage, Emphasizes Tool Correctness
Reducing loss contribution from thinking tokens (weight=0.001) improves multi-hop accuracy compared to equal weighting (weight=1.0). Gradient updates from CoT tokens are scaled down, reducing pressure to memorize specific reasoning phrasing while preserving learning signal for tool call parameters and final answers. Core assumption: The structural reasoning pattern matters more than exact natural language expression. Evidence: Accuracy peaks at weight=0.001 (89.2%/60.5%) vs weight=1.0 (87.9%/52.8%).

### Mechanism 3: Hierarchical Reward Enforces Reasoning Before Action
Multiplicative reward structure R_total = R_format × (0.1 + 0.9 × R_result) prevents shortcut behavior where models call tools without reasoning. Format compliance gates the reward; even correct answers receive ≤10% of maximum reward if format is violated. Core assumption: Models can learn format compliance via RL from sparse binary signals. Evidence: Model exhibited strong tendency to directly invoke traversal tools without proper reasoning first, requiring this enforcement mechanism.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Stage 1 and Stage 3 both use PPO for RL training; understanding the clipping mechanism explains why the approach is stable.
  - **Quick check question:** Can you explain why PPO uses a clipped objective instead of unconstrained policy gradient updates?

- **Concept: Actor-Critic Architecture**
  - **Why needed here:** The training pipeline separates policy (actor) and value estimation (critic); GAE computation depends on this structure.
  - **Quick check question:** What is the role of the value function V(s) in computing the advantage estimate Â_t?

- **Concept: Schema-Free Knowledge Graphs**
  - **Why needed here:** Unlike structured KGs with fixed ontologies, LightRAG constructs graphs without predefined schemas, requiring semantic entity matching rather than exact key lookups.
  - **Quick check question:** How does a schema-free graph change the entity linking challenge compared to a knowledge base with canonical entity IDs?

## Architecture Onboarding

- **Component map:** KG Tool Service (FastAPI) -> Actor Model (Qwen3-14B) -> Critic Model -> Training Pipeline (LLaMA Factory + custom PPO)
- **Critical path:** 1) Construct KGs via LightRAG from domain corpora, 2) Generate multi-hop QA pairs, 3) Stage 1 RL with rule-based reward, 4) Generate multi-turn reasoning trajectories, 5) Stage 2 SFT with weighted loss mask, 6) Stage 3 RL with repetition penalty
- **Design tradeoffs:** Full SFT vs LoRA (Full achieves higher accuracy but requires more compute), Think mode vs No-think mode (enabling think mode during inference degrades performance), 7-round limit (caps reasoning depth vs latency)
- **Failure signatures:** Repetitive tool calls (addressed by Stage 3 repetition penalty), Over-verbose CoT (fixed by non-zero weight), Missing graph_type in tool calls (enforced in system prompt)
- **First 3 experiments:** 1) Baseline probe: Run base Qwen3-14B on 50 questions with KG tools, 2) Loss weight sweep: Train Stage 2 SFT with weights [1.0, 0.1, 0.01, 0.001], 3) Tool ablation: Disable entity_matcher, force use of only node_info

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the multi-stage training workflow scale when applied to Large Language Models (LLMs) significantly larger than the 14B parameters tested?
- **Basis in paper:** [explicit] The Conclusion states that areas remaining to be explored include "exploring scaling curves when applying the framework to larger models."
- **Why unresolved:** All reported experiments were restricted to the Qwen2.5-14B and Qwen3-14B architectures. It is unclear if the weighted loss masking technique or the stability of the rule-based reinforcement learning phases will hold, degrade, or improve with models of 70B or 100B+ parameters.
- **What evidence would resolve it:** A comparative analysis of accuracy and training stability when executing the full three-stage Omne-R1 workflow on larger parameter sets (e.g., 70B).

### Open Question 2
- **Question:** Can a progress reward model be successfully integrated to distinguish between necessary reasoning and "over-thinking"?
- **Basis in paper:** [explicit] The Conclusion lists "integrating a progress reward model to ensure correct reasoning traces and prevent over-thinking" as a future direction.
- **Why unresolved:** The current framework relies on a repetition penalty and a weighted loss mask, but the paper notes that without a non-zero weight, the model reverts to verbose, "hesitant" reasoning. There is currently no mechanism to dynamically reward efficiency or penalize unnecessary reasoning steps during the RL phase.
- **What evidence would resolve it:** Designing and testing a "progress reward" function that measures reasoning efficiency (e.g., correct answer retrieval with minimal tokens/steps) and demonstrating improved accuracy-to-inference-cost ratios.

### Open Question 3
- **Question:** Is the optimal weighted loss mask value of 0.001 specific to the Qwen3 architecture?
- **Basis in paper:** [inferred] The experiments determined that a weight of 0.001 applied to CoT tokens was optimal, but this search was conducted solely on Qwen3-14B.
- **Why unresolved:** The sensitivity of the loss function to this specific weight may vary across different model architectures and pre-training distributions (e.g., Llama vs. Mistral). A weight low enough to prevent overfitting in Qwen might suppress necessary learning signals in other models.
- **What evidence would resolve it:** A hyperparameter sweep of the loss mask weight on non-Qwen foundation models to determine if the 0.001 setting generalizes or if architecture-specific tuning is required.

## Limitations

- The reliance on auto-generated synthetic data may limit robustness on noisy, human-curated knowledge graphs
- The 7-round limit caps reasoning depth, potentially limiting performance on very complex multi-hop questions
- The optimal weighted loss mask value of 0.001 was only tested on Qwen3 architecture, raising questions about generalizability

## Confidence

- **High Confidence:** The empirical accuracy improvements from multi-stage training are directly measurable from the reported results
- **Medium Confidence:** The claim that weighted loss masking improves performance is supported by the comparison between weights 0.001 and 1.0, but the causal mechanism and parameter sensitivity require further investigation
- **Low Confidence:** The assertion that capabilities learned in Stage 1 transfer positively to Stage 2 without catastrophic forgetting is assumed but not explicitly tested

## Next Checks

1. **PPO Dynamics Analysis:** Instrument the Stage 1 RL training to log reward distributions, format compliance rates, and KL divergence over training steps to characterize whether the hierarchical reward actually gates exploration as intended.

2. **Weighted Loss Sensitivity Sweep:** Systematically vary the thinking token weight parameter (0.0, 0.001, 0.01, 0.1, 1.0) on a held-out validation set and plot both accuracy and CoT verbosity to quantify the optimal point and test the mechanism.

3. **Stage Transfer Experiment:** Train a baseline model directly on the Stage 2 SFT data (bypassing Stage 1 RL) and compare performance to test whether the Stage 1 capabilities are necessary or if SFT alone achieves similar results.