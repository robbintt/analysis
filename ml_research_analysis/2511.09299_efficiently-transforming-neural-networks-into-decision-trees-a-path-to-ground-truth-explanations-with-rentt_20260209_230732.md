---
ver: rpa2
title: 'Efficiently Transforming Neural Networks into Decision Trees: A Path to Ground
  Truth Explanations with RENTT'
arxiv_id: '2511.09299'
source_url: https://arxiv.org/abs/2511.09299
tags:
- methods
- data
- rentt-fi
- feature
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RENTT provides an exact, scalable algorithm for transforming neural
  networks into equivalent multivariate decision trees, addressing the limitations
  of existing methods that lack exactness, scalability, or interpretability. The algorithm
  efficiently handles fully connected, convolutional, and recurrent networks with
  general activation functions, achieving runtime complexity of O(x^1.3-1.5) and memory
  complexity of O(x^1.7-1.9) for hidden neurons, compared to O(x^18-19) and O(x^8-9)
  for previous approaches.
---

# Efficiently Transforming Neural Networks into Decision Trees: A Path to Ground Truth Explanations with RENTT

## Quick Facts
- **arXiv ID**: 2511.09299
- **Source URL**: https://arxiv.org/abs/2511.09299
- **Reference count**: 22
- **Primary result**: RENTT provides exact, scalable neural-to-decision-tree transformation with O(x^1.3-1.5) runtime vs O(x^18-19) for prior methods

## Executive Summary
RENTT introduces an exact, scalable algorithm for transforming neural networks into equivalent multivariate decision trees, addressing fundamental limitations of existing methods that lack exactness, scalability, or interpretability. The approach efficiently handles fully connected, convolutional, and recurrent networks with general activation functions by building sparse decision trees through ante-hoc pruning—only creating nodes for activation patterns observed in training data. This enables ground truth feature importance calculations at local, regional, and global levels, revealing significant discrepancies between RENTT-FI and common approximation methods like LIME, SHAP, and BreakDown. Across multiple datasets, RENTT-FI runs 300-7,500 times faster than these methods while providing mathematically exact explanations.

## Method Summary
RENTT transforms trained neural networks into exact equivalent multivariate decision trees through a sparse construction approach. The algorithm extracts activation patterns from training data, computes effective weight matrices for observed patterns only, and builds a pruned decision tree that maintains equivalence up to machine precision. The transformation handles fully connected networks with piecewise linear activations (ReLU, LeakyReLU) and extends to convolutional and recurrent networks. Key innovations include ante-hoc pruning to avoid exponential memory growth, exact computation of activation boundaries using effective weight matrices, and decision rules that incorporate both neuron activations and pooling layer operations. The resulting trees enable ground truth feature importance calculations through RENTT-FI, which runs orders of magnitude faster than conventional approximation methods while providing mathematically exact explanations.

## Key Results
- Runtime complexity improved from O(x^18.9-19.0) to O(x^1.3-1.5) and memory from O(x^8.7-9.0) to O(x^1.7-1.9) for hidden neurons
- Exact equivalence maintained between neural networks and decision trees up to machine precision across all tested architectures
- RENTT-FI runs 300-7,500 times faster than LIME, SHAP, and BreakDown while providing mathematically exact feature importance
- Significant discrepancies found between RENTT-FI ground truth and approximation methods across multiple datasets

## Why This Works (Mechanism)
The algorithm's efficiency stems from pruning the decision tree construction process to only consider activation patterns actually observed in training data, avoiding the exponential explosion of possible paths. Exact equivalence is achieved by computing effective weight matrices that capture the combined effect of all preceding layers for each observed activation pattern. The ante-hoc pruning strategy ensures the transformed tree remains sparse while maintaining mathematical equivalence, as only branches corresponding to training data patterns are created.

## Foundational Learning
- **Piecewise linear activation functions**: Neural networks with ReLU/LeakyReLU are piecewise linear, enabling exact boundary computation - needed for determining decision boundaries; quick check: verify activation functions are piecewise linear
- **Ante-hoc pruning**: Creating nodes only for observed activation patterns during training - needed to avoid exponential memory growth; quick check: confirm training data covers most common activation patterns
- **Effective weight matrices**: Computing D_i matrices that combine effects of preceding layers for each activation pattern - needed for exact transformation; quick check: verify matrix computations maintain equivalence
- **Multivariate decision trees**: Decision nodes with multiple feature conditions simultaneously - needed for accurate neural network representation; quick check: ensure decision rules use Eq. 9 and Eq. 14 correctly
- **Ground truth feature importance**: Exact computation of feature contributions from decision tree structure - needed for benchmark comparisons; quick check: verify RENTT-FI calculations match tree traversal results

## Architecture Onboarding
**Component Map**: Training Data -> Neural Network -> Activation Pattern Extraction -> Effective Matrix Computation -> Pruned Decision Tree Construction -> RENTT-FI Feature Importance
**Critical Path**: NN training → activation pattern extraction → effective matrix computation → DT construction → feature importance calculation
**Design Tradeoffs**: Exactness vs memory (pruning reduces memory but may miss rare patterns), computational efficiency vs completeness (observed patterns only), mathematical rigor vs practical implementation
**Failure Signatures**: Memory exhaustion (>10^4 neurons), missing activation patterns at inference, incorrect bias handling, numerical precision issues
**First Experiments**: 1) Train small FcNN (2 layers [8,4]) on Iris dataset, 2) Transform with RENTT and verify identical predictions, 3) Compute RENTT-FI and compare with LIME/SHAP

## Open Questions the Paper Calls Out
**Open Question 1**: What are the neural network size and sample number thresholds beyond which RENTT-FI's transformation costs surpass those of conventional FI methods, and what are the optimal deployment ranges for RENTT-FI? The paper shows RENTT-FI is faster than conventional methods but hasn't systematically analyzed transformation overhead against varying network sizes and sample counts.

**Open Question 2**: Can RENTT-FI explanations provide measurable benefits for downstream human decision-making tasks compared to approximation-based methods? While the paper demonstrates mathematical correctness, it doesn't evaluate whether ground truth explanations improve user understanding, trust calibration, or decision quality.

**Open Question 3**: How can the transformation framework be extended to scale to complex image tasks and larger network architectures while maintaining tractable memory consumption? Networks with 10^4 hidden neurons require 31-630 GB of RAM, limiting applicability on standard hardware.

**Open Question 4**: How do common XAI correctness metrics compare when evaluated against ground truth explanations from RENTT-FI? Multiple correctness metrics exist but lack a ground truth benchmark for validation; RENTT-FI provides one but hasn't been used to evaluate these metrics.

## Limitations
- Memory requirements still prohibitive for state-of-the-art architectures (10^4 neurons need 31-630 GB RAM)
- Current implementation only covers fully connected networks, with CNN/RNN support mentioned but not fully implemented
- Lack of specific training hyperparameters introduces variability in learned weights
- Requires training data to cover most common activation patterns for effective pruning

## Confidence
- **Exact neural-to-decision-tree transformation**: High confidence (well-defined algorithm with clear mathematical basis)
- **Computational complexity improvement**: Medium confidence (theoretical claims supported, but scaling validation requires larger networks than typical research resources)
- **Feature importance accuracy and runtime advantage**: Medium confidence (comparative claims rely on specific implementations of competing methods not fully detailed)

## Next Checks
1. Verify exact equivalence by training a small 2-layer ReLU network (8,4 neurons) on Iris dataset, transforming with RENTT, and confirming identical predictions across training and test sets
2. Test the pruning mechanism by attempting inference on samples that trigger unseen activation patterns—document whether the system gracefully adds new paths or fails
3. Reproduce Table 1's scaling comparison by measuring runtime and memory for RENTT vs ECDT implementations on networks of increasing size (10^2 to 10^4 hidden neurons)