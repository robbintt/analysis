---
ver: rpa2
title: Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and
  Attribution
arxiv_id: '2511.16541'
source_url: https://arxiv.org/abs/2511.16541
tags:
- images
- generators
- image
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for AI-generated image
  detection and source attribution that addresses the challenge of rapidly evolving
  generative models. The proposed approach employs supervised contrastive learning
  with a MambaVision backbone to extract discriminative embeddings, followed by k-nearest
  neighbors classification in a few-shot learning paradigm.
---

# Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution

## Quick Facts
- **arXiv ID**: 2511.16541
- **Source URL**: https://arxiv.org/abs/2511.16541
- **Reference count**: 40
- **Primary result**: 91.3% detection accuracy with only 150 images per class

## Executive Summary
This paper introduces a novel framework for AI-generated image detection and source attribution that addresses the challenge of rapidly evolving generative models. The proposed approach employs supervised contrastive learning with a MambaVision backbone to extract discriminative embeddings, followed by k-nearest neighbors classification in a few-shot learning paradigm. This architecture enables effective generalization to previously unseen generators while requiring minimal training data. Experimental results demonstrate an average detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing methods, while requiring only 150 images per class. For source attribution in open-set classification, the approach achieves improvements of 14.70% in AUC and 4.27% in OSCR compared to state-of-the-art methods.

## Method Summary
The proposed framework combines supervised contrastive learning with a MambaVision backbone to extract discriminative embeddings from AI-generated images. The contrastive loss encourages embeddings from the same generator to cluster together while pushing apart embeddings from different generators. After training, a k-nearest neighbors classifier operates on the learned embeddings for both detection and attribution tasks. The few-shot learning paradigm requires only 150 images per class during training, addressing the data scarcity challenge in AI-generated image forensics. The open-set classification scenario allows the system to handle previously unseen generators while maintaining detection and attribution capabilities.

## Key Results
- Detection accuracy of 91.3%, representing a 5.2 percentage point improvement over existing methods
- Source attribution improvements of 14.70% in AUC and 4.27% in OSCR for open-set classification
- Effective few-shot learning performance requiring only 150 images per class

## Why This Works (Mechanism)
The approach leverages supervised contrastive learning to create discriminative embeddings that capture generator-specific artifacts and patterns. By training the model to distinguish between different generators while maintaining intra-class similarity, the learned embeddings become highly informative for both detection and attribution tasks. The k-nearest neighbors classifier then operates on these semantically rich embeddings without requiring complex decision boundaries. The few-shot learning paradigm ensures the model can adapt to new generators with minimal data, crucial for keeping pace with rapidly evolving generative AI landscape.

## Foundational Learning
- **Supervised Contrastive Learning**: A metric learning approach that pulls embeddings of the same class together while pushing apart embeddings of different classes - needed for creating discriminative representations; quick check: verify contrastive loss implementation and temperature parameter
- **MambaVision Backbone**: A vision transformer architecture using Mamba blocks for efficient sequence modeling - needed for capturing long-range dependencies in images; quick check: confirm attention mechanism configuration and receptive field size
- **Few-Shot Learning**: Learning paradigm where models generalize from very limited training examples - needed to handle data scarcity and adapt to new generators; quick check: verify sample selection strategy and evaluation protocol
- **Open-Set Classification**: Classification scenario where test data may contain classes unseen during training - needed for practical deployment against novel generators; quick check: confirm evaluation metric implementation (OSCR, AUC)
- **k-Nearest Neighbors Classification**: Non-parametric classification method using distance in embedding space - needed for simple, effective decision making on learned representations; quick check: verify distance metric and k parameter
- **Generative Model Artifacts**: Subtle patterns and inconsistencies left by different AI image generators - needed for distinguishing between sources; quick check: examine embedding distributions for different generators

## Architecture Onboarding
- **Component Map**: Raw Images -> MambaVision Backbone -> Supervised Contrastive Loss -> Embedding Space -> k-NN Classifier -> Detection/Attribution Output
- **Critical Path**: Image input → MambaVision feature extraction → Contrastive embedding learning → k-NN classification → final prediction
- **Design Tradeoffs**: The choice of MambaVision versus traditional transformers trades computational efficiency for potentially different feature extraction capabilities; few-shot learning trades extensive training for rapid adaptation
- **Failure Signatures**: Performance degradation on novel generators not seen during contrastive training; reduced accuracy with aggressive image compression or resizing; misclassification when generators share similar training data distributions
- **First Experiments**: 1) Ablation study removing contrastive loss to assess its contribution; 2) Training with varying numbers of few-shot samples (50, 150, 300) to find optimal trade-off; 3) Testing on completely unseen generators to evaluate generalization

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reported improvements are based on specific datasets and generator combinations that may not generalize to all real-world scenarios
- The framework's reliance on MambaVision has not been extensively validated against other state-of-the-art vision architectures
- The few-shot learning paradigm introduces potential variability depending on which samples are selected for training

## Confidence
- Detection accuracy improvement (91.3%, +5.2pp): Medium confidence - results are compelling but dataset-specific
- Source attribution gains (14.70% AUC, 4.27% OSCR): Medium confidence - metrics are meaningful but evaluation conditions need broader validation
- Few-shot learning effectiveness: Medium confidence - promising results but limited exploration of sample size sensitivity

## Next Checks
1. Cross-dataset generalization testing using completely independent generator models not seen during any training phase, including recently released models
2. Ablation studies comparing MambaVision against other modern backbones (ConvNeXt, Swin Transformer) to isolate the contribution of architecture versus contrastive learning
3. Robustness evaluation against common image manipulations (compression, resizing, filtering) and adversarial attacks specific to forensic detection systems