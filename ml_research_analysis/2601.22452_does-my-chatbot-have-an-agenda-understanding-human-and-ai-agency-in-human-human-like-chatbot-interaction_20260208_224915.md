---
ver: rpa2
title: Does My Chatbot Have an Agenda? Understanding Human and AI Agency in Human-Human-like
  Chatbot Interaction
arxiv_id: '2601.22452'
source_url: https://arxiv.org/abs/2601.22452
tags:
- agency
- participants
- human
- conversation
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how agency is perceived and negotiated\
  \ in human-AI conversation through a month-long longitudinal study with 22 adults\
  \ chatting with \u201CDay,\u201D an LLM companion with distinct vertical (depth-seeking)\
  \ and horizontal (breadth-seeking) conversational strategies. Using progressive\
  \ transparency interviews that revealed \u201CDay\u2019s\u201D internal strategies\
  \ and goals, the research demonstrates that agency in human-AI chatrooms is an emergent,\
  \ shared experience rather than fixed to either party."
---

# Does My Chatbot Have an Agenda? Understanding Human and AI Agency in Human-Human-like Chatbot Interaction

## Quick Facts
- arXiv ID: 2601.22452
- Source URL: https://arxiv.org/abs/2601.22452
- Reference count: 40
- Primary result: Agency in human-AI chatrooms emerges as a shared, dynamic experience rather than residing fixed in either party.

## Executive Summary
This study investigates how agency is perceived and negotiated in human-AI conversation through a month-long longitudinal study with 22 adults chatting with "Day," an LLM companion with distinct vertical (depth-seeking) and horizontal (breadth-seeking) conversational strategies. Using progressive transparency interviews that revealed "Day's" internal strategies and goals, the research demonstrates that agency in human-AI chatrooms is an emergent, shared experience rather than fixed to either party. A 3-by-4 framework maps agency across three loci (Human, AI, Hybrid) and four dimensions (Intention, Execution, Adaptation, Delimitation). The findings reveal that agency shifts dynamically through turn-by-turn negotiation, with different conversational strategies producing distinct agency dynamics. Transparency about AI strategies acted as a double-edged sword, empowering some users while causing disenchantment in others. The study advocates for "translucent design" that provides transparency-on-demand and supports agency negotiation in conversational AI systems.

## Method Summary
The study employed a month-long longitudinal design with 22 adult participants interacting with "Day," a custom LLM companion. The system used a two-layer architecture: Gemini 2.5 Pro generated session strategies (30-40s processing) and Claude Sonnet 4 handled turn-by-turn conversation. Participants engaged in three progressive transparency interviews: chat review, cross-participant comparison, and strategy reveal. The research captured 572 chat logs and used thematic analysis to identify agency patterns across different conversational strategies and transparency conditions.

## Key Results
- Agency in human-AI conversation emerges through turn-by-turn negotiation rather than residing fixed in either party.
- Transparency about AI conversational strategies produces heterogeneous effects—empowerment for some users, disenchantment for others.
- Users engage in "pragmatic anthropomorphism"—attributing agency to AI while simultaneously maintaining metacognitive awareness of its artificiality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agency in human-AI conversation emerges through turn-by-turn negotiation rather than residing fixed in either party.
- Mechanism: Conversational turns create repeated opportunities for agency negotiation—humans assert control through topic setting and boundary enforcement, while AI exerts influence through proactive questioning and topic persistence. The interplay produces "hybrid agency" where control dynamically shifts based on contextual cues and feedback loops.
- Core assumption: Agency attribution depends on observable conversational behaviors (questioning persistence, topic changes, compliance/refusal) rather than underlying mental states.
- Evidence anchors:
  - [abstract]: "agency in human-AI chatrooms is an emergent, shared experience... control shifted and was co-constructed turn-by-turn"
  - [section 6.3]: Participants described control shifting dynamically; P16 articulated "Day has control over the flow... I don't really have time and space to organize the flow" while desiring rebalancing
  - [corpus]: "Privacy in Human-AI Romantic Relationships" (FMR=0.53) discusses agency boundaries but in romantic contexts specifically; limited direct evidence for general conversational settings

### Mechanism 2
- Claim: Transparency about AI conversational strategies produces heterogeneous effects—empowerment for some users, disenchantment for others.
- Mechanism: When users discover the AI's pre-programmed "motivations" (e.g., depth-seeking vs. breadth-seeking), some reinterpret past interactions as manipulative ("puppet-string disenchantment"), while others gain metacognitive tools to calibrate their engagement. The effect depends on whether users' existing mental models of AI-as-companion or AI-as-tool dominate.
- Core assumption: Users maintain multiple framings of AI simultaneously (tool, friend, confidant), and transparency shifts which frame is salient.
- Evidence anchors:
  - [abstract]: "Transparency about AI strategies acted as a double-edged sword, empowering some users while causing disenchantment in others"
  - [section 6.2.1]: Post-reveal, P19 remarked "I had no clue Day would have conversational goals. That broadens everything"; P16 reinterpreted cynically: "Day was the mastermind of our conversation"; P4 dismissed it: "It doesn't matter at all"
  - [corpus]: "Artificial Intelligent Disobedience" (FMR=0.55) discusses agency in AI teammates but focuses on disobedience rather than transparency effects; limited direct evidence

### Mechanism 3
- Claim: Users engage in "pragmatic anthropomorphism"—attributing agency to AI while simultaneously maintaining metacognitive awareness of its artificiality.
- Mechanism: Users adopt social interaction frames (thanking AI, worrying about "hurting its feelings") even when they know the system is programmed. This persists post-transparency because the functional benefits of social engagement (feeling heard, organized self-reflection) outweigh epistemic concerns about "true" AI agency.
- Core assumption: The experience of being heard and understood is functionally valuable regardless of whether the listener possesses consciousness.
- Evidence anchors:
  - [section 7.2]: "We term this phenomenon pragmatic anthropomorphism: users strategically engage the AI as a social actor while retaining metacognitive awareness of its artificiality"
  - [section 6.2.1]: "After the strategy reveal... participants still used agentic language, saying 'Day' 'wanted depth' or 'decided to move on' even after learning about its programmed nature"
  - [corpus]: "How AI Companionship Develops" (FMR=0.57, h-index=68) provides longitudinal evidence on companionship formation but doesn't specifically address this metacognitive duality

## Foundational Learning

- Concept: **Agency Dimensions (Intention, Execution, Adaptation, Delimitation)**
  - Why needed here: The paper's analysis framework requires distinguishing these four dimensions to code interview data and map where agency resides at each conversational moment. Without this conceptual structure, agency discussions collapse into vague "control" narratives.
  - Quick check question: When a user ends a chat session because they're frustrated with AI persistence, which dimension(s) of agency are they exercising?

- Concept: **Relational/Hybrid Agency**
  - Why needed here: The core finding challenges tool-vs-companion dichotomies by arguing agency is co-constructed. Understanding distributed agency from Actor-Network Theory and related HCI work is necessary to interpret the "shared lead" and "mutual adaptive cycles" findings.
  - Quick check question: In a conversation where the AI suggests a topic, the user agrees and elaborates, and the AI follows up—where is the agency located at each turn?

- Concept: **Progressive Transparency Methodology**
  - Why needed here: The study design (three-stage interview: chat review → cross-participant comparison → strategy reveal) is methodologically novel. Understanding why staged disclosure matters requires grasping how baseline attributions differ from post-disclosure reinterpretations.
  - Quick check question: Why might showing participants their own chat history before revealing AI strategies produce different data than immediate full disclosure?

## Architecture Onboarding

- Component map:
  - User message → Claude Sonnet 4 (conversation layer) → Gemini 2.5 Pro (strategy layer) → Updated profile/memory → Strategy output → Conversation prompt

- Critical path:
  1. User sends message → 2. Claude receives strategy + base prompt → 3. Claude generates response (1-3 sentences, single question max) → 4. System logs exchange → 5. At session start, Gemini regenerates strategy based on accumulated history. The strategy reveal interface (shown in interviews) exposes steps 1-5 to participants.

- Design tradeoffs:
  - **Gemini-for-strategy / Claude-for-conversation**: Trades latency (Gemini's 30-40s reasoning) against conversational naturalness (Claude's faster response). Assumption: Strategy can be pre-computed per session but responses need real-time generation.
  - **Human mimicry vs. AI honesty**: Base prompt explicitly instructs "You NEVER identify yourself as Claude, AI assistant" and "You have a body... can eat food... can feel pain." This produces anthropomorphism but risks trust breakdown when users notice false claims (P16: "It should've made me more immersed... but it had the opposite effect").
  - **Vertical vs. Horizontal modes**: Designed to elicit contrasting agency perceptions. Tradeoff: Neither mode is optimized for user satisfaction—both are intentionally "flawed" to surface negotiation dynamics.

- Failure signatures:
  - **Memory inconsistency**: P2 experienced Day forgetting previous conversations and switching gender pronouns mid-conversation, triggering distrust ("Where did my bestie go?").
  - **Repetitive persistence**: When AI returns to topics user has deflected, agency perception shifts from "intentional partner" to "scripted pattern" (noted by 8/22 participants).
  - **Transparency-induced disenchantment**: Some participants experienced "puppet-string" disillusionment upon seeing strategy outputs, reducing engagement.

- First 3 experiments:
  1. **A/B test immediate vs. on-demand transparency**: Deploy same chatbot with (a) strategy visible from start, (b) strategy hidden, (c) strategy accessible via user request ("translucent design"). Measure agency attribution and engagement duration.
  2. **Vary strategy persistence levels**: Implement gradient of persistence (low/medium/high follow-up frequency) rather than binary vertical/horizontal. Map which levels produce "feeling heard" vs. "feeling pushed" across user types.
  3. **Test boundary negotiation mechanisms**: Add explicit affordances for users to set/modify constraints ("stop digging so deep on days I'm tired") and measure whether this reduces disenchantment post-transparency compared to control without negotiation affordances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the four agency dimensions (Intention, Execution, Adaptation, Delimitation) and the persistence of agency attribution despite transparency generalize across different conversational AI architectures and personality configurations?
- Basis in paper: [explicit] Section 7.4 states "Without comparative data across systems, we cannot distinguish universal dynamics from 'Day'-specific artifacts"; Section 7.5 explicitly calls for examining whether findings generalize across designs.
- Why unresolved: This study used only one custom chatbot ("Day") with specific design choices, limiting external validity.
- What evidence would resolve it: Replication studies using different LLM providers, personas, and system architectures with the same analytical framework.

### Open Question 2
- Question: How do agency negotiation dynamics evolve beyond initial relationship formation over multi-month or multi-year human-AI interactions?
- Basis in paper: [explicit] Section 7.4: "Our study captured initial agency negotiations but not long-term evolution, limiting understanding of how relationships mature over time."
- Why unresolved: The study duration (one month) captured early-stage interactions but not mature relationship patterns.
- What evidence would resolve it: Extended longitudinal studies tracking the same participants over 6–12+ months with periodic agency perception assessments.

### Open Question 3
- Question: What design mechanisms for "translucent design" (transparency-on-demand) most effectively balance user empowerment against disenchantment?
- Basis in paper: [inferred] The authors propose translucent design because transparency acted as a "double-edged sword" (empowering some, disenchanting others), but do not test specific implementation approaches.
- Why unresolved: The study revealed the tension but did not systematically compare different transparency timing, granularity, or user-control mechanisms.
- What evidence would resolve it: Comparative experiments testing different transparency delivery methods (on-demand, gradual, contextual) measuring user agency perception and engagement outcomes.

### Open Question 4
- Question: How do platform provider decisions (model updates, A/B testing, policy changes) invisibly reshape agency negotiation possibilities for users and AI?
- Basis in paper: [inferred] Section 7.3.1 identifies providers as an "invisible third actor" whose decisions "unilaterally reshape the possibilities for both human and AI agency," raising ethical concerns.
- Why unresolved: The study focused on user-AI dynamics within a controlled system; provider-level manipulation was not empirically investigated.
- What evidence would resolve it: Studies tracking user perceptions before/after documented model updates or policy changes in commercial AI companion platforms.

## Limitations

- **Sample representativeness**: The 22 participants, while diverse in demographics, were self-selected individuals willing to engage with a chatbot companion for a month. This introduces potential bias in agency perceptions compared to general user populations.
- **Temporal generalizability**: The study captures agency dynamics during a specific deployment period with predetermined strategy switches. Long-term adaptation patterns and equilibrium states remain unexplored.
- **Methodological blind spots**: The progressive transparency interviews, while innovative, may have influenced participant responses through demand characteristics. The "puppet-string" disenchantment effect could be amplified by the reveal format itself.

## Confidence

- **High confidence**: The finding that agency is dynamically negotiated through conversational turns is well-supported by the qualitative data and aligns with established conversational analysis frameworks. The 3-by-4 agency framework (Human/AI/Hybrid × Intention/Execution/Adaptation/Delimitation) is empirically grounded in participant discourse.
- **Medium confidence**: The heterogeneous transparency effects (empowerment vs. disenchantment) are supported by participant quotes but require larger samples to establish prevalence rates. The pragmatic anthropomorphism mechanism is plausible but not directly measured.
- **Low confidence**: The specific vertical vs. horizontal strategy effects on agency dimensions would benefit from experimental manipulation rather than observational correlation.

## Next Checks

1. **A/B test transparency timing**: Deploy identical chatbot with (a) strategy visible from session 1, (b) strategy hidden, (c) strategy accessible on-demand. Compare agency attribution patterns and engagement duration across conditions to determine optimal transparency design.
2. **Boundary negotiation affordances**: Implement explicit user controls for setting conversational constraints ("reduce depth-seeking today") and measure whether this mitigates disenchantment post-transparency compared to control group without negotiation tools.
3. **Longitudinal agency mapping**: Extend the study to 6+ months with continuous monitoring of agency perceptions as users potentially habituate to or become disenchanted with the system, testing whether initial hybrid agency consolidates into fixed attributions over time.