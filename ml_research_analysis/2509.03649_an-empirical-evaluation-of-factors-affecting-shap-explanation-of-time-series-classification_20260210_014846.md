---
ver: rpa2
title: An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series
  Classification
arxiv_id: '2509.03649'
source_url: https://arxiv.org/abs/2509.03649
tags:
- segmentation
- time
- series
- background
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of segmentation strategies on
  SHAP-based explanations for time series classification. The authors evaluate eight
  segmentation methods across multiple datasets and classifiers, comparing equal-length
  segmentation against more complex alternatives like ClaSP, information gain, and
  kernel-based methods.
---

# An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification

## Quick Facts
- arXiv ID: 2509.03649
- Source URL: https://arxiv.org/abs/2509.03649
- Authors: Davide Italo Serramazza; Nikos Papadeas; Zahraa Abdallah; Georgiana Ifrim
- Reference count: 30
- Primary result: Segment count determines explanation quality more than segmentation algorithm choice; equal-length segmentation often outperforms complex alternatives

## Executive Summary
This paper systematically evaluates how segmentation strategies affect SHAP-based explanations for time series classification. The authors test eight segmentation methods across multiple datasets and classifiers, finding that the number of segments has a greater impact on explanation quality than the specific segmentation algorithm used. A novel attribution normalization technique is introduced to ensure timepoint-level additivity of segment-based attributions, which consistently improves explanation scores. The study also reveals inconsistencies between two XAI evaluation measures, suggesting InterpretTime's effectiveness may be limited to deep learning classifiers.

## Method Summary
The paper evaluates SHAP explanations for time series classification by treating segments as super-features, reducing computational complexity from O(2^L) to O(2^n) where n is the number of segments. Eight segmentation algorithms are compared: equal-length, ClaSP, information gain, greedy Gaussian, NNSegment, binary segmentation, bottom-up segmentation, and kernel-based CPD. A length-weighted normalization technique redistributes segment-level SHAP attributions to timepoints to preserve additivity. Explanations are evaluated using InterpretTime (multi-distribution perturbation) and AUC Difference (opposite-class substitution). Experiments cover 11 time series datasets and multiple classifiers including Random Forest, XGBoost, neural networks, and specialized time series classifiers.

## Key Results
- Equal-length segmentation consistently outperforms most custom segmentation algorithms while being computationally simpler
- The number of segments has greater impact on explanation quality than the specific segmentation method chosen
- The proposed length-weighted normalization technique consistently improves attribution quality across all segmentation methods
- Background choice (zero vs. average) has minimal overall impact, though average background performs better in some cases
- Inconsistencies exist between InterpretTime and AUC Difference evaluation measures, suggesting InterpretTime may be limited to deep learning classifiers

## Why This Works (Mechanism)

### Mechanism 1
Segment count determines explanation quality more than segmentation algorithm choice. SHAP computational complexity scales with feature count, so segmentation reduces features from L timepoints to n segments, making SHAP tractable. The paper finds granularity (n) dominates over semantic boundary detection—equal-length segmentation matches or exceeds complex methods (ClaSP, IG, KernelCPD) across most datasets. This holds unless your time series has strongly regime-switching behavior where semantic boundaries are critical to interpretability.

### Mechanism 2
Length-weighted normalization preserves Shapley additivity and improves attribution scores for uneven segmentations. Without normalization, each timepoint f_i ∈ S gets φ_f_i = φ_S, violating additivity: ∑φ_f_i = |S| · φ_S ≠ φ_S. The proposed normalization assigns φ_f_i = φ_S / |S|, ensuring ∑φ_f_i = φ_S. This corrects importance ranking distortions for variable-length segments, though it assumes uniform importance distribution within segments.

### Mechanism 3
InterpretTime evaluation exhibits classifier-dependent reliability due to noise-augmentation assumptions. InterpretTime perturbs timepoints by injecting noise following attribution rankings. The method was originally designed with noise augmentation during training, which deep learning models tolerate but non-DL classifiers (Random Forest, QUANT, MiniRocket) may not. Without this augmentation, perturbation results vary unpredictably across perturbation strategies. This limits InterpretTime to neural networks.

## Foundational Learning

- **Shapley Values and Additivity**: Understanding that Shapley values should sum to the total contribution when aggregating features is essential for grasping the normalization argument. Quick check: If a segment has SHAP value 0.6 and contains 3 timepoints, what should the sum of the three timepoint attributions equal after proper normalization?

- **Time Series Segmentation as Feature Grouping**: Treating segments as super-features for SHAP is the paper's core strategy. You need to understand why this reduces computational cost from O(2^L) to O(2^n). Quick check: For a time series of length 1000, approximately how many segments would you need to reduce SHAP's feature space by 90%?

- **Perturbation-Based XAI Evaluation**: Both InterpretTime and AUCD evaluate explanations by corrupting inputs and measuring prediction decay. Understanding deletion/insertion curves and AUC metrics is essential for interpreting results. Quick check: In AUCD, what does a high AUIC (insertion) combined with a low AUDC (deletion) indicate about explanation quality?

## Architecture Onboarding

- **Component map**: Segmentation module (8 algorithms via sktime/ruptures/aeon) -> SHAP explainer (Shapley Value Sampling with segment-as-feature abstraction) -> Background selector (Zero vector vs. training-set average) -> Normalizer (Length-weighted redistribution of segment SHAP to timepoints) -> Evaluator (InterpretTime + AUCD)

- **Critical path**: 1) Choose classifier and train on dataset 2) Apply segmentation to each time series instance 3) Compute segment-level SHAP with chosen background 4) Normalize segment attributions to timepoint level 5) Evaluate with both InterpretTime and AUCD (do not rely on single metric)

- **Design tradeoffs**: Equal-length vs. semantic segmentation: Equal-length is faster and typically sufficient; semantic methods add computational cost without consistent gains (exception: highly non-stationary series like KLC). Zero vs. average background: Average provides more realistic baseline but zero is framework default; performance difference is usually marginal. Single-instance vs. sampled background: Sampling improves slightly but increases SHAP computation linearly—often not worth the cost.

- **Failure signatures**: InterpretTime shows flat S̄ curve or non-monotonic accuracy decay: Likely a non-DL classifier incompatible with noise-based perturbation; switch to AUCD. Normalization shows no improvement: Segmentation is already near-uniform (low entropy); this is expected behavior. Custom segmentation underperforms on long, noisy series: May be producing highly uneven segments; check normalized entropy.

- **First 3 experiments**: 1) Establish baseline: Run equal-length segmentation with 10-20 segments, zero background, normalized attributions on your dataset across at least 2 classifier types. Record both AUCSE and AUCD. 2) Test background sensitivity: Repeat with average background. If differences exceed 5% on either metric, dataset has non-zero-centered structure requiring average background. 3) Validate normalization effect: Compare normalized vs. unnormalized attributions on a non-uniform segmentation (e.g., ClaSP). Confirm improvement matches Figure 5 pattern; if not, inspect segment length distribution for extreme skew.

## Open Questions the Paper Calls Out

### Open Question 1
How can the background set used by SHAP be optimized to balance explanation quality with computational cost for time series classification? The authors limited their analysis to single-instance backgrounds due to linear computational cost of sampling. What evidence would resolve it: A study comparing various background sampling strategies (e.g., k-means centroids vs. random samples) that measures the trade-off between computational overhead and improvement in attribution faithfulness metrics.

### Open Question 2
Can InterpretTime be adapted to provide consistent and reliable evaluations for non-deep learning classifiers? The current implementation assumes models are robust to noise injection required for the perturbation process, an assumption that fails for classifiers like Random Forest or MiniRocket. What evidence would resolve it: A modified version of InterpretTime that uses perturbation strategies benign to tree-based/linear models, demonstrating consistent rankings with other metrics like AUC Difference.

### Open Question 3
How can the optimal number of segments be determined automatically for a given time series classification task? The paper finds that "the number of segments has a greater impact on explanation quality than the specific segmentation method," yet the authors used fixed or algorithm-determined counts without optimizing this hyperparameter. What evidence would resolve it: Experiments treating the segment count as a tunable hyperparameter to identify if an optimal density exists relative to time series length or dataset complexity.

## Limitations

- Dataset coverage primarily limited to UCR/UEA time series, which may not generalize to irregularly sampled or high-dimensional multivariate series
- Study focuses on interpretability rather than model performance, so it's unclear whether segmentation preferences would hold when explanation quality directly impacts downstream decision-making
- Evaluation framework relies on classifier-specific perturbations, making it difficult to establish universal explanation quality standards across different model architectures

## Confidence

- **High Confidence**: Segment count dominates segmentation method choice - supported by extensive across-dataset consistency and clear performance patterns
- **Medium Confidence**: Normalization technique's general improvement claims - while consistently positive, effect size varies significantly with segment uniformity
- **Low Confidence**: InterpretTime reliability critique - based on indirect evidence from performance patterns rather than controlled experiments

## Next Checks

1. **Cross-domain robustness test**: Apply the equal-length segmentation preference finding to non-UCR/UEA datasets (e.g., medical time series, sensor data) to verify the granularity-over-semantics principle holds across domains with different statistical properties.

2. **Classifier augmentation experiment**: Conduct controlled tests comparing InterpretTime performance on DL classifiers with and without noise augmentation during training to directly validate whether the noise robustness assumption drives the metric's reliability.

3. **Real-world interpretability study**: Partner with domain experts to evaluate whether equal-length segmentation's superior XAI metrics translate to better human understanding and decision-making compared to semantically-motivated segmentations in practical applications.