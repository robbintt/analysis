---
ver: rpa2
title: Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension
  using Large Language Models
arxiv_id: '2508.05581'
source_url: https://arxiv.org/abs/2508.05581
tags:
- features
- performance
- sedi
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can generate clinically meaningful,
  interpretable computable phenotypes (CPs) for hypertension-related conditions through
  iterative refinement. The study demonstrates that GPT-4o with a synthesize-execute-debug-instruct
  (SEDI) strategy can produce CPs achieving 0.85 AUPRC and 0.94 AUROC for apparent
  treatment-resistant hypertension, comparable to state-of-the-art ML methods while
  requiring significantly fewer training examples.
---

# Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models

## Quick Facts
- arXiv ID: 2508.05581
- Source URL: https://arxiv.org/abs/2508.05581
- Reference count: 40
- LLMs generate clinically meaningful, interpretable CPs for hypertension-related conditions with 0.85 AUPRC and 0.94 AUROC using iterative refinement

## Executive Summary
This study demonstrates that large language models can generate clinically meaningful and interpretable computable phenotypes for hypertension-related conditions through iterative refinement. The researchers used GPT-4o with a synthesize-execute-debug-instruct (SEDI) strategy to create CPs for apparent treatment-resistant hypertension that achieved performance metrics comparable to state-of-the-art machine learning methods. The approach produces interpretable Python programs that balance accuracy and transparency, offering a scalable alternative to traditional CP development methods that typically require extensive manual expert effort.

The iterative learning approach is particularly noteworthy because it requires significantly fewer training examples than traditional machine learning methods while maintaining high performance. Rich phenotype descriptions and expert-curated feature sets improve model performance, though the SEDI strategy enables competitive results even with minimal prompts. This represents a significant advancement in automated CP development, potentially reducing the time and expertise required to create clinically actionable phenotypes from EHR data.

## Method Summary
The study employed GPT-4o with a synthesize-execute-debug-instruct (SEDI) strategy to iteratively generate computable phenotypes for treatment-resistant hypertension. The process involved synthesizing phenotype definitions, executing them on EHR data, debugging errors, and refining instructions based on performance feedback. The method was evaluated using a dataset of 10,041 patients from the University of Michigan Health System, comparing the generated CPs against traditional machine learning approaches in terms of accuracy metrics including AUPRC and AUROC.

## Key Results
- GPT-4o with SEDI strategy achieved 0.85 AUPRC and 0.94 AUROC for apparent treatment-resistant hypertension CPs
- Performance metrics are comparable to state-of-the-art machine learning methods
- Iterative refinement process requires significantly fewer training examples than traditional ML approaches
- Generated CPs are interpretable Python programs that balance accuracy and transparency

## Why This Works (Mechanism)
The success of this approach stems from LLMs' ability to understand natural language phenotype descriptions and translate them into executable code. The SEDI iterative refinement strategy allows the model to progressively improve its outputs through execution feedback, similar to how human developers debug programs. This process leverages the LLM's reasoning capabilities to handle the complex logic required for phenotype definitions while maintaining interpretability through Python code generation.

## Foundational Learning
- **EHR Data Structure**: Understanding of structured clinical data formats and terminologies (ICD codes, medications, lab results) - needed for accurate phenotype definitions; quick check: can identify relevant data fields for hypertension
- **Computable Phenotype Logic**: Knowledge of temporal relationships and clinical criteria for treatment resistance - needed for accurate phenotype rules; quick check: can express multi-step clinical criteria in code
- **LLM Prompt Engineering**: Understanding of how to craft effective prompts for code generation - needed for quality output; quick check: can translate clinical requirements into actionable LLM instructions
- **Python Programming**: Basic programming concepts for data manipulation and logical operations - needed for executable CPs; quick check: can write simple data filtering and aggregation code

## Architecture Onboarding

**Component Map:** Clinical Phenotype Description -> LLM (GPT-4o) -> Python Code -> EHR Data -> Evaluation Metrics -> Feedback Loop

**Critical Path:** The critical execution path follows from phenotype description through LLM code generation to execution on EHR data, with performance evaluation feeding back into the SEDI refinement cycle.

**Design Tradeoffs:** The approach trades some potential accuracy (compared to highly optimized ML models) for interpretability and transparency. Using Python code generation enables debugging and modification but may limit optimization compared to specialized ML frameworks.

**Failure Signatures:** Poor phenotype descriptions lead to incorrect or incomplete code generation. Complex temporal logic may exceed the LLM's reasoning capabilities. Limited training data can result in overfitting to the specific institution's EHR schema.

**First Experiments:**
1. Test basic CP generation with simple, well-defined phenotypes to establish baseline performance
2. Evaluate the impact of different phenotype description richness on CP accuracy
3. Compare SEDI-generated CPs against manually curated CPs on the same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Single institution data from University of Michigan Health System limits generalizability to other healthcare systems
- Evaluation performed on relatively modest dataset of 10,041 patients; scaling to larger populations may reveal performance issues
- Focus exclusively on hypertension-related phenotypes; uncertain whether iterative learning approach generalizes to other clinical domains

## Confidence

**High Confidence:** LLMs can generate interpretable Python programs for CPs with competitive accuracy metrics (0.85 AUPRC, 0.94 AUROC)

**Medium Confidence:** SEDI strategy requires significantly fewer training examples than traditional ML methods (supported but needs direct comparison studies)

**Medium Confidence:** Rich phenotype descriptions improve performance (demonstrated but magnitude across different phenotype types unclear)

## Next Checks

1. External validation on multi-site EHR data from different healthcare systems to assess generalizability of generated CPs and performance metrics

2. Head-to-head comparison of SEDI-generated CPs against traditional expert-curated CPs on the same dataset to quantify efficiency gains and potential trade-offs in accuracy

3. Longitudinal analysis of CP performance over time to evaluate stability and identify any drift in model performance as new data patterns emerge in clinical practice