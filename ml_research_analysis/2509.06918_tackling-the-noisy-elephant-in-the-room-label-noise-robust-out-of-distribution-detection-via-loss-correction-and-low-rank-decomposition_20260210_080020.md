---
ver: rpa2
title: 'Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution
  Detection via Loss Correction and Low-rank Decomposition'
arxiv_id: '2509.06918'
source_url: https://arxiv.org/abs/2509.06918
tags:
- detection
- label
- fpr95
- noise
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of robust out-of-distribution (OOD)
  detection under noisy training labels, which is a critical challenge for real-world
  deployment of AI systems. The authors propose NOODLE, a novel framework that integrates
  loss correction techniques with low-rank and sparse decomposition methods to address
  this challenge.
---

# Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition

## Quick Facts
- arXiv ID: 2509.06918
- Source URL: https://arxiv.org/abs/2509.06918
- Authors: Tarhib Al Azad; Shahana Ibrahim
- Reference count: 40
- Primary result: NOODLE framework achieves up to 12.5% improvement in FPR@95 over state-of-the-art OOD detection methods under severe label noise

## Executive Summary
This paper addresses the critical challenge of out-of-distribution detection when training data contains noisy labels, which is common in real-world applications. The authors propose NOODLE, a novel framework that integrates loss correction techniques from noisy label learning with low-rank and sparse decomposition of latent features. By decomposing the feature matrix into low-rank components (capturing class structure) and sparse components (handling outliers), NOODLE cleans the feature space to improve ID-OOD separability. Extensive experiments demonstrate significant performance improvements over state-of-the-art methods, particularly under severe label noise conditions.

## Method Summary
NOODLE combines loss correction (transition matrix or SCE loss) with low-rank sparse decomposition of latent features. The method extracts features from a DenseNet-101 backbone, applies power iteration to estimate top-K singular vectors, and decomposes the feature matrix into low-rank (H_ID) and sparse (H_OOD) components. The total loss includes a modified cross-entropy term plus a regularization term on H_OOD. At test time, kNN or Mahalanobis distance is computed on the cleaned low-rank features to detect OOD samples. The approach is trained for 100 epochs with SGD, batch size 64, and λ regularization parameter.

## Key Results
- On CIFAR-10 with 50% synthetic label noise, NOODLE achieves average FPR@95 of 27.34%, representing up to 12.5% improvement over best baseline
- NOODLE shows consistent improvements across real-world noisy datasets including CIFAR-10N, Animal-10N, and CIFAR-100N
- The framework maintains strong OOD detection performance while preserving high in-distribution classification accuracy
- UMAP visualizations confirm NOODLE better preserves ID-ness characteristics and improves ID-OOD separability

## Why This Works (Mechanism)

### Mechanism 1: Loss Correction Reduces Gradient Contamination from Mislabelled Samples
Standard cross-entropy forces the model to fit noisy labels, corrupting gradients. Loss correction modifies the training loss to reduce this influence. Transition matrix methods model Pr(ỹ=k|y=k') to correct the loss, while robust losses like SCE blend CE with MAE-like robustness to outliers. This reduces gradient corruption from mislabelled samples.

### Mechanism 2: Low-rank Plus Sparse Decomposition Recovers Class-Structured Features from Corrupted Representations
In-distribution features naturally cluster by class, which is approximately low-rank. Label noise scatters these clusters. By solving H ≈ L + S (nuclear norm on L, column-sparsity on S), the method projects features onto a subspace that preserves "ID-ness" while isolating noise-driven outliers.

### Mechanism 3: Distance-Based Scoring on Cleaned Features Improves ID-OOD Separability
OOD detection scores (kNN distance to ID embeddings) rely on compact, well-separated ID clusters. Loss correction alone does not guarantee clean features. The low-rank projection explicitly enforces cluster compactness, making distance-based scores more discriminative.

## Foundational Learning

- **Label Noise in Deep Learning**: Understanding how noisy labels corrupt feature representations is prerequisite. Quick check: Can you explain why symmetric label noise at 50% is more damaging to feature clusterability than 10% noise?

- **Out-of-Distribution Detection Scoring Functions**: NOODLE uses kNN and Mahalanobis scores; understanding their assumptions is essential. Quick check: Why does a kNN-based score fail when training features are scattered due to label noise?

- **Low-rank and Sparse Matrix Decomposition**: The core innovation; understanding nuclear norm minimization and column-sparsity is key. Quick check: In H ≈ L + S, what does the low-rank term L capture, and what does the sparse term S capture in the context of label noise?

## Architecture Onboarding

- **Component map**: DenseNet-101 -> Feature extraction (penultimate layer + GAP + ℓ2-norm) -> Loss correction + low-rank regularization -> Low-rank decomposition (power iteration) -> H_ID = (QQ⊤)H, H_OOD = H - H_ID -> Total loss computation -> Backward pass

- **Critical path**: Training: forward pass → feature extraction → loss correction + low-rank regularization → backward pass. Inference: feature extraction → projection via Q → distance computation to stored ID embeddings → threshold comparison.

- **Design tradeoffs**: Loss correction: transition matrix (CM) vs. robust loss (SCE). CM better at low noise, SCE may scale better to many classes. Distance metric: kNN (non-parametric, robust) vs. Mahalanobis (parametric, assumes Gaussian). Rank K: set to number of classes; for fine-grained datasets, may need adjustment.

- **Failure signatures**: OOD samples incorrectly flagged as ID (low FPR95): cleaned features may be over-compressed, or OOD lies in ID subspace. ID samples misclassified as OOD (low accuracy): over-sparsification or rank K too low. No improvement over baseline: noise rate too high, or noise structure violates assumptions.

- **First 3 experiments**: 1) Sanity check on clean data: Run NOODLE on CIFAR-10 with 0% noise. 2) Ablation on noise levels: Test NOODLE (CM + kNN) vs. CM-only vs. kNN-only on CIFAR-10 with symmetric noise at 10%, 30%, 50%. 3) Component ablation: On CIFAR-10N, compare NOODLE(CM, kNN), NOODLE(SCE, kNN), NOODLE(CM, Mahalanobis).

## Open Questions the Paper Calls Out

- **Architectural generalization**: The paper only tests DenseNet-101 without exploring other architectures. Performance may vary significantly across different network designs.

- **Noise pattern diversity**: The synthetic experiments use symmetric noise, which may not capture the complexity of realistic noise distributions. Real-world noise often exhibits class-conditional or instance-dependent patterns.

- **Large-scale scalability**: While the method uses efficient power iteration, the relationship between rank K selection and class count remains heuristic, and performance at extreme label space sizes is untested.

## Limitations

- The paper does not specify the learning rate for SGD optimization, which could affect training stability
- The exact number of power iterations for low-rank decomposition is unspecified, potentially impacting decomposition quality
- Results heavily depend on hyperparameter λ for regularization strength, with optimal values varying across datasets
- The method assumes ID features have low intrinsic rank, which may not hold for highly diverse datasets

## Confidence

- High confidence in the core mechanism of combining loss correction with low-rank decomposition
- Medium confidence in the effectiveness of specific loss correction methods (transition matrix vs SCE) across all noise regimes
- Medium confidence in the scalability to very large label spaces (e.g., CIFAR-100N) given noted computational challenges
- Low confidence in the method's performance when noise is instance-dependent rather than class-conditional

## Next Checks

1. **Sanity check on clean data**: Run NOODLE on CIFAR-10 with 0% noise to verify that low-rank decomposition doesn't degrade OOD detection vs baseline kNN

2. **Ablation study on noise levels**: Test NOODLE (CM + kNN) vs CM-only vs kNN-only on CIFAR-10 with symmetric noise at 10%, 30%, 50% to quantify each component's contribution

3. **Component sensitivity analysis**: On CIFAR-10N, compare all four NOODLE variants (CM/kNN, CM/Mahalanobis, SCE/kNN, SCE/Mahalanobis) to identify optimal configuration for different noise settings