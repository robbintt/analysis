---
ver: rpa2
title: 'OpenECG: Benchmarking ECG Foundation Models with Public 1.2 Million Records'
arxiv_id: '2503.00711'
source_url: https://arxiv.org/abs/2503.00711
tags:
- data
- datasets
- learning
- dataset
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents OpenECG, a benchmark of 1.2 million 12-lead
  ECG recordings from nine centers, to evaluate ECG foundation models (ECG-FMs) trained
  on public datasets. The research investigates three self-supervised learning methods
  (SimCLR, BYOL, MAE) with ResNet-50 and Vision Transformer architectures, assessing
  model generalization through leave-one-dataset-out experiments and data scaling
  analysis.
---

# OpenECG: Benchmarking ECG Foundation Models with Public 1.2 Million Records

## Quick Facts
- arXiv ID: 2503.00711
- Source URL: https://arxiv.org/abs/2503.00711
- Reference count: 3
- One-line primary result: Publicly available ECG data can match or surpass proprietary datasets in training robust ECG foundation models, with BYOL and MAE achieving superior generalization and data efficiency.

## Executive Summary
This study introduces OpenECG, a benchmark of 1.2 million 12-lead ECG recordings from nine centers, to evaluate ECG foundation models (ECG-FMs) trained on public datasets. The research investigates three self-supervised learning methods (SimCLR, BYOL, MAE) with ResNet-50 and Vision Transformer architectures, assessing model generalization through leave-one-dataset-out experiments and data scaling analysis. Results demonstrate that pre-training on diverse datasets significantly improves generalization, with BYOL and MAE outperforming SimCLR. Data scaling experiments reveal performance saturation at 60-70% of total data for BYOL and MAE, while SimCLR requires more data. These findings show that publicly available ECG data can match or surpass proprietary datasets in training robust ECG-FMs, enabling scalable, clinically meaningful AI-driven ECG analysis.

## Method Summary
The study benchmarks ECG foundation models using 1.2 million 12-lead ECG recordings from nine public datasets. All ECGs are resampled to a fixed format (n, 12, 1000) and split by patient to prevent leakage. Three self-supervised learning methods—SimCLR (contrastive), BYOL (feature consistency), and MAE (generative)—are evaluated with ResNet-50 and Vision Transformer backbones. Models are pre-trained on aggregated datasets and tested via leave-one-dataset-out experiments to assess generalization. Performance is measured using F1 score and AUROC for 24-class rhythm classification. Data scaling experiments (1%-100%) identify saturation points for different SSL methods.

## Key Results
- BYOL and MAE outperform SimCLR in ECG foundation model pre-training, achieving superior generalization across diverse datasets.
- Performance saturates at 60-70% of total data for BYOL and MAE, while SimCLR requires more data for optimal performance.
- Multi-center pre-training on heterogeneous datasets significantly improves model generalization to unseen clinical environments.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-center pre-training data diversity improves generalization to unseen clinical environments.
- **Mechanism:** Aggregating data from nine distinct centers (different demographics, hardware, and noise profiles) forces the encoder to learn invariant representations rather than overfitting to site-specific artifacts. This acts as a regularizer, reducing the domain shift when applied to a new "left-out" center.
- **Core assumption:** The variance in signal characteristics across public datasets (e.g., MIMIC vs. Chapman) is representative of real-world deployment variance.
- **Evidence anchors:**
  - [abstract] "...pre-training on diverse datasets significantly improves generalization..."
  - [section IV] "The iterative leave-one-dataset-out (LODO) experiment highlights that models trained on more heterogeneous datasets tend to generalize better..."
  - [corpus] "Sensing Cardiac Health..." supports the premise that heterogeneous multi-modal data is foundational for robust cardiac monitoring.
- **Break condition:** If the target deployment site has a signal distribution radically different from all nine source centers (e.g., fundamentally different lead placement or sampling physics), generalization may degrade.

### Mechanism 2
- **Claim:** Feature-consistency (BYOL) and generative (MAE) objectives are more data-efficient for ECGs than contrastive (SimCLR) objectives.
- **Mechanism:** SimCLR relies on distinguishing positive pairs from many negative samples, requiring large batch sizes and data volume to avoid collision. BYOL (feature consistency) and MAE (reconstruction) do not rely on negative samples, allowing them to converge on meaningful representations with significantly less data (saturation at 60-70% of dataset).
- **Core assumption:** ECG features are structured enough that masking (MAE) or augmentation-invariance (BYOL) alone provides a sufficient supervisory signal without negative pairs.
- **Evidence anchors:**
  - [abstract] "Data scaling experiments reveal performance saturation at 60-70% of total data for BYOL and MAE, while SimCLR requires more data."
  - [section V] "...contrastive learning models rely more heavily on large datasets, whereas feature-consistency and generative models are more data-efficient."
  - [corpus] "Representation Learning via Non-Contrastive Mutual Information" supports the efficacy of non-contrastive strategies in reducing data requirements.
- **Break condition:** If the ECG signal is severely truncated or lacks structural redundancy, MAE’s reconstruction task may become trivial or impossible, failing to learn high-level semantics.

### Mechanism 3
- **Claim:** Data scaling saturation acts as a signal of data quality sufficiency for specific SSL architectures.
- **Mechanism:** The performance plateau observed in BYOL/MAE suggests that the model has extracted the maximal variance information available in the public data corpus. Unlike SimCLR, which continues to scale, these models indicate a "diminishing returns" point where more data does not yield new features, suggesting the bottleneck has shifted from data volume to model capacity or data quality.
- **Core assumption:** The saturation point is not caused by model under-fitting (capacity issues) but by the exhaustion of unique information in the dataset.
- **Evidence anchors:**
  - [abstract] "...performance saturates at 60-70% of total data for BYOL and MAE..."
  - [section V] "...simply increasing dataset size is not always beneficial—data quality, diversity, and augmentation strategies play a crucial role..."
  - [corpus] Weak direct evidence in corpus for this specific saturation metric; reliance is primarily on the paper's Section V results.
- **Break condition:** If the model capacity is significantly increased (e.g., larger ViT), the saturation point may shift, invalidating the 60-70% threshold.

## Foundational Learning
- **Concept: Self-Supervised Learning (SSL) Paradigms**
  - **Why needed here:** The benchmark utilizes unlabeled ECG records (e.g., MIMIC, CODE-15) to pre-train models. Distinguishing between *Contrastive* (SimCLR), *Non-Contrastive* (BYOL), and *Generative* (MAE) is required to interpret the performance differences.
  - **Quick check question:** Does the method require negative pairs (SimCLR), negative-free consistency (BYOL), or input reconstruction (MAE)?

- **Concept: 12-Lead ECG Standardization**
  - **Why needed here:** The paper unifies diverse datasets into a fixed format `(n, 12, 1000)`. Understanding that ECGs are multi-channel time-series (12 leads) representing heart electrical activity is essential for understanding why "Lead Masking" and "Temporal Masking" are valid augmentation strategies.
  - **Quick check question:** Can you explain why masking 100 consecutive points across all 12 leads simulates real-world signal loss?

- **Concept: Linear Probing vs. Fine-Tuning**
  - **Why needed here:** The paper evaluates "generalization" often by how well the pre-trained encoder transfers. In practice, this involves freezing the backbone (probing) or updating it (fine-tuning).
  - **Quick check question:** If a model saturates at 60% data during pre-training, should you prioritize fine-tuning the full model or just training a linear classifier on top for the downstream task?

## Architecture Onboarding
- **Component map:**
  - Input: Unified 12-lead tensor `(Batch, 12, 1000)`
  - Backbone: ResNet-50 (paired with SimCLR/BYOL) or Vision Transformer ViT (paired with MAE)
  - Pre-training Heads: Projection heads for SimCLR/BYOL; Decoder for MAE reconstruction
  - Downstream: Classification head for 24 SNOMED CT classes

- **Critical path:**
  1. Data Curation: Download 9 datasets; resample all to 500Hz (or unified length)
  2. Preprocessing: Reshape to `(n, 12, 1000)`; apply patient-level splits to prevent leakage
  3. Pre-training: Select SSL method (e.g., BYOL + ResNet-50). Train on 60-70% of aggregated data (efficiency cutoff)
  4. Evaluation: Run Leave-One-Dataset-Out (LODO) to test generalization

- **Design tradeoffs:**
  - SimCLR vs. MAE: Use SimCLR if data volume is massive and compute allows for large batch sizes; use MAE/BYOL for faster convergence and smaller compute budgets
  - Architecture: ResNet-50 is a robust default for local temporal features; ViT is better for long-range global dependencies but requires more data (unless pre-trained heavily)

- **Failure signatures:**
  - High Variance in LODO: If performance collapses on specific left-out datasets (e.g., INCART), the model has overfit to the majority distribution
  - Non-convergence in MAE: If reconstruction loss plateaus early without downstream accuracy, the model is likely focusing on low-frequency baseline wander rather than diagnostic morphological features

- **First 3 experiments:**
  1. Baseline LODO: Train a ResNet-50 Randomly Initialized vs. BYOL Pre-trained on the 8 datasets, testing on the 9th (e.g., Chapman). Compare AUROC
  2. Data Scaling Check: Train the BYOL model on 10%, 30%, 60%, and 100% of the MIMIC/CODE-15 subset. Plot the AUROC curve to verify the 60-70% saturation claim
  3. Masking Robustness: Evaluate MAE model on test sets with synthetic noise/zeroing applied. Compare against SimCLR to validate the "robustness to incomplete recordings" claim

## Open Questions the Paper Calls Out
- **Question:** Does integrating multi-modal data (clinical notes, imaging) significantly enhance ECG foundation model performance compared to signal-only training?
  - **Basis in paper:** [explicit] The Discussion states: "The next steps involve extending OpenECG to incorporate multi-modal data, such as clinical notes, imaging, and genetic information..."
  - **Why unresolved:** The current OpenECG benchmark is restricted to 12-lead ECG signals, excluding the rich clinical context available in EHRs.
  - **What evidence would resolve it:** Comparative benchmarks showing performance deltas between signal-only and multi-modal models on identical downstream tasks.

- **Question:** Can active learning or selective sampling strategies push model performance beyond the saturation points observed with random data scaling?
  - **Basis in paper:** [explicit] The authors note performance saturates at 60-70% of data and suggest "future work should explore optimal dataset scaling strategies, including active learning approaches..."
  - **Why unresolved:** The current scaling experiments used random subsets, potentially leaving "hard" or informative samples unused while adding redundant data.
  - **What evidence would resolve it:** Experiments demonstrating that targeted sampling (active learning) yields higher accuracy than random sampling at equivalent data volumes.

- **Question:** Do domain adaptation techniques effectively mitigate the performance fluctuations and overfitting observed in smaller, heterogeneous test sets?
  - **Basis in paper:** [explicit] The Discussion identifies performance fluctuations in PTB and Georgia datasets and suggests investigating "domain adaptation techniques to enhance ECG-FM efficiency."
  - **Why unresolved:** While the study highlights generalization issues (domain shift) across centers, it does not implement or test specific domain adaptation solutions to resolve them.
  - **What evidence would resolve it:** Improved stability and AUROC scores on leave-one-dataset-out experiments when domain adaptation layers are applied during fine-tuning.

## Limitations
- The study assumes that variance captured by nine source centers is representative of real-world clinical deployment, which may not hold for edge cases like different hardware or patient populations.
- Findings are limited to rhythm classification using SNOMED CT labels, leaving uncertainty about generalization to other ECG tasks like morphology analysis or anomaly detection.
- The claim that data quality and diversity are more important than volume for BYOL and MAE lacks strong empirical validation beyond the 60-70% saturation point observed.

## Confidence
- **High confidence:** Superiority of BYOL and MAE over SimCLR for ECG pre-training, supported by data scaling experiments and LODO results.
- **Medium confidence:** Multi-center pre-training improves generalization, but extent varies depending on target deployment environment.
- **Low confidence:** Data quality and diversity are more important than volume for BYOL and MAE; saturation point at 60-70% requires validation with larger datasets or different model architectures.

## Next Checks
1. **Cross-hardware validation:** Test pre-trained models on ECG data from hardware not represented in the original nine centers (e.g., different manufacturers or lead configurations) to verify domain generalization claims.
2. **Task generalization:** Evaluate foundation models on downstream tasks beyond rhythm classification, such as morphology-based anomaly detection or arrhythmia subtype identification.
3. **Scaling boundary test:** Train BYOL and MAE on datasets larger than 1.2 million recordings to determine if the 60-70% saturation point holds or shifts with increased data volume.