---
ver: rpa2
title: Linear Correlation in LM's Compositional Generalization and Hallucination
arxiv_id: '2502.04520'
source_url: https://arxiv.org/abs/2502.04520
tags:
- correlation
- knowledge
- linear
- city
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization ability of language
  models by uncovering a phenomenon of linear correlations in next token prediction
  logits. The authors find that certain related knowledge pairs exhibit a linear transformation
  between their output logits, which mirrors the linearity in human knowledge composition.
---

# Linear Correlation in LM's Compositional Generalization and Hallucination

## Quick Facts
- arXiv ID: 2502.04520
- Source URL: https://arxiv.org/abs/2502.04520
- Reference count: 40
- One-line primary result: Compositional generalization in LMs manifests as resilient linear correlations in next-token logits, where precise transformations enable correct generalization and imprecise ones cause hallucinations.

## Executive Summary
This paper uncovers a phenomenon of linear correlations in language models' next token prediction logits for related knowledge pairs. The authors demonstrate that certain compositional relationships (like city→country mappings) can be expressed as linear transformations between logit vectors, and this correlation persists through fine-tuning. They show that successful generalization requires both high correlation intensity and precision of the transformation matrix, while imprecise transformations lead to compositional hallucinations. The work also reveals that these linear correlations can be learned from pre-trained vocabulary representations alone, indicating that LM generalization heavily relies on the semantic structure of the embedding space.

## Method Summary
The method involves extracting next-token logits for pairs of compositional prompts (e.g., "X lives in the city of" and "X lives in the country of") across 10K input examples, then fitting a linear transformation (W, b) between the logit vectors using least squares regression on half the data and evaluating on the remainder. The authors measure label-wise Pearson correlation between predicted and actual logits, and analyze the precision of learned transformation weights by comparing them to ground-truth knowledge pairs. They also create a simplified probe model using mean-pooling and a single feedforward layer to test whether pre-trained vocabulary representations alone can capture these correlations.

## Key Results
- Linear transformations (W, b) can achieve Pearson correlations >0.8 between related knowledge pairs' logits
- Correlations persist through fine-tuning but can be disrupted by imprecise transformation weights
- Simplified probe models using pre-trained vocabulary embeddings achieve 97.66% generalization when mappings are correct, dropping to 22.66% when mappings are scrambled
- Successful generalization requires both high correlation intensity and precise transformation weights; imprecise weights cause compositional hallucinations

## Why This Works (Mechanism)

### Mechanism 1
Knowledge composition in language models manifests as a linear transformation between output logits of related knowledge pairs. For compositional prompts (e.g., "X lives in the city of" → "X lives in the country of"), a transformation matrix (W, b) can be fit to map source logits to target logits. This is found by sampling logit pairs for many inputs X, fitting on half, and testing on the rest. The weights in W often mirror real-world relationships. The linear relationship is weak or non-existent for knowledge pairs with low real-world correlation (e.g., City→Gender), making a precise linear transformation impossible to fit.

### Mechanism 2
Compositional generalization and hallucination are driven by the interaction of linear correlation intensity and the precision of the transformation matrix W. Updates to source knowledge propagate to target knowledge via the resilient linear correlation. If the correlation is high and W is precise (W assigns high weights to correct real-world pairs), the model generalizes correctly. If the correlation is high but W is imprecise, the update propagates an error, causing compositional hallucination (e.g., learning "Indianapolis" -> "India"). This mechanism does not explain generalization for knowledge pairs with low linear correlation, where the paper shows performance is near random.

### Mechanism 3
The linear correlation is largely caused by and can be learned from pre-trained vocabulary representations. A simplified model (mean-pooling + single feedforward layer) using pre-trained vocabulary embeddings can learn similar linear correlations for knowledge composition. Performance drops significantly when the semantic mapping in the vocabulary is scrambled (e.g., Paris→Japan), indicating the signal is in the embeddings, not the complex transformer architecture. This causal link might be weaker for knowledge domains where the vocabulary is less semantically structured or for relationships that require deep syntactic processing beyond simple semantic association.

## Foundational Learning

- **Next Token Prediction (NTP) Logits**
  - Why needed here: The paper's entire mechanism is built on analyzing the vector of logits for the next token, not the final probability. The linear correlation is found in this logit space.
  - Quick check question: Can you explain why the authors analyze logits instead of probabilities for fitting the linear transformation (W, b)? (Hint: See Eq. 1 & 3).

- **Linear Transformation (W, b)**
  - Why needed here: This is the core mathematical object of study. Understanding how a matrix W and bias vector b can map one logit vector to another is essential.
  - Quick check question: If F_City(X) is the logit vector for the city prompt and F_Country(X) is for the country prompt, what is the equation relating them according to the paper's hypothesis?

- **Compositional Generalization**
  - Why needed here: This is the key capability being explained. It refers to the model's ability to correctly infer new knowledge (e.g., country) from learned knowledge (e.g., city) based on known relationships.
  - Quick check question: According to the paper, what are the two conditions required for successful compositional generalization to occur?

## Architecture Onboarding

- **Component map**: Source/Target Knowledge Prompts -> Logit Extractor -> Linear Fitter -> Precision Analyzer -> Generalization/Hallucination Analyzer
- **Critical path**: The experimental flow is: Generate Prompt Pairs → Extract Logits → Fit Linear Transformation (W, b) → Evaluate Correlation & Precision → Connect to Generalization/Hallucination
- **Design tradeoffs**: The paper focuses on a label-wise Pearson correlation to eliminate the effect of the global bias (b) and focus on the structure of W. The choice of prompt templates and vocabulary subdomains (e.g., ignoring subwords) is critical for getting clean, interpretable results.
- **Failure signatures**:
  - Low/No Linear Correlation: For knowledge pairs the model hasn't learned a relationship for (e.g., "Language→Continent"). The W matrix will be noisy and not generalize.
  - High Correlation, Low Precision (Hallucination): A strong, consistent linear pattern that maps to wrong knowledge (e.g., "X+1=2" → "X+2=2").
- **First 3 experiments**:
  1. **Correlation Discovery**: Pick a known compositional pair (City→Country). For multiple X, extract logits for both prompts. Fit a linear model on half the Xs and measure the Pearson correlation on the other half. Confirm a high score (>0.8).
  2. **Precision Check**: For the (W, b) learned in step 1, inspect the weights in W. For a given country (e.g., France), is the highest weight from its corresponding city (Paris)? Calculate Hit@Top-N.
  3. **Generalization vs. Hallucination**: Fine-tune the model on a single new city-country fact (e.g., "Z lives in the city of Newcity" where Newcity is in CountryA). Check if the model's probability for "Z lives in the country of CountryA" increases (generalization) or if it incorrectly increases for a different country due to a mis-weighted W (hallucination).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What formal theoretical mechanisms regarding optimization dynamics or linguistic structures drive the emergence of resilient linear correlations between knowledge pairs?
- **Basis in paper:** [explicit] Appendix B lists "Theoretical Explanation" as a limitation, noting the authors describe the phenomenon but do not provide a formal theory for why it emerges.
- **Why unresolved:** The paper empirically demonstrates the existence and utility of linearity but does not mathematically derive how specific architectures or training objectives inevitably lead to these linear transformations in the logit space.
- **What evidence would resolve it:** A theoretical framework or proof linking transformer depth, attention heads, or gradient descent dynamics to the linear alignment of output logits for related knowledge.

### Open Question 2
- **Question:** How do specific properties of the pre-training data distribution influence the intensity and precision of linear correlations?
- **Basis in paper:** [explicit] Appendix B explicitly states that the study does not systematically analyze how training data influences formation and lists "Data Distribution Effects" as a future work topic.
- **Why unresolved:** It is currently unclear if the observed correlations are an inherent structural property of the network or if they are contingent on the frequency and co-occurrence statistics of the training corpus.
- **What evidence would resolve it:** Experiments training models from scratch on controlled datasets with varying entity co-occurrence frequencies to observe changes in correlation intensity.

### Open Question 3
- **Question:** Can the precision of the linear transformation matrix $W$ be directly manipulated to mitigate compositional hallucinations without retraining?
- **Basis in paper:** [inferred] The paper establishes that imprecise weights in $W$ cause hallucinations (Section 5) and attributes the correlation to vocabulary representations (Section 6), but implies a need for methods to utilize this for "generalizable learning."
- **Why unresolved:** The paper serves as a diagnostic tool (identifying the linear correlation), but does not propose or test an intervention method to correct an imprecise $W$ to stop hallucinations.
- **What evidence would resolve it:** Successful model editing interventions that specifically adjust the vocabulary representations or transformation weights to align $W$ with real-world relations, thereby reducing error rates.

## Limitations
- The universality of linear transformations across knowledge domains remains unclear - the paper demonstrates success with city-country mappings but it's uncertain whether such clean linear relationships exist for more complex or abstract knowledge pairs.
- The analysis focuses on LLaMA-3-8B specifically, raising questions about generalizability to other model architectures or scales.
- The paper doesn't fully address how contextual variations in prompts might affect the stability of these linear correlations.

## Confidence
- **High Confidence**: The empirical demonstration that linear transformations can fit certain knowledge pairs with high correlation (Pearson >0.8) and that these correlations persist through fine-tuning.
- **Medium Confidence**: The claim that compositional hallucinations arise specifically from imprecise linear transformations rather than other failure modes.
- **Low Confidence**: The assertion that pre-trained vocabulary representations are the primary driver of linear correlations, as the simplified probe model experiments don't definitively rule out contributions from the transformer architecture itself.

## Next Checks
1. **Cross-Domain Generalization Test**: Apply the linear correlation analysis to knowledge pairs from different semantic domains (e.g., historical figures to their achievements, chemical elements to their properties) to test whether the observed phenomenon extends beyond geographic relationships.
2. **Ablation on Vocabulary Structure**: Systematically scramble the vocabulary embeddings in the simplified probe model while preserving distributional statistics to isolate whether the semantic geometry specifically drives the linear correlations.
3. **Contextual Robustness Evaluation**: Test whether the linear correlations persist when the prompt templates include varying contextual modifiers (e.g., "X might live in the city of" vs. "X definitely lives in the city of") to assess the stability of the transformation across different linguistic contexts.