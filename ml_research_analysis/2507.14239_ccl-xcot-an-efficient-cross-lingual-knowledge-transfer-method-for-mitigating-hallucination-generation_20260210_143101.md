---
ver: rpa2
title: 'CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating
  Hallucination Generation'
arxiv_id: '2507.14239'
source_url: https://arxiv.org/abs/2507.14239
tags:
- language
- languages
- cross-lingual
- learning
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucinations in multilingual
  large language models, particularly in low-resource languages. The authors propose
  CCL-XCoT, a two-stage fine-tuning framework.
---

# CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation

## Quick Facts
- arXiv ID: 2507.14239
- Source URL: https://arxiv.org/abs/2507.14239
- Authors: Weihua Zheng; Roy Ka-Wei Lee; Zhengyuan Liu; Kui Wu; AiTi Aw; Bowei Zou
- Reference count: 13
- Primary result: CCL-XCoT reduces hallucination rates by up to 62% and improves factual knowledge transfer across language pairs

## Executive Summary
This paper addresses hallucinations in multilingual large language models, particularly for low-resource languages. The authors propose CCL-XCoT, a two-stage fine-tuning framework that combines curriculum-based contrastive learning with cross-lingual Chain-of-Thought prompting. The method achieves significant reductions in hallucination rates while improving factual knowledge transfer across language pairs, without requiring external retrieval or multi-model ensembles.

## Method Summary
CCL-XCoT employs a two-stage fine-tuning approach. First, curriculum-based contrastive learning progressively aligns multilingual semantic spaces through sentence-level then paragraph-level contrastive learning, jointly optimized with Next-Token Prediction to maintain language modeling capabilities. Second, cross-lingual Chain-of-Thought (XCoT) instruction tuning guides the model to reason in a high-resource language (typically English) before generating answers in the target low-resource language. The framework also employs targeted mid-layer fine-tuning, identifying mid-level transformer layers as optimal for cross-lingual knowledge transfer.

## Key Results
- Reduces hallucination rates by up to 62% across evaluated language pairs
- Improves factual knowledge transfer between languages
- Achieves cross-lingual consistency improvements without external retrieval systems

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-Based Contrastive Semantic Alignment
The method applies contrastive learning at sentence level using large-scale parallel corpora, then at paragraph level with higher-quality data. Aligned translations are treated as positive pairs while non-aligned in-batch samples serve as negatives, pulling semantically equivalent texts closer in shared embedding space. This is jointly optimized with Next-Token Prediction to maintain general language modeling capabilities. The core assumption is that standard NTP pretraining creates fragmented representations across languages that contrastive learning can bridge in decoder-only models.

### Mechanism 2: Cross-Lingual Chain-of-Thought (XCoT) for Reasoning Transfer
XCoT scaffolds reasoning in low-resource languages by forcing the model to use a high-resource language as an intermediate reasoning trace. The instruction-tuning strategy decomposes generation into three steps: reason in English, answer in English, then respond in the target language. This bridges the linguistic gap during inference. The core assumption is that factual knowledge is language-agnostic but its retrieval and logical application are more robust in high-resource languages due to training data imbalances.

### Mechanism 3: Targeted Mid-Layer Fine-Tuning
The authors empirically show that cross-lingual knowledge transfer is most effective when fine-tuning is restricted to the model's mid-level transformer layers. Mid-level layers act as the core bridge between low-level linguistic features and high-order reasoning. Updating these layers optimizes semantic alignment while avoiding disruption to syntactic patterns or abstract reasoning. The core assumption is that transformer layers specialize hierarchically, with mid-level layers serving as the primary locus for cross-lingual semantic representation.

## Foundational Learning

**Cross-Lingual Semantic Alignment**
Why needed here: The core problem is that knowledge in one language doesn't automatically transfer to another. The method solves this by explicitly forcing embeddings of semantically identical texts to be close in vector space.
Quick check question: If a model has strong semantic alignment, would the cosine similarity between an English sentence embedding and its Tamil translation be high or low?

**Catastrophic Forgetting**
Why needed here: The methodology explicitly combines contrastive learning with Next-Token Prediction to mitigate this problem. Without this understanding, the rationale for the joint loss function is unclear.
Quick check question: What risk does training a model on a new specialized task pose to its previously learned general capabilities?

**Chain-of-Thought (CoT) Prompting**
Why needed here: The XCoT component builds directly on this principle, extending it to a cross-lingual setting where the reasoning steps occur in a different language than the final answer.
Quick check question: What is the primary benefit of having a model generate intermediate reasoning steps before producing a final answer?

## Architecture Onboarding

**Component map:**
Parallel Corpora (Sentence → Paragraph) -> Curriculum Contrastive Pretraining (NTP + Contrastive Loss) -> Mid-layer Fine-tuning (Layers 9-19/Gemma or 11-22/LLaMA) -> XCoT Instruction Data -> LoRA Fine-tuning (All Layers) -> Inference with XCoT Prompt Template

**Critical path:**
1. Acquire/create parallel corpora (Sentence & Paragraph level)
2. Apply curriculum-based contrastive pretraining, targeting mid-level layers
3. Construct instruction dataset with XCoT reasoning traces
4. Perform instruction fine-tuning on all layers
5. Evaluate hallucination-free rates and cross-lingual consistency

**Design tradeoffs:**
- Sentence vs. Paragraph Alignment: Sentence-level is cheaper but misses long-context alignment; paragraph-level is expensive but crucial for real-world tasks
- Layer Selection: Targeting mid-level layers is efficient and stable but may underperform on tasks requiring deep syntactic or high-level logical changes
- Pivot Language: Using English as a pivot is effective but assumes the model's English reasoning is reliable and culturally neutral

**Failure signatures:**
- Low Semantic Similarity Scores: Indicates failed Stage 1 alignment (check data quality)
- High Hallucination Rate in High-Resource Pivot Language: Suggests XCoT reasoning itself is flawed (check instruction data quality)
- Degraded Fluency in Target Language: May indicate catastrophic forgetting or over-alignment (increase NTP loss weight or reduce learning rate)

**First 3 experiments:**
1. Train a baseline model with SFT only on English data; evaluate hallucination-free rate in all target languages to quantify the initial cross-lingual gap
2. Train with NTP only, then with NTP + Contrastive Learning (sentence-level only); measure semantic similarity scores to validate alignment improvement
3. Fine-tune three separate model instances—updating (a) low-level, (b) mid-level, and (c) high-level layers only; compare hallucination-free rates to identify most effective parameter-efficient tuning strategy

## Open Questions the Paper Calls Out
The authors explicitly note that while the strategy reduces overall hallucination rates, it "lacks effective suppression mechanisms for more fine-grained hallucination types" like years or road numbers. They also observe that "its impact on enhancing capabilities for complex logical reasoning tasks is limited." Additionally, they propose that "future work may explore integrating multilingual retrieval augmentation" to support broader cross-lingual generalization.

## Limitations
- The method's effectiveness heavily depends on availability of high-quality parallel corpora, particularly for low-resource languages
- The strategy assumes reliable reasoning in the high-resource pivot language, which may not hold for all domains
- The method shows limited impact on complex logical reasoning tasks despite success on knowledge understanding tasks

## Confidence
- **High Confidence:** The two-stage framework is well-defined and addresses a clear problem; layer-wise analysis identifying mid-level layers is empirically supported
- **Medium Confidence:** The XCoT mechanism for transferring reasoning is plausible but relies on pivot language reasoning quality
- **Low Confidence:** The claim that mid-level layers are universally optimal may not generalize beyond tested model architectures

## Next Checks
1. Conduct an ablation study comparing models trained with CCL-XCoT using different levels of parallel corpus quality to isolate the impact of data fidelity
2. Evaluate CCL-XCoT using a non-English pivot language to test robustness of the reasoning transfer mechanism
3. Apply the mid-layer fine-tuning strategy to a different architecture (e.g., mBERT or XLM-R) to assess whether layer specialization is universal or model-specific