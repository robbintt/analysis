---
ver: rpa2
title: Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models
arxiv_id: '2512.15372'
source_url: https://arxiv.org/abs/2512.15372
tags:
- image
- retrieval
- complexity
- images
- icar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICAR enables vision transformers to use less compute for simple
  images while processing complex images through full network depth. The approach
  solves cross-modal alignment challenges through dual-path training that produces
  compatible embeddings from both early-exit and full-depth paths.
---

# Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models

## Quick Facts
- **arXiv ID**: 2512.15372
- **Source URL**: https://arxiv.org/abs/2512.15372
- **Reference count**: 40
- **Primary result**: 20% faster image encoding while maintaining 95% instance-level performance

## Executive Summary
ICAR introduces an adaptive retrieval framework that dynamically adjusts vision transformer depth based on image complexity, achieving computational efficiency without significant performance loss. The approach uses a dual-path training strategy where images are processed through either early-exit or full-depth paths, with cross-modal alignment ensuring compatibility between the resulting embeddings. A novel ConvNeXt-IC model trained specifically for image complexity assessment demonstrates state-of-the-art correlation with human judgment (0.959 Pearson) and enables 4.4x faster complexity prediction. When evaluated on standard vision-language benchmarks and real-world web data, ICAR achieves 20% speedup in image encoding while maintaining category-level performance and 95% of instance-level performance.

## Method Summary
ICAR implements a dynamic depth control mechanism for vision transformers that routes images through early-exit paths for simple images or full-depth paths for complex images based on complexity assessment. The dual-path training approach uses a weighted combination of cross-entropy loss for early-exit embeddings and mean squared error loss for full-depth embeddings to ensure cross-modal alignment. A novel ConvNeXt-IC model is trained on MS-COCO to predict image complexity using features from multiple ViT layers, achieving state-of-the-art performance with 0.959 Pearson correlation to human judgment. The complexity-aware routing is integrated with BLIP-2's Q-Former architecture, using a threshold-based approach to determine processing depth. The method maintains a fixed embedding dimension across both paths, enabling seamless integration with existing vision-language models.

## Key Results
- 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance
- ConvNeXt-IC achieves state-of-the-art 0.959 Pearson correlation with human judgment for complexity assessment
- 4.4x faster complexity prediction compared to previous methods
- Consistent performance improvements across 12 image complexity metrics on MS-COCO

## Why This Works (Mechanism)
ICAR works by recognizing that not all images require full transformer depth for accurate representation. Simple images can be effectively encoded through early exits, while complex images benefit from deeper processing. The dual-path training ensures that embeddings from both processing routes remain compatible for cross-modal alignment, solving the challenge of maintaining retrieval performance when using variable-depth processing. The ConvNeXt-IC model provides accurate, fast complexity assessment that enables appropriate routing decisions without introducing significant computational overhead.

## Foundational Learning

**Vision Transformer Depth Control**
- Why needed: Traditional transformers process all images through full depth regardless of complexity
- Quick check: Compare FLOPs for early-exit vs full-depth processing on sample images

**Cross-Modal Alignment**
- Why needed: Variable-depth processing creates embeddings of different quality that must remain compatible
- Quick check: Verify cosine similarity between early-exit and full-depth embeddings

**Image Complexity Assessment**
- Why needed: Objective metric needed to determine optimal processing depth for each image
- Quick check: Correlation between predicted complexity and human judgment scores

## Architecture Onboarding

**Component Map**
Q-Former -> Image Encoder (Dual Path: Early-Exit or Full-Depth) -> Cross-Modal Alignment -> Text Encoder -> Fusion

**Critical Path**
Image complexity assessment -> Routing decision -> Appropriate transformer path -> Cross-modal alignment -> Retrieval

**Design Tradeoffs**
Fixed embedding dimension across paths vs variable dimension (simplifies integration but may limit optimal representation)

**Failure Signatures**
Poor complexity assessment leads to incorrect routing; misalignment between paths degrades retrieval performance

**First Experiments**
1. Benchmark ConvNeXt-IC complexity prediction on MS-COCO vs human judgments
2. Test retrieval performance with only early-exit vs only full-depth paths
3. Measure computation savings from adaptive routing on mixed-complexity dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Potential overfitting to MS-COCO dataset distribution for complexity assessment
- 5% performance loss at instance level may be unacceptable for precision-critical applications
- Dual-path training increases model complexity and deployment challenges

## Confidence

**High Confidence Claims**
- ConvNeXt-IC achieves strong quantitative performance on established complexity benchmarks

**Medium Confidence Claims**
- Cross-modal alignment claims need broader vision-language application testing
- Sustainability benefits require direct energy measurement beyond theoretical speedup

**Low Confidence Claims**
- Scalability claims limited by evaluation on specific model sizes and dataset scales

## Next Checks

1. Evaluate ICAR on out-of-distribution datasets including medical imaging, satellite imagery, and artistic content to assess generalization of the complexity assessment.

2. Conduct ablation studies isolating the contribution of the dual-path training versus the complexity-based routing mechanism.

3. Measure actual energy consumption and latency on diverse hardware platforms to quantify the claimed sustainability benefits beyond theoretical speedup metrics.