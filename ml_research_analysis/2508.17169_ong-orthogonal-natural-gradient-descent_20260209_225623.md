---
ver: rpa2
title: 'ONG: Orthogonal Natural Gradient Descent'
arxiv_id: '2508.17169'
source_url: https://arxiv.org/abs/2508.17169
tags:
- task
- gradient
- tasks
- natural
- descent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Orthogonal Natural Gradient Descent (ONG),
  a method for continual learning that combines natural gradients with orthogonal
  projections to mitigate catastrophic forgetting. The approach preconditions task
  gradients with an EKFAC approximation of the inverse Fisher information matrix and
  projects them onto the orthogonal complement of prior tasks' gradients.
---

# ONG: Orthogonal Natural Gradient Descent

## Quick Facts
- arXiv ID: 2508.17169
- Source URL: https://arxiv.org/abs/2508.17169
- Authors: Yajat Yadav; Patrick Mendoza; Jathin Korrapati
- Reference count: 23
- Primary result: ONG achieves 58.7% forgetting on Permuted MNIST vs OGD's 4.2%, with accuracy dropping from 82.7% to 32.5%

## Executive Summary
This paper proposes Orthogonal Natural Gradient Descent (ONG), a method for continual learning that combines natural gradients with orthogonal projections to mitigate catastrophic forgetting. The approach preconditions task gradients with an EKFAC approximation of the inverse Fisher information matrix and projects them onto the orthogonal complement of prior tasks' gradients. The authors prove that ONG maintains descent-direction guarantees under the Fisher metric. However, preliminary experiments on Permuted and Rotated MNIST show that ONG performs worse than vanilla OGD, with significantly higher forgetting rates and lower accuracy.

## Method Summary
ONG preconditions gradients with the inverse Fisher information matrix via EKFAC approximation, then projects them onto the orthogonal complement of prior tasks' natural gradients to prevent interference. The method maintains a basis of sensitive directions from previous tasks and updates parameters using the projected gradient. The approach aims to combine the benefits of natural gradient optimization (invariance to reparameterization) with orthogonal gradient projection (forgetting prevention).

## Key Results
- ONG achieves 58.7% forgetting on Permuted MNIST vs OGD's 4.2%
- ONG accuracy drops to 32.5% from OGD's 82.7% on Permuted MNIST
- Early-task accuracy collapses rapidly (Task 1: 5.44% for ONG vs 38.91% for OGD)
- Recent tasks maintain high accuracy (Task 15: 96.35% for ONG vs 97.29% for OGD)

## Why This Works (Mechanism)

### Mechanism 1: Natural Gradient Preconditioning via EKFAC
Preconditioning gradients with the inverse Fisher information matrix yields the steepest descent direction under the Riemannian metric of the parameter space. EKFAC approximates F⁻¹ via Kronecker factorization in the eigenbasis, making computation tractable for large networks.

### Mechanism 2: Orthogonal Projection for Interference Prevention
Projecting new task gradients onto the orthogonal complement of prior task gradients prevents interference with learned representations. The method maintains basis S spanning "sensitive directions" from prior tasks and constrains updates to the subspace orthogonal to prior task gradients.

### Mechanism 3: Descent Direction Preservation Under Projection
Lemma 4.1 proves that after Fisher preconditioning and orthogonal projection, the resulting update direction remains a valid descent direction under the Fisher metric. The proof shows the projected residual is orthogonal to the projection subspace, yielding ⟨−ĝ, F⁻¹g⟩ = −‖ĝ‖² ≤ 0.

## Foundational Learning

- **Concept: Fisher Information Matrix and Natural Gradients**
  - Why needed here: ONG's core modification is preconditioning with F⁻¹; understanding why this changes optimization geometry is essential for debugging failure modes.
  - Quick check question: Given a 2D loss landscape with Fisher matrix diag([100, 1]), which direction does the natural gradient emphasize relative to standard gradient descent?

- **Concept: Orthogonal Subspace Projection**
  - Why needed here: The continual learning mechanism depends on maintaining and projecting onto orthogonal complements; implementation errors here directly cause forgetting.
  - Quick check question: If basis vectors v₁, v₂ span prior task gradients, what is the projection of gradient g = [3, 4] onto their orthogonal complement?

- **Concept: Riemannian Geometry and Metric Compatibility**
  - Why needed here: The paper's central finding is geometric incompatibility between Euclidean projections and Fisher-metric descent; understanding inner products under different metrics is crucial for interpreting results.
  - Quick check question: In a Riemannian manifold with metric tensor G, how does the inner product ⟨u, v⟩_G differ from Euclidean ⟨u, v⟩, and what does this imply for "orthogonal" projections?

## Architecture Onboarding

- **Component map:**
  Input: Task gradient g = ∇L(w) → [EKFAC Preconditioner] → g̃ = F⁻¹g (natural gradient) → [Gradient Memory] → Basis S of prior task natural gradients → [Orthogonal Projector] → ĝ = g̃ − Σprojᵥ(g̃) for v∈S → [Parameter Update] → w ← w − ηĝ → [Memory Update] → Store new task gradients (also preconditioned) in S

- **Critical path:** The EKFAC approximation quality → preconditioned gradient accuracy → projection effectiveness → forgetting/accuracy tradeoff. Each stage compounds errors.

- **Design tradeoffs:**
  - EKFAC vs. exact Fisher: EKFAC is O(n²) per layer vs. O(n⁴) for exact, but approximation errors may accumulate
  - Memory buffer size (100 samples/task in experiments): Larger buffers capture more gradient directions but increase computation and storage
  - Preconditioning frequency: Every batch (current) vs. periodic recomputation trades accuracy for speed

- **Failure signatures:**
  - Observed: ONG achieves 58.7% forgetting vs. OGD's 4.2% on Permuted MNIST; accuracy drops from 82.7% → 32.5%
  - Pattern: Early-task accuracy collapses rapidly (Task 1: 5.44% for ONG vs. 38.91% for OGD), while recent tasks remain high (Task 15: 96.35%)
  - Hypothesis from paper: "Fundamental geometric incompatibility between Euclidean projections and Fisher-metric descent directions"—the projection inner product doesn't match the descent metric

- **First 3 experiments:**
  1. Ablate preconditioning location: Apply Fisher preconditioning only at projection time (not update time) or vice versa to isolate which combination causes failure
  2. Fisher-weighted projection: Replace Euclidean projection with Fisher-inner-product projection: projᵥ(g) = (⟨g, v⟩_F / ⟨v, v⟩_F) · v, testing the geometric compatibility hypothesis
  3. Gradient subspace analysis: Visualize overlap between stored natural gradient subspace and incoming task gradients across training—if overlap is systematically higher for ONG than OGD, this explains constrained learning and failure to acquire early tasks

## Open Questions the Paper Calls Out

- **Open Question 1:** How can projection operators be redesigned to be geometrically consistent with the Fisher metric to prevent the performance degradation observed in ONG?
  - The authors state the need for "robustly reconciling these geometric perspectives" and "developing a geometrically-meaningful way of performing 'projection' with natural gradients."

- **Open Question 2:** Can parallel transport be utilized to define a consistent inner product and basis for projecting gradients across different points on the parameter manifold?
  - The authors identify parallel transport as a "particularly promising" direction to "move tangent vectors... into the same tangent space" to define a "geometrically consistent inner product."

- **Open Question 3:** What specific role does the accuracy of the EKFAC approximation play in the failure of ONG?
  - The authors list "looking more closely at... the accuracy of the Fisher approximation estimates" as a specific avenue for future work to understand training dynamics.

- **Open Question 4:** Does the "geometric incompatibility" hypothesis hold in more challenging, uncorrelated task sequences?
  - The authors hypothesize that the observed limitations might be "partially attributable to the high task correlation in benchmarks like Permuted and Rotated MNIST."

## Limitations
- The paper's core finding is that ONG performs significantly worse than OGD on standard benchmarks, suggesting fundamental issues with the approach
- Geometric incompatibility between Euclidean projections and Fisher-metric descent directions remains an unresolved theoretical and practical challenge
- Evaluation is limited to highly correlated task sequences (Permuted and Rotated MNIST), limiting generalizability to more realistic continual learning scenarios

## Confidence
- **Medium Confidence**: The theoretical framework (descent direction preservation via Lemma 4.1) is mathematically sound, but the gap between local geometric guarantees and global empirical performance suggests missing analysis of projection metric compatibility.
- **Low Confidence**: The claim that ONG's poor performance stems from "fundamental geometric incompatibility" is compelling but requires validation through controlled ablation studies—the paper provides minimal empirical investigation of failure modes.
- **Medium Confidence**: The observation that early-task accuracy collapses while recent tasks remain high is a clear failure signature, though its interpretation requires further investigation of gradient subspace overlap dynamics.

## Next Checks
1. **Geometric Compatibility Test**: Replace Euclidean orthogonal projection with Fisher-inner-product projection and compare forgetting rates to isolate whether metric mismatch drives ONG's failure.

2. **Gradient Subspace Analysis**: Visualize and quantify the overlap between stored natural gradient subspaces and incoming task gradients across training—if overlap is systematically higher for ONG than OGD, this explains constrained learning and early-task forgetting.

3. **EKFAC Approximation Ablation**: Compare ONG with exact Fisher preconditioning (when computationally feasible) versus EKFAC approximation to determine whether approximation errors compound with projection to cause failure.