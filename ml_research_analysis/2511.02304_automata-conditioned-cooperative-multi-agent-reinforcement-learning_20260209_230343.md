---
ver: rpa2
title: Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning
arxiv_id: '2511.02304'
source_url: https://arxiv.org/abs/2511.02304
tags:
- policies
- tasks
- agents
- agent
- dfas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces ACC-MARL, a framework for learning decentralized
  multi-agent policies conditioned on temporal task specifications represented as
  deterministic finite automata (DFAs). The method addresses three core challenges:
  history dependency via Markovian state augmentation with DFA progress, credit assignment
  through potential-based reward shaping per agent, and representation bottlenecks
  using pretrained RAD embeddings.'
---

# Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.02304
- Source URL: https://arxiv.org/abs/2511.02304
- Reference count: 40
- Primary result: Introduces ACC-MARL framework for decentralized multi-agent RL with DFA-conditioned policies

## Executive Summary
This paper presents ACC-MARL, a framework for learning decentralized multi-agent policies conditioned on temporal task specifications represented as deterministic finite automata (DFAs). The method addresses three core challenges in cooperative multi-agent RL: handling history-dependent tasks through Markovian state augmentation, enabling effective credit assignment via potential-based reward shaping, and mitigating representation bottlenecks using pretrained RAD embeddings. The framework demonstrates improved performance and generalization on synthetic token collection tasks with 2-4 agents.

## Method Summary
ACC-MARL augments each agent's observation with DFA progress to make history-dependent tasks Markovian. The framework uses potential-based reward shaping to address credit assignment challenges in decentralized settings. RAD (Randomized Ensembled Double Q-learning) embeddings are employed to improve representation learning for coordination. The method learns value functions that enable optimal task assignment at test time, particularly beneficial in asymmetric environments with heterogeneous agent capabilities.

## Key Results
- ACC-MARL achieves higher success rates than ablations across token collection tasks
- Full framework (Markov reformulation + PBRS + RAD embeddings) shows best generalization across task classes
- Optimal task assignments improve team performance in asymmetric environments
- Emergent coordination skills observed: door-holding and task short-circuiting

## Why This Works (Mechanism)
The framework works by converting non-Markovian temporal specifications into Markovian decision processes through DFA augmentation. Potential-based reward shaping provides dense rewards that guide learning without altering optimal policies. The combination of decentralized execution with centralized training (through value function learning) enables effective credit assignment. RAD embeddings provide sufficient representational capacity for coordination while maintaining computational efficiency.

## Foundational Learning
- **Deterministic Finite Automata (DFAs)**: Why needed - to represent temporal task specifications; Quick check - can encode sequential, parallel, and choice-based task structures
- **Potential-Based Reward Shaping (PBRS)**: Why needed - to provide dense rewards in sparse reward environments; Quick check - preserves optimal policy while accelerating learning
- **RAD Embeddings**: Why needed - to improve representation learning for coordination; Quick check - pretrained embeddings transfer across task instances
- **Markov Property**: Why needed - to enable standard RL algorithms; Quick check - history-dependent tasks made Markovian through DFA state augmentation
- **Decentralized Execution**: Why needed - to scale to large agent teams; Quick check - each agent acts based only on local observations and communications
- **Centralized Training**: Why needed - to learn joint value functions for optimal task assignment; Quick check - enables evaluation of different task allocations

## Architecture Onboarding

Component Map:
Observation -> DFA Augmentation -> RAD Embedding -> Policy Network -> Action
                          |
                          v
                    Potential-Based Reward Shaping

Critical Path:
1. Environment observation
2. DFA state update based on global task progress
3. Observation + DFA state → RAD embedding
4. Embedding → policy network → action
5. Joint reward shaping based on potential functions

Design Tradeoffs:
- Centralized training vs. decentralized execution: balances coordination capability with scalability
- DFA complexity vs. learning difficulty: more complex specifications increase representational power but may slow learning
- Embedding dimensionality vs. computational cost: higher dimensions may improve coordination but increase training time

Failure Signatures:
- Poor performance when DFA states are not properly synchronized across agents
- Learning instability when potential functions are poorly designed
- Coordination failures when embedding dimensionality is insufficient

First Experiments:
1. Single-agent task completion with varying DFA complexities
2. Two-agent coordination tasks with simple temporal specifications
3. Ablation studies removing each component (Markov reformulation, PBRS, RAD embeddings)

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding scalability to more complex temporal tasks and larger agent teams, the effectiveness of the framework for arbitrary specifications without manual tuning, and the robustness of the approach to different potential function designs and embedding architectures.

## Limitations
- Scalability concerns for complex temporal tasks with larger agent teams
- Limited experimental scope (2-4 agents, simple token collection tasks)
- Handcrafted DFA structures and potential functions may limit generalization
- Effectiveness of RAD embeddings untested for more complex environments

## Confidence
- Scalability claims: Medium
- Performance improvements: High for tested task classes
- Generalization to arbitrary specifications: Low
- RAD embeddings sufficiency: Medium

## Next Checks
1. Test ACC-MARL on environments with longer temporal horizons and more complex DFA structures (e.g., nested or looping specifications)
2. Evaluate the framework's performance with larger agent teams (5+) and more heterogeneous capabilities
3. Assess the sensitivity of results to different potential function designs and embedding architectures to establish robustness