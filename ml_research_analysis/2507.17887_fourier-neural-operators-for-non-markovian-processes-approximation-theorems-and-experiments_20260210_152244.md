---
ver: rpa2
title: Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems
  and Experiments
arxiv_id: '2507.17887'
source_url: https://arxiv.org/abs/2507.17887
tags:
- mfno
- neural
- operator
- brownian
- sdes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the mirror-padded Fourier neural operator
  (MFNO), a novel architecture designed to learn solution operators of non-Markovian
  stochastic processes, including path-dependent stochastic differential equations
  and fractional Brownian motions. The key innovation is mirror padding, which addresses
  the periodicity assumption of standard Fourier neural operators, enabling effective
  handling of non-periodic stochastic inputs.
---

# Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments

## Quick Facts
- arXiv ID: 2507.17887
- Source URL: https://arxiv.org/abs/2507.17887
- Reference count: 4
- Introduces mirror-padded Fourier neural operator (MFNO) that learns solution operators for non-Markovian stochastic processes including path-dependent SDEs and fractional Brownian motions

## Executive Summary
This paper introduces the mirror-padded Fourier neural operator (MFNO), a novel architecture designed to learn solution operators of non-Markovian stochastic processes, including path-dependent stochastic differential equations and fractional Brownian motions. The key innovation is mirror padding, which addresses the periodicity assumption of standard Fourier neural operators, enabling effective handling of non-periodic stochastic inputs. The authors establish rigorous approximation theorems showing MFNOs can approximate these operators to arbitrary accuracy, building on Wong-Zakai type theorems and various approximation techniques. Empirically, MFNO demonstrates strong resolution generalization—a rare property among architectures like LSTMs, TCNs, and DeepONets—and achieves comparable or superior performance while offering significantly faster sample path generation than classical numerical schemes.

## Method Summary
The MFNO architecture extends standard Fourier neural operators with mirror padding to handle non-periodic stochastic inputs. The method involves reflecting Brownian paths about the midpoint to create symmetric extensions on [0, 2T], producing continuous periodic functions that preserve H¹ Sobolev space membership. The architecture uses lifting layers to project inputs to latent space, followed by multiple Fourier layers with truncated weights, and truncation layers to restrict outputs to the original domain. Training uses Adam optimizer with step learning rate decay on simulated SDE and fBM data. The approach builds on Wong-Zakai approximation theorems to bridge measurable operators to continuous ones suitable for FNO approximation.

## Key Results
- MFNO exhibits strong resolution generalization—a property rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet
- Mirror padding enables FNOs to handle non-periodic stochastic inputs while preserving the continuity required for universal approximation guarantees
- MFNO achieves comparable or superior performance to classical numerical schemes while offering significantly faster sample path generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mirror padding enables FNOs to handle non-periodic stochastic inputs while preserving the continuity required for universal approximation guarantees.
- **Mechanism:** Standard FNOs assume periodic inputs on the torus domain, but Brownian paths are non-periodic. Mirror padding reflects the input path about the midpoint T to create a symmetric extension on [0, 2T], producing a continuous and periodic function. This preserves H¹ Sobolev space membership, which zero-padding fails to do (creates discontinuities at boundaries unless B(T) = 0).
- **Core assumption:** The solution operator remains continuous when restricted to the compact subset of paths with bounded Sobolev norm (Assumption: high-probability paths lie in this set).
- **Evidence anchors:**
  - [abstract] "MFNO extends the standard Fourier neural operator (FNO) by incorporating mirror padding, enabling it to handle non-periodic inputs."
  - [Section 2.2] "This construction yields a continuous and periodic function on [0, 2T], thereby satisfying the conditions required by Theorem 2.2."
  - [corpus] Weak direct corpus support; neighbor papers focus on deterministic operator approximation (Cauchy Random Features, MCNO) rather than stochastic boundary handling.
- **Break condition:** If input paths have unbounded Sobolev norm with non-negligible probability, compactness assumption fails and approximation guarantees degrade.

### Mechanism 2
- **Claim:** Linearly interpolated Brownian motion bridges measurable operators on continuous paths to continuous operators on Sobolev spaces via Wong-Zakai approximation.
- **Mechanism:** The true solution operator F mapping (ξ, B) to X is merely measurable, not continuous—preventing direct application of FNO universal approximation theorems. Piecewise linear interpolation B_n lies in finite-dimensional H¹ subspaces, and Wong-Zakai theorems establish that solutions to SDEs driven by B_n converge to the true solution. This continuity enables MFNO approximation.
- **Core assumption:** Convergence rate of Wong-Zakai approximation is faster than MFNO approximation error (Theorem 3.1 provides |π_n|^(1/2 - 1/p) rate).
- **Evidence anchors:**
  - [abstract] "Our theoretical analysis builds on Wong–Zakai type theorems and various approximation techniques."
  - [Section 3.1] Theorem 3.2 explicitly conditions approximation on the partition fineness M₀ ≤ n ≤ M₀ + M.
  - [corpus] Corpus papers do not address Wong-Zakai bridges; focus is on deterministic PDE operators.
- **Break condition:** For very rough paths (e.g., fBM with low Hurst index H = 0.25), the interpolation converges slowly and generalization degrades (observed empirically in Figure 4).

### Mechanism 3
- **Claim:** Resolution generalization emerges from learning a continuous operator rather than discretization-specific mappings.
- **Mechanism:** Unlike LSTMs/TCNs tied to training grids, FNOs parameterize kernel integrals in Fourier space. The truncation layer T outputs to L²([0,T]) rather than fixed discretization. When evaluated at finer grids, the learned Fourier weights interpolate naturally. Mirror padding ensures boundary conditions remain consistent across resolutions.
- **Core assumption:** The spectral content of the solution operator is captured by the truncated frequency modes (width W) during training.
- **Evidence anchors:**
  - [abstract] "MFNO exhibits strong resolution generalization—a property rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet."
  - [Section 4.4] Figures 4-5 show MFNO/ZFNO maintain constant error across resolutions while vanilla FNO degrades.
  - [corpus] MCNO paper similarly claims no spectral assumptions, suggesting alternative resolution-invariant approaches exist.
- **Break condition:** If the operator requires high-frequency modes beyond training width W, super-resolution will fail; for rough processes (fBM H=0.25), all FNO variants show degraded generalization.

## Foundational Learning

- **Concept: Sobolev Spaces H^s([0,T], ℝ^d)**
  - **Why needed here:** Universal approximation theorems require continuous operators on Sobolev spaces; Lemma 2.3 establishes high-probability bounds on ||B_n||_{H^1}.
  - **Quick check question:** Can you explain why ||B_n||_{H^1} being bounded matters for the compactness argument in Theorem 3.2?

- **Concept: Non-Markovian Dynamics and Path-Dependent SDEs**
  - **Why needed here:** The paper targets processes where future evolution depends on full path history, not just current state; this motivates operator learning over recurrent architectures.
  - **Quick check question:** How does functional Itô calculus extend standard Itô calculus for path-dependent coefficients?

- **Concept: Fourier Transform and Periodic Extension**
  - **Why needed here:** Understanding why FNOs require periodicity, and how mirror padding creates artificial periodicity while preserving differentiability.
  - **Quick check question:** Why does zero-padding fail to preserve H¹ membership for generic Brownian paths?

## Architecture Onboarding

- **Component map:**
  Input: (ξ, B_n) ∈ H¹([0,T], ℝ^{m+ℓ})
      ↓
  Mirror Padding M: extend to [0, 2T] by reflection
      ↓
  Lifting R: project to dv-channel latent space
      ↓
  Fourier Layers (L×): 
      [FFT → Truncated weights P_θ → IFFT → σ(W·v + b)]
      ↓
  Truncation T: restrict output to [0, T]
      ↓
  Projection Q: map to output dimension du

- **Critical path:** Implementing mirror padding correctly is the single most failure-prone step. The reflection must be a(2T - t) for t ∈ (T, 2T], not a(T - (t-T)). Verify continuity at t = T numerically.

- **Design tradeoffs:**
  - **MFNO vs ZFNO:** MFNO preserves H¹ for theoretical guarantees; ZFNO matches empirical performance but introduces discontinuities. Choose MFNO when rigor is required; ZFNO for purely empirical work.
  - **Width W vs accuracy:** Larger W captures more frequencies but increases parameters. Paper uses W = 64; ablation not reported.
  - **Fourier layers L:** Paper uses 5 layers; deeper networks may capture more complex dependencies but risk overfitting.

- **Failure signatures:**
  1. **Resolution degradation:** If test resolution >> train resolution and error spikes, check if process roughness (low Hurst index) exceeds model capacity.
  2. **Boundary artifacts:** If output shows jumps near t=0 or t=T, verify mirror padding implementation; vanilla FNO will show wraparound artifacts.
  3. **Training instability for fBM H < 0.5:** TCN showed divergent errors; ensure learning rate is reduced and gradient clipping is applied for rough processes.

- **First 3 experiments:**
  1. **Validate Wong-Zakai convergence:** Train MFNO on paths interpolated at resolution n, evaluate on finer 2n interpolation. Confirm error decreases as |π_n|^(1/2 - 1/p) per Theorem 3.1.
  2. **Padding ablation:** Compare MFNO, ZFNO, and vanilla FNO on SDE (4.1) across resolutions 128, 256, 512. Replicate Figure 4 behavior; vanilla FNO should show linear error growth.
  3. **Computational scaling benchmark:** Time MFNO vs Euler-Maruyama for generating 10,000 sample paths at resolutions 128, 256, 512, 1024. Confirm O(n log n) vs O(n²) scaling from Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MFNO architecture be modified to improve resolution generalization for processes with low regularity, such as fractional Brownian motions with Hurst index H < 0.5?
- Basis in paper: [explicit] The authors state in the Conclusion that for rougher processes like fBM (H=0.25), all FNO-based models showed reduced generalization, reflecting "inherent difficulty" rather than just architectural limitations.
- Why unresolved: The current architecture struggles to generalize on high-frequency components of rough paths when moving from coarse to fine grids.
- What evidence would resolve it: A modification to the spectral layers or architecture that maintains constant error rates across resolutions for H=0.25, similar to the results seen for H=0.75.

### Open Question 2
- Question: Does the MFNO maintain its approximation guarantees and computational efficiency when applied to stochastic partial differential equations (SPDEs) involving high-dimensional spatial domains?
- Basis in paper: [inferred] The paper focuses exclusively on 1D temporal domains (d=1) for SDEs and fBM, while the method relies on spectral convolutions which may scale differently or face aliasing issues in complex spatial geometries.
- Why unresolved: The universality theorems provided rely on Sobolev embeddings for 1D continuous paths; extending this to the infinite-dimensional function spaces typical of SPDEs is non-trivial.
- What evidence would resolve it: Theorems extending the approximation results to d>1 spatial dimensions and empirical results showing MFNO outperforming Euler schemes on standard SPDE benchmarks.

### Open Question 3
- Question: Are there specific boundary conditions or non-periodic stochastic systems where MFNO strictly outperforms zero-padded FNOs (ZFNO) in empirical accuracy?
- Basis in paper: [inferred] The Conclusion notes that MFNO achieves performance "comparable" to ZFNO while offering better theoretical tractability, leaving the existence of a strict empirical advantage unverified.
- Why unresolved: ZFNO introduces discontinuities but might effectively mitigate them through learned padding, potentially matching MFNO's performance in practice despite flawed theory.
- What evidence would resolve it: A comparative study on problems with strong boundary effects where ZFNO's artificial discontinuities cause significant error divergence that MFNO avoids.

## Limitations

- Compactness assumption: MFNO approximation guarantees hold only for input paths with bounded Sobolev norm, excluding heavy-tailed or very rough processes from theoretical framework
- Lack of activation function and initialization details may affect reproducibility
- Focus on path-dependent SDEs and fBM leaves open questions about performance on other non-Markovian processes like regime-switching models or delay differential equations

## Confidence

- **High confidence**: MFNO architecture works as described and provides computational speedups over classical numerical schemes
- **Medium confidence**: Theoretical approximation theorems hold under stated assumptions, though practical applicability may exceed theoretical bounds
- **Medium confidence**: Resolution generalization is a genuine property of MFNO, though the mechanism is not fully explained by current theory

## Next Checks

1. **Compactness boundary test**: Systematically vary Hurst index H from 0.25 to 0.75 and measure both training performance and theoretical Sobolev norm bounds to identify where compactness assumption breaks down

2. **Wong-Zakai convergence verification**: Empirically verify the |π_n|^(1/2 - 1/p) convergence rate by training MFNO on multiple interpolation resolutions and measuring error decay

3. **Activation function ablation**: Test multiple activation functions (ReLU, GELU, Tanh) and initialization schemes to determine their impact on both training stability and approximation quality