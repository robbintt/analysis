---
ver: rpa2
title: The BoBW Algorithms for Heavy-Tailed MDPs
arxiv_id: '2602.01295'
source_url: https://arxiv.org/abs/2602.01295
tags:
- regret
- skip
- heavy-tailed
- loss
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper tackles the challenge of achieving best-of-both-worlds
  (BoBW) performance in episodic Markov Decision Processes (MDPs) with heavy-tailed
  feedback, where existing approaches struggle to adapt to both adversarial and stochastic
  environments. The authors develop two novel algorithms: HT-FTRL-OM for known transitions
  and HT-FTRL-UOB for unknown transitions.'
---

# The BoBW Algorithms for Heavy-Tailed MDPs

## Quick Facts
- **arXiv ID**: 2602.01295
- **Source URL**: https://arxiv.org/abs/2602.01295
- **Reference count**: 40
- **Primary result**: First Best-of-Both-Worlds algorithms for heavy-tailed MDPs achieving Õ(T^{1/α}) adversarial regret and O(log T) stochastic regret (known transitions) or O(log² T) stochastic regret (unknown transitions)

## Executive Summary
This paper introduces the first Best-of-Both-Worlds (BoBW) algorithms for episodic Markov Decision Processes (MDPs) with heavy-tailed feedback, where losses have bounded α-moments (α ∈ (1,2]). The authors develop two algorithms: HT-FTRL-OM for known transitions and HT-FTRL-UOB for unknown transitions. Both algorithms employ Follow-The-Regularized-Leader (FTRL) over occupancy measures with a 1/α-Tsallis entropy regularizer and introduce novel skipping loss estimators to handle heavy-tailed losses. The key innovation is a skipping estimator that truncates extreme losses at adaptive thresholds while maintaining bounded bias, enabling stable FTRL updates even when traditional sub-Gaussian concentration assumptions fail.

## Method Summary
The method uses FTRL over occupancy measures with 1/α-Tsallis entropy regularization, where the regularizer is Ψ_t(x) = -(1/η_t)Σ x(s,a)^{1/α} with η_t = β/(σ·t^{1/α}). The core innovation is a skipping loss estimator that truncates losses exceeding threshold τ_t(s,a) = C·σ·t^{1/α}·x_t(s,a)^{1/α}, then compensates bias via skipping bonus. For unknown transitions, the algorithm incorporates pessimistic estimation with upper occupancy bounds and Bernstein confidence sets updated in doubling epochs. The approach achieves nearly minimax-optimal instance-independent regret of Õ(T^{1/α}) in adversarial regimes and logarithmic instance-dependent regret in stochastic regimes through a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors.

## Key Results
- HT-FTRL-OM achieves Õ(T^{1/α}) adversarial regret and O(log T) stochastic regret for known transitions
- HT-FTRL-UOB achieves Õ(T^{1/α} + √T) adversarial regret and O(log² T) stochastic regret for unknown transitions
- Introduces first BoBW guarantees for heavy-tailed MDPs, bridging robust RL under heavy-tailed feedback and adaptive algorithm design
- Establishes novel skipping loss estimator and suboptimal mass propagation principle for heavy-tailed environments

## Why This Works (Mechanism)

### Mechanism 1: Skipping Loss Estimator for Heavy-Tailed Noise
Truncating extreme losses at adaptive thresholds enables stable FTRL updates while maintaining bounded bias. The skipping estimator uses threshold τ_t(s,a) = C·σ·t^{1/α}·x_t(s,a)^{1/α} to bound unobserved losses, then compensates bias via skipping bonus b_skip_t(s,a) = C^{1-α}·σ·t^{1/α-1}·x_t(s,a)^{1/α-1}. This is necessary because heavy-tailed distributions violate sub-Gaussian concentration assumptions.

### Mechanism 2: Local Control of Shifted Losses via Occupancy-Weighted Decomposition
Decomposing shifted losses into centered terms plus value-function differences enables pointwise bounds scaling as t^{1/α} x_t(s,a)^{1/α-1}. The shifted loss Q_{P,π_t}(s,a; ℓ̂_skip_t) - V_{P,π_t}(s; ℓ̂_skip_t) is decomposed as ℓ̂_center_t(s,a) + ΔV_t(s,a), where the value-difference term sums differential reachability weights across layers.

### Mechanism 3: Suboptimal Mass Propagation for Instance-Dependent Bounds
The total occupancy mass deviation of any policy π from deterministic reference π† is bounded by H times the cumulative suboptimal action mass. This bridges occupancy deviations across layers and enables logarithmic regret in stochastic regimes when the self-bounding condition holds.

## Foundational Learning

- **Occupancy Measure Polytope Q(P)**: FTRL operates over convex occupancy measures rather than policies directly; constraints encode transition dynamics. Quick check: Given occupancy ρ, can you recover the policy π(a|s) and verify ρ ∈ Q(P)?
- **Tsallis Entropy Regularization**: The 1/α-Tsallis regularizer provides the stability-penalty matching required for BoBW; α = 2 recovers standard bounds. Quick check: For α = 1.5, does x^{1/α-1} increase or decrease as x → 0?
- **Self-Bounding Regimes and Gap Functions**: BoBW guarantees require connecting regret to suboptimal action gaps Δ(s,a); the constraint Reg_T(π) ≥ E[Σ_t Σ_{s,a} ρ_{P,π_t}(s,a) Δ(s,a)] - C_sb enables logarithmic bounds. Quick check: In stochastic MDPs with optimal policy π, what is Δ(s, π(s))?

## Architecture Onboarding

- **Component map**: Occupancy solver (FTRL convex optimization over Q(P)) -> Loss estimator (skipping truncation + importance sampling + bias compensation) -> Regularizer (1/α-Tsallis entropy) -> Transition estimator (unknown case only) -> Pessimism module (unknown case only)
- **Critical path**: Initialize occupancy uniformly -> Solve FTRL → derive policy → collect trajectory → update skipping estimator → (unknown case) update transition counts and confidence sets
- **Design tradeoffs**: Higher α (closer to 2) gives better adversarial rate but requires lighter tails; larger skipping constant C reduces truncation bias but increases threshold; unknown vs. known transitions adds computational overhead
- **Failure signatures**: Exploding occupancy gradients (η_t too large relative to C), regret not decreasing in stochastic regime (gap Δ_min too small), numerical instability in FTRL solver (occupancy near zero)
- **First 3 experiments**: 1) Sanity check: Run HT-FTRL-OM on MDP with H=3, S=5, A=3, α=1.5, verify regret ~ O(log T); 2) Adversarial stress test: Generate losses with α-moment bounded but unbounded variance, verify Õ(T^{1/α}) scaling; 3) Unknown transition ablation: Compare HT-FTRL-UOB vs. HT-FTRL-OM (with oracle transitions) on same instance

## Open Questions the Paper Calls Out

1. **Function Approximation Extension**: Can BoBW regret guarantees for heavy-tailed feedback be extended to MDPs with function approximation? This would require adapting the tabular occupancy measure framework to high-dimensional function classes.

2. **Tightening Polynomial Dependence**: Can the polynomial dependence on state space S, action space A, and horizon H in the regret bounds be tightened? Current bounds involve high-degree polynomial factors that may be loose artifacts of the analysis.

3. **Parameter-Free Algorithms**: Is it possible to achieve these BoBW guarantees without prior knowledge of the heavy-tail parameters σ and α? Current algorithms explicitly require these parameters as inputs.

## Limitations

- **Computational Complexity**: Solving the FTRL optimization over the occupancy measure polytope is polynomial-time but requires specifying an algorithm not provided in the paper
- **Layered MDP Assumption**: The algorithms assume a layered MDP structure with transitions only between adjacent layers, which may not hold in all reinforcement learning applications
- **Parameter Dependence**: Both algorithms require prior knowledge of heavy-tail parameters α and σ, which may not be available in practice

## Confidence

- **High**: Adversarial regret bounds for both known and unknown transition cases
- **Medium**: Stochastic regret bounds and the self-bounding condition assumptions
- **Medium**: Numerical stability claims regarding the skipping estimator and Tsallis regularization

## Next Checks

1. **Self-Bounding Verification**: For a simple stochastic MDP with known gaps, verify that the self-bounding constraint holds and that the suboptimal mass propagation lemma correctly bounds regret by the cumulative suboptimal action mass.

2. **Skipping Estimator Bias Analysis**: Implement the skipping estimator and empirically measure the bias-variance tradeoff across different α values and threshold constants C. Confirm that the bias compensation term adequately controls the truncation bias.

3. **FTRL Solver Implementation**: Develop a practical solver for the constrained optimization over occupancy measures with the 1/α-Tsallis regularizer. Benchmark against a standard linear programming approach to verify polynomial-time complexity claims.