---
ver: rpa2
title: 'PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile
  Scenarios'
arxiv_id: '2601.22575'
source_url: https://arxiv.org/abs/2601.22575
tags:
- streaming
- video
- phostream
- forward
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoStream, the first mobile-centric streaming
  benchmark for evaluating omnimodal assistants in real-world streaming scenarios.
  Unlike existing benchmarks limited to multiple-choice questions or short clips,
  PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios
  (YouTube Vlog, Phone Tutorial, Phone Record, EgoBlind) with an average duration
  of 13.3 minutes.
---

# PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios

## Quick Facts
- arXiv ID: 2601.22575
- Source URL: https://arxiv.org/abs/2601.22575
- Reference count: 23
- This paper introduces the first mobile-centric streaming benchmark for evaluating omnimodal assistants in real-world streaming scenarios

## Executive Summary
PhoStream addresses the critical gap in evaluating omnimodal assistants for streaming scenarios by introducing a mobile-centric benchmark with 5,572 open-ended QA pairs from 578 videos across four real-world scenarios. Unlike existing benchmarks limited to multiple-choice questions or short clips, PhoStream requires models to make temporal decisions about when to respond, not just what to say. The benchmark reveals a severe "Early Response" bias where models struggle to exercise temporal patience, with state-of-the-art models like Gemini 3 Pro dropping from 80+ scores on Instant/Backward tasks to just 16.40 on Forward tasks due to premature responses before required evidence appears.

## Method Summary
PhoStream employs an automated generative pipeline with human verification to create timestamped QA pairs requiring temporal reasoning across three tasks: Backward, Instant, and Forward. Videos are preprocessed with HEVC re-encoding and 2 FPS resampling, then split into segments under 30MB. The Online Inference Pipeline processes 1-second stream updates with a 60-second visual sliding window and full text history preservation. Models receive queries at questioning timestamps and must decide whether to respond or stay "Silent" until evidence becomes available. Evaluation uses LLM-as-a-Judge scoring (0-5, scaled to 0-100) with additional tracking of Early Response and No Response rates for Forward tasks.

## Key Results
- Models achieve high scores (80+) on Instant and Backward tasks but drop sharply to 16.40 on Forward tasks due to early responses
- Qwen3-Omni exhibits 97.89% Early Response rate on Forward tasks despite strong Instant/Backward performance
- Adding audio improves Instant/Backward scores but increases Early Response rates in Forward tasks
- Proprietary models (Gemini 3 Pro) outperform open-source models but still struggle with temporal patience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal asymmetry emerges from response timing failures, not content understanding deficits
- Mechanism: Models process visual/audio content competently when evidence is present (Instant/Backward), but lack mechanisms to defer responses until future evidence arrives (Forward), causing premature guessing rather than patient waiting
- Core assumption: The gap between Backward (~80) and Forward (~16) scores reflects a timing control problem rather than a perception problem
- Evidence anchors:
  - [abstract] "models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before required visual and audio cues appear"
  - [section 4.2] "Forward is the bottleneck... Early Response is a major driver of Forward degradation for many models"
  - [corpus] WorldSense (2502.04326) similarly evaluates omnimodal temporal understanding but focuses on collaboration across modalities rather than response timing

### Mechanism 2
- Claim: Stronger perceptual capability correlates with increased Early Response tendency
- Mechanism: Models optimized for rich visual/audio understanding become more confident in generating responses, which paradoxically increases premature answering when evidence hasn't yet appeared—capability becomes impatience
- Core assumption: Models cannot distinguish between "I have enough evidence" and "I can generate a plausible answer"
- Evidence anchors:
  - [abstract] "current MLLMs struggle to decide when to speak, not just what to say"
  - [section 4.2] "Better Instant and Backward performance often comes with stronger Early Response tendency... Qwen3-Omni shows strong Instant and Backward performance but exhibits a very high ER rate of 97.89%"
  - [corpus] LiveStar (2511.05299) addresses simultaneous frame processing and response timing but doesn't quantify this capability-impatience tradeoff

### Mechanism 3
- Claim: Audio modality strengthens retrospective reasoning but encourages earlier responses
- Mechanism: Audio provides additional cues that improve evidence integration for Instant/Backward tasks, but also increases model confidence to respond before the proactive timestamp in Forward scenarios
- Core assumption: Richer multimodal signals don't inherently teach temporal patience
- Evidence anchors:
  - [section 4.3] "adding audio improves Instant and Backward scores... However, audio does not necessarily benefit the Forward setting... enabling audio slightly decreases Forward scores and increases the ER rate"
  - [table 4] Gemini 3 Pro audio ablation shows +3.37 Instant improvement but +3.78 ER increase
  - [corpus] video-SALMONN S (2510.11129) uses memory-enhanced streaming for 3+ hour videos but doesn't isolate audio's effect on response timing

## Foundational Learning

- Concept: **Temporal Scope Taxonomy (Backward/Instant/Forward)**
  - Why needed here: PhoStream's core contribution is distinguishing these three temporal reasoning types; Forward tasks require waiting for future evidence, which current models fundamentally struggle with
  - Quick check question: Given a question asked at timestamp T about an event at T+30s, which temporal scope applies and what should the model do?

- Concept: **Sliding Window Memory in Streaming**
  - Why needed here: The Online Inference Pipeline uses a 60-second visual sliding window with full text history; understanding this constraint is essential for interpreting why models fail on long-context aggregation
  - Quick check question: If a Forward question requires evidence from 90 seconds before the proactive timestamp, will the 60-second window capture it?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: Open-ended QA lacks single ground truths; the 0-5 scoring rubric emphasizes causal reasoning over exact matching, which affects how to interpret benchmark results
  - Quick check question: Under the judging rubric, would a partially correct answer with sound reasoning score higher than a precise answer without explanation?

## Architecture Onboarding

- Component map: Video Preprocessing (Download → HEVC re-encoding → 2 FPS resampling → 30MB segmentation) → Automated QA Generation (Gemini 3 Pro generates scene scripts → candidate QA pairs → automated cutoff verification) → Human Verification (10 experts, 2 rounds, verify temporal constraints and remove unsafe content) → Online Inference Pipeline (1-second stream updates, 60-second visual sliding window, single query issuance at questioning timestamp) → Evaluation (Response window validation → Early Response/No Response classification → LLM-as-a-Judge scoring)

- Critical path: The Forward task evaluation path—query issued at Timestamp Question → model must stay "Silent" until Timestamp Proactive → response validated within 2-second window → scored 0 if early/missing

- Design tradeoffs:
  - 2 FPS vs higher frame rates: Reduces processing cost but may miss fast actions
  - 60-second sliding window vs full video: Manages memory but loses long-range visual context
  - Single query issuance vs repeated polling: More realistic but requires models to self-determine response timing
  - Audio inclusion: Improves Backward/Instant but may increase Forward ER rates

- Failure signatures:
  - **Early Response (ER)**: Non-silent output before Timestamp Proactive → score 0; indicates impatience/hallucination
  - **No Response (NR)**: Only "Silent" or placeholder within response window → score 0; indicates triggering failure or over-caution
  - **High ER + High Backward**: Characteristic of capable but impatient models (e.g., Qwen3-Omni: 97.89% ER)
  - **High NR + Low Overall**: Characteristic of under-capable models (e.g., MMDuet2: 59.21% NR)

- First 3 experiments:
  1. Run baseline evaluation on your model across all three temporal scopes; if Forward score < 20% of Backward score, diagnose ER vs NR distribution
  2. Ablate audio input on Forward tasks only; if ER decreases, your model is using audio as a confidence signal prematurely
  3. Test with extended sliding window (120s) on long-context questions from EgoBlind subset; if scores improve, memory window is the bottleneck rather than timing control

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training paradigm shifts are necessary to instill "temporal patience" in MLLMs so they can reliably suppress responses until evidence appears in Forward tasks?
- Basis in paper: [explicit] "This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say." (Abstract)
- Why unresolved: The authors show that even state-of-the-art models like Gemini 3 Pro exhibit a 79.12% Early Response rate, failing to wait for cues despite high perceptual capability.
- What evidence would resolve it: A model architecture that explicitly decouples evidence accumulation from generation gating, demonstrating significantly reduced Early Response rates on PhoStream without increasing "No Response" failures.

### Open Question 2
- Question: How can multimodal models utilize audio to improve perception without triggering the overconfidence that leads to premature responses in Forward tasks?
- Basis in paper: [explicit] "Adding audio... increases the ER rate, indicating that audio can make models more confident to respond before the proactive timestamp." (Sec 4.3)
- Why unresolved: The ablation study reveals a trade-off: audio improves Backward scores but degrades Forward timing performance by encouraging the model to answer before the proactive timestamp.
- What evidence would resolve it: A training method that normalizes confidence thresholds across modalities, showing that adding audio reduces the Early Response rate relative to video-only baselines while maintaining perception gains.

### Open Question 3
- Question: Is the "Early Response" failure mode primarily driven by a lack of uncertainty estimation or a generative bias toward hallucination in the absence of evidence?
- Basis in paper: [inferred] The paper notes models "tend to guess immediately," but the internal mechanism causing this impatience (uncertainty vs. hallucination) is not isolated.
- Why unresolved: While the benchmark quantifies the failure (low Forward scores), it does not determine if models are "guessing" because they are unaware of their ignorance or simply compelled to generate text.
- What evidence would resolve it: Analysis of model calibration curves during Forward tasks, checking if low output probabilities correlate with correct "Silent" decisions.

## Limitations
- The automated QA generation pipeline's cutoff verification may not perfectly capture temporal dependencies, potentially creating false positives for Early Response classification
- The LLM-as-a-Judge evaluation introduces another layer of uncertainty since different judge models might score the same responses differently
- Generalization of findings to all streaming scenarios is limited, as the benchmark focuses specifically on mobile-centric cases across four defined scenarios

## Confidence
- **High Confidence**: The temporal scope taxonomy (Backward/Instant/Forward) and its implementation are well-defined and clearly demonstrate the Forward task's unique challenges. The automated QA generation pipeline with human verification is transparently described, and the benchmark construction methodology is reproducible.
- **Medium Confidence**: The claim that audio modality increases Early Response rates while improving Instant/Backward performance is supported by ablation studies, but the causal mechanism (audio as confidence signal) remains speculative without deeper model introspection.
- **Low Confidence**: The generalization of findings to all streaming scenarios is limited, as the benchmark focuses specifically on mobile-centric cases. The paper doesn't address potential domain-specific factors in YouTube Vlog, Phone Tutorial, Phone Record, and EgoBlind scenarios that might influence model behavior differently than other real-world streaming contexts.

## Next Checks
1. **Temporal Dependency Validation**: Manually verify a random sample of Forward task QA pairs to confirm that answers genuinely require future evidence beyond the proactive timestamp, ensuring the automated cutoff verification hasn't introduced false positives for Early Response classification.
2. **Judge Model Sensitivity Analysis**: Rerun key experiments using different LLM-as-a-Judge models (e.g., GPT-4, Claude, Llama) to quantify scoring variance and assess whether the observed performance gaps are consistent across different evaluation standards.
3. **Cross-Domain Generalization**: Evaluate models on PhoStream's temporal tasks using videos from different domains (e.g., cooking tutorials, sports commentary, news broadcasts) to determine if the Early Response bias persists across varied streaming contexts beyond the four mobile-centric scenarios.