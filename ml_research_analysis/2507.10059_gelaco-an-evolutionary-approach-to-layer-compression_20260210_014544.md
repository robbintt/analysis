---
ver: rpa2
title: 'GeLaCo: An Evolutionary Approach to Layer Compression'
arxiv_id: '2507.10059'
source_url: https://arxiv.org/abs/2507.10059
tags:
- compression
- similarity
- gelaco
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeLaCo, an evolutionary approach to large
  language model compression via layer merging. The method employs a module-wise similarity
  fitness function that captures attention, feed-forward, and hidden state representations
  during the evolutionary search for optimal layer merge configurations.
---

# GeLaCo: An Evolutionary Approach to Layer Compression

## Quick Facts
- **arXiv ID**: 2507.10059
- **Source URL**: https://arxiv.org/abs/2507.10059
- **Reference count**: 20
- **Primary result**: GeLaCo outperforms state-of-the-art compression methods across Llama-2 and Llama-3.1 models using evolutionary layer merging with module-wise similarity fitness

## Executive Summary
GeLaCo introduces an evolutionary approach to transformer layer compression that employs differential weight layer merging and a novel module-wise similarity fitness function. The method searches for optimal compression configurations by evaluating attention, feed-forward, and hidden state representations during evolutionary optimization. GeLaCo supports both single and multi-objective optimization, establishing the first Pareto frontier for layer compression trade-offs between model size and quality.

## Method Summary
GeLaCo uses evolutionary algorithms to search for optimal layer merging configurations in transformer models. The core innovation is a module-wise similarity fitness function that evaluates attention, feed-forward, and hidden state representations during inference on calibration data. The method employs differential weight accumulation when merging consecutive layers rather than direct removal, preserving more functional capacity. GeLaCo supports both single-objective optimization for target compression ratios and multi-objective optimization using NSGA-II to establish Pareto frontiers between compression ratio and quality metrics.

## Key Results
- GeLaCo achieves 0.499 average task performance at 0.281 compression on Llama-2 7B, outperforming perplexity (0.481) and KL divergence (0.478) fitness functions
- At 0.312 compression ratio, GeLaCo outperforms LaCo by more than 15% average task score on Llama-3.1 8B
- Multi-objective results show GeLaCo solutions dominate the Pareto front, with critical compression thresholds identified beyond which model performance degrades significantly

## Why This Works (Mechanism)

### Mechanism 1: Differential Weight Layer Merging
Merging consecutive layers via differential weight accumulation preserves more functional capacity than direct removal. For a merge spanning layers l to l+m, the merged parameter θ*_l = θ_l + Σ(θ_{l+k} - θ_l), accumulating incremental weight differences rather than discarding later layers entirely. This works because layer weight deltas encode learnable transformations that remain partially transferable when consolidated.

### Mechanism 2: Module-wise Similarity as Fitness Proxy
A composite fitness function measuring attention, FFN, and hidden state cosine similarity better predicts downstream task preservation than KL divergence or perplexity alone. During inference on calibration data, compute cosine similarity for (Q, K, V, output projections), (gate, up, down projections), and final hidden states; average these three component scores as the fitness signal for evolutionary selection. This works because component-level similarity correlates with functional preservation across diverse downstream tasks.

### Mechanism 3: Population-Based Search with Solution Caching
Evolutionary search with caching efficiently explores the exponential merge configuration space by avoiding redundant fitness evaluations. Encode solutions as triples (base_index, end_index, active_flag); cache fitness scores for identical merge sets regardless of encoding path; use NSGA-II for multi-objective optimization (compression ratio vs. similarity). This works because the compression solution space contains exploitable structure where multiple encoding paths yield identical merge configurations.

## Foundational Learning

- **Evolutionary Algorithms (Genetic Algorithms + NSGA-II)**
  - Why needed here: GeLaCo encodes merge configurations as chromosomes and evolves them via selection, crossover, and mutation; multi-objective optimization requires understanding non-dominated sorting and Pareto fronts.
  - Quick check question: Can you explain why NSGA-II maintains diversity better than single-objective GA for compression-quality trade-offs?

- **Transformer Architecture (Attention, FFN, Hidden States)**
  - Why needed here: The fitness function requires extracting and comparing Q/K/V projections, FFN gate/up/down layers, and residual stream representations.
  - Quick check question: For a Llama-style decoder layer, which three weight matrices would you extract to compute attention similarity?

- **Cosine Similarity and Distribution Divergence Metrics**
  - Why needed here: The paper explicitly compares cosine similarity against KL divergence and perplexity as fitness signals; understanding their differences is essential for fitness function design.
  - Quick check question: Why might KL divergence between output distributions fail to capture structural preservation that cosine similarity on weights captures?

## Architecture Onboarding

- **Component map**: Encoding Layer -> Fitness Evaluator -> Evolutionary Engine -> Caching System -> Post-training Module

- **Critical path**:
  1. Load model and calibration dataset
  2. Initialize population of random merge encodings
  3. For each individual: apply merges → compute fitness → cache result
  4. Evolve population via selection/crossover/mutation until max evaluations (10K single-obj, 30K multi-obj)
  5. Select best configuration → run post-training recovery

- **Design tradeoffs**:
  - **Population size vs. compute budget**: Larger populations (200 vs. 100) improve Pareto coverage but require more evaluations
  - **Calibration data volume**: 64 sentences sufficient for fitness signal; more data increases cost without proportional gain
  - **Compression aggressiveness**: Beyond ~50% compression, larger models (70B) show sharp quality cliffs; smaller models (7B/8B) tolerate higher ratios

- **Failure signatures**:
  - **Zero or negative merge lengths in encoding**: Ignored by design; check index mapping logic
  - **Fitness = -1.0**: Indicates repair mechanism failed to achieve target compression ratio after trials
  - **Stagnant population diversity**: Observed in Figure 2; consider increasing mutation probability or crossover distribution index
  - **Pareto front collapse**: All solutions cluster at similar compression ratios; likely insufficient search budget or overly constrained encoding

- **First 3 experiments**:
  1. **Fitness function ablation**: Replicate Table 1 on Llama-2 7B at 0.281 compression comparing KL divergence, perplexity, and module-wise similarity; verify reported task performance ranking (module > perplexity > KL).
  2. **Single-objective compression sweep**: Run GeLaCo on Llama-3.1 8B at ratios [0.125, 0.187, 0.312] with population 100, 10K evaluations; confirm GeLaCo outperforms LaCo by >15% average task score.
  3. **Pareto frontier baseline comparison**: Generate multi-objective Pareto front for Llama-2 7B with NSGA-II (200 pop, 30K evals); verify LaCo solutions are dominated by GeLaCo points as shown in Figure 3.

## Open Questions the Paper Calls Out
None

## Limitations

- **Evolutionary Search Constraints**: The population-based search may miss optimal compression configurations in the exponential solution space despite caching mechanisms, with limited search depth compared to combinatorial possibilities.
- **Post-training Recovery Bottleneck**: Compressed models require fine-tuning on specialized datasets (Fineweb-Edu or LaMini), introducing significant computational overhead that scales with model size, particularly problematic for 70B models.
- **Calibration Data Sensitivity**: The fitness function depends on 64 Wikipedia sentences for calibration, which may not represent the full distribution of downstream tasks, potentially leading to over-optimized solutions.

## Confidence

- **High Confidence (4/5)**: The differential weight merging mechanism is well-grounded in transformer architecture principles with explicit mathematical formulation; module-wise similarity superiority is empirically demonstrated with clear statistical margins.
- **Medium Confidence (3/5)**: Population-based search with caching shows efficiency improvements but lacks ablation studies on caching effectiveness; the claimed "first Pareto frontier" is plausible but direct comparisons with other multi-objective compression methods are limited.
- **Low Confidence (2/5)**: Quality cliff extrapolations at extreme compression ratios are based on limited data points, particularly for 70B models; the assumption of linear relationship between module-wise similarity and task performance may break down at higher compression ratios.

## Next Checks

1. **Calibration Data Robustness Test**: Run GeLaCo with calibration datasets from different domains (e.g., ArXiv papers, code repositories) and measure variance in final compression configurations and downstream task performance to validate whether 64 Wikipedia sentences are truly representative.

2. **Search Budget Sensitivity Analysis**: Systematically vary the evolutionary search budget (5K, 20K, 50K evaluations) and measure Pareto front coverage and solution quality for Llama-3.1 8B at 0.25 compression ratio to quantify the trade-off between computational cost and solution optimality.

3. **Cross-Model Transferability Evaluation**: Apply compression configurations optimized for Llama-2 7B to Llama-3.1 8B and measure performance degradation to test whether the evolutionary search discovers model-specific optimizations or general compression principles applicable across architectures.