---
ver: rpa2
title: 'Poor Alignment and Steerability of Large Language Models: Evidence from College
  Admission Essays'
arxiv_id: '2503.20062'
source_url: https://arxiv.org/abs/2503.20062
tags:
- essays
- engineering
- identity
- have
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) fail to produce college application
  essays that resemble human writing, even when prompted with demographic identity
  details. Analysis of 87,696 essays (30,000 human + synthetic pairs) showed LLM outputs
  form distinct clusters from human essays in embedding space (PCA) and are easily
  classified (F1=0.998).
---

# Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays

## Quick Facts
- arXiv ID: 2503.20062
- Source URL: https://arxiv.org/abs/2503.20062
- Authors: Jinsook Lee; AJ Alvero; Thorsten Joachims; René Kizilcec
- Reference count: 40
- Key outcome: Large language models fail to produce college application essays that resemble human writing, even with demographic identity prompts

## Executive Summary
Large language models (LLMs) produce college application essays that are distinctly different from human writing, failing to capture authentic personal narratives even when explicitly prompted with demographic identity details. Analysis of 87,696 essays shows LLM outputs form separate clusters from human essays in embedding space and are easily classified with near-perfect accuracy (F1=0.998). Identity prompting does not improve alignment and sometimes reduces human-likeness, as models tend to use prompt keywords and abstract language rather than personal narratives. These findings indicate LLMs are misaligned and poorly steerable in high-stakes contexts requiring authentic, diverse personal expression.

## Method Summary
The study generated 30,000 synthetic college application essays using 8 different LLMs (GPT-4o, Claude, Llama, Mistral) with two prompt conditions: Default (essay question only) and Identity-Prompted (essay question plus demographic metadata). These were compared against 29,232 human-authored essays from 2019-2023 Common App applications. Essays were encoded using T5 sentence embeddings and TF-IDF vectors, then analyzed through PCA visualization, logistic regression classification, and pairwise cosine similarity. The generation process used single-turn prompts with max tokens=867 and varying temperatures, while outputs were cleaned of conversational wrappers before analysis.

## Key Results
- LLM-generated essays form distinct clusters from human essays in PCA visualization, showing clear structural separation
- Classifier distinguishes human from LLM essays with F1 score of 0.998, indicating near-perfect separability
- Identity prompting fails to improve alignment with human writing patterns and sometimes reduces human-likeness
- LLMs exhibit lexical anchoring on prompt keywords rather than generating personal narratives
- Model outputs show homogenization across demographic groups, failing to capture authentic voice diversity

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Keyword Anchoring and Semantic Drift
LLMs anchor heavily on lexical content from prompts rather than generating semantically distant, idiosyncratic narratives. The model assigns high probability to words explicitly present in input instructions (e.g., "challenge," "growth") or conceptually adjacent abstractions, minimizing perplexity but maximizing semantic distance from human essays that rely on temporal and interpersonal specificities.

### Mechanism 2: RLHF-Induced Narrative Smoothing
Reinforcement Learning from Human Feedback conditions models toward "safe," positive, and uncontroversial narrative arcs, stripping away messy or specific authenticity found in human high-stakes writing. The preference tuning process rewards coherent, upbeat, and structurally sound outputs, creating a "generic, homogenized" style that is statistically distinct from diverse human variance.

### Mechanism 3: Rigid Identity Injection vs. Style Transfer
Prompting with demographic identity functions as rigid keyword insertion rather than latent style transfer, failing to replicate linguistic patterns of target demographics. When prompted with "I am [Race]... from [Location]," the model generates text that explicitly references these tags rather than adopting subtle, implicit syntactic structures characteristic of that group.

## Foundational Learning

- **Distributional Semantics & Vector Space Models**
  - Why needed: Paper relies on T5 embeddings and cosine similarity to prove LLM text is quantitatively different from human text
  - Quick check: If two essays have cosine similarity of 0.95, does that mean they say the same thing, or just use similar sentence structures?

- **Classifier F1 Score & Separability**
  - Why needed: High F1 score (0.998) in distinguishing Human vs. LLM means the gap is structural and predictable, not random noise
  - Quick check: If a classifier can easily distinguish Group A from Group B, what does that imply about the overlap of their features?

- **Algorithmic Monoculture**
  - Why needed: This is the core risk identified—widespread LLM use reduces diversity of expression in high-stakes environments
  - Quick check: If all students use the same LLM to write essays, does the variance of essay embeddings increase or decrease?

## Architecture Onboarding

- **Component map:** Input Layer -> Synthetic Generation Engine -> Encoder -> Analysis Module
- **Critical path:** Generation of "Identity-Prompted" corpus where demographics are injected as information rather than style instructions
- **Design tradeoffs:**
  - Encoding: Uses both T5 (semantic/syntactic) and TF-IDF (lexical) to validate robustness
  - Prompting: Uses single-turn prompt assuming students paste and get result, ignoring iterative co-writing
- **Failure signatures:**
  - Llama 3 8B disproportionately refuses to write for "White/Male" identities
  - Models prepend conversational filler requiring regex cleaning before encoding
- **First 3 experiments:**
  1. Baseline Separation: Train classifier on T5 embeddings of Human vs. GPT-4o (Default). Target F1 < 0.90 for successful mimicry.
  2. Steerability Stress Test: Compare cosine similarity of "Human vs. LLM(Default)" vs. "Human vs. LLM(Identity-Prompted)". Target: significant increase with identity prompt.
  3. Lexical Fingerprinting: Run Logistic Regression on TF-IDF vectors. Target: top coefficients are prompt-words or demographic tags.

## Open Questions the Paper Calls Out

- **Iterative human-AI co-writing:** Does iterative human-AI co-writing mitigate stylistic misalignment observed in single-turn generation? The paper notes this was not examined and could complement the study.
- **Elaborate prompt engineering:** Can providing personal writing samples successfully steer LLMs to replicate authentic individual voices where simple identity prompting fails? The authors acknowledge this as a limitation.
- **RLHF contribution to homogenization:** How do specific model tuning processes, particularly RLHF, contribute to observed homogenization of voice in high-stakes writing? The study suggests this needs more attention.
- **Narrative arc replication:** Do LLMs structurally fail to replicate narrative arcs of human-authored essays even when lexical patterns appear similar? The paper suggests exploring this higher-level structural analysis.

## Limitations

- Study relies on assumption that pre-2023 human essay corpus represents authentic, diverse personal expression
- Does not account for iterative human-LLM collaboration which may yield different alignment results
- Specific claim about demographic group homogenization requires proprietary human corpus for full validation
- Analysis doesn't test whether human essays generated with similar demographic prompts would produce comparable effects

## Confidence

- **High Confidence:** Classifier performance (F1=0.998) and PCA separation between human and LLM essays
- **Medium Confidence:** RLHF-induced narrative smoothing mechanism (supported by reduced variance but relies on external citations)
- **Medium Confidence:** Steerability analysis showing identity prompting's ineffectiveness (methodologically sound but influenced by prompt choices)
- **Low Confidence:** Specific claim about demographic groups being "homogenized" without full proprietary data

## Next Checks

1. Test base models (without RLHF) to determine whether distinctness stems from safety training versus fundamental model architecture
2. Conduct controlled experiment where human writers are given same demographic prompts to assess whether lexical anchoring is unique to LLMs
3. Implement prompt engineering study comparing style-transfer prompts ("Write like a student from the Midwest") versus identity-statement prompts to isolate poor steerability mechanism