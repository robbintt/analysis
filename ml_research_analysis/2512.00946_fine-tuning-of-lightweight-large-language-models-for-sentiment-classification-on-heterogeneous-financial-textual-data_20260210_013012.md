---
ver: rpa2
title: Fine-tuning of lightweight large language models for sentiment classification
  on heterogeneous financial textual data
arxiv_id: '2512.00946'
source_url: https://arxiv.org/abs/2512.00946
tags:
- sentiment
- financial
- data
- datasets
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lightweight open-source large language models can achieve competitive
  sentiment classification performance on heterogeneous financial textual data, even
  when trained on as little as 5% of available training data. This approach is particularly
  effective for researchers and practitioners with limited computational resources,
  offering a cost-effective alternative to proprietary models like FinBERT.
---

# Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data

## Quick Facts
- arXiv ID: 2512.00946
- Source URL: https://arxiv.org/abs/2512.00946
- Authors: Alvaro Paredes Amorin; Andre Python; Christoph Weisser
- Reference count: 34
- Key outcome: Lightweight open-source LLMs (DeepSeek-LLM 7B, Llama3 8B Instruct, Qwen3 8B) outperform FinBERT in financial sentiment classification, even with minimal training data (5%), especially on Chinese datasets.

## Executive Summary
This study evaluates three open-source lightweight LLMs against the proprietary FinBERT model for financial sentiment classification across five heterogeneous datasets in English and Chinese. The research demonstrates that lightweight models can achieve competitive or superior performance, even when trained on as little as 5% of available data. The balanced learning procedure, which addresses class imbalance by subsampling large domains and oversampling small ones, is particularly effective for the DeepSeek LLM 7B model. The findings suggest that these models offer a cost-effective alternative for researchers and practitioners with limited computational resources.

## Method Summary
The study compares three lightweight open-source LLMs (DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B) against FinBERT across five financial sentiment datasets in English and Chinese. The models are evaluated using three fine-tuning strategies: zero-shot, few-shot, and supervised fine-tuning (SFT). The SFT process involves hyperparameter optimization and employs a domain-balanced learning pipeline to address class imbalance by subsampling large domains and oversampling small ones. Performance is measured using accuracy and macro F1 scores, with the latter being particularly relevant for imbalanced datasets. The study also explores the impact of training data proportions (1%, 5%, 10%, 20%, 50%, 100%) on model performance.

## Key Results
- Lightweight LLMs (Llama3 8B and Qwen3 8B) consistently outperform FinBERT across most datasets and training proportions.
- Qwen3 8B achieves the highest overall performance, with accuracy scores of 0.87 on Financial PhraseBank and 0.93 on Chinese Finance Sentiment.
- Even with minimal training data (5%), Llama3 and Qwen3 maintain high performance, demonstrating their effectiveness in low-resource scenarios.
- The balanced learning procedure significantly improves performance for DeepSeek LLM 7B, though its efficacy for Llama3 and Qwen3 remains unverified.

## Why This Works (Mechanism)
The superior performance of lightweight LLMs stems from their ability to generalize effectively from limited training data, combined with the domain-balanced learning pipeline that mitigates class imbalance. The models' pre-training on diverse financial corpora enables them to capture nuanced sentiment patterns, while the balanced strategy ensures robust performance across underrepresented classes. Additionally, the fine-tuning process optimizes hyperparameters to maximize accuracy and macro F1 scores, particularly in low-resource settings.

## Foundational Learning
- **Fine-tuning strategies**: Zero-shot, few-shot, and supervised fine-tuning are used to adapt pre-trained models to specific tasks. Why needed: Different levels of available training data require tailored approaches to maximize performance. Quick check: Compare performance across the three strategies on a validation set.
- **Domain-balanced learning**: A pipeline that addresses class imbalance by subsampling large domains and oversampling small ones. Why needed: Prevents bias toward overrepresented classes and improves generalization. Quick check: Visualize class distribution before and after balancing.
- **Macro F1 score**: A metric that averages precision and recall across all classes, treating them equally. Why needed: Provides a fair evaluation for imbalanced datasets. Quick check: Calculate macro F1 for each class separately.
- **Cross-lingual sentiment analysis**: Evaluating models on datasets in multiple languages to assess generalizability. Why needed: Ensures the approach works across diverse linguistic contexts. Quick check: Test models on a multilingual financial sentiment dataset.
- **Hyperparameter optimization**: Systematically tuning model parameters to maximize performance. Why needed: Ensures the model is configured for the specific task and dataset. Quick check: Use grid search or Bayesian optimization to find optimal parameters.

## Architecture Onboarding

**Component Map**: Dataset -> Pre-trained LLM -> Fine-tuning Strategy -> Hyperparameter Optimization -> Evaluation (Accuracy, Macro F1)

**Critical Path**: The critical path involves loading the dataset, applying the fine-tuning strategy, optimizing hyperparameters, and evaluating performance. Each step is sequential, with the balanced learning pipeline integrated during fine-tuning.

**Design Tradeoffs**: The study prioritizes lightweight models for cost-effectiveness, but this may limit performance on highly complex tasks. The balanced learning pipeline improves generalization but adds computational overhead during fine-tuning.

**Failure Signatures**: Poor performance on underrepresented classes may indicate insufficient balancing. Overfitting to the training data could suggest excessive hyperparameter tuning or inadequate regularization.

**First Experiments**:
1. Evaluate zero-shot performance on a subset of the dataset to establish a baseline.
2. Test the balanced learning pipeline on a single model (e.g., DeepSeek LLM 7B) to verify its efficacy.
3. Compare macro F1 scores across all models and datasets to identify the best-performing configuration.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the domain-balanced training pipeline provide consistent performance improvements over sequential training strategies for architectures other than DeepSeek LLM 7B? [inferred] Appendix C provides a visual comparison of balanced versus sequential training only for the DeepSeek LLM 7B model, leaving the efficacy of this specific training pipeline unverified for the higher-performing Llama3 and Qwen3 models.
- **Open Question 2**: Can lightweight LLMs maintain high sentiment classification performance on multilingual financial datasets that include a "neutral" class, as opposed to the binary classification setup used for the Chinese dataset? [inferred] The authors attribute the exceptionally high performance (0.97 F1 score) on the Chinese Finance Sentiment (CSD) dataset to the fact that it only contains two classes (positive/negative), whereas the English datasets contain three (positive/negative/neutral).
- **Open Question 3**: To what extent does integrating Retrieval-Augmented Generation (RAG) improve the contextual understanding and accuracy of lightweight LLMs in financial sentiment tasks? [explicit] In the Conclusion, the authors explicitly identify "integrating RAG for more contextual predictions" as a direction for future work.
- **Open Question 4**: Do the superior few-shot and fine-tuning capabilities of lightweight LLMs transfer effectively to sentiment analysis tasks in domains outside of finance? [explicit] The Conclusion suggests future work should explore "extending to sentiment tasks outside of finance."

## Limitations
- The study is limited to five specific financial sentiment datasets, which may not fully represent the diversity of financial textual data in real-world applications.
- Potential biases in the models or datasets are not explored, which could affect generalization across different financial contexts or regions.
- The computational resource requirements for fine-tuning are not explicitly detailed, leaving uncertainty about scalability for larger datasets or more complex tasks.

## Confidence
- **High**: The claim that lightweight LLMs outperform FinBERT on the tested datasets is supported by robust experimental results.
- **Medium**: The generalizability of the approach to broader financial contexts or non-financial domains remains unverified.
- **High**: The assertion that these models are cost-effective for low-resource environments is validated by the demonstrated performance with minimal training data.

## Next Checks
1. **Dataset Generalization**: Test the models on additional financial sentiment datasets, particularly those from different regions or domains, to assess robustness and generalizability.
2. **Bias Analysis**: Conduct a thorough bias analysis of the models and datasets to identify potential disparities in performance across different financial contexts or demographic groups.
3. **Scalability Assessment**: Evaluate the computational resource requirements for fine-tuning and inference at scale, including memory usage, training time, and hardware specifications, to confirm the feasibility for large-scale deployment.