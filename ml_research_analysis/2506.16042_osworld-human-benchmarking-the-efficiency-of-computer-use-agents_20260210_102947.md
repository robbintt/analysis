---
ver: rpa2
title: 'OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents'
arxiv_id: '2506.16042'
source_url: https://arxiv.org/abs/2506.16042
tags:
- steps
- agents
- task
- agent
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first in-depth study on the temporal performance
  of computer-use agents, focusing on the latency implications of state-of-the-art
  agents on the OSWorld benchmark. Through detailed analysis of Agent S2, the leading
  open-source system, the authors find that large model calls for planning and reflection
  account for the majority (75%-94%) of overall latency, with per-step latency growing
  3x longer as tasks progress.
---

# OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents

## Quick Facts
- **arXiv ID**: 2506.16042
- **Source URL**: https://arxiv.org/abs/2506.16042
- **Reference count**: 8
- **Primary result**: First in-depth study on temporal performance of computer-use agents, finding planning/reflection LLM calls account for 75-94% of latency and even successful agents take 1.4-2.7x more steps than human-optimal trajectories

## Executive Summary
This paper presents the first comprehensive temporal analysis of computer-use agents on the OSWorld benchmark. Through detailed profiling of Agent S2, the leading open-source system, the authors identify that large model calls for planning and reflection dominate overall latency (75%-94%) and that per-step latency grows up to 3x longer as tasks progress due to context accumulation. To address efficiency gaps, they construct OSWorld-Human, a manually annotated dataset containing human-optimal trajectories for all 369 OSWorld tasks. Evaluation reveals that even the highest-scoring agents take 1.4-2.7x more steps than necessary, highlighting significant efficiency gaps that need to be addressed for practical deployment.

## Method Summary
The authors conduct latency analysis on Agent S2 running OSWorld tasks with 50 max steps, instrumenting code to log per-component timing (planning, reflection, grounding, etc.) and token counts per step. They manually construct human-optimal trajectories through a two-pass annotation process by two CS graduate students, creating both single-action and grouped-action versions. The evaluation uses efficiency-weighted success metrics (WES+, WES-) and step efficiency ratios to quantify performance gaps. The study focuses on 37 tasks from the OSWorld subset for detailed latency analysis, with the full 369-task OSWorld-Human dataset intended for future public release.

## Key Results
- Planning and reflection LLM calls account for 75-94% of total task latency across applications
- Per-step latency increases up to 3x as tasks progress due to accumulated context history
- Even successful agents take 1.4-2.7x more steps than human-optimal trajectories
- Accessibility trees can reduce steps for some applications but add 3-26s overhead and thousands of prompt tokens
- Grouped actions can significantly reduce step counts by combining consecutive actions into single observation-action cycles

## Why This Works (Mechanism)

### Mechanism 1: LLM Call Latency Dominance
Planning and reflection LLM calls account for 75-94% of total task latency, making them the primary bottleneck. Each step requires calling large foundation models (e.g., GPT-4.1) with full trajectory history, where prefill stage dominates due to thousands of prompt tokens.

### Mechanism 2: Context Accumulation Scaling
Per-step latency increases up to 3x as tasks progress because prompt tokens accumulate with full trajectory history. The linear context growth causes quadratic or worse latency growth due to transformer attention complexity over long sequences.

### Mechanism 3: Step Efficiency Gap
Even successful agents take 1.4-2.7x more steps than human-optimal trajectories due to trial-and-error, exploration, and suboptimal action sequences. Agents lack task-specific knowledge that humans apply (keyboard shortcuts, direct navigation paths).

## Foundational Learning

- **Concept**: Agentic Loop Latency Composition
  - Why needed: Understanding where time is spent (planning vs. reflection vs. grounding vs. action) is prerequisite for targeted optimization
  - Quick check: In a 10-step task taking 200 seconds total with 85% planning/reflection time, what's the maximum speedup from optimizing grounding by 50%?

- **Concept**: Prompt Context Accumulation in Transformers
  - Why needed: The prefill stage latency scales with prompt length. Understanding why later steps take longer (history injection) is essential before designing context management strategies
  - Quick check: If step 1 uses 2,000 prompt tokens and each step adds 1,500 tokens of history, how many tokens does step 20 process?

- **Concept**: Action Grouping for Efficiency
  - Why needed: The paper introduces grouped-action trajectories where consecutive actions (click, type, enter) share one observation. This directly reduces LLM calls without changing model capabilities
  - Quick check: A task has 12 single actions. If 8 can be grouped into 2 groups of 4, how many LLM planning calls are saved?

## Architecture Onboarding

- **Component map**: Screenshot capture → A11y tree generation → Set-of-Marks overlay → Planning module → Grounding module → Action execution → Reflection module → Retrieval module (once at task start)

- **Critical path**: Task initialization → Retrieval (once) → Per-step cycle: Screenshot → Planning → Grounding → Action → Reflection → Repeat until completion or failure. Planning + Reflection = 75-94% of total time.

- **Design tradeoffs**: A11y trees can reduce steps for some apps (GIMP: 49→17) but add 3-26s generation overhead. SoM overlay generally reduces steps but requires preprocessing. Max steps setting affects WES- penalty. Grouped actions reduce LLM calls but require predicting UI state validity.

- **Failure signatures**: Runaway latency (step N >3x step 1), step inflation (>2x human steps), WES+ collapse (high success but low efficiency), WES- near -1 (full step budget on failures).

- **First 3 experiments**: 1) Run Agent S2 on 10 OSWorld tasks with per-step timing breakdown to confirm planning+reflection >75%. 2) Manually group consecutive actions in existing trajectories to measure step reduction. 3) Modify planner to use only last N steps of history to establish baseline tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
How can agent architectures be redesigned to execute grouped actions in a single LLM call without compromising the ability to recover from unexpected UI changes? Current systems rely on iterative observe-plan-act loops. Grouping actions requires predicting future UI states accurately without intermediate verification, risking compounding errors.

### Open Question 2
Can adaptive perception strategies selectively utilize accessibility trees to avoid token bloat in complex applications? The study shows A11y trees help in GIMP but drastically increase latency in LibreOffice due to thousands of extra tokens. No mechanism proposed for dynamically switching observation modalities based on UI complexity.

### Open Question 3
What specific context management techniques can mitigate the 3x latency growth observed in later steps? The paper identifies context accumulation as a major bottleneck but does not evaluate methods to compress or summarize trajectory history to stabilize per-step inference time.

## Limitations
- Dataset generalization concerns due to single annotation methodology without detailed inter-annotator agreement scores
- Latency measurement environment not fully specified, potentially affecting the 75-94% planning/reflection dominance claims
- Architecture specificity limits generalizability to other CUA systems beyond Agent S2's compositional framework

## Confidence

- **High Confidence**: LLM call dominance (75-94%) is well-supported by direct measurements in Table 1
- **Medium Confidence**: Context accumulation scaling effect (3x latency increase) is supported by Figure 3 but lacks control for task complexity
- **Low Confidence**: Generalizability of findings to other architectures and precise impact of UI optimizations require further validation

## Next Checks

1. **Cross-Architecture Validation**: Run the same latency profiling methodology on at least two other CUA systems (e.g., UI-Evol, OpenCUA) to determine if the 75-94% planning/reflection dominance is architecture-specific or universal.

2. **Controlled Latency Isolation**: Design an experiment that separates model call overhead from context accumulation effects by testing agents with fixed short prompts vs. growing prompts while holding model latency constant.

3. **Human Trajectory Validation**: Conduct a user study where multiple human participants complete the same OSWorld tasks, recording their exact step sequences and timing to validate OSWorld-Human annotations and assess inter-human variability.