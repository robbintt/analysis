---
ver: rpa2
title: Adaptive Querying for Reward Learning from Human Feedback
arxiv_id: '2412.07990'
source_url: https://arxiv.org/abs/2412.07990
tags:
- feedback
- learning
- robot
- human
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Adaptive Feedback Selection (AFS), a method
  for robots to learn to avoid negative side effects (NSEs) by querying humans for
  feedback in multiple formats. AFS selects critical states for querying based on
  information gain and chooses feedback formats (approval, corrections, ranking, demonstration-action
  mismatch) that maximize learning while accounting for user effort and feedback likelihood.
---

# Adaptive Querying for Reward Learning from Human Feedback

## Quick Facts
- **arXiv ID:** 2412.07990
- **Source URL:** https://arxiv.org/abs/2412.07990
- **Reference count:** 40
- **Primary result:** AFS reduces NSEs more effectively than baselines while lowering user workload.

## Executive Summary
This paper presents Adaptive Feedback Selection (AFS), a method for robots to learn to avoid negative side effects (NSEs) by querying humans for feedback in multiple formats. AFS selects critical states for querying based on information gain and chooses feedback formats (approval, corrections, ranking, demonstration-action mismatch) that maximize learning while accounting for user effort and feedback likelihood. The approach iteratively refines the NSE penalty function and terminates querying based on KL-divergence convergence. Experiments in three simulated domains and a user study with a Kinova Gen3 arm show AFS reduces NSEs more effectively than baselines while lowering user workload. Users rated AFS as more competent and trustworthy compared to alternative approaches, demonstrating its practical effectiveness in balancing safety learning with human effort.

## Method Summary
AFS learns an NSE penalty function through iterative querying. States are clustered by feature similarity using KMeans with Jaccard distance, then critical states are selected based on KL-divergence between observed feedback distribution and predicted NSE distribution. For each query, AFS selects the feedback format that maximizes utility balancing information gain, user effort cost, and feedback likelihood. A Random Forest classifier predicts NSE labels (none/mild/severe mapped to penalties 0/-5/-10), and querying terminates when KL-divergence between predicted and observed distributions falls below threshold δ or budget B is exhausted. The learned penalty function combines with task reward to create the final policy.

## Key Results
- AFS achieved lowest user workload (M=2.34) among all conditions in user study
- Approval feedback had lowest individual workload (M=2.11) vs. DAM highest (M=2.62), p=0.026
- AFS with KLD stopping achieved comparable NSE penalty to budget=400 while using fewer queries
- Users rated AFS as more competent and trustworthy compared to alternative approaches

## Why This Works (Mechanism)

### Mechanism 1: Information-Gap-Guided Critical State Selection
Prioritizing queries in states with high prediction uncertainty accelerates NSE model convergence. States are clustered by feature similarity, with each cluster receiving a weight proportional to KL-divergence between observed and predicted NSE distributions. Critical states are sampled proportionally from high-divergence clusters where the model is most wrong. This assumes NSEs correlate with specific state features. Evidence shows critical states cluster near obstacles with format selection adapting over iterations. If NSEs are uncorrelated with state features, random sampling may outperform.

### Mechanism 2: Cost-Aware Feedback Format Selection
Balancing information gain against user effort reduces perceived workload while maintaining learning efficiency. For each format, compute information gain via KL-divergence if feedback received, then select format maximizing utility balancing gain, cost, and likelihood. Higher-cost formats must provide proportionally more information to be selected. This assumes the user preference model is stationary and state-independent. Evidence shows approval lowest workload vs. DAM highest, with AFS achieving lowest overall workload. If preference model parameters are mismeasured, format selection may repeatedly choose suboptimal formats.

### Mechanism 3: Iterative NSE Model Refinement with Early Stopping
KL-divergence provides a principled stopping criterion that terminates queries once the learned model approximates the true NSE distribution. Each iteration updates NSE labels, retrains classifier, and checks KL-divergence. Terminate when divergence falls below threshold across all states or budget exhausted. This assumes human feedback accurately reflects true NSE severity. Evidence shows AFS with KLD stopping achieved comparable performance to fixed budget while using fewer queries. If threshold is set too high, learning terminates prematurely; if too low, user burden increases without proportional gain.

## Foundational Learning

- **KL-Divergence as Information Gain**: Core metric for both critical state selection and format selection. Quick check: Can you explain why D_KL(p||q) ≠ D_KL(q||p), and which direction the paper uses?

- **MDP Reward Composition**: The final policy optimizes R(s,a) = θ₁R_T + θ₂R̂_N, requiring understanding of reward shaping and trade-off weights. Quick check: What happens to task performance if θ₂ is set too high relative to θ₁?

- **Active Learning Query Strategies**: AFS is an active learning approach sampling states based on uncertainty rather than randomly. Quick check: Why does the paper sample at least one state from each cluster even if its weight is low?

## Architecture Onboarding

- **Component map:**
  [State Space S] → [Clustering Module] → [Cluster Weights via IG]
         ↓                                      ↓
  [Feedback Preference Model D] ←──────── [Format Selector]
         ↓                                      ↓
  [Human User] → [Feedback in format f*] → [NSE Label Update p_t]
                                                ↓
                                        [RF Classifier P] → [Predicted q_t]
                                                ↓
                                    [KLD Check] → Stop or Continue
                                                ↓
                                    [Penalty R̂_N] → [Combined Policy]

- **Critical path:**
  1. State clustering (offline or incremental) — must capture NSE-correlated features
  2. KL-divergence computation — requires both p_t (observed) and q_t (predicted) distributions
  3. Format selection — depends on accurate ψ(f) and C(f) from preference model
  4. Classifier training — Random Forest with 3-fold CV hyperparameter search

- **Design tradeoffs:**
  - **Cluster count K**: Higher K = more refined queries but risk of sparse clusters. Paper uses K=2-5 based on feature combinations
  - **NSE label granularity**: 3 categories (none/mild/severe) vs. continuous — more categories require more feedback
  - **δ threshold**: Lower = more thorough learning but more queries. Domain-specific tuning required
  - **Feedback format set F**: More formats increase flexibility but complicate preference model estimation

- **Failure signatures:**
  - **High NSE penalty despite many queries**: Check if critical states cluster correctly; features may not capture NSE correlates
  - **Single format repeatedly selected**: ψ(f) may be overestimated or C(f) underestimated for that format
  - **KLD never converges**: Possible label noise, classifier underfitting, or contradictory feedback across similar states
  - **User fatigue complaints**: C(f) values likely misaligned with actual effort; recalibrate preference model

- **First 3 experiments:**
  1. **Validate clustering on your domain**: Run KMeans on state features; verify clusters separate NSE vs. non-NSE states (if ground truth available)
  2. **Calibrate preference model**: Pilot with 5-10 users to estimate ψ(f) and C(f) for your format set; compare self-reported vs. behavioral measures
  3. **Ablate KLD stopping**: Run AFS with fixed budget vs. KLD threshold; plot NSE penalty vs. query count to validate early termination preserves performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Adaptive Feedback Selection (AFS) be extended to continuous state and action spaces?
- **Basis in paper:** [explicit] The authors state that future work will focus on extending AFS to continuous domains, noting challenges in identifying critical states and estimating information gain in high-dimensional settings.
- **Why unresolved:** The current formulation relies on discrete state clustering and specific KL-divergence calculations that are computationally intractable or undefined in continuous environments.
- **Evidence:** A modified AFS algorithm using function approximation or density estimation that maintains sample efficiency in continuous robotics tasks.

### Open Question 2
- **Question:** How can the system identify and mitigate the impact of biased human feedback?
- **Basis in paper:** [explicit] The conclusion notes that "biased feedback can misguide the robot and lead to unintended NSEs," and understanding when such biases arise remains an open challenge.
- **Why unresolved:** AFS currently assumes feedback is an accurate proxy for the true safety reward (Assumption 2), lacking mechanisms to filter or adjust for systematic user error.
- **Evidence:** A bias-aware inference mechanism integrated into AFS that outperforms the baseline under conditions of simulated user bias.

### Open Question 3
- **Question:** Is AFS robust to violations of the assumption that human feedback is immediate and accurate?
- **Basis in paper:** [inferred] Assumption 2 asserts immediate and accurate feedback, but real-world human-robot interaction often involves latency, noise, or temporary inattention.
- **Why unresolved:** The algorithm updates the NSE model and information gain based on the strict reception of the requested format, potentially degrading if feedback is inconsistent.
- **Evidence:** Empirical results measuring performance degradation when introducing feedback delays or labeling noise into the simulation.

## Limitations

- The effectiveness of clustering for continuous/high-dimensional state spaces remains unvalidated
- The preference model D = ⟨F, ψ, C⟩ is treated as stationary and state-independent, but user preferences may shift based on context, fatigue, or learning progress
- The Random Forest classifier propagates any systematic bias in human judgments, potentially learning incorrect penalty functions

## Confidence

- **High confidence:** AFS reduces user workload compared to baselines (supported by user study p=0.026 for approval vs. DAM comparison)
- **Medium confidence:** KLD-based stopping criterion effectively balances query efficiency with learning quality (empirical validation across three domains)
- **Medium confidence:** Multi-format selection outperforms single-format approaches (user preference data supports this, though limited to six participants)

## Next Checks

1. Test AFS with noisy or biased feedback oracles to quantify sensitivity to Assumption 2 violations
2. Evaluate clustering performance on continuous state spaces with high-dimensional features
3. Validate whether the stationary preference model D holds across multiple querying sessions with the same users