---
ver: rpa2
title: What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis
  of Prompts, Models, and Adaptation
arxiv_id: '2509.14979'
source_url: https://arxiv.org/abs/2509.14979
tags:
- feature
- recommendation
- sequential
- performance
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically analyzes the LLM-as-feature-extractor
  paradigm in sequential recommendation by decomposing the pipeline into four modular
  components: data processing, feature extraction, feature adaptation, and sequential
  modeling. The authors evaluate design choices within each module through controlled
  experiments on four public datasets, identifying optimal configurations for prompt
  construction (attributes flatten), LLM fine-tuning (CPT+SFT), feature aggregation
  (mean pooling), and feature adaptation (PCA-enhanced MoE).'
---

# What Matters in LLM-Based Feature Extractor for Recommender? A Systematic Analysis of Prompts, Models, and Adaptation

## Quick Facts
- arXiv ID: 2509.14979
- Source URL: https://arxiv.org/abs/2509.14979
- Authors: Kainan Shi; Peilin Zhou; Ge Wang; Han Ding; Fei Wang
- Reference count: 40
- Key outcome: RecXplore framework achieves up to 18.7% relative improvement in NDCG@5 and 15.1% in HR@5 over strong baselines

## Executive Summary
This paper systematically analyzes the LLM-as-feature-extractor paradigm in sequential recommendation by decomposing the pipeline into four modular components: data processing, feature extraction, feature adaptation, and sequential modeling. The authors evaluate design choices within each module through controlled experiments on four public datasets, identifying optimal configurations for prompt construction (attributes flatten), LLM fine-tuning (CPT+SFT), feature aggregation (mean pooling), and feature adaptation (PCA-enhanced MoE). By assembling these best practices, the proposed RecXplore framework achieves up to 18.7% relative improvement in NDCG@5 and 15.1% in HR@5 over strong baselines, demonstrating that systematic modular analysis can yield superior performance without architectural over-engineering. The framework is model-agnostic and generalizable across downstream sequential recommenders, with zero online latency due to precomputed embeddings.

## Method Summary
The authors propose a 4-module pipeline for sequential recommendation using LLMs as feature extractors: (1) Data processing with Attributes Flatten prompt construction to format item metadata (brand, category, title), (2) Feature extraction using LLaMA2-7B with CPT+SFT fine-tuning and mean pooling to generate 4096-dim embeddings, (3) Feature adaptation with PCA (4096→1536) followed by MoE adapter (8 experts, 128-dim output), and (4) Sequential modeling with SASRec backbone using semantic embeddings instead of ID embeddings. The framework is evaluated on Steam and three Amazon category datasets with HR@{5,10} and NDCG@{5,10} metrics, using 100 random negatives per test item.

## Key Results
- RecXplore achieves 18.7% relative improvement in NDCG@5 and 15.1% in HR@5 over strong baselines
- Attributes Flatten prompt construction outperforms Stepwise Highlight and No Context by 5-8% in HR@5
- Mean pooling significantly outperforms max pooling (0.43 vs 0.18 HR@5 on Beauty dataset)
- CPT+SFT fine-tuning yields 6-10% improvements over other fine-tuning strategies

## Why This Works (Mechanism)
The paper demonstrates that LLM-based feature extraction can capture rich semantic relationships in item metadata that traditional ID-based embeddings miss. By systematically analyzing each pipeline component, the authors identify that the combination of structured prompt engineering, domain-adapted fine-tuning, appropriate feature aggregation, and efficient dimensionality reduction creates embeddings that better capture sequential patterns. The zero-online-latency design enables practical deployment while maintaining strong recommendation performance.

## Foundational Learning
1. **LLM-as-feature-extractor paradigm**: Why needed - Traditional ID embeddings lack semantic information; quick check - Compare semantic vs ID embeddings on downstream task performance
2. **Prompt engineering strategies**: Why needed - Different prompt formats affect semantic extraction quality; quick check - A/B test prompt variations on same LLM
3. **Fine-tuning strategies (CPT+SFT)**: Why needed - Domain adaptation improves semantic relevance for recommendation; quick check - Measure domain shift before/after fine-tuning
4. **Feature aggregation methods**: Why needed - Pooling strategy affects semantic preservation; quick check - Compare mean vs max pooling on semantic similarity tasks
5. **Dimensionality reduction (PCA)**: Why needed - Reduces computational cost while preserving semantic structure; quick check - Plot explained variance vs target dimension
6. **MoE adapter architecture**: Why needed - Enables efficient feature adaptation for downstream models; quick check - Compare performance vs full fine-tuning

## Architecture Onboarding
**Component map**: Data Processing -> Feature Extraction -> Feature Adaptation -> Sequential Modeling
**Critical path**: Item metadata → LLM embedding → PCA reduction → MoE projection → SASRec prediction
**Design tradeoffs**: Structured metadata assumption vs unstructured data, semantic richness vs computational efficiency, zero-latency vs real-time adaptation
**Failure signatures**: Max pooling causes severe performance degradation; MoE concatenation provides no benefit over replacement
**First experiments**: 1) Compare prompt construction methods on same LLM, 2) Test different pooling strategies on extracted features, 3) Evaluate PCA dimension sensitivity on downstream performance

## Open Questions the Paper Calls Out
None

## Limitations
- The modular decomposition approach assumes independence between components, but empirical evidence shows feature adaptation strategies interact strongly with downstream sequential modeling performance
- The optimal configuration (CPT+SFT fine-tuning + mean pooling + MoE) may be dataset-specific rather than universally applicable across domains
- The evaluation focuses exclusively on public datasets with structured metadata, limiting generalizability to domains with richer or noisier item descriptions

## Confidence
**High confidence**: The relative ordering of prompt constructions (Attributes Flatten > Stepwise Highlight > No Context), the superiority of mean pooling over max pooling for semantic aggregation, and the effectiveness of CPT+SFT fine-tuning over other adaptation strategies are supported by robust ablation experiments across multiple datasets.

**Medium confidence**: The claim that RecXplore achieves "superior performance without architectural over-engineering" is substantiated for the tested datasets but requires validation on more diverse recommendation scenarios. The assertion that the framework is "model-agnostic" needs broader empirical support across different LLM architectures and sequential modeling backbones.

**Low confidence**: The optimal parameter choices (8 experts, 128-dim output, 1536-dim PCA target) are presented without systematic sensitivity analysis, making it difficult to assess robustness to hyperparameter variations.

## Next Checks
1. Cross-domain robustness test: Evaluate RecXplore on datasets with unstructured or multimodal item metadata to assess generalizability beyond the structured metadata assumption
2. Real-time performance validation: Measure actual latency and throughput in an online deployment scenario to verify the "zero online latency" claim, including preprocessing pipeline overhead
3. Ablation on interaction effects: Conduct factorial experiments testing all combinations of the four best-performing configurations to quantify interaction effects and identify whether the modular decomposition approach captures the true optimization landscape