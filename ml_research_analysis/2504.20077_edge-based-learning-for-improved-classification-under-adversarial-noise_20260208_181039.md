---
ver: rpa2
title: Edge-Based Learning for Improved Classification Under Adversarial Noise
arxiv_id: '2504.20077'
source_url: https://arxiv.org/abs/2504.20077
tags:
- adversarial
- noise
- images
- clean
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of adversarial noise on image
  classification, specifically focusing on the Fast Gradient Sign Method (FGSM) attacks.
  The research hypothesized that edge-based features in images might remain more stable
  under adversarial perturbations compared to raw image data.
---

# Edge-Based Learning for Improved Classification Under Adversarial Noise

## Quick Facts
- arXiv ID: 2504.20077
- Source URL: https://arxiv.org/abs/2504.20077
- Reference count: 33
- Primary result: Edge-based models show greater robustness to FGSM adversarial attacks than models trained on raw images

## Executive Summary
This study investigates the impact of adversarial noise on image classification, specifically focusing on Fast Gradient Sign Method (FGSM) attacks. The research hypothesizes that edge-based features in images remain more stable under adversarial perturbations compared to raw image data. Experiments conducted on brain tumor and COVID datasets with multiple deep learning models demonstrate that models trained on edge-based representations show significantly greater robustness to adversarial attacks while maintaining high accuracy on clean images.

## Method Summary
The study applies Canny edge detection (thresholds 100, 200) to convert images to binary edge maps, then trains multiple deep learning models including CNN, ResNet50, VGG16/19, InceptionV3, and DenseNet121 on both original and edge-processed images. FGSM adversarial noise is generated using each model's own loss function. Models are evaluated on clean images and FGSM-perturbed images, then retrained on a combination of clean and noisy examples (800 each, 1:1 ratio) to assess improvement in robustness.

## Key Results
- Models trained on edge representations maintained 86-91% accuracy under FGSM noise compared to 56-74% for original-image models
- CNN trained on edges achieved 95% clean accuracy (vs 97% on original) but showed much smaller drop under adversarial attack
- Retraining on combined clean and noisy data improved robustness for both approaches, with original data models showing marginally greater improvement
- Edge-trained models showed better transferability resistance when exposed to noise generated from other models

## Why This Works (Mechanism)

### Mechanism 1
Edge-based representations provide greater robustness against FGSM adversarial noise because Canny edge detection converts images to a binary representation of structural boundaries. Adversarial perturbations, designed to exploit complex non-edge regions (texture, gradients, color), have significantly less surface area to manipulate in a sparse edge map. The edge extraction acts as a low-pass filter, inadvertently discarding much of the adversarial signal before it reaches the classifier.

### Mechanism 2
Training on edge maps forces models to rely on structural shape features rather than exploitable texture and color features. Models learn to classify based primarily on geometric shape and contours, making their learned structural representations more stable and less fooled when adversarial perturbations result in different patterns when converted to edge maps.

### Mechanism 3
Combining clean and adversarial examples during retraining improves model robustness by allowing it to learn a more robust decision boundary that accounts for perturbations. Edge-training models already start with higher robustness, leaving less room for improvement compared to original-data models that benefit more from the retraining process.

## Foundational Learning

- **Adversarial Attacks (FGSM)**: The study's central problem is defending against FGSM. FGSM works by adding a small, imperceptible perturbation in the direction of the gradient of the loss with respect to the input. Quick check: If you have a model and an image, what two pieces of information do you need to calculate a single-step FGSM attack?

- **Canny Edge Detection**: This is the proposed defense mechanism. It's a multi-step algorithm (noise reduction, gradient calculation, non-maximum suppression, double thresholding) to extract binary structural lines from an image. Quick check: Why is the Gaussian blurring step critical in the Canny algorithm before calculating gradients?

- **Feature Squeezing / Dimensionality Reduction**: This is the broader class of defense to which this paper's method belongs. By reducing the complexity of the input space (from a full RGB image to a sparse binary edge map), you limit the ways an attacker can manipulate the input to fool the model. Quick check: How does reducing the number of possible input values conceptually make it harder to craft a successful adversarial example?

## Architecture Onboarding

- **Component map**: Original Image -> Canny Preprocessor -> Edge Map -> Classifier -> Prediction

- **Critical path**: The critical path for the proposed defense is the Canny Edge Detection step. All robustness gains hinge on the properties of this transformation. The parameters of this edge detection (threshold values of 100 and 200) are therefore critical hyperparameters.

- **Design tradeoffs**:
  - **Robustness vs. Baseline Accuracy**: Slight drop in clean-image accuracy for some models (e.g., InceptionV3 drops from 94% on original clean to 89% on edge clean) is the core tradeoff: sacrificing a small amount of ideal-world performance for a large gain in worst-world robustness.
  - **Generalization vs. Specificity**: The method is tested on FGSM attacks only; it is unclear how well it generalizes to other attack types that might perturb edges more aggressively.

- **Failure signatures**:
  - **Low Baseline Accuracy**: If your edge-trained model has <80% accuracy on clean images, the edge extraction is likely discarding too much critical diagnostic information. Review Canny thresholds.
  - **High Fooling Rate on Edges**: If the edge-trained model is still fooled at a high rate (>50%), the attack may be perturbing edge features directly. Inspect visualizations of noisy vs. clean edge maps.
  - **No Improvement from Retraining**: If adversarial retraining doesn't improve robustness, the adversarial examples in the training set may not be representative or the model may be overfitting. Check the epsilon value used for generating training noise.

- **First 3 experiments**:
  1. **Baseline & Robustness Evaluation**: Train a standard classifier on a clean dataset. Evaluate its accuracy on clean images and then on FGSM-perturbed images. This establishes the vulnerability baseline.
  2. **Implement and Validate Edge Preprocessing**: Implement Canny edge detection with the paper's specified thresholds (100, 200). Train the same classifier architecture on the edge-processed dataset. Evaluate accuracy on clean edges and FGSM-perturbed edges. Compare the drop in accuracy to the baseline.
  3. **Adversarial Retraining Experiment**: Create a combined dataset with a 1:1 ratio of clean and FGSM-perturbed examples. Retrain the models on this combined data and evaluate the performance gain on a held-out set of noisy examples.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the robustness of edge-based learning persist against stronger iterative adversarial attacks like Projected Gradient Descent (PGD) or AutoAttack? The study explicitly limits its evaluation to FGSM, a single-step attack, while acknowledging that more complex attacks exist.

- **Open Question 2**: Does edge-based learning maintain classification accuracy on complex natural image datasets where texture and color are primary discriminative features? The paper validates exclusively on medical imaging datasets (Brain Tumor and COVID-19 X-rays), which rely heavily on structural shape.

- **Open Question 3**: Can a hybrid architecture that fuses raw pixel data with edge representations outperform the single-modality approaches? The paper notes that accuracy improvement after retraining is "marginally more in the original data as compared to the edges," suggesting raw data retains specific features that edges discard.

## Limitations

- The defense has only been validated against FGSM attacks at specific epsilon values, with no testing against more sophisticated attacks like PGD or C&W.
- The effectiveness depends on the assumption that adversarial noise primarily targets non-edge regions, which may not hold for all attack types.
- The marginal improvement from retraining on combined clean and noisy data suggests diminishing returns for edge-based models that already start with higher robustness.

## Confidence

- **High confidence**: The core finding that edge-based models show greater robustness to FGSM attacks (e.g., CNN: 95%→86% vs 97%→56%) is well-supported by experimental results.
- **Medium confidence**: The claim that adversarial retraining provides marginally more benefit to original data models is supported but requires verification across different epsilon values and attack types.
- **Low confidence**: The generalizability of these results to other attack methods and medical imaging tasks beyond brain tumors and COVID-19 remains unproven.

## Next Checks

1. **Test Against Multiple Attack Types**: Validate the edge-based defense against PGD, C&W, and other adaptive attacks that may explicitly target edge features, measuring accuracy drops across all attack types.

2. **Parameter Sensitivity Analysis**: Systematically vary Canny edge detection thresholds and epsilon values to identify the optimal tradeoff between baseline accuracy and adversarial robustness across all tested models.

3. **Cross-Dataset Generalization**: Apply the edge-based defense to different medical imaging datasets (e.g., skin lesions, diabetic retinopathy) and non-medical image classification tasks to assess whether the structural feature preservation assumption holds universally.