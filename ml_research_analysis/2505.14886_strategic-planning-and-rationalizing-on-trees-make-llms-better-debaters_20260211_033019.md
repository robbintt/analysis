---
ver: rpa2
title: Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters
arxiv_id: '2505.14886'
source_url: https://arxiv.org/abs/2505.14886
tags:
- debate
- tree
- flow
- human
- trees
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses strategic planning challenges in competitive
  debates, where time constraints and lack of objective reward signals make it difficult
  for large language models to allocate resources effectively. To address this, the
  authors propose TreeDebater, which uses two tree structures: a Rehearsal Tree to
  anticipate attacks and defenses for each claim, and a Debate Flow Tree to track
  the evolving debate status and identify candidate actions.'
---

# Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters

## Quick Facts
- arXiv ID: 2505.14886
- Source URL: https://arxiv.org/abs/2505.14886
- Reference count: 40
- Primary result: TreeDebater outperforms Agent4Debate by +15.6% in stage-level persuasiveness and +10% in debate-level opinion shift win rate

## Executive Summary
This paper introduces TreeDebater, a strategic planning system for LLMs in competitive debates. The system addresses time constraints and lack of objective rewards by using two tree structures: a Rehearsal Tree for pre-debate argument strength calculation and a Debate Flow Tree for live debate state tracking. Human evaluations show significant improvements over existing multi-agent debate systems, with better persuasiveness scores and opinion shift win rates. The system also demonstrates more diverse action selection aligned with human expert strategies.

## Method Summary
TreeDebater employs a planning-then-rationalizing framework using two complementary tree structures. The Rehearsal Tree pre-computes argument strengths by simulating potential opponent responses through a minimax algorithm, while the Debate Flow Tree maintains the evolving debate state during live matches. The system integrates a speech time controller using TTS estimation and binary search, plus a simulated audience feedback loop for iterative refinement. Two LLaMA-3.2-3B reward models trained on Kialo data classify argument impact scores, and the system retrieves relevant arguments using semantic similarity (threshold 0.8) from a human debate corpus.

## Key Results
- TreeDebater achieves +15.6% improvement in stage-level persuasiveness over Agent4Debate using DeepSeek
- System shows +10% higher debate-level opinion shift win rate compared to baseline
- TreeDebater elicits more diverse debate actions and better aligns with human expert strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Rehearsal Tree enables forward-looking strategic planning by simulating potential opponent counterarguments and calculating argument strength.
- Mechanism: The system constructs a tree of potential claims and counter-arguments to a given depth (L). It then uses a minimax-like recursive algorithm to calculate a "strength score" ($f_k$) for each node. This score propagates up from leaf nodes, discounted by a decay factor ($\gamma$), representing the expected utility of an argument assuming optimal play from the opponent.
- Core assumption: Debate can be modeled as a turn-based zero-sum game where a minimax-style evaluation of potential future states correlates with persuasiveness.
- Evidence anchors:
  - [abstract] "The Rehearsal Tree anticipates the attack and defenses to evaluate the strength of the claim..."
  - [section] Section 3.2 details the recursive calculation: $f_k(x_l) = f_0(x_l) - \gamma \cdot \max_{x_{l+1} \in \text{Child}(x_l)} f_{k-1}(x_{l+1})$.
  - [corpus] This use of a search tree for planning is similar to adaptive systems in other complex domains like game AI (Adaptive Command, max_h_index=71).
- Break condition: The mechanism's effectiveness may degrade if the simulated opponent responses generated by the LLM do not match the actual opponent's strategy, or if the LLM's generated arguments for the tree are of low quality.

### Mechanism 2
- Claim: The Debate Flow Tree provides a structured state representation that allows the system to maintain context and select contextually relevant actions.
- Mechanism: The system parses the ongoing debate into a tree structure that tracks the state of claims. Each node represents a claim with arguments, a status (e.g., `proposed`, `attacked`), and a visit count. This tree serves as a structured memory, allowing the system to identify which points are unresolved and select actions like "attack," "rebut," or "reinforce" accordingly.
- Core assumption: A structured tree representation of debate state is more effective for long-term planning than a linear history or an unstructured context window.
- Evidence anchors:
  - [abstract] "...Debate Flow Tree to track the debate status to identify the active actions."
  - [section] Section 3.3 describes how the tree "tracks the debate status by keeping all proposed claims with the corresponding attack and defense in a tree structure" and extracts candidate actions.
  - [corpus] Related work like "R-Debater" emphasizes maintaining "argumentative memory" for debate generation, which aligns with the need for structured state tracking.
- Break condition: This mechanism will fail if the parser incorrectly maps arguments to nodes, fragmenting the state representation, or if the tree grows too large and complex for the LLM to effectively reason over.

### Mechanism 3
- Claim: The Speech Time Controller and simulated audience feedback create a refinement loop that aligns the final statement with strict constraints and human-like persuasiveness.
- Mechanism: After an initial draft is created, a text-to-speech model estimates its duration. If it falls outside the target range $[t_l, t_r]$, a binary search algorithm adjusts the word budget, and the LLM re-generates the statement. Simultaneously, a "simulated audience" (another LLM prompted with retrieved examples of human debate flow) provides feedback on clarity and engagement, guiding further revision.
- Core assumption: Persuasiveness and time compliance can be improved through iterative, feedback-driven revision, and that a TTS model can accurately predict speech duration from text.
- Evidence anchors:
  - [abstract] "TreeDebater allocates its time budget among candidate actions and uses the speech time controller and feedback from the simulated audience to revise its statement."
  - [section] Section 3.5 describes using the FastSpeech model to estimate speech length and employing a binary search for a new word budget $n$.
  - [corpus] Corpus evidence for this specific combination of mechanisms is weak, but iterative refinement with feedback is a common theme in generative AI.
- Break condition: The loop may not converge if the LLM fails to adhere to the new word budget, or if the simulated audience's feedback is not actionable or contradicts human preferences.

## Foundational Learning

- Concept: Game Trees and Minimax Search
  - Why needed here: Understanding how the Rehearsal Tree uses a minimax algorithm to evaluate argument strength by simulating future moves is key to understanding the system's planning capability.
  - Quick check question: How does the system calculate the strength score of a claim in the Rehearsal Tree?

- Concept: Tree-based State Representation
  - Why needed here: The Debate Flow Tree is not just a list of arguments but a structured representation of the debate's current state, which is fundamental to the system's ability to decide on its next action.
  - Quick check question: What is the primary function of the Debate Flow Tree during a live debate?

- Concept: Iterative Refinement with Feedback
  - Why needed here: The system's final output is not a single-pass generation but the result of a loop involving a time controller and a simulated audience. Understanding this process is crucial.
  - Quick check question: What are the two main constraints or feedback sources that drive the revision of the debate statement?

## Architecture Onboarding

- Component map:
  - Planner (Pre-debate) -> Rehearsal Trees -> Backbone LLM -> Reward Models
  - State Manager (Live) -> Debate Flow Tree -> Action Selector -> Backbone LLM
  - Writer -> Speech Time Controller (FastSpeech + Binary Search) -> Simulated Audience -> Refined Statement

- Critical path: The core inference loop is: 1. `Listen` & Update `Debate Flow Tree`. 2. `Retrieve` from `Rehearsal Trees` for candidate actions. 3. `Draft` statement. 4. `Revise` via `Speech Time Controller` & `Simulated Audience` feedback.

- Design tradeoffs:
  - **Pre-computation vs. Live Generation:** A large, deep Rehearsal Tree improves preparation but increases pre-debate latency and memory use. The paper uses a max depth L, which is a tunable hyperparameter.
  - **LLM vs. Rule-based Parsing:** Parsing debate statements into the tree likely uses an LLM, which is flexible but can be error-prone compared to rigid, rule-based parsers.
  - **Time Control Granularity:** Using a binary search on word count is an indirect way to control speech time. A more complex approach might involve fine-grained sentence-level editing.

- Failure signatures:
  - **State Desynchronization:** The Debate Flow Tree becomes corrupted or out of sync with the actual debate, leading to nonsensical responses (e.g., attacking a point the opponent never made).
  - **Time Control Divergence:** The binary search loop fails to converge, causing the system to either produce a statement that is too short/long or to exceed maximum iterations.
  - **Retrieval Mismatch:** The system fails to find a relevant argument in its Rehearsal Tree for a novel opponent point, forcing it to generate a response from scratch, potentially weakening its position.

- First 3 experiments:
  1. **Ablate Tree Structures:** Run debates with the `Rehearsal Tree` only, `Debate Flow Tree` only, and neither. Measure the drop in persuasiveness and win rate to quantify each component's contribution.
  2. **Vary Rehearsal Tree Depth:** Experiment with different maximum depths (`L`) for the Rehearsal Tree. Analyze the trade-off between improved strategic foresight (deeper tree) and increased computation/latency.
  3. **Stress Test Time Control:** Feed the Speech Time Controller with deliberately difficult prompts (e.g., emotional, complex) and measure its convergence rate and the final time deviation from the target.

## Open Questions the Paper Calls Out

- To what extent do TreeDebater's persuasiveness improvements generalize to larger, more demographically diverse, or expert-heavy evaluation groups?
  - Basis in paper: [explicit] The Limitations section states the study's conclusions are potentially limited by the scale of human evaluations and explicitly suggests future work could involve "larger, more diverse evaluation groups."
  - Why unresolved: The current study relied on 212 participants from a specific platform (Prolific), which may not capture the nuances of expert judging or cross-cultural persuasion norms.
  - What evidence would resolve it: Large-scale human evaluations involving professional debate judges or demographically distinct audience pools showing consistent win rates and persuasiveness scores.

## Limitations

- The paper does not specify the maximum depth (L) and branching factor (B) for the Rehearsal Tree, which are critical hyperparameters for the strategic planning mechanism.
- The "stage-specific prompts from the AIDebater 2024 competition" used as a base are referenced but not provided, which could affect the system's performance.
- While the Kialo dataset preprocessing is mentioned, the exact logic for creating the 3691/3676 argument trees is not fully detailed, which could impact the reward model training.

## Confidence

- **High Confidence:** The core architectural components (Rehearsal Tree, Debate Flow Tree, Speech Time Controller) are well-defined and their individual mechanisms are clearly explained.
- **Medium Confidence:** The ablation study shows quantitative improvements over baselines, but the lack of specified hyperparameters and external prompts introduces uncertainty about exact reproducibility.
- **Low Confidence:** The specific prompts and the detailed preprocessing of the Kialo dataset are not fully provided, which are necessary for a complete reproduction.

## Next Checks

1. **Hyperparameter Sweep:** Conduct experiments to determine the optimal values for the Rehearsal Tree depth (L) and branching factor (B), and measure their impact on both planning quality and computational cost.
2. **Prompt Transparency:** Request or reconstruct the AIDebater 2024 stage-specific prompts to ensure the system's behavior is not dependent on proprietary or unavailable external information.
3. **Reward Model Audit:** Recreate the Kialo preprocessing pipeline and retrain the support/attack classifiers to verify the claimed impact score distributions and the resulting tree strength calculations.