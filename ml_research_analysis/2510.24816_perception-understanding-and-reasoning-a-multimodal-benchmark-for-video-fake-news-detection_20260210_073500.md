---
ver: rpa2
title: Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake
  News Detection
arxiv_id: '2510.24816'
source_url: https://arxiv.org/abs/2510.24816
tags:
- news
- video
- fake
- text
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POVFNDB, a process-oriented benchmark for
  video fake news detection (VFND) that evaluates MLLMs across 10 tasks spanning perception,
  understanding, and reasoning capabilities. The benchmark includes 36,240 human-annotated
  question-answer pairs across 15 evaluation dimensions, enabling fine-grained assessment
  of the entire detection process rather than just final accuracy.
---

# Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection

## Quick Facts
- arXiv ID: 2510.24816
- Source URL: https://arxiv.org/abs/2510.24816
- Reference count: 40
- Introduces POVFNDB benchmark for video fake news detection across perception, understanding, and reasoning tasks

## Executive Summary
This paper presents POVFNDB, a comprehensive benchmark for video fake news detection that evaluates multimodal large language models (MLLMs) across 10 tasks spanning perception, understanding, and reasoning capabilities. The benchmark includes 36,240 human-annotated question-answer pairs across 15 evaluation dimensions, enabling fine-grained assessment of the entire detection process rather than just final accuracy. The authors identify key limitations in current MLLMs, particularly in creator-added text perception and multi-element temporal grounding. To address these challenges, they propose POVFND-CoT, a chain-of-thought framework that generates process-aware reasoning data, achieving state-of-the-art performance of 81.14% accuracy on VFND through fine-tuning Qwen2.5VL-7B-Instruct.

## Method Summary
The authors constructed POVFNDB by collecting 1,400+ real-world videos with 36,240 human-annotated question-answer pairs across 15 evaluation dimensions. The benchmark systematically evaluates MLLMs across three phases: perception (visual and textual element extraction), understanding (context comprehension and relationship identification), and reasoning (logical inference and temporal analysis). To address identified limitations, they developed POVFND-CoT, which generates reasoning traces through a chain-of-thought approach, creating process-aware data for fine-tuning. The framework leverages Qwen2.5VL-7B-Instruct as the base model, fine-tuned on the generated reasoning data to achieve improved performance on video fake news detection tasks.

## Key Results
- POVFNDB achieves 81.14% accuracy on VFND through fine-tuned Qwen2.5VL-7B-Instruct
- Comprehensive evaluation reveals MLLMs struggle with creator-added text perception and multi-element temporal grounding
- Fine-tuning on POVFND-CoT data significantly outperforms general capabilities across all 15 evaluation dimensions

## Why This Works (Mechanism)
The approach succeeds by decomposing video fake news detection into systematic perception, understanding, and reasoning phases, allowing targeted assessment and improvement of each capability. The chain-of-thought framework generates explicit reasoning traces that capture the multi-step nature of fake news detection, enabling models to learn domain-specific strategies rather than relying on general multimodal understanding.

## Foundational Learning
- **Multimodal video processing**: Needed to extract and integrate visual and textual information from videos; quick check: ability to identify objects, text, and temporal relationships
- **Chain-of-thought reasoning**: Required for breaking down complex fake news detection into sequential logical steps; quick check: generation of coherent reasoning traces
- **Video temporal grounding**: Essential for understanding how elements relate across time in video content; quick check: accurate identification of when specific events or text appear
- **Fake news detection methodology**: Provides domain-specific knowledge for identifying misinformation patterns; quick check: recognition of common manipulation techniques
- **Human annotation consistency**: Critical for ensuring reliable ground truth across diverse video content; quick check: inter-rater agreement metrics
- **Multilingual video analysis**: Important for generalizability across different cultural contexts; quick check: performance across varied linguistic content

## Architecture Onboarding
**Component Map**: Video Input -> Perception Module -> Understanding Module -> Reasoning Module -> Output Classification

**Critical Path**: The most critical path is Perception -> Understanding -> Reasoning, as errors in early stages compound through the pipeline. The reasoning module is particularly crucial as it determines the final classification accuracy.

**Design Tradeoffs**: The framework trades computational efficiency for comprehensive evaluation across 15 dimensions, prioritizing thorough assessment over speed. The chain-of-thought approach increases reasoning time but enables better interpretability and targeted improvements.

**Failure Signatures**: Common failures include misidentification of creator-added text (often due to OCR limitations), temporal grounding errors (especially with rapid scene changes), and logical inference mistakes when connecting multiple elements across the video timeline.

**First Experiments**:
1. Test baseline MLLM performance on isolated perception tasks to establish capability gaps
2. Evaluate reasoning trace generation quality from POVFND-CoT framework
3. Conduct ablation study removing chain-of-thought components to measure their contribution

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The dataset focuses primarily on Chinese social media content, potentially limiting generalizability to other linguistic and cultural contexts
- Human annotation may introduce subjective biases in what constitutes "fake news" across different contexts and temporal periods
- The evaluation focuses on final accuracy metrics without detailed analysis of remaining error types or whether improvements reflect genuine reasoning versus pattern matching

## Confidence
**High confidence in**: The systematic evaluation framework across perception, understanding, and reasoning tasks; architectural improvements through POVFND-CoT; empirical results showing performance gains over baseline models

**Medium confidence in**: The claim that MLLMs specifically struggle with creator-added text perception and multi-element temporal grounding; generalizability beyond the specific dataset domain

**Low confidence in**: The extent to which POVFND-CoT represents genuine reasoning improvement versus sophisticated pattern matching; interpretability of reasoning traces without detailed analysis

## Next Checks
1. Conduct cross-cultural validation by testing the benchmark and models on video fake news datasets from different linguistic and geographic regions to assess generalizability

2. Perform ablation studies isolating the contribution of each reasoning component in POVFND-CoT to determine which aspects drive performance improvements versus potential memorization effects

3. Implement detailed error analysis categorizing failure modes across all 15 evaluation dimensions to understand whether remaining errors reflect fundamental reasoning limitations or dataset-specific issues