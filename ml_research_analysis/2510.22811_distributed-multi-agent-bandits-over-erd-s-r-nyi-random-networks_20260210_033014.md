---
ver: rpa2
title: "Distributed Multi-Agent Bandits Over Erd\u0151s-R\xE9nyi Random Networks"
arxiv_id: '2510.22811'
source_url: https://arxiv.org/abs/2510.22811
tags:
- regret
- graph
- bound
- communication
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies a distributed multi-agent multi-armed bandit\
  \ problem with heterogeneous rewards over time-varying Erd\u0151s-R\xE9nyi random\
  \ communication graphs. The key challenge is that agents must learn optimal actions\
  \ while communicating over random networks where each edge in a base graph is present\
  \ with probability p, and the resulting graph may be disconnected at each time step."
---

# Distributed Multi-Agent Bandits Over Erdős-Rényi Random Networks

## Quick Facts
- arXiv ID: 2510.22811
- Source URL: https://arxiv.org/abs/2510.22811
- Authors: Jingyuan Liu; Hao Qiu; Lin Yang; Mengfan Xu
- Reference count: 40
- Primary result: O(∑k:∆k>0 logT/∆k + N²logT/(pλN−1(Lap(G))) + KN²logT/p) regret bound for distributed multi-agent bandits over random networks

## Executive Summary
This paper addresses the challenge of distributed multi-agent multi-armed bandit learning over time-varying Erdős-Rényi random communication graphs. The key innovation is a fully distributed algorithm called Gossip Successive Elimination that integrates arm elimination with a gossip protocol, enabling agents to learn optimal actions while communicating over networks where each edge is present with probability p. The authors establish both upper and lower bounds on regret, demonstrating that their algorithm achieves logarithmic regret growth that matches the optimal centralized performance plus communication-dependent terms.

The theoretical contributions include a novel confidence interval design that accounts for both estimation and consensus errors from random communication, and a refined weight matrix based on the graph Laplacian. The regret bound reveals a clear trade-off between communication efficiency (controlled by p and graph connectivity) and learning performance, with the upper bound holding for any p ∈ (0,1] without requiring connectivity assumptions.

## Method Summary
The paper proposes Gossip Successive Elimination, a distributed algorithm that combines successive arm elimination with gossip-based communication. The algorithm operates by having agents maintain local estimates of arm rewards and communicate through a gossip protocol over Erdős-Rényi random networks. At each time step, a random subgraph is sampled where each edge from a base graph is present with probability p. The communication weight matrix is constructed using the graph Laplacian to facilitate consensus among agents.

The key technical innovation is the confidence interval design that incorporates both the standard estimation error and an additional consensus error term arising from the random communication structure. The algorithm uses these intervals to eliminate suboptimal arms and converge to the optimal action while maintaining the ability to explore through the random network topology.

## Key Results
- Regret upper bound of O(∑k:∆k>0 logT/∆k + N²logT/(pλN−1(Lap(G))) + KN²logT/p) that matches optimal centralized regret plus communication terms
- Nearly optimal regret lower bound of Ω(∑k:∆k>0 logT/∆k) proving the centralized component is tight
- Numerical experiments validate logarithmic regret growth and demonstrate the trade-off between communication efficiency and regret across different graph topologies
- The bound holds for any p ∈ (0,1] without requiring connectivity assumptions

## Why This Works (Mechanism)
The algorithm succeeds by combining successive elimination with gossip communication to achieve both exploration and consensus. The key insight is that the gossip protocol with Laplacian-based weights enables agents to maintain approximate consensus on arm value estimates despite the random communication structure. The confidence intervals are carefully designed to account for the additional uncertainty introduced by random communication failures, ensuring that suboptimal arms can still be eliminated with high probability.

The mechanism works because the Laplacian-based weights facilitate information diffusion across the network, while the successive elimination ensures that exploration is focused on promising arms. The random communication pattern, while potentially disconnected, still allows for sufficient information mixing over time to maintain the algorithm's convergence properties.

## Foundational Learning

1. **Multi-armed bandit problem fundamentals**
   - Why needed: Core problem setup where agents must balance exploration and exploitation
   - Quick check: Can identify regret definition and understand the trade-off between exploring suboptimal arms and exploiting known good ones

2. **Erdős-Rényi random graphs**
   - Why needed: Communication network model where each edge exists with probability p
   - Quick check: Can explain properties like expected degree and connectivity thresholds

3. **Graph Laplacians and algebraic connectivity**
   - Why needed: Used to construct communication weights and characterize network efficiency
   - Quick check: Can compute λN−1(Lap(G)) and understand its role in information diffusion

4. **Gossip protocols**
   - Why needed: Distributed communication mechanism that enables consensus without centralized coordination
   - Quick check: Can describe how agents exchange information with neighbors and update beliefs

5. **Successive elimination algorithms**
   - Why needed: Strategy for identifying and eliminating suboptimal arms over time
   - Quick check: Can explain how confidence intervals are used to eliminate arms with high probability

6. **Distributed optimization over networks**
   - Why needed: Framework for understanding how agents can coordinate to solve optimization problems
   - Quick check: Can describe how consensus is achieved through local communication

## Architecture Onboarding

Component map: Agent local estimates -> Confidence intervals -> Arm elimination -> Gossip communication -> Consensus updates -> Updated local estimates

Critical path: Each agent maintains local reward estimates → Computes confidence intervals incorporating consensus error → Eliminates suboptimal arms → Exchanges estimates via gossip protocol → Updates estimates based on received information → Repeats

Design tradeoffs: The algorithm trades off between exploration (maintaining uncertainty about arms) and exploitation (eliminating clearly suboptimal arms), while also balancing communication efficiency (fewer messages when p is small) against convergence speed (better connectivity when p is large).

Failure signatures: Poor performance when p is very small (network fragmentation), when the base graph has poor connectivity (small λN−1), or when confidence intervals are not tight enough to enable effective elimination.

First experiments:
1. Validate regret scaling with time T on a small network with known optimal arm
2. Test algorithm sensitivity to communication probability p across different base graphs
3. Compare performance against centralized successive elimination baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes synchronous communication without accounting for delays or failures
- Analysis focuses on homogeneous settings without considering heterogeneous reward structures
- Practical performance under varying network conditions requires further validation
- Lower bound tightness for heterogeneous settings is not fully established

## Confidence

| Claim | Confidence |
|-------|------------|
| Regret upper bound derivation | High |
| Lower bound tightness | Medium |
| Algorithm practicality | Medium |
| Communication model assumptions | Low |

## Next Checks

1. Empirical validation of the algorithm's performance across different network densities and topologies
2. Analysis of the algorithm's robustness to communication delays and asynchronous updates
3. Investigation of potential improvements in the lower bound for heterogeneous reward settings