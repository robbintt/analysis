---
ver: rpa2
title: Additive Large Language Models for Semi-Structured Text
arxiv_id: '2511.11922'
source_url: https://arxiv.org/abs/2511.11922
tags:
- calm
- patient
- feature
- what
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of interpretability in large
  language models (LLMs) for clinical text classification, where physicians need to
  understand which parts of a patient's record drive risk predictions. The proposed
  method, CALM (Classification with Additive Large Language Models), is an inherently
  interpretable framework designed for semi-structured text inputs composed of semantically
  meaningful components, such as sections of admission notes or question-answer fields
  from intake forms.
---

# Additive Large Language Models for Semi-Structured Text

## Quick Facts
- **arXiv ID:** 2511.11922
- **Source URL:** https://arxiv.org/abs/2511.11922
- **Reference count:** 40
- **Primary result:** CALM achieves interpretable clinical text classification with performance within 0.02-0.03 AUC-PR of black-box fine-tuning

## Executive Summary
This paper introduces CALM (Classification with Additive Large Language Models), a framework for interpretable classification of semi-structured clinical text. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions directly observable at the logit level. The framework achieves performance comparable to conventional LLM classifiers while providing faithful explanations at both patient and population levels. The authors evaluate CALM across three real-world clinical outcome tasks using seven open-source LLMs, demonstrating that additive LLM-based models can provide transparency without sacrificing performance.

## Method Summary
CALM encodes each textual component independently using a shared LLM backbone with LoRA fine-tuning, then applies component-specific classification heads to produce per-component logits. These logits are aggregated via mean plus a learned bias to produce final predictions. The additive structure inherently exposes component-level contributions for interpretability. Extensions include CALM2 (incorporating pairwise interactions) and CALM-Distill (knowledge distillation from fine-tuned teachers). The framework operates on semi-structured text inputs that can be decomposed into semantically meaningful components.

## Key Results
- CALM achieves performance comparable to black-box fine-tuning with only minor AUC-PR drops (0.02-0.03) across three clinical datasets
- The framework provides both global interpretability through feature importance scores and local interpretability through patient-level risk contributions
- CALM-Distill improves performance by transferring dark knowledge from fine-tuned teachers while preserving interpretability
- Component-level risk curves enable clear visualizations of how each section contributes to predictions

## Why This Works (Mechanism)

### Mechanism 1: Additive Decomposition via Component-Level Logits
CALM's additive structure directly exposes component influence at the logit level by encoding each section independently and summing contributions. This enables faithful explanations without post-hoc approximation, though the strict additivity constraint may miss complex interactions between components.

### Mechanism 2: Shared Backbone with Per-Component Heads
A shared LLM backbone across all components enables computational efficiency while per-component classification heads allow specialized feature learning for each section type. This design balances parameter sharing with specialization.

### Mechanism 3: Knowledge Distillation Transfers Dark Knowledge
Distilling from black-box teachers to CALM students improves accuracy by transferring relative probability distributions and uncertainties while maintaining the interpretable additive architecture.

## Foundational Learning

- **Neural Additive Models (NAMs):** CALM extends NAMs from scalar tabular features to textual components. Quick check: Can you explain why summing independent neural network outputs per feature produces an interpretable model, and what the "no interaction" constraint means for performance?
- **LoRA (Low-Rank Adaptation) Fine-Tuning:** All experiments use LoRA fine-tuning with specific hyperparameters. Quick check: Why would LoRA be preferred over full fine-tuning when comparing multiple architectures across seven different LLM backbones?
- **Semi-Structured vs. Unstructured Clinical Text:** CALM requires inputs decomposable into meaningful components. Quick check: Given a raw clinical note without section headers, what preprocessing steps would be required before applying CALM?

## Architecture Onboarding

**Component map:**
Input: M textual components [x_1, ..., x_M] -> Shared LLM Backbone f_T(·; w_T) -> Hidden representations [h_1, ..., h_M] -> Component-specific heads [f_1^{last}, ..., f_M^{last}] -> Aggregation: z = (1/M)Σℓ_i + b -> softmax -> predictions

**Critical path:**
1. Input segmentation quality determines interpretability granularity
2. Backbone encoding must capture task-relevant semantics
3. Per-component heads must learn to weight section importance
4. Aggregation must preserve calibrated probabilities

**Design tradeoffs:**
- Additivity vs. expressivity: Strict additivity provides faithful explanations but disallows cross-component interactions
- Efficiency vs. padding waste: Practical batching with padding can increase cost to M·L_max²
- Interpretability granularity vs. computation: More components = finer-grained explanations but higher overhead

**Failure signatures:**
- Semantic equivalence sensitivity: Minor textual variations produce different risk scores
- Cross-component dependencies missed: Complex interactions between sections may not be captured
- Component boundary errors: Poor segmentation produces misleading attributions

**First 3 experiments:**
1. Baseline parity check: Compare CALM vs. black-box fine-tuning on MIMIC Admission Notes
2. Component ablation: Systematically remove one component at a time and measure performance drop
3. Distillation hyperparameter sweep: Test α ∈ {0.2, 0.4, 0.6} and T ∈ {1, 2, 4}

## Open Questions the Paper Calls Out
- The paper does not provide mechanisms for practitioners to modify or correct CALM's learned feature contributions based on clinical domain knowledge
- CALM's performance and interpretability generalizability to clinical tasks beyond mortality prediction remains unexplored
- No human evaluation with clinicians validates whether CALM's explanations support clinical workflow or decision-making

## Limitations
- The strict additive constraint may degrade accuracy when clinical decisions require complex higher-order interactions
- Performance is sensitive to minor textual variations in semantically equivalent inputs
- All evaluation focuses on mortality prediction, limiting understanding of generalizability to other clinical tasks

## Confidence

**High Confidence:** CALM's interpretability mechanism (additive component contributions exposed at logit level) is directly verifiable from the architecture and mathematical formulation

**Medium Confidence:** Performance claims (near-parity with black-box methods) are supported by extensive experiments across 7 models and 3 datasets, but depend on dataset construction details that aren't fully specified

**Low Confidence:** Generalizability claims about semi-structured text prevalence and CALM2/Distill extension effectiveness are supported by limited empirical evidence

## Next Checks
1. **Component Sensitivity Analysis:** Systematically measure how performance degrades when individual components are removed or corrupted
2. **Cross-Institution Generalization:** Test CALM on clinical notes from a different hospital system or specialty
3. **Unstructured-to-Structured Pipeline Evaluation:** Implement ClinStructor-style automatic structuring on raw clinical notes and measure impact on CALM's interpretability and accuracy