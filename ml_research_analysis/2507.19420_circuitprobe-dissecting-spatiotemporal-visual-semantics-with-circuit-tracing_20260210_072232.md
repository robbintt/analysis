---
ver: rpa2
title: 'CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing'
arxiv_id: '2507.19420'
source_url: https://arxiv.org/abs/2507.19420
tags:
- visual
- tokens
- layers
- video
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CircuitProbe, a systematic framework for\
  \ dissecting how large vision-language models (LVLMs) process spatiotemporal visual\
  \ semantics. The framework employs three circuits\u2014visual auditing, semantic\
  \ tracing, and attention flow\u2014to analyze information flow from visual tokens\
  \ through LLM layers."
---

# CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing

## Quick Facts
- **arXiv ID**: 2507.19420
- **Source URL**: https://arxiv.org/abs/2507.19420
- **Reference count**: 13
- **Primary result**: CircuitProbe framework reveals LVLMs process spatiotemporal semantics through localized object tokens and two-stage reasoning, with semantic concepts emerging in mid-to-late layers.

## Executive Summary
This paper introduces CircuitProbe, a systematic framework for dissecting how large vision-language models (LVLMs) process spatiotemporal visual semantics. The framework employs three circuits—visual auditing, semantic tracing, and attention flow—to analyze information flow from visual tokens through LLM layers. Key findings reveal that crucial visual semantics are highly localized to specific object tokens, with removing these degrading performance by up to 92.6%. Abstract semantic concepts emerge and consolidate in middle-to-late layers, where object and action representations develop. The model uses a two-stage reasoning process: early layers process broad context while middle-to-late layers focus on object-specific details. These mechanistic insights demonstrate functional localization in LVLMs and provide a foundation for designing more robust, interpretable models.

## Method Summary
CircuitProbe systematically analyzes LVLM spatiotemporal reasoning through three complementary circuits. Visual auditing ablates object vs. random tokens to test spatial localization of semantics. Semantic tracing uses logit lens to decode intermediate hidden states, revealing when semantic concepts emerge layer-wise. Attention flow masks token relationships to identify functional layer specialization in reasoning. The framework was validated on LLaVA-NeXT and LLaVA-OV models using a curated STAR dataset subset with 641 samples, each containing 4 key frames with object bounding boxes.

## Key Results
- Removing object-specific visual tokens degrades performance by up to 92.6%, compared to only 10.7% from ablating random tokens
- Semantic concepts of objects and actions emerge and become progressively refined in middle-to-late LLM layers
- Early layers process broad context while middle-to-late layers focus on object-specific details, demonstrating two-stage reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual semantics are spatially localized to specific object tokens within video embeddings
- Mechanism: The CLIP encoder + adapter pipeline preserves spatial correspondence—patches containing objects map to specific visual token positions. Ablating these tokens removes semantic information disproportionately compared to random tokens.
- Core assumption: Spatial locality from the visual encoder survives projection into the LLM embedding space.
- Evidence anchors:
  - [abstract] "visual semantics are highly localized to specific object tokens--removing these tokens can degrade model performance by up to 92.6%"
  - [section 3.1, Table 1] Ablating ~573 object tokens caused 92.6% drop vs. 10.7% from ablating 900 random tokens
  - [corpus] Related work on visual grounding confirms attention heads localize image regions for text descriptions, but corpus does not directly validate token-level spatial localization in video LVLMs
- Break condition: If adapter architecture fundamentally restructures spatial information (e.g., aggressive spatiotemporal pooling without positional encoding), localization would weaken or disappear.

### Mechanism 2
- Claim: Abstract semantic concepts emerge and consolidate in mid-to-late LLM layers
- Mechanism: Hidden states at early layers do not decode to meaningful tokens via the logit lens. Correspondence rate and answer probability for correct object labels increase sharply at layers 20-30 (varies by model), indicating semantic consolidation through feedforward and attention layers.
- Core assumption: The logit lens (unembedding intermediate hidden states) provides a valid proxy for the model's internal semantic representation.
- Evidence anchors:
  - [abstract] "interpretable concepts of objects and actions emerge and become progressively refined in the middle-to-late layers"
  - [section 3.2, Figure 4] Sharp increase in correspondence rate C(l)_R and answer probability A(l)_P at mid-to-late layers
  - [corpus] Weak direct evidence; corpus papers focus on benchmark evaluation rather than layer-wise semantic emergence
- Break condition: If the language head's unembedding projection is not meaningful for intermediate layers, logit lens interpretations become artifacts rather than evidence of semantic emergence.

### Mechanism 3
- Claim: LVLMs employ a two-stage reasoning process with functional localization across layer depth
- Mechanism: Attention masking shows context masking hurts performance more in early-to-mid layers, while object-centric masking hurts more in mid-to-late layers. Information flows from broad context integration → focused object interpretation.
- Core assumption: Attention knockout isolates causal contributions without introducing confounding distributional shifts.
- Evidence anchors:
  - [abstract] "early layers process broad context while middle-to-late layers focus on object-specific details"
  - [section 3.3, Table 2] Contextual masking in early-to-mid layers causes severe accuracy collapse; object-centric masking effects concentrated in mid-to-late layers
  - [corpus] Related work on attention head interpretation confirms image-to-text flow through attention heads, but two-stage layer specialization is not directly validated externally
- Break condition: If attention masking introduces artifacts (e.g., out-of-distribution hidden states), observed performance drops may not reflect natural reasoning processes.

## Foundational Learning

- Concept: **Visual token patch correspondence**
  - Why needed here: Understanding that video frames are divided into patches, each encoded into a token with preserved spatial position, is essential for interpreting Circuit ➀ ablation results.
  - Quick check question: Given a 224×224 image with patch size 14×14, how many visual tokens are generated before the adapter?

- Concept: **Logit lens / unembedding**
  - Why needed here: Circuit ➁ relies on projecting intermediate hidden states through the language head to vocabulary logits. Without this concept, "semantic emergence" claims are opaque.
  - Quick check question: If a hidden state at layer 15 decodes to "towel" with probability 0.02, and at layer 25 to "towel" with probability 0.78, what does this suggest about semantic consolidation?

- Concept: **Attention masking / knockout**
  - Why needed here: Circuit ➂ uses attention masking to block information flow from specific token sets. Understanding causal intervention vs. observation is critical.
  - Quick check question: If you mask attention from all non-object tokens to the final prediction token and accuracy drops from 80% to 10%, what can you infer about the model's reasoning dependency?

## Architecture Onboarding

- Component map:
  - Visual Encoder (CLIP ViT-L/14) -> Adapter Network (A) -> LLM Backbone -> Language Head (H_L) -> Vocabulary
  - [E_I; E_T] -> Transformer layers 1 to N -> Hidden states h_i^(l) -> Logit lens for semantic tracing -> Attention patterns for masking

- Critical path:
  1. Frame → CLIP patches → Adapter → Visual tokens
  2. Visual tokens + text tokens → LLM layers 1 to N
  3. Hidden states at each layer → Logit lens for semantic tracing
  4. Attention patterns → Masked to isolate context vs. object contributions

- Design tradeoffs:
  - **Token ablation granularity**: Object-only vs. object+buffer affects specificity vs. coverage
  - **Layer window size**: Broader windows give more stable results but lose layer-specific resolution
  - **Masking vs. injection**: Masking tests necessity; injection tests sufficiency (text injection showed strong performance recovery)

- Failure signatures:
  - Random token ablation causing similar degradation to object token ablation → spatial localization not present
  - Flat correspondence rate across layers → semantic emergence not occurring or logit lens invalid
  - Context masking and object masking causing identical patterns → no functional layer specialization

- First 3 experiments:
  1. **Token ablation sanity check**: Replicate Table 1 on your target LVLM—ablate object tokens vs. random tokens, verify asymmetric degradation (>50% gap).
  2. **Layer-wise logit lens**: Run semantic tracing on a held-out video QA sample, plot C(l)_R and A(l)_P across layers, identify the emergence threshold layer.
  3. **Attention knockout pilot**: For one video question, apply context masking at layers 5-15 and object masking at layers 15-25, compare accuracy drops to validate two-stage reasoning for your model variant.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the observed functional localization and two-stage reasoning processes persist across diverse LVLM architectures beyond the LLaVA family?
- **Basis in paper:** [explicit] The authors explicitly state in the conclusion that "Future work should extend our framework to more diverse architectures and tasks."
- **Why unresolved:** The empirical validation is restricted to LLaVA-NeXT and LLaVA-OV, leaving the generalizability of the "mid-to-late layer" semantic consolidation to other projector designs (e.g., Q-Former) or model families unconfirmed.
- **What evidence would resolve it:** Replicating the CircuitProbe framework on architectures like InternVideo or proprietary models to verify if object-centric attention shifts occur in the same layer depth windows.

### Open Question 2
- **Question:** Can targeted "circuit surgery" interventions effectively mitigate hallucinations or enhance reasoning without causing catastrophic forgetting?
- **Basis in paper:** [explicit] The paper concludes by suggesting future work should "leverage our findings to create targeted interventions... through techniques like 'circuit surgery'."
- **Why unresolved:** While the paper identifies the circuits responsible for spatiotemporal semantics, it stops short of implementing active modifications to correct model behavior during generation.
- **What evidence would resolve it:** Applying causal interventions to suppress specific attention heads in the identified mid-to-late layers and measuring the resulting change in hallucination rates versus baseline accuracy.

### Open Question 3
- **Question:** Does the performance gain from text injection imply that current visual adapters fail to generate optimally distinct semantic representations?
- **Basis in paper:** [inferred] The paper notes that replacing visual tokens with text labels boosts accuracy beyond the original baseline (Observation 2), suggesting raw visual tokens are "noisy" compared to "clean, symbolic" text.
- **Why unresolved:** It is unclear if the adapter is discarding necessary visual details during alignment or if the LLM simply lacks the capacity to process soft visual prompts as efficiently as discrete text.
- **What evidence would resolve it:** Analyzing the cosine similarity and cluster separation of adapter outputs versus text embeddings for the same concepts to quantify the "semantic noise" gap.

## Limitations

- The core claim of spatial localization depends on the adapter preserving CLIP's spatial mapping into LLM space, which is not directly validated
- The semantic emergence timeline derived from logit lens analysis could be confounded by the language head's architecture being calibrated only for final-layer outputs
- Results are limited to LLaVA-NeXT and LLaVA-OV models, raising questions about generalizability to other LVLM architectures

## Confidence

- **High confidence**: The two-stage reasoning pattern (context in early layers, objects in mid-to-late layers) is supported by consistent attention masking results across both LLaVA-NeXT and LLaVA-OV models
- **Medium confidence**: The spatial localization claim depends on adapter behavior that is not directly examined, though ablation results strongly suggest localization
- **Low confidence**: The semantic emergence timeline could be artifacts of the logit lens methodology if the language head's unembedding is not meaningful for intermediate layers

## Next Checks

1. **Adapter spatial mapping validation**: Visualize the adapter's transformation of visual tokens to confirm that spatial locality from CLIP patches is preserved in the LLM embedding space. Compare attention distributions on ablated vs. non-ablated tokens to verify that spatial information drives the observed performance drops.

2. **Logit lens calibration test**: Generate synthetic hidden states at intermediate layers that should decode to specific tokens with high confidence. Verify that the language head's unembedding produces meaningful probabilities across all layers, not just the final layer.

3. **Cross-model replication**: Apply the three circuits to a third LVLM architecture (e.g., CogVLM or MiniGPT-4) to test whether the observed patterns are general properties of vision-language integration or specific to the tested models' architectural choices.