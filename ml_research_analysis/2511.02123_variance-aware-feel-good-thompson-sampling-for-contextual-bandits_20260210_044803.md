---
ver: rpa2
title: Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits
arxiv_id: '2511.02123'
source_url: https://arxiv.org/abs/2511.02123
tags:
- regret
- bandits
- contextual
- linear
- holds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FGTS-VA, a variance-aware Thompson Sampling
  algorithm for contextual bandits with general reward functions. The algorithm extends
  Feel-Good Thompson Sampling (FGTS) by incorporating variance-dependent weights and
  a new feel-good exploration term in the posterior distribution.
---

# Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits

## Quick Facts
- **arXiv ID**: 2511.02123
- **Source URL**: https://arxiv.org/abs/2511.02123
- **Reference count**: 40
- **Primary result**: FGTS-VA achieves Õ(√(dc·log|F|·Λ) + dc) regret, matching optimal O(d√Λ) for linear contextual bandits while adapting to variance

## Executive Summary
This paper introduces FGTS-VA, a variance-aware Thompson Sampling algorithm for contextual bandits that adapts regret bounds to cumulative noise variance Λ rather than time horizon T. The key innovation is combining weighted log-likelihood by inverse variance with a "Type B" feel-good exploration term applied only to the current context. This enables optimal variance-dependent regret without requiring horizon knowledge, improving upon previous variance-aware methods that achieved suboptimal bounds. The algorithm introduces the generalized decoupling coefficient as a unified complexity measure that generalizes Eluder dimension and enables regret analysis for general function classes.

## Method Summary
FGTS-VA samples functions from a posterior distribution that combines weighted log-likelihood (weighted by inverse variance) with a feel-good exploration term. At each round t, the algorithm receives context xt and noise variance σ²t, computes weights ηs = σ̄⁻²s and exploration parameter λt = c√Λt/σ̄²t, then samples ft from the posterior distribution using Langevin dynamics. The agent selects action at = argmax_a ft(xt, a), observes reward rt, and updates its history. The variance-dependent weighting allows regret to scale with cumulative noise Λ rather than time T, while the feel-good term ensures sufficient exploration without horizon knowledge.

## Key Results
- FGTS-VA achieves Õ(√(dc·log|F|·Λ) + dc) regret, where dc is the generalized decoupling coefficient
- For linear contextual bandits, this matches the optimal bound of O(d√Λ + d)
- The algorithm outperforms standard FGTS and approaches Weighted OFUL+ performance in sparse noise settings
- Type B feel-good exploration (current-step only) is provably necessary for variance-aware bounds

## Why This Works (Mechanism)

### Mechanism 1
Weighting log-likelihood by inverse variance (ηs = σ̄⁻²s) normalizes contributions from high and low variance observations, allowing regret to scale with cumulative noise Λ rather than time T. The floor parameter σ̄t = max{σt, α} prevents numerical instability when variance approaches zero. This weighting enters the posterior distribution, replacing uniform weighting in standard FGTS.

### Mechanism 2
The "Type B" feel-good exploration term λt·max_a f(xt, a), applied only to the current context, enables variance-dependent regret without requiring horizon knowledge T. This creates a decomposition where the feel-good term directly penalizes pessimistic function samples. The parameter λt = c√Λt/σ̄²t scales exploration inversely with current-step noise and proportionally with cumulative uncertainty.

### Mechanism 3
The generalized decoupling coefficient dc provides a unified complexity measure that bounds how prediction errors at new points can be "decoupled" from errors at previously observed points. For linear functions, dc ≤ 2d log(1 + εT/(dλ)) = Õ(d). For general functions, dc ≤ generalized Eluder dimension. This connects model complexity to achievable regret.

## Foundational Learning

- **Standard Thompson Sampling for Contextual Bandits**: FGTS-VA modifies the standard TS posterior (equation 3.1 → 4.1); understanding the baseline clarifies what each modification accomplishes.
  - Quick check question: In standard TS, the posterior p(f|St-1) ∝ p₀(f)·exp(-∑(rs - f(xs, as))²). What two modifications does FGTS-VA make to this form?

- **Variance-Dependent vs. Worst-Case Regret**: The paper's central claim is improving regret from Õ(d√T) to Õ(d√Λ), where Λ can be much smaller than T in low-noise settings.
  - Quick check question: If rewards are deterministic (Λ = 0), what regret does FGTS-VA achieve, and why is this better than standard contextual bandit algorithms?

- **Eluder Dimension and Model Complexity**: The generalized decoupling coefficient generalizes Eluder dimension; both measure how "independent" new observations are from past data.
  - Quick check question: For linear bandits in d dimensions, what is the Eluder dimension, and how does this relate to the generalized decoupling coefficient bound in Proposition 5.2?

## Architecture Onboarding

- **Component map**: Context/σ²t → Compute ηs, λt → Sample ft from posterior → Select at = argmax_a ft(xt, a) → Observe rt → Update history

- **Critical path**: 
  1. Receive context xt and variance σ²t
  2. Compute ηs for all historical steps and λt for current step
  3. Sample ft from posterior (equation 4.1)—this is the computational bottleneck
  4. Select at = argmax_a ft(xt, a)
  5. Observe rt, update history

- **Design tradeoffs**: 
  - Type B vs. Type A feel-good: Type B (current-step only) enables variance-aware bounds; Type A provably cannot achieve them
  - Known vs. unknown variance: Algorithm requires σ²t revealed at each step; extension to unknown variance is open
  - Computation vs. theoretical guarantees: Sampling-based approach is empirically more efficient than UCB but requires MCMC; regret analysis is more complex

- **Failure signatures**: 
  - Regret scales as O(√T) rather than O(√Λ) → Check that ηs is correctly computed as σ̄⁻²s, not constant
  - Numerical instability when σ²t ≈ 0 → Verify α floor parameter is set (default α = 1/√T)
  - Poor performance relative to baselines → Hyperparameter c may need tuning; ablation in Figure 2 shows c = 0.003 works well for sparse noise, c ≈ 0 for dense noise

- **First 3 experiments**:
  1. **Sparse noise validation**: Linear bandits (d=5) with σ²t ∈ {0, 1} where p=0.1. This tests whether FGTS-VA correctly exploits low-variance rounds.
  2. **Dense noise stress test**: σ²t sampled from χ²(1) distribution. Tests robustness to heterogeneous, heavy-tailed variance.
  3. **Hyperparameter ablation**: Vary c ∈ {0, 0.001, 0.003, 0.01, 0.03, 0.1} on both noise models. This identifies the feel-good strength optimal for each regime.

## Open Questions the Paper Calls Out

### Open Question 1
Can an FGTS-based algorithm be developed for the "unknown variance" setting where σt² is not revealed to the agent? The current FGTS-VA algorithm operates under the "weak adversary with variance revealing" assumption, requiring noise variance at the start of each step. An algorithm that estimates variance online while maintaining optimal regret bounds without oracle knowledge would resolve this.

### Open Question 2
Can the variance-aware FGTS techniques be extended to reinforcement learning (RL) settings? The current work focuses on contextual bandits, and it is unclear if the generalized decoupling coefficient handles the complexities of long-horizon RL. Deriving a variance-aware regret bound for an FGTS-based algorithm in a standard RL setting would resolve this.

### Open Question 3
Is it possible to achieve variance-aware regret bounds using "Type A" feel-good exploration (augmenting the log-likelihood of historic steps)? Section 6.1 states that Type A "cannot yield variance-aware regret bounds with existing techniques," leading the authors to adopt "Type B." A novel theoretical analysis or algorithmic modification that achieves variance-dependent regret using the Type A posterior structure would resolve this.

## Limitations

- Algorithm requires known variance σ²t at each step, limiting applicability to settings where variance must be estimated
- Computational cost of posterior sampling via MCMC may become prohibitive for high-dimensional problems
- Theoretical guarantees depend on the generalized decoupling coefficient, which may be difficult to compute or bound for complex function classes

## Confidence

- **Mechanism 1 (Variance Weighting)**: High confidence - well-established approach with rigorous complexity measures
- **Mechanism 2 (Type B Feel-Good)**: Medium confidence - theoretically necessary but proof structure is more fragile
- **Mechanism 3 (Generalized Decoupling)**: Medium confidence - elegant connection but practical bounds may be pessimistic

## Next Checks

1. **Robustness to variance estimation error**: Test FGTS-VA when variance estimates σ̂²t are perturbed by additive noise or bias, quantifying the degradation in regret bounds.

2. **Empirical decoupling coefficient estimation**: For synthetic function classes (e.g., kernelized bandits), empirically estimate the generalized decoupling coefficient and compare against theoretical bounds.

3. **Computational scaling experiments**: Systematically evaluate the trade-off between SGLD steps K and regret performance across dimensions d ∈ {5, 10, 20, 50}, measuring both wall-clock time and statistical efficiency.