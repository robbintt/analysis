---
ver: rpa2
title: Collaborative Problem-Solving in an Optimization Game
arxiv_id: '2505.15490'
source_url: https://arxiv.org/abs/2505.15490
tags:
- coins
- agent
- room
- path
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present TRAVELING ADVENTURERS, a novel two-player collaborative
  game environment for collecting task-oriented dialogue, based on a two-player TSP.
  The agents solve the problem collaboratively, with each player having partial information
  about the graph, necessitating negotiation and collaboration.
---

# Collaborative Problem-Solving in an Optimization Game

## Quick Facts
- **arXiv ID**: 2505.15490
- **Source URL**: https://arxiv.org/abs/2505.15490
- **Reference count**: 12
- **Key outcome**: Neurosymbolic agent with external TSP solver achieves 45% optimal self-play and outperforms baseline in human-agent collaboration

## Executive Summary
This paper introduces TRAVELING ADVENTURERS, a novel two-player collaborative game environment based on a two-player TSP variant where agents must negotiate a joint optimal path with partial information. The authors develop a neurosymbolic agent that combines GPT-4o with external symbolic modules for state tracking and optimization. The agent achieves high correctness (86%) and moderate optimality (45%) in self-play, and demonstrates superior human-agent collaboration compared to a baseline LLM-only agent.

## Method Summary
The method employs a neurosymbolic architecture combining GPT-4o with symbolic state-tracking modules (Visited/Remaining lists) and an exact external TSP solver for computing Intermediate Best Paths. Four agent variants are evaluated: Baseline (LLM + action history), Grounding (adds Partner WSR), State-Tracking (adds Visited/Remaining), and Problem-Solving (adds external solver). The game uses 6-node graphs with partial edge weight visibility, requiring collaborative negotiation. Agents generate actions via ReAct-style prompting with modified templates for negotiation and optimization.

## Key Results
- Problem-Solving agent achieves 98% correct and 45% optimal in self-play (vs Baseline 71% correct, 17% optimal)
- Node-by-node negotiation strategy dominates (96% self-play, 91.6% human-agent games)
- In human-agent trials, Problem-Solving agent achieves 83% ≥90 score vs Baseline 58%
- Grounding module alone decreased performance (65% correct), but combined with state-tracking improved it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offloading state tracking to symbolic structures improves correctness by reducing context burden on the LLM.
- Mechanism: Replace unpruned Action History with dynamically updated Visited/Remaining structures encoding only relevant state.
- Core assumption: LLM reasoning degrades with increasing context length from redundant action traces.
- Evidence anchors: 86% vs 71% correctness in self-play; state-tracking outperforms baseline.

### Mechanism 2
- Claim: Coupling external exact optimizer with LLM dialogue improves optimality without harming fluency.
- Mechanism: Compute Intermediate Best Path (IBP) using exact TSP solver on joint graph; LLM uses IBP for generating suggestions.
- Core assumption: LLM unreliable at optimization but capable of dialogue generation when given structured input.
- Evidence anchors: 45% optimal self-play; solver steers model without negatively impacting conversation management.

### Mechanism 3
- Claim: Incremental node-by-node negotiation emerges as preferred strategy when action space supports partial proposals.
- Mechanism: Agents overwhelmingly prefer single-node extensions (argument length 2) over full-path proposals (argument length 7).
- Core assumption: Incremental proposals reduce cognitive load and enable faster error correction.
- Evidence anchors: 96% self-play, 91.6% human-agent games use node-by-node; suggest action typically occurs once per turn.

## Foundational Learning

- **Traveling Salesman Problem (TSP)**: The game is a two-player TSP variant; understanding NP-hardness explains why humans cannot immediately find optimal solutions and why external solver is necessary. Quick check: Given 4 cities with known pairwise distances, can you enumerate all possible tours and identify the shortest?

- **Conversational Grounding (Clark 1996)**: Agent must track shared vs private information; grounding errors lead to incorrect assumptions about partner's world state. Quick check: In dialogue, if partner says "I'll take the high route," what must be established before you can act on this?

- **Neurosymbolic Architecture**: Agent combines LLM prompting (dialogue generation) with symbolic modules (state tracking and optimization). Understanding this separation clarifies component contributions. Quick check: If symbolic solver provides optimal path but LLM generates incoherent explanation, where does failure originate?

## Architecture Onboarding

- **Component map**: LLM Core -> Partner WSR Extraction -> IBP Solver -> Action Parser -> Game Framework
- **Critical path**: Partner message → Partner WSR extraction → IBP solver → LLM generation → Action parser execution
- **Design tradeoffs**: Grounding module complexity vs correctness (Partner WSR alone decreased performance; pairing with state-tracking recovered it); IBP unknown-weight handling (zeroing safe but conservative); memory pruning (improves correctness but may lose negotiation nuance)
- **Failure signatures**: Agent suggests partner's optimum instead of joint optimum (grounding module not informing IBP correctly); agent forgets shared information (Partner WSR extraction failing on co-references); agent submits incomplete path (Remaining list not consulted); agent never rejects proposals (instruction-tuned LLM appeasement bias)
- **First 3 experiments**: 1) Ablate IBP solver to measure contribution to optimality, 2) Vary graph size (4, 5, 7, 8 nodes) to assess scalability, 3) Test unknown weight heuristics (zeroing vs mean vs optimistic imputation) to measure optimality impacts

## Open Questions the Paper Calls Out
- How does agent performance change when applied to optimization problems other than TSP or larger graphs? The authors note it would be informative to test on other problems and larger graph sizes.
- Can agent learn strategic social reasoning to persuade partner to accept joint optimum conflicting with individual greedy optimum? Future work suggested on investigating agents' social reasoning.
- Does lack of backtracking capability limit achievement of global optimality? The agent cannot revise previous agreements on nodes, only complete paths.

## Limitations
- Limited to 6-node graphs with 100 self-play games, may not generalize to larger or more complex instances
- Exact TSP solver implementation and complete prompt templates not fully specified, potentially hindering replication
- Human-agent study (n=20) provides underpowered evidence for collaboration quality differences between agents
- Grounding module effectiveness is mixed, with Partner WSR alone decreasing performance, suggesting complex interaction effects

## Confidence
- **High Confidence**: Symbolic state tracking (Visited/Remaining) improves correctness over action history (86% vs 71% correct); node-by-node negotiation pattern is clearly documented (96% self-play, 92% human-agent)
- **Medium Confidence**: Optimality improvement from external solver (45% vs 17% optimal) depends on exact implementation and unknown-weight handling; human-agent performance differences (83% vs 58% ≥90 score) are suggestive but limited by sample size
- **Low Confidence**: Grounding module contribution claims are weakest, as it decreased performance alone (65% correct) and only improved when combined with state-tracking

## Next Checks
1. **Ablate the external solver**: Replace IBP module with LLM-only path suggestion in Problem-Solving agent to measure drop in optimality from 45%
2. **Vary graph size and complexity**: Test agent performance on 4, 5, 7, and 8-node graphs to assess scalability of symbolic state tracking and solver latency
3. **Test alternative unknown-weight heuristics**: Compare zeroing unknown weights against mean imputation and optimistic imputation strategies to measure impacts on optimality and convergence speed