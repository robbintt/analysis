---
ver: rpa2
title: Skywork-R1V3 Technical Report
arxiv_id: '2507.06167'
source_url: https://arxiv.org/abs/2507.06167
tags:
- reasoning
- learning
- wang
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skywork-R1V3 introduces an RL-enhanced vision-language model that
  achieves 76.0% accuracy on the MMMU benchmark, matching entry-level human performance.
  Its core innovation is transferring reasoning capabilities from text-only LLMs to
  visual tasks using an elaborate post-training RL framework without additional pretraining.
---

# Skywork-R1V3 Technical Report

## Quick Facts
- arXiv ID: 2507.06167
- Source URL: https://arxiv.org/abs/2507.06167
- Reference count: 18
- Key outcome: Achieves 76.0% accuracy on MMMU benchmark, matching entry-level human performance

## Executive Summary
Skywork-R1V3 is an RL-enhanced vision-language model that transfers reasoning capabilities from text-only LLMs to visual tasks without additional pretraining. The model achieves 76.0% accuracy on the MMMU benchmark, demonstrating strong generalization across domains including math, science, and humanities. Its core innovation lies in using reinforcement learning to enhance visual reasoning while maintaining broad knowledge through curriculum learning and connector-only tuning strategies.

## Method Summary
Skywork-R1V3 employs an elaborate post-training RL framework that enhances reasoning capabilities without additional pretraining. The approach leverages a connector module for robust cross-modal alignment and introduces critical token entropy as an indicator for selecting high-quality RL checkpoints. Training includes curriculum learning, reinforcement finetuning, and connector-only tuning on multi-domain data to balance reasoning strength with broad knowledge retention.

## Key Results
- Achieves 76.0% accuracy on MMMU benchmark, matching entry-level human performance
- Outperforms larger open-source VLMs and approaches proprietary models on general and domain-specific tasks
- Demonstrates strong generalization from math to other subjects through RL-enhanced reasoning transfer

## Why This Works (Mechanism)
The RL framework transfers reasoning capabilities from text-only LLMs to visual tasks by using curriculum learning to gradually increase task complexity, reinforcement finetuning to optimize reasoning performance, and connector-only tuning to maintain cross-modal alignment. The critical token entropy metric identifies optimal checkpoints by measuring uncertainty in reasoning decisions, ensuring robust performance across diverse visual reasoning tasks.

## Foundational Learning
1. **Reinforcement Learning for VLMs** - why needed: Enables reasoning transfer from text to visual domains without pretraining; quick check: Verify reward shaping functions align with reasoning objectives
2. **Curriculum Learning Design** - why needed: Gradually builds reasoning capabilities from simple to complex tasks; quick check: Confirm task difficulty progression follows logical ordering
3. **Cross-Modal Alignment** - why needed: Ensures visual and language representations integrate effectively; quick check: Validate connector module reduces modality-specific biases
4. **Critical Token Entropy** - why needed: Provides principled checkpoint selection for RL training; quick check: Test entropy correlation with downstream reasoning performance
5. **Multi-Domain Training Balance** - why needed: Prevents overfitting to specific domains while maintaining reasoning strength; quick check: Monitor performance variance across subject areas

## Architecture Onboarding

**Component Map:** Vision Encoder -> Connector Module -> LLM Backbone -> RL Head -> Reward Function

**Critical Path:** Input Image → Vision Encoder → Connector → LLM → Reasoning Output → RL Reward → Parameter Updates

**Design Tradeoffs:** The model prioritizes reasoning transfer efficiency over raw parameter count, sacrificing some general knowledge breadth for specialized visual reasoning capabilities. Connector module complexity trades off against computational overhead during inference.

**Failure Signatures:** Poor cross-modal alignment manifests as inconsistent reasoning across visual domains, while inadequate curriculum design shows as performance plateaus at moderate difficulty levels. Overfitting to specific domains appears as dramatic performance drops when task distribution shifts.

**First Experiments:** 1) Test connector module ablation to isolate its contribution to reasoning performance, 2) Validate critical token entropy selection against random checkpoint selection, 3) Compare curriculum learning progression against uniform difficulty training

## Open Questions the Paper Calls Out
None

## Limitations
- Missing detailed RL hyperparameters and connector architecture specifications needed for reproduction
- Lack of efficiency metrics regarding model size and training compute requirements
- Insufficient ablation studies to isolate individual component contributions

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| 76.0% MMMU accuracy | High |
| RL framework effectiveness | Medium |
| Connector module contribution | Medium |
| Efficiency/resource usage claims | Low |

## Next Checks
1. Replicate the RL finetuning process with detailed hyperparameter sweeps to verify the critical token entropy checkpoint selection method works consistently across different random seeds
2. Conduct controlled experiments isolating the connector module's contribution by comparing full Skywork-R1V3 against versions without the connector on both reasoning and general knowledge tasks
3. Benchmark against human performance using the same evaluation protocol to verify the "entry-level human performance" claim with statistical significance testing