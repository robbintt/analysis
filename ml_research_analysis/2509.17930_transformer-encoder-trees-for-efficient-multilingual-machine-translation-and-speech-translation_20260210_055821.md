---
ver: rpa2
title: Transformer-Encoder Trees for Efficient Multilingual Machine Translation and
  Speech Translation
arxiv_id: '2509.17930'
source_url: https://arxiv.org/abs/2509.17930
tags:
- translation
- languages
- speech
- target
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Transformer Encoder Tree (TET) for efficient
  multilingual machine translation and speech translation. The core idea is to organize
  target languages into a hierarchical tree based on linguistic similarity, allowing
  intermediate representations to be shared among related languages and reducing redundant
  computation.
---

# Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation

## Quick Facts
- **arXiv ID**: 2509.17930
- **Source URL**: https://arxiv.org/abs/2509.17930
- **Reference count**: 0
- **Primary result**: Transformer Encoder Tree (TET) reduces WER by 13-53% vs monolingual encoders and 32% vs shared encoders while enabling 7-14× faster speech translation

## Executive Summary
This paper introduces a Transformer Encoder Tree (TET) architecture for efficient multilingual machine translation and speech translation. The core innovation is organizing target languages into a hierarchical tree based on linguistic similarity, enabling intermediate representations to be shared among related languages while reducing redundant computation. TET uses non-autoregressive encoder-only models with CTC loss, allowing parallel generation of all target languages in a single forward pass. Experiments demonstrate significant accuracy improvements over standard baselines while achieving substantial speedups for speech translation applications.

## Method Summary
TET organizes target languages into a hierarchical tree where related languages share intermediate Transformer encoder layers. The architecture uses non-autoregressive encoder-only models trained with CTC loss, enabling parallel token generation across all target languages. For speech translation, TET is combined with Wav2Vec2 ASR backbone. The model processes input through shared root layers, then branches according to language family, with language-specific leaf layers producing CTC logits. Training uses Adam optimizer with random-padding strategy (+50 tokens) to ensure CTC functionality. The approach supports multilingual MT, speech-to-text translation (S2TT), and speech-to-speech translation (S2ST).

## Key Results
- TET reduces average WER by 32% compared to shared encoder baselines on Multi30K extended dataset
- For low-resource languages, TET achieves 13-53% WER reduction versus monolingual encoder baselines
- Speech translation pipeline (Wav2Vec2 + TET) achieves 7-14× speedup over autoregressive baselines while maintaining comparable accuracy
- Linguistic tree topology (TET) outperforms random topology (TET-Rnd) on average across all language pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hierarchical parameter sharing based on linguistic similarity improves translation accuracy for low-resource languages by enabling positive transfer.
- **Mechanism**: TET organizes target languages into a tree structure where intermediate representations are shared among related languages (e.g., Romance languages like French, Spanish, Italian share a branch). This allows low-resource languages to benefit from the training signal of higher-resource linguistic relatives.
- **Core assumption**: Languages within the same family share structural and lexical properties that can be captured in shared intermediate neural representations.
- **Evidence anchors**: [abstract]: "By sharing intermediate representations among linguistically similar target languages, TET can improve accuracy on low-resource languages..."; [section 3]: "As shown in Figure 1, when training a single Transformer-encoder model for all EN-X MT tasks, the learned embeddings spontaneously cluster by linguistic similarity..."; [corpus]: Weak direct evidence for transfer mechanism; related work on hierarchical architectures for multilingual MT [4, 5] exploits typological language family trees to guide parameter sharing, supporting the plausibility of this approach.
- **Break condition**: The mechanism may fail if linguistic similarity does not correlate with transferability for the specific language pairs, or if the tree topology is suboptimal (e.g., random assignment, as in TET-Rnd).

### Mechanism 2
- **Claim**: Non-autoregressive (NAR) generation with Connectionist Temporal Classification (CTC) enables parallel decoding of all target languages, drastically reducing inference latency.
- **Mechanism**: Unlike autoregressive (AR) models that generate tokens sequentially, NAR encoder-only models with CTC generate all output tokens in a single forward pass. The CTC loss allows the model to predict unaligned sequences by introducing blank tokens, which are collapsed during post-processing. This eliminates the sequential bottleneck inherent in decoder-based models.
- **Core assumption**: The quality gap between NAR and AR models can be closed sufficiently for practical use, particularly in multilingual settings.
- **Evidence anchors**: [abstract]: "...allow generating all target languages in a single forward pass, thus eliminating sequential bottlenecks and improving parallelism."; [section 1]: "Unlike typical AR decoder-only approaches, our NAR encoder-only model enables parallel token generation, significantly speeding up translation."; [corpus]: Related work confirms NAR models with CTC are effective for speech translation [11] and can be competitive with AR models in specific settings.
- **Break condition**: The mechanism relies on sufficient input padding for CTC to function correctly. It may produce shorter outputs or lack target-side context, as noted in the S2ST experiments, potentially degrading quality.

### Mechanism 3
- **Claim**: Computation reuse across the tree structure reduces the total operations required for multilingual translation compared to running separate models per language.
- **Mechanism**: The hierarchical tree design reuses activations from shared layers (closer to the root) for all languages in a subtree, while language-specific layers (closer to leaves) handle divergence. This means that for N target languages and a tree depth of D, not every language requires a full D-layer pass, reducing total computation from O(N·D) to something closer to O(D + branch complexity).
- **Core assumption**: The overhead of managing the tree structure and routing activations is negligible compared to the savings from computation reuse.
- **Evidence anchors**: [section 3]: "To translate an English input sentence into all 8 target languages, only 24 encoder layers need to be processed in TET, instead of 8×6=48 for a standard approach where each language receives its own sequence of layers."; [abstract]: "...reduce computational redundancy..."; [corpus]: No direct corpus evidence on this specific efficiency claim; related work focuses on transfer benefits rather than computation savings.
- **Break condition**: Efficiency gains diminish if the tree is highly unbalanced or if languages in the same branch are not sufficiently similar, forcing deeper language-specific sub-trees.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**
  - Why needed here: TET uses CTC loss to train encoder-only models for sequence-to-sequence translation without alignment labels. Understanding how blank tokens work and why padding is required is essential for debugging output length mismatches.
  - Quick check question: If your model outputs fewer tokens than the target sequence, what is the first hyperparameter to check?

- **Non-autoregressive vs. Autoregressive Decoding**
  - Why needed here: The paper's speed gains (7-14× faster) come from NAR generation. You need to understand the trade-offs: parallelism vs. potential quality degradation due to lack of target-side context.
  - Quick check question: Why can't a standard CTC-based model condition on previously generated tokens, and how might this affect fluency?

- **Linguistic Typology / Language Families**
  - Why needed here: TET's tree topology is constructed based on linguistic similarity (e.g., Indo-European family branches). Understanding why French and Italian might share layers while German and Italian might not is key to designing or modifying the tree.
  - Quick check question: If you add a new target language (e.g., Polish) to a TET trained on Romance and Germanic branches, where should it attach, and what data might you need to fine-tune?

## Architecture Onboarding

- **Component map**: Source text/audio → ASR (if speech) → Tokenization with random padding → Root Transformer layer → Branch layers (path determined by target language) → Leaf layer → CTC logits → CTC collapse → Detokenized output

- **Critical path**: Source text/audio → ASR (if speech) → Tokenization with random padding → Root Transformer layer → Branch layers (path determined by target language) → Leaf layer → CTC logits → CTC collapse → Detokenized output

- **Design tradeoffs**:
  - **Tree depth vs. breadth**: Deeper trees allow more fine-grained sharing but increase latency for leaf languages. Shallower trees share more but may mix dissimilar languages.
  - **Linguistic vs. random topology**: Linguistically-motivated trees (TET) outperform random (TET-Rnd) on average, but optimal topology is not proven and may vary by dataset.
  - **NAR speed vs. quality**: CTC-based NAR models are faster but may lag behind AR models (e.g., T5, Whisper) on complex corpora or for S2ST (high WER noted).

- **Failure signatures**:
  - **Short/Repeated Outputs**: CTC collapse may produce degenerate outputs if the model over-predicts blanks or repeats tokens. Check input padding strategy.
  - **High WER on Specific Languages**: If a language at a deep leaf or on a branch with dissimilar neighbors underperforms, consider restructuring the tree or adding language-specific layers.
  - **Slow Inference Despite NAR**: Ensure you are reusing activations from shared layers. The forward pass should compute shared layers once, then branch, not run independent paths.

- **First 3 experiments**:
  1. **Reproduce TET vs. TEnc/lang and TEnc/all baselines** on Multi30K extended dataset. Use WER as the primary metric. Confirm that TET shows >30% average WER reduction over TEnc/all.
  2. **Ablate tree topology**: Compare the proposed linguistic tree (TET) against a random assignment tree (TET-Rnd) on a subset of languages. Measure per-language WER to identify which languages benefit most from correct clustering.
  3. **Profile inference speed**: Measure RTFx for the full NAR pipeline (Wav2Vec2 ASR + TET) against an AR baseline (Whisper + T5-small) on the Tatoeba subset. Target the claimed 7-14× speedup for S2TT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the TET architecture be effectively extended to support many-to-many translation?
- **Basis in paper**: [explicit] The authors state in the Future Work section their desire to "extend it to many-to-many translation."
- **Why unresolved**: The current study only validates the model in a one-to-many setting (English to multiple targets), leaving the feasibility of handling multiple source languages within the tree structure unproven.
- **What evidence would resolve it**: Experimental results showing TET performance when various source languages are used as input to the tree root.

### Open Question 2
- **Question**: Can alternative decoding strategies or loss functions improve the low accuracy observed in Speech-to-Speech Translation (S2ST)?
- **Basis in paper**: [explicit] The authors note that the S2ST performance was limited (79.8% WER) and explicitly state that "Further exploration of decoding strategies and alternative loss functions is warranted."
- **Why unresolved**: The current CTC-based setup tends to produce shorter outputs and lacks target-side context, significantly degrading S2ST performance compared to MT or S2TT tasks.
- **What evidence would resolve it**: Experiments comparing CTC against other non-autoregressive objectives (e.g., knowledge distillation) demonstrating improved S2ST BLEU scores.

### Open Question 3
- **Question**: Does TET maintain its efficiency and accuracy when processing real-world acoustic data?
- **Basis in paper**: [explicit] The authors identify "evaluate on real speech data" as a specific goal for future work, acknowledging that current ST experiments relied on synthesized speech.
- **Why unresolved**: The use of synthesized speech (pyttsx3) may not capture the acoustic variability and noise present in natural speech, leaving the model's robustness unverified.
- **What evidence would resolve it**: Benchmarking the model on datasets with natural speech, such as LibriSpeech or FLEURS, to confirm robustness.

## Limitations

- **Tree topology sensitivity**: Performance gains are highly dependent on the linguistic tree structure used; optimal topology is not proven and may vary by dataset.
- **CTC padding strategy**: The random-padding approach (+50 tokens) is critical but under-specified, potentially making the CTC mechanism fragile to hyperparameter choices.
- **Speech translation quality**: S2ST performance shows high WER (79.8%), suggesting the CTC-based NAR approach may lack sufficient target-side context for fluent speech output.

## Confidence

- **High Confidence**: The core mechanism of hierarchical parameter sharing based on linguistic similarity is well-supported by empirical results (TET vs TET-Rnd) and related work on multilingual MT.
- **Medium Confidence**: The non-autoregressive CTC-based generation enables parallel decoding and reduces latency, but the quality gap between NAR and AR models is not fully closed, especially for S2ST.
- **Low Confidence**: The generalization of TET to truly low-resource languages and unseen language families is not validated; the tree topology construction is manual and not automated.

## Next Checks

1. **Ablate Tree Topology on Low-Resource Subset**: Train TET and TET-Rnd on a subset of Tatoeba where half the languages are low-resource (<10k sentences). Measure per-language WER to identify which languages benefit most from correct linguistic clustering.

2. **Profile CTC Padding Impact**: Systematically vary the random-padding length (e.g., +0, +25, +50, +100 tokens) and measure its effect on WER and output length. If WER spikes or outputs are consistently short without +50 padding, the CTC mechanism is fragile to this hyperparameter.

3. **Compare NAR vs AR on S2ST Fluency**: Generate speech translations for a held-out S2ST test set using TET + Wav2Vec2 + HiFi-GAN and Whisper + T5-small + TTS. Conduct a human evaluation (e.g., MOS) to assess if the 7-14× speedup comes at an unacceptable cost to fluency or intelligibility.