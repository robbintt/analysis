---
ver: rpa2
title: Unified Attention Modeling for Efficient Free-Viewing and Visual Search via
  Shared Representations
arxiv_id: '2506.02764'
source_url: https://arxiv.org/abs/2506.02764
tags:
- visual
- attention
- search
- free-viewing
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether a shared representation exists between
  free-viewing and task-specific visual search in human attention modeling. Building
  upon the Human Attention Transformer (HAT) model, the authors propose a neural network
  architecture with shared and task-specific layers in the pixel decoder, enabling
  the reuse of features learned from free-viewing for visual search.
---

# Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations

## Quick Facts
- arXiv ID: 2506.02764
- Source URL: https://arxiv.org/abs/2506.02764
- Authors: Fatma Youssef Mohammed; Kostas Alexis
- Reference count: 32
- Primary result: Free-viewing and visual search can share a common representation with only 3.86% performance drop

## Executive Summary
This paper investigates whether free-viewing and task-specific visual search can share a common representation for human attention modeling. Building upon the Human Attention Transformer (HAT) model, the authors propose a neural network architecture with shared and task-specific layers in the pixel decoder, enabling efficient reuse of features learned from free-viewing for visual search tasks. The approach demonstrates that a unified attention model can achieve comparable performance to task-specific models while significantly reducing computational costs and model complexity.

## Method Summary
The authors extend the Human Attention Transformer (HAT) model by introducing a dual-task architecture that combines shared and task-specific components. The model consists of shared transformer encoder layers that learn common features from free-viewing data, followed by task-specific decoder layers that adapt these features for either free-viewing or visual search. The pixel decoder is modified to include both shared and task-specific layers, allowing knowledge transfer between tasks. The model is trained on the SALICON dataset, with free-viewing data used for pretraining and visual search data for fine-tuning. The semantic sequence score (SemSS) is used as the primary evaluation metric to compare performance across tasks.

## Key Results
- Free-viewing and visual search share a common representation with only 3.86% performance drop in SemSS
- Computational efficiency improved by 92.29% reduction in GFLOPs
- Model complexity reduced by 31.23% fewer trainable parameters
- Shared representation outperforms baseline models in both tasks

## Why This Works (Mechanism)
The model works by leveraging the common bottom-up attention mechanisms shared between free-viewing and visual search tasks. The shared transformer encoder layers capture low-level visual features and saliency patterns that are relevant to both tasks, while task-specific decoder layers adapt these features for the specific requirements of each task. The semantic sequence score (SemSS) metric effectively captures the alignment between predicted and human attention patterns, enabling quantitative comparison across tasks.

## Foundational Learning

**Transformer Architecture**: Used for processing sequential data and capturing long-range dependencies in visual attention modeling. Needed for handling the spatial and temporal aspects of eye movement patterns. Quick check: Verify transformer layer configurations and attention mechanism implementations.

**Human Attention Modeling**: Involves predicting where humans look in images based on bottom-up and top-down factors. Needed to understand the biological basis of attention and design appropriate neural network architectures. Quick check: Validate attention prediction outputs against human gaze data.

**Semantic Sequence Score (SemSS)**: A metric that evaluates the similarity between predicted and actual attention sequences. Needed to quantify model performance in capturing human attention patterns. Quick check: Compare SemSS values with other attention metrics like AUC and NSS.

## Architecture Onboarding

**Component Map**: Image Input -> Shared Transformer Encoder -> Shared Pixel Decoder + Task-Specific Decoder Layers -> Attention Maps

**Critical Path**: Image features flow through shared encoder layers, then split into task-specific paths through the modified pixel decoder. The critical path involves feature extraction, shared representation learning, and task adaptation.

**Design Tradeoffs**: The architecture balances between shared representation learning and task-specific adaptation. The main tradeoff is between model efficiency (through shared layers) and task-specific performance (through dedicated decoder layers).

**Failure Signatures**: Poor performance may indicate insufficient shared representation capacity or inadequate task-specific adaptation. Common failure modes include loss of task-specific features in shared layers or overfitting in task-specific layers.

**First Experiments**:
1. Test shared representation capacity by gradually increasing shared layer depth
2. Evaluate task-specific layer effectiveness by comparing with fully shared model
3. Assess SemSS sensitivity to different attention prediction thresholds

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises implicit questions about the generalizability of shared representations to other attention-related tasks and the potential for extending the approach to multi-task learning scenarios beyond free-viewing and visual search.

## Limitations

- Reliance on SALICON dataset may limit generalizability to other datasets or real-world scenarios
- Small visual search dataset (300 images) compared to free-viewing dataset (5000 images) may introduce bias
- Evaluation limited to SemSS metric, potentially overlooking other important attention modeling aspects

## Confidence

**Major Claim Clusters Confidence:**
1. **Shared representation effectiveness (High)**: Well-supported by experimental results with only 3.86% performance drop
2. **Computational efficiency (Medium)**: Significant reductions reported but may vary with implementation details
3. **Generalizability to real-world scenarios (Low)**: Limited by dataset-specific training and controlled conditions

## Next Checks

1. Evaluate the model's performance on additional datasets with varying image complexities and viewing conditions to assess generalizability beyond SALICON.

2. Conduct ablation studies to quantify the individual contributions of shared versus task-specific layers in the pixel decoder, and test different architectural configurations.

3. Test the model's performance in real-world applications where free-viewing and visual search tasks may not be as clearly separated, such as in autonomous driving or human-computer interaction scenarios.