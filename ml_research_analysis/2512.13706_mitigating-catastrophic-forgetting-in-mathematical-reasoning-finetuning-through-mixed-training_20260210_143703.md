---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through
  Mixed Training
arxiv_id: '2512.13706'
source_url: https://arxiv.org/abs/2512.13706
tags:
- training
- performance
- mixed
- mathematical
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates catastrophic forgetting when finetuning
  language models for specialized tasks, specifically mathematical reasoning. The
  author finetunes Flan-T5-Base on the DeepMind Mathematics dataset and observes severe
  forgetting: math accuracy improves from 3.1% to 12.0%, but NLI accuracy catastrophically
  drops from 81.0% to 16.5%.'
---

# Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training

## Quick Facts
- arXiv ID: 2512.13706
- Source URL: https://arxiv.org/abs/2512.13706
- Reference count: 3
- Catastrophic forgetting eliminated: Mixed training preserves NLI accuracy (86.2%) while achieving math performance (12.0%)

## Executive Summary
This paper addresses catastrophic forgetting when finetuning language models for specialized tasks. When Flan-T5-Base is finetuned on mathematical reasoning, it dramatically improves math performance but catastrophically forgets natural language inference capabilities. The proposed solution uses mixed training strategies that interleave mathematical and NLI examples during training. The 1:1 mixing ratio completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance, demonstrating that specialization need not require forgetting general capabilities.

## Method Summary
The study finetunes Flan-T5-Base (250M) on the DeepMind Mathematics dataset (linear algebra 1D subset, 392,702 examples) while monitoring catastrophic forgetting in MultiNLI. The proposed solution interleaves mathematical and NLI examples during training using various mixing ratios (1:1 to 15:1). Training uses FusedAdam optimizer (LR 3e-4, batch size 256, 3 epochs) with cosine LR schedule and bfloat16 on a single NVIDIA A100 (40GB). The key innovation is the mixed training approach that maintains NLI examples in the training stream to prevent catastrophic forgetting.

## Key Results
- Math-only finetuning causes catastrophic forgetting: Math accuracy improves from 3.1% to 12.0%, but NLI accuracy catastrophically drops from 81.0% to 16.5%
- Mixed 1:1 training completely eliminates catastrophic forgetting while maintaining math performance (12.0% math accuracy, 86.2% NLI accuracy)
- Even minimal NLI exposure (6.2% in 15:1 ratio) provides effective regularization against forgetting
- Performance is consistent across mixing ratios: all ratios achieve similar math accuracy (11.7-12.0%)

## Why This Works (Mechanism)
The mechanism appears to be that interleaving NLI examples during math finetuning provides gradient updates that preserve general language capabilities while the model specializes in mathematical reasoning. The regularization effect is surprisingly strong even with minimal NLI exposure, suggesting that sparse auxiliary task signals can maintain representational stability for general capabilities.

## Foundational Learning
- **Catastrophic forgetting**: When training on a new task, neural networks tend to overwrite previously learned capabilities, causing performance degradation on original tasks
  - Why needed: Understanding the problem being solved
  - Quick check: Observe NLI accuracy drop from 81% to 16.5% during math-only training

- **Mixed training strategies**: Alternating between different task examples during training to maintain multiple capabilities
  - Why needed: The core technique proposed to prevent forgetting
  - Quick check: Verify NLI accuracy stays high (~86%) in mixed training

- **Mixing ratios**: The proportion of examples from different tasks in the training stream
  - Why needed: Controls the trade-off between specialization and retention
  - Quick check: Test different ratios (1:1, 3:1, 15:1) to find optimal balance

## Architecture Onboarding

**Component map:**
Flan-T5-Base (250M) -> Training loop with interleaved data streams -> Mixed batches -> Forward pass -> Loss calculation -> Gradient update

**Critical path:**
Data preparation -> Mixed training loop implementation -> Validation of catastrophic forgetting baseline -> Validation of mixed training effectiveness -> Testing mixing ratio sensitivity

**Design tradeoffs:**
The study uses a single global learning rate and static mixing ratios rather than exploring dynamic schedules or task-specific optimization settings. This simplifies implementation but may miss opportunities for improved performance through adaptive strategies.

**Failure signatures:**
- Math accuracy remains low (~3%): Indicates model isn't properly learning the math task
- NLI accuracy drops in mixed training: Suggests mixing logic is broken or NLI exposure is insufficient
- Inconsistent performance across runs: May indicate implementation issues with data shuffling or batch composition

**Exactly 3 first experiments:**
1. Run math-only training for 3 epochs to verify catastrophic forgetting baseline (NLI: 81%→16.5%, Math: 3.1%→12.0%)
2. Implement and run 1:1 mixed training to verify simultaneous performance (Math: ~12.0%, NLI: ~86.2%)
3. Test 3:1 and 5:1 mixing ratios to validate minimal exposure effectiveness

## Open Questions the Paper Calls Out
1. Does mixed training yield superior specialized performance compared to task-only training in larger models (1B+ parameters), rather than merely maintaining parity?
2. Why does minimal auxiliary task exposure (as low as 6.2%) provide such effective regularization against catastrophic forgetting?
3. Do dynamic mixing ratios or task-specific learning rates improve the trade-off between specialization and retention compared to fixed mixing strategies?
4. Is the finding that "specialization need not require forgetting" generalizable to other task pairs beyond mathematical reasoning and NLI?

## Limitations
- Results based on single model size (Flan-T5-Base) and dataset pair, limiting generalizability
- Precise implementation details unclear for batch size adjustment across mixing ratios
- Subsampling strategy for reducing math dataset not fully specified
- Does not explore dynamic mixing ratios or task-specific learning rates

## Confidence
- **High confidence**: The core observation of catastrophic forgetting (NLI dropping from 81% to 16.5% while math improves from 3.1% to 12.0%) is well-supported
- **Medium confidence**: The effectiveness of 1:1 mixed training at eliminating catastrophic forgetting while maintaining math performance
- **Low confidence**: Broader claims about scaling to larger models and the general principle that "specialization need not require forgetting"

## Next Checks
1. Verify catastrophic forgetting baseline by running math-only finetuning for 3 epochs and measuring both math accuracy (target ~12.0%) and NLI accuracy (target catastrophic drop to ~16.5%)
2. Validate mixed training implementation by running 1:1 mixed training and confirming simultaneous achievement of both metrics (math accuracy ~12.0%, NLI accuracy ~86.2%)
3. Test mixing ratio sensitivity by running mixed training with at least two different ratios (e.g., 3:1 and 5:1) and measuring both math and NLI performance to validate minimal exposure effectiveness