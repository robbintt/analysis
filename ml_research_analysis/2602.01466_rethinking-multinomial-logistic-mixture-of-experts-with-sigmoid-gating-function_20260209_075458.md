---
ver: rpa2
title: Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function
arxiv_id: '2602.01466'
source_url: https://arxiv.org/abs/2602.01466
tags:
- gating
- sigmoid
- page
- convergence
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper conducts a comprehensive analysis of the sample efficiency
  of sigmoid-gated multinomial logistic mixture-of-experts (MLMoE) models. The key
  findings include: (1) the sigmoid gate is more sample-efficient than the softmax
  gate, yielding faster parameter and expert estimation rates; (2) a modified sigmoid
  gate with a positive scaling parameter ensures convergence in over-specified settings;
  (3) incorporating a temperature parameter into the standard inner product affinity
  score leads to exponentially slow convergence rates due to an intrinsic interaction
  with gating parameters; (4) replacing the inner product score with a Euclidean affinity
  score eliminates this interaction, substantially improving convergence to polynomial
  order.'
---

# Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function

## Quick Facts
- **arXiv ID**: 2602.01466
- **Source URL**: https://arxiv.org/abs/2602.01466
- **Reference count**: 40
- **Key outcome**: Sigmoid gating in MLMoE provides faster convergence than softmax gating, especially in over-specified settings, with improved sample efficiency when using Euclidean affinity scores over inner product scores.

## Executive Summary
This paper provides a comprehensive theoretical analysis of Multinomial Logistic Mixture of Experts (MLMoE) models with sigmoid gating functions. The key contribution is demonstrating that sigmoid gating achieves better sample efficiency than softmax gating through faster parameter estimation rates. The analysis reveals critical interactions between gating mechanisms, temperature parameters, and affinity scores that affect convergence. The paper introduces a modified sigmoid gate with positive scaling parameters to ensure convergence in over-specified settings and shows that replacing inner product affinity scores with Euclidean affinity scores eliminates problematic parameter interactions, substantially improving convergence rates from exponential to polynomial order.

## Method Summary
The paper analyzes MLMoE models using sigmoid gating functions instead of traditional softmax gating. The method employs Expectation-Maximization (EM) algorithm for maximum likelihood estimation, with numerical optimization (L-BFGS-B) for the M-step since gating parameters lack closed-form updates. Experiments use synthetic data with $k^*=2$ true experts and $k \in \{3,4\}$ fitted experts in over-specified settings. The analysis focuses on convergence rates measured through custom Voronoi loss functions ($D_1, D_{2,2}, D_3$) that compare estimated experts to ground truth experts. Key modifications include adding a positive scaling parameter $e^{\gamma_i}$ to the sigmoid gate and replacing inner product affinity scores with Euclidean distance to improve theoretical guarantees.

## Key Results
- Sigmoid gating exhibits lower sample complexity than softmax gating for both parameter and expert estimation in over-specified settings
- Modified sigmoid gate with positive scaling parameter $e^{\gamma_i}$ ensures convergence when the number of fitted experts exceeds true experts ($k > k^*$)
- Temperature-controlled sigmoid gates with Euclidean affinity scores achieve polynomial convergence rates ($O_P([\log(n)/n]^{1/4})$), while inner product affinity scores lead to exponentially slow convergence due to parameter interactions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sigmoid gating provides better sample efficiency than softmax gating in MLMoE models.
- **Mechanism:** Softmax gates suffer from slow convergence due to PDE interactions between gating parameters and expert parameters when expert intercepts are zero. Sigmoid gate structure inherently avoids these specific parameter interactions, allowing polynomial convergence rates.
- **Core assumption:** Over-specified setting with $k > k^*$ and distinct expert parameters.
- **Evidence anchors:** Abstract shows sigmoid gate has lower sample complexity; Page 6 explains sigmoid avoids parameter interactions that cause slow convergence in softmax.
- **Break condition:** Benefits may diminish if PDE interactions are mitigated by other means or if sigmoid gate is modified to reintroduce similar dependencies.

### Mechanism 2
- **Claim:** Positive scaling parameter $e^{\gamma_i}$ within sigmoid gate is required for convergence in over-specified settings.
- **Mechanism:** In over-specified settings, multiple fitted components approximate a single true expert. Standard sigmoid cannot sum to match required scaling without multiplicative factor $e^{\gamma_i}$, which allows local approximations to aggregate correctly to ground truth conditional density.
- **Core assumption:** Voronoi cell decomposition where multiple fitted atoms correspond to a single true atom.
- **Evidence anchors:** Page 4 states inclusion of $e^{\gamma_i}$ plays vital role in conditional density convergence in absence of standard sigmoid.
- **Break condition:** If model is exactly specified ($k = k^*$), density might converge without modification, but over-specified regime requires this change.

### Mechanism 3
- **Claim:** Euclidean affinity score prevents exponentially slow convergence compared to inner product score with temperature.
- **Mechanism:** Standard inner product with temperature creates PDE interactions coupling temperature with gating parameters, preventing effective Hellinger distance bounds. Euclidean score mathematically decouples these derivatives, restoring polynomial convergence.
- **Core assumption:** Bounded input space $\mathcal{X}$ and compact parameters.
- **Evidence anchors:** Page 7 identifies intrinsic interaction between temperature and gating parameters; Page 9 shows Euclidean affinity eliminates this interaction.
- **Break condition:** If Euclidean norm is replaced by function re-introducing linear dependencies on $\alpha$ inside scaling argument relative to $\tau$.

## Foundational Learning

- **Concept:** Mixture of Experts (MoE) Architecture
  - **Why needed here:** Base system being analyzed; understanding gating mechanism routing inputs to experts is fundamental.
  - **Quick check question:** Can you explain how a gating network decides which expert to use for a given input?

- **Concept:** Sample Complexity / Convergence Rates ($O_P$)
  - **Why needed here:** Paper's core contribution is theoretical comparison of how fast parameters converge as data increases; must distinguish polynomial (fast) from exponential/logarithmic (slow) rates.
  - **Quick check question:** Why is $O_P(n^{-1/2})$ considered "faster" than $O_P(1/\log(n))$?

- **Concept:** Partial Differential Equations (PDEs) in Identifiability
  - **Why needed here:** "Failure mode" of softmax and inner-product-with-temperature mechanisms described via PDE interactions causing parameters to be indistinguishable or slowly converging.
  - **Quick check question:** How can a relationship between derivatives (PDE) prevent a model from learning parameters independently?

## Architecture Onboarding

- **Component map:** Input ($X$) → Gating Network (Sigmoid function with Euclidean affinity $||\alpha-X||$ and Temperature $\tau$) → Expert Weights ($\pi_i$) → Input ($X$) → Expert $i$ (Multinomial Logistic Classifier) → Class Probability ($f(Y|X)$) → Aggregation: $\sum \pi_i \cdot f_i(Y|X)$

- **Critical path:** Calculation of affinity score (Euclidean vs. Inner Product) and application of temperature scaling $\tau$ within sigmoid gate determines convergence speed.

- **Design tradeoffs:**
  - Inner Product vs. Euclidean Affinity: Inner product is computationally standard but theoretically brittle when temperature is used; Euclidean adds norm operation but restores polynomial convergence guarantees.
  - Softmax vs. Sigmoid: Sigmoid offers better sample efficiency in classification; Softmax is traditionally more common but suffers from parameter interactions in specific regimes.

- **Failure signatures:**
  - Exponentially slow convergence: Using Inner Product affinity with learnable temperature $\tau$ causes loss to decrease as $O(1/\log(n))$, effectively stalling learning as data scales.
  - Non-convergence of density: Using standard sigmoid (without $e^{\gamma}$ scaling) in over-specified settings causes model parameters not to converge to ground truth.

- **First 3 experiments:**
  1. Replicate Sigmoid vs. Softmax: Train MLMoE on synthetic data with $k > k^*$ to verify Sigmoid achieves lower Voronoi loss ($D_1$) with fewer samples than Softmax in "Regime 2" conditions.
  2. Isolate Temperature Interaction: Train with Temperature + Inner Product vs. Temperature + Euclidean affinity; plot convergence rates on log-log scale to observe polynomial vs. slow/exponential gap.
  3. Verify Modified Sigmoid: Train standard sigmoid MoE vs. modified ($e^{\gamma}$) sigmoid MoE in over-specified setting and measure bias in conditional density estimation.

## Open Questions the Paper Calls Out

- **Question:** Can convergence rates for parameter and expert estimation be generalized to MLMoE where experts are deep neural networks rather than linear functions?
  - **Basis in paper:** Authors state in Limitations section they can generalize results to arbitrary neural networks.
  - **Why unresolved:** Current proofs rely on specific multinomial logistic expert form; complex neural networks may alter gating-expert parameter interactions.
  - **What evidence would resolve it:** Theoretical extension of Voronoi loss and convergence bounds accounting for non-linearity and non-convexity of deep expert networks.

- **Question:** What are convergence guarantees for modified sigmoid-gated MLMoE under model misspecification?
  - **Basis in paper:** Authors note developing theory under misspecification remains open problem; geometric foundation may break down.
  - **Why unresolved:** Under misspecification, MLE converges to pseudo-true measure but uniqueness is difficult to characterize; Voronoi-cell decomposition may not hold if pseudo-true experts are not well-separated.
  - **What evidence would resolve it:** Convergence analysis establishing rates to KL divergence minimizer over non-convex parameter space.

- **Question:** Does Euclidean affinity score maintain theoretical sample efficiency advantages in high-dimensional, unbounded real-world data?
  - **Basis in paper:** Paper validates Euclidean score on synthetic bounded data but claims practical implications for LLMs; remains unverified in complex domains.
  - **Why unresolved:** High-dimensional spaces may affect gradient dynamics and numerical stability differently than inner product, potentially introducing optimization difficulties.
  - **What evidence would resolve it:** Empirical benchmarks comparing convergence and performance using Euclidean vs. inner product affinity scores on large-scale, high-dimensional datasets.

## Limitations
- Analysis relies entirely on synthetic data experiments without real-world validation
- Theoretical analysis assumes bounded input spaces and compact parameter sets
- EM optimization details are not fully specified (solver type, learning rates, initialization variance)
- Convergence rates are analyzed asymptotically but practical implications for finite samples are not explored in depth

## Confidence
- **Medium confidence**: Theoretical framework is rigorous with provided proofs, but validation relies on synthetic data with specific parameter configurations; real-world results would strengthen practical relevance.

## Next Checks
1. **Real Data Validation**: Apply proposed sigmoid gating with Euclidean affinity to real-world classification dataset (e.g., CIFAR-10 or tabular dataset) to verify sample efficiency benefits translate beyond synthetic settings.

2. **Temperature Scaling Study**: Conduct controlled experiment varying temperature parameter τ across range of values for both inner product and Euclidean affinity scores, measuring actual convergence rates to quantify exponential vs. polynomial gap empirically.

3. **Over-specification Robustness**: Test modified sigmoid gate (with $e^{\gamma_i}$ scaling) across different levels of over-specification (varying $k/k^*$ ratios) and different ground truth distributions to assess robustness of convergence guarantees in more challenging settings.