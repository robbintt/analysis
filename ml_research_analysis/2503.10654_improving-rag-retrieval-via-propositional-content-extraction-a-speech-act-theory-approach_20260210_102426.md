---
ver: rpa2
title: 'Improving RAG Retrieval via Propositional Content Extraction: a Speech Act
  Theory Approach'
arxiv_id: '2503.10654'
source_url: https://arxiv.org/abs/2503.10654
tags:
- content
- speech
- propositional
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the mismatch between user queries and document
  embeddings in Retrieval-Augmented Generation (RAG) systems, caused by the presence
  of pragmatic markers like interrogatives and polite requests in queries. Drawing
  on speech act theory, the study proposes extracting the propositional content from
  queries by removing illocutionary force indicators before embedding.
---

# Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach

## Quick Facts
- **arXiv ID:** 2503.10654
- **Source URL:** https://arxiv.org/abs/2503.10654
- **Reference count:** 40
- **Primary result:** Removing illocutionary markers from queries improves semantic similarity with declarative knowledge bases in RAG systems.

## Executive Summary
This paper addresses the retrieval mismatch in RAG systems when user queries contain pragmatic markers like interrogatives and polite requests, which create embedding vector misalignment with assertive document embeddings. Drawing on speech act theory, the study proposes extracting the propositional content from queries by removing illocutionary force indicators before embedding. The method transforms queries into simplified statements, aligning them better with the declarative format of typical knowledge bases. Experiments on a Brazilian telecommunications news corpus with 63 user queries demonstrate that propositional content extraction significantly improves semantic similarity between query and document embeddings, particularly for maximum and mean similarity scores across most speech act types.

## Method Summary
The approach uses GPT-4 with a structured system prompt to extract propositional content from user queries by removing illocutionary markers (interrogatives, politeness terms, performative verbs) while preserving core semantic content. Both original and transformed queries are embedded using OpenAI's text-embedding-3-large model, truncated to 256 dimensions via Matryoshka Representation Learning. The method compares cosine similarity between query embeddings and a corpus of 189,585 passages from 64,983 Brazilian telecommunications news articles. The study evaluates 63 queries across seven speech act types (assertive, interrogative, directive, expressive, commissive, indirect, declarative) by comparing similarity metrics between original and propositional forms.

## Key Results
- Propositional extraction significantly improved maximum similarity scores across most speech act types: expressive (0.7504→0.8196), directive (0.8008→0.8124), and commissive (0.6786→0.7530)
- Assertive queries showed negligible improvement (0.8137→0.8100), confirming they already align well with declarative documents
- The method successfully handled complex indirect speech acts by extracting their underlying propositional content
- Over-aggressive trimming occasionally harmed similarity when key phrases were removed

## Why This Works (Mechanism)

### Mechanism 1
Removing illocutionary markers from queries improves embedding similarity with declarative knowledge bases because user queries often contain pragmatic markers that shift the embedding vector away from core informational content. Embedding models trained on natural text encode syntactic and pragmatic features, not just semantic content, while documents in knowledge bases are mostly declarative. The transformation direction should reverse if knowledge bases contain mostly interrogative or directive content.

### Mechanism 2
Propositional content extraction increases maximum and mean cosine similarity scores across most speech act types by converting interrogatives, directives, and expressives into simplified statements or noun phrases. The reduced query length and focused content yield higher similarity at top ranks. Over-aggressive removal of content words can reduce similarity by losing key terms, so the extraction must distinguish between pragmatic markers and core content.

### Mechanism 3
Assertive queries show negligible improvement because they already embed propositional content directly and match document style. The method adds little value for already-aligned inputs, and embedding models are sensitive to even small text modifications, though the effect is neutral or slightly negative for assertive queries. If assertive queries contain excessive hedging or meta-commentary, extraction may still help.

## Foundational Learning

- **Speech Act Theory (Austin/Searle):** Provides the theoretical distinction between propositional content (what is said) and illocutionary force (intent behind saying it), which is the basis for query transformation. *Quick check:* Can you identify the propositional content in "Could you please explain why the system failed?"

- **Dense Embeddings & Semantic Similarity:** RAG retrieval depends on cosine similarity between query and document vectors; understanding how embeddings encode syntax vs. semantics is essential. *Quick check:* Why might a question and its answer statement have different embeddings despite sharing the same core information?

- **Query Reformulation Strategies:** This method is one of several approaches (alongside HyDE, synthetic question generation) to address query-document mismatch; understanding alternatives helps evaluate tradeoffs. *Quick check:* How does propositional extraction differ from generating a hypothetical answer document (HyDE)?

## Architecture Onboarding

- **Component map:** Query → Propositional Extraction → Embedding → Similarity Search → Top-k Chunks → Generator
- **Critical path:** User query flows through LLM-based propositional extraction, then embedding, similarity search against corpus, and finally to RAG generator
- **Design tradeoffs:** LLM-based extraction handles nuanced speech acts but adds latency and cost versus faster rule-based alternatives; character reduction risks losing key terms while preserving content; method assumes assertive-document corpus requiring inversion for other styles
- **Failure signatures:** Over-stripping removes core terms dropping similarity (paper notes max similarity fell from 0.7444 to 0.7334); assertive-only inputs show no improvement adding unnecessary processing; non-informational speech acts yield empty or meaningless output
- **First 3 experiments:** 1) Baseline comparison: Run original queries vs. propositional queries on same corpus measuring max/mean similarity and top-k overlap 2) Ablation by speech act type: Test each category separately to identify which benefit most 3) Precision/recall validation: Use labeled subset to compute standard IR metrics comparing against semantic similarity proxy

## Open Questions the Paper Calls Out

- **Does propositional content extraction improve standard retrieval metrics or final answer quality in RAG systems?** While cosine similarity increased, this does not guarantee retrieved chunks were more relevant or that LLM generated better answers. A comparative evaluation using labeled benchmark datasets or human assessment of RAG-generated responses would resolve this.

- **How can the extraction process be optimized to prevent accidental removal of key semantic phrases that negatively impact similarity?** The current method does not always distinguish effectively between pragmatic markers and core content keywords. An ablation study comparing different extraction prompt strategies that penalize removal of named entities or domain-specific terminology would help.

- **Is performance consistent across different embedding models and languages?** The study is limited to a single embedding model and specific domain. Cross-lingual experiments using diverse embedding models on non-English or mixed-language corpora would determine if improvements are universal.

## Limitations

- The evaluation uses a single Brazilian telecommunications corpus with declarative-style articles, limiting generalization to knowledge bases with different structural patterns
- Aggressive removal of pragmatic markers can inadvertently strip essential content words, reducing semantic similarity in some cases
- The method relies on GPT-4 for propositional extraction, introducing latency, API costs, and potential variability across model versions

## Confidence

- **High Confidence:** The core mechanism that declarative knowledge bases benefit from declarative-style queries is theoretically sound and empirically supported within the evaluated corpus
- **Medium Confidence:** The improvement in similarity metrics is convincing for this corpus, but the absence of standard IR metrics and lack of cross-domain validation limits generalizability claims
- **Low Confidence:** The claim that this approach is broadly applicable across diverse RAG use cases is not yet substantiated without testing on non-assertive-heavy corpora or comparison to alternative query reformulation strategies

## Next Checks

1. **Cross-Domain Replication:** Apply the propositional extraction method to a distinctly different corpus (e.g., legal documents, FAQs, or mixed-style technical documentation) and measure whether similarity improvements persist or require inversion of the transformation logic.

2. **Precision/Recall Benchmarking:** Using a subset of the corpus with human-labeled relevance judgments, compute standard IR metrics (precision@K, recall@K, nDCG) for both original and propositional queries to validate that similarity improvements translate into actual retrieval quality gains.

3. **Ablation Study on Over-Trimming:** Systematically test the effect of varying the aggressiveness of marker removal by speech act type, measuring the tradeoff between character reduction and semantic preservation to identify optimal thresholds that avoid content loss.