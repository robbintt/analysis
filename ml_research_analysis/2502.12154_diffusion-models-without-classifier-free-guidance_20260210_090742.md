---
ver: rpa2
title: Diffusion Models without Classifier-free Guidance
arxiv_id: '2502.12154'
source_url: https://arxiv.org/abs/2502.12154
tags:
- diffusion
- guidance
- ours
- training
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model-guidance (MG), a novel training objective
  for diffusion models that eliminates the need for Classifier-free Guidance (CFG).
  MG incorporates the posterior probability of conditions directly into the diffusion
  model's training process by using the model itself as an implicit classifier, guiding
  the denoising process without requiring separate unconditional models or multiple
  forward passes during inference.
---

# Diffusion Models without Classifier-free Guidance

## Quick Facts
- arXiv ID: 2502.12154
- Source URL: https://arxiv.org/abs/2502.12154
- Authors: Zhicong Tang; Jianmin Bao; Dong Chen; Baining Guo
- Reference count: 18
- Primary result: Eliminates need for classifier-free guidance (CFG) in diffusion models by incorporating posterior probability into training, achieving 6.5x faster training and 2x faster inference while matching or exceeding CFG performance on ImageNet benchmarks.

## Executive Summary
This paper introduces Model-guidance (MG), a novel training objective for diffusion models that eliminates the need for Classifier-free Guidance (CFG). MG incorporates the posterior probability of conditions directly into the diffusion model's training process by using the model itself as an implicit classifier, guiding the denoising process without requiring separate unconditional models or multiple forward passes during inference. The proposed method significantly accelerates training (6.5x faster convergence), doubles inference speed, and achieves state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.

## Method Summary
MG modifies the standard noise prediction target during training by incorporating the difference between conditional and unconditional EMA model predictions, scaled by guidance weight w. The modified target is: ε′ = ε + w·sg(ε̃_EMA(x_t,t,c) - ε̃_EMA(x_t,t,∅)), where sg(·) is stop-gradient and ε̃_EMA is the EMA model. This trains the model to directly predict the guided score rather than learning conditional and unconditional scores separately. During inference, only a single forward pass is needed regardless of guidance scale, as the model has learned to output the guided prediction directly. The method requires minimal code modifications and maintains the same model architecture as standard diffusion models.

## Key Results
- Achieves 6.5x faster training convergence compared to standard CFG training
- Doubles inference speed by eliminating the second forward pass required by CFG
- Sets new state-of-the-art FID of 1.34 on ImageNet 256×256 benchmarks
- Demonstrates effective scaling across different model sizes (DiT-B/2, SiT-B/2) and resolutions (256×256, 512×512)
- Shows robust performance across different guidance scales without requiring w as input during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating posterior probability p(c|xt) directly into training converges faster and removes need for inference-time guidance
- Mechanism: MG modifies training target to: ε′ = ε + w·sg(ε̃_θ(x_t,t,c) - ε̃_θ(x_t,t,∅)), where EMA model provides stable score estimates
- Core assumption: EMA model provides stable estimates of conditional/unconditional score difference that approximates ∇_xt log p(c|xt) via Bayes' rule
- Evidence anchors: [abstract] "incorporates posterior probability directly"; [Section 3.2] training loss L_MG with modified target
- Break condition: Early training instability if EMA model provides noisy posterior estimates

### Mechanism 2
- Claim: MG doubles inference speed by eliminating second forward pass required by CFG
- Mechanism: Standard CFG requires computing both conditional and unconditional predictions; MG-trained models directly output guided prediction in single forward pass
- Core assumption: Trained model generalizes to inference-time w without requiring w as input during training
- Evidence anchors: [abstract] "doubles inference speed"; [Section 3.3] "our models require only one forward per step"
- Break condition: Quality degradation if inference-time w differs significantly from training w

### Mechanism 3
- Claim: MG improves sample quality by avoiding CFG's distribution distortion while maintaining diversity
- Mechanism: CFG pushes samples toward high-posterior regions causing "exaggerated truncation and mode dropping"; MG learns joint distribution that produces more balanced quality-diversity trade-offs
- Core assumption: MG's training objective corresponds to valid diffusion process, whereas CFG's joint distribution "does not represent valid heat diffusion"
- Evidence anchors: [Figure 2] 2D toy example showing better coverage; [Section 3.1] theoretical claims about distribution validity
- Break condition: Modified objective may introduce its own distribution artifacts not captured by FID/IS metrics

## Foundational Learning

- Concept: **Score-based diffusion and denoising objective**
  - Why needed here: MG modifies standard noise prediction target; understanding ε-prediction vs. score prediction is essential
  - Quick check question: Can you explain why ∇_xt log p(x_t|c) ∝ -ε_θ(x_t,t,c)/σ_t?

- Concept: **Classifier-free Guidance (CFG) mechanics**
  - Why needed here: MG is derived from CFG; must understand Equation 9 to see why MG modifies training rather than inference
  - Quick check question: Why does CFG require both conditional and unconditional model outputs at inference?

- Concept: **Bayes' rule for score decomposition**
  - Why needed here: MG uses Bayes' rule to transform diffusion model into implicit classifier
  - Quick check question: How does log p(c|xt) relate to log p(xt|c) and log p(xt)?

## Architecture Onboarding

- Component map:
  - Model architecture (DiT/SiT) -> Training loop with MG loss -> EMA model maintenance -> Condition dropout -> Optional w-input network

- Critical path:
  1. Initialize model + EMA copy
  2. For each batch: sample (x_0, c), noise ε, timestep t
  3. Compute x_t via forward process
  4. **Key modification**: Compute ε′ = ε + w·sg(ε̃_EMA(x_t,c,t) - ε̃_EMA(x_t,∅,t))
  5. Train model to predict ε′ (not ε)
  6. Update EMA

- Design tradeoffs:
  - Empty class vs. total probability: Using ∅ is simpler but multitask learning may hurt capability; summing over all classes is cleaner but expensive for large label spaces
  - Fixed vs. adaptive w: Manual w search is costly; adaptive w via EMA is convenient but less controllable
  - w-input vs. fixed w: w-input offers inference flexibility but slightly worse performance

- Failure signatures:
  - Training instability early-on: EMA model may provide poor estimates initially; consider warmup with w=0
  - Mode collapse: If w is too high, posterior term dominates; check sample diversity (Recall metric)
  - Quality degradation at inference w ≠ training w: Verify w generalization via Figure 4-style sweeps

- First 3 experiments:
  1. Reproduce MG on DiT-B/2 with ImageNet subset (100K iterations) to validate training loop and FID improvement vs. vanilla CFG
  2. Ablate EMA vs. online model for stop-gradient target to assess stability requirements
  3. Test w generalization: train with w=1.5, evaluate FID at inference w ∈ {1.0, 1.5, 2.0, 2.5} to characterize robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Model-guidance scale effectively to complex text-to-image generation tasks involving dense captions or abstract prompts?
- Basis in paper: Experimental validation restricted to class-conditional ImageNet benchmarks
- Why unresolved: Method has not been tested on high-dimensional, open-vocabulary text embeddings used in modern large-scale generative models
- What evidence would resolve it: Applying MG to text-to-image benchmarks (MS-COCO, MJHQ-30K) and comparing alignment scores and visual coherence against CFG baselines

### Open Question 2
- Question: Can a single MG-trained model provide the same granular control over quality-diversity trade-off as CFG?
- Basis in paper: Section 3.3 discusses "w-input" variant but shows slightly degraded performance
- Why unresolved: "w-input" variant underperforms compared to fixed w variant, suggesting MG may lock model into specific guidance behavior
- What evidence would resolve it: Study evaluating interpolation capability of MG models across continuous range of guidance scales at inference time

### Open Question 3
- Question: Is the MG objective compatible with consistency distillation or progressive distillation techniques for few-step generation?
- Basis in paper: Section 2.3 distinguishes MG from two-stage distillation approaches
- Why unresolved: Paper does not investigate if modified score field is amenable to further compression into one-step or few-step generators
- What evidence would resolve it: Experiments applying consistency distillation or progressive distillation on top of MG-trained model to evaluate few-step performance

## Limitations

- Theoretical foundation for convergence benefits relies on unproven claims about EMA stability providing accurate posterior estimates
- Method has not been validated on complex text-to-image generation tasks with dense captions or abstract prompts
- Single MG-trained model may not provide the same granular control over quality-diversity trade-off as CFG at inference time

## Confidence

- **High confidence**: Training speedup (6.5×), inference speedup (2×), and basic MG implementation details are empirically demonstrated and straightforward to verify
- **Medium confidence**: FID improvements and state-of-the-art claims on ImageNet benchmarks, though theoretical justification for quality gains is incomplete
- **Low confidence**: Theoretical claims about valid diffusion processes and distribution preservation lack formal proof within the paper

## Next Checks

1. **EMA stability verification**: Train MG with different EMA decay rates (0.999, 0.9999, 0.99999) and measure early training FID curves to identify instability thresholds
2. **w generalization bounds**: Systematically test inference FID across w ∈ [0.5, 3.0] for models trained with w ∈ {1.0, 1.5, 2.0} to quantify performance degradation outside training w
3. **Distribution preservation analysis**: Compare samples from MG vs. CFG at equal FID using perceptual metrics (LPIPS, KID) and diversity measures (Recall) to verify quality-diversity claims hold perceptually