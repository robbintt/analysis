---
ver: rpa2
title: 'Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly
  an NLL Estimator'
arxiv_id: '2502.04567'
source_url: https://arxiv.org/abs/2502.04567
tags:
- preference
- completions
- mc-po
- optimization
- dispreferred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for preference optimization
  by formulating it as a negative log-likelihood (NLL) estimation problem. The key
  insight is that dispreferred completions in preference optimization can be viewed
  as samples used to estimate the intractable normalization constant in NLL.
---

# Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator

## Quick Facts
- arXiv ID: 2502.04567
- Source URL: https://arxiv.org/abs/2502.04567
- Reference count: 34
- Key outcome: Proposes MC-PO algorithm using contrastive divergence sampling for hard negative selection in preference optimization

## Executive Summary
This paper introduces a novel framework for preference optimization by reformulating it as a negative log-likelihood (NLL) estimation problem. The authors propose that dispreferred completions can be viewed as samples used to estimate the intractable normalization constant in NLL. They develop MC-PO, an offline algorithm that leverages contrastive divergence sampling to select hard negatives that closely resemble preferred completions, making it more challenging for the model to distinguish between them. An extension to online settings, OnMC-PO, is also introduced.

## Method Summary
The paper proposes a framework that casts preference optimization as NLL estimation, where the reward model implicitly estimates the normalization constant. MC-PO uses contrastive divergence (CD) sampling to generate hard negatives - dispreferred completions that closely resemble preferred ones. The algorithm iteratively samples from the current policy and uses these samples to approximate the partition function in the NLL objective. OnMC-PO extends this to online settings by continuously updating the sampling distribution. Both methods aim to create more challenging optimization landscapes by ensuring hard negatives are sufficiently similar to preferred samples, forcing the model to learn finer-grained distinctions.

## Key Results
- MC-PO outperforms existing state-of-the-art baselines on popular alignment benchmarks
- OnMC-PO leads to further improvements, achieving higher winrates against GPT-4 in online evaluations
- Theoretical analysis shows that sampling preferred completions from the target policy provides unbiased gradient estimation of the normalization constant

## Why This Works (Mechanism)
The method works by recognizing that preference optimization implicitly involves estimating a partition function (normalization constant). By using contrastive divergence sampling, the algorithm generates dispreferred completions that are intentionally difficult to distinguish from preferred ones. This creates a more challenging optimization problem where the model must learn subtle differences between high-reward and low-reward completions. The hard negatives act as informative samples that improve the estimate of the partition function, leading to more effective gradient updates and better alignment with human preferences.

## Foundational Learning

**Negative Log-Likelihood Estimation**
- Why needed: Forms the theoretical foundation for casting preference optimization as a probabilistic inference problem
- Quick check: Verify that the NLL formulation correctly captures the preference optimization objective

**Contrastive Divergence Sampling**
- Why needed: Provides a method to generate hard negatives that closely resemble preferred completions
- Quick check: Confirm that CD sampling produces samples with appropriate similarity to preferred completions

**Partition Function Estimation**
- Why needed: Essential for normalizing the probability distribution in the NLL framework
- Quick check: Validate that the estimated partition function converges and provides stable gradient estimates

## Architecture Onboarding

**Component Map**
Reward Model -> Contrastive Divergence Sampler -> Hard Negative Generator -> Policy Update -> (Optional: Online Feedback Loop)

**Critical Path**
1. Generate preferred completions from current policy
2. Use CD sampling to generate hard negatives
3. Compute NLL objective with estimated partition function
4. Update policy parameters via gradient descent
5. (OnMC-PO only) Incorporate online feedback to refine sampling distribution

**Design Tradeoffs**
- Quality vs. computational cost of hard negative generation
- Bias-variance tradeoff in partition function estimation
- Exploration-exploitation balance in online sampling

**Failure Signatures**
- Poor hard negative quality leading to ineffective gradients
- Instability in partition function estimation causing training divergence
- Overfitting to synthetic hard negatives that don't generalize to real preferences

**3 First Experiments**
1. Compare MC-PO performance with random negative sampling baselines
2. Ablation study on CD sampling temperature/hyperparameters
3. Evaluate partition function estimation accuracy across different sample sizes

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several theoretical and practical considerations remain unaddressed regarding the scalability and robustness of the proposed methods.

## Limitations
- Theoretical foundations require more rigorous validation, particularly around unbiased gradient estimation assumptions
- Empirical evaluation is limited to specific alignment benchmarks and may not generalize broadly
- Computational overhead of the algorithm compared to simpler preference optimization methods is not thoroughly characterized

## Confidence

| Claim | Confidence |
|-------|------------|
| Mathematical formulation of preference optimization as NLL estimation | High |
| Contrastive divergence sampling approach for generating hard negatives | Medium |
| Scalability and practical applicability to large-scale tasks | Low |

## Next Checks
1. Conduct ablation studies to quantify the impact of hard negative sampling quality on final model performance, comparing against simpler negative sampling strategies
2. Test the algorithm on a wider range of alignment tasks and datasets beyond the current benchmarks to assess generalizability
3. Measure and report the computational overhead of MC-PO and OnMC-PO compared to standard preference optimization methods across different model scales