---
ver: rpa2
title: 'Trust Modeling in Counseling Conversations: A Benchmark Study'
arxiv_id: '2501.03064'
source_url: https://arxiv.org/abs/2501.03064
tags:
- trust
- patient
- therapist
- counseling
- therapeutic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces trust as a novel metric for assessing the
  therapeutic bond in counseling conversations, conceptualizing it as a dynamic trajectory
  observable through textual interactions. To facilitate trust modeling, the authors
  present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of
  212 counseling sessions with seven expert-verified ordinal trust levels.
---

# Trust Modeling in Counseling Conversations: A Benchmark Study

## Quick Facts
- arXiv ID: 2501.03064
- Source URL: https://arxiv.org/abs/2501.03064
- Authors: Aseem Srivastava; Zuhair Hasan Shaik; Tanmoy Chakraborty; Md Shad Akhtar
- Reference count: 37
- Primary result: Mental-BART achieves 89.03% accuracy in ordinal classification of patient trust levels from counseling dialogues

## Executive Summary
This paper introduces trust as a novel metric for assessing the therapeutic bond in counseling conversations, conceptualizing it as a dynamic trajectory observable through textual interactions. To facilitate trust modeling, the authors present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of 212 counseling sessions with seven expert-verified ordinal trust levels. They project their problem statement as an ordinal classification task and propose TrustBench, a benchmarking suite of classical and state-of-the-art language models evaluated on MENTAL-TRUST. The best-performing model, Mental-BART, achieves 89.03% accuracy, 88.96% F1-weighted, and 2.53 ordinal cross-entropy loss. Analysis reveals that smaller models like BART, BERT, and DeBERTa outperform larger language models in capturing trust fluctuations, with Mental-BART showing the closest alignment to ground-truth trust trajectories across increasing, decreasing, and neutral cases.

## Method Summary
The paper introduces trust as an ordinal classification task using MENTAL-TRUST dataset with 212 counseling sessions (12.9K utterances) annotated with 7 trust levels. The method uses a context window of preceding utterances to predict trust scores for each patient utterance. Models are fine-tuned on this dataset using ordinal cross-entropy loss, with Mental-BART achieving the best performance at 89.03% accuracy. The approach leverages encoder-decoder architectures and explicitly models trust as a dynamic trajectory rather than static sentiment.

## Key Results
- Mental-BART achieves 89.03% accuracy, 88.96% F1-weighted, and 2.53 ordinal cross-entropy loss on MENTAL-TRUST
- Smaller models (BART, BERT, DeBERTa) outperform larger language models in capturing trust fluctuations
- Mental-BART shows closest alignment to ground-truth trust trajectories across increasing, decreasing, and neutral cases
- Context windowing of preceding utterances significantly improves trust detection accuracy

## Why This Works (Mechanism)

### Mechanism 1: Contextual Ordinality over Static Sentiment
- Modeling trust as a dynamic trajectory using context windows and ordinal labels appears more effective than treating utterances as isolated sentiment classification tasks
- The architecture utilizes a sliding context window of preceding utterances to capture the "willingness to disclose" and frames the output as ordinal classification (1 to 4 with intermediate steps)
- Core assumption: Trust is not inherent in a single sentence but is a relational state dependent on the preceding interaction history
- Break condition: If the context window k is too small to capture the "digression" phase, the model may misinterpret a short response as "low trust" rather than a pause in an otherwise high-trust interaction

### Mechanism 2: Specialized Fine-Tuning vs. General Reasoning
- Smaller, domain-specific models (e.g., Mental-BART) outperform Large Language Models (LLMs) because they optimize for specific psycholinguistic patterns rather than broad world knowledge
- Fine-tuning forces the model to align its weights with the specific "expert-verified" ordinal labels of the MENTAL-TRUST dataset
- Core assumption: The signals for "trust" (hesitation, self-disclosure, topic alignment) are distinct from general conversational fluency and require supervised adaptation
- Break condition: If the fine-tuning dataset lacks diversity, the smaller model will overfit to specific linguistic markers that may not generalize

### Mechanism 3: Topic Alignment as a Trust Proxy
- Monitoring topic adherence serves as a robust proxy for detecting trust levels (specifically "digression" vs. "opening up")
- The annotation process explicitly links trust levels to topic focus, with "Building Trust" allowing some digression while "Achieved Trust" requires alignment with the core issue
- Core assumption: Patients only fully disclose sensitive information when they stop deflecting to peripheral topics
- Break condition: If the model cannot distinguish between a "topic shift" (negative) and a "deepening of the topic" (positive), it will produce false negatives in trust trajectory

## Foundational Learning

- **Concept: Ordinal Regression/Classification**
  - Why needed here: Trust exists on a scale (1-4) where the distance between levels matters
  - Quick check question: If a model predicts Trust Level 3 when the ground truth is 3.5, is this error better or worse than predicting Level 1?

- **Concept: Context Windowing in Dialogues**
  - Why needed here: A patient saying "I guess" might mean low trust initially but something different 10 turns later
  - Quick check question: How does increasing the context window size k affect the computational complexity and potential noise in the trust signal?

- **Concept: Therapeutic Alliance (The "Bond")**
  - Why needed here: This is the theoretical underpinning, involving "willingness to depend" despite "lack of control"
  - Quick check question: Does the definition of trust rely more on the therapist's empathy or the patient's behavior (self-disclosure)?

## Architecture Onboarding

- **Component map:** Tokenizer -> Encoder (Mental-BART) -> Classification Head -> Loss Function (Ordinal Cross-Entropy)
- **Critical path:** 1) Data Prep: Group utterances into dialogue histories with sliding windows 2) Fine-Tuning: Train Mental-BART on MENTAL-TRUST 3) Evaluation: Use OLCE and trajectory analysis
- **Design tradeoffs:** Use SLMs like BART for accuracy and trajectory adherence; use LLMs only if topic summarization is the goal, not trust quantification
- **Failure signatures:** "Chitchat" Confound (predicting high trust due to talking volume but patient is digressing) and Flat-lining (predicting neutral baseline for all inputs)
- **First 3 experiments:** 1) Baseline Verification: Reproduce Mental-BART vs. GPT-4o comparison 2) Context Ablation: Vary history window k (0, 3, 5, 10 turns) 3) Topic-Trust Correlation: Train dual-input model (Dialogue + Topic Label)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can specialized neural architectures or loss functions tailored for ordinal classification outperform the current fine-tuned Mental-BART baseline in modeling trust trajectories?
- **Open Question 2:** To what extent does the integration of multimodal data (vocal prosody and facial expressions) improve the prediction of trust levels compared to text-only analysis?
- **Open Question 3:** How effectively do trust modeling frameworks transfer across different languages and cultural contexts in therapeutic settings?

## Limitations
- Dataset is not yet publicly released, requiring recreation from HOPE dataset with expert annotation
- Optimal context window size k remains unspecified, creating ambiguity in model architecture design
- Study is limited to English-language interactions, restricting applicability in multilingual contexts

## Confidence

- **High Confidence:** Ordinal classification approach for trust modeling and superiority of smaller fine-tuned models over LLMs
- **Medium Confidence:** Therapeutic alliance proxy mechanisms (topic alignment and digression detection) as indicators of trust levels
- **Low Confidence:** Generalizability of trust trajectory patterns across different cultural contexts and counseling modalities

## Next Checks

1. **Cross-Cultural Validation:** Test Mental-BART on counseling conversations from diverse cultural backgrounds to assess whether trust indicators generalize beyond the original dataset's demographic scope

2. **Temporal Stability Analysis:** Track trust level predictions across multiple counseling sessions with the same patient to verify that the model captures persistent trust trajectories rather than session-specific artifacts

3. **Clinical Outcome Correlation:** Compare model-predicted trust trajectories with actual therapeutic outcomes (e.g., symptom reduction, session attendance) to validate whether the ordinal trust levels serve as reliable predictors of counseling effectiveness