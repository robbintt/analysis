---
ver: rpa2
title: Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification
  and Object Detection
arxiv_id: '2502.07821'
source_url: https://arxiv.org/abs/2502.07821
tags:
- attacks
- attack
- image
- rfpar
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a query-based black-box adversarial attack
  method called Remember and Forget Pixel Attack using Reinforcement Learning (RFPAR),
  which extends pixel attacks from image classification to object detection. The core
  idea is to use a one-step reinforcement learning algorithm with a "Remember" process
  that stores high-reward perturbations and a "Forget" process that resets the agent
  to prevent overfitting.
---

# Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection

## Quick Facts
- **arXiv ID**: 2502.07821
- **Source URL**: https://arxiv.org/abs/2502.07821
- **Reference count**: 40
- **Primary result**: RFPAR achieves 64.1% attack success rate on ImageNet-1K with 211 L0 norm and 613 queries for ViT-B

## Executive Summary
This paper introduces Remember and Forget Pixel Attack using Reinforcement Learning (RFPAR), a query-based black-box adversarial attack method that extends pixel attacks from image classification to object detection. The core innovation is a one-step reinforcement learning algorithm with a "Remember" process that stores high-reward perturbations and a "Forget" process that resets the agent to prevent overfitting. Experiments demonstrate RFPAR achieves state-of-the-art performance with 64.1% success rate on ImageNet-1K while reducing queries by 26.0% and L0 norm by 41.1% compared to existing methods.

The method is further extended to object detection tasks on MS-COCO and Argoverse datasets, showing comparable performance to existing methods while significantly reducing query costs. On Argoverse, RFPAR achieves high object removal rates above 0.9 with minimal pixel modifications (0.1%), demonstrating effectiveness on larger-scale datasets. The "Forget" mechanism proves particularly valuable for maintaining attack effectiveness across diverse scenarios and model architectures.

## Method Summary
RFPAR employs a reinforcement learning framework where an agent iteratively perturbs pixels in images to maximize adversarial rewards. The agent uses a one-step Q-learning algorithm to select pixel modifications based on the current state and estimated Q-values. The key innovation is the dual-process mechanism: the "Remember" process stores successful perturbations in a buffer for future reference, while the "Forget" process resets the agent after each successful attack to prevent overfitting to specific image features. This approach allows the agent to learn generalizable attack patterns rather than memorizing specific vulnerabilities in individual images.

For object detection, RFPAR extends the classification framework by modifying the reward function to account for both classification accuracy and bounding box detection metrics. The agent optimizes perturbations that simultaneously degrade classification performance and object detection capabilities. The method demonstrates effectiveness across multiple detection architectures including YOLOv5 and Faster R-CNN, with particular success in removing detected objects from scenes while maintaining minimal pixel modifications.

## Key Results
- Achieves 64.1% attack success rate on ImageNet-1K with 211 L0 norm and 613 queries for ViT-B
- Outperforms state-of-the-art methods by 12.1% in success rate while reducing queries by 26.0% and L0 norm by 41.1%
- For object detection on MS-COCO, achieves comparable mAP reduction to GARSDC while reducing queries by 52.8%
- On Argoverse dataset, achieves object removal rates above 0.9 with only 0.1% pixel modification

## Why This Works (Mechanism)

The effectiveness of RFPAR stems from its ability to balance exploration and exploitation through the Remember/Forget mechanism. By storing high-reward perturbations, the agent can quickly identify promising attack directions without exhaustive search. The Forget mechanism prevents the agent from overfitting to specific image features by resetting its learned policy after each successful attack. This combination allows RFPAR to discover minimal, transferable perturbations that work across diverse images and models, rather than exploiting dataset-specific vulnerabilities.

## Foundational Learning

**Reinforcement Learning Basics**: Understanding Q-learning and policy optimization is essential for grasping how RFPAR learns to select optimal pixel perturbations. Quick check: Verify understanding of the Bellman equation and temporal difference learning.

**Black-box Adversarial Attacks**: Knowledge of query-based attack strategies and the distinction between white-box and black-box settings is crucial. Quick check: Compare the query efficiency of black-box vs white-box attacks on a simple dataset.

**Pixel-level Perturbations**: Understanding L0 norm constraints and their impact on imperceptibility is important for interpreting results. Quick check: Calculate the percentage of pixels modified in a 224x224 image with L0=211.

**Object Detection Metrics**: Familiarity with mAP (mean Average Precision) and its variants is necessary for understanding object detection attack results. Quick check: Compute mAP from precision-recall curves for a sample detection task.

**Transferability in Adversarial Attacks**: Understanding how perturbations generalize across models and datasets is key to interpreting RFPAR's effectiveness. Quick check: Test if adversarial examples transfer between different model architectures on CIFAR-10.

## Architecture Onboarding

**Component Map**: Input Image -> RL Agent (with Q-network) -> Pixel Modification -> Black-box Model -> Reward Calculation -> Buffer Update -> Policy Update

**Critical Path**: The most time-consuming aspect is the iterative query process where the agent tests pixel modifications and observes black-box model responses. Each query requires a forward pass through the target model, making query efficiency crucial for practical applicability.

**Design Tradeoffs**: The paper balances attack success rate against imperceptibility (L0 norm) and query efficiency. The Remember/Forget mechanism trades increased memory usage for improved attack generalization and reduced queries. Alternative approaches might use different RL algorithms or gradient estimation techniques, but these could increase computational complexity or reduce transferability.

**Failure Signatures**: Attacks may fail when the black-box model has strong input preprocessing defenses, when the reward function poorly captures the target model's behavior, or when the perturbation budget is insufficient for the specific image content. The Forget mechanism may also occasionally reset progress on particularly vulnerable images.

**First Experiments**:
1. Test RFPAR on a small subset of ImageNet with different L0 budgets to establish baseline performance
2. Compare query counts and success rates against a simple random pixel perturbation baseline
3. Evaluate the impact of the Forget mechanism by running experiments with and without it on the same image set

## Open Questions the Paper Calls Out

The paper acknowledges several open questions regarding scalability and generalization. While RFPAR demonstrates strong performance on ViT-B and Swin-B for image classification, and YOLOv5 and Faster R-CNN for object detection, results are limited to specific model families. The effectiveness of RFPAR on other architectures like ConvNeXt or Deformable DETR remains unexplored. Additionally, the "Forget" mechanism's ability to prevent overfitting is empirically demonstrated but lacks theoretical justification for why resetting the agent after each successful attack prevents the model from learning spurious correlations.

## Limitations

- Results are primarily demonstrated on ViT-B and Swin-B architectures, limiting generalizability to other model families
- The effectiveness of minimal perturbations (0.1% for Argoverse) may not hold against models with different training distributions or defense mechanisms
- Higher variance in object detection results across different models suggests architecture-specific vulnerabilities rather than universal attack patterns
- Limited theoretical analysis of why the Forget mechanism prevents overfitting, relying primarily on empirical validation

## Confidence

**High confidence**: The core reinforcement learning framework with Remember/Forget mechanisms is technically sound and well-implemented

**Medium confidence**: The quantitative improvements over baseline methods are valid but may be partially attributed to hyperparameter tuning specific to the tested models

**Medium confidence**: The extension from classification to object detection is methodologically valid but the object detection results show higher variance across different models

## Next Checks

1. Test RFPAR against a broader range of model architectures including ConvNeXt, Deformable DETR, and other vision transformers to assess generalizability

2. Evaluate the attack's robustness against common adversarial defense techniques like adversarial training and input preprocessing

3. Conduct ablation studies to quantify the individual contributions of the Remember and Forget mechanisms to overall performance