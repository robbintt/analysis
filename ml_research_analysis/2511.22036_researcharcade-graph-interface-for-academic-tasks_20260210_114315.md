---
ver: rpa2
title: 'ResearchArcade: Graph Interface for Academic Tasks'
arxiv_id: '2511.22036'
source_url: https://arxiv.org/abs/2511.22036
tags:
- academic
- table
- text
- arxiv
- openreview
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResearchArcade introduces a unified graph-based interface for academic
  tasks by integrating multi-source (ArXiv, OpenReview), multi-modal (text, figures,
  tables), and dynamically evolving academic data. The framework organizes data into
  a coherent multi-table format with graph structures, enabling efficient modeling
  of highly relational and heterogeneous academic relationships.
---

# ResearchArcade: Graph Interface for Academic Tasks

## Quick Facts
- arXiv ID: 2511.22036
- Source URL: https://arxiv.org/abs/2511.22036
- Reference count: 40
- Key outcome: Unified graph-based interface improves academic task performance by up to 67% through multi-source, multi-modal, and dynamically evolving data integration

## Executive Summary
ResearchArcade introduces a unified graph-based interface for academic tasks by integrating multi-source (ArXiv, OpenReview), multi-modal (text, figures, tables), and dynamically evolving academic data. The framework organizes data into a coherent multi-table format with graph structures, enabling efficient modeling of highly relational and heterogeneous academic relationships. A two-step task definition scheme allows unified formulation of predictive and generative tasks across various models. Experiments on six academic tasks show that incorporating graph structures consistently improves performance over baseline methods, with GNN- and GWM-based models achieving up to 67% gains. The multi-modal approach significantly enhances results, demonstrating the effectiveness of the framework in supporting diverse academic research tasks.

## Method Summary
The framework constructs a heterogeneous graph from multi-source academic data (ArXiv papers, OpenReview submissions, reviews, revisions) organized in a multi-table schema with node and edge tables. Multi-modal features (text, figures, tables) are extracted using specialized encoders (Longformer, LLaVA, Qwen3) and integrated into the graph structure. Tasks are formulated via a two-step process: identifying a target entity and retrieving its multi-hop neighborhood. Models include GNNs (HANConv) for predictive tasks and LLMs (Qwen3, GPTOSS) for generative tasks, trained with task-specific losses (cross-entropy, InfoNCE, BCE, SFT). The system preserves temporal evolution through revision tracking and peer review feedback integration.

## Key Results
- GNN-based models with graph structures achieve up to 67% performance gains over text-only baselines across six academic tasks
- Multi-modal integration (figures, tables) significantly improves generative tasks like paragraph generation and rebuttal generation
- Neighborhood expansion (1→3 hops) improves acceptance prediction accuracy by 30.9% but shows task-specific sensitivity
- Graph structures enable modeling of complex academic relationships (citations, authorship, review-revision cycles) that flat text approaches cannot capture

## Why This Works (Mechanism)

### Mechanism 1
Organizing disparate academic entities into a unified multi-table schema that maps directly to a heterogeneous graph structure enables relational learning that single-source datasets cannot support. The framework converts raw data into node tables and edge tables, allowing GNNs to propagate signals across distinct entity types like linking paragraph nodes to referenced figure nodes. This works under the assumption that academic tasks depend on structural relationships rather than just local semantic content. Performance degrades if graph connectivity is too sparse or heterogeneous edges are noisy.

### Mechanism 2
The two-step task definition scheme—identifying a target entity and retrieving its multi-hop neighborhood—provides the necessary context window for models to solve tasks requiring cross-modal or cross-document reasoning. Instead of feeding whole documents, the system constructs local academic graphs around targets, forcing models to ground generation in structural dependencies. This assumes relevant context is localized within 1-3 hops around the target. The mechanism fails if neighborhoods introduce noise rather than signal, as seen when expanding to 5 hops in paragraph generation.

### Mechanism 3
Integrating temporal evolution and peer review feedback into graph structures allows models to capture the dynamic process of research refinement that static corpora miss. The system links manuscript versions and connects reviews to revisions, enabling learning of deltas between rejected and accepted versions. This assumes paper quality correlates with evolution in response to community feedback over time. The mechanism breaks when review comments are ambiguous or lack explicit revision instructions, confusing the causal link between review and revision nodes.

## Foundational Learning

- **Concept: Heterogeneous Graphs (HINs)**
  - Why needed here: The core innovation treats academic data as networks of different node types (Papers, Authors, Figures) connected by specific edge types (Cites, WrittenBy)
  - Quick check question: Can you explain why a standard Graph Convolutional Network (GCN) might fail on this data compared to a Heterogeneous Graph Attention Network (HAN) as used in the paper?

- **Concept: Multi-hop Message Passing**
  - Why needed here: Performance gains are attributed to aggregating information from neighbors multiple steps away (e.g., Paper -> Author -> Institution)
  - Quick check question: If a 1-hop neighborhood gives you a paper's direct references, what does a 3-hop neighborhood capture in the context of "Acceptance Prediction"?

- **Concept: Multi-modal Alignment**
  - Why needed here: The system converts figures to text descriptions to feed into text-based models
  - Quick check question: How does converting a visual figure into a textual caption (using LLaVA) allow a Large Language Model to utilize "graph-structured" figure nodes?

## Architecture Onboarding

- **Component map:** PostgreSQL/CSVs (raw tables) -> Graph Engine (constructs heterogeneous graph) -> Task Interface (extracts subgraphs) -> Model Layer (encoders + predictors)
- **Critical path:** The transition from Static Tables to Dynamic Subgraphs, requiring understanding how the openreview_arxiv table bridges review data and paper content to form a single connected component
- **Design tradeoffs:** Hop Depth vs. Noise (1→3 hops improves citation prediction but introduces noise in paragraph generation); Text vs. Graph (GNNs excel at structural tasks, LLMs needed for generative tasks but struggle with long contexts)
- **Failure signatures:** Over-smoothing when too many hops are used; Context Limits when LLMs cannot ingest full review histories; Missing Links when papers aren't on OpenReview, disconnecting the graph
- **First 3 experiments:**
  1. Baseline Validation: Run "Citation Prediction" using text-only Embedding model (Longformer) vs. Graph model (HANConv) to replicate ~30% MCC improvement
  2. Ablation on Modalities: For "Paragraph Generation," remove Figure and Table nodes from input graph and measure drop in SBERT/Rouge-L scores
  3. Neighborhood Sensitivity: In "Acceptance Prediction," sweep hop count from 1 to 5 to find sweet spot where accuracy peaks before noise degrades performance

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains highly dependent on quality of graph structure construction and may not generalize to all academic domains
- Two-step task formulation shows mixed results across tasks with optimal hop depth being task-specific rather than universal
- Effectiveness assumes clean causal relationships between reviews and revisions, which may not exist when reviewer comments are ambiguous

## Confidence
- **High Confidence:** Core architecture of organizing academic data into heterogeneous graphs with multi-table schemas is technically sound and well-supported by experimental results
- **Medium Confidence:** Specific task formulation scheme is innovative but shows mixed results across different tasks with hop depth requiring task-specific tuning
- **Low Confidence:** Claim that framework "enables efficient modeling of highly relational and heterogeneous academic relationships" overstates generality given performance dependency on data connection quality

## Next Checks
1. Apply the framework to a different academic domain (e.g., computer science → biology) to test cross-domain generalization
2. Conduct manual audit of ArXiv-OpenReview linked papers to quantify false positive/negative rate in title matching and assess how linking errors propagate
3. Systematically vary hop count and sampling strategy for each task to create "task-specific optimal hop depth" chart and test transferability across similar tasks