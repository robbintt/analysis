---
ver: rpa2
title: 'FoQA: A Faroese Question-Answering Dataset'
arxiv_id: '2502.07642'
source_url: https://arxiv.org/abs/2502.07642
tags:
- faroese
- dataset
- question
- questions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FoQA, the first Faroese extractive question-answering
  dataset with 2,000 validated samples. The authors develop a semi-automated methodology
  that combines GPT-4-turbo for initial QA generation with human validation by native
  speakers.
---

# FoQA: A Faroese Question-Answering Dataset

## Quick Facts
- arXiv ID: 2502.07642
- Source URL: https://arxiv.org/abs/2502.07642
- Reference count: 7
- First Faroese extractive QA dataset with 2,000 validated samples

## Executive Summary
This paper introduces FoQA, the first Faroese extractive question-answering dataset containing 2,000 validated samples. The dataset was created using a semi-automated methodology combining GPT-4-turbo for initial QA generation with human validation by native Faroese speakers. Questions are rephrased to increase complexity, and answers must appear verbatim in the source text. The authors release the dataset and code open-source to support Faroese language technology development.

## Method Summary
The authors developed a semi-automated methodology for creating the FoQA dataset. GPT-4-turbo was used to generate initial question-answer pairs from Faroese source texts, with questions rephrased to increase complexity. Human native speakers then validated these samples, resulting in a final dataset of 2,000 validated samples. The dataset includes three versions: the validated set, a complete set of 10,001 generated samples, and 2,395 rejected samples for error analysis. The verbatim answer constraint ensures answers appear exactly as written in the source text.

## Key Results
- GPT-4-turbo achieved the highest performance with F1 score of 77.6 and exact match of 55.6
- Encoder models like FoBERT and mDeBERTa-v3 performed significantly worse (F1 scores ~30)
- Dataset includes 2,000 validated samples, 10,001 generated samples, and 2,395 rejected samples

## Why This Works (Mechanism)
The dataset creation methodology combines the efficiency of large language models with human expertise to ensure quality. GPT-4-turbo can rapidly generate diverse question-answer pairs from source text, while native speaker validation ensures linguistic accuracy and relevance for the Faroese language. The rephrasing strategy increases question complexity, creating a more challenging dataset that better evaluates model capabilities. The verbatim answer constraint provides clear ground truth for evaluation, though it may limit dataset applicability to other QA formats.

## Foundational Learning
1. **Extractive QA task definition**: Why needed - Establishes the specific format where answers must be verbatim spans from source text; Quick check - Verify all answers in dataset appear exactly as written in source material.
2. **Semi-automated dataset creation**: Why needed - Balances efficiency of automated generation with quality control of human validation; Quick check - Compare error rates between generated-only vs. validated samples.
3. **Faroese language characteristics**: Why needed - Understanding this low-resource language's unique features informs dataset design; Quick check - Analyze rejected samples for patterns related to Faroese-specific linguistic phenomena.

## Architecture Onboarding

**Component map:** GPT-4-turbo -> Question rephrasing -> Human validation -> Dataset versions (validated/complete/rejected)

**Critical path:** The validation step is critical as it determines which samples enter the final dataset. Human validators review generated QA pairs and either accept them into the validated set or reject them, with rejected samples analyzed for error patterns.

**Design tradeoffs:** The verbatim answer constraint ensures clear evaluation metrics but may limit dataset versatility. Semi-automation speeds creation but introduces potential model bias that validation must address. Three dataset versions provide different use cases but require managing multiple data splits.

**Failure signatures:** High rejection rates indicate generation methodology issues; consistently poor encoder model performance may suggest dataset bias toward generative approaches; linguistic errors in validated samples point to validation process gaps.

**First experiments:** 1) Benchmark additional model architectures beyond LLMs and encoder models, 2) Analyze rejected sample patterns for systematic generation failures, 3) Test performance across question complexity levels within the dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 2,000 validated samples may not fully capture Faroese language complexity
- GPT-4-turbo generation introduces potential bias toward model's understanding of QA relationships
- Verbatim answer constraint may limit applicability to other QA formats and broader NLP tasks

## Confidence
- Performance metrics: High (human-annotated validation sets used)
- Dataset utility for broader NLP tasks: Medium (verbatim constraint may limit versatility)
- Representativeness of Faroese language usage: Medium (dataset size substantial but may not capture full complexity)

## Next Checks
1. Test the dataset with additional model architectures beyond the currently evaluated LLMs and encoder models to establish whether performance patterns are consistent across different model families.

2. Conduct linguistic analysis of the rejected samples (2,395 items) to identify systematic patterns in generation failures and assess whether these reveal limitations in either the generation methodology or the source text corpus.

3. Evaluate model performance on subset variations of the dataset (e.g., questions requiring different levels of reasoning complexity) to determine whether the performance differences between models correlate with question difficulty or type.