---
ver: rpa2
title: 'Natural Language Processing for Electronic Health Records in Scandinavian
  Languages: Norwegian, Swedish, and Danish'
arxiv_id: '2503.18539'
source_url: https://arxiv.org/abs/2503.18539
tags:
- methods
- clinical
- swedish
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This systematic review assessed the state-of-the-art clinical\
  \ natural language processing (NLP) methods for mainland Scandinavian languages\u2014\
  Norwegian, Swedish, and Danish. Among 113 articles, 64% focused on Swedish clinical\
  \ text, 18% on Norwegian, and 10% on Danish."
---

# Natural Language Processing for Electronic Health Records in Scandinavian Languages: Norwegian, Swedish, and Danish

## Quick Facts
- arXiv ID: 2503.18539
- Source URL: https://arxiv.org/abs/2503.18539
- Reference count: 40
- Primary result: Systematic review reveals Swedish clinical NLP research dominates mainland Scandinavian languages, with minimal Norwegian adoption of transformer models despite linguistic similarities.

## Executive Summary
This systematic review assesses the state-of-the-art clinical natural language processing (NLP) methods for mainland Scandinavian languagesâ€”Norwegian, Swedish, and Danish. Among 113 articles analyzed, Swedish clinical text research dominates (64%), while Norwegian research lags significantly with minimal transformer adoption starting only in 2024. The review identifies critical disparities in research activity, resource sharing (only 18.6% share code/models), and clinical validation (5.3% in real-world settings). Despite linguistic similarities among the languages, only 19.5% of studies employed adaptation or transfer learning techniques. Information extraction and classification tasks dominate (67.3%), with named entity recognition as the primary application. The authors emphasize the need for collaborative efforts, improved data accessibility, and greater focus on under-resourced languages like Norwegian and Danish.

## Method Summary
The systematic review followed PRISMA guidelines, identifying 113 articles from PubMed, ScienceDirect, Google Scholar, ACM, and IEEE Xplore databases published between 2010 and 2024. Five independent authors conducted screening using inclusion/exclusion criteria focused on free-text EHR processing in Swedish, Norwegian, or Danish. Studies were categorized into publication insights, data characteristics, NLP techniques, and clinical characteristics. The analysis examined publication trends, task distributions, model adoption rates, and resource availability through quantitative aggregation and qualitative assessment of clinical validation practices.

## Key Results
- Swedish clinical NLP research shows earliest transformer adoption (2021) while Norwegian adoption remains minimal
- Only 19.5% of studies employed adaptation or transfer learning despite linguistic similarities among Scandinavian languages
- Resource availability is severely limited: 18.6% shared code/models, 31.9% made datasets available
- Most studies (67.3%) focus on information extraction and classification, with named entity recognition as dominant task
- Clinical validation in real-world settings is rare (5.3% of studies)

## Why This Works (Mechanism)

### Mechanism 1: Infrastructure-Driven Data Aggregation
Research maturity correlates strongly with centralized data access rather than solely population size or language complexity. The Swedish Health Record Research Bank aggregates scattered clinical notes into sufficient volume for training data-hungry models like Transformers, creating a positive feedback loop for research output. Institutional barriers to data access are the primary bottleneck, not modeling techniques.

### Mechanism 2: Cross-Lingual Transfer Potential
Models trained on high-resource Scandinavian languages (e.g., Swedish) likely serve as efficient base models for low-resource counterparts (e.g., Norwegian/Danish) due to linguistic proximity. Shared vocabulary and grammatical structures allow embeddings from Swedish Clinical BERT to initialize weights for Norwegian models, requiring less labeled data to converge on local clinical jargon.

### Mechanism 3: Task-Specific Feature-Model Coupling
The effectiveness of an NLP architecture is contingent on matching feature representation to interpretability requirements of the clinical task. Classification tasks favor interpretable methods (Logistic Regression/SVM) or statistical features for explainability, whereas Information Extraction benefits from non-linear feature learning of Transformers/LSTMs.

## Foundational Learning

- **De-identification vs. Anonymization**: Clinical text contains Protected Health Information (PHI). The paper notes this is a barrier to sharing datasets. Understanding how to strip identifiers without losing clinical context is prerequisite to building "Research Banks."
  - Quick check: Can you identify the trade-off between de-identification intensity and downstream model performance?

- **Feature Representation (Statistical vs. Learned)**: The paper documents a shift from statistical features (TF-IDF, Bag-of-Words) dominant in 2010-2014 to learned embeddings (Word2Vec, BERT) in 2020-2024. Understanding when to use sparse vs. dense vectors determines if you can train on limited Norwegian data or need massive Swedish corpora.
  - Quick check: Which feature representation technique is exclusively used by Transformer models according to Table 7?

- **Clinical Validation Gaps**: The paper highlights a critical gap: only 5.3% of studies clinically validated models. Learning how to move from "lab performance" to "workflow integration" is the missing step.
  - Quick check: Why might high accuracy on a test set fail in a "real-world clinical setting"?

## Architecture Onboarding

- **Component map**: Data Ingestion (EHR Documents) -> Preprocessing (De-identification, Abbreviation Expansion) -> Representation (Statistical Features vs. Contextual Embeddings) -> Model Layer (Rule-based -> Statistical -> Deep Learning) -> Task Layer (Information Extraction/Classification)

- **Critical path**: Resource Sharing. The architecture breaks at "Data Access" node (31.9% availability). The system relies on Synthetic Data generation or Transfer Learning to bypass this.

- **Design tradeoffs**:
  - Swedish vs. Norwegian Stack: For Norwegian, you likely cannot train a Transformer from scratch. You must trade architectural control for the efficiency of adapting a Swedish model, despite low current adoption rates.
  - Accuracy vs. Interpretability: For Classification tasks, you must often trade raw accuracy of Transformers for interpretability of SVMs/Decision Trees to satisfy clinical requirements.

- **Failure signatures**:
  - Hallucination in Generative Models: GPT models might generate "superficially probable but incorrect answers"
  - Overfitting on Sparse Data: Deploying high-capacity models on low-resource languages without transfer learning will likely fail to generalize

- **First 3 experiments**:
  1. Cross-Lingual Zero-Shot Test: Run Swedish Clinical BERT on Norwegian clinical notes to quantify transfer performance drop
  2. Synthetic vs. Real Validation: Train NER model on synthetic Norwegian data and validate against human-annotated set
  3. De-identification Stress Test: Measure utility loss vs. privacy gain from rule-based de-identification

## Open Questions the Paper Calls Out

### Open Question 1
Can data augmentation, adaptation, and transfer learning effectively leverage linguistic similarities between Swedish, Norwegian, and Danish to overcome low-resource constraints? Research currently treats these languages in isolation rather than exploiting shared linguistic roots, with only 19.5% adaptation rates and 18.6% resource sharing.

### Open Question 2
How can clinical NLP models for Scandinavian languages be successfully transitioned from research to real-world clinical settings? Only 5.3% of studies achieved clinical validation due to regulatory requirements, privacy concerns, and integration difficulties with existing hospital workflows.

### Open Question 3
What specific infrastructure and resource investments are required to accelerate transformer adoption for Norwegian clinical text to match Swedish progress? The disparity is attributed to lack of shared resources and inadequate research infrastructure in Norway compared to Sweden's Health Record Research Bank.

## Limitations
- Database coverage may bias toward English-language publications, potentially missing relevant Scandinavian-language research
- Subjective categorization of borderline methods during screening could skew model adoption statistics
- Qualitative interpretation of clinical validation status (5.3%) lacks standardized clinical outcome measures

## Confidence
- **High**: Claims about publication trends and task distributions based on direct quantitative analysis of 113 articles
- **Medium**: Claims about infrastructure effects and transfer learning potential based on logical inference from observed patterns
- **Low**: Clinical validation findings rely on qualitative author interpretation rather than standardized clinical outcome measures

## Next Checks
1. Implement zero-shot inference testing of Swedish clinical NLP models on Norwegian/Danish datasets to quantify practical transfer potential
2. Survey Scandinavian healthcare institutions to measure actual barriers to data sharing beyond publication records, particularly regarding de-identification utility loss
3. Analyze the specific 5.3% of studies that achieved clinical validation to extract common factors enabling real-world deployment and test generalizability to under-resourced languages