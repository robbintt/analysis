---
ver: rpa2
title: Non-convergence to the optimal risk for Adam and stochastic gradient descent
  optimization in the training of deep neural networks
arxiv_id: '2503.01660'
source_url: https://arxiv.org/abs/2503.01660
tags:
- satisfy
- holds
- corollary
- every
- assume
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves that stochastic gradient descent (SGD) and Adam
  methods do not converge to the optimal true risk in training deep neural networks.
  The authors show that for a wide range of activation functions, loss functions,
  and network architectures, there is a strictly positive probability that the true
  risk of the optimization process does not converge to the optimal value.
---

# Non-convergence to the optimal risk for Adam and stochastic gradient descent optimization in the training of deep neural networks

## Quick Facts
- arXiv ID: 2503.01660
- Source URL: https://arxiv.org/abs/2503.01660
- Reference count: 40
- This work proves that stochastic gradient descent (SGD) and Adam methods do not converge to the optimal true risk in training deep neural networks.

## Executive Summary
This paper establishes a fundamental limitation of SGD and Adam optimization in deep neural networks: these methods do not converge to the optimal true risk with strictly positive probability. The authors prove that for a wide range of activation functions, loss functions, and network architectures, inactive neurons can freeze their associated weights, preventing the network from reaching the optimal risk value. The probability of non-convergence increases with network depth, approaching certainty for very deep networks. This theoretical result applies to various popular optimizers including standard SGD, momentum SGD, Nesterov accelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and AMSGrad.

## Method Summary
The paper proves theoretical results about non-convergence using a combination of abstract lower bounds on non-convergence probability and concrete conditions ensuring these bounds apply. The analysis is performed on fully-connected feedforward deep neural networks with ReLU, clipping, or RePU activation functions. The authors establish that when neurons enter a "inactive" state (output remains constant), their associated weights remain frozen throughout training, restricting the network to a sub-space of functions that cannot minimize the true risk optimally. The proof relies on generalized gradients and assumes standard random initialization schemes.

## Key Results
- Stochastic gradient descent and Adam do not converge to the optimal true risk in training deep neural networks
- The probability of non-convergence is strictly positive for a wide range of activation functions and network architectures
- For very deep networks, the probability of non-convergence approaches 1 (certainty)
- Inactive neurons (which output constant values) prevent convergence to the optimal risk

## Why This Works (Mechanism)

### Mechanism 1: Inactive Neuron Invariance
The paper establishes that neurons can enter a permanent "inactive" state where their output remains constant, freezing their associated weights. If a neuron's pre-activation output falls within a region where the activation function's derivative is zero (e.g., ReLU's negative regime), the gradient of the loss with respect to that neuron's input weights becomes zero. Since the SGD update step depends on this gradient, the weights feeding into this neuron remain unchanged for all future time steps. The core assumption is that the activation function has intervals where it is constant or has zero derivative, and the learning update is driven by the gradient.

### Mechanism 2: Constant Realization Sub-optimality
The presence of inactive neurons restricts the network's realization function to a sub-space of functions that cannot minimize the true risk optimally. If a neuron is inactive over the entire input domain, it outputs a constant value. The paper argues that for problems where the target function is non-constant, a network realization restricted to constant functions cannot reach the infimum of the true risk. The optimizer thus converges to a strictly suboptimal value. The core assumption is that the conditional expectation of the target data given the input is non-constant.

### Mechanism 3: Depth-Aggravated Probability
As network depth increases, the probability of encountering non-convergence to the optimal risk approaches 1 (certainty). Deeper networks provide more opportunities for weights to initialize in configurations that cause neuron inactivity in hidden layers. The paper proves a lower bound on the probability of non-convergence that increases with network depth. The core assumption is that random initialization schemes are used, and the network is fully-connected and feedforward.

## Foundational Learning

- **Concept: True Risk vs. Empirical Risk**
  - Why needed here: The paper proves non-convergence to the *true* risk (expected error over the data distribution), not just the *empirical* risk (error on the training batch). Distinguishing these is vital to understanding why "training loss going down" might still result in a suboptimal model theoretically.
  - Quick check question: If a model minimizes empirical risk perfectly, does it guarantee the true risk is optimal? (Answer: No, often due to overfitting or, as here, capacity restrictions).

- **Concept: ReLU and "Dead" Neurons**
  - Why needed here: The mechanism depends entirely on activation functions like ReLU having a "dead zone" ($x<0$) where the gradient is zero. Without understanding this saturation point, the concept of "invariant inactive neurons" is opaque.
  - Quick check question: For a ReLU neuron $y = \max(0, wx+b)$, if the bias $b$ is initialized to a large negative value and $wx$ is always small, what is the gradient during backprop? (Answer: Zero).

- **Concept: Convergence in Probability**
  - Why needed here: The paper uses specific mathematical definitions of convergence. It proves that the risk *does not* converge in probability to the infimum. This is a stronger statement than just "the loss fluctuates."
  - Quick check question: If a sequence of random variables $X_n$ converges in probability to $L$, what happens to $P(|X_n - L| > \epsilon)$ as $n \to \infty$? (Answer: It goes to 0. This paper says this does *not* happen).

## Architecture Onboarding

- **Component map:** Input -> Fully-connected layers -> Output
- **Critical path:**
  1. **Initialization:** Weights $\Theta_0$ are sampled (e.g., Standard Normal)
  2. **Forward Pass:** Compute neuron outputs $N_{\ell, \theta, A}$
  3. **Inactivity Check:** Identify neurons where output is in the "flat" region of activation (e.g., negative pre-activation for ReLU)
  4. **Gradient Computation:** Compute $G_n$ (generalized gradient). Note that $G_n = 0$ for parameters feeding inactive neurons
  5. **Update:** Weights update; inactive weights remain frozen

- **Design tradeoffs:**
  - **Depth vs. Trainability:** While depth adds representational power, this paper proves it exponentially increases the probability of getting stuck in a suboptimal basin due to inactive neurons
  - **Adam vs. SGD:** The paper treats both similarly regarding this failure mode; adaptive learning rates do not solve the "zero gradient" issue of inactive neurons

- **Failure signatures:**
  - **Neuron Constancy:** Identifying neurons that output the exact same value for every input in a batch
  - **Non-decreasing Risk Floor:** The true risk (or validation loss) hits a floor strictly above zero and stops improving, despite continued training

- **First 3 experiments:**
  1. **Monitor Dead Neurons:** Instrument a standard ResNet training loop to log the percentage of ReLU units outputting 0 across the full training set. Verify if this correlates with the "risk floor."
  2. **Depth Scaling:** Train shallow (e.g., 2-layer) vs. deep (e.g., 10-layer) MLPs on a synthetic dataset with a non-constant target. Plot the probability of reaching near-zero error over 100 random seeds to observe the depth-probability correlation.
  3. **Activation Replacement:** Rerun the failure cases using Leaky ReLU or Softplus (which have non-zero gradients) to test if the "non-convergence" theory holds when the "flat region" assumption is removed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the precise lower bounds for the probability of non-convergence to the optimal risk for networks of intermediate depth?
- **Basis in paper:** Section 1.2 states that Theorem 1.2 "does not provide us insights how large this probability actually is" for general cases, contrasting with the high probability result for very deep networks in Theorem 1.3.
- **Why unresolved:** The theoretical analysis distinguishes only between strictly positive probability and probability approaching one as depth increases.
- **What evidence would resolve it:** Analytical derivation of tighter probability bounds or extensive empirical simulation across varying depths.

### Open Question 2
- **Question:** Does the non-convergence result persist in architectures utilizing residual connections (ResNets)?
- **Basis in paper:** The failure mechanism relies on "inactive neurons" preventing gradient flow to earlier layers. ResNets use skip connections that preserve gradient flow, potentially circumventing the obstruction.
- **Why unresolved:** The mathematical framework and main results are restricted to fully-connected feedforward networks.
- **What evidence would resolve it:** An extension of the invariance proof applied to the skip-connection topology.

### Open Question 3
- **Question:** Can initialization strategies that strictly avoid the "inactive" region guarantee convergence to the global minimum?
- **Basis in paper:** The analysis hinges on the existence of inactive neurons at initialization which remain inactive indefinitely.
- **Why unresolved:** The paper assumes standard random initializations where there is a non-zero probability of falling into the inactive region.
- **What evidence would resolve it:** A convergence proof under initialization distributions with support strictly outside the inactive set.

## Limitations
- The theoretical results are limited to fully-connected feedforward networks and do not extend to convolutional networks or residual connections
- The probability bounds are established through abstract lower bounds that may not be tight in practical scenarios
- The analysis assumes standard random initialization schemes and may not hold for carefully designed initialization strategies

## Confidence

**Confidence Labels:**
- High confidence: The mechanism of inactive neurons preventing convergence to optimal risk under ReLU activation
- Medium confidence: The theoretical probability bounds for non-convergence in practical training scenarios
- Medium confidence: The claim that depth increases non-convergence probability, pending empirical validation

## Next Checks
1. **Empirical validation**: Train networks with varying depths (2-20 layers) on synthetic non-constant target functions with multiple random seeds to empirically measure the probability of hitting risk floors.

2. **Activation function sensitivity**: Compare ReLU with Leaky ReLU and Softplus on identical architectures to test if the non-convergence theory holds when flat regions are eliminated.

3. **Gradient analysis**: Instrument training runs to track the percentage of neurons that become permanently inactive and correlate this with the observed risk floors to validate the core mechanism.