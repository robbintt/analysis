---
ver: rpa2
title: 'BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian
  Context'
arxiv_id: '2508.07090'
source_url: https://arxiv.org/abs/2508.07090
tags:
- bias
- context
- across
- english
- indian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BharatBBQ is a multilingual bias benchmark for question answering
  in the Indian context, addressing the gap in bias evaluation for non-Western societies.
  The benchmark covers 13 social categories, including caste, region, and intersectional
  groups, with 49,108 examples in English and 392,864 examples across eight Indian
  languages.
---

# BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context

## Quick Facts
- arXiv ID: 2508.07090
- Source URL: https://arxiv.org/abs/2508.07090
- Authors: Aditya Tomar; Nihar Ranjan Sahoo; Pushpak Bhattacharyya
- Reference count: 30
- Primary result: Models exhibit higher social bias in Indian languages compared to English, with Stereotypical Bias Score (SBS) more effective than traditional metrics

## Executive Summary
BharatBBQ addresses the gap in bias evaluation for non-Western societies by introducing a multilingual benchmark for question answering in the Indian context. The benchmark covers 13 social categories including caste, region, and intersectional groups across eight Indian languages. Evaluation of five multilingual language models revealed persistent biases, particularly in disability status, religion, and sexual orientation. Notably, models exhibited higher bias in Indian languages compared to English, emphasizing the need for culturally grounded benchmarks. The introduction of a stereotypical bias score metric proved more effective in capturing model biases than traditional bias metrics.

## Method Summary
BharatBBQ is a multilingual bias benchmark for question answering in the Indian context, covering 13 social categories with 49,108 examples in English and 392,864 examples across eight Indian languages. The benchmark evaluates 5 multilingual language models using log-likelihood scoring across zero-shot and few-shot settings. Models are assessed on ambiguous and disambiguated contexts with negative and non-negative questions, computing Accuracy, Bias Score, and Stereotypical Bias Score metrics.

## Key Results
- Models exhibit higher bias in Indian languages compared to English, particularly for disability status, religion, and sexual orientation
- Stereotypical Bias Score (SBS) metric proved more effective than traditional Bias Score in capturing model biases
- Sarvam-2B model showed near-zero bias on Western benchmark BBQ but exhibited significant bias on BharatBBQ, demonstrating the importance of cultural context

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multilingual models likely rely more on stereotypical heuristics in Indian languages than in English due to resource disparities.
- **Mechanism:** The paper hypothesizes that in disambiguated contexts, models often fail to utilize explicit contextual cues in Indian languages because of limited pretraining data and weaker syntactic/semantic alignment. This forces the model to fall back on learned statistical associations (stereotypes) to resolve ambiguity, resulting in higher Stereotypical Bias Scores (SBS) in non-English languages.
- **Core assumption:** The observed bias gap is caused by the quality/quantity of training data per language, not merely by the translation quality of the benchmark itself.
- **Evidence anchors:**
  - [section 6.7] "For Indian languages, limited pretraining data and weaker contextual understanding reduce accuracy and lead models to rely more on stereotypes, increasing SBS for disambiguated contexts."
  - [section 6.7] "Larger LLMs consistently exhibit lower BS and SBS in English... [but] BS for Indian languages exceeds that for English."
  - [corpus] The related paper *EsBBQ and CaBBQ* similarly adapts bias benchmarks for Spanish/Catalan, supporting the hypothesis that bias evaluation requires localized adaptation rather than direct translation.

### Mechanism 2
- **Claim:** The Stereotypical Bias Score (SBS) provides a more accurate signal of model bias than traditional Bias Score (BS) by filtering out lexical preferences.
- **Mechanism:** Standard metrics may conflate a "lexical bias" (a model preferring a specific word, e.g., "Man," regardless of context) with a "stereotypical bias" (associating "Man" with a negative trait). The SBS metric isolates this by specifically counting instances where the model selects the *stereotyped* group for negative questions *and* the *non-stereotyped* group for non-negative questions in ambiguous settings.
- **Core assumption:** A model answering a negative question with Group A and a non-negative question with Group B is exhibiting a coherent worldview (stereotype) rather than a random or strictly lexical error.
- **Evidence anchors:**
  - [section 4] "Always choosing the stereotype option, irrespective of the question type, reflects a general lexical bias... Through SBS, we separately analyze the negative and the non-negative questions."
  - [section 6.1] "Llama's bias score (BS) for English in zero-shot... is low, but its stereotypical bias score (SBS) is significantly higher... reinforcing the need for our SBS metric."
  - [abstract] "The introduction of a stereotypical bias score metric proved more effective in capturing model biases than traditional bias metrics."

### Mechanism 3
- **Claim:** Cultural adaptation of templates surfaces biases that are invisible to Western-centric benchmarks.
- **Mechanism:** Direct translation of Western datasets (like BBQ) misses culturally unique bias axes (e.g., Caste in India) and includes irrelevant ones (e.g., antisemitism). By replacing entities and creating new templates for specific Indian sociocultural dynamics (e.g., "Region×Gender"), the benchmark activates latent biases in the model regarding groups like "Northeastern women" or "Baniya caste" that would otherwise go unmeasured.
- **Core assumption:** Models trained on web data have internalized region-specific stereotypes (e.g., North vs. South India dynamics) that differ from Western racial dynamics.
- **Evidence anchors:**
  - [section 3.1] "Stereotypes related to Jewish communities... were removed... Conversely, Pakistan is often a target of stereotypes related to terrorism in India but was absent in BBQ."
  - [section 6.5] "Sarvam shows near-zero bias scores on BBQ... However, it exhibits bias on BharatBBQ, suggesting that the localized social contexts... expose biased behaviors."
  - [corpus] *IndRegBias* is cited as related work focusing on Indian regional biases, reinforcing the importance of specific regional axes.

## Foundational Learning

- **Concept: Intersectionality in Bias Evaluation**
  - **Why needed here:** The benchmark introduces intersectional categories (e.g., Religion×Gender, Age×Gender) which are critical because bias is often not additive but multiplicative (e.g., "Muslim women" face different stereotypes than just "Muslims" or "women").
  - **Quick check question:** Can you explain why evaluating "Gender" and "Religion" separately might fail to detect a stereotype specifically targeting "Muslim women"?

- **Concept: Ambiguous vs. Disambiguated Contexts**
  - **Why needed here:** The core logic of the BBQ framework relies on distinguishing what a model does when it *doesn't* know the answer (Ambiguous—should say "Unknown") vs. when it *does* know (Disambiguated—should pick the factually correct group).
  - **Quick check question:** If a model answers an Ambiguous context correctly with "Unknown" but answers a Disambiguated context incorrectly, does it suffer from a reasoning failure or a social bias failure? (Trick: It could be both, but the latter confirms bias when the wrong answer aligns with a stereotype).

- **Concept: Log-Likelihood Scoring for Generative Models**
  - **Why needed here:** The paper evaluates models by comparing the log-likelihood of generating the three answer options rather than just taking the generated text output. This is essential for deterministic comparison.
  - **Quick check question:** Why is comparing the probability the model assigns to the answer string "The man" vs. "The woman" more robust than just prompting the model to "Generate an answer"?

## Architecture Onboarding

- **Component map:** Template Engine -> Translation Pipeline -> Evaluation Harness -> Metric Calculator
- **Critical path:** Curation (validating stereotypes) -> Generation (converting concepts to templates) -> Evaluation (feeding to model) -> Interpretation (using SBS to distinguish bias types)
- **Design tradeoffs:**
  - **Translation vs. Native Creation:** The authors generate in English first and translate, admitting this might miss "idioms... and sociocultural jargon" unique to the native tongue, but accept this trade-off for scale (392K examples).
  - **Metric Sensitivity:** They trade off the simplicity of "Accuracy" for the complexity of "SBS" to gain specificity in distinguishing bias types.
- **Failure signatures:**
  - **High Ambiguous Accuracy + High SBS:** The model is safe/refuses to answer when uncertain, but when it *does* answer (or in disambiguated cases), it is highly biased.
  - **Negative Bias Score:** Indicates a "reverse bias" or over-correction (preference for the marginalized group in scenarios where it doesn't fit), as seen in the "Religion" category for some models (Section 6.5).
  - **Low Cosine Similarity in Translation:** Indicates the translation pipeline failed to preserve the semantic nuance required for the bias probe.
- **First 3 experiments:**
  1. **Sanity Check (Metric Validation):** Evaluate a model known to be biased (e.g., an older baseline) on BharatBBQ and verify that SBS > BS in ambiguous contexts to confirm the metric is capturing the intended signal.
  2. **Proper vs. Common Noun Ablation:** Run the evaluation on the "Religion" and "Caste" subsets using only Proper Nouns (e.g., "Sharma") vs. only Common Nouns (e.g., "Brahmin") to quantify how much bias is triggered by names vs. explicit group labels.
  3. **Cross-Lingual Transfer:** Compare the bias scores of a model (e.g., Gemma) on the English subset vs. its native Hindi subset to quantify the "English vs. Indian Language" bias gap described in Section 6.7.

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific linguistic, data-driven, or architectural factors contribute to the amplification of social biases in Indian languages compared to English?
  - **Basis in paper:** [explicit] The Limitations section states, "we do not investigate the underlying causes of this behavior," referring to the finding that models exhibit higher bias in Indian languages.
  - **Why unresolved:** The paper identifies the phenomenon of bias amplification in non-English contexts but stops short of determining if the root cause is insufficient training data, tokenization issues, or cross-lingual alignment errors.
  - **What evidence would resolve it:** Ablation studies controlling for dataset size and quality across languages, or mechanistic interpretability analyses comparing how bias-associated concepts are represented in English vs. Indic language embedding spaces.

- **Open Question 2:** What mitigation strategies are effective for reducing the biases identified in BharatBBQ without compromising the linguistic capabilities of the models?
  - **Basis in paper:** [explicit] The Limitations section notes that BharatBBQ "focuses on evaluating biases but does not propose direct mitigation strategies, which we leave as an avenue for future research."
  - **Why unresolved:** While the benchmark reveals persistent biases in areas like caste and region, it is unknown if standard debiasing techniques (e.g., RLHF, data augmentation) work for these culturally specific, non-Western categories.
  - **What evidence would resolve it:** Experiments applying specific debiasing interventions to models evaluated on BharatBBQ, resulting in a significant drop in Stereotypical Bias Scores (SBS) while maintaining accuracy on general NLP benchmarks.

- **Open Question 3:** How does the inclusion of code-mixed text (e.g., Hinglish) affect the evaluation of social biases in multilingual models?
  - **Basis in paper:** [explicit] The Conclusion & Future Works section states it would be valuable to extend the dataset to include "code-mixed texts, to improve its representational depth."
  - **Why unresolved:** The current benchmark is restricted to standardized monolingual formats, which may not reflect the reality of Indian language usage where code-switching is common; bias triggers might differ in mixed-language contexts.
  - **What evidence would resolve it:** A new evaluation using code-mixed prompts derived from BharatBBQ, comparing the resulting bias scores against those of the current pure-language baselines.

## Limitations
- Translation methodology may miss culturally specific idioms and nuanced sociocultural jargon present in Indian languages
- Benchmark covers only five model families, limiting generalizability across the broader landscape of multilingual models
- SBS metric requires careful interpretation as it may conflate genuine stereotyping with systematic calibration issues in model probability estimates

## Confidence
- **High Confidence**: The existence of higher bias scores in Indian languages compared to English (Section 6.7), and the general effectiveness of the SBS metric over BS in capturing stereotypical behavior (Section 6.1).
- **Medium Confidence**: The mechanism attributing language-specific bias gaps to pretraining data disparities (Mechanism 1).
- **Medium Confidence**: The claim that cultural adaptation of templates surfaces unique biases invisible to Western benchmarks (Mechanism 3).

## Next Checks
1. **Translation Quality Validation**: Conduct human evaluation of 100 randomly selected translated examples across all eight Indian languages to quantify semantic drift and cultural nuance loss. Compare bias scores using gold-standard native examples versus translated versions to measure the impact of translation artifacts on reported bias gaps.

2. **Pretraining Data Analysis**: For each model and language pair, analyze the overlap between pretraining data and BharatBBQ examples. Quantify the proportion of examples containing n-grams or entities present in the model's training corpus to validate whether bias differences correlate with data availability rather than inherent model properties.

3. **Calibration Across Languages**: Evaluate model calibration by comparing expected calibration error (ECE) across languages using BharatBBQ's ambiguous contexts. If models systematically over-or under-estimate probabilities in Indian languages, this could explain bias score differences independently of stereotyping behavior, validating or challenging the SBS metric's assumptions.