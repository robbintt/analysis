---
ver: rpa2
title: 'Modular Arithmetic: Language Models Solve Math Digit by Digit'
arxiv_id: '2508.02513'
source_url: https://arxiv.org/abs/2508.02513
tags:
- circuit
- digit
- tens
- unit
- circuits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) perform
  simple arithmetic, focusing on whether they process numbers digit-by-digit or as
  whole units. The authors hypothesize that LLMs employ digit-position-specific arithmetic
  circuits composed of modular subgroups of MLP neurons, each independently responsible
  for generating digits in units, tens, and hundreds positions.
---

# Modular Arithmetic: Language Models Solve Math Digit by Digit

## Quick Facts
- **arXiv ID**: 2508.02513
- **Source URL**: https://arxiv.org/abs/2508.02513
- **Reference count**: 25
- **Primary result**: Large language models solve arithmetic through modular, digit-position-specific MLP circuits rather than as whole units.

## Executive Summary
This paper investigates how large language models perform simple arithmetic, focusing on whether they process numbers digit-by-digit or as whole units. The authors hypothesize that LLMs employ digit-position-specific arithmetic circuits composed of modular subgroups of MLP neurons, each independently responsible for generating digits in units, tens, and hundreds positions. Using Fisher Score-based feature selection, they identify neuron groups sensitive to each digit position across layers, finding these circuits are wide (comprising 60-90% of available neurons per layer) and highly sufficient for representing digit-specific tasks. Causal interventions confirm these circuits are causally involved: targeted manipulation of unit, tens, or hundreds position circuits selectively changes only the corresponding digit in the model's output while leaving others unchanged, with significant probability shifts (e.g., 45.56 percentage points for hundreds digit intervention). The circuits are largely distinct across addition and subtraction, and remain effective even when introducing carries. These findings provide evidence that LLMs solve arithmetic through structured, compositional digit-wise processing rather than mere heuristics, revealing a modular and interpretable internal architecture.

## Method Summary
The authors employ Fisher Score-based feature selection to identify MLP neurons sensitive to specific digit positions (units, tens, hundreds) during arithmetic operations. They compute Fisher Scores for each neuron with respect to digit-pair labels from arithmetic prompts, then threshold these scores to define digit-position-specific circuits. Interchange interventions are performed using the pyvene library, where activations from identified circuits in source prompts are swapped into base prompts to test causal effects. The methodology focuses on mid-to-late MLP layers in Llama 3 8B, using datasets of 1000 addition and 1000 subtraction samples with 3-digit operands, plus intervention datasets with 200 paired base/source prompts each.

## Key Results
- Digit-position-specific MLP circuits exist and are causally involved in arithmetic generation, with 60-90% of neurons per layer participating in these circuits.
- Targeted interventions on these circuits selectively alter only the corresponding digit position in outputs (e.g., units circuit changes only units digit with +30.93% probability).
- Circuits are largely distinct across addition and subtraction operations, showing limited overlap.
- The identified circuits remain effective even when introducing carry operations, though carry propagation appears handled by separate mechanisms.

## Why This Works (Mechanism)

### Mechanism 1: Digit-Position-Specific MLP Circuits for Parallel Arithmetic
LLMs perform arithmetic using distinct, modular subgroups of MLP neurons that operate independently on each digit position (units, tens, hundreds). A Fisher Score-based feature selection method identifies neurons whose activations are sensitive to specific digit subtasks (e.g., all inputs where the tens digits sum to 4). These neurons form digit-position-specific circuits that generate the corresponding digit in the output. Intervening on one circuit selectively alters only that digit in the result.

Core assumption: The identified Fisher Score sensitivity translates to a causal role in generating the output digit.

Evidence anchors:
- [abstract]: "...modular subgroups of MLP neurons that operate independently on different digit positions..."
- [Page 2, Section 1 Introduction]: "...altering the activation of the units-digit circuit for instance selectively changes only the predicted units digit while leaving tens and hundreds unchanged..."
- [Page 5, Table 2]: Shows targeted probability increases for the intended digit variant (e.g., +30.93% for 'bbs' on unit circuit intervention) with minimal effect on non-target digits.
- [corpus]: Corpus papers discuss symbolic/pattern-matching in LLM arithmetic but do not directly confirm this modular circuit structure.

Break condition: If interventions on a circuit for one digit position consistently affect multiple digit positions or have no effect, the claim of independent, modular circuits is invalidated.

### Mechanism 2: Operand Injection via Attention for Context Building
Before arithmetic computation begins, operand information is routed to the final token position by specific attention layers. Attention heads move information about the input operands (e.g., "347" and "231") to the residual stream at the final token (the "=" position). This creates the necessary context for the MLP layers to perform their computation.

Core assumption: The attention layers identified by intervention are responsible for making operand information available, not just correlating with it.

Evidence anchors:
- [Page 13, Appendix C]: "...intervention on specific attention modules dramatically increases the probability of sss in all models. This suggests that these modules are responsible for injecting operand information..."
- [corpus]: Corpus papers do not provide specific evidence for this attention-based operand injection mechanism.

Break condition: If MLP interventions in early layers (before proposed operand injection) are sufficient to alter results, this mechanism is challenged.

### Mechanism 3: Separate Handling of Carry Propagation
The digit-position-specific circuits do not manage carry propagation; a separate mechanism handles it. Interventions that introduce a carry bit (e.g., from units to tens) do not propagate the carry as expected. This suggests the digit circuit only computes its local subtask, and carry information is processed by a distinct, unexplored part of the network.

Core assumption: The model's failure to propagate the carry under intervention is because the carry logic is separate, not because the circuit is fundamentally disrupted.

Evidence anchors:
- [Page 6, Section 4.2]: "...carry information is likely determined and processed by separate mechanisms, rather than being embedded in the digit-position circuits."
- [corpus]: Corpus papers do not directly address carry propagation mechanisms.

Break condition: If future work identifies neurons within the digit circuits that explicitly encode carry information, this claim would be falsified.

## Foundational Learning

- **Concept: MLP (Multi-Layer Perceptron) Layer**
  - **Why needed here**: The identified arithmetic circuits are subgroups of neurons specifically within MLP layers. Understanding that MLPs are the primary computational engines (as opposed to attention) is critical.
  - **Quick check question**: What component in a transformer layer is most responsible for processing and transforming information at a token position, as opposed to moving information between tokens?

- **Concept: Mechanistic Interpretability & Causal Intervention**
  - **Why needed here**: The paper's conclusions rely on interpreting model components not through correlation, but through causal "interchange interventions" where activations are swapped to test their effect.
  - **Quick check question**: What is the difference between observing that a neuron activates during a task and proving it causally contributes to the task's output?

- **Concept: Residual Stream**
  - **Why needed here**: The paper describes how operand information is "injected" into the "residual stream" at the final token. This is the central communication channel in a transformer, to and from which all layers read and write.
  - **Quick check question**: In a transformer, where is the accumulated information from all previous layers stored and passed to the next layer?

## Architecture Onboarding

- **Component map**: Input prompts -> Early attention layers (operand injection) -> Mid-to-late MLP layers (digit-position circuits) -> Unexplored carry mechanism -> Output generation

- **Critical path**:
  1. Prompt is tokenized.
  2. Early attention heads copy operand information to the final token position (operand injection).
  3. Mid-to-late MLP layers process this context.
  4. Within these MLP layers, specific subgroups of neurons (the digit circuits) independently compute the value for each digit position.
  5. A separate mechanism adjusts for carry bits.
  6. The results are written to the residual stream and decoded into the final output token.

- **Design tradeoffs**:
  - Tokenization: The mechanism works in both multi-digit and single-digit tokenization schemes, though the task is slightly different (generating a full number vs. generating the next digit).
  - Circuit Identification: The Fisher Score method is simpler and faster than gradient-based methods but may select less precise circuits. The threshold for circuit membership is a hyperparameter.

- **Failure signatures**:
  - Lack of Modularity: Interventions on the "units" circuit consistently change the "tens" digit.
  - Carry Propagation: An intervention on the units circuit that introduces a carry results in the tens digit being incremented, suggesting carry logic is embedded in the digit circuit, not separate.
  - No Operand Injection: Interventions on early attention layers show no effect on the final result.

- **First 3 experiments**:
  1. **Replicate Fisher Score Analysis**: Using the provided dataset of arithmetic prompts, compute the Fisher Score for each MLP neuron with respect to digit position classes. Plot the distribution of top-scoring neurons across layers.
  2. **Run Basic Interchange Intervention**: Implement the interchange intervention code (using `pyvene` or similar). Swap the activations of the top-scoring neurons for the "units" position from a source prompt (e.g., "261 + 512") into a base prompt (e.g., "347 + 231"). Verify that the output changes only in the units digit.
  3. **Test Carry Propagation**: Create source prompts designed to induce a carry (e.g., 9+5). Intervene on the units circuit of a base prompt with no carry. Observe whether the output shows the carry propagated to the tens digit or only the local units digit change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the modular, digit-position-specific circuit architecture extend to non-linear arithmetic operations like multiplication?
- Basis in paper: [explicit] The authors explicitly list this as a limitation: "More complex operations, such as multiplication and division, are not addressed in this work. Extending our framework to these tasks is an important direction for future work."
- Why unresolved: Addition and subtraction operate largely column-wise (with occasional carries), whereas multiplication involves complex cross-digit dependencies where every digit of one operand interacts with every digit of the other. It is unclear if a strictly modular circuit structure can handle this non-linearity.
- What evidence would resolve it: Successful identification and causal validation of position-specific MLP circuits in LLMs performing n-digit multiplication and division tasks.

### Open Question 2
- Question: What are the specific circuits responsible for composing independently computed digit results into the final output token?
- Basis in paper: [explicit] The authors state: "We also leave the analysis of circuits responsible for composing digit-level results into final outputs to future work."
- Why unresolved: While the paper identifies distinct circuits that compute the value for the units, tens, and hundreds positions separately, it does not explain the mechanism that aggregates these parallel computations into a single coherent output (e.g., generating the token "578" rather than isolated digits).
- What evidence would resolve it: Identification of a "composer" circuit or mechanism in later layers that takes the activations from the distinct digit circuits and integrates them to predict the final vocabulary token.

### Open Question 3
- Question: Are "heuristic" neurons (e.g., parity detectors) functionally embedded components of the digit-position circuits?
- Basis in paper: [inferred] The authors perform a "small-scale exploratory analysis" in Section 5 suggesting heuristics may be "fragments of digit-wise circuits," but acknowledge these are "preliminary results" requiring further verification.
- Why unresolved: The link between interpretable heuristic neurons (e.g., "result % 5 == 0") and the broader modular circuits is currently qualitative and visual, based on inspecting heatmaps of a small subset of neurons (top 20 per circuit).
- What evidence would resolve it: A large-scale quantitative overlap analysis between neurons identified via heuristic-based methods and those identified via the Fisher Score digit-position method.

## Limitations
- The carry propagation mechanism remains unexplored, with the paper only noting that carries appear handled by separate mechanisms based on intervention failure.
- The analysis is limited to Llama 3 8B and specific tokenization schemes, potentially constraining generalizability to other model architectures.
- The claim about separate carry handling is based on negative evidence (failure to propagate carries under intervention) rather than positive identification of carry-handling neurons.

## Confidence
- **High confidence**: Digit-position-specific circuits exist and are sufficient for generating corresponding digits in the output.
- **Medium confidence**: Circuits are largely distinct across addition and subtraction.
- **Low confidence**: Carry propagation is handled by entirely separate mechanisms.

## Next Checks
1. **Cross-model validation**: Apply the Fisher Score and intervention methodology to different model architectures (e.g., Mistral, Gemma) and sizes to test whether modular digit circuits are a general LLM phenomenon or specific to Llama 3 8B.

2. **Carry circuit identification**: Systematically search for and characterize neurons that explicitly encode carry information. This could involve analyzing activations during prompts that induce carries, or testing whether targeted interventions on hypothesized carry neurons enable proper carry propagation.

3. **Temporal computation analysis**: Use activation patching or causal tracing to determine the exact sequence of operations: whether attention layers complete operand injection before MLP computation begins, or whether these processes overlap/interleave in ways not captured by layer-wise intervention.