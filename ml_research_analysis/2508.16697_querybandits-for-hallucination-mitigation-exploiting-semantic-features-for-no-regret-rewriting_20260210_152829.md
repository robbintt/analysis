---
ver: rpa2
title: 'QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for
  No-Regret Rewriting'
arxiv_id: '2508.16697'
source_url: https://arxiv.org/abs/2508.16697
tags:
- https
- query
- reward
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QueryBandits, a bandit-based framework for
  mitigating LLM hallucinations through context-aware query rewriting. By leveraging
  17 linguistic features extracted from queries, QueryBandits learns to select optimal
  rewrite strategies (paraphrase, simplify, disambiguate, expand, clarify) that maximize
  a reward model combining LLM judgment, fuzzy matching, and BLEU scores.
---

# QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting

## Quick Facts
- **arXiv ID**: 2508.16697
- **Source URL**: https://arxiv.org/abs/2508.16697
- **Reference count**: 40
- **Primary result**: 87.5% win rate over no-rewrite baseline using contextual Thompson Sampling for query rewriting

## Executive Summary
This paper proposes QueryBandits, a bandit-based framework for mitigating LLM hallucinations through context-aware query rewriting. By leveraging 17 linguistic features extracted from queries, QueryBandits learns to select optimal rewrite strategies (paraphrase, simplify, disambiguate, expand, clarify) that maximize a reward model combining LLM judgment, fuzzy matching, and BLEU scores. Across 13 QA benchmarks and 1,050 lexically perturbed queries per dataset, the contextual Thompson Sampling variant achieves an 87.5% win rate over a no-rewrite baseline and outperforms static prompting strategies by 42.6-60.3%. Notably, some static prompts worsen hallucination compared to no rewriting.

## Method Summary
QueryBandits employs a multi-armed bandit framework where each arm represents a different query rewriting strategy. The system extracts 17 linguistic features from each query to create a contextual feature vector, then uses contextual Thompson Sampling to select among five rewrite strategies: paraphrase, simplify, disambiguate, expand, and clarify terms. The reward function combines an LLM-based judgment (0.6 weight), fuzzy matching similarity (0.3 weight), and BLEU score (0.1 weight). The approach learns which rewrite strategies work best for different types of queries without requiring model retraining or gradient-based adaptation, demonstrating that guided rewriting based on query semantics can significantly reduce hallucinations.

## Key Results
- Contextual Thompson Sampling achieves 87.5% win rate over no-rewrite baseline
- Outperforms static prompting strategies by 42.6-60.3%
- Some static prompts actually worsen hallucination compared to no rewriting
- Tested across 13 QA benchmarks with 1,050 lexically perturbed queries per dataset
- Demonstrates significant hallucination reduction without model retraining

## Why This Works (Mechanism)
QueryBandits works by recognizing that different types of queries benefit from different rewriting strategies based on their linguistic characteristics. The 17 features capture various aspects like anaphora, negation, polysemy, and ambiguity that can trigger hallucinations in LLMs. By using contextual Thompson Sampling, the system learns which rewrite strategies are most effective for specific feature combinations, allowing it to adapt its rewriting approach dynamically rather than using one-size-fits-all prompts. The reward model provides immediate feedback on whether rewrites improve answer quality, enabling the bandit algorithm to converge on optimal strategies.

## Foundational Learning

**Thompson Sampling**: Bayesian algorithm for balancing exploration and exploitation in bandit problems. Why needed: To learn optimal rewrite strategies without explicit supervision. Quick check: Verify bandit explores different arms early and converges to stable choices.

**Linguistic Feature Extraction**: Process of identifying anaphora, negation, polysemy, ambiguity, etc. in queries. Why needed: To provide context for selecting appropriate rewrite strategies. Quick check: Validate feature extractor on known examples from Table 1.

**Reward Modeling**: Combining LLM judgment, fuzzy matching, and BLEU scores into single metric. Why needed: To quantify hallucination reduction from rewrites. Quick check: Ensure reward scores correlate with human judgment on sample queries.

**Lexical Perturbation**: Creating semantically invariant but lexically different query variants. Why needed: To generate training data where rewrites can demonstrate value. Quick check: Verify perturbations change surface form while preserving meaning.

## Architecture Onboarding

**Component Map**: Query -> Feature Extractor -> Contextual Thompson Sampling -> Rewrite Strategy -> GPT-4o -> Reward Model -> Update Bandit

**Critical Path**: Feature extraction → arm selection → rewriting → LLM inference → reward computation → bandit update

**Design Tradeoffs**: Linear reward model vs. complex judgment; contextual vs. non-contextual bandits; binary vs. continuous features

**Failure Signatures**: Bandit converges to NO-REWRITE (memorization), static prompting outperforms bandit (feature extraction errors), high variance across runs (hyperparameter sensitivity)

**First Experiments**:
1. Verify feature extraction accuracy on 100 sample queries
2. Test bandit on unperturbed queries to check for memorization
3. Run ablation study with different reward model weightings

## Open Questions the Paper Calls Out

**Higher-Order Feature Interactions**: Whether pairwise polynomial feature interactions exacerbate hallucination beyond individual effects. The paper treats features as independent but suggests future work could capture interactions.

**LLM-as-Judge Bias**: Extent to which the LLM judgment component introduces systematic bias. The paper acknowledges this limitation and suggests comparative evaluation with human annotators.

**Cross-Model Transferability**: Whether learned feature-arm weight mappings transfer across different LLM architectures or are model-specific. All experiments used GPT-4o exclusively.

**Continuous vs. Binary Features**: Whether the framework could benefit from continuous rather than binary feature representations for finer-grained reward prediction.

## Limitations

- Reward model relies on LLM-as-judge which may introduce bias
- Binary feature representation may lose predictive signal compared to continuous features
- Results may be specific to GPT-4o architecture and not transfer to other LLMs
- Perturbation methodology and exact prompt templates are underspecified

## Confidence

- **High Confidence**: General framework structure and superiority of context-aware rewriting over no-rewrite baseline
- **Medium Confidence**: Specific claim that contextual Thompson Sampling outperforms other bandit variants
- **Medium Confidence**: Magnitude of performance difference between contextual bandit and static prompts

## Next Checks

1. **Prompt Template Verification**: Implement exact rewrite strategy prompts and GPT-4o judge prompt, then run small-scale experiment (100 queries) to verify reward scores align with reported ranges.

2. **Perturbation Sensitivity Analysis**: Generate perturbed queries using multiple methods and test whether bandit performance degrades when perturbations are less semantically faithful.

3. **Hyperparameter Robustness**: Systematically vary exploration-exploitation parameters and noise variance across grid search to determine if reported results are robust or configuration-dependent.