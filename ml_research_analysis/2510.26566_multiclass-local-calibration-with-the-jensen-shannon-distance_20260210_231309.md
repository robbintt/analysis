---
ver: rpa2
title: Multiclass Local Calibration With the Jensen-Shannon Distance
arxiv_id: '2510.26566'
source_url: https://arxiv.org/abs/2510.26566
tags:
- calibration
- local
- probability
- kernel
- conf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a local perspective on multiclass calibration,
  addressing the problem of proximity bias in existing methods. The authors define
  multiclass local calibration, analyze evaluation metrics under this definition,
  and propose LoCal Nets (LCNs), a neural network architecture that jointly learns
  reduced feature representations and new logits.
---

# Multiclass Local Calibration With the Jensen-Shannon Distance

## Quick Facts
- **arXiv ID:** 2510.26566
- **Source URL:** https://arxiv.org/abs/2510.26566
- **Reference count:** 40
- **One-line primary result:** LoCal Nets (LCNs) improve local calibration (LCE, MLCE) and accuracy on CIFAR-10/100 and TissueMNIST.

## Executive Summary
This paper introduces a local perspective on multiclass calibration to address proximity bias in existing methods. The authors define multiclass local calibration, analyze evaluation metrics under this definition, and propose LoCal Nets (LCNs), a neural network architecture that jointly learns reduced feature representations and new logits. LCNs align predicted probabilities with local estimates of class frequencies using the Jensen-Shannon distance. Empirical results show that LCNs consistently outperform existing methods on local calibration metrics while maintaining competitive performance on global calibration metrics and improving model accuracy.

## Method Summary
The method involves training a LoCal Net (LCN) that learns reduced feature representations and new logits jointly. The LCN takes frozen backbone features, applies PCA reduction to 50 dimensions, and processes them through a hidden layer (64-256 units) with two heads: one for new logits and one for new PCA features. The loss combines Jensen-Shannon distance between predictions and local class frequency estimates (computed via Nadaraya-Watson kernel estimation) with cross-entropy between ground truth and local estimates. The model is trained on a calibration split with Adam optimizer (LR 1e-3), batch size 1024, and hyperparameters λ=1, kernel bandwidth γ=10.

## Key Results
- LCNs consistently outperform existing methods on local calibration metrics (LCE and MLCE)
- LCNs maintain competitive performance on global calibration metrics (ECE, ECCE)
- LCNs improve model accuracy across datasets, achieving up to 2.7% accuracy gains and lowest NLL scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Aligning predictions with local class frequency estimates reduces miscalibration in sparse regions.
- **Mechanism**: The Jensen-Shannon distance between the model's predicted probability vector and kernel-estimated local frequencies is minimized. This forces the model to produce probabilities that match the empirical distribution of labels in each input's neighborhood, addressing proximity bias where sparse regions would otherwise inherit calibration from denser, potentially different regions.
- **Core assumption**: Kernel estimators provide consistent approximations of true conditional class probabilities given sufficient local samples.
- **Evidence anchors**: [abstract] "...enforces alignment between predicted probabilities and local estimates of class frequencies using the Jensen-Shannon distance."
- **Break condition**: If kernel bandwidth γ is too small, local estimates have high variance; if too large, locality is lost and bias dominates.

### Mechanism 2
- **Claim**: Learning reduced feature representations reshapes the geometry to produce better-calibrated neighborhoods.
- **Mechanism**: Unlike post-hoc calibrators that treat logits as fixed, LCNs jointly learn a reduced-dimension feature representation φ'(x) alongside new logits. This allows the model to restructure the representation space so that nearby points have more similar label distributions, improving the validity of local frequency estimates.
- **Core assumption**: The original backbone features contain redundant or poorly structured information that can be compressed while preserving or improving calibration-relevant structure.
- **Evidence anchors**: [Figure 1a] Diagram shows φ'(x) learned alongside new logits, contrasting with post-hoc methods.
- **Break condition**: If the reduction is too aggressive, discriminative information is lost; if too conservative, no improvement over fixed features.

### Mechanism 3
- **Claim**: The similarity term encourages same-class points to cluster locally without collapsing class representations.
- **Mechanism**: The cross-entropy between ground truth and kernel estimates acts as a regularizer: points with the same label are attracted to nearby neighbors, but the kernel's locality prevents distant same-class points from exerting strong influence. This creates tight, well-separated clusters while maintaining fine-grained structure.
- **Core assumption**: Local attraction within classes improves calibration without harming discrimination.
- **Evidence anchors**: [Section 7] "points that share fine-grained similarities are placed closer together, while more distinct variants remain further apart, but within the same class cluster."
- **Break condition**: If λ (similarity term weight) is too high or kernel bandwidth too wide, class representations can collapse.

## Foundational Learning

- **Concept: Jensen-Shannon Distance**
  - Why needed here: Used as the alignment metric between predictions and local frequency estimates; symmetric and bounded unlike KL divergence.
  - Quick check question: What property makes JSD preferable to KL divergence for this alignment task?

- **Concept: Nadaraya-Watson Kernel Estimation**
  - Why needed here: Provides the local class frequency estimates θ̂(y|x) that predictions are aligned against.
  - Quick check question: How does bandwidth γ affect the bias-variance tradeoff in local frequency estimation?

- **Concept: Strong vs. Confidence Calibration**
  - Why needed here: The paper extends local calibration beyond confidence calibration (top-class only) to multiclass strong calibration (full probability vector).
  - Quick check question: Why does confidence calibration fail to guarantee calibration for less frequent classes in multiclass settings?

## Architecture Onboarding

- **Component map**: Backbone (frozen ResNet-50/152) → φ(x) → PCA reduction → φ_PCA(x) → LCN hidden layer → [New Logits, New PCA Features] → Residual combination → φ'_PCA(x), g'(x)
- **Critical path**: Feature reduction quality → Kernel estimate reliability → JSD alignment → Calibration improvement. The bandwidth γ must be tuned before λ; incorrect γ invalidates kernel estimates.
- **Design tradeoffs**:
  - Smaller γ: Better locality, higher variance in estimates, risk of training collapse
  - Larger hidden layer: More expressive reshaping, slower training, potential overfitting
  - Higher λ: Stronger clustering, risk of class collapse
  - Residual weights: Initialized to 1; too far from 1 can destabilize
- **Failure signatures**:
  - Training loss NaN or divergence: γ too small (empty/low-variance kernel weights)
  - No calibration improvement: γ too large (locality lost) or λ too small (no feature reshaping)
  - Accuracy drops significantly: λ too high or reduction dimension too low
- **First 3 experiments**:
  1. **Bandwidth sweep**: Fix λ=1, sweep γ ∈ {1, 5, 10, 20, 50} on validation set; monitor LCE and training stability. Select smallest γ that converges.
  2. **Ablation on similarity term**: Compare LCE/accuracy with λ ∈ {0, 0.5, 1, 2} to verify feature reshaping contribution.
  3. **Reduction dimension test**: Vary PCA dimension ∈ {10, 25, 50, 100}; measure tradeoff between calibration and accuracy.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can adaptive or learned similarity functions effectively replace fixed kernels to improve local calibration in high-dimensional feature spaces?
  - Basis: [explicit] The authors state in the Limitations and Future Work section: "A promising extension is to replace fixed kernels with adaptive or learned similarity functions, which may improve local calibration in high-dimensional or heterogeneous feature spaces."
  - Why unresolved: The current LCN implementation uses a fixed bandwidth γ, which is sensitive to tuning and may not capture complex data geometry.
  - What evidence would resolve it: Empirical results showing that a method using a learned similarity metric achieves lower Local Calibration Error (LCE) than the fixed-kernel LCN on high-dimensional datasets without requiring manual bandwidth tuning.

- **Open Question 2**: How can adaptive kernel choices and scalable training procedures be designed to manage the bias-variance trade-off in finite-sample regimes?
  - Basis: [explicit] The paper notes: "In finite-sample regimes, kernel estimates may suffer from non-negligible bias... An important research direction is to explore adaptive kernel choices and scalable training procedures that better manage the bias–variance tradeoff inherent to local estimation."
  - Why unresolved: The theoretical consistency of the loss function (Theorem 4) is asymptotic. In practice (finite samples), tight kernels reduce bias but increase variance, while broad kernels do the opposite.
  - What evidence would resolve it: A modified training procedure or adaptive kernel scheme that demonstrates reduced kernel estimation error in finite sample settings compared to the static approach.

- **Open Question 3**: Can the proposed local calibration framework be extended to enforce local calibration in model classes other than neural networks?
  - Basis: [explicit] The authors conclude: "Finally, while our method is tailored to neural networks, extending it to enforce local calibration across other classes of models remains an important open direction."
  - Why unresolved: The LCN architecture is explicitly designed to learn feature representations and logits within a neural network structure.
  - What evidence would resolve it: Successful application of a local calibration mechanism (derived from the paper's framework) to a non-neural network classifier, resulting in improved LCE/MLCE scores.

## Limitations
- Kernel estimator formulation (bandwidth scaling, kernel normalization) is not fully specified
- PCA fitting source (train vs. calibration set) affects the reduced representation space
- Residual initialization strategy impact on training stability is not quantified

## Confidence
- **High**: Local calibration error metrics (LCE, MLCE) are consistently improved; the architecture structure (LCN with two heads) is clearly specified.
- **Medium**: Accuracy and NLL improvements are reported, but the mechanism for accuracy gains is not fully disentangled.
- **Low**: The claim that LCNs outperform all existing methods across all metrics is not fully supported—some baselines are only marginally worse on global metrics.

## Next Checks
1. **Bandwidth sensitivity analysis**: Reproduce the LCE vs. γ sweep on CIFAR-10 validation to confirm the claimed optimal value and identify the stability threshold.
2. **Ablation on similarity term**: Train LCN variants with λ=0 and λ=1 on the same data split to quantify the contribution of feature-space clustering to calibration gains.
3. **PCA source validation**: Fit PCA on both training and calibration features separately, train LCNs on each, and compare calibration metrics to assess sensitivity to feature-space domain.