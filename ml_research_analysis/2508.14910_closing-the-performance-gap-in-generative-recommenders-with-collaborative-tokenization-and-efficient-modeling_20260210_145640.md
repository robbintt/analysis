---
ver: rpa2
title: Closing the Performance Gap in Generative Recommenders with Collaborative Tokenization
  and Efficient Modeling
arxiv_id: '2508.14910'
source_url: https://arxiv.org/abs/2508.14910
tags:
- item
- marius
- generative
- items
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the performance gap between generative and
  ID-based recommender systems. The authors identify two key limitations: the lack
  of collaborative signal in item tokenization and inefficiencies in encoder-decoder
  architectures.'
---

# Closing the Performance Gap in Generative Recommenders with Collaborative Tokenization and Efficient Modeling

## Quick Facts
- arXiv ID: 2508.14910
- Source URL: https://arxiv.org/abs/2508.14910
- Reference count: 40
- Generative recommenders with collaborative tokenization match or exceed ID-based models' performance while reducing inference cost

## Executive Summary
This paper addresses the performance gap between generative and ID-based recommender systems by identifying two key limitations: the lack of collaborative signals in item tokenization and inefficiencies in encoder-decoder architectures. The authors propose COSETTE, a contrastive tokenization method that integrates collaborative information into discrete item representations, and MARIUS, a lightweight generative model that decouples timeline modeling from item decoding. Experiments on Amazon review datasets show that their approach narrows or eliminates the performance gap, with MARIUS achieving R@10 scores of 5.3 on Arts & Crafts and 3.45 on Automotive, outperforming both traditional generative models and strong ID-based baselines like SASRec++. The method also reduces inference cost through architectural improvements.

## Method Summary
The approach combines collaborative tokenization with efficient generative modeling. COSETTE extends residual quantization with a contrastive loss that aligns quantized representations of co-occurring items, jointly optimizing for content reconstruction and recommendation relevance. MARIUS uses a Temporal Transformer to process item tokens, then a Depth Transformer to autoregressively generate item codes, reducing inference cost by avoiding full-sequence attention during decoding. The system is trained on Amazon review datasets using leave-last-out splits, with item metadata encoded via Sentence-T5-XL embeddings.

## Key Results
- MARIUS with COSETTE achieves R@10 scores of 5.3 on Arts & Crafts and 3.45 on Automotive datasets
- COSETTE's collaborative loss improves R@10 by 1.18% on Video Games compared to reconstruction-only quantization
- MARIUS generates items 3× faster than TIGER at N=50 through decoupled architecture and KV-caching
- The method reduces popularity bias, predicting more mid-popularity items compared to SASRec++

## Why This Works (Mechanism)

### Mechanism 1: Collaborative Tokenization
Integrating collaborative signals into item tokenization improves downstream recommendation quality compared to content-only quantization. COSETTE extends RQ-VAE with a contrastive loss that aligns quantized representations of items that co-occur in user timelines, jointly optimizing for content reconstruction and recommendation relevance.

### Mechanism 2: Decoupled Architecture
Decoupling temporal sequence modeling from item code decoding reduces inference cost while maintaining or improving accuracy. MARIUS separates timeline processing (Temporal Transformer) from code generation (Depth Transformer), enabling efficient KV-caching during inference.

### Mechanism 3: Semantic Generalization
Generative recommenders with collaborative tokenization scale more effectively than ID-based models because semantic sharing enables better generalization, particularly for mid-popularity items. The collaborative quantization creates shared code structures across semantically and behaviorally similar items.

## Foundational Learning

- **Residual Quantization (RQ-VAE)**: Multi-level quantization creates hierarchical codes (coarse-to-fine) for tree-structured item spaces suitable for autoregressive generation. *Quick check*: Given a latent vector and three codebooks, can you trace how RQ-VAE produces a 3-tuple code by sequentially quantizing residuals?

- **Contrastive Learning with Sigmoid Loss**: Sigmoid loss (vs. softmax) is used for multi-positive pairs, with temperature/bias affecting training dynamics. *Quick check*: Why would standard InfoNCE loss be problematic when each anchor item has multiple positive pairs from co-occurrence?

- **Transformer Decoding with KV-Caching**: Causal attention enables KV-caching, where information must be cached and re-used when generating token t+1 after having generated tokens 1 through t. *Quick check*: In a causal transformer, what information must be cached and re-used when generating token t+1 after having generated tokens 1 through t?

## Architecture Onboarding

- **Component map**: Sentence-T5-XL encoder → MLP projection → COSETTE encoder → Residual Quantizer (L=4 levels, K=256 codes) → Decoder → MARIUS embedding fusion → Temporal Transformer → Linear projection → Depth Transformer

- **Critical path**: Train COSETTE on item metadata + interaction data → Convert user timelines to sequences of semantic ID tuples → Train MARIUS on these sequences → Inference: run Temporal Transformer once, cache output, beam search through Depth Transformer

- **Design tradeoffs**: Larger K reduces collisions but increases Depth Transformer vocabulary; Temporal benefits from ~4 layers regardless of dataset size; λ=10⁻³ balances MSE-scale reconstruction against cross-entropy-scale contrastive loss

- **Failure signatures**: High collision rate (>5%) indicates insufficient codebook capacity; TIGER outperforming MARIUS at small scale suggests under-capacity Depth Transformer; validation loss diverging suggests overfitting to co-occurrence patterns

- **First 3 experiments**:
  1. Train COSETTE and RQ-VAE on same dataset, measure collision rates and visualize code distributions
  2. Ablate collaborative loss with λ ∈ {0, 10⁻⁴, 10⁻³, 10⁻²}, plot validation R@10 vs. λ
  3. Implement MARIUS and TIGER with matched parameters, measure items/second at N=20, 50, 100

## Open Questions the Paper Calls Out

1. How can generative recommenders be optimized to improve accuracy on rare, long-tail items without sacrificing gains on mid-popular items? The current approach predicts less rare items while improving mid-popular predictions.

2. Can the gap between generative and ID-based models be fully closed on small-scale datasets with limited item catalogs? High-dimensional embeddings struggle to form distinct codes when catalog size is small (<20k items).

3. Is it possible to develop a differentiable tokenization objective that guarantees unique semantic IDs without relying on post-hoc reallocation? The current collision handling is heuristic and non-differentiable.

## Limitations

- Performance claims rely on Amazon review datasets with strong 5-core filtering, which may not generalize to sparser real-world scenarios
- Optimal hyperparameters (λ=10⁻³, L=4 levels, K=256 codes) are not thoroughly validated across diverse catalog sizes and domains
- The study focuses exclusively on sequential recommendation, leaving unclear whether benefits extend to session-based or graph-based methods

## Confidence

- **High confidence**: Efficiency improvements of MARIUS over encoder-decoder architectures are well-supported by the decoupled architecture design and KV-caching mechanism
- **Medium confidence**: Collaborative tokenization improvements via COSETTE are demonstrated through ablation studies, but magnitude of gains may depend heavily on dataset characteristics
- **Medium confidence**: Generalization benefits (reduced popularity bias) are supported by frequency distribution analysis, but mechanism assumes semantic sharing consistently outweighs ID embedding inductive bias

## Next Checks

1. Train COSETTE and MARIUS on datasets with varying interaction densities (1-core to 5-core Amazon subsets, MovieLens, Last.fm) to validate whether collaborative tokenization benefits persist when co-occurrence signals weaken

2. Implement TIGER and MARIUS with exactly matched parameter counts and training compute, then measure R@10 and inference throughput across sequence lengths N=10, 20, 50, 100 to isolate architectural efficiency gains

3. Intentionally increase dataset size by 10× while keeping codebook capacity fixed, then measure collision rates and performance degradation to determine practical limits of current quantization strategy