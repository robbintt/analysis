---
ver: rpa2
title: 'KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned
  to Diverse Visual Contexts'
arxiv_id: '2508.19944'
source_url: https://arxiv.org/abs/2508.19944
tags:
- system
- arxiv
- reasoning
- korean
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KRETA, a large-scale benchmark for Korean
  reading and reasoning in text-rich VQA attuned to diverse visual contexts. The benchmark
  comprises 2,577 samples categorized across 15 domains and 26 image types, using
  a dual-level reasoning framework (System 1 for basic recognition, System 2 for advanced
  reasoning).
---

# KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts

## Quick Facts
- arXiv ID: 2508.19944
- Source URL: https://arxiv.org/abs/2508.19944
- Reference count: 9
- Introduces KRETA, a large-scale Korean VQA benchmark with 2,577 samples across 15 domains and 26 image types

## Executive Summary
This paper introduces KRETA, a large-scale benchmark for Korean reading and reasoning in text-rich VQA attuned to diverse visual contexts. The benchmark comprises 2,577 samples categorized across 15 domains and 26 image types, using a dual-level reasoning framework (System 1 for basic recognition, System 2 for advanced reasoning). A semi-automated VQA generation pipeline is developed, leveraging stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. Experiments reveal that while VLMs perform well on basic Korean text recognition (System 1), they struggle significantly with higher-order reasoning tasks (System 2), especially open-source models. These findings highlight the need for targeted training incorporating Korean cultural and domain-specific knowledge, complex layouts, and multi-step reasoning. The authors release the dataset, generation pipeline, and evaluation code to support multilingual VLM research.

## Method Summary
KRETA employs a semi-automated VQA generation pipeline that combines stepwise image decomposition with human-in-the-loop validation to create 2,577 Korean text-rich VQA samples. The benchmark organizes data across 15 domains and 26 image types, using a dual-level reasoning framework based on dual-process theory. System 1 tasks focus on basic text recognition, while System 2 tasks require advanced reasoning and multi-step inference. A comprehensive seven-metric evaluation protocol ensures data quality through checks for answer correctness, reasoning validity, and cultural appropriateness. The pipeline generates questions that probe both visual understanding and Korean language comprehension, with particular attention to domain-specific contexts like signage, documents, and cultural artifacts.

## Key Results
- VLMs demonstrate strong performance on System 1 basic text recognition tasks but struggle significantly with System 2 advanced reasoning
- Open-source models show particular difficulty with higher-order reasoning compared to proprietary models
- The benchmark reveals gaps in current VLM capabilities regarding Korean cultural context integration and multi-step reasoning

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to capturing real-world Korean text-rich scenarios through diverse domain coverage and image types. The dual-level reasoning framework aligns with cognitive science principles of dual-process theory, creating a structured evaluation of both intuitive and deliberative reasoning capabilities. The semi-automated generation pipeline ensures scalability while maintaining quality through rigorous evaluation metrics. By focusing specifically on Korean language and cultural contexts, KRETA addresses the unique challenges of non-English VQA tasks, including character recognition, contextual understanding, and cultural knowledge integration.

## Foundational Learning
- Dual-process theory: Explains System 1 (fast, intuitive) vs System 2 (slow, deliberative) reasoning - needed to structure benchmark tasks appropriately, check by validating task categorization
- Korean text processing: Addresses unique challenges of Hangul character recognition and layout understanding - needed for accurate VQA generation, check by testing on diverse Korean document types
- Cross-modal reasoning: Combines visual perception with language understanding - needed for text-rich VQA, check by evaluating performance on multi-step inference tasks

## Architecture Onboarding

**Component Map**
Image Decomposition -> VQA Generation -> Seven-Metric Evaluation -> System 1/2 Categorization

**Critical Path**
1. Image selection and decomposition into analyzable components
2. VQA question generation targeting specific reasoning levels
3. Multi-metric quality validation and categorization
4. Domain and type organization for systematic evaluation

**Design Tradeoffs**
- Automated vs human validation: Prioritized automation for scalability while maintaining quality through multiple metrics
- Domain breadth vs depth: Selected 15 domains to ensure diversity while maintaining sufficient samples per domain
- System 1 vs System 2 balance: Ensured adequate representation of both reasoning levels to capture model capabilities

**Failure Signatures**
- Poor performance on System 2 tasks indicates limitations in multi-step reasoning and cultural context integration
- Errors in Korean character recognition suggest inadequate language-specific training
- Inconsistent performance across domains reveals gaps in domain knowledge transfer

**First 3 Experiments to Run**
1. Test VLM performance on pure Korean text recognition tasks to establish baseline System 1 capabilities
2. Evaluate multi-step reasoning performance on culturally-specific Korean scenarios
3. Compare proprietary vs open-source model performance across all 15 domains

## Open Questions the Paper Calls Out
None

## Limitations
- The 15 domains and 26 image types may not fully represent emerging real-world scenarios like social media content
- The dual-level reasoning framework relies on subjective categorization that may not consistently capture task complexity
- Automated evaluation metrics may not fully capture nuances of human judgment in assessing answer correctness

## Confidence

**High Confidence**
- Dataset generation pipeline implementation and benchmark structural organization
- Basic text recognition performance results and methodology

**Medium Confidence**
- System 2 reasoning task difficulty assessment
- Conclusion about open-source VLM limitations
- Need for Korean cultural knowledge integration (requires further empirical validation)

## Next Checks
1. Conduct human evaluation studies comparing model performance against human reasoning capabilities across all 15 domains to establish baseline performance expectations and validate the difficulty calibration of System 1 vs. System 2 tasks.

2. Test model performance on out-of-distribution samples from domains not represented in the benchmark to assess the generalization capability of current VLMs and identify potential blind spots in the benchmark's domain coverage.

3. Implement and evaluate a pilot training approach incorporating Korean cultural knowledge and complex layout understanding to empirically validate the proposed solution for improving System 2 reasoning performance.