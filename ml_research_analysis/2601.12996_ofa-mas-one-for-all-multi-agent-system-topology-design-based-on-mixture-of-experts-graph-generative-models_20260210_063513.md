---
ver: rpa2
title: 'OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts
  Graph Generative Models'
arxiv_id: '2601.12996'
source_url: https://arxiv.org/abs/2601.12996
tags:
- graph
- task
- topology
- design
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OFA-MAS introduces a one-for-all framework for generating adaptive
  collaboration topologies in multi-agent systems. It combines a task-aware graph
  state encoder (TAGSE) with a mixture-of-experts (MoE) architecture and a three-stage
  training strategy to enable a single model to design effective agent topologies
  across diverse domains.
---

# OFA-MAS: One-for-All Multi-Agent System Topology Design based on Mixture-of-Experts Graph Generative Models

## Quick Facts
- arXiv ID: 2601.12996
- Source URL: https://arxiv.org/abs/2601.12996
- Reference count: 40
- One-line primary result: Achieves 93.02% average accuracy across six benchmarks using a single universal model that generates adaptive MAS topologies from natural language queries.

## Executive Summary
OFA-MAS introduces a one-for-all framework for generating adaptive collaboration topologies in multi-agent systems. It combines a task-aware graph state encoder (TAGSE) with a mixture-of-experts (MoE) architecture and a three-stage training strategy to enable a single model to design effective agent topologies across diverse domains. The approach achieves 93.02% average accuracy across six benchmarks, outperforming specialized one-for-one models and demonstrating strong out-of-distribution generalization on the GAIA benchmark. The framework leverages LLM-driven data synthesis to overcome data scarcity and employs a universal role pool to enable flexible cross-domain role composition.

## Method Summary
OFA-MAS generates adaptive MAS collaboration topologies from natural language queries using an autoregressive graph generation approach. The method employs a Task-Aware Graph State Encoder (TAGSE) that filters task-relevant node information via sparse gating, combined with a mixture-of-experts (MoE) architecture that dynamically selects specialized sub-networks for different domains. The three-stage training strategy includes unconditional pre-training on canonical graph structures, large-scale conditional pre-training with LLM-synthesized (query, topology) pairs, and supervised fine-tuning on empirically validated pairs. The model predicts both agent roles and communication edges sequentially, with a universal role pool of 19 roles enabling cross-domain flexibility.

## Key Results
- Achieves 93.02% average accuracy across six benchmarks (MMLU, GSM8K, AQuA, MultiArith, SVAMP, HumanEval)
- Outperforms specialized one-for-one models on cross-domain generalization tasks
- Demonstrates strong out-of-distribution generalization with 92.15% accuracy on GAIA benchmark without fine-tuning
- Ablation studies confirm the importance of TAGSE, MoE, and three-stage training curriculum

## Why This Works (Mechanism)

### Mechanism 1: Task-Aware Sparse Gating for Selective Information Flow
Context-gated message passing filters task-relevant node information, improving topology generation quality. The Task-Aware Graph State Encoder (TAGSE) uses a gating network controlled by the task vector z that element-wise modulates messages: m_v = σ(W_g[h_v || z]) ⊙ ReLU(W_m·h_v). L1 regularization enforces sparsity, ensuring only task-relevant channels activate.

### Mechanism 2: MoE-Based Domain Specialization with Dynamic Routing
Mixture-of-Experts enables a single model to learn domain-specific generation strategies while maintaining shared knowledge. A gating network computes expert weights w = Softmax(MLP_gate(z)), and K=8 experts contribute weighted predictions for node roles and edge connections. Different task types activate different expert subsets.

### Mechanism 3: Curriculum-Based Progressive Training for Cross-Domain Generalization
A three-stage training curriculum enables universal topology generation by progressively grounding structural priors, then task-topology mappings, then empirical validation. Stage 1 learns structural "grammar" on 800 canonical graphs. Stage 2 aligns task semantics to structures via 1,000 LLM-synthesized pairs. Stage 3 refines on 1,000 empirically validated pairs from real MAS execution.

## Foundational Learning

- **Graph Neural Networks / Message Passing**
  - Why needed here: TAGSE updates node representations through attention-weighted aggregation of neighbor messages
  - Quick check question: Given a 4-node chain graph with initial features h_0, h_1, h_2, h_3, what information does node 2 receive after one message-passing layer?

- **Autoregressive Sequence Modeling**
  - Why needed here: OFA-MAS generates graphs by sequential node+edge predictions with probability factorization
  - Quick check question: Why might greedy decoding produce different graphs than beam search in this autoregressive setting?

- **Mixture-of-Experts Routing**
  - Why needed here: The gating network decides which experts contribute; understanding load balancing prevents training instability
  - Quick check question: If all routing weights collapse to a single expert, what happens to model capacity and how does L_balance prevent this?

## Architecture Onboarding

- **Component map:**
  - Task query Q (text) → Sentence-BERT → task vector z (128-dim)
  - TAGSE: 4-layer context-gated GNN with role-aware attention
  - MoE Module: 8 experts + gating network
  - Output: Generated DAG topology G = (V, E)

- **Critical path:**
  1. Encode query → task vector z
  2. Initialize partial graph with START token
  3. For each generation step t: TAGSE encodes G_{<t} → MoE predicts next role v_t → MoE predicts incoming edges E_t
  4. Stop when END token predicted (max 6 nodes)

- **Design tradeoffs:**
  - Universal vs. specialized role pools: 19 roles enable cross-domain flexibility but increase prediction space
  - Number of experts: K=8 balances specialization with routing complexity
  - Generation strategy: Greedy decoding is deterministic but may miss optimal topologies

- **Failure signatures:**
  - Gate collapse: All experts receive similar weights
  - Invalid DAG generation: Cycles appear in output
  - Role repetition: Same role predicted multiple times
  - OOD degradation: Performance drops sharply on unseen domains

- **First 3 experiments:**
  1. Ablate MoE: Replace MoE heads with single MLP; compare accuracy on MMLU vs. HumanEval
  2. Visualize gate activations: Run inference on 50 queries per domain; plot expert weight distributions
  3. Test Stage 2 data quality: Train model with real vs. synthetic (query, topology) pairs only

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality and diversity of LLM-generated synthetic topologies affect the final performance of OFA-MAS, and can we develop validation mechanisms to ensure these synthetic topologies align with empirically optimal structures? The paper acknowledges that collecting sufficient task queries with ground-truth optimal topologies is "prohibitively expensive" and uses LLM-driven synthesis as a solution, but the alignment between synthetic and real optimal topologies remains unverified.

### Open Question 2
Can OFA-MAS effectively scale to generate larger and more complex multi-agent topologies beyond the current 6-node maximum, and what architectural modifications would be necessary? The current autoregressive generation mechanism and MoE architecture may face combinatorial explosion as graph size increases, and the role pool of 19 roles may be insufficient for larger-scale systems.

### Open Question 3
How can OFA-MAS be extended to support continuous or online adaptation where new domains and task types emerge after deployment, without requiring full retraining? The current approach still requires a full three-stage training process when incorporating new domains, lacking mechanisms for incremental learning of genuinely novel task types.

### Open Question 4
How would OFA-MAS integrate with tool-augmented or multimodal agents, where topology design must consider not only agent-to-agent communication but also agent-to-tool interactions? The current graph generation framework models agent nodes and edges but does not represent external tools or multimodal inputs as first-class entities.

## Limitations
- Reliance on LLM-synthesized training data without systematic validation of synthetic-to-real topology alignment
- Limited evaluation to single-communication-round executions, leaving multi-round performance uncertain
- Universal role pool of 19 roles may not cover all domain-specific requirements for larger-scale systems

## Confidence
- **High confidence:** The effectiveness of the MoE architecture for domain specialization
- **Medium confidence:** The curriculum-based training strategy benefits
- **Medium confidence:** OOD generalization on GAIA benchmark

## Next Checks
1. Validate task representation capacity by training ablations with 64-dim, 128-dim, and 256-dim task vectors
2. Test synthetic data bias by replacing Stage 2 synthetic pairs with real (query, topology) pairs from additional domains
3. Evaluate multi-round performance by extending MAS execution to 2-3 communication rounds per task