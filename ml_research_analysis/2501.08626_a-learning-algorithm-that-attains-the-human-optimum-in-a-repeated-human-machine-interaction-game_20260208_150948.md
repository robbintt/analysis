---
ver: rpa2
title: A Learning Algorithm That Attains the Human Optimum in a Repeated Human-Machine
  Interaction Game
arxiv_id: '2501.08626'
source_url: https://arxiv.org/abs/2501.08626
tags:
- human
- cost
- algorithm
- trial
- experiments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a learning algorithm that enables a machine
  to find the minimum of a cost function known only to a human through repeated interactions.
  The algorithm updates the machine's estimate of the human's optimum by observing
  human responses to the machine's actions, without requiring knowledge or measurements
  of the cost function itself.
---

# A Learning Algorithm That Attains the Human Optimum in a Repeated Human-Machine Interaction Game

## Quick Facts
- arXiv ID: 2501.08626
- Source URL: https://arxiv.org/abs/2501.08626
- Authors: Jason T. Isa; Lillian J. Ratliff; Samuel A. Burden
- Reference count: 16
- Primary result: Machine learns to find human's cost minimum through repeated interactions without knowing the cost function

## Executive Summary
This work presents a learning algorithm enabling a machine to find the minimum of a cost function known only to a human through repeated interactions. The algorithm updates the machine's estimate of the human's optimum by observing human responses to the machine's actions, without requiring knowledge or measurements of the cost function itself. Human subjects experiments demonstrate consistent convergence to the human's cost minimum across scalar and multidimensional instantiations of the game, with results closely matching theoretical simulations. The algorithm shows promise for applications in assistive devices and human-robot interaction where the human's cost function is unknown to the machine.

## Method Summary
The algorithm operates through a repeated game where a machine estimates a human's cost minimum by observing responses to its actions. At each iteration, the machine plays an affine policy m = L(h - ĥ*) + m̂*, observes the human's best-response h', then perturbs the policy to L + Δ and observes the response h''. The machine updates its estimates: ĥ*[k+1] ← h' and m̂*[k+1] ← m̂*[k] + α(m'' - m̂*[k]). The process repeats for K iterations with parameters Δ=1, α=1, and L=0. Human subjects interact via mouse movements with visual cost feedback, and convergence is measured via L-1 error between estimated and true optima.

## Key Results
- Consistent convergence to human's cost minimum across 1×1, 1×2, 2×1, and 2×2 dimensional instantiations
- Human subject experiment results closely match theoretical simulations for quadratic cost functions
- Median L-1 error decreases with iteration count, demonstrating effective learning
- Algorithm works without requiring knowledge or measurements of the cost function itself

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing the machine's affine policy reveals directional information about the human's cost minimum.
- Mechanism: The machine plays policy m = L(h - ĥ*) + m̂*, then perturbs L → L + Δ. The human's shifted best-response to the perturbed policy provides a gradient-like update direction for m̂* without requiring explicit cost function knowledge.
- Core assumption: Humans rapidly play best-responses to machine policies (supported by prior work cited: Chasnov et al., 2023; Isa et al., 2024).
- Evidence anchors:
  - [abstract] "learning algorithm that works solely by observing human actions to find the cost minimum, avoiding the need to solve an inverse problem"
  - [section 2] Algorithm 1 shows the perturbation step: (h″, m″) ← trial(L + Δ, ĥ*[k], m̂*[k]) and the update rule: m̂*[k+1] ← m̂*[k] + α(m″ − m̂*[k])
  - [corpus] Weak/no direct corpus support; neighbor papers address unrelated HMI domains (surgical assistance, speech-workload estimation) but not this perturbation-based mechanism.
- Break condition: If humans cannot consistently play best-responses within trial duration, or if response latencies exceed trial windows, the perturbation-observation signal degrades.

### Mechanism 2
- Claim: The human's best-response to an affine policy encodes sufficient information to localize the joint optimum (h*, m*).
- Mechanism: Under the quadratic cost c(h,m) = ½h^T h + ½m^T m, the human's best-response to the machine's affine policy has a closed-form (Equation 2): argmin_h c(h, L(h-ĥ*) + m̂*) = (I + L^T L)^{-1}(L^T L ĥ*^T - L^T m̂*^T). This maps observed actions to estimates of the optimum.
- Core assumption: The cost function is quadratic (or locally approximable as quadratic); the human has perfect knowledge of this cost.
- Evidence anchors:
  - [section 2] Equation (2) derives the best-response formula; Equation (3) shows the resulting linear dynamics.
  - [section 2] "We studied the simple quadratic cost c(h,m) = ½h^T h + ½m^T m"
  - [corpus] No direct corpus validation for this specific best-response encoding mechanism.
- Break condition: If the cost function is non-quadratic with complex curvature, the affine-policy best-response mapping may not yield a stable linear system.

### Mechanism 3
- Claim: The algorithm's update rule forms an affine discrete-time system with predictable convergence properties for quadratic costs.
- Mechanism: Combining the human best-response with the update formulas yields Equation (3)—a linear state-space system in (ĥ*, m̂*). This enables simulation-based convergence analysis and parameter tuning.
- Core assumption: System matrix eigenvalues lie within the unit circle for chosen (L, Δ, α).
- Evidence anchors:
  - [section 2] "We use this linear system to simulate the human-machine interaction and assess convergence"
  - [section 4.1] "Simulation data is overlaid on the experiment data, showing excellent agreement between theory and experiment"
  - [corpus] No corpus support for linear-system-based human-machine learning algorithms.
- Break condition: Poorly chosen (α, Δ, L) may yield unstable eigenvalues; non-stationary human behavior breaks linearity assumptions.

## Foundational Learning

- Concept: **Best-response dynamics in game theory**
  - Why needed here: The entire algorithm depends on predicting how a rational agent responds to a fixed policy.
  - Quick check question: Given cost c(h,m) and fixed m = Lh + b, can you derive the optimal h?

- Concept: **Affine/linear control policies**
  - Why needed here: The machine's policy class is affine; understanding how parameter changes affect outputs is essential.
  - Quick check question: What happens to the output of m = L(h - ĥ*) + m̂* if ĥ* increases by δ?

- Concept: **Discrete-time linear system stability**
  - Why needed here: Convergence analysis reduces to checking eigenvalues of the update matrix in Equation (3).
  - Quick check question: For a 2×2 discrete system x_{k+1} = Ax_k, what condition on A ensures convergence to zero?

## Architecture Onboarding

- Component map: Trial function -> Perturbation module -> Update rule -> Cost renderer
- Critical path: Initialize (ĥ*[0], m̂*[0]) → Run baseline trial → Run perturbed trial(s) → Extract mean actions from last 5s → Apply update rule → Repeat for K iterations
- Design tradeoffs:
  - Larger Δ: Stronger perturbation signal but may displace human from near-optimal behavior
  - Larger α: Faster convergence but risk of oscillation/instability
  - Trial duration: Longer trials improve action averaging but increase experiment time; paper uses 10s (1D) and 25s (2D)
- Failure signatures:
  - Divergence or oscillation of (ĥ*, m̂*) suggests unstable (α, Δ) combination
  - High variance in observed actions across trials suggests human not reaching steady-state best-response
  - Cost not decreasing across iterations suggests mismatch between prescribed and actual human cost
- First 3 experiments:
  1. Replicate the 1×1 scalar case with 8 initialization points; verify median trajectories match Figure 2a and L-1 error decreases as in Figure 2b,c
  2. Vary α ∈ {0.5, 1.0, 1.5} while fixing Δ = 1; measure convergence rate and stability margin against simulation predictions from Equation (3)
  3. Introduce a non-quadratic cost (e.g., c(h,m) = |h|^p + |m|^p for p ≠ 2); assess whether the affine update still converges or requires modification

## Open Questions the Paper Calls Out

- Question: Can the algorithm converge to an optimum when the human's cost function is intrinsic and non-quadratic, such as metabolic cost-of-transport in assistive devices?
  - Basis in paper: [explicit] The authors state, "A key limitation of the present work is that the cost function is prescribed to the human subjects," and propose applying the algorithm to "metabolic cost-of-transport."
  - Why unresolved: The theoretical derivation and human experiments relied exclusively on simple quadratic cost functions, whereas real-world biological costs are often complex and non-quadratic.
  - What evidence would resolve it: Successful convergence demonstrated in physical human-robot interaction tasks (e.g., exoskeleton tuning) where the optimum is verified via respirometry.

- Question: What formal conditions guarantee convergence and what bounds exist for the convergence rate of the algorithm?
  - Basis in paper: [explicit] "Future work could further explore this system's dynamics, potentially providing conditions that guarantee convergence or bounding the convergence rate."
  - Why unresolved: While the paper derives a discrete-time linear system and demonstrates empirical convergence, it does not provide a formal proof of stability or rate limits for the learning dynamics.
  - What evidence would resolve it: A theoretical derivation establishing sufficient conditions on learning parameters (e.g., $\alpha, \Delta$) and cost structures that ensure stability.

- Question: How does the performance of this observation-only algorithm compare to state-of-the-art human-in-the-loop optimization algorithms that rely on direct cost measurements?
  - Basis in paper: [explicit] The authors note that in the context of assisted mobility, "our algorithm could be compared against state-of-the-art human-in-the-loop optimization algorithms that rely on cost measurements."
  - Why unresolved: The current study validates the algorithm against a known ground truth in a virtual game but has not benchmarked its efficiency or accuracy against existing measurement-based methods.
  - What evidence would resolve it: A comparative user study measuring time-to-convergence and error relative to gold-standard optimization techniques.

## Limitations
- The algorithm assumes humans consistently play best-responses to affine policies within trial durations, which may not hold for complex or unfamiliar tasks
- Performance with non-quadratic costs remains untested, limiting applicability to real-world scenarios with complex cost structures
- Small sample sizes in human experiments (8 participants) limit generalizability to broader populations

## Confidence
**High confidence**: The linear system convergence analysis for quadratic costs, simulation-experiment agreement, and multidimensional generalization (1×2, 2×1, 2×2 cases).
**Medium confidence**: Human subject experiment results given small sample sizes and lack of quantified human response variability.
**Low confidence**: Generalization to non-quadratic costs, participants with different cognitive abilities, or scenarios requiring real-time adaptation.

## Next Checks
1. **Behavioral validation**: Measure human response times and action variance during trials to quantify how closely participants approximate best-response behavior. Compare variance across perturbation magnitudes (Δ).
2. **Non-quadratic cost testing**: Implement the algorithm with costs like c(h,m) = |h|^p + |m|^p for p ∈ {1.5, 2.5, 3.0} to identify break points where the affine-policy assumption fails.
3. **Stability sensitivity analysis**: Systematically vary (α, Δ, L) parameters to map the stability region predicted by Equation 3 eigenvalues against empirical convergence behavior.