---
ver: rpa2
title: Intelligently Weighting Multiple Reference Models for Direct Preference Optimization
  of LLMs
arxiv_id: '2512.10040'
source_url: https://arxiv.org/abs/2512.10040
tags:
- reference
- mrpo
- preference
- methods
- ultrafeedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work evaluates methods for setting reference weights in multiple-reference\
  \ preference optimization (MRPO). The authors propose four new strategies\u2014\
  two offline (validation-based discrimination and accuracy weighting) and two online\
  \ (sliding-window cumulative weighting and Thompson sampling)."
---

# Intelligently Weighting Multiple Reference Models for Direct Preference Optimization of LLMs

## Quick Facts
- **arXiv ID:** 2512.10040
- **Source URL:** https://arxiv.org/abs/2512.10040
- **Authors:** Skyler Wu; Aymen Echarghaoui
- **Reference count:** 6
- **Primary result:** Single-reference DPO consistently outperforms all tested multiple-reference approaches

## Executive Summary
This paper investigates methods for setting reference weights in multiple-reference preference optimization (MRPO) for large language models. The authors propose four new strategies for intelligent reference weightingâ€”two offline (validation-based discrimination and accuracy weighting) and two online (sliding-window cumulative weighting and Thompson sampling). Experiments on UltraFeedback and SafeRLHF datasets using Qwen2.5-0.5B as policy model with seven reference models demonstrate that all four proposed methods outperform existing MRPO approaches in test accuracy. Surprisingly, the study finds that single-reference DPO with any of six out of seven reference models consistently outperforms all tested multiple-reference approaches, suggesting that multiple-reference methods may not provide practical advantages in current implementations.

## Method Summary
The paper evaluates reference model weighting strategies for multiple-reference preference optimization (MRPO). The authors introduce four new approaches: offline validation-based discrimination and accuracy weighting, and online sliding-window cumulative weighting and Thompson sampling. These methods aim to intelligently select which reference model to trust for each preference pair during DPO training. The experiments use Qwen2.5-0.5B as the policy model with seven different reference models on UltraFeedback and SafeRLHF datasets. All proposed methods are compared against existing MRPO approaches, with test accuracy as the primary evaluation metric. The study reveals that reference models primarily function as grammatical coherence checks rather than providing task-specific distillation benefits.

## Key Results
- All four proposed reference weighting methods (validation-based discrimination, accuracy weighting, sliding-window cumulative weighting, Thompson sampling) outperform existing MRPO approaches in test accuracy
- Single-reference DPO with any of six out of seven reference models consistently outperforms all tested multiple-reference approaches
- Thompson sampling and sliding-window methods show potential but face challenges from noisy validation rewards and gradient instability when switching between reference models
- Reference models appear to function primarily as grammatical coherence checks rather than sources of task-specific distillation

## Why This Works (Mechanism)
The proposed methods work by intelligently assigning weights to multiple reference models during preference optimization. Offline methods (validation-based discrimination and accuracy weighting) use static information about reference model performance to set weights before training. Online methods (sliding-window cumulative weighting and Thompson sampling) dynamically adjust weights during training based on observed performance. Thompson sampling treats reference model selection as a multi-armed bandit problem, balancing exploration of different references with exploitation of the currently best-performing model. The sliding-window approach accumulates performance metrics over recent training examples to smooth out noisy judgments. These strategies aim to leverage the strengths of multiple reference models while mitigating their individual weaknesses, though the surprising finding that single-reference approaches outperform all multi-reference methods suggests this goal may be difficult to achieve in practice.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A reinforcement learning-free method for aligning language models to human preferences using preference pairs
  - *Why needed*: Provides the training framework within which reference model weighting operates
  - *Quick check*: DPO uses pairwise comparisons rather than reward modeling to update model parameters

- **Multi-armed bandit problems**: Sequential decision-making framework where actions (arms) have uncertain rewards and the goal is to maximize cumulative reward
  - *Why needed*: Thompson sampling applies this framework to reference model selection during training
  - *Quick check*: Each reference model is treated as an arm with unknown reward distribution

- **Thompson sampling**: A probabilistic algorithm for multi-armed bandit problems that maintains posterior distributions over arm rewards and samples from them to make decisions
  - *Why needed*: Enables exploration-exploitation tradeoff in dynamically selecting reference models
  - *Quick check*: Samples from posterior distributions to balance trying new references vs. using known good ones

- **Grammatical coherence checking**: The role of reference models as ensuring linguistic quality rather than providing task-specific guidance
  - *Why needed*: Explains why single-reference approaches may be sufficient in many cases
  - *Quick check*: Reference models primarily validate output quality rather than teach specific skills

## Architecture Onboarding

**Component Map:** Data loaders -> Reference models -> Weight assignment module -> DPO trainer -> Policy model

**Critical Path:** Preference pairs are processed through reference models, weighted by the selected strategy, then used to compute DPO gradients that update the policy model

**Design Tradeoffs:** The paper explores the tradeoff between using multiple reference models (potentially capturing diverse strengths) versus the simplicity and stability of single-reference approaches. The surprising result that single-reference consistently wins suggests the added complexity of multiple-reference methods may not justify their implementation costs.

**Failure Signatures:** Thompson sampling shows gradient instability when switching between reference models, indicating sensitivity to abrupt changes in optimization objectives. Sliding-window methods suffer from noisy validation rewards that can lead to suboptimal weight assignments. The reference model diversity appears insufficient to provide complementary benefits across preference pairs.

**3 First Experiments:**
1. Test single-reference DPO with each of the seven reference models individually to establish baseline performance
2. Implement and evaluate the four proposed weighting strategies against existing MRPO baselines
3. Analyze the distribution of reference model selections over training to understand when different models are trusted

## Open Questions the Paper Calls Out
None

## Limitations
- The finding that single-reference DPO consistently outperforms all multiple-reference approaches, if reproducible, fundamentally challenges the premise that multiple reference models provide value in DPO settings
- The reference model's role as primarily a "grammatical coherence check" rather than a task-specific distillation source is based on a limited set of seven reference models with unclear selection criteria
- Thompson sampling's reported improvements may not be robust due to noted "gradient instability when switching between reference models"
- Validation reward noise presents a fundamental limitation that could undermine the theoretical foundations of many reference weighting approaches

## Confidence
- Single-reference superiority: **Medium** - counterintuitive result that may be dataset-specific
- Grammatical coherence interpretation: **Medium** - limited reference model diversity tested
- Thompson sampling performance: **Low** - faces technical challenges with gradient instability
- Validation reward reliability: **Medium** - fundamental issue affecting multiple strategies

## Next Checks
1. **Dataset generalization test**: Replicate experiments on a broader range of preference datasets including non-safety and non-instruction-following tasks to verify whether single-reference superiority holds across different reward landscapes

2. **Reference model diversity analysis**: Systematically vary the capabilities and training distributions of reference models to test whether the "grammatical coherence check" interpretation holds when reference models span different domains or exhibit more diverse failure modes

3. **Scale sensitivity evaluation**: Test whether Thompson sampling's gradient instability persists or diminishes when applied to larger model sizes (e.g., 7B or 70B parameters) where optimization dynamics may differ significantly from the 0.5B scale studied here