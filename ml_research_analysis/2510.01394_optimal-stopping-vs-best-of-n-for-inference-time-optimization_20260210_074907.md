---
ver: rpa2
title: Optimal Stopping vs Best-of-$N$ for Inference Time Optimization
arxiv_id: '2510.01394'
source_url: https://arxiv.org/abs/2510.01394
tags:
- reward
- algorithm
- adaptive
- arxiv
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of optimizing inference-time compute
  allocation in large language models by deciding when to stop generating responses
  based on quality and cost trade-offs. It introduces a principled approach based
  on the Pandora's Box problem from optimal stopping theory, treating each generation
  as opening a costly box with random reward.
---

# Optimal Stopping vs Best-of-$N$ for Inference Time Optimization

## Quick Facts
- arXiv ID: 2510.01394
- Source URL: https://arxiv.org/abs/2510.01394
- Authors: Yusuf Kalayci; Vinod Raman; Shaddin Dughmi
- Reference count: 40
- Key outcome: Adaptive UCB-style algorithm achieves same performance as Best-of-N with 15-35% fewer generations through optimal stopping theory

## Executive Summary
This paper addresses the challenge of optimizing inference-time compute allocation in large language models by determining when to stop generating responses based on quality and cost trade-offs. The authors introduce a principled approach based on the Pandora's Box problem from optimal stopping theory, treating each generation as opening a costly box with random reward. By developing a UCB-style algorithm that adaptively learns stopping thresholds without prior knowledge of reward distributions, they demonstrate significant computational savings while maintaining performance quality.

## Method Summary
The method treats inference-time optimization as a sequential decision problem where each LLM generation has a random reward and associated cost. The authors develop a UCB-style algorithm that adaptively learns when to stop generating based on the distribution of rewards. Key technical innovations include using confidence bounds on fair-cap values computed from shifted exponential tail fits, and employing a Bradley-Terry inspired transformation to normalize rewards across prompts and learn stopping thresholds dynamically. The algorithm balances exploration (gathering more samples) with exploitation (stopping when the best reward is sufficiently good) through rigorous statistical guarantees derived from optimal stopping theory.

## Key Results
- Adaptive algorithm achieves same performance as non-adaptive Best-of-N sampling with 15-35% fewer generations
- Win rate of 62.3% vs 61.8% under fixed compute budgets (1280 total samples)
- Sample savings range from 15-35% across different acceptance rate targets (0.60-1.0)
- Profit maximization shows superior performance across multiple cost-to-utility ratios

## Why This Works (Mechanism)
The algorithm works by modeling each LLM generation as a Pandora's Box problem where opening a box (generating a response) has a random reward but fixed cost. The UCB-style approach maintains confidence bounds on the tail distribution of rewards, allowing the algorithm to estimate when the expected value of stopping exceeds the cost of continuing. The Bradley-Terry inspired transformation normalizes rewards across different prompts, enabling the algorithm to learn universal stopping thresholds rather than prompt-specific ones. By computing fair-cap values through Riemann integration of the tail distribution, the algorithm makes principled decisions about when the best observed reward is good enough to stop.

## Foundational Learning
- **Pandora's Box problem**: A classical optimal stopping problem where each option has a random reward and fixed cost; needed for the theoretical framework of when to stop sampling
- **UCB (Upper Confidence Bound)**: A bandit algorithm that balances exploration and exploitation using confidence intervals; needed for adaptive learning without prior distribution knowledge
- **Shifted exponential distribution**: A heavy-tailed distribution used to model the upper tail of rewards; needed for accurate estimation of stopping thresholds
- **Fair-cap computation**: The expected value of max(v-τ,0) equals cost c; needed for principled stopping criteria based on cost-benefit analysis
- **Bradley-Terry transformation**: A normalization technique for ranking and preference data; needed to handle reward scaling across different prompts

## Architecture Onboarding
**Component map**: LLM generator -> Reward model -> Adaptive stopping algorithm -> Output selection
**Critical path**: Generation → Reward evaluation → Tail estimation → Fair-cap computation → Stopping decision
**Design tradeoffs**: The algorithm trades off between exploration (gathering more samples to better estimate the reward distribution) and exploitation (stopping when current best is good enough). This creates a fundamental tension between accuracy and efficiency.
**Failure signatures**: 
- Premature stopping on difficult prompts with low-reward distributions
- Overly conservative behavior on easy prompts with high-reward distributions  
- Instability in tail estimation when minimum sample threshold is too low
**3 first experiments**:
1. Vary minimum sample threshold t and measure stopping time vs. prompt difficulty
2. Test robustness to different confidence parameters δ in UCB computation
3. Compare performance using different tail distributions (shifted exponential vs. other heavy-tailed distributions)

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on accurate tail estimation of reward distributions, which may be unstable with moderate sample sizes
- Algorithm requires multiple generations per prompt during deployment, limiting applicability for latency-sensitive applications
- Quality of stopping decisions degrades if the reward model has systematic biases or low correlation with human preferences

## Confidence
- Theoretical framework and algorithm design: **High** - Well-established connection to Pandora's Box problem and UCB-style confidence bounds
- Empirical performance gains (15-35% savings): **Medium** - Convincing on tested datasets but may not generalize to different prompt distributions
- Practical applicability for real-world deployment: **Medium-Low** - Theoretical soundness doesn't fully translate to practical barriers like generation cost and reward model quality

## Next Checks
1. Test robustness to different minimum sample thresholds by running with t=5, 10, 15 samples and measuring performance degradation on prompts with varying reward distributions
2. Evaluate performance when using different reward models (human preference scores vs. model-based rewards) to assess sensitivity to reward model quality
3. Benchmark against alternative stopping criteria like confidence interval width on mean reward or probability of improvement over current best