---
ver: rpa2
title: 'RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement
  Learning in Ride-Hailing'
arxiv_id: '2512.13727'
source_url: https://arxiv.org/abs/2512.13727
tags:
- matching
- experts
- pickup
- demand
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce RAST-MoE-RL, a regime-aware reinforcement learning
  framework for adaptive delayed matching in ride-hailing. By formalizing the problem
  as a Regime-Aware Spatio-Temporal MDP (RAST-MDP) and equipping the policy with a
  self-attention Mixture-of-Experts (MoE) encoder, we enable specialization across
  heterogeneous demand-supply regimes.
---

# RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing

## Quick Facts
- arXiv ID: 2512.13727
- Source URL: https://arxiv.org/abs/2512.13727
- Reference count: 40
- Achieves 13% improvement in total reward with 10% and 15% reductions in matching and pickup delays

## Executive Summary
RAST-MoE-RL introduces a regime-aware reinforcement learning framework for adaptive delayed matching in ride-hailing systems. The framework formalizes the problem as a Regime-Aware Spatio-Temporal MDP (RAST-MDP) and incorporates a self-attention Mixture-of-Experts (MoE) encoder to enable specialization across heterogeneous demand-supply regimes. A physics-informed congestion surrogate and adaptive reward scheme further enhance realism and training stability. Tested on real-world Uber data from San Francisco, the model demonstrates significant improvements over baselines while using only 12M parameters.

## Method Summary
The RAST-MoE-RL framework addresses ride-hailing dispatch challenges by modeling the problem as a regime-aware spatio-temporal Markov Decision Process. The core innovation is a self-attention Mixture-of-Experts encoder that learns to specialize across different demand-supply regimes. The framework incorporates a physics-informed congestion surrogate to model traffic dynamics efficiently and an adaptive reward scheme to improve training stability. The model was trained and evaluated on Uber data from San Francisco, demonstrating superior performance compared to strong baselines.

## Key Results
- 13% improvement in total reward compared to strong baselines
- 10% reduction in matching delay
- 15% reduction in pickup delay
- Achieves superior performance using only 12M parameters

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to adapt to heterogeneous demand-supply regimes through the MoE architecture. The self-attention mechanism allows the model to dynamically weigh different expert networks based on the current spatio-temporal context, enabling specialized responses to varying traffic and demand patterns. The physics-informed congestion surrogate provides realistic traffic modeling without the computational overhead of full simulation, while the adaptive reward scheme helps stabilize training across different operational conditions.

## Foundational Learning
- **Regime-Aware Spatio-Temporal MDP**: Needed to capture the dynamic and heterogeneous nature of ride-hailing environments; Quick check: Verify state transitions properly encode spatial and temporal dependencies
- **Mixture-of-Experts Architecture**: Required for specialization across different operational regimes; Quick check: Confirm gating network effectively routes inputs to appropriate experts
- **Physics-Informed Congestion Modeling**: Essential for realistic traffic simulation without excessive computation; Quick check: Validate surrogate predictions against real traffic data
- **Adaptive Reward Schemes**: Necessary for stable reinforcement learning across varying conditions; Quick check: Monitor reward stability during training across different demand patterns

## Architecture Onboarding

**Component Map**: State Encoder -> MoE Router -> Expert Networks -> Value Estimator -> Policy Network

**Critical Path**: Raw spatio-temporal data → State encoding → MoE gating → Expert specialization → Value/policy output

**Design Tradeoffs**: 
- MoE provides specialization but increases parameter complexity
- Physics-informed surrogate sacrifices some accuracy for computational efficiency
- Adaptive rewards improve stability but may complicate credit assignment

**Failure Signatures**:
- Poor gating decisions leading to inappropriate expert selection
- MoE experts failing to specialize, reducing to average performance
- Congestion surrogate inaccuracies causing suboptimal dispatch decisions

**3 First Experiments**:
1. Validate MoE gating network learns meaningful regime distinctions on held-out data
2. Test individual expert network performance on regime-specific subsets
3. Compare physics-informed congestion surrogate accuracy against full simulation

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Trained and tested only on Uber data from San Francisco, limiting generalizability
- Physics-informed congestion surrogate may not capture all real-world traffic dynamics
- Performance improvements lack confidence intervals or statistical significance testing
- Claims about robustness to unseen demand patterns are not empirically demonstrated

## Confidence

**High confidence**: The architectural design and implementation of the RAST-MoE-RL framework, including the MoE encoder and adaptive reward scheme

**Medium confidence**: The reported performance improvements on the San Francisco dataset

**Low confidence**: Claims about generalizability to other cities and robustness to unseen demand patterns

## Next Checks
1. Conduct cross-city validation using ride-hailing data from multiple urban areas with different characteristics (e.g., New York, London, Tokyo) to assess generalizability
2. Implement statistical significance testing with confidence intervals for all reported performance metrics to quantify the reliability of the improvements
3. Design controlled experiments with synthetic demand patterns that differ significantly from the training data to rigorously test robustness claims