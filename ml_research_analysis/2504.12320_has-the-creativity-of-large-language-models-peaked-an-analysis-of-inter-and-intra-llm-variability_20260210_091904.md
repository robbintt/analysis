---
ver: rpa2
title: Has the Creativity of Large-Language Models peaked? An analysis of inter- and
  intra-LLM variability
arxiv_id: '2504.12320'
source_url: https://arxiv.org/abs/2504.12320
tags:
- pholm
- creative
- creativity
- human
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the creative potential of 14 widely used
  large language models (LLMs) using two standardized creativity tests: the Divergent
  Association Task (DAT) and the Alternative Uses Task (AUT). The research examines
  both inter-model differences (performance across models) and intra-model variability
  (consistency within the same model).'
---

# Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability

## Quick Facts
- arXiv ID: 2504.12320
- Source URL: https://arxiv.org/abs/2504.12320
- Reference count: 40
- No evidence of increased creative performance over the past 18-24 months; GPT-4 performing worse than in previous studies

## Executive Summary
This study evaluates the creative potential of 14 widely used large language models using two standardized creativity tests: the Divergent Association Task (DAT) and the Alternative Uses Task (AUT). The research examines both inter-model differences (performance across models) and intra-model variability (consistency within the same model). Key findings include: no evidence of increased creative performance over the past 18-24 months, with GPT-4 performing worse than in previous studies; all models performed on average better than the average human on the AUT, but only 0.28% of responses reached the top 10% of human creativity benchmarks; substantial intra-model variability, where the same LLM can produce outputs ranging from below-average to original; and performance differences influenced by prompt design, with some models performing better when aware of the test context.

## Method Summary
The study evaluated 14 LLMs using LiteLLM for unified API access, conducting 100 trials per model × 2 prompt conditions for DAT (aware/unaware), and 4 trials × 100 responses each for AUT (practical/creative) across 16 common objects. DAT scores were submitted to the official DAT website for semantic distance calculation, while AUT originality scores were obtained via OpenScoring API against human benchmarks (n=8,907 for DAT, n=151 for AUT). All testing occurred February 25–March 3, 2025 using default API parameters without carryover effects between sessions.

## Key Results
- GPT-4o performed worse on DAT than GPT-4 in 2023 (d = 2.04-2.41), showing no improvement trend over 18-24 months
- All models scored above average human on AUT, but only 0.28% of responses reached top 10% of human creativity benchmarks
- Intra-model variability exceeded inter-model differences, with the same model producing outputs ranging from below-average to original
- Prompt framing significantly affected performance, with Claude 3.5 and Grok 2 improving when told they were taking a creativity test (d > 1.0)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intra-model variability in creative output stems from stochastic sampling interacting with semantic space structure
- Mechanism: LLMs generate creative responses via probabilistic token selection. Even with identical prompts, the high-dimensional semantic space combined with default temperature settings produces output variance ranging from below-average to original. This variability exceeds that seen in structured tasks because divergent thinking tasks invite broader exploration with lower constraints
- Core assumption: Default API temperature and sampling parameters drive meaningful variance in creative tasks (not tested directly in study)
- Evidence anchors:
  - [abstract] "substantial intra-model variability: the same LLM, given the same prompt, can produce outputs ranging from below-average to original"
  - [section 5, Discussion] "The variability we observed exceeds that previously reported in LLM evaluations focused on structured or closed-ended tasks"
- Break condition: If temperature/sampling parameters are systematically controlled or set to deterministic values, intra-model variability should substantially decrease

### Mechanism 2
- Claim: Prompt framing effects on creativity are model-specific, reflecting differential sensitivity to task context encoding
- Mechanism: Some models (Claude 3.5, Grok 2) showed large performance gains when explicitly told they were taking a creativity test (DAT-aware vs. unaware, Cohen's d > 1.0), while others (DeepSeek R1 Distill) performed worse. This suggests that how models encode task intent interacts with their training/alignment to affect creative output strategies
- Core assumption: The observed effects reflect genuine differences in how models process task framing rather than random noise
- Evidence anchors:
  - [abstract] "performance differences influenced by prompt design, with some models performing better when aware of the test context"
  - [section 3.2.1] Two prompt variants: DAT-aware vs. DAT-unaware, testing whether disclosure affects performance
- Break condition: If the framing effect is purely noise, repeated experiments with larger sample sizes should show no consistent model-specific patterns

### Mechanism 3
- Claim: LLM creativity plateau reflects training data saturation and benchmark memorization rather than fundamental capability limits
- Mechanism: GPT-4's 2025 DAT performance was substantially worse than 2023 benchmarks (d = 2.04-2.41). The AUT showed more stable performance, possibly because it is widely used in GenAI research and may be overrepresented in training data. Models excel at combinatorial creativity (recombining existing patterns) but rarely produce top-decile original ideas (only 0.28% reached top 10%)
- Core assumption: AUT's prevalence in training/evaluation pipelines contributes to inflated performance stability
- Evidence anchors:
  - [section 4.1] "GPT-4o performed worse... than GPT-4 in the data from Cropley in 2023, t(200) = 14.46, p < .001, d = 2.04"
  - [section 5, Discussion] "One explanation may be that the AUT, being widely used in GenAI research, is overrepresented in training data or has influenced model optimization"
- Break condition: If new evaluation paradigms outside training distribution show continued improvement, the plateau claim weakens; if novel tasks also show stagnation, structural limits become more plausible

## Foundational Learning

- Concept: Divergent vs. Convergent Thinking
  - Why needed here: The study uses DAT and AUT specifically to measure divergent thinking (generating multiple varied ideas). Understanding this distinction is critical because LLMs may excel at divergent generation while struggling with convergent evaluation/selection
  - Quick check question: Can you explain why measuring only divergent thinking might overestimate overall creative capability?

- Concept: Percentile Benchmarking Against Human Distributions
  - Why needed here: All results are reported as percentiles relative to human baselines (n=8,907 for DAT, n=151 for AUT). Interpreting "better than average human" requires understanding what the 50th percentile represents and why top-decile performance matters differently
  - Quick check question: If an LLM scores at the 70th percentile but 0% of responses reach the 90th percentile, what does this tell you about the distribution of its outputs?

- Concept: Intra-model Variability vs. Inter-model Differences
  - Why needed here: The study's key contribution is documenting that the same model can produce outputs spanning from below-average to exceptional. This is methodologically critical—single-shot evaluations risk dramatic over- or under-estimation of creative capability
  - Quick check question: Why might previous studies using single or few responses have reached unreliable conclusions about LLM creativity?

## Architecture Onboarding

- Component map:
  LiteLLM -> 14 model APIs -> DAT/AUT prompt execution -> Scoring APIs (DAT website, OpenScoring) -> JSON storage with metadata

- Critical path:
  1. Define prompt variants (aware/unaware for DAT; practical/creative for AUT)
  2. Execute N trials per model-object pair with isolated sessions (no carryover effects)
  3. Submit responses to scoring APIs, retrieve scores and percentiles
  4. Compute intra-model variability (within-model SD, range) and inter-model comparisons

- Design tradeoffs:
  - Default API parameters (temperature, sampling) -> Ecological validity vs. controlled reproducibility
  - 100 trials per condition -> Statistical power vs. API cost/time
  - Two prompt variants only -> Manageable scope vs. comprehensive prompt space coverage
  - English-only tasks -> Comparability with existing benchmarks vs. multilingual capability assessment

- Failure signatures:
  - Models generating fewer than requested responses (noted especially for lower-capacity models in AUT)
  - Responses below 50th percentile despite model reputation (e.g., GPT-4.5 had 6/100 responses below human average in DAT-aware)
  - Large SD relative to mean indicates unstable creative output (e.g., DeepSeek R1 Distill: M=22.91, SD=23.70)

- First 3 experiments:
  1. Replicate a single model's DAT with 500 trials to characterize the full output distribution and identify what fraction of responses fall below human median
  2. Systematically vary temperature (0.0, 0.5, 1.0, 1.5) on a fixed model-task pair to quantify the causal effect on intra-model variance and mean creative score
  3. Design a novel divergent thinking task not present in existing benchmarks and compare LLM performance to AUT to test whether AUT performance is inflated by training data familiarity

## Open Questions the Paper Calls Out

- How do temperature and sampling parameter variations systematically influence LLM performance on divergent thinking tasks?
  - Basis in paper: [explicit] The authors acknowledge using default API settings and state: "Future research should systematically vary such parameters to explore their effects on divergent thinking performance"
  - Why unresolved: This study isolated performance at default settings, leaving the causal link between decoding randomness and the originality/variability of creative output untested
  - What evidence would resolve it: A controlled experiment benchmarking DAT and AUT scores across a matrix of temperature and top-p values for multiple models

- Do LLMs exhibit consistent creative potential and intra-model variability across non-English languages?
  - Basis in paper: [explicit] The authors list the focus on English-language tasks as a limitation, noting that "LLMs may behave differently across languages" and "this remains an important area for future exploration"
  - Why unresolved: Current benchmarks are English-centric; it is unclear if the semantic distance metrics and prompt sensitivities observed transfer to multilingual contexts
  - What evidence would resolve it: Replicating the DAT and AUT protocols with localized prompts across a diverse set of languages and comparing the resulting percentile distributions

- Is the observed performance on the Alternative Uses Task (AUT) inflated by the inclusion of similar tasks in model training data?
  - Basis in paper: [inferred] The authors suggest the AUT may be "overrepresented in training data or has influenced model optimization," potentially leading to inflated and stable performance compared to the less common Divergent Association Task (DAT)
  - Why unresolved: High performance might stem from memorization or specific over-fitting rather than generalized divergent thinking capabilities
  - What evidence would resolve it: Testing models on novel, bespoke divergent thinking tasks that are provably absent from public training corpora

## Limitations
- Default API parameters (temperature, sampling) were not controlled, introducing uncertainty about whether observed variability reflects genuine creative instability or artifactual variance from stochastic generation
- Benchmark comparisons rely on human datasets with limited size (n=151 for AUT) and potential temporal mismatch with LLM training data
- The 0.28% top-decile performance finding may reflect ceiling effects in human benchmarks rather than true LLM limitations

## Confidence

- **High confidence**: Intra-model variability exists and exceeds cross-LLM differences (supported by large SDs relative to means across multiple models)
- **Medium confidence**: Performance plateau over 18-24 months (GPT-4 decline is robust, but limited temporal sampling and training data overlap concerns apply)
- **Medium confidence**: Prompt framing effects are model-specific (consistent patterns observed, but small sample of models and prompt variants)
- **Low confidence**: 0.28% top-decile performance represents fundamental capability limit (depends heavily on benchmark quality and potential training data familiarity)

## Next Checks

1. **Parameter control validation**: Run a fixed model-task pair (e.g., GPT-4o on DAT) with systematic temperature variation (0.0, 0.5, 1.0, 1.5) across 100 trials each, measuring the causal effect on intra-model variance and mean scores to quantify how much stochastic parameters drive observed variability

2. **Novel task validation**: Design and validate a divergent thinking task with no presence in training data (e.g., culturally specific or recently created creative challenge), then compare LLM performance on this novel task versus AUT to test whether AUT's stable performance reflects genuine capability or training data familiarity

3. **Temporal stability validation**: Conduct longitudinal monitoring of a representative model (e.g., GPT-4) using identical prompts and parameters at quarterly intervals to distinguish between true performance plateaus versus temporary fluctuations or training data effects