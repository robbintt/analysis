---
ver: rpa2
title: Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning
arxiv_id: '2505.19213'
source_url: https://arxiv.org/abs/2505.19213
tags:
- medical
- reasoning
- open-ended
- close-ended
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedCCO, the first curriculum-guided reinforcement
  learning framework for medical VQA that unifies close-ended and open-ended tasks.
  The method first fine-tunes a VLM on close-ended VQA tasks, then progressively adapts
  it to open-ended tasks using a two-stage GRPO-based reinforcement learning approach
  with refined data consistency.
---

# Improving Medical Reasoning with Curriculum-Aware Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.19213
- Source URL: https://arxiv.org/abs/2505.19213
- Reference count: 40
- Primary result: MedCCO achieves 11.4% gains on in-domain medical VQA tasks and 5.7% on out-of-domain tasks using curriculum-guided reinforcement learning

## Executive Summary
This paper introduces MedCCO, the first curriculum-guided reinforcement learning framework for medical visual question answering (VQA) that unifies close-ended and open-ended tasks. The method first fine-tunes a visual language model on close-ended VQA tasks, then progressively adapts it to open-ended tasks using a two-stage GRPO-based reinforcement learning approach with refined data consistency. The curriculum strategy improves stability and mitigates reward conflicts between task types. Experiments on eight benchmarks show MedCCO achieves state-of-the-art performance and demonstrates strong cross-modal generalization.

## Method Summary
MedCCO implements a three-stage pipeline: (1) VQA-consistency refinement using a 72B model to clean open-ended VQA pairs, (2) Stage 1 GRPO fine-tuning on close-ended data with binary accuracy rewards, and (3) Stage 2 GRPO fine-tuning on refined open-ended data using hybrid rewards (BLEU-1, ROUGE-1, BERTScore). The curriculum-based sequential training first stabilizes on simpler close-ended tasks before adapting to harder open-ended tasks, using KL regularization to preserve close-ended capabilities. The framework employs rule-based rewards instead of learned reward models, computing group-relative advantages from multiple verifiable metrics.

## Key Results
- Achieves 11.4% gains on in-domain medical VQA tasks compared to baseline
- Demonstrates 5.7% improvements on out-of-domain medical VQA tasks
- Sets new state-of-the-art performance across eight medical VQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Curriculum-based sequential training reduces gradient conflicts
Sequential training on close-ended → open-ended tasks improves final performance compared to joint training by avoiding gradient imbalance. Close-ended tasks use discrete binary rewards with sharper gradients, while open-ended tasks use continuous semantic similarity rewards. The curriculum allows the policy to stabilize on simpler close-ended tasks first, then progressively adapt to harder open-ended tasks without disrupting acquired capabilities. The KL penalty (β=0.01) assumes sufficient retention of close-ended reasoning during open-ended fine-tuning.

### Mechanism 2: Rule-based multi-reward eliminates reward model dependencies
Verifiable rule-based rewards (accuracy, BLEU-1, ROUGE-1, BERTScore) substitute for learned reward models while maintaining training stability. GRPO computes group-relative advantages from rule-based scores, normalizing across 10 candidate responses per sample. This removes the need for a separately trained reward model while providing stable learning signals through relative comparison. The core assumption is that reward functions adequately capture medical reasoning quality through semantic similarity metrics.

### Mechanism 3: VQA consistency refinement stabilizes open-ended RL
Pre-processing open-ended VQA pairs with a VQA-Consistency Auditor improves training efficiency and stability. The auditor (Qwen2.5-VL-72B) enforces consistency between question scope and answer specificity, granularity matching, and open-ended phrasing. This reduces ambiguous learning signals during RL by addressing granularity mismatches in medical VQA datasets. The assumption is that the auditor model's judgments align with clinical reasoning requirements.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Core RL algorithm replacing PPO; operates without a learned value function by computing advantages relative to group samples
  - Quick check question: Can you explain how GRPO's group-relative advantage normalization differs from PPO's value-function-based advantage estimation?

- **Concept: Reward engineering for structured outputs**
  - Why needed here: Understanding the multi-component reward (correctness + semantic alignment + format) is essential for debugging training instability
  - Quick check question: Given λ=0.7 and γ=0.8, what is the effective weight of BERTScore in the final reward for an open-ended response?

- **Concept: Curriculum learning principles**
  - Why needed here: The paper's central contribution relies on task-ordering effects; understanding when/why curriculum helps is critical for extension
  - Quick check question: What are two failure modes where curriculum learning would not help compared to joint training?

## Architecture Onboarding

- **Component map:** VQA consistency refinement (72B auditor) → Stage 1 GRPO on close-ended → Stage 2 GRPO on open-ended → Structured output parsing

- **Critical path:**
  1. Verify base VLM (Qwen2.5-VL-7B-Instruct) responds to medical prompts
  2. Run VQA consistency refinement on training data → check refinement logs
  3. Train Stage 1 (close-ended) until validation accuracy plateaus
  4. Train Stage 2 (open-ended) with KL penalty monitoring
  5. Evaluate on in-domain (VQA-RAD, SLAKE, PathVQA) and out-of-domain (PMC-VQA, MedXpertQA, Quilt-VQA, MMMU) benchmarks

- **Design tradeoffs:**
  - Sequential vs. joint training: Sequential avoids gradient conflicts but requires 2× training time
  - Rule-based vs. learned rewards: Rule-based is simpler but may miss nuanced clinical reasoning
  - 3B vs. 7B backbone: 7B gives +5.2% out-of-domain gain but requires more GPU memory

- **Failure signatures:**
  - Training loss oscillates wildly → check reward normalization, increase group size G
  - Open-ended responses become repetitive/short → reward may be gaming format without semantic content; inspect Ropen vs Rformat balance
  - Close-ended performance drops during open-ended fine-tuning → increase KL penalty β
  - Refined questions become too narrow → auditor over-specifying; review refinement criteria

- **First 3 experiments:**
  1. **Reproduction baseline:** Train MedCCO-3B on VQA-RAD only; verify ~64% close-ended accuracy matches Table 4
  2. **Curriculum vs. joint ablation:** Compare sequential (close→open) vs. joint GRPO on SLAKE; expect +2-4% from curriculum
  3. **VQA refinement impact:** Train with vs. without refinement on PathVQA open-ended; expect +2-3% on out-of-domain benchmarks per Figure 3

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we develop automated methods to evaluate the clinical quality of reasoning chains in medical VLMs beyond surface-level lexical and semantic similarity metrics?
- **Basis in paper:** [explicit] "Moreover, automatically evaluating the quality of reasoning remains an open problem."
- **Why unresolved:** Current open-ended rewards rely on BLEU-1, ROUGE-1, and BERTScore, which capture lexical and semantic overlap but not factual clinical accuracy or logical coherence of medical reasoning.
- **What evidence would resolve it:** A validated evaluation framework correlating automated reasoning quality scores with expert clinician assessments across diverse medical scenarios.

### Open Question 2
- **Question:** How can reinforcement learning approaches effectively mitigate hallucinations in medical VLM reasoning while maintaining flexibility?
- **Basis in paper:** [explicit] "Enabling VLMs to perform accurate reasoning remains difficult due to limitations in visual perception, domain-specific knowledge, and the tendency to generate hallucinations."
- **Why unresolved:** RL-based training encourages exploration but may amplify hallucinations; the trade-off between generative flexibility and factual grounding remains unaddressed.
- **What evidence would resolve it:** Demonstration of RL training strategies that reduce hallucination rates (measured via expert annotation) without degrading open-ended reasoning performance.

### Open Question 3
- **Question:** Does the curriculum-based RL approach generalize to more than two stages or to alternative task orderings?
- **Basis in paper:** [inferred] The paper explores only a two-stage curriculum (close-ended → open-ended) and one joint-training alternative. No exploration of multi-stage curricula, dynamic difficulty adjustment, or reverse ordering is conducted.
- **Why unresolved:** The optimal curriculum structure for medical VQA tasks is unknown; it is unclear whether gains come specifically from the task ordering or simply from staged training.
- **What evidence would resolve it:** Systematic ablation across multiple curriculum configurations (e.g., three-stage, difficulty-ranked mixing, reverse order) showing consistent or superior performance.

### Open Question 4
- **Question:** Can clinically-grounded reward functions (e.g., entity-level accuracy, diagnostic correctness) outperform lexical-semantic metrics for open-ended medical VQA?
- **Basis in paper:** [inferred] The hybrid reward Ropen combines BLEU-1, ROUGE-1, and BERTScore, but these metrics may reward plausible-sounding yet clinically incorrect answers. The paper does not explore medical-knowledge-aware reward designs.
- **Why unresolved:** Medical correctness requires domain-specific validation (e.g., matching medical entities, correct diagnostic reasoning) that surface-level metrics fail to capture.
- **What evidence would resolve it:** Comparative experiments with clinically-informed reward functions (using medical knowledge bases or expert-validated parsers) demonstrating improved diagnostic accuracy on held-out benchmarks.

## Limitations

- The 72B auditor model creates a scalability bottleneck for broader adoption; no ablation studies on auditor quality vs. model size
- Rule-based rewards may not adequately capture clinical accuracy; no clinical validation that high scores correspond to clinically correct answers
- Curriculum approach requires 2× training time compared to joint training, though efficiency tradeoffs are not quantified

## Confidence

- **High confidence:** Sequential curriculum training improves performance over joint training
- **Medium confidence:** Rule-based rewards can substitute for learned reward models
- **Medium confidence:** VQA-consistency refinement improves stability

## Next Checks

1. **Clinical validation study:** Have medical experts evaluate a subset of MedCCO-generated answers against ground truth to verify that high semantic similarity scores correlate with clinical accuracy, particularly for open-ended responses where hallucination risk is highest.

2. **Ablation on auditor quality:** Replace the 72B auditor with progressively smaller models (7B, 3B) to determine the minimum model size required for effective VQA-consistency refinement, quantifying the tradeoff between refinement quality and computational cost.

3. **Long-term retention evaluation:** After open-ended fine-tuning, measure close-ended performance decay over time with different KL penalty values (β=0.01, 0.05, 0.1) to determine if the model truly retains both capability types or if performance gradually degrades.