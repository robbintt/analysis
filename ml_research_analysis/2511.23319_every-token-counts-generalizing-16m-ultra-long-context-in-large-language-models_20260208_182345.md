---
ver: rpa2
title: 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language
  Models'
arxiv_id: '2511.23319'
source_url: https://arxiv.org/abs/2511.23319
tags:
- context
- attention
- arxiv
- length
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building machines with long-term
  memory by framing it as efficient ultra-long context modeling. The authors argue
  that this requires sparsity, random-access flexibility, and length generalization.
---

# Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models

## Quick Facts
- arXiv ID: 2511.23319
- Source URL: https://arxiv.org/abs/2511.23319
- Reference count: 40
- Primary result: HSA-UltraLong achieves >90% retrieval accuracy on 16M-token contexts while maintaining in-domain performance

## Executive Summary
This paper introduces Hierarchical Sparse Attention (HSA) to enable efficient ultra-long context modeling in large language models. The authors address the challenge of building machines with long-term memory by creating a sparse attention mechanism that provides both sparsity and random-access flexibility while generalizing to contexts up to 16 million tokens. HSA-UltraLong, an 8B-parameter MoE model trained on over 8 trillion tokens, demonstrates that the approach can match full-attention baselines on standard tasks while achieving strong performance on ultra-long context retrieval.

## Method Summary
The authors propose Hierarchical Sparse Attention (HSA), a novel attention mechanism that uses chunk-wise attention with retrieval-score weighted fusion. The architecture combines sliding window attention (SWA) for local context with HSA for global retrieval. HSA learns to select relevant chunks from past contexts using dot products with chunk landmarks, then performs attention on each chunk independently before fusing outputs weighted by retrieval scores. The model uses a hybrid positional encoding strategy: NoPE for HSA layers and RoPE for SWA. Training involves a 5-stage process starting with warm-up using small SWA windows, followed by pretraining, long-context mid-training, annealing, and SFT fine-tuning.

## Key Results
- HSA-UltraLong achieves >90% accuracy on most NIAH in-context retrieval tasks with 16M tokens
- The model performs comparably to full-attention baselines on in-domain context lengths
- Retrieval accuracy degrades progressively with longer contexts when training data lacks sufficient long-range examples
- HSA learns a form of retrieval-based extrapolation, with training on longer effective contexts (>32K) substantially improving performance

## Why This Works (Mechanism)

### Mechanism 1
Retrieval-score-weighted fusion enables end-to-end learnable chunk selection. Each token computes dot products with landmark representations of past chunks to generate retrieval scores, selects top-k chunks, performs attention with each chunk independently, then fuses outputs weighted by softmax-normalized retrieval scores—this integrates retrieval directly into the forward pass, allowing gradient updates to optimize chunk selection for next-token prediction. The core assumption is that the model can learn to assign higher retrieval scores to chunks that are causally relevant for prediction.

### Mechanism 2
NoPE in HSA layers combined with RoPE in sliding window attention supports length extrapolation. Sliding-window attention's KV cache employs RoPE for local precision, while HSA uses No Positional Encoding to avoid position-based attention decay that would penalize distant tokens—this hybrid approach preserves short-range modeling while enabling generalization beyond training lengths. The core assumption is that positional encoding introduces length-dependent biases that harm extrapolation to unseen context lengths.

### Mechanism 3
Chunk-wise attention with retrieval training on short contexts transfers to long contexts. HSA learns retrieval patterns on manageable context lengths (e.g., 16K-32K tokens); because the retrieval mechanism operates on chunk landmarks rather than absolute positions, the learned selection behavior generalizes to much longer sequences (up to 16M tokens) without retraining. The core assumption is that the retrieval skill learned on short sequences is structurally similar to retrieval on long sequences.

## Foundational Learning

- **Concept: Sliding Window Attention (SWA)**
  - Why needed here: HSA-UltraLong uses SWA for local information retrieval while HSA handles global retrieval; understanding the interaction between these two mechanisms is critical for diagnosing the "seesaw problem" where larger SWA windows can weaken HSA's learning
  - Quick check question: Can you explain why a 4K sliding window might prevent HSA from learning useful short-range retrieval patterns?

- **Concept: Mixture of Experts (MoE) routing**
  - Why needed here: The paper explicitly draws an analogy between HSA's top-k chunk retrieval and MoE's top-k expert routing; understanding how MoE routers receive gradients through sparse selection helps explain how HSA's retrieval scores remain learnable
  - Quick check question: How does the top-k selection mechanism in MoE allow gradient flow to the router despite being a discrete operation?

- **Concept: Query-Key Normalization**
  - Why needed here: The paper explicitly states that QK normalization is "very important for the stability of HSA in practical trillion-token scale training"—this is a practical implementation detail that prevents attention score instability during long training runs
  - Quick check question: What specific numerical instability does QK normalization prevent in scaled dot-product attention?

## Architecture Onboarding

- **Component map:**
  Lower decoder (L/2 layers) -> Standard Transformer with Sliding Window Attention (4K window) -> Produces H_{L/2} -> H_{L/2} chunked -> Bi-directional encoder -> Chunk summaries E[i] and landmarks L_i -> Upper decoder HSA layers: query retrieves top-k chunks via landmarks, attends to each chunk separately, fuses by retrieval scores -> MoE FFN processes with expert routing

- **Critical path:**
  1. Input → Lower decoder processes with SWA → produces H_{L/2}
  2. H_{L/2} chunked → bi-directional encoder → landmarks + KV cache
  3. Upper decoder HSA layers: query retrieves top-k chunks via landmarks, attends to each chunk separately, fuses by retrieval scores
  4. MoE FFN processes with expert routing

- **Design tradeoffs:**
  - SWA window size: Larger (4K) preserves in-domain performance but can weaken HSA learning; smaller (512) improves extrapolation but may hurt downstream tasks
  - Top-k value: Must be tuned during training phases—starts large (covers full sequence) during warmup, then reduced
  - Chunk size (default 64): Aligned to hardware; smaller chunks increase retrieval granularity but raise compute overhead

- **Failure signatures:**
  - Extrapolation collapse: Model performs well at training length but degrades rapidly beyond—likely caused by insufficient warmup or training data with short effective context length
  - In-domain degradation: Short-context benchmarks suffer—likely caused by too-small SWA window or excessive HSA reliance
  - HSA/SWA seesaw: After SFT on short sequences, long-context retrieval degrades—caused by SWA absorbing short-range patterns, leaving HSA undertrained

- **First 3 experiments:**
  1. **Warmup ablation:** Train identical models with and without the short-SWA/full-HSA warmup phase; evaluate on both in-domain (PG19 perplexity) and extrapolation (MQ-NIAH at 1M+ tokens) to validate the warmup necessity
  2. **SWA window sweep:** Test SWA window sizes [512, 1K, 2K, 4K] with fixed HSA configuration; plot in-domain perplexity vs. 16M retrieval accuracy to characterize the seesaw tradeoff
  3. **Effective context length analysis:** Train on datasets with controlled effective context lengths (4K, 16K, 32K, 64K); measure extrapolation accuracy vs. training effective context length to validate the data scaling hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
How can the "seesaw effect" between Sliding Window Attention (SWA) and Hierarchical Sparse Attention (HSA) be mitigated so that large SWA windows do not degrade the model's ability to learn short-range dependencies necessary for long-range generalization? The authors identify the trade-off where large windows handle short dependencies, starving HSA of meaningful gradients, but did not propose a structural fix.

### Open Question 2
Can kernel-level optimizations alleviate the 16:1 query-to-key-value head ratio constraint in HSA to reduce the information bottleneck? Current implementation faces hardware constraints limiting the attention head configuration.

### Open Question 3
Can HSA be optimized to match or exceed FlashAttention-3 efficiency on short sequences? Current HSA kernels incur more memory accesses than FlashAttention-3 on short sequences due to sparsity overhead.

## Limitations

- The paper relies heavily on retrieval accuracy metrics rather than comprehensive task-based validation across diverse ultra-long context scenarios
- Model evaluation was primarily on English-language benchmarks with limited testing on multilingual contexts or real-world applications requiring 16M tokens of continuous context
- Implementation details remain underspecified, particularly regarding the bi-directional encoder for chunk representations and exact MoE routing mechanisms

## Confidence

**High Confidence Claims:**
- HSA architecture is implementable and can be trained on trillion-token scales
- The model achieves strong retrieval accuracy (>90%) on NIAH tasks at 16M tokens
- Warm-up training phase with smaller SWA windows is necessary for successful HSA learning

**Medium Confidence Claims:**
- HSA provides better extrapolation than full attention baselines when context exceeds training lengths
- The hybrid positional encoding strategy (NoPE for HSA, RoPE for SWA) effectively supports length generalization
- Chunk-wise attention with retrieval training transfers from short to long contexts

**Low Confidence Claims:**
- HSA consistently outperforms full attention across all ultra-long context applications
- The retrieval-score-weighted fusion mechanism is optimal compared to alternative sparse attention designs
- The specific architectural choices (chunk size 64, top-k 64) are universally optimal

## Next Checks

1. **Downstream Task Validation at Ultra-Long Contexts:** Evaluate HSA-UltraLong on practical tasks requiring continuous 16M-token contexts (e.g., extended document analysis, multi-document question answering, or code analysis across entire codebases). Measure not just retrieval accuracy but actual task completion quality to validate that high NIAH scores translate to practical utility.

2. **Ablation Study on Positional Encoding Strategy:** Systematically compare different positional encoding configurations: full NoPE, full RoPE, and the hybrid approach. Measure both retrieval accuracy and task performance across context lengths to quantify the benefit of the NoPE/RoPE combination and identify potential boundary effects between HSA and SWA layers.

3. **Robustness Testing Across Domain Shifts:** Test HSA-UltraLong on ultra-long contexts from domains not represented in training (e.g., scientific literature, legal documents, or multilingual corpora). Evaluate whether the retrieval mechanism generalizes beyond the web/text/code mixture used in training, and identify domain-specific failure modes that might limit practical deployment.