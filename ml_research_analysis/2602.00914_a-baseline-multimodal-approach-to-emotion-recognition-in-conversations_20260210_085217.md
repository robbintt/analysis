---
ver: rpa2
title: A Baseline Multimodal Approach to Emotion Recognition in Conversations
arxiv_id: '2602.00914'
source_url: https://arxiv.org/abs/2602.00914
tags:
- emotion
- recognition
- multimodal
- audio
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a lightweight multimodal baseline for emotion
  recognition in conversations using the SemEval-2024 Task 3 dataset. The approach
  combines transformer-based text classifiers (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa)
  with a self-supervised speech representation model (Wav2Vec2), employing simple
  late-fusion ensemble.
---

# A Baseline Multimodal Approach to Emotion Recognition in Conversations

## Quick Facts
- arXiv ID: 2602.00914
- Source URL: https://arxiv.org/abs/2602.00914
- Reference count: 40
- This study presents a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset. The approach combines transformer-based text classifiers (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa) with a self-supervised speech representation model (Wav2Vec2), employing simple late-fusion ensemble. Under a limited training protocol, the ensemble model achieved 62.97% accuracy, outperforming unimodal text models (RoBERTa: 50.68%) and audio models (Wav2Vec2: 35.43%). The work documents an accessible reference implementation and highlights the benefits of multimodal fusion, while acknowledging limitations including lightweight evaluation, limited tuning, and domain-specific dataset constraints.

## Executive Summary
This paper introduces a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset. The approach combines transformer-based text classifiers with a self-supervised speech representation model through simple late-fusion ensemble. Under a limited training protocol, the ensemble model achieved 62.97% accuracy, outperforming unimodal text models (RoBERTa: 50.68%) and audio models (Wav2Vec2: 35.43%). The work documents an accessible reference implementation and highlights the benefits of multimodal fusion, while acknowledging limitations including lightweight evaluation, limited tuning, and domain-specific dataset constraints.

## Method Summary
The method employs a late-fusion ensemble combining four text models (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa) with three audio models (HuBERT, Wav2Vec2, Wav2Vec2-large-robust-12-ft-emotion-msp-dim). Each model is fine-tuned independently on the SemEval-2024 Task 3 dataset, which consists of text utterances and corresponding audio clips from the Friends sitcom. The ensemble combines the RoBERTa text model with the Wav2Vec2 audio model through simple probability averaging. Audio is standardized to 16kHz sampling rate, and the dataset is split 80/20 for training and testing. The best unimodal performance is achieved by RoBERTa (50.68% accuracy), while the multimodal ensemble reaches 62.97% accuracy.

## Key Results
- Ensemble model achieved 62.97% accuracy, significantly outperforming individual unimodal models
- RoBERTa text model achieved 50.68% accuracy, the highest among text models
- Wav2Vec2 audio model achieved 35.43% accuracy, outperforming other audio models
- Simple late-fusion ensemble of RoBERTa and Wav2Vec2 provides substantial performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A late-fusion ensemble of text and audio models yields higher accuracy than unimodal approaches.
- **Mechanism:** The architecture decouples feature extraction. The text branch (RoBERTa) captures semantic context, while the audio branch (Wav2Vec2) captures paralinguistic features like tone and pitch. By combining predictions at the decision level, the ensemble mitigates errors where one modality is ambiguous (e.g., sarcasm often contradicts text).
- **Core assumption:** The modalities contain complementary information; errors in one branch are uncorrelated with errors in the other.
- **Evidence anchors:**
  - [abstract]: "The multimodal ensemble model achieved 62.97% accuracy, significantly outperforming individual unimodal models."
  - [section]: Section 2.3.1 defines late fusion as processing modalities independently and combining predictions to improve robustness.
  - [corpus]: Neighbors like "Centering Emotion Hotspots" validate that fusing text, audio, and video is a standard effective strategy, though they propose more complex alignment methods than the simple late fusion here.
- **Break condition:** If modalities are misaligned temporally or if one modality contains strictly noise (low SNR), simple late fusion may degrade performance compared to a robust single modality.

### Mechanism 2
- **Claim:** Textual transformer models (specifically RoBERTa) provide a stronger signal for emotion recognition than the audio models tested in this specific dataset.
- **Mechanism:** RoBERTa utilizes bidirectional attention to capture long-range semantic dependencies and context within the "Friends" dialogue scripts. This appears more discriminative for the labeled emotions than the acoustic features extracted from the audio clips.
- **Core assumption:** The dataset labels are more semantically aligned with the dialogue content than vocal expression.
- **Evidence anchors:**
  - [abstract]: "Among text models, RoBERTa achieved the highest accuracy (50.68%) while Wav2Vec2 outperformed audio models (35.43%)."
  - [section]: Section 3.4.3 shows the ensemble (62.97%) lifts the text baseline (50.68%) significantly, implying text carried a heavy weight or audio corrected key errors.
  - [corpus]: Corpus papers (e.g., "GatedxLSTM") suggest text often dominates multimodal tasks, though advanced audio processing (like in "Qieemo") can close the gap in specific contexts.
- **Break condition:** If the conversation relies heavily on irony where text semantics are misleading, the text-centric model would fail without a strong audio corrective signal.

### Mechanism 3
- **Claim:** Self-supervised speech representations (Wav2Vec2) provide a robust baseline for audio emotion recognition without extensive labeled data.
- **Mechanism:** Wav2Vec2 pre-trains on massive unlabeled audio to learn latent representations of speech waveforms. Fine-tuning these weights allows the model to detect subtle vocal cues (stress, intonation) that traditional hand-crafted features (MFCCs) might miss.
- **Core assumption:** The pre-training domain of Wav2Vec2 transfers effectively to the specific acoustic environment of the "Friends" dataset.
- **Evidence anchors:**
  - [section]: Section 2.2.2 explicitly contrasts Wav2Vec 2.0 with traditional MFCC/HMM approaches, noting its ability to capture "subtle vocal cues... often overlooked by text-only systems."
  - [section]: Section 3.4.2 reports Wav2Vec2 achieved 35.43% accuracy, outperforming HuBERT (31.08%).
  - [corpus]: Weak corpus evidence for Wav2Vec2's specific superiority over *all* audio features in this exact paper context; the paper itself frames it as a "baseline" comparison against other deep models (HuBERT).
- **Break condition:** Performance relies on fine-tuning; without it, the raw embeddings may not map linearly to emotion classes.

## Foundational Learning

- **Concept: Late Fusion**
  - **Why needed here:** This is the structural glue of the paper's top-performing model. You must understand that late fusion averages predictions *after* the models have run independently, rather than mixing raw data (early fusion).
  - **Quick check question:** If RoBERTa predicts "Joy" with 0.9 confidence and Wav2Vec2 predicts "Sadness" with 0.6 confidence, how would a simple averaging late-fusion model likely classify this?

- **Concept: Self-Supervised Learning (SSL) for Audio**
  - **Why needed here:** Wav2Vec2 is an SSL model. Understanding that it learns to reconstruct masked audio segments allows you to grasp why it can work on the SemEval dataset despite limited labeled training data.
  - **Quick check question:** Why might a model pre-trained on raw radio speech (Wav2Vec2) need fine-tuning to detect specific emotions like "anger" in a sitcom?

- **Concept: The "Baseline" Constraint**
  - **Why needed here:** The authors explicitly disclaim novelty. Understanding this prevents over-interpreting the 62.97% result as a state-of-the-art breakthrough; it is a reference point for future work.
  - **Quick check question:** The paper lists "Limited tuning budget" as a limitation. How might this affect the reported gap between the ensemble and the unimodal models?

## Architecture Onboarding

- **Component map:** Raw text strings + 16kHz Audio Waveforms -> Text Encoder (RoBERTa) + Audio Encoder (Wav2Vec2) -> Logits Extraction -> Ensembling (Probability Averaging)
- **Critical path:** 1) Data Preprocessing (Text cleaning, Audio resampling to 16kHz) 2) Unimodal Inference (Pass text through RoBERTa, Audio through Wav2Vec2) 3) Extraction of Logits/Probabilities for emotion classes 4) Ensembling (Combining the two probability vectors)
- **Design tradeoffs:**
  - **Speed vs. Accuracy:** RoBERTa is accurate (50.68%) but slow (74.6s); DistilBERT is fast (5.61s) but less accurate (41.83%)
  - **Complexity vs. Reproducibility:** The authors chose simple late fusion over complex cross-modal attention (seen in corpus papers like "Centering Emotion Hotspots") to prioritize reproducibility and simplicity
- **Failure signatures:**
  - **Dominant Text Bias:** If the ensemble barely improves over RoBERTa alone, the audio model is likely providing negligible signal or noise
  - **Overfitting:** The paper notes a "lightweight evaluation" and limited splits; a failure would manifest as high training accuracy but low test accuracy on the SemEval hold-out set
- **First 3 experiments:**
  1. **Reproduce Unimodal Baselines:** Isolate RoBERTa and Wav2Vec2 on the test set to verify the 50.68% and 35.43% accuracies
  2. **Ablation Study:** Systematically remove one modality from the ensemble to quantify the marginal gain of adding audio to text (and vice versa)
  3. **Modality Weights:** Instead of simple averaging, experiment with weighting the text prediction (e.g., 0.7) higher than audio (0.3) to see if 62.97% can be exceeded, given the text model's superior base performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the reported performance hold when evaluated using class-imbalance-aware metrics such as Macro-F1?
- **Basis in paper:** [Explicit] The authors state in Section 4 that they "primarily report accuracy" and that "class-imbalance-aware metrics (e.g., Macro-F1)... are left for future work."
- **Why unresolved:** High accuracy can be achieved by predicting only the majority classes, masking potential failures in recognizing less frequent emotions.
- **What evidence would resolve it:** Re-evaluating the test set using Macro-F1 scores and per-class confusion matrices to ensure minority emotions are detected.

### Open Question 2
- **Question:** How does the late-fusion ensemble perform when the audio modality contains environmental noise or is missing?
- **Basis in paper:** [Explicit] Section 4 lists "Modality coverage" as a limitation, noting that "missing/noisy audio segments can reduce the contribution of the speech modality."
- **Why unresolved:** Real-world application often involves degraded audio; the ensemble's reliance on clean Wav2Vec2 features may result in performance drops below the text-only baseline.
- **What evidence would resolve it:** Ablation studies injecting varying levels of signal-to-noise ratio (SNR) or simulating data dropouts to measure the robustness of the fusion mechanism.

### Open Question 3
- **Question:** Can the baseline generalize to non-scripted, naturalistic conversations outside of the *Friends* sitcom domain?
- **Basis in paper:** [Explicit] Section 4 explicitly lists "Dataset and domain constraints," warning that the dataset is derived from *Friends* and "may not generalize to other domains."
- **Why unresolved:** Sitcom data involves exaggerated acting and specific cultural contexts, potentially inflating performance compared to subtle, real-world human interactions.
- **What evidence would resolve it:** Cross-domain validation by testing the model on a naturalistic dataset (e.g., IEMOCAP or DAIC-WOZ) without retraining.

## Limitations

- The paper explicitly frames itself as a lightweight baseline rather than a novel contribution, with minimal evaluation protocol
- The dataset constraint (Friends sitcom) introduces domain specificity that may not generalize to other conversational contexts
- The limited tuning budget mentioned as a constraint likely caps the true performance ceiling of the multimodal system

## Confidence

- **High confidence:** The unimodal baseline results (RoBERTa 50.68%, Wav2Vec2 35.43%) are reproducible given the specified model architectures and dataset
- **Medium confidence:** The ensemble accuracy of 62.97% is plausible but depends on unspecified fusion parameters and hyperparameter choices during fine-tuning
- **Low confidence:** Claims about why text outperforms audio in this specific dataset are speculative without ablation studies isolating modality contributions

## Next Checks

1. **Ablation study:** Systematically remove one modality from the ensemble to quantify whether the 62.97% result reflects genuine multimodal complementarity or dominant text performance with minor audio correction
2. **Cross-dataset validation:** Test the trained models on an independent emotion recognition corpus (e.g., IEMOCAP or MELD) to assess domain transfer and generalization beyond the Friends dataset
3. **Hyperparameter sensitivity analysis:** Vary learning rates, batch sizes, and ensemble weighting schemes to establish whether the reported results are robust to implementation choices or sensitive to specific parameter settings