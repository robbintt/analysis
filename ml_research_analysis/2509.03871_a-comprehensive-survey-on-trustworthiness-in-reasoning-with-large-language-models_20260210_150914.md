---
ver: rpa2
title: A Comprehensive Survey on Trustworthiness in Reasoning with Large Language
  Models
arxiv_id: '2509.03871'
source_url: https://arxiv.org/abs/2509.03871
tags:
- reasoning
- arxiv
- preprint
- large
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first comprehensive survey on trustworthiness\
  \ in reasoning with large language models, systematically reviewing recent work\
  \ across five core dimensions: truthfulness, safety, robustness, fairness, and privacy.\
  \ It identifies that while reasoning techniques improve model performance and interpretability,\
  \ they introduce new vulnerabilities\u2014particularly in safety, robustness, and\
  \ privacy\u2014often matching or exceeding non-reasoning models."
---

# A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2509.03871
- Source URL: https://arxiv.org/abs/2509.03871
- Reference count: 40
- Primary result: First comprehensive survey identifying that reasoning techniques introduce new vulnerabilities across safety, robustness, and privacy dimensions, often matching or exceeding non-reasoning models

## Executive Summary
This survey provides the first systematic examination of trustworthiness in large reasoning models (LRMs), categorizing recent research across five dimensions: truthfulness, safety, robustness, fairness, and privacy. The work reveals that while reasoning techniques improve performance and interpretability, they introduce distinct vulnerabilities including amplified hallucinations in longer chain-of-thoughts, expanded attack surfaces through reasoning process hijacking, and trade-offs between safety alignment and general capabilities. The survey identifies significant gaps in fairness and privacy research specific to reasoning contexts, and calls for standardized benchmarks and evaluation methods to address these emerging challenges.

## Method Summary
The survey conducts a chronological and thematic literature review of 40 papers up to June 30, 2025, organizing findings into a taxonomy covering reasoning techniques (CoT, Long-CoT) and trustworthiness dimensions. Methods include qualitative synthesis of attack vectors (jailbreaks, hallucinations), defense mechanisms (guardrails, PRMs), and empirical observations from benchmark studies. The approach validates taxonomy through random sampling of papers and sanity checks on safety tax claims using toxicity benchmarks.

## Key Results
- Reasoning techniques amplify hallucinations rather than reduce them, particularly with longer CoTs and outcome-based RL fine-tuning
- Safety alignment imposes measurable "tax" on general capabilities across reasoning models, with stronger alignment correlating with greater performance degradation
- Fairness and privacy dimensions remain significantly under-researched in reasoning contexts compared to safety and robustness
- Reasoning models exhibit expanded attack surfaces through phase-skipping vulnerabilities that bypass safety mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extended Chain-of-Thought reasoning can amplify hallucinations rather than reduce them, particularly when CoT length increases or training relies purely on outcome-based RL.
- Mechanism: Longer reasoning chains provide more opportunities for factual errors to compound; outcome-based RL fine-tuning creates high variance in policy gradients and entropy in predictions, leading models toward spurious local optima where plausible-but-incorrect reasoning paths are reinforced. Evidence suggests models increasingly rely on language priors over actual inputs as CoTs lengthen, particularly in multimodal settings.
- Core assumption: The relationship between CoT length and hallucination rate is causal, not merely correlational.
- Evidence anchors:
  - [section 3.1.2] "Lu et al. reported that hallucinations tend to occur more frequently in longer CoTs compared to those with correct answers. Similarly, Liu et al. observed that as CoTs become longer, models increasingly rely on language priors over visual inputs, a shift that often leads to visual hallucinations."
  - [section 3.1.2] "Li et al. similarly identified outcome-based RL fine-tuning as a contributor to hallucinations, highlighting three critical factors: high variance in policy gradients, high entropy in predictions, and the presence of spurious local optima."
  - [corpus] Limited direct corpus support; neighbor papers focus on domain-specific trustworthiness rather than reasoning-specific hallucination mechanisms.
- Break condition: If shorter CoTs consistently produce equal or higher hallucination rates than longer ones across controlled experiments, the mechanism is invalidated.

### Mechanism 2
- Claim: Reasoning models exhibit expanded attack surfaces because their structured thinking processes can be hijacked to bypass safety alignment through CoT manipulation.
- Mechanism: Adversarial prompts inject crafted reasoning content that obfuscates harmful intent, causing models to skip justification phases. The H-CoT attack pads detailed execution steps into prompts, redirecting the reasoning trajectory before safety checks occur. This exploits the mismatched generalization between instruction-following capability (which scales with reasoning ability) and safety alignment (which may not).
- Core assumption: Safety mechanisms in reasoning models primarily operate during specific phases of the reasoning process that can be identified and bypassed.
- Evidence anchors:
  - [section 4.2.2] "Kuo et al. proposed H-CoT... padding detailed execution steps could hijack the thinking process, skip the justification phase, and elicit harmful generation."
  - [abstract] "The complexity of jailbreak attacks exploiting reasoning"
  - [corpus] Neighbor paper "Safety in Large Reasoning Models: A Survey" confirms safety concerns in LRMs but provides limited mechanistic detail.
- Break condition: If safety alignment is uniformly distributed across all reasoning steps and cannot be selectively bypassed, the phase-skipping mechanism fails.

### Mechanism 3
- Claim: Safety alignment imposes a "safety tax" on reasoning models, degrading general performance proportionally to safety gains because alignment objectives compete with reasoning capability objectives during training.
- Mechanism: Fine-tuning on safety datasets modifies model weights in ways that conflict with representations supporting complex reasoning. RLHF processes sacrifice capabilities in translation, reading comprehension, and QA. The trade-off mirrors adversarial training in CNNs where robustness comes at accuracy cost. Model merging can partially mitigate but not eliminate this tax.
- Core assumption: The performance degradation is a direct consequence of the alignment process itself, not data quality or training inefficiency.
- Evidence anchors:
  - [section 4.3.3] "Huang et al. fine-tuned a large reasoning model with two safety alignment datasets, finding that better safety performance corresponded to more severe sacrifices on model general capabilities."
  - [section 4.3.3] "Lin et al. firstly conducted a comprehensive study on alignment tax, highlighting that the RLHF process would sacrifice multiple model capabilities"
  - [corpus] No direct corpus evidence; neighbor papers do not address safety-performance trade-offs in reasoning contexts.
- Break condition: If safety-aligned reasoning models can match or exceed unaligned models on both safety and general benchmarks simultaneously, the tax mechanism is falsified.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting Paradigms**
  - Why needed here: The entire survey distinguishes between few-shot-CoT, zero-shot-CoT, and end-to-end trained reasoning models—understanding these distinctions is prerequisite to interpreting any vulnerability or defense mechanism.
  - Quick check question: Can you explain why zero-shot-CoT ("Let's think step by step") might produce different trustworthiness characteristics than few-shot-CoT with human-annotated reasoning paths?

- Concept: **Process Reward Models (PRM) vs. Outcome Reward Models (ORM)**
  - Why needed here: Multiple defense and evaluation mechanisms rely on PRMs for stepwise verification; understanding the difference is essential for interpreting hallucination detection, safety guardrails, and training methodologies like GRPO.
  - Quick check question: Why would a PRM potentially detect hallucinations that an ORM misses during reasoning evaluation?

- Concept: **Test-Time Compute Scaling**
  - Why needed here: Safety defenses (Best-of-N, Saffron-1) and robustness improvements leverage inference-time scaling; understanding how compute allocation affects safety vs. capability trade-offs is central to deployment decisions.
  - Quick check question: If increasing test-time compute improves both reasoning accuracy and safety robustness, what might explain why this doesn't eliminate the safety tax?

## Architecture Onboarding

- Component map: Input → Reasoning Trace Generation → Intermediate Guardrail Check → Final Answer Generation → Post-hoc Safety Evaluation. The most failure-prone nodes are reasoning trace generation (where hallucinations compound and CoT hijacking occurs) and the alignment layer (where safety tax degrades capability).

- Critical path: The most vulnerable stages are reasoning trace generation, where hallucinations compound and CoT hijacking occurs, and the alignment layer, where safety tax degrades capability.

- Design tradeoffs: (1) CoT length vs. hallucination risk—longer reasoning improves accuracy on complex tasks but increases hallucination surface. (2) Safety alignment depth vs. general capability—stronger alignment taxes performance. (3) Inference-time scaling vs. deployment cost—more compute improves robustness but may be economically prohibitive. (4) Guardrail model specificity vs. generalization—specialized detectors (e.g., FG-PRM) are more accurate but require additional training.

- Failure signatures: (1) Consistent correct answers with unfaithful CoT—model fabricates post-hoc explanations. (2) Overthinking on unanswerable questions—model generates excessive reasoning without recognizing invalid premises. (3) Underthinking under prompt injection—adversarial inputs truncate reasoning. (4) Safety capability imbalance across languages—multilingual inputs bypass alignment.

- First 3 experiments:
  1. **Faithfulness Baseline**: Implement CoT intervention (truncation + error injection) on your target reasoning model across 3 task types; compute AOC scores to establish faithfulness baselines before any safety modifications.
  2. **Attack Surface Mapping**: Test 3 jailbreak variants (H-CoT, Mousetrap, cipher-based) against your model with and without guardrail protection; measure ASR on reasoning traces separately from final answers to identify where safety fails.
  3. **Safety Tax Quantification**: Fine-tune on safety alignment dataset with controlled data ratios; plot safety score vs. general benchmark performance (GSM8K, HumanEval) to establish your model's specific trade-off curve before deployment decisions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can standardized faithfulness evaluation metrics be developed to resolve the contradiction where stronger models appear less faithful due to relying on internal knowledge rather than explicit reasoning steps?
- **Basis in paper:** [explicit] Section 8 states that current intervention-based metrics lead to divergent conclusions (e.g., larger models being both more and less faithful) and that "eliminating the evaluation bias caused by model performance remains a critical open problem."
- **Why unresolved:** Current intervention methods (e.g., truncating reasoning) cannot distinguish between a model ignoring a step because it is "unfaithful" versus it being "robust" and relying on internalized correct knowledge.
- **What evidence would resolve it:** A new metric that quantifies the causal dependency of the final answer on intermediate steps without penalizing the model for possessing prior knowledge, validated across different model scales.

### Open Question 2
- **Question:** To what extent does the "safety tax" (performance degradation from alignment) depend on the specific reinforcement learning algorithm (e.g., GRPO vs. DPO) versus the quality of the CoT alignment data?
- **Basis in paper:** [explicit] Section 8 notes that "empirical understanding of how reinforcement learning contributes to safety... remains limited" and calls for research to "disentangle the extent to which performance gains stem from the learning algorithm... versus the influence of higher-quality data."
- **Why unresolved:** Most alignment studies conflate the learning algorithm with the dataset, making it difficult to isolate the specific cause of the general capability degradation often observed in safety-aligned reasoning models.
- **What evidence would resolve it:** Ablation studies comparing alignment performance and general capability retention across different RL algorithms (like PPO, GRPO, DPO) using identical, well-curated CoT datasets.

### Open Question 3
- **Question:** What specific, fine-grained benchmarks are required to evaluate privacy and fairness vulnerabilities in Large Reasoning Models (LRMs) given the current reliance on homogenized safety datasets?
- **Basis in paper:** [explicit] Section 8 highlights that "evaluations on privacy inference and fairness have comparatively received less emphasis" and calls for "new benchmarks that are more discriminative, detailed, and robust."
- **Why unresolved:** Current safety benchmarks focus primarily on attack success rates (ASR) via jailbreaks, resulting in data homogenization that fails to capture reasoning-specific risks like privacy leakage in intermediate thoughts or bias amplification.
- **What evidence would resolve it:** The creation and adoption of benchmarks specifically designed to test reasoning-induced privacy leaks (e.g., inferring attributes from CoT) and bias propagation in intermediate reasoning steps.

## Limitations
- Safety tax mechanism lacks sufficient empirical validation across diverse reasoning architectures and safety datasets
- Fairness and privacy dimensions remain significantly under-researched with limited literature specific to reasoning models
- The survey cannot fully capture the dynamic nature of adversarial techniques, as new jailbreak methods emerge faster than systematic defenses can be evaluated
- Relationship between CoT length and hallucination rate remains primarily correlational rather than causal

## Confidence

- **High confidence**: Truthfulness mechanisms (CoT hallucination amplification, PRM vs ORM distinctions) - supported by multiple experimental studies and mechanistic explanations
- **Medium confidence**: Safety alignment trade-offs - well-documented but varies significantly by model architecture and dataset quality
- **Medium confidence**: Jailbreak exploitation of reasoning processes - demonstrated through multiple attack vectors but lacks comprehensive defense evaluation
- **Low confidence**: Fairness and privacy in reasoning contexts - extremely limited literature base, mostly speculative extrapolations from general LLM research

## Next Checks

1. **Causal CoT Length Experiment**: Conduct controlled experiments varying CoT length (2-10 steps) on identical reasoning tasks while measuring hallucination rates and faithfulness scores to establish direct causation rather than correlation

2. **Cross-Architecture Safety Tax Benchmark**: Evaluate the same safety alignment dataset across three distinct reasoning architectures (end-to-end trained, RLHF-only, hybrid) to quantify architecture-specific safety tax variations

3. **Phase-Skipping Vulnerability Test**: Systematically probe whether safety mechanisms operate uniformly across all reasoning phases by measuring ASR when adversarial prompts target different CoT segments versus final answer generation