---
ver: rpa2
title: 'Learning from Sufficient Rationales: Analysing the Relationship Between Explanation
  Faithfulness and Token-level Regularisation Strategies'
arxiv_id: '2511.16353'
source_url: https://arxiv.org/abs/2511.16353
tags:
- rationales
- rationale
- bert
- language
- sufficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explores how sufficiency, a metric for measuring rationale
  informativeness, relates to two modeling paradigms: token classification and attention
  regularization. By analyzing sufficiency scores across multiple tasks and model
  types (BERT, Pythia, ModernBERT, GPT-Neo), the research reveals that high sufficiency
  values primarily capture the impact of non-rationale context rather than the absolute
  informativeness of rationales.'
---

# Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies

## Quick Facts
- arXiv ID: 2511.16353
- Source URL: https://arxiv.org/abs/2511.16353
- Reference count: 26
- Primary result: Sufficiency metrics capture contextual impact rather than rationale informativeness, with attention regularization showing inconsistent benefits across models and tasks

## Executive Summary
This paper investigates how sufficiency—a metric measuring how well rationales alone can predict labels—relates to two learning paradigms: token classification and attention regularization. By analyzing sufficiency scores across multiple datasets and four model architectures (BERT, Pythia, ModernBERT, GPT-Neo), the researchers discover that high sufficiency values primarily reflect the impact of non-rationale context rather than the absolute informativeness of rationales themselves. The study finds that attention regularization can improve cross-domain performance, particularly for BERT in argument mining tasks, but results vary significantly across models and tasks. Surprisingly, the ability to classify rationale tokens does not correlate with sufficiency scores, revealing a complex relationship between rationales and their context that sufficiency alone cannot capture.

## Method Summary
The study analyzes sufficiency (reframed as Contextual Impact or CI) alongside two modeling paradigms: Token Classification (TC) and Attention Regularization (AR). CI is computed as the probability difference between full input and isolated rationales. TC involves training a separate classifier to predict rationale membership per token, while AR adds an auxiliary attention loss term during fine-tuning to align model focus with human rationales. The researchers test these approaches across six datasets with binary rationale masks using four transformer architectures: BERT-base, Pythia-160M, ModernBERT-base, and GPT-Neo-125M. Models are fine-tuned with standard hyperparameters (3 seeds, batch 16, LR 3e-5, max 10 epochs), and AR training incorporates BCE loss between last-layer attention weights and rationale masks.

## Key Results
- Sufficiency (CI) primarily captures contextual impact—the interference of non-rationale context with rationale information—rather than rationale informativeness
- Attention regularization shows potential for improving cross-domain performance, particularly for BERT models in argument mining tasks (AR=1.14 vs. 1.01 in-domain)
- The ability to classify rationale tokens (TC) does not correlate with sufficiency scores, suggesting a complex relationship between rationales and their context
- Aggregation strategy matters: union aggregation of multi-annotator rationales outperforms intersection for hate speech detection

## Why This Works (Mechanism)

### Mechanism 1: Contextual Impact Captures Context Interference, Not Rationale Informativeness
- Claim: Sufficiency (reframed as Contextual Impact or CI) does not measure rationale quality; it measures how non-rationale context interferes with or complements rationale information.
- Mechanism: CI is computed as `M(x)j - M(r)j`—the probability difference between full input and isolated rationales. High CI indicates the context contributes meaningfully to predictions; low CI indicates rationales are relatively self-contained. Counter-intuitively, high CI correlates with correct predictions because both rationale and context jointly contribute informative signals.
- Core assumption: The ablation operation (removing non-rationale tokens) preserves the semantic relationship between rationales and labels without introducing out-of-distribution artifacts.
- Evidence anchors:
  - [abstract] "Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input."
  - [section 5.4] "The fact that high instance CI leans to correct predictions seems to indicate that CI does not capture rationale informativeness in the way we originally hypothesized... CI appears to capture information about the way in which rationales and non-rationale contexts interact."
  - [corpus] Weak corpus support; related papers focus on rationale utility but do not validate this specific reframing of sufficiency.
- Break condition: If removal-based ablation creates grammatically invalid inputs that cause irregular model behavior unrelated to rationale importance, CI may confound distributional shift with genuine context impact.

### Mechanism 2: Attention Regularization Aligns Model Focus with Human Rationales
- Claim: Adding an auxiliary attention loss term during fine-tuning can guide models toward rationale tokens, with effects that vary by architecture and task.
- Mechanism: The regularized model R is trained with `L = L_task(y, ŷ) + L_attention(a, â)`, where attention weights from the last layer are penalized for deviating from the binary rationale mask. Bidirectional models (BERT, ModernBERT) benefit more than autoregressive models (Pythia, GPT-Neo) due to full-sequence attention access.
- Core assumption: The last-layer attention weights are sufficiently representative of the model's token-level decision process to serve as a proxy for rationale alignment.
- Evidence anchors:
  - [section 5.1] "We extract the attention weights â. The regularised model is then optimised with a second loss term L_attention (binary cross-entropy) measuring the distance between rationale mask a and attention weights â."
  - [section 5.2] "Regularisation is task- and model-dependent... We find that the lower bounds of the intervals indicate consistent improvement in 83% of cases for BERT... Consistent improvements are lower for the other four models: 43%, 25% and 0%."
  - [corpus] Related work (Attanasio et al. 2022, Stacey et al. 2022) finds heterogeneous results for attention regularization; corpus provides mixed validation.
- Break condition: If the task inherently requires context beyond rationales (e.g., syntactic tasks), forcing attention alignment may degrade performance by blocking necessary signals.

### Mechanism 3: Rationale Aggregation Strategy Affects Signal-to-Noise Ratio
- Claim: Aggregating multi-annotator rationales via union (inclusive) yields better model guidance than intersection (exclusive), at least for hate speech detection.
- Mechanism: Union aggregation captures disputed tokens that carry partial predictive signal; intersection restricts to unanimously agreed tokens, potentially discarding informative words. Higher rationale density (|r|/|x|) correlates with better token classification performance.
- Core assumption: Annotator disagreement reflects genuine ambiguity in token importance rather than annotation noise.
- Evidence anchors:
  - [section 6] "CI is lower for HateXplain ∪ than for – ∩ (for all four models), with also TC and AR being mostly higher... collecting different human perspectives on word importance (aggregated through union) is preferred over a single annotator that might label too strictly."
  - [section 5.2] "The classifiers with the highest TC are the ones that were trained on the datasets with highest rationale density |r|/|x|."
  - [corpus] No direct corpus evidence on aggregation strategies; this remains an open question beyond the paper's scope.
- Break condition: If union aggregation includes too many noisy tokens (low annotation quality), the signal-to-noise ratio may degrade, reducing model benefits.

## Foundational Learning

- Concept: **Sufficiency vs. Comprehensiveness (Faithfulness Metrics)**
  - Why needed here: The paper reframes sufficiency as "contextual impact," but understanding the original definition (rationale-only prediction quality) versus comprehensiveness (prediction change when removing rationales) is essential to interpret CI correctly.
  - Quick check question: If a model achieves high accuracy using only rationale tokens (sufficiency ≈ 0), does this guarantee the rationale is causally responsible for the prediction?

- Concept: **Attention Regularization Loss Formulation**
  - Why needed here: The mechanism depends on understanding how auxiliary losses combine with task losses, and why BCE is appropriate for aligning continuous attention weights with binary masks.
  - Quick check question: Why might L1 or L2 loss be less suitable than binary cross-entropy for attention-rationale alignment?

- Concept: **In-Domain vs. Cross-Domain Generalization**
  - Why needed here: The paper's most consistent finding is that attention regularization helps BERT on cross-domain argument mining (AR=1.14 vs. 1.01 in-domain). Understanding domain shift is critical to interpreting this result.
  - Quick check question: If regularization improves cross-domain performance but not in-domain, what does this suggest about what the model learns from rationales?

## Architecture Onboarding

- Component map:
  - **Baseline Model M**: Standard fine-tuned transformer for sequence classification
  - **Token Classifier T**: Binary classifier predicting rationale membership per token (trained separately)
  - **Regularized Model R**: M + auxiliary attention loss during training only
  - **CI Computation**: Forward pass on full input x and ablated rationale r, compute probability difference
  - **Metrics**: TC (token F1 ratio vs. majority baseline), AR (sequence F1 ratio: R vs. M)

- Critical path:
  1. Fine-tune baseline M on task (no rationale exposure)
  2. Compute CI per instance by comparing M(x) and M(r) predictions
  3. Train token classifier T independently (for TC metric)
  4. Fine-tune regularized model R with combined task + attention loss
  5. Evaluate R on non-rationalized test set; compute AR = f1_R / f1_M

- Design tradeoffs:
  - **Removal vs. Masking for CI**: Removal creates ungrammatical inputs but avoids token semantics; masking preserves sequence length but introduces [MASK] token distribution shift. Paper finds minimal difference (Appendix A.1).
  - **Layer Selection for Attention Loss**: Paper uses last layer only; earlier layers may capture different linguistic abstractions but increase tuning complexity.
  - **Token-Subtoken Alignment**: Rationales annotated at word level must be mapped to subword tokens; alignment strategy can introduce noise.

- Failure signatures:
  - **AR < 1.0 across all tasks**: Model architecture may not benefit from attention guidance (observed for GPT-Neo); consider alternative regularization approaches.
  - **High CI with low AR**: Rationales may be informative but model already captures their signal; regularization provides no marginal benefit.
  - **TC ≈ 1.0**: Token classifier fails to exceed majority baseline; rationales may lack discriminative token-level features.

- First 3 experiments:
  1. Replicate CI computation on a held-out dataset with rationales; verify that removal and masking produce correlated scores (τ > 0.7 as in Appendix A.1).
  2. Train regularized BERT on AURC-8 CD; confirm AR > 1.10 with 95% CI lower bound > 1.0 before expanding to other architectures.
  3. Compare union vs. intersection aggregation on a multi-annotator rationale dataset; expect union to yield lower CI and higher TC/AR for tasks with annotation disagreement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the metric of comprehensiveness (rationale ablation) provide a more reliable signal for rationale learnability than sufficiency (contextual impact)?
- Basis in paper: [explicit] The authors conclude that "new insights about contextual information can lead to reconsidering comprehensiveness... to further investigate the balance between rationale and context information."
- Why unresolved: The study focused on sufficiency (re-framed as Contextual Impact) and found it reflects non-rationale interference rather than rationale informativeness. The complementary ablation metric, comprehensiveness, was not analyzed for its relationship to the learnability metrics.
- What evidence would resolve it: An analysis correlating comprehensiveness scores with the token classification (TC) and attention regularization (AR) metrics across the same datasets to see if it yields the negative correlation initially hypothesized for sufficiency.

### Open Question 2
- Question: To what extent do language-specific structures and cultural annotator perceptions influence the definition and learnability of "sufficient" rationales?
- Basis in paper: [explicit] The authors state, "Investigating datasets in different languages has the additional advantage that it can provide insight into both language- and culture-specific components in human perception of sufficient information."
- Why unresolved: The experiments were limited to English datasets. Consequently, the results cannot differentiate between universal properties of rationale learning and those specific to English syntax or the cultural biases of the English-speaking annotators.
- What evidence would resolve it: A replication of the TC and AR experiments on multilingual datasets with unconstrained rationales, analyzing whether the correlation between Contextual Impact and model performance varies significantly across languages.

### Open Question 3
- Question: Can a novel metric be formulated to systematically capture rationale informativeness in a way that consistently predicts model improvement?
- Basis in paper: [explicit] The abstract and conclusion state that results "exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation."
- Why unresolved: The paper demonstrated that sufficiency (Contextual Impact) is not a good indicator for prioritizing training information, as it failed to consistently correlate with the ability of models to learn from rationales (Token Classification and Attention Regularization).
- What evidence would resolve it: The derivation of a new metric that correlates strongly and significantly with the learnability metrics (AR and TC) across diverse tasks and architectures, unlike the inconsistent correlations found for Contextual Impact.

### Open Question 4
- Question: Why does attention regularization specifically enhance cross-domain performance for bidirectional models (BERT) but not for autoregressive models (Pythia, GPT-Neo)?
- Basis in paper: [inferred] The results section notes that attention regularization "shows potential for improving cross-domain performance, particularly for BERT models," while the effect was inconsistent or negative for other models.
- Why unresolved: The paper establishes the empirical discrepancy but does not isolate the specific architectural or pre-training mechanisms (e.g., bidirectional attention vs. causal masking) that allow BERT to utilize rationale guidance effectively in cross-domain settings.
- What evidence would resolve it: Ablation studies controlling for attention mechanisms and pre-training objectives to identify the specific model components that enable successful knowledge transfer from rationales during cross-domain learning.

## Limitations

- **CI Metric Reframing**: The conclusion that sufficiency primarily captures contextual impact relies on the assumption that ablation operations don't introduce distributional artifacts, though Appendix A.1 shows high correlation between methods.
- **Attention Weight Representativeness**: The assumption that last-layer attention weights adequately represent the model's token-level decision process may not hold for all architectures or tasks.
- **Architecture-Specific Effects**: Results show strong model-dependent variation, with BERT benefiting consistently while GPT-Neo shows no improvement, but underlying reasons remain unexplained.

## Confidence

- **High Confidence**: The CI reframing itself (sufficiency as contextual impact) and the basic implementation of token classification and attention regularization methods.
- **Medium Confidence**: The conclusion that attention regularization improves cross-domain performance for BERT in argument mining.
- **Low Confidence**: The generalizability of aggregation strategy recommendations (union vs. intersection) and the broader claim that sufficiency cannot serve as a measure of rationale informativeness across different contexts.

## Next Checks

1. **Cross-Domain Validation**: Replicate the attention regularization experiment on at least two additional cross-domain tasks beyond AURC-8 CD to verify whether BERT's cross-domain advantage is consistent or task-specific.

2. **Attention Layer Ablation**: Test whether using earlier attention layers (rather than only the last layer) for regularization changes the effectiveness of the approach, particularly for architectures like GPT-Neo that showed no improvement.

3. **Alternative Ablation Methods**: Compare CI scores computed using token masking, deletion, and novel approaches like scrambling non-rationale tokens to better understand how input perturbation methods affect the metric's interpretation.