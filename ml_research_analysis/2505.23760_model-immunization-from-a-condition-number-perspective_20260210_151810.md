---
ver: rpa2
title: Model Immunization from a Condition Number Perspective
arxiv_id: '2505.23760'
source_url: https://arxiv.org/abs/2505.23760
tags:
- immunization
- number
- condition
- rill
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for analyzing model immunization
  using the condition number of Hessian matrices. The goal is to pre-train models
  that are difficult to fine-tune on harmful tasks while maintaining utility on non-harmful
  tasks.
---

# Model Immunization from a Condition Number Perspective

## Quick Facts
- arXiv ID: 2505.23760
- Source URL: https://arxiv.org/abs/2505.23760
- Reference count: 40
- Primary result: Framework uses condition number optimization to pre-train models resistant to harmful fine-tuning while maintaining primary task performance

## Executive Summary
This paper introduces a novel approach to model immunization that protects pre-trained models from being easily fine-tuned for harmful purposes. The method leverages the condition number of Hessian matrices as a proxy for fine-tuning difficulty, proposing regularizers that increase the condition number for harmful tasks while maintaining it for beneficial tasks. The framework combines theoretical analysis with empirical validation across linear models and deep networks, demonstrating effective immunization against harmful fine-tuning while preserving performance on the original task.

## Method Summary
The method uses a dual regularization framework during pre-training: R_well minimizes the condition number for the primary task to ensure fast convergence, while R_ill maximizes the condition number for the harmful task to slow down fine-tuning. The regularizers are applied to the Hessian of the linear probing loss, with gradients computed via singular value decomposition. For deep networks, the approach approximates Hessians using sampled feature covariances. The immunization effectiveness depends on the alignment between singular vectors of pre-training and harmful task covariance matrices, with better immunization achieved when these structures are misaligned.

## Key Results
- Immunization effectiveness correlates with relative alignment between pre-training and harmful task covariance matrices
- The proposed R_ill regularizer successfully increases condition numbers for harmful tasks while preserving performance on primary tasks
- Experiments show RIR values significantly above 1 (successful immunization) across multiple task pairs and model architectures
- The method outperforms baseline approaches in most cases while maintaining primary task accuracy within acceptable bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The condition number of the Hessian matrix controls gradient descent convergence speed, providing a mathematically grounded proxy for "hardness to fine-tune."
- Mechanism: For strongly convex loss with Hessian eigenvalues σ_max/σ_min, steepest descent converges at rate (1 - σ_min/σ_max)^t. When κ = σ_max/σ_min approaches 1, optimization is well-conditioned (fast); when κ is large, optimization is ill-conditioned (slow).
- Core assumption: Assumption: Fine-tuning uses gradient-based methods with constant step size; the loss landscape is approximately strongly convex locally.
- Evidence anchors:
  - [Section 2] Explicitly states convergence rate bound and notes "a larger condition number corresponds to a slower convergence"
  - [Proposition 3.2] Derives analytical singular values of Hessian HH(θ) = θ^T K_H θ, showing dependence on alignment between θ's singular vectors and data covariance eigenvectors
  - [corpus] Limited direct corpus support for this specific immunization-condition-number connection; related work on condition numbers in optimization exists but not for adversarial settings
- Break condition: Non-convex loss landscapes (deep nets) where Hessian spectrum doesn't predict convergence; adaptive optimizers (Adam) that may partially compensate for ill-conditioning

### Mechanism 2
- Claim: The proposed R_ill regularizer guarantees monotonic increase of the condition number when updated via gradient descent under uniqueness assumptions.
- Mechanism: R_ill(S) = 1/(½k||S||²_F - ½(σ_min)^2) upper-bounds 1/log(κ(S)). Gradient ∇_S R_ill = [σ_min u_k v_k^T - (1/k)S] / [denominator]² shares singular vectors with S, preserving spectral structure during updates. With bounded step size, σ_min decreases while σ_max increases → κ grows.
- Core assumption: The minimum singular value σ_min is unique (multiplicity 1); step size η < [k/(k-1)][denominator]²
- Evidence anchors:
  - [Theorem 4.1] Proves nonnegativity, upper bound, differentiability, and monotonic increase guarantees
  - [Theorem 4.3] Extends guarantees to H(θ) = θ^T K θ structure, proving κ(θ'^T K θ') > κ(θ^T K θ) for harmful task with appropriate step size
  - [corpus] Condition number minimization (R_well) has precedent in Nenov et al. 2024; maximization regularizer appears novel
- Break condition: Non-unique minimum singular value (gradient undefined); step size violating bounds; simultaneous updates with task loss gradient breaking monotonicity (acknowledged in Section 4.4 as limitation)

### Mechanism 3
- Claim: Immunization effectiveness depends on the geometric alignment between singular vectors of pre-training and harmful task covariance matrices.
- Mechanism: From Proposition 3.2, singular values of HH(θ) = Σ_j (σ_θ,i · cos(θ_i, q_j) · √γ_j)² where cos(θ_i, q_j) = u_θ,i^T q_j measures alignment. If pre-training and harmful datasets share aligned principal directions, no single θ can simultaneously maximize κ_HH and minimize κ_HP—they compete for the same spectral structure.
- Core assumption: Assumption: Data covariance matrices K_P and K_H capture task structure; linear feature extractor without dimensionality reduction
- Evidence anchors:
  - [Section 3.1] "the strength of the immunization depends on the relative angle between the singular vectors of K_P and K_H"
  - [Proposition 3.2 proof, Appendix B.1] Full derivation showing how alignment term (u_θ,i^T q_j) enters singular value formula
  - [corpus] No corpus papers directly address this covariance alignment constraint for immunization
- Break condition: Tasks with orthogonal covariance structure (ideal for immunization); tasks with identical covariance structure (immunization impossible); non-linear feature extractors where covariance doesn't capture task geometry

## Foundational Learning

- Concept: **Condition Number and its Relation to Optimization Convergence**
  - Why needed here: The entire framework rests on using κ(Hessian) as a proxy for "difficulty to fine-tune." Without this intuition, the regularizers appear arbitrary.
  - Quick check question: For a 2×2 Hessian with eigenvalues [100, 1], what is κ? How many iterations to reduce ||w-w*|| by 10x with steepest descent?

- Concept: **Singular Value Decomposition and Spectral Structure**
  - Why needed here: The gradient of R_ill preserves singular vectors while modifying singular values. Understanding this requires comfort with SVD and how matrix operations affect spectra.
  - Quick check question: If S = UΣV^T and we compute S' = S - η·σ_1 u_1 v_1^T, what are the singular values of S'?

- Concept: **Hessian of Linear Models with Feature Extractors**
  - Why needed here: The paper derives HH(θ) = θ^T K_H θ for linear probing. Understanding why the Hessian takes this form is essential for implementing the regularizers.
  - Quick check question: For linear regression with loss ||(Xθ)w - y||² and frozen θ, what is ∇²_w L with respect to w?

## Architecture Onboarding

- Component map:
  - Data Preparation -> Feature Extractor θ -> Regularizer Computation -> Optimizer

- Critical path:
  1. Initialize θ_0 (random for linear; pre-trained checkpoint for deep nets)
  2. For each batch: forward pass through f_θ → compute supervised loss on D_P
  3. Compute Hessians H_P(θ), H_H(θ) via feature covariance (O(d²) memory for d=D_hid)
  4. Extract σ_max, σ_min and singular vectors (SVD)
  5. Compute ∇_θ R_well(H_P) and ∇_θ R_ill(H_H) using Theorem 4.2 closed forms
  6. Apply preconditioning: multiply gradients by K_P^{-1} and K_H^{-1} respectively (or regularized inverse for stability)
  7. Update θ with combined gradient: ∇_θ L + λ_P K_P^{-1} ∇R_well + λ_H K_H^{-1} ∇R_ill

- Design tradeoffs:
  - **Memory vs. approximation**: Computing full Hessians requires O(D_hid²) memory; paper samples 20 groups of 100 samples for deep nets to approximate
  - **Hyperparameter sensitivity**: λ_P and λ_H must balance gradient magnitudes; paper reports 4-5 orders of magnitude difference (λ_H ∼ 10^6-10^8)
  - **Which layers to immunize**: Full network vs. partial (paper immunizes only last 2 blocks of ResNet, last block of ViT); deeper immunization may harm pre-training performance more

- Failure signatures:
  - **R_ill exploding**: Check if σ_min is non-unique or near-zero; denominator ½k||H||²_F - ½σ_min² approaches zero → add numerical buffer
  - **Pre-training accuracy collapse**: λ_H too large relative to λ_P; condition number maximization on harmful task degrades shared features needed for primary task
  - **No immunization effect**: K_P and K_H may have aligned structure (Section 3.1 constraint); try different pre-training/harmful task splits or increase model capacity
  - **Gradient instability during SVD**: σ_min computation has subgradient issues when non-unique; add small perturbation to break ties

- First 3 experiments:
  1. **Linear model sanity check on MNIST pairs**: Train linear θ with algorithm on one digit pair (e.g., 0 vs 1 as D_P, 2 vs 3 as D_H). Verify: (a) RIR > 1, (b) convergence plots show slowed fine-tuning on D_H, (c) convergence maintained on D_P. This validates implementation before scaling.
  2. **Ablation on λ_H/λ_P ratio**: Fix architecture (ResNet18), sweep λ_H while holding λ_P constant. Plot RIR_θ0 vs. ImageNet accuracy. Identify the Pareto frontier where immunization is achieved with <3% accuracy drop.
  3. **Covariance alignment diagnostic**: Before immunization, compute SVD of K_P and K_H, measure alignment as Σ_i |u_i^T v_i| for top-k singular vectors. Correlate alignment score with achievable RIR across multiple (D_P, D_H) pairs. This tests the theoretical claim from Section 3.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical guarantees for monotonic condition number control be extended to non-linear feature extractors where the Hessian structure differs from the linear case?
- Basis in paper: [explicit] The authors state in Section 4.4 (Limitations) that the current framework focuses on linear feature extractors and that there is a "theoretical gap" when applying the method to deep-nets, which are treated empirically.
- Why unresolved: The proofs for Theorem 4.3 rely on the closed-form Hessian of the form $H(\theta) = \theta^\top K \theta$, which does not hold for non-linear architectures.
- What evidence would resolve it: A theoretical extension of the regularizer's properties to non-convex settings, or empirical proof that singular value updates remain linear in deep networks.

### Open Question 2
- Question: Is the immunization robust against adversaries who utilize full fine-tuning (updating all weights) rather than the assumed linear probing (frozen features)?
- Basis in paper: [inferred] The methodology in Section 2 and 3 strictly defines the attack model as "Transfer learning via linear probing," where the feature extractor $\theta$ is fixed. Real-world model misuse often involves updating the entire network.
- Why unresolved: The proposed regularization modifies the landscape for a linear classifier on top of fixed features; if the features themselves can be updated, the adversary may circumvent the ill-conditioned landscape.
- What evidence would resolve it: Experiments demonstrating the method's effectiveness when the attacker uses full fine-tuning or non-linear adapters on the harmful dataset $D_H$.

### Open Question 3
- Question: Can the immunization framework be adapted to generalize to unseen harmful tasks without requiring explicit access to the harmful dataset $D_H$ during the regularization phase?
- Basis in paper: [inferred] Algorithm 1 requires $X_H$ (harmful task inputs) as a direct input to compute the regularizer $R_{ill}$. This assumes the defender possesses the specific data they wish to immunize against.
- Why unresolved: The regularizer relies on the specific covariance matrix $K_H$ of the harmful data to align the singular vectors. Without this data, the optimization target is undefined.
- What evidence would resolve it: A method that successfully immunizes a model against a specific harmful concept using a proxy dataset or semantic constraints, validated against the actual harmful dataset.

## Limitations

- The theoretical guarantees are limited to linear models and do not extend to non-linear deep networks where the Hessian structure differs significantly
- The immunization effectiveness critically depends on the alignment between pre-training and harmful task covariance matrices, which may limit applicability in some settings
- The method requires access to the specific harmful dataset during pre-training, which may not be feasible in all real-world scenarios

## Confidence

- **High confidence**: The theoretical framework for linear models (Section 3), the regularizer formulations (R_well, R_ill), and the convergence guarantees under stated assumptions
- **Medium confidence**: The empirical results on deep networks, as implementation details are incomplete and results depend on unexplained hyperparameter choices
- **Medium confidence**: The claim that immunization preserves primary task performance, though this is supported by experiments but with limited ablation studies

## Next Checks

1. **Implementation verification**: Reproduce the linear model experiments on MNIST pairs (0vs1 as D_P, 2vs3 as D_H) with Algorithm 1, verifying RIR > 1 and convergence slowdown on D_H while maintaining D_P performance
2. **Covariance alignment analysis**: Compute and compare singular vector alignments between K_P and K_H across multiple dataset pairs to empirically validate the theoretical constraint from Section 3.1
3. **Layer selection ablation**: Systematically test different layer subsets for immunization in ResNet18 to identify optimal trade-offs between immunization strength and primary task performance