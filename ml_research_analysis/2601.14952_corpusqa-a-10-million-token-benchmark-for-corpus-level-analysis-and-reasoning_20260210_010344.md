---
ver: rpa2
title: 'CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning'
arxiv_id: '2601.14952'
source_url: https://arxiv.org/abs/2601.14952
tags:
- reasoning
- arxiv
- benchmark
- data
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CorpusQA, a benchmark for corpus-level reasoning
  over documents up to 10 million tokens. It addresses the gap in existing long-context
  benchmarks that assume sparse evidence, whereas real-world corpus analysis requires
  holistic integration of highly dispersed information.
---

# CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning

## Quick Facts
- **arXiv ID:** 2601.14952
- **Source URL:** https://arxiv.org/abs/2601.14952
- **Reference count:** 20
- **Primary result:** Even state-of-the-art models fail at corpus-level reasoning over 10M tokens; memory-augmented agents show greater resilience.

## Executive Summary
CorpusQA introduces a benchmark for corpus-level reasoning over document collections up to 10 million tokens, addressing the gap in existing long-context benchmarks that assume sparse evidence. The benchmark employs a schema-driven pipeline to extract structured data from documents and generate complex queries, ensuring reliable evaluation through programmatic ground-truth computation. Experiments reveal that standard RAG systems fail entirely at large scales, while memory-augmented agentic architectures demonstrate superior performance. Fine-tuning models on synthesized corpus-level data also improves performance on external long-context tasks, highlighting the need for advanced architectures beyond simple context extension.

## Method Summary
CorpusQA employs a six-step pipeline: document collection and filtering, multi-model schema extraction with consensus voting, manual query template creation with LLM paraphrasing, JSON schema aggregation to a global data table, NL2SQL execution for ground truth computation, and QA pair assembly. The benchmark focuses on real-world PDFs from finance, education, and real estate domains, filtered for rich statistical content. Training uses 480 synthesized samples with Qwen3-4B-Thinking fine-tuned via 25 steps of GRPO. Evaluation employs LLM-as-a-Judge (DeepSeek-V3) for semantic equivalence scoring.

## Key Results
- Memory-augmented agents significantly outperform RAG at corpus scale (44.95% vs. 5.45% at 1M tokens)
- Performance degrades sharply with scale: memory agents drop to 11.13% accuracy at 10M tokens
- Fine-tuning on synthesized data improves performance on both CorpusQA and external long-context benchmarks (LongBenchV2, FRAMES)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schema-driven pipeline produces verifiable ground-truth answers without LLM annotation.
- Mechanism: Documents are first converted to structured JSON schemas via multi-model voting; queries are translated to SQL and executed against an aggregated data table. This decouples reasoning from unstructured text representation.
- Core assumption: Schema extraction faithfully captures the relevant information from each document.
- Evidence anchors:
  - [abstract] "generates complex queries, and programmatically computes ground-truth answers, ensuring reliable evaluation without relying on fallible LLM annotations"
  - [section 3.4] "Step 5: NL2SQL Execution... This SQL statement is then executed directly against the aggregated data table... ensures that every ground truth answer is 100% accurate"
  - [corpus] Related paper "Towards Global RAG" similarly targets corpus-level reasoning but does not use programmatic verification.
- Break condition: If schema extraction misses critical fields or introduces systematic errors, ground-truth answers will be wrong regardless of SQL correctness.

### Mechanism 2
- Claim: Memory-augmented agents outperform RAG at corpus scale due to iterative aggregation.
- Mechanism: Memory agents process documents chunk-by-chunk, maintaining a fixed-size memory buffer that accumulates information. This enables global synthesis without requiring all context in a single forward pass.
- Core assumption: The memory buffer can retain task-relevant information across millions of tokens without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "Memory-augmented agentic architectures show greater resilience"
  - [section 4.3] "At 4M and 10M tokens, RAG nearly fails, whereas Memory Agent remains functional"
  - [corpus] Limited external validation; related work on global RAG mentions similar limitations but different architectural solutions.
- Break condition: If memory update mechanisms discard dispersed evidence needed for aggregation, performance will collapse similar to RAG.

### Mechanism 3
- Claim: Training on synthesized corpus-level data transfers to external long-context tasks.
- Mechanism: GRPO fine-tuning on 480 synthesized samples teaches models to perform multi-step aggregation and comparison, which generalizes to other long-context benchmarks.
- Core assumption: The reasoning patterns learned are transferable, not benchmark-specific.
- Evidence anchors:
  - [abstract] "Fine-tuning models on the benchmark's synthesized data also improves performance on external long-context tasks"
  - [section 4.4] "The fine-tuned model achieved a significant gain on CorpusQA... also showed notable improvements on both LongBenchV2 and FRAMES"
  - [corpus] Weak external evidence; no corpus neighbors validate this transfer claim directly.
- Break condition: Assumption: Transfer may not hold for tasks with fundamentally different reasoning patterns (e.g., temporal reasoning, code analysis).

## Foundational Learning

- Concept: **High evidence dispersion**
  - Why needed here: The core challenge of CorpusQA is that relevant information is scattered across hundreds of documents, invalidating sparse retrieval assumptions.
  - Quick check question: Can you identify a real-world task where the answer requires aggregating data from 50+ documents, none of which individually contains the answer?

- Concept: **Programmatic ground-truth generation**
  - Why needed here: Evaluating corpus-level reasoning is impossible if we cannot verify answers; LLM-as-judge is unreliable for complex numerical aggregation.
  - Quick check question: Given a corpus of financial reports, how would you construct a query whose answer can be computed via SQL rather than inferred?

- Concept: **Memory-constrained long-context processing**
  - Why needed here: No current model handles 10M tokens natively; understanding how memory agents compress and retain information is essential for building systems that work at this scale.
  - Quick check question: If you process a 10M-token corpus in 100K-token chunks, what information must persist in memory between chunks to answer a global aggregation query?

## Architecture Onboarding

- Component map:
  Schema Extraction Module -> Query Generator -> NL2SQL Engine -> Memory Agent -> LLM-as-Judge

- Critical path:
  1. Document collection → Schema extraction (validates data quality)
  2. Schema aggregation → Global data table construction
  3. Query generation → NL2SQL → Ground truth computation
  4. Model evaluation → Judge comparison → Accuracy reporting

- Design tradeoffs:
  - **Schema extraction quality vs. coverage**: Multi-model voting improves precision but may discard documents lacking consensus (section 3.6 notes ~5% rejection rate)
  - **Memory buffer size vs. retention**: Larger buffers retain more information but increase compute; paper does not specify optimal buffer dimensions
  - **LLM-as-Judge vs. exact matching**: Flexible but introduces non-determinism; acknowledged limitation in paper

- Failure signatures:
  - RAG retrieving fewer chunks than documents needed (marked † in Table 4: impossible to answer correctly)
  - Base LLM context overflow at 4M+ tokens
  - Human timeout rates exceeding 90% at 4M+ tokens (Table 5)

- First 3 experiments:
  1. **Baseline RAG vs. Memory Agent at 128K**: Replicate Table 4 results to validate your evaluation pipeline; expect RAG ~55% vs. Memory Agent ~67%.
  2. **Schema extraction validation**: Sample 100 documents and manually verify extraction accuracy against the paper's reported 94-99% range (section 3.6).
  3. **Scale degradation analysis**: Run the same model across 128K, 1M, 4M, 10M settings and plot accuracy decay curves; compare slope to Table 3 patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do complex, multi-tool "deep research" agentic architectures perform on ultra-long corpus reasoning compared to the Memory Agents evaluated?
- **Basis in paper:** [explicit] The Limitations section states, "our experiments did not extend to more complex agentic architectures, such as those performing deep research... leaving their performance on CorpusQA as an important area for future investigation."
- **Why unresolved:** The paper only evaluates single-model Memory Agents and standard RAG. While Memory Agents outperformed RAG, their accuracy still dropped to ~11% at 10M tokens. It is unknown if agents capable of planning, tool use, or iterative browsing can close this performance gap.
- **What evidence would resolve it:** Benchmarking systems designed for deep research (e.g., agents with search APIs or code execution) on the 4M and 10M token settings of CorpusQA.

### Open Question 2
- **Question:** Can the schema-driven synthesis pipeline effectively generate ground-truth data for qualitative or non-numeric reasoning tasks?
- **Basis in paper:** [inferred] The methodology explicitly filters for documents with "rich statistical tables" and relies on NL2SQL for ground truth. The current benchmark focuses heavily on aggregation and comparison of structured data extracted from text.
- **Why unresolved:** Real-world corpus analysis often involves qualitative synthesis (e.g., legal arguments or literary themes) which may not map easily to the rows/columns schema or SQL execution required by the current pipeline.
- **What evidence would resolve it:** Extending the data generation pipeline to domains lacking structured statistical tables (e.g., novels or case law) and validating the logical consistency of programmatic ground truths in those domains.

### Open Question 3
- **Question:** Is the "Lost in the Middle" phenomenon or context confusion the primary driver of performance degradation as token count increases from 1M to 10M?
- **Basis in paper:** [inferred] The results show a sharp drop in accuracy for Memory Agents as scale increases (44.95% at 1M vs. 11.13% at 10M). While the paper attributes this to "high information dispersion," it does not isolate whether the failure is due to retrieval miss or reasoning error over accumulated memory.
- **Why unresolved:** The paper establishes that the task is difficult, but does not perform error analysis to determine if models fail because they cannot retrieve dispersed evidence or because they cannot reason over the accumulated information.

## Limitations

- The schema extraction pipeline may systematically discard edge-case documents that contain critical evidence but lack format standardization
- The SQL-ground-truth constraint excludes reasoning tasks requiring temporal reasoning, causal inference, or qualitative judgment
- The benchmark's focus on structured data limits its applicability to real-world corpus analysis tasks involving qualitative synthesis

## Confidence

- **High Confidence**: The claim that RAG fails at corpus scale is well-supported by systematic experiments showing accuracy collapse as context length increases. The memory agent's superior performance at 4M+ tokens is reproducible.
- **Medium Confidence**: The transferability of fine-tuning gains to external benchmarks is demonstrated but based on limited data (only two external datasets). The mechanism for why learned reasoning patterns generalize remains underspecified.
- **Low Confidence**: The assertion that the benchmark captures "real-world corpus analysis" is overstated given the SQL-ground-truth constraint and the absence of validation with domain experts performing actual analysis tasks.

## Next Checks

1. **Schema Extraction Stress Test**: Systematically vary document formats (tables, prose-heavy reports, mixed content) and measure extraction accuracy degradation. Identify the point where consensus voting begins rejecting valid documents.

2. **Ground Truth Diversity Audit**: For 100 randomly sampled queries, manually categorize the reasoning type required (aggregation, comparison, temporal, causal). Measure what fraction require SQL-computable answers versus qualitative inference.

3. **Expert Task Mapping**: Recruit 5 domain analysts to map their actual corpus analysis workflows to CorpusQA queries. Document where the benchmark's constraints prevent representation of real analysis tasks.