---
ver: rpa2
title: 'Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study'
arxiv_id: '2512.00931'
source_url: https://arxiv.org/abs/2512.00931
tags:
- prompt
- were
- abstract
- alignment
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates prompt engineering methods to reduce context
  inconsistency hallucinations in zero-shot LLM summarisation of scientific texts.
  Seven prompt methods were tested across six LLMs using eight yeast biotechnology
  abstracts: a baseline prompt, two levels of increasing instruction complexity (PE-1,
  PE-2), and two levels of context repetition (CR-K1, CR-K2) and random addition (RA-K1,
  RA-K2) of abstract sentences.'
---

# Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study

## Quick Facts
- arXiv ID: 2512.00931
- Source URL: https://arxiv.org/abs/2512.00931
- Reference count: 40
- Seven prompt engineering methods tested across six LLMs using eight yeast biotechnology abstracts to reduce context inconsistency hallucinations

## Executive Summary
This pilot study systematically evaluates prompt engineering techniques for reducing context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts. The researchers tested seven distinct prompt variations across six different LLMs using eight abstracts from yeast biotechnology research. Through rigorous statistical analysis, they demonstrate that context repetition and random addition of abstract sentences significantly improve lexical alignment with source material, while increased instruction complexity paradoxically reduces semantic alignment. The study provides empirical evidence that carefully designed prompts can effectively mitigate hallucination risks in scientific text summarisation.

## Method Summary
The study employed a controlled experimental design testing seven prompt engineering methods across six large language models. The prompt variations included a baseline prompt, two levels of increasing instruction complexity (PE-1, PE-2), and two levels each of context repetition (CR-K1, CR-K2) and random addition (RA-K1, RA-K2) of abstract sentences. Each of the eight yeast biotechnology abstracts was processed through all prompt variations across all six LLMs, generating 336 summaries total. Evaluations utilized six metrics: ROUGE-1, ROUGE-2, ROUGE-L for lexical overlap, BERTScore and METEOR for semantic similarity, and cosine similarity for vector space alignment. Statistical significance was assessed using Wilcoxon signed-rank tests with Bonferroni-Holm correction and BCa bootstrap confidence intervals.

## Key Results
- Context repetition and random addition significantly improved lexical alignment with source abstracts (H1, H2 supported)
- Increased instruction complexity reduced semantic alignment with source abstracts (H3 not supported)
- Context repetition showed weak improvement in alignment with repeated key sentences (H4 partially supported)

## Why This Works (Mechanism)
Unknown: The paper does not explicitly explain the underlying mechanisms for why context repetition and random addition improve lexical alignment while increased instruction complexity reduces semantic alignment. The observed effects may relate to how different prompt structures influence the LLM's attention patterns and generation strategies, but specific mechanisms are not detailed in the source material.

## Foundational Learning
- Prompt engineering: Systematic modification of input instructions to influence LLM behavior - needed to control hallucination tendencies in zero-shot settings - quick check: test different instruction phrasings on same input
- Context inconsistency hallucination: LLM generating content that contradicts or omits source context - critical failure mode in scientific summarisation - quick check: compare generated vs source content for contradictions
- Zero-shot summarisation: Generating summaries without task-specific fine-tuning - allows rapid deployment but increases hallucination risk - quick check: measure summary quality without adaptation
- Lexical vs semantic alignment: Different evaluation approaches measuring word overlap vs meaning preservation - both needed for comprehensive assessment - quick check: correlate ROUGE scores with BERTScore
- Statistical significance testing: Methods like Wilcoxon signed-rank with multiple comparison correction - essential for reliable comparison of prompt variants - quick check: apply Bonferroni-Holm correction to multiple tests

## Architecture Onboarding
Component map: Abstract -> Prompt Engineering -> LLM -> Summary -> Evaluation Metrics
Critical path: Prompt design → LLM generation → Multi-metric evaluation → Statistical validation
Design tradeoffs: Increased instruction complexity improves precision but reduces semantic coverage; context repetition improves lexical alignment but may create redundancy
Failure signatures: High lexical alignment with low semantic alignment indicates surface-level matching without understanding; high semantic alignment with low lexical alignment suggests paraphrasing without content fidelity
First experiments:
1. Test baseline prompt across all six LLMs to establish performance baseline
2. Apply CR-K1 variation to assess impact of single context repetition
3. Compare PE-1 vs PE-2 to evaluate instruction complexity effects

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions in the source material provided. The study appears to focus on presenting empirical results rather than identifying future research directions or unresolved theoretical questions.

## Limitations
- Narrow domain focus on yeast biotechnology abstracts (N=8) limits generalizability to other scientific fields with different writing styles and complexity
- Evaluation framework emphasizes lexical and semantic alignment but does not explicitly measure factual accuracy or completeness of key information
- Small experimental space with six LLMs and eight abstracts may not capture full variability in LLM behavior and abstract characteristics

## Confidence
- High confidence: Context repetition and random addition improve lexical alignment metrics
- Medium confidence: Increased instruction complexity reduces semantic alignment
- Medium confidence: Prompt engineering effectively reduces context inconsistency hallucinations given narrow domain scope

## Next Checks
1. Test the same prompt engineering methods across 3-5 diverse scientific domains (e.g., medicine, chemistry, computer science) to assess generalizability
2. Implement human evaluation protocols specifically measuring factual accuracy and completeness of key information in summaries
3. Conduct ablation studies to determine optimal repetition frequency and random addition placement within prompts