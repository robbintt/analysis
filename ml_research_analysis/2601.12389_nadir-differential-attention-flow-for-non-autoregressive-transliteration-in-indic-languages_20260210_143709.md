---
ver: rpa2
title: 'NADIR: Differential Attention Flow for Non-Autoregressive Transliteration
  in Indic Languages'
arxiv_id: '2601.12389'
source_url: https://arxiv.org/abs/2601.12389
tags:
- transliteration
- attention
- differential
- nadir
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-autoregressive (NAR) transliteration
  in Indic languages, where traditional autoregressive models offer high accuracy
  but slow inference, while NAR models are fast but suffer from hallucinations and
  poor length control. The authors introduce NADIR, a novel NAR architecture that
  integrates a Differential Transformer to reduce attention noise and a Mixture-of-Experts
  (MoE) mechanism to enable dynamic token-specific computation.
---

# NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages

## Quick Facts
- arXiv ID: 2601.12389
- Source URL: https://arxiv.org/abs/2601.12389
- Reference count: 4
- Primary result: 13× speedup over AR baseline with competitive CER

## Executive Summary
NADIR addresses non-autoregressive transliteration in Indic languages, where traditional autoregressive models offer high accuracy but slow inference, while NAR models are fast but suffer from hallucinations and poor length control. The authors introduce a novel NAR architecture that integrates a Differential Transformer to reduce attention noise and a Mixture-of-Experts mechanism to enable dynamic token-specific computation. NADIR achieves over a 13× speedup compared to the state-of-the-art autoregressive baseline, maintaining a competitive mean Character Error Rate of 15.78% (vs. 14.44% for AR) and significantly reducing all four types of NAR hallucinations: Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%.

## Method Summary
NADIR uses a 4-layer encoder with Differential Attention and Mixture-of-Experts (MoE) modules, followed by a lightweight MLP-based decoder for parallel token generation. The Differential Attention mechanism partitions queries and keys into two groups, computes separate attention scores, and subtracts the secondary from the primary to suppress irrelevant context. MoE uses 5 experts with Top-2 routing per token to enable script-specific specialization. Implicit sequence termination is achieved by appending [EOS] tokens to targets and truncating outputs at the first [EOS]. The model is trained on the Aksharantar dataset (24.8M training samples across 21 Indic languages) using a composite loss function combining token-level cross-entropy (α=0.8) and load-balancing (β=0.2).

## Key Results
- 13.06× inference speedup over autoregressive baseline
- CER: 15.78% (NAR) vs 14.44% (AR) across 21 Indic languages
- Hallucination reduction: Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, Insertion errors by 16.87%

## Why This Works (Mechanism)

### Mechanism 1: Differential Attention for Noise Suppression
NADIR reduces attention noise via differential subtraction, mitigating NAR hallucinations. The mechanism partitions queries and keys into two groups, computes separate softmax attention scores, then subtracts the secondary from the primary scaled by learnable λ. This suppresses irrelevant context signals that cause hallucinations when sequential inductive bias is removed. Evidence shows differential NAR significantly improves substitutions, omissions, and repetitions.

### Mechanism 2: Mixture-of-Experts for Script-Specialized Computation
MoE enables dynamic, token-specific computation that reduces insertion and remaining repetition errors. Each encoder layer contains 5 expert FFNs with a learned gating network computing routing probabilities for Top-2 experts per token. This allows different experts to specialize for different scripts or character patterns. Error analysis shows MoE leads to reductions in insertion, substitution, and repetition errors.

### Mechanism 3: Implicit Sequence Termination via EOS Prediction
NADIR eliminates explicit length prediction networks and instead learns EOS boundaries within parallel generation. By appending [EOS] tokens to all targets during training and computing loss only up to the first [EOS], the model learns sequence boundaries without auxiliary networks. This strategy is most effective for tasks with clearly defined termination points like transliteration.

## Foundational Learning

- **Non-Autoregressive Decoding**: Understanding the conditional independence assumption and its failure modes (hallucinations) is prerequisite. Quick check: Can you explain why NAR models suffer from "hallucinations" like repetition and insertion errors that AR models typically avoid?
- **Attention Noise in Transformers**: The paper's core hypothesis is that standard attention assigns irrelevant scores; differential attention directly addresses this. Quick check: In a standard multi-head attention layer, what happens when multiple heads capture conflicting features for the same output position?
- **Mixture-of-Experts Routing**: NADIR uses Top-2 routing with load-balancing loss; understanding sparse activation and expert collapse is essential. Quick check: What is the purpose of the load-balancing loss term `L_load` in MoE training, and what symptom appears if it is removed?

## Architecture Onboarding

- **Component map**: Input → RoPE embeddings → 4× [Differential Attention → MoE] → MLP decoder → parallel token predictions → truncate at first [EOS]
- **Critical path**: Input text flows through learnable token embeddings with RoPE positional encodings, through 4 encoder layers each combining Differential Attention and MoE, then through a lightweight MLP decoder that outputs target script vocabulary tokens in parallel, with outputs truncated at the first [EOS] prediction.
- **Design tradeoffs**: Differential attention adds computation (dual Q/K projections) but reduces hallucination cleanup; MoE increases parameter count (27M total) but enables script specialization without proportional inference cost; implicit EOS avoids auxiliary length predictor complexity but may limit generalization to variable-length tasks.
- **Failure signatures**: Expert collapse if all tokens routed to 1-2 experts (increase load-balancing weight β or reduce expert count); persistent insertions indicate MoE needs attention (check expert utilization); premature EOS if outputs truncated too early (inspect EOS prediction accuracy).
- **First 3 experiments**: (1) Train three variants—Standard NAR baseline, +Differential Attention only, +Differential Attention + MoE—and measure CER and hallucination breakdown; (2) Log gating probabilities across validation set per language/script to verify no expert collapse and check script-specific specialization; (3) Replicate batch size scaling test on target hardware to identify optimal batch size for inference throughput.

## Open Questions the Paper Calls Out

- Can NADIR's architecture effectively generalize to other structured sequence generation tasks with local dependencies, such as code refactoring or grammatical error correction? The authors explicitly state intent to extend NADIR beyond transliteration but experimental evaluation is strictly limited to this task.

- How does the implicit sequence termination strategy perform on tasks with highly ambiguous or variable-length output distributions? The paper notes this strategy "will not generalize to tasks with highly ambiguous or variable-length outputs" but validation is restricted to transliteration.

- Does the Differential Attention mechanism's focus on noise reduction impair the modeling of necessary long-range global dependencies? The paper argues NADIR suits tasks relying on "local dependencies" but doesn't test on tasks requiring strong global sequential context.

## Limitations
- Architectural details underspecified: Decoder architecture described only as "lightweight MLP-based" without layer count or dimensions; λ initialization value not provided; training batch size unspecified.
- Limited ablation granularity: No ablations for decoder architecture choices, embedding dimensions, or explicit vs implicit length control comparison.
- Dataset specifics unclear: Vocabulary construction method, maximum sequence lengths, and tokenization details for Indic scripts not fully detailed.

## Confidence
- NADIR achieves 13× speedup vs AR baseline: High confidence
- Differential Attention reduces hallucination errors: Medium confidence
- MoE enables script-specialized computation: Medium confidence
- Implicit EOS termination is effective: Low confidence

## Next Checks
1. Implement and train NADIR variants with different decoder configurations to determine the decoder's contribution to overall performance.
2. During validation, log and visualize gating probabilities and expert activation patterns across different Indic scripts to verify specialization patterns and check for expert collapse.
3. Implement a NADIR variant with explicit length prediction and compare CER, hallucination rates, and inference speed against the implicit EOS approach.