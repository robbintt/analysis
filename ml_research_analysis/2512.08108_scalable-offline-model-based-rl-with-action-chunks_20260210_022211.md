---
ver: rpa2
title: Scalable Offline Model-Based RL with Action Chunks
arxiv_id: '2512.08108'
source_url: https://arxiv.org/abs/2512.08108
tags:
- learning
- policy
- tasks
- model-based
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of scaling offline model-based
  reinforcement learning (MBRL) to complex, long-horizon tasks. The key problem is
  the trade-off between reducing bias in value bootstrapping (favoring longer model
  rollouts) and preventing error accumulation in model predictions (favoring shorter
  rollouts).
---

# Scalable Offline Model-Based RL with Action Chunks

## Quick Facts
- arXiv ID: 2512.08108
- Source URL: https://arxiv.org/abs/2512.08108
- Reference count: 40
- Achieves state-of-the-art performance on large-scale offline MBRL benchmarks, particularly for long-horizon tasks

## Executive Summary
This paper introduces Model-Based RL with Action Chunks (MAC), a novel approach to offline model-based reinforcement learning that addresses the fundamental trade-off between value estimation bias and model prediction error accumulation. MAC uses action-chunk models that predict future states over sequences of actions rather than single-step predictions, reducing the number of recursive model calls and thus error accumulation. The method employs rejection sampling from an expressive behavioral action-chunk policy trained with flow matching to prevent model exploitation from out-of-distribution actions. Experimental results on datasets up to 100M transitions demonstrate that MAC achieves state-of-the-art performance among offline MBRL algorithms, particularly excelling at challenging long-horizon tasks like humanoid maze navigation and puzzle solving.

## Method Summary
MAC addresses the bias-variance trade-off in offline MBRL by introducing action-chunk models that predict state transitions over multi-step action sequences. The method combines a deterministic dynamics model with a behavioral policy learned via flow matching, using rejection sampling to ensure actions are sampled from the training distribution. The action-chunk model takes as input a state and a sequence of actions, outputting the state after executing all actions in the sequence. This reduces the number of recursive model calls compared to traditional multi-step models, limiting error accumulation. The policy is trained to maximize the action-chunk Q-values while being constrained to sample actions from the behavioral policy through rejection sampling. The approach is evaluated on large-scale offline datasets, demonstrating superior performance on long-horizon tasks compared to both previous MBRL methods and some model-free approaches.

## Key Results
- MAC achieves state-of-the-art performance among offline MBRL algorithms on large-scale datasets (up to 100M transitions)
- Outperforms previous MBRL approaches on challenging long-horizon tasks including humanoid maze navigation, cube manipulation, and puzzle solving
- Often surpasses state-of-the-art model-free RL methods on tested benchmarks
- Demonstrates the importance of action chunking in reducing model errors and improving performance on long-horizon tasks

## Why This Works (Mechanism)
MAC addresses the fundamental trade-off in MBRL between value estimation bias (favoring longer model rollouts) and model prediction error accumulation (favoring shorter rollouts). By using action-chunk models that predict multiple steps at once, MAC reduces the number of recursive model calls, limiting error propagation while still enabling long-horizon planning. The flow-matching behavioral policy with rejection sampling ensures that the model is not exploited by out-of-distribution actions, maintaining the validity of the offline training paradigm. This combination allows MAC to effectively balance the bias-variance trade-off, achieving superior performance on long-horizon tasks where traditional MBRL approaches struggle.

## Foundational Learning
- **Offline Reinforcement Learning**: Learning policies from fixed datasets without environment interaction; needed to prevent distributional shift and ensure safe deployment; quick check: verify the dataset contains sufficient coverage of relevant state-action space
- **Model-Based Reinforcement Learning**: Using learned dynamics models for planning and policy optimization; needed to enable sample-efficient learning and long-horizon reasoning; quick check: assess model prediction accuracy on held-out data
- **Flow Matching**: A generative modeling technique for learning expressive distributions; needed to train the behavioral policy that can cover the action space adequately; quick check: evaluate the diversity of actions sampled from the learned policy
- **Rejection Sampling**: A technique for sampling from a target distribution using a proposal distribution; needed to ensure actions are drawn from the behavioral policy while maintaining computational efficiency; quick check: monitor the acceptance rate during training
- **Action Chunking**: Predicting state transitions over sequences of actions rather than single steps; needed to reduce model error accumulation in long-horizon tasks; quick check: compare single-step vs. multi-step model prediction errors
- **Value Expansion**: Using model predictions to estimate value functions; needed to enable planning and credit assignment over long horizons; quick check: analyze value prediction accuracy as a function of rollout length

## Architecture Onboarding

**Component Map**: Dataset -> Data Preprocessing -> Action-Chunk Model -> Behavioral Policy (Flow Matching) -> Q-Function -> Policy Optimization -> Action Rejection Sampling -> Final Policy

**Critical Path**: The critical computational path involves training the action-chunk dynamics model, learning the behavioral policy via flow matching, and performing policy optimization with rejection sampling. The action-chunk model is trained first, followed by the behavioral policy, and finally the Q-function and policy are optimized jointly.

**Design Tradeoffs**: The primary tradeoff is between chunk length and computational complexity. Longer chunks reduce model error accumulation but increase the dimensionality of the action space, making policy learning more challenging. The flow-matching approach for the behavioral policy provides expressiveness but may be computationally expensive for very large datasets. The deterministic dynamics model is simpler but may struggle with stochastic or contact-rich environments.

**Failure Signatures**: Performance degradation may occur when: (1) the action-chunk model cannot accurately predict multi-step transitions, (2) the behavioral policy fails to cover the relevant action space, (3) rejection sampling becomes too restrictive, limiting exploration, or (4) the Q-function cannot effectively learn from action-chunk value estimates. These failures often manifest as poor performance on long-horizon tasks or instability during training.

**First Experiments**:
1. Ablation study comparing MAC with different chunk lengths to quantify the bias-variance tradeoff
2. Comparison of MAC's performance on tasks with varying levels of temporal abstraction requirements
3. Analysis of model prediction error accumulation as a function of rollout length in the action-chunk model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating more expressive generative models or latent dynamics models into the MAC framework overcome the performance gap in contact-rich, long-horizon locomotion tasks?
- Basis in paper: Section 5.1 states that MAC struggles with contact-rich dynamics (e.g., humanoidmaze) due to discontinuities and leaves the incorporation of "more expressive generative models or latent dynamics models" for future work.
- Why unresolved: The current deterministic MLP dynamics model accumulates severe errors in erratic, contact-rich environments, and action chunking alone is insufficient to close the performance gap with model-free methods.
- What evidence would resolve it: Demonstrating that a MAC variant using a latent variable model matches or exceeds state-of-the-art model-free performance on the humanoidmaze-giant benchmark.

### Open Question 2
- Question: How does replacing the deterministic dynamics model with a stochastic generative model affect MAC's performance in partially observable or stochastic environments?
- Basis in paper: Section 4.2 notes that while a deterministic MLP was sufficient for the benchmarks used, it is "possible to replace the MLP with an expressive flow model... in stochastic or partially observable environments."
- Why unresolved: The paper only evaluates deterministic environments; the efficacy of action-chunk value expansion when the dynamics model must account for state uncertainty remains untested.
- What evidence would resolve it: Benchmarking MAC with a probabilistic dynamics head on a standard stochastic control task (e.g., DMControl with noise) and analyzing value prediction stability.

### Open Question 3
- Question: Can the policy learning difficulties caused by exponentially growing action spaces be mitigated to allow for the use of larger action chunks?
- Basis in paper: Appendix D observes that while larger chunks reduce model error, they degrade performance at size 25 because the "action space grows exponentially," complicating Q-function estimation.
- Why unresolved: There is a tension where larger chunks improve model accuracy but hurt policy optimization; current flow rejection sampling does not fully bridge this gap for very long chunks.
- What evidence would resolve it: An ablation study showing improved performance on long-horizon tasks when using chunk sizes >25 combined with a method that scales gracefully with action dimensionality.

## Limitations
- Struggles with contact-rich dynamics and discontinuous environments due to the deterministic MLP dynamics model
- Computational complexity of flow-matching behavioral policy training may not scale well to extremely large datasets or high-dimensional action spaces
- Assumes Markovian dynamics over multi-step action sequences, which may not hold for all environments
- Performance may degrade when the offline dataset has significant distribution shift or limited coverage of relevant state-action space

## Confidence
- **High Confidence**: The empirical performance improvements over baseline MBRL methods on the tested tasks; the mathematical formulation of the action-chunk model and its relationship to traditional MBRL is sound
- **Medium Confidence**: The generalization of results to other long-horizon tasks beyond those tested; the computational efficiency claims relative to model-free baselines
- **Low Confidence**: The robustness of the approach to distributional shifts in the offline data and the sensitivity to hyperparameters like chunk length and rejection sampling thresholds

## Next Checks
1. **Scalability Test**: Evaluate MAC on datasets exceeding 100M transitions with higher-dimensional action spaces to assess computational tractability of the flow-matching policy
2. **Generalization Study**: Test MAC on a diverse set of long-horizon tasks with varying levels of temporal abstraction requirements to validate the breadth of the action-chunking approach
3. **Robustness Analysis**: Systematically evaluate MAC's performance under different levels of distribution shift in the offline data, comparing it to model-free methods that may be more robust to such shifts