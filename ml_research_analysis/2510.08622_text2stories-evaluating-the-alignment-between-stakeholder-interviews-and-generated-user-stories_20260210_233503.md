---
ver: rpa2
title: 'Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and
  Generated User Stories'
arxiv_id: '2510.08622'
source_url: https://arxiv.org/abs/2510.08622
tags:
- stories
- user
- story
- chunks
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TEXT2STORIES, a task and metric for quantifying
  how well user stories derived from elicitation interviews align with the source
  material. It formalizes alignment as matching between transcript chunks and stories,
  measuring correctness (proportion of stories supported by the interview) and completeness
  (proportion of interview covered by stories).
---

# Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories

## Quick Facts
- **arXiv ID:** 2510.08622
- **Source URL:** https://arxiv.org/abs/2510.08622
- **Reference count:** 17
- **Primary result:** LLM judges achieve 0.86 macro-F1 in pairwise alignment; blocking reduces token usage by up to 3× with minimal recall loss.

## Executive Summary
TEXT2STORIES introduces a task and metric for quantifying how well user stories derived from elicitation interviews align with the source material. It formalizes alignment as matching between transcript chunks and stories, measuring correctness (proportion of stories supported by the interview) and completeness (proportion of interview covered by stories). The method uses a two-stage pipeline with chunking, pairwise matching via LLM judges or embedding models, and an optional embedding-based blocking step to reduce computational cost. Experiments on 17 datasets show that LLM judges outperform embedding similarity and cross-encoders, while blocking reduces token usage significantly with minimal recall loss. Metrics reveal that larger models generate more complete story sets, while correctness remains consistently high across models.

## Method Summary
The method formalizes alignment between interview transcripts and user stories as a chunk-story matching task. Transcripts are segmented into overlapping 3-turn windows, and for each story-chunk pair, a binary classifier (LLM judge, cross-encoder, or bi-encoder) determines if the chunk supports the story. An optional embedding-based blocking step retrieves Top-K chunks per story to reduce pairwise comparisons. The final alignment is quantified via Correctness (stories supported by interview) and Completeness (interview covered by stories). Experiments use 17 datasets with automatic and human-annotated labels, measuring matcher performance via macro-F1 and blocking efficiency via token reduction and recall retention.

## Key Results
- LLM judges achieve 0.86 macro-F1 in pairwise alignment, outperforming embedding similarity and cross-encoders.
- Blocking reduces token usage by up to 3× with minimal recall loss (0.9–0.95 recall retention).
- Larger models generate more complete story sets; correctness remains consistently high across model sizes.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If transcripts are segmented into overlapping, multi-turn chunks, pairwise alignment accuracy improves compared to processing the full context.
- **Mechanism:** Long transcripts often exceed context windows or cause "lost-in-the-middle" degradation. By segmenting transcripts into 3-turn overlapping windows (chunks), the model isolates local conversational context. This forces the matcher to verify support against specific evidence rather than hallucinating based on broad topics.
- **Core assumption:** The relevant evidence for a user story is contained within a local window of speaker turns (approx. 3 turns).
- **Evidence anchors:**
  - [section 6.4] Table 4 shows pairwise matching on chunks (0.841 F1) significantly outperforms a full-context baseline (0.489 F1).
  - [section 4.1] Explains that elicitation interviews are Q&A sequences providing contextual information spanning multiple turns.
- **Break condition:** If a requirement is derived from scattered fragments across the entire interview rather than a localized discussion, fixed-window chunking may sever the logical link, lowering recall.

### Mechanism 2
- **Claim:** If an embedding-based blocking step precedes the LLM matcher, computational cost decreases significantly with minimal loss in alignment quality.
- **Mechanism:** Pairwise comparison scales quadratically ($O(N \times M)$). Blocking uses cheap bi-encoder embeddings to retrieve only the Top-$K$ similar chunks for each story. This prunes the candidate set before the expensive LLM judge processes the pairs.
- **Core assumption:** Embedding similarity correlates sufficiently with semantic support such that true positives are rarely filtered out.
- **Evidence anchors:**
  - [section 6.3] Figure 5 and text indicate that blocking reduces tokens by $\approx 3\times$ while maintaining 0.9–0.95 recall.
  - [abstract] Notes that blocking reduces token usage by up to 3× with minimal recall loss.
- **Break condition:** If the interview uses highly specific terminology not captured by general embeddings (domain drift), the blocker may discard valid but lexically dissimilar chunk-story pairs.

### Mechanism 3
- **Claim:** If LLMs are used as pairwise judges, they outperform embedding similarity and cross-encoders in determining "support" between colloquial speech and structured stories.
- **Mechanism:** User stories are compositional (Role + Goal + Benefit). Bi-encoders rely on vector proximity, which misses negation or conditional logic. LLMs can reason over the text to determine if a goal is *justified* by the chunk, handling colloquialisms and implicit needs better than metric learning.
- **Core assumption:** The LLM has sufficient reasoning capability to map unstructured interview dialogue to the structured "As a... I want..." format.
- **Evidence anchors:**
  - [section 6.1] Table 3 shows LLM judges (e.g., Llama3.3-70B) achieving 0.859 F1, outperforming cross-encoders and bi-encoders.
  - [section 4.2] Defines the matcher as answering: "Does this interview chunk justify this user story?"
- **Break condition:** If the interview contains ambiguous or contradictory statements, the LLM may hallucinate a justification (over-generation) where none exists.

## Foundational Learning

- **Concept:** User Stories (Connextra Format)
  - **Why needed here:** The paper specifically evaluates alignment to the standard "As a [role], I want to [action], so that [benefit]" template. Understanding this structure is required to define what "correctness" means.
  - **Quick check question:** Can you identify the Role, Action, and Benefit in the sentence: "As a manager, I want to approve registrations to maintain integrity"?

- **Concept:** Entity Resolution / Blocking
  - **Why needed here:** The pipeline uses blocking to reduce search space. Understanding this concept explains why the method scales to long interviews without quadratic cost explosion.
  - **Quick check question:** Why would comparing every story to every chunk be computationally prohibitive for a 2-hour interview?

- **Concept:** LLM-as-a-Judge
  - **Why needed here:** The core alignment step relies on an LLM classifying a relationship (supports/does not support) rather than generating text.
  - **Quick check question:** How does prompting an LLM for a binary classification (0 or 1) differ from prompting it for a summary?

## Architecture Onboarding

- **Component map:** Chunked transcript -> Optional blocker (Top-K retrieval) -> LLM matcher (binary classification) -> Correctness/Completeness aggregation
- **Critical path:** The **Matcher** dictates accuracy (macro-F1), while the **Blocker** dictates latency and cost. The "Blocking Efficiency" (Section 6.3) is the lever for production viability.
- **Design tradeoffs:**
  - *Chunk Size:* 3-turn window balances context vs. noise. Smaller chunks increase precision but lose context; larger chunks increase cost and noise.
  - *Matcher Selection:* Cross-encoders are faster but require threshold calibration; LLMs are more accurate but slower/costlier.
  - *Blocking Threshold:* Higher $K$ increases recall but reduces efficiency gains.
- **Failure signatures:**
  - *High Correctness, Low Completeness:* The system generates safe, obvious stories but misses edge cases or specific stakeholder needs (common in smaller models).
  - *Low Inter-Annotator Agreement:* If humans disagree on "support" ($\kappa=0.47$ in paper), automatic aligners will struggle to be consistent.
- **First 3 experiments:**
  1. **Replicate Matcher Baseline:** Take the "Public Interview" dataset, chunk it, and run a small LLM (e.g., 8B parameter) as a matcher on the full Cartesian product to verify F1 scores against Table 3.
  2. **Stress Test Blocking:** Implement the blocker and sweep $K$ (Top-5, Top-10, Top-20) to plot the curve in Figure 5 (Tokens vs. Recall) for a specific project.
  3. **Human vs. LLM Generation:** Generate stories from a transcript using a large model (e.g., 70B) and measure Completeness against the ground truth human stories to verify the "Completeness Gap" shown in Figure 4.

## Open Questions the Paper Calls Out

- **Question:** Does the *completeness* metric (transcript coverage) correlate strongly with true requirements completeness as validated by expert catalogs?
  - **Basis in paper:** [explicit] The authors state in the Limitations that "a rigorous correlation study aligning transcript coverage with expert-validated requirement catalogs is needed to establish the metric's effectiveness."
  - **Why unresolved:** The paper proxies completeness via text coverage, but covering text does not guarantee capturing all distinct, high-level stakeholder needs or "unknown unknowns."
  - **Evidence:** A user study where professional analysts produce a "gold standard" set of requirements, which is then compared to the TEXT2STORIES coverage scores for the same interview.

- **Question:** How can chunking strategies be optimized to prevent artificial score inflation caused by overlapping windows?
  - **Basis in paper:** [explicit] The Limitations section notes that overlapping windows "may inflate completeness" and that counting a chunk only once even if it supports multiple stories "potentially underestimates the effective coverage."
  - **Why unresolved:** The current binary formulation of the metrics fails to distinguish between a chunk containing a single requirement versus one containing multiple, leading to potential noise in the completeness score.
  - **Evidence:** Comparative analysis of sliding-window chunking against semantic or turn-based segmentation, measuring the precision of the alignment relative to manual annotation of requirement density.

- **Question:** Does the TEXT2STORIES pipeline transfer effectively to industrial settings with specialized domain jargon and longer interview contexts?
  - **Basis in paper:** [explicit] The authors acknowledge in the Limitations that the datasets "are limited in scope" and "rely primarily on student projects, which may not fully reflect industrial elicitation practices."
  - **Why unresolved:** Student projects likely lack the complexity, ambiguity, and technical depth of real-world stakeholder interviews, leaving the robustness of the LLM judge and blocking mechanism untested in high-stakes domains.
  - **Evidence:** Evaluation of the pipeline's F1-score and blocking efficiency on proprietary, domain-specific industrial transcripts (e.g., healthcare or aerospace) compared to the student dataset baselines.

## Limitations
- **Human annotation quality:** Inter-annotator agreement (Cohen's κ=0.47) is only moderate, suggesting the "support" relation is inherently subjective.
- **Chunking heuristic:** The 3-turn sliding window is a strong inductive bias; if needs are expressed over longer conversational arcs, alignment may become artificially hard.
- **Public dataset scarcity:** Only two datasets are public; main empirical claims rest on private student-project data, limiting reproducibility and generalizability.

## Confidence
- **High:** Chunking improves matcher performance vs. full-context baseline (Table 4, Section 6.4).
- **High:** Blocking reduces tokens by ~3× with minimal recall loss (Section 6.3, Figure 5).
- **Medium:** LLM judges outperform embedding/cross-encoders in F1 (Table 3, Section 6.1) - dependent on specific models.
- **Medium:** Larger models generate more complete story sets (Figure 4) - based on limited public datasets.
- **Low:** "Correctness remains consistently high" - the paper does not provide variance or significance testing across datasets.

## Next Checks
1. **Replicate with alternative LLM judges:** Run the pairwise matching pipeline using GPT-4, Claude, or open-weight models not in the original experiments. Compare macro-F1 and output examples to test model dependence.
2. **Chunk size ablation study:** Repeat alignment experiments with 2-turn, 4-turn, and full-context baselines to quantify the impact of the 3-turn assumption and identify break conditions.
3. **Industrial transcript test:** Apply the pipeline to a real, multi-stakeholder industrial elicitation transcript (e.g., from open-source software projects) and measure alignment metrics against manually annotated stories.