---
ver: rpa2
title: Online Statistical Inference for Contextual Bandits via Stochastic Gradient
  Descent
arxiv_id: '2212.14883'
source_url: https://arxiv.org/abs/2212.14883
tags:
- regression
- policy
- page
- assumption
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies online statistical inference for model parameters
  in a contextual bandit framework, where data is adaptively collected based on past
  observations. The authors propose a general framework for updating decision rules
  via weighted stochastic gradient descent (SGD), allowing different weighting schemes
  for the stochastic gradient.
---

# Online Statistical Inference for Contextual Bandits via Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2212.14883
- Source URL: https://arxiv.org/abs/2212.14883
- Reference count: 40
- Key outcome: This paper studies online statistical inference for model parameters in a contextual bandit framework, where data is adaptively collected based on past observations.

## Executive Summary
This paper proposes a general framework for performing online statistical inference for model parameters in contextual bandit settings using weighted stochastic gradient descent (SGD). The authors develop a method that allows different weighting schemes for the stochastic gradient while establishing asymptotic normality of the parameter estimator. Their approach significantly improves asymptotic efficiency compared to previous methods using inverse probability weights.

## Method Summary
The method uses weighted SGD updates where the stochastic gradient is multiplied by a weight $w_t$ before updating parameters. The framework decouples the policy parameters used for data sampling from the optimization parameters by defining a population loss function $L_{\theta'}(\theta)$. The authors establish asymptotic normality through Polyak-Ruppert averaging of the iterates and provide plug-in estimators for constructing confidence intervals. The method is demonstrated through modified $\epsilon$-greedy and exponential policies in linear and quantile regression settings.

## Key Results
- The proposed framework allows general weighting schemes in SGD updates for contextual bandits, with the vanilla weighting ($w_t=1$) achieving optimal asymptotic efficiency in linear regression.
- The authors establish asymptotic normality of the weighted SGD estimator and provide a Bahadur representation showing slower convergence rates compared to classical SGD due to adaptive data collection.
- The modified $\epsilon$-greedy policy using a Hodges-type threshold prevents inference failure in degenerate models where treatment effects are identical.

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Population Loss for Adaptive Data
The framework achieves asymptotic normality in adaptive environments by defining a population loss function $L_{\theta'}(\theta)$ that decouples policy parameters ($\theta'$) from optimization parameters ($\theta$). This allows the stochastic gradient to remain an unbiased estimator of $\nabla L_{\theta_{t-1}}(\theta_{t-1})$, enabling use of standard SGD convergence proofs even with shifting data distributions.

### Mechanism 2: Polyak-Ruppert Averaging for Variance Reduction
Averaging the SGD iterates ($\bar{\theta}_t$) mitigates noise from adaptive data collection and achieves optimal asymptotic covariance $H^{-1}SH^{-1}$. The "Vanilla" weighting scheme ($w_t=1$) dominates power-law weightings in linear settings by smoothing noise without the high variance of probability weighting.

### Mechanism 3: Modified $\epsilon$-Greedy for Degenerate Models
A modified policy using a Hodges-type threshold prevents inference failure in degenerate models by ensuring continuity in the policy function. Standard $\epsilon$-greedy creates a discontinuity at $\theta_0 = \theta_1$, violating assumptions for asymptotic normality. The modified policy detects degenerate regimes and enforces a fixed 50/50 exploration rate.

## Foundational Learning

- **Concept:** Stochastic Gradient Descent (SGD) with Weighting
  - **Why needed here:** The core engine is a modification of standard SGD (Equation 7). Understanding how weighting the gradient ($w_t \nabla \ell$) affects the optimization path and final stationary distribution is crucial.
  - **Quick check question:** How does the "Vanilla" weight ($w_t=1$) differ from Inverse Probability Weighting ($w_t \propto 1/P(A|X)$) in terms of gradient bias and variance?

- **Concept:** Contextual Bandits and Adaptive Data
  - **Why needed here:** The data is not i.i.d.; action $A_t$ depends on history $H_{t-1}$. This adaptivity is why standard central limit theorems fail and specialized analysis (Bahadur representation) is required.
  - **Quick check question:** Why does adaptive data collection typically lead to slower convergence rates ($O_p(t^{-\alpha/4})$ remainder) compared to classical i.i.d. settings?

- **Concept:** Bahadur Representation
  - **Why needed here:** This tool decomposes estimator error into a leading linear term (asymptotically Normal) and a remainder term. Analyzing the remainder reveals specific costs of adaptivity.
  - **Quick check question:** What does the remainder term in the Bahadur representation tell us about the impact of step-size $\alpha$ on convergence speed?

## Architecture Onboarding

- **Component map:** Input Layer: $X_t$ -> Policy Module: Modified $\epsilon$-greedy or Exponential policy -> Environment: Returns $Y_t$ -> Weighting Engine: Computes $w_t$ -> SGD Updater: $\theta_t \leftarrow \theta_{t-1} - \eta_t w_t \nabla \ell$ -> Inference Module: Maintains $\bar{\theta}_t$ and computes plug-in estimators

- **Critical path:** The "Weighting Engine" is the critical new component. If weights are calculated incorrectly (e.g., dividing by near-zero probability without clipping), the system experiences gradient explosion.

- **Design tradeoffs:**
  - **Vanilla vs. IPW:** Vanilla ($w_t=1$) provides better asymptotic efficiency but targets a weighted population loss $L_{\theta'}$ rather than standard equal-weighted loss. IPW targets standard loss but suffers massive variance inflation.
  - **Step size $\alpha$:** Higher $\alpha$ (slower decay) helps convergence in difficult landscapes but slows the rate of the remainder term in the Bahadur representation. The paper suggests $\alpha=0.8$ as optimal balance.

- **Failure signatures:**
  - **Exploding CIs:** If using IPW with low exploration $\epsilon$, plug-in variance estimator becomes unstable or infinite.
  - **Non-coverage in Degeneracy:** If using standard $\epsilon$-greedy in degenerate model ($\theta_0 = \theta_1$), coverage drops due to violated asymptotic normality assumptions.

- **First 3 experiments:**
  1. **Linear Regression Baseline:** Implement weighted SGD with Vanilla ($w=1$) vs. IPW in non-degenerate setting. Verify Vanilla achieves narrower CIs while maintaining coverage.
  2. **Degenerate Model Stress Test:** Run modified $\epsilon$-greedy policy in setting where $\theta_0^* = \theta_1^*$. Compare coverage rates against standard $\epsilon$-greedy to verify fix for discontinuity issue.
  3. **Exponential Policy Check:** Switch policy to Exponential (Softmax) policy. Verify uniform convergence property by checking coverage stability across different parameter initializations.

## Open Questions the Paper Calls Out

### Open Question 1
Can theoretical guarantees for weighted SGD be extended to policies that depend on the entire history of observations rather than just the most recent parameter estimate? The current work focuses on policies dependent only on $X_t, \theta_{t-1}$, with history-dependent statistics left for future work due to the complex dependency structure they introduce.

### Open Question 2
Can online inference methods other than the plug-in estimator (such as batch-means or bootstrap) be effectively applied to this adaptive weighted SGD framework, particularly for non-smooth loss functions? While the plug-in estimator requires estimating the Hessian matrix, alternative estimators like batch-means avoid this but haven't been validated for the specific adaptive, weighted setting.

### Open Question 3
Can an online inference procedure for $\epsilon$-greedy policies be designed that provides uniform validity across both degenerate ($\theta^*_0 = \theta^*_1$) and non-degenerate models? The modified $\epsilon$-greedy policy achieves pointwise but not uniform asymptotic normality due to the superefficiency of the Hodges estimator construction used to handle the degenerate case.

### Open Question 4
Is the "vanilla" weighting scheme ($w_t = 1$) asymptotically optimal for general convex losses and non-Gaussian covariates, or is its optimality limited to the linear regression setting with Gaussian features? The explicit derivation of the covariance matrix and optimality proof rely on specific structure of Gaussian integration and closed-form expression of the expected Hessian.

## Limitations

- **Inference non-uniformity**: Asymptotic normality is not uniform over the parameter space; there is no universal time $T_0$ after which the Gaussian approximation error is bounded for all parameters.
- **Adaptive data cost**: The Bahadur representation explicitly shows adaptive data collection leads to slower convergence rates ($O_p(t^{-\alpha/4})$ remainder) compared to classical i.i.d. settings.
- **Computational complexity**: The method requires maintaining and updating Hessian and Gram matrix estimators, which may be computationally intensive for high-dimensional problems.

## Confidence

- **High confidence**: Asymptotic normality framework and weighting mechanism (Mechanisms 1-2). The mathematical derivations for decoupling of $\theta'$ and $\theta$, and the Polyak-Ruppert averaging approach, are well-grounded and directly supported by the text.
- **Medium confidence**: Modified $\epsilon$-greedy policy for degenerate models (Mechanism 3). While the discontinuity issue is clearly identified and the fix is logically sound, validation is limited.
- **Medium confidence**: Exponential policy uniform asymptotic normality claim. The paper asserts uniform coverage but provides limited empirical verification compared to the linear regression case.

## Next Checks

1. **Degeneracy coverage verification**: Implement the modified $\epsilon$-greedy policy and systematically test coverage rates across varying degrees of model degeneracy ($\theta_0^* = \theta_1^*$). Compare against standard $\epsilon$-greedy to confirm the discontinuity fix works as intended.

2. **Weighting scheme efficiency test**: Run controlled simulations comparing the "Vanilla" ($w_t=1$) and "IPW" weighting schemes in both non-degenerate and highly skewed action probability scenarios. Quantify the trade-off between targeting the correct loss function (IPW) and achieving lower asymptotic variance (Vanilla).

3. **Bahadur remainder sensitivity**: Conduct a numerical study to empirically measure the $O_p(t^{-\alpha/4})$ remainder term for different values of $\alpha$ (e.g., 0.6, 0.8, 0.9). This will validate the theoretical prediction about the convergence speed penalty imposed by adaptive data collection.