---
ver: rpa2
title: 'Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy
  Dynamics'
arxiv_id: '2512.04006'
source_url: https://arxiv.org/abs/2512.04006
tags:
- singular
- dynamics
- values
- hadamard
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first rigorous proof of convergence to
  neural collapse under multi-class cross-entropy training in a non-convex setting.
  The key innovation is showing that Hadamard initialization diagonalizes the softmax
  operator, freezing the singular vectors of the weight matrices and reducing the
  dynamics entirely to their singular values.
---

# Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics

## Quick Facts
- arXiv ID: 2512.04006
- Source URL: https://arxiv.org/abs/2512.04006
- Authors: Connall Garrod; Jonathan P. Keating; Christos Thrampoulidis
- Reference count: 40
- This paper provides the first rigorous proof of convergence to neural collapse under multi-class cross-entropy training in a non-convex setting.

## Executive Summary
This paper establishes the first rigorous proof of neural collapse convergence under multi-class cross-entropy training in a non-convex setting. The key innovation is demonstrating that Hadamard initialization diagonalizes the softmax operator, which freezes the singular vectors of weight matrices and reduces dynamics to singular values only. This enables extending exact dynamics analysis from MSE to CE loss. The authors construct an explicit KL divergence Lyapunov function that proves global convergence despite the non-convex landscape's spurious critical points, providing the first path-level analysis of implicit bias in the unconstrained features model with CE loss.

## Method Summary
The authors show that Hadamard initialization diagonalizes the softmax operator, effectively freezing the singular vectors of weight matrices and reducing the dynamics to their singular values. This key insight enables extending exact dynamics analysis from MSE to CE loss. They construct an explicit KL divergence Lyapunov function to establish global convergence to neural collapse despite the presence of spurious critical points in the non-convex landscape. The analysis focuses on the unconstrained features model and provides a path-level understanding of the implicit bias that emerges during CE training.

## Key Results
- First rigorous proof of neural collapse convergence under multi-class cross-entropy training in a non-convex setting
- Hadamard initialization diagonalizes softmax, freezing singular vectors and reducing dynamics to singular values
- KL divergence Lyapunov function establishes global convergence despite spurious critical points
- Non-monotonic convergence behavior for K≥8 classes, with explicit initializations showing natural distance metrics initially increase
- Exponential slowdown in dynamics as logit norms grow

## Why This Works (Mechanism)
The mechanism relies on Hadamard initialization's unique property of diagonalizing the softmax operator. When softmax is diagonalized, the singular vectors of weight matrices become frozen, and only their singular values evolve during training. This simplification transforms the complex non-convex optimization landscape into a tractable form where dynamics can be exactly analyzed. The KL divergence Lyapunov function then provides a global convergence certificate by measuring the distance between the current state and the neural collapse solution, ensuring that optimization progresses despite the presence of spurious critical points.

## Foundational Learning
- **Softmax diagonalization**: Understanding how Hadamard initialization transforms softmax into a diagonal operator is crucial for grasping why singular vectors freeze. Quick check: Verify that Hadamard matrices are orthogonal and have entries of ±1/√n.
- **Neural collapse phenomenon**: The convergence to feature-class alignment and uniform within-class variation requires understanding the equilibrium structure. Quick check: Confirm that neural collapse involves both feature normalization and class-mean separation.
- **KL divergence as Lyapunov function**: The construction of a proper Lyapunov function for non-convex optimization requires careful analysis. Quick check: Verify that KL divergence is non-negative and zero only at the neural collapse equilibrium.
- **Unconstrained features model**: Understanding the mathematical simplification when features are treated as optimization variables rather than network outputs. Quick check: Confirm that this model assumes linear classifiers with freely optimized features.
- **Non-convex optimization dynamics**: The behavior of gradient flow in landscapes with spurious critical points requires specialized analysis techniques. Quick check: Verify that spurious critical points don't prevent global convergence in this specific setting.
- **Exponential slowdown phenomenon**: Understanding how increasing logit norms affect optimization speed requires asymptotic analysis. Quick check: Confirm that the slowdown is logarithmic in the logit norm.

## Architecture Onboarding

Component Map:
- Hadamard initialization -> Diagonalized softmax -> Frozen singular vectors -> Simplified dynamics -> KL divergence Lyapunov function -> Global convergence proof

Critical Path:
The critical path involves initializing with Hadamard matrices, which diagonalizes the softmax operator. This diagonalization freezes the singular vectors of weight matrices, reducing the optimization to tracking only singular values. The KL divergence Lyapunov function then provides the convergence certificate by measuring progress toward the neural collapse equilibrium. Each step depends on the previous: without Hadamard initialization, the softmax remains non-diagonal; without frozen singular vectors, the dynamics remain intractable; without the Lyapunov function, global convergence cannot be established.

Design Tradeoffs:
The main tradeoff is between initialization specificity and theoretical tractability. Hadamard initialization enables rigorous analysis but may not reflect practical training scenarios where random initialization is standard. The unconstrained features model simplifies analysis but loses architectural realism. The paper addresses this by showing that standard random initialization also converges to neural collapse, though the theoretical analysis requires the Hadamard case.

Failure Signatures:
If Hadamard initialization fails to diagonalize softmax (e.g., due to numerical precision issues), the singular vectors won't freeze and the dynamics won't simplify. If the KL divergence doesn't decrease monotonically, the Lyapunov function construction fails. If spurious critical points trap optimization, global convergence cannot be established. These failures would manifest as either non-convergence or convergence to incorrect equilibria.

First Experiments:
1. Verify Hadamard initialization diagonalizes softmax by computing the Jacobian of softmax with Hadamard-initialized weights
2. Test non-monotonic convergence empirically by tracking distance metrics during training for K≥8 classes
3. Measure exponential slowdown by plotting optimization speed against logit norm growth

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes unconstrained features model, limiting direct applicability to standard deep networks with architectural constraints
- Non-monotonic convergence behavior for K≥8 classes lacks extensive empirical validation across different architectures and datasets
- Exponential slowdown as logit norms grow is identified theoretically but practical training efficiency implications remain unclear
- Theoretical results rely on specific assumptions about initialization and loss landscape structure that may not hold in practical scenarios

## Confidence

High: Hadamard initialization mechanism for CE loss dynamics
Medium: Global convergence proof using KL divergence Lyapunov function
Low: Non-monotonic convergence behavior for K≥8 classes
Low: Practical training implications of exponential slowdown

## Next Checks

1. Verify non-monotonic convergence empirically across multiple datasets and architectures for K≥8 classes
2. Test the exponential slowdown effect by measuring training time degradation as logit norms increase
3. Extend analysis to constrained feature models and standard deep network architectures with regularization