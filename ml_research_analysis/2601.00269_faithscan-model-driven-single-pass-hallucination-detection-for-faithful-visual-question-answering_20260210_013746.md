---
ver: rpa2
title: 'FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual
  Question Answering'
arxiv_id: '2601.00269'
source_url: https://arxiv.org/abs/2601.00269
tags:
- hallucination
- visual
- uncertainty
- answer
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FaithSCAN is a single-pass, model-driven framework for detecting\
  \ faithfulness hallucinations in VQA. It leverages multiple internal uncertainty\
  \ signals from vision-language models\u2014including token-level generation uncertainty,\
  \ visual patch embeddings, and cross-modal alignment features\u2014processed via\
  \ branch-wise encoding and uncertainty-aware attention fusion."
---

# FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering

## Quick Facts
- arXiv ID: 2601.00269
- Source URL: https://arxiv.org/abs/2601.00269
- Reference count: 40
- Primary result: FaithSCAN achieves up to 0.927 AUROC on POPE by fusing token-level, visual patch, and cross-modal uncertainty signals in a single forward pass

## Executive Summary
FaithSCAN is a single-pass framework for detecting hallucinations in VQA by leveraging internal uncertainty signals from frozen VLMs. It extracts token-level decoding uncertainty, visual patch embeddings, and cross-modal alignment features, then fuses them via uncertainty-aware attention. Supervision is generated by a strong external VLM acting as a judge. The method avoids repeated sampling, enabling fast and scalable hallucination detection.

## Method Summary
FaithSCAN operates by extracting four types of uncertainty signals (log-likelihoods, hidden states, visual patches, aligned visual tokens) from a frozen VLM in a single forward pass. These signals are compressed via branch encoders and fused using a gated residual attention module. A lightweight MLP classifier outputs a binary hallucination probability. Training labels are generated via a Visual-NLI protocol using a strong external judge (Qwen2.5-VL-32B-Instruct). The model is trained with BCE loss, AdamW optimizer, and early stopping on validation AUROC.

## Key Results
- Achieves up to 0.927 AUROC on POPE and 0.842 on VQA v2
- Visual uncertainty signals significantly improve detection on object-level hallucinations
- Single-pass inference is 10–100× faster than repeated-sampling baselines
- Strong label quality with 0.950–0.990 agreement to human annotations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing heterogeneous internal uncertainty signals captures distinct hallucination failure pathways better than output-only metrics.
- **Mechanism:** FaithSCAN extracts token-level decoding uncertainty, intermediate visual representations, and cross-modal aligned features. These are processed separately and aggregated. Object hallucinations often stem from perceptual failure (captured by visual patches), while reasoning errors manifest in decoding states.
- **Core assumption:** Hallucinations leave reliable, distinct traces in the model's internal hidden states and attention patterns that are not fully captured by final output logits alone.
- **Evidence anchors:** Abstract: "...leveraging internal uncertainty signals... including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features." Section VI-C1: "On POPE... integrating pure visual patch uncertainty leads to significant performance gains... suggests that object hallucinations are closely tied to perceptual failures."
- **Break condition:** May fail if the target VLM's intermediate layers are not accessible or if the architecture encodes uncertainty in layers not selected by the current extraction strategy.

### Mechanism 2
- **Claim:** A supervised classifier trained on "frozen" VLM states can approximate hallucination risk without repeated sampling.
- **Mechanism:** Instead of generating multiple samples to calculate Semantic Entropy, FaithSCAN performs a single forward pass and treats the internal state as a static feature vector. A lightweight MLP learns to map these vectors to a hallucination probability, conditioned on model-generated labels.
- **Core assumption:** The internal activation state of a single greedy forward pass contains sufficient information to predict the variance and reliability that would typically require stochastic sampling to observe.
- **Evidence anchors:** Abstract: "The method operates in a single forward pass without repeated sampling or external verification..." Section IV-B: "...our learning objective is not to approximate the full output distribution... but to directly model [the hallucination probability] using uncertainty signals... encoded in the VLM during a single forward pass."
- **Break condition:** Efficacy drops if the model is overconfident (low uncertainty) on hallucinated content, a phenomenon known as "calibration error," where internal states misleadingly indicate high certainty.

### Mechanism 3
- **Claim:** Model-driven "Visual-NLI" labeling provides a low-cost, scalable supervision signal that correlates highly with human judgment.
- **Mechanism:** The authors use a strong external VLM (Qwen2.5-VL-32B) to act as a judge. It assesses the consistency between the generated answer, the image, and the ground truth, outputting "Entailment," "Contradiction," or "Uncertain." This automates the labeling of training data for the detector.
- **Core assumption:** The external Judge VLM is significantly more capable and reliable than the target VLM at verifying visual grounding, and its judgments can be treated as ground truth for training a detector for the weaker model.
- **Evidence anchors:** Section IV-C1: "A strong external VLM serves as a semantic judge... producing model-aware labels." Section VI-B: "Quantitatively (Table V), the proposed supervision exhibits strong agreement with human annotations, achieving agreement scores in the range of 0.950–0.990."
- **Break condition:** If the Judge VLM itself has systematic blind spots or biases, it will propagate noisy labels, confusing the detector.

## Foundational Learning

- **Concept: Multimodal Internal Representations**
  - **Why needed here:** You must understand how VLMs process images (vision encoder → alignment layer → LLM embedding). FaithSCAN relies on hooking into these specific layers to extract the "visual patch" and "cross-modal" uncertainty signals.
  - **Quick check question:** Can you identify which layer in a standard LLaVA architecture represents "cross-modal alignment" versus "raw visual features"?

- **Concept: Uncertainty vs. Hallucination**
  - **Why needed here:** High predictive entropy often correlates with hallucinations, but they are not identical. This paper exploits the correlation but acknowledges that confident hallucinations (low entropy) are harder to detect.
  - **Quick check question:** Why might a model generate a fluent, confident answer that is entirely hallucinated?

- **Concept: Supervised Anomaly Detection**
  - **Why needed here:** Unlike traditional uncertainty estimation (unsupervised statistics), this method frames detection as a binary classification task. Understanding class imbalance and metric selection (AUROC vs. Accuracy) is crucial for replicating the results.
  - **Quick check question:** Why is AUROC preferred over Accuracy when evaluating a hallucination detector trained on potentially imbalanced datasets?

## Architecture Onboarding

- **Component map:** Input VLM → Feature extractors (log-likelihoods, hidden states, visual patches, aligned tokens) → Branch encoders (d=64) → Uncertainty-aware attention fusion → Binary classifier

- **Critical path:** The most sensitive step is the **extraction hook placement**. The paper notes that using the wrong layer (too early or too deep) degrades performance. For LLaVA, this is the projector output; for InstructBLIP, it is the Q-Former output. Incorrect hooking results in "missing" the visual uncertainty signal.

- **Design tradeoffs:**
  - **Generality vs. Specificity:** The detector is "model-dependent." You cannot train on LLaVA embeddings and apply directly to Qwen-VL without retraining the Branch Encoders.
  - **Efficiency vs. Granularity:** It operates in a single pass (fast) but currently provides only an *answer-level* score, not token-level localization, which limits fine-grained explainability.

- **Failure signatures:**
  - **High False Positives:** The model flags correct answers as hallucinations when the visual evidence is subtle (e.g., counting tasks) and the internal entropy is naturally high.
  - **OOD Collapse:** Performance degrades significantly on out-of-distribution datasets if the hallucination types differ (object vs. relation).

- **First 3 experiments:**
  1. **Hook Verification:** Extract features from a known hallucinated sample vs. a faithful one. Visualize the t-SNE of the "Visual Patch" vs. "Aligned Token" embeddings to confirm they separate.
  2. **Ablation by Modality:** Disable the visual branch (text-only) and measure the AUROC drop on POPE (object hallucinations) to validate that visual uncertainty is indeed the driver for that failure type.
  3. **Label Agreement Check:** Run the Visual-NLI labeling pipeline on 50 random samples and manually verify the Judge's "Contradiction" label matches human intuition before training.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can hallucination detection models maintain robust performance when encountering distribution shifts across different hallucination types (e.g., from object-level to false-premise hallucinations)?
  - **Basis in paper:** Table IV shows OOD AUROC values as low as 0.393–0.514 when transferring between datasets with different hallucination profiles; the authors state "a performance gap between ID and OOD underscores the challenge of hallucination detection under distribution shifts, motivating future work on improving generalization."
  - **Why unresolved:** Multi-source training partially mitigates but does not close the gap; the fundamental challenge of generalizing across hallucination taxonomies remains unaddressed.
  - **What evidence would resolve it:** A systematic study across controlled hallucination-type manipulations, combined with domain adaptation techniques applied to the uncertainty signal space.

- **Open Question 2:** What determines the optimal decoder layer for uncertainty signal extraction, and can layer selection be automated for novel VLM architectures?
  - **Basis in paper:** Fig. 7 shows layer-wise AUROC varies by up to 0.235 across models; the authors manually select intermediate layers (2/3 depth for most models, 1/2 for InstructBLIP) but do not establish a principled selection criterion.
  - **Why unresolved:** The relationship between layer position, semantic abstraction level, and hallucination-relevant uncertainty is characterized empirically but not theoretically.
  - **What evidence would resolve it:** Probing experiments correlating layer-wise representations with specific failure pathways (perceptual vs. reasoning failures), or a learnable layer-selection mechanism.

- **Open Question 3:** Can the contribution of visual uncertainty signals to hallucination detection be systematically predicted based on question complexity or hallucination type?
  - **Basis in paper:** Fig. 5 shows visual patch uncertainty yields significant gains on POPE (object-level) but limited improvement on VQA v2 (complex reasoning); the authors note "the source of hallucination is architecture-dependent and cannot be attributed to a single modality."
  - **Why unresolved:** The paper provides qualitative explanations but no quantitative model predicting when visual uncertainty will be informative.
  - **What evidence would resolve it:** Regression analysis mapping question/hallucination features to per-branch attention weights, or dynamic branch gating conditioned on input characteristics.

## Limitations
- **Model dependence:** The detector is tuned to specific VLMs and requires retraining for new architectures.
- **Distribution sensitivity:** Performance degrades on out-of-distribution datasets with different hallucination types.
- **Answer-level only:** Provides only answer-level hallucination scores, not token- or object-level localization.

## Confidence
- **High:** The fusion of heterogeneous uncertainty signals is empirically validated and well-supported by ablation studies.
- **Medium:** The supervised learning approach depends on the quality of Visual-NLI labels; while agreement with humans is high, label noise could affect performance.
- **Low:** The method's generalization to novel VLM architectures and hallucination types remains uncertain due to its model-dependent nature.

## Next Checks
1. **Hook Verification:** Extract features from a known hallucinated sample vs. a faithful one. Visualize the t-SNE of the "Visual Patch" vs. "Aligned Token" embeddings to confirm they separate.
2. **Ablation by Modality:** Disable the visual branch (text-only) and measure the AUROC drop on POPE (object hallucinations) to validate that visual uncertainty is indeed the driver for that failure type.
3. **Label Agreement Check:** Run the Visual-NLI labeling pipeline on 50 random samples and manually verify the Judge's "Contradiction" label matches human intuition before training.