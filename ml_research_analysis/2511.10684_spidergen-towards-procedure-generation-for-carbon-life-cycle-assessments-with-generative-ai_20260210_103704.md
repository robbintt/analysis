---
ver: rpa2
title: 'SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments
  with Generative AI'
arxiv_id: '2511.10684'
source_url: https://arxiv.org/abs/2511.10684
tags:
- product
- processes
- spidergen
- process
- products
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpiderGen is an LLM-based workflow that automates generation of
  Product Category Rules Process Flow Graphs (PCR PFGs) for carbon Life Cycle Assessments.
  It uses world knowledge and text processing to derive upstream, core, and downstream
  processes, then orders them into a Directed Acyclic Graph.
---

# SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI

## Quick Facts
- **arXiv ID**: 2511.10684
- **Source URL**: https://arxiv.org/abs/2511.10684
- **Reference count**: 33
- **Key outcome**: LLM-based workflow that automates generation of Product Category Rules Process Flow Graphs (PCR PFGs) for carbon Life Cycle Assessments

## Executive Summary
SpiderGen is an LLM-based workflow that automates generation of Product Category Rules Process Flow Graphs (PCR PFGs) for carbon Life Cycle Assessments. It uses world knowledge and text processing to derive upstream, core, and downstream processes, then orders them into a Directed Acyclic Graph. SpiderGen was evaluated on 65 real-world LCA documents and achieved an F1-score of 65%, outperforming a one-shot prompting baseline at 53%. It costs under $1 and takes under 10 minutes per PFG, compared to traditional methods costing over $25,000 and taking up to 21 days.

## Method Summary
SpiderGen is a 4-step pipeline: (1) LLM generates 15 diverse sample products per category, (2) LLM generates process lists per product with lifecycle phase labels, (3) SBERT embeddings + K-means clustering per phase (Davies-Bouldin optimization) + LLM cluster summarization, (4) ordering via implicit phase ordering (upstream→core→downstream) + LLM explicit ordering within phases. The system outputs a Directed Acyclic Graph with upstream, core, and downstream processes, plus subprocess edges.

## Key Results
- Achieved 65% F1-score on 65 real-world LCA documents, outperforming 53% one-shot prompting baseline
- Cost under $1 and under 10 minutes per PFG, compared to $25,000+ and 21 days for traditional methods
- Uses o1-mini model recommended for cost-sensitive deployment: $0.36/PFG, PMI 0.046

## Why This Works (Mechanism)

### Mechanism 1: Diversity-Sampling via Product Instantiation
- Claim: Generating multiple diverse sample products before process extraction improves generalization across product categories.
- Mechanism: SpiderGen first prompts an LLM to list diverse products within a category, queries processes for each, then clusters to find commonalities. This prevents overfitting to a single product variant.
- Core assumption: Process overlap across diverse products within a category indicates category-general processes.
- Evidence anchors:
  - [abstract]: "clustering of sample product processes" enables PFG construction
  - [section 4]: "asking for too few products would result in PFG information that was overfitted to a specific niche within a product category"
  - [corpus]: Weak direct evidence; neighbor papers focus on LCA automation, not sampling strategies
- Break condition: Highly heterogeneous categories where few shared processes exist; niche categories may degrade with more samples (paper notes "Moka Coffee" performed worse with more products)

### Mechanism 2: Taxonomy-Constrained Graph Structure
- Claim: Enforcing upstream → core → downstream ordering constraints improves DAG accuracy.
- Mechanism: SpiderGen uses LCA ontology to label processes by lifecycle phase, then applies implicit ordering across phases before LLM-based explicit ordering within phases. This reduces the search space for edge placement.
- Core assumption: LCA phase taxonomy aligns with real-world process dependencies.
- Evidence anchors:
  - [abstract]: SpiderGen "leverages ontology-based process generation" for "upstream, core, and downstream phases"
  - [section 4]: "using implicit ordering based on life cycle phases dictated by the taxonomy of LCA, SpiderGen is able to generate more accurate explicit orderings"
  - [corpus]: "Entity Linking using LLMs for Automated Product Carbon Footprint Estimation" uses similar taxonomy-guided approaches for LCA automation
- Break condition: Products with overlapping or iterative phases (e.g., maintenance loops) violate strict phase ordering

### Mechanism 3: Embedding-Based Process Generalization
- Claim: SBERT embeddings + K-means clustering produce category-level processes from instance-level process lists.
- Mechanism: Each process is embedded using all-mpnet-base-v2, clustered separately per lifecycle phase using K-means with Davies-Bouldin optimization, then an LLM summarizes each cluster into a coarse process description.
- Core assumption: Semantic similarity in embedding space corresponds to functional equivalence in LCA processes.
- Evidence anchors:
  - [abstract]: SpiderGen achieves 65% F1 vs. 53% baseline, demonstrating improved process capture
  - [section 4]: Clustering "only processes that are in the same life cycle phase" prevents cross-phase conflation
  - [corpus]: Weak direct evidence; neighbor papers don't evaluate clustering for LCA specifically
- Break condition: Jargon-heavy or domain-specific process names with poor SBERT representation; clusters may merge semantically similar but functionally distinct processes

## Foundational Learning

- **Life Cycle Assessment (LCA) Phases**: Upstream (raw materials, extraction), Core (manufacturing), Downstream (use, disposal)
  - Why needed here: SpiderGen's DAG structure and ordering logic depend entirely on phase labels
  - Quick check question: For a "laptop" product, would "battery cell fabrication" be upstream or core?

- **Directed Acyclic Graphs (DAGs) with Subprocess Edges**
  - Why needed here: SpiderGen outputs two edge types—main process ordering (E_m) and subprocess relationships (E_s)
  - Quick check question: In a PFG, what does edge (A, B) ∈ E_s indicate vs. (A, B) ∈ E_m?

- **Davies-Bouldin Score for Cluster Selection**
  - Why needed here: SpiderGen uses this metric to select optimal K for each lifecycle phase's clusters
  - Quick check question: Would a lower or higher Davies-Bouldin score indicate better clustering?

## Architecture Onboarding

- **Component map**:
  - Step 1: Product Generator (LLM) → list of 15 diverse products per category
  - Step 2: Process Extractor (LLM) → process lists per product with phase labels
  - Step 3: Embedder (SBERT: all-mpnet-base-v2) → process embeddings
  - Step 4: Clusterer (K-means + Davies-Bouldin) → process groups per phase
  - Step 5: Summarizer (LLM) → coarse process descriptions
  - Step 6: Graph Assembler (LLM + taxonomy) → final DAG with E_m and E_s edges

- **Critical path**: Step 1 (product diversity) → Step 2 (process coverage) → Step 3-4 (clustering quality). Errors propagate; missing a key process early cannot be recovered later.

- **Design tradeoffs**:
  - More sample products: Better coverage but risk over-generalization for niche categories
  - o1-preview vs o1-mini: o1-preview achieves higher PMI (0.051 vs 0.046) but costs 13× more; o1-mini recommended for cost-sensitive deployment
  - Clustering per-phase vs global: Per-phase prevents cross-phase conflation but requires separate K selection

- **Failure signatures**:
  - High PMI variance in downstream processes (IQR wider than upstream/core): Indicates underspecified ground truth or over-generation of disposal scenarios
  - Missing auxiliary processes (e.g., "machine maintenance"): LLM focused on primary manufacturing, not peripheral processes
  - Hallucinated processes labeled "wrong": System boundary mismatch between SpiderGen assumptions and product-specific norms

- **First 3 experiments**:
  1. **Reproduce baseline comparison**: Run SpiderGen vs LLMDirect vs LLMExample on 5 held-out product categories from the paper's dataset; verify PMI and F1 deltas match reported ~12% F1 improvement
  2. **Ablate sample product count**: Test 5, 10, 15, 20 products for "Grain Mill Products" and "Moka Coffee"; plot PMI to confirm paper's observation that niche categories degrade with more samples
  3. **Stress-test system boundaries**: Run SpiderGen on a product category not in EPD dataset (e.g., "laptops"); manually evaluate whether auxiliary processes like "data center usage" or "rare-earth mining" are captured

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can human-AI collaboration methods be developed to dynamically define appropriate system boundaries for specific product categories?
- Basis in paper: [explicit] The authors state that strict adherence to ISO standard boundaries causes errors, and "exploring human-AI collaboration methods to make appropriate choices for system boundaries is a promising next step."
- Why unresolved: SpiderGen currently relies on static assumptions for system boundaries, leading to the omission of auxiliary processes or the hallucination of irrelevant ones.
- What evidence would resolve it: An interactive workflow where expert feedback on boundaries demonstrably reduces the count of "missing" and "wrong" labels in the F1-score evaluation.

### Open Question 2
- Question: Can Retrieval Augmented Generation (RAG) be utilized to increase the transparency and traceability of generated PFGs?
- Basis in paper: [explicit] The discussion suggests that "utilizing Retrieval Augmented Generation (RAG)... will be a promising method for increasing the transparency and traceability of PFG generation."
- Why unresolved: The current zero-shot implementation relies on the LLM's internal weights without external citations, making it difficult to verify the source of specific process inclusions.
- What evidence would resolve it: A RAG-enhanced version of SpiderGen that successfully links generated process nodes to specific literature or database entries with high accuracy.

### Open Question 3
- Question: How can the uncertainty of LLM-generated PFGs be measured and communicated to LCA practitioners?
- Basis in paper: [explicit] The paper highlights the need for "measuring and indicating the uncertainty of the PFG generation" to achieve better transparency.
- Why unresolved: The current system outputs a deterministic graph without confidence scores, preventing experts from assessing the reliability of specific process inclusions.
- What evidence would resolve it: A method that attaches probability scores to generated nodes, validated by showing correlation between low confidence scores and the "wrong" or "subprocess" error categories.

## Limitations

- **System boundary alignment**: SpiderGen's assumption of strict ISO system boundaries leads to omissions of auxiliary processes and generation of hallucinated ones that don't match ground truth
- **Evaluation subjectivity**: F1-score relies on manual node matching with subjective judgment about whether processes are "specific," "subprocess," or "wrong" relative to ground truth
- **Niche category performance**: For highly specialized categories like "Moka Coffee," increasing sample product count can degrade performance as diversity decreases

## Confidence

- **High confidence**: Cost and time efficiency claims (under $1 and 10 minutes vs. $25k and 21 days) - these are straightforward to verify with API pricing and benchmark timing
- **Medium confidence**: F1-score of 65% - while the metric is clearly defined, the manual node matching process introduces subjective judgment, and the paper acknowledges many "wrong" nodes are boundary issues rather than errors
- **Low confidence**: The 12% F1 improvement over baseline - this depends heavily on the specific prompts used in baselines, which are provided, but the internal SpiderGen prompts that drive the improvement are not

## Next Checks

1. **Reproduce baseline comparison**: Run SpiderGen vs LLMDirect vs LLMExample on 5 held-out product categories from the paper's dataset; verify PMI and F1 deltas match reported ~12% F1 improvement
2. **Ablate sample product count**: Test 5, 10, 15, 20 products for "Grain Mill Products" and "Moka Coffee"; plot PMI to confirm paper's observation that niche categories degrade with more samples
3. **Stress-test system boundaries**: Run SpiderGen on a product category not in EPD dataset (e.g., "laptops"); manually evaluate whether auxiliary processes like "data center usage" or "rare-earth mining" are captured