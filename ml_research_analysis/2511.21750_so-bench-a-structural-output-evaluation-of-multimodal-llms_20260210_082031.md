---
ver: rpa2
title: 'SO-Bench: A Structural Output Evaluation of Multimodal LLMs'
arxiv_id: '2511.21750'
source_url: https://arxiv.org/abs/2511.21750
tags:
- schema
- structured
- arxiv
- output
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SO-Bench, the first benchmark designed to\
  \ evaluate multimodal large language models' ability to generate structured, schema-compliant\
  \ outputs grounded in visual inputs. The benchmark covers four visual domains\u2014\
  UI screens, natural images, documents, and charts\u2014with over 1.8K image-schema\
  \ pairs and 6.5K diverse JSON schemas."
---

# SO-Bench: A Structural Output Evaluation of Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2511.21750
- **Source URL**: https://arxiv.org/abs/2511.21750
- **Reference count**: 39
- **Primary result**: Introduces first benchmark for evaluating multimodal LLMs' structured output generation from visual inputs

## Executive Summary
SO-Bench addresses a critical gap in multimodal LLM evaluation by introducing the first benchmark specifically designed to assess structured output generation grounded in visual inputs. The benchmark covers four visual domains (UI screens, natural images, documents, and charts) with over 1.8K image-schema pairs and 6.5K diverse JSON schemas. Evaluation metrics include schema validation accuracy, field matching accuracy (exact and fuzzy), and full structure matching accuracy, providing comprehensive assessment of structured output quality.

The benchmark reveals significant limitations in current state-of-the-art models, with even top performers like Gemini-2.5-Pro achieving only up to 20% full structure matching accuracy despite 98% schema validation accuracy. Training experiments demonstrate that supervised fine-tuning and reinforcement learning can substantially improve performance, with best models showing up to 20% and 13% gains on schema validation and field matching accuracy respectively. These results highlight the substantial gap between visual understanding and reliable structured generation capabilities.

## Method Summary
SO-Bench evaluates multimodal LLMs' ability to generate structured, schema-compliant outputs from visual inputs across four domains: UI screens, natural images, documents, and charts. The benchmark uses over 1.8K image-schema pairs with 6.5K diverse JSON schemas. Evaluation metrics include schema validation accuracy (whether output conforms to JSON schema), field matching accuracy (exact and fuzzy matching of schema fields to output), and full structure matching accuracy (overall structural compliance). The benchmark provides both evaluation methodology and training datasets for improving structured output capabilities through supervised fine-tuning and reinforcement learning approaches.

## Key Results
- State-of-the-art models achieve up to 98% schema validation accuracy but only up to 20% full structure matching accuracy
- None of the evaluated models exceed 20% full structure matching accuracy (fuzzy matching)
- Supervised fine-tuning and reinforcement learning training experiments show substantial improvements, with gains up to 20% and 13% on schema validation and field matching accuracy respectively
- Visual understanding does not directly translate to reliable structured output generation

## Why This Works (Mechanism)
SO-Bench works by providing a standardized framework for evaluating multimodal LLMs' structured output capabilities, bridging the gap between visual perception and structured reasoning. The benchmark's diverse schema library and multi-domain visual inputs create realistic evaluation scenarios that capture the complexity of real-world structured output tasks. The combination of strict schema validation and flexible field matching metrics allows for nuanced assessment of model performance across different aspects of structured generation.

## Foundational Learning
- **JSON Schema Validation**: Understanding how JSON schemas define structured output requirements; needed to assess schema compliance and design evaluation metrics
- **Visual-to-Structured Mapping**: Process of translating visual input into structured JSON output; needed to understand the core task being evaluated
- **Fuzzy Matching Algorithms**: Techniques for comparing field names with partial matches; needed to evaluate field matching accuracy with realistic tolerances
- **Multimodal Grounding**: Connecting visual information with structured output generation; needed to understand how models integrate visual and textual reasoning
- **Supervised Fine-Tuning**: Training approach using labeled examples to improve model performance; needed to evaluate effectiveness of training interventions
- **Reinforcement Learning for Structured Output**: Training method using reward signals for output quality; needed to assess alternative training approaches

## Architecture Onboarding

**Component Map**: Visual Input -> Multimodal Encoder -> Structured Output Generator -> JSON Schema Validator -> Field Matcher

**Critical Path**: The critical path flows from visual input through multimodal processing to structured output generation, with evaluation occurring through schema validation and field matching. Performance bottlenecks occur primarily in the structured output generator's ability to maintain schema compliance while accurately extracting information from visual inputs.

**Design Tradeoffs**: The benchmark trades comprehensive coverage (four visual domains, diverse schemas) for evaluation complexity, requiring multiple accuracy metrics to capture different aspects of performance. The choice between exact and fuzzy matching reflects the tradeoff between strict correctness and practical usability.

**Failure Signatures**: Common failure modes include schema validation failures (incorrect output structure), field matching failures (missing or incorrect field names), and full structure matching failures (both structural and field errors). These failures indicate gaps in either visual understanding, structured reasoning, or both.

**First Experiments**:
1. Evaluate baseline models on single visual domain to establish domain-specific performance baselines
2. Test impact of schema complexity on model performance to identify complexity thresholds
3. Compare exact vs. fuzzy matching results to determine practical accuracy bounds

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Benchmark's reliance on JSON schema validation may not fully capture real-world structured output complexity
- 20% full structure matching ceiling could reflect benchmark difficulty rather than fundamental model limitations
- Training experiments use relatively small dataset (6.5K schemas) that may not generalize to broader use cases
- Fuzzy matching threshold may be overly stringent for practical applications

## Confidence

**High confidence in schema validation results and overall benchmark design**
**Medium confidence in field matching accuracy interpretations due to fuzzy matching parameters**
**Medium confidence in training experiment outcomes given limited dataset size**

## Next Checks
1. Test model performance with relaxed fuzzy matching thresholds to establish practical accuracy bounds
2. Evaluate benchmark transferability by applying schemas to novel visual domains outside training distribution
3. Conduct ablation studies varying schema complexity and visual input quality to identify performance bottlenecks