---
ver: rpa2
title: 'Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models
  with Native Multi-Agent Intelligence'
arxiv_id: '2512.08743'
source_url: https://arxiv.org/abs/2512.08743
tags:
- multi-agent
- uni00000025
- uni00000011
- uni00000013
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether scaling single-agent capabilities\
  \ in foundation models (FMs) naturally leads to robust multi-agent intelligence.\
  \ Through extensive empirical evaluation across 41 models and 7 benchmarks, it finds\
  \ that while performance on single-agent tasks improves substantially with model\
  \ scale and generation, multi-agent abilities\u2014such as understanding, planning,\
  \ and coordination\u2014show only modest gains."
---

# Single-Agent Scaling Fails Multi-Agent Intelligence: Towards Foundation Models with Native Multi-Agent Intelligence

## Quick Facts
- arXiv ID: 2512.08743
- Source URL: https://arxiv.org/abs/2512.08743
- Reference count: 40
- Single-agent scaling produces diminishing returns for multi-agent capabilities

## Executive Summary
This paper investigates whether scaling single-agent capabilities in foundation models (FMs) naturally leads to robust multi-agent intelligence. Through extensive empirical evaluation across 41 models and 7 benchmarks, it finds that while performance on single-agent tasks improves substantially with model scale and generation, multi-agent abilities—such as understanding, planning, and coordination—show only modest gains. The results demonstrate that scaling single-agent performance does not automatically yield multi-agent competence. The authors argue that multi-agent intelligence requires distinct capabilities and propose new directions in dataset construction, evaluation, training paradigms, and safety considerations to address this gap.

## Method Summary
The study evaluates 41 instruction-tuned models from Qwen and LLaMA families (0.5B-235B parameters) on both single-agent and multi-agent benchmarks using vLLM with official checkpoints and recommended hyperparameters. Single-agent tasks include MATH-500, MMLU-Pro, HumanEval, and GPQA. Multi-agent benchmarks comprise ToMBench and EmoBench (understanding) and CoordinationQA (planning). The evaluation compares accuracy scaling trends across model generations to identify whether improvements in single-agent performance translate to multi-agent capabilities.

## Key Results
- Single-agent task accuracy improves sharply with model scale (e.g., ~8B models rise from 0.23 to 0.64), while multi-agent understanding accuracy increases only modestly (from 0.44 to 0.55)
- The correlation between single-agent accuracy and multi-agent planning is logarithmic (R²≈0.58-0.61), indicating diminishing returns
- Multi-agent benchmarks require distinct capabilities including theory-of-mind reasoning, partner modeling, and adaptive coordination that don't scale with single-agent training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-agent scaling produces diminishing returns for multi-agent capabilities
- Mechanism: Single-agent benchmarks reward isolated reasoning without requiring partner modeling. As models scale, they optimize for patterns in single-agent training data (math, code, knowledge), which do not contain the strategic, social, and organizational dynamics present in multi-agent settings. The correlation between single-agent accuracy and multi-agent planning is logarithmic (R²≈0.58-0.61), meaning even large gains in single-agent performance yield only marginal multi-agent improvements.
- Core assumption: Training data distributions determine which capabilities scale reliably.
- Evidence anchors: "scaling single-agent performance alone does not automatically yield robust multi-agent intelligence" [abstract]; "SA task accuracy of ~8B models rises sharply from 0.23 to 0.64, while their MA understanding task accuracy increases only from 0.44 to 0.55" [section 3.2]
- Break condition: If future training corpora explicitly encode multi-agent dynamics (negotiation traces, coordination episodes), scaling laws may begin to apply differently.

### Mechanism 2
- Claim: Multi-agent competence requires partner-dependent objective functions
- Mechanism: In multi-agent settings, "good" behavior is context-dependent—it varies based on who the partner is, their goals, and how these evolve. Standard training paradigms (SFT, RLHF) assume stable, well-specified reward signals with single correct answers. This misalignment means models trained with single-agent objectives cannot learn to adapt strategies based on partner characteristics.
- Core assumption: Reward specification determines what behaviors are learnable.
- Evidence anchors: "A behavior that is cooperative, aligned, or strategically sound with one partner may be ineffective or even counterproductive with another" [section 4.3]; "training models with genuine multi-agent intelligence will require training frameworks that embrace partner-dependent objectives" [section 4.3]
- Break condition: If a universal partner representation can be learned that generalizes across interaction contexts, simpler objective functions may suffice.

### Mechanism 3
- Claim: Population-based training induces multi-agent generalization through diverse interactions
- Mechanism: Training a single model against a fixed partner causes overfitting to that partner's beliefs, preferences, and strategies. A population of models with heterogeneous roles and objectives creates a distribution of interaction partners, forcing agents to learn robust strategies that generalize across partner types. This supports emergent behaviors like norm formation that only arise through multi-party interactions.
- Core assumption: Generalization requires exposure to diversity during training.
- Evidence anchors: "A population of models, each differing in capabilities, objectives, or preferences, naturally induces a rich distribution of interaction partners" [section 4.4]; "Training within such a population helps prevent overfitting to a particular partner" [section 4.4]
- Break condition: If single-model meta-learning can efficiently simulate diverse partners, population training may not be necessary.

## Foundational Learning

### Theory of Mind (ToM)
- Why needed here: Multi-agent understanding requires reasoning about others' beliefs, desires, and intentions. The paper evaluates this via ToMBench.
- Quick check question: Can you explain why predicting another agent's action requires modeling their beliefs separately from your own?

### Decentralized Multi-Agent Planning
- Why needed here: CoordinationQA tests planning under limited information and asynchronous communication—core challenges distinct from single-agent planning.
- Quick check question: How does planning change when you cannot observe other agents' actions directly?

### Partner-Dependent Rewards
- Why needed here: The paper argues that multi-agent training requires objectives that vary based on interaction context, unlike fixed single-agent reward functions.
- Quick check question: Why might a cooperative strategy fail when facing a competitive partner?

## Architecture Onboarding

### Component map:
Multi-Agent Understanding Module -> Coordination Planning Module -> Communication Protocol Layer -> Adaptation Engine

### Critical path:
1. Define target multi-agent capabilities (understanding vs planning vs communication vs adaptation)
2. Construct or source multi-agent training data (interaction traces, game trajectories, negotiation logs)
3. Design partner-dependent reward functions or population-based training setup
4. Implement evaluation beyond QA-style benchmarks (interactive testbeds requiring repeated trials)

### Design tradeoffs:
- Single large model vs population of smaller models: Compute efficiency vs interaction diversity
- QA-style benchmarks vs interactive environments: Evaluation speed vs behavioral fidelity
- Natural language communication vs structured protocols: Fluency vs coordination efficiency

### Failure signatures:
- High single-agent accuracy but flat multi-agent performance across model generations
- Strategies that succeed with one partner type but fail with others
- Verbose communication that degrades coordination timing
- Inability to recover from unexpected partner behaviors

### First 3 experiments:
1. Replicate the scaling divergence: Evaluate 3+ model generations from one family on both single-agent (MMLU-Pro, HumanEval) and multi-agent (ToMBench, CoordinationQA) benchmarks to confirm the gap.
2. Partner diversity test: Train or fine-tune a model against a single fixed partner, then evaluate against novel partners to measure generalization failure.
3. Data intervention pilot: Fine-tune a baseline model on a small multi-agent interaction dataset (e.g., negotiation traces or coordination game episodes) and measure improvement on CoordinationQA relative to single-agent-trained baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific data-construction strategies can effectively transform raw, noisy multi-agent interaction trajectories (e.g., game logs) into high-quality training datasets for Foundation Models?
- Basis in paper: [explicit] The authors state that raw game trajectories are "ill-suited" due to redundant actions and sparse rewards, explicitly calling for "new data-construction strategies" such as extracting key decision points or generating structured interaction episodes.
- Why unresolved: Current datasets are imbalanced and raw interaction data lacks the clear instructional signals required for standard FM training pipelines.
- What evidence would resolve it: A dataset methodology that filters noise and densifies rewards, resulting in significant performance gains on multi-agent planning benchmarks compared to models trained on raw logs.

### Open Question 2
- Question: How can evaluation benchmarks balance the need for broad multi-agent ability coverage with the time and cost efficiency of Question-Answer (QA) style evaluation?
- Basis in paper: [explicit] The paper notes a lack of standard benchmarks and highlights the trade-off where interactive environments are accurate but costly, whereas QA is efficient but limited. It calls for "benchmark designs that cover a broader range of multi-agent abilities while retaining the efficiency."
- Why unresolved: Interactive testbeds require significant engineering and compute for repeated trials, while QA formats struggle to capture dynamic adaptation and emergence.
- What evidence would resolve it: A standardized benchmark suite that evaluates adaptation and planning capabilities via a QA or efficient format that correlates strongly with performance in full interactive environments.

### Open Question 3
- Question: How can standard FM training paradigms be adapted to handle partner-dependent objectives where "good" behavior is context-specific rather than fixed?
- Basis in paper: [explicit] The authors argue that current alignment paradigms (like RLHF) rely on stable, single-answer rewards, whereas multi-agent settings require "training frameworks that embrace partner-dependent objectives" and dynamic feedback.
- Why unresolved: Supervised finetuning and RLHF presume a single preferred trajectory, which conflicts with the need to adapt strategies based on diverse and evolving partner behaviors.
- What evidence would resolve it: A training framework utilizing dynamic feedback loops that outperforms static SFT/RLHF on coordination tasks requiring adaptation to diverse or adversarial partners.

### Open Question 4
- Question: Does population-based training provide superior generalization for multi-agent intelligence compared to simply scaling up a single Foundation Model?
- Basis in paper: [inferred] While the paper notes that scaling single models fails, it proposes population-based training as a distinct direction to induce diversity and prevent overfitting. It remains unverified if this approach is strictly necessary or computationally superior to other interventions.
- Why unresolved: It is unclear if the emergent behaviors and robustness gained from training a diverse population of models outweigh the efficiency of the dominant single-model paradigm.
- What evidence would resolve it: Empirical results showing that a population of smaller, diverse models achieves higher coordination scores and robustness than a single, larger model trained in isolation with the same compute budget.

## Limitations

- Evaluation scope is limited to QA-style benchmarks that may not capture the full complexity of real multi-agent interactions
- Analysis assumes current training corpora lack sufficient multi-agent interaction data without direct examination of datasets
- Results may reflect architectural limitations of transformers rather than fundamental capability gaps in multi-agent reasoning

## Confidence

**High Confidence**: The empirical finding that multi-agent benchmark performance shows minimal improvement across model generations while single-agent performance scales substantially. This is directly measurable and the dataset spans multiple model families.

**Medium Confidence**: The interpretation that this scaling gap represents a fundamental limitation of single-agent scaling rather than evaluation artifacts. While the data supports the observation, alternative explanations exist.

**Medium Confidence**: The proposed directions (population training, partner-dependent rewards) represent plausible approaches, but their effectiveness remains theoretical without empirical validation.

## Next Checks

1. Implement an interactive multi-agent testbed where models engage in repeated coordination tasks with dynamic partners, measuring performance beyond static benchmarks.

2. Examine actual training corpora for frequency and diversity of multi-agent interaction patterns to quantify the data gap hypothesis.

3. Modify transformer architectures with explicit partner modeling mechanisms (attention heads dedicated to opponent modeling) and evaluate whether this closes the scaling gap.