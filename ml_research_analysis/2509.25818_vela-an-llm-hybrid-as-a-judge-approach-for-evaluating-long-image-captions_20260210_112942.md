---
ver: rpa2
title: 'VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions'
arxiv_id: '2509.25818'
source_url: https://arxiv.org/abs/2509.25818
tags:
- image
- captions
- evaluation
- long
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VELA, a novel automatic evaluation metric
  for long image captions generated by multimodal large language models. The key innovation
  is the LLM-Hybrid-as-a-Judge framework, which combines a non-autoregressive LLM
  branch (R2C-LLM) for analyzing caption-reference relationships with a visual alignment
  branch (I2C-Align) using Long-CLIP for image-caption correspondence.
---

# VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image Captions

## Quick Facts
- **arXiv ID:** 2509.25818
- **Source URL:** https://arxiv.org/abs/2509.25818
- **Reference count:** 40
- **Key outcome:** Introduces VELA, a hybrid LLM-as-a-Judge metric that achieves superhuman performance on long image caption evaluation with 5x faster inference than autoregressive baselines.

## Executive Summary
This paper introduces VELA, a novel automatic evaluation metric for long image captions generated by multimodal large language models. The key innovation is the LLM-Hybrid-as-a-Judge framework, which combines a non-autoregressive LLM branch (R2C-LLM) for analyzing caption-reference relationships with a visual alignment branch (I2C-Align) using Long-CLIP for image-caption correspondence. This design enables faster inference compared to existing LLM-as-a-Judge approaches while maintaining high correlation with human judgments.

To train and validate VELA, the authors constructed LongCap-Arena, a benchmark containing 7,805 images, human-provided long reference captions, long candidate captions, and 32,246 human judgments across three perspectives: Descriptiveness, Relevance, and Fluency. VELA significantly outperformed existing metrics including CLIP-based approaches and other LLM-based judges. Notably, it achieved superhuman performance on LongCap-Arena, surpassing both reference-free and reference-based GPT-4o by 2.3-22.0 points in Descriptiveness, 1.7-15.6 points in Relevance, and 32.9-14.6 points in Fluency. The metric demonstrated inference speeds approximately five times faster than existing LLM-based approaches while maintaining superior alignment with human judgments.

## Method Summary
VELA employs a two-branch architecture where the R2C-LLM branch uses Qwen2.5-3B in non-autoregressive mode to process prompts containing candidate captions and references, extracting mean-pooled hidden states as semantic representations. The I2C-Align branch independently encodes images and captions using Long-CLIP ViT-L/14, computing element-wise difference and Hadamard product to capture visual-textual alignment. An MLP fusion layer combines both representations to predict three scores (Descriptiveness, Relevance, Fluency) on a sigmoid scale. The model trains with 3.68M parameters using MSE loss against normalized human judgments, achieving ~260ms inference time per sample versus 1300-2000ms for autoregressive baselines.

## Key Results
- VELA achieved superhuman performance on LongCap-Arena, surpassing GPT-4o by 2.3-22.0 points in Descriptiveness, 1.7-15.6 points in Relevance, and 32.9-14.6 points in Fluency
- Inference speed is approximately 5x faster than existing LLM-based approaches at ~260ms/sample
- VELA demonstrated superior Kendall's τ correlation with human judgments across all three evaluation perspectives
- The reference-free variant still outperformed GPT-4o in most dimensions despite performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive LLM inference enables ~5x speedup over autoregressive LLM-as-a-Judge baselines while maintaining evaluation quality.
- Mechanism: The R2C-LLM branch extracts hidden states from a text-only LLM (Qwen2.5-3B) in a single forward pass rather than token-by-token generation. Mean pooling over the hidden states produces a fixed representation $g_{r2c}$ for downstream scoring.
- Core assumption: The semantic information needed for evaluation is captured in hidden states without requiring generative output.
- Evidence anchors:
  - [abstract] "combines a non-autoregressive LLM branch (R2C-LLM)... This design enables faster inference compared to existing LLM-as-a-Judge approaches"
  - [section 4.1] "we employ a non-autoregressive approach that significantly reduces the inference time"
  - [corpus] Related work on LFQA-E (arXiv 2410.01945) highlights evaluation challenges in long-form generation, supporting the need for efficient metrics.
- Break condition: If evaluation requires multi-step reasoning that cannot be captured in static representations, non-autoregressive approaches will underperform.

### Mechanism 2
- Claim: Late fusion of visual information through Long-CLIP avoids sequence length explosion while providing image grounding.
- Mechanism: The I2C-Align branch independently encodes image ($h_{img}$) and caption ($h_{cand}$) using Long-CLIP, then computes element-wise difference and Hadamard product to produce $g_{i2c}$. This representation is concatenated with $g_{r2c}$ for final scoring.
- Core assumption: Separate encoding of visual and textual modalities preserves sufficient information for alignment assessment.
- Evidence anchors:
  - [abstract] "visual alignment branch (I2C-Align) using Long-CLIP for image-caption correspondence"
  - [section 4.2] "Unlike existing metrics based on CLIP... employs Long-CLIP to overcome the 77-token limit"
  - [corpus] Weak corpus evidence—no directly comparable late-fusion evaluation metrics found in neighbors.
- Break condition: If fine-grained visual-linguistic reasoning (e.g., spatial relations) is required, separate encoding may lose cross-modal interactions.

### Mechanism 3
- Claim: Multi-perspective scoring (Descriptiveness, Relevance, Fluency) prevents criterion collapse seen in single-score metrics.
- Mechanism: Three separate sigmoid outputs $\hat{y}_{desc}, \hat{y}_{rel}, \hat{y}_{flu}$ are trained with MSE loss against normalized human judgments. Each perspective has distinct evaluation criteria encoded in the prompt.
- Core assumption: Human evaluation naturally decomposes along these dimensions; optimizing them independently improves overall alignment.
- Evidence anchors:
  - [section 1] "This prevents certain evaluation criteria from being ignored, which is a common issue in metrics that output only a single score"
  - [section 3] Three perspectives defined with explicit criteria
  - [corpus] SPECS (arXiv 2509.03897) addresses specificity in long caption evaluation, supporting multi-dimensional assessment.
- Break condition: If perspectives are highly correlated in practice, separate outputs may not provide additional signal over a single score.

## Foundational Learning

- **Non-autoregressive vs. Autoregressive Inference**:
  - Why needed here: Understanding why VELA achieves speed gains requires distinguishing single-pass encoding from sequential token generation.
  - Quick check question: Can you explain why autoregressive decoding requires $O(n)$ forward passes for a sequence of length $n$?

- **CLIP Token Limitations and Long-CLIP Extensions**:
  - Why needed here: Standard CLIP's 77-token limit makes it unsuitable for long captions (>100 words); Long-CLIP removes this constraint.
  - Quick check question: What happens to a 150-word caption passed through standard CLIP?

- **Kendall's $\tau$ Correlation for Evaluation Metrics**:
  - Why needed here: The paper uses $\tau_b$ and $\tau_c$ to measure alignment with human judgments; understanding these metrics is essential for interpreting results.
  - Quick check question: Why might Kendall's $\tau$ be preferred over Pearson correlation for ordinal human judgments?

## Architecture Onboarding

- **Component map**:
  - Prompt → Qwen2.5-3B (frozen) → Hidden states → Mean pooling → $g_{r2c}$
  - Image + Caption → Long-CLIP ViT-L/14 (frozen) → Difference + Hadamard product → $g_{i2c}$
  - $[g_{r2c}, g_{i2c}]$ → Linear layer + Sigmoid → 3 scores
  - Trainable parameters: ~3.68M (fusion layer only)

- **Critical path**:
  1. Construct prompt from candidate + references
  2. Forward pass through R2C-LLM (non-autoregressive)
  3. Encode image and caption through Long-CLIP
  4. Concatenate representations and compute final scores
  - Inference time: ~260ms/sample (vs. 1300-2000ms for LLM baselines)

- **Design tradeoffs**:
  - MLP fusion vs. Transformer fusion: MLP chosen for lower computational cost; Transformer achieved comparable but slightly worse Descriptiveness scores (Table 6).
  - Reference-based vs. reference-free: Reference-free mode available by removing references from prompt; performance drops but still exceeds GPT-4o in most dimensions (Table 7).

- **Failure signatures**:
  - Overestimation when semantically important objects occupy small image regions (I2C-Align fails to recognize them)
  - Over-reliance on references when they contain errors or insufficient detail
  - Named entities in candidates cannot be verified from images alone
  - Short candidates may be scored inappropriately (training data biased toward long captions)

- **First 3 experiments**:
  1. **Ablate each branch**: Run R2C-LLM only and I2C-Align only to isolate contribution (expect ~10-45 point drops per Table 2).
  2. **Test on short-caption benchmarks**: Zero-shot evaluation on Composite and Flickr8K-Expert to verify no DCI overfitting (Table 8 shows competitive performance).
  3. **Stress test failure modes**: Construct adversarial examples with small-but-important objects or erroneous references to characterize limitations described in Appendix E.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating cross-attention mechanisms (e.g., Flamingo-style gated xattn-dense layers) between the R2C-LLM and I2C-Align branches improve grounding for semantically important small objects?
- Basis in paper: [explicit] The authors explicitly state in Section 7 and Appendix E that errors from overlooking small objects "could be attributed to insufficient grounding in the I2C-Align branch and the suboptimal integration of outputs" and propose "a mechanism that integrates visual information while leveraging a pretrained language model, similar to the gated xattn-dense layer in Flamingo" as a potential solution.
- Why unresolved: The current late-fusion approach using MLP-based combination of branch outputs does not allow fine-grained visual-linguistic interaction, causing the I2C-Align branch to fail on small regions.
- What evidence would resolve it: Ablation experiments comparing current MLP fusion against cross-attention fusion on a targeted subset of images containing small but semantically important objects, with analysis of τc changes specifically for Relevance scores.

### Open Question 2
- Question: Can the reference dependency be reduced while maintaining evaluation quality, particularly for Descriptiveness assessment?
- Basis in paper: [inferred] The error analysis (Appendix E, Table 5) identifies "Insufficient detail or accuracy in references" as the primary failure mode (12/25 errors). Additionally, reference-free VELA shows degraded performance compared to reference-based VELA in Table 7, yet still outperforms GPT-4o, suggesting room for improvement in reference-free settings.
- Why unresolved: The R2C-LLM branch heavily relies on reference captions to judge descriptiveness, and when references lack detail or contain errors, the metric inherits these limitations.
- What evidence would resolve it: Experiments varying reference quality systematically (detailed vs. sparse, accurate vs. noisy) and measuring correlation degradation, combined with analysis of whether I2C-Align features can compensate for weak references.

### Open Question 3
- Question: How can the framework be adapted to support closed-source LLM backbones that do not expose last hidden states?
- Basis in paper: [explicit] Section 7 states: "the R2C-LLM branch requires access to last hidden states, which prevents the direct use of closed-source models" as an important limitation.
- Why unresolved: The non-autoregressive extraction of representations depends on internal hidden states, which commercial APIs (e.g., GPT-4, Claude) typically do not provide.
- What evidence would resolve it: Prototyping alternative approaches such as using output logits, embedding-based methods from API-provided embeddings, or prompt-based scoring with distillation to show whether comparable performance is achievable without hidden-state access.

## Limitations

- **Dataset Dependency**: VELA's performance is evaluated exclusively on LongCap-Arena with long captions (>100 words), limiting generalizability to shorter captions or different domains
- **Architectural Assumptions**: The non-autoregressive approach assumes hidden states capture sufficient semantic information, which may not hold for multi-step reasoning tasks
- **Human Judgment Correlation**: High correlation with human judgments does not guarantee absolute evaluation quality and may inherit annotator biases

## Confidence

**High Confidence (9/10)**: Architectural design and implementation details are clearly specified, with measurable inference speed advantage of ~5x over autoregressive baselines

**Medium Confidence (7/10)**: Performance claims relative to existing metrics are well-supported by LongCap-Arena experiments, though generalization requires further validation

**Low Confidence (4/10)**: Assumptions about performance transfer to domains outside LongCap-Arena or shorter captions are speculative

## Next Checks

1. **Zero-shot Transfer Evaluation**: Test VELA on established short-caption benchmarks (Flickr8K, Composite) without fine-tuning to verify it doesn't overfit to LongCap-Arena's long-caption domain

2. **Ablation Studies with Adversarial Examples**: Construct test cases where VELA is expected to fail—small but semantically important objects, erroneous references, or named entities unverifiable from images

3. **Runtime Performance Benchmarking**: Measure actual inference latency across different hardware configurations (GPU vs. CPU) and batch sizes to validate the claimed ~5x speedup