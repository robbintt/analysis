---
ver: rpa2
title: Closing the Gap Between Synthetic and Ground Truth Time Series Distributions
  via Neural Mapping
arxiv_id: '2501.17553'
source_url: https://arxiv.org/abs/2501.17553
tags:
- time
- series
- mapping
- timevqv
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Neural Mapper for Vector Quantized Time Series
  Generator (NM-VQTSG), a novel method to address fidelity challenges in vector quantized
  (VQ) time series generation. VQ-based methods, such as TimeVQVAE, suffer from information
  loss during compression into discrete latent spaces and deviations in the learned
  prior distribution from the ground truth distribution, resulting in synthetic time
  series with compromised fidelity.
---

# Closing the Gap Between Synthetic and Ground Truth Time Series Distributions via Neural Mapping

## Quick Facts
- arXiv ID: 2501.17553
- Source URL: https://arxiv.org/abs/2501.17553
- Reference count: 28
- Primary result: NM-VQTSG achieves ~151% average FID improvement, ~7% IS improvement, and ~37% cFID improvement over TimeVQVAE on 13 UCR datasets

## Executive Summary
This paper addresses fidelity challenges in vector quantized (VQ) time series generation by proposing Neural Mapper for Vector Quantized Time Series Generator (NM-VQTSG). VQ-based methods like TimeVQVAE suffer from information loss during compression into discrete latent spaces and deviations in the learned prior distribution from the ground truth distribution, resulting in synthetic time series with compromised fidelity. NM-VQTSG introduces a U-Net-based neural mapping model that bridges the distributional gap between synthetic and ground truth time series. The model refines synthetic data by addressing artifacts introduced during generation, effectively aligning the distributions of synthetic and real data.

## Method Summary
NM-VQTSG operates in three stages: (1) Tokenization using TimeVQVAE to learn discrete representations, (2) Prior learning using a bidirectional transformer to model the discrete token distribution, and (3) Neural mapping using a U-Net to transform synthetic samples toward ground truth. The key innovation is Stochastic Vector Quantization (SVQ), which enables training-pair construction by sampling tokens from a softmax over negative distances. The U-Net mapper is trained with L1 reconstruction loss to minimize the gap between stochastic and ground truth distributions.

## Key Results
- 151% average improvement in FID over baseline TimeVQVAE
- 7% improvement in Inception Score (IS)
- 37% improvement in conditional FID (cFID)
- Visual inspection confirms distributional alignment in both data and latent spaces

## Why This Works (Mechanism)

### Mechanism 1
A learnable neural mapping can reduce distributional discrepancy between synthetic and ground truth time series. A U-Net-based model $f_\phi$ is trained to map stochastically perturbed samples $\tilde{x}'$ back toward their ground truth $x$, using L1 reconstruction loss. This teaches the model to recognize and refine synthetic-specific artifacts.

### Mechanism 2
Stochastic Vector Quantization (SVQ) enables training-pair construction for the mapping model. Instead of hard argmin quantization, tokens are sampled from a softmax over negative distances with temperature $\tau$. This produces $\tilde{x}'$ with controlled variance relative to $x$.

### Mechanism 3
U-Net architecture effectively refines time series while preserving temporal structure. The encoder-decoder structure with skip connections allows coarse artifact correction without losing fine-grained temporal detail. Self-attention layers augment convolutional blocks for longer-range dependencies.

## Foundational Learning

- **Vector Quantized Variational Autoencoders (VQVAE)**: Why needed: NM-VQTSG operates as a post-hoc corrector for VQ-based generators; understanding discrete latent spaces and codebooks is prerequisite. Quick check: Can you explain why VQVAE uses discrete latents instead of continuous Gaussians, and what "posterior collapse" refers to?

- **U-Net Architectures with Skip Connections**: Why needed: The mapper uses U-Net to transform synthetic series; skip connections preserve information across scales. Quick check: What happens to gradient flow and detail preservation if you remove skip connections from a U-Net?

- **FrÃ©chet Inception Distance (FID) and Inception Score (IS) for Time Series**: Why needed: FID/IS quantify fidelity improvements; $\tau$ selection relies on FID computed with ROCKET features. Quick check: Why is FID computed in a feature space rather than directly on raw time series?

## Architecture Onboarding

- **Component map**: Encoder $E$ -> Vector Quantizer $Q$ -> Decoder $D$ -> Prior model -> Neural Mapper $f_\phi$ -> Refined output

- **Critical path**:
  1. Train VQVAE encoder/decoder/quantizer (Stage 1, ~20K steps)
  2. Train prior model on discrete tokens (Stage 2, ~40K steps)
  3. Generate synthetic samples $\hat{X}$ and search for optimal $\tau$ via FID between $\hat{X}$ and $\tilde{X}'$
  4. Train U-Net mapper with L1 loss using $(\tilde{x}', x)$ pairs (Stage 3, ~30K steps)
  5. Inference: $\hat{x}_R = f_\phi(\hat{x})$

- **Design tradeoffs**:
  - Single $\tau$ vs. per-step $\tau$: Paper uses uniform $\tau$ for simplicity; varying variance across sampling iterations is noted but deferred
  - L1 vs. GAN loss: L1 is simple but may produce averaging; GAN loss suggested for sharper outputs
  - Deterministic vs. stochastic mapping: Current design is one-to-one; stochastic mapping would handle multimodal scenarios

- **Failure signatures**:
  - FID degradation on high-fidelity baselines: Datasets like Crop where $\hat{X} \approx X$ show negative change (-45%)
  - Class imbalance sensitivity: Wafer (10%/90% split) shows IS drop (-11.7%) due to poor minority-class mapping
  - Over-smoothing: L1 loss may blur details; visual inspection remains essential

- **First 3 experiments**:
  1. Baseline sanity check: Reproduce TimeVQVAE FID/IS on a small UCR dataset (e.g., ECG5000) without mapper
  2. Ablation on $\tau$ search: Run $\tau \in \{0.1, 0.5, 1, 2, 4\}$ on one dataset; plot FID($\hat{X}$, $\tilde{X}'$)
  3. Mapper overhead analysis: Measure inference time and memory for $f_\phi$ alone

## Open Questions the Paper Calls Out

### Open Question 1
Does implementing a dynamic temperature schedule for Stochastic Vector Quantization (SVQ) provide a more accurate approximation of the generative distribution than a fixed temperature? The authors state they do not account for varying variances across sampling steps and "leave this to future work."

### Open Question 2
Can the integration of stochasticity into the neural mapping model improve performance in datasets where a synthetic sample corresponds to multiple plausible real samples? The authors identify the "Deficiency of Stochasticity" in the Discussion.

### Open Question 3
Does replacing the L1 loss with a GAN loss or multi-scale STFT loss preserve fine-grained details and prevent the averaging effect observed in pixel-wise losses? The authors suggest in the Discussion that a GAN loss could help produce "sharp and crisp data."

## Limitations
- The distributional similarity hypothesis $p(\hat{X}) \approx p(\tilde{X}')$ is validated only using FID metrics on ROCKET features without alternative feature spaces or statistical tests
- Critical U-Net architectural details are referenced to an incomplete appendix
- Only 13 UCR datasets tested; performance on long, multivariate, or non-sequential time series remains unknown

## Confidence

| Claim | Confidence |
|-------|------------|
| FID/IS improvements on diverse UCR datasets | High |
| Conceptual mechanism of using SVQ as proxy for generator artifacts | Medium |
| U-Net's effectiveness in refining time series artifacts | Low |

## Next Checks

1. Apply Kolmogorov-Smirnov or Wasserstein distance tests between $\hat{X}$ and $\tilde{X}'$ across multiple feature spaces (ROCKET, raw, Fourier) to rigorously validate the distributional similarity assumption
2. Implement NM with alternative architectures (ResNet, Transformer) and loss functions (GAN, perceptual) to isolate the contribution of the U-Net design
3. Generate synthetic data with known multimodality to test whether the deterministic mapping produces blurring artifacts or if stochastic variants would be superior