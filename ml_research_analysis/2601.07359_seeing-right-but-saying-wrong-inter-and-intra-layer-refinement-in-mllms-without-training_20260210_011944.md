---
ver: rpa2
title: 'Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs
  without Training'
arxiv_id: '2601.07359'
source_url: https://arxiv.org/abs/2601.07359
tags:
- uni00000015
- uni00000014
- attention
- uni00000018
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses a key inconsistency in Multimodal Large Language\
  \ Models (MLLMs) where, despite correct visual understanding in deeper layers, final\
  \ predictions are often misled by noisy attention from earlier layers\u2014termed\
  \ \"seeing it right but saying it wrong.\" To resolve this, the authors propose\
  \ DualPD, a training-free dual-perspective decoding refinement strategy that improves\
  \ visual grounding without additional training. DualPD operates in two ways: (1)\
  \ layer-wise attention-guided contrastive logits, which identify and compare logits\
  \ between layers with the largest attention shift to capture evolving visual understanding,\
  \ and (2) head-wise information filtering, which suppresses low-contribution attention\
  \ heads to reduce noise."
---

# Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training

## Quick Facts
- arXiv ID: 2601.07359
- Source URL: https://arxiv.org/abs/2601.07359
- Reference count: 40
- Primary result: Training-free method improves MLLM visual grounding accuracy by up to 2.5% across six benchmarks

## Executive Summary
This work addresses a fundamental inconsistency in Multimodal Large Language Models (MLLMs) where correct visual understanding in deeper layers fails to translate into accurate final predictions due to noisy attention from earlier layers. The authors propose DualPD, a training-free refinement strategy that operates through two complementary mechanisms: layer-wise attention-guided contrastive logits that capture evolving visual understanding between layers, and head-wise information filtering that suppresses low-contribution attention heads to reduce noise. Extensive experiments across multiple LLaVA and Qwen-VL models demonstrate consistent accuracy improvements on six multimodal benchmarks, validating the method's effectiveness without requiring additional training.

## Method Summary
DualPD is a training-free refinement framework that improves MLLM visual grounding by addressing the "seeing right but saying wrong" problem. The method employs layer-wise attention-guided contrastive logits to identify and compare logits between layers with the largest attention shifts, capturing evolving visual understanding throughout the model. Simultaneously, it uses head-wise information filtering to suppress attention heads with minimal contribution, reducing noise in the final predictions. This dual-perspective approach operates entirely during inference without modifying the underlying model architecture or requiring additional training data, making it broadly applicable across different MLLM architectures.

## Key Results
- Achieves average accuracy improvements of up to 2.5% across six multimodal benchmarks
- Demonstrates consistent performance gains across multiple LLaVA and Qwen-VL model variants
- Shows effectiveness without requiring any additional training or architectural modifications
- Validates the "seeing right but saying wrong" phenomenon through empirical evidence

## Why This Works (Mechanism)
The method works by addressing the temporal inconsistency in MLLM attention patterns where visual understanding evolves throughout the layers but noisy early-layer attention corrupts final predictions. By identifying layers with the largest attention shifts, DualPD captures moments where visual understanding improves and uses these transitions to guide logit refinement. The head-wise filtering component removes attention heads that contribute minimal information, effectively denoising the attention mechanism. Together, these approaches create a refinement process that leverages the model's own evolving understanding without requiring external supervision or architectural changes.

## Foundational Learning
- Multimodal Large Language Models (MLLMs): Neural architectures that integrate vision and language processing through cross-attention mechanisms
  - Why needed: Understanding MLLM architecture is essential to grasp how visual and textual information interact through attention layers
  - Quick check: Verify understanding of cross-attention and how visual tokens are processed through transformer layers

- Attention Mechanism in VLMs: The process by which models weight different visual regions when generating text responses
  - Why needed: Core to understanding how DualPD identifies and filters attention patterns
  - Quick check: Confirm knowledge of how attention scores are computed and used in transformer layers

- Layer-wise Processing: How information flows and transforms through successive transformer layers
  - Why needed: Critical for understanding how visual understanding evolves and where refinement occurs
  - Quick check: Trace information flow through 2-3 transformer layers in a sample architecture

## Architecture Onboarding
Component Map: Input -> Vision Encoder -> Cross-Attention Layers -> Language Decoder -> Output
Critical Path: Visual input → Early transformer layers → Attention refinement → Later transformer layers → Final prediction
Design Tradeoffs: Training-free refinement vs. model fine-tuning, computational overhead vs. accuracy gains, attention-based vs. feature-based refinement
Failure Signatures: Degradation on specialized domains, attention manipulation without actual understanding improvement, computational bottlenecks during inference
First Experiments:
1. Baseline accuracy measurement on standard multimodal benchmarks
2. Layer-wise attention shift analysis to identify refinement opportunities
3. Head contribution analysis to determine filtering thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to six multimodal benchmarks, potentially limiting generalizability
- Reliance on attention patterns as proxies for visual understanding may not capture true task performance
- Computational overhead characterization incomplete, affecting practical deployment assessment

## Confidence
High confidence in: The existence of the "seeing right but saying wrong" phenomenon in MLLMs
Medium confidence in: The effectiveness of attention-based refinement strategies
Low confidence in: The generalizability of DualPD across diverse MLLM architectures and real-world applications

## Next Checks
1. Conduct ablation studies systematically removing each component (layer-wise contrast, head filtering) to quantify their individual contributions
2. Evaluate DualPD on specialized domains (medical imaging, satellite imagery, fine-grained object detection) to assess performance degradation
3. Perform qualitative analysis comparing attention patterns before and after DualPD application to verify meaningful visual understanding improvement