---
ver: rpa2
title: Stability-Aware Prompt Optimization for Clinical Data Abstraction
arxiv_id: '2601.22373'
source_url: https://arxiv.org/abs/2601.22373
tags:
- prompt
- stability
- flip
- accuracy
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Clinical LLM prompts are often tuned for accuracy without considering
  stability to semantic variations. This work measures prompt sensitivity via flip
  rates across two clinical abstraction tasks and multiple models, showing that accuracy
  and stability are not inherently linked.
---

# Stability-Aware Prompt Optimization for Clinical Data Abstraction

## Quick Facts
- arXiv ID: 2601.22373
- Source URL: https://arxiv.org/abs/2601.22373
- Reference count: 22
- Clinical LLM prompts tuned for accuracy without considering stability to semantic variations

## Executive Summary
Clinical LLM prompts are often optimized for accuracy without considering their stability to semantic variations. This work measures prompt sensitivity via flip rates across two clinical abstraction tasks and multiple models, showing that accuracy and stability are not inherently linked. Larger models tend to be both more accurate and stable, but prompt-level instability can persist even with high accuracy. An LLM-in-the-loop optimizer that jointly optimizes accuracy and stability significantly reduces flip rates across tasks, sometimes with modest accuracy trade-offs. The results demonstrate that prompt stability should be an explicit objective in clinical LLM validation, as calibration and uncertainty metrics alone do not fully capture prompt sensitivity.

## Method Summary
The study evaluates prompt stability through flip rates—the proportion of examples where predictions change across semantically equivalent prompt paraphrases. Using MedAlign v1.3 and an internal MS subtype corpus, the authors measure baseline stability and jointly optimize prompts for accuracy and stability using an LLM-in-the-loop approach. The optimizer receives failure examples (high-flip cases and misclassifications) and generates candidate prompts targeting these specific patterns. The joint objective J(P) = λ_perf·F(P) + λ_stab·S(P) explicitly rewards prompts that maintain accuracy while reducing flip rates across paraphrases, comparing accuracy-only versus joint optimization across 9 model-task combinations.

## Key Results
- Accuracy and stability are decoupled at the prompt level—higher accuracy does not guarantee lower flip rates
- Larger models (GPT-4, Claude-3.5-Sonnet) show better stability-accuracy trade-offs than smaller models
- Joint optimization improves stability in 8 of 9 model-task combinations, sometimes with modest accuracy trade-offs
- Stability-calibration correlation is modest (Spearman ρ = 0.204), indicating certainty alone is an unreliable proxy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Accuracy and prompt stability are decoupled at the prompt level—improving one does not automatically improve the other.
- **Mechanism:** A prompt encodes task specification in language. Small semantic-preserving paraphrases can shift model attention or change implicit priors without meaningfully altering task-relevant reasoning, creating variance in outputs even when accuracy remains constant.
- **Core assumption:** The paraphrase distribution used in evaluation reflects realistic prompt variations encountered in deployment (vendor templates, institution-specific formatting, editorial tweaks).
- **Evidence anchors:**
  - [abstract] "higher accuracy does not guarantee prompt stability, and that models can appear well-calibrated yet remain fragile to paraphrases"
  - [section 5.2, E2] Figure 3 shows "substantial vertical spread: prompts with similar accuracy can have very different flip rates"
  - [corpus] Prompt Stability Matters (Chen et al., 2025) similarly finds outcome-centric evaluation overlooks reliability factors, though focuses on stochastic reproducibility rather than paraphrase sensitivity
- **Break condition:** If evaluation paraphrases are systematically more adversarial than real-world prompt edits, measured flip rates may overestimate deployment instability.

### Mechanism 2
- **Claim:** Stable predictions correlate with higher model certainty (larger prediction margins), but certainty alone is an unreliable proxy for stability.
- **Mechanism:** When a model has high confidence (large margin between top predictions), its decision boundary is farther from the input; prompt perturbations are less likely to push it across that boundary. However, the correlation is modest because confidence reflects within-prompt uncertainty, not cross-prompt robustness.
- **Core assumption:** Prediction margin computed from token-level logprobs faithfully represents model confidence for classification tasks.
- **Evidence anchors:**
  - [section 5.1, E1] "Examples that never flip (flip rate = 0) have margins concentrated near 1.0... As flip rate increases, median margin decreases"
  - [section 6] "Spearman ρ = 0.204 between margin and stability... the correlation is modest, indicating that certainty alone is not a reliable proxy"
  - [corpus] No direct corpus evidence on margin-stability relationship; ProSA relates sensitivity to decoding confidence but at task level
- **Break condition:** For API-only models without logprob access, this relationship cannot be directly validated; flip rate becomes the only stability signal.

### Mechanism 3
- **Claim:** Adding an explicit stability term to prompt optimization reduces flip rates, sometimes with modest accuracy trade-offs.
- **Mechanism:** An LLM-in-the-loop optimizer receives concrete failure cases (high-flip examples and misclassifications) and generates candidate prompts targeted at those specific patterns. The joint objective J(P) = λ_perf·F(P) + λ_stab·S(P) explicitly rewards prompts that maintain accuracy while reducing flip rates across paraphrases.
- **Core assumption:** The optimizer LLM can identify and correct instability patterns from failure examples; candidate generation produces meaningful diversity.
- **Evidence anchors:**
  - [section 3.1] Equation 1 defines the joint objective; "Setting λ_stab = 0 recovers accuracy-only optimisation"
  - [section 5.3, E3] "joint optimization improves stability in 8 of 9 model-task combinations... The largest gains occur when baseline stability is poor"
  - [corpus] ProSA and multi-prompt evaluation frameworks measure sensitivity but "stop at measurement rather than proposing concrete optimisation procedures"
- **Break condition:** If λ weighting is poorly tuned, the optimizer may过度-penalize accuracy for stability gains, or fail to improve stability if λ_stab is too low.

## Foundational Learning

- **Concept:** Flip rate as a stability metric
  - **Why needed here:** This is the core measurement tool. You must understand that flip rate = proportion of examples where prediction changes between base prompt and paraphrased variants (K=3 in this work).
  - **Quick check question:** If a model has 10% flip rate with K=3 paraphrases, what does that mean in terms of individual example stability?

- **Concept:** Calibration vs. stability as distinct phenomena
  - **Why needed here:** A key finding is that well-calibrated models can still be prompt-sensitive. Calibration measures whether confidence matches accuracy; stability measures consistency across prompt variants.
  - **Quick check question:** Can a model with ECE = 0.02 (well-calibrated) still have flip rate = 0.35? What does this paper say?

- **Concept:** Multi-objective optimization with explicit trade-off weights
  - **Why needed here:** The joint objective J(P) requires setting λ_perf and λ_stab. Understanding that these control the accuracy-stability trade-off is essential for tuning.
  - **Quick check question:** If clinical deployment prioritizes safety over peak accuracy, should λ_stab be increased or decreased?

## Architecture Onboarding

- **Component map:**
  1. Prompt variant generator → Evaluation engine → Failure identifier → Candidate generator → Scorer and selector
- **Critical path:** Evaluation → Failure identification → Candidate generation → Scoring → Selection. If any stage fails (e.g., paraphrases aren't semantic equivalents, optimizer produces low-quality candidates), the loop degrades.
- **Design tradeoffs:**
  - Higher K (more paraphrases) improves flip rate estimate but increases compute cost 3–5×
  - Higher N (more candidates) explores more prompt space but requires more inference
  - λ_stab too high → accuracy may drop significantly; too low → stability gains minimal
  - Assumption: Paraphrase quality matters—LLM-generated variants may not cover real-world prompt drift patterns
- **Failure signatures:**
  - Flip rate doesn't decrease over iterations → optimizer not generating stability-improving edits; check failure feedback quality
  - Accuracy drops sharply → λ_stab may be too high; reduce and re-run
  - High variance across seeds → evaluation set too small; increase N or aggregate more runs
- **First 3 experiments:**
  1. Baseline flip rate measurement: Take an existing production prompt, generate K=3 paraphrases, measure flip rate on 50–100 examples. This establishes current stability.
  2. Accuracy-only vs. joint optimization comparison: Run the optimization loop with λ_stab=0 and λ_stab=0.5 on the same task/model. Compare final accuracy and flip rate (Table 1 pattern).
  3. Stability-confidence correlation check: For models with logprob access, plot prediction margin vs. flip rate (Figure 2 pattern). Confirm that stable predictions cluster at high margins before relying on confidence as a proxy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does improved prompt stability translate to reduced clinical harm or better patient outcomes in deployed systems?
- Basis in paper: [explicit] The authors state in limitations: "We show that joint optimization reduces flip rates, but we do not demonstrate that this translates to improved clinical outcomes or reduced harm. The link between prompt stability and patient safety remains indirect."
- Why unresolved: The study measures technical metrics (flip rates, accuracy) on retrospective datasets but lacks prospective clinical validation or safety outcome tracking.
- What evidence would resolve it: A prospective deployment study tracking patient-level outcomes or near-miss events when comparing stability-optimized versus accuracy-only prompts in a production clinical abstraction system.

### Open Question 2
- Question: Do the stability-accuracy trade-offs and optimization dynamics generalize beyond the two clinical abstraction tasks studied?
- Basis in paper: [explicit] The limitations section states: "Our experiments cover two clinical abstraction settings (MedAlign and MS subtype). Generalization to other clinical NLP tasks, such as entity extraction, temporal reasoning, or multi-document summarization, remains untested."
- Why unresolved: The paper evaluates only applicability judgments, correctness assessments, and phenotype classification—all classification tasks with unstructured evidence.
- What evidence would resolve it: Replication of the joint optimization experiments across diverse clinical NLP tasks, particularly structured extraction and reasoning-heavy tasks where prompt sensitivity may manifest differently.

### Open Question 3
- Question: What is the true relationship between prompt stability and model calibration for API-based models that do not expose output probabilities?
- Basis in paper: [explicit] The authors note: "The E1 analysis joining flip rates with conformal set sizes is only possible for models providing output probabilities (HuggingFace). For Bedrock models (Claude, Llama, Qwen), we can measure flip rates but cannot assess the stability-calibration relationship. This limits generalizability to API-based deployments."
- Why unresolved: The stability-calibration correlation (ρ=0.204, p=0.056) was borderline significant and only measurable for local models; whether this relationship holds for widely-used proprietary models remains unknown.
- What evidence would resolve it: Development of proxy calibration metrics for API-only models, or collaboration with API providers to access log-probabilities for stability-calibration analysis on clinical tasks.

### Open Question 4
- Question: How robust is the stability signal when paraphrases reflect realistic deployment variation (vendor templates, non-native phrasings, formatting changes) rather than LLM-generated rewrites?
- Basis in paper: [explicit] The limitations state: "Flip rates depend on the paraphrases used. We generate variants via LLM rewriting, which may not reflect the full range of prompt variations encountered in deployment (e.g., vendor templates, institution-specific formatting, non-native speaker phrasings). The stability we measure is relative to our paraphrase distribution, not absolute."
- Why unresolved: LLM-generated paraphrases may share systematic biases with the evaluation model, potentially over- or under-estimating true deployment stability.
- What evidence would resolve it: Collecting real prompt variations from multi-team clinical deployments and measuring flip rates under these naturally-occurring paraphrase distributions.

## Limitations
- Flip rates depend on LLM-generated paraphrases that may not reflect real-world prompt variations encountered in clinical deployment
- The stability-calibration correlation is modest (ρ = 0.204) and only measurable for models with probability access, limiting generalizability to API-based deployments
- The LLM-in-the-loop optimizer's effectiveness depends on quality of failure feedback and candidate generation, which weren't exhaustively validated across diverse failure modes

## Confidence
- **High confidence**: The empirical finding that accuracy and stability are decoupled at the prompt level (supported by consistent patterns across 9 model-task combinations)
- **Medium confidence**: The claim that joint optimization improves stability with modest accuracy trade-offs, as results show variability across models and tasks
- **Low confidence**: The generalizability of paraphrase-based flip rates to real-world clinical prompt drift without further validation on actual deployment scenarios

## Next Checks
1. **Real-world drift validation**: Test whether LLM-generated paraphrases predict stability under actual prompt variations encountered in clinical workflows (e.g., different template styles from multiple vendors)
2. **Cross-task generalizability**: Apply the stability-aware optimization framework to additional clinical tasks (e.g., radiology report summarization) to assess whether larger models consistently show better stability-accuracy trade-offs
3. **Cost-benefit analysis**: Quantify the operational overhead of stability-aware prompt optimization versus the clinical risk reduction from improved reliability, including compute costs for K paraphrases and N candidates per iteration