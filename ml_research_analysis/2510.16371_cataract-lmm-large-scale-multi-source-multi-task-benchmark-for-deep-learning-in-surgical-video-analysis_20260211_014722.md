---
ver: rpa2
title: 'Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning
  in Surgical Video Analysis'
arxiv_id: '2510.16371'
source_url: https://arxiv.org/abs/2510.16371
tags:
- case
- surgical
- dataset
- instrument
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cataract-LMM, a large-scale, multi-source,
  multi-task dataset for deep learning in surgical video analysis. The dataset comprises
  3,000 phacoemulsification cataract surgery videos from two clinical centers, featuring
  diverse procedural variations from surgeons of varying experience levels.
---

# Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis

## Quick Facts
- **arXiv ID:** 2510.16371
- **Source URL:** https://arxiv.org/abs/2510.16371
- **Reference count:** 36
- **Primary result:** Introduces Cataract-LMM, a large-scale dataset for deep learning in surgical video analysis with 3,000 phacoemulsification cataract surgery videos from two clinical centers.

## Executive Summary
Cataract-LMM is a comprehensive dataset designed to advance deep learning applications in surgical video analysis. It contains 3,000 phacoemulsification cataract surgery videos from two clinical centers, featuring diverse procedural variations from surgeons of varying experience levels. The dataset provides four annotation layers: temporal phase labels for 13 surgical phases, instance segmentation masks for instruments and anatomical structures, spatiotemporal interaction tracking, and quantitative skill scores based on established rubrics. Technical validation demonstrates strong performance in surgical phase recognition, instance segmentation, and skill assessment tasks, with a notable 22% performance drop observed when models trained on one center are evaluated on another, highlighting the dataset's utility for domain adaptation research.

## Method Summary
The dataset comprises 3,000 phacoemulsification cataract surgery videos from two clinical centers (Farabi and Noor), with different hardware specifications and surgeon skill levels creating a challenging domain shift scenario. The annotation layers include temporal phase labels for 13 surgical phases across 150 videos, instance segmentation masks for instruments and anatomical structures in 6,094 frames, spatiotemporal interaction tracking in 170 videos, and quantitative skill scores based on established rubrics for 170 videos. Technical validation was performed using benchmarking experiments with state-of-the-art deep learning models including MViT for phase recognition, YOLOv11 for segmentation, and TimeSformer for skill assessment. The dataset establishes a domain adaptation baseline by training models on one center and evaluating on another, revealing significant performance degradation in out-of-domain testing.

## Key Results
- Establishes a domain adaptation baseline with 22% performance drop when models trained on one center are evaluated on another
- Demonstrates strong performance in surgical phase recognition with MViT achieving high macro F1 scores
- Shows effective instrument segmentation with YOLOv11 achieving high mAP scores across different class granularities
- Validates the utility of quantitative skill scores correlated with kinematic features from tracking data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-source data acquisition exposes domain fragility in surgical models, serving as a stress-test for generalization.
- **Mechanism:** The dataset introduces "hard" domain shifts by combining data from two centers using different hardware (720×480 vs 1920×1080 resolution) and surgeon skill levels. This heterogeneity forces models to learn robust features rather than overfitting to a single center's imaging artifacts or workflow idiosyncrasies.
- **Core assumption:** The performance drop observed is primarily driven by the technical and procedural differences between centers (domain shift) rather than random noise.
- **Evidence anchors:**
  - The paper explicitly reports a "22% performance drop in out-of-domain testing."
  - Page 11 describes the experimental design where models trained on Center 1 (Farabi) were evaluated on Center 2 (Noor).
  - Related work like *EndoARSS* supports the difficulty of domain adaptation in surgical scenes due to "confused image features" and variability.
- **Break condition:** If the 22% performance drop were attributable solely to the smaller sample size of the test set (70 videos) rather than distributional shift, the mechanism of "heterogeneity-induced robustness" would be weakened.

### Mechanism 2
- **Claim:** Linking instrument tracking trajectories to skill scores establishes a quantitative proxy for surgical competence.
- **Mechanism:** The dataset connects high-frequency spatiotemporal tracking data (instrument tip coordinates) with low-frequency expert ratings. This allows models to learn the correlation between "motion economy" (smoothness, path length) and "competence" (ICO-OSCAR scores), transforming subjective assessment into objective, learnable kinematic features.
- **Core assumption:** The 6-indicator rubric (adapted from GRASIS/ICO-OSCAR) captures the true quality of surgery and is reliably reflected in the video data.
- **Evidence anchors:**
  - Page 16 (Figure 11) visually compares the "smooth" trajectory of a competent surgeon against the "hesitant" path of a novice.
  - Page 10 notes that skill scores were "negatively correlated with procedural duration," validating the rubric's construct validity.
  - *RP-SAM2* highlights the importance of accurate instrument segmentation for skill assessment, reinforcing the utility of the tracking layer.
- **Break condition:** If kinematic metrics (velocity, jerk) show no significant correlation with expert scores during regression analysis, the mechanism linking motion dynamics to skill evaluation would fail.

### Mechanism 3
- **Claim:** Hierarchical class grouping mitigates visual ambiguity in instrument segmentation.
- **Mechanism:** Surgical instruments often share high visual similarity (e.g., different knives or forceps). By providing a 12-class taxonomy but benchmarking 3-class (tissue vs. instrument) and 9-class (grouped instruments) tasks, the dataset allows models to trade semantic granularity for accuracy, effectively managing the "inter-class similarity" problem.
- **Core assumption:** The performance degradation in the 12-class task is due to visual similarity and class imbalance rather than insufficient model capacity.
- **Evidence anchors:**
  - Page 15 (Table 10) shows Task 2 (9 classes) achieves the highest mAP (75.17), outperforming Task 3 (12 classes, 73.19 mAP), suggesting a "balanced approach reduces ambiguity."
  - Mentions "instance segmentation masks for instruments and anatomical structures" as a core utility.
  - *CAT-SG* emphasizes the need for fine-grained understanding, while this mechanism provides a pathway to handle the difficulty of such fine distinctions.
- **Break condition:** If distinct classes (e.g., "Phaco Handpiece") showed significantly higher error rates than grouped classes purely due to occlusion rather than similarity, the mechanism would need adjustment.

## Foundational Learning

- **Concept: Domain Adaptation (Transfer Learning)**
  - **Why needed here:** The paper centers on the performance degradation when moving from one clinical center to another. Understanding how to map features from a "source" domain (Farabi) to a "target" domain (Noor) is critical for using this dataset effectively.
  - **Quick check question:** If a model achieves 85% accuracy on Farabi data but 65% on Noor data, is the failure likely due to overfitting on source resolution or distinct surgical styles?

- **Concept: Semantic Segmentation Granularity**
  - **Why needed here:** The dataset defines three distinct tasks (3-class, 9-class, 12-class). A user must understand that "more classes" often implies "lower accuracy" due to fine-grained visual confusion (e.g., distinguishing a Secondary Knife from a Primary Knife).
  - **Quick check question:** For a safety-critical application requiring only tool presence detection, should you train on Task 1 or Task 3?

- **Concept: Spatiotemporal Feature Extraction**
  - **Why needed here:** The paper benchmarks Temporal Convolutional Networks (TeCNO) and Vision Transformers (MViT) against CNNs. Understanding that surgical phases are defined by *sequences* of actions (time), not just static frames, is essential for model selection.
  - **Quick check question:** Why would a Transformer (MViT) potentially outperform a simple CNN+LSTM on the phase recognition task?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Heterogeneous video sources (S1: 720p@30fps, S2: 1080p@60fps) → Preprocessing (Downsampling to 4fps, Resizing to 224x224)
  - Data Layers: 4 Parallel Annotation Tracks (Phase CSV, Segmentation JSON/YOLO, Tracking JSON, Skill CSV)
  - Model Layer: Encoder (ResNet/EfficientNet) → Temporal Aggregator (LSTM/Transformer/TeCNO) → Task Head

- **Critical path:**
  1. Data Curation: Ensure video splits are done at the *video level* (not frame level) to prevent leakage (Page 11)
  2. Domain Split: Adhere to the defined split where Training/Val is Center 1, and Test is mixed Center 1/Center 2 to properly benchmark domain shift (Page 10)
  3. Sampling: Apply the 0.5s minimum frame interval for segmentation tasks to ensure diversity (Page 6)

- **Design tradeoffs:**
  - Semantic Granularity: High (12 classes) vs. General (3 classes). Choose based on the application need for tool differentiation vs. pure detection reliability
  - Temporal Resolution: 4 fps is used for phase recognition (computational efficiency) vs. raw 30/60 fps for tracking (kinematic precision)
  - Architecture: Two-stage (CNN backbone + Temporal model) offers flexibility for feature analysis; End-to-end (Video Transformer) offers higher performance but less interpretability

- **Failure signatures:**
  - Capsule Polishing Confusion: Expect low F1-scores here due to visual similarity with other phases (Page 13)
  - Domain Collapse: A sudden 20%+ accuracy drop indicates the model has overfit to the source center's imaging characteristics (lighting, resolution)
  - Class Imbalance: The "Phacoemulsification" phase dominates (30% of frames), potentially biasing loss functions if weighting is not applied (Page 5)

- **First 3 experiments:**
  1. Establish Phase Baseline: Train MViT or Swin-T on the training split and evaluate on the "Out-of-Distribution" (Noor) test set to quantify the domain gap
  2. Granularity Test: Train YOLOv11-L on Task 1 vs. Task 3 to measure the mAP trade-off between general "Instrument" detection vs. specific instrument classification
  3. Skill Correlation: Extract motion trajectories from the tracking dataset and calculate the Pearson correlation with the provided skill scores to validate the kinematic signal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain adaptation techniques effectively mitigate the 22% performance drop observed in surgical phase recognition when transferring models from the Farabi clinical center to the Noor center?
- Basis in paper: The paper explicitly establishes a domain adaptation baseline and reports a consistent 22% average degradation in Macro F1-score when evaluating models on the out-of-distribution (Noor) test set, describing this domain shift as a "key challenge."
- Why unresolved: While the paper identifies the performance gap caused by heterogeneous hardware (resolution, fps) and clinical settings, it only establishes the baseline error rate without implementing or testing specific domain adaptation algorithms to correct it.
- What evidence would resolve it: A study applying Unsupervised Domain Adaptation (UDA) or Domain Generalization methods to this dataset, demonstrating a statistically significant reduction in the performance gap between the in-domain and out-of-distribution test sets.

### Open Question 2
- Question: Does multi-task learning (simultaneous training) on the Cataract-LMM annotation layers improve performance on individual tasks compared to the single-task baselines provided?
- Basis in paper: The paper title and motivation emphasize the creation of a "Multi-Task" benchmark to develop "generalizable multi-task deep learning models," yet the technical validation section benchmarks phase recognition, segmentation, and skill assessment strictly in isolation.
- Why unresolved: It remains untested whether the semantic relationships between tasks (e.g., the correlation between instrument presence and surgical phase) could be leveraged through shared representations to boost overall accuracy or data efficiency.
- What evidence would resolve it: Benchmark results from a multi-task architecture trained concurrently on the dataset's various annotations, comparing the per-task performance against the single-task baselines (e.g., MViT-B for phases, YOLOv11 for segmentation) reported in the paper.

### Open Question 3
- Question: Can the fusion of quantitative kinematic data (motion trajectories) with video features improve the performance or explainability of automated surgical skill assessment?
- Basis in paper: The authors note that the linkage between tracking data and skill scores enables the development of "explainable, multi-modal models... that can fuse high-level video features with precise instrument dynamics," but the provided benchmark relies solely on video-based classification models like TimeSformer.
- Why unresolved: The qualitative analysis shows a visual correlation between motion economy and skill, but the paper does not quantify whether explicitly feeding kinematic features (velocity, jerk, path length) into the assessment models improves classification accuracy over visual features alone.
- What evidence would resolve it: Experimental results comparing the skill classification accuracy of a kinematics-only or multi-modal (video + kinematics) model against the video-only baselines (e.g., R3D-18, TimeSformer) established in the paper.

## Limitations
- The dataset focuses on a single surgical procedure (phacoemulsification), potentially limiting generalizability to other surgical types
- Domain adaptation experiments are constrained to only two clinical centers, potentially underrepresenting full surgical environment variability
- Skill assessment relies on expert-generated scores that may contain subjective bias despite standardized rubrics
- The tracking dataset's instrument-tissue interaction annotations cover only 170 videos out of 3,000 total

## Confidence
- **High Confidence:** The multi-source design creating measurable domain shift (22% performance drop) is well-supported by experimental data and aligns with established principles of domain adaptation
- **Medium Confidence:** The mechanism linking instrument tracking to skill assessment is theoretically sound and visually supported, but requires more extensive validation across diverse surgical styles
- **Medium Confidence:** The hierarchical class grouping strategy for segmentation is practical and demonstrated, but the optimal granularity level depends heavily on specific application requirements

## Next Checks
1. **Domain Generalization Stress Test:** Train a model on the combined Farabi-Noor dataset and evaluate on a third, unseen cataract surgery dataset (if available) to test true out-of-distribution generalization beyond the two-center setup
2. **Skill Assessment Correlation Analysis:** Perform a comprehensive statistical analysis (Pearson/Spearman correlation, regression) between the kinematic features extracted from tracking data and the expert-derived skill scores across all 170 videos to quantify the strength of this relationship
3. **Temporal Resolution Impact Study:** Systematically vary the frame rate used for phase recognition (e.g., 2fps, 4fps, 8fps, 16fps) and measure the trade-off between computational efficiency and recognition accuracy, particularly for fine-grained phases like "Capsule Polishing" that may require higher temporal resolution