---
ver: rpa2
title: Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting
  via Experience Replay
arxiv_id: '2601.13456'
source_url: https://arxiv.org/abs/2601.13456
tags:
- drift
- replay
- fedavg
- buffer
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles catastrophic forgetting in federated learning
  under temporal concept drift. The authors simulate seasonal class availability on
  Fashion-MNIST, where only subsets of classes are available in each phase, causing
  standard FedAvg to collapse from 74% to 28% accuracy.
---

# Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay

## Quick Facts
- arXiv ID: 2601.13456
- Source URL: https://arxiv.org/abs/2601.13456
- Authors: Sahasra Kokkula; Daniel David; Aaditya Baruah
- Reference count: 5
- One-line result: Client-side experience replay prevents catastrophic forgetting in federated learning under seasonal concept drift, recovering accuracy from 28% to 78-82% with 50 samples/class buffer.

## Executive Summary
This work tackles catastrophic forgetting in federated learning under temporal concept drift. The authors simulate seasonal class availability on Fashion-MNIST, where only subsets of classes are available in each phase, causing standard FedAvg to collapse from 74% to 28% accuracy. They propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training, requiring no server-side changes. With a 50-sample-per-class buffer, accuracy recovers to 78-82%, effectively preventing forgetting. An ablation study shows clear memory-accuracy trade-offs as buffer size increases. The method works by continuously re-exposing the model to previously seen classes during local updates, addressing the root cause of forgetting in non-stationary federated settings.

## Method Summary
The authors propose client-side experience replay for federated learning under temporal concept drift. Each client maintains a small buffer of past samples and mixes these with current data during local training. The method requires no server-side changes, preserving the FedAvg aggregation protocol. Clients train on the union of their current seasonal data and buffered samples using standard SGD. The buffer is initialized empty and filled during the first phase with IID samples from all classes. When classes become unavailable in subsequent seasons, the buffer preserves their representations by maintaining gradient signal during training. Buffer updates follow a fill-until-capacity policy, with no specified sampling strategy beyond this.

## Key Results
- Standard FedAvg suffers catastrophic forgetting under seasonal drift, with accuracy dropping from 74% to 28%
- Client-side experience replay with 50 samples/class buffer recovers accuracy to 78-82%
- Ablation study shows diminishing returns: 0→28%, 10→65%, 25→74%, 50→78%, 100→80% accuracy
- Per-class heatmap shows complete knowledge loss for classes not in current season without replay

## Why This Works (Mechanism)

### Mechanism 1: Gradient Interference from Non-Stationary Class Distributions
- Claim: When training data excludes previously learned classes, SGD updates overwrite class-specific representations, causing accuracy collapse on absent classes.
- Mechanism: Local SGD optimizes only for available classes per season; without gradient signal from absent classes, their learned representations degrade through interference from current-class updates.
- Core assumption: Model capacity is shared across classes, so optimizing for current classes directly affects weights encoding previous classes.
- Evidence anchors:
  - [abstract] "standard FedAvg suffers catastrophic forgetting under seasonal drift... accuracy dropping from 74% to 28%"
  - [Page 5, Figure 2] "heatmap shows complete knowledge loss for classes not in current season. During Summer (Rounds 16-20), only T-shirt, Dress, and Sandal retain accuracy; all other classes drop to 0%"
  - [corpus] Weak direct evidence; related work FedDAA discusses "real drift (shift in conditional distribution P(y|x))" but focuses on clustering rather than replay mechanisms.
- Break condition: If classes were learned in completely disjoint parameter subspaces (e.g., explicit task-specific heads), forgetting would not occur—but standard CNNs share representations.

### Mechanism 2: Rehearsal-Based Gradient Preservation via Buffer Mixing
- Claim: Mixing buffered past samples with current data during local training maintains gradient signal for all classes, preventing representation drift.
- Mechanism: Each local training batch now includes examples from previously seen classes, producing gradients that preserve their representations alongside current-class optimization.
- Core assumption: Buffer contains representative samples from past classes and is sufficiently diverse to maintain decision boundaries.
- Evidence anchors:
  - [abstract] "client-side experience replay... maintains a small buffer of past samples mixed with current data during local training"
  - [Page 2, Algorithm 1] "Train locally for E epochs on D^(s)_k ∪ R_k using SGD(η)" where R_k is the replay batch
  - [corpus] "Non-Uniform Memory Sampling in Experience Replay" (arXiv:2502.11305) confirms rehearsal mitigates catastrophic forgetting but doesn't address FL-specific constraints.
- Break condition: If buffer size is too small relative to class complexity, or if sampling is biased, decision boundaries may still degrade.

### Mechanism 3: Memory-Performance Scaling with Diminishing Returns
- Claim: Accuracy recovery scales monotonically with buffer size but exhibits diminishing returns beyond a dataset-dependent threshold.
- Mechanism: More buffer samples provide better coverage of class-conditional distributions, improving decision boundary preservation, but marginal utility decreases as coverage saturates.
- Core assumption: Fashion-MNIST class distributions are relatively simple; more complex datasets may require larger buffers.
- Evidence anchors:
  - [Page 6, Table 1] Ablation shows: 0→28%, 10→65%, 25→74%, 50→78%, 100→80%
  - [Page 7] "Returns diminish beyond 50 samples/class for this dataset size"
  - [corpus] No direct corpus evidence on buffer-size scaling in FL settings.
- Break condition: For datasets with higher intra-class variance or finer-grained distinctions, the saturation point would shift upward.

## Foundational Learning

- **Concept: FedAvg Aggregation**
  - Why needed here: Understanding that server-side aggregation remains unchanged is critical—the intervention is purely client-side.
  - Quick check question: If you modified server aggregation instead of local training, would that require protocol changes?

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - Why needed here: This is the core failure mode being addressed; understanding *why* neural networks forget helps diagnose when replay will help.
  - Quick check question: Why does training on new data degrade performance on old data in a shared-capacity model?

- **Concept: Experience Replay / Rehearsal Methods**
  - Why needed here: The proposed solution adapts continual learning techniques to federated settings.
  - Quick check question: What is the minimum information needed to preserve a learned decision boundary?

## Architecture Onboarding

- **Component map:**
  Server: Broadcast weights → Aggregate client updates (FedAvg) → Broadcast updated weights
              ↓                                                              ↑
  Client: Receive weights → Load seasonal data D^(s) → Sample buffer R_k → 
           Train on D^(s) ∪ R_k → Update local buffer → Send weight update Δw

- **Critical path:**
  1. Buffer initialization (empty at start)
  2. First phase fills buffer with initial IID samples
  3. Season transitions trigger buffer-only preservation of disappearing classes
  4. Buffer update policy (fill-until-capacity) determines long-term retention

- **Design tradeoffs:**
  - **Memory vs. accuracy:** 10 samples/class (78KB, 65%) vs. 50 samples/class (392KB, 78%) vs. 100 samples/class (784KB, 80%)
  - **Privacy vs. utility:** Local buffers preserve FL privacy guarantees but increase client storage; gradient leakage from model updates remains a separate concern
  - **Buffer policy choices:** "Fill-up" vs. reservoir sampling—current implementation stops adding once full, which may cause temporal bias

- **Failure signatures:**
  - Accuracy drops sharply at season transitions → forgetting without replay
  - Per-class accuracy collapses to 0% for non-current classes → complete representation overwrite
  - Recovery doesn't occur despite buffer → buffer too small, sampling bias, or buffer not being used in training

- **First 3 experiments:**
  1. Reproduce baseline forgetting: Run FedAvg with seasonal drift and no buffer; verify 74%→28% drop
  2. Buffer size ablation: Test 0, 10, 25, 50, 100 samples/class; plot accuracy curve to find saturation point
  3. Generalization check: Apply same configuration to CIFAR-10 with analogous seasonal splits; observe whether 50 samples/class still suffices or if scaling is needed

## Open Questions the Paper Calls Out
None

## Limitations
- Buffer sampling strategy (random vs. class-balanced) and replay batch mixing ratio are unspecified, affecting reproducibility
- The 50 samples/class buffer size may not generalize to more complex datasets without further validation
- The claim that "no server-side changes are required" assumes buffer updates don't leak privacy, but gradient-based model updates could still reveal information about buffer contents

## Confidence
- **High confidence:** The core mechanism (rehearsal prevents forgetting by maintaining gradient signal for absent classes) and baseline accuracy drop (74%→28%) are well-supported by the results
- **Medium confidence:** The 50 samples/class buffer size is effective for Fashion-MNIST but may not generalize to more complex datasets without further validation
- **Low confidence:** The claim that "no server-side changes are required" assumes buffer updates don't leak privacy, but gradient-based model updates could still reveal information about buffer contents

## Next Checks
1. **Buffer sampling policy:** Test both random sampling and class-balanced selection when adding to buffers; compare whether class distribution preservation affects accuracy
2. **Mixing ratio impact:** Systematically vary the proportion of buffer vs. current data in training batches (e.g., 50-50, 75-25, 90-10) to identify optimal replay intensity
3. **Dataset scaling:** Apply the same methodology to CIFAR-10 with seasonal splits; measure whether 50 samples/class remains sufficient or if scaling relationships exist with dataset complexity