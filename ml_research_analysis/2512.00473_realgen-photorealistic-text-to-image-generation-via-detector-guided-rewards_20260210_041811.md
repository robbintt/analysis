---
ver: rpa2
title: 'RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards'
arxiv_id: '2512.00473'
source_url: https://arxiv.org/abs/2512.00473
tags:
- image
- arxiv
- generation
- images
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RealGen addresses the lack of photorealism in modern text-to-image
  models by introducing a detector-guided reward mechanism that quantifies AI artifacts
  using both semantic-level and feature-level synthetic image detectors. The method
  employs a two-stage optimization pipeline: first optimizing an LLM for prompt refinement
  to generate richer descriptions, then optimizing a diffusion model via GRPO to reduce
  artifacts and enhance realism.'
---

# RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards

## Quick Facts
- arXiv ID: 2512.00473
- Source URL: https://arxiv.org/abs/2512.00473
- Reference count: 40
- Key outcome: RealGen achieves state-of-the-art photorealism with 96.73% win rate against GPT-Image-1 and 50.15% win rate against real images

## Executive Summary
RealGen addresses the critical gap in photorealism for text-to-image generation by introducing a detector-guided reward mechanism that quantifies AI artifacts using both semantic-level and feature-level synthetic image detectors. The method employs a two-stage optimization pipeline: first refining text prompts through LLM optimization, then optimizing diffusion models via GRPO to reduce artifacts and enhance realism. This approach achieves state-of-the-art performance on the proposed RealBench benchmark, outperforming commercial models like GPT-Image-1, Qwen-Image, and FLUX-Krea while also excelling on established benchmarks like HPDv2 Photo.

## Method Summary
RealGen introduces a novel two-stage optimization pipeline that first refines text prompts through LLM optimization to generate richer, more detailed descriptions, then optimizes diffusion models via GRPO (Group Relative Policy Optimization) using detector-guided rewards. The core innovation lies in quantifying AI artifacts through synthetic image detectors operating at both semantic and feature levels, allowing the model to learn what makes images appear artificial. This approach specifically targets common failure modes in diffusion models, such as unnatural textures, inconsistent lighting, and implausible object arrangements, by providing explicit feedback on artifact presence rather than relying solely on human preference or reconstruction loss.

## Key Results
- Achieves 96.73% win rate against GPT-Image-1 on RealBench benchmark
- Scores 71.34 (Forensic-Chat) and 56.93 (OmniAID) on HPDv2 Photo subset
- Demonstrates 50.15% win rate against real images in arena-style comparisons

## Why This Works (Mechanism)
RealGen works by providing explicit, quantifiable feedback on AI artifacts through detector-guided rewards, which allows the model to understand and correct specific failure modes in photorealistic generation. The semantic-level detectors identify implausible object arrangements and contextual inconsistencies, while feature-level detectors catch subtle texture and lighting artifacts that humans might miss but detectors can reliably identify. This dual-detection approach creates a comprehensive feedback signal that guides the diffusion model away from common AI-generated patterns and toward more natural, photograph-like outputs.

## Foundational Learning

**Diffusion Models** - Why needed: Core backbone for generating images from text prompts through iterative denoising
Quick check: Verify understanding of denoising diffusion probabilistic models and their training objective

**Group Relative Policy Optimization (GRPO)** - Why needed: Reinforcement learning algorithm that optimizes policy relative to group performance rather than absolute values
Quick check: Confirm understanding of policy gradient methods and how GRPO differs from PPO

**Synthetic Image Detectors** - Why needed: AI models trained to distinguish real from synthetic images, providing quantitative feedback on realism
Quick check: Validate knowledge of binary classification models and their application to authenticity detection

**Text-to-Image Prompt Engineering** - Why needed: Rich, detailed prompts significantly improve generation quality and reduce ambiguity
Quick check: Test understanding of prompt optimization techniques and their impact on generation quality

## Architecture Onboarding

**Component Map**: Text Prompt → LLM Refiner → Text Encoder → Diffusion Model → Detector (Semantic + Feature) → Reward → GRPO Optimizer

**Critical Path**: The detector-guided reward signal flows from the image detectors through the GRPO optimizer back to the diffusion model, creating a feedback loop that progressively reduces AI artifacts. This path is critical because it directly addresses the realism gap that traditional reconstruction and adversarial losses fail to capture.

**Design Tradeoffs**: The method trades computational efficiency for photorealism by adding detector inference and GRPO optimization steps, but this tradeoff is justified by the significant quality improvements. The two-stage pipeline also requires more training time and resources compared to single-stage fine-tuning approaches.

**Failure Signatures**: Common failure modes include detector over-reliance leading to overly smoothed outputs, prompt refinement creating overly complex descriptions that confuse the diffusion model, and reward hacking where the model learns to fool specific detectors rather than improving general realism.

**First 3 Experiments**: 1) Run baseline diffusion model without detector rewards to establish performance floor, 2) Test detector-guided rewards with fixed prompts to isolate reward impact, 3) Evaluate LLM prompt refinement impact separately before full pipeline integration.

## Open Questions the Paper Calls Out
None

## Limitations
- Detector-guided reward system relies heavily on specific detection models (Forensic-Chat and OmniAID) that may not generalize to all AI artifact types
- Self-contained evaluation framework using RealBench benchmark could introduce bias since authors developed this benchmark
- Two-stage optimization pipeline requires significant computational resources, limiting accessibility for smaller research groups

## Confidence

**High confidence**: Method architecture and implementation details are well-described and reproducible
**Medium confidence**: Benchmark performance claims, as they depend on the validity and comprehensiveness of the RealBench dataset
**Medium confidence**: Comparison against commercial models (GPT-Image-1, Qwen-Image, FLUX-Krea), given potential variability in evaluation conditions

## Next Checks

1. Test RealGen's performance on established external benchmarks beyond RealBench to verify generalizability of results
2. Evaluate detector-guided reward effectiveness across diverse domains (e.g., medical imaging, satellite imagery) not represented in current training data
3. Conduct ablation studies to quantify individual contributions of semantic-level versus feature-level detectors to overall performance improvement