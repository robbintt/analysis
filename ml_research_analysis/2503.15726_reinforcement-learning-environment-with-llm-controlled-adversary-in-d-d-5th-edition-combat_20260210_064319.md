---
ver: rpa2
title: Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th
  Edition Combat
arxiv_id: '2503.15726'
source_url: https://arxiv.org/abs/2503.15726
tags:
- llms
- agents
- strategic
- action
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how LLMs like GPT-4o and LLaMA 3 can serve
  as adversarial agents in a D&D 5E combat simulation to train RL agents. Using a
  Deep Q-Network, smaller RL agents were trained to compete against LLM-controlled
  opponents in a rule-based environment.
---

# Reinforcement Learning Environment with LLM-Controlled Adversary in D&D 5th Edition Combat

## Quick Facts
- arXiv ID: 2503.15726
- Source URL: https://arxiv.org/abs/2503.15726
- Reference count: 15
- LLM adversaries in D&D 5E combat help RL agents avoid exploitable patterns and improve robustness

## Executive Summary
This paper introduces a D&D 5th Edition combat simulation environment where Large Language Models (LLMs) like GPT-4o and LLaMA 3 serve as adversarial agents to train reinforcement learning (RL) agents. Using a Deep Q-Network architecture, smaller RL agents were trained to compete against LLM-controlled opponents in a rule-based environment. The results demonstrate that while RL agents generally outperform LLM adversaries in standard win/loss metrics, training against LLM opponents leads to faster convergence and stronger overall performance, particularly in complex, multi-class scenarios. The approach reveals that LLM-driven adversaries help avoid exploitable patterns present in rule-based AI, producing more robust strategic agents.

## Method Summary
The researchers created a D&D 5E combat environment using the natural_20.py library, implementing a Deep Q-Network (DQN) agent with a convolutional neural network to process 7x7 viewport states with 16 channels. The RL agent was trained against LLM adversaries (GPT-4o, GPT-4o-mini, LLaMA 3, Mistral) using an 80/20 mix of rules-based and LLM-generated trajectories. State information was serialized into ASCII format and passed to LLMs via structured prompts, with GPT-4o using function calling for stable JSON responses. Training used a reward structure based on damage dealt relative to opponent health, with epsilon-greedy exploration and experience replay.

## Key Results
- RL agents trained against LLM adversaries achieved higher win rates in multi-class tournaments (146 wins vs 135 wins for rules-based trained agents)
- LLM adversaries helped RL agents avoid exploitable patterns present in deterministic rule-based AI
- Training against LLMs led to faster convergence compared to pure rules-based training
- GPT-4o provided the most stable responses using function calling, while smaller LLMs often returned invalid actions requiring fallback to random selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-controlled adversaries produce more robust RL agents by eliminating exploitable patterns in training.
- Mechanism: Rules-based AI follows deterministic patterns (e.g., "if attack possible, attack; else move closer"). RL agents learn to exploit these patterns rather than developing generalizable strategies. LLMs introduce stochastic, context-aware decision-making that varies behavior across similar states, forcing RL agents to learn fundamental tactical reasoning instead of pattern-matching.
- Core assumption: LLM behavioral diversity approximates meaningful strategic variation rather than random noise.
- Evidence anchors:
  - [Discussion]: "bugs in the rules-based AI get consistently taken advantage of during the RL training process, this exploitation of behavior does not provide a strong signal for the agent to perform well in the real world"
  - [Results, Table 5]: LLM-adversary trained RL agents (rl_gpt4_trained: 146 wins) outperformed rules-based trained agents (rl_rules_trained: 135 wins) in multi-class tournaments
  - [corpus]: Related work on hierarchical hybrid AI (arXiv:2512.00249) notes scripted agents offer "predictability and consistency" but lack adaptability
- Break condition: If LLM responses become predictable (e.g., due to temperature=0 or fine-tuning collapse), the mechanism reverts to rules-like exploitability.

### Mechanism 2
- Claim: Mixed-trajectory training (80% rules-based, 20% LLM) balances computational cost with strategic diversity.
- Mechanism: Rules-based AI provides fast, high-volume trajectory generation for basic mechanics learning. LLM trajectories, though slower and costlier, inject strategic complexity at critical decision points. The 80/20 split ensures sufficient exposure to complex scenarios without prohibitive training time.
- Core assumption: Basic mechanics learning from rules-based AI transfers to LLM-challenge scenarios.
- Evidence anchors:
  - [Methodology, Table 1]: Explicit configuration of "80% Rules-based AI + 20% LLM" trajectory generation method
  - [Results, Fig. 7]: Training against LLMs achieved higher best rewards (gpt-4o-mini: 8.01) vs rules-based (5.96) despite same training iterations
  - [corpus]: Limited direct corpus evidence on mixed-trajectory training; related work focuses on either pure RL or pure LLM approaches
- Break condition: If LLM trajectory proportion is too low, agents may overfit to rules-based patterns; if too high, training becomes prohibitively slow.

### Mechanism 3
- Claim: Structured prompting with tool/function-calling improves LLM action selection stability in game environments.
- Mechanism: LLMs receive ASCII map representations, state descriptions, and numbered action lists. GPT-4o's "tools" functionality enforces JSON-formatted responses, reducing parsing failures. Smaller LLMs (LLaMA, Mistral) without tool support fall back to random actions when responses are unparseable.
- Core assumption: ASCII representation captures sufficient spatial information for tactical reasoning.
- Evidence anchors:
  - [Methodology, Fig. 4]: Full prompt template with ASCII map, health percentages, and numbered action list
  - [Discussion]: "GPT-4o based LLM with its function to use tools has shown to provide the most stable response... LLaMA3 in contrast does not respond well to the same prompt and results in providing invalid responses"
  - [corpus]: Pokemon LLM tournament paper (arXiv:2508.01623) similarly uses structured battle state descriptions for LLM decision-making
- Break condition: If state complexity exceeds prompt context limits, or if action spaces grow beyond what can be enumerated, the mechanism degrades.

## Foundational Learning

- Concept: Deep Q-Networks (DQN)
  - Why needed here: Core RL algorithm for training combat agents. Must understand Q-learning, experience replay, target networks, and epsilon-greedy exploration to interpret training dynamics and hyperparameter choices.
  - Quick check question: Explain why the Bellman equation update includes a discount factor (γ=0.99) and what happens if γ is set too low.

- Concept: LLM Prompt Engineering for Structured Outputs
  - Why needed here: LLM adversary performance depends entirely on prompt design. Must understand how to convert game states to text, enumerate valid actions, and enforce parseable response formats.
  - Quick check question: Given the prompt template in Fig. 4, identify three failure modes where an LLM might return an unparseable action.

- Concept: D&D 5E Combat Mechanics (SRD)
  - Why needed here: Environment rules define state/action spaces and reward dynamics. Must understand turn structure, action types, movement, terrain effects, and class-specific abilities to debug agent behavior.
  - Quick check question: Why might a rules-based AI that always attacks when possible be exploitable in scenarios with cover and movement options?

## Architecture Onboarding

- Component map:
  Environment Layer (natural_20.py) -> State Encoder (7x7 viewport, CNN) -> DQN Agent (FC layers) -> Action Selection -> Environment Execution -> Reward Calculation

- Critical path:
  1. Environment generates valid actions for current state
  2. State serialized to ASCII prompt format
  3. LLM selects action (or rules-based fallback)
  4. Environment executes, returns next state and reward
  5. DQN stores transition, samples batch, updates Q-values

- Design tradeoffs:
  - **LLM choice**: GPT-4o (stable, expensive) vs open-source (cheaper, less stable response parsing)
  - **Trajectory mix**: Higher LLM percentage improves strategic diversity but increases training time from 4 hours to ~24 hours
  - **State representation**: 7x7 viewport limits tactical depth but keeps network tractable; larger viewports increase compute quadratically
  - **Reward shaping**: Damage-scaled loss rewards (-10 * health/max_health) improve convergence but may bias toward conservative play

- Failure signatures:
  - LLM returning action indices outside valid range → fallback to random action (degrades training quality)
  - RL agent exploiting rules-based patterns → high win rate against rules-based AI but poor performance against LLMs
  - Training divergence → Q-values exploding; check learning rate, reward scale, or target network update frequency
  - Slow convergence → epsilon decay too fast, replay buffer too small, or reward signal too sparse

- First 3 experiments:
  1. Replicate fighter-only tournament with rules-based adversary to establish baseline win rate and convergence time (expected: ~6 average reward, 4 hours training)
  2. Swap to GPT-4o-mini adversary with 20% trajectory mix; compare convergence speed and final performance (expected: faster convergence, ~8 average reward)
  3. Ablate reward shaping by using binary win/loss rewards instead of damage-scaled losses; measure sample efficiency impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What prompting strategies and/or fine-tuning approaches can improve LLM response stability for game action selection?
- Basis in paper: [explicit] Authors state: "LLaMA3 in contrast does not respond well to the same prompt and results in providing invalid responses... A different prompting and response parsing strategy should be further explored to improve on this as well as fine tuning the LLM to obtain a response stable variant."
- Why unresolved: Different LLMs respond inconsistently to identical prompt templates; current approach falls back to random actions when parsing fails, introducing noise into training.
- What evidence would resolve it: Comparative study of prompting strategies across LLMs with parsing success rates, action validity metrics, and resulting RL agent performance.

### Open Question 2
- Question: Can LLM-controlled adversaries effectively train RL agents in multi-party cooperative D&D scenarios requiring coordination among multiple party members?
- Basis in paper: [explicit] Authors identify as future work: "Enhancing the framework to incorporate multi-agent training in order to capture cooperation in a multiplayer D&D 5E game where there are multiple party members all cooperating to defeat multiple opponents."
- Why unresolved: Current study only examined 1v1 combat; multi-agent coordination introduces exponentially larger action spaces, shared state representations, and emergent cooperative strategy challenges.
- What evidence would resolve it: Experiments in 3v3 or larger party scenarios measuring win rates, coordination metrics, and qualitative analysis of emergent cooperative behaviors.

### Open Question 3
- Question: What is the optimal ratio of LLM-generated versus rules-based adversarial trajectories for efficient RL training?
- Basis in paper: [inferred] The paper uses an arbitrary 80/20 split (20% LLM) primarily for cost mitigation, noting LLM adversaries help avoid exploitable patterns in rules-based AI, but no ablation systematically varied this ratio.
- Why unresolved: No experiments tested whether different proportions of LLM vs. rules-based training affect convergence speed, final performance, or behavioral robustness.
- What evidence would resolve it: Ablation study testing multiple ratios (100/0, 50/50, 20/80, 0/100) with metrics on convergence, tournament performance, and strategy diversity.

### Open Question 4
- Question: Can hybrid architectures integrating LLMs as internal agent components outperform agents trained only against external LLM adversaries?
- Basis in paper: [explicit] Authors propose: "investigating hybrid models that combine the strengths of RL agents and LLMs could lead to more robust and adaptable AI systems."
- Why unresolved: Current approach uses LLMs purely as external opponents during training, not as architectural components within the agent itself.
- What evidence would resolve it: Comparative experiments between adversarial-only training and integrated architectures (e.g., LLM-based action embeddings, policy guidance modules) on identical benchmarks.

## Limitations
- LLM responses can be inconsistent and sometimes unparseable, leading to fallback random actions that may degrade training quality
- The computational cost of LLM-based adversaries is significant—up to 24 hours per training run compared to 4 hours for rules-based AI
- The state representation (7x7 viewport) may limit tactical depth, and the trajectory generation mix (80/20) was chosen heuristically without ablation studies

## Confidence
- High confidence: RL agents outperform LLM adversaries in standard win/loss metrics (Table 5: 146 vs 135 wins)
- Medium confidence: LLM adversaries produce more robust RL agents by eliminating exploitable patterns (mechanism supported but not directly validated)
- Medium confidence: Mixed-trajectory training (80/20) balances computational cost with strategic diversity (config explicit but not empirically justified)

## Next Checks
1. **Pattern exploitability test**: Train RL agents against a rule-based AI with a known exploitable pattern (e.g., always attacks when possible), then test against a rule-based AI with the pattern removed. Compare win rates to validate Mechanism 1.
2. **Trajectory mix ablation**: Repeat the fighter-only tournament training with different trajectory generation ratios (100% rules, 50/50, 20/80, 100% LLM). Plot convergence curves and final performance to empirically justify the 80/20 split.
3. **Response stability measurement**: Log all LLM responses during training, categorize parseable vs unparseable outcomes, and measure the frequency of fallback to random actions. Correlate parseability rates with training performance degradation.