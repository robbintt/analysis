---
ver: rpa2
title: Conformal Set-based Human-AI Complementarity with Multiple Experts
arxiv_id: '2508.06997'
source_url: https://arxiv.org/abs/2508.06997
tags:
- human
- experts
- conformal
- subset
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of leveraging multiple human experts
  in conjunction with AI decision support systems for improved multiclass classification.
  The key challenge lies in selecting the most relevant subset of experts for each
  instance, given that only certain experts may be relevant for each data sample.
---

# Conformal Set-based Human-AI Complementarity with Multiple Experts

## Quick Facts
- arXiv ID: 2508.06997
- Source URL: https://arxiv.org/abs/2508.06997
- Reference count: 40
- Primary result: Greedy conformal-based expert subset selection outperforms naive approaches (ALL HUMANS, RANDOM SUBSET) with 98.48% accuracy on CIFAR-10H

## Executive Summary
This paper addresses the problem of selecting optimal subsets of human experts for multiclass classification when working alongside AI systems. The key insight is that not all experts are equally useful for every instance, and conformal prediction sets can guide this selection by narrowing the label space. The authors propose a greedy algorithm that selects experts most likely to choose labels within the conformal set, improving classification accuracy over naive approaches like using all experts or random subsets. The framework is validated on CIFAR-10H and ImageNet-16H datasets, demonstrating significant improvements over baselines.

## Method Summary
The approach combines conformal prediction with expert subset selection for human-AI complementarity. First, a pre-trained classifier generates probability scores on calibration data to construct conformal sets C(x) with coverage guarantee P[y ∈ C(x)] ≥ 1−α. Expert confusion matrices are estimated from historical predictions on an estimation set. For each test instance, the algorithm computes conditional success probabilities for each expert given C(x), selects a pseudo-label maximizing the product of these probabilities, and includes experts with conditional success > 0.5 in the subset. The final prediction uses majority vote among selected experts. The method is training-free and theoretically grounded with provable coverage guarantees.

## Key Results
- Greedy selection achieves 98.48% accuracy on CIFAR-10H (5 experts), significantly outperforming ALL HUMANS (p<0.001)
- Maintains effectiveness on ImageNet-16H even as expert count increases to 6
- Conformal sets provide better guidance for expert selection than top-k set predictors
- Robust performance even with limited calibration data (10-20% of dataset)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conformal prediction sets tighten the theoretical lower bound on collaborative accuracy compared to full label space
- **Mechanism:** By restricting expert predictions to C(x) ⊆ Y with coverage guarantee, the framework increases conditional success probability for aligned experts, yielding tighter bounds
- **Core assumption:** True label lies within C(x) with probability at least 1−α; experts maintain conditional independence
- **Evidence anchors:** [abstract] "conformal prediction sets to narrow down the label space"; [Section 4, Lemma 2] conformal set yields tighter lower bound; [corpus] conformal sets provide reliable uncertainty quantification

### Mechanism 2
- **Claim:** Greedy expert subset selection outperforms naive approaches by filtering for experts with predicted alignment to C(x)
- **Mechanism:** Algorithm computes conditional success ratios for each expert and label in C(x), selects pseudo-label maximizing product, includes expert only if ratio > 1
- **Core assumption:** Confusion matrices accurately estimated from historical data; expert error patterns are stable
- **Evidence anchors:** [abstract] "greedy algorithm... maximizes conditional success probability"; [Section 4, Algorithm 1] full greedy procedure; [Section 5.2, Figure 2] outperforms baselines

### Mechanism 3
- **Claim:** Restricting experts to predict from C(x) rather than Y improves accuracy even before subset selection
- **Mechanism:** Conformal set excludes irrelevant labels, raising baseline success probability by eliminating low-probability error modes
- **Core assumption:** Experts are more likely to select correct label when label space is reduced and still contains truth
- **Evidence anchors:** [Section 3.1] desired inequality chain for success probabilities; [Section 4, Lemma 1] lower bound on combined accuracy

## Foundational Learning

- **Concept: Conformal Prediction (split-conformal variant)**
  - **Why needed here:** Provides distribution-free, finite-sample coverage guarantees for prediction sets; enables narrowing label space with provable reliability
  - **Quick check question:** Given calibration scores s(xᵢ, yᵢ) = 1−f̂_{yᵢ}(xᵢ), how do you compute the quantile q̂_α and construct C(x_test)?

- **Concept: Confusion Matrix and Conditional Success Probability**
  - **Why needed here:** Algorithm requires estimating P[expert predicts y' | true label = y] for each expert; this drives subset selection
  - **Quick check question:** If an expert has confusion matrix C where C[true=dog, pred=cat]=0.15 and C[true=dog, pred=dog]=0.70, what is their conditional success probability for dog given conformal set {dog, cat}?

- **Concept: Human-AI Complementarity via Set-Valued Predictors**
  - **Why needed here:** Frames goal as AI providing narrowed options rather than single predictions, enabling human judgment on ambiguous cases
  - **Quick check question:** Why might a set-valued predictor enable complementarity where a point predictor does not?

## Architecture Onboarding

- **Component map:** Pre-trained classifier f̂ -> Conformal set builder -> Expert confusion matrix store -> Greedy subset selector (Algorithm 1) -> Final combination rule π -> Ŷ

- **Critical path:** 1) Calibration phase: Compute scores on D_cal, fit q̂_α 2) Estimation phase: Build confusion matrices Cⁱ for each expert 3) Inference per sample: (a) Get C(x) from conformal builder, (b) Collect initial predictions p̂ᵢ from all experts, (c) Run greedy selector to get S*(x), (d) Selected experts make final prediction from C(x), (e) Apply π to get Ŷ

- **Design tradeoffs:**
  - α tolerance: Lower α → higher coverage but larger sets (less filtering); paper uses α ≈ 0.001–0.01 for CIFAR-10H
  - Calibration data size: More data → better quantile estimate but requires more labeled examples; experiments show robustness even at 10% calibration
  - Combination rule π: Majority is simple; more sophisticated rules (weighted voting, scoring) are unexplored
  - Expert independence assumption: Violated if experts share biases; not modeled in current framework

- **Failure signatures:**
  - Large conformal sets (|C(x)| close to n): Model is poorly calibrated or α too low; sets become uninformative
  - Empty or single-expert subsets S*(x): All experts have conditional success ≤ 0.5 for C(x); indicates estimation failure or truly ambiguous cases
  - Coverage failure (y ∉ C(x)): Exceeds expected α rate; check calibration data distribution and model quality
  - No improvement over ALL HUMANS: Likely insufficient calibration or estimation data

- **First 3 experiments:**
  1. Sanity check: Implement split-conformal on held-out calibration set; verify coverage P[y_test ∈ C(x_test)] ≈ 1−α on test data
  2. Baseline comparison: On CIFAR-10H with h=5 experts, compare greedy vs. ALL HUMANS vs. RANDOM SUBSET; reproduce Figure 2 curves for accuracy vs. calibration data percentage
  3. Ablation on set predictor: Replace conformal sets with top-k sets (k = |C(x)|_avg); confirm greedy + conformal outperforms greedy + top-k

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be modified to handle inter-expert dependencies, given that the current theoretical lower bound relies on an independence assumption?
- **Basis in paper:** [explicit] Discussion states "the assumption of independence among experts may not hold, as their decisions can influence one another," suggesting modeling interdependencies as future work
- **Why unresolved:** Probability factorization in Equation 2 and subsequent optimization rely on independence of expert predictions
- **What evidence would resolve it:** Theoretical extension of Lemma 1 accounting for correlation, along with empirical simulations showing performance when experts influence each other

### Open Question 2
- **Question:** How does performance degrade in scenarios with significant class imbalance or non-representative calibration data?
- **Basis in paper:** [explicit] Authors note experiments assume well-distributed calibration data, indicating need for further exploration in scenarios with multiple classes and class imbalances
- **Why unresolved:** Conformal prediction set sizes and confusion matrix estimation may be skewed if calibration and estimation data don't represent true class distribution
- **What evidence would resolve it:** Experimental results on datasets with artificial class imbalance applied to calibration set, measuring drop in empirical success probability

### Open Question 3
- **Question:** Can more sophisticated methods for estimating expert confusion matrix improve human subset selection compared to current MLE approach?
- **Basis in paper:** [explicit] Discussion mentions "Exploring more sophisticated methods for estimating the confusion matrix would be advantageous," as current method relies on straightforward MLE
- **Why unresolved:** Accuracy of greedy selection depends heavily on confusion matrix parameters; simple MLE might be prone to overfitting or high variance with small estimation sets
- **What evidence would resolve it:** Comparative study evaluating framework's accuracy using Bayesian estimation or online updating techniques versus standard MLE on small estimation datasets

### Open Question 4
- **Question:** Would alternative combination policies, such as weighted scoring systems, outperform majority decision rule currently used?
- **Basis in paper:** [explicit] Authors suggest "Future work could enhance this framework by... exploring alternative collaboration methods, such as a scoring system instead of simple subset selection"
- **Why unresolved:** Current method selects subset and uses simple majority vote; unclear if utilizing confidence scores or specific probabilities from selected experts could further maximize conditional success probability
- **What evidence would resolve it:** Simulation results comparing majority decision rule against weighted aggregation function on CIFAR-10H and ImageNet-16H datasets

## Limitations

- Theoretical analysis assumes expert independence and stable confusion matrices, which may not hold in practice
- Empirical evaluation relies on synthetic expert subsets from existing datasets rather than real-time expert interactions
- Performance with larger expert pools (h=10-20) is extrapolated from h=5-6 experiments without direct validation

## Confidence

- **High Confidence:** Greedy selection algorithm's superiority over baselines is well-supported with statistical significance (p<0.001 on CIFAR-10H); theoretical framework is rigorously derived
- **Medium Confidence:** Claim that conformal sets provide better guidance than top-k sets is supported but limited to specific datasets and α ranges; confusion matrix estimation from limited data is plausible but not extensively validated
- **Low Confidence:** Method's scalability to larger expert pools is extrapolated without direct validation; impact of expert correlation and bias is acknowledged but not empirically tested

## Next Checks

1. Test the framework's performance on a new dataset with varying levels of class imbalance to evaluate robustness claims
2. Implement and evaluate Bayesian estimation techniques for confusion matrices to compare against the current MLE approach
3. Conduct experiments with correlated expert pools where decision-making dependencies are explicitly modeled to validate independence assumptions