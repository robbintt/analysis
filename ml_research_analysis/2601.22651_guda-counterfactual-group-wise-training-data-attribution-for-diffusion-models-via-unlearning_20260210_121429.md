---
ver: rpa2
title: 'GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models
  via Unlearning'
arxiv_id: '2601.22651'
source_url: https://arxiv.org/abs/2601.22651
tags:
- attribution
- style
- counterfactual
- training
- logo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identifying which training
  data groups contributed most to a generated output, which is critical for copyright
  assessment, fair compensation, and debugging. The core method, GUDA, uses machine
  unlearning to approximate counterfactual models that would be trained without specific
  groups, then measures group influence by comparing likelihood-based scoring rules
  (ELBO differences) between the full model and each unlearned counterfactual.
---

# GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning

## Quick Facts
- **arXiv ID:** 2601.22651
- **Source URL:** https://arxiv.org/abs/2601.22651
- **Authors:** Naoki Murata; Yuhta Takida; Chieh-Hsin Lai; Toshimitsu Uesaka; Bac Nguyen; Stefano Ermon; Yuki Mitsufuji
- **Reference count:** 40
- **Primary result:** GUDA identifies primary contributing groups more reliably than semantic similarity baselines, gradient-based attribution, and instance-level unlearning approaches, while achieving approximately 100x speedup on CIFAR-10 over LOGO retraining

## Executive Summary
This paper addresses the challenge of identifying which training data groups contributed most to a generated output, which is critical for copyright assessment, fair compensation, and debugging. The core method, GUDA, uses machine unlearning to approximate counterfactual models that would be trained without specific groups, then measures group influence by comparing likelihood-based scoring rules (ELBO differences) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity baselines, gradient-based attribution, and instance-level unlearning approaches, while achieving approximately 100x speedup on CIFAR-10 over LOGO retraining.

## Method Summary
GUDA (Group-wise Data Attribution) computes group-level training data attribution for diffusion models by creating counterfactual models through machine unlearning. For each group, the method fine-tunes the full model with a composite loss that combines a forget term (to remove group-specific knowledge) and a preserve term (to maintain performance on the retained dataset). The attribution score for each group is computed as the difference in ELBO (Evidence Lower Bound) between the full model and the unlearned counterfactual model. This approach approximates the gold-standard Leave-One-Group-Out (LOGO) retraining method while avoiding the computational cost of training N separate models from scratch.

## Key Results
- GUDA identifies primary contributing groups more reliably than semantic similarity baselines on CIFAR-10
- Achieves approximately 100x speedup over LOGO retraining on CIFAR-10
- Outperforms gradient-based attribution methods and instance-level unlearning approaches on artistic style attribution with Stable Diffusion

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Proxy via Unlearning
- **Claim:** Unlearning fine-tuning provides a computationally efficient proxy for the gold-standard Leave-One-Group-Out (LOGO) retrained model.
- **Mechanism:** Instead of initializing and training $N$ models from scratch, GUDA starts with the full model $\theta_{full}$ and applies a composite loss (forget + preserve) to shift weights toward the counterfactual state $\theta_{logo}^{-k}$.
- **Core assumption:** The optimization landscape allows the model to converge to a basin functionally equivalent to the LOGO model without suffering catastrophic forgetting on the retain set.
- **Evidence anchors:**
  - [abstract]: "approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch."
  - [section 4.2]: "GUDA approximates the counterfactual model $\theta_{logo}^{-k}$ by applying machine unlearning... rather than retraining from scratch."
  - [corpus]: Corpus mentions "Group-robust Machine Unlearning" generally, but specific links to this LOGO-proxy mechanism are weak.
- **Break condition:** If the unlearning rate is too high or preservation weight $\lambda$ is too low, the model collapses, failing to serve as a valid counterfactual proxy

## Foundational Learning

### 1. Machine Unlearning
**Why needed:** Traditional model retraining for counterfactual analysis is computationally prohibitive when evaluating many groups, requiring $N$ separate training runs from scratch.
**Quick check:** Verify that unlearning can approximate the model that would be trained without specific data by comparing ELBO differences on held-out validation sets.

### 2. Leave-One-Group-Out (LOGO) Framework
**Why needed:** Provides ground truth for evaluating group attribution methods by training separate models for each group combination.
**Quick check:** Ensure that the unlearned counterfactual model's performance on the full dataset matches the LOGO model's performance when that group is excluded.

### 3. ELBO Scoring for Attribution
**Why needed:** ELBO differences between full and counterfactual models provide a principled likelihood-based measure of group influence on generated samples.
**Quick check:** Confirm that ELBO differences correlate with expected attribution by testing on synthetic datasets with known group contributions.

## Architecture Onboarding

### Component Map
Diffusion Model $\theta_{full}$ -> Group-wise Unlearning Fine-tuning -> Counterfactual Models $\{\theta^{-k}\}$ -> ELBO Comparison -> Group Attribution Scores

### Critical Path
Full model training -> Unlearning optimization for each group -> ELBO computation for generated samples -> Attribution score aggregation

### Design Tradeoffs
- **Unlearning vs. Retraining:** Unlearning provides 100x speedup but may not perfectly approximate LOGO models
- **Preservation weight $\lambda$:** Balances forgetting group-specific knowledge while maintaining general capability
- **ELBO vs. other metrics:** ELBO provides likelihood-based attribution but may miss other influence factors

### Failure Signatures
- High variance in attribution scores across groups suggests unlearning didn't properly isolate group effects
- ELBO differences near zero for all groups indicates attribution method isn't capturing meaningful variation
- Performance collapse on preserve set indicates insufficient $\lambda$ or too aggressive unlearning rate

### First Experiments to Run
1. **Ablation on $\lambda$:** Systematically vary the preservation weight to find optimal balance between forgetting and retention
2. **Group overlap testing:** Evaluate attribution performance when groups share substantial semantic content
3. **Runtime scaling:** Measure unlearning time as group count increases from 10 to 100+ groups

## Open Questions the Paper Calls Out
None

## Limitations

- **Scalability to large group counts:** Unlearning process may become prohibitive for datasets with many groups or high-dimensional domains
- **Preservation mechanism fragility:** Balance may break down for groups with high semantic overlap or shared feature representations
- **Counterfactual validity without ground truth:** Evaluation relies on relative comparisons rather than absolute validation against ground truth group attribution

## Confidence

- **High confidence:** GUDA's computational efficiency advantage over LOGO retraining (supported by runtime measurements)
- **Medium confidence:** GUDA's superiority over semantic similarity and gradient-based baselines (relative comparisons only)
- **Medium confidence:** The unlearning mechanism successfully approximates counterfactual models (based on qualitative evaluation)
- **Low confidence:** Absolute accuracy of group attribution in real-world deployment scenarios (no ground truth validation)

## Next Checks

1. **Scaling analysis:** Measure GUDA runtime and attribution quality as group count increases from 10 to 100+ groups on CIFAR-10 and a text-to-image dataset, documenting the point where unlearning becomes impractical.

2. **Ground truth validation:** Create synthetic datasets where the true contribution of each group to generated samples is known (e.g., through controlled mixing of group-specific features), then benchmark GUDA against this ground truth.

3. **Preservation robustness testing:** Systematically vary $\lambda$ and the overlap between groups to identify conditions where the preservation mechanism fails, documenting the failure modes and their frequency across different dataset characteristics.