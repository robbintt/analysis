---
ver: rpa2
title: 'Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with
  Question-Conditioned Mixture-of-Experts'
arxiv_id: '2512.08814'
source_url: https://arxiv.org/abs/2512.08814
tags:
- personality
- user
- arxiv
- posts
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of personality detection from\
  \ social media posts, which is constrained by limited supervision signals and the\
  \ difficulty of mapping abstract psychological constructs to user language. Existing\
  \ approaches rely on post \u2192 user vector \u2192 labels paradigms, which are\
  \ limited by label scarcity and lack of intermediate supervision."
---

# Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts

## Quick Facts
- **arXiv ID**: 2512.08814
- **Source URL**: https://arxiv.org/abs/2512.08814
- **Reference count**: 13
- **Primary result**: ROME achieves up to 15.41% Macro-F1 improvement on MBTI personality detection by injecting questionnaire-grounded LLM role-play answers into a question-conditioned MoE framework.

## Executive Summary
The paper addresses personality detection from social media posts, constrained by limited supervision and the difficulty of mapping abstract psychological constructs to user language. Existing approaches rely on post → user vector → labels paradigms, which are limited by label scarcity and lack of intermediate supervision. The authors propose ROME, a novel framework that injects psychological knowledge into personality detection by leveraging large language models (LLMs) to simulate user responses to validated psychometric questionnaires. ROME generates question-level answers from free-form user posts, providing questionnaire-grounded evidence that links linguistic cues to personality labels. A question-conditioned Mixture-of-Experts (MoE) module jointly routes over post and question representations to answer questionnaire items, with the predicted answers summarized into an interpretable answer vector and fused with user representations for final personality prediction. Extensive experiments on two real-world datasets demonstrate that ROME significantly outperforms state-of-the-art baselines, achieving improvements of up to 15.41% in Macro-F1 on the Kaggle dataset, while enhancing interpretability through psychologically grounded predictions.

## Method Summary
ROME implements a three-stage pipeline for MBTI personality detection: (1) "Ask" stage uses GPT-4o to generate questionnaire answers from user posts and labels via role-play, (2) "Answer" stage encodes posts and questions with BERT, routes through a 32-expert MoE conditioned on the question and personality dimension, and predicts answers via L1 regression, and (3) "Detect" stage computes reliability (inverse variance) and importance (class separability) weights for each question, applies construct-specific masks, and fuses weighted evidence with post representations for binary classification. The framework uses two-stage training: pre-training the Answer module with regression loss only, then joint optimization with both answer prediction and classification losses. The method requires generating synthetic questionnaire answers as intermediate supervision, then using a question-conditioned MoE to route over post-question pairs, with gating and fusion to combine evidence with original post embeddings.

## Key Results
- ROME achieves 89.78% average Macro-F1 on Kaggle dataset, outperforming BERTmean (65.77%) and ETM (77.79%) baselines
- The framework demonstrates 15.41% improvement over state-of-the-art ETM baseline on Kaggle dataset
- ROME shows strong data efficiency, matching ETM performance with only 40% of training data
- Performance peaks at 32 experts (89.78%) and drops with fewer (8 experts: 79.89%) or more (40 experts: 88.21%)

## Why This Works (Mechanism)

### Mechanism 1: Questionnaire-Grounded Intermediate Supervision
Decomposing coarse personality labels into fine-grained questionnaire answers provides richer supervision signals than direct label prediction alone. The framework uses LLM role-play to simulate how each user would answer validated psychometric items, creating an intermediate supervision layer (60 questions → 4 binary dimensions) that densifies the training signal and provides an explicit semantic reasoning chain from text to personality. Core assumption: LLM role-play conditioned on user posts and personality labels can approximate how users would respond to psychometric questionnaires.

### Mechanism 2: Question-Conditioned Expert Specialization
Routing different questionnaire items to specialized experts prevents conflation of heterogeneous psycholinguistic cues. A question-conditioned router jointly encodes user post embeddings, question text embeddings, and a one-hot construct indicator, driving experts to specialize in distinct psychological constructs while shared experts capture cross-dimensional knowledge. Core assumption: Different psychological constructs require distinct linguistic pattern recognition; a single encoder would entangle these signals.

### Mechanism 3: Adaptive Evidence Weighting via Reliability and Importance Scores
Weighting questionnaire answers by their estimated reliability and discriminative importance improves evidence quality. Each question receives a composite weight based on inverse sampling variance across role-play generations and absolute mean difference between class-specific answer distributions. Core assumption: High-variance role-play answers are unreliable; questions with low class separability provide weak diagnostic signals.

## Foundational Learning

- **Concept: Mixture-of-Experts with sparse routing**
  - Why needed: Understanding how different experts specialize and how the router selects them is essential for debugging the question-conditioned architecture.
  - Quick check: Can you explain why the router receives [user embedding; question embedding; construct indicator] as input rather than just the user embedding?

- **Concept: Multi-task learning with auxiliary objectives**
  - Why needed: ROME trains answer prediction (regression) and personality classification (binary classification) jointly; understanding gradient flow between tasks is critical.
  - Quick check: What would happen if λ_q were set to 0, removing the answer prediction loss entirely?

- **Concept: Psychometric questionnaire design (MBTI structure)**
  - Why needed: Understanding that questions map to specific constructs (I/E, S/N, T/F, P/J) explains why construct-specific masks prevent cross-construct interference.
  - Quick check: Why does the model apply binary masks to ensure each construct only integrates evidence from its corresponding questions?

## Architecture Onboarding

- **Component map**: Posts → BERT encoder → user embedding v_u; Question + user + construct → router → expert aggregation → predicted answers â_u,i; weighted aggregation → s_u; gated fusion → z_u → classifier → personality prediction.

- **Critical path**: The framework generates synthetic questionnaire answers via LLM role-play, then uses a question-conditioned MoE to predict answers from post-question pairs, computes reliability and importance weights, applies construct-specific masks, and fuses weighted evidence with post embeddings for final personality classification.

- **Design tradeoffs**: GPT-4o yields best results (89.78%) but GPT-3.5 (86.20%) still beats baselines; cost vs. quality. 32 experts optimal; fewer (8: 79.89%) underfits; more (40: 88.21%) introduces routing complexity. 60-item questionnaire best but 12 items (84.16%) still outperforms ETM baseline.

- **Failure signatures**: High variance in role-play answers across samples suggests unreliable supervision; check q_rel scores. Experts with uniform attention across dimensions indicate failed specialization. Large performance gap between pretrain-only and full model suggests insufficient Answer module stabilization. P/J dimension consistently weaker than others; may require construct-specific tuning.

- **First 3 experiments**:
  1. **Baseline sanity check**: Run ROME with 8 experts and 12 questions; confirm it still beats BERTmean baseline (~66% on Kaggle). If not, check data pipeline and encoder loading.
  2. **Ablation: Evidence-only vs. Posts-only**: Compare ROME w/o posts (75.12%) vs. ROME w/o evidence (65.77%) on validation set; confirm complementary contributions as in Table 3.
  3. **Expert specialization visualization**: After training, plot the 32×4 expert attention matrix (Figure 8 pattern); verify dominant dimension per expert emerges. If uniform, check router gradient flow and learning rate.

## Open Questions the Paper Calls Out

- **Role-play accuracy validation**: To what extent do LLM-generated role-play responses accurately reflect the ground-truth answers a user would actually provide on a psychometric questionnaire? The framework depends on synthetic intermediate supervision, creating a risk of propagation errors if the role-play hallucinates answers inconsistent with the user's actual psychology.

- **Generalization to continuous traits**: Can the questionnaire-conditioned architecture effectively generalize to continuous trait-based models like the Big Five, or is it structurally limited to categorical typologies? The methodology and construct-specific binary masks are exclusively designed for the categorical MBTI taxonomy.

- **Sparse post evidence handling**: How can the "Ask" stage maintain reliability when user posts are too sparse or fragmented to provide sufficient evidence for role-play? The framework currently lacks a mechanism to detect or handle low-confidence role-play scenarios where the source text provides insufficient evidence for the 60-item questionnaire.

## Limitations

- The core claims of superiority and interpretability rest on assumptions that require validation, particularly the reliability of LLM-generated role-play responses as accurate personality proxies.
- Expert specialization results may reflect questionnaire structure artifacts rather than genuine psycholinguistic differentiation, and the reliability/importance weighting scheme assumes variance and class separation are meaningful quality metrics.
- The 15.41% Macro-F1 improvement may represent dataset-specific optimization rather than genuine capability advancement, and the framework's performance is sensitive to expert count and questionnaire length.

## Confidence

**High Confidence**: The MoE architecture functions as described and improves over strong baselines (BERTmean, ETM). The two-stage training procedure (pretrain then joint optimization) demonstrably outperforms single-stage training. The construct-specific masking correctly isolates evidence by dimension.

**Medium Confidence**: The questionnaire-grounded supervision provides meaningful intermediate training signals. The reliability/importance weighting improves performance over unweighted baselines. Expert specialization captures meaningful psychological distinctions rather than superficial patterns.

**Low Confidence**: The LLM role-play generates accurate personality-relevant responses from limited social media evidence. The reliability/importance metrics genuinely reflect evidence quality rather than questionnaire artifacts. The 15.41% Macro-F1 improvement represents genuine capability advancement rather than dataset-specific optimization.

## Next Checks

1. **Role-play validation study**: Collect actual user responses to the 60-item questionnaire for a subset of users (n=100-200) and compute correlation between LLM-generated and human answers. This directly tests the core assumption that role-play provides reliable intermediate supervision.

2. **Cross-dataset generalization test**: Train ROME on Kaggle and evaluate on Pandora without fine-tuning. Measure whether the performance gap persists or indicates overfitting to specific questionnaire formats.

3. **Expert intervention analysis**: For each expert, identify top-activating questions and compute the marginal contribution of that expert to final predictions. This tests whether specialization reflects genuine psycholinguistic expertise or coincidental activation patterns.