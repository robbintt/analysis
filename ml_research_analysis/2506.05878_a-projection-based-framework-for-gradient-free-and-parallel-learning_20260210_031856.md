---
ver: rpa2
title: A projection-based framework for gradient-free and parallel learning
arxiv_id: '2506.05878'
source_url: https://arxiv.org/abs/2506.05878
tags:
- projection
- learning
- graph
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a projection-based framework for neural\
  \ network training that reformulates the learning problem as a large-scale feasibility\
  \ problem instead of traditional loss minimization. The method uses projection operators\
  \ and iterative projection algorithms to find network parameters and states satisfying\
  \ local constraints derived from the network\u2019s elementary operations."
---

# A projection-based framework for gradient-free and parallel learning

## Quick Facts
- **arXiv ID:** 2506.05878
- **Source URL:** https://arxiv.org/abs/2506.05878
- **Reference count:** 40
- **Primary result:** Projection-based training reformulates neural network learning as a feasibility problem, enabling gradient-free and parallel training with competitive accuracy on standard benchmarks

## Executive Summary
This paper introduces a projection-based framework for neural network training that reformulates the learning problem as a large-scale feasibility problem rather than traditional loss minimization. The method uses projection operators and iterative projection algorithms to find network parameters and states satisfying local constraints derived from the network's elementary operations. A key contribution is PJAX, a JAX-based software framework that automatically composes projection operators for elementary operations, enabling parallel and GPU/TPU-accelerated training. The approach is evaluated on diverse architectures across standard benchmarks, showing the method is a viable alternative to gradient-based training with clear advantages in parallelism and handling non-differentiable operations.

## Method Summary
The method reformulates neural network training as a feasibility problem: finding network parameters and states that satisfy local constraints derived from elementary operations. A computation graph with edge variables represents the network, and projection operators are defined for each primitive function (dot product, ReLU, etc.). Iterative algorithms like Alternating Projections and Douglas-Rachford alternately project onto constraint sets. The framework leverages bipartite graph partitioning to enable fully parallel updates within each partition, eliminating sequential dependencies of backpropagation. The PJAX framework automatically composes projection operators for user-defined models, making the approach practical for diverse architectures including MLPs, CNNs, and RNNs.

## Key Results
- Projection methods achieve competitive accuracy with SGD/Adam on MNIST and HIGGS, with DR approaching SGD's accuracy on MLPs
- The method provides significant parallelization benefits, with step time scaling more favorably with network depth than gradient-based methods
- Skip connections are crucial for effective training in deep architectures using projection methods
- The approach handles non-differentiable operations naturally, making it suitable for quantized networks and other specialized architectures

## Why This Works (Mechanism)

### Mechanism 1: Feasibility Reformulation of Training
Reformulating training as a feasibility problem (finding points in constraint intersections) rather than loss minimization enables gradient-free learning with comparable effectiveness. Training decomposes into finding parameters and activation states satisfying local constraints from primitive operations, data inputs, and target labels. Iterative projection algorithms converge to feasible points in this intersection.

### Mechanism 2: Parallelization via Bipartite Graph Partitioning
Structuring the computation graph as bipartite enables fully parallel projection updates across all nodes in each partition, eliminating sequential dependencies of backpropagation. The computation graph is partitioned into two disjoint sets where no edges exist within each set, allowing independent parallel execution of projections.

### Mechanism 3: Local Projection Operators for Primitive Functions
Efficient closed-form or numerically tractable projection operators exist for common neural network primitives, making iterative feasibility-solving computationally practical. Each primitive function has an associated projection operator onto its graph, computed locally using averaging, function graph projection, and edge variable updates.

## Foundational Learning

- **Concept: Projection Operators**
  - **Why needed here:** Core mathematical tool; entire framework replaces gradients with projections onto constraint sets
  - **Quick check question:** Given set C = {x ∈ R² : x₁ + x₂ = 1}, compute PC((3, 0)). (Answer: (2, -1))

- **Concept: Alternating Projections and Douglas-Rachford Algorithms**
  - **Why needed here:** These iterative algorithms solve the feasibility problem; understanding their convergence properties is essential for debugging training
  - **Quick check question:** For two convex sets C₁, C₂ with non-empty intersection, does x{k+1} = PC₁(PC₂(x{k})) always converge? (Answer: Yes, to a point in C₁ ∩ C₂)

- **Concept: Computation Graphs with Edge Variables**
  - **Why needed here:** Framework represents training as constraint satisfaction over edge variables z{uv} (values on graph edges), not just parameters θ
  - **Quick check question:** For network f(x) = ReLU(Wx), what edge variables exist? (Answer: Edges from input x to dot product nodes, from W parameters to dot product nodes, from dot products to ReLU nodes, from ReLU to output)

## Architecture Onboarding

- **Component map:** User Model Definition (pjax.nn) -> Computation Graph G = (V, E) with edge variables z -> Bipartition: V = A ∪ B -> Projection Operators PCv for each node type -> Iterative Algorithm (AP/DR/CP) for K steps -> Extract consensus parameter values from edges

- **Critical path:**
  1. Define model using `pjax.nn.Module` API (mirrors Flax/JAX)
  2. PJAX traces computation, builds graph with primitive functions and edge variables
  3. Optimizer applies K projection steps per batch
  4. Parameter extraction via averaging edges from parameter nodes

- **Design tradeoffs:**
  | Dimension | Gradient-Based | Projection-Based |
  |-----------|----------------|------------------|
  | Memory | O(\|V\|) - store activations + gradients | O(\|E\|) - store all edge variables; higher for weight sharing |
  | Parallelism | Sequential backprop | Fully parallel within bipartition |
  | Differentiability | Required | Not required; handles quantization, non-smooth ops |
  | Convergence speed | Fast with Adam | Often more steps; DR typically outperforms AP |
  | Accuracy (current) | State-of-the-art | Competitive but gap exists, especially in deep nets |

- **Failure signatures:**
  - Oscillating loss without convergence: Projection algorithm trapped in non-convex region; try Douglas-Rachford instead of AP
  - Deep network underperforms shallow: Information not propagating through local projections; add skip connections
  - Memory OOM on CNNs/RNNs: Weight sharing creates many edge variable copies; reduce hidden units or batch size
  - Slow step times despite parallelism: Memory bandwidth bottleneck from edge variable access patterns

- **First 3 experiments:**
  1. Validate projection correctness on single sample: Define 2-layer MLP (128 hidden units), trace computation graph, manually verify that fixed point of projections produces correct forward pass
  2. Compare AP vs DR vs CP convergence on MNIST MLP: Train shallow MLP (1x128) with all three algorithms, plot validation accuracy vs steps and vs wall-clock time
  3. Characterize depth scaling with and without skip connections: Train 4-layer MLP on MNIST with DR, with and without skip connections

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive step sizes, preconditioning, or acceleration techniques be developed for projection-based methods to close the convergence speed and accuracy gap with adaptive gradient methods like Adam?
- **Basis in paper:** The Discussion section states opportunities exist to design more advanced projection algorithms focusing on improving approximation through adaptive step sizes, preconditioning, or acceleration techniques, noting that current accuracy lags behind Adam.
- **Why unresolved:** The current study utilizes standard projection algorithms (DR, AP, CP) without adaptive mechanisms, resulting in slower convergence and lower final accuracy compared to Adam on benchmarks like CIFAR-10.
- **What evidence would resolve it:** Empirical validation of an adaptive projection algorithm achieving parity with Adam in convergence speed and test accuracy on standard deep learning benchmarks.

### Open Question 2
- **Question:** How can the memory footprint of projection-based training be reduced to efficiently handle architectures with significant parameter sharing, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)?
- **Basis in paper:** The Complexity Analysis and Discussion sections highlight that storing the state vector $z \in \mathbb{R}^{|E|}$ leads to high memory usage ($O(|E|)$) compared to backpropagation ($O(|V|)$), particularly with weight sharing, necessitating strategies to reduce the memory footprint.
- **Why unresolved:** In the experiments, high memory demands forced reduction of hidden units in the 4-layer CNN to fit within GPU memory, indicating a significant scalability bottleneck.
- **What evidence would resolve it:** A modified formulation or implementation strategy allowing training of standard-sized CNNs (e.g., ResNet-50) on typical hardware without memory errors while retaining the projection-based update mechanism.

### Open Question 3
- **Question:** What tailored network architectures or specialized initialization methods are required to facilitate information propagation in deep networks trained via local projections without reliance on skip connections?
- **Basis in paper:** The Discussion notes that the "iterative nature of information propagation via local updates may necessitate tailored network architectures beyond simple skip connections, and specialized initialization methods could enhance robustness and performance."
- **Why unresolved:** Empirical results show that deep MLPs (4 layers) suffer significant performance degradation without skip connections, suggesting local projection updates struggle to propagate information through many layers effectively.
- **What evidence would resolve it:** Identification of a specific architectural motif or initialization scheme enabling a deep projection-trained network (>10 layers) to converge to high accuracy without using skip connections.

## Limitations
- Performance gap versus gradient-based optimizers on deep networks remains substantial without architectural modifications
- Weight sharing introduces significant memory overhead that scales poorly with network size
- Convergence guarantees rely on convexity assumptions that don't hold for non-convex neural network losses

## Confidence
- **High:** Projection framework correctly implements feasibility reformulation and parallel projection algorithms; method is mathematically sound and produces valid network outputs
- **Medium:** Performance claims for shallow networks (MLPs, RNNs) are well-supported; method provides genuine advantages in parallelization and handling non-differentiable operations
- **Low:** Claims about deep network performance without skip connections; scalability to large-scale models; comparison against modern adaptive optimizers beyond Adam

## Next Checks
1. Implement monitoring of projection residuals across iterations to quantify when feasibility problem becomes infeasible during training
2. Measure actual edge variable count and memory usage for CNNs with shared parameters across different hidden layer sizes
3. Test method on ResNet-18/50 variants on CIFAR-10 to characterize performance gap versus SGD/Adam on deeper architectures