---
ver: rpa2
title: 'DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval'
arxiv_id: '2502.07219'
source_url: https://arxiv.org/abs/2502.07219
tags:
- retrieval
- document
- identifier
- generative
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of existing generative retrieval
  methods, which only learn the relationship between queries and document identifiers,
  lacking direct interaction between queries and documents. To overcome this, the
  authors propose DOGR, a novel framework that leverages document-oriented contrastive
  learning to improve generative retrieval.
---

# DOGR: Leveraging Document-Oriented Contrastive Learning in Generative Retrieval

## Quick Facts
- **arXiv ID:** 2502.07219
- **Source URL:** https://arxiv.org/abs/2502.07219
- **Reference count:** 22
- **Primary result:** Outperforms state-of-the-art generative retrieval methods on NQ320k and MS MARCO datasets with significant gains in Recall@1, Recall@10, and MRR metrics.

## Executive Summary
This paper addresses a fundamental limitation in generative retrieval methods that only learn query-to-document-identifier relationships, lacking direct query-to-document relevance modeling. DOGR introduces a novel two-stage framework that first learns identifier generation and then applies document-oriented contrastive learning to capture semantic relevance. The method employs innovative negative sampling strategies and semantic fusion scoring, achieving substantial improvements over existing generative retrieval approaches on standard benchmarks.

## Method Summary
DOGR adopts a two-stage learning strategy to bridge the gap between query-document identifier generation and actual relevance modeling. In Stage 1, a T5-base encoder-decoder is trained with sequence-to-sequence loss to learn connections between queries and document identifiers. Stage 2 fine-tunes the model using contrastive learning objectives with two negative sampling methods: prefix-oriented (documents sharing identifier prefixes) and retrieval-augmented (sampling from retrieved documents with different identifiers). The framework computes semantic relevance scores through dot product of encoder hidden states and combines generation probability with semantic score for final ranking, outperforming state-of-the-art generative retrieval methods on standard benchmarks.

## Key Results
- Achieves significant improvements in Recall@1, Recall@10, and MRR metrics on NQ320k and MS MARCO datasets
- Outperforms state-of-the-art generative retrieval methods through document-oriented contrastive learning
- Demonstrates effectiveness across different identifier construction techniques

## Why This Works (Mechanism)
DOGR works by directly modeling query-document relevance through contrastive learning rather than relying solely on identifier generation. The two-stage approach first establishes the mapping between queries and identifiers, then refines the model to understand semantic relationships. The prefix-oriented negative sampling exploits identifier structure to create semantically challenging negatives, while retrieval-augmented sampling brings in diverse negative examples from the retrieval space. The semantic fusion of generation probability and semantic scores captures both the likelihood of correct identifier generation and the underlying semantic relevance, creating a more robust ranking mechanism.

## Foundational Learning
- **Keyword-based docid construction** - Extracts top keywords from documents to create lexical identifiers; needed to provide a finite identifier space for generative retrieval; quick check: verify keyword extraction quality and identifier uniqueness rate
- **Contrastive learning with negative sampling** - Uses semantically challenging negatives to improve representation learning; needed to capture query-document relevance beyond identifier matching; quick check: measure negative sampling diversity and impact on MRR
- **Semantic score computation** - Calculates relevance via dot product of encoder hidden states; needed to directly model query-document semantic similarity; quick check: validate semantic score correlation with relevance judgments
- **Constrained beam search with prefix tree** - Generates candidate identifiers while respecting identifier structure; needed to efficiently explore identifier space during inference; quick check: confirm beam search maintains identifier validity
- **Identifier conflict analysis** - Evaluates collision rate in identifier space; needed to quantify lexical identifier space efficiency; quick check: compute conflict rate vs identifier length

## Architecture Onboarding

**Component Map:**
T5 Encoder-Decoder -> Stage 1 (Identifier Generation) -> Stage 2 (Document Ranking) -> Inference with Constrained Beam Search

**Critical Path:**
Query → Encoder Hidden States → Semantic Score → Generation Probability → Fusion Relevance → Document Ranking

**Design Tradeoffs:**
- Two-stage training increases computational cost but enables specialized learning
- Keyword-based identifiers balance expressivity and generation complexity
- Semantic fusion combines generation confidence with learned relevance

**Failure Signatures:**
- High docid conflict rates leading to collapsed ranking resolution
- Generation capability degradation during Stage 2 fine-tuning
- Computational overhead from retrieval-augmented negative sampling

**3 First Experiments:**
1. Compute docid conflict rates across different identifier lengths to quantify lexical identifier space efficiency
2. Monitor query-to-identifier accuracy during Stage 2 to verify auxiliary generation loss necessity
3. Compare against standard in-batch negative sampling to isolate contribution of proposed negative sampling strategies

## Open Questions the Paper Calls Out
The paper identifies three key open questions for future research: 1) How to adapt the two-stage DOGR framework for end-to-end training to seamlessly integrate document information during retrieval, 2) Whether the retrieval-augmented negative sampling strategy introduces prohibitive computational overhead during training, and 3) How DOGR performs on corpora significantly larger than MS MARCO regarding memory usage and inference speed.

## Limitations
- High computational cost due to two-stage training (3M→1M steps) and complex negative sampling
- Significant identifier collision risk in large document spaces, particularly for MS MARCO
- Limited statistical validation with no significance testing reported for performance claims

## Confidence
- **Methodological soundness:** Medium - novel approach but computationally intensive with hyperparameter sensitivity
- **Reproducibility:** Medium - clear training procedure but unknown details on segment sampling and beam search implementation
- **Performance claims:** Medium - substantial gains reported but marginal improvements in MRR@100 suggest potential rank degradation
- **Scalability:** Low - limited evaluation on large-scale corpora, computational overhead concerns unaddressed

## Next Checks
1. Compute and report docid conflict rates across identifier lengths to quantify the lexical identifier space efficiency
2. Run ablations removing the auxiliary generation loss L_g in Stage 2 to verify its necessity for maintaining generation quality
3. Compare against a standard in-batch negative sampling baseline to isolate the contribution of the proposed negative sampling strategies