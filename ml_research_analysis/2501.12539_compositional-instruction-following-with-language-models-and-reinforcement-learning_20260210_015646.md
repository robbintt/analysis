---
ver: rpa2
title: Compositional Instruction Following with Language Models and Reinforcement
  Learning
arxiv_id: '2501.12539'
source_url: https://arxiv.org/abs/2501.12539
tags:
- tasks
- language
- learning
- agent
- symbol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning to follow compositional
  language instructions in reinforcement learning settings. The key idea is to combine
  compositional value function representations with large language models to enable
  agents to solve complex, language-specified tasks.
---

# Compositional Instruction Following with Language Models and Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.12539
- Source URL: https://arxiv.org/abs/2501.12539
- Authors: Vanya Cohen; Geraud Nangue Tasse; Nakul Gopalan; Steven James; Matthew Gombolay; Ray Mooney; Benjamin Rosman
- Reference count: 8
- Key outcome: Achieved 92% success rate on 162 compositional BabyAI tasks after 600k steps, outperforming non-compositional baselines by 12 percentage points

## Executive Summary
This paper introduces CERLLA, a method that combines compositional reinforcement learning with large language models to follow complex natural language instructions in a grid-world environment. The key innovation is pretraining a set of World Value Functions (WVFs) for primitive object attributes, then composing them using Boolean operators to represent complex tasks. A language model maps instructions to these compositional expressions, using environment feedback to improve its semantic parsing through in-context learning. The method achieves significant sample efficiency gains and demonstrates strong compositional generalization to novel tasks.

## Method Summary
CERLLA operates through two phases: pretraining and learning. During pretraining, the agent learns 9 basis WVFs representing primitive attributes (colors and object types) using CNN-DQN. These WVFs can be composed using Boolean operations (AND, OR, NOT) to represent complex goals. For learning, a large language model (GPT-4) maps natural language instructions to Boolean expressions over the WVF symbols. The system executes rollouts for candidate expressions and adds successful (instruction, expression) pairs to an in-context example set, which the LLM uses to refine future parses. This creates a feedback loop where the LLM learns to map language to compositional value functions that the RL agent can execute.

## Key Results
- Achieved 92% success rate across 162 compositional tasks after 600k environment steps
- Outperformed non-compositional baseline by 12 percentage points (92% vs 80%)
- Demonstrated strong compositional generalization to held-out tasks
- Reduced sample complexity by over 30x compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Boolean composition of pretrained value functions reduces sample complexity by allowing the agent to solve novel tasks through composition rather than learning from scratch. The agent pretrains 9 basis WVFs for primitive attributes, then combines them using Boolean algebra to represent complex goals.

### Mechanism 2
Environment rollouts provide reinforcement learning signal to ground natural language semantics via in-context learning. The LLM maps language to candidate Boolean expressions, executes them, and adds successful pairs to its context buffer for future refinement.

### Mechanism 3
Separating policy composition from semantic parsing enables generalization. By keeping the meaning (Boolean logic) separate from execution (WVF composition), the system avoids overfitting to specific language patterns while maintaining zero-shot execution capability.

## Foundational Learning

- **World Value Functions (WVFs)**: These map (state, goal, action) to value, allowing evaluation of reaching any goal state. Understanding WVFs is critical to grasp how agents can compose goals without retraining. Quick check: How does the reward function differ from standard RL when goals aren't state-based?

- **In-Context Learning (ICL)**: The LLM learns solely by expanding its prompt with successful examples, not through gradient descent. Understanding the difference between training weights and building context is critical. Quick check: Why use temperature 1.0 during training but 0.0 during evaluation?

- **Boolean Algebra for Task Specification**: The method relies on Boolean logic properties to guarantee that combining value functions results in valid value functions. Quick check: How is negation of a value function mathematically defined using Q_MAX and Q_MIN?

## Architecture Onboarding

- **Component map**: Environment (BabyAI) -> WVF Module (Pretrained DQN) -> LLM Parser (GPT-4) -> Evaluator -> Environment feedback
- **Critical path**: WVF pretraining (19M steps) is the primary bottleneck. In deployment, the critical path is LLM inference followed by 100 rollouts per candidate.
- **Design tradeoffs**: Uses abstract symbols rather than semantic labels for environment-agnostic parsing; performance is brittle to LLM choice (GPT-4 succeeded, GPT-3.5 failed).
- **Failure signatures**: Low success rate with high variance (caused by noisy semantic parses); syntax errors from invalid Boolean expressions.
- **First 3 experiments**: 1) Test individual WVF composition in isolation, 2) Measure LLM parsing ability without environment feedback using fixed examples, 3) Run full CERLLA loop on 10 tasks to validate the 92% success threshold.

## Open Questions the Paper Calls Out

### Open Question 1
Can the basis set of World Value Functions (WVFs) and the language-instruction semantic parser be learned simultaneously, rather than requiring separate pretraining? The authors identify this as a limitation and plan experiments to address it.

### Open Question 2
Can CERLLA maintain sample efficiency and success rates when transferred to continuous action spaces or high-dimensional real-world robotic tasks? The current discrete action space may limit applicability to real-world RL tasks.

### Open Question 3
Is the method's success fundamentally dependent on GPT-4's reasoning capabilities, or can smaller models achieve similar performance with the proposed in-context learning feedback? The dramatic performance gap between GPT-4 and GPT-3.5 raises this question.

## Limitations
- Performance depends critically on the assumption that all tasks can be decomposed into a fixed set of 9 primitive attributes, which is never empirically validated
- The 92% success threshold for adding examples to the in-context set is arbitrary and not justified through sensitivity analysis
- Computational cost of running 100 rollouts per candidate expression is substantial, with no cost-benefit analysis provided

## Confidence
- **High Confidence**: The compositional value function representation works as described; the mathematical framework is sound
- **Medium Confidence**: LLM's ability to learn semantic parsing through in-context examples is demonstrated but may be sensitive to specific LLM used
- **Low Confidence**: Scalability to more complex environments with richer attribute spaces is untested; robustness to different LLM architectures is not thoroughly examined

## Next Checks
1. **Basis Set Completeness Test**: Systematically remove individual WVFs and measure performance degradation to identify essential vs. redundant attributes
2. **Threshold Sensitivity Analysis**: Vary the 92% success threshold (85%, 95%, 99%) to determine optimal setting and its impact on learning
3. **Environment Complexity Scaling**: Test CERLLA on BabyAI environments with additional attributes to evaluate scalability of the 9-basis assumption