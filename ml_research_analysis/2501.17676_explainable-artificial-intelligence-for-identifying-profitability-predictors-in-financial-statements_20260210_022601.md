---
ver: rpa2
title: Explainable Artificial Intelligence for identifying profitability predictors
  in Financial Statements
arxiv_id: '2501.17676'
source_url: https://arxiv.org/abs/2501.17676
tags:
- features
- financial
- data
- classification
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates profitability prediction in Italian listed
  companies using raw financial statements data and Machine Learning (ML) models.
  The research applies tree-based ensemble methods (XgBoost, Random Forest) and compares
  their performance against SVM and Logistic Regression on both raw financial data
  and financial ratios.
---

# Explainable Artificial Intelligence for identifying profitability predictors in Financial Statements

## Quick Facts
- arXiv ID: 2501.17676
- Source URL: https://arxiv.org/abs/2501.17676
- Authors: Marco Piazza; Mauro Passacantando; Francesca Magli; Federica Doni; Andrea Amaduzzi; Enza Messina
- Reference count: 24
- Key outcome: Tree-based ML models (XgBoost, Random Forest) outperform linear models and SVM on raw financial data, with SHAP analysis identifying Financial Profile features as most important predictors

## Executive Summary
This study applies Machine Learning and eXplainable AI to predict company profitability using raw financial statements from Italian listed companies. The research compares tree-based ensemble methods against traditional ML approaches on both raw data and financial ratios, achieving higher accuracy with raw features. SHAP (Shapley Additive Explanations) is used to identify the most influential predictors and validate their importance, demonstrating that Financial Profile features consistently rank highest in importance across models.

## Method Summary
The methodology extracts 321 raw financial features from Italian companies' financial statements (Balance Sheet, Income Statement, Financial Profile, and Ratios sections), then trains tree-based ensemble models (XgBoost, Random Forest) and compares them against SVM and Logistic Regression. Models predict binary Return on Investment direction using data from 2013-2020 as training set and 2021 as test set. SHAP values are computed to explain individual predictions and identify important features, with PartitionSHAP for group-level insights and KernelSHAP for feature-level rankings. The approach validates feature importance through ablation studies removing low-importance features.

## Key Results
- Tree-based ensembles (XgBoost, Random Forest) outperform linear models and SVM on profitability prediction
- Raw financial data yields higher accuracy (0.64-0.67) than financial ratios (0.54-0.55)
- SHAP analysis reveals Financial Profile features are consistently the most important predictors
- Removing bottom 100 features improves accuracy from 0.64 to 0.68, validating SHAP-based feature selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Raw high-dimensional financial data captures complex relationships that simplified ratios miss
- Mechanism: Tree-based ensembles can learn non-linear interactions across hundreds of raw features (321 in this study), preserving information that ratio aggregation eliminates. The model discovers which combinations matter rather than relying on pre-defined financial ratios.
- Core assumption: The information loss from ratio construction outweighs the noise reduction benefit when using non-linear classifiers.
- Evidence anchors:
  - [abstract] "Raw data yielded higher accuracy (0.64-0.67) than financial ratios (0.54-0.55), confirming that high-dimensional features better capture complex relationships"
  - [section 3.1] "using high-dimensional raw data is beneficial in representing complex relationships and enhancing final classification accuracy"
  - [corpus] Weak direct support; related paper "Explaining the Unexplainable" reviews XAI in finance but does not validate raw vs. ratio comparisons
- Break condition: If raw data contains excessive noise or missing values that exceed model capacity to regularize, performance gap may reverse; not tested in this paper.

### Mechanism 2
- Claim: Tree-based ensembles outperform linear models for profitability classification from financial statements
- Mechanism: Gradient boosting (XGBoost) and Random Forest partition the high-dimensional feature space into regions with homogeneous profitability outcomes, capturing threshold effects and interactions without explicit specification.
- Core assumption: Profitability signals in financial statements exhibit non-linear patterns that linear decision boundaries cannot efficiently separate.
- Evidence anchors:
  - [abstract] "tree-based ensembles (XgBoost, Random Forest) outperforming linear models and SVM"
  - [section 2] "tree-based ensembles, particularly XgBoost outperform neural networks in classifying tabular data" citing Shwartz-Ziv & Armon (2022)
  - [corpus] "Enhancing ML Models Interpretability for Credit Scoring" corroborates ML outperforming traditional regression in financial prediction tasks
- Break condition: If dataset size decreases substantially or linear relationships dominate the signal, ensemble advantage may diminish; paper does not test this boundary.

### Mechanism 3
- Claim: SHAP-based feature removal can improve classification accuracy by eliminating noisy dimensions
- Mechanism: Shapley values estimate each feature's marginal contribution to model predictions; removing low-contributing features reduces dimensionality and potentially overfitting, while retaining predictive power.
- Core assumption: Features with consistently low SHAP values are noise rather than weak but systematic signals.
- Evidence anchors:
  - [abstract] "Model performance remained stable when using only top-ranked features (accuracy 0.61) and improved when removing least important ones (accuracy 0.68)"
  - [section 3.3] Table 2 shows accuracy increase from 0.64 to 0.68 after removing bottom 100 features
  - [corpus] Weak direct validation; "Explainable AI for Predicting and Understanding Mathematics Achievement" uses SHAP but does not test feature removal effects
- Break condition: If low-SHAP features contain rare but critical signal (e.g., tail-risk indicators), removal could harm generalization to out-of-distribution cases.

## Foundational Learning

- Concept: Shapley Values and Cooperative Game Theory
  - Why needed here: SHAP explanations are grounded in Shapley value theory; understanding marginal contribution to coalitions helps interpret why certain features rank higher
  - Quick check question: If feature A has Shapley value 0.15 and feature B has 0.05, what does this mean for their average contribution across all feature coalitions?

- Concept: Tree Ensemble Decision Boundaries
  - Why needed here: Understanding how XGBoost/Random Forest create piecewise-constant decision surfaces explains why they capture non-linear financial relationships
  - Quick check question: Why might a tree ensemble capture the interaction "high debt AND low cash flow → profitability decline" more naturally than logistic regression?

- Concept: Financial Statement Structure (Balance Sheet, Income Statement, Ratios)
  - Why needed here: Paper organizes features into four groups; knowing what each contains helps interpret SHAP findings about which sections matter
  - Quick check question: Which financial statement section would you expect to contain revenue and expense items?

## Architecture Onboarding

- Component map:
  AIDA database → 321 raw features across 4 groups (Financial Profile, Balance Sheet, Income Statement, Ratios) → XGBoost/Random Forest classifier → SHAP explanations (PartitionSHAP + KernelSHAP) → Feature validation through ablation studies

- Critical path:
  1. Extract raw financial features from source (exclude 20 non-numeric fields)
  2. Train tree ensemble on 2013-2020 data (train), predict 2021 (test)
  3. Compute SHAP values for test predictions
  4. Rank features by Top-50 frequency across test samples
  5. Retrain on reduced feature sets to validate importance rankings

- Design tradeoffs:
  - Raw data vs. ratios: Raw preserves information but increases dimensionality (321 vs. ~20 ratios)
  - XGBoost vs. Random Forest: Paper shows RF slightly higher accuracy (0.67 vs 0.64) on raw; XGBoost may generalize better on larger datasets
  - PartitionSHAP vs. KernelSHAP: PartitionSHAP faster for group-level insights; KernelSHAP required for fine-grained feature ranking

- Failure signatures:
  - Accuracy drops below 0.55: Likely using only financial ratios or linear models; switch to raw data + tree ensemble
  - SHAP values uniformly distributed: Model may not have learned meaningful patterns; check data quality and target definition
  - Feature removal hurts accuracy: Low-SHAP features may contain systematic signal; validate on held-out time periods

- First 3 experiments:
  1. Replicate baseline: Train XGBoost on raw data, report accuracy and ROC-AUC; verify 0.64±0.02 range
  2. SHAP feature audit: Compute KernelSHAP on 50 test samples; confirm Financial Profile features appear in Top-10 at higher frequency than Balance Sheet
  3. Feature ablation: Remove bottom 100 SHAP-ranked features; retrain and confirm accuracy improvement toward 0.68

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Relatively small test sample (2021 data only) with potential temporal dependencies not addressed through time-series cross-validation
- Binary ROI classification threshold and dataset composition could significantly impact results if altered
- While SHAP provides local interpretability, the study does not validate whether identified important features align with established financial theory

## Confidence
- **High confidence** in the core finding that raw financial data outperforms ratio-based features (accuracy 0.64-0.67 vs 0.54-0.55)
- **Medium confidence** in the generalizability of the specific feature rankings due to the Italian market focus and single-decade timeframe
- **Medium confidence** in the improvement from feature removal (0.64 to 0.68 accuracy) which requires external validation on different datasets

## Next Checks
1. Test the methodology on non-Italian market data to assess geographic generalizability of the feature importance rankings
2. Implement time-series cross-validation to ensure results are not artifacts of specific train-test temporal split
3. Conduct ablation studies varying the ROI classification threshold to determine sensitivity of the model performance to this critical parameter