---
ver: rpa2
title: Knowledge Graph Completion by Intermediate Variables Regularization
arxiv_id: '2506.02749'
source_url: https://arxiv.org/abs/2506.02749
tags:
- regularization
- tensor
- rank
- have
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses knowledge graph completion using tensor decomposition-based
  models, which suffer from overfitting despite their expressiveness. The authors
  propose Intermediate Variables Regularization (IVR), a novel regularization technique
  that minimizes the norms of intermediate variables involved in computing the predicted
  tensor, rather than just embedding norms.
---

# Knowledge Graph Completion by Intermediate Variables Regularization

## Quick Facts
- arXiv ID: 2506.02749
- Source URL: https://arxiv.org/abs/2506.02749
- Authors: Changyi Xiao; Yixin Cao
- Reference count: 40
- Primary result: IVR improves MRR from 0.446 to 0.501 on WN18RR for TuckER

## Executive Summary
This paper addresses knowledge graph completion using tensor decomposition-based models, which suffer from overfitting despite their expressiveness. The authors propose Intermediate Variables Regularization (IVR), a novel regularization technique that minimizes the norms of intermediate variables involved in computing the predicted tensor, rather than just embedding norms. IVR is theoretically grounded, with proofs showing it serves as an upper bound for the overlapped trace norm, promoting low-rank structure and reducing overfitting. Experiments on WN18RR, FB15k-237, and YAGO3-10 datasets demonstrate IVR's effectiveness across multiple models (CP, ComplEx, SimplE, ANALOGY, QuatE, TuckER), achieving state-of-the-art results. For example, TuckER with IVR improves MRR from 0.446 to 0.501 on WN18RR. Ablation studies confirm the upper bound's reliability, and theoretical analysis supports its efficacy in regularizing embeddings. The method is computationally tractable and broadly applicable to most tensor decomposition-based models.

## Method Summary
The paper introduces Intermediate Variables Regularization (IVR) as a novel regularization technique for tensor decomposition-based knowledge graph completion models. Unlike traditional regularization that focuses on embedding norms, IVR minimizes the norms of intermediate variables used in computing the predicted tensor. This approach is theoretically grounded, with proofs demonstrating that IVR serves as an upper bound for the overlapped trace norm, which promotes low-rank structure and reduces overfitting. The method is designed to be broadly applicable across multiple tensor decomposition models, including CP, ComplEx, SimplE, ANALOGY, QuatE, and TuckER. Experimental results validate IVR's effectiveness, showing consistent improvements in MRR and Hits@K across multiple datasets.

## Key Results
- TuckER with IVR improves MRR from 0.446 to 0.501 on WN18RR
- Consistent improvements across CP, ComplEx, SimplE, ANALOGY, QuatE, and TuckER models
- State-of-the-art performance achieved on WN18RR, FB15k-237, and YAGO3-10 datasets
- Ablation studies confirm the reliability of IVR's upper bound formulation

## Why This Works (Mechanism)
IVR works by regularizing the intermediate variables in tensor decomposition models rather than just the embeddings. This approach provides a tighter upper bound for the overlapped trace norm, which promotes low-rank structure and effectively reduces overfitting. By focusing on the intermediate computation steps, IVR captures more information about the model's behavior and constraints, leading to better generalization. The theoretical analysis demonstrates that minimizing the norms of these intermediate variables directly corresponds to promoting the desired low-rank structure in the tensor decomposition, which is crucial for effective knowledge graph completion.

## Foundational Learning

1. **Tensor Decomposition in Knowledge Graphs**
   - Why needed: Core mathematical framework for modeling multi-relational data
   - Quick check: Can decompose knowledge graph triples into factor matrices

2. **Overlapped Trace Norm**
   - Why needed: Regularization technique promoting low-rank structure in tensors
   - Quick check: Provides theoretical foundation for IVR's effectiveness

3. **Knowledge Graph Completion**
   - Why needed: Task of predicting missing links in knowledge graphs
   - Quick check: Uses scoring functions to rank candidate triples

4. **Regularization in Neural Networks**
   - Why needed: Prevents overfitting by constraining model complexity
   - Quick check: Traditional methods focus on embedding norms, IVR innovates on intermediate variables

## Architecture Onboarding

Component Map: Input embeddings → Intermediate variables computation → Scoring function → IVR regularization → Loss function → Parameter updates

Critical Path: The key innovation is adding IVR regularization to the loss function during training, which operates on intermediate variables rather than directly on embeddings.

Design Tradeoffs: IVR trades computational overhead for better generalization. The method requires computing and regularizing intermediate variables, but this provides tighter control over model complexity compared to traditional embedding regularization.

Failure Signatures: Models may fail to converge if IVR regularization strength is too high, or show minimal improvement if too low. The intermediate variable computation must be correctly implemented for each specific tensor decomposition model.

3 First Experiments:
1. Compare MRR/Hits@K on WN18RR with TuckER baseline vs TuckER+IVR
2. Test IVR across different tensor decomposition models (CP, ComplEx, SimplE)
3. Ablation study varying IVR regularization strength

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical analysis establishes IVR as an upper bound but practical significance across different architectures needs further validation
- Performance on larger, more diverse knowledge graphs and low-resource scenarios not thoroughly investigated
- Computational overhead analysis lacks detailed comparison with other regularization techniques
- Sensitivity to hyperparameters and interaction with other regularization methods not explored

## Confidence

High:
- Experimental results show consistent improvements in MRR and Hits@K across multiple models and datasets
- State-of-the-art performance achieved on benchmark datasets

Medium:
- Theoretical grounding as upper bound for overlapped trace norm is well-articulated but practical implications need validation
- Broad applicability demonstrated experimentally but generalizability to non-tensor models untested

## Next Checks

1. Scalability Analysis: Conduct experiments on larger, more diverse knowledge graphs to assess IVR's scalability and performance in low-resource settings.

2. Hyperparameter Sensitivity: Investigate the sensitivity of IVR to hyperparameters and its interaction with other regularization techniques to optimize its effectiveness.

3. Theoretical-Practical Gap: Perform a detailed analysis of the practical significance of IVR's theoretical upper bound in preventing overfitting across different model architectures and datasets.