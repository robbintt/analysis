---
ver: rpa2
title: Sensorimotor features of self-awareness in multimodal large language models
arxiv_id: '2505.19237'
source_url: https://arxiv.org/abs/2505.19237
tags:
- memory
- awareness
- robot
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether multimodal large language models
  (LLMs) can develop self-awareness through sensorimotor experiences in a robotic
  embodiment. An omnidirectional mobile robot equipped with RGB-D cameras, LiDAR,
  encoders, and an IMU is integrated with a multimodal LLM (Gemini 2.0 Flash).
---

# Sensorimotor features of self-awareness in multimodal large language models

## Quick Facts
- arXiv ID: 2505.19237
- Source URL: https://arxiv.org/abs/2505.19237
- Reference count: 40
- A multimodal LLM embodied in a robot can develop self-awareness through iterative sensorimotor experiences

## Executive Summary
This study investigates whether multimodal large language models can develop self-awareness through sensorimotor experiences in a robotic embodiment. An omnidirectional mobile robot equipped with RGB-D cameras, LiDAR, encoders, and an IMU is integrated with Gemini 2.0 Flash to generate iterative self-predictions across four dimensions: entity identity, physical dimensions, movement modality, and environmental context. The system demonstrates that iterative memory-prediction loops, particularly when grounded in visual input, enable progressive refinement of self-awareness.

## Method Summary
A Mecabot Pro omnidirectional mobile robot with RGB-D cameras, LiDAR, encoders, and IMU streams sensor data at 1 Hz to Gemini 2.0 Flash. The MM-LLM integrates current sensor data with episodic memory of prior predictions to generate JSON-formatted self-assessments iteratively. Evaluation uses an LLM-as-Judge framework scoring predictions 0-5 across four dimensions. Ablation tests remove specific sensors or memory to quantify their contributions, while structural equation modeling analyzes causal relationships between sensory inputs, memory, and self-awareness constructs.

## Key Results
- Entity recognition scores averaged 3.27/5, demonstrating coherent self-identification
- Visual input ablation caused fundamental category errors (wheeled robot → flying drone)
- SEM analysis revealed self-identification causally precedes movement awareness
- Memory ablation caused complete incoherence with oscillating scores between 0 and 5

## Why This Works (Mechanism)

### Mechanism 1
Iterative memory-prediction loops enable progressive refinement of self-awareness by grounding each prediction in prior estimates. At each iteration i+1, the MM-LLM receives current sensor data and the prediction summary from iteration i, generating an updated JSON-formatted self-assessment that becomes context for iteration i+2. This creates structured knowledge accumulation rather than isolated snapshots. Memory ablation causes complete incoherence—scores alternate between 0 and 5 across iterations, demonstrating that without memory "there is no continuity of action; instead, the system perceives only disconnected snapshots in time."

### Mechanism 2
Visual input is the dominant channel for environmental awareness; its absence causes cascading errors in self-identification and movement inference. RGB-D camera imagery provides grounding for scene understanding and body-schema verification (e.g., ground contact). Without it, the system defaults to semantically plausible but physically incorrect inferences. Image ablation drops self-identification to 1.66/5; the system misclassifies itself as an "Autonomous inspection drone" because it lacks ground-contact cues, demonstrating that environmental misconceptions propagate to identity and movement inference.

### Mechanism 3
Self-identification causally precedes movement awareness—identity constrains motion interpretation, not the reverse. SEM path analysis reveals that Self-Identification strongly influences Movement Awareness, while Movement Awareness has negligible effect on Self-Identification. The system must first establish "what am I" before correctly inferring "how do I move." When identity fails (image ablation), movement inference fails accordingly (rolling → flying), but movement errors do not cascade to identity errors in the same way.

## Foundational Learning

- **Episodic Memory Integration**
  - Why needed here: The system depends on maintaining a running summary of prior predictions to achieve temporal coherence. Without this, each iteration is an isolated inference with no accumulated context.
  - Quick check question: Can you explain why memory ablation causes oscillation between 0 and 5 scores rather than uniformly low scores?

- **Structural Equation Modeling (SEM) for Latent Variable Analysis**
  - Why needed here: SEM is used to infer hierarchical causal relationships between sensor inputs, latent constructs (Past-Present Memory, Dimension Awareness), and observable outputs (rubric scores).
  - Quick check question: What does a CFI of 0.97 and TLI of 0.95 indicate about model fit, and why does this matter for interpreting the causal claims?

- **LLM-as-Judge Evaluation with Rubrics**
  - Why needed here: The study uses a separate LLM (Gemini 2.0 Flash) to score predictions on a 0-5 scale using detailed rubrics for each dimension. Understanding this evaluation method is critical for interpreting the reported scores.
  - Quick check question: What are the potential failure modes of using an LLM to evaluate another LLM's outputs, and how might rubric design mitigate or exacerbate these?

## Architecture Onboarding

- Component map: ROS2 topics -> JSON formatting (1 Hz sampling) -> Gemini 2.0 Flash API -> JSON output (Dimensions, Movement, Entity, Environment) -> Episodic memory storage -> Next iteration
- Critical path: 1) Sensor data collection via ROS2 2) JSON formatting with current readings + prior prediction summary 3) MM-LLM inference -> JSON output 4) Store output as memory for next iteration 5) Parallel evaluation via LLM-as-Judge rubric scoring
- Design tradeoffs: LiDAR data reduced to 8 sectors (nearest obstacle per direction) to avoid LLM saturation—trades spatial resolution for token efficiency; 1 Hz sampling rate trades temporal granularity for computational feasibility; single decimal precision trades measurement accuracy for reduced context length
- Failure signatures: Memory ablation: Oscillating scores (0↔5), no temporal coherence; Image ablation: Identity collapse to incorrect category (robot→drone), movement inference follows; Individual sensor ablation (odometry/IMU/LiDAR): Minor score variations due to modality redundancy
- First 3 experiments: 1) Baseline replication: Run full system for ~3.5 minutes with all sensors enabled; verify scores stabilize near reported values 2) Memory ablation: Disable episodic memory; confirm score oscillation and document specific failure patterns in movement prediction 3) Image ablation with logging: Remove visual input; capture specific misclassification examples and verify drone/flight inference pattern replicates

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed self-awareness generalize across different multimodal LLM architectures, or is it specific to Gemini 2.0 Flash? The study uses only "Gemini 2.0 Flash" without comparison to other models, leaving model-dependence unexplored. No cross-model comparison was conducted; findings may reflect architecture-specific capabilities rather than general properties of MM-LLMs.

### Open Question 2
Can embodied MM-LLMs achieve precise physical self-knowledge (e.g., accurate dimensional estimates) beyond categorical self-identification? Dimension estimates showed "55.6%, 50.7% and 41.4%" relative errors despite coherent entity recognition (3.27/5), suggesting a gap between qualitative and quantitative self-awareness. The system identifies itself as "a mobile robot" but cannot pinpoint exact dimensions or model.

### Open Question 3
Is the hierarchical relationship (self-identification → movement awareness) a fundamental property of artificial self-awareness or an artifact of the current system design? SEM revealed that "recognizing oneself as a specific type of agent... appears to be a prerequisite for correctly interpreting movement," with movement awareness showing negligible reverse influence. This counterintuitive finding was observed in one architecture; whether it represents a universal cognitive constraint or design-specific behavior requires broader validation.

## Limitations
- The study relies heavily on LLM-as-Judge evaluation, which introduces potential bias through rubric interpretation and self-referential evaluation
- The 3.5-minute evaluation period may not capture long-term stability or learning effects
- Ablation tests demonstrate strong dependencies but cannot fully establish causal mechanisms beyond statistical correlations from SEM

## Confidence
- High confidence: Sensorimotor integration is essential for coherent self-awareness (confirmed by ablation tests showing complete system failure when removing key modalities)
- Medium confidence: Visual input is non-substitutable for environmental awareness (supported by strong quantitative drops but lacks mechanistic explanation)
- Medium confidence: Self-identification causally precedes movement awareness (SEM results show directional relationships but cannot definitively prove causation)
- Low confidence: Iterative memory loops guarantee progressive refinement (demonstrated empirically but theoretical guarantees are absent)

## Next Checks
1. Test long-term stability by extending evaluation beyond 3.5 minutes to assess whether scores converge or diverge over time
2. Conduct cross-evaluation using a different LLM-as-Judge instance to validate rubric consistency and detect evaluator bias
3. Implement controlled sensorimotor perturbation experiments (e.g., wheel slippage detection) to test whether the system can detect and adapt to changes in its own physical capabilities