---
ver: rpa2
title: 'Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT'
arxiv_id: '2509.12471'
source_url: https://arxiv.org/abs/2509.12471
tags:
- powergpt
- statistical
- power
- test
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PowerGPT, an AI-driven system, automates statistical test selection
  and sample size calculation for clinical trial design, addressing the complexity
  and expertise barriers in traditional methods. Integrating large language models
  with statistical engines, it enables one-click power analysis through natural language
  interaction.
---

# Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT

## Quick Facts
- arXiv ID: 2509.12471
- Source URL: https://arxiv.org/abs/2509.12471
- Reference count: 0
- Primary result: PowerGPT achieved 99.3% completion rates and 94.1% accuracy in sample size calculation versus 77.8% and 55.4% for traditional methods

## Executive Summary
PowerGPT is an AI-driven system that automates statistical test selection and sample size calculation for clinical trial design. By integrating large language models with statistical engines, it enables one-click power analysis through natural language interaction. In a randomized trial, PowerGPT significantly outperformed traditional methods across completion rates, accuracy, and task completion time, demonstrating consistent performance gains regardless of statistical test type or user expertise level.

## Method Summary
The study evaluated PowerGPT, an agent-based system using OpenAI ChatGPT with function calling, against traditional methods for clinical trial design. PowerGPT uses a Python-R API wrapper via rpy2 to execute R statistical packages (powerSurvEpi v0.1.3, pwr) through OpenAPI 3.1 endpoints. The system was deployed on Google Cloud Run with HIPAA-compliant UTHealth Enterprise OpenAI Service. Evaluation involved 24 medical students completing 8 statistical test scenarios comparing two-sample t-tests, ANOVA, proportion tests, Cox models, and log-rank tests.

## Key Results
- 99.3% task completion rate versus 77.8% for traditional methods
- 94.1% accuracy in sample size calculation versus 55.4% for traditional methods
- Average task time reduced from 9.3 minutes to 4.0 minutes (p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
Separating natural language interpretation from numerical computation preserves accuracy while reducing expertise barriers. An LLM parses user intent and constructs structured API calls, while dedicated R statistical libraries execute actual power calculations. The LLM never performs arithmetic—it orchestrates.

### Mechanism 2
Interactive clarification loops recover missing parameters that would otherwise cause silent failures. Rather than requiring complete inputs upfront, PowerGPT detects absent or ambiguous parameters and prompts users conversationally before invoking computation.

### Mechanism 3
Standardizing on established R packages grounds AI outputs in validated statistical methods. PowerGPT wraps specific R functions with OpenAPI 3.1 endpoints, ensuring computational results match published methodological standards.

## Foundational Learning

- Concept: Statistical Power and Sample Size Relationship
  - Why needed here: PowerGPT automates this computation; understanding what power means (probability of detecting a true effect) and its determinants is essential for debugging unreasonable outputs
  - Quick check question: A user requests 95% power to detect a 1-unit difference with sd=10. Should the sample size surprise you if the system returns n=12 per group?

- Concept: LLM Function Calling / Tool Use
  - Why needed here: PowerGPT relies on ChatGPT's action API to construct structured calls; understanding how function schemas guide LLM behavior helps diagnose bad parameter mappings
  - Quick check question: If the OpenAPI schema for a t-test endpoint omits the "alternative" parameter, what might the LLM do when a user specifies a one-sided hypothesis?

- Concept: Effect Size Specification
  - Why needed here: Effect size is often the most uncertain input; PowerGPT's 94.1% accuracy depends on either users providing valid estimates or the system prompting appropriately
  - Quick check question: A user says "we expect about 20% improvement." What additional information must PowerGPT elicit before computing sample size?

## Architecture Onboarding

- Component map: Frontend GUI + CLI → Internal API Gateway → LLM Layer → Python-R Wrapper → Statistical Compute Layer → Storage → Infrastructure
- Critical path: User query → LLM intent parsing → parameter extraction → JSON API call construction → Python-R wrapper → R function execution → JSON result → LLM explanation → user response
- Design tradeoffs: Containerized Cloud Run enables 0-200+ instance scaling but introduces cold-start latency; HIPAA-compliant OpenAI Enterprise ensures data non-retention but may limit model version flexibility
- Failure signatures: Syntactically valid but semantically wrong JSON → R executes, returns plausible-looking wrong answer; missing required parameter not caught by schema → R error propagates as cryptic backend failure
- First 3 experiments:
  1. Validate end-to-end on a two-sample t-test: submit delta=1.5, sd=0.5, power=0.8; confirm returned n≈102 matches direct R execution
  2. Test clarification behavior: submit vague query ("compare two groups") and verify the system prompts for effect size, variance, and power before computing
  3. Audit the log-rank survival pathway: call /api/v1/log_rank_test with known inputs and compare against manual `ssizeCT.default` output from powerSurvEpi

## Open Questions the Paper Calls Out

- How does PowerGPT perform on non-standard trial designs, such as multi-arm trials, adaptive methodologies, or studies requiring real-time data integration for effect size refinement?
- What are the long-term effects of PowerGPT on user trust, potential over-reliance, and the development of statistical reasoning skills among non-statistician users?
- Can mechanisms for identifying invalid queries and verifying input coherence be developed to reduce misinterpretation risks in AI-assisted statistical reasoning?
- Does PowerGPT's assistance lead to actual improvements in the quality and success rates of subsequently conducted clinical trials?

## Limitations

- Study population consisted entirely of medical students at a single institution, limiting external validity
- Sample size of 24 participants provides adequate power for primary outcomes but limits detection of interactions
- System lacks documented safeguards against nonsensical or contradictory user inputs

## Confidence

- High: Architecture separation, performance metrics (completion rates, accuracy, time), deployment infrastructure
- Medium: Interactive clarification effectiveness, comparative advantage over traditional methods
- Low: Comprehensive validation across all statistical scenarios, robustness to edge cases

## Next Checks

1. Replicate the two-sample t-test scenario with deliberately ambiguous natural language input to verify the clarification mechanism's effectiveness
2. Test the system's handling of edge cases: negative effect sizes, zero variance, or conflicting parameters to assess error handling and validity checking
3. Conduct a small-scale external validation with experienced biostatisticians across multiple institutions to assess generalizability beyond medical students