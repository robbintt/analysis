---
ver: rpa2
title: 'DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge
  Transfer'
arxiv_id: '2507.23554'
source_url: https://arxiv.org/abs/2507.23554
tags:
- arxiv
- learning
- in-context
- selection
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability of in-context learning (ICL)
  in LLM-based agents, where the effectiveness of demonstrations is highly sensitive
  to example selection. Poor demonstrations can introduce spurious dependencies that
  degrade agent performance across reasoning steps.
---

# DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer

## Quick Facts
- arXiv ID: 2507.23554
- Source URL: https://arxiv.org/abs/2507.23554
- Reference count: 40
- Agents using DICE achieve 7-9% higher Exact Match scores on HotpotQA compared to baseline selection methods

## Executive Summary
This paper addresses the instability of in-context learning (ICL) in LLM-based agents, where the effectiveness of demonstrations is highly sensitive to example selection. Poor demonstrations can introduce spurious dependencies that degrade agent performance across reasoning steps. The authors propose DICE, a dynamic in-context example selection framework that adaptively selects the most relevant demonstrations at each time step by maximizing transferable knowledge and minimizing task-specific noise. DICE is training-free, framework-agnostic, and operates entirely at inference time.

Extensive experiments across diverse domains demonstrate its effectiveness: on HotpotQA, DICE improves Exact Match scores by 7-9% across different agent frameworks; on AlfWorld, it boosts success rates by 7-10% across subcategories; on Webshop, it improves both average score and success rate. Theoretical analysis shows that selecting demonstrations via DICE yields tighter generalization bounds. Stepwise selection outperforms static approaches, and DICE remains effective even with low-quality demonstrations, highlighting its robustness and practical utility for enhancing LLM agent performance.

## Method Summary
DICE dynamically selects demonstrations for LLM agents by extracting transferable knowledge (TK) from both demonstrations and current agent context using a Knowledge Retriever (gemma-2-2b-it). At each timestep, it computes TK representations and retrieves the most relevant demonstrations via InfoNCE-based similarity ranking. The selection criterion balances compression of task-specific details against preservation of action-predictive knowledge. DICE operates training-free, is compatible with various agent frameworks (ReAct, Reflexion, LATS), and refreshes demonstrations at each reasoning step rather than using static examples throughout a task.

## Key Results
- On HotpotQA, DICE improves Exact Match scores by 7-9% compared to baseline selection methods across different agent frameworks
- On AlfWorld, DICE boosts success rates by 7-10% across subcategories compared to KATE and random selection
- Stepwise selection (41.4 EM) outperforms taskwise selection (36.3 EM) on HotpotQA, demonstrating the benefit of dynamic adaptation
- DICE remains effective even with low-quality demonstrations, requiring only 3 selected demos to match the performance of 6 random demos

## Why This Works (Mechanism)

### Mechanism 1: Causal Decomposition of Demonstration Knowledge
- Claim: Demonstrations contain both transferable knowledge (TK) that aids generalization and task-specific noise (εD) that introduces spurious dependencies, and selectively retrieving TK while suppressing εD improves agent performance.
- Mechanism: The paper models ICL through a causal graph where D ← TK → At forms the beneficial path, while εD → D opens a collider that creates backdoor information flow from spurious noise to actions. By explicitly retrieving for TK rather than raw demonstrations, DICE interrupts this spurious dependency path.
- Core assumption: The transferable knowledge component exists and can be meaningfully separated from task-specific noise via a learned encoder.
- Evidence anchors:
  - [abstract] "decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, showing how the latter can introduce spurious dependencies that impair generalization"
  - [section 2.1] Figure 2 shows the causal structure with εD → D ← TK → At creating the problematic backdoor path
  - [corpus] Related work on demonstration selection (KATE, EPR) lacks this causal decomposition; corpus papers focus on similarity-based retrieval without distinguishing transferable vs. spurious knowledge
- Break condition: If demonstrations contain no separable TK component (e.g., purely procedural tasks with no cross-task patterns), the causal decomposition provides no benefit.

### Mechanism 2: Information Bottleneck Selection Criterion
- Claim: Selecting demonstrations by minimizing I(d; TK_d) − βI(TK_d; A_t) balances compression of task-specific details against preservation of action-predictive knowledge.
- Mechanism: The first term penalizes including non-generalizable information (compression), while the second term encourages selecting demonstrations whose transferable knowledge maximally predicts the next action. When β=1, this reduces to minimizing I(d; TK_d | A_t), directly tightening the generalization bound.
- Core assumption: The mutual information terms can be meaningfully approximated using a fixed-capacity encoder and contrastive estimation.
- Evidence anchors:
  - [abstract] "selects the most relevant demonstrations at each step of reasoning"
  - [section 2.2] Equation (1) defines the objective; section 2.3 Theorem 2.3 proves this yields tighter generalization bounds
  - [corpus] "Leveraging In-Context Learning for Language Model Agents" and other corpus papers use similarity-based selection without IB grounding
- Break condition: If the encoder cannot reliably extract TK (e.g., domain mismatch between encoder training and task), the IB objective degrades to noisy retrieval.

### Mechanism 3: Stepwise Context-Adaptive Retrieval
- Claim: Retrieving different demonstrations at each timestep based on current agent context outperforms static, task-level selection.
- Mechanism: At each step t, DICE extracts TK_t from current history H_t and retrieves demonstrations whose TK_d maximizes similarity via InfoNCE. This adapts examples to the specific subproblem (e.g., handling "Could not find" errors) rather than using fixed demonstrations throughout.
- Core assumption: Agent state evolves meaningfully across timesteps, creating distinct subtask contexts that benefit from different demonstrations.
- Evidence anchors:
  - [abstract] "dynamic in-context example selection framework that adaptively selects the most relevant demonstrations at each time step"
  - [section 2.4] Uses pre-trained LLM as Knowledge Retriever with InfoNCE: d* ≈ arg max log exp(sim(TK_d, TK_t)) / Σ exp(sim(TK_d', TK_t))
  - [section 3.4] Table 4 shows stepwise DICE (41.4 EM) outperforms taskwise DICE (36.3 EM) on HotpotQA
  - [corpus] "Process In-Context Learning" explores dynamic demonstration insertion for math reasoning, suggesting stepwise adaptation is an emerging pattern
- Break condition: If agent trajectories are highly uniform across steps (little contextual variation), stepwise selection adds retrieval overhead without benefit.

## Foundational Learning

- Concept: Causal Graphs and Collider Bias
  - Why needed here: Understanding why conditioning on demonstrations opens spurious information paths requires grasping collider structures (D ← TK → At with εD → D).
  - Quick check question: Can you explain why conditioning on a collider variable creates spurious correlations between its parents?

- Concept: Information Bottleneck Principle
  - Why needed here: The selection criterion directly instantiates IB—compressing input while preserving predictive information about targets.
  - Quick check question: Given a representation T of input X, what does I(X; T) − βI(T; Y) optimize for?

- Concept: Mutual Information Estimation via InfoNCE
  - Why needed here: The paper approximates intractable MI terms using contrastive learning objectives; understanding this is essential for implementation.
  - Quick check question: How does the InfoNCE bound relate mutual information to similarity scores in a contrastive learning setup?

## Architecture Onboarding

- Component map: Demonstration Pool -> Knowledge Retriever -> Selection Module -> Agent Framework

- Critical path:
  1. At timestep t, agent has history H_t = (Task, prior actions/observations)
  2. Knowledge Retriever encodes H_t → TK_t
  3. For each d_i in pool, compute TK_di via same encoder
  4. Rank by sim(TK_di, TK_t), select top-M demonstrations
  5. Inject selected demos into agent prompt, proceed with action generation
  6. Repeat at next timestep with updated H_t

- Design tradeoffs:
  - Encoder choice: Larger encoders (e.g., gemma-2-9b) may extract richer TK but increase latency; paper uses gemma-2-2b-it for efficiency
  - Pool size vs. retrieval cost: Larger pools provide better coverage but linear retrieval cost; paper doesn't specify optimal pool sizes
  - M (number of selected demos): More demos provide more context but may include noise; paper shows 3 DICE demos match 6 random demos (Figure 3b)

- Failure signatures:
  - Performance matches baseline: Encoder fails to distinguish TK from noise (check encoder-task domain alignment)
  - High variance across runs: Retrieval is unstable; check similarity score distributions and consider temperature tuning
  - Latency unacceptable: Profile encoder inference; consider caching TK representations for static demonstration pool
  - No improvement on taskwise variant: Stepwise context not sufficiently discriminative; task may not benefit from dynamic selection

- First 3 experiments:
  1. Replicate HotpotQA ablation (Table 4): Compare DICE (stepwise) vs. DICE (taskwise) vs. KATE vs. random selection on 100-question subset to validate stepwise benefit
  2. Encoder ablation: Swap gemma-2-2b-it for different encoder (e.g., sentence-transformers, larger LLM) and measure impact on TK extraction quality and downstream EM
  3. Demonstration quality analysis (Figure 3a replication): Bin episodes by average demo relevance score, plot performance curve to verify scoring metric correlates with actual utility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating a trainable encoder to capture latent transferable knowledge improve DICE's performance compared to the current training-free approach using a fixed pre-trained LLM?
- Basis in paper: [explicit] The conclusion states: "One limitation of our work is that we only explore a single instantiation of the framework; more expressive implementations—such as incorporating a trainable encoder to capture latent transferable knowledge—are left for future exploration."
- Why unresolved: The current implementation uses gemma-2-2b-it as a fixed Knowledge Retriever, but the theoretical framework does not require this constraint.
- What evidence would resolve it: Empirical comparison between the current training-free DICE and a variant with a fine-tuned encoder on the same benchmarks (HotpotQA, AlfWorld, Webshop).

### Open Question 2
- Question: How sensitive is DICE's performance to the choice and capacity of the Knowledge Retriever model used to extract transferable knowledge?
- Basis in paper: [inferred] The paper uses only gemma-2-2b-it as the Knowledge Retriever across all experiments, without ablation on different retriever models or sizes.
- Why unresolved: The approximation of mutual information via InfoNCE relies on the retriever's representations, yet alternative models or architectures remain unexplored.
- What evidence would resolve it: Ablation experiments varying the Knowledge Retriever (e.g., different model families, sizes) while holding other components constant.

### Open Question 3
- Question: How does DICE generalize across different base LLMs beyond gpt-3.5-turbo, particularly with smaller or open-weight models?
- Basis in paper: [inferred] All experiments use gpt-3.5-turbo exclusively; no results are reported for other agent backbones.
- Why unresolved: The framework's training-free design should be model-agnostic, but empirical validation across diverse LLMs is absent.
- What evidence would resolve it: Benchmarking DICE on the same tasks using alternative LLMs (e.g., Llama, Claude, smaller open models) with identical experimental settings.

## Limitations
- The causal decomposition framework assumes demonstrations contain separable transferable knowledge and task-specific noise components, but empirical validation of this separation is limited
- Knowledge Retriever implementation details remain underspecified, with exact prompt templates for extracting transferable knowledge not provided
- The claim that stepwise selection consistently outperforms taskwise selection needs broader validation across all domains

## Confidence

**High confidence**: The empirical results showing DICE outperforms baseline selection methods (KATE, EPR, random) across multiple domains and agent frameworks. The HotpotQA EM improvements (7-9%) and AlfWorld success rate gains (7-10%) are statistically significant and robust across different experimental conditions.

**Medium confidence**: The theoretical generalization bounds and causal analysis. While Theorem 2.3 provides formal justification for the selection criterion, the assumptions about separable knowledge components and the applicability of these bounds to complex LLM agents remain untested empirically.

**Low confidence**: The claim that stepwise selection consistently outperforms taskwise selection across all domains. The HotpotQA results show clear benefits, but AlfWorld and Webshop results don't report this comparison. The mechanism paper suggests stepwise adaptation is beneficial, but this needs broader validation.

## Next Checks

1. **Encoder sensitivity analysis**: Systematically vary the Knowledge Retriever capacity (gemma-2-2b-it vs. larger models vs. smaller encoders) and measure impact on TK extraction quality and downstream agent performance. Report correlation between retrieval quality metrics and actual task success.

2. **Demonstration quality decomposition**: Bin demonstrations by transferability scores and plot performance curves showing how DICE performance varies with demonstration pool quality. Include error analysis identifying failure modes when TK extraction fails or spurious dependencies dominate.

3. **Stepwise vs. taskwise generalization**: Extend stepwise vs. taskwise comparison to AlfWorld and Webshop domains. Measure not just overall performance but also per-step action accuracy to quantify where dynamic selection provides the most benefit.