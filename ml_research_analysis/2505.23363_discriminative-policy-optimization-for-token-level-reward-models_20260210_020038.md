---
ver: rpa2
title: Discriminative Policy Optimization for Token-Level Reward Models
arxiv_id: '2505.23363'
source_url: https://arxiv.org/abs/2505.23363
tags:
- reward
- q-rm
- policy
- rewards
- token-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving credit assignment
  in reinforcement learning for language models by developing a token-level reward
  model that decouples reward modeling from language generation. The proposed Q-function
  Reward Model (Q-RM) uses a discriminative policy to explicitly learn token-level
  Q-functions from preference data without requiring fine-grained annotations.
---

# Discriminative Policy Optimization for Token-Level Reward Models

## Quick Facts
- arXiv ID: 2505.23363
- Source URL: https://arxiv.org/abs/2505.23363
- Reference count: 40
- Key outcome: Token-level reward modeling using Q-functions improves RLHF credit assignment and policy optimization

## Executive Summary
This paper addresses the challenge of improving credit assignment in reinforcement learning for language models by developing a token-level reward model that decouples reward modeling from language generation. The proposed Q-function Reward Model (Q-RM) uses a discriminative policy to explicitly learn token-level Q-functions from preference data without requiring fine-grained annotations. Q-RM is integrated into reinforcement learning algorithms such as PPO and REINFORCE, providing more precise token-level feedback compared to sequence-level or step-level reward models. Experiments across mathematical reasoning, machine reading comprehension, and instruction-following tasks show that Q-RM consistently improves policy performance.

## Method Summary
The paper proposes Q-function Reward Model (Q-RM), a token-level reward model that learns Q-functions from preference data to provide fine-grained credit assignment in RLHF. Unlike outcome reward models (ORMs) that assign rewards only at sequence end, or step-level reward models (PRMs) that assign rewards per step, Q-RM learns to predict the expected cumulative reward for each token choice. This is achieved by training a discriminative policy on preference data to estimate Q-values, which are then used as token-level rewards during policy optimization. The approach is integrated with standard RL algorithms like PPO and REINFORCE, enabling more precise credit assignment during training.

## Key Results
- PPO/REINFORCE with Q-RM achieves up to 5.85/4.70 higher Pass@1 scores on GSM8K/MATH compared to ORM baseline
- Q-RM demonstrates up to 12x faster convergence than ORM on GSM8K and 11x faster than step-level PRM on MATH
- Consistent improvements across mathematical reasoning, machine reading comprehension, and instruction-following tasks

## Why This Works (Mechanism)
Q-RM improves credit assignment by learning token-level Q-functions that predict expected future rewards, enabling the policy to make better decisions at each generation step. By training on preference data rather than requiring fine-grained annotations, Q-RM captures complex human preferences while maintaining computational efficiency. The discriminative policy formulation allows Q-RM to focus on distinguishing between good and bad token choices based on their contribution to overall sequence quality, rather than modeling the full generation process.

## Foundational Learning
**Reinforcement Learning from Human Feedback (RLHF)**: RLHF aligns language models with human preferences through reward modeling and policy optimization. Why needed: Standard RLHF uses outcome rewards that provide sparse feedback, making credit assignment difficult for long sequences. Quick check: Verify understanding of reward modeling vs. policy optimization phases in RLHF.

**Credit Assignment in Sequential Decision Making**: Credit assignment determines how rewards are distributed across time steps in sequential tasks. Why needed: Language generation involves long sequences where individual token choices significantly impact overall quality. Quick check: Understand difference between temporal difference learning and Monte Carlo methods for credit assignment.

**Q-learning and Value Functions**: Q-functions estimate expected cumulative rewards for state-action pairs in reinforcement learning. Why needed: Q-RM uses Q-functions to provide token-level reward signals that capture future value contributions. Quick check: Verify understanding of Bellman equation and how Q-values are computed recursively.

**Discriminative vs. Generative Policies**: Discriminative policies learn to distinguish between good and bad outcomes rather than generating outputs directly. Why needed: Q-RM uses a discriminative approach to learn Q-functions without modeling the full generation process. Quick check: Understand the difference between conditional probability modeling and preference discrimination.

**Preference-based Learning**: Learning from pairwise or ranking preferences rather than absolute labels. Why needed: Q-RM is trained on preference data, making it more robust to subjective quality judgments. Quick check: Verify understanding of Bradley-Terry model or other preference learning frameworks.

## Architecture Onboarding

**Component Map**: Preference Data -> Q-Function Reward Model -> PPO/REINFORCE Policy Optimization -> Improved Language Model

**Critical Path**: The core innovation flows from preference data through Q-function learning to policy optimization. The Q-RM serves as an intermediate reward model that transforms preference signals into token-level rewards, which are then used by standard RL algorithms to update the language model policy.

**Design Tradeoffs**: Q-RM trades increased model complexity for more precise credit assignment. While simpler outcome reward models are computationally cheaper, they provide only end-of-sequence feedback. Step-level PRMs offer intermediate granularity but may not capture long-range dependencies as effectively as Q-functions.

**Failure Signatures**: Q-RM may fail when preference data is noisy or when the Q-function approximation cannot capture complex preference patterns. The model might also struggle with very long sequences where Q-value estimation becomes increasingly uncertain.

**First Experiments**: 
1. Compare Q-RM token-level rewards against ground truth human preferences on a validation set
2. Ablation study removing Q-RM from the RL pipeline to measure performance degradation
3. Test Q-RM with different preference dataset sizes to evaluate data efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to three tasks (GSM8K, MATH, MMLU) and two model scales
- Computational overhead of Q-RM compared to simpler reward models not quantified
- Potential biases from using preference data to train Q-function reward model not addressed

## Confidence

High confidence: The core technical contribution (Q-function Reward Model framework) and its integration with PPO/REINFORCE are well-defined and empirically validated within the tested task scope.

Medium confidence: The claim of "up to 12x faster convergence" is based on limited comparison points and may vary across different task distributions.

Low confidence: The assertion that Q-RM enables "more effective and efficient policy optimization" in general is an extrapolation beyond the tested domains.

## Next Checks

1. Evaluate Q-RM on diverse language tasks outside mathematical reasoning and instruction following to test domain generalization.

2. Conduct ablation studies comparing Q-RM's computational cost against ORM and step-level PRMs across varying sequence lengths.

3. Test Q-RM with smaller model scales (7B and below) to assess whether performance gains persist under computational constraints.