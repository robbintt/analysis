---
ver: rpa2
title: 'MobiAgent: A Systematic Framework for Customizable Mobile Agents'
arxiv_id: '2509.00531'
source_url: https://arxiv.org/abs/2509.00531
tags:
- task
- agent
- action
- mobile
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MobiAgent is a comprehensive mobile agent system that addresses
  challenges in real-world task execution, particularly accuracy and efficiency. It
  introduces three core components: MobiMind-series agent models with a multi-role
  architecture (Planner, Decider, Grounder), the AgentRR acceleration framework that
  leverages record-and-replay with multi-level experiences, and the MobiFlow benchmarking
  suite based on Directed Acyclic Graphs.'
---

# MobiAgent: A Systematic Framework for Customizable Mobile Agents

## Quick Facts
- arXiv ID: 2509.00531
- Source URL: https://arxiv.org/abs/2509.00531
- Reference count: 35
- Outperforms GPT-5, Gemini 2.5-pro, and specialized GUI agent models in real-world mobile scenarios

## Executive Summary
MobiAgent is a comprehensive mobile agent system designed to address challenges in real-world task execution, particularly accuracy and efficiency. The system introduces three core components: MobiMind-series agent models with a multi-role architecture (Planner, Decider, Grounder), the AgentRR acceleration framework that leverages record-and-replay with multi-level experiences, and the MobiFlow benchmarking suite based on Directed Acyclic Graphs. By employing an AI-assisted agile data collection pipeline, MobiAgent achieves state-of-the-art performance while reducing manual annotation costs.

## Method Summary
MobiAgent addresses GUI-based mobile agent task execution through a three-component system. The core consists of multi-role agent models: Planner (4B) handles task planning, Decider (7B) selects actions, and Grounder (3B) localizes UI elements. The AgentRR acceleration framework enables experience replay by building ActTrees with Tracer, training task embedding and reranking models for latent memory, and using OmniParser for UI change detection. The system employs a two-phase post-training approach: Warm-up SFT with ReAct-style prompts followed by Two-stage curriculum GRPO training. Performance is evaluated on the MobiFlow benchmark using DAG-based milestone evaluation, measuring task completion rate, action replay rate, and 2-3x latency reduction.

## Key Results
- Achieves state-of-the-art performance in real-world mobile scenarios, outperforming both general-purpose LLMs (GPT-5, Gemini 2.5-pro) and specialized GUI agent models (UI-TARS-1.5-7B)
- AgentRR framework delivers 2-3x optimization on task completion latency with 60%-85% action replay rates and >99% correctness under power law user distributions
- Real-world deployment demonstrates successful task completion across diverse mobile applications

## Why This Works (Mechanism)
MobiAgent works by decomposing complex mobile tasks into specialized roles that handle distinct aspects of the execution pipeline. The Planner interprets high-level goals and creates step-by-step strategies, while the Decider selects specific actions based on current context, and the Grounder accurately locates UI elements for interaction. The AgentRR framework accelerates execution by recognizing and replaying previously successful action sequences, reducing redundant reasoning. This multi-level experience system combines explicit replay of exact traces with latent memory-based recognition of similar tasks, achieving high efficiency without sacrificing accuracy.

## Foundational Learning
- **Multi-role agent architecture (Planner/Decider/Grounder)**: Needed because single monolithic models struggle with the complexity of mobile task execution; Quick check: Verify each role has distinct training objectives and evaluation metrics
- **GRPO reinforcement learning with curriculum**: Required to optimize action selection beyond supervised learning; Quick check: Monitor reward progression across curriculum stages
- **Experience replay with latent memory**: Essential for achieving 2-3x latency improvements; Quick check: Measure replay rate vs. accuracy trade-off across different threshold settings
- **DAG-based benchmarking with semantic validation**: Necessary to handle multiple valid execution paths; Quick check: Test benchmark's ability to recognize semantically equivalent but structurally different solutions

## Architecture Onboarding

**Component Map**: Task input -> Planner -> Decider -> Grounder -> Execution -> AgentRR Cache -> Evaluation

**Critical Path**: Mobile GUI screenshot + task description → Planner → Decider → Grounder → Action execution → Result evaluation

**Design Tradeoffs**: The system prioritizes accuracy over pure speed, using multi-role specialization rather than end-to-end models. This increases model complexity but enables better handling of complex mobile tasks. The AgentRR framework trades some storage overhead for significant latency improvements.

**Failure Signatures**: 
- Sparse rewards during GRPO indicate poor warm-up SFT quality
- High false positive replay rates suggest inadequate UI change detection
- Task non-termination reveals issues with "done" action training distribution

**3 First Experiments**:
1. Test multi-role inference pipeline on simple predefined tasks to verify role coordination
2. Evaluate AgentRR replay accuracy on known successful traces with controlled UI modifications
3. Run GRPO training on a small curriculum subset to validate reward shaping effectiveness

## Open Questions the Paper Calls Out
- How can benchmark frameworks effectively distinguish between an agent failure and the discovery of a novel, valid trajectory not predefined in the evaluation DAG?
- What is the quantitative latency impact of recovery operations triggered by false positive reuse decisions in the AgentRR framework?
- How robust is the OmniParser-based invalidation mechanism against functional UI changes that preserve visual similarity?

## Limitations
- Several critical hyperparameters (GRPO learning rates, curriculum schedules, replay thresholds) remain unspecified
- Hardware requirements for training the complete multi-agent system are not documented
- Evaluation methodology lacks transparency regarding human evaluation protocols and inter-rater reliability measures

## Confidence
- **High Confidence**: Multi-role architecture concept and GRPO training methodology are technically sound
- **Medium Confidence**: 2-3x latency optimization claim is plausible but depends heavily on unspecified replay thresholds
- **Low Confidence**: Exact dataset characteristics and self-evolution pipeline effectiveness remain unknown

## Next Checks
1. Replicate GRPO hyperparameter sensitivity testing with varying α, β, and curriculum schedules
2. Systematically evaluate AgentRR replay performance across different threshold settings
3. Test trained models on unseen mobile applications to assess true generalization capability