---
ver: rpa2
title: 'The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game'
arxiv_id: '2508.18467'
source_url: https://arxiv.org/abs/2508.18467
tags:
- points
- round
- each
- game
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether informing large language models
  (LLMs) that they are playing against themselves versus another AI agent affects
  their cooperation in an iterated public goods game. The authors conducted three
  studies using GPT-4o, Claude Sonnet 4, Llama 4 Maverick, and Qwen3 across "no-name"
  and "name" conditions with various system prompts emphasizing collective welfare,
  neutrality, or self-interest.
---

# The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game

## Quick Facts
- **arXiv ID**: 2508.18467
- **Source URL**: https://arxiv.org/abs/2508.18467
- **Reference count**: 40
- **Key outcome**: Telling LLMs they are playing against themselves (vs. another AI) significantly changes their public goods game contributions in 5-7 out of 9-12 prompt pairings, depending on whether the prompt emphasizes collective welfare or self-interest.

## Executive Summary
This paper investigates whether informing large language models (LLMs) that they are playing against themselves versus another AI agent affects their cooperation in an iterated public goods game. Across three studies using GPT-4o, Claude Sonnet 4, Llama 4 Maverick, and Qwen3, models contributed significantly differently under "name" (self) versus "no-name" (other AI) conditions in most prompt pairings. Notably, collective prompts led to reduced contributions when models thought they were playing against themselves, while selfish prompts led to increased contributions. The effect persisted even in four-player setups with less information about others' decisions. These findings suggest that identity cues can influence LLM cooperation in multi-agent settings, with implications for autonomous system design.

## Method Summary
The authors conducted three studies using an iterated public goods game where LLMs decided how many of their 10-point endowment to contribute each round (multiplier = 1.6). Models (GPT-4o, Claude Sonnet 4, Llama 4 Maverick, Qwen3) played under two conditions: "no-name" (told they play another AI agent) and "name" (told they play themselves). Six prompt variants combined collective, neutral, and selfish framings with name/no-name conditions. Each round's context included round history (total contribution, personal gain, accumulated points). Study 1 used 100 games (2 players), Study 3 used 50 games (4 players). JSON output formatting was enforced. Temperature = 1.0 for all calls.

## Key Results
- Telling models they are playing against themselves significantly changed contributions in 5-6 out of 9 prompt pairings in Studies 1 and 2
- In Study 3 (four-player game), 7 out of 12 prompt pairings showed significant differences between name/no-name conditions
- Models with collective prompts contributed less when told they were playing against themselves
- Models with selfish prompts contributed more when told they were playing against themselves
- Effects persisted even with reduced information about others' decisions in multi-agent setup

## Why This Works (Mechanism)
The paper does not establish a clear mechanism for why identity cues affect LLM cooperation. The authors explicitly state uncertainty about why the name/no-name distinction produces such pronounced differences, speculating it may relate to models' knowledge of their own capabilities. Reasoning traces rarely mentioned playing against themselves, and sentiment analysis was limited by lost raw data. The mechanism remains an open question requiring further investigation.

## Foundational Learning
- **Iterated public goods game**: A repeated game where players decide contributions to a common pool that benefits all players, testing cooperation dynamics over time
  - Why needed: Provides a controlled environment to measure cooperative behavior and how it changes under different identity framings
  - Quick check: Verify the payoff formula (total contributions ร multiplier รท players) and that each player starts with equal endowment each round
- **System prompt framing**: How instructions are worded (collective, neutral, selfish) to prime different behavioral motivations
  - Why needed: Allows testing whether identity effects interact with pre-existing motivational biases
  - Quick check: Confirm six prompt variants are correctly constructed with appropriate framing and name/no-name conditions
- **JSON output enforcement**: Requiring structured responses to ensure clean parsing of contributions and reasoning
  - Why needed: Prevents conversational drift and enables automated analysis across thousands of game rounds
  - Quick check: Validate that API responses strictly follow the specified JSON schema

## Architecture Onboarding

**Component map:**
- API call -> Model response (JSON) -> Parse contribution/reasoning -> Calculate scores -> Append round history -> Next round

**Critical path:**
1. Construct system prompt with appropriate framing and identity condition
2. API call to model with context window containing game history
3. Parse JSON response for contribution value and reasoning text
4. Calculate player scores using the public goods formula
5. Append formatted round summary to context for next iteration
6. Repeat for 20 rounds, then for 100 games (Study 1) or 50 games (Study 3)

**Design tradeoffs:**
- Separate API calls (vs. true multi-agent simulation) trade realism for experimental control
- JSON enforcement ensures clean data but may constrain natural reasoning expression
- Temperature = 1.0 maximizes response diversity but reduces determinism
- Context window management balances historical information with token limits

**Failure signatures:**
- JSON parsing errors indicate models failed to follow structured output requirements
- Missing or malformed round history suggests context window management issues
- Inconsistent contributions across identical conditions may indicate randomness or prompt sensitivity
- Models expressing confusion about repeated rules suggest context confusion

**First experiments to run:**
1. Replicate Study 1's Collective-Collective pairing with GPT-4o vs. Claude Sonnet 4 to verify basic effect
2. Test JSON enforcement robustness by running with relaxed output requirements and comparing results
3. Run a single 20-round game with full logging to verify context window management and score calculations

## Open Questions the Paper Calls Out
**Open Question 1:** Why does telling LLMs they are playing against themselves (vs. another AI) significantly change their cooperation behavior?
- Basis in paper: [explicit] The authors state uncertainty about the mechanism, suspecting it may relate to knowledge of their own capabilities, but reasoning traces rarely mentioned this directly
- Why unresolved: Limited analysis of reasoning traces due to lost raw data; models rarely explicitly referenced playing against themselves
- What evidence would resolve it: Systematic analysis of reasoning traces with better controls; ablation studies isolating capability-knowledge vs. identity cues

**Open Question 2:** How would LLMs behave if told they are playing against humans rather than themselves or another AI?
- Basis in paper: [explicit] The authors suggest testing human opponent conditions would be interesting
- Why unresolved: This condition was not tested in any of the three studies
- What evidence would resolve it: Replicate the experimental paradigm with a "human opponent" condition and compare contribution patterns

**Open Question 3:** Does the identity-cue effect on cooperation persist in multi-agent settings where agents can converse with each other?
- Basis in paper: [explicit] Future work should test similar conditions in frameworks allowing agent-to-agent dialogue
- Why unresolved: Current study used separate model calls with no inter-agent communication
- What evidence would resolve it: Run comparable experiments in frameworks allowing agent-to-agent dialogue

**Open Question 4:** Do these findings generalize to more complex, real-world economic scenarios beyond the toy public goods game?
- Basis in paper: [inferred] Authors acknowledge results are not generalizable to real-world risk scenarios
- Why unresolved: Study used stylized game with artificial stakes and no real consequences
- What evidence would resolve it: Test identity-framing effects in simulated supply chain coordination or resource allocation tasks

## Limitations
- The underlying mechanism for why identity cues affect cooperation remains unclear and unexplained
- Effect sizes are modest (e.g., 0.5-point contribution shifts) despite statistical significance
- Generalizability to real-world multi-agent systems or longer-term strategic interactions is not established
- Model version specificity is uncertain (no API or weight hash provided for Llama 4 Maverick and Qwen3)

## Confidence

**High confidence:** The core experimental methodology and directional effects (models contributing less under "self" identity with collective prompts, more with selfish prompts) are robust across multiple studies and model pairings.

**Medium confidence:** The claim that "identity cues change cooperation" is supported, but the underlying psychological mechanism (self-recognition, strategic reasoning, or simple prompt compliance) remains unclear.

**Low confidence:** Generalizability to real-world multi-agent systems or longer-term strategic interactions is not established.

## Next Checks
1. Re-run Study 1 for the Collective-Collective pairing with GPT-4o vs. Claude Sonnet 4, ensuring exact prompt and JSON output formatting as specified in Appendix B.1
2. Conduct a robustness check by varying the multiplier or endowment size to see if identity effects persist under altered game parameters
3. Extend the game to 50 rounds and compare convergence patterns between name and no-name conditions to assess whether short-term prompt effects translate into stable behavioral differences