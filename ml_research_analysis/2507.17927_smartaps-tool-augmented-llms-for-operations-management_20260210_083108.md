---
ver: rpa2
title: 'SMARTAPS: Tool-augmented LLMs for Operations Management'
arxiv_id: '2507.17927'
source_url: https://arxiv.org/abs/2507.17927
tags:
- tool
- user
- conversation
- operations
- smartaps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMARTAPS addresses the high cost and complexity of using traditional
  Advanced Planning Systems (APS) by introducing a conversational interface powered
  by tool-augmented LLMs. The system allows operations planners to interact with APS
  using natural language for tasks like querying plans, counterfactual reasoning,
  and scenario analysis.
---

# SMARTAPS: Tool-augmented LLMs for Operations Management

## Quick Facts
- arXiv ID: 2507.17927
- Source URL: https://arxiv.org/abs/2507.17927
- Authors: Timothy Tin Long Yu; Mahdi Mostajabdaveh; Jabo Serge Byusa; Rindra Ramamonjison; Giuseppe Carenini; Kun Mao; Zirui Zhou; Yong Zhang
- Reference count: 25
- Primary result: Conversational APS interface reduces analysis time from 1-2 days to a few hours

## Executive Summary
SMARTAPS introduces a conversational interface for Advanced Planning Systems (APS) using tool-augmented LLMs, enabling operations planners to interact with complex optimization systems through natural language. The system addresses the high cost and complexity of traditional APS by providing intuitive query, counterfactual reasoning, and scenario analysis capabilities without requiring OR consultants. Tested in production planning scenarios, SMARTAPS demonstrates significant time savings while maintaining sophisticated operational analysis capabilities through a modular architecture that integrates semantic search, intent detection, and tool orchestration.

## Method Summary
SMARTAPS implements a three-module architecture where a conversation manager handles intent detection and response refinement using MISTRAL-7B-INSTRUCT-V0.1, a tool retriever performs semantic similarity matching between user queries and API descriptions using BGE-LARGE-EN-V1.5 embeddings stored in ChromaDB, and a tool manager extracts required parameters and executes selected APIs from a curated catalog. The system integrates with OptVerse AI Solver for optimization tasks and provides a Chainlit-based chat interface that renders results as text, tables, or visualizations. Tool selection uses squared L2 distance for similarity computation, while parameter extraction relies on LLM inference from conversation context and APS state.

## Key Results
- Tool retrieval achieves accurate selection in 150 test cases across five tool categories
- Analysis time reduced from 1-2 days to a few hours in production planning scenarios
- System successfully handles query plan, why-not, what-if, compare plan, and display plan operations
- Conversational interface eliminates need for OR consultant expertise for routine APS interactions

## Why This Works (Mechanism)

### Mechanism 1
Semantic similarity matching between user queries and tool API descriptions enables accurate tool selection from a curated catalog. The Tool Retriever encodes both user queries and concatenated tool descriptions (description + example queries) into dense vectors using BGE-LARGE-EN-V1.5. Squared L2 distance computes similarity, and the tool with minimum distance is selected. This replaces keyword matching with semantic understanding.

### Mechanism 2
LLM-mediated intent detection and response refinement contextualizes raw tool outputs within ongoing conversation. The Conversation Manager uses MISTRAL-7B-INSTRUCT-V0.1 for two tasks: (1) classifying intent as CASUAL CONVERSATION or OPERATIONS PLANNING via prompted classification; (2) refining tool outputs by passing the tool's natural language response plus conversation history to the LLM, which produces a contextually grounded response.

### Mechanism 3
Parameter extraction from conversation history enables tool execution without explicit user specification of all inputs. The Tool Manager receives the retrieved API and uses an LLM to extract required input parameters from the user query plus conversation context. It also identifies which optimization model and APS data to use based on conversation history. Missing parameters trigger clarifying questions back through the Conversation Manager.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: SMARTAPS adapts RAG principles but retrieves tool APIs rather than text passages. Understanding standard RAG helps grasp how semantic search connects queries to structured resources.
  - Quick check question: Can you explain why embedding-based retrieval outperforms keyword search for matching "How would delayed rubber delivery affect my plan?" to a what-if analysis tool?

- **Concept: Tool-Augmented LLMs / Function Calling**
  - Why needed here: The core architecture depends on LLMs acting as orchestrators that invoke external tools. Understanding the pattern of LLM → tool selection → parameter extraction → execution → response synthesis is essential.
  - Quick check question: What happens when a required tool parameter cannot be inferred from the user query or conversation history?

- **Concept: Operations Research / Advanced Planning Systems**
  - Why needed here: The domain context shapes tool categories (query plan, why-not, what-if, compare plan, display plan). Without understanding what an APS does, the tool taxonomy and use cases are opaque.
  - Quick check question: Why might a planner need "why-not" analysis versus "what-if" analysis, and how do these differ in terms of what changes in the optimization model?

## Architecture Onboarding

- **Component map:**
  Chainlit (Client) -> Conversation Manager (MISTRAL-7B-INSTRUCT-V0.1) -> Tool Retriever (BGE-LARGE-EN-V1.5 + ChromaDB) -> Tool Manager (LLM parameter extraction) -> OptVerse AI Solver -> APS Data

- **Critical path:**
  1. User submits natural language query via Chainlit
  2. Conversation Manager classifies intent
  3. If OPERATIONS PLANNING: Tool Retriever computes query embedding, performs similarity search
  4. Tool Manager extracts parameters from query + conversation history + APS state
  5. Tool Manager executes selected tool (may call OptVerse solver)
  6. Tool output returned to Conversation Manager for refinement
  7. Refined response rendered in Chainlit (text, table, or visualization)

- **Design tradeoffs:**
  - Lightweight LLM (7B) vs larger models: Chosen for cost and latency; may limit complex reasoning
  - Curated tool catalog vs dynamic tool generation: Current approach requires OR consultants to create tools; future work suggests automated API generation
  - Single-user vs multi-user: Current limitation; real operations involve multiple planners with different objectives
  - Synchronous execution vs async job queue: Optimization solvers can take hours; overnight jobs common in practice but not yet integrated

- **Failure signatures:**
  - Tool retrieval returns irrelevant API → check if query phrasing matches tool description patterns
  - Parameter extraction fails → missing context in conversation; may need explicit user clarification
  - Intent misclassified as CASUAL when user wants analysis → prompt engineering issue in classification prompt
  - Long solver times blocking conversation → need async task manager (noted as future work)
  - Multi-turn context drift → conversation state not properly maintained across sessions

- **First 3 experiments:**
  1. Tool retrieval accuracy baseline: Test semantic similarity retrieval on held-out queries across all five tool categories. Measure precision@1 and analyze failure modes where L2 distance selects wrong tool.
  2. Intent classification stress test: Submit queries at the boundary of CASUAL and OPERATIONS PLANNING (e.g., "Can you help me understand the plan?"). Measure classification accuracy and identify prompt improvements.
  3. End-to-end latency profiling: Instrument each component (embedding, retrieval, LLM calls, tool execution) to identify bottlenecks. Test with queries requiring solver invocation vs simple data lookups.

## Open Questions the Paper Calls Out
None

## Limitations
- System relies entirely on curated tool catalog created by OR consultants - no evaluation of what happens when users request analyses for which no appropriate tool exists
- Single-user design limitation explicitly acknowledged but not addressed, despite multi-planner scenarios being common in operations management
- "Few hours vs 1-2 days" time reduction claim appears anecdotal without controlled benchmarking

## Confidence

- **High confidence**: Semantic search retrieves appropriate tools when query semantics align with tool descriptions (150/150 test cases)
- **Medium confidence**: LLM-based intent classification and response refinement work for clear, single-turn queries
- **Low confidence**: System handles multi-turn conversations, ambiguous parameter extraction, and novel analysis requests outside the curated catalog

## Next Checks

1. Tool retrieval breakdown by category: Analyze retrieval accuracy separately for each of the five tool types to identify if certain analysis types (e.g., what-if vs why-not) show systematic failure patterns
2. Multi-turn conversation robustness: Design test scenarios where users switch contexts mid-conversation or reference previous tool outputs to assess if conversation manager maintains appropriate context
3. Catalog coverage stress test: Systematically generate user queries that should map to existing APS capabilities but lack corresponding tools to measure what fraction of reasonable requests cannot be satisfied