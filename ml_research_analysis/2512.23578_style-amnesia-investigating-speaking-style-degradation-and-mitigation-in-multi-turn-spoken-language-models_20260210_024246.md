---
ver: rpa2
title: 'Style Amnesia: Investigating Speaking Style Degradation and Mitigation in
  Multi-Turn Spoken Language Models'
arxiv_id: '2512.23578'
source_url: https://arxiv.org/abs/2512.23578
tags:
- style
- slms
- mini
- gpt-4o
- speaking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spoken language models (SLMs) struggle to maintain user-specified
  speaking styles like emotion, accent, volume, or speed across multi-turn dialogues,
  a phenomenon termed "style amnesia." The authors evaluate five SLMs (three proprietary,
  two open-source) by instructing them to follow a specific style from the start of
  a conversation and measuring their adherence across turns using automatic judges.
  Results show that while models follow the style well in the first turn, performance
  degrades significantly in subsequent turns, with degradation rates ranging from
  0.7% to 65.3% depending on the style and model.
---

# Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models

## Quick Facts
- arXiv ID: 2512.23578
- Source URL: https://arxiv.org/abs/2512.23578
- Reference count: 35
- SLMs struggle to maintain user-specified speaking styles across multi-turn dialogues, a phenomenon termed "style amnesia."

## Executive Summary
Spoken language models (SLMs) fail to consistently maintain user-specified speaking styles across multi-turn dialogues, despite initially following instructions well. This "style amnesia" manifests as significant degradation in style adherence from the first to subsequent turns, with rates ranging from 0.7% to 65.3% depending on the style and model. The problem persists even when models can recall the style instruction when explicitly asked, indicating a gap between instruction retention and expressive control. Placing style instructions in system messages instead of user messages worsens performance, contradicting their intended higher priority. A recall mechanism partially mitigates but doesn't fully resolve the issue, highlighting fundamental limitations in current SLM architectures for sustained stylistic control.

## Method Summary
The study evaluates five SLMs (three proprietary, two open-source) using a user simulator framework that generates 4-turn dialogues. Models receive a style instruction at conversation start and must maintain it throughout. Automatic judges evaluate style adherence for each turn: Emotion2vec for emotion, Voxlect for accent, PyLoudnorm for volume, and Parakeet TDT v2 for speed. First-turn instruction-following (IF) rates and degradation rates are computed, with recall rates measured when models are asked to restate the style. The evaluation covers 10 styles across 100 conversation topics, generating 1000 dialogues total.

## Key Results
- SLMs follow speaking styles well in the first turn (IF rates >80%) but show significant degradation in subsequent turns
- Degradation rates range from 0.7% to 65.3% across different styles and models
- Models recall style instructions well (97-100% for proprietary models) but fail to express them consistently
- System message placement of instructions causes up to 80% performance drops, contradicting intended design
- Recall mechanism (asking "What speaking style should you use?") partially mitigates degradation by ~25%

## Why This Works (Mechanism)

### Mechanism 1
Explicit recall prompts before each turn partially restore style adherence by re-anchoring attention to the style instruction. When the model is asked "What speaking style should you use?" before generating a response, the style instruction becomes part of the immediate context rather than distant conversation history. This reduces the attention dilution that occurs as dialogue turns accumulate. The core assumption is that the model's attention mechanism prioritizes recent context over earlier instructions when generating speech. Evidence shows GPT-4o mini achieves roughly 25% reduction in degradation rate with recall process. This mechanism breaks down when models have low recall rates (e.g., Step-Audio 2 mini at 55-89%), as the instruction itself is not retained.

### Mechanism 2
Style instructions placed in system messages are underweight relative to user messages in current SLM architectures. Despite system messages being designed for global behaviors, the audio generation pathway may not attend to them as strongly as user messages during speech synthesis. The core assumption is that the text-to-speech or audio generation component processes user message context more directly than system message context. Evidence shows GPT-4o mini exhibits nearly 80% drop for Indian English accent when instructed via system message. This mechanism would break if the style instruction requires semantic complexity that user messages cannot cleanly encode, making system messages necessary despite degraded performance.

### Mechanism 3
Style degradation occurs because models retain instructions but fail to condition their audio generation on them. The model's language component maintains instruction memory, but the audio synthesis pathway operates semi-independently and reverts to default speaking patterns absent explicit per-turn conditioning. The core assumption is that SLMs have a separation between instruction understanding and acoustic expression that is wider than in text-only models. Evidence shows proprietary models demonstrate near-perfect recall rates (97-100%) even as style IF degrades 20-65%. This mechanism shifts to memory failure rather than expression failure if recall rates drop significantly (as in Step-Audio 2 mini).

## Foundational Learning

- **End-to-end SLM architecture (audio encoder → LLM → vocoder)**: Style instructions must propagate through this pipeline; degradation may occur at any junction. Quick check: Can you trace where style conditioning is injected in the model you are debugging?

- **Instruction-following rate (IF rate) measurement**: Quantifying degradation per turn is essential for diagnosing whether fixes work. Quick check: How would you compute IF rate for a style that lacks an automatic judge?

- **Cascaded SLM baseline (ASR → LLM → TTS with per-turn style injection)**: Provides an upper-bound comparison; if cascaded systems don't degrade, the issue is architectural in end-to-end models. Quick check: What tradeoff does a cascaded system make compared to an end-to-end SLM?

## Architecture Onboarding

- **Component map**: Evaluated SLM (receives speech + style instruction) → User simulator (ASR → GPT-5 mini → GPT-4o mini TTS) → Style judge (emotion: Emotion2vec, accent: Voxlect, volume: LUFS, speed: WPM) → IF rate computation per turn

- **Critical path**: Style instruction at turn 1 → model generates turn 1 speech → IF rate measured → user simulator responds → model generates turn 2 speech (without re-prompting) → IF rate degrades → repeat for 4 turns

- **Design tradeoffs**: Using automatic judges enables scale (1000 dialogues) but introduces classifier noise; human validation shows MCC of 0.48-0.81 depending on attribute. Recall process improves adherence but adds latency and token cost per turn.

- **Failure signatures**: (1) First-turn IF rate high (>80%) but degradation rate >20% indicates style amnesia; (2) Recall rate high but IF rate low indicates expression gap; (3) System message IF rate comparable to random baseline indicates instruction underweighting.

- **First 3 experiments**:
  1. Establish baseline IF rate per turn for a single style (e.g., "speak sadly") across 4 turns using the user simulator framework.
  2. Add recall process: inject "What speaking style should you use?" before each response and measure degradation reduction.
  3. Ablate prompt position: run the same style instruction in system message vs. user message and compare first-turn IF rate.

## Open Questions the Paper Calls Out

- **What architectural or training modifications are required to bridge the gap between style instruction retention and actual stylistic expression during generation?** The paper identifies the symptom (amnesia despite recall) but doesn't propose a structural solution to ensure latent instructions influence the vocoder or audio generation layers consistently.

- **Why do SLMs fail to adhere to style instructions placed in system messages, often performing significantly worse than when instructions are in user messages?** The paper demonstrates this counter-intuitive phenomenon but doesn't investigate the underlying attention mechanisms or training data biases causing system prompts to be ignored during speech synthesis.

- **How do SLMs perform when tasked with maintaining composite speaking styles (e.g., "speak fast and sadly") over multiple turns?** The paper notes analysis is restricted to single styles because "most current SLMs struggle to perform this kind of complex style" and current judges are insufficient.

## Limitations

- **Automatic judge reliability**: Human validation shows mixed quality (MCC 0.48-0.81), meaning some degradation rates may be artifacts of classifier noise rather than true style amnesia.

- **Model access constraints**: Several evaluated models (GPT-5 mini, Gemini-TTS) appear to be future or unreleased systems, limiting reproducibility.

- **Fixed conversation length**: Four-turn dialogues may not capture longer-term style degradation patterns that could emerge in extended conversations.

## Confidence

- **High confidence**: Models maintain style well in first turn but degrade in subsequent turns.
- **Medium confidence**: Style degradation is not due to forgetting instructions but to failure in expressive control.
- **Low confidence**: System message placement fundamentally undermines instruction following.

## Next Checks

1. **Human validation study**: Conduct end-to-end human evaluation of style adherence across all 1000 dialogues to validate automatic judge measurements and quantify classifier noise impact.

2. **Cascaded system baseline**: Implement a cascaded ASR→LLM→TTS system with per-turn style injection to establish whether end-to-end SLMs perform worse than traditional pipelines.

3. **Attention analysis**: Use model interpretability tools to examine attention patterns in both text and audio generation pathways when system messages versus user messages contain style instructions.