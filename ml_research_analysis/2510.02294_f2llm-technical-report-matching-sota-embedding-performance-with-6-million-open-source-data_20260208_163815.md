---
ver: rpa2
title: 'F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million
  Open-Source Data'
arxiv_id: '2510.02294'
source_url: https://arxiv.org/abs/2510.02294
tags:
- https
- datasets
- huggingface
- given
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: F2LLM introduces a suite of embedding models trained solely on
  open-source non-synthetic data, achieving SOTA performance with a budget-friendly
  approach. Unlike prior models relying on massive synthetic data or multi-stage training,
  F2LLM is directly fine-tuned from foundation models on 6 million high-quality query-document-negative
  tuples curated from diverse open datasets.
---

# F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data

## Quick Facts
- arXiv ID: 2510.02294
- Source URL: https://arxiv.org/abs/2510.02294
- Authors: Ziyin Zhang; Zihan Liao; Hang Yu; Peng Di; Rui Wang
- Reference count: 40
- Key outcome: F2LLM achieves SOTA embedding performance with 6M open-source data, ranking 2nd among 4B models and 1st in 1B-2B range without synthetic data.

## Executive Summary
F2LLM introduces a suite of embedding models trained solely on open-source non-synthetic data, achieving SOTA performance with a budget-friendly approach. Unlike prior models relying on massive synthetic data or multi-stage training, F2LLM is directly fine-tuned from foundation models on 6 million high-quality query-document-negative tuples curated from diverse open datasets. The models (0.6B, 1.7B, 4B) achieve strong MTEB leaderboard performance: F2LLM-4B ranks 2nd among models around 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st in the 1B-2B range. The training employs a unified contrastive learning framework with hard negative mining, in-batch loss, and a custom multitask dataloader to optimize sample efficiency. By releasing models, training data, and code, F2LLM provides a reproducible, cost-effective baseline for future embedding research.

## Method Summary
F2LLM uses single-stage contrastive fine-tuning from Qwen3 foundation models (0.6B, 1.7B, 4B) on 6 million curated query-document-negative tuples. Training employs margin-based adaptive hard negative mining, in-batch loss restricted to retrieval tasks, and a custom multitask dataloader. The unified framework combines hard negative loss (all tasks) with in-batch loss (retrieval only) using cosine similarity and temperature 0.05. Models are trained for 2 epochs with AdamW optimizer, cosine learning rate decay, and gradient checkpointing. Hard negatives are mined offline using Qwen3-Embedding-0.6B with margin-based filtering to ensure semantic difficulty.

## Key Results
- F2LLM-4B ranks 2nd among models around 4B parameters and 7th overall on MTEB leaderboard (73.67 average)
- F2LLM-1.7B ranks 1st in the 1B-2B parameter range (72.01 average)
- F2LLM-0.6B achieves competitive performance (70.03 average) while maintaining efficiency
- Models achieve strong results across retrieval, clustering, classification, STS, and reranking tasks

## Why This Works (Mechanism)

### Mechanism 1: Margin-Based Adaptive Hard Negative Mining
Selecting hard negatives based on relative score margins improves contrastive learning sample efficiency compared to random negative sampling. The approach uses a teacher model to retrieve top-100 candidates per query, excludes top-5 to avoid false negatives, filters by absolute threshold (<0.8) and relative margin (<95% of positive score), then selects top-24 remaining passages. This pushes the model to learn finer-grained distinctions by focusing on semantically challenging examples.

### Mechanism 2: Task-Conditional In-Batch Loss with Multitask Dataloader
Computing in-batch loss only within same-source batches improves sample efficiency when blending retrieval and non-retrieval tasks. A custom multitask dataloader ensures each micro-batch contains samples from a single data source. Hard negative loss is computed for all tasks while in-batch loss is restricted to retrieval tasks, preventing cross-task interference in the in-batch similarity matrix while maintaining hard-negative contrastive pressure across all tasks.

### Mechanism 3: Single-Stage Fine-Tuning from Strong Foundation Models
High-quality curated data can substitute for multi-stage weakly-supervised pretraining when starting from capable foundation models. The approach skips billion-scale weakly supervised contrastive pretraining and directly fine-tunes Qwen3 (0.6B, 1.7B, 4B) on 6M curated tuples for 2 epochs. The foundation model's pre-existing textual understanding provides initialization quality that previously required separate pretraining stages.

## Foundational Learning

- Concept: Contrastive Learning (InfoNCE-style loss)
  - Why needed here: F2LLM's core training objective uses contrastive loss to pull query-positive pairs closer and push query-negative pairs apart in embedding space.
  - Quick check question: Can you explain why increasing temperature τ makes the softmax distribution softer and how this affects gradient magnitude?

- Concept: In-Batch Negatives vs Hard Negatives
  - Why needed here: F2LLM combines both loss types; understanding their trade-offs is critical for reproducing or modifying the training pipeline.
  - Quick check question: What is the computational difference between using other samples in the batch as negatives vs pre-mined hard negatives?

- Concept: Task-Specific Instructions for Embedding Models
  - Why needed here: F2LLM prepends task instructions to queries (e.g., "Given a question, retrieve passages that answer the question") to condition embeddings on task context.
  - Quick check question: How might instruction-prefixing interact with models that use bidirectional vs causal attention?

## Architecture Onboarding

- Component map:
  Qwen3 backbone -> Unified data format (query, positive, hard-negatives×n) -> Hard negative miner (Qwen3-Embedding-0.6B + margin filtering) -> Custom multitask dataloader -> Forward pass with instruction-prefixed queries -> Dual loss computation -> AdamW optimizer with cosine decay

- Critical path:
  1. Data curation (MTEB training sets + decontamination)
  2. Hard negative mining (offline, per dataset)
  3. Multitask dataloader (per-GPU same-source sampling)
  4. Forward pass with instruction-prefixed queries
  5. Dual loss computation (hard-negative always, in-batch retrieval-only)
  6. Gradient accumulation across GPUs for in-batch loss

- Design tradeoffs:
  - No synthetic data: Lower cost, higher reproducibility, but may cap upper performance vs synthetic-augmented models
  - Single-stage training: Simpler pipeline, but depends heavily on foundation model quality
  - 24 hard negatives per sample: Higher memory/compute per batch, but stronger gradient signal
  - In-batch loss restricted to retrieval: Reduces cross-task interference, but may limit transfer learning

- Failure signatures:
  - Clustering underperforms retrieval: Check if in-batch loss is incorrectly applied to clustering batches
  - High false-negative rate: Hard negative miner may be too aggressive; relax margin threshold or exclude top-k > 5
  - Training instability with small models: Learning rate may be too high; reduce per Table 1 scaling
  - Poor cross-domain generalization: Data diversity may be insufficient; inspect per-domain performance

- First 3 experiments:
  1. Ablate hard negative count: Train with 7 vs 24 hard negatives to quantify memory/performance trade-off on MTEB clustering and retrieval subsets.
  2. Enable in-batch loss for all tasks: Compare against baseline to validate the paper's claim that restricting to retrieval improves sample efficiency.
  3. Swap foundation model: Fine-tune a weaker backbone (e.g., smaller or earlier-generation LLM) to test the single-stage training assumption and measure performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can the F2LLM performance be further improved by augmenting the 6M non-synthetic tuples with synthetic data, or does the high quality of open-source data negate the need for synthetic generation? The paper establishes a strong baseline using real data but does not ablate the impact of adding synthetic data to this specific high-quality mixture.

### Open Question 2
How does the strict separation of in-batch loss (retrieval tasks only) versus hard negative loss (all tasks) affect the embedding space compared to applying in-batch loss universally? It is unclear if this restriction prevents the model from learning superior universal representations by limiting the diversity of in-batch negatives for non-retrieval tasks.

### Open Question 3
Is the effectiveness of the single-stage training pipeline generalizable to foundation models outside the Qwen family, or is it dependent on Qwen3's specific pretraining? The methodology relies entirely on the Qwen3 backbone, attributing success to the data quality, but does not test other popular architectures like Mistral or Llama.

## Limitations

- Core claims about sample efficiency gains rest on internal ablation comparisons without direct external benchmarks
- Dependence on foundation model quality represents a significant limitation - single-stage training's success is contingent on starting from strong pre-trained model
- The 6 million sample training set, while carefully curated, represents a relatively small dataset for contrastive learning

## Confidence

**High Confidence**: Reported MTEB leaderboard rankings and absolute scores are directly measurable and reproducible given the same evaluation conditions. Architectural components and training hyperparameters are explicitly specified and verifiable.

**Medium Confidence**: Claims about cost-effectiveness and reproducibility benefits of avoiding synthetic data are supported by methodology description but lack direct cost comparisons. Mechanism explanations for design choices are logically sound but not empirically validated through ablations.

**Low Confidence**: Broader claims about general applicability of single-stage training and superiority of specific hard negative mining strategy over alternatives cannot be independently verified without additional experimental evidence.

## Next Validation Checks

1. Ablate hard negative mining strategy: Train identical models using alternative negative sampling approaches (random negatives, adversarial mining, different margin thresholds) to quantify the specific contribution of the proposed margin-based method to final performance.

2. Test foundation model dependency: Reproduce the training pipeline starting from weaker backbone models (smaller parameter counts or earlier-generation LLMs) to empirically validate whether the single-stage approach's success is contingent on foundation model quality.

3. Enable in-batch loss for all tasks: Compare the baseline restricted approach against a version applying in-batch loss to all tasks to directly measure the claimed sample efficiency gains and investigate potential cross-task transfer effects.