---
ver: rpa2
title: Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games
arxiv_id: '2510.13060'
source_url: https://arxiv.org/abs/2510.13060
tags:
- lemma
- have
- games
- policy
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the sample efficiency of KL-regularized zero-sum
  games in online learning settings, where agents learn from noisy bandit feedback.
  The core idea is to exploit the fact that best responses to fixed opponent strategies
  admit closed-form solutions under KL regularization, enabling the design of algorithms
  that achieve logarithmic regret.
---

# Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games

## Quick Facts
- arXiv ID: 2510.13060
- Source URL: https://arxiv.org/abs/2510.13060
- Reference count: 40
- One-line primary result: First logarithmic regret guarantees (O(log²T)) for KL-regularized zero-sum games, improving upon O(√T) regret in unregularized settings.

## Executive Summary
This paper addresses the sample efficiency challenge in online learning for zero-sum Markov games with bandit feedback. By introducing KL regularization to the game-theoretic setting, the authors show that best responses to fixed opponent strategies admit closed-form Gibbs distributions, enabling logarithmic regret algorithms. The key insight is that KL regularization transforms the game into one where optimistic best-response sampling becomes tractable, achieving O(log²T) regret instead of the standard O(√T).

The approach combines game-theoretic equilibrium computation with reinforcement learning techniques. For matrix games, the OMG algorithm uses optimistic best-response sampling with UCB-style bonuses, while SOMG extends this to Markov games using superoptimistic bonuses to handle unbounded value functions. Both methods achieve regret that scales only logarithmically with the number of episodes T, representing a fundamental improvement in the sample efficiency of learning in game-theoretic environments.

## Method Summary
The paper proposes two algorithms: OMG for matrix games and SOMG for Markov games. Both algorithms exploit the closed-form best-response structure under KL regularization. The method works by computing Nash equilibria using extragradient methods, then separately computing optimistic best responses to each other's fixed strategies. This decomposition allows regret to be bounded by O(Σ bonus²) rather than O(Σ bonus), avoiding the √T factor from Jensen's inequality. The superoptimistic bonus in SOMG combines standard optimistic bonuses with MSE bonuses to handle unbounded value functions in Markov games.

## Key Results
- OMG achieves regret bound O(β⁻¹d²log²(T/δ)) for matrix games
- SOMG achieves regret bound O(β⁻¹d³H⁷log²(T/δ)) for Markov games
- First logarithmic regret guarantees for game-theoretic settings under KL regularization
- Improves upon standard O(√T) regret in unregularized settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL regularization enables logarithmic regret by converting best responses to closed-form Gibbs distributions, allowing regret to scale with squared bonuses instead of bonuses directly.
- Mechanism: Under KL regularization, the best response to a fixed opponent strategy becomes a Gibbs distribution with an explicit formula. This structure allows the algorithm to bound regret by O(Σ bonus²) rather than O(Σ bonus), which avoids the √T factor from Jensen's inequality and yields O(log²T) via the elliptical potential lemma.
- Core assumption: β > 0 (regularization strength is positive); the reference policy has full support so KL divergence is well-defined.
- Evidence anchors:
  - [abstract] "the best response of a player to a fixed opponent strategy admits a Gibbs distribution with closed-form expression"
  - [Section 4.1] "we can bound the regret by the sum of squared bonuses cβ⁻¹Σᵗ E[bₜ(i,j)²] which enables us to circumvent the need for Jensen's inequality which contributes the √T term"
  - [corpus] "Logarithmic Regret for Online KL-Regularized Reinforcement Learning" shows similar benefits in single-agent RL, confirming regularization as the key enabler
- Break condition: If β → 0, the closed-form expressions become singular and the O(β⁻¹) term diverges; the algorithm gracefully degrades to O(√T) regret via the regularization-independent bound.

### Mechanism 2
- Claim: Superoptimistic bonuses handle unbounded value functions in game settings while preserving the "superoptimistic gap" property needed for logarithmic regret analysis.
- Mechanism: In KL-regularized games, value functions contain positive KL terms that can be arbitrarily large. The superoptimistic bonus b_sup = b + 2b_mse combines standard optimistic bonus (b) with MSE bonus (2b_mse), and the projection operator ceiling is raised to 3(H−h+1)². This ensures Q⁺ − Q ≥ Q − Q^μ (the superoptimistic gap property in Lemma C.4).
- Core assumption: Linear MDP structure (Assumption 3) so that Q-functions can be parameterized and confidence ellipsoids apply.
- Evidence anchors:
  - [Section 3.2] "Superoptimistic bonuses are chosen such that the superoptimistic Q-function exceeds its standard optimistic estimate"
  - [Lemma C.4] establishes the formal superoptimistic gap condition: "2|Q⁺ − Q| ≥ |Q⁺ − Q^μ|"
  - [corpus] "Optimism Without Regularization" achieves constant regret in games but relies on unregularized settings; this paper extends optimism to regularized games via superoptimism
- Break condition: If horizon H is very large, the bonus scales as O(H²) and projection ceiling as O(H²), which may require careful tuning of bonus constants η₁, η₂.

### Mechanism 3
- Claim: Best-response sampling bypasses the absence of closed-form Nash equilibrium expressions by treating the fixed opponent policy as part of the environment.
- Mechanism: Unlike single-agent RL where optimal policies have closed forms, Nash equilibria in games do not. The algorithm exploits that best responses to fixed opponent policies DO have closed forms (as Gibbs distributions). By computing NE policies (μₜ, νₜ) and then separately computing optimistic best responses (μ̃ₜ, ν̃ₜ) to each other's fixed strategies, the algorithm can decompose regret into terms treatable via bandit/RL analysis.
- Core assumption: The game is zero-sum; best responses are well-defined and unique under KL regularization with full-support reference policies.
- Evidence anchors:
  - [Section 1.1] "Our algorithms systematically leverage this property by collecting best-response pairs and exploiting the resulting structure"
  - [Section 4.1] "As a result of fixed νₜ, one can view the min-player strategy νₜ as part of the environment and bound T₁ the same way as done in bandits"
  - [corpus] "Nash Policy Gradient" addresses equilibrium finding but requires iterative regularization refinement; this work avoids that complexity through best-response sampling
- Break condition: In general-sum games, best responses may not be unique and the dual-gap decomposition may not hold; the method is specific to zero-sum structure.

## Foundational Learning

- Concept: **KL Divergence and Gibbs Distributions**
  - Why needed here: The entire method hinges on KL(p||q) = Σ p(i) log(p(i)/q(i)) and how it creates closed-form best responses. Without understanding that softmax/softmax-with-temperature = Gibbs distribution under KL regularization, the algorithm's sampling strategy won't make sense.
  - Quick check question: Can you derive why argmax_μ [E[Q] − βKL(μ||μ_ref)] yields a Gibbs distribution proportional to μ_ref · exp(Q/β)?

- Concept: **Regret Bounds and Sample Complexity**
  - Why needed here: The paper's main claim is improving from Õ(√T) to O(log²T) regret. You need to understand what regret measures (cumulative dual gap), how it relates to sample complexity (via regret-to-batch conversion), and why logarithmic regret is "fast" vs sublinear "slow."
  - Quick check question: If an algorithm achieves O(log²T) regret, how many episodes T are needed to reach ε-Nash equilibrium? (Answer: roughly O(1/ε), vs O(1/ε²) for √T regret)

- Concept: **Optimism and UCB Bonuses in RL**
  - Why needed here: Both OMG and SOMG use UCB-style bonuses (∥ϕ∥Σ⁻¹) to construct optimistic estimates that overestimate true Q-values with high probability. The confidence ellipsoid lemma (Lemma A.3) is used repeatedly. Without this, you won't understand why bonuses enable exploration.
  - Quick check question: Why does the standard optimistic bonus b = η∥ϕ∥Σ⁻¹ alone fail in this game setting, necessitating the superoptimistic bonus b_sup = b + 2b_mse?

## Architecture Onboarding

- Component map: Q-function (estimated via regularized least squares) -> NE computation (extragradient methods) -> Best-response computation (closed-form Gibbs) -> Data collection (best-response pairs) -> Bonus computation (covariance matrices)
- Critical path:
  1. Initialize parameters θ, θ⁺, θ⁻ for all h ∈ [H]
  2. For each episode t: backward pass through h=H→1 computing Q, Q⁺, Q⁻ and NE policies
  3. Compute best responses using Q⁺, Q⁻ against NE policies
  4. Sample two trajectories using best-response pairs
  5. Update covariance matrices and parameters via least squares
  6. The logarithmic regret emerges from the squared-bonus summation over all time steps
- Design tradeoffs:
  - Bonus scaling: η₁ = O(√dH) vs η₂ = O(dH²) — larger superoptimistic bonuses enable tighter regret but may slow practical convergence
  - Projection ceiling: [0, 3H²] vs standard [0, H] — larger ceiling handles unbounded values but may introduce numerical instability
  - Computation vs sample efficiency: NE computation uses extragradient (linear convergence) vs unregularized O(1/T); best-response pairs double data collection
- Failure signatures:
  - Bonus explosion: If regularization λ in Σ = λI + Σϕϕ^T is too small, Σ⁻¹ may have large entries and bonuses explode
  - Numerical underflow: Gibbs distribution exp(Q/β) may underflow if β is very small; consider log-space computations
  - Reference policy mismatch: If μ_ref, ν_ref don't have full support, KL divergences become infinite; algorithm will crash or produce NaN
  - Horizon dependence: O(H⁷) in regret bound means long-horizon games may need impractically many episodes
- First 3 experiments:
  1. Matrix game validation: Implement OMG on a small 10×10 payoff matrix with known NE; verify regret scales as O(log²T) not O(√T) by plotting regret vs T
  2. Bonus sensitivity: On a simple Markov game (H=5, small state space), sweep β ∈ {0.1, 1, 10} and check if regret bound O(β⁻¹ log²T) holds empirically
  3. Superoptimism ablation: Compare full SOMG vs variant using only standard optimistic bonuses (no 2×b_mse term); expect the ablated version to fail on games with large KL penalties

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the optimistic best-response sampling methodology be adapted to the offline reinforcement learning setting?
- Basis in paper: [explicit] The authors state in the conclusion that developing "offline counterparts of optimistic best-response sampling" under reasonable coverage assumptions is an open avenue.
- Why unresolved: The proposed OMG and SOMG algorithms are designed exclusively for online learning environments where agents actively explore via bandit feedback.
- What evidence would resolve it: A proposed offline algorithm with provable sample efficiency guarantees under partial coverage assumptions.

### Open Question 2
- Question: Is it possible to extend these logarithmic regret guarantees to general-sum games where optimal policies do not admit closed-form expressions?
- Basis in paper: [explicit] The conclusion lists extending methods to general multi-agent settings (e.g., computing CCE) where "best responses or optimal policies do not admit a closed-form expression" as a promising direction.
- Why unresolved: The theoretical analysis in this paper heavily relies on the specific Gibbs distribution structure of best responses available in KL-regularized zero-sum games.
- What evidence would resolve it: A theoretical proof showing logarithmic regret for a general-sum game algorithm that does not rely on closed-form policy solutions.

### Open Question 3
- Question: Do instance-dependent or gap-dependent regret bounds exist that capture the influence of the reference policy on learning efficiency?
- Basis in paper: [explicit] The authors explicitly call for "deriving instance/gap-dependent regret guarantees... that also capture the dependence on reference policies" in the conclusion.
- Why unresolved: Current regret bounds are worst-case and treat the reference policy as a static parameter without characterizing how its quality affects convergence speed.
- What evidence would resolve it: A regret bound that includes terms scaling with the divergence between the reference policy and the optimal policy.

## Limitations
- The logarithmic regret guarantees rely heavily on the zero-sum game structure and may not extend to general-sum games
- The superoptimistic bonus construction adds significant computational overhead compared to standard optimistic algorithms
- The method requires careful tuning of multiple hyperparameters including bonus scaling, projection bounds, and regularization strength

## Confidence

### Major Uncertainties
The paper's logarithmic regret guarantees rely critically on three conditions: (1) the closed-form best-response expressions under KL regularization must be exact and computable, (2) the superoptimistic bonus must maintain the required gap property in the presence of unbounded value functions, and (3) the confidence ellipsoids must hold with the stated probability across all horizon steps. While the theoretical analysis is rigorous, the practical sensitivity to hyperparameter choices (bonus scaling, projection bounds, regularization strength) remains unclear.

### Confidence Assessment
- **High Confidence**: The core mechanism linking KL regularization to closed-form best responses (Mechanism 1) is well-established and directly verifiable from the Gibbs distribution formula. The matrix game algorithm (OMG) has a relatively straightforward analysis with bounded payoffs.
- **Medium Confidence**: The superoptimistic bonus construction (Mechanism 2) and its theoretical properties (Lemma C.4) are sound, but practical implementation requires careful tuning of the projection bounds and bonus constants. The extension from matrix games to Markov games introduces multiple layers of approximation.
- **Medium Confidence**: The best-response sampling strategy (Mechanism 3) elegantly circumvents the lack of Nash equilibrium closed forms, but its efficiency depends on the assumption that best responses to fixed opponent strategies are "easy" while full Nash equilibria are "hard" - this distinction holds mathematically but may not capture all practical considerations.

## Next Checks

1. **Empirical regret scaling**: Implement OMG on a 10×10 random matrix game and plot cumulative regret vs √T and log²T on log-log scale to verify the theoretical rate.

2. **Bonus sensitivity analysis**: In SOMG, systematically vary β and the superoptimistic bonus coefficient (currently 2×) to identify the threshold where regret transitions from logarithmic to √T scaling.

3. **Best-response approximation error**: Measure the gap between the true best response and its closed-form approximation under finite-precision computation, especially for small β values where the Gibbs distribution becomes sharply peaked.