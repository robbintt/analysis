---
ver: rpa2
title: 'Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic
  Model Checking'
arxiv_id: '2508.00500'
source_url: https://arxiv.org/abs/2508.00500
tags:
- safety
- agent
- runtime
- enforcement
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pro2Guard introduces a proactive runtime enforcement framework
  for LLM agents that anticipates safety violations by learning probabilistic models
  of agent behavior and intervening before violations occur. The core innovation is
  constructing a Discrete-Time Markov Chain from execution traces, then estimating
  the probability of reaching unsafe states at runtime to trigger early interventions
  when risk exceeds a threshold.
---

# Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking

## Quick Facts
- arXiv ID: 2508.00500
- Source URL: https://arxiv.org/abs/2508.00500
- Reference count: 36
- Key outcome: Achieves 100% prediction accuracy for traffic violations with up to 38.66s advance warning and 93.6% safety enforcement on embodied agents

## Executive Summary
Pro2Guard introduces a proactive runtime enforcement framework for LLM agents that anticipates safety violations by learning probabilistic models of agent behavior and intervening before violations occur. The core innovation is constructing a Discrete-Time Markov Chain from execution traces, then estimating the probability of reaching unsafe states at runtime to trigger early interventions when risk exceeds a threshold. Evaluated across autonomous vehicles and embodied agents, Pro2Guard achieves 100% prediction accuracy for traffic law violations and collisions in AV scenarios, with up to 38.66 seconds advance warning, and enforces safety on 93.6% of unsafe embodied agent tasks while maintaining 80.4% task completion.

## Method Summary
Pro2Guard operates by first collecting execution traces from LLM agents in simulation environments, then abstracting these concrete states into symbolic states using domain-specific Boolean predicates. It learns a Discrete-Time Markov Chain (DTMC) from these traces with Laplace smoothing to prevent zero-probability transitions, ensuring the model satisfies PAC (Probably Approximately Correct) bounds for statistical accuracy. At runtime, the system computes probabilistic reachability of unsafe states using model checking (via PRISM) and intervenes with reflective prompts when predicted violation probability exceeds a user-defined threshold, achieving proactive safety enforcement before violations occur.

## Key Results
- 100% prediction accuracy for traffic law violations and collisions in autonomous vehicle scenarios
- Up to 38.66 seconds advance warning before violations occur
- 93.6% safety enforcement rate on embodied agent tasks while maintaining 80.4% task completion
- 5-30ms overhead for embodied agents and 100ms for AV scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning a DTMC from execution traces provides a statistically reliable model of agent behavior for predicting future violations.
- Mechanism: The system abstracts concrete agent states into symbolic states using domain-specific Boolean predicates, then estimates transition probabilities from observed execution traces. Valid-transition-aware Laplace smoothing prevents zero-probability transitions on sparse data while maintaining semantic consistency. The PAC bound determines sufficient sampling size to guarantee |P̂_M(φ) - P_M(φ)| ≤ ε with confidence ≥ 1-δ.
- Core assumption: Past execution traces accurately represent future agent behavior distributions; the abstraction predicates capture all safety-relevant state features.
- Evidence anchors:
  - [abstract] "Pro2Guard abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces."
  - [section 3.2, Definition 3-4] Formalizes DTMC construction and PAC-correctness guarantee with specific sampling bound formula.
  - [corpus] AgentSpec (predecessor) uses reactive rules; corpus lacks comparative DTMC approaches, limiting external validation.
- Break condition: If agent behavior distribution shifts significantly after DTMC training (e.g., new prompts, environment changes), predicted probabilities may diverge from ground truth. The PAC guarantee holds only under the sampled distribution.

### Mechanism 2
- Claim: Probabilistic reachability analysis over the learned DTMC predicts the likelihood of reaching unsafe states before violations occur.
- Mechanism: Given a partial trace π = s₀s₁...sᵢ, the system computes P̂_M[ψ|π] where ψ is a CTL safety property (e.g., AG¬unsafe). For bounded-response properties (common in traffic laws), it constructs a synchronous product of the DTMC with a monitor automaton, reducing "respond within K steps" to reachability of a violation state. PRISM model checker performs the probabilistic computation.
- Core assumption: The abstracted symbolic state space is sufficiently small for tractable model checking while preserving safety-relevant distinctions.
- Evidence anchors:
  - [abstract] "predicts future safety risks using probabilistic reachability analysis and intervenes before violations occur when the predicted risk exceeds a user-defined threshold"
  - [section 4.1, Definition 6] Details monitor automaton construction for K-bounded response properties.
  - [corpus] Corpus lacks papers using probabilistic model checking for LLM agents; related work focuses on symbolic rules or static constraints.
- Break condition: If the DTMC state space grows large (>100 states), model checking complexity may cause unacceptable latency. The paper reports 3-11 states in AV experiments.

### Mechanism 3
- Claim: Threshold-based intervention with reflective prompts steers agents toward safer behaviors while preserving task completion.
- Mechanism: When P_safe < θ (user-defined threshold), Pro2Guard augments the agent's prompt with: (1) current abstracted state, (2) the at-risk property ψ, and (3) the predicted violation probability. The LLM revises its policy based on this risk feedback. Multiple modes available: "stop" halts execution; "reflect" prompts reconsideration; "human-in-the-loop" requests user approval.
- Core assumption: LLMs can meaningfully interpret probabilistic risk alerts and adjust plans accordingly; the intervention timing (based on threshold) allows sufficient reaction time.
- Evidence anchors:
  - [section 3.3] "Pro2Guard augments the trajectory τ≤ᵢ with an alerting prompt containing the current abstract state sᵢ, the about-to-violate requirement ψ, and an explanation"
  - [Table 3] Reflect mode at θ=0.1 achieves 14.07% unsafe rate vs. 40.63% baseline while maintaining 47.74% completion; stop mode achieves 2.60% unsafe but only 10.42% completion.
  - [corpus] ToolSafe and AgenTRIM implement step-level guardrails but lack probabilistic prediction horizons.
- Break condition: If threshold θ is set too high (risk-averse), task completion degrades severely. If too low, safety enforcement weakens. Calibration per domain is required.

## Foundational Learning

- Concept: **Discrete-Time Markov Chains (DTMCs)**
  - Why needed here: Core mathematical model for capturing stochastic agent behavior; enables probabilistic prediction of state transitions.
  - Quick check question: Given states {A, B, C} and transition probabilities P(A→B)=0.3, P(A→C)=0.7, what is the probability of reaching C from A in exactly 2 steps?

- Concept: **Computation Tree Logic (CTL) and Probabilistic Model Checking**
  - Why needed here: Formal language for specifying safety properties (e.g., AG¬unsafe = "globally, unsafe state never reached"); PRISM computes satisfaction probabilities over DTMCs.
  - Quick check question: What does AF φ mean vs. EF φ? Which is appropriate for "eventually unsafe state is always reached regardless of path"?

- Concept: **PAC (Probably Approximately Correct) Learning Bounds**
  - Why needed here: Provides statistical guarantees that learned transition probabilities are within ε of true values with confidence 1-δ; determines minimum trace samples needed.
  - Quick check question: If ε=0.1 and δ=0.05, what does this guarantee say about the relationship between estimated and true violation probabilities?

## Architecture Onboarding

- Component map:
  - Trace Collector -> Abstraction Module -> DTMC Learner -> Monitor Automaton -> Probability Engine (PRISM) -> Intervention Dispatcher

- Critical path:
  1. **Offline**: Collect traces → Abstract to symbolic states → Learn DTMC → Validate PAC criterion
  2. **Runtime**: Observe state → Abstract → Query probability → If P_safe < θ: intervene
  3. Latency-critical step: Probability lookup (5-30ms via caching; 430ms without caching)

- Design tradeoffs:
  - **Abstraction granularity**: More predicates → finer safety distinctions but larger state space → slower inference
  - **Threshold θ**: Lower → safer but lower task completion; higher → more permissive
  - **Intervention mode**: "stop" = maximum safety, minimum completion; "reflect" = balanced; "human" = accountable but slow
  - **Sampling vs. speed**: More traces → better PAC guarantees but longer training time

- Failure signatures:
  - **False positives (over-cautious)**: Threshold too low; agent frequently interrupted; task completion drops
  - **False negatives (missed violations)**: Distribution shift after training; abstraction omits relevant predicates; PAC bound insufficient
  - **Latency spikes**: State space unexpectedly large; caching not configured; synchronous product explosion
  - **Unresponsive intervention**: LLM ignores risk alerts; prompt format ineffective for specific model

- First 3 experiments:
  1. **Validate DTMC learning**: Run agent on held-out traces; compare predicted transition frequencies vs. observed; verify PAC bound holds (histogram of |P̂ - P| across states).
  2. **Threshold calibration**: Sweep θ ∈ {0.1, 0.3, 0.5, 0.7} on validation tasks; plot unsafe% vs. completion% curve; identify Pareto-optimal point for target use case.
  3. **Ablate intervention modes**: Compare "stop" vs. "reflect" vs. "human-in-the-loop" on same task distribution; measure safety improvement, task completion, and wall-clock overhead per mode.

## Open Questions the Paper Calls Out
None

## Limitations
- Distribution shift vulnerability: The DTMC-based prediction assumes stationarity of agent behavior post-training, but real LLM agents may exhibit significant behavioral drift when prompted with new inputs or encountering novel environments.
- Abstraction fidelity: The symbolic abstraction relies on hand-crafted Boolean predicates that must capture all safety-relevant state features. Missing predicates create blind spots where violations occur in the concrete space but are abstracted away.
- Generalization across domains: All empirical validation occurs within controlled simulation environments (CARLA, CoppeliaSim). The framework's performance on open-world, unstructured environments with complex state spaces remains untested.

## Confidence

**High Confidence**: The probabilistic model checking mechanism has strong theoretical foundations in formal verification literature. The PRISM tool has decades of validation for DTMC analysis, and the synchronous product construction for bounded properties follows established patterns. The 100% prediction accuracy on AV violations is empirically demonstrated with clear methodology.

**Medium Confidence**: The proactive intervention mechanism shows promise but depends heavily on LLM responsiveness to risk prompts. While the paper reports 93.6% enforcement rate, this assumes the LLM will meaningfully revise plans when alerted. The effectiveness may vary significantly across different LLM architectures and prompt engineering approaches.

**Low Confidence**: The PAC correctness guarantee provides theoretical bounds, but practical sufficiency depends on unknown factors like state space complexity and transition dynamics. The 0.2% training data used suggests efficiency, but may not provide robust guarantees across diverse agent behaviors. The claim of reduced manual effort versus symbolic rules lacks quantitative comparison.

## Next Checks
1. **Distribution Shift Experiment**: Retrain Pro2Guard on AV traces from one city (e.g., urban) and evaluate on traces from a structurally different city (e.g., suburban). Measure degradation in prediction accuracy and violation detection rates to quantify sensitivity to behavioral distribution changes.

2. **State Space Scalability Test**: Implement Pro2Guard on an agent with 1000+ symbolic states (e.g., complex gridworld with multiple objects). Measure model checking latency, memory usage, and whether the 5-30ms overhead bound holds. Compare against alternative monitoring approaches.

3. **Cross-LLM Intervention Validation**: Deploy the same Pro2Guard configuration (thresholds, predicates, intervention prompts) across three different LLM architectures (e.g., GPT-4, Claude, Llama). Quantify variation in task completion rates and safety enforcement to assess dependency on specific model characteristics.