---
ver: rpa2
title: 'NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes
  Understanding'
arxiv_id: '2504.09249'
source_url: https://arxiv.org/abs/2504.09249
tags:
- document
- arxiv
- reasoning
- question
- handwritten
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "NoTeS-Bank addresses the challenge of multimodal reasoning in\
  \ handwritten academic notes, where current models struggle with unstructured layouts,\
  \ mathematical notation, and visual diagrams. It introduces two tasks\u2014Evidence-Based\
  \ VQA (answering with bounding-box evidence) and Open-Domain VQA (domain classification\
  \ plus retrieval across notes)\u2014evaluated using metrics like ANLS, IoU, MRR,\
  \ and NDCG@5."
---

# NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding

## Quick Facts
- arXiv ID: 2504.09249
- Source URL: https://arxiv.org/abs/2504.09249
- Authors: Aniket Pal; Sanket Biswas; Alloy Das; Ayush Lodh; Priyanka Banerjee; Soumitri Chattopadhyay; Dimosthenis Karatzas; Josep Llados; C. V. Jawahar
- Reference count: 40
- Primary result: Introduces a benchmark exposing modality gaps in current document AI for handwritten academic notes

## Executive Summary
NoTeS-Bank addresses the challenge of multimodal reasoning in handwritten academic notes, where current models struggle with unstructured layouts, mathematical notation, and visual diagrams. It introduces two tasks—Evidence-Based VQA (answering with bounding-box evidence) and Open-Domain VQA (domain classification plus retrieval across notes)—evaluated using metrics like ANLS, IoU, MRR, and NDCG@5. The benchmark includes 19 scientific domains and 8 local content categories, requiring joint visual-language reasoning. Experiments show significant performance gaps for VLMs, OCR+LLM, and retrieval-augmented models compared to human experts, with evidence localization and domain inference being particularly challenging. NoTeS-Bank reveals the modality gap in current document AI and sets a new standard for multimodal document understanding.

## Method Summary
The benchmark evaluates models on two tasks: Evidence-Based VQA requiring bounding-box evidence for answers, and Open-Domain VQA combining domain classification with cross-note retrieval. Models are assessed using ANLS (Answer-based Normalized Levenshtein Similarity), IoU for evidence localization, and MRR/NDCG@5 for retrieval performance. The dataset covers 19 scientific domains with 8 content categories including mathematical notation and diagrams, requiring models to perform joint visual-language reasoning on unstructured handwritten notes.

## Key Results
- Current VLMs, OCR+LLM, and retrieval-augmented models show significant performance gaps compared to human experts in multimodal reasoning on scientific notes
- Evidence localization and domain inference are particularly challenging tasks, revealing fundamental modality gaps in document AI
- The benchmark sets a new standard for multimodal document understanding with its focus on unstructured handwritten academic content

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on multimodal reasoning challenges inherent in scientific note-taking, where visual layout, mathematical notation, and domain-specific content must be integrated for accurate understanding. The dual-task structure (Evidence-Based and Open-Domain VQA) forces models to develop capabilities in both precise localization and broader domain reasoning, while the 19 scientific domains ensure comprehensive coverage of academic knowledge. The combination of ANLS, IoU, and retrieval metrics provides a holistic evaluation framework that captures both accuracy and the quality of evidence-based reasoning.

## Foundational Learning
- Visual Layout Understanding: Required to parse unstructured handwritten notes with complex spatial arrangements; quick check: test model on varied note-taking styles
- Mathematical Notation Processing: Essential for interpreting equations and symbols common in academic notes; quick check: evaluate on notes with varying mathematical complexity
- Domain-Specific Knowledge Integration: Needed to reason across 19 scientific domains; quick check: measure performance degradation when domain context is removed
- Multimodal Evidence Localization: Critical for bounding-box evidence tasks; quick check: assess localization accuracy across different content types
- Cross-Note Retrieval: Required for Open-Domain VQA task; quick check: test retrieval performance with varying note similarity
- Visual-Language Joint Reasoning: Fundamental for answering questions requiring both visual and textual understanding; quick check: compare performance on pure text vs. multimodal questions

## Architecture Onboarding

Component map: OCR/OCR+LLM/Visual Language Models -> Evidence Localization Module -> Domain Classification -> Retrieval Engine -> Evaluation Metrics

Critical path: Document Input -> Visual Preprocessing -> Text Extraction (OCR) -> Multimodal Fusion -> Question Answering/Retrieval -> Evidence Generation -> Evaluation

Design tradeoffs: The benchmark balances between requiring precise evidence localization (IoU) and flexible retrieval performance (MRR/NDCG@5), but may favor models with strong text understanding over pure visual reasoning.

Failure signatures: Poor performance on mathematical notation indicates weakness in symbolic reasoning; low IoU scores reveal difficulties in spatial understanding; domain misclassification suggests insufficient knowledge integration.

First experiments: 1) Evaluate baseline OCR performance on scientific notation; 2) Test VLM performance on isolated evidence localization; 3) Assess retrieval accuracy across different scientific domains.

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the generalizability of the benchmark across different languages and cultural note-taking styles, the potential for transfer learning between scientific domains, and the scalability of the approach to larger document collections. Additionally, it questions how well current multimodal models can handle the increasing complexity of scientific notation and whether the benchmark can evolve to include emerging scientific domains.

## Limitations
- Evaluation setup relies on human expert annotations without reported inter-annotator agreement statistics
- Performance gaps between models and humans lack confidence intervals or statistical significance tests
- Benchmark focuses exclusively on English-language scientific notes, limiting generalizability
- The complexity of scientific notation may create an artificial barrier for models that perform well on general document understanding tasks
- Limited coverage of non-Western scientific domains and note-taking conventions

## Confidence
- Claim: Current VLMs, OCR+LLM, and retrieval-augmented models show significant performance gaps compared to humans in multimodal reasoning on scientific notes (Medium)
- Claim: Evidence localization and domain inference are particularly challenging tasks (High)
- Claim: NoTeS-Bank sets a new standard for multimodal document understanding benchmarks (Medium)

## Next Checks
1. Report inter-annotator agreement statistics and confidence intervals for all human expert annotations to establish reliability of the ground truth
2. Conduct ablation studies isolating the impact of visual layout understanding, mathematical notation, and domain-specific knowledge on model performance
3. Test model performance across different note-taking styles and languages to assess generalizability beyond the current English scientific domain scope