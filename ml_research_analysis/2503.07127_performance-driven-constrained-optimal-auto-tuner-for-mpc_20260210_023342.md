---
ver: rpa2
title: Performance-driven Constrained Optimal Auto-Tuner for MPC
arxiv_id: '2503.07127'
source_url: https://arxiv.org/abs/2503.07127
tags:
- coat-mpc
- parameters
- performance
- function
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tuning Model Predictive Control
  (MPC) cost function parameters while ensuring system performance stays above a certain
  threshold. The authors propose COAT-MPC, a constrained optimal auto-tuner that uses
  Gaussian processes to model the unknown performance function and explores the parameter
  space while satisfying performance constraints.
---

# Performance-driven Constrained Optimal Auto-Tuner for MPC

## Quick Facts
- arXiv ID: 2503.07127
- Source URL: https://arxiv.org/abs/2503.07127
- Reference count: 30
- One-line primary result: COAT-MPC ensures MPC cost function tuning satisfies performance constraints with high probability while converging to optimal parameters efficiently.

## Executive Summary
This paper addresses the challenge of tuning Model Predictive Control (MPC) cost function parameters while ensuring system performance stays above a certain threshold. The authors propose COAT-MPC, a constrained optimal auto-tuner that uses Gaussian processes to model the unknown performance function and explores the parameter space while satisfying performance constraints. The method maintains pessimistic and optimistic estimates of the safe parameter set, exploring toward optimistic estimates while only sampling from pessimistic ones, ensuring performance constraints are satisfied with high probability while efficiently converging to optimal parameters.

## Method Summary
COAT-MPC uses a Gaussian Process to model the unknown performance function of MPC cost function parameters. It maintains two sets: a pessimistic safe set $S_n^p$ (guaranteed to satisfy performance constraints) and an optimistic set $S_n^{o,\epsilon}$ (potentially safe). The algorithm selects a "goal" parameter from the optimistic set with the highest predicted performance, then samples the point in the pessimistic set closest to this goal but with highest uncertainty. This Goal-Oriented expansion strategy ensures efficient exploration while maintaining safety guarantees through Lipschitz continuity assumptions. The method terminates when the confidence interval around the optimal parameter is sufficiently narrow.

## Key Results
- COAT-MPC achieves 0 constraint violations vs. 1.8-16.4 violations for baseline methods in autonomous racing experiments
- The method converges in 20-30 iterations compared to 70 iterations for baselines
- COAT-MPC achieves the lowest cumulative regret in experiments on both simulation and a 1:28 scale RC racecar platform
- Theoretical analysis shows finite-time convergence with sample complexity bounds that improve upon previous work

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** COAT-MPC ensures safety by strictly separating the parameter space into pessimistic (safe) and optimistic (potentially safe) sets.
- **Mechanism:** The algorithm constructs confidence intervals $[l_n(\theta), u_n(\theta)]$ using a Gaussian Process (GP). It defines a "pessimistic" set $S_n^p$ using lower bounds to guarantee the performance constraint $q(\theta) \geq \tau$ holds (with high probability). It only samples parameters from this pessimistic set, while using the "optimistic" set $S_n^{o,\epsilon}$ to identify promising regions for future expansion.
- **Core assumption:** The performance function $q$ is $L$-Lipschitz continuous (Assumption 2) and has bounded RKHS norm, allowing local confidence bounds to propagate safety guarantees to neighboring parameters.
- **Evidence anchors:**
  - [abstract] "...maintain pessimistic and optimistic estimates... while only sampling from pessimistic ones."
  - [section IV-B] Definitions of $p_n(S)$ and $o_n^\epsilon(S)$ operators relying on Lipschitz continuity.
  - [corpus] Corpus neighbors indicate BO is standard for MPC tuning, but this method specifically targets the *constraint* mechanism lacking in standard BO.
- **Break condition:** If the Lipschitz constant $L$ is underestimated, the safety propagation fails, and the algorithm may sample "safe" parameters that actually violate constraints.

### Mechanism 2
- **Claim:** The method achieves sample efficiency by using a "Goal-Oriented" expansion strategy rather than exploring the entire safe space.
- **Mechanism:** Instead of exploring all reachable safe parameters (like SafeOpt), COAT-MPC selects a "goal" parameter $\theta_n^g$ with the highest predicted performance (UCB) in the optimistic set. It then recommends sampling the point in the current *pessimistic* set that is closest to this goal but still has high uncertainty (information gain).
- **Core assumption:** The optimal parameter is reachable from the initial seed $S_0$ via a path of safe parameters (Assumption 1).
- **Evidence anchors:**
  - [section V] "The objective is to reach the goal while ensuring that sufficient information is gained for exploration."
  - [algorithm 1] "Recommend: $\arg\min_{\theta \in S_{n-1}^p} ||\theta_n^g - \theta||^2$"
  - [corpus] "Centrum" neighbor mentions rethinking GP-BO efficiency, aligning with COAT-MPC's focus on improving sample complexity bounds.
- **Break condition:** If the initial seed $S_0$ is isolated from the global optimum by a region of unsafe parameters (performance $< \tau$), the algorithm will converge to a local optimum inside the reachable safe set $S^{q,\epsilon}$.

### Mechanism 3
- **Claim:** Finite-time convergence is guaranteed by a termination criterion based on uncertainty reduction.
- **Mechanism:** The algorithm terminates when the "goal" parameter lies inside the pessimistic set AND the confidence interval width $w_n(\theta_g) < \epsilon$. This ensures the performance of the recommended parameter is known to within $\epsilon$ of the true optimum reachable set.
- **Core assumption:** The information capacity $\gamma_n$ grows sublinearly (Assumption 3), which holds for standard kernels (Matérn, SE).
- **Evidence anchors:**
  - [section VI] Theorem 1 guarantees finite time convergence: $\exists n \le n^*$ satisfying the optimality condition.
  - [section V] Line 5 of Algorithm 2 defines the termination condition.
- **Break condition:** If the noise $\sigma_\eta$ is very high or the kernel hyperparameters (length-scale) are misspecified, the confidence bounds may fail to shrink below $\epsilon$, preventing termination.

## Foundational Learning

- **Concept: Gaussian Processes (GPs) for Bayesian Optimization**
  - **Why needed here:** GPs provide the posterior distribution (mean $\mu_n$ and variance $\sigma_n^2$) required to construct the upper/lower confidence bounds (Section IV-A). Without understanding GPs, the mechanism for defining "safe" vs. "unsafe" parameters is opaque.
  - **Quick check question:** How does the choice of kernel length-scale affect the size of the pessimistic safe set? (Hint: A shorter length-scale implies less correlation between distant points, potentially slowing safe expansion).

- **Concept: Model Predictive Control (MPC) Tuning**
  - **Why needed here:** The "parameters" $\theta$ being tuned are the cost function weights (e.g., $Q_{contour}, Q_{lag}$ in Eq. 9). Understanding that these weights trade off speed vs. stability is necessary to define the performance threshold $\tau$ (e.g., maximum allowable lap time).
  - **Quick check question:** If the MPC cost function weights are set to zero, what happens to the control signal, and would this likely satisfy the performance constraint?

- **Concept: Lipschitz Continuity**
  - **Why needed here:** This is the mathematical glue for safety. It assumes that if parameter $\theta$ is safe, a nearby parameter $\theta'$ is *probably* safe because the performance function cannot change arbitrarily fast (Section IV-B).
  - **Quick check question:** Why is the Lipschitz constant $L$ subtracted in the safe set operator $q(\theta') - L d(\theta, \theta') \ge \tau$?

## Architecture Onboarding

- **Component map:** Initial Seed ($S_0$) -> GP Model -> Set Operators -> Acquisition (Algorithm 1) -> MPC Controller
- **Critical path:** The **Constrained Expansion (Algorithm 1)**. This logic determines *where* to sample. If this component correctly balances "moving toward the goal" and "reducing uncertainty," the system converges. If it gets stuck (e.g., cannot find a point satisfying $w_n(\theta) \ge \epsilon$), the loop stalls.
- **Design tradeoffs:**
  - **Discretization Resolution:** The paper discretizes the 2D parameter space into 10,000 points. Finer resolution improves optimal parameter precision but increases the computational cost of calculating set expansions ($S_n^p$, etc.) at every iteration.
  - **Conservativeness ($\beta_n$):** High $\beta$ (scaling factor for confidence bounds) ensures safety but slows convergence (larger confidence intervals take longer to shrink below $\epsilon$).
- **Failure signatures:**
  - **Stalling:** Algorithm runs indefinitely without termination. Likely cause: Confidence bounds $w_n(\theta)$ are not shrinking, possibly due to high process noise or a poorly specified GP kernel.
  - **Constraint Violations:** The car crashes or lap times exceed $\tau$. Likely cause: The assumed Lipschitz constant $L$ was too small, or the discretization was too coarse, failing to capture a sharp drop in performance between two adjacent grid points.
- **First 3 experiments:**
  1.  **Sanity Check (1D Simulation):** Implement the safe expansion logic on a simple 1D function (e.g., $f(x) = \sin(x)$) to visualize the expansion of $S_n^p$ and verify it matches Figure 3.
  2.  **Ablation on Initial Seed:** Run the tuner with a very small $S_0$ vs. a large $S_0$. Quantify the difference in iterations required to reach the optimum to verify the dependency on the "reachable set" size.
  3.  **Hardware in Loop (HIL) Noise Test:** Introduce state estimation noise in the simulation (mimicking the RC car) and verify if the theoretical $\beta_n$ scaling factor (Corollary 1) is sufficient to prevent violations, or if heuristic tuning of $\beta$ is required.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can COAT-MPC be extended to efficiently handle high-dimensional parameter spaces without suffering from exponential complexity in computing pessimistic and optimistic sets?
- Basis in paper: [explicit] "Finally, an interesting line of future work could be to extend the method to work in high-dimensional parameter space. Our approach discretizes the parameter space to compute the pessimistic and optimistic sets, which leads to exponential space complexity."
- Why unresolved: The current discrete domain approach requires computing sets over a grid, which scales exponentially with parameter dimension.
- What evidence would resolve it: A modified algorithm demonstrating sub-exponential or polynomial scaling in parameter dimension while preserving safety guarantees, tested on problems with 5+ tuning parameters.

### Open Question 2
- Question: How can COAT-MPC be adapted to handle multiple simultaneous performance constraints rather than a single threshold?
- Basis in paper: [inferred] The formulation (Eq. 2) addresses only one constraint $q(\theta) \geq \tau$. Many real-world MPC applications require satisfying multiple competing constraints simultaneously (e.g., tracking accuracy, energy consumption, and actuator limits).
- Why unresolved: The pessimistic/optimistic set construction relies on a scalar performance function; extending to vector-valued constraints would require fundamentally different set operators.
- What evidence would resolve it: Theoretical analysis showing constraint satisfaction bounds for multiple constraints, plus empirical validation on tasks with two or more constrained objectives.

### Open Question 3
- Question: How sensitive is COAT-MPC's safety guarantee to misspecification of the Lipschitz constant L?
- Basis in paper: [inferred] The safe exploration operators ($p_n$, $o_n^\epsilon$) rely explicitly on the Lipschitz constant L to propagate safety guarantees between parameters. The paper assumes L is known but provides no guidance on estimation or robustness to incorrect values.
- Why unresolved: Overestimating L leads to overly conservative exploration, while underestimating could violate safety guarantees—yet the paper does not analyze this trade-off.
- What evidence would resolve it: Sensitivity analysis showing violation rates and convergence speed as L varies, or an adaptive/robust method that handles unknown L.

### Open Question 4
- Question: Can COAT-MPC transfer learned performance models across different environments or tracks without restarting from the initial seed?
- Basis in paper: [inferred] The experiments evaluate only a single track, and the introduction notes "the cost function parameters often depend on the specific environment and system dynamics, making it difficult to design a single set of parameters that can perform well in all scenarios."
- Why unresolved: The method learns from scratch each time; no mechanism exists to leverage prior knowledge when conditions change.
- What evidence would resolve it: Multi-track experiments showing reduced iterations or cumulative regret when transferring GP hyperparameters or safe set priors between environments.

## Limitations

- The method requires discretizing the parameter space, leading to exponential space complexity in the number of parameters, making it challenging to scale to high-dimensional tuning problems.
- The Lipschitz constant L must be specified or estimated; misspecification could lead to either overly conservative exploration or unsafe behavior.
- The method assumes the optimal parameter is reachable from the initial safe seed through a path of safe parameters, which may not hold if the performance landscape contains disconnected safe regions.

## Confidence

- **High**: Theoretical safety guarantees (finite-time convergence, constraint satisfaction with high probability)
- **Medium**: Practical safety under parameter uncertainty, experimental generalizability
- **Low**: Exact parameter values for Lipschitz constant and termination threshold

## Next Checks

1. Implement the algorithm on a simple 1D benchmark function to verify the safe expansion logic matches Figure 3 and test sensitivity to Lipschitz constant misspecification
2. Run an ablation study varying the initial safe seed size to quantify the impact on convergence speed and verify the "reachable set" dependency
3. Test the algorithm with injected state estimation noise to verify whether the theoretical $\beta_n$ scaling is sufficient or requires empirical tuning for hardware deployment