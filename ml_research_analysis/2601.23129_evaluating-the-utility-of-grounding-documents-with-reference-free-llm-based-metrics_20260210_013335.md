---
ver: rpa2
title: Evaluating the Utility of Grounding Documents with Reference-Free LLM-based
  Metrics
arxiv_id: '2601.23129'
source_url: https://arxiv.org/abs/2601.23129
tags:
- grogu
- utility
- documents
- gold
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GROGU, a reference-free metric for evaluating
  the utility of grounding documents in RAG systems. GROGU defines utility as the
  change in LLM generation confidence (measured via entropy) when conditioning on
  documents versus not.
---

# Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics

## Quick Facts
- arXiv ID: 2601.23129
- Source URL: https://arxiv.org/abs/2601.23129
- Reference count: 16
- Primary result: GROGU metric enables annotation-free DPO training of query-rewriters, achieving 13.5-18.2 point MRR gains and 5.6-9.4 point accuracy improvements across multiple retrievers and benchmarks.

## Executive Summary
This paper introduces GROGU, a reference-free metric for evaluating the utility of grounding documents in RAG systems. Unlike traditional relevance scores, GROGU measures the change in LLM generation confidence (via entropy) when conditioning on documents versus not. The method defines "utility" as how much a document improves the LLM's confidence in generating correct answers, capturing nuances like different utilities for different LLMs. GROGU enables training query-rewriters using Direct Preference Optimization without requiring any annotated labels, addressing the key bottleneck of expensive gold-passage annotations.

## Method Summary
GROGU computes grounding utility by generating a grounded answer greedily, then measuring the entropy change for each token when conditioning on documents versus not. Tokens where entropy change exceeds a threshold (α=0.05) are marked as "key tokens," and their average entropy difference becomes the GROGU score. The method uses KeyEntropy (entropy on key tokens) as the primary metric. For training query-rewriters, GROGU scores are computed for multiple rewrites, the highest-scoring rewrite is used for SFT warmup, and highest/lowest pairs are used for DPO training with 50% of pairs filtered by smallest GROGU gaps. The approach is evaluated on TopiOCQA and QReCC conversational datasets using BM25 and ANCE retrievers.

## Key Results
- KeyEntropy outperforms plain entropy and perplexity metrics in identifying gold documents versus distractors (77.9-95.8% win rates vs 74.7-91.5% for baseline entropy)
- GROGU captures utility differences that relevance scores miss: less relevant documents can provide more utility for certain LLMs
- Query-rewriter trained with GROGU-based DPO achieves 13.5-18.2 point MRR improvements and 5.6-9.4 point accuracy gains over baselines
- Model-specificity confirmed: documents selected by one LLM's GROGU score may not be optimal for another LLM

## Why This Works (Mechanism)

### Mechanism 1: Key Token Selection Filters Noise
Isolating "key tokens" yields more robust utility estimates than averaging over all generated tokens. The method generates a grounded answer greedily, compares per-token entropy with and without grounding document, and marks tokens with |H(y_i|q,D) - H(y_i|q)| > α as key tokens. Only these contribute to the utility score. This filters out tokens unaffected by the document, focusing on actual information gain.

### Mechanism 2: Model-Specific Utility Captures LLM-Dependent Document Value
The same document can have different utility for different LLMs, which relevance scores cannot capture. GROGU is computed per-model, measuring confidence changes specific to that model's architecture and capabilities. A document that helps a 14B model may confuse a 1.5B model. This captures the true utility for the specific generator being used.

### Mechanism 3: GROGU Enables Annotation-Free DPO Training
GROGU replaces gold-passage labels for constructing preference pairs in Direct Preference Optimization. Multiple query rewrites are generated, GROGU scores computed for each, and highest/lowest GROGU rewrites paired for training. This eliminates the need for expensive annotated labels while still providing meaningful preference signals for improving retrieval components.

## Foundational Learning

- **Entropy as Uncertainty/Confidence**: GROGU uses entropy H = -Σ p_j log(p_j) to quantify model confidence; lower entropy = higher confidence in a token. Quick check: If a model assigns probability 0.9 to one token and 0.1 to all others, is entropy high or low? (Answer: Low)

- **Direct Preference Optimization (DPO)**: The paper uses DPO to train a query-rewriter from preference pairs constructed via GROGU, without a separate reward model. Quick check: How does DPO differ from RLHF in terms of requiring an explicit reward model? (Answer: DPO doesn't need a separate reward model)

- **RAG Pipeline Components**: GROGU is positioned as a metric for tuning RAG components; understanding the pipeline clarifies where utility matters. Quick check: In a conversational RAG system, why might the original user query be insufficient for retrieval? (Answer: Original queries may be too short or ambiguous; rewriting can add context)

## Architecture Onboarding

- Component map:
Query → [Query-Rewriter] → Rewritten Query → [Retriever] → Top-K Documents → [Generator LLM] ← Grounding Context → GROGU Scorer (KeyEntropy) → Utility Score → [DPO Trainer] ← Preference Pairs

- Critical path:
  1. Generate grounded answer with greedy decoding
  2. Compute token-level entropy with and without grounding document
  3. Identify key tokens (entropy change > α, or top-K% fallback)
  4. Average key-token entropy → GROGU score
  5. For training: pair rewrites by GROGU gap, filter, apply DPO

- Design tradeoffs:
  - **α threshold vs. K% fallback**: Lower α selects more tokens (more signal but more noise); higher α is stricter but may yield too few key tokens. Paper uses α=0.05, K=10%.
  - **Entropy vs. Perplexity**: Entropy-based metrics outperformed perplexity in gold-document identification (Table 1).
  - **Same vs. different model for scoring and generation**: Using the same model for utility scoring and answer generation yields best results (Table 4); but larger models can score for smaller rewriter models (Appendix F.2).

- Failure signatures:
  - **Low GROGU gap between rewrites**: Filter out pairs with small gaps (paper removes 50%) to avoid noisy training signal.
  - **No key tokens identified**: Falls back to top-K% highest-entropy tokens; if these are uninformative, utility score degrades.
  - **Cross-model utility mismatch**: Using model A's GROGU to select documents for model B yields lower accuracy than model B selecting its own.

- First 3 experiments:
  1. **Validate KeyEntropy vs. baseline metrics**: On NQ/SQuAD, measure win rates of gold documents vs. distractors using Entropy, KeyEntropy, PPL, KeyPPL. Replicate Table 1 to confirm KeyEntropy advantage.
  2. **Test "less relevance, more utility" phenomenon**: Replicate Section 4.2 setup—compare 1-gold+4-random vs. 1-gold+4-retrieved contexts; verify KeyEntropy correlates with accuracy (τ > 0) while relevance scores do not.
  3. **End-to-end query-rewriter training**: On TopiOCQA/QReCC, train a query-rewriter using GROGU-based DPO (following Section 5). Compare MRR and accuracy against Base SFT and RetPO baselines; expect 10-18 point MRR gain.

## Open Questions the Paper Calls Out

None

## Limitations

- **Model-specificity boundary**: Limited experimental evidence for model-specific utility effects (only two model pairs tested).
- **Generalizability of α threshold**: Fixed α=0.05 may not work across different domains, languages, or generation styles.
- **End-to-end validation scope**: Strong results limited to conversational datasets; effectiveness on non-conversational QA tasks unproven.

## Confidence

- **High confidence**: GROGU effectively differentiates document utility better than relevance scores for tested tasks; KeyEntropy variant consistently outperforms baselines.
- **Medium confidence**: GROGU enables effective annotation-free DPO training for query-rewriting with strong MRR/accuracy gains, but limited to conversational datasets.
- **Low confidence**: Claim that GROGU captures "nuances like different utilities for different LLMs" is weakly supported with only two model comparisons.

## Next Checks

1. **Cross-task generalization test**: Apply GROGU-based DPO to non-conversational datasets like Natural Questions or SQuAD to verify 10-18 point MRR gains transfer to factoid QA.

2. **Model-specificity stress test**: Systematically evaluate GROGU across 4-5 diverse LLMs (1.5B to 70B, different architectures) to quantify how often model A's GROGU selection outperforms model B's own selection.

3. **Ablation of the entropy mechanism**: Replace entropy-based utility with alternative confidence measures (self-consistency or logit entropy) and compare performance on gold-vs-distractor discrimination task to determine if specific entropy calculation is essential.