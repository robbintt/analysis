---
ver: rpa2
title: Do LLM Evaluators Prefer Themselves for a Reason?
arxiv_id: '2504.03846'
source_url: https://arxiv.org/abs/2504.03846
tags:
- llama-3
- qwen2
- self-preference
- reasoning
- evaluator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  harmful self-preference bias when evaluating their own outputs compared to those
  from other models. Prior work showed LLMs tend to favor their own responses, but
  it was unclear if this was due to bias or objective quality differences.
---

# Do LLM Evaluators Prefer Themselves for a Reason?

## Quick Facts
- arXiv ID: 2504.03846
- Source URL: https://arxiv.org/abs/2504.03846
- Authors: Wei-Lin Chen; Zhepei Wei; Xinyu Zhu; Shi Feng; Yu Meng
- Reference count: 40
- Primary result: LLM evaluators show self-preference bias, but much of it is legitimate; harmful bias persists when models err and can be mitigated through reasoning-enhanced evaluation

## Executive Summary
This paper investigates whether large language models exhibit harmful self-preference bias when evaluating their own outputs versus those from other models. Through large-scale experiments on verifiable benchmarks (mathematical reasoning, factual knowledge, and code generation), the authors demonstrate that stronger models exhibit more self-preference, but much of this preference aligns with objectively superior performance. The study introduces a decomposition of self-preference into legitimate (quality-based) and harmful (bias-based) components, showing that harmful bias is particularly pronounced when evaluator models err as generators. The authors find that inference-time scaling strategies, particularly generating long Chain-of-Thought reasoning before evaluation, effectively reduce harmful self-preference. These findings were validated on real-world subjective tasks from LMArena.

## Method Summary
The study uses LLM-as-a-judge pairwise evaluation across 11 judge models and 7 evaluatee models on three verifiable benchmarks: MATH500, MMLU, and MBPP+. The methodology involves generating responses from all models, then having each evaluator compare its own response against evaluatee responses using a pairwise comparison format. Position bias is mitigated through order-swapping aggregation. The key innovation is decomposing overall self-preference (SPR) into Legitimate Self-Preference Ratio (LSPR) and Harmful Self-Preference Propensity (HSPP) using ground-truth correctness labels. The study also tests inference-time scaling strategies, comparing standard evaluation with Chain-of-Thought and long Chain-of-Thought reasoning modes, particularly using DeepSeek-R1-Distill models.

## Key Results
- Stronger models exhibit more self-preference, but much of this preference aligns with objectively superior performance (legitimate preference)
- When evaluator models err as generators, stronger models show more pronounced harmful self-preference bias (HSPP up to 86% for Qwen2.5-72B on MATH500)
- Inference-time scaling strategies, particularly long Chain-of-Thought reasoning, significantly reduce harmful self-preference
- Pearson correlation between task accuracy and judge accuracy is strong (r = 0.795 for MATH500, 0.708 for MMLU, 0.899 for MBPP+)
- Findings validated on real-world subjective tasks from LMArena

## Why This Works (Mechanism)

### Mechanism 1: Legitimate Self-Preference via Capability Correlation
Stronger models' self-preference often reflects genuinely superior outputs rather than bias because task performance (generator accuracy) positively correlates with evaluation accuracy. As models improve at solving problems correctly, they also improve at recognizing correct solutions—including their own. The self-preference signal is partially grounded in objective quality detection.

### Mechanism 2: Harmful Self-Preference Amplification Under Error
When models err as generators, stronger models exhibit more pronounced harmful self-preference than weaker models due to higher confidence calibration that becomes overconfidence when wrong. Higher capability models fail to recognize their own incorrect outputs, persisting in self-favoring behavior even when objectively outperformed.

### Mechanism 3: Reasoning-Mediated Bias Reduction
Extended Chain-of-Thought generation before evaluation reduces harmful self-preference by forcing deeper analysis that creates opportunities for self-correction. The additional compute enables models to reconsider initial impressions, identify their own errors, and more fairly evaluate alternatives.

## Foundational Learning

- **LLM-as-a-Judge pairwise evaluation**: Why needed here: The entire methodology hinges on models comparing two responses anonymously. Understanding position bias handling and three-way verdicts (A/B/tie) is essential.
  - Quick check question: Given responses y_A and y_B, how would you aggregate verdicts from two orderings to mitigate position bias?

- **Verifiable benchmarks with ground truth**: Why needed here: The paper's key innovation is distinguishing harmful from legitimate self-preference using objective correctness (math solutions, code tests, factual answers).
  - Quick check question: Why can't subjective tasks (summarization, dialogue) distinguish legitimate from harmful self-preference?

- **Self-Preference Ratio (SPR) vs Legitimate Self-Preference Ratio (LSPR) vs Harmful Self-Preference Propensity (HSPP)**: Why needed here: These three metrics decompose overall self-preference into distinct phenomena requiring different interpretations and interventions.
  - Quick check question: If SPR = 55% but LSPR = 97%, what does this tell you about a model's self-preference behavior?

## Architecture Onboarding

- Component map: Generator models (evaluatees) → Response pairs (y_J, y_G) → Evaluator models (judges) ← Query x + Response pairs → Verdict generation (A/B/T) → Aggregation (order swap) → J* verdict → Metrics: SPR, LSPR, HSPP, Judge Accuracy

- Critical path:
  1. Generate responses from all evaluatee models on benchmark D
  2. For each judge J, compare J's response against all evaluatee responses
  3. Apply position-bias mitigation (swap order, aggregate per Equation 1)
  4. Compute metrics using ground-truth correctness labels
  5. Analyze correlation between task accuracy and self-preference metrics

- Design tradeoffs:
  - Fixed evaluatee set vs. matched capability: Fixed set enables cross-judge comparison but may exaggerate self-preference when judges overpower evaluatees
  - No reasoning vs. CoT vs. Long CoT: More reasoning reduces HSPP but increases latency and cost
  - Temperature settings: Reasoning models require temperature > 0 (paper uses 0.6) to avoid repetition

- Failure signatures:
  - Position bias dominance: If SPR differs significantly when y_J appears first vs. second, aggregation is failing
  - Stylistic self-preference: High SPR on D_agree (both correct/both incorrect) indicates style-driven rather than correctness-driven preference
  - Incoherent CoT: Reasoning models producing endless repetition suggest temperature is too low
  - Confidence calibration gap: HSPP instances show lower internal confidence than overall SPR instances

- First 3 experiments:
  1. Baseline SPR measurement: Select 2-3 judge models of varying capability (e.g., 7B, 32B, 70B). Compute SPR against a fixed evaluatee set on MATH500. Verify positive correlation between task accuracy and SPR.
  2. Harmful vs. Legitimate decomposition: For your strongest model, compute HSPP on instances where it errs. Compare HSPP to overall SPR to quantify the error-condition amplification effect.
  3. CoT mitigation test: Implement standard CoT and long CoT evaluation variants. Measure HSPP reduction. If reduction is minimal, check whether reasoning traces are substantive or superficial.

## Open Questions the Paper Calls Out

- Do alternative inference-time scaling strategies, such as self-consistency decoding or multi-agent verification, reduce harmful self-preference more effectively than long Chain-of-Thought reasoning? The study focused primarily on long CoT reasoning; other scaling methods involve different mechanisms that were not tested.

- Do the findings regarding legitimate and harmful self-preference generalize to non-pairwise evaluation formats, such as scalar scoring or pointwise ranking? The dynamics of self-preference might differ when a model assigns a score in isolation versus choosing between two options.

- Can the lower internal confidence associated with harmful self-preference judgments be leveraged to reliably automate the detection and filtering of biased evaluations? While a correlation is observed, it is untested whether this uncertainty signal is robust enough to serve as a practical mitigation mechanism.

## Limitations

- Findings rely heavily on verifiable benchmarks where ground truth exists, raising questions about generalizability to subjective domains
- The correlation between generation and evaluation capabilities assumes linear scaling that may not hold for specialized evaluation models or domains requiring different cognitive skills
- The study focused on pairwise comparison and did not explore scalar scoring or ranking formats

## Confidence

- **High Confidence:** The existence of self-preference bias and its correlation with model capability (SPR findings)
- **Medium Confidence:** The decomposition of SPR into legitimate vs. harmful components (HSPP/LSPR)
- **Low Confidence:** The generalizability of reasoning-based mitigation strategies to non-mathematical domains

## Next Checks

1. **Domain Transfer Test:** Apply the same methodology to open-ended creative tasks (story generation, summarization) where ground truth is subjective, measuring whether HSPP remains predictive of bias patterns.

2. **Temporal Stability Analysis:** Track self-preference metrics across multiple inference sessions with the same model to assess whether the observed correlations represent stable properties or transient artifacts.

3. **Capability Threshold Investigation:** Systematically vary the capability gap between evaluators and evaluatees to identify the inflection point where harmful self-preference emerges, testing whether this aligns with the paper's proposed mechanisms.