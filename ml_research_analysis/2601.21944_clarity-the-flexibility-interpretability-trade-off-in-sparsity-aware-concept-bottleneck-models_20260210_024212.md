---
ver: rpa2
title: 'Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept
  Bottleneck Models'
arxiv_id: '2601.21944'
source_url: https://arxiv.org/abs/2601.21944
tags:
- color
- concept
- sparsity
- bernoulli
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the interpretability challenge in Concept\
  \ Bottleneck Models (CBMs) by introducing a new metric called \"clarity\" that captures\
  \ the trade-off between downstream performance, sparsity, and precision of learned\
  \ concept representations. The authors propose an amortized approach for sparsity-aware\
  \ concept selection using three different formulations: \u21131, \u21130, and Bernoulli-based\
  \ methods."
---

# Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models

## Quick Facts
- arXiv ID: 2601.21944
- Source URL: https://arxiv.org/abs/2601.21944
- Reference count: 15
- Key outcome: Introduces clarity metric for evaluating interpretability trade-offs in sparsity-aware CBMs, demonstrating that strong task performance doesn't guarantee interpretable representations

## Executive Summary
This paper addresses the interpretability challenge in Concept Bottleneck Models (CBMs) by introducing a new metric called "clarity" that captures the trade-off between downstream performance, sparsity, and precision of learned concept representations. The authors propose an amortized approach for sparsity-aware concept selection using three different formulations: ℓ1, ℓ0, and Bernoulli-based methods. Through extensive experiments on CUB and SUN datasets, they demonstrate that sparsity-aware methods can exhibit markedly different behaviors even at comparable accuracy levels. The results show that achieving strong task performance does not guarantee interpretable representations, with the Bernoulli-based method performing favorably in terms of clarity. The paper highlights the importance of explicitly evaluating interpretability metrics beyond just sparsity and task performance when comparing sparsity-aware methods.

## Method Summary
The authors propose an amortized approach for sparsity-aware concept selection in CBMs, where a learned amortization matrix W_s produces binary masks Z that determine which concepts to activate. Three sparsity methods are implemented: ℓ1 (using sigmoid on logits), ℓ0 (using Hard Concrete sampling), and Bernoulli (using Concrete relaxation during training). The framework learns W_s and a classification matrix W_c jointly for 1,500 epochs, then freezes W_s and retrains W_c for 200 epochs across thresholds. Concept scores are obtained either through VLM-based methods (CLIP zero-shot similarity) or predictor-based methods (linear layer trained with BCE on ground-truth attributes). The clarity metric is defined as H(Acc, Sparsity, Precision) = 3·Acc·Sparsity·Prec / (Acc·Sparsity + Acc·Sparsity + Acc·Prec + Sparsity·Prec), requiring ground-truth attribute alignment for evaluation.

## Key Results
- The Bernoulli-based sparsity method achieved the highest clarity scores on both CUB and SUN datasets, demonstrating superior interpretability-performance trade-offs
- Different sparsity-aware methods exhibited markedly different behaviors even at comparable accuracy levels, highlighting the importance of explicit interpretability evaluation
- High classification accuracy did not guarantee interpretable representations, with some methods showing high accuracy but near-zero clarity due to poor precision
- The predictor-based backbone generally outperformed the VLM-based backbone in terms of clarity, suggesting that learned concept representations may be more interpretable than zero-shot VLM scores

## Why This Works (Mechanism)
The paper's mechanism relies on amortized sparsity learning, where a dedicated matrix learns to select which concepts are relevant for each prediction. By incorporating sparsity-aware regularization during training, the model learns to identify and prioritize meaningful concepts while suppressing irrelevant ones. The clarity metric then quantifies this trade-off by measuring the harmonic mean of accuracy, sparsity, and precision, ensuring that only models that are both accurate and interpretable achieve high scores. The use of different sparsity formulations (ℓ1, ℓ0, Bernoulli) allows exploration of various ways to enforce sparsity while maintaining differentiability for end-to-end training.

## Foundational Learning
- **Concept Bottleneck Models**: Models that predict intermediate concept representations before final task prediction, enabling interpretability through concept-level explanations. Why needed: Provides the foundational framework for interpretable classification that the paper builds upon.
- **Amortized Inference**: Using a learned network to approximate complex inference processes, here applied to concept selection. Why needed: Enables efficient, differentiable sparsity learning during training rather than post-hoc selection.
- **Concrete Relaxation**: A continuous approximation of discrete random variables using temperature-controlled sampling, enabling gradient-based optimization. Why needed: Allows the Bernoulli and ℓ0 methods to maintain differentiability while producing binary concept selections.
- **Harmonic Mean Metrics**: Metrics that penalize extreme values by computing the harmonic mean of multiple factors. Why needed: Ensures that high clarity requires balanced performance across accuracy, sparsity, and precision rather than optimizing any single factor.

## Architecture Onboarding

**Component Map**: Image Embeddings → Concept Predictor/CLIP → Amortized Sparsity Matrix → Classification Head → Final Prediction

**Critical Path**: The critical path involves computing concept scores (either via predictor or VLM), applying the learned sparsity mask to select relevant concepts, and using these concepts for final classification. The amortized sparsity matrix W_s is central to the interpretability mechanism.

**Design Tradeoffs**: The paper trades some classification accuracy for interpretability by enforcing sparsity in concept selection. The choice between predictor-based and VLM-based backbones represents a tradeoff between learned interpretability (predictor) versus zero-shot generalization (VLM). Different sparsity formulations (ℓ1, ℓ0, Bernoulli) offer varying degrees of sparsity enforcement and training stability.

**Failure Signatures**: High classification accuracy with near-zero clarity indicates the model has learned to cheat by selecting concepts that maximize accuracy but lack semantic meaning or alignment with ground truth. Training instability with Hard Concrete or Concrete relaxation methods manifests as NaN losses or concept selections that cluster at boundaries without meaningful gradation.

**First Experiments**: 
1. Train predictor-based CBM on CUB without sparsity to establish baseline accuracy and precision
2. Implement and train ℓ1 sparsity method with varying λ values to observe sparsity-accuracy trade-off
3. Compare concept precision of VLM-based vs. predictor-based methods on SUN dataset to validate backbone choice

## Open Questions the Paper Calls Out
- Can clarity be effectively evaluated or adapted for datasets lacking ground-truth concept annotations?
- How do more informative priors in the Bernoulli-based formulation affect sparsification quality and interpretability?
- Does the clarity metric correlate with human subjective assessments of interpretability?
- How does the proposed framework detect or mitigate information leakage in concept bottleneck representations?

## Limitations
- The clarity metric requires ground-truth concept annotations, limiting its applicability to datasets like CUB and SUN
- The framework relies on pre-trained image embeddings, making it dependent on the quality and bias of these models
- The Bernoulli prior is fixed at a uniform value (10^-4), potentially missing opportunities for more informed sparsity patterns
- The paper does not address potential information leakage in concept bottleneck representations

## Confidence
High in the validity of the clarity metric as a meaningful evaluation tool; Medium in the exact numerical comparisons between methods due to unspecified hyperparameters; Medium in the absolute clarity values without access to the precise implementation details.

## Next Checks
1. Verify the precision calculation within the clarity metric by reproducing the aggregate precision score across the CUB dataset using the provided implementation details
2. Confirm the reproducibility of the sparsity-aware amortization training by reimplementing the ℓ1, ℓ0, and Bernoulli methods and comparing the average active concepts at equivalent accuracy levels
3. Validate the robustness of the clarity metric by conducting an ablation study on the SUN dataset with a fixed random seed for the train/test split and comparing the relative performance ordering of the three sparsity methods