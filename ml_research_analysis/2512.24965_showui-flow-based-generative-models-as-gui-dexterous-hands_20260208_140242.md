---
ver: rpa2
title: "ShowUI-$\u03C0$: Flow-based Generative Models as GUI Dexterous Hands"
arxiv_id: '2512.24965'
source_url: https://arxiv.org/abs/2512.24965
tags:
- drag
- action
- showui
- trajectory
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ShowUI-\u03C0 introduces the first flow-based generative model\
  \ for GUI automation, unifying discrete clicks and continuous drags under a single\
  \ lightweight architecture. By predicting incremental cursor adjustments from streaming\
  \ visual observations via flow matching, it enables smooth, closed-loop trajectories\
  \ like dragging sliders or rotating objects\u2014tasks where traditional token-based\
  \ GUI agents fail."
---

# ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands

## Quick Facts
- arXiv ID: 2512.24965
- Source URL: https://arxiv.org/abs/2512.24965
- Reference count: 40
- First flow-based generative model for GUI automation unifying clicks and drags

## Executive Summary
ShowUI-π introduces a novel flow-based generative model for GUI automation that enables smooth, closed-loop cursor trajectories for both discrete clicks and continuous drags. By predicting incremental cursor adjustments via flow matching rather than discrete tokens, the model achieves human-like dexterous control over GUI elements like sliders, rotation controls, and Captchas. The approach unifies diverse interaction modes under a single lightweight architecture and demonstrates superior performance compared to both open-source and proprietary GUI agents across five application domains.

## Method Summary
ShowUI-π extends SmolVLA-450M with a flow matching-based action expert that predicts incremental cursor adjustments from streaming visual observations. The model represents both clicks and drags as sequences of (x, y, m) triplets, where clicks are minimal 2-step drags [(x,y,down), (x,y,up)]. During training, the action expert learns a conditional vector field that transports a base distribution to action trajectories via ODE-based sampling. The architecture uses cross-attention between the VLM backbone and action expert, with directional regularization and reweighted loss emphasizing trajectory endpoints. The model is trained on ScreenDrag, a 20K-trajectory dataset spanning PowerPoint, Captcha, Adobe Premiere, OS Desktop, and Handwriting domains.

## Key Results
- Achieves 26.98% online success rate with 450M parameters, outperforming Gemini-2.5-CUA (22.18%) and Operator (13.27%)
- Shows 40.99% improvement on Captcha tasks with directional regularization
- Unifies click and drag actions in a single head while reducing parameters by 100M compared to separate heads
- Demonstrates superior performance on free-form drags and tasks requiring precise endpoint control

## Why This Works (Mechanism)

### Mechanism 1: Flow Matching for Continuous Trajectory Generation
- Claim: Flow matching enables smooth, closed-loop cursor trajectories that token-based agents cannot produce
- Mechanism: Learns conditional vector field v_θ transporting base distribution to action trajectories via ODE sampling: dâ(s)/ds = v_θ(â(s), s | o_t, Q)
- Core assumption: GUI drag tasks require temporally coherent, high-degree-of-freedom motions
- Evidence anchors: Abstract emphasizes smooth trajectories where token-based agents fail; Section 4.2 describes flow-based formulation with reweighted loss

### Mechanism 2: Unified Action Representation (Clicks as Micro-Drags)
- Claim: Representing clicks and drags as (x, y, m) sequences enables single-model handling of diverse interactions
- Mechanism: Clicks become minimal 2-step drags; drags become extended sequences, removing rigid action format requirements
- Core assumption: Spatial-dynamic differences between clicks and drags can be captured within shared trajectory distribution
- Evidence anchors: Abstract mentions integrating discrete clicks and continuous drags; Table 5 shows unified head outperforms separate heads

### Mechanism 3: Directional Regularization
- Claim: Penalizing directional misalignment improves trajectory coherence for orientation-sensitive tasks
- Mechanism: Adds L_reg = (1/T)Σ(1 - cos(â_t, u_t)) to flow matching objective, ensuring predicted velocity aligns with ground-truth direction
- Core assumption: GUI tasks have directional constraints that standard magnitude-based flow matching doesn't enforce
- Evidence anchors: Section 4.3 explains directional consistency problem; Table 6 shows regularization improves overall success rate

## Foundational Learning

- **Flow Matching / Rectified Flow**
  - Why needed here: Enables deterministic, smooth trajectory generation without iterative denoising
  - Quick check question: Can you explain why flow matching avoids iterative denoising and how reweighting scheme changes training signal?

- **Vision-Language-Action (VLA) Architectures**
  - Why needed here: ShowUI-π extends SmolVLA-450M with cross-attention action expert
  - Quick check question: How does cross-attention between VLM backbone and action expert differ from self-attention-only architectures?

- **Closed-Loop vs Open-Loop Evaluation in Embodied AI**
  - Why needed here: ScreenDrag introduces both offline and online evaluation methods
  - Quick check question: Why might a model achieve low offline trajectory error but fail in online evaluation?

## Architecture Onboarding

- **Component map**: Visual observation + instruction → VLM backbone → Hidden states → Action expert (cross-attention) → Predicted velocity → ODE integration → Action chunk → Execute action → Observe new state → Repeat

- **Critical path**: 1) 1024×576 screenshot + task instruction → Vision encoder → Visual tokens; 2) Visual + text tokens → VLM LLM → Hidden states; 3) Hidden states + noisy action → Action expert (cross-attention) → Predicted velocity; 4) ODE integration over k steps → Clean action chunk; 5) Execute action(s), observe new state, repeat

- **Design tradeoffs**: Chunk size (10 vs 20) affects latency vs trajectory capture; execution steps before re-observation balance reactivity vs drift; unified vs separate heads trade parameter efficiency vs modeling flexibility

- **Failure signatures**: Baseline models predict linear drags when arcs needed, halt mid-trajectory, refuse Captcha tasks due to safety policies, misinterpret instructions; ShowUI-π may overfit to training patterns if directional regularization too strong or accumulate drift if chunk size/exec steps mismatched

- **First 3 experiments**:
  1. Ablate flow matching vs diffusion vs language modeling on 20K ScreenDrag data; expect flow matching highest endpoint accuracy and lowest trajectory error
  2. Vary reweighting scale w ∈ {1, 5, 10, 15} on online success rate; expect w=10 optimal, w=15 over-emphasizes endpoints
  3. Test data-driven closed-loop evaluation by matching predicted actions to nearest recorded trajectory waypoint; verify tolerance ε=20px provides valid state transitions

## Open Questions the Paper Calls Out
- **Scaling question**: How does ShowUI-π's performance scale with parameter count and training data volume beyond current 450M parameter, 20K trajectory setup? (Explicit limitation stated by authors)
- **Planning integration**: Can ShowUI-π effectively integrate with high-level, text-centric planning modules for complex multi-step reasoning tasks? (Future work mentioned in conclusion)
- **Evaluation correlation**: To what extent does data-driven closed-loop evaluation correlate with success rates in live, real-time operating system environments? (Inferred from data-driven evaluation approach)

## Limitations
- ScreenDrag dataset may not capture full diversity of real-world GUI tasks including edge cases with nested menus or asynchronous UI updates
- Evaluation relies heavily on synthetic data-driven rollouts rather than true closed-loop execution, potentially overestimating real-world performance
- Directional regularization tuned specifically for ScreenDrag domains may not generalize to tasks with inherently ambiguous trajectory patterns

## Confidence
- **High Confidence**: Flow matching superiority over discrete token prediction is well-supported by theoretical arguments and empirical results
- **Medium Confidence**: Unified action representation demonstrates parameter efficiency but limited ablation comparisons; fundamental assumption needs further validation
- **Medium Confidence**: Directional regularization shows dramatic improvements but optimal settings appear task-specific; alternative regularization approaches not compared

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate on GUI tasks from domains not in ScreenDrag (web apps, mobile interfaces, enterprise software) to assess unified action representation and directional regularization generalization
2. **Real-World Closed-Loop Validation**: Deploy in actual operating environment with live GUI interactions, measuring performance degradation compared to data-driven rollouts
3. **Computational Overhead Characterization**: Benchmark inference latency and resource utilization against token-based baselines across different chunk sizes and execution frequencies