---
ver: rpa2
title: 'Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations
  in Large Language Models'
arxiv_id: '2510.12137'
source_url: https://arxiv.org/abs/2510.12137
tags:
- attention
- uncertainty
- credal
- transformer
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of hallucination in Large Language
  Models, which generate factually incorrect yet confident assertions. The authors
  argue this stems from the Transformer's Softmax function in the attention mechanism,
  which collapses ambiguous attention scores into a single probability distribution,
  discarding uncertainty information at each layer and leading to overconfidence.
---

# Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2510.12137
- Source URL: https://arxiv.org/abs/2510.12137
- Authors: Shihao Ji; Zihui Song; Jiajie Huang
- Reference count: 4
- Primary result: Introduces Credal Transformer with Credal Attention Mechanism (CAM) to quantify uncertainty via credal sets and reduce confident hallucinations by abstention

## Executive Summary
The paper addresses the problem of hallucinations in Large Language Models, which generate factually incorrect yet confident assertions. The authors argue this stems from the Transformer's Softmax function in the attention mechanism, which collapses ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer and leading to overconfidence. To fix this, they introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a "credal set" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. This is implemented by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. The contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model.

## Method Summary
The Credal Transformer replaces standard Softmax-based attention with Credal Attention Mechanism (CAM), which reinterprets attention scores as evidence masses for a Dirichlet distribution. Each attention score s_ij is converted to evidence e_ij = exp(s_ij), then to concentration parameters α_ij = e_ij + 1. Instead of a single attention vector, this yields a credal set—a convex set of distributions—whose volume directly quantifies epistemic uncertainty. The expected attention weights are computed as â_ij = α_ij / α_i0 where α_i0 = Σα_ik, and total uncertainty (vacuity) is calculated as U_i = L / α_i0 where L is sequence length. This vacuity serves as a differentiable measure of epistemic uncertainty that can be monitored at output layers to trigger abstention when evidence is insufficient.

## Key Results
- Introduces Credal Attention Mechanism (CAM) that replaces Softmax with evidential Dirichlet-based attention
- Demonstrates OOD detection capability through meaningful uncertainty separation
- Shows significant reduction in confident errors on unanswerable questions via abstention
- Maintains computational efficiency with minimal overhead (+4.4% inference, +11.6% training)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Softmax with an evidential formulation preserves ambiguity signals that would otherwise be discarded.
- Mechanism: Raw attention scores s_ij are converted to evidence e_ij = exp(s_ij), which parameterize a Dirichlet distribution with concentration parameters α_ij = e_ij + 1. Instead of a single attention vector, this yields a credal set—a convex set of distributions—whose volume directly quantifies epistemic uncertainty.
- Core assumption: Attention scores can be meaningfully interpreted as evidence masses; the Dirichlet conjugate prior appropriately models uncertainty over categorical attention distributions.
- Evidence anchors:
  - [abstract] "We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution."
  - [section 3] "The concentration parameters α_i of the Dirichlet distribution for the i-th query are set as: α_ij = e_ij + 1"
  - [corpus] Related work on credal learning (CGNNs, Credal Ensemble Distillation) supports credal set representations for uncertainty, but direct application to Transformer attention is novel and not yet independently validated.
- Break condition: If attention scores carry semantic meaning that evidence mass interpretation distorts (e.g., negative interactions, attention head specialization), the Dirichlet parameterization may misrepresent true uncertainty.

### Mechanism 2
- Claim: A closed-form uncertainty signal (vacuity) can be extracted per attention head without sampling, enabling differentiable uncertainty propagation.
- Mechanism: Total uncertainty U_i = L / α_i0 where α_i0 = Σα_ik and L is sequence length. This "vacuity" measures lack of total evidence and serves as a differentiable epistemic uncertainty proxy available at every layer.
- Core assumption: Vacuity computed from Dirichlet parameters accurately reflects epistemic uncertainty; higher-order moments (e.g., conflict among evidence sources) are not necessary for this task.
- Evidence anchors:
  - [abstract] "The size of this set serves as a direct, differentiable measure of the model's epistemic uncertainty."
  - [section 3] "Crucially, the total uncertainty (also called vacuity) of this distribution can be directly and differentiably calculated as the lack of total evidence: U_i = L / α_i0"
  - [corpus] Evidential Deep Learning (EDL) literature provides precedent for vacuity-based uncertainty, but application within attention mechanisms (vs. output layers) is an architectural extension not yet externally validated.
- Break condition: If vacuity conflates different uncertainty sources (e.g., aleatoric vs. epistemic) or is insensitive to structured ambiguity, the signal may not reliably discriminate hallucination-prone states.

### Mechanism 3
- Claim: High uncertainty signals can trigger abstention, reducing confident errors on inputs the model should not answer.
- Mechanism: The uncertainty signal U_i is monitored at output layers; when U_i exceeds a threshold, the model abstains rather than generating a confident but potentially fabricated response. This is demonstrated on unanswerable question benchmarks.
- Core assumption: A threshold on vacuity meaningfully separates answerable from unanswerable cases; abstention can be defined and implemented without disrupting generation quality for answerable inputs.
- Evidence anchors:
  - [abstract] "In a question-answering benchmark, significantly reduces confident errors on unanswerable questions by abstaining from prediction."
  - [section 4.2] "The Credal Transformer can significantly reduce these confident errors. By leveraging its internal uncertainty measure, the model can abstain from prediction when it lacks sufficient evidence."
  - [corpus] Corpus papers on uncertainty quantification for LLMs discuss abstention-like strategies, but specific thresholding schemes and their calibration remain underexplored; no independent replication of this abstention result is available.
- Break condition: If uncertainty thresholds are miscalibrated (e.g., high false positive rate on ambiguous but answerable questions), abstention may degrade utility without proportionally reducing risk.

## Foundational Learning

- Concept: Dirichlet distribution as conjugate prior to categorical distributions.
  - Why needed here: CAM parameterizes attention uncertainty using Dirichlet concentration parameters; understanding how α controls distribution sharpness/diffuseness is essential.
  - Quick check question: Given α = [3, 1, 1] for a 3-class problem, would you expect a concentrated or diffuse distribution over class probabilities?

- Concept: Epistemic vs. Aleatoric uncertainty in deep learning.
  - Why needed here: The paper claims to capture epistemic uncertainty (model's lack of knowledge) via credal sets; distinguishing this from aleatoric uncertainty (inherent data noise) clarifies what the uncertainty signal represents.
  - Quick check question: If an input has multiple plausible interpretations due to inherent ambiguity (e.g., lexical ambiguity), is this epistemic or aleatoric uncertainty?

- Concept: Evidential Deep Learning (EDL) basics—evidence, belief masses, and vacuity.
  - Why needed here: CAM builds directly on EDL principles but applies them within the attention mechanism rather than at the output layer.
  - Quick check question: In EDL, what does high total evidence (large α_0) imply about vacuity U?

## Architecture Onboarding

- Component map:
  Standard Attention: Query-Key dot product → Softmax → attention weights → Value aggregation.
  Credal Attention (CAM): Query-Key dot product → evidence e_ij = exp(s_ij) → Dirichlet parameters α_ij = e_ij + 1 → expected attention weights â_ij = α_ij / α_i0 → uncertainty signal U_i = L / α_i0.

- Critical path:
  1. Verify evidence computation is numerically stable (exp(s_ij) without overflow/underflow).
  2. Confirm α_i0 > 0 for all queries to avoid division errors in â_ij and U_i.
  3. Check that expected attention weights preserve normalization and can replace standard attention outputs in downstream layers.
  4. Monitor U_i distribution during training to ensure it varies meaningfully (not collapsed to constant).

- Design tradeoffs:
  - Computational overhead: GFLOPs unchanged; inference +4.4%, training step +11.6% per paper benchmarks.
  - Abstraction cost: Interpreting attention scores as evidence adds theoretical assumptions; may not align with all attention head behaviors (e.g., heads encoding syntactic vs. semantic roles).
  - Threshold calibration: Abstention requires choosing U thresholds; paper does not specify calibration procedure.
  - Scalability: Benchmarked on smaller models; 100B+ scalability not yet validated.

- Failure signatures:
  - Uncertainty collapse: U_i near 0 or 1 for all inputs, indicating signal is not discriminative.
  - Attention distortion: Expected attention weights diverge significantly from standard Softmax outputs in ways that harm task performance.
  - Threshold drift: Abstention rate varies unpredictably across domains, suggesting miscalibration.
  - Numerical instability: Underflow in evidence computation for very negative s_ij values.

- First 3 experiments:
  1. Replicate OOD detection experiment: Train a Credal Transformer on ID data, evaluate U_i distributions for ID vs. OOD vs. nonsense inputs. Confirm separation.
  2. Ablate Dirichlet parameterization: Compare CAM against standard Softmax attention on same task; measure both performance and uncertainty discrimination.
  3. Threshold sensitivity analysis: On a QA dataset with labeled unanswerable questions, sweep abstention thresholds and plot precision-recall tradeoff for hallucination reduction vs. answer coverage.

## Open Questions the Paper Calls Out
None

## Limitations
- Evidence-mass interpretation of attention scores not yet validated across all head types
- No explicit calibration procedure for vacuity thresholds provided
- Scalability to 100B+ parameter models not yet validated

## Confidence

**High Confidence:**
- The credal set construction from Dirichlet parameters is mathematically sound.
- Vacuity computation U_i = L / α_i0 is correct and differentiable.
- The core idea that discarding ambiguity via Softmax contributes to overconfidence is plausible.

**Medium Confidence:**
- Empirical gains in OOD detection and abstention on unanswerable questions.
- Computational overhead claims (inference +4.4%, training +11.6%).
- Interpretation of attention scores as evidence masses across all head types.

**Low Confidence:**
- Generalization of abstention performance to new domains and scales.
- Calibration stability of vacuity thresholds.
- Absence of adversarial scenarios where vacuity can be manipulated.

## Next Checks

1. **Scale Validation:** Retrain a medium-sized (e.g., 1-3B) Transformer and Credal Transformer on a standard language modeling task; compare both perplexity and vacuity calibration on held-out OOD vs. ID data. Confirm vacuity signal remains discriminative at scale.

2. **Threshold Calibration Experiment:** On a QA dataset with human-labeled unanswerable questions, sweep vacuity thresholds and plot precision-recall curves for abstention. Quantify tradeoff between hallucination reduction and answer coverage. Compare against calibrated baseline abstention strategies.

3. **Adversarial Uncertainty Manipulation:** Design inputs that systematically inflate or deflate vacuity (e.g., via attention score poisoning or sequence length manipulation). Test whether the uncertainty signal remains robust or can be gamed to bypass abstention safeguards.