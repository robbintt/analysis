---
ver: rpa2
title: Crosslingual Reasoning through Test-Time Scaling
arxiv_id: '2505.05408'
source_url: https://arxiv.org/abs/2505.05408
tags:
- reasoning
- language
- english
- languages
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the crosslingual reasoning capabilities
  of English-centric reasoning language models (RLMs) through test-time scaling. The
  authors find that increasing inference compute for RLMs improves multilingual mathematical
  reasoning performance across high-resource and low-resource languages, to the extent
  that smaller models can outperform models twice their size.
---

# Crosslingual Reasoning through Test-Time Scaling

## Quick Facts
- arXiv ID: 2505.05408
- Source URL: https://arxiv.org/abs/2505.05408
- Reference count: 40
- Primary result: Test-time scaling improves multilingual reasoning for English-centric RLMs, with smaller models outperforming larger ones when scaled.

## Executive Summary
This paper investigates crosslingual reasoning capabilities of English-centric reasoning language models (RLMs) through test-time scaling. The authors find that increasing inference compute for RLMs improves multilingual mathematical reasoning performance across high-resource and low-resource languages, to the extent that smaller models can outperform models twice their size. They observe that RLMs exhibit a "quote-and-think" language-mixing pattern when reasoning about non-English inputs, where they quote non-English phrases and reason about them in English. The study also reveals that forcing RLMs to reason in high-resource languages yields better performance and efficiency than in low-resource languages. However, the benefits of reasoning finetuning do not generalize well to out-of-domain tasks, particularly cultural commonsense knowledge.

## Method Summary
The study evaluates English-centric reasoning language models (RLMs) fine-tuned on English reasoning data using test-time scaling to improve multilingual mathematical reasoning. The base models are multilingual pretrained LLMs (Qwen2.5-Instruct) at various scales (1.5B to 32B parameters), fine-tuned on 1k English reasoning samples distilled from DeepSeek-R1. The evaluation uses MGSM benchmark across 11 languages, measuring task accuracy, language compliance, and inference FLOPs. The method includes budget forcing (truncation or extrapolation) and language forcing strategies (translated wait, prefix seeding, system prompts) to control reasoning behavior.

## Key Results
- Scaling up inference compute for English-centric RLMs improves multilingual mathematical reasoning across many languages including low-resource languages, outperforming models twice their size
- RLMs consistently follow a quote-and-think pattern to reason about quoted non-English inputs
- Models perform better and more efficiently in high-resource languages, with Swahili requiring approximately 3.5 times more compute than French for the same task
- The benefits of reasoning finetuning do not generalize well to out-of-domain tasks, particularly cultural commonsense knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Allocating more inference compute to English-centric reasoning models improves multilingual mathematical reasoning.
- Mechanism: The base model's multilingual pretraining is preserved during data-efficient English fine-tuning. Extended reasoning chains allow the model to iteratively parse non-English inputs, decompose problems, and self-correct errors—leveraging transfer from English reasoning patterns.
- Core assumption: English reasoning fine-tuning does not catastrophically overwrite multilingual understanding.
- Evidence anchors:
  - [abstract] "scaling up inference compute for English-centric reasoning language models (RLMs) improves multilingual mathematical reasoning across many languages including low-resource languages, to an extent where they outperform models twice their size"
  - [section 4.1] Figure 1(a) shows accuracy gains of +7.0 to +9.4 for models ≥3B parameters when scaling from 0.5k to 8k thinking tokens
  - [corpus] "Multilingual Test-Time Scaling via Initial Thought Transfer" (arXiv:2505.15508) provides complementary evidence for test-time scaling in multilingual settings
- Break condition: Model capacity below ~3B parameters; the paper shows 1.5B models gain only +1.8% and plateau early.

### Mechanism 2
- Claim: English-centric RLMs naturally exhibit a "quote-and-think" pattern when processing non-English inputs, quoting foreign phrases and reasoning about them in English.
- Mechanism: The model preserves multilingual parsing ability from pretraining while fine-tuning shifts the dominant reasoning language to English. When encountering non-English inputs, the model extracts key phrases verbatim, interprets them, and continues reasoning in English—this is a crosslingual transfer of quoting behaviors present in the training data.
- Core assumption: The fine-tuning data contains quote-and-think patterns that transfer across languages.
- Evidence anchors:
  - [abstract] "they consistently follow a quote-and-think pattern to reason about quoted non-English inputs"
  - [section 5.2] "at least 92.5% of the sentences in s1's CoTs are in English only... when s1 mixes languages during reasoning, it primarily follows... quote-and-think"
  - [corpus] Limited direct corpus evidence; most related work focuses on multilingual evaluation rather than mechanistic analysis of language mixing
- Break condition: Heavy fine-tuning on large datasets (e.g., 800k samples) may cause catastrophic forgetting of multilingual parsing, reducing this capability.

### Mechanism 3
- Claim: High-resource languages (HRLs) support better and more token-efficient reasoning than low-resource languages (LRLs) when forced as the reasoning language.
- Mechanism: HRLs benefit from denser tokenization and richer pretraining representations, enabling more efficient reasoning. LRLs require more tokens to express the same reasoning steps and exhibit weaker transfer from English fine-tuning, leading to both higher computational cost and lower accuracy.
- Core assumption: Tokenization efficiency and pretraining density correlate with reasoning quality.
- Evidence anchors:
  - [abstract] "models perform better and more efficiently in high-resource languages"
  - [section 6.3] Figure 5 shows -0.811 correlation between token count and accuracy; "reasoning in Swahili requires approximately 3.5 times more compute than French"
  - [corpus] "Parallel Scaling Law" (arXiv:2510.02272) discusses cross-linguistic reasoning generalization with related findings
- Break condition: Even HRLs may underperform if forced via strict "combined" language control; mixed strategies (e.g., translated_wait) often outperform pure in-language forcing.

## Foundational Learning

- Concept: Test-Time Scaling (Inference Compute Scaling)
  - Why needed here: Core paradigm enabling longer chain-of-thought at inference to improve reasoning without changing model weights.
  - Quick check question: If you double the allowed thinking tokens, what tradeoffs should you expect?

- Concept: Catastrophic Forgetting in Fine-Tuning
  - Why needed here: Explains why s1 (1k samples, 5 epochs) retains multilingual capability while larger-scale fine-tuning (e.g., 800k samples) degrades it.
  - Quick check question: What happens to a multilingual model's low-resource language performance after aggressive monolingual fine-tuning?

- Concept: Tokenization Disparity Across Languages
  - Why needed here: LRLs require more tokens per concept, directly increasing inference cost and affecting reasoning efficiency.
  - Quick check question: Why might Swahili cost 3.5× more compute than French for the same task?

## Architecture Onboarding

- Component map: Base model (multilingual pretrained LLM) -> Reasoning fine-tuning (small-scale English-only long CoT data) -> Budget forcing (truncation or extrapolation) -> Language forcing (translated wait, prefix seeding, system prompts, or combined)
- Critical path: 1. Preserve multilingual base → 2. Apply data-efficient English reasoning fine-tuning → 3. At inference, enable budget forcing → 4. Optionally layer language forcing for user-facing consistency
- Design tradeoffs:
  - Accuracy vs. language compliance: Strict forcing (combined) maximizes compliance but often reduces accuracy.
  - Model size vs. compute: 14B model offers strong performance with lower inference FLOPs than 32B; models <3B struggle to benefit from scaling.
  - Domain specificity: STEM reasoning training does not reliably transfer to cultural/commonsense domains.
- Failure signatures:
  - Sub-3B models: flat or minimal gains from increased thinking tokens.
  - LRL reasoning: high token counts, low accuracy, poor language compliance.
  - Non-STEM domains: scaling thinking tokens can hurt performance ("overthinking").
  - Heavy fine-tuning: catastrophic forgetting of low-resource languages (e.g., R1-Distill-Qwen's sw/te collapse).
- First 3 experiments:
  1. Scaling sweep: Run s1-7B/14B on MGSM with thinking token caps {0.5k, 2k, 4k, 8k}; compare against Qwen baselines to reproduce scaling curves.
  2. Pattern analysis: Collect CoTs for 100 non-English queries; manually annotate quote-and-think, intersentential, and intrasentential mixing frequencies.
  3. Language forcing ablation: On 3 HRLs and 3 LRLs, compare translated_wait vs. combined strategies for both accuracy and language compliance.

## Open Questions the Paper Calls Out
None

## Limitations
- Test-time scaling benefits are strongly dependent on model size, with models below 3B parameters showing minimal gains
- The quote-and-think mechanism is observed but not fully understood mechanistically
- Reasoning patterns learned from STEM data do not reliably transfer to cultural commonsense domains

## Confidence
**High Confidence**
- Test-time scaling benefits for models ≥3B parameters on multilingual mathematical reasoning
- The existence and prevalence of the quote-and-think language-mixing pattern
- High-resource languages requiring fewer tokens and showing better accuracy than low-resource languages for forced reasoning

**Medium Confidence**
- The claim that data-efficient English fine-tuning preserves multilingual parsing without catastrophic forgetting
- The assertion that the quote-and-think pattern represents transfer from training data rather than emergent behavior
- The effectiveness of mixed forcing strategies (e.g., translated_wait) over strict combined approaches

**Low Confidence**
- The generalizability of reasoning patterns from STEM to cultural commonsense domains
- The claim that English-centric RLMs can be effectively deployed in low-resource language contexts
- The mechanism by which test-time scaling enables crosslingual reasoning transfer

## Next Checks
1. **Model Size Threshold Validation**: Conduct a systematic sweep across model sizes from 500M to 14B parameters on MGSM with test-time scaling, measuring both accuracy gains and thinking token utilization curves to precisely identify where scaling benefits emerge and whether the 3B threshold is consistent across different base models.

2. **Mechanism Isolation Experiment**: Design controlled fine-tuning experiments that systematically vary the amount of English reasoning data (100 samples, 1k samples, 10k samples, 100k samples) while measuring multilingual parsing capability on non-English inputs before and after fine-tuning, to determine the exact point where catastrophic forgetting begins and whether the quote-and-think pattern persists.

3. **Cross-Domain Generalization Probe**: Create a mixed-domain benchmark combining STEM reasoning, cultural commonsense, and general knowledge questions, then apply test-time scaling across all domains while measuring both domain-specific accuracy and any transfer effects, to determine whether domain-specific fine-tuning could enable broader generalization or if the reasoning capabilities are fundamentally compartmentalized.