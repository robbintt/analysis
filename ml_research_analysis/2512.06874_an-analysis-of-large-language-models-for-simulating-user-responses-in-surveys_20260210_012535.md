---
ver: rpa2
title: An Analysis of Large Language Models for Simulating User Responses in Surveys
arxiv_id: '2512.06874'
source_url: https://arxiv.org/abs/2512.06874
tags:
- demographic
- llms
- claims
- children
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle to accurately simulate user responses
  to survey questions across diverse demographic groups, even with advanced prompting
  strategies. Direct prompting and chain-of-thought prompting yield similar performance,
  while the proposed CLAIMSIM method improves response diversity by eliciting multiple
  viewpoints as contextual input.
---

# An Analysis of Large Language Models for Simulating User Responses in Surveys

## Quick Facts
- **arXiv ID:** 2512.06874
- **Source URL:** https://arxiv.org/abs/2512.06874
- **Reference count:** 40
- **Primary result:** LLMs show poor accuracy in simulating demographic-specific survey responses, with CLAIMSIM improving diversity but not accuracy.

## Executive Summary
This study evaluates large language models' ability to simulate user responses to survey questions across different demographic groups. The research finds that even advanced prompting strategies like chain-of-thought prompting fail to significantly improve response accuracy. The proposed CLAIMSIM method, which elicits multiple viewpoints as contextual input, shows promise in improving response diversity but does not substantially increase accuracy. The findings suggest fundamental limitations in how LLMs process demographic information and maintain fixed viewpoints regardless of user characteristics.

## Method Summary
The study employs three prompting strategies: direct prompting, chain-of-thought prompting, and the proposed CLAIMSIM method. CLAIMSIM improves response diversity by generating multiple claims from different perspectives to serve as contextual input. The evaluation framework uses demographic features (age, gender, income) to condition responses and measures accuracy against ground truth survey data. The research tests these methods across diverse survey questions and demographic combinations to assess performance variations.

## Key Results
- All prompting methods achieve only slightly above random accuracy in simulating survey responses
- CLAIMSIM improves response diversity by eliciting multiple viewpoints
- 50% of questions generate single-perspective claims regardless of demographic variations
- LLMs maintain fixed viewpoints even when explicitly prompted with conflicting claims

## Why This Works (Mechanism)
The study identifies that RLHF fine-tuning embeds dominant cultural viewpoints too deeply for demographic conditioning to overcome. LLMs struggle with perspective-taking and reasoning over conflicting claims, maintaining fixed viewpoints regardless of demographic features. This suggests fundamental constraints in how these models process user characteristics and generate responses conditioned on demographic inputs.

## Foundational Learning

### LLM Prompt Engineering
- **Why needed:** Different prompting strategies (direct, chain-of-thought, CLAIMSIM) significantly impact response quality and diversity
- **Quick check:** Compare accuracy and diversity metrics across prompting methods using standardized survey datasets

### Demographic Conditioning
- **Why needed:** Understanding how to effectively incorporate user characteristics into LLM responses for survey simulation
- **Quick check:** Measure response variation across demographic groups for identical questions

### Response Diversity Metrics
- **Why needed:** Evaluating the range and variety of perspectives generated by LLMs for survey questions
- **Quick check:** Analyze claim diversity scores and perspective distribution across different demographic conditions

## Architecture Onboarding

### Component Map
Survey Questions -> Prompting Strategy (Direct/CoT/CLAIMSIM) -> LLM Generation -> Demographic Conditioning -> Response Evaluation

### Critical Path
Survey question selection → Prompt construction → LLM generation → Demographic feature integration → Response evaluation and accuracy measurement

### Design Tradeoffs
Accuracy vs. diversity: CLAIMSIM improves diversity but not accuracy. Simple prompting vs. complex reasoning: Chain-of-thought adds complexity without significant accuracy gains. Demographic conditioning depth vs. model capability: Deeper conditioning doesn't overcome fixed viewpoint limitations.

### Failure Signatures
Single-perspective claims despite varying demographics (50% of questions). Fixed viewpoints maintained regardless of explicit conflicting prompts. Minimal accuracy improvement across all prompting strategies.

### 3 First Experiments
1. Test CLAIMSIM with base model vs. RLHF-finetuned model to isolate training effects
2. Evaluate response diversity across different demographic feature combinations
3. Measure accuracy improvement when using human-annotated claims vs. model-generated claims as context

## Open Questions the Paper Calls Out
None

## Limitations
- RLHF fine-tuning may have embedded dominant cultural viewpoints too deeply
- Models demonstrate limited reasoning capabilities for perspective-taking
- Demographic conditioning fails to overcome fixed viewpoint maintenance

## Confidence

**High confidence:** Overall poor performance across all prompting strategies; CLAIMSIM's improvement in response diversity

**Medium confidence:** RLHF reinforcing dominant viewpoints; inability to reason over conflicting claims

## Next Checks

1. Test CLAIMSIM methodology with smaller, specialized language models to determine if response diversity improvements are model-size dependent or prompting-method dependent.

2. Conduct ablation studies removing RLHF fine-tuning from base models to isolate the impact of human feedback training on demographic response variation.

3. Evaluate responses using human raters from diverse demographic backgrounds to assess whether model-generated claims align with human perspectives within specific demographic groups.