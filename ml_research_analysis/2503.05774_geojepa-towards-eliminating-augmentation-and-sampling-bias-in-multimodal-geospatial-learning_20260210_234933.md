---
ver: rpa2
title: 'GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal
  Geospatial Learning'
arxiv_id: '2503.05774'
source_url: https://arxiv.org/abs/2503.05774
tags:
- data
- page
- learning
- image
- geojepa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of creating multimodal geospatial
  representations that eliminate reliance on augmentation and sampling biases. The
  authors propose GeoJEPA, a novel model built on the Joint-Embedding Predictive Architecture
  (JEPA), which leverages self-supervised learning to produce semantic representations
  of urban regions and map entities from OpenStreetMap data and aerial imagery.
---

# GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning

## Quick Facts
- **arXiv ID**: 2503.05774
- **Source URL**: https://arxiv.org/abs/2503.05774
- **Reference count**: 40
- **Primary result**: GeoJEPA performs well on qualitative similarity tasks, identifying semantically similar regions and entities, but struggles with preserving fine-grained details and degrades with added modalities, especially images.

## Executive Summary
This thesis addresses the challenge of creating multimodal geospatial representations that eliminate reliance on augmentation and sampling biases. The authors propose GeoJEPA, a novel model built on the Joint-Embedding Predictive Architecture (JEPA), which leverages self-supervised learning to produce semantic representations of urban regions and map entities from OpenStreetMap data and aerial imagery. GeoJEPA integrates tag, geometry, and image modalities through a unified transformer encoder, trained to predict latent representations of masked tokens. Evaluations show GeoJEPA excels at identifying semantically similar regions and entities, but struggles with preserving fine-grained details and degrades with added modalities, especially images, likely due to pretrained tokenizers leaking global information incompatible with JEPA's objective.

## Method Summary
GeoJEPA is a multimodal geospatial representation learning model that uses Joint-Embedding Predictive Architecture (JEPA) to eliminate augmentation and sampling biases. The model processes OpenStreetMap (OSM) data (tags, geometries, relations) and National Aerial Imagery Program (NAIP) imagery for 300x300m urban tiles. It employs frozen tokenizers (Voyage-3-large for tags, PolygonGNN for geometries, ViT-B/16 for images) to convert data into tokens, which are then projected to a shared dimension and processed by a single-stream transformer encoder. The model is trained to predict latent representations of masked tokens using a Smooth L1 reconstruction loss combined with VICReg variance-covariance regularization. Evaluation uses both synthetic regression tasks and qualitative kNN retrieval.

## Key Results
- GeoJEPA performs well on qualitative similarity tasks, excelling at identifying semantically similar regions and entities
- Performance degrades significantly with added image modality, likely due to pretrained tokenizers leaking global information
- Model shows better performance at 100-200 epochs than at final 300 epochs, suggesting potential late-stage training issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JEPA's latent-space prediction objective may reduce augmentation/sampling bias compared to contrastive methods.
- Mechanism: Instead of reconstructing pixels or contrasting augmented views, the predictor network learns to predict target encoder embeddings from context encoder embeddings. This forces semantic abstraction without requiring hand-designed positive/negative pairs.
- Core assumption: High-level semantic prediction in latent space provides a richer learning signal than pixel-level reconstruction or distance-based contrastive objectives.
- Evidence anchors: [abstract]: "GeoJEPA, a novel model built on the Joint-Embedding Predictive Architecture (JEPA), which leverages self-supervised learning to produce semantic representations...eliminate reliance on augmentation and sampling biases"; [Section 4.5.3]: "JEPA combines joint-embedding and generative approaches by masking and reconstructing latent space variables"; [corpus]: Limited direct corpus validation—neighbor papers focus on contrastive/geospatial SSL but do not confirm JEPA superiority.
- Break condition: If context-target masking lacks sufficient variability, the prediction task becomes trivial, causing representation collapse.

### Mechanism 2
- Claim: Multimodal token fusion via single-stream transformers enables cross-modal attention between map entities and aerial imagery.
- Mechanism: Tag tokens (from LLM embeddings), geometry tokens (from PolygonGNN), and image patch tokens (from ViT-B/16) are projected to a shared dimension, concatenated, and processed by one transformer encoder with positional encodings.
- Core assumption: Late fusion through shared attention allows the model to learn cross-modal correlations without modality-specific architectural branches.
- Evidence anchors: [Section 7.6.2]: "a single concatenated token sequence is formed, encompassing tag, geometry, and image information"; [Section 7.6.3]: "significant advantage of the single-stream architecture...inherent flexibility in handling scenarios where one or more modalities are missing"; [corpus]: S2Vec paper uses similar discretized cell embeddings; no JEPA-style multimodal fusion validated.
- Break condition: If one modality dominates token count (e.g., 197 image tokens vs. 5–1250 entity tokens), reconstruction loss may bias toward that modality.

### Mechanism 3
- Claim: Pretrained tokenizers with global information leakage (especially ViT-B/16) may undermine JEPA's prediction task difficulty.
- Mechanism: JEPA requires high variability between context and target tokens. ViT patch embeddings contain CLS-token-influenced global information, making latent prediction easier than intended, reducing learned abstraction.
- Core assumption: Token-level representations from pretrained encoders should be local/informationally-isolated to preserve prediction challenge.
- Evidence anchors: [Section 10.3]: "latent image patch representations generated by ViT-B/16 contain global information, reducing reconstruction complexity and undermining training dynamics"; [Section 8.7]: "addition of image data impairs the model's performance significantly"; [corpus]: No corpus papers directly validate this tokenizer leakage hypothesis.
- Break condition: If all modalities use purely local tokenizers (e.g., geometry encoder without global pooling), cross-modal benefits may emerge—but this remains untested.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA) target encoder**
  - Why needed here: JEPA uses EMA of context encoder as target encoder, creating a slowly-updating "teacher" that stabilizes training and provides consistent prediction targets.
  - Quick check question: Is the target encoder properly initialized as EMA of context encoder, not randomly initialized?

- **Concept: Representation collapse in joint-embedding methods**
  - Why needed here: JEPA can initially collapse (all representations converge to similar values) before escaping; monitoring variance/covariance metrics is essential.
  - Quick check question: Are you tracking token variance and covariance during training, not just reconstruction loss?

- **Concept: Masking ratio and strategy for non-uniform sequences**
  - Why needed here: High masking ratios (75–90%) and structured (not random) masking are empirically important; geospatial data requires modality-aware and position-aware strategies.
  - Quick check question: Does your masking strategy account for variable sequence lengths and modality boundaries?

## Architecture Onboarding

- **Component map**: Data loading -> Tokenization (frozen encoders) -> Projection -> Masking (context/target split) -> Encoder pass -> Predictor -> Loss computation -> EMA update

- **Critical path**: Data loading → Tokenization (frozen encoders) → Projection → Masking (context/target split) → Encoder pass → Predictor → Loss computation → EMA update

- **Design tradeoffs**:
  - Single-stream vs. dual-stream: Single-stream chosen for modality-dropout robustness; dual-stream would enable parallel modality processing but complicate missing-modality handling.
  - ViT-B/16 vs. domain-specific ScaleMAE: ViT-B/16 chosen for smaller size; however, global information leakage hypothesis suggests this may harm JEPA training.
  - Mixed masking vs. single strategy: Mixed (random/area/modality) chosen for diversity; ablation shows VICReg helps but batch size increases hurt.

- **Failure signatures**:
  - Near-zero loss early in training with uniform representations → collapse state (normal transient; should escape)
  - Covariance loss not decreasing → redundant features encoding same information
  - Image-inclusive variants underperforming tag-only → tokenizer global leakage suspected
  - Performance degrading after ~200 epochs → overtraining; early stopping may be needed

- **First 3 experiments**:
  1. **Unimodal tag-only baseline (GeoJEPA-T)**: Establish training dynamics, verify collapse-escape behavior, confirm quantitative task performance exceeds dummy baseline.
  2. **Tokenizer ablation**: Replace ViT-B/16 with a local-only image tokenizer (e.g., patch-level CNN without CLS influence) to test global leakage hypothesis.
  3. **Modality-dropout stress test**: Train with 50% modality dropout, evaluate whether model generalizes to missing-modality inference scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can tokenization methods be modified to prevent global information leakage in JEPA-based geospatial models?
- Basis in paper: [explicit] The authors identify "Major Flaw 1" in Section 10.3, noting that pretrained vision transformers (ViT) encode global information in patch tokens, which undermines JEPA's need for high variability between context and targets.
- Why unresolved: Using standard frozen pretrained encoders introduces global semantics too early, making the latent prediction task trivial and degrading performance.
- What evidence would resolve it: Demonstrating improved performance using encoders trained with local constraints or patch-specific features that lack global class information.

### Open Question 2
- Question: What causes performance degradation in GeoJEPA during the later stages of training?
- Basis in paper: [explicit] Section 10.2 notes that the model performs better at 100–200 epochs than at the final 300 epochs, stating, "We are yet to identify the reason for this."
- Why unresolved: The reconstruction loss does not correlate with downstream utility, and the dynamics of JEPA's optimization landscape remain poorly understood.
- What evidence would resolve it: Identifying specific metrics or regularization techniques that stabilize performance and prevent late-stage overfitting or collapse.

### Open Question 3
- Question: How can the ratio of contextual to entity-specific information be controlled in token-level map entity representations?
- Basis in paper: [inferred] Section 10.7 discusses the limitation that GeoJEPA representations often over-emphasize regional context over the entity itself, and calls for future work to "observe, or impose more control over" this distribution.
- Why unresolved: Standard transformer attention mixes local entity features with global regional context, making it difficult to isolate entity semantics.
- What evidence would resolve it: Successfully retrieving specific map entities based on intrinsic properties rather than their surrounding neighbors.

## Limitations
- The thesis identifies critical architectural weaknesses but does not definitively resolve them, particularly the performance degradation with image modality
- Evaluation scope may not fully capture practical utility of geospatial representations for downstream applications
- Methodological gap: lacks direct quantitative comparison between GeoJEPA and state-of-the-art contrastive baselines on identical datasets and tasks

## Confidence
- **High Confidence**: The mechanism by which single-stream transformer fusion enables cross-modal attention is well-supported by the architecture description and ablation results showing modality-dropout benefits.
- **Medium Confidence**: The hypothesis that ViT-B/16's global information leakage undermines JEPA training is plausible given the performance degradation, but requires experimental validation through tokenizer ablation.
- **Low Confidence**: The claim that JEPA inherently eliminates augmentation bias compared to contrastive methods lacks direct empirical comparison and relies primarily on theoretical arguments about latent-space prediction.

## Next Checks
1. **Tokenizer Ablation Study**: Replace ViT-B/16 with a local-only image tokenizer (e.g., patch-level CNN without CLS influence) and measure whether multimodal performance improves relative to tag-only baseline.

2. **Direct Contrastive Comparison**: Implement a state-of-the-art contrastive baseline (e.g., SimSiam or Barlow Twins) on identical geospatial data and evaluate both qualitative retrieval and quantitative task performance to directly test JEPA's bias-elimination claims.

3. **Latent Space Analysis**: Visualize and quantify the latent representations using PACMAP or UMAP to verify that the model escapes initial collapse and that image-inclusive variants show distinct clustering patterns compared to tag-only models.