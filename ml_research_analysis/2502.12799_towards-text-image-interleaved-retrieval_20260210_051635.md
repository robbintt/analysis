---
ver: rpa2
title: Towards Text-Image Interleaved Retrieval
arxiv_id: '2502.12799'
source_url: https://arxiv.org/abs/2502.12799
tags:
- image
- text
- interleaved
- query
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the text-image interleaved retrieval (TIIR)
  task, where both queries and documents contain interleaved text and images. To enable
  research in this direction, the authors construct the wikiHow-TIIR dataset by generating
  high-quality interleaved queries from wikiHow tutorials.
---

# Towards Text-Image Interleaved Retrieval

## Quick Facts
- arXiv ID: 2502.12799
- Source URL: https://arxiv.org/abs/2502.12799
- Authors: Xin Zhang, Ziqi Dai, Yongqi Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Jun Yu, Wenjie Li, Min Zhang
- Reference count: 32
- Introduces text-image interleaved retrieval (TIIR) task with novel dataset and Matryoshka Multimodal Embedder (MME)

## Executive Summary
This paper introduces the text-image interleaved retrieval (TIIR) task, where both queries and documents contain interleaved text and images. The authors construct the wikiHow-TIIR dataset by generating high-quality interleaved queries from wikiHow tutorials. They find that adapting existing single-image or text-only models to this task is challenging, motivating the need for specialized approaches. To address this, they propose a Matryoshka Multimodal Embedder (MME) that compresses visual tokens at different granularities, improving both performance and efficiency. Experiments show that MME outperforms strong baselines while using fewer visual tokens.

## Method Summary
The paper proposes the text-image interleaved retrieval (TIIR) task, which handles documents and queries containing both text and images in interleaved order. The authors construct the wikiHow-TIIR dataset from wikiHow tutorials, generating high-quality interleaved queries. They introduce the Matryoshka Multimodal Embedder (MME) that compresses visual tokens at multiple granularities, allowing for efficient and effective retrieval. The MME architecture leverages matryoshka representations to maintain performance while reducing computational costs through visual token compression.

## Key Results
- MME outperforms strong baselines on the wikiHow-TIIR dataset while using fewer visual tokens
- Interleaved context preservation is crucial for effective retrieval performance
- Visual token compression in MME achieves favorable performance-efficiency tradeoffs
- The proposed approach demonstrates scalability advantages over traditional multimodal models

## Why This Works (Mechanism)
The Matryoshka Multimodal Embedder works by leveraging hierarchical visual token compression that preserves essential information at multiple granularities. This allows the model to maintain retrieval accuracy while reducing computational costs. The matryoshka (Russian doll) architecture enables progressive compression, where higher-level tokens capture coarse-grained visual information while lower-level tokens retain fine details. This multi-scale representation is particularly effective for interleaved retrieval because it can efficiently handle the varying importance of visual elements across different document contexts.

## Foundational Learning

**Multimodal Retrieval**: Understanding how to combine text and image representations for search tasks is essential because real-world documents often contain both modalities. Quick check: Can the model handle documents with varying text-to-image ratios?

**Visual Token Compression**: Reducing the number of visual tokens while preserving semantic information is crucial for efficiency in large-scale retrieval. Quick check: What compression ratio maintains acceptable performance?

**Interleaved Context Modeling**: Processing text and images in their original interleaved order (rather than concatenating all text then all images) is important for preserving document structure. Quick check: How does the model handle long-range dependencies across modalities?

**Matryoshka Representations**: Hierarchical representations that allow for progressive compression are valuable for balancing accuracy and efficiency. Quick check: Can the model reconstruct or approximate original information from compressed tokens?

**Cross-Modal Alignment**: Ensuring that text and image representations are properly aligned in the shared embedding space is critical for retrieval performance. Quick check: Are the embeddings from different modalities comparable in the joint space?

## Architecture Onboarding

**Component Map**: Text Encoder -> Visual Encoder (with Matryoshka compression) -> Cross-Modal Fusion -> Retrieval Head

**Critical Path**: The critical path involves encoding both text and visual inputs, compressing visual tokens through the matryoshka layers, fusing the multimodal representations, and producing the final retrieval embedding. The visual token compression layer is particularly critical as it directly impacts both efficiency and performance.

**Design Tradeoffs**: The main tradeoff is between compression ratio and retrieval accuracy. Higher compression reduces computational costs but may lose important visual details. The matryoshka design allows for dynamic adjustment of this tradeoff based on deployment constraints.

**Failure Signatures**: Models may fail when visual compression removes critical discriminative features, when cross-modal alignment is poor, or when the interleaved context is not properly preserved during encoding. Common failure modes include retrieving documents with similar visual content but different textual context.

**First Experiments**: 
1. Ablation study varying compression ratios to establish the performance-efficiency curve
2. Comparison of interleaved vs. non-interleaved processing to quantify the importance of context preservation
3. End-to-end retrieval evaluation on the wikiHow-TIIR dataset with different model variants

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks detailed error analysis showing failure cases of MME and comparison baselines
- Dataset construction quality lacks quantitative validation through human evaluation or inter-annotator agreement scores
- No exploration of user-centered evaluation or real-world deployment scenarios for interleaved retrieval
- Efficiency claims need more thorough ablation studies across different compression ratios

## Confidence

**High confidence**: The core contribution of introducing the TIIR task and dataset construction methodology is well-supported and clearly presented. The experimental results demonstrating MME's performance advantages over baselines are robust and statistically significant.

**Medium confidence**: The efficiency claims regarding visual token compression need more thorough ablation studies across different compression ratios to establish the full tradeoff space. The analysis of how visual token compression impacts model behavior is suggestive but could benefit from more systematic investigation.

**Low confidence**: Claims about the general importance of preserving interleaved context would benefit from controlled experiments isolating this factor from other architectural differences between models.

## Next Checks

1. Conduct ablation studies varying the compression ratio in MME to establish the full performance-efficiency tradeoff curve and identify optimal configurations for different deployment scenarios.

2. Perform human evaluation studies to assess the quality and naturalness of the interleaved queries in the wikiHow-TIIR dataset, including inter-annotator agreement metrics.

3. Implement and evaluate a fine-tuning baseline using existing multimodal models (like BLIP or Flamingo) on the TIIR task to better understand the gap between adaptation and specialized architectures.