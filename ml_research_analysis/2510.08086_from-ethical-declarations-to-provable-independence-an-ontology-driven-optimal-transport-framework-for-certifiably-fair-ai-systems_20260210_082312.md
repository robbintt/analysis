---
ver: rpa2
title: 'From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport
  Framework for Certifiably Fair AI Systems'
arxiv_id: '2510.08086'
source_url: https://arxiv.org/abs/2510.08086
tags:
- bias
- fairness
- sensitive
- ontology
- algebra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for provably fair AI that overcomes
  the limits of current bias mitigation methods by systematically removing all sensitive
  information and its proxies. Using ontology engineering in OWL 2 QL, it formally
  defines sensitive attributes and infers their proxies through logical reasoning,
  constructing a sigma algebra G that captures the full structure of biased patterns.
---

# From Ethical Declarations to Provable Independence: An Ontology-Driven Optimal-Transport Framework for Certifiably Fair AI Systems

## Quick Facts
- arXiv ID: 2510.08086
- Source URL: https://arxiv.org/abs/2510.08086
- Reference count: 19
- Primary result: Framework achieves provable fairness by systematically removing sensitive information and proxies via ontology engineering and optimal transport

## Executive Summary
This paper presents a framework for provably fair AI systems that systematically removes all sensitive information and its proxies using ontology engineering and optimal transport. The approach overcomes limitations of current bias mitigation methods by formally defining sensitive attributes and inferring their proxies through logical reasoning, constructing a sigma algebra that captures biased patterns. Fair representations are obtained via Delbaen-Majumdar optimal transport, generating variables independent of the bias structure while preserving accuracy. The result is a certifiable, mathematically grounded method for trustworthy AI that guarantees true independence rather than mere decorrelation.

## Method Summary
The framework uses OWL 2 QL ontologies to encode domain knowledge about sensitive attributes and their proxies. A reasoner infers class memberships that represent biased information, which are then compiled into a sigma algebra G. The optimal transport component solves a Delbaen-Majumdar problem to project data into a fair representation Y that is independent of G while minimizing L2 distance to preserve accuracy. This mathematical approach ensures complete fairness by treating bias as dependence between sigma algebras rather than individual features.

## Key Results
- Systematically removes all sensitive information and proxies through ontology-based inference
- Guarantees true independence (not just decorrelation) between fair representations and biased information
- Provides mathematical certifiability of fairness through optimal transport theory

## Why This Works (Mechanism)

### Mechanism 1: Ontology-Based Proxy Compilation
The framework uses OWL 2-QL to systematically identify proxy attributes that correlation-based methods might miss. Domain knowledge is encoded in TBox axioms, and a reasoner infers class memberships (e.g., if `livesInZIP(x, y)` and `Redlined(y)`, then `ProxyForRace(x)`). These are mapped to subsets of the sample space Ω. The core assumption is that the ontology accurately captures the causal structure of bias; if a proxy path is missing, the method fails to detect it.

### Mechanism 2: Sigma-Algebra as the Structure of Bias
Bias is formalized as a sub-sigma-algebra G ⊆ F representing the complete "information content" of sensitive attributes. The framework constructs G by generating the smallest sigma-algebra containing all sensitive event sets. This mathematical closure ensures all logical combinations of biased events are captured. The core assumption is that the probability space is finite and discrete, allowing G to be computable via standard query answering.

### Mechanism 3: Optimal Transport for Independence
The framework achieves "true independence" by solving a Delbaen-Majumdar optimal transport problem to project data into a fair representation. It constructs a random variable Y such that Y ⊥⊥ G (independence) while minimizing the L2 distance E[||X - Y||^2] (accuracy preservation). This maps the original distribution to one where the conditional probability P(Y ∈ B | G) is constant.

## Foundational Learning

- **Concept:** **Measure-Theoretic Probability (σ-algebras)**
  - **Why needed here:** The paper moves beyond "features" to "information structures." You cannot understand how Y ⊥⊥ G differs from "dropping a column" without grasping that a σ-algebra represents total information available to an observer.
  - **Quick check question:** If G is the σ-algebra generated by "Race," does it contain information about "Zip Code"? (Only if Zip Code is mathematically derivable from Race in the dataset/ontology).

- **Concept:** **Description Logic (OWL 2 QL)**
  - **Why needed here:** The system automates proxy discovery. Understanding the trade-off between OWL 2 QL (query efficiency) vs. OWL 2 DL (expressivity) is crucial for knowing what logical rules can be written.
  - **Quick check question:** Can OWL 2 QL handle complex role inclusions (e.g., R ∘ S ⊆ T)? (The paper uses it for query rewriting, implying specific computational tractability).

- **Concept:** **Optimal Transport Theory**
  - **Why needed here:** This is the engine of the solution. Unlike adversarial debiasing, this maps distributions geometrically.
  - **Quick check question:** Why does minimizing Wasserstein distance (or L2 norm here) preserve utility better than adding random noise?

## Architecture Onboarding

- **Component map:** Knowledge Base (OWL file + Data) -> σ-Algebra Generator (converts ontology queries → binary mask matrix M) -> Solver (Fair Representation Engine - PyTorch implementation) -> Output (Certifiable fair variable Y)

- **Critical path:** The σ-Algebra Generator. If entailment queries (O ⊨ C(ω)) fail to identify a proxy, the optimal transport solver will optimize for the wrong objective, leaving bias intact.

- **Design tradeoffs:**
  - Completeness vs. Tractability: A richer ontology captures more bias but increases complexity of generating mask matrix and solving transport
  - Independence vs. Utility: Strict independence (Y ⊥⊥ G) generally lowers L2 accuracy compared to the original biased variable X

- **Failure signatures:**
  - Empty σ-algebra: Ontology axioms too strict; no proxies found; output Y ≈ X (high accuracy, high bias)
  - Total σ-algebra: Ontology axioms too loose; everything is a proxy; output Y is noise (zero bias, zero accuracy)
  - HSIC Failure: Post-hoc independence tests show p < 0.05, indicating optimal transport didn't fully converge or "atomless" condition was violated

- **First 3 experiments:**
  1. Unit Test the Reasoner: Verify known proxy (ZIP → Income) in controlled dummy dataset triggers ProxyClass assertion
  2. Toy Data Transport: Generate synthetic 2D data where one axis is sensitive attribute; visualize optimal transport map to confirm it "smears" data while preserving global shape
  3. Ablation on Ontology: Run full pipeline on COMPAS with empty TBox vs. rule-based TBox to measure delta in fairness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the proposed optimal transport framework maintain predictive accuracy while guaranteeing fairness when applied to large-scale, real-world benchmark datasets?
- **Basis in paper:** Section 7 states empirical validation across diverse domains remains essential
- **Why unresolved:** Paper presents theoretical blueprint and conceptual case study but lacks experimental results on standard fairness benchmarks
- **What evidence would resolve it:** Quantitative results from PyTorch implementation showing accuracy-fairness trade-offs on COMPAS or CelebA

### Open Question 2
- **Question:** Can computational complexity of discrete optimal transport solver be sufficiently reduced to handle datasets with millions of rows?
- **Basis in paper:** Section 6.4 notes naive complexity is O(N^2d) and admits performance estimates using Sinkhorn iterations "remain to be validated"
- **Why unresolved:** Authors propose GPU acceleration to reduce wall-clock time, but implementation is currently just "planned" blueprint
- **What evidence would resolve it:** Profiling data demonstrating end-to-end latency for datasets where N ≈ 10^6

### Open Question 3
- **Question:** How robust is ontology-guided sigma algebra G_O under distribution shift or data drift?
- **Basis in paper:** Section 7 identifies "behavior under distribution shift" as direction that "merit[s] careful study"
- **Why unresolved:** Framework relies on fixed ontology and probability space; unclear if independence guarantees hold when underlying data distribution evolves
- **What evidence would resolve it:** Analysis of fairness drift metrics over time-series data where correlation between proxies and sensitive attributes changes

## Limitations

- **Ontology completeness:** Framework's effectiveness depends entirely on domain experts accurately encoding all bias pathways in the ontology
- **Computational scalability:** Naive optimal transport complexity is O(N^2d), making it potentially intractable for large datasets
- **Conditional atomlessness:** The theoretical guarantee of unique independent solution requires an unproven assumption about the data distribution

## Confidence

- **High Confidence:** Mathematical framework of representing bias as sub-sigma-algebra G is theoretically sound and well-established
- **Medium Confidence:** OWL 2 QL reasoner will correctly identify explicit proxy relationships encoded in ontology
- **Low Confidence:** Framework will achieve true independence on real-world datasets with imperfect ontologies and complex distributions

## Next Checks

1. **Ontology Coverage Experiment:** Evaluate framework performance on COMPAS using ontologies of varying completeness (empty, rule-based, expert-curated) and measure resulting fairness metrics

2. **Conditional Atomlessness Test:** Empirically test the "atomless" condition on datasets with discrete sensitive attributes; if violated, propose and validate relaxation for atomic spaces

3. **Scalability Benchmark:** Implement full pipeline on large credit scoring database (millions of records) and measure runtime for generating G and solving optimal transport, comparing to adversarial debiasing methods