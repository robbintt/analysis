---
ver: rpa2
title: '"Just a strange pic": Evaluating ''safety'' in GenAI Image safety annotation
  tasks from diverse annotators'' perspectives'
arxiv_id: '2507.16033'
source_url: https://arxiv.org/abs/2507.16033
tags:
- annotators
- safety
- harm
- moral
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines how annotators evaluate the safety of AI-generated
  images, focusing on qualitative reasoning beyond structured categories. Analyzing
  5,372 open-ended comments from 637 diverse annotators, the study finds that annotators
  invoke moral, emotional, and contextual reasoning not captured by standard taxonomies.
---

# "Just a strange pic": Evaluating 'safety' in GenAI Image safety annotation tasks from diverse annotators' perspectives

## Quick Facts
- arXiv ID: 2507.16033
- Source URL: https://arxiv.org/abs/2507.16033
- Authors: Ding Wang; Mark Díaz; Charvi Rastogi; Aida Davani; Vinodkumar Prabhakaran; Pushkar Mishra; Roma Patel; Alicia Parrish; Zoe Ashwood; Michela Paganini; Tian Huey Teh; Verena Rieser; Lora Aroyo
- Reference count: 19
- Key outcome: Diverse annotators invoke moral, emotional, and contextual reasoning in AI image safety tasks, revealing gaps in standard taxonomies and showing that harm-to-others perceptions differ systematically across demographic groups.

## Executive Summary
This paper examines how annotators evaluate the safety of AI-generated images, focusing on qualitative reasoning beyond structured categories. Analyzing 5,372 open-ended comments from 637 diverse annotators, the study finds that annotators invoke moral, emotional, and contextual reasoning not captured by standard taxonomies. They consistently perceive harm to others as greater than harm to themselves, with this difference varying across demographic groups. Image quality, distortions, and prompt-image mismatches also contribute to perceived harm dimensions. Annotators' comments reveal gaps in existing frameworks, showing that safety judgments are shaped by moral foundations like Care and Purity. The findings call for evaluation designs that scaffold moral reflection, differentiate harm types, and integrate subjective, context-sensitive insights into AI safety assessments.

## Method Summary
The study analyzed 1,000 adversarial prompt-image pairs from the Adversarial Nibbler dataset, annotated by 637 annotators stratified across 30 demographic intersections (gender × age × ethnicity). Annotators rated harm-to-self and harm-to-others on 5-point scales, selected from predefined harm categories, and provided optional open comments. Analysis combined thematic analysis of comments, moral sentiment classification using a GPT-4o autorater (validated on Moral Foundations Reddit Corpus), permutation tests for demographic differences, and regression analysis linking moral foundations to harm scores and agreement patterns.

## Key Results
- Annotators consistently rated harm-to-others higher than harm-to-self, with this gap varying across demographic groups (women and non-White annotators showed smaller deltas)
- Image quality artifacts and prompt-image mismatches contributed to perceived harm dimensions, with annotators interpreting distortions as safety concerns
- Expression of Care, Equality, and Purity moral foundations in comments correlated with higher harm ratings and lower annotator agreement
- 33.4% of annotators provided zero comments, while 16.8% provided all their comments, suggesting vocal annotators may overrepresent certain perspectives

## Why This Works (Mechanism)

### Mechanism 1: Moral Foundation Elicitation
- Claim: Annotators apply moral reasoning frameworks (Care, Purity, Equality) that extend beyond predefined safety categories, influencing both harm scores and disagreement patterns.
- Mechanism: When adversarial prompts generate sensitive imagery, annotators activate moral intuitions—particularly Care (harm prevention) and Purity (disgust/contamination)—which the task instructions do not explicitly scaffold. These moral sentiments correlate with higher harmfulness ratings and lower annotator agreement.
- Core assumption: Moral Foundations Theory accurately captures the structure of annotators' ethical reasoning in this context.
- Evidence anchors:
  - [abstract] "Annotators' comments reveal gaps in existing frameworks, showing that safety judgments are shaped by moral foundations like Care and Purity."
  - [Findings, p.7] "A linear regression analysis shows that expression of Care (β = .79, p < .001), Equality (β = .76, p < .001), and Purity (β = .85, p < .001) are all associated with an increase in the level of harmfulness."
  - [corpus] PluriHarms paper addresses similar limitations of binary harm frameworks, suggesting this is a broader pattern in safety evaluation—though causal mechanisms remain underexplored.
- Break condition: If task instructions explicitly scaffold moral reflection, the gap between imposed categories and annotator reasoning may narrow.

### Mechanism 2: Dual-Perspective Harm Attribution
- Claim: Separating harm-to-self from harm-to-others ratings reveals systematic demographic differences in harm sensitivity that would be obscured by single-score approaches.
- Mechanism: Annotators rate harm-to-others higher than harm-to-self overall, but the magnitude of this difference varies by demographic group. Women and non-White annotators showed smaller self-other gaps, potentially reflecting differential sensitivity to collective vulnerability.
- Core assumption: The self-other rating difference reflects genuine differences in harm conceptualization rather than task artifact.
- Evidence anchors:
  - [abstract] "They consistently perceive harm to others as greater than harm to themselves, with this difference varying across demographic groups."
  - [Findings, p.7, Table 1] Women showed a score delta of 0.22 vs. 0.36 for men; Black annotators showed 0.14 vs. 0.34 for White annotators.
  - [corpus] Limited direct corpus evidence on this specific mechanism; related work on annotation subjectivity exists but does not replicate this dual-rating design.
- Break condition: If "others" is undefined, annotators may implicitly reference different populations, reducing comparability across annotators.

### Mechanism 3: Quality-Safety Conflation
- Claim: Annotators interpret image quality artifacts (distortions, glitches, prompt-image mismatches) as semantically meaningful safety signals rather than neutral technical flaws.
- Mechanism: Visual distortions trigger emotional responses (fear, disgust, uncanniness) that annotators map onto harm categories even when the task does not instruct them to consider quality. This conflates technical failure with safety violation.
- Core assumption: Annotators cannot or will not fully separate quality assessment from safety assessment when both are perceptually salient.
- Evidence anchors:
  - [abstract] "Image quality, distortions, and prompt-image mismatches also contribute to perceived harm dimensions."
  - [Findings, p.5] "Annotators often described distortions or visual glitches, such as disfigured faces, as 'disturbing' or 'indicative of harm', rather than as neutral technical flaws."
  - [corpus] Computational Safety for Generative AI paper addresses signal processing perspectives on safety but does not specifically address quality-safety conflation—evidence gap.
- Break condition: Explicit separation of quality and safety assessment prompts may reduce conflation, though this remains untested.

## Foundational Learning

- Concept: **Moral Foundations Theory (MFT)**
  - Why needed here: The paper uses MFT to analyze annotator reasoning; understanding Care, Purity, Equality, Proportionality, Loyalty, and Authority helps interpret why annotators disagree beyond simple "noise."
  - Quick check question: Can you name at least three moral foundations and explain how each might manifest in an image safety judgment?

- Concept: **Harm-to-Self vs. Harm-to-Others Distinction**
  - Why needed here: The study's key finding is that these ratings differ systematically; understanding this distinction is critical for designing tasks that capture the intended signal.
  - Quick check question: Why might a demographic group show a smaller gap between self and other harm ratings?

- Concept: **CrowdTruth / Disagreement-as-Signal**
  - Why needed here: The paper uses CrowdTruth's unit quality scores and treats disagreement as meaningful rather than noise. This frames the entire analytical approach.
  - Quick check question: What does low annotator agreement suggest in a safety annotation context, under this framework?

## Architecture Onboarding

- Component map:
  - 1000 Adversarial Nibbler prompt-image pairs -> 637 annotators (demographically stratified) -> Dual harm ratings (self/other) -> Structured harm categories + optional comments -> Thematic analysis + moral sentiment autorater -> Regression analysis of moral foundations to harm scores

- Critical path:
  1. Recruit diverse annotator pool (demographics must be collected upfront)
  2. Present prompt-image pairs with dual harm questions (self/other)
  3. Collect structured labels AND optional open comments
  4. Apply moral sentiment classifier to both prompts and comments
  5. Compare harm score deltas across demographic groups and content types

- Design tradeoffs:
  - **Optional vs. required comments**: Optional comments yielded 16.8% commenting rate, sufficient for thematic analysis but may miss quieter annotators
  - **Predefined harm categories vs. open-ended**: Predefined categories standardize but miss emotional/moral reasoning; open comments capture nuance but require qualitative analysis
  - **Diverse vs. representative pool**: Diverse pool surfaces differential sensitivity but does not enable population-level inference

- Failure signatures:
  - High unit quality scores for harm-to-self but lower for harm-to-others may indicate "others" is under-specified
  - Moral sentiment autorater shows low recall for Purity (0.20) and Loyalty (0.28)—interpret classification results cautiously
  - Annotators leaving zero comments (33.4%) may have systematically different reasoning patterns than frequent commenters

- First 3 experiments:
  1. **A/B test explicit "others" definition**: Provide half of annotators with a specific definition of "others" (e.g., general public, vulnerable groups) and compare score deltas and agreement rates.
  2. **Separate quality and safety prompts**: Add a dedicated image quality rating before safety questions to test whether conflation decreases.
  3. **Scaffold moral reflection**: Include a moral reflection prompt (e.g., "Does this image conflict with any values you hold?") and measure whether moral foundation expression in comments increases and whether agreement patterns shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can annotation task designs effectively scaffold moral reflection without unduly influencing annotators' natural reasoning processes?
- Basis in paper: [explicit] The authors state "it remains unclear how much evaluators can or should intervene in the natural reasoning processes of annotators" and call for designs that "scaffold moral reflection."
- Why unresolved: The paper identifies the tension but does not test specific scaffolding approaches or measure their effects on annotation quality or annotator experience.
- What evidence would resolve it: Experimental comparison of different scaffolding interventions (e.g., moral foundation prompts, reflection questions) measuring both alignment with task goals and preservation of annotator agency.

### Open Question 2
- Question: What demographic or experiential factors explain the systematic differences in harm-to-self versus harm-to-others ratings across groups, given that identity representation in images did not account for these patterns?
- Basis in paper: [explicit] The authors found demographic subgroups showed different rating patterns "that were not driven by specific sensitivity to their own identity representation" and acknowledge "it is not clear if annotators from different demographic subgroups were considering different 'others.'"
- Why unresolved: The study could not determine whether demographic rating differences stemmed from distinct interpretations of who "others" refers to, different assumptions about sensitivity, or broader sociocultural reasoning patterns.
- What evidence would resolve it: Follow-up studies that explicitly ask annotators to define who they consider "others" and probe the reasoning behind their harm-to-others ratings through interviews or structured questions.

### Open Question 3
- Question: How should safety evaluation frameworks disentangle image quality artifacts from content safety when annotators interpret technical distortions as semantically meaningful harms?
- Basis in paper: [explicit] The authors note that "annotators often interpreted quality artifacts as semantically meaningful" and state this "reveals a critical insight" about sources of harm "not typically captured in task design," calling for approaches that "disentangle these dimensions."
- Why unresolved: The study documents the entanglement but does not propose or test methods for separating quality-related perceptions from content-based harm assessments.
- What evidence would resolve it: Experimental task designs that separately elicit quality and safety judgments, followed by analysis of whether and how these dimensions interact and can be meaningfully distinguished.

## Limitations

- The 16.8% comment rate means qualitative insights may overrepresent vocal annotators rather than the full participant pool
- The moral sentiment autorater shows notably lower recall for Purity (0.20) and Loyalty (0.28), suggesting potential blind spots in detecting certain moral intuitions
- Demographic analyses rely on a non-representative sample of 637 annotators across 30 intersections, limiting generalizability to population-level patterns
- The undefined "others" in harm-to-others questions may have led to inconsistent interpretation across annotators

## Confidence

- **High**: Image quality and distortions contribute to perceived harm dimensions
- **High**: Moral foundations (Care, Equality, Purity) are expressed in annotator comments and correlate with harm ratings
- **Medium**: Systematic demographic differences in self-other harm perception (though sample composition limits inference)
- **Low**: Specific quantitative effect sizes for moral foundations on disagreement (due to potential autorater noise)

## Next Checks

1. Replicate the demographic score delta patterns using a larger, population-representative annotator pool
2. Implement explicit "others" definition (e.g., "vulnerable populations" vs. "general public") and measure impact on agreement and score patterns
3. Conduct a controlled study separating image quality assessment from safety evaluation to quantify quality-safety conflation effects