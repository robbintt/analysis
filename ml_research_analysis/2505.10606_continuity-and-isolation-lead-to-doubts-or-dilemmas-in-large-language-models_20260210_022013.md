---
ver: rpa2
title: Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models
arxiv_id: '2505.10606'
source_url: https://arxiv.org/abs/2505.10606
tags:
- sequence
- sequences
- python
- input
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates two fundamental limitations of decoder-only
  Transformers with compact positional encoding: continuity and isolation. Continuity
  implies that small input perturbations lead to small output changes, while isolation
  means that any two sequences learnable by the same Transformer must be sufficiently
  far apart in relative Hamming distance.'
---

# Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models

## Quick Facts
- arXiv ID: 2505.10606
- Source URL: https://arxiv.org/abs/2505.10606
- Reference count: 40
- Primary result: Decoder-only Transformers with compact positional encoding cannot learn all simple periodic sequences and are insensitive to small input perturbations, revealing fundamental representational constraints.

## Executive Summary
This paper proves two fundamental limitations of decoder-only Transformers with compact positional encoding: continuity (small input changes produce small output changes) and isolation (learnable sequences must be sufficiently far apart in relative Hamming distance). These mathematical results explain why certain simple patterns are unlearnable despite appearing straightforward to humans. The authors validate these limitations empirically across modern LLMs, showing that small perturbations rarely affect predictions unless they exceed model-specific thresholds, and that models struggle to learn periodic sequences beyond critical period lengths.

## Method Summary
The study evaluates continuity by measuring next-token prediction changes when sequences are randomly perturbed, using NTS_γ to quantify sensitivity to γ-proportion input changes. Isolation is tested by attempting to learn periodic sequences β^r_p = (0^(p-1)1)^r 0 for periods p ∈ [2,40], measuring success rate and certainty of continuation. Experiments use inference-only evaluation across 11 models (GPT-2, Gemma, Llama, Phi families) with 100 samples per condition. Position sensitivity is analyzed using Beta-Binomial sampling to bias perturbations toward sequence ends.

## Key Results
- Small input perturbations (γ < 0.05) rarely affect next-token predictions in most models, demonstrating continuity.
- Models fail to learn periodic sequences beyond critical period lengths, with performance degrading as pattern complexity increases.
- Position-sensitive analysis shows perturbations near sequence ends have larger effects than those near the beginning.
- Theoretical proofs establish that these limitations arise from compact positional encoding and cannot be overcome without breaking continuity.

## Why This Works (Mechanism)

### Mechanism 1: Continuity Under Compact Positional Encoding
The continuity proof shows that for sequences sharing the same final token with relative Hamming distance ≤ δ, output distributions are bounded by ε. This arises because compact positional encodings (bounded in norm) combined with continuous attention/value/activation functions preserve input proximity through all layers. The δ threshold depends only on the Transformer, not on sequence length.

### Mechanism 2: Isolation Through Representational Collapse
Continuity implies that within a ball of radius δ around a learned sequence α, any other sequence β gets mapped to nearly identical output distributions. If β differs from α at infinitely many positions, the Transformer makes infinitely many mistakes predicting β. The isolation theorem formalizes this: for any eventually learnable α, there exists δ > 0 such that no β with dH(α, β) ≤ δ and infinitely many differences is also learnable.

### Mechanism 3: Position-Dependent Sensitivity in Practice
Decoder-only architectures with causal masking create asymmetric attention distribution where earlier tokens accumulate more attention mass across layers. Late-position perturbations affect fewer downstream computations but are closer to the final prediction point. Experiments confirm: flips near position n cause more next-token changes than flips near position 1.

## Foundational Learning

- **Relative Hamming Distance (dH)**: Measures asymptotic fraction of positions where two infinite sequences differ. Why needed: Core metric defining isolation; determines which sequence families are mutually exclusive. Quick check: If sequences α and β differ at 5 positions out of 100, what is dH(α₁...₁₀₀, β₁...₁₀₀)?

- **Compact Positional Encoding**: Positional embeddings bounded within a compact set K ⊆ ℝᵈ for all positions. Why needed: The paper's theorems require bounded positional embeddings; distinguishes which architectures these limitations apply to. Quick check: Does rotary positional encoding (RoPE) qualify as compact? Why or why not?

- **Attractor Basin / Representational Collapse**: Explains why similar inputs produce identical outputs; underlies both continuity benefits and isolation limitations. Why needed: Core concept explaining why Transformers must choose between learning certain simple patterns while being unable to learn others. Quick check: If all sequences within distance δ of α produce the same next-token prediction, what happens to a sequence β with dH(α, β) = δ/2 that should logically have a different continuation?

## Architecture Onboarding

- **Component map**: Input embedding e(σ, i) -> Attention layer L -> Activation F -> Projection P
- **Critical path**: Identify CPE usage -> Determine δ threshold experimentally -> Map mutually exclusive sequence families based on dH distances -> Assess task requirements for distinguishing high-dH vs low-dH sequences
- **Design tradeoffs**:
  - Standard softmax + CPE: Stable predictions, but cannot learn all periodic patterns
  - ssmax (log-length scaling): Higher sensitivity to perturbations, potentially breaks continuity
  - Chain-of-thought (unbounded iterations): May escape limitations but not covered by current theory
- **Failure signatures**: Perfect learning of pattern A but failure on seemingly equally simple pattern B; unchanged predictions under small input corruption; periodic generation succeeding up to critical period then abruptly failing
- **First 3 experiments**:
  1. Zero-sequence perturbation test: Generate α = 190 zeros; create β variants with γ% random flips; measure NTS_γ
  2. Periodic pattern extrapolation: For periods p ∈ {2, 4, 8, ..., 40}, prompt with r repetitions of (0^(p-1)1); measure success rate and certainty gap
  3. Minimal syntax pair test: Build prompt pairs (α, β) differing in one token that changes correct answer; plot P(σ|α) vs P(σ|β)

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework excludes models with log-length scaling or unbounded positional embeddings
- Periodic sequence experiments use artificially simple patterns not reflecting real-world complexity
- Python syntax error dataset is not publicly provided
- Fine-tuning scenarios may exhibit different limitations not captured by inference-only experiments

## Confidence

**High Confidence (90%+):** Mathematical proofs of continuity and isolation theorems are rigorous and well-established.

**Medium Confidence (70-80%):** Interpretation that isolation prevents learning certain periodic sequences is supported but not definitively proven for all sequence families.

**Low Confidence (50-60%):** Practical impact on real-world LLM performance remains speculative despite mathematical limitations existing.

## Next Checks

1. Test continuity and isolation experiments on models using log-length scaling (ssmax) or unbounded positional encodings to confirm when limitations apply.

2. Fine-tune models on periodic sequence tasks and syntax error detection to determine if training can overcome or mitigate inherent limitations.

3. Construct more complex, naturally occurring patterns (e.g., code structure, linguistic patterns) subject to these limitations and test whether models fail in predictable ways consistent with theoretical framework.