---
ver: rpa2
title: 'OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World
  Data'
arxiv_id: '2510.15096'
source_url: https://arxiv.org/abs/2510.15096
tags:
- your
- uncertainty
- about
- reasoning
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenEstimate is a benchmark for evaluating language models on reasoning
  under uncertainty using real-world data. It constructs derived variables from public
  datasets (Glassdoor, Pitchbook, NHANES) and asks models to express beliefs as Bayesian
  priors.
---

# OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data

## Quick Facts
- arXiv ID: 2510.15096
- Source URL: https://arxiv.org/abs/2510.15096
- Reference count: 40
- Primary result: Frontier language models are systematically overconfident and often underperform simple statistical baselines on probabilistic estimation tasks

## Executive Summary
OpenEstimate is a benchmark for evaluating language models on reasoning under uncertainty using real-world data. It constructs derived variables from public datasets (Glassdoor, Pitchbook, NHANES) and asks models to express beliefs as Bayesian priors. Models are prompted to parameterize these priors as Gaussian or Beta distributions. Performance is measured in terms of accuracy (error ratio relative to statistical baselines) and calibration (quartile expected calibration error). Across six frontier models, elicited priors are often inaccurate and overconfident, with error ratios no better than five samples from the true distribution. Performance varies by domain and is largely unaffected by changes in temperature, reasoning effort, or prompt design. The benchmark offers a challenging evaluation for frontier models and a platform for developing models that better handle probabilistic estimation and reasoning under uncertainty.

## Method Summary
The benchmark generates 178 derived variables from three public datasets by sampling conditional statistics with 1-3 attributes, filtering for minimum sample size and meaningful variation from marginal statistics. Zero-shot evaluation uses three elicitation protocols (Direct, Quantile, Mean-Variance) to prompt models for Gaussian or Beta distribution parameters. Models include GPT-4, o3-mini/o4-mini, Llama 3.1 70B, and Qwen3-235B-A22B. Accuracy is measured via MAE, error ratio, and win rate against N-sample statistical baselines; calibration is assessed using Expected Calibration Error across quartile bins.

## Key Results
- Elicited priors from frontier models are often inaccurate and overconfident
- Error ratios show no better performance than five samples from the true distribution
- Best models cover 80% of ground truth values within two to three standard deviations rather than the ~95% implied by 2σ for well-calibrated Gaussians

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Derived conditional variables reduce information leakage while preserving evaluability
- Mechanism: The benchmark constructs fine-grained conditional statistics from public datasets that are empirically unlikely to appear verbatim in pretraining corpora, unlike marginal statistics
- Core assumption: Conditional statistics with 2-3 attributes are sufficiently rare in pretraining data to elicit reasoning rather than retrieval
- Evidence anchors: [abstract] "derived conditional random variables which are systematically generated using existing public, observational datasets"; [section 2.1] "conditional variants... represent fine-grained combinations of attributes that are almost never reported in textual sources"

### Mechanism 2
- Claim: Distributional elicitation exposes calibration failures invisible to point-estimate evaluation
- Mechanism: By requiring models to output full probability distributions, the benchmark evaluates both where the model places its central estimate and how tightly it concentrates probability mass
- Core assumption: Gaussian/Beta parameterizations adequately capture models' true beliefs about these quantities
- Evidence anchors: [abstract] "elicited priors are often inaccurate and overconfident"; [section 3.1] "best models cover 80% of ground truth values within two to three standard deviations"

### Mechanism 3
- Claim: Elicitation protocol modulates expressed uncertainty, with domain-dependent effects
- Mechanism: Different prompting strategies influence how much uncertainty models express, improving calibration when increased uncertainty is expressed
- Core assumption: The elicitation protocol affects expressed uncertainty rather than underlying beliefs
- Evidence anchors: [abstract] "performance improves modestly depending on how uncertainty is elicited from the model"; [section 3.2] "elicitation protocol has an effect on calibration"

## Foundational Learning

- Concept: Bayesian prior vs. posterior
  - Why needed here: The benchmark evaluates priors (beliefs before data) against posteriors (updated beliefs after observing samples)
  - Quick check question: If an LM prior has mean 100 with σ=15, and you observe 5 samples with mean 95, which distribution should you trust more for prediction?

- Concept: Calibration in probabilistic prediction
  - Why needed here: The core finding is that LMs are overconfident; they assign narrow distributions that exclude ground truth too often
  - Quick check question: A model predicts a 90% confidence interval [80, 120]. If the ground truth falls outside this interval 30% of the time across many predictions, is the model overconfident, underconfident, or well-calibrated?

- Concept: Parametric distribution specification (Gaussian/Beta)
  - Why needed here: Models must output distribution parameters (μ, σ for Gaussian; α, β for Beta)
  - Quick check question: For a Beta(α=2, β=8) distribution, what is the implied mean? What happens to uncertainty if you double both parameters?

## Architecture Onboarding

- Component map: Variable Generator -> Prior Elicitation Module -> Evaluation Layer -> Baseline Generator
- Critical path: Variable generation → Prompt construction → LM inference → Parameter parsing → Ground truth comparison → Metric aggregation
- Design tradeoffs:
  - Gaussian/Beta assumption: Simplifies evaluation but may not match true distributions (skewed variables, multimodality)
  - Sample size threshold (n): Filters noisy estimates but reduces benchmark coverage
  - Conditioning depth (k=0-3): More conditions increase difficulty but may produce unintelligible queries
- Failure signatures:
  - Llama 3.1 8B excluded for instruction-following failures (units errors producing orders-of-magnitude MAE)
  - Overconfident narrow distributions: Ground truth consistently falls in outer quartiles
  - Systematic overestimation: All models overweight upper tails across domains
- First 3 experiments:
  1. Replicate zero-shot evaluation on a single domain with 2 model families, comparing error ratio and ECE against the 5-sample baseline
  2. Ablate elicitation protocol: Run direct vs. quantile elicitation on 20 variables, measuring change in expressed σ and corresponding calibration shift
  3. Test information leakage proxy: Compare model performance on marginal statistics vs. 3-condition statistics; larger gaps suggest greater reasoning demands

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be trained or prompted to autonomously select appropriate distributional forms (beyond Gaussian and Beta) for their probabilistic priors?
- Basis in paper: [explicit] The authors state: "The benchmark itself is agnostic to the choice of parameterization, and future work could evaluate the extent to which models can also select an appropriate functional form for their priors."
- Why unresolved: Current experiments constrain models to Gaussian or Beta distributions, leaving open whether models can correctly identify when skewed, multimodal, or heavy-tailed distributions would be more appropriate
- What evidence would resolve it: Experiments where models freely specify distribution families, evaluated against ground-truth empirical distributions

### Open Question 2
- Question: Do training-time interventions for uncertainty awareness improve calibration on open-ended estimation tasks?
- Basis in paper: [explicit] The authors note: "studying training-time interventions for uncertainty awareness and domain adaptation would be a complementary next step in future work."
- Why unresolved: All experiments use zero-shot inference without fine-tuning or specialized training
- What evidence would resolve it: Comparing priors from baseline models versus models fine-tuned on uncertainty-aware objectives

### Open Question 3
- Question: Can retrieval-augmented generation improve prior accuracy without causing information leakage?
- Basis in paper: [inferred] The authors evaluate only zero-shot methods "without retrieval augmentation" as a deliberate limitation
- Why unresolved: Retrieval could provide relevant contextual information for estimation, but may surface exact statistics that bypass genuine reasoning
- What evidence would resolve it: Ablations comparing RAG-enabled models using controlled document corpora against zero-shot baselines

## Limitations
- Information leakage risk from pretraining on public observational datasets, though conditional statistics are claimed to be rare in textual sources
- Gaussian and Beta parameterizations may poorly capture true variable distributions (skewed, multimodal, or heavy-tailed)
- Modest effects of elicitation protocol changes suggest limited exploration of prompt engineering space

## Confidence
- High Confidence: The core finding that frontier LMs are systematically overconfident and often underperform simple statistical baselines
- Medium Confidence: The claim that conditional statistics effectively prevent information retrieval
- Low Confidence: The assertion that elicitation protocol is the primary lever for calibration improvement

## Next Checks
1. Systematically test model performance on marginal vs. conditional statistics across increasing conditioning depth to quantify reasoning vs. retrieval
2. Extend the benchmark to support mixture distributions and heavy-tailed families to determine if richer parameterization improves accuracy and calibration
3. Design and test a broader space of elicitation strategies including chain-of-thought reasoning and multi-step refinement to identify optimal approaches for different models and domains