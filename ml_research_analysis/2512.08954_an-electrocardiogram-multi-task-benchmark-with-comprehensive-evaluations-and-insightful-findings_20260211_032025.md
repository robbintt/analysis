---
ver: rpa2
title: An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and
  Insightful Findings
arxiv_id: '2512.08954'
source_url: https://arxiv.org/abs/2512.08954
tags:
- foundation
- time-series
- data
- tasks
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study benchmarks the effectiveness of foundation models on
  ECG analysis, comparing them with traditional time-series deep learning models across
  five tasks: RR interval estimation, age estimation, gender classification, potassium
  abnormality prediction, and arrhythmia detection. Evaluated models include general
  time-series and ECG-specific foundation models (TSFM, ECG-FM), large language models
  (LLM), and time-series deep learning models (TSDL).'
---

# An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings

## Quick Facts
- arXiv ID: 2512.08954
- Source URL: https://arxiv.org/abs/2512.08954
- Authors: Yuhao Xu; Jiaying Lu; Sirui Ding; Defu Cao; Xiao Hu; Carl Yang
- Reference count: 18
- Primary result: TSFM and ECG-FM foundation models achieve 80% top performance rate on ECG tasks, outperforming LLMs and TSDL models.

## Executive Summary
This paper benchmarks foundation models on ECG analysis across five clinically relevant tasks: RR interval estimation, age estimation, gender classification, potassium abnormality prediction, and arrhythmia detection. The study evaluates seven models including general time-series foundation models (TSFM), ECG-specific foundation models (ECGFM), large language models (LLM), and traditional time-series deep learning models (TSDL). Results show that foundation models significantly outperform LLMs and traditional TSDL approaches when sufficient fine-tuning data is available, though they require adequate samples to achieve good performance. The study provides comprehensive evaluation metrics and insightful findings about the strengths and limitations of current foundation models for physiological waveform analysis.

## Method Summary
The study benchmarks foundation models on ECG analysis using the MIMIC-IV-ECG dataset containing 800,035 diagnostic ECGs from 161,352 patients. Five downstream tasks are evaluated: RR interval estimation and age estimation (regression), gender classification and potassium abnormality prediction (binary classification), and arrhythmia detection (15-class classification). Seven models are compared: TimesNet, DLinear (TSDL); GPT-2, Llama 3.1 (LLM); MOMENT, TEMPO (TSFM); and ECG-FM (ECGFM). Models are evaluated under three regimes: zero-shot, few-shot, and fine-tuning. The evaluation uses MIMIC-IV-ECG with 12-lead, 10-second ECGs at 500 Hz, downsampled to match each model's input requirements. Performance is measured using MAE for regression tasks and F1/Accuracy for classification tasks.

## Key Results
- TSFM and ECG-FM foundation models achieve a top performance rate of 80% across all tasks, significantly outperforming LLMs and traditional TSDL models.
- General TSFM models perform competitively with specialized ECGFM models, suggesting temporal dynamics transfer effectively from non-physiological time-series to ECG.
- Foundation models require sufficient fine-tuning data, with zero-shot and few-shot performances insufficient for clinical ECG tasks.
- LLM performance degrades significantly when processing ECG data through feature-based prompts, highlighting the importance of raw signal preservation.

## Why This Works (Mechanism)

### Mechanism 1
Time-series foundation models (TSFM) and ECG-specific foundation models (ECGFM) outperform large language models (LLMs) and traditional time-series deep learning (TSDL) models for ECG analysis tasks. Pre-training on time-series or ECG data enables models to learn temporal dynamics, waveform morphology, and signal-level representations directly from raw ECG traces. LLMs receive ECG data via prompt-engineered features, which discards critical temporal information. TSFM/ECGFM leverage masked prediction and autoregressive objectives on waveform data, preserving sequential structure.

### Mechanism 2
Specialized ECG foundation models (ECGFM) do not significantly outperform general time-series foundation models (TSFM) on downstream ECG tasks. General TSFM pre-training on diverse time-series domains (ETT, Monash, UCR/UEA) provides a robust understanding of temporal dynamics—trends, seasonality, and local variation—that transfers effectively to ECG. ECG-specific pre-training adds marginal benefit because the core temporal modeling capability is already captured.

### Mechanism 3
Foundation models require sufficient fine-tuning data; zero-shot and few-shot performance is insufficient for clinical ECG tasks. Pre-training data (general time-series or even ECG) differs in distribution from target downstream tasks. Without fine-tuning, models cannot align learned representations to task-specific ECG patterns, label schemas, or clinical definitions.

## Foundational Learning

- **Self-supervised learning (SSL) for time-series (masked prediction, autoregressive forecasting)**
  - Why needed here: TSFM and ECGFM use SSL objectives to learn representations without labels. Understanding masking and forecasting pre-training is essential to interpret why these models transfer.
  - Quick check question: Can you explain how masked time-series prediction differs from autoregressive forecasting in terms of what the model learns?

- **Foundation model transfer and fine-tuning paradigms (zero-shot, few-shot, full fine-tuning)**
  - Why needed here: The benchmark explicitly evaluates three regimes. Knowing when each applies—and their data requirements—is critical for practical deployment.
  - Quick check question: For a new ECG classification task with 500 labeled examples, which regime would you start with and why?

- **ECG signal basics (leads, sampling rate, RR intervals, arrhythmia types)**
  - Why needed here: The five downstream tasks range from simple (RR interval) to complex (multi-class arrhythmia detection). Interpreting saliency maps and failure modes requires basic ECG literacy.
  - Quick check question: What does the RR interval represent, and why might it be easier to predict than arrhythmia type?

## Architecture Onboarding

- **Component map:**
  Input: 12-lead ECG, 10-second strips at 500 Hz (6000 samples). Downsampled to match each model's required input length.
  Backbone options: TimesNet/DLinear (TSDL), GPT-2/Llama 3.1 (LLM via prompt), MOMENT/TEMPO (TSFM), ECG-FM (ECGFM).
  Heads: Regression (RR, Age), Binary classification (Gender, Potassium), Multi-class classification (Arrhythmia, 15 classes).
  Training modes: Zero-shot, few-shot, fine-tuning.

- **Critical path:**
  1. Data preprocessing: Load MIMIC-IV-ECG, resample to target input length, normalize.
  2. Load pre-trained checkpoint for chosen backbone (MOMENT, TEMPO, or ECG-FM recommended).
  3. Attach task-specific head; initialize randomly.
  4. Fine-tune on labeled downstream task data (use full fine-tuning regime based on results).
  5. Evaluate using MAE (regression) or F1/Accuracy (classification); generate saliency maps for interpretability.

- **Design tradeoffs:**
  - **MOMENT (TSFM)** vs **ECG-FM (ECGFM)**: MOMENT achieves highest overall win rate (40%); ECG-FM competitive but not superior. If compute or data is limited, MOMENT may be preferable due to broader pre-training.
  - **LLM (GPT-2, Llama 3.1)**: Not recommended for raw ECG; requires prompt engineering and underperforms even with fine-tuning.
  - **TSDL (TimesNet, DLinear)**: Faster to train from scratch but lower ceiling on complex tasks and less interpretable.

- **Failure signatures:**
  - Near-zero F1 on binary classification (e.g., Potassium abnormality zs/fs for most models): indicates domain gap and class imbalance not addressed by pre-training.
  - LLM performance collapses on Gender/Age tasks (F1 ≈ 0): feature extraction into text loses discriminative signal.
  - High variance in few-shot (e.g., ECG-FM RR few-shot MAE 698.2 ± 96.7): insufficient samples cause unstable adaptation.

- **First 3 experiments:**
  1. **Reproduce baseline**: Fine-tune MOMENT on Gender Classification (balanced, binary). Target: F1 ≥ 0.65 (Table 2: 0.68).
  2. **Ablate fine-tuning data scale**: Train on 1%, 10%, 100% of labeled data for Arrhythmia Detection; plot accuracy vs. data fraction to quantify sample efficiency.
  3. **Interpretability check**: Generate saliency maps for RR Interval Estimation using MOMENT and ECG-FM; verify attention peaks align with R-waves (compare to Figure 1).

## Open Questions the Paper Calls Out

### Open Question 1
How can foundation models be optimized to perform effective ECG analysis under zero-shot and few-shot settings without requiring extensive fine-tuning data? The authors conclude that current state-of-the-art models "still need amounts of fine-tuning samples" and explicitly call for "more efforts in the future to develop more advanced methods to pre-train or adapt the foundation model on ECG under zero and few-shots settings." The experimental results demonstrated that zero-shot and few-shot performances were consistently inferior to fine-tuned models across nearly all tasks and model architectures.

### Open Question 2
Can Retrieval-Augmented Generation (RAG) techniques enable Large Language Models (LLMs) to overcome their current limitations in processing ECG data? The Discussion states that applying LLMs to ECG "still needs specialized prompt design and external knowledge, which might require a retrieval-augmented generation (RAG) technique." The paper only evaluated LLMs using feature-calculated prompts, where they performed poorly; the utility of RAG for this specific data type remains untested.

### Open Question 3
Does incorporating general domain time-series data during pre-training provide performance benefits over ECG-specific data for foundation models? Based on the finding that general time-series models (TSFM) did not underperform specialized models (ECGFM), the authors suggest "The inclusion of general domain time-series could also boost the model performance on ECG." The study evaluated single-domain models against each other but did not experiment with mixed-domain pre-training curricula.

## Limitations

- Limited comparison to contemporary ECG foundation models beyond TSFM and ECGFM (e.g., newer ECG-specific models not benchmarked).
- No ablation of ECG-specific pre-training objectives (e.g., domain-specific masking strategies).
- Potential data leakage concerns in potassium abnormality task due to exact timestamp matching.
- Zero/few-shot performance evaluation lacks sensitivity analysis for shot count variations.

## Confidence

- **High Confidence**: TSFM outperforms LLM/TSDL on most tasks; foundation models require fine-tuning to match performance.
- **Medium Confidence**: ECGFM does not significantly outperform TSFM; this may be dataset-dependent.
- **Low Confidence**: Clinical relevance of zero/few-shot results given the reported low performance.

## Next Checks

1. Replicate Table 2 performance for MOMENT fine-tuning on Gender Classification and RR Interval Estimation to confirm top-2 ranking.
2. Vary few-shot sample sizes (e.g., 5, 10, 50) for arrhythmia detection to identify inflection point for stable performance.
3. Conduct domain shift analysis: compare pre-training corpus statistics (e.g., signal length, lead configuration) against MIMIC-IV-ECG to quantify distributional differences.