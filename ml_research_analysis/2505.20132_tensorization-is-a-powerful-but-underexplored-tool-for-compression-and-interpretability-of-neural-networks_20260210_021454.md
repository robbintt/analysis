---
ver: rpa2
title: Tensorization is a powerful but underexplored tool for compression and interpretability
  of neural networks
arxiv_id: '2505.20132'
source_url: https://arxiv.org/abs/2505.20132
tags:
- tensor
- networks
- neural
- network
- tnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that tensorizing neural networks by
  reshaping dense weight matrices into higher-order tensors and approximating them
  using low-rank tensor network decompositions is a powerful yet underexplored approach
  for both model compression and interpretability. The authors highlight that tensorized
  neural networks (TNNs) introduce structured inductive biases, enable novel scaling
  directions (e.g., bond dimension inflation), and offer new interpretability avenues
  through "bond indices" that serve as latent feature channels.
---

# Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks

## Quick Facts
- arXiv ID: 2505.20132
- Source URL: https://arxiv.org/abs/2505.20132
- Authors: Safa Hamreras; Sukhbinder Singh; Román Orús
- Reference count: 40
- Primary result: Tensorizing neural networks offers powerful compression and interpretability benefits through low-rank tensor network decompositions

## Executive Summary
This position paper argues that tensorizing neural networks by reshaping dense weight matrices into higher-order tensors and approximating them using low-rank tensor network decompositions is a powerful yet underexplored approach for both model compression and interpretability. The authors highlight that tensorized neural networks (TNNs) introduce structured inductive biases, enable novel scaling directions (e.g., bond dimension inflation), and offer new interpretability avenues through "bond indices" that serve as latent feature channels. TNNs can also be seamlessly combined with other compression methods like quantization and pruning, and they show promise for accelerating both forward and backward passes. The paper identifies key challenges—such as hardware limitations, hyperparameter complexity, and integration with other methods—and outlines future research directions toward fully tensorized networks.

## Method Summary
The method involves reshaping dense weight matrices in neural networks into higher-order tensors and approximating them using low-rank tensor network decompositions such as matrix product states (MPS), tensor train (TT), and hierarchical Tucker (HT) formats. This tensorization process introduces structured inductive biases that enable compression while maintaining representational capacity. The approach allows for parameter reduction through low-rank approximations while preserving key network functionalities. The method can be applied to various network components including fully connected layers, convolutional layers, and attention mechanisms. Additionally, tensorization can be combined with other compression techniques like quantization and pruning for enhanced efficiency.

## Key Results
- Tensorized neural networks enable structured inductive biases through higher-order tensor representations
- Bond dimension inflation provides a novel scaling direction for improving model capacity
- Tensorization offers interpretability through "bond indices" that serve as latent feature channels
- The approach can be seamlessly combined with other compression methods like quantization and pruning
- Tensorized networks show promise for accelerating both forward and backward passes

## Why This Works (Mechanism)
Tensorization works by exploiting the inherent redundancy and structure in neural network weight matrices. By reshaping these matrices into higher-order tensors and applying low-rank tensor network decompositions, the method captures the underlying correlations and dependencies in the data more efficiently than traditional dense representations. The tensor structure naturally encodes multi-way interactions between features, allowing for more compact representations that preserve essential information while eliminating redundancy. The bond indices in tensor networks serve as latent feature channels, providing an interpretable representation of the network's internal representations. This structured approach to compression maintains the model's expressive power while significantly reducing the number of parameters, leading to both computational efficiency and potential interpretability gains.

## Foundational Learning
1. **Tensor Network Decompositions** - Why needed: Core mathematical framework for tensorizing neural networks. Quick check: Can you explain the difference between MPS, TT, and HT formats?
2. **Low-rank Approximation** - Why needed: Enables compression while preserving essential information. Quick check: How does low-rank approximation reduce parameters while maintaining accuracy?
3. **Inductive Biases** - Why needed: Understanding how tensor structure introduces prior knowledge. Quick check: What structural assumptions does tensorization impose on the learned representations?
4. **Bond Dimension** - Why needed: Controls the expressiveness and compression trade-off. Quick check: How does bond dimension inflation affect model capacity and performance?
5. **Tensor Contraction** - Why needed: Fundamental operation for tensor network computations. Quick check: Can you describe the computational complexity of tensor contractions in different formats?
6. **Hardware Acceleration** - Why needed: Critical for practical deployment of tensorized networks. Quick check: What are the current limitations of GPU support for tensor network operations?

## Architecture Onboarding

Component Map: Input features -> Tensor reshaping -> Tensor network decomposition (MPS/TT/HT) -> Bond indices -> Output predictions

Critical Path: Data flow through tensorized layers where computational bottlenecks typically occur during tensor contractions and decompositions

Design Tradeoffs:
- Compression vs. Accuracy: Lower ranks provide more compression but may sacrifice performance
- Bond Dimension: Higher dimensions increase expressiveness but also computational cost
- Hardware Efficiency: Tensor operations may not be optimized on current hardware accelerators
- Hyperparameter Complexity: Multiple decomposition parameters require careful tuning

Failure Signatures:
- Underfitting when bond dimensions are too low for the task complexity
- Numerical instability in tensor contractions with ill-conditioned decompositions
- Poor performance when tensorization is applied to layers that require full-rank representations
- Excessive computational overhead on hardware without tensor network optimizations

First Experiments:
1. Apply tensorization to a single fully connected layer in a simple MLP and measure parameter reduction vs. accuracy drop
2. Compare different tensor network formats (MPS, TT, HT) on a small-scale image classification task
3. Evaluate bond dimension inflation effects on a pretrained transformer model's attention layers

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research: How can tensorized networks be systematically integrated with other compression techniques like pruning and quantization? What are the optimal strategies for choosing tensor network formats and hyperparameters for different network architectures? How can we develop efficient hardware acceleration for tensor network operations? What are the theoretical limits of compression achievable through tensorization? How can tensorization be extended to recurrent and graph neural networks? What are the best practices for training tensorized networks from scratch versus fine-tuning pretrained models?

## Limitations
- Hardware support for tensor operations remains nascent, with most implementations relying on CPU-based routines
- The hyperparameter space for tensor network decompositions is complex and difficult to navigate
- Empirical validation of interpretability claims through bond indices is limited
- Integration with other compression techniques requires more systematic validation

## Confidence
- High confidence: Tensorization's mathematical validity and theoretical compression potential
- Medium confidence: Claims about interpretability benefits and scaling directions
- Low confidence: Practical deployment feasibility and integration with existing compression methods

## Next Checks
1. Benchmark tensorized networks against established compression methods (pruning, quantization) on standard tasks with full experimental protocols
2. Develop and test GPU-optimized kernels for tensor network operations to assess practical speed gains
3. Conduct systematic ablation studies on bond dimension effects across different network architectures and datasets