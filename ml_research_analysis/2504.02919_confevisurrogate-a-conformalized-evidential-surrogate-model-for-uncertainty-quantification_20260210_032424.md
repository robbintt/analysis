---
ver: rpa2
title: 'ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty
  Quantification'
arxiv_id: '2504.02919'
source_url: https://arxiv.org/abs/2504.02919
tags:
- uncertainty
- data
- prediction
- simulation
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes ConfEviSurrogate, a novel evidential surrogate
  model that accurately predicts simulation outputs while effectively quantifying
  and distinguishing two types of uncertainty: aleatoric (data-induced) and epistemic
  (model-induced). The model leverages a Normal Inverse-Gamma (NIG) prior over Gaussian
  parameters to efficiently capture both uncertainties without requiring multiple
  inference runs.'
---

# ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty Quantification

## Quick Facts
- **arXiv ID**: 2504.02919
- **Source URL**: https://arxiv.org/abs/2504.02919
- **Authors**: Yuhan Duan; Xin Zhao; Neng Shi; Han-Wei Shen
- **Reference count**: 32
- **Primary result**: Novel evidential surrogate model combining Normal Inverse-Gamma priors with conformal prediction to quantify aleatoric and epistemic uncertainties, achieving 1.0–9.6 dB PSNR improvements over baselines on cosmology, ocean dynamics, and fluid dynamics datasets.

## Executive Summary
ConfEviSurrogate introduces a novel evidential surrogate modeling approach that accurately predicts simulation outputs while effectively quantifying both aleatoric (data-induced) and epistemic (model-induced) uncertainties. The model leverages a Normal Inverse-Gamma (NIG) prior over Gaussian parameters to efficiently capture both uncertainty types without requiring multiple inference runs. To ensure rigorous coverage guarantees for prediction intervals, the paper integrates conformal prediction for post-hoc calibration. Experiments on three scientific datasets demonstrate state-of-the-art accuracy with significant PSNR improvements and high correlation between predicted epistemic uncertainty and actual prediction errors.

## Method Summary
The method combines evidential deep learning with conformal prediction to create a surrogate model for scientific simulations. The core innovation is using a Normal Inverse-Gamma (NIG) prior to parameterize the evidential model, which allows simultaneous learning of both aleatoric and epistemic uncertainties in a single forward pass. The evidential framework treats model outputs as random variables with learned distributions rather than point estimates. Conformal prediction is then applied post-hoc to calibrate prediction intervals, ensuring they achieve the desired coverage probability. This approach avoids the computational expense of multiple inference runs while maintaining rigorous uncertainty quantification guarantees.

## Key Results
- PSNR improvements of 1.0–9.6 dB over baseline surrogate models across three scientific datasets
- Voxel-level correlation between predicted epistemic uncertainty and actual errors ranging from 0.24 to 0.73
- Member-level correlation ranging from 0.60 to 0.99, demonstrating strong uncertainty-error alignment
- Prediction intervals achieve coverage accuracy closely matching target levels (e.g., 95% target coverage)

## Why This Works (Mechanism)
The model works by leveraging the Normal Inverse-Gamma distribution's conjugacy properties to efficiently learn both uncertainty types simultaneously. The NIG prior allows the model to capture data uncertainty (aleatoric) through observation noise parameters while capturing model uncertainty (epistemic) through parameter uncertainty. The conformal prediction layer then post-processes these uncertainty estimates to ensure calibration without requiring distributional assumptions to be perfectly met. This combination provides both computational efficiency and statistical rigor.

## Foundational Learning
- **Normal Inverse-Gamma Distribution**: A conjugate prior for Gaussian likelihoods that enables closed-form Bayesian inference; needed because it naturally separates aleatoric and epistemic uncertainty parameters; quick check: verify the four-parameter NIG parameterization (μ, λ, α, β) is correctly implemented
- **Aleatoric vs Epistemic Uncertainty**: Aleatoric represents irreducible data noise while epistemic represents model ignorance; needed to distinguish between different uncertainty sources for proper risk assessment; quick check: confirm the model can recover known uncertainty types in controlled synthetic experiments
- **Conformal Prediction**: A distribution-free method for constructing prediction intervals with guaranteed coverage; needed to ensure prediction intervals are well-calibrated without parametric assumptions; quick check: verify marginal coverage holds on held-out calibration data
- **Evidential Deep Learning**: Treats model outputs as parameters of a distribution over functions rather than point estimates; needed to capture uncertainty in surrogate predictions; quick check: confirm evidential outputs form valid probability distributions
- **Surrogate Modeling**: Creating computationally efficient approximations of expensive simulations; needed to enable rapid scientific exploration; quick check: verify prediction accuracy on held-out simulation runs
- **Covariance Estimation**: The model must learn the full covariance structure of simulation outputs; needed for accurate uncertainty quantification in multivariate settings; quick check: confirm learned covariances match empirical covariances on validation data

## Architecture Onboarding
**Component Map**: Input Data -> Evidential Network (NIG parameterization) -> Aleatoric & Epistemic Uncertainty Outputs -> Conformal Calibration -> Prediction Intervals

**Critical Path**: The evidential network with NIG parameterization is the critical component, as it must accurately learn both uncertainty types. The conformal layer is applied post-hoc and depends entirely on the quality of the evidential uncertainty estimates.

**Design Tradeoffs**: Single-pass uncertainty estimation vs. multiple inference runs (computational efficiency vs. potential accuracy); parametric NIG assumptions vs. flexibility of non-parametric methods; post-hoc conformal calibration vs. end-to-end uncertainty learning.

**Failure Signatures**: Poor uncertainty calibration when NIG assumptions violated; overconfident predictions when epistemic uncertainty underestimated; coverage degradation under covariate shift; computational bottlenecks in high-dimensional parameter spaces.

**First Experiments**:
1. Verify NIG parameter learning on synthetic data with known aleatoric and epistemic uncertainty components
2. Test conformal coverage guarantees on held-out calibration data from each scientific domain
3. Conduct ablation study removing conformal layer to quantify its contribution to coverage accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability of NIG prior parameterization to higher-dimensional scientific problems remains unverified
- PSNR improvements demonstrated only on three specific datasets, limiting generalizability claims
- Correlation metrics show variable performance (0.24–0.73 voxel-level), with some datasets exhibiting weaker uncertainty-error alignment
- Conformal prediction guarantees depend on underlying distributional assumptions holding in real-world scientific data

## Confidence
- **High**: Methodological framework combining evidential modeling with conformal calibration
- **Medium**: Performance claims across diverse scientific domains
- **Low**: Long-term stability under distribution shift and extrapolation beyond training conditions

## Next Checks
1. Test ConfEviSurrogate on higher-dimensional, less structured scientific simulations (e.g., molecular dynamics or climate models with >100 dimensions) to assess scalability limits
2. Conduct systematic ablation studies removing the conformal calibration layer to quantify its specific contribution to coverage accuracy across all datasets
3. Evaluate prediction performance under covariate shift by testing on simulation outputs from slightly modified physical parameters not seen during training