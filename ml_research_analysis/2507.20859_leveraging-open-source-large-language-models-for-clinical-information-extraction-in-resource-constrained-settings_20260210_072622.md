---
ver: rpa2
title: Leveraging Open-Source Large Language Models for Clinical Information Extraction
  in Resource-Constrained Settings
arxiv_id: '2507.20859'
source_url: https://arxiv.org/abs/2507.20859
tags:
- tasks
- performance
- task
- dragon
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates nine open-source large language models on
  28 clinical information extraction tasks in Dutch, using a newly developed framework
  called llmextractinator. The models were tested in a zero-shot setting on the DRAGON
  benchmark, which includes tasks such as binary classification, multi-class classification,
  regression, and named entity recognition.
---

# Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings

## Quick Facts
- arXiv ID: 2507.20859
- Source URL: https://arxiv.org/abs/2507.20859
- Reference count: 40
- Primary result: 14B+ parameter open-source LLMs achieve ~0.75 utility on Dutch clinical extraction tasks when using native language input and structured output validation

## Executive Summary
This study evaluates nine open-source large language models on 28 Dutch clinical information extraction tasks using a new framework called llm_extractinator. The models were tested in zero-shot settings on the DRAGON benchmark, which includes binary classification, multi-class classification, regression, and named entity recognition tasks. Results show that models with at least 14B parameters (particularly Llama-3.3-70B, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B) achieve the highest performance with utility scores near 0.75, while smaller models fail consistently. The study also found that translating Dutch medical reports to English before inference degrades performance across all models, highlighting the importance of native-language processing.

## Method Summary
The study employs the llm_extractinator framework with Ollama backend and LangChain for prompts to evaluate nine open-source LLMs on Dutch clinical information extraction. Models are run in zero-shot settings with 4-bit quantization (q4_K_M for most models, q4_0 for Mistral/Gemma2) and chain-of-thought prompting at temperature=0. The framework uses a JSON Taskfile schema to define tasks and validates outputs with up to three retry attempts for invalid JSON. Evaluation uses the DRAGON benchmark containing 28,824 annotated Dutch medical reports across five institutions, with per-task metrics aggregated into S_DRAGON (arithmetic mean across all tasks).

## Key Results
- 14B+ parameter models (Llama-3.3-70B, Phi-4-14B, Qwen-2.5-14B, DeepSeek-R1-14B) achieved utility scores near 0.75
- Smaller models (Llama-3.2-3B, Gemma-2-2B) failed consistently, with Gemma-2-2B unable to produce valid JSON
- Translation to English degraded performance across all models, while native Dutch inference preserved clinical nuance
- Models performed well on classification/regression tasks but poorly on NER (F1 < 0.5)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A minimum parameter scale (approx. 14B) is conditionally required for zero-shot clinical extraction in non-English languages, while smaller models (â‰¤3B) exhibit high failure rates.
- **Mechanism:** Models with ~14B parameters possess sufficient capacity to encode multilingual medical semantics and instruction-following capabilities required for zero-shot generalization. Smaller models lack the representational density to handle domain-specific jargon and complex output formatting simultaneously.
- **Core assumption:** The performance drop in smaller models is due to capacity limitations rather than suboptimal quantization (all models were run in 4-bit).
- **Evidence anchors:** [abstract] "Smaller models like Llama-3.2-3B failed consistently." [section: Results/Model-Level Performance] Notes Llama-3.2-3B achieved "Minimal to Fail" performance, while Gemma-2-2B failed to produce valid JSON.

### Mechanism 2
- **Claim:** Native-language inference preserves clinical nuance better than pipelined translation, even for models predominantly trained on English data.
- **Mechanism:** Translation introduces intermediate errors and dilutes domain-specific terminology. Direct inference on the native language allows the model to leverage its full pre-training distribution (including any non-English tokens) without the signal degradation inherent in the translation step.
- **Core assumption:** The tested open-source models have sufficient multilingual exposure in their pre-training data to outperform the translation-artifact noise.
- **Evidence anchors:** [abstract] "Translation to English degraded performance across all models..." [section: Discussion] "In-context translation consistently harms downstream performance."

### Mechanism 3
- **Claim:** Enforcing structured output constraints (JSON) via an agentic framework (llm_extractinator) bridges the gap between generative text and machine-readable data.
- **Mechanism:** By defining a strict schema in a "Taskfile" and employing a retry mechanism for validation errors, the system forces the generative LLM to map reasoning into specific keys. This converts free-form generation into reliable API-like outputs.
- **Core assumption:** The model's instruction-following capability is strong enough to adhere to the schema within a few retries.
- **Evidence anchors:** [section: Methods/llm_extractinator] Describes the validation loop: "Outputs that do not conform are automatically resubmitted... with up to three attempts."

## Foundational Learning

- **Concept:** Zero-shot Chain-of-Thought (CoT) Prompting
  - **Why needed here:** The study relies on the model's ability to reason through medical reports without task-specific training data. CoT prompts the model to generate intermediate reasoning steps, which the paper cites as a core strategy for improving extraction quality.
  - **Quick check question:** Can you distinguish between providing examples (few-shot) and asking the model to "think step-by-step" (zero-shot CoT)?

- **Concept:** Quantization (4-bit)
  - **Why needed here:** To run 14B and 70B parameter models on consumer-grade hardware (12GB VRAM), the study uses 4-bit quantization. Understanding this is critical to replicating the "resource-constrained" deployment.
  - **Quick check question:** Does reducing model precision from 16-bit to 4-bit primarily affect memory usage, inference speed, or both?

- **Concept:** Named Entity Recognition (NER) vs. Classification
  - **Why needed here:** The paper highlights a distinct performance gap: models excelled at classification/regression but failed at NER. Understanding the difference between classifying a whole document and extracting specific token spans explains why the utility scores varied by task type.
  - **Quick check question:** Why might a generative model struggle to output exact text spans compared to predicting a binary label?

## Architecture Onboarding

- **Component map:** Input (Dutch clinical reports) -> Controller (llm_extractinator) -> Config (Taskfile) -> Inference Engine (Ollama) -> Validator (LangChain parser)
- **Critical path:** 1. Define the Taskfile (Schema + Description) 2. Load quantized model via Ollama 3. Execute zero-shot inference with CoT prompting 4. Validate JSON output; retry up to 3 times if invalid
- **Design tradeoffs:**
  - 14B vs. 70B: The 70B model offers marginal accuracy gains (+1.2% utility) but significantly increases hardware demands. The paper suggests 14B is the "sweet spot" for resource-constrained settings.
  - Native vs. Translated: Always default to native language input; do not translate to English pre-inference.
- **Failure signatures:**
  - Schema Hallucination: Model returns valid JSON but with keys not defined in the schema (handled by validator retry)
  - NER Collapse: Model fails to extract entities or hallucinates non-existent text spans (observed low F1 scores in paper)
  - Small Model Incoherence: Models <4B parameters may fail to generate valid JSON syntax at all (observed with Gemma-2-2B)
- **First 3 experiments:**
  1. Reproducibility Test: Run llm_extractinator with Qwen-2.5-14B (q4_K_M) on a sample Dutch report to verify JSON compliance and VRAM usage
  2. Ablation on Scale: Compare Llama-3.2-3B vs. Llama-3.1-8B on a single classification task to quantify the "failure cliff" mentioned in the results
  3. Translation Stress Test: Run one regression task (e.g., PSA extraction) in native Dutch vs. machine-translated English to empirically verify the reported performance degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** How do few-shot prompting, lightweight instruction-tuning, or retrieval-augmented generation impact the performance of open-source LLMs on Dutch clinical information extraction tasks?
  - **Basis in paper:** [explicit] The authors explicitly state there is "room for future research to explore the effects of few-shot prompting, lightweight instruction-tuning, or retrieval-augmented generation."
  - **Why unresolved:** The current study restricted evaluation to zero-shot settings to ensure reproducibility without task-specific optimization.
  - **What evidence would resolve it:** Performance benchmarks on the DRAGON dataset utilizing these advanced techniques.

- **Open Question 2:** Do the largest open-source LLMs (larger than 70B parameters) offer significant performance gains over 14B models, justifying their higher computational cost?
  - **Basis in paper:** [explicit] The authors note that due to resource constraints, they "only evaluated one model over 15B parameters" and suggest future work "should explore [larger models'] capabilities."
  - **Why unresolved:** The study established a lower bound for model scale but did not determine the performance ceiling of the largest available open-source models.
  - **What evidence would resolve it:** A comparative evaluation of large-scale open-source models on the same clinical tasks, analyzing marginal performance gain versus computational cost.

- **Open Question 3:** Can alternative output formats improve the poor performance of generative LLMs on token-level Named Entity Recognition (NER) tasks?
  - **Basis in paper:** [inferred] The authors note that generative models are "not naturally suited" for sparse token-level lists and suggest results are "only a conservative estimate."
  - **Why unresolved:** The low NER scores (F1 < 0.5) may result from the evaluation methodology rather than model incompetence.
  - **What evidence would resolve it:** Evaluation using alternative NER formats (e.g., JSON-based entity extraction) rather than strict token-level lists.

## Limitations
- The study's conclusions are based on zero-shot inference only, leaving the potential benefits of fine-tuning unexplored.
- The DRAGON benchmark's test labels are not publicly available, requiring access through Grand Challenge, which limits independent verification.
- The framework's reliance on JSON schema validation through retries assumes models can self-correct, but this mechanism wasn't stress-tested with pathological inputs.

## Confidence

- **High confidence:** The minimum parameter threshold (~14B) for reliable zero-shot extraction and the consistent performance degradation from translation are well-supported by empirical results across multiple models and tasks.
- **Medium confidence:** The framework's effectiveness in converting generative outputs to structured data is demonstrated, but the scalability of the retry mechanism for edge cases remains uncertain.
- **Low confidence:** The claim that open-source LLMs offer "scalable and privacy-conscious solutions" is aspirational; the study doesn't address deployment costs, model maintenance, or long-term data privacy guarantees.

## Next Checks

1. **Fine-tuning Impact:** Compare zero-shot performance against few-shot or fine-tuned versions of the same models on a subset of DRAGON tasks to quantify the trade-off between data requirements and accuracy gains.
2. **Resource Profiling:** Measure actual VRAM usage, inference latency, and energy consumption for 14B vs. 70B models during batch processing of DRAGON reports to validate the "resource-constrained" deployment claim.
3. **Translation Ablation:** Systematically test translation quality (e.g., Google Translate vs. DeepL) and its interaction with model performance to determine if high-quality translation could mitigate the reported degradation.