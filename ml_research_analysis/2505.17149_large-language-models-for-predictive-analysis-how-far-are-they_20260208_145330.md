---
ver: rpa2
title: 'Large Language Models for Predictive Analysis: How Far Are They?'
arxiv_id: '2505.17149'
source_url: https://arxiv.org/abs/2505.17149
tags:
- data
- code
- llms
- predictive
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PredictiQ, a comprehensive benchmark for
  evaluating Large Language Models (LLMs) on predictive analysis tasks. It includes
  1,130 queries across 44 real-world datasets spanning 8 domains, with a three-part
  evaluation protocol assessing textual analysis, code generation, and text-code alignment.
---

# Large Language Models for Predictive Analysis: How Far Are They?

## Quick Facts
- arXiv ID: 2505.17149
- Source URL: https://arxiv.org/abs/2505.17149
- Authors: Qin Chen; Yuanyi Ren; Xiaojun Ma; Yuyang Shi
- Reference count: 40
- Primary result: GPT-4o and GPT-4o-3-mini show the most balanced results on PredictiQ benchmark, but all models still struggle with predictive analysis tasks

## Executive Summary
This paper introduces PredictiQ, a comprehensive benchmark for evaluating Large Language Models (LLMs) on predictive analysis tasks. It includes 1,130 queries across 44 real-world datasets spanning 8 domains, with a three-part evaluation protocol assessing textual analysis, code generation, and text-code alignment. Testing 12 leading LLMs, the study finds that existing models still struggle with predictive analysis—often missing data preprocessing steps and producing low-quality code—though fine-tuning on code generation can improve performance. GPT-4o and GPT-4o-3-mini show the most balanced results, while Llama models lag behind. The work highlights that while LLMs have made progress, they are not yet fully capable of robust, efficient predictive analysis.

## Method Summary
The PredictiQ benchmark evaluates LLMs on 1,130 predictive analysis queries across 44 datasets from 8 domains (Economics, Marketing/Sales, Industry Analysis, Traffic, Healthcare, Social Study, Human Resource, Education). Each query requires both textual analysis and executable code generation. The evaluation protocol scores 7 aspects (0-4 scale) across three domains: Text Analysis (Relevance, Depth), Code Generation (Usefulness, Functional Correctness), and Text-Code Alignment (Descriptive Accuracy, Coverage, Clarity). GPT4Turbo serves as the primary evaluator with 90.5% human alignment. Models are tested with temperature=0.7, top_p=0.95, and max tokens=4096 (or 32,768 for reasoning models).

## Key Results
- GPT-4o and GPT-4o-3-mini demonstrate the most balanced performance across all evaluation dimensions
- Existing models frequently omit data preprocessing steps, with omission rates ranging from 51% to 92% across different models
- Code fine-tuning improves overall predictive analysis performance beyond parameter scaling alone
- Llama family models produce no code in 53-64% of cases, significantly underperforming other models

## Why This Works (Mechanism)

### Mechanism 1: Code Fine-Tuning Transfer
Fine-tuning LLMs on code generation appears to improve overall predictive analysis performance, including text analysis and text-code alignment, beyond what parameter scaling alone would predict. The mechanism suggests code training strengthens the model's ability to structure logical reasoning and produce coherent text-to-code mappings, which transfers to data analysis workflows. Evidence shows CodeLlama2-7B outperforming ChatLlama2-13B despite having 46% fewer parameters. However, if code fine-tuning data is too specialized (e.g., C++ kernel code rather than data analysis), it may harm executable code quality.

### Mechanism 2: Preprocessing Oversight Gap
LLMs frequently omit data preprocessing steps (handling missing values, filtering), even when generating otherwise valid analysis code. This reflects a lack of explicit scaffolding for the multi-step pipeline nature of data analysis—models jump from problem framing directly to modeling without intermediate data hygiene. Evidence shows GPT4O3Mini omits preprocessing in 51% of cases and ChatLlama2-70B in 87%. Explicit prompting or multi-step decomposition may reduce this gap, though prompt engineering has minimal effect on code generation.

### Mechanism 3: Text-Code Interdependence
Textual analysis quality and code generation quality appear correlated, suggesting shared underlying capabilities rather than independent skills. Both require understanding the predictive task, selecting appropriate methods, and reasoning about data—suggesting a common analytical reasoning substrate. Code fine-tuning improved text analysis and text-code alignment, not just code generation. However, models optimized purely for code generation without textual explanation may break this interdependence.

## Foundational Learning

- **Concept:** Predictive analysis pipeline (preprocessing → feature engineering → algorithm selection → evaluation → interpretation)
  - **Why needed here:** The benchmark assumes this workflow; LLM failures cluster at preprocessing and algorithm justification (Depth scores: 1.31–2.91/4)
  - **Quick check question:** Can you explain why handling missing values before model fitting affects prediction validity?

- **Concept:** Text-code alignment evaluation
  - **Why needed here:** Three of seven evaluation metrics (Descriptive Accuracy, Coverage, Clarity) measure whether text explanations match generated code
  - **Quick check question:** If code implements a logistic regression but text describes a decision tree, which alignment metric fails?

- **Concept:** LLM-as-evaluator calibration
  - **Why needed here:** GPT4Turbo was selected as evaluator based on 90.5% alignment with human experts; evaluator choice affects score distributions
  - **Quick check question:** Why might Phi3Medium's tendency to assign uniformly high scores make it unsuitable as an evaluator?

## Architecture Onboarding

- **Component map:** Input (query, dataset CSV, data summary) → Formatted prompt → LLM (generates text/code) → GPT4Turbo evaluation (7 aspects) → Score aggregation

- **Critical path:**
  1. Prompt construction (query + data summary + CSV) → Section 3.3
  2. LLM inference (temperature=0.7, max_tokens=4096, or 32,768 for reasoning models)
  3. Code execution (manual) for functional correctness
  4. GPT4Turbo evaluation for remaining 6 aspects
  5. Score aggregation across text/code/alignment domains

- **Design tradeoffs:**
  - Context length vs. reasoning quality: GPT4O1/GPT4O3Mini need 32K tokens for reasoning; shorter contexts degrade performance
  - Evaluator selection: GPT4Turbo has highest human alignment (90.5%) but GPT4O is more lenient; Phi3Medium lacks differentiation
  - Prompt engineering: Role-play and data summary help some models (Phi4, CohereRPlus) but minimally affect reasoning models (GPT4O3Mini)

- **Failure signatures:**
  - No code generated: Llama family produces no code in 53-64% of cases
  - Import errors: Correlate with model scale—41.8% for ChatLlama2-7B vs. 0.3% for GPT4O1
  - Logic errors: Persist across all models (12.7-64%); larger models sometimes show higher logic error rates
  - Preprocessing omission: Check for missing value handling, type conversion, filtering before modeling
  - Round number bias: Evaluators like Phi3Medium assign near-identical high scores regardless of quality

- **First 3 experiments:**
  1. **Baseline reproduction:** Run GPT4O on 20 queries from Economics domain; measure functional correctness and preprocessing omission rate. Compare against reported 66%/81% figures.
  2. **Ablation check:** Test GPT4O with/without data summary on 20 queries. Expect <0.5 point difference in total score based on Table 6.
  3. **Evaluator calibration:** Have GPT4Turbo and GPT4O evaluate the same 10 responses; compare score gaps to quantify positive bias (expected: GPT4O scores ~2-4 points higher per Table 5).

## Open Questions the Paper Calls Out

- **Open Question 1:** How do LLMs perform on prescriptive and diagnostic analysis tasks compared to the predictive analysis tasks evaluated in this study?
  - **Basis:** The authors state in the Limitations section that this work centers on predictive analysis and "does not extend to other advanced data analysis fields such as prescriptive analysis or diagnostic analysis."
  - **Why unresolved:** The PredictiQ benchmark is explicitly restricted to predictive queries, lacking evaluation protocols for suggesting actions (prescriptive) or identifying root causes (diagnostic).
  - **What evidence would resolve it:** An expansion of the PredictiQ benchmark to include diagnostic and prescriptive queries, evaluated using the existing text-code alignment protocol.

- **Open Question 2:** Can LLMs maintain predictive analysis performance when applied to non-tabular data structures like graphs or images?
  - **Basis:** Section F (Future Works) suggests diversifying the structural forms of data, specifically proposing the incorporation of "image data or graph-based data" alongside the tabular data currently used.
  - **Why unresolved:** The current study is confined to CSV-formatted tabular datasets, leaving the models' ability to handle complex structural data in predictive contexts unknown.
  - **What evidence would resolve it:** Evaluation results from a benchmark comprising graph and image datasets, assessing the models' ability to generate code and textual analysis for non-tabular predictions.

- **Open Question 3:** Why does code-focused fine-tuning in smaller models (e.g., 7B parameters) improve textual alignment but degrade the functional correctness of executable data analysis code?
  - **Basis:** Finding 3 reports that CodeLlama2-7B outperformed ChatLlama2-13B in text/code alignment but suffered a lower executable code rate (15% vs 18%), which the authors suggest may be due to fine-tuning on code irrelevant to data analysis (e.g., C++ kernel code).
  - **Why unresolved:** The paper identifies the trade-off but does not isolate whether the failure is caused by model capacity limits or the specific nature of the fine-tuning data.
  - **What evidence would resolve it:** A controlled study comparing small models fine-tuned on general code versus a dataset strictly curating data analysis libraries (pandas, scikit-learn) to observe changes in logic error rates.

## Limitations

- The benchmark focuses exclusively on tabular data, excluding structured prediction tasks (tabular ML, time series) that may require different LLM capabilities
- Evaluation relies on GPT4Turbo as an automated scorer, which may introduce systematic bias in scoring nuanced aspects like Depth or Text-Code Alignment
- Functional correctness metric depends on manual execution, which doesn't scale and introduces potential variability across evaluators

## Confidence

- **High Confidence:** GPT-4o and GPT-4o-3-mini demonstrate superior performance across all evaluated dimensions compared to Llama family models
- **Medium Confidence:** Code fine-tuning transfers to improved text analysis and text-code alignment, not just code generation quality
- **Low Confidence:** Preprocessing omission rates (51-92%) reflect fundamental LLM limitations rather than prompt engineering artifacts

## Next Checks

1. **Cross-Domain Generalization:** Evaluate GPT-4o on structured prediction tasks (e.g., tabular classification) to test whether exploratory analysis strengths transfer to traditional ML workflows

2. **Evaluator Bias Calibration:** Compare GPT4Turbo scores against a subset of human expert evaluations on ambiguous cases (e.g., Depth 2 vs 3) to quantify systematic scoring drift

3. **Preprocessing Intervention Study:** Test a controlled prompt engineering intervention ("First handle missing values, then...") across 50 queries to measure marginal improvements in preprocessing inclusion rates