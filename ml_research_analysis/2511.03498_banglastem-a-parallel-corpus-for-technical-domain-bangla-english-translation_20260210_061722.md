---
ver: rpa2
title: 'BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation'
arxiv_id: '2511.03498'
source_url: https://arxiv.org/abs/2511.03498
tags:
- translation
- bangla
- technical
- banglastem
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating technical STEM
  content from Bangla to English, which is critical for enabling Bangla speakers to
  use English-focused language models effectively. The authors present BanglaSTEM,
  a high-quality parallel corpus of 5,000 Bangla-English sentence pairs across computer
  science, mathematics, physics, chemistry, and biology domains.
---

# BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation

## Quick Facts
- arXiv ID: 2511.03498
- Source URL: https://arxiv.org/abs/2511.03498
- Reference count: 15
- Primary result: 5,000 curated Bangla-English parallel sentences enabling 82.5% code generation accuracy and 79% math problem-solving accuracy

## Executive Summary
This paper addresses the critical challenge of translating technical STEM content from Bangla to English, enabling Bangla speakers to leverage English-focused language models effectively. The authors present BanglaSTEM, a high-quality parallel corpus of 5,000 Bangla-English sentence pairs across five STEM domains. Through careful curation using multiple LLMs and human evaluation, the dataset achieves accurate preservation of technical terminology. A T5-based translation model fine-tuned on this corpus demonstrates significant improvements on downstream tasks, achieving 82.5% accuracy on code generation and 79.0% on mathematical problem-solving.

## Method Summary
The authors created BanglaSTEM by first generating over 12,000 Bangla-English translations using three proprietary LLMs (GPT-4o, Claude Sonnet 4, Gemini 2.5 Pro) with few-shot prompts (8-12 examples per domain). Three human evaluators then curated the highest quality pairs, resulting in 5,000 final sentence pairs covering computer science (52%), mathematics (25%), physics (11.4%), chemistry (5.6%), and biology (5.6%). A BanglaT5 model (247M parameters) was fine-tuned using the dataset with learning rate 5e-4, warmup 25 steps, batch size 16 with gradient accumulation of 4 (effective 64), and 8 epochs. The model was evaluated on two downstream tasks: code generation (mHumanEval-style) and mathematical problem-solving (DL Sprint 3.0 Math Olympiad), both using English LLMs for final evaluation.

## Key Results
- BanglaSTEM dataset: 5,000 curated parallel sentence pairs across 5 STEM domains
- Translation model achieves 82.5% accuracy on code generation (400 problems)
- Translation model achieves 79.0% accuracy on math problem-solving (100 problems)
- 63% of technical terminology expressed through transliteration in Bangla
- Significant improvement over baseline BanglaT5-Base (59.75% code generation accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Domain-specific fine-tuning
- Claim: Domain-specific translation fine-tuning improves downstream LLM task performance more than general-purpose translation models
- Mechanism: General NMT models lack exposure to technical terminology distributions; fine-tuning on curated STEM parallel data aligns vocabulary mapping with domain-specific semantics
- Core assumption: Technical terminology precision is the primary bottleneck
- Evidence anchors: 82.5% vs 59.75% code generation accuracy improvement; technical terms often mistranslated by existing systems

### Mechanism 2: Multi-model generation with human selection
- Claim: Multi-model generation with human selection yields higher-quality parallel data than single-model approaches
- Mechanism: Different LLMs produce translation candidates with varying strengths; human evaluators select optimal combinations
- Core assumption: No single model consistently produces best translations across all technical domains
- Evidence anchors: 12,000+ initial translations reduced to 5,000 curated pairs through human evaluation

## Foundational Learning

### Parallel corpus construction
- Why needed: High-quality parallel data is essential for training translation models; technical domains require specialized vocabulary
- Quick check: Dataset contains balanced representation across STEM domains with accurate technical terminology

### Fine-tuning methodology
- Why needed: Base translation models lack domain-specific knowledge; fine-tuning adapts them to technical terminology
- Quick check: Model achieves improved performance on domain-specific downstream tasks

### Transliteration handling
- Why needed: Technical terms in Bangla often use English transliterations, creating ambiguity
- Quick check: 63% of technical terminology uses transliteration; model must handle multiple possible spellings

## Architecture Onboarding

### Component map
BanglaSTEM corpus -> BanglaT5 model -> Fine-tuning (90% train, 10% val) -> Evaluation on code generation and math tasks

### Critical path
1. Generate candidate translations using multiple LLMs
2. Human curation and selection of highest quality pairs
3. Fine-tune BanglaT5 on curated corpus
4. Evaluate on downstream tasks using English LLMs

### Design tradeoffs
- Dataset size vs. quality: Prioritized curation over quantity (5K curated vs. 12K+ generated)
- General vs. domain-specific: Focused on technical accuracy over general fluency
- Proprietary vs. open models: Used best available LLMs despite reproducibility concerns

### Failure signatures
- Poor technical terminology translation: Model fails on domain-specific vocabulary
- Overfitting: Validation loss diverges before epoch 8
- Transliteration issues: Inconsistent handling of English terms in Bangla

### 3 first experiments
1. Translate 100 held-out technical Bangla sentences and manually evaluate terminology accuracy
2. Fine-tune with reduced learning rate if validation loss increases during training
3. Test model on code generation with beam search parameters varied (beam=1,2,4,8)

## Open Questions the Paper Calls Out

### Dataset scaling
- Question: How does performance scale with dataset size beyond 5,000 pairs?
- Basis: Authors suggest 20,000-50,000 pairs would improve robustness
- Resolution: Training curves comparing performance at incremental dataset sizes

### Model generalization
- Question: Does BanglaSTEM fine-tuning generalize to other open-source LLMs?
- Basis: Evaluation limited to Gemma-27B and Llama-3.1-8B
- Resolution: Systematic evaluation with additional models on same tasks

### Open-source reproducibility
- Question: Can high-quality technical corpora be generated using only open-source models?
- Basis: Pipeline relied on proprietary LLMs (GPT-4o, Claude, Gemini)
- Resolution: Ablation study replacing proprietary generators with open alternatives

### Domain imbalance effects
- Question: Does domain imbalance (52% programming vs. 5.6% biology) cause performance disparities?
- Basis: Coverage uneven across domains
- Resolution: Domain-stratified evaluation measuring translation quality per field

## Limitations

- Small dataset size (5,000 pairs) may not capture full technical terminology diversity
- Human evaluation process lacks detailed methodology and inter-annotator agreement metrics
- Indirect evaluation through downstream tasks makes it difficult to isolate translation quality improvements

## Confidence

- High Confidence: Existing Bangla-English translation systems struggle with technical terminology
- Medium Confidence: Domain-specific fine-tuning is necessary for technical translation
- Low Confidence: BanglaSTEM is the first resource specifically designed for technical domain Bangla-English translation

## Next Checks

1. **Dataset Quality Audit**: Independent human evaluation of 200 randomly sampled translation pairs to assess inter-rater reliability and identify systematic biases

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned model on technical Bangla content from domains not represented in training data to assess generalization

3. **Direct Translation Quality Assessment**: Human evaluation of translation quality on held-out test set using standardized metrics rather than relying solely on downstream task performance