---
ver: rpa2
title: 'GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised
  Learning'
arxiv_id: '2509.06975'
source_url: https://arxiv.org/abs/2509.06975
tags:
- graph
- pretraining
- learning
- methods
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GSTBench, the first systematic benchmark for
  evaluating the transferability of graph self-supervised learning (GSSL) methods
  across datasets. The authors conduct large-scale pretraining on ogbn-papers100M
  and evaluate five representative SSL methods across diverse target graphs.
---

# GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning

## Quick Facts
- arXiv ID: 2509.06975
- Source URL: https://arxiv.org/abs/2509.06975
- Reference count: 40
- Primary result: GSTBench is the first systematic benchmark for evaluating GSSL transferability across datasets, revealing that most methods struggle with cross-domain transfer.

## Executive Summary
This paper introduces GSTBench, the first systematic benchmark for evaluating the transferability of graph self-supervised learning (GSSL) methods across datasets. The authors conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across diverse target graphs. Their standardized experimental setup decouples confounding factors to enable rigorous comparisons focused solely on pretraining objectives. Surprisingly, most GSSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance.

## Method Summary
The study presents a standardized experimental framework for evaluating GSSL transferability by decoupling pretraining objectives from downstream adaptation strategies. The authors pretrain five representative SSL methods on the large-scale ogbn-papers100M dataset and systematically evaluate their performance across diverse target graphs. The benchmark employs three adaptation strategies (linear probing, in-context learning, and fine-tuning) to assess how different pretraining objectives influence transfer performance. The experimental design specifically isolates the effect of pretraining objectives by controlling for other variables, enabling rigorous comparison of GSSL methods' cross-domain generalization capabilities.

## Key Results
- Most GSSL methods struggle to generalize across domains, with some performing worse than random initialization
- GraphMAE consistently improves transfer performance across different target graphs
- Generative SSL methods (GraphMAE, VGAE) generally outperform contrastive methods for cross-domain transfer

## Why This Works (Mechanism)
The study demonstrates that the effectiveness of graph self-supervised learning methods for transfer learning depends critically on the pretraining objective's ability to capture domain-invariant structural patterns. Generative approaches like GraphMAE succeed because they learn to reconstruct local graph structures in a way that generalizes across different domains, while contrastive methods often overfit to domain-specific features. The benchmark's controlled experimental design reveals that pretraining objectives must encode transferable structural representations rather than domain-specific patterns to achieve successful cross-domain transfer.

## Foundational Learning
- Graph Self-Supervised Learning: Why needed - Enables learning from unlabeled graph data; Quick check - Understand basic SSL objectives like contrastive and generative approaches
- Transfer Learning: Why needed - Evaluates whether pretraining benefits generalize to new domains; Quick check - Distinguish between transductive and inductive settings
- Masked Autoencoders: Why needed - GraphMAE's success relies on masked node reconstruction; Quick check - Understand how masking works in graph contexts
- Adaptation Strategies: Why needed - Different ways to leverage pretrained models for downstream tasks; Quick check - Compare linear probing vs fine-tuning effectiveness

## Architecture Onboarding
Component map: ogbn-papers100M pretraining -> Five SSL methods (GraphMAE, VGAE, MVGRL, DGI, GCA) -> Target graphs evaluation -> Adaptation strategies

Critical path: Large-scale pretraining on source graph → SSL method pretraining → Target graph evaluation → Adaptation strategy application → Transfer performance measurement

Design tradeoffs: Generative SSL methods vs contrastive approaches for transfer learning; Linear probing simplicity vs fine-tuning flexibility; Single large pretraining source vs multiple diverse sources

Failure signatures: Poor transfer performance indicates overfitting to source domain; Worse-than-random results suggest negative transfer; Method-specific failure modes (e.g., contrastive methods struggling with cross-domain transfer)

First experiments: 1) Compare GraphMAE vs random initialization on diverse target graphs; 2) Evaluate all five SSL methods on same target graph; 3) Test different adaptation strategies on best-performing pretraining method

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Reliance on single large pretraining dataset (ogbn-papers100M) may not represent all source graph types
- Limited evaluation to only five SSL methods despite many variants existing
- Focus on transductive settings may limit generalizability to inductive scenarios

## Confidence
High confidence claims:
- Most GSSL methods struggle with cross-domain transfer
- GraphMAE consistently superior across different target graphs
- Generative SSL methods outperform contrastive ones for transfer

Medium confidence claims:
- Specific ranking of adaptation strategies effectiveness
- Generalizability of findings across different graph domains

Low confidence claims:
- Absolute performance comparisons between methods given limited SSL approaches evaluated
- Extent of result generalizability to completely different graph types

## Next Checks
1. Evaluate the benchmark using alternative large-scale pretraining datasets beyond ogbn-papers100M to assess robustness to source domain variations
2. Expand the evaluation to include additional SSL methods, particularly recent contrastive approaches and hybrid techniques, to validate the observed performance hierarchy
3. Test the benchmark in inductive settings where node features and graph structures differ between pretraining and target tasks to evaluate real-world applicability