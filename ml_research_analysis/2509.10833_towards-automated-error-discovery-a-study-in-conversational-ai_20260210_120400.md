---
ver: rpa2
title: 'Towards Automated Error Discovery: A Study in Conversational AI'
arxiv_id: '2509.10833'
source_url: https://arxiv.org/abs/2509.10833
tags:
- error
- dialogue
- types
- detection
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEEED, an encoder-based method for detecting
  and defining errors in conversational AI. It combines a dialogue summary generation
  step with a soft clustering approach using an enhanced Soft Nearest Neighbor Loss
  and a novel Label-Based Sample Ranking strategy.
---

# Towards Automated Error Discovery: A Study in Conversational AI

## Quick Facts
- arXiv ID: 2509.10833
- Source URL: https://arxiv.org/abs/2509.10833
- Reference count: 40
- Primary result: SEEED achieves up to 8-point improvement in detecting unknown error types compared to LLM-based baselines

## Executive Summary
This paper introduces SEEED, an encoder-based method for detecting and defining errors in conversational AI. It combines dialogue summary generation with soft clustering using enhanced Soft Nearest Neighbor Loss and Label-Based Sample Ranking. Experiments show SEEED outperforms LLM-based baselines (GPT-4o and Phi-4) by up to 8 points in detecting unknown error types and generalizes well to intent detection tasks.

## Method Summary
SEEED processes dialogue contexts through dual BERT encoders—one for the full context and one for LLM-generated summaries (Llama-3.1 8B). The encoded representations are combined via a linear layer and trained with joint loss using enhanced Soft Nearest Neighbor Loss with margin and Label-Based Sample Ranking for contrastive learning. Inference uses NNK-Means soft clustering to assign error types, with unknown errors routed to a definition generation LLM. Training uses 50 epochs with batch size 16 and learning rate 1e-5.

## Key Results
- SEEED outperforms GPT-4o and Phi-4 by up to 8 points in H-Score on unknown error detection
- Dialogue summaries provide largest single-component gain (18-point H-Score drop when removed)
- Enhanced Soft Nearest Neighbor Loss with margin improves over standard SNL (H-Score from 0.24 to 0.36)
- Generalizes to intent detection tasks on CLINC, BANKING, and StackOverflow datasets

## Why This Works (Mechanism)

### Mechanism 1
Label-Based Sample Ranking improves contrastive learning by selecting hard negatives near decision boundaries and hard positives that are misclassified. It categorizes samples into four groups based on ground-truth labels versus cluster assignments, using hard negatives to push representations apart and hard positives to refine boundaries. Evidence shows removing LBSR drops H-Score from 0.36 to 0.26.

### Mechanism 2
Enhanced Soft Nearest Neighbor Loss with margin parameter amplifies distance weighting for negative pairs, improving cluster separability. The margin explicitly penalizes negative pairs by subtracting m·I(yi≠yj) from similarity scores, forcing larger separation between different error types. Evidence shows removing margin drops H-Score from 0.36 to 0.24.

### Mechanism 3
Dialogue summaries generated by LLMs focus contextual information on error-indicative patterns, mitigating noise from lengthy dialogue histories. Raw contexts contain irrelevant utterances that dilute error signals, while condensed summaries extract only information relevant to potential errors. Evidence shows removing summaries drops H-Score from 0.36 to 0.18.

## Foundational Learning

- **Concept: Contrastive Learning with Hard Negative Mining**
  - Why needed here: SEEED's core training relies on pushing apart similar-but-different error types while pulling same-class samples together. Without understanding how hard negatives sharpen decision boundaries, the margin-enhanced SNL and LBSR contributions will be opaque.
  - Quick check question: Given two samples with different labels but high embedding similarity, should they be treated as easy or hard negatives, and why does this distinction matter?

- **Concept: Soft vs. Hard Clustering**
  - Why needed here: SEEED uses NNK-Means (soft clustering) where points can belong to multiple clusters with weighted membership. This is contrasted with k-Means used in baselines like SynCID. Understanding why soft assignments help with ambiguous error types is critical.
  - Quick check question: If a dialogue exhibits both "Ignore Question" and "Ignore Request" patterns, which clustering approach would better capture this ambiguity?

- **Concept: Generalized Category Discovery (GCD)**
  - Why needed here: The problem is framed as GCD, where training sees only known error types but inference must handle both known and unknown types. This differs from standard classification where all classes are seen during training.
  - Quick check question: In GCD, why would a model trained only on known error types be evaluated on unknown types, and what inductive biases enable this generalization?

## Architecture Onboarding

- **Component map:**
  Dialogue Context → [Summary LLM (Llama-3.1 8B)] → Summary
                    ↓                                      ↓
              [Context Encoder (BERT)]           [Summary Encoder (BERT)]
                    ↓                                      ↓
                    └──────────→ [Linear Layer] ←─────────┘
                                       ↓
                              [Aggregated Representation]
                                       ↓
                           [NNK-Means Soft Clustering]
                                       ↓
                              Predicted Error Type
                                       ↓
                         [If unknown → Definition LLM]

- **Critical path:**
  1. Summary generation quality directly impacts all downstream performance (18-point H-Score drop if removed)
  2. Contrastive loss with LBSR determines representation space quality
  3. NNK-Means clustering assigns final error labels; soft memberships handle ambiguity

- **Design tradeoffs:**
  - **Encoder size vs. LLM baselines**: BERT (110M params) vs. GPT-4o (~200B). SEEED is 8x faster to train (8h vs. 72h for LOOP) but requires separate summary generation step.
  - **Soft vs. hard clustering**: NNK-Means allows multi-cluster membership (better for overlapping errors) but requires estimating total cluster count a priori.
  - **Summary-only vs. summary+context encoding**: Using both captures error-focused summary plus full dialogue signals but doubles encoding computation.

- **Failure signatures:**
  - **Ambiguous error types**: "Ignore Expectation" vs. "Ignore Request" frequently confused (overlapping definitions)
  - **Severe class imbalance**: "Conversationality" in FEDI (64 samples) vs. "Ignore Question" (2,356 samples)
  - **LLM summary failures**: Phi-4-mini produces brief, generic summaries failing to highlight errors
  - **Unknown error detection**: GPT-4o and Phi-4 default to "No Error Found" for novel types

- **First 3 experiments:**
  1. **Reproduce ablation on FEDI-Error with single-component removal**: Validate that summaries provide the largest contribution (H-Score drop from 0.36 to 0.18) and that margin-enhanced SNL provides marginal gains over standard SNL.
  2. **Test LBSR sample exhaustion behavior**: Construct a small dataset where NNK-Means consistently misassigns samples, verifying whether hard_pos exhaustion causes training failure or graceful degradation.
  3. **Cross-dataset summary quality analysis**: Generate summaries with Llama-3.1, Phi-4-mini, and DeepSeek-R1, evaluate with FineSurE metrics (Faithfulness, Completeness, Conciseness), and correlate with downstream Acc-U to validate the claim that reasoning models improve summary quality and error detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Automated Error Discovery be extended to handle multi-label scenarios where agent utterances exhibit multiple or overlapping errors simultaneously?
- Basis in paper: Limitations section states: "We frame error detection as a multi-class classification problem... However, in practice, agent utterances may exhibit multiple or overlapping errors."
- Why unresolved: Current formulation forces single-error assignment per utterance, limiting applicability to real dialogues where errors often co-occur.
- What evidence would resolve it: Experiments adapting SEEED with multi-label classification heads and evaluation on datasets annotated for multiple concurrent errors per utterance.

### Open Question 2
- Question: What is the impact of continual learning techniques on SEEED's performance in simulated practical deployments where new error types emerge over time?
- Basis in paper: Limitations section states: "Consequently, our results do not provide insights into the impact of continual learning techniques. However, related work has already shown that these can significantly increase the quality of generated responses in simulated practical deployments."
- Why unresolved: Single training phase was sufficient for analysis, but deployment requires adapting to novel errors without catastrophic forgetting.
- What evidence would resolve it: Longitudinal experiments with sequential introduction of new error types, measuring both adaptation to new errors and retention of previously learned error detection capabilities.

### Open Question 3
- Question: How well does SEEED generalize to error detection in non-English dialogues across different linguistic and cultural contexts?
- Basis in paper: Limitations section states: "Given all datasets in this work include only English dialogues, our results exhibit limited generalizability to error detection in dialogue from other linguistic and cultural contexts."
- Why unresolved: Error types and their linguistic manifestations may differ significantly across languages and cultures; cultural norms affect what constitutes socially inappropriate behavior.
- What evidence would resolve it: Cross-lingual transfer experiments on multilingual dialogue datasets with native-speaker error annotations, comparing performance against monolingual baselines.

### Open Question 4
- Question: Can the error definition generation component be enhanced to prevent semantically duplicate definitions while maintaining definitional quality?
- Basis in paper: Limitations section states: "The error definition generation prompt does not prevent duplicate definitions. While not observed in our experiments, this might become an issue in practical applications, e.g., if the threshold is set too low."
- Why unresolved: Current prompt design has no deduplication mechanism; long-term deployment could accumulate redundant error types that fragment the error taxonomy.
- What evidence would resolve it: Extended experiments measuring definition semantic similarity across many generation rounds, with analysis of clustering thresholds that balance granularity against redundancy.

## Limitations
- Evaluation relies on synthetic error injection datasets and limited human-annotated data, constraining generalizability to real-world error distributions
- Summary generation step introduces additional LLM dependency that could fail catastrophically if summarizer cannot extract error signals
- LBSR mechanism's theoretical justification assumes NNK-Means clustering quality without specifying fallback procedures for sample exhaustion
- Margin parameter for enhanced SNL is empirically chosen without systematic sensitivity analysis across datasets

## Confidence
- **SEEED's superior performance on unknown error detection (8-point H-Score improvement):** Medium confidence
- **Dialogue summaries significantly improve error detection (18-point H-Score drop when removed):** High confidence
- **Enhanced SNL with margin provides meaningful improvement over standard SNL:** Low confidence

## Next Checks
1. **Validate LBSR sample exhaustion behavior**: Construct a synthetic dataset where NNK-Means clustering is deliberately tuned to misclassify a controlled proportion of samples with correct ground-truth labels. Run SEEED training and monitor whether the algorithm gracefully degrades when hard_pos samples are exhausted, or if it fails catastrophically as the theoretical limitation suggests.

2. **Cross-dataset summary quality vs. downstream performance correlation**: Using the same dialogue contexts, generate summaries with three different models (Llama-3.1, Phi-4-mini, DeepSeek-R1) and evaluate using FineSurE metrics (Faithfulness, Completeness, Conciseness). Compute Pearson correlation between summary quality scores and Acc-U performance across all datasets to empirically validate the claim that reasoning models improve summary quality and error detection.

3. **Robustness to extreme class imbalance**: Create an artificially imbalanced version of FEDI-Error by downsampling the most frequent error types (e.g., "Ignore Question") while keeping rare types fixed. Evaluate SEEED's per-class accuracy across different imbalance ratios to determine the minimum viable sample size for reliable error detection, addressing the severe imbalance limitations observed in the original dataset.