---
ver: rpa2
title: 'Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement
  Gaps'
arxiv_id: '2601.21624'
source_url: https://arxiv.org/abs/2601.21624
tags:
- memory
- learning
- training
- state
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modern deep-learning training is not memoryless. Updates depend
  on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement,
  staged augmentations and replay), the nonconvex path, and auxiliary state (teacher
  EMA/SWA, contrastive queues, BatchNorm statistics).
---

# Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps

## Quick Facts
- arXiv ID: 2601.21624
- Source URL: https://arxiv.org/abs/2601.21624
- Reference count: 40
- Primary result: Introduces a causal protocol for measuring how much training history influences DNN performance across optimizer states, sampling, and auxiliary buffers.

## Executive Summary
This survey reveals that modern deep-learning training is inherently stateful, not memoryless. Updates depend on optimizer moments, data-order policies, and auxiliary state like BatchNorm statistics. The paper introduces portable perturbation primitives and a causal estimation framework to quantify these dependencies. It proposes a "Branch-and-Hold" protocol to isolate and measure the impact of specific history sources on model behavior.

## Method Summary
The paper proposes a "Branch-and-Hold" causal estimation protocol. At a chosen step, the full training state is snapshotted and two execution branches are forked: a Control branch continues normally, and a Treatment branch applies a specific perturbation (e.g., momentum reset) while forcing identical data order. Function-space metrics (e.g., Total Variation, Calibration Error) are compared at the intervention window and final training horizon. The method uses seed-paired experiments and portable perturbation primitives to ensure portability and reproducibility.

## Key Results
- Training history significantly impacts model performance; optimizer states (momentum, Adam moments) and data order contribute measurable effects.
- The proposed causal estimands and perturbation primitives enable portable measurement of history dependence across models and tasks.
- A reporting checklist with audit artifacts (order hashes, buffer checksums, RNG contracts) is introduced to ensure reproducibility.

## Why This Works (Mechanism)
The approach works by exploiting causal inference techniques to isolate the effect of specific training state components. By snapshotting and forking execution, it creates controlled experiments that measure the impact of perturbing individual history sources while holding other variables constant. The use of function-space metrics and seed-paired experiments provides robust and portable measurements.

## Foundational Learning
- **Optimizer State Dependencies**: Momentum, Adam moments, and other optimizer buffers accumulate history and influence updates. Why needed: To quantify how much optimizer history contributes to final model behavior. Quick check: Perturb momentum buffers and measure performance change.
- **Data Order Effects**: Random reshuffling vs. with-replacement, and staged augmentations, create history dependencies. Why needed: To understand how data ordering policies impact training outcomes. Quick check: Compare training with fixed vs. shuffled data order.
- **BatchNorm Statistics**: Running mean and variance accumulate over training. Why needed: To assess how normalization history affects model generalization. Quick check: Reset BN stats mid-training and observe changes.
- **Causal Inference**: Using Average Treatment Effects (ATEs) to measure intervention impacts. Why needed: To attribute performance changes to specific sources of history dependence. Quick check: Apply ATE framework to a simple training perturbation.
- **Reproducibility**: Audit artifacts like order hashes and RNG contracts ensure results are portable. Why needed: To enable independent verification and comparison across studies. Quick check: Replicate results using provided audit artifacts.

## Architecture Onboarding

**Component Map**
Snapshotter -> Forker -> Perturber -> Evaluator -> Auditor
(Optimizer, Sampler, BN, Teacher) -> (Control/Treatment) -> (Momentum Reset, Order Swap, Queue Change) -> (Function-Space Metrics) -> (Audit Artifacts)

**Critical Path**
1. Train to mid-point $t_0$ and snapshot full state.
2. Fork into Control and Treatment branches.
3. Apply perturbation in Treatment branch for window $W$.
4. Measure divergence using function-space metrics.
5. Aggregate results across seeds with ATE estimation.

**Design Tradeoffs**
- **Portability vs. Fidelity**: Using portable perturbations may not capture all real-world history dependencies.
- **Overhead vs. Precision**: Frequent snapshots increase overhead but improve measurement accuracy.
- **Generality vs. Specificity**: Broad perturbation primitives may miss task-specific history effects.

**Failure Signatures**
- **Order Drift**: Data loading changes sequence between branches, invalidating causal link. Diagnostic: Log order hashes to verify identity.
- **Stale Normalization**: Perturbations alter feature distributions, but BN stats retain stale history. Diagnostic: Recalibrate BN stats before evaluation.
- **Nondeterministic Kernels**: GPU atomic operations break paired-seed guarantee. Diagnostic: Use deterministic kernels or increase seed count.

**First Experiments**
1. **Momentum Reset on CIFAR-10/ResNet-18**: Apply momentum zeroing at $t_0$ and measure performance change at $t_0+W$ and end-of-training.
2. **Order Swap on SST-2/DistilBERT**: Swap data order in Treatment branch and compare function-space metrics.
3. **BN Reset on ImageNet**: Reset BatchNorm running statistics mid-training and evaluate impact on generalization.

## Open Questions the Paper Calls Out
None

## Limitations
- Claims are largely proof-of-concept on small/medium models; scaling to large transformers remains untested.
- Causal identification assumes perfect state snapshot/replay; nondeterministic kernels can introduce unmeasured drift.
- Functional form of perturbations may not capture all real-world history dependencies, especially in RL or non-iid settings.

## Confidence
- **High confidence**: Historical dependence exists and is measurable via proposed causal estimands and perturbation library.
- **Medium confidence**: Branch-and-Hold protocol is implementable and yields consistent results across seeds in tested settings.
- **Low confidence**: Cross-model generality claims (e.g., momentum reset explains >X% variance) without large-scale validation.

## Next Checks
1. **Stress-test with large-scale models**: Apply protocol to GPT-2-sized transformers and Vision Transformers to see if effects persist or diminish.
2. **Evaluate under non-iid and RL settings**: Check whether perturbation primitives remain valid when data order or transitions are non-stationary.
3. **Measure statistical power**: Perform formal power analysis to determine required seeds/interventions to detect realistic treatment effects in high-dimensional parameter spaces.