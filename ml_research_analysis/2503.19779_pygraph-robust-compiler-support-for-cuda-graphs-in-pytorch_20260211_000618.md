---
ver: rpa2
title: 'PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch'
arxiv_id: '2503.19779'
source_url: https://arxiv.org/abs/2503.19779
tags:
- cuda
- graph
- pygraph
- kernel
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PyGraph is a compiler framework designed to maximize the performance
  of CUDA Graphs in PyTorch by addressing key deployment challenges. It introduces
  three novel optimizations: CUDA Graph-aware code transformations that restructure
  programs to eliminate CPU-resident tensors and scalars, parameter indirection that
  converts data copies to pointer copies during graph replay, and selective CUDA Graph
  deployment that uses profiling to determine when graph usage is beneficial.'
---

# PyGraph: Robust Compiler Support for CUDA Graphs in PyTorch

## Quick Facts
- arXiv ID: 2503.19779
- Source URL: https://arxiv.org/abs/2503.19779
- Reference count: 40
- Primary result: PyGraph more than doubles CUDA Graph performance benefits compared to PyTorch2, achieving up to 3.36x speedup on H100 GPU

## Executive Summary
PyGraph is a compiler framework that enhances CUDA Graph performance in PyTorch by addressing deployment challenges through three key optimizations. The framework introduces CUDA Graph-aware code transformations to eliminate CPU-resident tensors, parameter indirection to convert data copies to pointer copies during graph replay, and selective deployment using profiling to determine optimal graph usage. PyGraph achieves significant performance improvements across 25 ML workloads while guaranteeing no application slowdown, requiring no programmer intervention and building upon PyTorch2's compilation framework.

## Method Summary
PyGraph implements three novel optimizations to maximize CUDA Graph performance: CUDA Graph-aware code transformations that restructure programs to eliminate CPU-resident tensors and scalars, parameter indirection that converts data copies to pointer copies during graph replay, and selective CUDA Graph deployment that uses profiling to determine when graph usage is beneficial. The framework requires no programmer intervention and is built atop PyTorch2's compilation framework, enabling seamless integration while expanding CUDA Graph coverage and eliminating parameter copying overhead.

## Key Results
- More than doubles performance benefit compared to PyTorch2's CUDA Graph implementation
- Achieves up to 3.36x speedup on H100 GPU across 25 diverse ML workloads
- Guarantees no application slowdown through profiling-driven deployment

## Why This Works (Mechanism)
PyGraph's optimizations address fundamental limitations in CUDA Graph deployment by restructuring programs to eliminate CPU-resident tensors that break graph capture, converting expensive data copies to lightweight pointer copies during parameter indirection, and using profiling to selectively deploy graphs only when beneficial. This approach expands the coverage of CUDA Graphs beyond simple workloads while eliminating the overhead of copying kernel parameters, enabling performance gains without requiring programmer intervention.

## Foundational Learning
- CUDA Graphs: Precompiled sequences of GPU kernels that eliminate launch overhead; needed for high-performance ML workloads
- PyTorch2 compilation framework: Infrastructure for optimizing PyTorch programs; provides foundation for PyGraph integration
- Parameter indirection: Technique converting data copies to pointer copies; reduces overhead during graph replay
- Code transformations: Program restructuring to eliminate CPU-resident tensors; enables broader CUDA Graph coverage
- Profiling-driven deployment: Runtime analysis to determine optimal graph usage; ensures no performance degradation

## Architecture Onboarding

Component map: PyTorch2 framework -> PyGraph compiler -> CUDA Graph runtime

Critical path: Source code → PyTorch2 compilation → PyGraph optimizations → CUDA Graph capture → GPU execution

Design tradeoffs: PyGraph prioritizes zero programmer intervention and guaranteed no slowdown over maximum theoretical performance, accepting some overhead from profiling to ensure robustness across diverse workloads.

Failure signatures: Dynamic tensor shapes, graph topology changes, and complex control flow may prevent successful graph capture or reduce optimization effectiveness.

First experiments:
1. Measure performance improvement on a simple ML workload with known CPU-resident tensors
2. Verify parameter indirection reduces data copy overhead during graph replay
3. Test profiling-driven deployment on a workload where graphs provide mixed benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to 25 ML workloads, potentially missing broader PyTorch ecosystem behaviors
- Effectiveness may be constrained by CUDA Graphs' handling of dynamic tensor shapes and graph topology changes
- Reliance on PyTorch2 compilation framework introduces compatibility dependencies

## Confidence
- Performance claims: Medium-High (measured improvements but limited workload diversity)
- "No slowdown" guarantee: Medium (profiling-based approach shows promise but needs broader validation)
- Compiler framework robustness: Medium (depends on PyTorch2 integration and CUDA Graph limitations)

## Next Checks
1. Test PyGraph across a more diverse set of PyTorch workloads, including non-ML applications, to verify generalization
2. Evaluate the framework's behavior under dynamic tensor shapes and graph topology changes that may occur during training
3. Benchmark against alternative CUDA Graph optimization approaches to establish relative performance benefits