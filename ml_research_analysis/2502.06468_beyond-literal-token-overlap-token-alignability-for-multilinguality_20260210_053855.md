---
ver: rpa2
title: 'Beyond Literal Token Overlap: Token Alignability for Multilinguality'
arxiv_id: '2502.06468'
source_url: https://arxiv.org/abs/2502.06468
tags:
- language
- transfer
- ru-zh
- bg-ru
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces token alignability as a metric for understanding
  cross-lingual knowledge transfer in multilingual language models. While previous
  metrics like literal token overlap and distributional similarity are limited for
  language pairs with different scripts, token alignability captures statistical correspondences
  between subword tokens.
---

# Beyond Literal Token Overlap: Token Alignability for Multilinguality

## Quick Facts
- arXiv ID: 2502.06468
- Source URL: https://arxiv.org/abs/2502.06468
- Reference count: 40
- Key outcome: Token alignability better predicts cross-lingual transfer than literal overlap, especially for different-script language pairs

## Executive Summary
This paper introduces token alignability as a novel metric for understanding cross-lingual knowledge transfer in multilingual language models. While traditional metrics like literal token overlap fail for language pairs with different scripts, token alignability captures statistical correspondences between subword tokens using word alignment techniques. The authors demonstrate that their eflomal-based alignability score better predicts downstream cross-lingual transfer performance than distributional overlap measures like JSD, particularly for different-script language pairs. The findings suggest that token alignability can guide the development of more effective multilingual tokenizers and help identify optimal language pairs for cross-lingual transfer.

## Method Summary
The authors compute token alignability by running statistical word alignment (eflomal) on tokenized parallel corpora, extracting alignment scores that capture subword correspondences beyond literal token overlap. They compare this metric against distributional overlap (JSD) by computing Spearman correlations with downstream transfer results (XNLI, POS, UD, NER) and embedding alignment measures (FLORES retrieval F1). The analysis uses FLORES-200 as test data, OPUS-100 and MultiCCAligned for aligner training, and pre-trained encoder/decoder models. Tokenization is tested across BPE, Unigram, and TokMix variants with 120k vocabulary sizes.

## Key Results
- Eflomal alignability scores predict cross-lingual transfer better than JSD for different-script language pairs
- Strong correlation between token alignability and cross-lingual embedding alignment (r=-0.83 for FLORES retrieval F1)
- Proportion of 1-1 alignments shows weaker correlation than eflomal score, which captures more nuanced correspondences
- Results vary for decoder models, suggesting different cross-linguality mechanisms compared to encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical token alignment predicts cross-lingual transfer better than literal token overlap for different-script language pairs.
- Mechanism: Languages sharing scripts produce overlapping subword tokens, enabling shared vocabulary embeddings. For different-script pairs (e.g., Hindi-Urdu), literal overlap is near zero, so distributional metrics like JSD fail. Token alignability uses word alignments to discover which subword tokens correspond translationally, bypassing surface-form differences entirely.
- Core assumption: Statistical word alignment quality (from eflomal) reflects the underlying translational correspondence that models exploit during pre-training.
- Evidence anchors:
  - [abstract] "This metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low."
  - [page 3, Table 1] Eflomal correlations stronger for different-script pairs across tasks (e.g., POS: r=-0.64 vs JSD r=-0.45 for Unigram).
  - [corpus] "False Friends Are Not Foes" confirms vocabulary overlap effects are mixed and confounded by frequency, supporting alignment-based approaches.
- Break condition: If word aligners fail on noisy or low-resource parallel data, alignability scores become unreliable.

### Mechanism 2
- Claim: Eflomal alignment scores capture nuanced correspondences that simple 1-1 alignment ratios miss.
- Mechanism: The proportion of 1-1 alignments is crude. The eflomal score—a log-probability estimate—captures many-to-one, one-to-many, and uncertain alignments, reflecting true cross-lingual subword mapping complexity.
- Core assumption: The alignment model's final sampling log-probability correlates with how easily a language model learns cross-lingual representations.
- Evidence anchors:
  - [page 3] "the proportion of one-to-one alignments shows weaker or no correlation... while the eflomal score, as an estimate of log-probability, captures more nuance."
  - [page 3, §3.2b] Eflomal score defined as "maximum unnormalized log-probability of links in the last sampling iteration."
  - [corpus] Limited direct evidence on why log-probability outperforms—this is an internal finding.
- Break condition: Poorly trained alignment priors (insufficient parallel data) degrade score quality.

### Mechanism 3
- Claim: Token alignability correlates with representation alignment, but representation alignment is only one transfer factor.
- Mechanism: Good token alignability yields better cross-lingual embedding similarity. However, downstream tasks involve additional factors (task heads, fine-tuning dynamics), so correlation is imperfect.
- Core assumption: Middle encoder layers (layer 7) best capture cross-lingual alignment.
- Evidence anchors:
  - [page 3-4, Table 2] Strong correlation between eflomal and FLORES retrieval F1 (r=-0.83 for different-script).
  - [page 4] "cross-lingual embedding alignment, as measured by similarity, is just one factor in the cross-lingual transfer ability."
  - [corpus] "RomanLens" suggests latent romanization is another factor beyond tokenization.
- Break condition: Decoder models show different patterns (Table 4 mixed results).

## Foundational Learning

- Concept: **Word Alignment in Statistical MT**
  - Why needed here: The paper uses eflomal to compute token alignability. Understanding alignment (IBM models, symmetrization) clarifies what scores represent.
  - Quick check question: Given parallel sentences "the cat" ↔ "le chat", what alignments would a word aligner discover?

- Concept: **Spearman Rank Correlation**
  - Why needed here: All predictive claims use Spearman correlations between metrics and transfer performance.
  - Quick check question: Why use rank correlation instead of Pearson for evaluating metric quality?

- Concept: **Subword Tokenization (BPE vs Unigram)**
  - Why needed here: The study tests BPE, Unigram, and TokMix; vocabulary construction differences explain result variance.
  - Quick check question: How does BPE build vocabulary differently from Unigram LM?

## Architecture Onboarding

- Component map:
Parallel Corpus (OPUS-100 / MultiCCAligned) -> Subword Tokenizer (BPE/Unigram/TokMix) -> Statistical Word Aligner (eflomal) -> Alignment Score -> Token Alignability Metric <-> JSD Baseline -> Correlation Analysis -> Downstream Transfer (XNLI, POS, UD, NER) OR Embedding Alignment (Retrieval F1, average margin, Tatoeba accuracy)

- Critical path:
1. Tokenize parallel corpus with target multilingual tokenizer
2. Train eflomal priors on separate parallel data (≤300k sentence pairs)
3. Run alignment on test corpus (FLORES-200)
4. Extract eflomal score (averaged over both directions)
5. Compute Spearman correlation with downstream transfer results from pre-trained encoder models or decoder models

- Design tradeoffs:
- **Alignment training data**: OPUS-100 vs domain-matched data affects prior quality
- **Symmetrization**: Averaging both directions vs directional scores (paper uses averaged)
- **Tokenizer type**: TokMix trades vocabulary fairness for slightly weaker alignability correlation (Table 1c)

- Failure signatures:
- Near-zero correlation for XNLI (sentence-level, fewer language pairs)
- JSD clustering all different-script pairs together regardless of transfer performance
- Decoder models (Llama3, Aya23) showing opposite patterns to encoders

- First 3 experiments:
1. **Validate alignability on your tokenizer**: Compute eflomal scores for target language pairs using OPUS-100 priors; correlate with existing transfer benchmarks.
2. **Ablate alignment training data**: Test if domain-matched parallel data improves correlation vs general OPUS data.
3. **Pilot tokenizer improvement**: For a low-alignability pair, test if vocabulary merging based on alignment links improves downstream transfer.

## Open Questions the Paper Calls Out

- Question: How can token alignability be efficiently approximated and integrated into the vocabulary learning process (e.g., BPE merges or Unigram pruning) to guide the construction of better multilingual tokenizers?
  - Basis in paper: [explicit] The authors state in the Future Work section that "future work in this area will require finding suitable approximations, like calculating alignability score difference for some fraction... of all candidate tokens at a time," as a naive implementation is too computationally intensive.
  - Why unresolved: Calculating full alignment scores at every step of tokenizer training is currently impractical, preventing the metric from being used directly to optimize vocabularies.
  - What evidence would resolve it: A method that approximates alignability during training sufficiently well to produce tokenizers that yield higher downstream cross-lingual transfer performance or better vocabulary fairness compared to standard baselines.

- Question: Why does token alignability show inconsistent predictive power for cross-lingual alignment in large decoder models compared to encoder models?
  - Basis in paper: [explicit] The Discussion section notes that while eflomal scores predict transfer in encoders, results vary in decoders (Mistral, Aya23, Llama3). The authors suggest "cross-linguality in these decoder models works differently than in encoder models, or that they do rely more on literal token matches."
  - Why unresolved: The study was limited to analyzing correlations in existing pre-trained decoders and did not perform the controlled experiments necessary to distinguish between different mechanisms of cross-linguality in these architectures.
  - What evidence would resolve it: Controlled experiments on decoder models (e.g., probing or fine-tuning studies) that isolate whether cross-lingual transfer is driven by statistical alignment correspondences or relies more heavily on surface-form token overlaps.

- Question: How can the token alignability metric be reformulated to apply to word-level tasks rather than just corpus-wide scores?
  - Basis in paper: [explicit] The Limitations section states that "In its present formulation, alignability is also a corpus-wide score, meaning it would require reformulating for word-level tasks."
  - Why unresolved: The current metric aggregates alignment probabilities over a whole corpus, obscuring how well specific words or subword units align, which limits the granularity of the analysis.
  - What evidence would resolve it: A modified formulation of the metric that correlates with transfer performance at the word or subword level, validated against word-level translation or tagging tasks.

## Limitations

- The study relies on correlation-based validation rather than causal experiments, preventing definitive proof that improving token alignability will improve downstream performance
- Token alignability depends on the quality of parallel training data and the alignment model's assumptions, which could yield unreliable scores for noisy or low-resource corpora
- The metric is formulated as a corpus-wide score, limiting its application to word-level tasks without reformulation

## Confidence

**High confidence**: The core finding that eflomal token alignability outperforms literal token overlap (JSD) for predicting cross-lingual transfer, particularly for different-script language pairs.

**Medium confidence**: The claim that token alignability specifically captures "statistical correspondences" beyond surface form.

**Medium confidence**: The assertion that "representation alignment is just one factor" in transfer ability.

## Next Checks

1. **Causal validation**: For a language pair with low token alignability, explicitly modify the tokenizer vocabulary based on alignment links (merging problematic tokens) and measure the impact on actual downstream transfer performance.

2. **Alignment quality ablation**: Systematically vary the size and domain of parallel data used to train eflomal priors, measuring how this affects the correlation between alignability scores and transfer performance.

3. **Factor contribution analysis**: For a fixed language pair, decompose the variance in transfer performance by correlating with multiple factors simultaneously (token alignability, pre-training data size per language, vocabulary overlap, embedding alignment).