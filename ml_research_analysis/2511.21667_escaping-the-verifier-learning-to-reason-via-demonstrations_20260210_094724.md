---
ver: rpa2
title: 'Escaping the Verifier: Learning to Reason via Demonstrations'
arxiv_id: '2511.21667'
source_url: https://arxiv.org/abs/2511.21667
tags:
- policy
- reasoning
- critic
- raro
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RARO (Relativistic Adversarial Reasoning Optimization) addresses
  the challenge of training reasoning-capable language models when task-specific verifiers
  are unavailable. The method employs Inverse Reinforcement Learning through an adversarial
  game between a policy and a relativistic critic, using only expert demonstrations.
---

# Escaping the Verifier: Learning to Reason via Demonstrations
## Quick Facts
- arXiv ID: 2511.21667
- Source URL: https://arxiv.org/abs/2511.21667
- Reference count: 40
- RARO significantly outperforms verifier-free baselines across diverse reasoning tasks

## Executive Summary
RARO (Relativistic Adversarial Reasoning Optimization) introduces a novel approach for training reasoning-capable language models without task-specific verifiers. By leveraging Inverse Reinforcement Learning through an adversarial game between a policy and relativistic critic, RARO learns to produce expert-like reasoning outputs using only pairwise expert demonstrations. The method achieves performance close to RLVR methods on verifiable tasks while extending to open-ended reasoning domains where explicit verification signals are unavailable.

## Method Summary
RARO employs an adversarial training framework where a policy network generates reasoning outputs while a critic learns to distinguish between expert and policy-generated responses through pairwise comparison. Both components are trained jointly via reinforcement learning, with stabilization techniques including data mixing and replay buffers. The relativistic critic evaluates responses by comparing them directly rather than assigning absolute scores, enabling the system to learn from demonstrations without requiring explicit correctness labels.

## Key Results
- RARO significantly outperforms strong verifier-free baselines across Countdown, DeepMath, and Poetry Writing tasks
- Achieves near-RLVR performance on verifiable tasks while working in verifier-free settings
- Demonstrates effective scaling with both model size and reasoning budget across diverse reasoning domains

## Why This Works (Mechanism)
The adversarial framework creates a natural learning signal through the critic's ability to discriminate between expert and policy outputs. By framing the problem as a relativistic comparison rather than absolute scoring, RARO captures the relative quality differences in reasoning traces without requiring explicit verification mechanisms. The joint training ensures both policy and critic evolve together, maintaining balance in the adversarial game.

## Foundational Learning
- **Inverse Reinforcement Learning**: Learns reward functions from expert demonstrations rather than explicit rewards; needed because direct supervision is unavailable, quick check: verify expert demonstration quality
- **Adversarial Training**: Uses competing objectives between policy and critic; needed to create learning signals without verifiers, quick check: monitor training stability
- **Reinforcement Learning**: Updates policy based on critic feedback; needed for continuous improvement, quick check: track policy performance trends
- **Relativistic Evaluation**: Compares pairs of responses rather than absolute scoring; needed for verifier-free learning, quick check: ensure consistent comparison logic
- **Replay Buffers**: Stores past experiences for training stability; needed to prevent catastrophic forgetting, quick check: verify buffer diversity
- **Data Mixing**: Combines different data sources during training; needed for robust generalization, quick check: monitor mixing ratios

## Architecture Onboarding
- **Component Map**: Expert Demonstrations -> Data Processor -> Policy Network -> Critic Network -> Policy Updates
- **Critical Path**: Expert Demonstrations → Policy Generation → Pairwise Comparison → Critic Evaluation → Policy Reinforcement
- **Design Tradeoffs**: Pairwise comparison enables verifier-free learning but requires more computation per update; adversarial training provides rich signals but needs careful stability management
- **Failure Signatures**: Critic collapse (always preferring one type), policy collapse (producing degenerate outputs), training instability (oscillating performance)
- **First Experiments**: 1) Verify pairwise comparison logic with synthetic data, 2) Test critic stability with balanced expert/policy data, 3) Validate reinforcement learning updates maintain policy diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Requires high-quality expert demonstrations which may not be available for all reasoning tasks
- Performance bounded by demonstration quality and diversity
- Adversarial training introduces complexity and may require careful hyperparameter tuning

## Confidence
- High confidence: Core methodology and experimental design appear sound with clear implementation details
- Medium confidence: Scalability claims and performance on complex tasks are reasonably supported but evaluation scope could be broader
- Low confidence: Long-term stability of adversarial training and effectiveness on highly complex open-ended tasks remain uncertain

## Next Checks
1. Evaluate RARO's performance on broader range of reasoning tasks with longer chains and complex dependencies
2. Conduct ablation studies to quantify impact of individual components (data mixing, replay buffers, pairwise comparison)
3. Test method's robustness to varying qualities and quantities of expert demonstrations, including noisy or incomplete cases