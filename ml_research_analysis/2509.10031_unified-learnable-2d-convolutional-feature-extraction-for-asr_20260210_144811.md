---
ver: rpa2
title: Unified Learnable 2D Convolutional Feature Extraction for ASR
arxiv_id: '2509.10031'
source_url: https://arxiv.org/abs/2509.10031
tags:
- feature
- layer
- extraction
- first
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified learnable 2D convolutional feature
  extraction front-end for ASR that minimizes reliance on handcrafted methods while
  achieving competitive performance with existing supervised learnable feature extractors.
  The proposed architecture uses 2D convolutions with a randomly initialized filterbank
  as the first layer, followed by multiple 2D convolutional layers with strides of
  2 in the time dimension to achieve the desired frame rate of 40 ms.
---

# Unified Learnable 2D Convolutional Feature Extraction for ASR

## Quick Facts
- **arXiv ID**: 2509.10031
- **Source URL**: https://arxiv.org/abs/2509.10031
- **Reference count**: 34
- **Primary result**: 2D learnable convolutional front-end achieves competitive ASR performance with only 0.3M parameters vs 12.4M for existing methods

## Executive Summary
This paper presents a unified learnable 2D convolutional front-end for automatic speech recognition that minimizes reliance on handcrafted methods while achieving competitive performance with existing supervised learnable feature extractors. The proposed architecture uses 2D convolutions with a randomly initialized filterbank as the first layer, followed by multiple 2D convolutional layers with strides of 2 in the time dimension to achieve the desired frame rate of 40 ms. Experiments on LibriSpeech show that this generic front-end performs on par with existing learnable feature extractors like SCF and wav2vec feature extractors, while being highly parameter-efficient.

## Method Summary
The proposed architecture begins with a 1D learnable filterbank layer (kernel=256, stride=10, channels=80 or 128) applied to raw audio waveforms with random initialization. This is followed by 6 layers of 2D convolutions (3×3 kernel, stride 2 in time dimension, ReLU activation) to achieve the target 40ms frame rate. The acoustic model is a 12-layer Conformer with CTC loss, trained using AdamW optimizer with One Cycle learning rate schedule. The system uses SpecAugment in the STFT domain before feature extraction, along with speed perturbation augmentation.

## Key Results
- Achieves competitive WER on LibriSpeech dev/test sets compared to existing learnable feature extractors
- Highly parameter-efficient: only 0.3M parameters in most efficient configuration vs 12.4M for SCF
- Learned filters exhibit bandpass characteristics with adjacent ascending/descending center frequencies
- Performance on par with existing learnable feature extractors like SCF and wav2vec feature extractors

## Why This Works (Mechanism)
The architecture works by learning optimal time-frequency representations directly from raw waveforms through a combination of initial random filterbank and subsequent 2D convolutional layers. The 2D convolutions with time strides effectively downsample the temporal dimension while learning increasingly abstract features. The random initialization of the first filterbank layer allows the network to discover optimal frequency decomposition without prior assumptions.

## Foundational Learning
- **2D convolutions in speech**: 2D convolutions can learn spatial patterns in time-frequency representations, enabling the network to discover optimal feature hierarchies. This is needed because traditional 1D approaches cannot capture the joint time-frequency structure of speech.
- **Filterbank initialization**: Random initialization of the first layer allows the network to learn optimal frequency decomposition without handcrafted constraints. Quick check: verify the first layer weights are indeed random at initialization.
- **Temporal downsampling**: Using stride=2 in time dimension progressively reduces temporal resolution to match the desired 40ms frame rate. Quick check: calculate output dimensions after each layer to ensure correct frame rate.
- **CTC loss for ASR**: Connectionist Temporal Classification enables training without frame-level alignments. This is needed because phoneme boundaries are not precisely known. Quick check: verify label alignment and CTC blank token handling.
- **SpecAugment application point**: Applying augmentation in STFT domain before learnable layers prevents filter learning from being biased by augmented artifacts. Quick check: confirm augmentation pipeline order in training config.
- **Parameter efficiency trade-offs**: The 2D architecture achieves competitive performance with far fewer parameters by learning compact representations. Quick check: count total parameters in each configuration variant.

## Architecture Onboarding

**Component Map**: Raw waveform → Conv1D filterbank → 6×Conv2D(stride=2) → Conformer → CTC loss

**Critical Path**: The first Conv1D layer with kernel=256 and stride=10 is critical - smaller kernels cause training failure. The 6 Conv2D layers with stride=2 in time dimension must be correctly configured to achieve 40ms frame rate.

**Design Tradeoffs**: Random initialization vs pretrained filters trades convergence speed for flexibility. 2D vs 1D convolutions adds parameters but enables joint time-frequency learning. SpecAugment in STFT domain vs raw waveform affects filter learning dynamics.

**Failure Signatures**: Training divergence with kernel size < 256, shape mismatches with Conformer if frame rate incorrect, performance degradation if SpecAugment applied after learnable layers.

**First Experiments**:
1. Train with kernel=256 vs kernel=16 to verify sensitivity
2. Test frame rate calculation after each Conv2D layer
3. Compare WER when SpecAugment is applied before vs after feature extraction

## Open Questions the Paper Calls Out

**Open Question 1**: Can the performance gap between the proposed 2D learnable front-end and log Mel filterbanks be closed by utilizing stronger audio perturbation techniques? The authors suggest this gap "could be at least partially caused by a lack of audio perturbation."

**Open Question 2**: Can specific fusion methods for real and imaginary STFT components outperform magnitude-only inputs in the unified 2D architecture? The paper notes that while using complex components theoretically avoids information loss, "an improved fusion method... could possibly improve the results."

**Open Question 3**: Does the parameter-efficient 2D convolutional architecture transfer effectively to speech processing tasks other than ASR? The paper explicitly identifies "possibilities for further studies on features trained from scratch, possibly also for other tasks besides ASR."

## Limitations
- Exact SpecAugment parameters (mask counts, widths) are unspecified
- Phoneme lexicon construction details beyond Sequitur G2P are not provided
- Conformer implementation specifics (relative positional encoding details) are assumed from standard references

## Confidence
- **High confidence** in architectural claims: The 2D convolutional front-end design is clearly specified and reproducible
- **Medium confidence** in performance claims: WER results depend on unspecified SpecAugment parameters and phoneme lexicon details
- **Medium confidence** in learned filter interpretation: Qualitative analysis is presented but statistical significance is not established

## Next Checks
1. **Parameter sensitivity validation**: Systematically test first-layer kernel sizes (16, 64, 128, 256 samples) to confirm reported sensitivity
2. **SpecAugment parameter ablation**: Conduct controlled experiments varying mask counts, widths, and application points to verify critical dependencies
3. **Cross-framework reproducibility**: Implement identical architecture in PyTorch/TensorFlow to verify performance characteristics are not RETURNN-specific artifacts