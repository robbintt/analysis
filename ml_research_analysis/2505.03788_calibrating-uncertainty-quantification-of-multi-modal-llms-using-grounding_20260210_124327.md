---
ver: rpa2
title: Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding
arxiv_id: '2505.03788'
source_url: https://arxiv.org/abs/2505.03788
tags:
- confidence
- baseline
- arxiv
- grounding
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of miscalibrated confidence in
  uncertainty quantification (UQ) of multi-modal large language models (LLMs), where
  self-consistency-based UQ approaches can yield high confidence in consistently incorrect
  responses. The authors propose a method to improve UQ calibration by grounding LLM-generated
  textual responses to the corresponding visual input, leveraging cross-modal consistency.
---

# Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding

## Quick Facts
- arXiv ID: 2505.03788
- Source URL: https://arxiv.org/abs/2505.03788
- Reference count: 10
- Primary result: Grounding-based calibration reduces ECE by 32.5-96.5% across VQA and medical image QA tasks

## Executive Summary
This paper addresses the problem of miscalibrated confidence in multi-modal LLM uncertainty quantification, where self-consistency-based approaches can yield high confidence in consistently incorrect responses. The authors propose grounding textual responses to visual inputs as evidence of correctness, using a grounding model's confidence to calibrate self-consistency-based UQ scores. Experiments on VQA and Slake (medical) datasets show significant ECE reductions (32.5-96.5%) compared to both standard and temperature-scaled self-consistency baselines, with reliability diagrams confirming improved calibration.

## Method Summary
The method generates multiple LLM responses per input (N=20, temperature=0.5, top_p=1), computes self-consistency-based confidence scores (LexSim, SemEnt, PredEnt, NumSets), and grounds each response to the image using a grounding model (GM) that outputs confidence scores. A temperature scaling parameter T and constant C are learned on a validation set to calibrate the GM's confidence. Final confidence is computed as Conf = Conf_baseline × Conf_GM^(1/T) + C, combining both sources. The approach uses domain-appropriate GMs (foundation VLMs for VQA, BiomedCLIP for medical images).

## Key Results
- Grounding-based calibration achieves 73.1% ECE reduction on VQA and 34.3-96.5% on Slake across various GM choices
- Calibration significantly outperforms both uncalibrated baselines and temperature-scaled baselines alone
- Reliability diagrams show improved calibration, especially for semantic entropy-based UQ
- Domain-specific GMs (BiomedCLIP on Slake) outperform general models by 87-96% improvement margins

## Why This Works (Mechanism)

### Mechanism 1
Grounding textual responses to visual inputs provides evidence of correctness that self-consistency alone cannot capture. A grounding model generates confidence scores for whether responses accurately reflect visual input - correct responses are more likely to be successfully grounded while incorrect responses fail grounding, reducing confidence.

### Mechanism 2
Temperature scaling calibrates the grounding model's confidence to align with actual response accuracy. A single temperature parameter T scales the GM's logits, sharpening or softening its confidence distribution. Both T and C are fit on a validation set to optimize calibration.

### Mechanism 3
Multiplying self-consistency confidence with calibrated grounding confidence yields better-calibrated overall confidence. The product Conf = Conf_baseline × Conf_GM^(1/T) + C penalizes consistently incorrect responses that fail grounding while preserving well-grounded correct responses.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Quantifies the gap between predicted confidence and actual accuracy. Essential for interpreting the paper's main metric.
  - Quick check: If a model has confidence 0.8 on a set of predictions, what should the accuracy be for perfect calibration?

- **Self-consistency-based Uncertainty Quantification**: Baseline methods (LexSim, SemEnt, NumSets, PredEnt) rely on consistency among multiple LLM outputs. Understanding their failure mode (consistently incorrect = high confidence) is the paper's motivation.
  - Quick check: Why might an LLM generate semantically similar but incorrect responses across multiple prompts?

- **Visual Grounding**: Linking text to image regions/semantics. The core intervention uses grounding models to verify text responses against images.
  - Quick check: How would a segmentation-based grounding model verify the statement "there is a cat on the sofa"?

## Architecture Onboarding

- **Component map**: Multi-modal LLM -> Self-consistency UQ module -> Grounding Model -> Calibration module -> Evaluation
- **Critical path**: 
  1. Load LLaVA/LLaVA-Med and appropriate GM
  2. Generate 20 responses per input (temperature=0.5, top_p=1)
  3. Compute Conf_baseline using chosen UQ method
  4. Query GM for grounding confidence on each response; average to get Conf_GM
  5. Optimize T and C on validation set to minimize ECE
  6. Apply Eq. (2) on test set and report ECE, reliability diagrams

- **Design tradeoffs**:
  - GM choice: Foundation VLMs better for VQA; domain-specific models better for medical
  - Temperature scaling vs. direct GM use: Requires sufficient validation data
  - Multiple responses (N=20): More samples improve estimates but increase cost

- **Failure signatures**:
  - High ECE despite grounding: GM miscalibrated or domain-mismatched
  - Peaky reliability diagrams at high confidence: Small number of inputs in high-confidence bins
  - Negative improvement: Specific GM+baseline combinations may be incompatible

- **First 3 experiments**:
  1. Replicate VQA results with LLaVA + LLaMA3.2V: Compute ECE for SemEnt baseline with and without grounding
  2. Ablate GM choice on Slake: Compare BiomedCLIP vs. general CLIP; quantify domain-specific benefit
  3. Validate temperature scaling necessity: Run with T=1, C=0 (no calibration) and compare ECE

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on grounding model quality and domain match
- Temperature scaling requires sufficient validation data for proper calibration
- Synthetic post-training of Biomed-QwenVL may cause compatibility issues with certain baselines

## Confidence
- **Reproducibility of results**: High - clear methodology and hyperparameter choices specified
- **Generalizability**: Medium - performance varies significantly with GM choice and domain
- **Calibration effectiveness**: High - substantial ECE improvements demonstrated across multiple datasets

## Next Checks
1. Verify temperature scaling implementation correctly calibrates GM confidence before combining with baseline
2. Test domain-specific GM performance on both VQA and Slake to quantify cross-domain effects
3. Measure per-bin sample counts in reliability diagrams to ensure statistical significance of improvements