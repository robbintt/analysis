---
ver: rpa2
title: A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language
  Models in Customer Support
arxiv_id: '2507.22542'
source_url: https://arxiv.org/abs/2507.22542
tags:
- nguyen
- villms
- vietnamese
- customer
- csconda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CSConDa, the first large-scale Vietnamese
  dataset for customer support QA, addressing the gap in domain-specific benchmarks
  for evaluating Vietnamese LLMs. The dataset comprises over 9,000 QA pairs derived
  from real customer interactions, annotated and categorized into three types (General,
  Simple, Complex) based on conversational complexity.
---

# A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support

## Quick Facts
- arXiv ID: 2507.22542
- Source URL: https://arxiv.org/abs/2507.22542
- Reference count: 33
- First large-scale Vietnamese dataset for customer support QA with over 9,000 real-world interactions

## Executive Summary
This paper introduces CSConDa, the first large-scale Vietnamese dataset for customer support QA, addressing the gap in domain-specific benchmarks for evaluating Vietnamese LLMs. The dataset comprises over 9,000 QA pairs derived from real customer interactions, annotated and categorized into three types (General, Simple, Complex) based on conversational complexity. The authors present a comprehensive evaluation framework benchmarking 11 lightweight open-source ViLLMs using automatic metrics and syntactic analysis. Results show that while models like Vistral 7B and SeaLLMs 7B lead in overall performance, all ViLLMs struggle with generating concise, human-like responses and exhibit structural rigidity. The study highlights the need for structurally-aware fine-tuning to improve fluency and adaptability in customer support applications. CSConDa is publicly available on Hugging Face, providing a robust benchmark for future ViLLM development.

## Method Summary
The authors evaluate 11 lightweight Vietnamese LLMs (7-9B parameters) using zero-shot prompting on CSConDa test split containing 1,500 questions. The evaluation combines automatic metrics (BLEU-2, ROUGE-L, METEOR, BERTScore, Cosine Similarity, Hallucination Score) with syntactic analysis (Word Count, POS Ratio, Phrase Ratio, Named Entity Difference, Dependency Length). Models run on single NVIDIA A100 (40GB) with responses constrained to match human reference length. A penalty factor ρ is applied for failed generations. NLP processing uses Stanza and underthesea libraries for Vietnamese syntactic analysis.

## Key Results
- Vistral 7B and SeaLLMs 7B achieve highest overall performance but still struggle with generating concise, human-like responses
- All ViLLMs exhibit structural inflation with high dependency lengths correlating with increased hallucination rates
- Models show significant difficulty handling informal linguistic structures (teencode, abbreviations) present in real customer interactions
- Zero-shot evaluation reveals fundamental limitations in lightweight ViLLMs' ability to synthesize responses for complex customer support queries

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Linguistic Stress Testing
- **Claim:** Standard benchmarks fail to expose model failures in customer support because they rely on formal, structured text (e.g., Wikipedia), whereas real-world interactions contain high frequencies of "teencode," abbreviations, and typos.
- **Mechanism:** By introducing CSConDa, which contains authentic informal linguistic features (abbreviation frequency 0.10, typo frequency 0.02), the evaluation forces models to decode noisy input and generate relevant responses. If a model is trained primarily on clean, formal data, it misinterprets these signals, leading to low lexical overlap scores (BLEU/ROUGE) despite potentially high semantic similarity.
- **Core assumption:** The presence of informal linguistic structures is the primary stressor causing performance degradation in existing ViLLMs, rather than lack of reasoning capacity.
- **Evidence anchors:** Mentions queries include "teencode, abbreviations, and domain-specific jargon" missing in other datasets; Existing Vietnamese QA datasets fail to reflect the nature of real-world customer interactions.
- **Break condition:** If models fine-tuned on formal data perform equally well on CSConDa, the informal linguistic stress hypothesis is invalid.

### Mechanism 2: Structural Inflation and Semantic Drift
- **Claim:** Lightweight ViLLMs (7–9B parameters) exhibit "structural inflation" (verbosity and high dependency lengths) which correlates with increased hallucination rates.
- **Mechanism:** Models generate longer responses with complex syntactic dependencies (high Dep. Length) to approximate human-like helpfulness. However, without strict grounding, this verbosity increases the probability of "semantic drift"—deviating from the prompt's constraints. The paper notes that human responses are concise (lower word count), whereas models over-generate, creating opportunities for fabrication.
- **Core assumption:** Brevity enforces grounding; verbosity is a symptom of weak constraint adherence in the model's decoding strategy.
- **Evidence anchors:** ViLLMs are verbose and structurally inflated... verbosity induces semantic drift, reducing alignment with the original query; Hallu. Score is lowest for general-type questions, where ViLLMs provide the shortest answers.
- **Break condition:** If shorter answers generated by the model result in higher hallucination scores (due to lack of context), the mechanism is broken.

### Mechanism 3: Syntactic Rigidity vs. Human Fluidity
- **Claim:** Evaluating only lexical or semantic overlap (BLEU/BERTScore) masks a model's inability to replicate human "structural flexibility."
- **Mechanism:** The framework measures the ratio of content-to-function words (POS Ratio) and dependency lengths. Humans use shorter, fluid sentence structures for efficiency. Models, often trained on formal instruction sets, default to rigid, complex syntax. This mismatch causes low Cosine Similarity despite high BERTScore—the content is semantically close but structurally "alien" to human norms.
- **Core assumption:** High dependency length (complexity) is a proxy for "robotic" or unnatural flow, which degrades user experience in customer support.
- **Evidence anchors:** Human responses consistently have the shortest dependency lengths... ViLLMs... exhibit higher dependency parsing scores, leading to rigid sentence constructions; Shows low Cosine Similarity scores relative to BERTScore, indicating structural/surface-form deviation.
- **Break condition:** If human evaluators prefer the "rigid" complex responses over concise ones, the mechanism of penalizing high dependency length is flawed.

## Foundational Learning

- **Concept: Dependency Parsing in Syntactic Evaluation**
  - **Why needed here:** The paper introduces "Dependency Length" as a novel metric for evaluating LLM fluency. You must understand how parsers measure the distance between "heads" and "dependents" in a sentence tree to interpret why high scores indicate rigidity.
  - **Quick check question:** Does a longer average dependency distance indicate a more complex or simpler sentence structure?

- **Concept: Zero-Shot Inference Constraints**
  - **Why needed here:** The benchmarking strictly uses zero-shot prompting (no external knowledge/RAG) to test intrinsic capability. Understanding this is vital to distinguish model limitations from retrieval-augmented capabilities.
  - **Quick check question:** If a model performs poorly on the "Complex" category in this setup, does it lack knowledge or the ability to synthesize existing weights?

- **Concept: Penalty Factor in Metrics**
  - **Why needed here:** The authors introduce a penalty factor (ρ) in Equation 1 to punish models that fail to generate valid responses (nonsensical loops). This adjustment is critical for ranking models that crash frequently versus those that answer poorly.
  - **Quick check question:** How does the penalty factor change the score of a model that answers only 50% of questions perfectly compared to one that answers 100% poorly?

## Architecture Onboarding

- **Component map:** CSConDa Test Split (1,500 samples) -> 11 ViLLMs (7–9B parameters) -> Automatic Metrics + Syntactic Analysis Pipeline
- **Critical path:**
  1. Data Loading: Load the 1,500 test questions (balanced across General/Simple/Complex)
  2. Inference: Run Zero-Shot generation with constrained max tokens (approx. 30–40 tokens to match human length)
  3. Metric Calculation: Compute standard metrics (BLEU, ROUGE) and apply the penalty factor (ρ) for failed generations
  4. Syntactic Extraction: Run outputs through Vietnamese toolkits (Stanza) to extract POS tags and Dependency Trees
- **Design tradeoffs:**
  - Conciseness vs. Hallucination: The paper suggests constraining output length reduces hallucinations but may lower recall for complex queries
  - Semantic vs. Syntactic: Relying on BERTScore hides structural rigidity; you must balance semantic similarity scores with syntactic analysis (Dep. Length) to detect "robotic" responses
- **Failure signatures:**
  - Structural Inflation: Model generates 2x the average human word count
  - Looping/Invalid: Model fails to generate a valid stop token (handled by ρ penalty)
  - High BERTScore / Low Cosine: Model understands the topic but uses unnatural phrasing or rigid syntax
- **First 3 experiments:**
  1. Baseline Assessment: Run the top 3 models (Vistral 7B, SeaLLMs 7B, Sailor 7B) on the "Simple" test set and measure the gap between BERTScore and Human Cosine Similarity
  2. Stress Test: Evaluate the "Complex" set specifically for Hallucination Score correlation with Word Count to verify the verbosity-drift hypothesis
  3. Ablation on Length: Force a max-new-tokens limit of 20 on a subset and observe if Hallucination Score drops significantly

## Open Questions the Paper Calls Out

- **Question:** Does incorporating external contextual information into CSConDa improve ViLLM accuracy in complex customer support scenarios?
  - **Basis in paper:** The authors state in Section 5 that future work involves "incorporating contextual information" to enhance the dataset, moving beyond intrinsic evaluation.
  - **Why unresolved:** The current study evaluates models solely on intrinsic capabilities using zero-shot prompting, without providing external knowledge bases or documents for retrieval-augmented generation (RAG).
  - **Evidence to resolve it:** A comparative benchmark on an extended version of CSConDa including relevant context documents, showing performance differences between RAG-based and zero-shot approaches.

- **Question:** Can "structurally-aware fine-tuning" successfully reduce the verbosity and rigidity observed in current ViLLM outputs?
  - **Basis in paper:** The analysis concludes that addressing the identified lack of fluidity requires "a refined fine-tuning approach that enhances structural efficiency [and] syntactic adaptability."
  - **Why unresolved:** The paper identifies the problem (high dependency length/word count) and proposes the solution conceptually, but does not implement or validate this specific fine-tuning method.
  - **Evidence to resolve it:** A training run using a loss function based on syntactic metrics (e.g., Dependency Length) resulting in models that achieve lower Word Count and higher human-likeness scores.

- **Question:** How strongly do the proposed syntactic metrics (e.g., POS Ratio, Phrase Ratio) correlate with human judgments of helpfulness in customer support?
  - **Basis in paper:** The paper establishes a comprehensive automatic evaluation framework but notes the "primary limitation is focusing solely on intrinsic model capabilities" without direct human feedback on the specific syntactic criteria.
  - **Why unresolved:** It is unclear if the structural rigidity identified by the tools (e.g., lack of function phrases) actually degrades the user experience in a live deployment setting.
  - **Evidence to resolve it:** A human annotation study correlating user satisfaction scores with the paper's proposed syntactic analysis metrics.

## Limitations

- The study identifies significant challenges in the generalization of Vietnamese LLMs to customer support contexts, particularly their inability to handle informal linguistic structures (teencode, abbreviations) present in real-world interactions.
- The focus on zero-shot evaluation, while valuable for intrinsic capability assessment, limits understanding of how RAG or retrieval-augmented approaches might mitigate these issues.
- The paper does not address whether limitations stem from architectural constraints or insufficient fine-tuning on domain-specific data.

## Confidence

- **High confidence**: The observation that models struggle with informal linguistic features (teencode, abbreviations) is well-supported by the dataset composition and evaluation results. The correlation between verbosity and hallucination scores is empirically demonstrated through syntactic analysis.
- **Medium confidence**: The claim that structural inflation (high dependency length) directly causes semantic drift requires further validation. While the paper shows correlation, causation is not definitively established, and the mechanism by which verbosity leads to hallucination could have alternative explanations.
- **Medium confidence**: The assertion that existing benchmarks fail to reflect real-world customer interactions is reasonable but could be strengthened with direct comparisons to model performance on other Vietnamese QA datasets like ViMM-Retrieval or VLegal-Bench.

## Next Checks

1. **Structural Rigidity Validation**: Conduct human preference studies comparing "rigid" high-dependency responses against concise ones to verify whether users actually prefer the simpler structures that the paper suggests are more natural.

2. **Length-Hallucination Causality Test**: Design an ablation study varying output length constraints (short vs. long) while measuring hallucination scores to determine if brevity truly reduces hallucination or if the correlation observed is coincidental.

3. **Informal Language Stress Test**: Fine-tune a subset of models on augmented data containing teencode and abbreviations, then re-evaluate on CSConDa to determine whether performance degradation is due to lack of exposure to informal linguistic patterns rather than fundamental architectural limitations.