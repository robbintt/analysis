---
ver: rpa2
title: Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence
arxiv_id: '2601.21320'
source_url: https://arxiv.org/abs/2601.21320
tags:
- samples
- overconfidence
- noise
- inputs
- cifar-10
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of out-of-distribution (OOD) overconfidence
  in deep neural networks, where models produce overly confident predictions on unseen
  inputs. The core idea is to exploit geometric singularities in semi-discrete optimal
  transport to identify semantically ambiguous regions.
---

# Optimal Transport-Induced Samples against Out-of-Distribution Overconfidence

## Quick Facts
- arXiv ID: 2601.21320
- Source URL: https://arxiv.org/abs/2601.21320
- Authors: Keke Tang; Ziyong Du; Xiaofei Wang; Weilong Peng; Peican Zhu; Zhihong Tian
- Reference count: 16
- Key outcome: Method exploits OT singularities to generate OOD samples that suppress overconfidence without auxiliary datasets

## Executive Summary
This paper addresses out-of-distribution (OOD) overconfidence in deep neural networks by leveraging geometric singularities in semi-discrete optimal transport. The method identifies semantically ambiguous regions where classifiers are prone to unwarranted high-confidence predictions and generates "optimal transport-induced OOD samples" (OTIS) near these boundaries. During training, a confidence suppression loss is applied to OTIS to guide the model toward more calibrated predictions in structurally uncertain regions. Experiments show significant improvements in OOD detection metrics while maintaining strong in-distribution accuracy.

## Method Summary
The approach uses an autoencoder to obtain latent embeddings of in-distribution data, then solves a semi-discrete optimal transport problem mapping a continuous base distribution to these discrete latent points. Brenier's theorem guarantees a gradient-based transport map that partitions space into Laguerre cells, with boundaries (singularities) marking semantic transitions. The top 10% of boundaries with largest angular deviations are selected, and OTIS samples are generated via inverse-distance weighted interpolation between adjacent cell centroids. A classifier is trained with a 50/50 mix of in-distribution data (cross-entropy loss) and OTIS samples (suppression loss encouraging uniform softmax distribution).

## Key Results
- Significant reductions in OOD MMC across multiple datasets (e.g., 13.18% on SVHN vs. 84.22% baseline)
- Maintains competitive or improved in-distribution accuracy while improving OOD detection
- Outperforms state-of-the-art methods like EOOD and TIE without requiring auxiliary datasets
- Consistent improvements across diverse OOD settings including adversarial samples and natural images

## Why This Works (Mechanism)

### Mechanism 1
Semi-discrete optimal transport creates geometrically meaningful boundaries marking semantic transitions. Brenier's theorem guarantees a gradient-based transport map partitioning space into Laguerre cells, with singularities at cell boundaries where transport direction changes abruptly. These correspond to structurally unstable classification zones where overconfidence is likely. Assumes latent space preserves semantic structure such that transport discontinuities align with classifier decision boundaries.

### Mechanism 2
Boundaries with larger angular deviations between adjacent cell centroids capture regions where models produce overconfident predictions. The singularity score quantifies directional change across boundaries, with top 10% ranked boundaries capturing structurally unstable zones where semantic assignments are most ambiguous. Assumes large angular deviations correlate with semantic ambiguity and overconfidence risk.

### Mechanism 3
Training on OTIS samples with uniform-label suppression loss reduces overconfidence on ambiguous inputs without degrading in-distribution accuracy. OTIS samples are generated via inverse-distance weighted interpolation between centroids of cells adjacent to singular boundaries. The suppression loss encourages uniform softmax distribution across classes. Assumes OTIS samples adequately represent the distribution of ambiguous OOD inputs the model may encounter at test time.

## Foundational Learning

- **Concept: Semi-discrete Optimal Transport**
  - Why needed here: Core mathematical framework defining how continuous distributions map to discrete targets, enabling boundary identification
  - Quick check question: Explain why Brenier's theorem guarantees a gradient-based transport map for absolutely continuous source measures

- **Concept: Power Diagrams / Laguerre Cells**
  - Why needed here: Understanding the geometric partition structure and how cell boundaries form
  - Quick check question: How do Laguerre cells generalize Voronoi diagrams via the offset parameters h_i?

- **Concept: Confidence Calibration vs. Suppression**
  - Why needed here: Distinguishing between merely lowering confidence and achieving well-calibrated uncertainty estimates
  - Quick check question: Why does reducing OOD MMC not automatically imply improved calibration on ID data?

## Architecture Onboarding

- **Component map:**
  Autoencoder -> OT Solver -> Boundary Selector -> OTIS Generator -> Classifier

- **Critical path:**
  1. Train autoencoder on ID data → obtain latent embeddings
  2. Solve semi-discrete OT → estimate cell volumes, compute offsets h
  3. Identify singular boundaries (top 10% angular score)
  4. Generate OTIS via smoothed interpolation near boundaries
  5. Train classifier with mixed batch: CE on ID, suppression on OTIS

- **Design tradeoffs:**
  - Base distribution: Uniform outperforms Gaussian slightly
  - AE depth: 5-layer offers best OOD/ID trade-off
  - Boundary ratio: 10% optimal; lower degrades, higher adds noise
  - Batch composition: 50/50 ID-OTIS used consistently

- **Failure signatures:**
  - High OOD MMC + low ID MMC: OTIS not targeting correct boundaries → check angular score distribution
  - Degraded ID accuracy: Suppression loss dominating → reduce OTIS ratio or loss weight
  - OTIS resembles noise: Poor AE reconstruction → increase latent dim or AE capacity
  - Uniform boundary scores: Latent space lacks semantic structure → improve encoder

- **First 3 experiments:**
  1. Boundary selection ablation: Compare top 5%, 10%, 20%, random to validate singularity ranking importance
  2. OTIS vs. naive interpolation: Compare against L-Inter/I-Inter baselines to isolate OT geometry contribution
  3. Base distribution sensitivity: Test Gaussian vs. uniform with fixed AE to confirm robustness claim

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The theoretical foundation linking OT singularities to semantic ambiguity relies on strong assumptions about latent space geometry
- Optimal hyperparameters (10% boundary ratio, 50/50 batch mixing) appear dataset-dependent without systematic sensitivity analysis
- Computational overhead of solving semi-discrete OT for each dataset could limit scalability
- While suppression reduces overconfidence, it's unclear whether this translates to well-calibrated probabilities

## Confidence
- **High confidence**: Empirical results showing consistent OOD MMC improvements across multiple datasets and architectures
- **Medium confidence**: Theoretical justification connecting OT singularities to overconfidence-prone regions requires further validation
- **Low confidence**: Generalization of OTIS samples to truly represent OOD distributions in the wild remains unproven

## Next Checks
1. Evaluate OTIS performance when using latent spaces from different autoencoders (VAE, VAE with different architectures, or contrastive embeddings) to test robustness of OT singularity hypothesis
2. Test on truly novel OOD datasets not seen during development to assess whether OTIS captures general patterns of ambiguity
3. Measure proper calibration metrics (ECE, Brier score) on both ID and OOD data to verify confidence suppression leads to well-calibrated uncertainty rather than indiscriminate confidence reduction