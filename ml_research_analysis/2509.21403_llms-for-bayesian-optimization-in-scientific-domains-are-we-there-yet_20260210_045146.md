---
ver: rpa2
title: 'LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?'
arxiv_id: '2509.21403'
source_url: https://arxiv.org/abs/2509.21403
tags:
- genes
- llmnn
- gene
- hits
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper evaluates whether instruction-tuned large language models
  (LLMs) can perform effective in-context experimental design for scientific tasks.
  Using BioDiscoveryAgent with both open-source and closed-source LLMs, it finds that
  LLMs show no sensitivity to experimental feedback: replacing true outcomes with
  randomly permuted labels has no impact on performance.'
---

# LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?

## Quick Facts
- **arXiv ID:** 2509.21403
- **Source URL:** https://arxiv.org/abs/2509.21403
- **Reference count:** 28
- **Primary result:** LLMs fail to perform in-context experimental design for scientific tasks; feedback-insensitive performance persists even with label permutations.

## Executive Summary
This paper investigates whether instruction-tuned large language models can perform effective in-context experimental design for scientific tasks. Using BioDiscoveryAgent with both open-source and closed-source LLMs, the authors find that LLMs show no sensitivity to experimental feedback: replacing true outcomes with randomly permuted labels has no impact on performance. Across benchmarks, classical methods such as linear bandits and Gaussian process optimization consistently outperform LLM agents. The paper introduces LLM-guided Nearest Neighbour (LLMNN), a hybrid method that combines LLM prior knowledge with nearest-neighbor sampling in embedding space. LLMNN achieves competitive or superior performance across domains without requiring significant in-context adaptation. These results suggest that current LLMs do not perform in-context experimental design in practice and highlight the need for hybrid frameworks that decouple prior-based reasoning from batch acquisition with updated posteriors.

## Method Summary
The study evaluates LLMs for in-context experimental design using the BioDiscoveryAgent framework. Experiments test LLMs' sensitivity to feedback by replacing true outcomes with randomly permuted labels. Classical optimization baselines (linear bandits, GP optimization) are compared against LLM agents. A hybrid approach, LLM-guided Nearest Neighbour (LLMNN), is introduced, combining LLM prior knowledge with nearest-neighbor sampling in embedding space. Performance is assessed across synthetic and real-world scientific benchmarks.

## Key Results
- LLMs show no sensitivity to experimental feedback; performance unchanged when true labels are replaced with random permutations.
- Classical methods (linear bandits, GP optimization) consistently outperform LLM agents across all benchmarks.
- LLM-guided Nearest Neighbour (LLMNN) achieves competitive or superior performance without requiring in-context adaptation.

## Why This Works (Mechanism)
LLMs fail to perform in-context experimental design because they lack true feedback sensitivity. Unlike classical optimization methods that iteratively update beliefs based on observed outcomes, LLMs rely on static prior knowledge encoded during pre-training. When outcomes are permuted, LLMs continue to generate responses based on their original embeddings, demonstrating that they are not updating their internal models in response to new data. This static behavior contrasts with Bayesian optimization, which explicitly conditions future decisions on posterior updates from past experiments.

## Foundational Learning

### Bayesian Optimization
**Why needed:** Provides the theoretical framework for sequential experimental design, updating beliefs based on observed outcomes.
**Quick check:** Can the method iteratively improve performance as more data is collected?

### In-Context Learning
**Why needed:** Core mechanism by which LLMs adapt to new tasks without parameter updates.
**Quick check:** Does the model's behavior change meaningfully when given different in-context examples?

### Gaussian Processes
**Why needed:** Standard probabilistic model for Bayesian optimization, providing uncertainty estimates for decision-making.
**Quick check:** Does the method provide calibrated uncertainty estimates for unexplored regions?

## Architecture Onboarding

### Component Map
BioDiscoveryAgent -> LLM (encoder/decoder) -> Prompt Template -> Experimental Design -> Outcome Evaluation -> Feedback Loop

### Critical Path
Prompt construction → LLM inference → Experimental design selection → Outcome observation → Performance evaluation

### Design Tradeoffs
The study trades pure LLM-based experimental design for hybrid approaches that combine LLM priors with classical optimization methods, sacrificing some flexibility for more reliable performance.

### Failure Signatures
- No performance difference between true and permuted outcomes
- Consistent underperformance compared to classical methods
- Failure to adapt behavior based on observed results

### First Experiments
1. Test sensitivity to gradual feedback degradation (Gaussian noise, label corruption)
2. Evaluate performance on high-dimensional, real-world scientific datasets
3. Conduct ablation studies on prompt engineering and context length

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focused on relatively simple, synthetic benchmarks
- Performance on complex real-world scientific domains with multimodal, noisy, or high-dimensional data remains untested
- The BioDiscoveryAgent framework and prompt engineering choices may constrain LLM performance in ways not explored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLMs do not adapt to feedback in experimental design | High |
| Classical methods outperform LLM agents | Medium |
| LLMNN hybrid method is effective | Medium |

## Next Checks
1. Test LLM-guided experimental design on high-dimensional, real-world scientific datasets with multimodal inputs and noisy measurements.
2. Conduct ablation studies on prompt engineering, context length, and agent architecture to isolate whether observed LLM limitations are intrinsic or due to implementation choices.
3. Evaluate gradual degradation of feedback quality (e.g., Gaussian noise, label corruption) to assess whether LLMs exhibit any sensitivity to feedback signal strength.