---
ver: rpa2
title: 'Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies
  and Techniques'
arxiv_id: '2506.04788'
source_url: https://arxiv.org/abs/2506.04788
tags:
- language
- wang
- zhang
- fusion
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically analyzes 125 MLLMs by proposing a novel
  taxonomy that classifies modality integration mechanisms into four types: Projection,
  Abstraction, Semantic Embedding, and Cross-attention. It further distinguishes three
  fusion levels (Early, Intermediate, Hybrid) and categorizes representation learning
  as Joint, Coordinated, or Hybrid.'
---

# Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques

## Quick Facts
- **arXiv ID:** 2506.04788
- **Source URL:** https://arxiv.org/abs/2506.04788
- **Reference count:** 40
- **Primary result:** Systematically analyzes 125 MLLMs using a novel taxonomy classifying integration mechanisms into Projection, Abstraction, Semantic Embedding, and Cross-attention, plus fusion levels and representation learning approaches.

## Executive Summary
This survey provides a comprehensive framework for understanding multimodal large language model (MLLM) architectures by proposing a systematic taxonomy that classifies integration mechanisms into four distinct types: Projection, Abstraction, Semantic Embedding, and Cross-attention. The analysis distinguishes between three fusion levels (Early, Intermediate, Hybrid) and three representation learning approaches (Joint, Coordinated, Hybrid). By examining 125 MLLM papers, the study reveals how architectural components serve different functional purposes across models, offering clarity for researchers designing multimodal systems. The framework addresses a significant gap in existing literature by providing a unified view of architectural strategies, representation approaches, and training paradigms across diverse MLLM implementations.

## Method Summary
The survey employs qualitative architectural analysis to map 125 MLLM papers into a structured taxonomy based on three dimensions: fusion mechanisms, fusion levels, and representation learning approaches. The methodology involves extracting architectural diagrams and method sections from papers, then applying decision logic to classify components based on their functional roles. For mechanism classification, the analysis considers whether components reduce token counts (Abstraction), map dimensions without reduction (Projection), incorporate text instructions (Semantic Embedding), or enable dynamic fusion inside LLM layers (Cross-attention). The study further distinguishes fusion levels by entry points into the LLM architecture and categorizes representation learning by whether modality-specific encoders are trained jointly or separately. The approach relies on inferring "researcher intention" when explicit specifications are unclear.

## Key Results
- Proposes a novel taxonomy classifying MLLM integration mechanisms into four types: Projection, Abstraction, Semantic Embedding, and Cross-attention
- Distinguishes three fusion levels (Early, Intermediate, Hybrid) and three representation learning approaches (Joint, Coordinated, Hybrid)
- Analyzes 125 MLLMs to reveal how architectural components serve different functional purposes across models
- Identifies emerging patterns and design principles showing how projection layers map features into LLM embedding space, abstraction layers control token counts, and cross-attention enables dynamic modality fusion

## Why This Works (Mechanism)
The taxonomy works by systematically categorizing MLLM components based on their functional roles rather than their architectural form. This approach resolves ambiguity in component classification by focusing on "researcher intention" - whether a component reduces token count, maps dimensions, incorporates instructions, or enables internal fusion. The framework captures the fundamental design decisions in MLLM architecture: how to transform diverse modality features into a form compatible with LLMs, where to inject these features in the LLM architecture, and how to train the integrated system. By providing clear decision criteria for each mechanism type, the taxonomy enables consistent classification across diverse MLLM implementations and reveals meaningful patterns in architectural evolution.

## Foundational Learning
- **Abstraction vs Projection:** Abstraction reduces token count while Projection maintains it; needed to distinguish feature reduction from dimensional mapping; quick check: does the component change the number of tokens?
- **Fusion Levels:** Early fusion injects at input, Intermediate at internal layers, Hybrid combines both; needed to understand where modality information enters the LLM; quick check: identify the entry point of external features into the LLM architecture
- **Cross-attention:** Enables dynamic fusion by allowing modality features to interact with LLM layers; needed for understanding how modalities influence LLM processing; quick check: does the component enable interaction between modality features and LLM hidden states?
- **Semantic Embedding:** Incorporates text instructions into modality processing; needed for instruction-tuned MLLMs; quick check: does the component use text queries or instructions?
- **Joint vs Coordinated Training:** Joint trains encoders together, Coordinated trains separately; needed to understand training paradigms; quick check: are modality-specific encoders trained with the LLM simultaneously or separately?
- **Q-Former Ambiguity:** Can serve as Abstractor or Semantic Embedder depending on text query usage; needed to handle edge cases; quick check: does the Q-Former receive text instruction tokens as input?

## Architecture Onboarding

**Component Map:** Modality Encoders → Connector (Projection/Abstraction/Semantic) → LLM Input/Layers → Output Head

**Critical Path:** The path from raw modality inputs through connectors to LLM layers determines model performance. Cross-attention mechanisms create additional critical paths within LLM layers for dynamic fusion.

**Design Tradeoffs:** Early fusion provides strong initial guidance but may overwhelm LLM with irrelevant features. Intermediate fusion allows LLM to process text first but requires effective internal fusion mechanisms. Projection preserves information but may not reduce computational complexity. Abstraction reduces tokens but risks information loss.

**Failure Signatures:** Misclassification of Q-Former usage leads to incorrect mechanism assignment. Confusing fusion level with mechanism type results in architectural misunderstanding. Overlooking text instruction integration misses semantic embedding cases. Ambiguous component roles create inconsistent taxonomy application.

**First Experiments:**
1. Classify 10 random models from the corpus to test taxonomy consistency
2. Trace Q-Former usage across 20 models to quantify edge case frequency
3. Map fusion level distributions across publication years to verify temporal trends

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on inferring "researcher intention" introduces subjectivity into component classification
- Analysis depends on published descriptions that may miss implementation details or architectural nuances
- Rapidly evolving MLLM research may shift classification boundaries as new architectural patterns emerge
- Subjective interpretation required for edge cases where components serve multiple functions may lead to inconsistencies

## Confidence
- **High Confidence:** Four mechanism types are clearly defined and distinguishable based on observable architectural properties
- **High Confidence:** Three fusion levels have well-defined entry points into LLM architecture
- **Medium Confidence:** Classification of representation learning approaches may involve interpretation of training objectives
- **Medium Confidence:** Mapping specific components to mechanisms requires careful analysis of architectural diagrams
- **Low Confidence:** "Researcher intention" criterion for edge cases where components serve multiple functions

## Next Checks
1. Reproduce classification consistency by independently classifying 10 randomly selected models from the corpus across three reviewers to measure inter-rater reliability
2. Plot distribution of mechanism types and fusion levels across publication years (2021-2025) to verify claimed trends in architectural evolution
3. Trace specific architectural components (e.g., Q-Former) across at least 20 different models to document cases where the same component serves different functional roles and quantify edge case frequency