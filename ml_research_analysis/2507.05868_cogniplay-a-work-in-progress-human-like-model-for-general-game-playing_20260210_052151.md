---
ver: rpa2
title: 'CogniPlay: a work-in-progress Human-like model for General Game Playing'
arxiv_id: '2507.05868'
source_url: https://arxiv.org/abs/2507.05868
tags:
- human
- games
- search
- system
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CogniPlay, a human-like model for General
  Game Playing (GGP) based on cognitive psychology principles, specifically the Double-Process
  Theory of Cognition (DPTC). The model combines intuitive pattern recognition (System
  1) using Spatial Action-State Features for action-set partitioning with selective,
  memory-bounded Monte-Carlo Tree Search (MCTS) for analytical decisions (System 2).
---

# CogniPlay: a work-in-progress Human-like model for General Game Playing

## Quick Facts
- arXiv ID: 2507.05868
- Source URL: https://arxiv.org/abs/2507.05868
- Authors: Aloïs Rautureau; Éric Piette
- Reference count: 40
- The paper introduces a dual-process cognitive architecture combining intuitive pattern recognition with selective Monte Carlo Tree Search for human-like game playing.

## Executive Summary
CogniPlay is a novel approach to General Game Playing (GGP) that aims to replicate human decision-making through a dual-process cognitive architecture. The system combines System 1's intuitive pattern recognition using Spatial Action-State Features for rapid action-set partitioning with System 2's selective, memory-bounded Monte Carlo Tree Search (MCTS) for analytical decisions. The model is trained through expert iteration without requiring human-generated data, learning human-like biases algorithmically from its own uncertain decisions. The paper identifies key challenges in evaluating human-likeness, including the need for both quantitative metrics (like move matching) and qualitative assessments (such as Turing tests).

## Method Summary
CogniPlay implements a two-system cognitive architecture based on Kahneman's Double-Process Theory. System 1 uses Spatial State-Action Features to rapidly partition legal actions into intuitively good and bad subsets, providing a certainty assessment. When uncertain (multiple good actions), System 2 invokes memory-bounded MCTS on the pruned action set; when certain, System 1 acts directly. The system is trained via expert iteration where System 1 learns from System 2's MCTS-derived policies only on states where System 1 exhibits uncertainty. This approach aims to produce human-like biases without human training data. The authors plan to implement and evaluate the model first in Renju, then across Ludii's 1,400+ games to assess generalizability.

## Key Results
- The paper presents a theoretical framework for human-like GGP agents based on dual-process cognition
- Identifies key challenges in measuring human-likeness including move-matching limitations and need for Turing-style tests
- Proposes action-set partitioning and memory-bounded MCTS as core mechanisms for selective search
- Claims to achieve human-like behavior without human-generated training data through expert iteration

## Why This Works (Mechanism)

### Mechanism 1: Dual-Process Cognitive Architecture
- Claim: Separating intuitive (System 1) and analytical (System 2) decision processes produces more human-like game-playing behavior than monolithic approaches.
- Mechanism: System 1 uses Spatial State-Action Features for rapid action-set partitioning. When the policy is "doubtful" (multiple actions deemed good), System 2 (memory-bounded MCTS) is invoked. When "certain" (single good action), System 2 is bypassed entirely.
- Core assumption: Human cognition operates via two interacting systems as described in Kahneman's Double-Process Theory, and this architecture transfers to game-playing agents.
- Evidence anchors:
  - [abstract]: "combining intuitive decisions through pattern-based chunking with selective search for analytical decisions"
  - [section II]: "System 1 (intuitive brain) is fast, biased, based on intuition and the recognition of memorized patterns. System 2 (analytical brain) is slow, based on logical inference."
  - [corpus]: He et al. (2024) demonstrated similar dual-process architecture improving LLM dialogue agents over baseline planners

### Mechanism 2: Action-Set Partitioning for Selective Search
- Claim: Partitioning legal actions into "intuitively good" and "intuitively bad" subsets before search mimics human selective tree pruning.
- Mechanism: System 1's heuristic classifies all legal actions into two subsets. Only the "good" subset is passed to MCTS for expansion. This reduces search space while preserving plausible moves.
- Core assumption: Human players prune large portions of the game tree intuitively before conscious analysis, rather than searching exhaustively.
- Evidence anchors:
  - [section III-A]: "System 1's heuristic is used to partition the set of legal actions into two subsets: intuitively good actions and intuitively bad actions. Only the set of intuitively good actions is searched further"
  - [section II]: "These searches are also highly selective, as human players prune a large chunk of the game tree intuitively instead of exploring it exhaustively"
  - [corpus]: Corpus lacks direct validation of action-set partitioning in GGP; closest is Moriarty & Miikkulainen (1994) on focused Minimax in Othello

### Mechanism 3: Expert Iteration with Doubtful-State Filtering
- Claim: Training System 1 only on states where it exhibits uncertainty allows human-like biases to emerge without human-generated training data.
- Mechanism: When System 1 produces a doubtful policy, System 2's MCTS-derived policy serves as a training target. This creates an internal learning loop. The paper explicitly states this approach aims to produce biases algorithmically rather than through imitation.
- Core assumption: Cognitive biases emerge naturally from learning which patterns warrant deeper analysis vs. immediate response.
- Evidence anchors:
  - [abstract]: "trained through expert iteration without requiring human-generated data, aiming to produce human-like behavior"
  - [section III-A]: "System 1 is trained using expert iteration only on states where it exhibits a doubtful policy... We expect this method to allow the emergence of biases in the model."
  - [corpus]: No direct corpus evidence validates doubtful-state filtering specifically; expert iteration generally is well-established (Anthony et al. 2017)

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed here: Core algorithm for System 2; memory-bounded variant requires understanding standard MCTS first (selection, expansion, simulation, backpropagation).
  - Quick check question: Can you explain how the UCB1 formula balances exploration vs. exploitation in node selection?

- **Concept: Spatial State-Action Features**
  - Why needed here: Technical foundation for System 1's pattern recognition; enables generalization across games without game-specific engineering.
  - Quick check question: How would you extract spatial features from a game described only by formal rules (e.g., in Ludii's ludemic language)?

- **Concept: Expert Iteration**
  - Why needed here: Training methodology connecting System 2 output to System 1 learning; distinct from pure imitation learning or pure RL.
  - Quick check question: What is the difference between expert iteration and behavioral cloning from self-play data?

## Architecture Onboarding

- **Component map:**
  - Game state -> Spatial State-Action Feature extraction -> System 1 policy network -> action partitioning + certainty assessment -> If doubtful -> memory-bounded MCTS on pruned action set -> refined policy and value -> If certain -> return System 1 action directly

- **Critical path:**
  1. Game state → Spatial State-Action Feature extraction
  2. Features → System 1 policy network → action partitioning + certainty assessment
  3. If doubtful → invoke MCTS on pruned action set; if certain → return System 1 action directly
  4. After action selection, if state was doubtful → store (state, MCTS policy) as training sample
  5. Periodically retrain System 1 on accumulated samples

- **Design tradeoffs:**
  - Memory bound parameter: Lower = more human-like limitations but weaker play
  - Partitioning aggressiveness: More aggressive pruning = faster but higher risk of pruning good moves
  - Training sample buffer size: Larger = more stable learning but slower adaptation

- **Failure signatures:**
  - System 1 always certain: Network overconfident, no MCTS invocation, no training signal accumulation
  - System 1 always doubtful: No pruning benefit, reverts to standard MCTS behavior
  - High move-matching but low Turing ratings: Model captures statistical patterns without human-like reasoning process
  - Large performance variance across games: Spatial features not generalizing; may need game-specific tuning

- **First 3 experiments:**
  1. **Renju baseline**: Implement full CogniPlay pipeline for single game; measure (a) playing strength vs. standard MCTS, (b) move-matching rate vs. human game database; compute self-consistency ceiling as reference
  2. **Memory bound sweep**: Test 3-5 memory limit values on Renju; correlate with estimated human search depth (paper cites 4.8-5.3 plies for ~2000 Elo chess players) to identify human-like operating point
  3. **Cross-game transfer in Ludii**: Deploy identical model (no game-specific retraining) across 5+ diverse games; measure both move-matching accuracy and qualitative expert ratings via Turing-style evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generalizable opponent modeling be implemented for GGP without requiring domain-specific knowledge?
- Basis in paper: [explicit] "To our knowledge, no effort has been made to generalize opponent modeling. We identify it as a promising research direction for human-like agents, but its practical implementation in GGP systems requires substantial additional research."
- Why unresolved: Existing opponent modeling approaches (Jansen, Donkers et al.) rely on extensive domain knowledge and predefined opponent models unavailable to GGP agents.
- What evidence would resolve it: A working opponent-modeling component within a GGP system that adapts across multiple games without game-specific configuration.

### Open Question 2
- Question: How should human-likeness be measured accurately, and what are the limitations of current quantitative and qualitative metrics?
- Basis in paper: [explicit] "Notably, existing measures of human-likeness are often imprecise and prone to false positives. A thorough investigation is needed to refine these metrics, enhance their accuracy, and explore alternative evaluation methods."
- Why unresolved: Move-matching lacks a performance ceiling; Turing-style tests depend on observer expertise and game familiarity.
- What evidence would resolve it: Comparative studies across games with established human baselines showing consistent correlation between quantitative and qualitative measures.

### Open Question 3
- Question: Does memory-bounded MCTS produce more human-like playing styles, and how does it affect playing strength?
- Basis in paper: [explicit] "This method slightly weakens the playing strength of MCTS agents, although no measurements have yet been made on its impact to their playing style."
- Why unresolved: CogniPlay is a work-in-progress; the proposed memory-bounded MCTS has not been empirically evaluated.
- What evidence would resolve it: Ablation studies comparing standard MCTS vs. memory-bounded MCTS on both Elo ratings and human-likeness metrics across multiple games.

### Open Question 4
- Question: Can a single human-like GGP model trained without human-generated data generalize across diverse games?
- Basis in paper: [explicit] "Our future work includes an implementation of CogniPlay for the game of Renju, to first evaluate its performance within a single game. This will be followed by an implementation within the Ludii game-playing system to assess its generalizability."
- Why unresolved: The model remains unimplemented; generalizability claims are theoretical until tested across Ludii's 1,400+ games.
- What evidence would resolve it: Evaluation results showing CogniPlay maintaining human-like behavior across structurally diverse games in Ludii without retraining or human data.

## Limitations
- The partitioning mechanism lacks precise thresholds for what constitutes "intuitively good" vs "bad" actions and how "certainty" is quantified
- No empirical validation data is provided, making claims about human-likeness currently theoretical
- The spatial state-action feature extraction method is referenced but not detailed sufficiently for direct implementation

## Confidence
- **High Confidence**: The dual-process cognitive architecture concept and its theoretical foundation in DPTC psychology
- **Medium Confidence**: The general methodology of action-set partitioning and expert iteration, as these are established techniques in AI
- **Low Confidence**: Specific claims about achieving human-like behavior, as these require empirical validation that is not yet completed

## Next Checks
1. Implement the action-set partitioning threshold tuning on Renju and measure the pruning efficiency vs. search quality tradeoff curve
2. Compare move-matching accuracy against self-consistency baseline to establish whether the model captures human-like patterns beyond simple statistical regularities
3. Conduct the proposed Turing-style evaluation with human experts across multiple game types to validate qualitative human-likeness claims