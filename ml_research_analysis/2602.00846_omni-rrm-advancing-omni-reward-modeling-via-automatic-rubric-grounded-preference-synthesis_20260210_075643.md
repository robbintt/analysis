---
ver: rpa2
title: 'Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference
  Synthesis'
arxiv_id: '2602.00846'
source_url: https://arxiv.org/abs/2602.00846
tags:
- reward
- preference
- arxiv
- score
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of effective multimodal reward models
  (RMs) by introducing Omni-RRM, the first open-source rubric-grounded reward model
  that generates structured, multi-dimension preference judgments with justifications
  across text, image, video, and audio. The core innovation is Omni-Preference, a
  fully automated dataset created by contrasting outputs from strong and weak models
  and using teacher models to reconcile and filter preferences while providing rubric-grounded
  rationales.
---

# Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis

## Quick Facts
- arXiv ID: 2602.00846
- Source URL: https://arxiv.org/abs/2602.00846
- Reference count: 40
- Key outcome: First open-source rubric-grounded multimodal reward model achieving state-of-the-art accuracy on video (80.2% on ShareGPT-V) and audio (66.8% on Audio-HH-RLHF) benchmarks

## Executive Summary
Omni-RRM introduces the first open-source rubric-grounded reward model capable of generating structured, multi-dimension preference judgments with justifications across text, image, video, and audio. The core innovation is Omni-Preference, a fully automated dataset created by contrasting outputs from strong and weak models and using teacher models to reconcile and filter preferences while providing rubric-grounded rationales. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded output structure, followed by reinforcement learning (GRPO) to improve discrimination on low-margin pairs. Comprehensive evaluations show Omni-RRM achieves state-of-the-art accuracy on video and audio benchmarks, substantially outperforms existing open-source RMs on image tasks, and improves downstream performance via Best-of-N selection while transferring to text-only preference tasks.

## Method Summary
Omni-RRM is a rubric-grounded reward model that outputs structured JSON judgments across five dimensions (fluency, relevance, accuracy, reasoning, safety) with a final preference verdict. The model is trained on Omni-Preference, an automatically synthesized dataset created by contrasting outputs from strong and weak models, with teacher reconciliation providing rubric-grounded labels. Training proceeds in two stages: supervised fine-tuning (SFT) to learn the structured output schema using LoRA adapters, followed by GRPO reinforcement learning to sharpen discrimination on low-margin preference pairs. The backbone is Qwen2.5-Omni with LoRA (rank 8, α=32) on all linear layers, and evaluation uses a composite reward function combining preference accuracy, rubric quality, and format validity.

## Key Results
- Achieves 80.2% accuracy on ShareGPT-V video benchmark, setting new state-of-the-art
- Reaches 66.8% accuracy on Audio-HH-RLHF audio benchmark, surpassing previous best by 2.5 points
- Outperforms existing open-source RMs on image tasks with 17.7% absolute gain in overall accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rubric-grounded structured outputs improve both interpretability and preference discrimination accuracy
- Mechanism: A fixed five-criterion rubric forces the model to produce dimension-wise justifications before the final verdict, creating explicit reasoning traces that constrain the decision space and expose failure modes
- Core assumption: Explicit criterion-level decomposition transfers across modalities and provides beneficial inductive bias for preference learning
- Evidence anchors:
  - [abstract] "produces structured, multi-dimension preference judgments with dimension-wise justifications"
  - [section 5.3] Omni-RRM-7B outperforms Omni-RM-7B (no-rationale) by 7.4 absolute points on Overall accuracy (71.8% vs 64.4%)
  - [corpus] Related work (OpenRubrics, RaR) supports rubric-grounded supervision, but corpus lacks direct multimodal rubric comparisons

### Mechanism 2
- Claim: Two-stage SFT→GRPO training sharpens discrimination on low-margin preference pairs
- Mechanism: SFT establishes stable schema compliance for rubric-structured outputs; GRPO then optimizes a composite reward using group-relative advantages, focusing updates on hard pairs where simple imitation fails
- Core assumption: Schema-stable initialization allows GRPO to improve discrimination rather than repair output structure
- Evidence anchors:
  - [abstract] "trained in two stages: supervised fine-tuning to learn the rubric-structured output interface, followed by GRPO to sharpen discrimination on low-margin pairs"
  - [appendix A.7] On hard pairs (teacher margin ≤2), Omni-RRM improves +12.0 pp vs +6.6 pp on easy pairs
  - [corpus] GRPO (DeepSeekMath) is validated for reasoning tasks, but multimodal reward application remains underexplored

### Mechanism 3
- Claim: Unified omni-modal training enables cross-modal transfer, particularly benefiting lower-resource modalities
- Mechanism: Shared rubric criteria transfer across modalities; joint training provides transferable rubric-level signals that benefit modalities with less training data
- Core assumption: Preference signals at the rubric level are modality-agnostic enough for positive transfer
- Evidence anchors:
  - [section 5.5] Full-modality training (I+V+A) yields strongest performance; dropping any modality degrades others
  - [section 5.5] Audio-only training achieves 63.8% on Audio-HH vs 65.1% with omni training
  - [corpus] Omni-Reward paper also reports generalist reward modeling, but direct transfer evidence is limited

## Foundational Learning

- Concept: Reward Modeling as Structured Generation
  - Why needed here: Understanding that Omni-RRM reformulates RM from scalar regression to autoregressive JSON generation with explicit reasoning
  - Quick check question: Can you explain why a fixed rubric schema constrains both training and inference differently than scalar reward prediction?

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: The second training stage uses GRPO with composite rewards; understanding group-normalized advantages and PPO-style clipping is essential
  - Quick check question: How does computing advantages relative to a sampled group differ from standard PPO advantage estimation?

- Concept: Capability-Gap Data Synthesis
  - Why needed here: Omni-Preference uses strong-weak model contrast to generate candidate pairs, with teacher reconciliation for final labels
  - Quick check question: Why does the pipeline require independent teacher annotation rather than assuming the "strong" generator is always preferred?

## Architecture Onboarding

- Component map:
  Backbone (Qwen2.5-Omni) -> Input formatter -> Structured prompt -> Output schema (JSON) -> Composite reward function -> Training pipeline (SFT -> GRPO)

- Critical path:
  1. SFT teaches schema compliance and rubric-structured output generation
  2. GRPO refines discrimination using composite reward with format/preference/rubric components
  3. Inference: Model generates full JSON; extract "better" or "final_verdict" for preference signal

- Design tradeoffs:
  - **Unified vs. modality-specific RMs**: Unified enables cross-modal transfer but requires balancing modality coverage in training data
  - **Rubric complexity vs. supervision cost**: Fixed 5-criterion rubric provides structure but may miss modality-specific nuances
  - **Latency vs. interpretability**: Structured rationales increase output length (~265 tokens vs. ~120 for scalar RMs) but enable auditability

- Failure signatures:
  - Schema drift during GRPO: Malformed JSON outputs indicate insufficient format reward weight or unstable SFT initialization
  - Verdict-justification mismatch: Rationale argues for A but verdict selects B; signals weak R_rub component
  - Cross-modal collapse: Performance drops on one modality after training on another; indicates negative transfer

- First 3 experiments:
  1. **SFT-only baseline**: Train with only supervised fine-tuning (no GRPO) to isolate the contribution of RL refinement
  2. **Hard vs. easy pair evaluation**: Stratify test set by teacher margin (≤2 vs. >2) to verify GRPO improves hard-pair discrimination
  3. **Modality ablation**: Train with drop-one-modality data to quantify cross-modal transfer effects and identify dependency patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Teacher Quality and Bias: The synthetic dataset relies on two strong models with specific reconciliation thresholds, introducing uncertainty about label reliability and potential systematic bias
- Generalizability of Fixed Rubric: The five-criterion rubric's effectiveness across all task types and domains is not proven, potentially missing modality-specific nuances
- GRPO Hyperparameter Sensitivity: Critical GRPO parameters are unspecified, making it unclear whether reported gains are robust to hyperparameter tuning

## Confidence
- **High Confidence**: Two-stage training framework and structured output schema are well-specified and reproducible; superiority over open-source RMs is clearly demonstrated
- **Medium Confidence**: Rubric-grounded supervision's effectiveness for interpretability and discrimination is supported by ablation studies, but mechanism robustness is uncertain
- **Low Confidence**: Exact contribution of each training stage and long-term stability of GRPO policy are not fully characterized due to missing hyperparameter details

## Next Checks
1. **Rubric Schema Ablation**: Systematically vary the number and definition of rubric criteria to measure impact on interpretability and discrimination accuracy
2. **Teacher Agreement Analysis**: Conduct human evaluation on Omni-Preference subset to measure inter-teacher agreement and identify systematic biases
3. **GRPO Hyperparameter Sweep**: Perform optimization over missing GRPO parameters to determine whether performance is robust or highly sensitive to choices