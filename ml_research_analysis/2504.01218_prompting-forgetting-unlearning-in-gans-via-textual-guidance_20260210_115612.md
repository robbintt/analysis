---
ver: rpa2
title: 'Prompting Forgetting: Unlearning in GANs via Textual Guidance'
arxiv_id: '2504.01218'
source_url: https://arxiv.org/abs/2504.01218
tags:
- unlearning
- latent
- images
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Text-to-Unlearn, a novel method for unlearning
  specific concepts from pre-trained GANs using only text prompts, without requiring
  additional datasets or supervised fine-tuning. The approach leverages CLIP embeddings
  and latent space manipulations to guide the unlearning process, enabling tasks like
  feature removal, identity unlearning, and multi-attribute control.
---

# Prompting Forgetting: Unlearning in GANs via Textual Guidance

## Quick Facts
- arXiv ID: 2504.01218
- Source URL: https://arxiv.org/abs/2504.01218
- Authors: Piyush Nagasubramaniam; Neeraj Karamchandani; Chen Wu; Sencun Zhu
- Reference count: 40
- Key outcome: Text-to-Unlearn achieves up to 0.99 degree of unlearning (γ) scores, outperforming CLIP baseline on feature removal from pre-trained GANs using only text prompts, without additional datasets or supervised fine-tuning.

## Executive Summary
This paper introduces Text-to-Unlearn, a novel method for unlearning specific visual concepts from pre-trained GANs using only text prompts. The approach leverages CLIP embeddings to define semantic directions in latent space and employs a directional unlearning loss to steer the generator away from undesirable features while preserving overall image quality. The method is validated on the FFHQ dataset using StyleGAN2, demonstrating effective unlearning of attributes like "purple hair" and "glasses" while maintaining model utility. Quantitative and qualitative evaluations show superior performance compared to a CLIP-based baseline, with high disentanglement of features and minimal impact on unrelated attributes.

## Method Summary
Text-to-Unlearn operates by first precomputing a reference vector in CLIP space that represents the semantic shift of adding the unwanted feature. It then fine-tunes the generator to align its transformation with the opposite of this reference vector, effectively "subtracting" the feature. A Latent Mapper dynamically generates negative samples containing the unwanted feature, eliminating the need for external datasets. The total loss combines directional unlearning with LPIPS and ID regularization to preserve image fidelity and identity. The method is applied in two phases: precomputing the reference vector and performing the unlearning optimization.

## Key Results
- Achieves up to 0.99 degree of unlearning (γ) scores, outperforming CLIP baseline.
- Demonstrates effective feature removal (e.g., "purple hair," "glasses") while preserving unrelated attributes.
- Maintains model utility with minimal FID score degradation during unlearning.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Semantic Steering
- **Claim:** Textual descriptions can guide the removal of visual concepts from a GAN by operating in a joint image-text embedding space (CLIP), provided a reference direction can be mathematically defined.
- **Mechanism:** The method computes a reference unit vector $\vec{i}$ representing the semantic shift of "adding" the unwanted feature. It then fine-tunes the generator $G_t$ so that the transformation from the frozen generator $G_f$ (which produces the feature) to $G_t$ aligns with this reference direction in CLIP space.
- **Core assumption:** The vector calculated from a batch of images in Phase 1 sufficiently captures the semantic direction of the concept to be removed.
- **Evidence anchors:**
  - [Section 4.2]: "We capture the change of adding p... in the embedding space and later unlearn along this direction."
  - [Equation 3 & 5]: Defines $\vec{i}$ as the normalized difference between feature-present and feature-absent embeddings, and $L_{dir}$ as the cosine similarity between $\vec{i}$ and the generator shift $\vec{j}$.
- **Break condition:** If the computed reference vector $\vec{i}$ points toward the feature but the loss alignment forces the generator to shift *towards* that vector, the model would theoretically *amplify* the feature rather than remove it.

### Mechanism 2: Synthetic Negative Sampling
- **Claim:** Unlearning can proceed without external datasets by dynamically generating "negative" samples (images containing the unwanted feature) from the model's own latent space.
- **Mechanism:** A shallow "Latent Mapper" ($M_p$) is trained to shift random latent codes $w$ towards codes $\hat{w}$ that produce the unwanted feature. These $\hat{w}$ codes are fed to the frozen generator $G_f$ to create targets for the unlearning loss.
- **Core assumption:** The Latent Mapper is capable of successfully generating the unwanted feature in isolation.
- **Evidence anchors:**
  - [Section 4.1]: "Mp is a shallow neural network... used to edit any image according to a text description."
  - [Section 4.2]: "Gf will constantly generate negative samples, i.e., images containing undesirable attributes described by p."
- **Break condition:** If the concept to be unlearned is not already present or readily accessible in the GAN's latent space, the mapper cannot define a direction, and the method fails.

### Mechanism 3: Disentangled Preservation via Regularization
- **Claim:** Unlearning a specific attribute can be achieved without destroying the generator's fidelity by regularizing the update to preserve perceptual similarity and identity.
- **Mechanism:** The total loss $L_u$ combines the directional unlearning loss with LPIPS (perceptual) loss and ID (identity) loss. This forces the trainable generator $G_t$ to produce an image that is "close" to the original image while strictly moving away from the specific feature vector.
- **Core assumption:** The feature to be removed is sufficiently disentangled in the CLIP/W+ latent space such that it can be "subtracted" without destroying correlated essential features.
- **Evidence anchors:**
  - [Section 4.2]: "While unlearning the features, we need to preserve the usability... thus, we regularize the training process using ID loss and LPIPS loss."
  - [Table 3]: Shows marginal shift in scores for unrelated features, supporting the claim of disentanglement.
- **Break condition:** If the unwanted feature is highly entangled with core identity features, the regularization losses may prevent effective unlearning, or the unlearning may cause identity degradation.

## Foundational Learning

- **Concept: CLIP (Contrastive Language-Image Pre-training) Space**
  - **Why needed here:** This paper relies entirely on CLIP to define *what* to unlearn. You cannot understand "Directional Unlearning" without grasping that CLIP maps both images and text into a shared high-dimensional sphere where cosine distance equates to semantic similarity.
  - **Quick check question:** If two images have a cosine similarity of 0.9 in CLIP space, are they semantically similar or dissimilar?

- **Concept: StyleGAN Latent Space ($W+$)**
  - **Why needed here:** The method operates by manipulating vectors in $W+$ space. Unlike standard latent spaces, $W+$ allows for "disentangled" control (changing one feature without changing others), which is a prerequisite for the Latent Mapper to isolate the unwanted feature.
  - **Quick check question:** Why is the $W+$ space preferred over the standard $Z$ space for finding semantic directions in StyleGAN?

- **Concept: Mode Collapse**
  - **Why needed here:** A major risk in GAN fine-tuning is mode collapse, where the generator produces only one type of image. The paper explicitly mentions keeping a frozen generator $G_f$ and using specific losses to prevent this failure mode during the unlearning optimization.
  - **Quick check question:** In the context of this paper, what would happen if the unlearning loss was applied without the LPIPS/ID regularization constraints?

## Architecture Onboarding

- **Component map:** $M_p$ (Latent Mapper) -> $G_f$ (Frozen Generator) -> $E_I$ (CLIP Encoder) -> Compute $\vec{i}$ -> $G_t$ (Trainable Generator) -> Compute $L_{dir}$ + $L_{lpips}$ + $L_{id}$ -> Update $G_t$

- **Critical path:**
  1. **Phase 1 (Precompute):** Sample random latents $\to$ Mapper adds feature $\to$ Generate pairs $(x_0, \hat{x}_0)$ $\to$ Compute reference vector $\vec{i} = \text{norm}(E_I(\hat{x}_0) - E_I(x_0))$.
  2. **Phase 2 (Unlearn):** Sample random latents $\to$ Mapper adds feature $\to$ Generate images from both $G_f$ and $G_t$ $\to$ Compute alignment $\vec{j}$ $\to$ Minimize $1 - \cos(\vec{i}, \vec{j})$ + regularization.

- **Design tradeoffs:**
  - **Speed vs. Precision:** The method is much faster than full retraining but requires careful hyperparameter tuning ($\lambda_{ID}, \lambda_{LPIPS}$) to avoid degrading image quality (FID).
  - **Flexibility vs. Dependence:** It allows unlearning via text (high flexibility) but relies heavily on CLIP's pre-trained biases. If CLIP doesn't "understand" the prompt, unlearning fails.

- **Failure signatures:**
  - **Feature Retention:** If the directional loss weight is too low compared to identity loss, the model retains the unwanted feature.
  - **Identity Loss:** If the directional loss weight is too high, the output faces become generic or distorted (losing the source identity).
  - **Vector Misalignment:** If the implementation of $\vec{i}$ vs $\vec{j}$ does not match the math, the model might learn to *enhance* the feature instead of removing it.

- **First 3 experiments:**
  1. **Sanity Check (Vector Alignment):** Visualize the images $G_f(\hat{w})$ and $G_t(\hat{w})$ after just 10 steps. If $G_t$ looks *more* like the unwanted feature than $G_f$, invert the sign of the reference vector $\vec{i}$.
  2. **Hyperparameter Sweep (Lambda):** Run a grid search on $\lambda_{ID}$ (e.g., $10^{-2}, 10^{-3}$) for a simple attribute like "Spectacles" and plot FID vs. Degree of Unlearning ($\gamma$) to find the stability cliff.
  3. **Disentanglement Test:** Unlearn "purple hair" and then attempt to generate "blonde hair" using a new latent mapper. Check if the generator still possesses the capability to render hair textures correctly.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do inherent societal biases in pre-trained Vision-Language Models (VLMs) impact the stability and fairness of the textual guidance during the unlearning process?
  - **Basis in paper:** [explicit] The authors explicitly acknowledge that "pre-trained VLMs like CLIP are known to contain harmful societal biases and these can adversely influence the unlearning procedure."
  - **Why unresolved:** The current framework relies on standard CLIP models to guide the unlearning, potentially propagating or amplifying existing biases in the generator without correction mechanisms.
  - **What evidence would resolve it:** A comparative study measuring the presence of specific biases in generated images before and after unlearning, contrasting standard CLIP guidance with guidance from debiased VLMs.

- **Open Question 2:** Can the framework be adapted to unlearn identities or concepts that lack strong textual representations in the VLM's embedding space?
  - **Basis in paper:** [explicit] The paper states it is currently limited to "unlearning identities that are accessible through CLIP's text encoder," excluding identities the model has not "seen" or learned sufficiently.
  - **Why unresolved:** The method depends on the VLM's pre-existing semantic knowledge to compute the reference direction; concepts unknown to the VLM cannot be targeted by text prompts alone.
  - **What evidence would resolve it:** Extending the methodology to support few-shot image references or alternative encoders to successfully unlearn non-celebrity or obscure identities.

- **Open Question 3:** Does the directional unlearning approach generalize effectively to GANs trained on non-face domains where semantic latent directions are less structured?
  - **Basis in paper:** [inferred] The experimental validation and qualitative results are exclusively restricted to the FFHQ dataset (human faces) and StyleGAN2.
  - **Why unresolved:** Human faces possess highly regular geometric and semantic features (e.g., eyes, mouth), which may make latent disentanglement easier compared to unstructured datasets like landscapes or abstract art.
  - **What evidence would resolve it:** Quantitative evaluation of the "degree of unlearning" ($\gamma$) and FID scores when applying Text-to-Unlearn to diverse datasets such as ImageNet or LSUN.

## Limitations

- The approach is fundamentally constrained by CLIP's semantic understanding and the degree to which the target concept is represented in the GAN's latent space.
- The method assumes the feature to be removed is already accessible in the GAN's $W+$ space through latent manipulation—if the GAN never generates the feature naturally or with the Latent Mapper, unlearning is impossible.
- For some attributes (like skin tone), the concept may be too entangled with identity, making complete unlearning without collateral damage difficult.

## Confidence

- **High Confidence:** The mechanism of using CLIP space for semantic steering is theoretically sound and well-supported by the mathematical formulation.
- **Medium Confidence:** The claim that the method achieves high disentanglement is supported by quantitative metrics (Table 3), but real-world disentanglement may vary with more complex, correlated features.
- **Low Confidence:** The assertion that no external datasets are required is technically true, but the method's success critically depends on the quality of the Latent Mapper, which is not extensively validated for all types of concepts.

## Next Checks

1. **Robustness to Concept Complexity:** Validate the method on a diverse set of concepts ranging from simple visual attributes (e.g., "glasses") to more complex or abstract ones (e.g., "melancholic expression"). Track the failure rate and quality of unlearning.
2. **Cross-CLIP Generalization:** Repeat the unlearning process using different versions of CLIP (e.g., CLIP-ViT-B-32 vs. CLIP-ViT-L-14) to assess the method's dependence on a specific model's biases.
3. **Long-term Stability Test:** After unlearning, continue to generate images from the final $G_t$ for a large number of steps. Monitor for signs of "feature creep," where the unwanted feature gradually re-emerges due to the generator's internal dynamics.