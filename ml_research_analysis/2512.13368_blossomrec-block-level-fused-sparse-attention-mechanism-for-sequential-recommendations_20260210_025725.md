---
ver: rpa2
title: 'BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations'
arxiv_id: '2512.13368'
source_url: https://arxiv.org/abs/2512.13368
tags:
- attention
- blossomrec
- wang
- arxiv
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BlossomRec, a block-level fused sparse attention
  mechanism for sequential recommendations that models both long-term and short-term
  user interests through two distinct sparse attention patterns, achieving sub-quadratic
  complexity. The model selectively computes attention using importance-based block
  selection for long-range dependencies and power-law masking for short-term contexts,
  then fuses results via a learnable gating strategy.
---

# BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations

## Quick Facts
- arXiv ID: 2512.13368
- Source URL: https://arxiv.org/abs/2512.13368
- Reference count: 40
- Key outcome: Achieves SOTA performance on sequential recommendation with 11× memory reduction during training and 7× during inference

## Executive Summary
BlossomRec introduces a novel block-level fused sparse attention mechanism for sequential recommendations that models both long-term and short-term user interests through two distinct sparse attention patterns. The model achieves sub-quadratic complexity by selectively computing attention using importance-based block selection for long-range dependencies and power-law masking for short-term contexts, then fusing results via a learnable gating strategy. Experiments on four public datasets demonstrate that BlossomRec achieves state-of-the-art or comparable performance to transformer-based models while significantly reducing computational resources, making it practical for large-scale recommendation systems.

## Method Summary
BlossomRec employs a two-stage sparse attention mechanism that captures different temporal scales of user behavior. The long-term attention component uses a block-level selection strategy where a learned importance score determines which blocks of the sequence contribute to attention computation, focusing on semantically important interactions. The short-term attention component uses power-law masking that emphasizes recent interactions based on temporal decay patterns observed in user behavior. These two attention mechanisms are fused through a learnable gating network that dynamically balances their contributions. The entire architecture maintains transformer-like capabilities while reducing complexity from quadratic to sub-quadratic, enabling efficient processing of long user sequences without sacrificing recommendation quality.

## Key Results
- Achieves state-of-the-art or comparable performance to transformer-based models on four public datasets
- Reduces memory usage by up to 11× during training and 7× during inference
- Demonstrates significant computational speed improvements on long sequences compared to full attention baselines
- Maintains strong recommendation quality while achieving substantial efficiency gains

## Why This Works (Mechanism)
The model works by decomposing sequential recommendation into two complementary attention patterns that mirror how users form preferences. Long-term attention captures stable, semantically important interactions across the user history through selective block computation, while short-term attention captures immediate, recent preferences through power-law decay that reflects the natural recency bias in user behavior. The learnable fusion gate allows the model to adaptively weigh these two perspectives based on the specific context and user, creating a more nuanced representation than either approach alone. This decomposition allows the model to maintain the expressiveness of full attention where it matters most while avoiding unnecessary computation on less informative interactions.

## Foundational Learning
- **Sparse attention mechanisms**: Why needed - To reduce quadratic complexity of standard attention; Quick check - Verify that selected tokens cover diverse semantic content
- **Block-level selection**: Why needed - To identify semantically important segments without computing full attention; Quick check - Analyze importance score distribution across blocks
- **Power-law masking**: Why needed - To model the natural decay in relevance of older interactions; Quick check - Plot attention weight decay against temporal distance
- **Learnable fusion gates**: Why needed - To dynamically balance multiple attention perspectives; Quick check - Monitor gate values during training for stability
- **Sub-quadratic complexity analysis**: Why needed - To ensure theoretical efficiency gains translate to practice; Quick check - Measure actual FLOPs vs theoretical estimates
- **Sequential recommendation fundamentals**: Why needed - To understand user behavior patterns and evaluation metrics; Quick check - Review next-item prediction accuracy and ranking metrics

## Architecture Onboarding

Component Map: User Sequence -> Block Selection (Long-term) + Power-law Masking (Short-term) -> Fusion Gate -> Recommendation Head

Critical Path: Input embeddings → Sparse attention computation (two streams) → Fusion gate → Prediction layer

Design Tradeoffs:
- Block size selection: Larger blocks improve efficiency but may miss fine-grained patterns
- Power-law decay rate: Faster decay emphasizes recency but may lose valuable context
- Fusion gate complexity: More complex gates can better combine signals but increase parameters

Failure Signatures:
- Poor long-term performance: Indicates block selection importance scores are not capturing semantic relevance
- Weak short-term predictions: Suggests power-law decay parameters are misconfigured
- Unstable training: May indicate fusion gate conflicts between attention streams

First Experiments:
1. Ablation study removing long-term attention to quantify its contribution
2. Test with different block sizes to find optimal efficiency-accuracy tradeoff
3. Vary power-law decay parameters to understand their impact on recommendation quality

## Open Questions the Paper Calls Out
None

## Limitations
- Block-level selection relies on learned importance scores that may not generalize across different user behavior patterns
- Power-law masking assumes recent interactions dominate short-term interests, which may not hold for all recommendation scenarios
- Fusion strategy through learnable gates could introduce training instability, particularly with imbalanced importance scores

## Confidence
- High: Computational efficiency improvements (11× training memory reduction, 7× inference memory reduction)
- Medium: Performance comparisons on four public datasets (sensitive to hyperparameter tuning)
- Medium: Effectiveness of two-stage sparse attention in capturing both long-term and short-term interests (relies on assumptions about user behavior)

## Next Checks
1. Conduct ablation studies systematically removing either long-term or short-term sparse attention components to quantify individual contributions
2. Test model performance on datasets with varying sequence length distributions to evaluate robustness of block-level selection mechanism
3. Perform sensitivity analysis on importance threshold (β) and power-law decay parameters to understand impact on performance and efficiency