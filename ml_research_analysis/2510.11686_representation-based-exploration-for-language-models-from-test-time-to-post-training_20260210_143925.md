---
ver: rpa2
title: 'Representation-Based Exploration for Language Models: From Test-Time to Post-Training'
arxiv_id: '2510.11686'
source_url: https://arxiv.org/abs/2510.11686
tags:
- exploration
- arxiv
- language
- pass
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether deliberate exploration with representation-based\
  \ diversity bonuses can expand the reasoning capabilities of language models beyond\
  \ simply sharpening behaviors present in the base model. The authors introduce a\
  \ simple, principled exploration strategy using elliptical bonuses derived from\
  \ the model\u2019s hidden states, and evaluate it in two settings: inference-time\
  \ selection and RL post-training."
---

# Representation-Based Exploration for Language Models: From Test-Time to Post-Training

## Quick Facts
- **arXiv ID**: 2510.11686
- **Source URL**: https://arxiv.org/abs/2510.11686
- **Reference count**: 40
- **Key outcome**: Representation-based exploration improves verifier efficiency by 50%+ for strong models and eliminates diversity collapse in RL post-training.

## Executive Summary
This paper introduces representation-based exploration using elliptical bonuses derived from language model hidden states to improve reasoning diversity. The authors demonstrate that deliberate exploration with these bonuses can discover novel reasoning strategies beyond simply sharpening behaviors present in the base model. They evaluate their approach in two settings: inference-time selection from candidate pools and RL post-training augmentation. Results show significant efficiency gains in inference-time selection (50%+ improvement on MATH and GSM8K) and elimination of diversity collapse in RL training, achieving comparable or better performance than strong baselines like GRPO.

## Method Summary
The authors introduce representation-based exploration through elliptical bonuses computed from model hidden states. For inference-time, they use a greedy selection algorithm that iteratively chooses responses maximizing the bonus (h^T Λ h) while maintaining an inverse covariance matrix to track diversity. For RL post-training, they augment GRPO rewards with representation-based bonuses to counteract diversity collapse. Representations are extracted by mean-pooling last-layer hidden states across all tokens, followed by sparse random projection to 512 dimensions. The approach is evaluated on mathematical reasoning tasks including MATH, GSM8K, and AIME 2024.

## Key Results
- Inference-time RepExp improves verifier efficiency by over 50% for strong models like Qwen-2.5-14B-Instruct on MATH and GSM8K
- RL post-training with RepExp eliminates diversity collapse, maintaining pass@k across all k values where vanilla GRPO degrades
- On AIME 2024, RepExp's pass@80 matches GRPO's pass@256, demonstrating a 3x improvement in sample efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Elliptical bonuses derived from hidden states quantify response novelty by measuring how poorly a candidate is represented by previously selected responses in the model's representation space.
- Mechanism: The bonus div(h|h_{1:i-1}) = h^T Σ^{-1}_i h, where Σ_i accumulates outer products of prior feature vectors, grows large for representations orthogonal to the current covariance span—indicating novelty.
- Core assumption: Hidden states encode semantically meaningful reasoning strategies such that geometric distance correlates with behavioral diversity.
- Evidence anchors: [abstract] "exploration with a simple, principled, representation-based bonus derived from the pre-trained language model's hidden states significantly improves diversity and pass@k rates"

### Mechanism 2
- Claim: Maintaining an inverse covariance matrix across selections provides history-aware diversity tracking that penalizes redundancy with all prior selections, not just immediate predecessors.
- Mechanism: After selecting y_t, the inverse covariance updates via Woodbury identity: Λ_t = Λ_{t-1} - (Λ_{t-1}h_t h_t^T Λ_{t-1})/(1 + h_t^T Λ_{t-1}h_t), enabling O(d²) updates. The quadratic form h^T Λ h then captures novelty relative to the full history.
- Core assumption: Covariance in representation space meaningfully captures redundancy in reasoning approaches.
- Evidence anchors: [section 3] "the method is history-aware: the covariance matrix summarizes all previously selected generations, and redundancy with previous selection (in representation space) is penalized"

### Mechanism 3
- Claim: Augmenting RL rewards with representation-based exploration bonuses counteracts the "sharpening" phenomenon where GRPO concentrates probability mass on a narrow set of known solutions.
- Mechanism: Modified reward r_i = R(x, y_i) + β · h̄_θ(x, y_i)^T Σ^{-1} h̄_θ(x, y_i) creates a joint signal optimizing both correctness and diversity, preserving pass@k at large k where vanilla GRPO collapses.
- Core assumption: The bonus coefficient β can be tuned to balance exploration (diversity) against exploitation (correctness) without reward hacking.
- Evidence anchors: [abstract] "integrating representation-based exploration eliminates the 'diversity collapse' phenomenon, with RepExp achieving comparable or better pass@k rates than both GRPO and the base model"

## Foundational Learning

- Concept: Elliptical bonuses and leverage scores (from linear bandits/active learning)
  - Why needed here: Understanding why h^T Σ^{-1}h measures uncertainty/novelty—this is the core bonus formula.
  - Quick check question: If Σ = λI (isotropic covariance), what does the bonus h^T Σ^{-1}h simplify to?

- Concept: Sherman-Morrison-Woodbury matrix identity
  - Why needed here: Enables O(d²) incremental updates to inverse covariance instead of O(d³) recomputation per selection.
  - Quick check question: Given (A + uv^T)^{-1}, how do you express it in terms of A^{-1} without full inversion?

- Concept: Pass@k as a diversity-aware metric
  - Why needed here: The paper's central claim is improving pass@k at large k; this metric captures both accuracy and coverage of the solution space.
  - Quick check question: If pass@1 = 0.4 but pass@256 = 0.8, what does the gap reveal about the model's behavioral diversity?

## Architecture Onboarding

- Component map: Representation extractor -> Covariance tracker -> Bonus computer -> Selector (inference-time) or Reward augmenter (RL)
- Critical path:
  1. Generate N candidate responses (e.g., N=6400 for inference-time, k=8 rollouts for RL)
  2. Extract h̄_θ(x, y_i) via mean-pooling hidden states, sparse project to 512D, mean-center
  3. Initialize Λ_0 = λ^(-1)I_d
  4. For inference-time: iterate k selections, each maximizing h^T Λ_t h, updating Λ via Woodbury
  5. For RL: compute bonuses for all rollouts in batch, augment rewards, proceed with GRPO update

- Design tradeoffs:
  - Pool size N: larger pools improve coreset quality but increase upfront generation cost
  - Projection dimension d: higher preserves information but slows O(d²) covariance operations
  - Representation pooling: Figure 4 shows mean-pooling all tokens is ~2× more efficient than last-token only
  - Bonus coefficient β: Table 3 uses β=0.01 for RL; too high risks reward hacking

- Failure signatures:
  - Weaker models degrade: Figure 5 shows Qwen-0.5B gets no benefit or slight harm—representations may not encode useful diversity
  - High-temperature + RepExp underperforms: Incoherent responses appear "novel" in representation space but lack correctness
  - Bonus domination: If β too high, pass@1 drops as model chases novelty over correctness
  - Covariance ill-conditioning: If features are highly correlated, Λ becomes numerically unstable

- First 3 experiments:
  1. Reproduce inference-time selection: On MATH test split with Qwen-2.5-7B-Instruct, generate N=6400 responses per question, select k=256 via RepExp vs. random sampling, measure samples-to-correct. Target: 50%+ efficiency gain over random.
  2. Ablate representation extraction: Compare mean-pooled (all tokens) vs. last-token vs. penultimate-token representations on GSM8K using Qwen-7B. Verify Figure 4 finding that pooling is ~2× more efficient.
  3. RL post-training integration: Train Qwen-2.5-7B-Instruct with RepExp-augmented GRPO (β=0.01, d=32 sparse projection) on DAPO-Math-17K, evaluate pass@k on AIME 2024. Compare against vanilla GRPO and Unlikeliness baseline across 3 seeds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can representation-based exploration be effectively extended to domains without verifiable rewards while mitigating reward hacking?
- Basis in paper: [explicit] The authors explicitly state in their final remarks: "Beyond verifiable rewards. How can we deliberately incentivize exploration in domains without verifiable rewards, while simultaneously mitigating reward hacking?"
- Why unresolved: All experiments in the paper use tasks with verifiable rewards (math correctness, code unit tests). No exploration strategy is tested or proposed for domains like creative writing, open-ended dialogue, or subjective tasks where correctness cannot be automatically verified.

### Open Question 2
- Question: What diversity metrics are most effective for autoregressive generation, and can implementation be optimized to achieve wall-clock compute improvements?
- Basis in paper: [explicit] In Section 6, the authors state regarding token-level exploration: "much remains to be done in terms of (1) understanding which diversity metrics are most helpful, and (2) optimizing the implementation to close the compute gap."
- Why unresolved: The token-level RepExp variant (Section 4.2) improves pass@k for large k but is computationally expensive due to requiring a forward pass through the model for each vocabulary token at each timestep.

### Open Question 3
- Question: Why does representation-based exploration benefit stronger models more and hurt weaker models?
- Basis in paper: [inferred] Figure 5 shows that RepExp hurts weaker models like Qwen-0.5B while benefiting stronger models like Qwen-14B and Qwen-32B. The authors hypothesize this relates to representation quality but do not fully investigate or resolve this phenomenon.
- Why unresolved: The paper observes the correlation between model strength and RepExp benefit but does not provide a mechanistic explanation.

## Limitations

- The approach assumes hidden-state geometry meaningfully captures behavioral diversity, which may not hold for all model architectures or tasks
- Benefits appear most pronounced for models of sufficient capability, with weaker models showing neutral or negative effects
- Inference-time gains rely on generating very large candidate pools (N=6400), incurring significant computational overhead
- RL post-training results are demonstrated primarily on mathematical reasoning tasks where diverse solution strategies are known to exist

## Confidence

- **High Confidence**: The inference-time selection experiments showing 50%+ efficiency gains on MATH and GSM8K with Qwen-2.5-14B-Instruct are well-supported by quantitative metrics and clear baselines.
- **Medium Confidence**: The RL post-training results eliminating diversity collapse and achieving 3x sample efficiency on AIME 2024 are promising but based on a single challenging dataset.
- **Low Confidence**: The theoretical grounding of elliptical bonuses as "uncertainty measures" in representation space is intuitively appealing but lacks rigorous mathematical justification beyond the linear regression analogy.

## Next Checks

1. **Ablation study on representation extraction**: Systematically compare mean-pooling all tokens vs. last-token vs. penultimate-token representations across multiple tasks (MATH, GSM8K, MBPP) with Qwen-2.5-7B-Instruct. Measure both efficiency gains and absolute pass@k to verify that the 2× improvement observed in Figure 4 generalizes beyond the single comparison shown.

2. **Cross-domain generalization test**: Apply RepExp-augmented GRPO to non-mathematical reasoning tasks such as CommonsenseQA, StrategyQA, or natural language inference. Evaluate whether the diversity preservation and efficiency gains observed on AIME 2024 transfer to domains where solution diversity manifests differently than in mathematical problem-solving.

3. **Capability threshold characterization**: Systematically test RepExp across a model capability spectrum (Qwen-0.5B, Qwen-1.5B, Qwen-3B, Qwen-7B, Qwen-14B) on standardized reasoning benchmarks. Quantify the exact point at which RepExp transitions from harmful to beneficial, and investigate whether this threshold correlates with specific model properties (parameter count, pretraining compute, or specific architectural features).