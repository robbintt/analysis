---
ver: rpa2
title: 'ESNLIR: A Spanish Multi-Genre Dataset with Causal Relationships'
arxiv_id: '2503.08803'
source_url: https://arxiv.org/abs/2503.08803
tags:
- dataset
- examples
- test
- language
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESNLIR, a large-scale Spanish multi-genre
  dataset for Natural Language Inference (NLI) that includes causal relationships.
  The dataset was created using an automated method based on linking phrases to extract
  premise-hypothesis pairs from diverse text genres.
---

# ESNLIR: A Spanish Multi-Genre Dataset with Causal Relationships

## Quick Facts
- **arXiv ID:** 2503.08803
- **Source URL:** https://arxiv.org/abs/2503.08803
- **Reference count:** 8
- **Primary result:** XLMRoBERTa achieved 0.676 accuracy and 0.676 macro F1 on ESNLIR

## Executive Summary
This paper introduces ESNLIR, a large-scale Spanish multi-genre dataset for Natural Language Inference (NLI) that includes causal relationships. The dataset was created using an automated method based on linking phrases to extract premise-hypothesis pairs from diverse text genres. The authors evaluated the dataset using BERT-based models and found strong baseline performance, with XLMRoBERTa achieving an accuracy of 0.676 and macro F1 score of 0.676. The dataset demonstrates good generalization across genres and is robust to annotation artifacts, though it shows lower performance on non-formal writing genres.

## Method Summary
The authors developed an automated method to generate premise-hypothesis pairs from multi-genre Spanish text by identifying linking phrases that establish causal relationships. The process extracts sentences containing these phrases and constructs inference pairs where the premise contains the linking phrase and the hypothesis represents the causal relationship. The dataset spans multiple genres including news articles, books, comments, and talks, creating a diverse benchmark for Spanish NLI tasks. The automated approach enables large-scale dataset creation while maintaining genre diversity and causal relationship coverage.

## Key Results
- XLMRoBERTa achieved accuracy of 0.676 and macro F1 of 0.676 on the ESNLIR dataset
- The dataset shows good generalization across different genres with consistent performance
- Human validation confirmed dataset quality with overall performance around 68%, indicating it's a challenging benchmark
- Lower performance observed on non-formal writing genres (comments and talks) with accuracy of 0.647 and 0.597 respectively

## Why This Works (Mechanism)
The automated extraction method works by leveraging linguistic linking phrases that naturally occur in Spanish text to identify potential causal relationships. By extracting these phrases and constructing premise-hypothesis pairs around them, the approach captures authentic language patterns rather than synthetic examples. The multi-genre approach ensures diverse linguistic contexts and prevents models from overfitting to specific writing styles. The method's scalability allows for creating large datasets while maintaining semantic relationships, making it practical for developing robust NLI benchmarks.

## Foundational Learning

**Natural Language Inference (NLI)**: The task of determining semantic relationships between text pairs (entailment, contradiction, neutral). Why needed: Forms the core task that ESNLIR benchmarks. Quick check: Can the model distinguish when a hypothesis follows from a premise versus when it doesn't?

**Causal Relationships in Text**: Identifying when one event or statement causes another within natural language. Why needed: ESNLIR specifically targets causal inference, a subset of NLI. Quick check: Does the model correctly identify causal links versus other semantic relationships?

**Multi-Genre Text Analysis**: Handling diverse writing styles and linguistic patterns across different text types. Why needed: ESNLIR spans news, books, comments, and talks, requiring genre-adaptive understanding. Quick check: Does performance degrade significantly across genre boundaries?

## Architecture Onboarding

**Component Map**: Raw Spanish Text -> Linking Phrase Extraction -> Premise-Hypothesis Pair Generation -> Dataset Construction -> Model Training -> Evaluation

**Critical Path**: The automated extraction pipeline is the critical component, as dataset quality directly determines model performance. Errors in linking phrase identification or pair generation propagate through the entire pipeline.

**Design Tradeoffs**: Automated extraction enables large-scale dataset creation but may introduce noise compared to manual annotation. The approach prioritizes scalability and genre diversity over perfect annotation accuracy, accepting some labeling uncertainty for broader coverage.

**Failure Signatures**: Poor performance on non-formal genres suggests the linking phrase method may not capture informal causal reasoning patterns. Confusion between reasoning and entailment classes indicates ambiguous cases where causal relationships overlap with other semantic relationships.

**First Experiments**:
1. Evaluate model performance on each genre separately to identify specific weaknesses
2. Test cross-lingual transfer by training on English NLI datasets and evaluating on ESNLIR
3. Conduct ablation studies removing ambiguous linking phrases to measure impact on reasoning/entailment confusion

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do Large Language Models (LLMs) perform on ESNLIR compared to BERT-based models, and why do they underperform?
- **Basis in paper:** [explicit] Authors state: "we experimented with more state-of-the-art Large Language Models (LLM), like GPT-4o. But the results were underwhelming when comparing with BERT-based models, and we decided to save these results for another paper in which we carefully explore why they underperform."
- **Why unresolved:** LLM evaluation was conducted but results were deferred to future work; the reasons for underperformance remain unanalyzed.
- **What evidence would resolve it:** Systematic comparison of LLM (e.g., GPT-4o, Llama) performance on ESNLIR with error analysis examining whether LLMs struggle with causal reasoning labels, genre diversity, or Spanish-specific phenomena.

### Open Question 2
- **Question:** Would full human annotation of ESNLIR significantly improve model performance and dataset quality?
- **Basis in paper:** [explicit] Authors propose: "Intensive annotation of test examples with human annotators to improve the quality of the dataset" as future work, and note budget constraints limited validation to 2,000 examples.
- **Why unresolved:** Human validation revealed only 974 of 2,000 examples (48.7%) had labels matching majority human judgment, suggesting substantial noise, but full annotation impact remains untested.
- **What evidence would resolve it:** Comparison of model performance on fully human-annotated versus automatically-extracted test sets, measuring accuracy gains and analyzing which classes and genres benefit most.

### Open Question 3
- **Question:** What factors explain the persistent confusion between reasoning and entailment classes?
- **Basis in paper:** [inferred] The paper notes these are "the most challenging categories" that "models often confuse with each other more than with other categories," and provides examples where "the relationship can be unclear without proper context."
- **Why unresolved:** While the paper observes the confusion, it does not determine whether this stems from ambiguous linking phrases, inherent overlap between specification/generalization and causation, or model limitations.
- **What evidence would resolve it:** Linguistic analysis of disputed examples, human annotation studies measuring inter-annotator agreement on reasoning versus entailment, and ablation studies testing whether removing ambiguous linking phrases reduces confusion.

### Open Question 4
- **Question:** How can performance on non-formal writing genres (comments, talks) be improved?
- **Basis in paper:** [inferred] Results show "datasets with the poorest performance are associated with non-formal genres, such as comments and talks," with accuracy of 0.647 and 0.597 respectively, attributed to non-formal writing "lacking rigid semantic rules."
- **Why unresolved:** The paper identifies the problem but does not propose or test solutions for handling variable semantic structures in informal text.
- **What evidence would resolve it:** Experiments with genre-adaptive training, data augmentation targeting informal writing patterns, or specialized preprocessing for non-formal text to measure whether the performance gap can be reduced.

## Limitations
- Automated extraction method may introduce noise and labeling inconsistencies compared to manual annotation
- Performance significantly lower on non-formal writing genres, suggesting method limitations with informal language
- Human validation showed only 48.7% agreement on labels, indicating substantial uncertainty in the dataset

## Confidence
- **Dataset Robustness:** Medium - Good generalization across genres but acknowledged weaknesses in non-formal writing
- **Automated Method Quality:** Medium - Innovative approach but validation shows significant label uncertainty
- **Model Performance Claims:** Medium - Strong baselines established but limited to transformer-based models without comparative analysis

## Next Checks
1. Conduct detailed error analysis across different genres to identify specific weaknesses and genre-specific patterns
2. Perform cross-linguistic comparison by testing the same models on English NLI datasets to establish baseline performance differences
3. Implement ablation studies to quantify the impact of the automated extraction method versus manual annotation on model performance and dataset quality