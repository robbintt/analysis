---
ver: rpa2
title: 'A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and
  Time Series Data with Graph Neural Networks'
arxiv_id: '2512.08567'
source_url: https://arxiv.org/abs/2512.08567
tags:
- news
- stock
- graph
- data
- companies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid stock market forecasting model that
  integrates financial news sentiment and historical time series data using Graph
  Neural Networks (GNNs). The core method idea is to represent companies, news articles,
  and industries as nodes in a heterogeneous graph, where edges capture relationships
  between entities, and use GraphSAGE to aggregate messages and predict stock price
  movements.
---

# A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2512.08567
- **Source URL:** https://arxiv.org/abs/2512.08567
- **Reference count:** 2
- **Primary result:** GNN model achieves 53% accuracy for binary classification and 4% precision gain for significance-based classification

## Executive Summary
This paper introduces a hybrid stock market forecasting model that combines financial news sentiment and historical time series data using Graph Neural Networks (GNNs). The model represents companies, news articles, and industries as nodes in a heterogeneous graph, capturing relationships between entities through edges. Using GraphSAGE for message aggregation, the model outperforms a baseline LSTM approach, particularly for companies with more associated news coverage. The study finds that headlines contain stronger predictive signals than full article content, suggesting that compressed information is more effective for short-term market predictions.

## Method Summary
The proposed method integrates LSTM for sequential encoding of 15-day historical close prices with GraphSAGE on a heterogeneous graph of companies, articles, and industries. News articles are embedded using finance-specific language models (Sigma or FinBERT), while company embeddings are derived from LSTM hidden states. The graph structure connects articles to main companies and mentioned companies, with companies linked to their industries. Three GraphSAGE layers with 128-dimensional embeddings aggregate messages across the graph, and the model is trained using AdamW optimizer with 0.0001 learning rate for 55 epochs. The approach supports two classification targets: direction-only movement and significance-based movement with a 0.04×std threshold.

## Key Results
- GNN model achieves 53% accuracy for binary classification, outperforming LSTM baseline
- Significance-based classification shows 4% precision gain over baseline
- Headlines consistently outperform full article content by 1-2% precision
- Companies with more associated news yield higher prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous graph structure enables relational information flow between companies, news, and industries that sequential models cannot capture. GraphSAGE aggregates messages from neighboring nodes (articles mentioning companies, industry affiliations), allowing cross-entity information propagation rather than treating each company in isolation. The LSTM encodes individual company history, while the GNN layer propagates context from related entities. This works when companies mentioned together in news or sharing industry affiliations exhibit related stock movements, though not purely homophilic relationships.

### Mechanism 2
Prediction accuracy correlates with news coverage density—companies with more associated articles achieve higher accuracy. More article-company edges create richer information pathways for message passing. The GNN can aggregate sentiment signals from multiple sources, reducing noise through redundancy. This mechanism works when news volume correlates with signal quality, as more coverage provides more opportunities for the model to learn company-specific patterns.

### Mechanism 3
Article headlines outperform full article content for short-term prediction tasks. Headlines compress the most behaviorally relevant information while filtering noise from full article content. Full text introduces irrelevant details that dilute embedding quality. This mechanism relies on the assumption that headlines disproportionately shape investor sentiment and market reactions.

## Foundational Learning

- **Concept:** Message Passing in GNNs
  - Why needed here: Core mechanism for propagating information across the heterogeneous graph (companies, articles, industries)
  - Quick check question: Given a company node connected to 5 article nodes and 1 industry node, how would GraphSAGE aggregate information differently from simply averaging all neighbor embeddings?

- **Concept:** LSTM for Sequential Encoding
  - Why needed here: Encodes 15-day historical price windows into company node features before GNN processing
  - Quick check question: Why use an LSTM rather than hand-crafted features (e.g., moving averages, RSI) directly as node embeddings?

- **Concept:** Domain-Specific Language Models (FinBERT/Sigma)
  - Why needed here: Converts financial news text into meaningful embeddings that capture sentiment and context relevant to market prediction
  - Quick check question: What failure mode might occur if you used a general-purpose BERT model instead of a finance-specific model?

## Architecture Onboarding

- **Component map:** LSTM (15-day close prices) → company embeddings; Sigma/FinBERT (news headlines) → article embeddings → GraphSAGE (3 layers, 128-dim) → Company node representation → Classification head → Prediction
- **Critical path:** News headline → Sigma embedding → Article node → GraphSAGE aggregation (3 layers) → Company node representation → Classification head → Prediction
- **Design tradeoffs:**
  - **GraphSAGE vs. GAT:** Paper shows GAT fails badly on this graph (25% precision vs. 55%) because the graph is heterophilic—connected nodes often have dissimilar labels. GraphSAGE's neighborhood sampling handles this better.
  - **Headlines vs. full content:** ~2% precision gain from headlines alone; full content introduces noise.
  - **Binary vs. significance target:** Significance target is more practical but creates class imbalance (requires precision-focused evaluation).
- **Failure signatures:**
  - **GAT with low precision (~25%):** Indicates heterophilic graph structure; switch to GraphSAGE.
  - **High variance in company-level accuracy:** Check news density—low-accuracy companies likely have sparse article edges.
  - **Significance target with near-zero recall:** Model predicting majority class; need better class balancing or threshold tuning.
- **First 3 experiments:**
  1. **Baseline sanity check:** Train LSTM-only model on historical close prices. Expect ~52% accuracy. If significantly lower, check data preprocessing.
  2. **Graph structure validation:** Compare GraphSAGE vs. GAT on same data. GraphSAGE should win by 20+ percentage points on precision. If not, verify edge construction logic.
  3. **News signal validation:** Run GraphSAGE with headlines-only vs. full-content embeddings. Headlines should show 1-2% precision improvement. If reversed, examine text preprocessing pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does extracting specific financial events (e.g., mergers, earnings) from article bodies outperform using raw headlines for stock prediction?
- **Basis in paper:** Section 6.1 recommends "Event Extraction" to isolate core signals and reduce noise, suggesting this could yield better performance than using only titles.
- **Why unresolved:** The current study only compared raw headlines against full article content, finding headlines superior due to the noise in full text.
- **What evidence would resolve it:** A comparative experiment showing that structured event embeddings from article bodies achieve higher precision than the current headline-only baseline.

### Open Question 2
- **Question:** Would incorporating distinct edge types for strategic relationships (e.g., competitors vs. partners) improve the model's ability to learn nuanced market influence?
- **Basis in paper:** Section 6.2 identifies the graph schema as "simplistic" because it treats all co-mentions as equal connections.
- **Why unresolved:** The current model connects companies based solely on co-mentions without differentiating the nature of their relationship.
- **What evidence would resolve it:** Evaluation of a modified heterogeneous graph where edges are labeled with relationship types, demonstrating improved accuracy over the co-mention baseline.

### Open Question 3
- **Question:** Can coupling the GNN with a Gaussian Process provide reliable confidence intervals for practical, risk-managed decision-making?
- **Basis in paper:** Section 6.3 highlights the "lack of robust uncertainty" as a key limitation for real-world application.
- **Why unresolved:** Standard neural network output probabilities are not robust measures of model uncertainty.
- **What evidence would resolve it:** A hybrid model that outputs calibrated confidence intervals alongside predictions, validated against out-of-distribution market data.

### Open Question 4
- **Question:** Does the GNN's performance advantage over the LSTM baseline disappear for stocks with very sparse news coverage?
- **Basis in paper:** Section 5.2 notes the overall improvement is diluted by "news-poor" companies defaulting to baseline performance.
- **Why unresolved:** The aggregate results mask performance variance relative to data density, leaving the failure threshold undefined.
- **What evidence would resolve it:** A stratified analysis plotting prediction accuracy against the frequency of news mentions per company.

## Limitations

- **Dataset accessibility:** US equities dataset source not specified; Bloomberg dataset requires author permission, creating significant reproducibility barriers
- **Parameter gaps:** Missing LSTM hidden dimension size and exact method for creating company embeddings from LSTM outputs
- **Performance constraints:** 53% accuracy for binary classification, which while better than baseline, may not generalize to broader market segments

## Confidence

- **High Confidence:** Heterogeneous graph structure's ability to capture relational information between entities
- **Medium Confidence:** Superiority of headlines over full articles for short-term predictions
- **Low Confidence:** Significance-based classification results due to extreme class imbalance from 0.04×std threshold

## Next Checks

1. **Dataset Independence Test:** Attempt to replicate the core finding (headlines vs. full content performance difference) using an openly available financial news dataset like Reuters or Yahoo Finance to validate generalization beyond proprietary datasets.

2. **Heterophily Validation:** Systematically test the model's performance on graphs with varying degrees of homophily (mixing parameter) to confirm whether GraphSAGE superiority over GAT is due to heterophilic graph structure rather than implementation differences.

3. **News Density Sensitivity Analysis:** Conduct a controlled experiment varying news coverage density across companies (e.g., by synthetically reducing news for high-coverage companies) to verify the claimed correlation between news density and prediction accuracy.