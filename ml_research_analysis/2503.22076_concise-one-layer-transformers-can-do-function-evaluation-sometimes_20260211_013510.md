---
ver: rpa2
title: Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)
arxiv_id: '2503.22076'
source_url: https://arxiv.org/abs/2503.22076
tags:
- position
- layer
- attention
- input
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the computational expressiveness of one-layer
  transformers for function evaluation, proving both upper and lower bounds. The authors
  show that concise one-layer transformers (polylogarithmic in input size) can perform
  function evaluation when keys and values are encoded in the same input position,
  but not when keys and values are split across consecutive positions.
---

# Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)

## Quick Facts
- **arXiv ID:** 2503.22076
- **Source URL:** https://arxiv.org/abs/2503.22076
- **Reference count:** 40
- **Key outcome:** One-layer transformers with polylogarithmic size can evaluate functions when keys and values share positions, but require polynomial size or two layers when keys and values are split across consecutive positions.

## Executive Summary
This paper establishes the computational expressiveness of one-layer transformers for function evaluation tasks, proving both upper and lower bounds on what concise transformers can compute. The authors show that when keys and values are encoded at the same input position, concise transformers (polylogarithmic in input size) can perform exact function evaluation using angular embeddings. However, when keys and values are split across consecutive positions, any concise one-layer transformer requires at least polynomial size. The paper provides explicit constructions for the former case and proves lower bounds for the latter, with experimental results validating the theoretical predictions about learnability.

## Method Summary
The authors study five input representation cases for function evaluation, ranging from same-position key-value pairs to consecutive positions with permuted keys. They prove upper bounds using angular embeddings that encode integers as points on the unit circle, enabling exact matching via dot products. For cases where keys and values share positions, they construct 4-dimensional embeddings requiring no position embeddings. For cases where positions implicitly encode keys, they use position embeddings with the same angular representation. They prove lower bounds for the consecutive-positions-permuted-keys case by reduction to communication complexity, showing that concise solutions would imply impossibly efficient communication protocols. Experiments train 1-2 layer transformers with single attention heads on random permutations, comparing learned models against theoretical constructions.

## Key Results
- Concise one-layer transformers (O(log n) bits precision) can evaluate functions exactly when keys and values share positions using 4-dimensional angular embeddings
- For consecutive positions with permuted keys, any concise one-layer transformer requires at least polynomial size Ω(n log n)
- Two-layer transformers can solve the consecutive-positions-permuted-keys case with polylogarithmic size by first copying keys to value positions
- Experimental results align with theory: models learn cases 1-4 well but struggle with case 5 using 1 layer, with some bimodality across runs

## Why This Works (Mechanism)

### Mechanism 1: Angular Key-Value Binding
One-layer transformers can perform function evaluation when keys and values occupy the same position by encoding integers as points on the unit circle. Each integer k ∈ [n] maps to angle θ_n(k) = 2πk/n with coordinates cs_n(k) = [cos(θ_n(k)), sin(θ_n(k))]. The dot product between two such vectors equals 1 iff the integers match, and ≤ 1 - 1/n² otherwise. Queries project the target key, keys project stored keys, and hard attention selects the matching position; the value vector at that position carries f(i*). Leftmost hard attention is available; O(log n) bits of precision suffice to distinguish the gap between matching and non-matching scores. This mechanism cannot coordinate both sources of information in a single layer when keys and values are split across positions.

### Mechanism 2: Position Embedding as Implicit Key
When keys are implicit (case 1: no keys) or separated but ordered (case 4), position embeddings can substitute for explicit key encoding. Position embedding p(i) encodes the integer i using the same angular representation. The attention score compares the target key's encoding (in the query at position n) against position embeddings (in the keys). This recovers key information without explicit key tokens. This requires keys to follow a known ordering that aligns with positions. If keys are permuted (case 5), position carries no key information, breaking the substitution.

### Mechanism 3: Two-Layer Key-Value Reunification
Two-layer transformers can solve case 5 (consecutive positions, permuted keys) by first copying keys to value positions, then applying same-position lookup. Layer 1 attends from each value position (odd indices) to its corresponding key position (even indices) using position-based attention (positions 2k and 2k+1 share a position-based identifier). This copies the key into the value position's representation via residuals. Layer 2 then applies the same-position construction from Theorem 3.1. This requires residual connections to allow layer 2 to access both the original input and layer 1's outputs; leftmost hard attention is available. If only one layer is available, no mechanism can bridge the key-value gap in case 5.

## Foundational Learning

- **Concept: Hard vs. Softmax Attention**
  - **Why needed here:** Theoretical constructions use hard attention (argmax) for clean proofs; practical models use softmax. Understanding both clarifies what's provably expressible vs. what's learnable.
  - **Quick check question:** Given attention scores [0.5, 0.3, 0.2], what weights does leftmost hard attention assign? What about softmax?

- **Concept: Communication Complexity**
  - **Why needed here:** The lower bound proof (Theorem 3.6) relies on showing that if a concise 1-layer transformer solved case 5, it would imply an impossibly efficient communication protocol.
  - **Quick check question:** If Alice knows values and Bob knows keys, how many bits must Alice send for Bob to compute f(0)? Why does this give a lower bound on transformer size?

- **Concept: Angular/Circular Embeddings**
  - **Why needed here:** The core technical construction encodes discrete tokens as continuous angles, enabling exact matching via dot products.
  - **Quick check question:** For n=8, what angle represents the integer 3? What is the dot product between the embeddings of integers 3 and 5?

## Architecture Onboarding

- **Component map:** Input embedding = token embedding + position embedding → W_Q, W_K, W_V matrices → hard attention (argmax) → residual connections → final output → U matrix to logits

- **Critical path:**
  1. Choose input representation (cases 1–5) based on task constraints
  2. If keys and values share positions: use angular encoding, no position embedding needed (Theorem 3.1)
  3. If positions encode keys implicitly: use position embeddings with angular encoding (Theorems 3.2, 3.3)
  4. If keys and values are split with arbitrary permutation: require 2 layers (Theorem 3.12) or accept O(n log n) size for 1 layer (Theorem 3.7)

- **Design tradeoffs:**
  - Hard attention → cleaner theoretical analysis; Softmax → practical trainability
  - Position embedding design: Learned (empirical success) vs. Angular (theoretical guarantee)
  - Embedding dimension: Theoretical minimum is 4 for same-position cases; experiments suggest d_token ≥ 8 for reliable learning

- **Failure signatures:**
  - Case 5 with 1 layer, small embedding: accuracy degrades to chance (Table 2, n ≥ 20)
  - No position embedding with implicit-key tasks (cases 1, 4): accuracy at chance (~1/n)
  - High embedding dimensions (d_token = 200) in case 5: performance collapse, possibly due to optimization difficulty

- **First 3 experiments:**
  1. **Validate angular encoding:** Implement Theorem 3.1 construction explicitly; verify exact function evaluation on synthetic data for n=8, 16, 32 with no training.
  2. **Test learnability gap:** Train 1-layer transformers on cases 3 vs. 5 (same vs. consecutive positions, both permuted) with matched hyperparameters; plot accuracy vs. n.
  3. **Probe position embedding role:** For case 1, ablate learned vs. angular position embeddings; measure whether learned embeddings converge to angular-like structure.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Why do training runs with identical hyperparameters exhibit bimodal accuracy distributions, with some runs achieving perfect accuracy while others stagnate at chance level?
**Basis in paper:** [explicit] The authors state "We observe fairly bimodal distributions in accuracy across different runs with the same hyperparameters for cases (1-4)... This behavior merits further investigation."
**Why unresolved:** The paper documents this phenomenon empirically but does not analyze its causes, such as initialization sensitivity, local minima, or optimization dynamics.
**What evidence would resolve it:** Systematic studies varying random seeds, learning rates, and initialization schemes; analysis of attention pattern development during training in successful vs. failed runs.

### Open Question 2
**Question:** Why does performance degrade at high embedding dimensions (e.g., d_token = 200) in the consecutive-positions-with-permuted-keys case?
**Basis in paper:** [explicit] Page 9 notes "for all four values of n, maximum performance is approximately at chance (1/n) for d_token = 200."
**Why unresolved:** This contradicts intuition that larger models should learn at least as well; the authors do not investigate whether this stems from optimization difficulties, overfitting, or architectural factors.
**What evidence would resolve it:** Experiments tracking loss curves, gradient norms, and attention patterns across embedding dimensions; analysis of whether the degradation occurs during training or generalization.

### Open Question 3
**Question:** Can the hard attention constructions in this paper be systematically converted to practical softmax attention transformers with comparable conciseness?
**Basis in paper:** [explicit] Footnote 2 states "It seems likely that the hard attention transformers that we present could be converted to softmax transformers with different input embeddings or with inverse polynomial temperature, using the approach of Yang et al. (2024)."
**Why unresolved:** This is stated as a likelihood rather than a proven result, and the practical learnability of such converted constructions remains untested.
**What evidence would resolve it:** Explicit constructions of softmax transformers matching the theoretical bounds; empirical verification that these can be learned from random initialization.

## Limitations
- The paper proves that concise one-layer transformers cannot solve case 5 but doesn't report experimental validation that 2-layer models successfully recover performance
- Experimental results use learned embeddings while theoretical constructions use specific angular embeddings, with no analysis of whether learned models discover similar mechanisms
- Evaluation methodology uses early stopping without specifying patience or validation criteria, potentially affecting comparison reliability

## Confidence

**High Confidence:** The upper bound results (Theorems 3.1-3.5) showing that concise one-layer transformers can solve cases 1-4 are highly reliable. The constructions are explicit, the angular encoding mechanism is well-established, and experimental results directly validate these claims across multiple embedding dimensions.

**Medium Confidence:** The lower bound proof for case 5 (Theorem 3.6) is mathematically sound, but its practical implications depend on the assumption that any concise solution would imply an impossibly efficient communication protocol. The experimental validation is partial—showing failure for 1-layer models but not demonstrating success for the proposed 2-layer alternative.

**Low Confidence:** The experimental claim about 2-layer models being necessary for case 5 lacks direct evidence. While the paper proves a 2-layer construction exists, it doesn't report experimental results showing 2-layer transformers actually achieving high accuracy on this case, which would be the definitive validation.

## Next Checks

**Check 1: Validate the 2-Layer Construction Experimentally**
Implement the two-layer construction from Theorem 3.12 for case 5 and measure accuracy across increasing n. This would directly validate whether the theoretical 2-layer solution translates to practical learnability, addressing the current gap between lower bound proofs and experimental results.

**Check 2: Analyze Learned Embedding Structure**
For successful runs on cases 1-4, analyze whether learned token and position embeddings converge to angular structures similar to the theoretical constructions. This would clarify whether learned models are discovering the same mechanism as the explicit constructions, or if they're using different strategies to achieve similar outcomes.

**Check 3: Systematic Ablation of Theoretical Components**
Create controlled experiments that isolate specific theoretical mechanisms: (a) remove position embeddings in cases 1 and 4 to confirm they're necessary; (b) test with fixed angular embeddings vs. learned embeddings to measure the impact on convergence and accuracy; (c) vary the attention mechanism between hard and softmax variants to understand the theoretical-practical gap.