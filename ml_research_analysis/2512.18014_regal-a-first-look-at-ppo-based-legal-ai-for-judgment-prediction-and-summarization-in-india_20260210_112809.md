---
ver: rpa2
title: 'ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization
  in India'
arxiv_id: '2512.18014'
source_url: https://arxiv.org/abs/2512.18014
tags:
- legal
- summarization
- judgment
- learning
- indian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReGal explores reinforcement learning for legal AI in India, focusing
  on judgment prediction and summarization using PPO. The framework combines instruction
  tuning with AI feedback but underperforms compared to supervised and proprietary
  models, with ROUGE-1 scores of 0.19 (prediction) and 0.41 (summarization) versus
  0.54 for GPT-3.5 Turbo.
---

# ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India

## Quick Facts
- arXiv ID: 2512.18014
- Source URL: https://arxiv.org/abs/2512.18014
- Reference count: 7
- Primary result: PPO-based ReGal underperforms supervised and proprietary models on Indian legal tasks, with ROUGE-1 scores of 0.19 (CJPE) and 0.41 (summarization) versus 0.54 for GPT-3.5 Turbo

## Executive Summary
ReGal explores reinforcement learning from AI feedback (RLAIF) using Proximal Policy Optimization (PPO) for two Indian legal tasks: Court Judgment Prediction and Explanation (CJPE) and Legal Document Summarization. The framework combines supervised fine-tuning with PPO-based alignment but underperforms compared to supervised and proprietary models, achieving ROUGE-1 scores of 0.19 on CJPE and 0.41 on summarization versus 0.54 for GPT-3.5 Turbo. Key challenges include reward model misalignment, legal language complexity, and lack of domain adaptation. Hallucinations are frequent in generated outputs, highlighting the need for stronger factuality constraints. While promising in concept, the findings underscore RL's current limitations for high-stakes legal tasks.

## Method Summary
ReGal employs a two-stage pipeline: (1) Supervised fine-tuning of Llama-2-7B on PredEx (12,178 train/3,044 test) and In-Abs datasets using instruction-tuning format, and (2) PPO optimization using reward models - InLegalBERT classifier for CJPE and ROUGE-based scorer for summarization. The PPO objective uses a clip ratio of [1-ε, 1+ε] with ε=0.1 to stabilize updates. Hyperparameters include learning rate 1.41e-5, batch size 4, and max tokens 500. Hardware: A100 80GB (~$100 cost).

## Key Results
- ROUGE-1 scores: 0.19 on CJPE (PredEx), 0.41 on summarization (In-Abs) versus 0.54 for GPT-3.5 Turbo
- PPO fails to improve upon supervised fine-tuning baseline
- Hallucinations are frequent in generated outputs
- Base model choice and reward model alignment critically impact performance
- Domain pretraining gap identified as major limitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPO clipping stabilizes policy updates in legal text generation by constraining deviation from the supervised baseline.
- Mechanism: The ratio $\frac{\pi_\theta(y|x)}{\pi_{SFT}(y|x)}$ is clipped to $[1-\epsilon, 1+\epsilon]$ (where $\epsilon=0.1$), preventing overly aggressive updates that could destabilize long-form legal reasoning outputs.
- Core assumption: The supervised fine-tuned model provides a reasonable reference distribution for legal language.
- Evidence anchors: [abstract] "integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO)"; [section] "The clipping function ensures that the ratio does not deviate too far from 1... helps stabilize training"
- Break condition: If the SFT reference policy is poorly aligned with legal reasoning patterns, clipping to it constrains rather than helps learning.

### Mechanism 2
- Claim: AI-generated reward signals can substitute for human feedback to guide legal text alignment, but with fidelity limitations.
- Mechanism: Task-specific reward models assign scalar scores—binary correctness for CJPE (InLegalBERT classifier), n-gram overlap for summarization—which PPO optimizes toward.
- Core assumption: Reward models capture legally meaningful quality signals.
- Evidence anchors: [abstract] "challenges include reward model alignment, legal language complexity, and domain-specific adaptation"; [section] "Our reward model, based on InLegalBERT, may not fully capture the fine-grained reasoning and interpretative nuances of Indian legal texts"
- Break condition: Reward misalignment causes optimization toward surface patterns (n-gram matching) rather than legal correctness, leading to hallucinations.

### Mechanism 3
- Claim: Base model capacity and domain adaptation are rate-limiting factors for PPO effectiveness in legal domains.
- Mechanism: Larger, domain-aligned models (LLaMA-2-7B SFT) provide the representational foundation; PPO refines behavior but cannot compensate for insufficient legal knowledge.
- Core assumption: Domain-specific pretraining encodes legal reasoning patterns necessary for downstream RL.
- Evidence anchors: [abstract] "ablation studies show that base model choice and reward model alignment critically impact performance"; [section] "Phi-3 Mini... severely limited the model's capacity to handle long and intricate legal texts... smaller models or generic pretrained ones lack the representational and contextual capacity required"
- Break condition: Small or domain-unadapted base models fail regardless of PPO tuning quality.

## Foundational Learning

### Proximal Policy Optimization (PPO)
- Why needed here: Core algorithm for RL-based alignment; requires understanding of policy gradients, clipping, and KL constraints.
- Quick check question: Can you explain why clipping the probability ratio prevents catastrophic forgetting during RL fine-tuning?

### Reward Modeling for RLAIF
- Why needed here: The reward model is the sole learning signal; misalignment propagates through PPO optimization.
- Quick check question: What happens to PPO training if the reward model assigns high scores to hallucinated but fluent legal text?

### Legal NLP Domain Constraints
- Why needed here: Legal texts have long contexts, rhetorical structure, and require factual fidelity—standard NLP assumptions may not hold.
- Quick check question: Why might standard n-gram metrics (ROUGE, BLEU) fail to capture legal correctness in summaries?

## Architecture Onboarding
- Component map: Legal case documents (PredEx / In-Abs datasets) -> LLaMA-2-7B -> Supervised Fine-Tuning -> π_SFT -> InLegalBERT classifier (CJPE) or ROUGE-based scorer (summarization) -> PPO Optimizer -> Updates π_θ using clipped objective with reward signals -> Predicted judgment + explanation OR summary

- Critical path: SFT training quality -> Reward model alignment -> PPO hyperparameters (learning rate, batch size, clip ε)

- Design tradeoffs:
  - Larger reward models capture more nuance but slow training and may overfit
  - Higher KL penalty preserves SFT behavior but limits RL adaptation
  - Binary rewards for CJPE are simple but provide sparse learning signal

- Failure signatures:
  - ROUGE improves but factual hallucinations increase -> reward model favors surface overlap
  - Loss plateaus early -> learning rate too low or reward signal uninformative
  - Outputs become generic/legal-sounding but substantively wrong -> reward hacking

- First 3 experiments:
  1. Baseline comparison: Run SFT-only inference vs. PPO-aligned inference on held-out PredEx cases; measure both lexical metrics and manual hallucination rate.
  2. Reward model ablation: Swap InLegalBERT classifier for a task-finetuned variant; assess whether alignment improves on explanation quality.
  3. Hyperparameter sweep: Vary learning rate (1e-5 to 5e-5) and clip ε (0.1 to 0.2); track training stability and final ROUGE/BERTScore to identify stable operating region.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reward models be designed to penalize hallucinations and enforce factual consistency in PPO-based legal AI systems?
- Basis in paper: [explicit] Authors state that "RLHF methods such as PPO must be augmented with stronger factuality constraints, hallucination-aware reward models, or human-in-the-loop feedback when applied to high-stakes legal AI applications."
- Why unresolved: The current reward model only captures surface-level accuracy (binary prediction correctness or n-gram overlap), not factual fidelity to case documents.
- What evidence would resolve it: Development of a factuality-aware reward model that reduces hallucination rates while maintaining or improving ROUGE/BLEU scores on PredEx and In-Abs datasets.

### Open Question 2
- Question: Would domain-adaptive pretraining on Indian legal corpora before PPO alignment significantly improve performance on CJPE and summarization tasks?
- Basis in paper: [explicit] Authors identify "domain pretraining gap" as a factor in underperformance, noting GPT-3.5 benefits from "extensive pretraining and reinforcement with human feedback across multiple domains."
- Why unresolved: The base LLaMA-2-7B model lacks deep Indian legal domain knowledge; experiments used only task-specific fine-tuning without prior domain adaptation.
- What evidence would resolve it: Comparing ReGal trained on a domain-adapted model (e.g., INLegalLlama) against the current approach, showing improved lexical and semantic metrics.

### Open Question 3
- Question: What human-in-the-loop feedback mechanisms would most effectively improve PPO alignment for legal reasoning while remaining cost-effective?
- Basis in paper: [explicit] Authors state "PPO's reliance on the reward model, and lack of human-in-the-loop supervision, likely prevents the system from learning subtle legal distinctions."
- Why unresolved: RLAIF was used as a cost-saving alternative, but AI feedback may not capture nuanced legal reasoning patterns.
- What evidence would resolve it: A comparative study of human-in-the-loop vs. RLAIF reward signals, measuring gains in BERTScore and hallucination reduction.

### Open Question 4
- Question: Can reward models be designed to capture fine-grained legal reasoning beyond binary classification accuracy or n-gram overlap?
- Basis in paper: [inferred] The binary reward (0/1 for prediction correctness) and ROUGE-based summarization rewards fail to capture interpretative nuances of Indian legal texts.
- Why unresolved: Current reward models assign noisy or misaligned scores that impair PPO optimization for coherent legal outputs.
- What evidence would resolve it: A multi-dimensional reward model evaluating legal coherence, citation accuracy, and argument structure, demonstrating improved qualitative legal expert evaluations.

## Limitations
- Reward model misalignment is a critical limitation, as the InLegalBERT classifier may not capture fine-grained legal reasoning nuances
- Hallucinations are frequent in generated outputs, indicating the need for stronger factuality constraints
- Domain pretraining gap identified as major limitation, with base models lacking deep Indian legal domain knowledge

## Confidence
- **High**: Base model capacity limits, SFT vs. PPO performance comparison, dataset details
- **Medium**: Reward model limitations, hallucination concerns, ablation study implications
- **Low**: Task definition precision, exact reward model training procedure

## Next Checks
1. **Reward model audit**: Evaluate InLegalBERT classifier accuracy on a held-out subset of PredEx with human-labeled correctness. Compare reward distribution variance across correct/incorrect predictions.
2. **Hallucination measurement**: Implement factuality checks comparing generated legal citations/facts against source documents. Calculate hallucination rate per task.
3. **Hyperparameter sensitivity**: Run controlled experiments varying KL penalty coefficient (0.1, 1.0, 10.0) and learning rate (1e-5, 5e-5, 1e-4) while holding other factors constant. Document stability and performance trade-offs.