---
ver: rpa2
title: A Systematic Review on the Generative AI Applications in Human Medical Genomics
arxiv_id: '2508.20275'
source_url: https://arxiv.org/abs/2508.20275
tags:
- data
- genetic
- clinical
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzed 172 studies on large language model
  (LLM) applications in human medical genomics. Automated keyword-based search was
  conducted across PubMed, bioRxiv, medRxiv, and arXiv, followed by deduplication
  and semantic filtering.
---

# A Systematic Review on the Generative AI Applications in Human Medical Genomics

## Quick Facts
- arXiv ID: 2508.20275
- Source URL: https://arxiv.org/abs/2508.20275
- Authors: Anton Changalidis; Yury Barbitoff; Yulia Nasykhova; Andrey Glotov
- Reference count: 40
- Primary result: Analyzed 172 studies on LLM applications in human medical genomics, finding advances in variant interpretation and imaging analysis but persistent challenges in multimodal integration and clinical implementation.

## Executive Summary
This systematic review examines the applications of large language models (LLMs) and transformer-based architectures in human medical genomics, focusing on rare and common disease diagnostics. The review systematically classified 172 studies into three diagnostic pipeline stages: pre-analytical (knowledge navigation and risk stratification), analytical (medical imaging analysis, variant effect analysis, and clinical variant interpretation), and post-analytical (patient clustering, data aggregation, and clinical report generation). The analysis reveals that while transformer-based models significantly advance disease stratification, variant interpretation, and medical imaging analysis, major challenges persist in integrating multimodal data into clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings.

## Method Summary
The review conducted automated keyword-based searches across PubMed, bioRxiv, medRxiv, and arXiv, combining genetics/medicine terms with transformer-related keywords. After deduplication, semantic filtering using TF-IDF and regular expressions reduced ~57,000 articles to ~550, followed by manual screening to identify 172 relevant studies. The corpus was classified into pre-analytical, analytical, and post-analytical diagnostic stages, with specific focus on transformer-based model applications in genomic variant identification, annotation, interpretation, and medical imaging.

## Key Results
- Transformer models significantly advance disease stratification, variant interpretation, and medical imaging analysis in genomics
- Major challenges persist in integrating multimodal data (genomic, imaging, clinical text) into unified diagnostic pipelines
- Current implementations face limitations in generalizability and practical clinical deployment despite technical advances

## Why This Works (Mechanism)

### Mechanism 1: Transformer Attention Enables Contextual Variant Interpretation
- Claim: LLMs leverage attention mechanisms to integrate phenotypic descriptions with variant data, supporting causal variant prioritization.
- Mechanism: The transformer architecture encodes sequential context from both text (phenotype descriptions, clinical notes) and genomic sequences. Attention weights highlight relationships between phenotypic entities (HPO terms, clinical findings) and genomic variants (SNPs, structural variants), allowing the model to "focus" on relevant features for interpretation.
- Core assumption: Phenotypic information and genomic sequence context contain learnable patterns that map to causal variant relationships.
- Evidence anchors: [abstract] examines LLM role in genetic research and diagnostics; [section 3.1.2] describes Genetic Transformer (GeneT) using fine-tuned LLM for causative variant identification; [corpus] weak evidence on transformer-based multimodal integration.

### Mechanism 2: Vision Transformers Extract Genotype-Relevant Features from Medical Images
- Claim: Vision Transformers (ViTs) can predict genetic mutations or syndromes by treating image patches as sequential tokens and learning spatial relationships linked to genotype.
- Mechanism: ViTs segment medical images (histopathology, MRI, facial photos) into patches, embed them as token sequences, and apply transformer attention to capture long-range dependencies across the image. The learned representations are mapped to genetic labels through supervised learning.
- Core assumption: Visual features reliably correlate with specific genetic alterations.
- Evidence anchors: [abstract] discusses medical imaging advancements through vision transformers; [section 3.1.2] describes ViTs identifying genetic mutations from medical images; [corpus] "Multimodal Foundation Models for Early Disease Detection" supports broader multimodal integration principles.

### Mechanism 3: Retrieval-Augmented Generation (RAG) Mitigates Hallucination in Clinical Reporting
- Claim: RAG improves factual accuracy of generated clinical reports by grounding LLM outputs in retrieved external knowledge (e.g., literature, guidelines).
- Mechanism: A retriever component fetches relevant documents based on the input query. The LLM conditions its generation on both the query and retrieved context, reducing reliance on internal memorization and decreasing hallucinations.
- Core assumption: Retrieved external knowledge is accurate, up-to-date, and relevant to the specific clinical context.
- Evidence anchors: [abstract] highlights challenges in integrating multimodal data; [section 4.4] discusses RAG-based systems improving factual consistency; [corpus] weak evidence on RAG in medical genomics specifically.

## Foundational Learning

- **Concept: Transformer Attention Mechanism**
  - Why needed here: All reviewed applications rely on attention to model relationships in sequential or tokenized data (DNA, text, image patches).
  - Quick check question: Can you explain, in simple terms, how attention allows a model to "focus" on different parts of a DNA sequence or clinical text simultaneously?

- **Concept: Multimodal Data Integration**
  - Why needed here: The review emphasizes the challenge and potential of combining genomic, imaging, and clinical text data into unified diagnostic pipelines.
  - Quick check question: Why is it more difficult to train a model on both MRI scans and genomic variants than on text alone?

- **Concept: Fine-Tuning vs. Few-Shot Learning**
  - Why needed here: The review notes that LLMs can be adapted to specialized genetics tasks via fine-tuning or prompt-based few-shot learning, each with trade-offs.
  - Quick check question: What is the primary difference between adapting an LLM by fine-tuning on a domain dataset versus designing a few-shot prompt?

## Architecture Onboarding

- **Component map:**
  - Input Layer (tokenizers for DNA, image patch embedders, text tokenizers) -> Encoder/Backbone (transformer encoder/decoder) -> Task-Specific Heads (classification, regression, decoder layers) -> Knowledge Retrieval (RAG module, optional) -> Output (predictions or reports)

- **Critical path:**
  1. Data preprocessing and tokenization aligned to the specific modality (sequence, image, text)
  2. Transformer backbone pre-training or loading (e.g., domain-specific foundation model like Nucleotide Transformer)
  3. Task-specific fine-tuning or prompt engineering on labeled clinical data
  4. Integration of retrieval component if using RAG
  5. Validation on held-out clinical cohorts to assess generalizability

- **Design tradeoffs:**
  - Encoder-only vs. Decoder-only: Encoder-only models (e.g., BERT) are better for extraction/classification; decoder-only (e.g., GPT) are better for generation/summarization. Choice depends on whether the task is interpretive (variant classification) or generative (report writing).
  - General vs. Domain-Specific Pre-training: Domain-specific pre-training can improve performance but requires significant compute and data. Fine-tuning can be effective even with limited task-specific data.
  - Single-Modal vs. Multimodal: Multimodal integration offers richer context but increases complexity, data requirements, and risk of failure if one modality is noisy or missing.

- **Failure signatures:**
  - Hallucination in generated reports: LLM generates plausible-sounding but incorrect clinical statements. Mitigate with RAG or human-in-the-loop validation.
  - Poor generalization to new populations: Model trained on one demographic or clinical setting fails on another. Mitigate with diverse training data and external validation.
  - Attention degradation with long sequences: Very long DNA sequences or complex images may exceed model context window or lead to diffuse attention patterns. Mitigate with efficient attention variants (e.g., sparse attention) or hierarchical processing.

- **First 3 experiments:**
  1. **Baseline Variant Classification:** Fine-tune a pre-trained genomic LLM (e.g., Nucleotide Transformer) on a labeled dataset of pathogenic/benign variants. Evaluate on an independent clinical cohort to measure accuracy, precision, and recall.
  2. **ViT for Syndrome Detection:** Train a Vision Transformer on a dataset of facial images labeled with genetic syndromes. Test on a held-out set to assess if visual features can predict syndrome class, and compare to CNN baselines.
  3. **RAG-Enhanced Report Generation:** Implement a RAG pipeline where an LLM generates a clinical summary conditioned on a patient's variant list and retrieved literature. Evaluate factual accuracy (e.g., via expert review) and compare to non-RAG generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific frameworks or fusion techniques are required to effectively integrate heterogeneous multimodal data (genomic sequences, imaging, and clinical records) into unified, clinically robust diagnostic pipelines?
- Basis in paper: [explicit] The abstract and conclusion explicitly state that "major challenges persist in integrating multimodal data... into unified and clinically robust pipelines."
- Why unresolved: Current approaches often struggle with generalizability across different data types, and the paper notes that practical implementation in clinical settings remains limited despite technical advancements.
- What evidence would resolve it: A comparative study benchmarking different multimodal fusion architectures (e.g., cross-attention vs. graph-based integration) on a standardized dataset containing genomic, imaging, and clinical data to determine the most robust approach.

### Open Question 2
- Question: How does the architectural choice between encoder-only (e.g., BERT) and decoder-only (e.g., GPT) models affect performance trade-offs in structured entity extraction versus generative interpretation tasks in genomics?
- Basis in paper: [inferred] Section 4.4 ("Model Strategies") notes that using decoder-only models for extraction tasks may be "suboptimal" despite their popularity, suggesting a lack of clarity on best practices for specific genomic tasks.
- Why unresolved: The review highlights a rising trend of using generative models for all tasks, but lacks conclusive evidence on whether this compromises the precision required for structured extraction compared to encoder-based methods.
- What evidence would resolve it: A rigorous benchmarking study comparing state-of-the-art encoder-based models against decoder-based models on identical genomic entity extraction and relation extraction tasks.

### Open Question 3
- Question: How can non-determinism and model drift be systematically detected and managed to ensure the safety of LLM applications in clinical variant interpretation and decision support?
- Basis in paper: [explicit] Section 3.1.1 warns that "nondeterminism and model drift must be addressed before deployment in diagnostic settings."
- Why unresolved: The stochastic nature of LLMs combined with frequent updates creates variability in outputs that is currently incompatible with the consistency required for clinical decision-making.
- What evidence would resolve it: Development and validation of automated monitoring tools that track output consistency and accuracy across model versions and prompt iterations using established clinical ground truths.

## Limitations
- The review relies entirely on published literature and preprints, potentially missing unpublished negative results or commercial applications.
- Semantic filtering depends on keyword matching that may exclude relevant studies using different terminology.
- The review synthesizes methodological claims without independent validation of the underlying models or datasets.

## Confidence
- Classification of applications into diagnostic stages: High
- Observation that transformer models advance variant interpretation and imaging analysis: High
- Claims about practical implementation challenges and generalizability: Medium
- Quantitative performance assessments of specific models: Low

## Next Checks
1. Replicate the systematic search using the exact keyword lists and filtering criteria to verify the 172-study count and classification
2. Extract quantitative performance metrics (accuracy, precision, recall) from the primary studies to assess the claimed advances in variant interpretation and imaging analysis
3. Contact authors of representative studies to verify implementation details and assess whether reported results generalize across different patient populations and clinical contexts