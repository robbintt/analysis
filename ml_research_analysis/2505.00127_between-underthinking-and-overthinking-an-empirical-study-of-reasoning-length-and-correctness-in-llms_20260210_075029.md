---
ver: rpa2
title: 'Between Underthinking and Overthinking: An Empirical Study of Reasoning Length
  and correctness in LLMs'
arxiv_id: '2505.00127'
source_url: https://arxiv.org/abs/2505.00127
tags:
- length
- reasoning
- questions
- responses
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies the relationship between reasoning
  length and answer correctness in large language models. It finds that models tend
  to overthink simple problems, generating unnecessarily long outputs, while underthinking
  harder ones, failing to extend reasoning when needed.
---

# Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs

## Quick Facts
- arXiv ID: 2505.00127
- Source URL: https://arxiv.org/abs/2505.00127
- Reference count: 18
- Primary result: Accuracy exhibits non-monotonic relationship with reasoning length; incorrect responses are 2-3x longer than correct ones.

## Executive Summary
This paper empirically studies how reasoning length affects answer correctness in large language models, revealing that models often misjudge problem difficulty by overthinking simple problems while underthinking harder ones. The study demonstrates a non-monotonic relationship between accuracy and reasoning length—improving up to an optimal point then declining—and shows that incorrect responses are significantly longer than correct ones. Using length-only preference optimization, the authors achieve 30-60% token reduction while maintaining acceptable accuracy without ground-truth supervision.

## Method Summary
The authors analyze two DeepSeek reasoning models (1.5B parameters) on GSM8K and MATH benchmarks. For each question, they generate N=10 samples with temperature=1.0 and rank them by length to compute accuracy at each rank. They classify questions by difficulty (Easy=10/10 correct, Medium=mixed, Hard=0/10 correct) and measure average token lengths for correct vs incorrect responses. For length optimization, they use Simple Preference Optimization (SimPO) with preference pairs preferring shorter responses, training on unlabeled data with β=2, γ=1 hyperparameters.

## Key Results
- Accuracy vs. reasoning length shows non-monotonic relationship: improves up to optimal rank then declines
- Incorrect responses are 2-3x longer than correct ones on average
- Length-only preference optimization reduces tokens by 30-60% while maintaining accuracy
- Models successfully increase length on problems where they have advantage but fail to extend reasoning on problems beyond their capability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Accuracy exhibits non-monotonic relationship with reasoning length—improving up to optimal point then declining.
- **Mechanism:** Excessive reasoning chains introduce error accumulation and irrelevant information that derail correct reasoning paths.
- **Core assumption:** Decline stems from compounding small mistakes rather than fundamental model limitations.
- **Evidence anchors:** Abstract confirms non-monotonic accuracy-length relationship; Section 4.1 shows accuracy improves then declines with increasing length; corpus papers document overthinking phenomenon.
- **Break condition:** When max token limits truncate reasoning or excessive steps introduce contradictions.

### Mechanism 2
- **Claim:** Models possess asymmetric difficulty recognition—successfully adapting length for easy problems but failing to extend reasoning for problems beyond capability.
- **Mechanism:** Models can calibrate response length within knowledge boundaries but lack self-awareness to recognize when problems exceed capabilities.
- **Core assumption:** Failure reflects deficient metacognition rather than deliberate resource conservation.
- **Evidence anchors:** Abstract notes overthinking simple problems while underthinking harder ones; Section 5.1 identifies difficulty awareness challenges; corpus papers identify underthinking/overthinking as dual failure modes.
- **Break condition:** When perplexity remains low despite high difficulty or when length distributions for zero-accuracy questions spread broadly.

### Mechanism 3
- **Claim:** Length-only preference optimization can reduce token generation by 30-60% while maintaining acceptable accuracy without ground-truth supervision.
- **Mechanism:** Since incorrect responses are significantly longer than correct ones, preferring shorter responses implicitly penalizes incorrect reasoning chains.
- **Core assumption:** Correlation between incorrectness and verbosity holds across domains beyond mathematics.
- **Evidence anchors:** Abstract confirms shorter responses maintain acceptable accuracy; Section 6 shows reduction driven by shortening incorrect responses; corpus papers use length rewards with accuracy constraints.
- **Break condition:** When correct answers require extended reasoning and shortening truncates necessary computation.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - **Why needed here:** Entire paper analyzes how CoT length affects correctness; understanding that reasoning models generate intermediate steps before final answers is prerequisite.
  - **Quick check question:** Can you explain why generating reasoning steps before an answer might improve performance on mathematical problems?

- **Concept: Preference Optimization (SimPO/DPO)**
  - **Why needed here:** Section 6 uses Simple Preference Optimization to reduce length; understanding how models learn from preference pairs without explicit rewards is essential.
  - **Quick check question:** How does preference optimization differ from supervised fine-tuning with labeled examples?

- **Concept: Sample vs. Question-Level Analysis**
  - **Why needed here:** Paper distinguishes between analyzing multiple samples for one question versus analyzing across questions of varying difficulty.
  - **Quick check question:** Why would you expect different conclusions when analyzing length-correctness relationships at sample level versus question level?

## Architecture Onboarding

- **Component map:** Data Generation Module -> Ranking Module -> Difficulty Classification -> Cross-model comparison -> SimPO Training Pipeline

- **Critical path:** Data sampling → Length ranking → Accuracy-at-rank computation → Difficulty stratification → Cross-model comparison → Preference pair construction → SimPO training

- **Design tradeoffs:**
  - N=10 samples balances statistical reliability against compute cost
  - Unlabeled-data training sacrifices accuracy guarantees for scalability and broader applicability
  - Length-only preference ignores correctness, relying on correlation between brevity and accuracy

- **Failure signatures:**
  - Overthinking: Correct shortest response exists but model generates 2-3x longer chains with declining accuracy
  - Underthinking: Zero-accuracy questions show broadly distributed lengths rather than maximum-length clustering
  - Overconfidence: Low perplexity on high-difficulty questions

- **First 3 experiments:**
  1. **Replicate sample-level analysis:** Generate 10 samples per question, rank by length, plot Accr vs. r to verify non-monotonic relationship and identify optimal rank r*
  2. **Cross-model difficulty analysis:** Compare two models of different sizes on shared-easy and model-specific-easy sets, measuring whether length increases on advantage sets indicate difficulty recognition
  3. **Length-only SimPO ablation:** Train with varying preference pair constructions, measure accuracy-length tradeoffs to isolate whether reduction comes from correct or incorrect response shortening

## Open Questions the Paper Calls Out

- **Open Question 1:** Do overthinking and underthinking behaviors emerge primarily from pretraining or post-training processes such as SFT or RL? [Section 8 states need to clarify whether patterns emerge primarily from pretraining or post-training processes.]

- **Open Question 2:** How does a model's internal assessment of problem difficulty correlate with the success of its self-correction mechanisms? [Section 8 suggests effectiveness of self-critique may depend on ability to recognize when problems are within capabilities.]

- **Open Question 3:** Can incorporating heuristics that deprioritize excessively long generations improve accuracy-to-efficiency trade-off in test-time aggregation strategies like self-consistency? [Section 8 proposes incorporating simple heuristics—e.g., deprioritizing excessively long generations—may improve both accuracy and efficiency.]

## Limitations

- Findings constrained by using only two small models (1.5B parameters) and two mathematical benchmarks
- Generalization to larger models, non-mathematical domains, and broader reasoning tasks remains uncertain
- SimPO methodology relies on unlabeled data and length-only supervision, potentially missing semantic quality differences

## Confidence

- **High confidence:** Incorrect responses are significantly longer than correct ones (2-3x) across both models and datasets
- **Medium confidence:** Non-monotonic relationship between accuracy and reasoning length, as this requires precise identification of optimal rank r*
- **Medium confidence:** Length-based preference optimization's effectiveness in reducing tokens by 30-60% while maintaining accuracy

## Next Checks

1. Apply sample-level analysis to non-mathematical reasoning tasks (commonsense QA, code generation) to verify whether overthinking/underthinking patterns persist beyond mathematical reasoning

2. Repeat analysis with larger models (7B-70B parameters) to determine if length-correctness relationship strengthens, weakens, or inverts as model capacity increases

3. Implement human evaluation or automated semantic similarity metrics to compare length-optimized responses against ground-truth reasoning chains, ensuring token reduction doesn't sacrifice logical coherence or mathematical validity