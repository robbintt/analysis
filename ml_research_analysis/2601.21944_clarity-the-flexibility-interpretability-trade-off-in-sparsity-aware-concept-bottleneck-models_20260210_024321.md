---
ver: rpa2
title: 'Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept
  Bottleneck Models'
arxiv_id: '2601.21944'
source_url: https://arxiv.org/abs/2601.21944
tags:
- color
- concept
- sparsity
- bernoulli
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the interpretability challenge in Concept\
  \ Bottleneck Models (CBMs) by introducing a new metric called \"clarity\" that captures\
  \ the trade-off between downstream performance, sparsity, and precision of learned\
  \ concept representations. The authors propose an amortized approach for sparsity-aware\
  \ concept selection using three different formulations: \u21131, \u21130, and Bernoulli-based\
  \ methods."
---

# Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models

## Quick Facts
- arXiv ID: 2601.21944
- Source URL: https://arxiv.org/abs/2601.21944
- Reference count: 15
- One-line primary result: Introduces clarity metric to measure interpretability in sparsity-aware concept bottleneck models, showing Bernoulli-based sparsity outperforms ℓ1 and ℓ0 methods.

## Executive Summary
This paper addresses the interpretability challenge in Concept Bottleneck Models (CBMs) by introducing a new metric called "clarity" that captures the trade-off between downstream performance, sparsity, and precision of learned concept representations. The authors propose an amortized approach for sparsity-aware concept selection using three different formulations: ℓ1, ℓ0, and Bernoulli-based methods. Through extensive experiments on CUB and SUN datasets, they demonstrate that sparsity-aware methods can exhibit markedly different behaviors even at comparable accuracy levels. The results show that achieving strong task performance does not guarantee interpretable representations, with the Bernoulli-based method performing favorably in terms of clarity. The paper highlights the importance of explicitly evaluating interpretability metrics beyond just sparsity and task performance when comparing sparsity-aware methods.

## Method Summary
The proposed approach introduces an amortized sparsity-aware concept selection mechanism for CBMs. Three sparsity formulations are explored: ℓ1 regularization (sigmoid transformation of logits), ℓ0 regularization (Hard Concrete distribution), and Bernoulli-based selection (variational inference with Concrete relaxation). A shared amortization matrix W_s maps image embeddings to per-concept selection logits, which are transformed via method-specific transformations to produce binary masks Z that gate concept contributions during classification. The clarity metric, defined as the harmonic mean of accuracy, sparsity, and precision, captures the interpretability trade-off. Experiments are conducted on CUB and SUN datasets using both VLM-based concept scores and predictor-based backbones.

## Key Results
- The clarity metric reveals that methods achieving similar accuracy can have vastly different interpretability profiles
- Bernoulli-based sparsity formulation achieves highest clarity (0.686-0.717) in predictor-based settings
- Strong task performance does not guarantee interpretable representations - models can achieve high accuracy with low clarity
- VLM-based concept scores are weak for CUB (16-32% mAP) but acceptable for SUN (61-74% mAP)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-example sparse concept selection via amortized inference enables interpretable representations without requiring ground-truth concept supervision during deployment.
- Mechanism: A shared amortization matrix W_s ∈ R^{K×M} maps image embeddings to per-concept selection logits. These logits are transformed via method-specific transformations (sigmoid for ℓ1, Hard Concrete sampling for ℓ0, Bernoulli probabilities for Bernoulli) to produce binary masks Z that gate concept contributions during classification.
- Core assumption: The downstream classification signal alone is sufficient to learn meaningful concept selectors that align with semantically correct concepts.
- Evidence anchors:
  - [abstract] "propose an amortized approach for sparsity-aware concept selection using three different formulations"
  - [section 4, Eq. 16-19] Details the transformation of embeddings through W_s to produce Z for each method
  - [corpus] Weak direct support; neighbor papers focus on CBM accuracy-interpretability trade-offs but not the amortized selection mechanism specifically
- Break condition: If concepts required for classification have no correspondence to ground-truth semantics, the amortized selector may learn spurious correlations that appear sparse but are not interpretable.

### Mechanism 2
- Claim: The clarity metric (harmonic mean of accuracy, sparsity, and precision) captures interpretability more faithfully than any individual component.
- Mechanism: Clarity = 3·Acc·Sparsity·Prec / (Acc·Sparsity + Acc·Prec + Sparsity·Prec). The harmonic mean structure ensures that poor performance on any single dimension cannot be compensated by strong performance on others.
- Core assumption: High precision of active concepts relative to ground truth is a necessary condition for interpretability.
- Evidence anchors:
  - [abstract] "measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation"
  - [section 4, Eq. 23] Formal definition of clarity metric
  - [corpus] "Quantifying the Accuracy-Interpretability Trade-Off" paper addresses similar trade-off quantification in CBMs
- Break condition: If ground-truth annotations are unavailable or unreliable, precision cannot be computed and clarity cannot be evaluated.

### Mechanism 3
- Claim: Probabilistic sparsity priors (Bernoulli-based) yield higher concept precision than deterministic regularization (ℓ1, ℓ0) at comparable accuracy levels.
- Mechanism: The Bernoulli formulation imposes an explicit sparsity-inducing prior p(z|π̃) and optimizes the ELBO, jointly inferring parameters θ and binary selectors z. This provides principled uncertainty quantification compared to penalty-based approaches.
- Core assumption: The sparsity prior encourages selection of only the most discriminative concepts, which tend to be semantically meaningful.
- Evidence anchors:
  - [abstract] "the Bernoulli-based method performing favorably in terms of clarity"
  - [section 6, Table 2] Bernoulli achieves highest clarity (0.686-0.717) in predictor-based settings
  - [corpus] Limited direct comparison; corpus does not contain comparative studies of sparsity formulations
- Break condition: If the prior sparsity level π̃ is misspecified relative to the true number of relevant concepts, the method may either over-sparsify (missing relevant concepts) or under-sparsify (including irrelevant ones).

## Foundational Learning

- Concept: **Concept Bottleneck Models (CBMs)**
  - Why needed here: The entire framework operates on CBM architectures where predictions must pass through a human-interpretable concept layer before classification.
  - Quick check question: Can you explain why forcing predictions through a bottleneck layer trades off accuracy for interpretability?

- Concept: **ℓ0 Regularization via Hard Concrete Distribution**
  - Why needed here: One of the three sparsity methods uses the Hard Concrete distribution to create differentiable approximations of discrete sparsity.
  - Quick check question: Why can't ℓ0 regularization be optimized directly via gradient descent, and how does the Hard Concrete distribution address this?

- Concept: **Variational Inference with Mean-Field Approximation**
  - Why needed here: The Bernoulli-based method uses variational inference to optimize the ELBO for joint parameter and latent variable learning.
  - Quick check question: What does the KL divergence term in the ELBO encourage regarding the sparsity of learned representations?

## Architecture Onboarding

- Component map:
  - Input: Image X → VLM encoder E_I(X) ∈ R^K
  - Concept Score Branch: E_I(X) → E_T(A)^T (VLM-based) OR W_pred^T E_I(X) (predictor-based) → concept scores ∈ R^M
  - Selection Branch: E_I(X) → W_s^T → method-specific transformation → binary mask Z ∈ {0,1}^M
  - Classification: Z · W_c^T · concept_scores → class predictions
  - Loss: Classification loss (CE) + sparsity penalty (method-dependent)

- Critical path: The amortization matrix W_s is the core learned component for interpretability. During training, W_s learns to predict which concepts are relevant per example using only the classification signal. During inference, threshold τ on Z determines active concepts.

- Design tradeoffs:
  - VLM-based vs. predictor-based: VLM requires no concept supervision but yields lower attribute prediction mAP (16-32% vs. 61-74%); predictor-based requires labeled concepts.
  - Backbone size (ViT-B/16 vs. ViT-L/14): Larger backbone provides modest accuracy gains but limited precision improvement.
  - Sparsity level vs. accuracy: Higher sparsity generally improves precision but may reduce accuracy; clarity captures this trade-off.

- Failure signatures:
  - High accuracy but low clarity: Model uses many non-ground-truth concepts (information leakage through bottleneck).
  - High sparsity but low precision: Selected concepts are sparse but semantically incorrect.
  - Binary accuracy misleading: High binary accuracy despite low precision in sparse settings (predicting many inactive concepts correctly inflates score).

- First 3 experiments:
  1. **Baseline attribute prediction**: Train predictor on ground-truth concepts; compute mAP and AUC to quantify backbone quality before sparsity experiments.
  2. **Sparsity sweep**: For each method (ℓ1, ℓ0, Bernoulli), train with varying λ values, freeze W_s, then retrain W_c across threshold range τ to generate accuracy-sparsity-precision curves.
  3. **Clarity comparison**: Select best model per method based on maximum clarity; compare accuracy, sparsity, precision, and binary accuracy to demonstrate that methods achieving similar accuracy can have vastly different interpretability profiles.

## Open Questions the Paper Calls Out

- **Question**: Can clarity be effectively approximated in datasets without ground-truth concept annotations, enabling broader application to real-world settings?
  - Basis in paper: [explicit] The authors state in Limitations: "A limitation of the proposed pipeline is its reliance on ground-truth concept annotations to evaluate clarity."
  - Why unresolved: The clarity metric requires ground-truth annotations to compute precision, restricting use to annotated benchmarks. No alternative is proposed.
  - What evidence would resolve it: Demonstrating an unsupervised proxy for precision that correlates strongly with ground-truth-based precision across diverse datasets.

- **Question**: How do more informative priors affect the interpretability and performance of Bernoulli-based sparsity-aware CBMs?
  - Basis in paper: [explicit] The authors state: "A promising direction for future work is the use of more informative priors, particularly in the Bernoulli-based formulation, where approximate individual or aggregate concept presence scores... can guide sparsification and improve interpretability."
  - Why unresolved: Only a fixed sparse prior (10^-4) is explored; leveraging approximate concept presence remains uninvestigated.
  - What evidence would resolve it: Experiments comparing fixed priors to informative priors derived from aggregate concept statistics, measuring clarity, accuracy, and sparsity improvements.

- **Question**: Does higher clarity correlate with improved human interpretability and trust in model explanations?
  - Basis in paper: [inferred] Clarity is proposed as an interpretability measure, but no human evaluation validates whether high-clarity models are genuinely more interpretable to users.
  - Why unresolved: The paper assumes sparsity, precision, and accuracy jointly capture interpretability without empirical human validation.
  - What evidence would resolve it: Human studies where participants rate explanation quality and helpfulness for models with varying clarity scores.

- **Question**: How does information leakage in the concept bottleneck affect the reliability of clarity as an interpretability metric?
  - Basis in paper: [inferred] Leakage is discussed as a major problem in CBMs, but the paper does not examine whether high-clarity models might still exploit leakage through non-interpretable representations.
  - Why unresolved: Models could achieve high accuracy and precision through leakage, misleading the clarity assessment.
  - What evidence would resolve it: Analysis measuring clarity before and after applying leakage detection/correction methods, or experiments showing whether high-clarity models exhibit less leakage.

## Limitations
- The clarity metric requires ground-truth concept annotations, limiting applicability to datasets without such annotations
- Performance may be sensitive to the quality of VLM concept scores, which are shown to be weak for CUB dataset
- The superiority of Bernoulli method could be sensitive to specific prior sparsity parameter settings

## Confidence

- **High**: The amortized sparsity selection mechanism and clarity metric formulation are clearly specified and reproducible.
- **Medium**: The comparative performance results across methods, particularly the Bernoulli method's advantage, are credible given the experimental setup but may not generalize to datasets with different concept distributions.
- **Low**: Claims about the mechanisms by which different sparsity formulations affect interpretability beyond what's captured by the clarity metric.

## Next Checks
1. Test clarity metric robustness by evaluating on datasets with varying ground-truth annotation quality to determine how sensitive interpretability assessment is to annotation reliability.
2. Conduct ablation studies on sparsity prior parameters (π̃ for Bernoulli, λ for ℓ1/ℓ0) to understand sensitivity of interpretability-performance trade-offs.
3. Evaluate whether methods that perform well on clarity also maintain interpretability when applied to out-of-distribution concepts or in few-shot learning scenarios.