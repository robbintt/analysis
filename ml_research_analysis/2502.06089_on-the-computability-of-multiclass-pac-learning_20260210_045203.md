---
ver: rpa2
title: On the Computability of Multiclass PAC Learning
arxiv_id: '2502.06089'
source_url: https://arxiv.org/abs/2502.06089
tags:
- computable
- dimension
- learnability
- learning
- cpac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes a meta-characterization of computable multiclass\
  \ PAC (CPAC) learnability by showing that the finiteness of computable distinguishers\
  \ characterizes learnability for finite label spaces. The authors first define computable\
  \ versions of the Natarajan dimension and more general computable \u03A8-dimensions\
  \ based on families of functions mapping label spaces to {0,1,}."
---

# On the Computability of Multiclass PAC Learning

## Quick Facts
- arXiv ID: 2502.06089
- Source URL: https://arxiv.org/abs/2502.06089
- Reference count: 10
- Primary result: Finite computable Natarajan dimension characterizes CPAC learnability for finite label spaces

## Executive Summary
This paper establishes a complete characterization of computable multiclass PAC (CPAC) learnability by showing that the finiteness of computable distinguishers determines learnability when label spaces are finite. The authors develop a computable version of the Natarajan dimension using witness functions and prove this dimension is both necessary and sufficient for CPAC learnability in the finite label case. They generalize this to any distinguisher family Ψ, showing that c-Ψ-dim(H) < ∞ iff H is CPAC learnable when Ψ is a valid distinguisher. The work builds on previous results about computable dimensions in binary classification and extends them to the multiclass setting.

## Method Summary
The authors define computable versions of classical shattering-based dimensions by introducing k-witness functions that constructively prove when sets cannot be shattered. For the Natarajan dimension, they show that a computable k-witness exists iff the class has finite computable Natarajan dimension. They then prove necessity by constructing a computable No-Free-Lunch argument where the learner itself serves as witness. For sufficiency, they embed the original hypothesis class into an extended class with computable empirical risk minimization while bounding the distinguisher dimension. The approach leverages computable versions of classical results from statistical learning theory, adapted to the computational constraints of CPAC learning.

## Key Results
- Finiteness of computable Natarajan dimension (c-N(H)) is necessary and sufficient for CPAC learnability when label space is finite
- For any distinguisher family Ψ, c-Ψ-dim(H) < ∞ characterizes CPAC learnability if and only if Ψ is a distinguisher
- The DS dimension cannot be expressed as a distinguisher, suggesting infinite label spaces require different techniques
- The separation between c-N(H) and c-G(H) can be arbitrarily large, showing the importance of dimension choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finite computable Natarajan dimension (c-N(H)) characterizes CPAC learnability for finite label spaces.
- Mechanism: A computable k-witness function w_N takes a set S of size k+1 and two labelings g₁,g₂, then outputs a subset I such that no hypothesis h∈H can match the pattern. This provides a constructive proof that no set larger than k can be N-shattered.
- Core assumption: The hypothesis class H is decidably representable (DR) or recursively enumerably representable (RER).
- Evidence anchors:
  - [abstract] "The key idea is to define a computable witness function that can prove when a set cannot be shattered, leading to the computable Natarajan dimension."
  - [Section 3, Definition 12] Formal definition of computable Natarajan dimension via k-witness.
  - [corpus] Related work on effective VC dimension (Sterkenburg 2022, Delle Rose et al. 2023) provides precedent for computable dimension characterizations in binary case.
- Break condition: Fails when label space Y is infinite (Lemma 27's bound includes |Y| in denominator; ERM computability requires finite Y).

### Mechanism 2
- Claim: Finiteness of c-N(H) is necessary for CPAC learnability even for infinite label spaces.
- Mechanism: A computable No-Free-Lunch argument (Lemma 17) constructs, for any computable learner A and set X of size 2m, a distribution where the learner fails. The learner itself becomes the witness function—its failure pattern proves non-shatterability.
- Core assumption: The learner is computable and outputs total computable functions.
- Evidence anchors:
  - [Section 3.1, Theorem 16] "Let H ⊆ Y^X be improperly CPAC learnable. Then c-N(H) < ∞."
  - [Section 3.1, Lemma 17] Multiclass analogue of computable No-Free-Lunch theorem.
  - [corpus] Weak direct corpus evidence; this extends binary CPAC techniques (Agarwal et al. 2020) to multiclass.
- Break condition: Does not guarantee sufficiency for infinite Y—only establishes necessity.

### Mechanism 3
- Claim: Distinguishers provide a meta-characterization: c-Ψ-dim(H) < ∞ iff H is CPAC learnable (for finite Y).
- Mechanism: A distinguisher Ψ is a family of functions Y→{0,1,*} that can distinguish any pair of distinct labels. The computable Ψ-dimension uses a witness function returning a 0-1 encoding unachievable by H. The "good functions" construction embeds H into H' with computable ERM while bounding Ψ-dim(H') ≤ k+1.
- Core assumption: Ψ must be a distinguisher; Y must be finite.
- Evidence anchors:
  - [Section 4.3, Theorem 33] "c-Ψ-dim(H) qualitatively characterizes CPAC learnability if and only if Ψ is a distinguisher."
  - [Section 4.2, Theorem 28] Sufficiency proof via embedding into H' with computable ERM.
  - [corpus] Ben-David et al. (1992) established distinguishers for non-computable PAC; this paper extends to CPAC.
- Break condition: DS dimension cannot be expressed as a distinguisher (Lemma 34), suggesting infinite label space CPAC requires different techniques.

## Foundational Learning

### CPAC Learning Framework (Agarwal et al. 2020)
- Why needed here: The entire paper operates within this framework where both learners AND their output hypotheses must be computable. Standard PAC results don't transfer.
- Quick check question: Can you explain why a class that is PAC learnable might fail to be CPAC learnable?

### Natarajan Dimension
- Why needed here: This is the starting point for multiclass complexity. The paper makes it "computable" via witness functions.
- Quick check question: How does N-shattering differ from VC-shattering? (Answer: uses two labelings g₁,g₂ per point rather than binary labels)

### Distinguishers (Ben-David et al. 1992)
- Why needed here: These provide the unifying framework capturing both Natarajan and graph dimensions as special cases of label embeddings into {0,1,*}.
- Quick check question: Why must a distinguisher map every distinct label pair to different non-* values for at least one ψ?

## Architecture Onboarding

### Component map
- k-witness function w → computable Natarajan dimension c-N(H)
- Distinguisher family Ψ → computable Ψ-dimension c-Ψ-dim(H)
- "Good functions" G → extended hypothesis class H' with computable ERM
- Function v(T) → computes all behaviors of H'|T for ERM implementation

### Critical path
1. Verify your hypothesis class H is RER (recursively enumerable representation exists)
2. Construct/verify a computable k-witness for Natarajan dimension
3. If c-N(H) = k < ∞ and |Y| < ∞, H is CPAC learnable via the embedding construction

### Design tradeoffs
- Finite vs infinite Y: Results only hold for finite label spaces. For infinite Y with computably bounded label ranges (Observation 20), sufficiency may still hold
- Proper vs improper: Results apply to improper learning; proper CPAC has stricter requirements
- The gap c-G(H) - c-N(H) can be arbitrarily large (Proposition 15), so choice of dimension matters for tight bounds

### Failure signatures
- If you cannot construct a computable witness for any finite k, the class is not CPAC learnable
- If Ψ fails to be a distinguisher (cannot separate some label pair), c-Ψ-dim < ∞ does NOT imply learnability (Proposition 32)
- DS dimension cannot be captured by distinguishers—infinite Y requires fundamentally different approach

### First 3 experiments
1. **Witness verification**: Given a hypothesis class H and claimed k-witness w, verify w(X, g₁, g₂) ∉ patterns achievable by H|X for all valid inputs of size k+1
2. **Embedding construction**: For a class with known c-N(H) = k, explicitly construct the "good functions" G and verify H' = H ∪ G has computable ERM
3. **Distinguisher test**: Given a family Ψ, verify it distinguishes all label pairs (necessary before c-Ψ-dim can characterize learnability)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What dimension characterizes CPAC learnability when the label space Y is infinite?
- Basis in paper: [explicit] The paper concludes that "characterizing CPAC learnability for infinite label spaces will potentially require significantly different techniques" since DS dimension cannot be expressed through distinguishers.
- Why unresolved: The meta-characterization via distinguishers fundamentally relies on Y being finite to implement computable ERM; the DS dimension (which characterizes standard PAC learnability for infinite Y) provably cannot be captured by the distinguisher framework.
- What evidence would resolve it: A computable version of the DS dimension, or an entirely new dimension concept, that is both necessary and sufficient for multiclass CPAC learnability with infinite labels.

### Open Question 2
- Question: Can the computable Natarajan dimension provide quantitative bounds on sample complexity, beyond the qualitative characterization of learnability?
- Basis in paper: [inferred] The paper focuses on qualitative characterization (finiteness implies learnability). The standard Natarajan dimension provides quantitative bounds, but whether c-N(H) similarly appears in sample complexity bounds is not addressed.
- Why unresolved: The construction of H′ in the sufficiency proof may introduce overhead that obscures the relationship between c-N(H) and optimal sample complexity.
- What evidence would resolve it: Proving upper and lower sample complexity bounds that explicitly depend on c-N(H).

### Open Question 3
- Question: Is there a separation between realizable and agnostic multiclass CPAC learnability, analogous to the binary case?
- Basis in paper: [inferred] The paper notes that binary CPAC learning exhibits a separation between realizable and agnostic settings, but only characterizes the agnostic setting for multiclass CPAC learning.
- Why unresolved: The necessary condition (Theorem 16) is proved for agnostic CPAC learning; whether different dimensions characterize realizable multiclass CPAC learning remains open.
- What evidence would resolve it: Either a proof that the same characterization holds, or an explicit hypothesis class that is realizable CPAC learnable but not agnostic CPAC learnable.

### Open Question 4
- Question: Can Observation 20's computable bound condition be weakened while still guaranteeing CPAC learnability for infinite Y?
- Basis in paper: [explicit] Observation 20 states that if labels are computably bounded (H|[n] ⊆ [c(n)][n] for computable c), then finite c-N(H) implies CPAC learnability even for infinite Y.
- Why unresolved: The condition captures specific infinite-label settings but may be stronger than necessary; the exact boundary of CPAC learnability for infinite Y remains unclear.
- What evidence would resolve it: Identifying a hypothesis class with infinite Y, finite c-N(H), no computable bound function c, yet still CPAC learnable.

## Limitations
- The characterization only applies to finite label spaces; infinite Y requires different techniques
- No quantitative sample complexity bounds provided, only qualitative learnability characterization
- Computational complexity of the embedding construction and "good functions" G is not analyzed

## Confidence
- High confidence: Necessity direction (Theorem 16) and meta-characterization via distinguishers (Theorem 33)
- Medium confidence: Sufficiency direction (Theorem 18) and embedding construction
- Low confidence: Negative result that DS dimension cannot be expressed as distinguisher (Lemma 34)

## Next Checks
1. Implement the computable k-witness for a simple hypothesis class and verify it correctly identifies non-shatterable sets
2. For a class with known finite c-N(H), explicitly construct the "good functions" G and verify the resulting H' has computable ERM
3. Take a known PAC-learnable class and verify that a suitable distinguisher Ψ exists, then check that c-Ψ-dim(H) < ∞ correctly predicts CPAC learnability