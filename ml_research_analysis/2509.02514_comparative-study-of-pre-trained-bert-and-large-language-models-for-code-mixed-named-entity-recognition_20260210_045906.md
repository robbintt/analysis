---
ver: rpa2
title: Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed
  Named Entity Recognition
arxiv_id: '2509.02514'
source_url: https://arxiv.org/abs/2509.02514
tags:
- code-mixed
- language
- entity
- named
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative study of pre-trained BERT and
  large language models for Named Entity Recognition (NER) in code-mixed Hindi-English
  text. The authors evaluate code-mixed fine-tuned models (HingBERT, HingMBERT, HingRoBERTa),
  non-code-mixed multilingual models (BERT Base Cased, IndicBERT, RoBERTa, MuRIL),
  and a zero-shot LLM (Google Gemini) on a benchmark Hinglish NER dataset.
---

# Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition

## Quick Facts
- arXiv ID: 2509.02514
- Source URL: https://arxiv.org/abs/2509.02514
- Authors: Mayur Shirke; Amey Shembade; Pavan Thorat; Madhushri Wagh; Raviraj Joshi
- Reference count: 13
- Primary result: Code-mixed models (HingBERT, HingMBERT) significantly outperform others on Hinglish NER; Gemini shows competitive zero-shot performance.

## Executive Summary
This paper presents a comparative study of pre-trained BERT and large language models for Named Entity Recognition (NER) in code-mixed Hindi-English text. The authors evaluate code-mixed fine-tuned models (HingBERT, HingMBERT, HingRoBERTa), non-code-mixed multilingual models (BERT Base Cased, IndicBERT, RoBERTa, MuRIL), and a zero-shot LLM (Google Gemini) on a benchmark Hinglish NER dataset. The study finds that code-mixed models, especially HingMBERT, significantly outperform others, including the closed-source LLM, due to domain-specific pretraining. Non-code-mixed models perform reasonably but show limited adaptability. Gemini exhibits competitive zero-shot performance, highlighting the generalization strength of modern LLMs. Overall, code-mixed models achieve the highest F1-scores (up to 79.74), while zero-shot LLMs lag behind, though still showing promise. The results underscore the importance of domain adaptation for code-mixed NER and the emerging potential of generative models in resource-constrained settings.

## Method Summary
The study evaluates six transformer-based models (HingBERT, HingMBERT, HingRoBERTa, BERT Base Cased, IndicBERT, RoBERTa, MuRIL) and one LLM (Google Gemini) on a benchmark Hinglish NER dataset. Models are fine-tuned with Optuna hyperparameter optimization (LR: 2e-5-5e-5, Batch: 16/32) on an 80/20 split, using a 128 token limit and early stopping. Label alignment masks subword tokens during loss computation. Gemini is evaluated zero-shot by prompting it to output BIO tags for the test set. Performance is measured using entity-level precision, recall, and F1-score with the seqeval library.

## Key Results
- Code-mixed models (HingBERT, HingMBERT, HingRoBERTa) significantly outperform non-code-mixed models on the Hinglish NER benchmark.
- HingMBERT achieves the highest F1-score of 79.74, outperforming the zero-shot LLM (Gemini).
- Zero-shot LLM (Gemini) shows competitive performance but lags behind fine-tuned models due to lack of supervision.
- Non-code-mixed models (MuRIL, IndicBERT, RoBERTa, BERT Base Cased) perform reasonably but are limited by domain mismatch.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-specific pretraining on code-mixed corpora provides superior feature alignment for NER compared to general multilingual training.
- **Mechanism:** Models like HingBERT and HingRoBERTa are exposed to the specific orthographic and syntactic irregularities of code-mixed text (e.g., transliteration, spelling variations) during pretraining. This allows the self-attention mechanism to learn cross-lingual context and boundary detection that standard multilingual models, trained on formal monolingual text, fail to capture.
- **Core assumption:** The distribution of code-mixed data in the pretraining corpus (HingCorpus) sufficiently overlaps with the target inference data.
- **Evidence anchors:**
  - [abstract] "Code-mixed models... outperform others... due to domain-specific pretraining."
  - [section 1] "Models such as HingBERT... have shown promise... due to their exposure to code-mixed corpora during pretraining."
  - [corpus] Related work (COMI-LINGUA) supports the necessity of large-scale code-mixed datasets for multitask NLP, reinforcing that data composition drives performance.
- **Break condition:** Performance gains likely diminish if the target code-mixed language pair differs significantly from the Hindi-English pair used in pretraining.

### Mechanism 2
- **Claim:** Precise subword-label alignment prevents gradient confusion during the fine-tuning of token classification tasks.
- **Mechanism:** Transformers break words into subwords. The paper utilizes a mechanism where only the first subword token retains the NER label, and subsequent subwords are masked (ignored) during loss computation. This prevents the loss function from penalizing the model for not assigning a "Begin" or "Inside" tag to a partial word fragment, stabilizing training on noisy text.
- **Core assumption:** The tokenizer's splitting behavior is deterministic and consistent between training and inference.
- **Evidence anchors:**
  - [section 3.2] "Label Alignment... subsequent subwords are masked during loss computation to prevent label misalignment."
  - [section 5] High F1-scores (up to 79.74) in code-mixed models validate the effectiveness of this preprocessing rigor.
  - [corpus] Corpus papers on NER (e.g., "Positional Attention for Efficient BERT-Based NER") generally highlight tokenization handling as a critical architectural step.
- **Break condition:** If using a character-level or word-level model architecture, this specific masking mechanism is irrelevant.

### Mechanism 3
- **Claim:** Zero-shot Large Language Models (LLMs) rely on semantic generalization but lack the rigid constraint adherence required for structured entity extraction.
- **Mechanism:** Generative models like Gemini solve NER as a text-to-text generation problem rather than a token-level classification problem. While they possess the "knowledge" to identify entities (high semantic capacity), they struggle to adhere to the strict boundaries and schemas (e.g., BIO tags) without fine-tuning, leading to lower precision/recall in zero-shot settings.
- **Core assumption:** The LLM has been sufficiently exposed to the concept of named entities and the specific languages during its massive general pretraining.
- **Evidence anchors:**
  - [abstract] "Gemini exhibits competitive zero-shot performance... [but] lag[s] behind."
  - [section 5] "Low macro-averaged precision and F1-score reveal that the model... struggles with fine-grained entity classification in the absence of supervision."
  - [corpus] Related study "Local LLM Ensembles for Zero-shot Portuguese NER" confirms that LLMs "often under-perform in NER," validating the mechanism that generative capacity does not equate to extraction precision.
- **Break condition:** Breaks if the prompt engineering is insufficient to define the output schema, or if the entity types are highly domain-specific (e.g., biochemical names) not well-represented in general pretraining.

## Foundational Learning

- **Concept: Code-Mixing vs. Code-Switching**
  - **Why needed here:** The paper specifically targets "Hinglish," which involves transliteration and informal mixing, distinct from standard multilingual translation. Understanding this noise factor is crucial for preprocessing.
  - **Quick check question:** Does the input text contain sentences where Latin script is used for non-English words (transliteration), or does it strictly switch between standard English and standard Hindi scripts?

- **Concept: The BIO Tagging Schema**
  - **Why needed here:** The paper evaluates performance based on entity-level matching using the BIO (Beginning, Inside, Outside) format.
  - **Quick check question:** If a model predicts "B-PER" for "John" but "O" for "Doe" in "John Doe," would this be a partial match or a complete failure for the "John Doe" entity?

- **Concept: Hyperparameter Optimization (HPO)**
  - **Why needed here:** The study uses Optuna to search for optimal learning rates and batch sizes during fine-tuning.
  - **Quick check question:** What is the trade-off between using a smaller search space (e.g., grid search) versus a larger one (e.g., Optuna) for hyperparameter optimization?

## Architecture Onboarding

- **Component Map:** Tokenization -> Subword Alignment -> Fine-tuning (BERT/RoBERTa) -> Zero-shot LLM Generation -> Entity Evaluation
- **Critical Path:** Tokenization and label alignment are critical for preventing label misalignment during training, which directly impacts F1-scores.
- **Design Tradeoffs:** Code-mixed pretraining vs. multilingual pretraining (domain specificity vs. general applicability); fine-tuning vs. zero-shot (supervision vs. generalization).
- **Failure Signatures:** High accuracy but low F1-score indicates class imbalance issues; low precision in zero-shot LLM suggests poor schema adherence.
- **Three First Experiments:**
  1. Test the impact of label alignment by training a model without masking subword labels and comparing F1-scores.
  2. Evaluate the effect of different prompt templates on zero-shot LLM performance by varying output formats and examples.
  3. Compare the performance of a non-code-mixed model (e.g., MuRIL) trained on the same HingCorpus as code-mixed models to isolate the effect of pretraining data composition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ensemble methods combining fine-tuned code-mixed transformers and generative LLMs outperform individual models?
- Basis in paper: [explicit] The Conclusion states future directions include "exploring ensemble methods" to build upon the comparison of specialized versus generalized models.
- Why unresolved: The current study only evaluates models in isolation without combining their distinct architectural strengths.
- What evidence would resolve it: Experimental results from hybrid systems (e.g., voting mechanisms) demonstrating superior F1-scores over HingMBERT.

### Open Question 2
- Question: Do the observed advantages of domain-specific pretraining hold for code-mixed language pairs other than Hindi-English?
- Basis in paper: [explicit] The authors explicitly propose "extending evaluations to other language pairs" as a necessary future direction.
- Why unresolved: The current analysis is restricted to Hinglish, which may possess unique linguistic characteristics not present in other code-mixed contexts.
- What evidence would resolve it: A comparative study on pairs like Tamil-English or Spanish-English showing similar performance trends.

### Open Question 3
- Question: How does the performance of zero-shot LLMs change when utilizing instruction-tuned variants rather than general-purpose versions?
- Basis in paper: [explicit] The Limitations section cites a "lack of domain-specific or instruction-tuned model exploration" as a constraint of the current methodology.
- Why unresolved: The study evaluated general Gemini models, leaving the potential benefits of instruction-following training for code-mixing untested.
- What evidence would resolve it: Benchmarking instruction-tuned variants (e.g., Gemini-Instruct) on the same Hinglish NER dataset.

## Limitations
- Dataset access and reproducibility: The benchmark Hinglish NER dataset (Singh et al., 2018) is referenced but not directly linked, hindering exact reproduction.
- LLM prompt specificity: The zero-shot evaluation of Gemini depends critically on prompt engineering, which is not disclosed.
- Hyperparameter transparency: Final optimal learning rates and batch sizes selected by Optuna are not explicitly stated.

## Confidence

- **High Confidence:** Code-mixed models (HingBERT, HingMBERT) significantly outperform non-code-mixed models on this specific Hinglish NER benchmark.
- **Medium Confidence:** The specific mechanism of "domain-specific pretraining on code-mixed corpora" is the primary driver of the performance gap.
- **Medium Confidence:** Zero-shot LLM performance (Gemini) is "competitive" but lags behind fine-tuned models.

## Next Checks
1. Replicate the prompt engineering: Systematically test the zero-shot Gemini performance using a range of prompt templates (e.g., with and without examples, different output formats) on the Hinglish dataset to establish the impact of prompt design on the reported scores.
2. Conduct an ablation study on pretraining data: Train a non-code-mixed multilingual model (e.g., MuRIL) on the same HingCorpus used for HingBERT/HingMBERT and evaluate its performance on the Hinglish NER benchmark. This would directly test whether the "code-mixed" nature of the pretraining data, not just the size, is responsible for the performance gain.
3. Test cross-domain robustness: Evaluate the top-performing code-mixed and non-code-mixed models on a different, publicly available code-mixed NER dataset (if one exists) or on a subset of the current data with a different topic/genre. This would assess whether the performance gains are task-specific or generalize to other code-mixed domains.