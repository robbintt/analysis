---
ver: rpa2
title: 'OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination'
arxiv_id: '2509.00723'
source_url: https://arxiv.org/abs/2509.00723
tags:
- audio
- omni
- wang
- zhang
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in omni-modal large language
  models (OLLMs), where models overly rely on textual priors and neglect visual and
  audio inputs, or fail to capture interactions between video and audio. The authors
  propose OMNI DPO, a preference optimization framework that extends DPO with modality-specific
  objectives to explicitly encourage attention to visual and auditory evidence.
---

# OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination

## Quick Facts
- **arXiv ID**: 2509.00723
- **Source URL**: https://arxiv.org/abs/2509.00723
- **Reference count**: 40
- **Primary result**: OMNI DPO significantly reduces hallucinations in omni-modal LLMs with 2.64-5.82% F1-score improvements on A VHBench

## Executive Summary
OmniDPO addresses a critical limitation in omni-modal large language models (OLLMs) where models overly rely on textual priors while neglecting visual and audio inputs. The framework extends Direct Preference Optimization (DPO) with modality-specific objectives to explicitly encourage attention to visual and auditory evidence. By introducing two types of preference pairs - audio-video alignment and modality-robustness pairs - the approach trains models to better integrate multi-modal information and resist hallucinations when individual modalities are degraded.

## Method Summary
OmniDPO builds upon the DPO framework by incorporating modality-specific preference objectives. The method generates two types of preference pairs: (1) audio-video alignment pairs created by comparing model outputs with and without audio cues, and (2) modality-robustness pairs generated by degrading video or audio inputs. These pairs form a new training dataset (OMNI DPO-10k) that teaches the model to maintain performance when individual modalities are compromised and to properly integrate cross-modal information. The framework is applied to two OLLMs (Qwen2.5-Omni and MiniCPM-o-2.6) through fine-tuning, resulting in improved hallucination resistance while maintaining or enhancing reasoning capabilities on benchmarks like MMAU and MMMU.

## Key Results
- OMNI DPO achieves average F1-score improvements of 5.82% on Qwen2.5-Omni and 2.64% on MiniCPM-o-2.6 for A VHBench
- Consistent gains in perception accuracy (PA) and hallucination resistance (HR) metrics on CMM benchmark
- Improved reasoning performance on MMAU and MMMU while reducing hallucinations
- Demonstrates effective reduction in omni-modal hallucinations through targeted preference optimization

## Why This Works (Mechanism)
The framework works by explicitly training models to recognize and utilize multi-modal evidence through preference optimization. By generating synthetic preference pairs that highlight the value of audio-visual alignment and the importance of robustness to modality degradation, the model learns to distribute attention across all available modalities rather than defaulting to textual priors. The preference pairs create contrastive learning signals that reinforce correct multi-modal reasoning while penalizing hallucination tendencies.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A preference-based fine-tuning method that optimizes models based on pairwise comparisons; needed to provide the underlying optimization framework that OmniDPO extends
- **Multi-modal hallucination**: When models generate responses inconsistent with or ignoring visual/audio inputs; critical to identify and measure the problem being addressed
- **Audio-visual alignment**: The relationship between audio and visual modalities in video content; essential for understanding how the model should integrate cross-modal information
- **Modality robustness**: A model's ability to maintain performance when individual modalities are degraded; fundamental to the preference pair generation strategy
- **Preference pair generation**: The process of creating synthetic training examples through contrastive comparisons; key to implementing the DPO extension
- **Cross-modal attention**: How models distribute focus across different input modalities; central to understanding hallucination mechanisms

## Architecture Onboarding

**Component Map**: Preference Pair Generator -> Dataset Construction -> OmniDPO Fine-tuning -> Evaluated OLLMs

**Critical Path**: The framework follows a pipeline where synthetic preference pairs are first generated through audio-video alignment and modality-robustness strategies, then used to construct the training dataset, which is finally applied to fine-tune target OLLMs through the extended DPO objective.

**Design Tradeoffs**: The approach trades increased training complexity and computational overhead for improved hallucination resistance. The synthetic preference generation introduces potential biases but provides scalable training data without requiring extensive human annotation.

**Failure Signatures**: Models may fail when audio information is actually irrelevant to the question, when the preference pairs don't capture realistic user preferences, or when the framework introduces new biases that weren't present in the original model.

**3 First Experiments**:
1. Ablation study comparing performance with only audio-video alignment pairs versus only modality-robustness pairs
2. Evaluation on additional OLLM architectures beyond the two tested models
3. User study measuring practical task completion and satisfaction with reduced hallucination behavior

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on synthetic preference data that may not fully capture real-world user preferences
- Modest performance improvements (2.64-5.82% F1-score gains) suggest partial rather than comprehensive solution
- Limited evaluation to only two specific OLLM architectures, restricting generalizability

## Confidence

**High confidence**: The core methodology of extending DPO with modality-specific objectives is technically sound and well-implemented

**Medium confidence**: The reported performance improvements on the evaluated benchmarks, though the modest gains suggest limited practical impact

**Low confidence**: The generalizability of results to other OLLM architectures and real-world deployment scenarios

## Next Checks
1. Conduct ablation studies to isolate the contribution of audio-video alignment pairs versus modality-robustness pairs to the overall performance gains
2. Evaluate the framework on additional OLLM architectures beyond Qwen2.5-Omni and MiniCPM-o-2.6 to assess generalizability
3. Perform user studies to determine whether the reduced hallucination rates translate to improved user satisfaction and task completion in practical applications