---
ver: rpa2
title: 'Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation
  of Large Language Model Agents'
arxiv_id: '2601.12560'
source_url: https://arxiv.org/abs/2601.12560
tags:
- agents
- arxiv
- agent
- preprint
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a unified taxonomy and architectural framework\
  \ for agentic AI systems based on large language models (LLMs), addressing the growing\
  \ complexity of autonomous agents that can perceive, reason, plan, and act. The\
  \ authors decompose agentic systems into six core dimensions\u2014Perception, Memory,\
  \ Action, Profiling, Cognitive Architecture, and Learning\u2014and describe how\
  \ agents evolve from single-loop reasoning to hierarchical multi-agent collaboration."
---

# Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents

## Quick Facts
- arXiv ID: 2601.12560
- Source URL: https://arxiv.org/abs/2601.12560
- Reference count: 40
- Primary result: Unified taxonomy and architectural framework for LLM-based autonomous agents with six core dimensions

## Executive Summary
This paper presents a comprehensive taxonomy and architectural framework for agentic AI systems built on large language models. The authors decompose agentic systems into six core dimensions—Perception, Memory, Action, Profiling, Cognitive Architecture, and Learning—and trace their evolution from single-loop reasoning to hierarchical multi-agent collaboration. The work emphasizes the shift from static tool APIs to flexible code execution and multimodal interfaces, introducing the emerging Model Context Protocol for standardized tool integration. The authors frame evaluation using CLASSic metrics (Cost, Latency, Accuracy, Security, Stability) and highlight critical challenges including hallucination in action, infinite loops, and prompt injection vulnerabilities.

## Method Summary
This survey paper provides a conceptual taxonomy and architectural framework for agentic AI systems based on LLMs, structured around a POMDP control loop with explicit memory and reasoning stages. The method involves reviewing 40+ references spanning agent frameworks (ReAct, Reflexion, ToT, ReAcTree), multi-agent systems (CAMEL, AutoGen, MetaGPT, LangGraph), and benchmarks (OSWorld, SWE-Bench, WebArena, GAIA). The authors propose a formal POMDP control loop (Φ perception, µ memory update, Ψ planning, π action policy) as a conceptual framework, though no implementation code or hyperparameters are specified. The CLASSic evaluation framework (Cost, Latency, Accuracy, Security, Stability) is introduced as a qualitative structure for agent assessment.

## Key Results
- Proposes a unified taxonomy breaking agents into six core dimensions: Perception, Brain, Planning, Action, Tool Use, and Collaboration
- Identifies the evolution from single-loop reasoning to hierarchical multi-agent collaboration with reflection mechanisms
- Highlights the shift from static tool APIs to flexible code execution and multimodal interfaces via Model Context Protocol
- Introduces CLASSic evaluation framework emphasizing the need for controllable, auditable architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Structuring the agent as a modified POMDP loop with explicit memory and reasoning stages improves grounding compared to stateless prompting
- **Mechanism**: The system processes observation ($O_t$) through perception module ($\Phi$), updates internal memory state ($M_t$) via retrieval or consolidation, generates reasoning trace ($Z_t$), and executes action ($A_t$). This explicit state management decouples thinking from acting.
- **Core assumption**: The LLM can reliably synthesize memory context ($M_t$) and observation ($O_t$) into coherent plan ($Z_t$) without drift
- **Evidence anchors**:
  - [abstract]: Proposes unified taxonomy breaking agents into "Perception, Brain, Planning, Action, Tool Use, and Collaboration"
  - [section]: Section 2.2 defines agent system $A$ as tuple $\langle S, O, M, T, \pi \rangle$ with control loop functions $\Phi, \mu, \Psi, \pi$
  - [corpus]: Related work reinforces necessity of modular protocols for coordination
- **Break condition**: If memory retrieval ($\mu$) returns irrelevant context or reasoning trace ($Z_t$) hallucinates steps unsupported by tool space, the loop fails

### Mechanism 2
- **Claim**: Shifting from open-ended chat loops to flow engineering (graph-based orchestration) increases system reliability and auditability
- **Mechanism**: Workflow mapped to state machine where nodes are tool calls or LLM invocations and edges are permissible transitions. LLM acts as local decision-maker within developer-defined graph structure
- **Core assumption**: Critical workflow path can be largely pre-defined, reducing agent's burden to plan high-level structure
- **Evidence anchors**:
  - [abstract]: Emphasizes "controllable, auditable architectures" and move to "standardized tool integration"
  - [section]: Section 5.1 describes "flow engineering" and frameworks like LangGraph operationalizing agent execution as graph traversal
  - [corpus]: Questions if complex structures reinvent wheel, suggesting architectural convergence
- **Break condition**: If task requires highly dynamic, non-deterministic exploration that cannot map to finite graph topology, approach will be too rigid

### Mechanism 3
- **Claim**: Hierarchical planning combined with reflection mechanisms mitigates error propagation in long-horizon tasks, conditional on increased compute cost
- **Mechanism**: High-level planner decomposes complex goal into sub-goals, low-level executor handles immediate actions. Separate reflection module critiques output before execution, allowing self-correction
- **Core assumption**: Cost of additional inference calls for reflection and hierarchy is acceptable given gain in task success rates
- **Evidence anchors**:
  - [abstract]: Highlights evolution from "single-loop reasoning to hierarchical multi-agent collaboration"
  - [section]: Section 4.2.1 compares linear ReAct loops with Tree of Thoughts and LATS, noting hierarchical structures handle complexity better but incur higher token costs
  - [corpus]: Analyzes trade-offs in collaborative agentic systems
- **Break condition**: If reflection mechanism itself hallucinates critique or hierarchy creates excessive latency for real-time requirements

## Foundational Learning

- **Concept: POMDP (Partially Observable Markov Decision Process)**
  - **Why needed here**: Mathematical backbone of proposed architecture. Understanding that agent only sees partial observations ($O_t$) and must maintain internal belief state (Memory, $M_t$) is critical for debugging why agent took specific action
  - **Quick check question**: Can you explain why a "stateless" LLM call fails to qualify as full POMDP agent in this framework?

- **Concept: Inference-Time Compute (Test-Time Compute)**
  - **Why needed here**: Paper notes shift to "native inference time reasoning models" (like o1) where model "thinks" longer. Changes architectural design from complex external prompting strategies to relying on model's internal search
  - **Quick check question**: How does increasing "inference time budget" affect CLASSic evaluation metrics (specifically Latency vs. Accuracy)?

- **Concept: Prompt Injection & Indirect Injection**
  - **Why needed here**: Primary security barrier. Since agents ingest untrusted content (e.g., web pages) and act on them, distinguishing trusted developer instructions from untrusted data is non-negotiable safety requirement
  - **Quick check question**: In "Computer Use" scenario, why is indirect prompt injection harder to mitigate than direct user-prompt injection?

## Architecture Onboarding

- **Component map**: Perception ($\Phi$) -> Memory ($M$) -> Brain ($\Psi$) -> Action ($T$)
- **Critical path**: The control loop flows from Perception (ingesting environment state) → Memory Retrieval (finding relevant history) → Cognitive Planning (generating $Z_t$) → Action Execution (calling tool $A_t$)
- **Design tradeoffs**:
  - **Cost vs. Accuracy**: Hierarchical planning increases accuracy but exponentially increases token consumption
  - **Flexibility vs. Safety**: "Computer Use" agents (pixels-to-actions) are maximally flexible but maximally vulnerable to hallucination and injection compared to constrained API agents
- **Failure signatures**:
  - **Infinite Loops**: Agent repeatedly retries failed action without strategy modification
  - **Hallucination in Action**: Executing valid-looking but non-existent API call or shell command
  - **Context Overflow**: Memory growth exceeding context window, leading to loss of critical instructions
- **First 3 experiments**:
  1. **Loop Debugging**: Run simple ReAct loop on noisy dataset and measure "error propagation rate" to establish baseline for stability
  2. **Memory Stress Test**: Implement long-horizon task (e.g., multi-step coding) with and without vector-db memory backend to quantify "context exhaustion" point
  3. **Graph vs. Chat Orchestration**: Compare free-form AutoGen chat loop against LangGraph state machine for same workflow, measuring debuggability (time to trace errors) and success rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can agents develop "meta-cognitive" capabilities to autonomously recognize and interrupt futile action loops without external hard-coded limits?
- **Basis in paper**: Section 8.2 notes agents often fail to recognize when stuck in "local optimum" loops and calls for "meta-cognitive modules" to solve this paralysis
- **Why unresolved**: Current reflection mechanisms can themselves degrade or loop, lacking stable internal signal for "giving up"
- **What evidence would resolve it**: Agent architecture demonstrating ability to autonomously exit repetitive failure states on benchmarks like WebArena with higher efficiency than static retry limits

### Open Question 2
- **Question**: Can system-level security architectures effectively mitigate indirect prompt injection in computer-use agents without significantly degrading tool utility?
- **Basis in paper**: Section 7.4 states robust security is increasingly "systems problem" involving sandboxes and audit logging, as prompt-only defenses are brittle against adaptive attacks
- **Why unresolved**: High-bandwidth observation channels (e.g., screenshots) expand attack surface, and attackers can craft injections that bypass static guards
- **What evidence would resolve it**: Defense mechanism maintaining >90% task success rates on OSWorld while successfully blocking 100% of adaptive indirect prompt injection attacks in standardized benchmark

### Open Question 3
- **Question**: How can the "Efficiency-Intelligence Trade-off" be resolved to allow hierarchical planning without incurring exponential computational costs?
- **Basis in paper**: Section 7.1 identifies hierarchical architectures like ReAcTree maximize reasoning depth but incur "exponential increases in token consumption" compared to standard loops
- **Why unresolved**: Deep reasoning requires extensive search or multi-agent debate, currently cost-prohibitive for real-time or large-scale deployment
- **What evidence would resolve it**: Hierarchical agent achieving comparable accuracy to Tree of Thoughts on complex reasoning tasks with linear or constant token overhead relative to input size

### Open Question 4
- **Question**: What evaluation metrics are required to effectively distinguish between benign failures and catastrophic safety violations in autonomous agents?
- **Basis in paper**: Section 7.5 argues simple "Success Rate" metrics mask reliability issues and calls for "failure mode analysis" that quantifies severity distribution of errors
- **Why unresolved**: Regulatory domains require high compliance stability, but current benchmarks often aggregate errors rather than weighing them by potential harm
- **What evidence would resolve it**: Benchmark suite reporting "Safety-Weighted Accuracy," penalizing agents specifically for irreversible actions versus recoverable errors

## Limitations
- Presents conceptual taxonomy rather than empirical validation, leaving claims about mechanism effectiveness theoretical
- Lacks quantitative performance data for CLASSic metrics and implementation details for proposed POMDP architecture
- Insufficient empirical grounding for security claims around prompt injection and architectural recommendations
- Rapid evolution of inference-time compute models may quickly render some architectural assumptions obsolete

## Confidence
- **High Confidence**: POMDP-based decomposition into Perception, Memory, Brain, and Action modules is well-grounded in existing literature and provides coherent theoretical framework
- **Medium Confidence**: CLASSic evaluation framework offers useful conceptual structure, though lack of precise metric definitions limits practical applicability
- **Low Confidence**: Claims about superiority of hierarchical planning and flow engineering over traditional prompting strategies lack empirical validation and may be overly optimistic about real-world reliability

## Next Checks
1. **Loop Stability Benchmark**: Implement basic POMDP control loop and measure error propagation rates across 100+ trials on noisy task dataset, establishing baseline stability metrics for infinite loop detection and prevention
2. **Memory Management Stress Test**: Compare long-horizon task performance with and without vector-database memory, quantifying context exhaustion point and retention policy effectiveness
3. **Graph vs. Chat Orchestration Comparison**: Replicate identical workflows using both free-form AutoGen chat loops and LangGraph state machines, measuring debuggability and success rates across 20+ varied task types