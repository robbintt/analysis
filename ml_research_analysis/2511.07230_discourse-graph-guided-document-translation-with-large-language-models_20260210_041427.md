---
ver: rpa2
title: Discourse Graph Guided Document Translation with Large Language Models
arxiv_id: '2511.07230'
source_url: https://arxiv.org/abs/2511.07230
tags:
- translation
- chunk
- type
- document
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TransGraph, a discourse-guided document\
  \ translation framework that improves translation quality and terminology consistency\
  \ by constructing structured discourse graphs to capture inter-chunk relationships\
  \ and selectively conditioning each translation segment on relevant graph neighborhoods.\
  \ Unlike memory-intensive agentic systems or generic sequential context, TransGraph\
  \ uses explicit discourse relation labels (e.g., Entity-Coreference, Core\u2192\
  Detail, Motivation\u2192Method) to provide compact, structured context during translation."
---

# Discourse Graph Guided Document Translation with Large Language Models

## Quick Facts
- arXiv ID: 2511.07230
- Source URL: https://arxiv.org/abs/2511.07230
- Reference count: 31
- Primary result: TransGraph achieves higher document-level BLEU, COMET, and terminology accuracy than sentence-level, single-pass, and agentic baselines while using fewer tokens.

## Executive Summary
This paper introduces TransGraph, a discourse-guided document translation framework that improves translation quality and terminology consistency by constructing structured discourse graphs to capture inter-chunk relationships. Unlike memory-intensive agentic systems or generic sequential context, TransGraph uses explicit discourse relation labels (e.g., Entity-Coreference, Core→Detail, Motivation→Method) to provide compact, structured context during translation. Evaluated on three document-level MT benchmarks spanning six languages, TransGraph consistently outperforms sentence-level, single-pass, and agentic baselines in document-level BLEU, COMET, and terminology accuracy while incurring significantly lower token overhead.

## Method Summary
TransGraph is a two-stage, non-training LLM-based pipeline for document-level machine translation. Stage 1: documents are chunked into topically coherent units using LLM-based segmentation (preserving sentence boundaries), then discourse relations are identified between chunks to build a directed labeled graph (10 relation types). Stage 2: translation proceeds iteratively—each chunk retrieves up to 5 in-neighbors from the graph, translates conditioned on this relation-labeled context, and outputs are concatenated. The approach relies solely on LLM inference, avoiding expensive training or agentic search.

## Key Results
- TransGraph consistently outperforms sentence-level, single-pass, and agentic baselines in document-level BLEU, COMET, and terminology accuracy across three benchmarks and six languages.
- Discourse relations are crucial: removing them (TRANSGRAPH(−REL)) only slightly improves over 1-PASS baseline; removing graph structure (SEQ) significantly degrades terminology accuracy.
- Graph-guided retrieval captures non-adjacent but semantically relevant context that sequential windows miss: ~35% of relevant relations are within 5-nearest chunks, so sequential-only retrieval loses 65%.
- TransGraph uses fewer tokens than agentic baselines (59k vs 104k–171k tokens) while achieving higher quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured discourse relations outperform generic sequential context for document translation
- Mechanism: TransGraph constructs a labeled directed graph where edges encode semantic relations (Entity-Coreference, Core→Detail, Motivation→Method, etc.). During translation, each chunk retrieves only its in-neighborhood, receiving context annotated with relation labels that explicitly signal *how* to use the information rather than treating context as undifferentiated text.
- Core assumption: LLMs can exploit explicit discourse role signals to maintain consistency better than raw sequential text; relation identification by LLMs is sufficiently accurate.
- Evidence anchors:
  - [abstract] "uses explicit discourse relation labels... to provide compact, structured context during translation"
  - [section 3.3.3] TRANSGRAPH(−REL) without relations performs "only slightly better than 1-PASS DOCMT baseline"; removing graph structure (SEQ) "significantly worse" on terminology accuracy
  - [corpus] GRAFT (arXiv:2507.03311) similarly uses graph structures for DocMT, suggesting convergent evidence for graph-based approaches
- Break condition: When LLM relation labeling accuracy degrades (smaller models: Qwen3-8B at 78–87% vs 32B at 94.5%), noisy labels may inject misleading context; high relation density documents increase retrieval overhead.

### Mechanism 2
- Claim: Graph-guided retrieval captures non-adjacent but semantically relevant context that sequential windows miss
- Mechanism: Rather than conditioning on fixed k-nearest chunks, TransGraph follows graph edges to retrieve *any* chunk with a meaningful relation, regardless of distance. Figure 2 shows ~35% of relevant relations are within 5-nearest chunks—meaning 65% would be lost with sequential-only retrieval.
- Core assumption: Non-adjacent chunks frequently contain terminology definitions or coreferences critical for translation consistency.
- Evidence anchors:
  - [section 3.3.3] "Most of the documents have low ratios around 35%, which means that if we only consider 5-nearest chunks... we are missing out 65% of the relations"
  - [section 3.3.5] Coreference cohesion: TransGraph achieves 98.38 avg accuracy vs 97.52 for single-pass; terminology accuracy gains of +8–10 points over sequential baselines on BWB
  - [corpus] BudgetMem (arXiv:2511.04919) addresses related problem of selective memory for long-context processing
- Break condition: When documents have sparse, local-only discourse structure, graph retrieval reduces to sequential context without corresponding gains.

### Mechanism 3
- Claim: Coherent LLM-based chunking preserves sentence boundaries and topical coherence better than fixed-size splits
- Mechanism: TransGraph prompts LLMs to split documents into chunks respecting sentence boundaries and topical coherence (Listing 1). Fixed-size chunking (TRANSGRAPH-FIXED) underperforms because "chunks are not split based on content, leading to unnecessary sentences in chunks, which confuses the LLMs in identifying the discourse relations."
- Core assumption: LLM chunking quality is language-dependent and scales with model capacity.
- Evidence anchors:
  - [section 3.3.1] Chunking overlap rates: Qwen3-32B achieves 90.96% (En), 91.72% (Zh) vs 80.23%/79.37% for Qwen3-8B
  - [section 3.3.3] TRANSGRAPH-FIXED shows "worse performance... on all benchmarks" compared to learned chunking
  - [corpus] Beyond Chunking (arXiv:2506.06313) similarly argues heuristic chunking overlooks discourse structure
- Break condition: For low-resource languages where LLM chunking quality degrades (e.g., Ja: 58–66% across model sizes), chunk boundaries may misalign with discourse units.

## Foundational Learning

- Concept: Rhetorical Structure Theory (RST) and discourse parsing
  - Why needed here: TransGraph's 10 relation types draw from RST; understanding Background→Core, Core→Detail distinctions helps debug relation identification failures.
  - Quick check question: Given two chunks, can you predict which RST-style relation should hold before prompting the LLM?

- Concept: Graph traversal and in-neighborhood retrieval
  - Why needed here: Translation proceeds via in-neighbors (N−(j)), exploiting the causal direction of discourse (antecedents precede anaphors).
  - Quick check question: For chunk 5 in a 10-chunk document, what determines which chunks appear in its context package?

- Concept: Document-level MT evaluation (d-BLEU, d-COMET, terminology accuracy)
  - Why needed here: TransGraph claims improvements require understanding what each metric captures—d-BLEU for n-gram overlap, COMET for semantic similarity, terminology accuracy for domain consistency.
  - Quick check question: Why might a system score higher on d-COMET but lower on terminology accuracy?

## Architecture Onboarding

- Component map: Chunking prompt (Listing 1) -> Relation labeling prompt (Listing 2) -> Directed labeled graph G=(V, E, ℓ) -> Graph traversal -> Context packaging (max 5 in-neighbors) -> Translation prompt (Listing 3) -> Concatenation
- Critical path: Relation labeling accuracy -> Graph quality -> Retrieved context relevance -> Translation quality. Table 5 shows this cascades: Qwen3-8B at 78% relation accuracy (De) yields lower translation gains than Qwen3-32B at 94.5%.
- Design tradeoffs:
  - Larger w (relation window) captures more long-range dependencies but increases Stage 1 LLM calls
  - Higher |N−(j)| provides more context but risks long-context degradation and token overhead
  - Stronger backbone improves both chunking and relation accuracy but increases cost
- Failure signatures:
  - Terminology drift across chunks -> Check Entity-Coreference/Terminology Definition edges are being identified and retrieved
  - Incoherent pronoun resolution -> Verify in-neighbor traversal direction (not out-neighbors)
  - High token costs despite graph approach -> Check if window w is too large or relations too dense
- First 3 experiments:
  1. Ablation on relation types: Remove Entity-Coreference and Terminology Definition relations only; measure terminology accuracy drop to quantify their contribution
  2. Scaling study: Plot relation labeling accuracy vs model size (8B/14B/32B) across languages to identify minimum viable backbone for target language pairs
  3. Cost-quality frontier: Compare total tokens vs d-COMET for TransGraph, TransAgent, and DeLTA on same documents to validate claimed efficiency gains (Table 7 shows ~59k vs 104k–171k tokens)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating retrieval-augmented compression or learned relation selection mechanisms further improve the framework's robustness and efficiency?
- Basis in paper: [explicit] The Limitations section states: "Future works could combine retrieval–augmented compression and learned relation selection to further improve robustness and efficiency."
- Why unresolved: The current implementation relies on LLM prompting for relation identification and heuristics for context packaging, which may be suboptimal compared to trained selectors or compressed contexts.
- What evidence would resolve it: A comparative study evaluating TransGraph against a variant that uses a fine-tuned classifier for relation selection or a summarization module for context compression.

### Open Question 2
- Question: How does the framework's performance degrade as the rate of LLM-induced mislabeling in the discourse graph increases?
- Basis in paper: [explicit] The authors acknowledge that "residual mislabels may inject noisy context," relying on the observation that "sporadic mislabeling" is handled robustly, without quantifying failure points.
- Why unresolved: It is unclear what threshold of labeling error causes a significant drop in translation quality, particularly for smaller models (e.g., Qwen3-8B) that exhibit lower graph accuracy.
- What evidence would resolve it: A sensitivity analysis where synthetic noise is systematically injected into the relation labels to measure the correlation between graph accuracy and BLEU/COMET scores.

### Open Question 3
- Question: How does TransGraph scale in terms of computational cost and latency when applied to documents significantly longer than those in the current benchmarks (avg. ~1,000–2,000 tokens)?
- Basis in paper: [inferred] While the paper claims efficiency over agentic baselines, the Limitations section notes that "Very long documents with dense relation structure increase retrieval and prompt assembly costs."
- Why unresolved: The current evaluation does not test scenarios where dense inter-chunk relations might create retrieval bottlenecks or excessive prompt assembly overheads common in book-length translation.
- What evidence would resolve it: Evaluation on long-form datasets (e.g., full novels) specifically tracking latency and token overhead relative to document length and graph density.

## Limitations

- The empirical gains depend critically on LLM quality for both chunking and relation identification, which vary substantially by language pair (78–94.5% accuracy) and are not broken down per relation type.
- The fixed 5-in-neighbor cap and relation window size w are not justified through sensitivity analysis, potentially limiting performance on documents with dense relation graphs.
- Efficiency claims rest on token counts alone without wall-clock time or cost-per-quality metrics, leaving practical deployment questions unanswered.
- The finding that "most documents have low ratios around 35%" for 5-nearest-chunk relations appears dataset-specific and may not generalize across document types and domains.

## Confidence

- **High Confidence**: The mechanism that structured discourse relations improve translation quality over sequential context is well-supported by ablation results (TRANSGRAPH vs TRANSGRAPH(−REL) and SEQ). The terminology accuracy gains (+8–10 points on BWB) are directly measurable and significant.
- **Medium Confidence**: The claim that TransGraph is more efficient than agentic systems rests on token counts alone. Without timing data or cost-per-quality metrics, the practical efficiency advantage remains unproven.
- **Low Confidence**: The paper's assertion that "most documents have low ratios around 35%" for 5-nearest-chunk relations appears to be dataset-specific. This ratio likely varies significantly with document type, length, and domain, but no cross-dataset validation is provided.

## Next Checks

1. **Relation Accuracy Diagnostics**: For each language pair, report per-relation labeling accuracy (Entity-Coreference, Core→Detail, etc.) and analyze common failure modes. This will reveal whether accuracy bottlenecks are relation-specific or model-capacity issues.

2. **Parameter Sensitivity Analysis**: Systematically vary the in-neighbor cap (1, 3, 5, 7) and relation window w (3, 5, 7) across document types. Plot the quality-cost frontier to identify optimal hyperparameters for different document characteristics (short vs. long, relation-dense vs. sparse).

3. **Efficiency Validation Beyond Tokens**: Measure actual inference time and cost (API calls × price) for TransGraph versus TransAgent and DeLTA on identical document sets. Calculate quality-per-dollar metrics to substantiate efficiency claims with real-world deployment considerations.