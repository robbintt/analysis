---
ver: rpa2
title: 'The aftermath of compounds: Investigating Compounds and their Semantic Representations'
arxiv_id: '2510.27477'
source_url: https://arxiv.org/abs/2510.27477
tags:
- glove
- bert
- semantic
- compound
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares static (GloVe) and contextualized (BERT) embeddings
  against human psycholinguistic judgments of English compound words. Using Spearman
  correlation and regression analysis, the research finds that BERT embeddings better
  capture human semantic transparency ratings than GloVe, with BERT''s ST values showing
  stronger correlation (r=0.23 for frequency, r=0.10 for predictability) and more
  accurate predictions (BERT: 3.31-4.25 vs.'
---

# The aftermath of compounds: Investigating Compounds and their Semantic Representations

## Quick Facts
- arXiv ID: 2510.27477
- Source URL: https://arxiv.org/abs/2510.27477
- Authors: Swarang Joshi
- Reference count: 2
- Primary result: BERT embeddings better capture human semantic transparency ratings than GloVe

## Executive Summary
This study investigates how static and contextualized word embeddings represent the semantic transparency of English compound words. The research compares GloVe (static embeddings) against BERT (contextualized embeddings) using human psycholinguistic judgments of semantic transparency. Through correlation and regression analysis, the study reveals that contextualized embeddings more accurately reflect human semantic judgments than static embeddings, with BERT showing stronger correlations and more accurate predictions of human ratings.

The findings demonstrate that predictability of meaning emerges as the strongest predictor of semantic transparency in both human judgments and model representations. BERT's superior performance suggests that contextualized embeddings better capture the compositional semantics involved in compound word processing, advancing our understanding of how different embedding approaches model linguistic phenomena in computational psycholinguistics.

## Method Summary
The study employed Spearman correlation analysis to compare human psycholinguistic judgments of English compound words with embedding-based representations. Both static (GloVe) and contextualized (BERT) embeddings were evaluated against human ratings for semantic transparency. Regression analysis was conducted to identify which factors (frequency, predictability, or combination) best predicted semantic transparency ratings. The comparison focused on how well each embedding type captured human semantic judgments and predicted rating values across different compound words.

## Key Results
- BERT embeddings show stronger correlation with human semantic transparency ratings (r=0.23 for frequency, r=0.10 for predictability) compared to GloVe
- BERT predictions (3.31-4.25) more closely match human ratings (4.04-4.93) than GloVe's compressed range (1.62-3.16)
- Predictability ratings emerge as the strongest predictor of semantic transparency in both human and model data

## Why This Works (Mechanism)
Contextualized embeddings like BERT capture semantic transparency better because they generate word representations conditioned on surrounding context, allowing them to model the compositional semantics inherent in compound processing. Unlike static embeddings that assign fixed vectors regardless of usage, BERT's dynamic representations can reflect how meaning combines from constituent parts in specific contexts. This contextual sensitivity enables BERT to better align with human psycholinguistic judgments that consider both constituent meanings and their combination patterns when evaluating semantic transparency.

## Foundational Learning
1. **Semantic Transparency** - The degree to which a compound's meaning can be predicted from its constituent parts; crucial for understanding how humans process and interpret compound words
2. **Static vs. Contextualized Embeddings** - Static embeddings assign fixed vectors while contextualized embeddings generate dynamic representations based on context; fundamental distinction affecting semantic modeling capabilities
3. **Spearman Correlation** - Non-parametric measure of monotonic relationship between variables; appropriate for comparing ranked human judgments with continuous embedding values
4. **Regression Analysis** - Statistical method for modeling relationships between dependent and independent variables; used to identify strongest predictors of semantic transparency
5. **Psycholinguistic Judgments** - Human ratings of linguistic phenomena based on intuition and processing difficulty; serve as gold standard for evaluating computational models
6. **Compositional Semantics** - How meanings of linguistic units combine to form larger meanings; central to understanding compound word processing

## Architecture Onboarding

**Component Map:**
BERT encoder -> Context-sensitive embeddings -> Semantic transparency prediction
GloVe matrix -> Static embeddings -> Semantic transparency prediction

**Critical Path:**
Input compounds → Embedding generation (BERT/GloVe) → Feature extraction (frequency/predictability) → Correlation analysis → Regression modeling

**Design Tradeoffs:**
Contextualized embeddings require significant computational resources and training data but capture nuanced semantic relationships; static embeddings are computationally efficient but miss context-dependent meaning variations. The choice between models involves balancing representational accuracy against computational efficiency.

**Failure Signatures:**
Poor correlation with human judgments indicates failure to capture compositional semantics; compressed rating ranges suggest inability to distinguish subtle semantic differences; systematic prediction errors reveal specific aspects of semantic transparency that models fail to represent.

**First Experiments:**
1. Compare BERT and GloVe performance across different compound types (endocentric vs. exocentric)
2. Test additional embedding models (RoBERTa, ELMo) to verify BERT's superiority is model-specific
3. Analyze error patterns to identify which semantic transparency aspects each embedding type fails to capture

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on English compound words, limiting generalizability to other languages
- Reliance on single dataset of human judgments may introduce sampling bias across demographic groups
- Moderate correlation effect sizes (r=0.23, r=0.10) indicate substantial unexplained variance remains

## Confidence
- BERT vs. GloVe comparison: High
- Predictability as strongest predictor: Medium
- Generalizability to other languages/models: Low

## Next Checks
1. Replicate analysis using additional contextualized embedding models (RoBERTa, ELMo) to determine if BERT's superiority is model-specific
2. Conduct cross-linguistic validation using compound words from morphologically rich languages
3. Perform demographic validation by collecting semantic transparency ratings from diverse participant groups