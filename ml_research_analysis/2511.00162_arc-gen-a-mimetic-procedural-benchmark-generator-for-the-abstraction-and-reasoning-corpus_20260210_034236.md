---
ver: rpa2
title: 'ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and
  Reasoning Corpus'
arxiv_id: '2511.00162'
source_url: https://arxiv.org/abs/2511.00162
tags:
- https
- examples
- learning
- urlhttps
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARC-GEN, a procedural benchmark generator
  for the Abstraction and Reasoning Corpus (ARC-AGI) that is both exhaustive (covering
  all 400 tasks) and mimetic (faithfully reproducing the original dataset's distributional
  properties). Unlike prior generators that focus on diversity, ARC-GEN aims to create
  examples that are behaviorally consistent with the original ARC-AGI-1 dataset.
---

# ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus

## Quick Facts
- arXiv ID: 2511.00162
- Source URL: https://arxiv.org/abs/2511.00162
- Reference count: 40
- Authors: Michael D. Moffitt
- Primary result: Introduces ARC-GEN, a procedural generator that achieves 100% solver compatibility by faithfully reproducing the distributional properties of ARC-AGI-1 tasks

## Executive Summary
ARC-GEN is a procedural benchmark generator for the Abstraction and Reasoning Corpus that focuses on distributional fidelity rather than diversity. Unlike previous generators that aim to create varied examples, ARC-GEN explicitly parameterizes each of the 400 ARC-AGI-1 tasks to reproduce the original dataset's characteristics. The generator was successfully used to create 100,000 examples for the 2025 Google Code Golf Championship to prevent overfitting. When tested against existing ARC solvers, ARC-GEN achieved 100% success rates, significantly outperforming competing generators that achieved only 3.7% and 16.25% compatibility.

## Method Summary
ARC-GEN uses parameterized sub-generators for each of the 400 ARC-AGI-1 tasks, with validation functions to ensure exact reproduction of original examples. The core API accepts parameters like rows, cols, and colors, defaulting to None for random generation or accepting specific values for validation. The generator separates parameter selection from grid synthesis, allowing the same code path to function for both exact reproduction and novel generation. Manual reverse-engineering of task parameters ensures the generated examples maintain the "core knowledge priors" of the original dataset while allowing controlled variations.

## Key Results
- Achieved 100% success rates when tested against BARC source programs and RE-ARC verifiers
- Outperformed competing generators (RE-ARC: 3.7% and 16.25% success rates)
- Successfully generated 100,000 examples for the 2025 Google Code Golf Championship
- Provides exhaustive coverage of all 400 ARC-AGI-1 tasks through manual parameterization

## Why This Works (Mechanism)

### Mechanism 1: Parameterized Distributional Mimicry
If the latent parameters of original ARC-AGI-1 tasks are manually reverse-engineered, subsequent random sampling within these parameter bounds produces examples statistically indistinguishable from the original dataset. The authors manually decomposed tasks into specific parameter sets (e.g., object counts, grid dimensions, color mappings) rather than using generic random seeds, preserving the "core knowledge" priors.

### Mechanism 2: Decoupling of Parameter Selection and Grid Synthesis
Separating the logic for selecting task parameters from the logic for rendering grids allows the same code path to function for both exact reproduction and novel generation. The `generate()` function uses a `None` default for parameters - if `None`, it synthesizes random values; if values are provided (as in `validate()`), it deterministically renders the grid.

### Mechanism 3: Solver-Aligned Verification
Generators that maximize diversity often violate the implicit constraints of specific solver programs, whereas a "mimetic" generator maintains 100% compatibility by adhering to the narrow input domain those programs were designed to handle. By generating examples that strictly mirror the original distribution, ARC-GEN ensures the generated inputs fall within the "competence zone" of existing programmatic solutions.

## Foundational Learning

- **Concept: Procedural Content Generation (PCG) via DSLs**
  - Why needed here: The paper relies on the idea that tasks are not static images but reproducible algorithms. Understanding how Domain Specific Languages encode rules is essential to reading the code.
  - Quick check question: Can you distinguish between the *parameters* of a rectangle (x, y, w, h) and the *rendering logic* that draws it on a grid?

- **Concept: Overfitting vs. Generalization in Benchmarking**
  - Why needed here: The paper explicitly targets the Code Golf context where contestants might "hardcode" outputs. Understanding why more data prevents memorization is key to the application section.
  - Quick check question: Why does increasing the number of test examples from 1 to 100 make it harder for a contestant to store the answers directly in the code?

- **Concept: Inverse Graphics / Program Synthesis**
  - Why needed here: The authors created ARC-GEN by looking at the output (the ARC tasks) and inferring the program (the generator). This is the inverse of the usual solver problem.
  - Quick check question: If I give you a 3x3 grid of blue pixels, can you write a single line of code that generates it, rather than just listing the pixel values?

## Architecture Onboarding

- **Component map:** `generate(rows, cols, ...)` -> `common` library helpers -> Grid synthesis logic
- **Critical path:** 1. Identify target Task ID 2. Run `validate()` to reproduce original examples 3. Run `generate()` with None arguments for new examples 4. Feed into solver for verification
- **Design tradeoffs:** Mimetic vs. Diverse - trades diversity for behavioral consistency; Manual vs. Automated - requires manual coding for each task rather than universal generation
- **Failure signatures:** Under-specification if generated data is too easy; Constraint Drift if random generation allows invalid states solvers cannot handle
- **First 3 experiments:** 1. Reproduction Test - clone repo and validate one task 2. Compatibility Stress Test - generate 100 examples and run against open-source solver 3. Distributional Analysis - generate 1,000 examples and calculate attribute frequencies

## Open Questions the Paper Calls Out

### Open Question 1
Does training ARC solvers on ARC-GEN's mimetic samples result in better generalization performance compared to training on diverse generators like RE-ARC? The paper evaluates generator compatibility with existing programs but does not measure the downstream utility of generated data for training machine learning models.

### Open Question 2
Can the specific procedural generation methodology used in ARC-GEN be adapted to cover the expanded "core knowledge priors" found in ARC-AGI-2 and ARC-AGI-3? The current library is exhaustive only for ARC-AGI-1 tasks, and newer datasets emphasize capabilities that may require entirely new generators.

### Open Question 3
How does the availability of a larger corpus of reference programs, such as those from the 2025 Google Code Golf Championship, affect the comparative evaluation of ARC generators? The evaluation metrics rely on a limited set of source programs that may not reveal edge cases where ARC-GEN's approach fails to capture the true solution space.

## Limitations
- Manual parameterization approach does not scale to ARC-AGI-2's expanded task set and creates maintenance burden
- Lack of statistical analysis comparing generated examples to original dataset beyond solver compatibility
- Parameterization based on ARC-AGI-1's object-centric tasks may not transfer to new reasoning capabilities in subsequent datasets

## Confidence
- **High Confidence**: 100% solver compatibility claim for ARC-GEN examples (directly measurable)
- **Medium Confidence**: "Mimetic" distributional properties claim (lacks statistical validation)
- **Low Confidence**: Scalability and maintenance implications of manual parameterization (not addressed)

## Next Checks
1. Generate 10,000 examples across multiple tasks and perform chi-squared tests comparing attribute distributions against the original ARC-AGI-1 dataset
2. Test whether solvers trained on ARC-GEN examples maintain performance on original ARC-AGI-1 tasks and whether ARC-GEN examples from one task generalize to other tasks
3. Manually parameterize 10 representative ARC-AGI-2 tasks using ARC-GEN's approach and evaluate solver performance to determine if methodology scales to new reasoning capabilities