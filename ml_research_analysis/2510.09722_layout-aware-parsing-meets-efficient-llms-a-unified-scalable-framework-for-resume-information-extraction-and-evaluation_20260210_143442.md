---
ver: rpa2
title: 'Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for
  Resume Information Extraction and Evaluation'
arxiv_id: '2510.09722'
source_url: https://arxiv.org/abs/2510.09722
tags:
- resume
- extraction
- text
- information
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation

## Quick Facts
- arXiv ID: 2510.09722
- Source URL: https://arxiv.org/abs/2510.09722
- Reference count: 33
- Primary result: 0.932 F1 on SynthResume and 0.964 F1 on RealResume with 1.54s latency using fine-tuned 0.6B LLM

## Executive Summary
This paper introduces a unified, scalable framework for resume information extraction that addresses both layout complexity and efficiency. The system combines layout normalization (segmentation + hierarchical reordering), parallel LLM extraction with index-based pointers, and a two-stage evaluation protocol. By fine-tuning a compact 0.6B model on 59.5K instruction samples, the framework achieves near-frontier accuracy while reducing inference latency by 3-4× compared to API-based LLMs. The approach is validated on both synthetic and real-world resume datasets, demonstrating robust performance across diverse document layouts.

## Method Summary
The framework processes resumes through a hybrid pipeline: first, a PDF parser extracts text via metadata and OCR, then a fine-tuned YOLOv10 segments the layout into linear blocks sorted by reading order. A compact Qwen3-0.6B model, fine-tuned on 59.5K instruction samples, performs parallel extraction of basic info, work experience, and education, returning JSON outputs with line-number pointers for long text fields. Post-processing re-extracts content using these pointers, and evaluation uses Hungarian alignment with field-specific matching strategies to compute F1 scores.

## Key Results
- Achieves 0.932 F1 on SynthResume and 0.964 F1 on RealResume
- Reduces inference latency to 1.54s per resume vs. 22.71s for naive Claude-4
- Ablation shows layout regenerator is critical: w/o it, overall accuracy drops from 0.932 to 0.916
- Pointer mechanism reduces token usage by 2-3× compared to verbatim generation

## Why This Works (Mechanism)

### Mechanism 1
Layout normalization (segmentation + hierarchical reordering) materially improves extraction quality on multi-column resumes. Fine-tuned YOLOv10 segments resumes into internally-linear blocks, then sorts inter-segment by top-left coordinate and intra-segment by reading order, converting spatially-disjoint text into a reading-order sequence. This is causally upstream of LLM extraction quality for ~20% of resumes with non-linear layouts. Ablation shows w/o Layout Generator drops overall accuracy from 0.932 to 0.916 on SynthResume.

### Mechanism 2
Task decomposition + index-based pointer outputs + SFT on a compact LLM jointly reduce inference latency without sacrificing accuracy. Parallelizing extraction into 3 independent prompts reduces per-prompt complexity; prompting LLM to return line-number spans instead of generating long descriptions reduces token usage and ensures exact content fidelity via post-hoc re-extraction; fine-tuning Qwen3-0.6B on 59.5K instruction samples bridges the gap between the compact model's base capacity and extraction needs. Qwen3-0.6B-SFT achieves F1=0.964 on RealResume at 1.54s latency vs 22.71s for naive Claude-4.

### Mechanism 3
Two-stage evaluation (Hungarian alignment → multi-strategy field matching) enables reliable automated F1 computation for list-style extractions. Building an M×N similarity matrix between ground-truth and predicted entities using key-field string similarity, then applying Hungarian algorithm for optimal 1:1 assignment, prevents inflated precision from spurious predictions due to quantity or order mismatches. Field-type-specific matching (edit distance for long text, partial match for named entities, normalized exact for dates) better reflects human judgment than uniform exact-match.

## Foundational Learning

- **Document Layout Analysis via Object Detection**: Understanding that layout parsing is framed as spatial segmentation (YOLO) rather than pure NLP enables reasoning about why bounding-box annotation matters and how visual coherence drives reading-order reconstruction. *Quick check*: Can you explain why a purely text-based NLP model would fail on a two-column resume with a sidebar, even if OCR output is perfect?

- **Instruction Tuning / Supervised Fine-Tuning (SFT) for LLMs**: The paper's efficiency gains hinge on SFT closing the gap between a 0.6B model and frontier models; understanding SFT helps assess whether your domain has sufficient labeled data to replicate this. *Quick check*: What minimum dataset characteristics (size, diversity, label quality) would you need to train Qwen3-0.6B-SFT for a new document type (e.g., invoices)?

- **Hungarian Algorithm for Bipartite Matching**: The evaluation stage treats prediction-to-ground-truth alignment as an assignment problem; you'll need to implement or adapt this for your own metrics pipeline. *Quick check*: Given 3 ground-truth work experiences and 5 predicted ones, how does the Hungarian algorithm prevent inflated precision from spurious predictions?

## Architecture Onboarding

- **Component map**: PDF Parser (Hybrid Extraction) -> Layout Regenerator (YOLOv10 segmentation + hierarchical sort) -> LLM Extractor (Parallel prompts) -> Post-Processor (Grounded re-extraction) -> Evaluation Module (Hungarian alignment + multi-strategy matching)
- **Critical path**: PDF input -> Layout Regenerator (without this, indexed text is unreliable) -> LLM Extractor (index pointers depend on stable indices) -> Post-Processor (content fidelity). If layout fails, LLM receives garbage ordering; if indices drift, pointer re-extraction returns wrong spans.
- **Design tradeoffs**: YOLO vs. heavier layout models (LayoutLM): paper chooses lightweight YOLO (~500 annotated resumes) for scalability; trades fine-grained segmentation for annotation efficiency. Compact SFT model vs. API LLMs: 0.6B model with SFT achieves near-frontier accuracy at 3–4× speedup; trades generalization breadth for domain specialization. JSON output without constrained decoding: prompt-based JSON generation + lightweight string extraction trades formal guarantees for simplicity; may occasionally produce malformed output.
- **Failure signatures**: Layout drift: multi-column resumes where segments overlap spatially but are semantically cross-referenced -> YOLO may create incorrect segment boundaries -> reading order corrupted. Pointer index misalignment: if OCR adds/omits lines after indexing, LLM-returned indices point to wrong content -> post-processor re-extracts incorrect spans. Hallucination leakage: post-processor source-text verification fails if key fields (company, title) appear in document but are misattributed across entities. Evaluation mismatch: Hungarian alignment pairs wrong entities if similarity matrix uses weak key fields -> F1 scores inflated/deflated.
- **First 3 experiments**: 1) Ablation on layout module: process 100 multi-column resumes with and without the Layout Regenerator; measure per-field F1 delta, focusing on Long Text fields. Expect significant drop without regenerator. 2) Pointer vs. verbatim generation: compare token count and latency for job description extraction using index pointers vs. full text generation on 50 resumes; verify content fidelity via exact match on re-extracted text. 3) Evaluation protocol validation: run Two-Stage Evaluation on 50 resumes with manual human labels; compute agreement rate between automated F1 and human judgment to validate the pipeline before scaling.

## Open Questions the Paper Calls Out
- **Dynamic, field-specific extraction strategies**: Future work will explore selectively applying different extraction models based on field type to further optimize performance. The current system applies a uniform pipeline to all fields, but naive LLMs sometimes outperform it on regular fields due to segmentation noise.
- **Layout regeneration refinement**: The current hierarchical re-ordering optimizes for general reading flow but introduces noise for simple, visually distinct fields like dates. It's unclear if the layout logic can simultaneously optimize for both global coherence and local preservation of simple entities.
- **Generalization to outlier formats**: While SFT boosts performance dramatically, the model's small size (0.6B parameters) makes it potentially brittle to layout heterogeneity not seen during the SFT process, such as radically different industry-specific formats.

## Limitations
- The claimed 20% prevalence of non-linear layouts is observational and not statistically validated across diverse resume corpora, potentially limiting generalizability to international or industry-specific formats.
- The specific YOLOv10 weights for layout segmentation are not provided, creating a reproducibility gap if the model overfits to the 500 annotated samples.
- The evaluation protocol's human validation is described but lacks quantitative inter-rater reliability metrics, leaving open the possibility of systematic bias in the "human judgment" baseline.

## Confidence
- **High**: Task decomposition + pointer-based extraction reduces latency (supported by direct latency measurements in Table 2)
- **Medium**: Layout normalization causally improves extraction quality (supported by ablation, but mechanism not fully isolated from other factors)
- **Medium**: Two-stage evaluation reflects human judgment (described but not quantitatively validated against human labels)

## Next Checks
1. **Layout ablation on domain-shifted resumes**: Test the Layout Regenerator on resumes from different countries (e.g., CVs with photo headers, academic CVs with publications) to measure robustness beyond the reported 20% figure.
2. **Pointer mechanism stress test**: Introduce synthetic line insertions/deletions in indexed text and measure whether LLM-returned indices still correctly map to intended spans, isolating content drift risk.
3. **Evaluation protocol bias audit**: Manually annotate 100 resume extractions and compute inter-rater agreement; compare automated F1 scores against multiple human labelers to quantify evaluation fidelity.