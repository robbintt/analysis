---
ver: rpa2
title: Improve LLM-based Automatic Essay Scoring with Linguistic Features
arxiv_id: '2502.09497'
source_url: https://arxiv.org/abs/2502.09497
tags:
- essay
- features
- linguistic
- language
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper combines linguistic features with large language model
  (LLM)-based automatic essay scoring to improve performance. By incorporating linguistic
  features into the zero-shot prompting approach, the proposed hybrid method achieves
  better alignment with human judgments compared to baseline models.
---

# Improve LLM-based Automatic Essay Scoring with Linguistic Features

## Quick Facts
- arXiv ID: 2502.09497
- Source URL: https://arxiv.org/abs/2502.09497
- Authors: Zhaoyi Joey Hou; Alejandro Ciuba; Xiang Lorraine Li
- Reference count: 35
- Primary result: LLM-based AES with linguistic features achieves QWK scores approaching GPT-4 performance

## Executive Summary
This paper proposes a hybrid approach to zero-shot automatic essay scoring (AES) that combines large language models (LLMs) with linguistic feature extraction. The method incorporates 10 empirically correlated linguistic features into the zero-shot prompting approach, achieving improved alignment with human judgments compared to baseline models. Experimental results show consistent improvements across both in-domain (ASAP dataset) and out-of-distribution (ELLIPSE dataset) essay scoring tasks. The study demonstrates that linguistic features can effectively enhance LLM-based zero-shot AES, though challenges remain in achieving generalizability across diverse essay types.

## Method Summary
The method combines linguistic feature extraction with zero-shot LLM prompting for automatic essay scoring. The approach computes 10 linguistic features (e.g., unique word count, lemma count) that have been shown to correlate with human scores, then injects these numerical values into zero-shot prompts as "Additional Information" framed as being "highly, positively correlated with the grade." The prompt structure includes an educational researcher persona, an explanation-first generation instruction, and the essay with its computed linguistic features. Mistral-7B-Instruct-v0.2 was used for inference with temperature=0 and max_tokens=4096, and outputs were parsed using a few-shot parsing LLM to extract structured scores.

## Key Results
- The hybrid method achieves better alignment with human judgments compared to baseline models without linguistic features
- Improvement in both in-domain (ASAP dataset) and out-of-distribution (ELLIPSE dataset) essay scoring tasks
- LLM performance approaches GPT-4 performance when linguistic features are included
- QWK scores improve from ~0.15 to ~0.25 on ELLIPSE dataset, though absolute performance remains limited

## Why This Works (Mechanism)

### Mechanism 1: Grounding Zero-Shot Prompts with Correlated Linguistic Signals
Providing explicit, empirically correlated linguistic features in the prompt gives the LLM concrete, quantitative anchors for scoring, potentially reducing reliance on poorly-calibrated priors. The paper computes 10 linguistic features selected based on prior work showing a Pearson's correlation of ≥0.6 with human scores, then injects these numerical values into the zero-shot prompt as "Additional Information" framed as being "highly, positively correlated with the grade."

### Mechanism 2: Anchoring LLM Evaluation with an Educational Persona
Establishing an "educational researcher" persona provides a task-specific context that guides the LLM's scoring behavior towards a more expert-like evaluation. The prompt begins with a persona pattern: "You are part of an educational research team analyzing the writing skills of students in grades 7 to 10."

### Mechanism 3: Improving Reliability with Explanation-First Generation
Requiring the LLM to generate an explanation before providing a score enforces a chain-of-thought process that can lead to more stable and justified evaluations. The analysis task instructs the model to first provide an explanation for its evaluation, and then conclude with a grade.

## Foundational Learning

- **Quadratic Weighted Kappa (QWK)**
  - Why needed here: This is the primary metric used to evaluate the system's performance. It measures the agreement between the predicted and human-annotated scores, correcting for chance agreement. A higher QWK indicates better alignment with human judgment.
  - Quick check question: What does a QWK score of 0.0 indicate compared to a score of 1.0?

- **Cross-Prompt / Out-of-Distribution (OOD) Evaluation**
  - Why needed here: The paper's core contribution is testing the system's generalizability. Understanding OOD is critical to appreciating why the method is tested on the ELLIPSE dataset, which is from a completely different source than the ASAP dataset used for prompt tuning.
  - Quick check question: Why is evaluating on an out-of-distribution dataset (like ELLIPSE) a more rigorous test of the method than only evaluating on a held-out portion of the training dataset (like ASAP)?

- **Zero-Shot Prompting**
  - Why needed here: The method operates in a zero-shot setting, meaning no task-specific training or fine-tuning is performed. The LLM is guided solely by the prompt. Understanding this is key to seeing why prompt engineering is the only lever for improvement.
  - Quick check question: In a zero-shot AES system, how is the model adapted to the specific grading rubric of a new essay prompt?

## Architecture Onboarding

- **Component map**: Feature Extraction Module -> Prompt Construction Module -> LLM Scoring Engine -> Output Parsing Module

- **Critical path**: The accuracy of the final score depends on the reliability of the feature extraction (garbage in, garbage out) and the effectiveness of the prompt construction in communicating those features to the LLM. The LLM Scoring Engine must correctly interpret all instructions. The Output Parsing Module must be robust enough to handle variations in the LLM's output format.

- **Design tradeoffs**: Feature Set Size (1, 3, or 10 features), Model Choice (Mistral-7B vs GPT-4), Prompting Strategy (Explanation → Scoring vs alternatives)

- **Failure signatures**: Output Parsing Failure (<7% error rate), Misinterpretation of Instructions, Context Mismatch (grade level mismatch between persona and target dataset)

- **First 3 experiments**: 1) Reproduce baseline without "Additional Information" section to establish QWK on ASAP, 2) Vary number of linguistic features (1, 3, 10) to confirm feature inclusion improves performance, 3) Generate batch of responses and run parsing module, manually inspecting failures to understand format deviations

## Open Questions the Paper Calls Out

- **Does the positive impact of linguistic features on zero-shot AES generalize across a wider variety of LLM architectures?**
  - Basis in paper: Authors note they only tested "one open-source and one close-source" model and state, "Future work could look at including more LLMs for comparison."
  - Why unresolved: Study restricted to Mistral-7B and GPT-4; unclear if feature injection method is robust across other model families.
  - What evidence would resolve it: Replicating pipeline on ASAP and ELLIPSE datasets using 3-5 diverse LLMs and comparing QWK improvements.

- **Can incorporating fine-grained rubric scores into the prediction process enhance the interpretability of the holistic score?**
  - Basis in paper: Authors state prediction target is "hard to interpret" and suggest "Incorporating them [rubric scores]... would make the overall score more transparent."
  - Why unresolved: Current method predicts single holistic score without utilizing available fine-grained annotations to explain reasoning.
  - What evidence would resolve it: Multi-step prompting experiment where model generates trait scores to support or condition final holistic prediction.

- **Does aligning the prompt's persona grade level with the target dataset demographics (grades 8-12) significantly improve scoring accuracy?**
  - Basis in paper: Authors list limitation where prompt says "grade 7 to 10" but ELLIPSE data is "grades 8 to 12," which "might lead to performance differences."
  - Why unresolved: Specific effect of this demographic mismatch on model's calibration and scoring reliability was not quantified.
  - What evidence would resolve it: Ablation study comparing scores from original prompt against modified prompt reflecting correct grade level.

## Limitations

- **Reproducibility of feature computation**: Word counts "are not normalized as in the original paper," suggesting potential inconsistencies in how linguistic features are computed.
- **Few-shot parser reliability**: Output parsing module has reported error rate of <7%, representing a non-trivial failure mode not fully characterized.
- **Context mismatch**: "Grades 7-10" persona may not generalize to ELLIPSE (grades 8-12), potentially creating subtle mismatch affecting scoring.

## Confidence

- **High confidence**: Incorporating linguistic features improves AES performance over baselines without features
- **Medium confidence**: Improvement comes specifically from "grounding" LLM with correlated linguistic signals
- **Medium confidence**: "Significant" improvement in cross-prompt generalizability, though absolute gains may not be practically significant

## Next Checks

1. **Feature normalization audit**: Implement exact linguistic feature extraction pipeline from Ridley et al. (2021b) and verify whether normalization differences affect correlation with human scores and subsequent LLM performance.

2. **Mechanism ablation study**: Systematically test three proposed mechanisms (linguistic features, educational persona, explanation-first generation) in isolation by creating prompts that test each component independently, then combinatorially.

3. **Cross-grade prompt adaptation**: Test hypothesis that "grades 7-10" persona creates context mismatch for ELLIPSE (grades 8-12) by modifying persona to match target grade range and measuring impact on performance.