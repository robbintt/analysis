---
ver: rpa2
title: Token Level Routing Inference System for Edge Devices
arxiv_id: '2504.07878'
source_url: https://arxiv.org/abs/2504.07878
tags:
- inference
- routing
- large
- token
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a token-level routing inference system designed
  for efficient deployment of large language models on edge devices. The system uses
  a small language model for on-device inference and selectively routes critical tokens
  to a cloud-based large language model, guided by a confidence-based MLP router.
---

# Token Level Routing Inference System for Edge Devices
## Quick Facts
- arXiv ID: 2504.07878
- Source URL: https://arxiv.org/abs/2504.07878
- Reference count: 5
- Primary result: 60% performance gain on CommonsenseQA using 0.5B model on M1 MacBook with <7% token routing to cloud LLM

## Executive Summary
This paper introduces a token-level routing inference system designed to efficiently deploy large language models on edge devices by combining on-device inference with selective cloud-based processing. The system uses a small language model for local inference while routing critical tokens to a cloud-based large language model based on confidence scores from a lightweight MLP router. The approach achieves significant performance improvements while maintaining low latency and minimizing the computational burden on edge devices.

The system demonstrates practical implementation by integrating the routing mechanism into a full client-server architecture compatible with edge deployment. Using ONNX for on-device inference and SGLang for server-side LLM serving, the system achieves a 60% performance gain on CommonsenseQA while keeping the routing overhead minimal, with under 7% of tokens being uploaded to the cloud. The average generation speed remains approximately 4 tokens per second on an M1 chip, making it suitable for real-world edge deployments.

## Method Summary
The routing inference system employs a small language model running locally on edge devices for most token generation, while a confidence-based MLP router determines when to route tokens to a larger cloud-based language model. The router evaluates each token's confidence score during generation, and tokens falling below a threshold are sent to the cloud for processing by a more capable model. The system uses ONNX for efficient on-device inference and SGLang for server-side LLM serving, creating a seamless client-server architecture that balances performance and resource constraints.

The routing mechanism is integrated directly into the generation process, allowing for dynamic decision-making at the token level. This approach minimizes both computational load on the edge device and the amount of data transmitted to the cloud, addressing two key challenges in edge AI deployment: limited local resources and network bandwidth constraints.

## Key Results
- 60% performance improvement on CommonsenseQA using only a 0.5B model on M1 MacBook
- Under 7% of tokens routed to the cloud-based large language model
- Average generation speed of approximately 4 tokens per second on M1 chip
- Low latency maintained despite routing mechanism, suitable for real-time applications

## Why This Works (Mechanism)
The system works by leveraging the complementary strengths of small and large language models through intelligent token routing. The small model handles the majority of token generation efficiently on-device, while the MLP router identifies tokens where the small model's confidence is low. These critical tokens are then routed to the more capable cloud-based LLM, ensuring high-quality outputs without overwhelming the edge device's resources. This selective approach optimizes the trade-off between computational efficiency and output quality.

The confidence-based routing mechanism is crucial because it allows the system to dynamically adapt to the complexity of the input and the capabilities of the local model. By only routing uncertain tokens, the system minimizes cloud communication overhead while maximizing the benefits of the larger model's capabilities where they are most needed.

## Foundational Learning
**Edge AI Deployment** - Why needed: Understanding constraints of edge devices (limited compute, memory, battery) that necessitate offloading to cloud; Quick check: Verify local model size and inference time measurements.

**Confidence-Based Routing** - Why needed: Mechanism for determining when local model performance is insufficient; Quick check: Examine confidence score distributions and routing thresholds.

**Client-Server Architecture** - Why needed: Framework for integrating local and cloud processing; Quick check: Review network communication patterns and latency measurements.

**ONNX Runtime** - Why needed: Optimized inference engine for edge deployment; Quick check: Confirm ONNX model compatibility and performance metrics.

**SGLang Serving** - Why needed: Efficient LLM serving infrastructure for cloud component; Quick check: Validate server-side inference times and throughput.

## Architecture Onboarding

**Component Map:** Edge Device (Small LM + ONNX Runtime + Router) -> Router Decision -> Cloud Server (Large LM + SGLang)

**Critical Path:** Token generation → Confidence scoring → Router evaluation → Local generation or cloud routing → Response assembly

**Design Tradeoffs:** The system balances three competing factors: local inference speed, cloud communication overhead, and output quality. Routing fewer tokens reduces bandwidth usage and cloud costs but may sacrifice accuracy. The confidence threshold must be tuned to optimize this balance for specific use cases.

**Failure Signatures:** Performance degradation may occur when: confidence threshold is set too high (excessive routing), too low (insufficient quality), network latency is high (routing delays), or local model capacity is inadequate for the task complexity (excessive routing).

**Three First Experiments:**
1. Vary the confidence threshold and measure impact on routing percentage and task performance
2. Compare routing performance across different local model sizes (0.1B, 0.5B, 1B)
3. Test system behavior under different network conditions (simulated latency, bandwidth variations)

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Scalability uncertainty for diverse tasks beyond CommonsenseQA
- Performance unknown on resource-constrained edge devices beyond M1 MacBook
- Effectiveness may vary significantly across different domains and question types
- Limited evaluation on a single task and device combination

## Confidence
- 60% performance improvement on CommonsenseQA: High
- System generalizability to other tasks: Medium
- Performance on other edge devices: Medium
- Under 7% token routing claim: High
- Low-latency claims under varying network conditions: Medium

## Next Checks
1. Evaluate the system's performance across a diverse set of benchmarks beyond CommonsenseQA to assess generalizability.
2. Test the routing mechanism's effectiveness on various edge devices with different computational capabilities to determine scalability.
3. Conduct experiments under varying network conditions to quantify the impact on latency and overall system performance.