---
ver: rpa2
title: Trapped in the past? Disentangling fluid and crystallized intelligence of large
  language models using chess
arxiv_id: '2601.16823'
source_url: https://arxiv.org/abs/2601.16823
tags:
- positions
- reasoning
- intelligence
- chess
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether Large Language Models (LLMs) rely
  on memorization or reasoning when solving chess problems. Using chess as a controlled
  testbed, the authors construct three categories of positions based on their likelihood
  of appearing in training data: within-distribution (common positions), near-distribution
  (structurally similar but unseen), and out-of-distribution (novel positions).'
---

# Trapped in the past? Disentangling fluid and crystallized intelligence of large language models using chess

## Quick Facts
- arXiv ID: 2601.16823
- Source URL: https://arxiv.org/abs/2601.16823
- Reference count: 18
- Primary result: LLMs perform worse on chess positions less likely to appear in training data, showing diminishing reasoning gains in out-of-distribution tasks

## Executive Summary
This study investigates whether Large Language Models rely on memorization or reasoning when solving chess problems. Using chess as a controlled testbed, the authors construct three categories of positions based on their likelihood of appearing in training data: within-distribution (common positions), near-distribution (structurally similar but unseen), and out-of-distribution (novel positions). They evaluate GPT-3.5, GPT-4, and GPT-5 on these positions using centipawn loss and illegal move rates as performance metrics.

Results show a clear gradient: performance degrades as positions become less likely to have been seen during training. In out-of-distribution positions, models perform no better than random play, even with reasoning enabled. Newer model generations show consistent but diminishing improvements, with the smallest gains in out-of-distribution tasks. While reasoning augmentation helps, its marginal benefit decreases with decreasing training data proximity. These findings suggest that current LLMs are limited in systematic generalization, relying primarily on crystallized intelligence and struggling with fluid reasoning in novel contexts.

## Method Summary
The study uses chess as a controlled testbed to disentangle fluid and crystallized intelligence by creating positions with varying training data proximity. The authors generate 1,500 chess positions across three categories: within-distribution (WD, positions appearing ≥1,000 times in Lichess Masters database), near-distribution (ND, 10 random legal moves from starting position excluded from database), and out-of-distribution (OOD, random placement of 10 pieces per side). They query GPT-3.5, GPT-4, and GPT-5 models via OpenAI API with standardized prompts requesting JSON responses containing moves and reasoning. Performance is evaluated using centipawn loss (CPL) against Stockfish 17.1 at depth 30, along with illegal move rates and random-normalized ACPL.

## Key Results
- Clear performance gradient: WD (best) > ND (moderate) > OOD (worst), with OOD performance no better than random play
- Newer model generations show consistent but diminishing improvements, with smallest gains in OOD tasks
- Chain-of-thought reasoning primarily amplifies crystallized knowledge retrieval rather than enabling de novo fluid reasoning
- Marginal ACPL improvement per reasoning token drops 88.56% for OOD vs WD positions

## Why This Works (Mechanism)

### Mechanism 1
Positional frequency in game databases approximates training data likelihood, enabling systematic variation of crystallized vs. fluid intelligence demands. Chess exhibits combinatorial explosion (~10^120 possible games), so early-game positions appear frequently in corpora while midgame positions are effectively novel. By sampling positions with known database frequencies (Lichess Masters, 2.7M+ games), the framework creates a gradient from memorizable (WD: ≥1,000 occurrences) to novel (OOD: random piece placement). Core assumption: Lichess Masters database coverage correlates with LLM training corpus exposure.

### Mechanism 2
Centipawn Loss (CPL) against engine evaluation quantifies reasoning quality with finer granularity than binary win/loss metrics. Stockfish 17.1 at depth 30 provides a computational optimum baseline. CPL measures decision regret as eval_before - eval_after, capturing incremental errors (CPL <10 = optimal, >100 = blunder). This distinguishes "sound but imperfect" from "effectively random" reasoning. Core assumption: Engine evaluations approximate ground truth for tactical soundness, though not for human-like strategic understanding.

### Mechanism 3
Chain-of-thought reasoning primarily amplifies crystallized knowledge retrieval rather than enabling de novo fluid reasoning. GPT-5 allocates more reasoning tokens to OOD positions (16,952 avg) than WD (3,426), yet marginal ACPL improvement per token drops 88.56% for OOD vs WD. This suggests CoT helps locate relevant memories but cannot construct solutions when no pattern anchors exist. Core assumption: Token allocation reflects model's internal uncertainty assessment.

## Foundational Learning

- **Concept: Fluid vs. Crystallized Intelligence (Cattell, 1963)**
  - Why needed here: The entire framework operationalizes this distinction—crystallized as WD performance (retrieval), fluid as OOD performance (generalization).
  - Quick check question: Can you explain why memorizing opening theory is crystallized, but finding a tactic in an unfamiliar position is fluid?

- **Concept: Distributional Shift and OOD Generalization**
  - Why needed here: The paper's core claim is that LLMs fail systematically under distribution shift; understanding this is prerequisite to interpreting the results.
  - Quick check question: If a model trained on positions 1-10 moves deep is tested on move 30 positions, what type of shift is this?

- **Concept: Centipawn Loss as Decision Regret**
  - Why needed here: Interpreting results requires understanding that CPL <50 = minor inaccuracy, CPL >100 = significant mistake, illegal move = 1000.
  - Quick check question: Why might average CPL be more informative than move accuracy percentage?

## Architecture Onboarding

- **Component map:** Position Generator → FEN Encoding → LLM API (GPT-3.5/4o/5) → Stockfish 17.1 ← Move Parser ← JSON Response → CPL Calculator → Aggregated Metrics by Condition
- **Critical path:** 1) Generate 500 positions per condition (WD via BFS from openings, ND via 10 random legal moves, OOD via random piece placement) 2) Query each model with identical prompts at temperature=0 3) Parse SAN notation, validate legality, compute CPL vs Stockfish 4) Aggregate ACPL and illegal-move rates by model×condition
- **Design tradeoffs:** OOD positions appear "unnatural"—intentionally breaks heuristic patterns but may conflate tactical vs. strategic reasoning. Cannot strictly preclude data leakage; results likely represent upper bound on fluid capability. Engine evaluation depth (30) trades computation cost for evaluation reliability.
- **Failure signatures:** High illegal-move rate (>30% for GPT-5 OOD) indicates rule-consistency failure, not just strategic weakness. ACPL worse than random baseline suggests negative transfer from memorized patterns. Diminishing improvement rates across generations (9.08%→5.73% for OOD) signals scaling plateau.
- **First 3 experiments:** 1) Replicate with different formal domains (e.g., theorem proving in Lean, circuit verification) to test generalization of the WD/OOD gradient pattern. 2) Introduce intermediate conditions between ND and OOD (e.g., 15-move random play, 8-piece random placement) to map the performance degradation curve more finely. 3) Ablate reasoning effort at finer granularity (minimal/low/medium/high) to identify where marginal returns approach zero for OOD tasks.

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed limitations in OOD generalization for chess extend to other formal domains such as mathematics and software synthesis? The authors state that "Given that chess serves as a high-fidelity proxy for other formal systems, these findings cast doubt on the ability of pure LLMs to robustly generalize in domains like mathematics or software synthesis without the support of dense training coverage." This remains a hypothesis without empirical validation across other formal systems with comparable distributional control.

### Open Question 2
What architectural mechanisms beyond scale could enable genuine fluid intelligence rather than merely amplifying crystallized knowledge? The paper concludes that "current architectures may face inherent challenges" and calls for "mechanisms beyond scale to achieve robust fluid intelligence" and "new forms of representation and inference capable of capturing and composing latent causal and relational structure."

### Open Question 3
Can chain-of-thought reasoning mechanisms be redesigned to function as engines of fluid discovery rather than primarily as retrieval optimizers? The authors find that "reasoning mostly amplifies WD performance but may not confer OOD generalization" and that current CoT "function primarily as retrieval optimizers–helping the model locate the correct memory–rather than as engines of first-principles derivation."

## Limitations
- Training corpus leakage cannot be strictly ruled out; results likely represent upper bound on fluid intelligence
- Engine evaluation reliability concerns, particularly for OOD positions where human and engine preferences may diverge
- Generalizability of chess-specific patterns to other domains remains uncertain

## Confidence
- **High Confidence:** The core finding that LLMs show degraded performance on less frequent positions (WD > ND > OOD gradient) is well-supported by consistent ACPL and illegal move rate patterns across multiple metrics and model generations.
- **Medium Confidence:** The interpretation that CoT primarily amplifies retrieval rather than enabling de novo reasoning is supported by token allocation data, but could reflect optimization toward expected evaluation metrics rather than genuine reasoning limitations.
- **Low Confidence:** Claims about fundamental architectural barriers to fluid intelligence are extrapolations beyond the empirical scope. The results demonstrate current limitations but don't definitively establish impossibility of future improvements.

## Next Checks
1. **Cross-Domain Replication:** Replicate the WD/ND/OOD framework in domains with different combinatorial properties (e.g., mathematical theorem proving, chemical synthesis planning) to test whether performance degradation patterns persist.
2. **Fine-Grained Position Analysis:** Generate intermediate conditions between ND and OOD (e.g., 15 random moves, 8-piece random placement) to map the precise shape of performance degradation and identify inflection points.
3. **Reasoning Effort Ablation Study:** Systematically vary reasoning token allocation at finer granularity (minimal/low/medium/high) to precisely locate where marginal returns approach zero for OOD tasks, distinguishing retrieval optimization from genuine reasoning limits.