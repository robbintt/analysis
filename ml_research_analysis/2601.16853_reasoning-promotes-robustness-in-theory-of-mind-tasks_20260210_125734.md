---
ver: rpa2
title: Reasoning Promotes Robustness in Theory of Mind Tasks
arxiv_id: '2601.16853'
source_url: https://arxiv.org/abs/2601.16853
tags:
- reasoning
- tasks
- test
- performance
- they
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether reasoning-oriented large language models
  (LLMs), trained via reinforcement learning with verifiable rewards (RLVR), exhibit
  improved performance on Theory of Mind (ToM) tasks compared to traditional LLMs.
  To test this, the authors use a battery of ToM tasks, including Sally-Anne tests,
  Strange Stories, Imposing Memory tests, and novel modifications of simple ToM tasks.
---

# Reasoning Promotes Robustness in Theory of Mind Tasks

## Quick Facts
- arXiv ID: 2601.16853
- Source URL: https://arxiv.org/abs/2601.16853
- Reference count: 40
- Primary result: Reasoning-oriented LLMs trained via RLVR show improved and more robust performance on Theory of Mind tasks compared to traditional LLMs.

## Executive Summary
This paper investigates whether reasoning-oriented large language models (LLMs), specifically those trained via reinforcement learning with verifiable rewards (RLVR), demonstrate enhanced Theory of Mind (ToM) capabilities compared to traditional LLMs. Through a comprehensive battery of ToM tasks including Sally-Anne tests, Strange Stories, Imposing Memory tests, and novel modifications, the authors find that reasoning models consistently outperform their non-reasoning counterparts and exhibit greater robustness to prompt variations. The study concludes that the observed improvements are more likely due to enhanced robustness in finding correct solution paths rather than fundamentally new forms of ToM reasoning. Analysis of reasoning traces reveals that while models can spontaneously generate task-specific strategies, these may sometimes reflect pattern-matching heuristics rather than genuine understanding.

## Method Summary
The study employs a battery of established and novel ToM tasks to evaluate reasoning models against traditional LLMs. Tasks include Sally-Anne tests, Strange Stories, Imposing Memory tests, and eight novel "trivial modifications" that vary object visibility and spatial relationships. Models are evaluated using temperature=0, with non-reasoning models prompted to generate answers followed by "Why?" follow-ups to elicit reasoning. Scoring uses a 0-2 point rubric based on correctness of both answer and reasoning trace, with special emphasis on "non-merging" (distinguishing character beliefs from reality) and "mentalizing" (requiring mental state attribution rather than simpler heuristics). The study compares reasoning models (Claude, R1, GPT-5) against non-reasoning baselines, with Claude's "thinking on/off" feature providing controlled ablation experiments.

## Key Results
- Reasoning models consistently outperform non-reasoning models on standard ToM benchmarks and show greater robustness to prompt perturbations.
- The improvement is attributed to enhanced robustness in finding correct solution paths rather than fundamentally new reasoning capabilities.
- All models fail "trivial modifications" involving transparent containers and spatial relationship changes, suggesting possible visual-spatial simulation limitations.
- Reasoning traces reveal spontaneous generation of task-specific strategies like "filtering facts" and "perspective taking" without explicit prompting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLVR training improves ToM task performance by enhancing robustness in finding correct solution paths, not by creating fundamentally new reasoning capabilities.
- Mechanism: RLVR trains models to generate chain-of-thought reasoning before answering, improving sampling efficiency from the base model's existing solution space and making models more consistent at arriving at answers they could already reach, particularly under prompt perturbations.
- Core assumption: Reasoning traces are at least partially representative of the solution strategy the model is using.
- Evidence anchors: Analysis indicates gains are attributed to increased robustness rather than new forms of ToM reasoning; narrowing of reasoning coverage may be precisely their strength; small LLM study suggests RL benefits may be efficiency-based.

### Mechanism 2
- Claim: Inference-time reasoning traces help models decompose ToM tasks by spontaneously generating task-specific strategies.
- Mechanism: Reasoning models trained with RLVR emergently use structured strategies like "filtering facts" and "perspective taking" in their CoTs without explicit prompting, decomposing complex mental state tracking into sequential, verifiable sub-steps.
- Core assumption: Verbalized steps in the CoT causally contribute to the final answer (faithfulness assumption).
- Evidence anchors: Reasoning models use perspective-taking steps as an emergent skill; CoT allows models to decompose problems and allocate time to sub-problems; DEL-ToM proposes inference-time scaling for ToM using dynamic epistemic logic.

### Mechanism 3
- Claim: Certain "trivial" ToM task modifications require visual-spatial simulation, creating a modality-specific failure mode for text-only models.
- Mechanism: Some task variants (transparent containers, object relocation) require understanding spatial relationships and visual access, which text-only LLMs attempt via semantic patterns, leading to systematic failures even when logical reasoning is sound.
- Core assumption: Human success relies on mental imagery, and LLM failure is primarily a modality limitation rather than a pure reasoning deficit.
- Evidence anchors: Trivial modifications 2A and 2B are not really trivial as they require visual mental depiction; universal failure on these tasks across all tested models; MindPower addresses ToM for VLM-based embodied agents.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: This is the core training paradigm distinguishing "reasoning models" from traditional LLMs and is central to the paper's question about ToM improvements.
  - Quick check question: How does a "verifiable reward" differ from the reward model used in standard RLHF?

- **Concept: Theory of Mind (ToM) Evaluation Criteria**
  - Why needed here: To critically assess the paper's claims, one must understand what constitutes a valid ToM test: "non-merging" (mental states differ from one's own) and "mentalizing" (success requires mental state attribution, not simpler heuristics).
  - Quick check question: Why is the "false belief test" considered the field's litmus test, and what two principles must a valid ToM task satisfy?

- **Concept: Inference-Time Scaling**
  - Why needed here: The paper frames CoT reasoning as a form of allocating more compute at inference time to improve performance, a key alternative to parameter scaling.
  - Quick check question: How does generating additional "thought" tokens before an answer constitute a form of scaling?

## Architecture Onboarding

- **Component map**: Base LLM -> RLVR Training Phase -> Learned Inference Strategy (CoT) -> Improved Robustness in Solution Search -> Higher ToM Task Accuracy & Consistency
- **Critical path**: RLVR Training → Learned CoT Strategy → Improved Robustness in Solution Search → Higher ToM Task Accuracy & Consistency
- **Design tradeoffs**:
  - Robustness vs. Capability Expansion: RLVR may narrow solution paths for consistency rather than expanding the set of solvable problems.
  - CoT Visibility vs. Competitive Protection: API providers may hide full CoT traces, limiting research interpretability.
  - Base Model vs. Reasoning Model Comparison: Perfect controlled comparisons are often impossible due to API constraints.
- **Failure signatures**:
  - Spurious Meta-Knowledge: CoT traces contain phrases like "this is a classic false belief test" without deep understanding.
  - Self-Correction Instability: Non-reasoning models may answer incorrectly, then correct themselves when asked "Why?".
  - Hallucination in Reasoning: Models infer facts not in the story or fail to separate question from narrative.
  - Visual-Spatial Blindness: Universal failure on tasks requiring mental visualization of scenes.
- **First 3 experiments**:
  1. Controlled Ablation (Thinking On/Off): Using Claude, run standard and perturbed ToM tasks comparing accuracy and reasoning trace quality with thinking toggled.
  2. Pass@k Sampling on Novel Tasks: Compare base LLM and RLVR counterpart using pass@k on ToM tasks not in public benchmarks.
  3. Modality-Specific Probe: Design matched text-only and image-based versions of "trivial modifications" to compare text-only LLMs against VLMs.

## Open Questions the Paper Calls Out
- Can the robustness gains of RLVR-trained models on ToM tasks be quantified using the pass@k metric when comparing them directly to their base model counterparts?
- To what extent can the observed performance improvements be attributed specifically to RLVR training versus other inference-time scaling techniques?
- Do the "trivial" modifications that all models failed require visual-spatial representations that current LLMs fundamentally lack?
- To what extent do reasoning traces accurately reflect the true cognitive processes underlying model responses in ToM tasks?

## Limitations
- Core claim about robustness vs. capability expansion is based on comparisons where controlled ablations are only available for Claude.
- Assessment of reasoning quality depends on the assumption that verbalized CoT traces faithfully represent the model's solution strategy.
- Systematic failure on "trivial modifications" is not conclusively established as a fundamental visual-spatial limitation.

## Confidence
- **High confidence**: Reasoning models outperform base models on standard ToM benchmarks and the improvement persists under prompt perturbations.
- **Medium confidence**: The mechanism of improvement is primarily robustness in solution finding rather than expansion of the reachable solution space.
- **Low confidence**: The specific claim about visual-spatial simulation requirements for certain task modifications, as the modality limitation hypothesis is not directly tested.

## Next Checks
1. **Controlled ablation across model families**: Run Sally-Anne and Strange Stories variants with and without enforced CoT generation on multiple reasoning models to verify the robustness effect generalizes.
2. **Novel task space exploration**: Design and test a battery of ToM tasks outside standard benchmarks, comparing pass@k performance between base and reasoning models to test whether RLVR expands the solvable space.
3. **Cross-modality replication**: Implement matched text-only and image-based versions of the "trivial modifications" and test with both text-only LLMs and VLMs to quantify the visual-simulation hypothesis.