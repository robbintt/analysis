---
ver: rpa2
title: Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large
  Language Models
arxiv_id: '2511.05184'
source_url: https://arxiv.org/abs/2511.05184
tags:
- reasoning
- language
- tasks
- llms
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the role of Chain-of-Thought (CoT) in distilling
  reasoning capability from larger Large Language Models (LLMs) to smaller ones using
  white-box Knowledge Distillation (KD). The study employs CoT data from the CoT-Collection
  dataset and evaluates distilled models on the BIG-Bench-Hard (BBH) benchmark.
---

# Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models

## Quick Facts
- arXiv ID: 2511.05184
- Source URL: https://arxiv.org/abs/2511.05184
- Authors: Cong-Thanh Do; Rama Doddipatola; Kate Knill
- Reference count: 16
- Primary result: Incorporating CoT into white-box KD improves distilled model performance by 5-7% on BBH tasks

## Executive Summary
This paper investigates whether Chain-of-Thought (CoT) rationales can enhance knowledge distillation from larger Large Language Models (LLMs) to smaller ones. Using white-box Knowledge Distillation with the CoT-Collection dataset, the study trains student models on complete CoT sequences (prompt + rationale + answer) rather than just final answers. Experiments with Qwen and Llama2 families show that CoT integration consistently improves average performance across natural language reasoning and understanding tasks, with gains of 5-7% relative to vanilla KD. The approach is particularly effective when vanilla KD fails to improve upon the baseline student performance.

## Method Summary
The study employs white-box Knowledge Distillation using the MiniLLM framework, where student models learn by minimizing KL divergence between their logits and the teacher's logits across entire CoT sequences. The CoT-Collection dataset provides 1.44M training instances with prompt, rationale, and answer. Two teacher-student pairs are tested: Qwen-7B → Qwen-1.8B and Llama2-13B → Llama2-7B. Training runs for 20,000 steps with LoRA optimization for the 7B parameter student. Performance is evaluated on the BIG-Bench-Hard benchmark using few-shot CoT prompting with 3 demonstrations.

## Key Results
- Qwen-1.8B+KD+CoT improved by 7.54% relative to Qwen-1.8B+KD
- Llama2-7B+KD+CoT improved by 5.22% relative to Llama2-7B+KD
- CoT integration rescues performance when vanilla KD fails to outperform baseline
- Improvements are consistent across natural language reasoning and understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
Incorporating CoT rationales into white-box KD transfers reasoning capability by matching the teacher's step-by-step probability distribution rather than just final answers. The student learns intermediate logical steps through back-propagation across extended sequences, internalizing the teacher's reasoning process.

**Core assumption:** The teacher possesses superior reasoning encoded in its output probabilities for rationale tokens, and the student has sufficient capacity to model these intermediate steps.

**Evidence anchors:**
- [abstract] "CoT enhances the effectiveness of white-box KD... eliciting and transferring the teacher LLM's reasoning capability."
- [section 3] "Incorporating these rationales... is expected to elicit the teacher LLM's reasoning capability... by minimising the distillation loss between the models' output probabilities."

**Break condition:** If the student lacks parameter capacity to represent rationale complexity, the CoT signal cannot be effectively assimilated.

### Mechanism 2
CoT data acts as a corrective signal that rescues performance when vanilla white-box KD fails to outperform baseline. Dense supervisory signals from intermediate steps appear to regularize training or provide a more gradual curriculum for alignment.

**Core assumption:** Vanilla KD degradation stems from lack of intermediate reasoning context rather than fundamental architecture mismatch.

**Evidence anchors:**
- [section 4.2.1] "In the Llama2-based experiments... the vanilla white-box KD yields improved performance for some tasks but not on average... In contrast, the KD+CoT method improves average performance."
- [table 2] Shows Llama2-7B+KD dropping to -0.56% relative improvement, while KD+CoT recovers it to +5.22%.

**Break condition:** If CoT contains flawed or hallucinated reasoning, the student will distill incorrect logic.

### Mechanism 3
CoT improves the student's ability to apply world knowledge and linguistic nuance by extending the inference context window during training. Long chains force the model to maintain context and perform multi-step retrieval over longer sequences.

**Core assumption:** Improvements in reasoning tasks generalize from the specific distribution of CoT-Collection training data.

**Evidence anchors:**
- [section 4.2.2] "The Llama2-7B+KD+CoT model successfully retrieves the correct German translation... and accurately identify the error." (Example iv)

**Break condition:** Tasks requiring strict algorithmic execution may show less improvement or regression if CoT data lacks sufficient diversity or rigor in formal logic.

## Foundational Learning

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** This is the specific loss function used for white-box distillation in the paper. Understanding it is required to grasp what signal is being transferred (probability distribution similarity).
  - **Quick check question:** Does minimizing KL divergence force the student to mimic the teacher's exact output weights or the shape of its output probability distribution?

- **Concept: White-box vs. Black-box Knowledge Distillation**
  - **Why needed here:** The paper explicitly differentiates its approach (using internal logits/probabilities) from black-box methods (using generated text only).
  - **Quick check question:** If you only have API access to a model (inputs in, text out), can you implement the KD method described in this paper?

- **Concept: Rationales in Chain-of-Thought**
  - **Why needed here:** The core intervention is adding "rationales" to the training data. Distinguishing a rationale (reasoning steps) from the final answer is crucial for data preparation.
  - **Quick check question:** In the CoT-Collection dataset used, is the "rationale" the final label or the intermediate text leading to the label?

## Architecture Onboarding

- **Component map:**
  Teacher Model (Frozen) -> Tokenizer (shared vocabulary) -> Student Model (Trainable) -> Loss Function (KL divergence) -> Optimizer (LoRA for 7B)

- **Critical path:**
  1. Data Prep: Ensure Teacher and Student share exact same vocabulary size and tokenizer
  2. Forward Pass: Feed training instances (including rationales) through both Teacher and Student
  3. Loss Calculation: Compute KL divergence between Student logits and Teacher logits (Temperature τ=1)
  4. Optimization: Back-propagate error to update Student weights only (using LoRA for larger students)

- **Design tradeoffs:**
  - Vocabulary Constraint: Cannot easily distill from Qwen to Llama using this white-box method because their vocabularies differ (N=151,936 vs N=32,000)
  - LoRA vs. Full Fine-tuning: Uses LoRA (rank=32) for Llama2-7B to reduce compute, which may cap potential distillation quality

- **Failure signatures:**
  - Rationale Hallucination: Student learns confident but incorrect logic (e.g., claiming a baseball player scored a touchdown)
  - Negative Transfer: Performance drops below baseline (e.g., TinyLlama on Boolean Expressions)

- **First 3 experiments:**
  1. Vanilla Baseline: Run white-box KD using only prompt and final answer (filter out rationales)
  2. KD+CoT Integration: Run white-box KD using full CoT-Collection instances (Prompt + Rationale + Answer)
  3. Capacity Check: Compare relative gain of KD+CoT on 1.8B vs 7B parameter model to determine student size bottleneck

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the results raise several implicit questions about the universality of CoT benefits, the impact of student-teacher architectural differences, and the potential for propagating hallucinations through the distillation process.

## Limitations

- Only tests two teacher-student model pairs, limiting generalizability across different architectures
- CoT-Collection dataset may not fully represent BBH diversity, creating potential distribution shifts
- Doesn't explore alternative distillation methods for comparison to assess CoT's unique advantages

## Confidence

**High Confidence:** The empirical observation that CoT improves average BBH performance by 5-7% relative to vanilla KD across multiple student models.

**Medium Confidence:** The claim that CoT "elicits and transfers the teacher LLM's reasoning capability" to the student, as performance improvements don't necessarily prove reasoning process learning.

**Low Confidence:** The assertion that CoT is particularly effective "when the vanilla approach does not outperform the baseline," based on a single Llama2 experiment that could be an outlier.

## Next Checks

1. **Cross-Architecture Distillation Test:** Attempt distillation from Qwen teacher to Llama student to verify whether CoT benefits persist when vocabulary alignment is not guaranteed.

2. **Ablation on Rationale Quality:** Generate corrupted CoT data where rationales contain logical errors or hallucinations, then measure whether KD+CoT still outperforms vanilla KD.

3. **Long-Term Stability Evaluation:** Track student performance across multiple checkpoints to determine if CoT provides consistent gains throughout training or if benefits emerge only in later stages.