---
ver: rpa2
title: 'Outbidding and Outbluffing Elite Humans: Mastering Liar''s Poker via Self-Play
  and Reinforcement Learning'
arxiv_id: '2511.03724'
source_url: https://arxiv.org/abs/2511.03724
tags:
- player
- solly
- game
- poker
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Solly, the first AI agent to achieve elite
  human-level play in reduced-format Liar's Poker, a multi-player imperfect-information
  game. The authors trained Solly using self-play with a model-free actor-critic deep
  reinforcement learning algorithm based on regularized Nash dynamics.
---

# Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.03724
- Source URL: https://arxiv.org/abs/2511.03724
- Authors: Richard Dewey; Janos Botyanszki; Ciamac C. Moallemi; Andrew T. Zheng
- Reference count: 12
- Key outcome: First AI agent to achieve elite human-level play in reduced-format Liar's Poker, winning over 50% of hands against world-class players

## Executive Summary
This paper introduces Solly, the first AI agent to achieve elite human-level performance in Liar's Poker, a multi-player imperfect-information game. Trained using self-play with a model-free actor-critic deep reinforcement learning algorithm based on regularized Nash dynamics, Solly demonstrates superior performance against both elite human players and large language models. The agent develops novel bidding strategies, effectively randomizes its play, and remains unexploitable by world-class opponents. The research shows that model-free reinforcement learning can achieve strong performance in multi-player imperfect-information games without requiring complex search algorithms or extensive compute resources.

## Method Summary
Solly is trained using the R-NaD (Regularized Nash Dynamics) actor-critic algorithm from OpenSpiel. The agent uses an MLP with 2 hidden layers of 256 neurons and a shared policy across all agents. Training is conducted through self-play, with trajectory cutoffs of 10-15 moves depending on game configuration. The algorithm operates on game states encoded as tensors containing the player's hand, position, bid history, challenge history, rebid state, and terminal flag. Training is performed on Mac machines (M2/M4, 8-24GB RAM) with speeds of 370k-600k iterations per day. At inference, action probabilities below 3% are set to zero to ensure efficient exploration.

## Key Results
- Solly won 48-55% of hands against elite human players depending on configuration
- Solly achieved 60% win rate against GPT-4.1 and 55% against OpenAI o3 in heads-up games
- The agent demonstrated positive equity and maintained unexploitable play across all tested configurations
- Solly developed novel bidding strategies not previously observed in human play

## Why This Works (Mechanism)
The R-NaD algorithm enables the agent to learn optimal strategies through self-play without requiring perfect information or extensive search. By using regularized Nash dynamics, the algorithm encourages exploration while maintaining convergence to near-optimal policies. The shared policy across all agents simplifies the learning process and ensures consistent behavior across different game positions. The trajectory cutoff mechanism prevents policy degradation in late-game scenarios while maintaining computational efficiency.

## Foundational Learning
- **Imperfect-information games**: Games where players have incomplete information about opponents' states. Why needed: Liar's Poker involves hidden hands and sequential bidding. Quick check: Can you identify the information sets in the game?
- **Self-play reinforcement learning**: Training through repeated games against versions of itself. Why needed: No external expert data available for Liar's Poker. Quick check: Does the agent improve when playing against stronger versions of itself?
- **Regularized Nash dynamics**: Optimization method that incorporates entropy regularization to encourage exploration. Why needed: Prevents premature convergence to suboptimal deterministic policies. Quick check: Does the policy maintain sufficient randomness in its actions?
- **Actor-critic architecture**: Uses separate networks for policy (actor) and value estimation (critic). Why needed: Balances exploration (policy) with exploitation (value). Quick check: Are both networks learning at similar rates?
- **Multi-agent coordination**: Shared policy across multiple agents. Why needed: Simplifies learning in symmetric games like Liar's Poker. Quick check: Does the shared policy perform well in all positions?
- **Best response exploitability**: Metric measuring how much a policy can be exploited by optimal counter-strategies. Why needed: Quantifies the strength of the learned policy. Quick check: Is the best response score below 0.3?

## Architecture Onboarding

**Component Map**
State encoding -> R-NaD MLP network (256x256) -> Policy output + Value output -> Self-play trajectory generation -> Policy update

**Critical Path**
Self-play generation → R-NaD updates → Best response evaluation → Win rate measurement

**Design Tradeoffs**
- Model-free approach: Simpler implementation but may require more training data
- Shared policy: Reduces network complexity but may limit position-specific optimization
- Trajectory cutoff: Improves computational efficiency but may miss late-game learning opportunities

**Failure Signatures**
- High best response score (>0.5): Policy is exploitable, needs more training
- Win rate <50% against baseline: Learning not occurring, check network capacity
- Degraded performance near cutoff: Increase trajectory length or adjust learning rate

**First Experiments**
1. Train for 100k iterations and verify best response score decreases from initial random policy
2. Evaluate against greedy EV baseline and confirm >60% win rate
3. Test win rate against GPT-4.1 in 3x3 2-player configuration to verify 60% claim

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the model-free R-NaD approach effectively scale to the full 8x10 Liar's Poker game?
**Basis in paper:** [explicit] Section 7 states, "We believe scaling the R-Nad algorithm to play the complete (8x10) Liar's Poker agent is straightforward," but identifies the challenge that "the strength of the reward signal is increasingly diluted as the length of rounds increases."
**Why unresolved:** The authors demonstrated success only on reduced formats (3x3, 5x5). In the full game, longer trajectories may weaken the learning signal because unreasonable bids are often rewarded merely because a sub-optimal opponent bids even higher, making convergence difficult.
**What evidence would resolve it:** Successful training of an agent on the full 8x10 game that maintains positive equity against elite human players or a best-response baseline.

### Open Question 2
**Question:** Would incorporating test-time compute (TTC), such as Monte Carlo Tree Search (MCTS), improve Solly's performance?
**Basis in paper:** [explicit] Section 6 notes, "It is very likely that Solly would have benefited from a technique like Monte Carlo tree search (MCTS) at test-time, but we leave that to future research efforts to explore."
**Why unresolved:** Solly currently uses nearly zero test-time compute, relying entirely on a pre-trained policy network. While computationally efficient, it lacks the search-based planning capabilities that have driven superhuman performance in other complex games like No-Limit Texas Hold'em.
**What evidence would resolve it:** A comparison of win rates between the current Solly agent and a modified agent utilizing MCTS during inference against elite humans.

### Open Question 3
**Question:** Can LLMs be trained or prompted to overcome their deterministic tendencies and compete at elite levels in Liar's Poker?
**Basis in paper:** [explicit] Section 8 suggests, "sharing the distributions output by Solly's policy network with the LLM might be an interesting line of research." Section 6 also notes that "instructing the LLM to consider bluffing... might be fruitful lines of future research."
**Why unresolved:** The study found that GPT-4.1 and o3 play conservatively and deterministically, failing to randomize or adapt to opponents. They currently lack the ability to effectively model the "theory of mind" required for elite bluffing without external guidance.
**What evidence would resolve it:** An LLM agent, prompted to consider opponent modeling or distilled from Solly's policy, achieving a win rate significantly higher than the current baseline of 40-45% against Solly.

## Limitations
- The approach was validated only on reduced game formats (3x3, 5x5) and may not scale effectively to the full 8x10 game
- The exact R-NaD hyperparameters and training iteration counts are not specified, making precise reproduction difficult
- The agent relies entirely on pre-trained policy without test-time computation, potentially limiting performance compared to search-based approaches

## Confidence
- **Win rate against humans (>50%)**: Medium-High - validated through multiple evaluations against diverse elite players
- **Performance vs LLMs (60% vs GPT-4.1)**: Medium-High - tested in controlled conditions with clear metrics
- **Novel strategy development**: Medium - inferred from win rates and exploitability scores, but specific strategies not detailed
- **Scalability to full game**: Low - only demonstrated on reduced formats, theoretical claims about scaling remain untested

## Next Checks
1. Implement the R-NaD algorithm with OpenSpiel and verify that the best response score decreases to below 0.3 within a reasonable number of training iterations
2. Evaluate the trained agent against the baseline greedy EV policy and confirm it achieves >60% win rate as reported
3. Test the agent's performance against GPT-4.1 and OpenAI o3 in the specified heads-up configuration to verify the claimed 60% and 55% win rates respectively