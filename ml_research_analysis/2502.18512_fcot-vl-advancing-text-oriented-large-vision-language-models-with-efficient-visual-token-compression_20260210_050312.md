---
ver: rpa2
title: FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient
  Visual Token Compression
arxiv_id: '2502.18512'
source_url: https://arxiv.org/abs/2502.18512
tags:
- visual
- arxiv
- performance
- training
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FCoT-VL, an efficient visual token compression
  framework for text-oriented Vision-Language Models (VLLMs) operating in high-resolution
  scenarios. The approach uses a lightweight self-distillation pre-training stage
  with limited image-text pairs to compress visual tokens, followed by a high-quality
  post-training stage to mitigate performance degradation.
---

# FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression

## Quick Facts
- **arXiv ID:** 2502.18512
- **Source URL:** https://arxiv.org/abs/2502.18512
- **Reference count:** 25
- **Primary result:** Achieves 2× and 4× visual token compression with minimal performance loss on text-oriented tasks

## Executive Summary
FCoT-VL introduces an efficient visual token compression framework for text-oriented Vision-Language Models (VLLMs) operating on high-resolution images. The method employs self-distillation pre-training with limited image-text pairs to compress visual tokens, followed by high-quality post-training to recover performance. Applied to InternVL2, FCoT-VL achieves compression ratios of 2 and 4 while outperforming baselines across text-oriented benchmarks, significantly reducing computational overhead with faster inference and better training efficiency.

## Method Summary
FCoT-VL uses a two-stage approach: first, self-distillation where a student model (InternVL2 with compression module) learns from a frozen teacher model (vanilla InternVL2) using 2M OCR-focused image-text pairs. Only the student's compression module and projector are trainable. Second, post-training on 4.5M high-quality instruction data with all parameters trainable. Model checkpoints are merged using Shapley value-based weighting to balance capabilities across tasks.

## Key Results
- Achieves 2× and 4× visual token compression with minimal performance loss on text-oriented tasks
- Outperforms baselines across DocVQA, InfoVQA, TextVQA, ChartQA, and OCRBench benchmarks
- Provides 1.5-2.4× speedup at 4× compression ratio while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
Self-distillation from token-rich teacher to token-compressed student enables efficient visual token compression with limited training data. The frozen ViT encoder and LLM decoder share knowledge, while only the compression module and projector are trained to minimize KL divergence between teacher and student logits plus cross-entropy loss.

### Mechanism 2
CNN-based token compression outperforms Qformer and pooling alternatives under data-constrained distillation. A 1D convolutional layer with stride 2 merges adjacent visual tokens, exploiting local spatial redundancy in visual representations.

### Mechanism 3
Post-training with high-quality instruction data and Shapley-weighted model merging recovers performance lost during compression. Different training stages capture complementary task-specific adaptations, and Shapley values fairly weight each checkpoint's contribution to final performance.

## Foundational Learning

- **Self-Distillation / Knowledge Distillation**: The core training paradigm requires understanding how soft labels from a teacher model guide a student to compress representations while preserving capability. *Quick check:* Can you explain why KL divergence between teacher and student logits is used instead of direct feature matching?

- **Visual Token Representation in ViT-based VLLMs**: FCoT-VL operates on the token stream between the ViT encoder and LLM decoder; understanding what visual tokens encode is essential for reasoning about compression. *Quick check:* What does each visual token typically represent in a patch-based ViT, and why might adjacent tokens be redundant?

- **Shapley Values for Model Merging**: The paper uses Shapley-based weighting to merge checkpoints; this requires understanding cooperative game theory basics. *Quick check:* How does a Shapley value quantify a player's (checkpoint's) contribution compared to simple averaging?

## Architecture Onboarding

- **Component map:** Image → ViT Encoder (frozen) → Visual Tokens → Compression Module (CNN) → Compressed Tokens → Projector (MLP) → LLM Decoder (frozen) → Output
- **Critical path:** Image → ViT → visual tokens (e.g., 1024 tokens) → Compression Module → compressed tokens (e.g., 256 tokens for 4×) → Compressed tokens + text embeddings → LLM → output logits
- **Design tradeoffs:** 4× compression gives 1.5-2.4× speedup but ~5% performance drop on high-resolution tasks; CNN is simple and effective but may lose precise spatial relationships
- **Failure signatures:** DocVQA/InfoVQA scores drop >10% (compression ratio too aggressive); training loss plateaus early (check projector learning rate or data quality); merged model underperforms final checkpoint (Shapley computation may have noisy estimates)
- **First 3 experiments:**
  1. Validate compression module: Compare CNN vs. pooling vs. Qformer on small SFT subset with 50% compression; measure DocVQA, ChartQA, InfoVQA
  2. Ablate compression ratio: Train FCoT-VL-2B at 25%, 50%, 75% compression; plot performance vs. inference speed
  3. Test merge strategies: Compare No Merge, Simple Averaging, Task Arithmetic, and Shapley-based merging using 5 intermediate checkpoints

## Open Questions the Paper Calls Out

- **Non-text modalities:** Can FCoT-VL effectively generalize to non-text-oriented visual domains like natural scene understanding or medical imaging? The method is optimized for text-oriented tasks where token redundancy patterns differ significantly from semantic structures in natural scenes or clinical imagery.

- **Adaptive compression:** Can an adaptive compression strategy mitigate performance degradation observed in extremely high-resolution tasks like InfoVQA? The current static approach may be too aggressive for dense infographic images.

- **Compression module architecture:** Would attention-based compression modules (e.g., Q-Former) surpass CNN performance if trained without data constraints? The CNN's superiority may be due to better inductive biases for compression or simply its ability to converge faster with limited data.

## Limitations
- Method does not extend to non-text image modalities such as natural scenes or medical imaging
- Fixed compression ratios show slight performance drops on extremely high-resolution tasks like InfoVQA
- Compression module architecture lacks detailed specifications (kernel sizes, padding, channel dimensions)

## Confidence

- **High Confidence:** Core claim that visual token compression can significantly reduce computational overhead while maintaining reasonable performance
- **Medium Confidence:** Assertion that CNN-based compression outperforms Qformer and pooling methods under data-constrained conditions
- **Low Confidence:** Claim that Shapley-based model merging reliably recovers performance lost during compression

## Next Checks

1. **Compression Module Sensitivity Analysis:** Systematically vary CNN kernel sizes, strides, and padding strategies while keeping other components fixed. Measure performance degradation across different compression ratios (25%, 50%, 75%) on OCR-intensive tasks to identify optimal configuration and failure thresholds.

2. **Data Efficiency Validation:** Train FCoT-VL using progressively smaller datasets (0.5M, 1M, 2M pairs) while maintaining same compression ratio. Plot performance vs. data quantity to quantify claimed data efficiency advantage over training from scratch.

3. **Model Merging Robustness Test:** Implement alternative merging strategies (simple averaging, task arithmetic, gradient-based merging) and compare their performance against Shapley-based merging. Conduct sensitivity analysis by varying checkpoint intervals and measuring impact on merged model performance.