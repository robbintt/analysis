---
ver: rpa2
title: Reasoning-Grounded Natural Language Explanations for Language Models
arxiv_id: '2503.11248'
source_url: https://arxiv.org/abs/2503.11248
tags:
- reasoning
- explanations
- language
- natural
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel technique for obtaining faithful natural
  language explanations from large language models (LLMs) by grounding them in a reasoning
  process. The core idea is to use a compressed chain-of-thought reasoning sequence
  that encodes all necessary partial decisions, which is then decoded into natural
  language explanations and answers.
---

# Reasoning-Grounded Natural Language Explanations for Language Models

## Quick Facts
- arXiv ID: 2503.11248
- Source URL: https://arxiv.org/abs/2503.11248
- Authors: Vojtech Cahlik; Rodrigo Alves; Pavel Kordik
- Reference count: 38
- Key outcome: Novel technique using compressed chain-of-thought reasoning to ground LLM explanations in reasoning sequences, achieving near-perfect alignment between answers and explanations in classifier settings.

## Executive Summary
This paper introduces a method for obtaining faithful natural language explanations from large language models by grounding them in a compressed chain-of-thought reasoning process. The approach uses a two-stage inference pipeline where reasoning sequences are generated and then included in the context when producing both answers and explanations. This shared reasoning context ensures high alignment between answers and explanations by encouraging the model to copy partial decisions rather than independently re-reasoning. The method is evaluated on synthetic logistic regression, decision tree classification, and natural language mortgage approval tasks, demonstrating that including reasoning sequences significantly improves both alignment rates and answer quality.

## Method Summary
The method uses a compressed chain-of-thought reasoning sequence that encodes all necessary partial decisions, which is then decoded into natural language explanations and answers. The approach works by generating a reasoning output R_n that captures intermediate decisions, then using this reasoning as part of the context when generating both answers and explanations through separate inference steps. The model is fine-tuned using LoRA on a dataset containing input-reasoning-answer and input-reasoning-explanation pairs, with training performed for one epoch using specified hyperparameters. Inference follows a two-stage process: first generating the reasoning for test inputs, then generating answers and explanations independently using the shared reasoning context.

## Key Results
- High alignment rates (near 100%) between answers and explanations when reasoning sequences are included in context
- The model often copies partial decisions from reasoning sequences rather than re-deriving them
- Two-stage inference prevents post-hoc rationalization by generating answers and explanations independently from the same reasoning root
- Inclusion of reasoning sequences improves answer quality compared to baselines without reasoning

## Why This Works (Mechanism)

### Mechanism 1: Context-Conditioned Grounding
If a reasoning sequence is generated and prepended to the context, the model conditions both the final answer and the natural language explanation on this shared prefix rather than generating them independently. The authors define a conversation history H_n and a reasoning output R_n, appending R_n to the history before issuing "ANSWER" or "EXPLAIN" commands.

### Mechanism 2: Copying over Reasoning
The primary driver of high alignment is the model's propensity to copy intermediate decisions directly from the context rather than re-deriving them. The "compressed chain-of-thought" encodes partial decisions (e.g., "0,0,0,1,1"), and the model simply reproduces these states in the final output.

### Mechanism 3: Joint Predict-Explain Independence
Generating the answer and explanation independently from the same reasoning root prevents post-hoc rationalization (fabulation). By using separate command messages but the same shared reasoning context, the explanation cannot condition on the answer token.

## Foundational Learning

**Faithfulness vs. Plausibility**
Why needed: The paper explicitly optimizes for faithfulness rather than plausibility. Confusing these leads to misinterpreting results.
Quick check: If a model gives the right answer for the wrong reason, is the explanation faithful? (No)

**Chain-of-Thought (CoT) Reasoning**
Why needed: The method relies on "compressed chain-of-thought." Understanding standard CoT is necessary to see how this method trades token-efficiency for grounded copying.
Quick check: Does standard CoT guarantee that the explanation caused the answer, or just that it preceded it? (Precedence, not necessarily causation)

**In-Context Learning / Conditioning**
Why needed: The mechanism works by conditioning the model on a specific reasoning sequence R_n. Understanding that LLMs adjust their output distribution based on this prefix is essential.
Quick check: Why is the reasoning sequence placed in the context window rather than hidden in the model's weights? (To allow explicit copying and inspection)

## Architecture Onboarding

**Component map:**
Reasoning Generator -> Context Manager -> Answer/Explanation Decoder

**Critical path:** The generation of the reasoning sequence R_n is the critical dependency. If R_n is inaccurate, both the answer and explanation will be confidently wrong (but perfectly aligned).

**Design tradeoffs:**
- Readability vs. Compactness: Uses "compressed" reasoning for efficiency and copying accuracy
- Compute Cost: Requires two forward passes (reasoning + answer/explanation) vs. single pass in standard prompting

**Failure signatures:**
- Low Alignment: Answer class differs from Explanation class
- Hallucination: Explanation contains facts not present in input or reasoning sequence
- Parse Errors: Model fails to generate specific classification token

**First 3 experiments:**
1. Baseline Alignment: Train without reasoning sequence in context, measure alignment
2. Reasoning Injection: Fine-tune with reasoning sequence included, verify alignment jumps to near 100%
3. Perturbation Test: Intentionally flip bits in reasoning sequence during inference, check if final answer/explanation changes accordingly

## Open Questions the Paper Calls Out

**Open Question 1:** Can the method be effectively scaled to general-purpose assistant datasets?
Basis: Authors state extending to general-purpose assistant datasets is left for future work
Why unresolved: Current study restricted to LLM-as-a-classifier setting with deterministic synthetic datasets
Evidence needed: Successful application to open-domain dialogue benchmarks with maintained alignment

**Open Question 2:** Would a training loss penalizing inconsistencies between answers, explanations, and reasoning improve faithfulness?
Basis: Authors suggest introducing such a training loss
Why unresolved: Current fine-tuning uses standard next-token prediction without consistency enforcement
Evidence needed: Comparative experiments showing consistency-penalizing loss yields higher alignment

**Open Question 3:** Can the framework be extended to a multitask setting with auxiliary commands?
Basis: Authors envision multitask setting with additional commands like intent classification
Why unresolved: Paper only evaluates two modes ("ANSWER" and "EXPLAIN")
Evidence needed: Single fine-tuned model successfully executing auxiliary tasks using same reasoning grounding

## Limitations

- Limited task diversity: All evaluated tasks are relatively simple classification problems with discrete outcomes
- Unknown reasoning dependency: Unclear whether model is truly reasoning or pattern-matching and copying
- Dataset construction opacity: Paper mentions "2000 samples" but doesn't fully specify generation methodology

## Confidence

**High Confidence:**
- Technical approach correctly implemented and produces reported results
- Alignment rates measurably high when reasoning sequences are included
- Method successfully prevents post-hoc rationalization

**Medium Confidence:**
- Primary mechanism is "copying" rather than genuine reasoning (evidence indirect)
- Approach generalizes beyond specific tasks tested (theoretical justification but limited validation)

**Low Confidence:**
- Compressed reasoning format is optimal (no ablation studies on format)
- Approach scales to more complex reasoning tasks (stated as future work, not demonstrated)

## Next Checks

1. Hidden Reasoning Test: Remove reasoning sequence from context during answer/explanation generation and measure if alignment drops to near 0%

2. Reasoning Format Ablation: Create alternative reasoning formats and measure if alignment rates change significantly

3. Out-of-Distribution Reasoning: Generate reasoning sequences for inputs not seen during training and measure if model produces aligned answers and explanations