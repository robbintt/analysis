---
ver: rpa2
title: Probability-Consistent Preference Optimization for Enhanced LLM Reasoning
arxiv_id: '2505.23540'
source_url: https://arxiv.org/abs/2505.23540
tags:
- preference
- pcpo
- pairs
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving mathematical reasoning
  in large language models by developing a more sophisticated method for selecting
  preference training pairs. While existing approaches focus solely on outcome-based
  criteria like answer correctness, the authors propose Probability-Consistent Preference
  Optimization (PCPO), which incorporates both final answer correctness and internal
  token-level probability consistency across responses.
---

# Probability-Consistent Preference Optimization for Enhanced LLM Reasoning

## Quick Facts
- arXiv ID: 2505.23540
- Source URL: https://arxiv.org/abs/2505.23540
- Reference count: 31
- PCPO improves LLM mathematical reasoning by incorporating token-level probability consistency into preference selection

## Executive Summary
This paper introduces Probability-Consistent Preference Optimization (PCPO), a novel method for enhancing mathematical reasoning in large language models by improving the selection of preference training pairs. Unlike existing approaches that rely solely on outcome-based criteria like answer correctness, PCPO incorporates both final answer correctness and internal token-level probability consistency across responses. The method calculates a weighted score between preferred and dispreferred answers by evaluating the conditional probability of each token, selecting preference pairs based on the highest weighted scores.

Extensive experiments demonstrate that PCPO consistently outperforms existing outcome-only criterion approaches across multiple benchmarks (GSM8K, MATH-500, Olympiadbench, AMC23) and diverse models (Llama-3-8B-Instruct, Mathstral-7B-v0.1, Qwen-2.5-7B-Instruct, Qwen-2.5-Math-7B-Instruct). The method achieves significant improvements in zero-shot Pass@1 and Maj@8 accuracy, with gains ranging from 0.4% to over 10% depending on the benchmark difficulty. Notably, PCPO shows consistent performance improvements across training iterations and demonstrates broad applicability by enhancing various DPO variants.

## Method Summary
Probability-Consistent Preference Optimization (PCPO) addresses the limitations of outcome-only preference selection by incorporating token-level probability consistency into the preference optimization process. The method calculates a weighted score between preferred and dispreferred answers by evaluating the conditional probability of each token in the response sequence. This probability consistency metric captures how consistently a model assigns probabilities to tokens that lead to correct answers, providing a more nuanced view of reasoning quality beyond just final answer correctness.

The preference selection process combines both final answer correctness and internal probability consistency using a weighted formula. For each response pair, PCPO computes the conditional probability of each token given previous tokens and aggregates these probabilities to form a consistency score. The method then selects preference pairs based on the highest combined scores, ensuring that the selected pairs represent both correct reasoning trajectories and high-confidence intermediate steps. This approach leads to more informative training signals that capture the quality of reasoning processes rather than just outcomes.

## Key Results
- On GSM8K with Llama-3-8B-Instruct, PCPO achieves 82.8% Pass@1 accuracy compared to 81.6% for the seed model
- Significant improvements on challenging benchmarks: 4.8% gain on MATH-500 (41.8% vs 37.0%) and 10.6% gain on AMC23 (42.0% vs 31.4%)
- Consistent performance improvements across training iterations and multiple DPO variants
- Demonstrated effectiveness across diverse model families including Llama, Mathstral, and Qwen architectures

## Why This Works (Mechanism)
PCPO works by capturing the quality of reasoning trajectories through token-level probability consistency. Traditional preference optimization methods only consider final answer correctness, missing important information about the reasoning process itself. By evaluating how consistently a model assigns probabilities to tokens that lead to correct answers, PCPO identifies preference pairs that represent not just correct outcomes but also coherent reasoning paths.

The method leverages the insight that high-quality reasoning involves consistent probability assignments at intermediate steps. When a model generates a correct answer through a reasoning path with high internal probability consistency, it indicates robust understanding of the problem structure. Conversely, correct answers achieved through low-probability token sequences may indicate lucky guesses or overfitting to specific patterns. PCPO's weighted combination of outcome correctness and probability consistency ensures that the selected preference pairs represent both reliable outcomes and coherent reasoning processes.

## Foundational Learning
- **Preference Optimization**: The process of fine-tuning language models using pairwise comparisons between responses to improve output quality. Why needed: Forms the foundation for aligning models to human preferences and improving task-specific performance.
- **Token-level Probability Consistency**: Measures how consistently a model assigns probabilities to tokens that lead to correct answers. Why needed: Captures the quality of reasoning trajectories beyond just final outcomes.
- **Conditional Probability Evaluation**: Computing the probability of each token given previous tokens in a sequence. Why needed: Enables the measurement of internal consistency in the reasoning process.
- **Weighted Preference Scoring**: Combining multiple criteria (correctness and consistency) using weighted formulas to select optimal training pairs. Why needed: Allows balanced consideration of different aspects of response quality.
- **Zero-shot Evaluation**: Testing model performance without task-specific fine-tuning. Why needed: Provides a standardized measure of general reasoning capabilities.
- **DPO Variants**: Different implementations of Direct Preference Optimization techniques. Why needed: Shows the broad applicability of PCPO across different optimization frameworks.

## Architecture Onboarding

**Component Map**: Input Data -> Token Probability Extraction -> Consistency Scoring -> Weighted Scoring -> Preference Pair Selection -> Model Training

**Critical Path**: The method flows from input response pairs through token probability extraction, consistency calculation, weighted scoring, and preference selection before model training. The consistency scoring and weighted scoring stages are the most computationally intensive components.

**Design Tradeoffs**: PCPO trades computational overhead for improved preference selection quality. The token-level probability calculations add processing time but provide more informative training signals. The weighted combination of correctness and consistency requires hyperparameter tuning but allows flexible adaptation to different domains.

**Failure Signatures**: Poor performance may indicate incorrect weight balancing between correctness and consistency, insufficient model capacity to capture probability patterns, or data quality issues in the preference pairs. Overfitting to probability consistency at the expense of correctness can lead to degraded performance on simple problems.

**First Experiments**: 
1. Validate PCPO on a small benchmark with known correct answers to ensure probability consistency improves with reasoning quality
2. Compare preference selection using only correctness versus only consistency to identify optimal weight parameters
3. Test PCPO on synthetic reasoning tasks where ground truth reasoning paths are available

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Scalability claims to larger models and non-mathematical reasoning tasks lack experimental validation
- Limited ablation studies on which DPO variants benefit most from probability consistency
- Potential computational overhead from token-level probability calculations may impact training efficiency
- Weight parameters between correctness and consistency require careful tuning for different domains

## Confidence
- **High confidence**: Core methodological innovation and experimental methodology are well-defined and rigorous
- **High confidence**: Consistent performance improvements across multiple benchmarks and model families
- **Medium confidence**: Claims about broad applicability across DPO variants due to limited systematic exploration
- **Low confidence**: Scalability to larger models and generalization beyond mathematical reasoning

## Next Checks
1. **Cross-domain generalization test**: Evaluate PCPO on non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation, or scientific reasoning) to verify claims about broader applicability beyond the mathematical domain.

2. **Scalability assessment**: Implement PCPO on larger language models (20B+ parameters) to validate whether the performance gains scale with model size or if there are diminishing returns or implementation challenges at scale.

3. **Ablation on preference selection criteria**: Conduct controlled experiments varying the weight between outcome correctness and probability consistency to determine optimal balance points and identify scenarios where probability consistency alone provides sufficient signal for preference selection.