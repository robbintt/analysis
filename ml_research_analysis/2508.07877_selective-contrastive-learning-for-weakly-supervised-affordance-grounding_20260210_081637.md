---
ver: rpa2
title: Selective Contrastive Learning for Weakly Supervised Affordance Grounding
arxiv_id: '2508.07877'
source_url: https://arxiv.org/abs/2508.07877
tags:
- learning
- object
- images
- parts
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses weakly supervised affordance grounding, aiming
  to identify functional object parts relevant to specific actions using only image-level
  labels and action text prompts. The core method introduces selective prototypical
  and pixel contrastive learning that adaptively leverages affordance-relevant cues
  at both part and object levels, depending on the granularity of available information.
---

# Selective Contrastive Learning for Weakly Supervised Affordance Grounding

## Quick Facts
- **arXiv ID**: 2508.07877
- **Source URL**: https://arxiv.org/abs/2508.07877
- **Reference count**: 40
- **Primary result**: State-of-the-art KLD scores of 1.124 (seen) and 1.243 (unseen) on AGD20K, significantly outperforming previous weakly supervised affordance grounding methods

## Executive Summary
This paper addresses weakly supervised affordance grounding by identifying functional object parts relevant to specific actions using only image-level labels and action text prompts. The core innovation is selective contrastive learning that adaptively leverages affordance-relevant cues at both part and object levels, depending on the granularity of available information. The method first discovers object-level clues using CLIP, then refines them to part-level clues through clustering and validation. Two contrastive objectives—prototypical (for object/part-level alignment) and pixel (for fine-grained localization)—are applied selectively based on the reliability of discovered parts. The approach achieves state-of-the-art performance on AGD20K and HICO-IIF datasets, demonstrating significant improvements particularly in unseen scenarios that better reflect real-world conditions.

## Method Summary
The method employs selective prototypical and pixel contrastive learning to adaptively discover and ground affordance-relevant cues at different granularities. Object-level cues are initially discovered using CLIP-based object affinity maps, which are then refined to part-level cues through K-means clustering and pIoU validation against DINO attention maps. When part-level prototypes are deemed reliable (pIoU > 0.6), they guide contrastive learning; otherwise, object-level prototypes serve as fallback. Pixel contrastive learning further refines localization by distinguishing affordance-relevant pixels within images. The approach integrates these mechanisms into a unified framework that trains on egocentric and exocentric image pairs, with inference calibrated by object affinity maps.

## Key Results
- Achieves state-of-the-art KLD scores of 1.124 (seen) and 1.243 (unseen) on AGD20K dataset
- Demonstrates significant performance gains in unseen scenarios (1.243 vs 1.353 for second-best method)
- Shows robust performance across both AGD20K and HICO-IIF datasets with consistent improvements
- Effective part discovery with pIoU scores validating the reliability of extracted affordance regions

## Why This Works (Mechanism)

### Mechanism 1: Selective Prototypical Contrastive Learning
The system adaptively switches between object-level and part-level prototypes based on the reliability of discovered parts. Part prototypes are extracted via K-means clustering on CAM × object affinity maps, then validated using pIoU against DINO attention maps. When pIoU exceeds threshold α (0.6), part-level prototypes guide contrastive learning; otherwise, object-level prototypes maintain focus on target objects versus background. This selectivity prevents unreliable part prototypes from introducing noise while preserving fine-grained precision when reliable.

### Mechanism 2: Pixel Contrastive Learning for Fine-Grained Localization
Pixel-level contrastive learning refines localization beyond prototypical learning by directly distinguishing affordance-relevant pixels. The method leverages CLIP's stronger activation to affordance parts in egocentric vs. exocentric images to establish threshold ρ. Pixels above ρ form positive set Q+, below form negative set Q-. Pixel features are contrasted to push apart affordance-relevant and irrelevant pixels within the same image, achieving fine-grained localization.

### Mechanism 3: CLIP-Based Object Affinity Map Discovery
ClearCLIP strategy removes residual connections and FFN in CLIP's final layer, then computes cosine similarity between visual features and action-prompted text ("an item to [action] with"). For exocentric images, multiplies action-prompted and entity-prompted maps with local averaging. These binarized maps filter part candidates and mask CAM predictions at inference, providing coarse localization that constrains and calibrates the final predictions.

## Foundational Learning

- **Concept: Weakly Supervised Object Localization (WSOL)**
  - **Why needed here**: The task uses only image-level labels to localize affordance parts, requiring understanding of CAM-based localization and its limitations (coverage issues, shortcut learning).
  - **Quick check question**: Can you explain why CAM activations often cover only the most discriminative object parts rather than the full object?

- **Concept: Contrastive Learning (InfoNCE-style objectives)**
  - **Why needed here**: Both prototypical and pixel contrastive losses use InfoNCE formulation with temperature τ, positive/negative sets, and dot-product similarity.
  - **Quick check question**: How does increasing temperature τ affect the hardness of contrastive learning?

- **Concept: Vision-Language Models (CLIP, ClearCLIP)**
  - **Why needed here**: Object affinity maps depend on CLIP's joint embedding space; ClearCLIP modifications affect spatial discriminability.
  - **Quick check question**: What is the purpose of removing the residual connection and FFN in ClearCLIP's final layer?

## Architecture Onboarding

- **Component map**: DINO ViT-S/16 (feature extraction) -> CLIP ViT-B/16 with ClearCLIP (object affinity) -> Classification branch (MLP projection + shared classifier) -> CAM generation -> K-means clustering -> pIoU validation -> Prototype selection -> Prototypical contrastive branch (Projection -> Φ+/Φ- prototypes) -> Pixel contrastive branch (Projection -> Q+/Q- sets) -> Inference (CAM ⊙ binarized object affinity map)

- **Critical path**: 1) Compute object affinity maps (A^ego_obj, A^exo_obj) via CLIP; 2) Threshold and combine with CAM for part candidate extraction (exocentric); 3) K-means clustering → pIoU validation → prototype selection; 4) Compute ρ threshold from exocentric activations (egocentric part discovery); 5) Apply selective contrastive losses based on reliability flags; 6) Calibrate inference CAM with binarized affinity map

- **Design tradeoffs**: Single shared threshold (γ1 = γ2 = 0.6) for simplicity vs. separate tuning per level; K=3 for clustering assumes exactly three semantic regions; using CLIP without fine-tuning preserves zero-shot capability but may limit dataset-specific precision; batching exocentric images (E=3) improves part discovery reliability but increases memory

- **Failure signatures**: Low pIoU across batch indicates α threshold too strict, causing model to default to object-level learning and lose part precision; CAM activates on class-discriminative but affordance-irrelevant parts suggests classification loss dominates; calibration removes valid activations indicates object affinity map threshold γ too aggressive

- **First 3 experiments**: 1) Baseline sanity check: train with classification loss only; verify CAM activations align with class-discriminative regions (not affordance parts). Expect KLD ~1.35 on AGD20K-Seen; 2) Ablation on selectivity: disable object-level fallback; compare full selective vs. part-only learning. If part-only degrades, selectivity provides value; 3) Threshold sensitivity: sweep α ∈ {0.4, 0.5, 0.6, 0.7, 0.8} while fixing γ=0.6; plot KLD to find optimal reliability threshold

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The efficacy of selective contrastive learning depends heavily on the reliability of part discovery via pIoU against DINO attention maps, which may not generalize across datasets with different object scales or occlusion patterns
- The assumption that CLIP responds more strongly to affordance parts in egocentric images may not hold when exocentric views are unobstructed or when action-object pairs are ambiguous
- Limited ablation on the choice of K=3 for K-means clustering and the single threshold (α=γ=0.6) may not be optimal across diverse scenes or affordance types

## Confidence
- **High confidence**: Selective prototypical contrastive learning mechanism and its integration with object/part-level cues
- **Medium confidence**: Pixel contrastive learning efficacy (mechanism described but no empirical validation of CLIP's differential activation hypothesis)
- **Medium confidence**: Object affinity map quality via ClearCLIP (mechanism specified but limited corpus support for ClearCLIP's suitability for this task)

## Next Checks
1. **Ablation study on clustering granularity**: Train models with K=2, K=3, and K=4 clusters; measure KLD and part discovery pIoU to determine optimal semantic separation
2. **Cross-dataset robustness test**: Evaluate the model on a dataset with varying object scales (e.g., COCO vs. AGD20K) to assess the reliability of pIoU-based part selection
3. **Activation consistency analysis**: Compare CLIP activation distributions between egocentric and exocentric views across multiple action-object pairs to validate the ρ threshold assumption