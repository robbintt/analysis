---
ver: rpa2
title: 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models
  for Scatterplot-Related Tasks'
arxiv_id: '2510.06071'
source_url: https://arxiv.org/abs/2510.06071
tags:
- chart
- zhang
- wang
- data
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic, annotated dataset of over 18,000
  scatterplots to address the lack of benchmarks for scatterplot-specific AI tasks.
  The dataset was generated from six data generators and 17 chart designs, featuring
  cluster detection, outlier identification, and related tasks.
---

# Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks

## Quick Facts
- arXiv ID: 2510.06071
- Source URL: https://arxiv.org/abs/2510.06071
- Reference count: 40
- Authors: João Palmeiro; Diogo Duarte; Rita Costa; Pedro Bizarro
- Primary result: Synthetic dataset of 18,921 scatterplots benchmarks proprietary VLMs; few-shot prompting achieves >90% accuracy on counting tasks, but localization tasks fail (<50% Precision/Recall)

## Executive Summary
This paper introduces BIY, a synthetic, annotated dataset of over 18,000 scatterplots designed to benchmark AI models on scatterplot-specific tasks. The dataset was generated from six data generators and 17 chart designs, enabling evaluation of cluster detection, outlier identification, and related tasks. Proprietary models from OpenAI and Google were evaluated using zero-shot, one-shot, and few-shot prompting strategies. Results showed that OpenAI models and Gemini 2.5 Flash performed well in counting clusters (over 90% Accuracy) and outliers (90%+ Accuracy for Flash). However, localization tasks such as detection and identification yielded poor results, with Precision and Recall near or below 50%, except for Flash in outlier identification (65.01%). Chart design had minimal impact, though scatterplots with wide aspect ratios (16:9, 21:9) or random colors should be avoided. Few-shot prompting consistently outperformed other strategies.

## Method Summary
The method involves generating a synthetic dataset of 18,921 scatterplots using six data generators (NumPy, PyOD, scikit-learn) and 17 chart designs (Vega-Lite). Ground truth annotations are extracted using the Vega View API. Proprietary VLMs (OpenAI GPT-4.1, o3, Gemini 2.5 Flash) are evaluated using zero-shot, one-shot, and few-shot prompting strategies via Batch APIs. Tasks include cluster counting, cluster detection (bounding boxes), cluster identification (centers), outlier counting, and outlier identification. Evaluation metrics include Accuracy and MAE for counting tasks, and Precision/Recall with strict thresholds (IoU ≥ 0.75, 10px distance) for localization tasks.

## Key Results
- Few-shot prompting achieved >90% Accuracy for cluster and outlier counting with top models
- Localization tasks (detection/identification) failed with Precision/Recall near or below 50%
- Chart design had minimal impact; wide aspect ratios (16:9, 21:9) and random colors should be avoided
- GPT-4o and Gemini 2.5 Flash performed best overall, with Flash achieving 65.01% Recall in outlier identification

## Why This Works (Mechanism)

### Mechanism 1: In-Context Calibration via Few-Shot Prompting
- Claim: Providing labeled examples (few-shot prompting) significantly improves model accuracy on quantitative counting tasks compared to zero-shot approaches.
- Mechanism: Examples likely condition the model's visual attention and decision boundary, aligning its internal definition of "cluster" or "outlier" with the specific density patterns of the synthetic dataset.
- Core assumption: The model already possesses sufficient visual acuity to resolve individual points but lacks the specific task definition without context.
- Evidence anchors:
  - [abstract] "Few-shot prompting consistently outperformed other strategies."
  - [section 3.3.1] Notes top models achieved over 90% Accuracy in counting tasks when few-shot prompted.
  - [corpus] *Chart-to-Experience* notes LLMs are often based on "overgeneralized assumptions," supporting the need for specific calibration; however, specific validation for scatterplot counting is absent in neighbors.
- Break condition: If the visual density of points exceeds the model's effective resolution (tokenization limit), examples will fail to bridge the gap.

### Mechanism 2: Precision-Recall Gap in Spatial Localization
- Claim: Current proprietary VLMs lack the fine-grained spatial mapping required to translate visual features into precise normalized coordinates.
- Mechanism: Models appear to process images for semantic "gist" (high-level patterns) rather than metric spatial regression, causing a collapse in Precision and Recall when asked for bounding boxes or specific coordinates.
- Core assumption: The visual encoder creates a downsampled or abstracted representation that loses the pixel-perfect alignment necessary for the strict IoU and distance thresholds (IoU75, 10px) used in evaluation.
- Evidence anchors:
  - [abstract] "Results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%."
  - [section 3.3.2] Notes that models like GPT-4.1 and Flash failed to surpass 25% Recall in detection/identification.
  - [corpus] Evidence is weak; neighbor papers focus on QA and captioning (semantic) rather than coordinate regression (spatial).
- Break condition: If the task is relaxed to coarse localization (e.g., quadrant identification) rather than exact coordinate generation, this mechanism may not apply.

### Mechanism 3: Visual Robustness to Chart Aesthetics
- Claim: Performance is resilient to most stylistic changes but degraded by aspect ratios or colors that disrupt standard visual processing.
- Mechanism: Wide aspect ratios (16:9, 21:9) likely distort the spatial attention maps, while random colors remove semantic grouping cues, introducing noise into the feature extraction process.
- Core assumption: Models are trained predominantly on standard aspect ratios and semantically meaningful color clusters.
- Evidence anchors:
  - [section 3.3.3] "Likelihood ratio test indicated that chart design significantly improved model fit... wide aspect ratios (16:9 and 21:9) or those colored randomly [should be avoided]."
  - [abstract] "Chart design had minimal impact, though... wide aspect ratios... should be avoided."
  - [corpus] *Unmasking Deceptive Visuals* confirms VLMs are brittle to visual manipulations, supporting the sensitivity to non-standard layouts.
- Break condition: If models are fine-tuned on diverse aspect ratios, this architectural bias may diminish.

## Foundational Learning

- Concept: **Zero-shot vs. Few-shot Prompting**
  - Why needed here: The benchmark relies entirely on prompting strategies to elicit results; understanding how examples condition the model is critical to replicating the 90%+ accuracy on counting tasks.
  - Quick check question: How does providing a JSON example of a "cluster" change the model's output format and reasoning compared to asking for it directly?

- Concept: **Intersection over Union (IoU) & Euclidean Distance**
  - Why needed here: The paper evaluates "detection" and "identification" using strict IoU (0.75) and pixel distance (10px). Understanding these metrics is required to interpret why localization "failed" (Precision/Recall < 50%).
  - Quick check question: If a model predicts a bounding box that overlaps 50% with the ground truth, is it a True Positive under the IoU75 threshold?

- Concept: **Normalized Coordinates**
  - Why needed here: The tasks require outputting coordinates in a [0, 1000] range regardless of image size.
  - Quick check question: How do you map a pixel coordinate (x=300) from a 600px wide image to the normalized coordinate system used in the benchmark?

## Architecture Onboarding

- Component map:
  - Data Generators: Python (NumPy, scikit-learn, PyOD) -> Generate raw (x,y) data
  - Renderer: Vega-Lite + FastHTML -> Converts data to 18,921 scatterplot images
  - Annotation Extractor: Vega View API -> Extracts ground truth (bounding boxes, centers) in normalized coordinates
  - Inference Engine: OpenAI/Google Batch APIs -> Accepts prompts + images, returns text
  - Evaluator: Python script -> Parses JSON, calculates Accuracy, MAE, Precision, Recall

- Critical path: Generating the ground truth annotations via the Vega View API is the most delicate step; it requires precise mapping of data coordinates to screen pixels to serve as the "source of truth" for evaluation.

- Design tradeoffs:
  - **Synthetic vs. Real:** The dataset is purely synthetic, allowing infinite scaling and perfect labels, but may lack the "noise" (overlapping labels, skewed axes) found in real-world reports.
  - **Strictness:** The paper uses strict IoU75 and 10px thresholds. This highlights failures but might undervalue models that are "almost correct" (e.g., IoU 0.60).

- Failure signatures:
  - **Hallucinated Count:** Model reports 5 clusters when only 3 exist (MAE > 1.0)
  - **Localization Collapse:** Model returns valid JSON, but coordinates are random or centered incorrectly (Recall < 25%)
  - **Consistency Error:** Model says "3 clusters" in text but outputs 4 bounding boxes

- First 3 experiments:
  1. **Replicate Counting Accuracy:** Run the cluster counting prompt (zero-shot vs. few-shot) on a sample of 50 "Gaussian blobs" to verify if you can achieve the >90% accuracy cited for GPT-4o/Gemini Flash
  2. **Test Localization Limits:** Run the "Outlier Identification" task with the strict 10px threshold vs. a relaxed 50px threshold to quantify the spatial precision drop-off
  3. **Aspect Ratio Stress Test:** Generate a mini-dataset of "Default" vs. "21:9" aspect ratio plots and compare the cluster counting accuracy to verify the statistical penalty reported in Section 3.3.3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning small, open models on the BIY dataset match or exceed the localization performance (Precision/Recall) of current proprietary models?
- Basis in paper: [explicit] The authors state they plan to "proceed to fine-tune small, open models and further assess the trade-offs of using significantly less demanding models."
- Why unresolved: The current benchmark only evaluated proprietary models (OpenAI and Google) using prompting strategies, which yielded unsatisfactory localization results (<50% Precision/Recall).
- What evidence would resolve it: A comparative benchmark showing the Precision and Recall of fine-tuned open-source models against the few-shot baselines established in this paper.

### Open Question 2
- Question: How do state-of-the-art models perform on scatterplot tasks requiring the identification of relationships between variables and cluster shapes?
- Basis in paper: [explicit] The authors list incorporating "tasks (such as identifying relationships between variables and cluster shapes)" as a specific future intention to enhance the benchmark.
- Why unresolved: The current study was restricted to cluster counting, detection, identification, and outlier analysis, excluding higher-level structural or relational pattern recognition.
- What evidence would resolve it: Extending the dataset with relevant annotations and reporting model accuracy on these specific shape and relationship identification tasks.

### Open Question 3
- Question: Does the impact of chart design choices (e.g., aspect ratio, color) on model accuracy vary significantly between different prompting strategies or model architectures?
- Basis in paper: [explicit] The authors note they "intend to further study the impact of chart design on performance" and mentioned that current findings suggest design is a "secondary factor" with low explanatory power.
- Why unresolved: While the initial benchmark found minimal impact (pseudo R²=0.0013), this may be an artifact of the specific proprietary models or few-shot strategies tested.
- What evidence would resolve it: Ablation studies across diverse model architectures showing statistical significance of design features on error rates.

## Limitations
- Evaluation relies on proprietary VLMs with opaque architectures and training data
- Synthetic dataset may not capture real-world scatterplot complexities like overlapping points or non-linear scales
- Strict evaluation thresholds (IoU75, 10px) may be overly punitive and undervalue good visual reasoning

## Confidence
- **High Confidence:** Few-shot prompting consistently improves counting accuracy (>90% for top models) - directly supported by quantitative results and intuitive in-context learning mechanism
- **Medium Confidence:** Localization failure stems from visual encoders' inability to produce precise spatial mappings - well-supported by metrics but lacks direct architectural evidence
- **Low Confidence:** Chart design sensitivity (aspect ratios, colors) - statistical significance shown, but practical impact appears minimal outside edge cases

## Next Checks
1. **Replication of Counting Accuracy:** Execute cluster counting prompts (zero-shot vs. few-shot) on a 50-plot subset to verify >90% accuracy for GPT-4o/Gemini Flash
2. **Spatial Precision Threshold Analysis:** Compare outlier identification results using strict (10px) vs. relaxed (50px) distance thresholds to quantify localization limits
3. **Aspect Ratio Impact Verification:** Generate and evaluate "Default" vs. "21:9" aspect ratio plots to confirm the statistical penalty reported for wide formats