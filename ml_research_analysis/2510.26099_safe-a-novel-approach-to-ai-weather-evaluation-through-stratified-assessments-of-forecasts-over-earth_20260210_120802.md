---
ver: rpa2
title: 'SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments
  of Forecasts over Earth'
arxiv_id: '2510.26099'
source_url: https://arxiv.org/abs/2510.26099
tags:
- z500
- t850
- weather
- lead
- rmse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth

## Quick Facts
- **arXiv ID:** 2510.26099
- **Source URL:** https://arxiv.org/abs/2510.26099
- **Reference count:** 28
- **Primary result:** Introduces a framework for evaluating AI weather prediction models across geographic strata rather than global averages, revealing performance disparities across territories, subregions, income levels, and landcover types

## Executive Summary
SAFE (Stratified Assessments of Forecasts over Earth) introduces a novel evaluation framework for AI weather prediction models that measures fairness across geographic strata rather than relying on global averages. The method applies stratified evaluation to mid-tropospheric variables (T850 and Z500) from the WeatherBench 2 dataset, revealing significant performance disparities across territories, subregions, income levels, and land/water boundaries. Using latitude-weighted RMSE within each stratum, SAFE quantifies model fairness through maximum absolute differences and variance of per-strata errors. The framework exposes how traditional global metrics can mask geographic inequities, with implications for ensuring equitable weather forecasting capabilities worldwide.

## Method Summary
SAFE evaluates AI weather prediction models by stratifying gridpoints according to geographic attributes and computing latitude-weighted RMSE within each stratum. The framework uses WeatherBench 2 data at 1.5° resolution, covering 240×121 grid cells, with predictions from six models (GraphCast, Keisler, Pangu-Weather, Spherical CNN, FuXi, NeuralGCM) for the year 2020. Gridpoints are converted to polygons and intersected with attribute strata from geoBoundaries (territory), UN definitions (subregion), World Bank (income), and binary land/water masks. Unlike standard spherical approximations, SAFE employs oblate spheroid surface area weighting (Calvimontes 2018) for more accurate polar region calculations. Fairness is quantified through two metrics: greatest absolute difference in per-strata RMSE and variance of per-strata RMSE across lead times from 12 to 240 hours.

## Key Results
- Models exhibit significant performance disparities across geographic strata, with some showing up to 20% higher RMSE in certain territories compared to others
- Spherical CNN demonstrates the highest fairness variance among evaluated models, while FuXi shows relatively consistent performance across income and territory strata
- Traditional global RMSE metrics mask these geographic inequities, potentially misleading model selection and deployment decisions
- Latitude-weighted stratification reveals that polar region weighting errors using spherical approximations can exceed 500% at high resolutions

## Why This Works (Mechanism)
The framework works by decomposing global weather prediction performance into geographically meaningful strata, revealing that averaging over heterogeneous regions obscures systematic biases. By applying proper oblate spheroid weighting and calculating per-stratum metrics, SAFE exposes performance patterns that would otherwise be hidden by conventional evaluation approaches.

## Foundational Learning
**Geographic Stratification**: Dividing prediction space by territory, subregion, income, and landcover to reveal performance patterns - needed to identify geographic inequities that global averages mask - quick check: verify strata counts match expected values
**Latitude-Weighted RMSE**: Using proper area weighting for spherical coordinates to ensure accurate error aggregation - needed because grid cell areas vary significantly with latitude - quick check: confirm polar weights differ from spherical approximation
**Fairness Metrics**: Quantifying performance equity through maximum absolute difference and variance across strata - needed to objectively measure geographic bias - quick check: ensure metrics capture relative performance gaps

## Architecture Onboarding
**Component Map**: ERA5 Ground Truth -> WeatherBench 2 Processing -> Model Predictions -> Stratification Engine -> Weighted RMSE Calculator -> Fairness Metrics
**Critical Path**: The stratification and area weighting components are critical - errors here propagate to all fairness metrics and could invalidate geographic comparisons
**Design Tradeoffs**: Uses 1.5° resolution for computational feasibility versus higher resolution for detail; employs oblate spheroid weighting for accuracy versus simpler spherical models for speed
**Failure Signatures**: Mismatched grid resolutions (not 240×121), incorrect area weighting (spherical vs oblate), or outdated geographic boundary data will produce invalid fairness scores
**First Experiments**: (1) Verify input array shapes before stratification, (2) Check area weighting differences between spherical and oblate implementations, (3) Compare territory counts from geoBoundaries data to expected values

## Open Questions the Paper Calls Out
**Open Question 1**: Can incorporating spatial stratification directly into the training objectives of AI weather models ameliorate the performance biases identified by the SAFE framework? The current study operates at inference time only, with no retrained models to test if stratified losses reduce disparities.

**Open Question 2**: What specific architectural or data-driven factors cause some models (e.g., FuXi) to exhibit significantly higher fairness across income and territory strata compared to others (e.g., Spherical CNN)? The paper benchmarks models but doesn't isolate which components correlate with reduced fairness variance.

**Open Question 3**: How does the assessment of forecast fairness change when stratifying by population density or coastal proximity rather than binary landcover? The current implementation lacks granular population data or specific coastal delineation, which the authors identify as important future directions.

## Limitations
- Results based on 2020 data may not generalize to different climate years or seasonal regimes
- Exclusive focus on mid-tropospheric variables (T850, Z500) limits conclusions about surface weather prediction capabilities
- While oblate spheroid weighting is more accurate, specific implementation details from the cited source are not fully specified

## Confidence
**High confidence**: The stratified evaluation methodology and fairness metrics are clearly defined and reproducible
**Medium confidence**: The core findings about relative model performance across strata are robust, though absolute values may vary slightly with different data versions
**Low confidence**: Generalization of fairness patterns to other variables, lead times beyond 10 days, or different climate conditions

## Next Checks
1. Verify the exact version of geoBoundaries data used matches the paper's results by comparing territory counts and border polygons
2. Cross-check the oblate spheroid area weighting implementation against Calvimontes (2018) Eq. 49 with specified Earth radii constants
3. Run a sensitivity analysis using a different climate year (e.g., 2019) to assess year-to-year variability in fairness metrics