---
ver: rpa2
title: Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors
arxiv_id: '2504.20106'
source_url: https://arxiv.org/abs/2504.20106
tags:
- preference
- helpful
- harmless
- alignment
- helpfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a modular approach to multi-preference alignment
  by training separate models on individual preferences (helpful vs unhelpful, harmless
  vs harmful) and then extracting preference vectors via parameter-wise subtraction.
  These vectors are combined at inference time, allowing user-controllable preference
  scaling and seamless integration of new preferences without retraining.
---

# Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors

## Quick Facts
- **arXiv ID:** 2504.20106
- **Source URL:** https://arxiv.org/abs/2504.20106
- **Reference count:** 28
- **Key outcome:** Modular multi-preference alignment improves helpfulness (2.00 vs 0.72 baseline) and reduces refusal rates compared to RLHF-based methods.

## Executive Summary
This paper proposes a modular approach to multi-preference alignment by training separate models on individual preferences (helpful vs unhelpful, harmless vs harmful) and extracting preference vectors via parameter-wise subtraction. These vectors are combined at inference time, allowing user-controllable preference scaling and seamless integration of new preferences without retraining. Experiments on Llama-3.2-3B, Llama-3.1-8B, and Mistral-7B show improved helpfulness and comparable harmlessness while reducing refusal rates compared to RLHF-based methods.

## Method Summary
The method trains four separate DPO models (helpful+, helpful-, harmless+, harmless-) from a base SFT checkpoint using PKU-SafeRLHF data with label switching. Preference vectors are extracted via parameter-wise subtraction (θHelpful+ - θHelpful- and θHarmless+ - θHarmless-). At inference, these vectors are added to the base model with user-controllable scaling coefficients (η), enabling dynamic preference adjustment without retraining.

## Key Results
- Helpfulness score improved from 0.72 (baseline) to 2.00 using preference vectors
- Harmlessness maintained at comparable levels while reducing refusal rates vs RLHF
- Cross-seed consistency of 0.998-0.999 for DPO-based vectors vs 0.208-0.257 for PPO-based vectors

## Why This Works (Mechanism)

### Mechanism 1
Parameter-wise subtraction between opposing preference models isolates behavior directions that can be linearly combined. The resulting vector captures a directional shift in parameter space that, when added to a base model, modulates behavior without joint optimization.

### Mechanism 2
Label switching creates contrastive preference datasets without additional annotation. Swapping winner/loser labels induces approximately opposite optimization pressures, amplifying behavior differences along the target axis.

### Mechanism 3
Inference-time scaling coefficients (η) enable user-controllable preference intensity without retraining. Scaling adjusts preference strength; negative values invert behavior, decoupling training from deployment-time trade-offs.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Preference Vector builds on DPO rather than RLHF; understanding the reformulation of preference learning as classification is essential.
  - Quick check question: Can you explain why DPO eliminates the need for an explicit reward model?

- **Concept: Task Arithmetic / Model Merging**
  - Why needed here: The paper's core operation (parameter-wise addition/subtraction) is derived from Ilharco et al. (2023); understanding task vectors is prerequisite.
  - Quick check question: What does it mean for two task vectors to interfere destructively when merged?

- **Concept: Multi-Objective Optimization Trade-offs**
  - Why needed here: Helpful/harmless alignment is inherently conflicting; the paper's modular approach addresses the same Pareto-frontier problem as Safe-RLHF but via decoupled training.
  - Quick check question: Why does constrained optimization (Safe-RLHF) risk reward hacking, and how does Preference Vector avoid it?

## Architecture Onboarding

- **Component map:** θBase -> Four DPO models -> Preference vectors (ϕHelpful, ϕHarmless) -> θAggregated = θBase + ηHelpful·ϕHelpful + ηHarmless·ϕHarmless

- **Critical path:**
  1. SFT on instruction-following data → θBase
  2. Train 4 DPO models (2 preferences × 2 directions) with label-switched datasets
  3. Extract preference vectors via parameter subtraction
  4. At inference, apply scaled vectors to θBase (no GPU required for step 4)

- **Design tradeoffs:**
  - Training overhead: 4 DPO runs vs. 1 joint optimization (4h vs. 1h for single DPO, but < RLHF baselines)
  - Flexibility vs. alignment tax: Extended preferences show gradual STI increase (0.41 → 2.33) with more vectors
  - DPO vs. PPO base: DPO vectors show higher cross-seed consistency (0.998–0.999 vs. 0.208–0.257 for PPO)

- **Failure signatures:**
  - Low-magnitude vectors (η near 0): No behavior change; check if DPO converged
  - Overly conservative outputs (high refusal): May indicate ηHarmless too high or reward hacking in base training
  - Incoherent outputs: η outside valid range; check commonsense retention

- **First 3 experiments:**
  1. Reproduce single-preference extraction: Train θHelpful+ and θHelpful− on a held-out split, verify ϕHelpful improves helpfulness score when ηHelpful = 1.0
  2. Controllability sweep: Vary ηHelpful, ηHarmless ∈ {−1.0, −0.5, 0.0, 0.5, 1.0} and plot helpfulness/harmlessness curves
  3. Extension test: Add a new preference (e.g., verbosity) by training θVerbose+/θVerbose−, extract ϕVerbose, and verify modular addition without retraining existing vectors

## Open Questions the Paper Calls Out

### Open Question 1
How can optimal scaling coefficients (η) be determined automatically or principledly without requiring validation-set sweeps? The authors state this remains an open challenge with no theoretical grounding for coefficient selection.

### Open Question 2
Can interference-aware composition strategies reduce alignment tax when scaling to many preference dimensions beyond four? Appendix F identifies this as future work, noting STI increases from 0.41 to 2.33 as preferences grow from 2 to 4.

### Open Question 3
Why do DPO-based preference vectors exhibit dramatically higher cross-seed consistency (cosine similarity ~0.999) compared to PPO-based vectors (~0.257)? The paper attributes PPO variability to "sensitivity to reward noise" but provides no mechanistic explanation.

## Limitations

- The linear vector arithmetic assumption lacks rigorous theoretical grounding for complex, high-dimensional preference interactions
- The method's scalability to more than two preferences remains untested with only one extension example provided
- The absence of explicit reward modeling could limit the ability to capture nuanced trade-offs between competing objectives

## Confidence

- **High**: The modular approach works as described, with clear improvements in helpfulness (2.00 vs 0.72 baseline) and reduced refusal rates compared to RLHF
- **Medium**: The inference-time controllability via scaling coefficients is demonstrated, but the linear regime assumption for large η values is not rigorously tested
- **Low**: The claim that the method seamlessly integrates new preferences without retraining is supported only for a single extension

## Next Checks

1. **Correlation Stress Test**: Systematically vary the correlation between helpfulness and harmlessness datasets and measure the impact on vector orthogonality and controllability

2. **High-Dimensional Preference Scaling**: Extend the framework to three or more preferences and evaluate whether the linear aggregation assumption holds or leads to destructive interference

3. **Long-Term Knowledge Retention**: Conduct comprehensive evaluation of commonsense and factual knowledge retention after applying large preference vectors (η > 1.0) to ensure no catastrophic forgetting occurs