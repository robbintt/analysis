---
ver: rpa2
title: Exploring the Effects of Alignment on Numerical Bias in Large Language Models
arxiv_id: '2601.16444'
source_url: https://arxiv.org/abs/2601.16444
tags:
- bias
- score
- numerical
- evaluation
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how alignment processes affect numerical\
  \ bias in LLM-as-a-judge systems. Through experiments comparing pre- and post-alignment\
  \ models on MTQE, GECQE, and LCP tasks, the study finds that alignment significantly\
  \ increases numerical bias\u2014post-alignment models produce highly skewed score\
  \ distributions concentrated around specific values, particularly for high-resource\
  \ languages."
---

# Exploring the Effects of Alignment on Numerical Bias in Large Language Models

## Quick Facts
- arXiv ID: 2601.16444
- Source URL: https://arxiv.org/abs/2601.16444
- Reference count: 25
- Primary result: Alignment significantly increases numerical bias in LLM-as-a-judge systems, with mitigation most effective through score range adjustment

## Executive Summary
This paper investigates how alignment processes affect numerical bias in LLM-as-a-judge systems through experiments comparing pre- and post-alignment models on machine translation quality estimation, grammatical error correction quality estimation, and linguistic complexity prediction tasks. The study reveals that alignment significantly increases numerical bias, causing post-alignment models to produce highly skewed score distributions concentrated around specific values, particularly for high-resource languages. While alignment improves instruction-following and overall evaluation accuracy, it simultaneously introduces stronger numerical bias that correlates with lower evaluation performance.

The research explores multiple mitigation strategies including temperature scaling, distribution calibration, and score range adjustment, finding that adjusting the score range proves most effective at reducing bias and improving accuracy. The findings suggest that score range should be treated as a tunable hyperparameter rather than a fixed design choice in LLM-as-a-judge systems, as the alignment process fundamentally alters how models distribute numerical scores across evaluation tasks.

## Method Summary
The study compares pre- and post-alignment models across three evaluation tasks: machine translation quality estimation (MTQE), grammatical error correction quality estimation (GECQE), and linguistic complexity prediction (LCP). Models are evaluated on their ability to produce numerical scores that align with human judgments, with particular attention paid to the distribution characteristics of these scores. The researchers implement and test three mitigation strategies: temperature scaling to modify output probability distributions, distribution calibration techniques to align model outputs with target distributions, and score range adjustment to modify the numerical bounds within which models operate. Performance is measured using standard evaluation metrics for each task, with additional analysis of score distribution skewness and correlation with evaluation accuracy.

## Key Results
- Alignment significantly increases numerical bias, with post-alignment models producing highly skewed score distributions concentrated around specific values
- The numerical bias introduced by alignment correlates with lower evaluation performance across all three tasks
- Score range adjustment proves most effective as a mitigation strategy, reducing bias while improving evaluation accuracy
- High-resource languages show more pronounced bias effects compared to low-resource languages after alignment

## Why This Works (Mechanism)
The mechanism underlying numerical bias in aligned LLMs relates to how alignment processes optimize for instruction-following and overall task accuracy at the expense of preserving natural score distribution characteristics. When models undergo alignment, they learn to prioritize producing outputs that match expected patterns and instructions rather than maintaining the statistical properties of their pre-alignment scoring behavior. This optimization pressure causes the model to collapse its score distribution toward values that maximize reward or minimize loss during fine-tuning, resulting in the observed skewness. The correlation between bias and reduced performance suggests that natural score variability, which may encode important information about confidence and uncertainty, is lost during alignment.

## Foundational Learning
- **Numerical bias in LLM outputs**: Understanding how LLMs distribute scores is crucial for judge systems where score interpretation matters beyond simple ranking
- **Alignment methodology impacts**: Different alignment approaches (SFT, RLHF, DPO) can produce varying bias patterns, affecting downstream evaluation quality
- **Score distribution characteristics**: Skewness, kurtosis, and modality in model outputs provide insights into model behavior and potential evaluation artifacts
- **Cross-lingual evaluation consistency**: Bias patterns may vary across language pairs, affecting the generalizability of judge systems
- **Hyperparameter sensitivity**: Score range and other design choices significantly impact bias and accuracy trade-offs
- **Correlation vs. causation in model behavior**: Understanding whether bias directly causes performance degradation or reflects underlying model limitations

## Architecture Onboarding

Component map: Pre-alignment model -> Alignment process -> Post-alignment model -> Evaluation task -> Score distribution analysis -> Performance metrics

Critical path: Pre-alignment model performance → Alignment optimization → Post-alignment bias introduction → Score distribution distortion → Evaluation accuracy degradation → Mitigation implementation

Design tradeoffs: Alignment improves instruction-following but introduces numerical bias; mitigation strategies balance bias reduction against maintaining task accuracy; high-resource languages benefit more from alignment but suffer more from bias

Failure signatures: Highly skewed score distributions, concentration of scores around specific values, reduced score variability, correlation between bias magnitude and evaluation performance degradation

First experiments:
1. Compare score distributions between pre- and post-alignment models across multiple tasks to quantify bias introduction
2. Test temperature scaling effects on score distribution characteristics and evaluation accuracy
3. Implement score range adjustment and measure impact on both bias reduction and task performance

## Open Questions the Paper Calls Out
None

## Limitations
- Research focuses primarily on English and Chinese languages, limiting generalizability to other language pairs
- The specific alignment method employed may not capture the full spectrum of alignment approaches that could produce different bias patterns
- Evaluation metrics may not fully capture all aspects of judge quality, particularly inter-annotator agreement or task-specific nuances

## Confidence
- Alignment significantly increases numerical bias: High confidence based on systematic comparison across three distinct evaluation tasks
- Score range adjustment as most effective mitigation: Medium confidence, dependent on specific experimental setup
- Correlation between numerical bias and lower evaluation performance: Medium confidence, causality difficult to establish definitively

## Next Checks
1. Replicate experiments across a broader range of language pairs and language families to assess cross-linguistic generalizability
2. Test additional alignment methods (constitutional AI, direct preference optimization) to determine if bias patterns vary by alignment approach
3. Conduct human evaluation studies to validate whether reduced numerical bias through score range adjustment translates to improved alignment with human judgment quality