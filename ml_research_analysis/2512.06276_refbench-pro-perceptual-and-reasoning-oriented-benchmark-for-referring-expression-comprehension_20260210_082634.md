---
ver: rpa2
title: 'RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression
  Comprehension'
arxiv_id: '2512.06276'
source_url: https://arxiv.org/abs/2512.06276
tags:
- object
- visual
- arxiv
- reasoning
- referring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RefBench-PRO, a new benchmark for referring
  expression comprehension (REC) that evaluates both perceptual and reasoning capabilities
  of multimodal large language models (MLLMs). The benchmark is divided into six tasks:
  attribute, position, interaction (perceptual), and relation, commonsense, reject
  (reasoning).'
---

# RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension

## Quick Facts
- arXiv ID: 2512.06276
- Source URL: https://arxiv.org/abs/2512.06276
- Authors: Tianyi Gao; Hao Li; Han Fang; Xin Wei; Xiaodong Dong; Hongbo Sun; Ye Yuan; Zhongjiang He; Jinglin Xu; Jingmin Xin; Hao Sun
- Reference count: 40
- Introduces RefBench-PRO, a new benchmark for referring expression comprehension with 200k training samples and 6k test pairs across 6 tasks

## Executive Summary
RefBench-PRO is a comprehensive benchmark designed to evaluate both perceptual and reasoning capabilities of multimodal large language models (MLLMs) in referring expression comprehension tasks. The benchmark introduces a fine-grained annotation pipeline that generates high-quality training and test samples across six distinct task categories. To address the challenges identified by RefBench-PRO, the authors propose Ref-R1, a two-stage training framework incorporating chain-of-thought supervision and DyIoU-GRPO optimization. Experimental results demonstrate that current MLLMs show significant performance gaps, particularly in reasoning tasks, while Ref-R1 achieves substantial improvements across all benchmark categories.

## Method Summary
The authors developed RefBench-PRO through a systematic approach that categorizes referring expression comprehension into six tasks: three perceptual (attribute, position, interaction) and three reasoning-oriented (relation, commonsense, reject). A fine-grained annotation pipeline was employed to create 200,000 high-quality training samples and 6,000 test pairs. The proposed Ref-R1 framework consists of two training stages: first, models are trained with chain-of-thought supervision to enhance reasoning capabilities, followed by DyIoU-GRPO optimization to improve localization accuracy. This approach addresses both the perceptual aspects of grounding and the higher-level reasoning required for complex referring expressions.

## Key Results
- RefBench-PRO exposes significant performance gaps in current MLLMs, particularly on reasoning tasks compared to perceptual tasks
- Ref-R1 framework achieves strong performance gains across all six benchmark categories
- The two-stage training approach with chain-of-thought supervision and DyIoU-GRPO demonstrates improved localization accuracy and reasoning ability

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive task categorization that separates perceptual from reasoning capabilities, allowing for targeted evaluation of MLLM limitations. The fine-grained annotation pipeline ensures high-quality training data that captures nuanced aspects of referring expressions. The Ref-R1 framework works by first developing reasoning capabilities through chain-of-thought supervision, then refining localization precision with DyIoU-GRPO, creating a synergistic approach that addresses both aspects of referring expression comprehension.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: AI systems that process and integrate both visual and textual information to perform complex reasoning tasks. Why needed: Essential foundation for understanding how models handle referring expressions that require both visual perception and language understanding. Quick check: Verify model can correctly associate textual descriptions with corresponding visual regions.

**Referring Expression Comprehension (REC)**: Task of localizing objects in images based on natural language descriptions. Why needed: Core task being evaluated and improved by the benchmark and proposed framework. Quick check: Ensure model can accurately identify objects from simple descriptive phrases.

**Chain-of-Thought Supervision**: Training approach that encourages step-by-step reasoning by providing intermediate reasoning steps. Why needed: Critical for developing the reasoning capabilities that current MLLMs lack in complex referring expressions. Quick check: Validate model can break down complex reasoning into logical intermediate steps.

**DyIoU-GRPO (Dynamic IoU-Gradient Reward Policy Optimization)**: Optimization technique that improves localization accuracy by dynamically adjusting reward based on Intersection over Union metrics. Why needed: Addresses the precision requirements for accurate object localization in REC tasks. Quick check: Confirm improved localization accuracy through quantitative IoU metrics.

## Architecture Onboarding

**Component Map**: Input Image + Text Description -> Visual Encoder -> Text Encoder -> Cross-Modal Fusion -> Reasoning Module -> Localization Head -> Output Bounding Box

**Critical Path**: The reasoning module represents the critical path, as it determines the model's ability to handle complex referring expressions requiring commonsense or relational understanding. Performance bottlenecks here directly impact overall task success.

**Design Tradeoffs**: The two-stage training approach trades computational efficiency during training for improved final performance, requiring separate optimization phases. The fine-grained annotation pipeline requires significant human effort but produces higher-quality training data that better captures complex reasoning scenarios.

**Failure Signatures**: Models typically fail on reasoning tasks by either oversimplifying complex relationships, ignoring contextual information, or making incorrect commonsense inferences. Perceptual tasks fail primarily due to localization inaccuracies rather than reasoning errors.

**First Experiments**:
1. Evaluate baseline MLLM performance on each of the six RefBench-PRO tasks to establish performance baselines
2. Test Ref-R1's performance gains on the simplest perceptual tasks before evaluating complex reasoning tasks
3. Conduct ablation studies removing either chain-of-thought supervision or DyIoU-GRPO to isolate their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark focuses on specific reasoning types (relation, commonsense, reject) and may not capture all forms of reasoning required in real-world scenarios
- Performance gains from Ref-R1 may be influenced by the controlled, high-quality data used in training, raising questions about generalization to less curated datasets
- The fine-grained annotation pipeline's contribution to performance improvements is not fully isolated from the model architecture's inherent capabilities

## Confidence

**High Confidence**: Benchmark construction methodology, dataset statistics (200K training samples, 6K test pairs), task categorization (6 tasks: 3 perceptual + 3 reasoning), and two-stage training framework description are well-documented and verifiable.

**Medium Confidence**: Claims of "significant gaps" in current MLLM grounding performance require deeper scrutiny as they depend heavily on specific MLLMs tested and their evaluation protocols.

**Medium Confidence**: Performance gains attributed to Ref-R1 may be partially influenced by the specific evaluation setup and fine-grained annotations used during training, with uncertain transfer to more general datasets.

## Next Checks
1. Evaluate Ref-R1 on a broader range of MLLMs and compare performance across different model architectures to ensure gains are not model-specific
2. Test the benchmark's generalization by applying it to less curated, real-world datasets to assess model robustness in uncontrolled environments
3. Conduct ablation studies to isolate the contribution of fine-grained annotations versus the two-stage training framework to observed performance improvements