---
ver: rpa2
title: 'Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise
  Workflows'
arxiv_id: '2512.13168'
source_url: https://arxiv.org/abs/2512.13168
tags:
- workflows
- workflow
- spreadsheets
- data
- spreadsheet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FINCH, a finance and accounting benchmark
  for evaluating AI agents on real-world, enterprise-grade professional workflows.
  FINCH is sourced from authentic enterprise workspaces, including 15,000 spreadsheets
  and 500,000 emails from 150 employees at Enron and other financial institutions.
---

# Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows

## Quick Facts
- arXiv ID: 2512.13168
- Source URL: https://arxiv.org/abs/2512.13168
- Reference count: 40
- Primary result: Even best AI models pass fewer than 40% of real-world enterprise finance workflows

## Executive Summary
FINCH is a finance and accounting benchmark for evaluating AI agents on authentic enterprise workflows. Derived from 15,000 spreadsheets and 500,000 emails across 150 employees at Enron and other institutions, it comprises 172 workflows with 384 tasks involving 1,710 spreadsheets containing 27 million cells. Evaluations of frontier AI systems reveal that even the best models achieve fewer than 40% workflow pass rates, highlighting substantial challenges in handling real-world enterprise complexity.

## Method Summary
The benchmark employs LLM-assisted workflow discovery from authentic enterprise artifacts including email threads and versioned spreadsheets, followed by expert annotation to normalize business intent into precise task instructions. Spreadsheets are encoded using semantic-rich tuple serialization preserving cell addresses, values, types, and formulas. Evaluation uses both human annotators and an LLM-as-judge framework with structured diffing and screenshot analysis. The dataset and evaluation framework are publicly available at https://huggingface.co/FinWorkBench.

## Key Results
- Best models achieve fewer than 40% workflow pass rates on authentic enterprise tasks
- Pass rates drop sharply from 44.3% to 23.5% when workflows contain more than two tasks
- Major error sources include data retrieval errors (25%), formula reasoning errors (35%), and off-by-one range errors in irregular layouts

## Why This Works (Mechanism)

### Mechanism 1: LLM-Assisted Workflow Discovery from Authentic Enterprise Artifacts
- Claim: Workflows derived from real email threads and versioned spreadsheets capture authentic enterprise complexity better than synthetic tasks
- Mechanism: GPT-5 identifies collaborative email threads stating explicit business goals with spreadsheet attachments; LLM-based differencing on versioned spreadsheet pairs infers underlying workflow intent. Expert annotators normalize conversational text into precise task instructions while preserving business intent.
- Core assumption: Enterprise email threads and spreadsheet version histories encode coherent, recoverable professional workflows rather than noise
- Evidence anchors:
  - [abstract]: "LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files"
  - [section 2.1.1]: "We first mine real-world enterprise email threads to surface workflows... prompt GPT-5 to identify collaborative messages that (i) explicitly state a business goal... and (ii) reference one or more attached spreadsheets"
  - [corpus]: SODBench paper documents spreadsheet operation challenges in business contexts, supporting the need for systematic workflow capture methods
- Break condition: If email communications are too ambiguous or version changes represent incidental churn rather than intentional workflows

### Mechanism 2: Semantic-Rich Spreadsheet Encoding Preserves Latent Formula Logic
- Claim: Tuple-based encoding (Address, Value, Type, Formula) enables models to access business logic that values alone obscure
- Mechanism: Each cell serialized as a structured tuple preserving cell references, data types, and formula expressions. This addresses the failure mode where models ignore formulas that encode temporal assumptions and fine-grained dependencies not visible from displayed values.
- Core assumption: Models will use formula information when explicitly encoded rather than defaulting to value-only reasoning
- Evidence anchors:
  - [section 3.1.2]: "Each cell is encoded as a tuple (Address, Value, Type, Formula), where Address denotes the cell reference... Formula records the cell formula"
  - [section 3.3]: "formulas encode latent structure and logic... models typically prioritize cell values and under-use formulas, leading to systematic misinterpretations"
  - [corpus]: Limited direct corpus evidence for this specific encoding approach; related spreadsheet benchmarks don't emphasize formula preservation
- Break condition: If context limits force truncation removing formula dependency chains, or models still underweight formula information

### Mechanism 3: LLM-as-Judge with Structured Differencing for Scalable Evaluation
- Claim: Automated evaluation using computed diffs and visual screenshots achieves high agreement with human evaluators
- Mechanism: Framework computes structured diffs between input→reference and input→model outputs, creates compact snapshots (first/last 10 rows plus changed cells), renders screenshots for layout-sensitive properties. Judge evaluates completeness, correctness, and over-edit avoidance with binary pass/fail plus rationale.
- Core assumption: Binary scoring with NL rationale captures sufficient nuance for workflow-level evaluation
- Evidence anchors:
  - [section 3.2.1]: "automated evaluation largely aligns with human judgments: for GPT 5.1 Pro and Claude Sonnet 4.5, the judge agrees with human labels on 82.1% and 90.2% of workflows"
  - [section 2.3.2]: "This LLM-as-judge framework not only automates large-scale evaluation but also surfaces subtle spreadsheet errors (such as formulas silently replaced with static values)"
  - [corpus]: UI-CUBE benchmark emphasizes enterprise-grade evaluation beyond task accuracy, supporting rigorous automated assessment needs
- Break condition: If subtle numerical errors or layout distortions escape both diff analysis and screenshot review

## Foundational Learning

- Concept: Spreadsheet formula dependency chains and cross-sheet references
  - Why needed here: FINCH workflows average 21.5K formulas (median 212) with long dependency chains; 92.4% involve multiple sheets; understanding how formulas propagate logic across sheets is critical for debugging agent failures
  - Quick check question: Given a cell with formula `=A5*Volumes!B6*Curves!G7+25*Volumes!B7`, can you trace all upstream dependencies and identify what business logic this encodes?

- Concept: Error accumulation in multi-step compositional workflows
  - Why needed here: Pass rates drop from 44.3% (≤2 tasks) to 23.5% (>2 tasks); small early retrieval errors cascade through subsequent steps; 78.5% of workflows are multi-task
  - Quick check question: In a workflow requiring data entry → cross-sheet retrieval → calculation → validation → reporting, at which step would a column-offset error cause the largest downstream damage?

- Concept: Irregular spreadsheet layout parsing
  - Why needed here: Real enterprise spreadsheets have merged cells, multi-level headers, nested subtotals, blank rows/columns, and bespoke layouts that break assumptions about clean rectangular data
  - Quick check question: How would your code handle extracting data from a table where headers span rows 3-4 with merged cells and actual data starts at row 6 with intermittent blank separator rows?

## Architecture Onboarding

- Component map: Workflow construction pipeline (email mining → LLM summarization → expert annotation; version diffing → LLM inference → expert validation) → Spreadsheet encoding layer (semantic-rich tuple serialization with context-aware truncation) → Agent execution environment (product-side web agents with iterative tool calls vs API-based single-shot code generation) → Evaluation framework (human gold standard + LLM-as-judge with diff/screenshot multimodal inputs)

- Critical path: Source artifact selection → workflow instruction authoring → spreadsheet encoding → agent execution → diff computation → LLM-as-judge → human validation calibration

- Design tradeoffs:
  - Product-side agents (iterative execution, self-correction, ~40% pass rate) vs API-based single-shot (no feedback loop, ~32% pass rate but more controlled)
  - Compact snapshots reduce tokens but risk omitting context; full-sheet encoding preserves fidelity but hits context limits
  - Binary pass/fail simplifies evaluation but may obscure partial credit for multi-step workflows with early failures

- Failure signatures:
  - Off-by-one range errors when layouts have irregular structures (section 3.3: "even tiny misinterpretations of these layouts... propagate into globally incorrect outputs")
  - Formula-to-constant conversion silently losing business logic (section 3.2.1: formulas "silently replaced with static values")
  - Over-editing: models making unintended changes beyond instruction scope (explicit rubric criterion)
  - Cross-sheet retrieval selecting wrong ranges due to lexically similar but semantically different headers (e.g., "adjusted vs. unadjusted metrics")

- First 3 experiments:
  1. Validate encoding approach: Compare semantic-rich tuple encoding vs text-only table representation on 20 calculation-heavy workflows to isolate formula reasoning improvements
  2. Quantify composition degradation: Plot pass rate vs task count to establish baseline error accumulation curve for your agent architecture
  3. Calibrate judge agreement: Run LLM-as-judge on 30 held-out workflows with parallel human evaluation to measure precision/recall before relying on automated scoring at scale

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-turn agentic frameworks with iterative self-correction close the performance gap between API-based models and product-side agents on complex financial workflows?
- Basis in paper: [explicit] Appendix C states, "It’s desirable for future work to explore agentic methods with multiple rounds of API calls" to overcome the limitations of the single-shot API protocol used in the study.
- Why unresolved: The experiments showed a significant gap between product agents (which can use tools iteratively) and API models (which were restricted to a single call), but the specific contribution of multi-turn execution to solving these specific spreadsheet errors was not isolated.
- What evidence would resolve it: An ablation study applying a multi-turn, code-execution agent framework (e.g., based on SpreadsheetBench) to the FINCH dataset to measure the improvement over single-shot baselines.

### Open Question 2
- Question: How can models be improved to prioritize latent business logic encoded in spreadsheet formulas over displayed cell values?
- Basis in paper: [explicit] Section 3.3 (Error Analysis) notes that "models typically prioritize cell values and under-use formulas, leading to systematic misinterpretations" of columns that encode complex logic like payment timing schedules.
- Why unresolved: While the paper identifies this as a major error source, it does not propose or test specific methods (e.g., specialized attention mechanisms or pre-training objectives) to force models to reason over formula dependencies.
- What evidence would resolve it: Evaluating models specifically on "formula-heavy" workflows where cell values are ambiguous without inspecting the underlying formula, comparing standard models against those explicitly trained or prompted to analyze formula graphs.

### Open Question 3
- Question: What mechanisms are most effective at mitigating error accumulation in long-horizon workflows where the number of tasks exceeds two?
- Basis in paper: [inferred] Section 3.2 highlights that "long-horizon composition is a key bottleneck," observing that pass rates for GPT 5.1 Pro drop sharply from 44.3% to 23.5% when workflows contain more than two tasks.
- Why unresolved: The paper quantifies the drop but does not determine if the failure is primarily due to context truncation, loss of instruction adherence over time, or compounding errors in intermediate data states.
- What evidence would resolve it: A detailed failure analysis of intermediate steps in long-horizon workflows to pinpoint if models fail to retrieve context, forget the initial instruction, or fail to validate intermediate data artifacts.

## Limitations

- LLM-assisted workflow construction relies on potentially ambiguous enterprise communications where business intent may be unclear
- Formula preservation benefits depend on models actually utilizing semantic-rich encoding despite context truncation pressures
- Automated evaluation accuracy hinges on binary pass/fail scoring capturing meaningful workflow success despite multi-step complexity

## Confidence

- **High**: Enterprise workflow authenticity, spreadsheet scale (27M cells, 1,710 files), documented agent performance gaps (40% max pass rate)
- **Medium**: LLM-assisted workflow derivation methodology, semantic-rich encoding benefits, evaluation framework calibration
- **Low**: Generalization to other enterprise domains, long-term formula dependency chain handling, cross-cultural business communication parsing

## Next Checks

1. **Encoding Validation**: Systematically compare semantic-rich tuple encoding vs text-only representations on 50+ calculation-heavy workflows to quantify formula reasoning improvements
2. **Workflow Boundary Testing**: Manually audit 30 randomly sampled workflows to verify LLM-derived business intent alignment with human-annotated goals
3. **Error Propagation Analysis**: Instrument agent executions to trace and quantify how early retrieval errors compound through multi-step workflows, measuring actual vs claimed degradation rates