---
ver: rpa2
title: 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining
  Data Selection'
arxiv_id: '2505.07293'
source_url: https://arxiv.org/abs/2505.07293
tags:
- data
- attentioninfluence
- arxiv
- heads
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AttentionInfluence is a training-free data selection method that\
  \ leverages attention head activation patterns in pretrained language models to\
  \ identify reasoning-intensive pretraining data. The method detects retrieval heads\u2014\
  attention heads critical for reasoning and in-context learning\u2014and selects\
  \ samples causing the largest performance drop when these heads are masked."
---

# AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection

## Quick Facts
- arXiv ID: 2505.07293
- Source URL: https://arxiv.org/abs/2505.07293
- Reference count: 40
- Primary result: Training a 7B model on 73B tokens selected by AttentionInfluence from SmolLM-Corpus improved performance by 1.4-3.5 percentage points across knowledge-intensive and reasoning-heavy benchmarks.

## Executive Summary
AttentionInfluence introduces a training-free data selection method that leverages attention head activation patterns in pretrained language models to identify reasoning-intensive pretraining data. The approach detects retrieval heads—attention heads critical for reasoning and in-context learning—and selects samples causing the largest performance drop when these heads are masked. When applied to select 73B tokens from SmolLM-Corpus using a 1.3B model, training a 7B model on the selected data improved performance by 1.4-3.5 percentage points across knowledge-intensive and reasoning-heavy benchmarks including MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval. The method demonstrates a weak-to-strong scaling property where smaller models can effectively select data that improves the performance of larger models.

## Method Summary
AttentionInfluence operates by first identifying retrieval heads in a pretrained model through attention masking experiments. These heads are attention heads critical for reasoning and in-context learning capabilities. The method then processes a large corpus, measuring how much each sample's performance degrades when retrieval heads are masked. Samples causing the largest performance drops are selected as they are deemed most reasoning-intensive. The selected subset is then used to pretrain larger models, leveraging the weak-to-strong scaling property where a smaller model's selection decisions benefit larger models' performance.

## Key Results
- 7B model trained on 73B tokens selected by AttentionInfluence outperformed baseline selection methods by 1.4-3.5 percentage points on MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval
- Selected data was found to be higher quality, more diverse, and more reasoning-intensive than baseline selection methods
- Demonstrated weak-to-strong scaling property where 1.3B model selection decisions improved 7B model performance

## Why This Works (Mechanism)
AttentionInfluence works by exploiting the relationship between attention head activation patterns and reasoning capabilities in language models. Retrieval heads, which are attention heads critical for reasoning and in-context learning, show distinct activation patterns when processing reasoning-intensive data. By masking these heads and measuring performance degradation, the method identifies which samples the model relies on these heads to process effectively. This creates a proxy measure for reasoning difficulty and importance. The weak-to-strong scaling property emerges because the fundamental reasoning patterns detected by smaller models are preserved in larger models, making the selected data beneficial across model sizes.

## Foundational Learning
- **Attention Mechanisms**: Understanding how attention heads process information and contribute to model reasoning capabilities. Needed to grasp why certain heads are critical for reasoning tasks. Quick check: Can identify retrieval heads and their role in model performance.
- **Weak-to-Strong Scaling**: The concept that smaller models can effectively select data that benefits larger models. Needed to understand the core scaling property of the method. Quick check: Can explain how selection decisions transfer across model sizes.
- **Pretraining Data Selection**: The process of filtering large corpora to improve model quality. Needed to contextualize AttentionInfluence within existing data selection approaches. Quick check: Can compare training-free vs training-based selection methods.

## Architecture Onboarding

**Component Map**: Corpus -> Attention Masking Experiments -> Retrieval Head Detection -> Sample Scoring -> Selected Subset -> Model Training

**Critical Path**: The core workflow involves running inference on the corpus with attention heads masked, detecting retrieval heads through performance degradation analysis, scoring samples based on degradation magnitude, and selecting top-scoring samples for training.

**Design Tradeoffs**: The method trades computational cost of attention masking experiments against the benefit of training-free selection. While avoiding the need to train multiple models for selection, it requires significant inference compute to process large corpora with various masking configurations.

**Failure Signatures**: Poor retrieval head detection leads to ineffective sample scoring. False positives in head detection or heads serving multiple functions can result in selecting suboptimal data. The method may also fail to generalize across different domains or languages if attention patterns differ significantly.

**3 First Experiments**:
1. Verify retrieval head detection by comparing masked vs unmasked performance on reasoning benchmarks
2. Test sample scoring consistency across multiple runs of the masking experiments
3. Validate weak-to-strong scaling by training models of different sizes on the same selected data

## Open Questions the Paper Calls Out
None

## Limitations
- The method's effectiveness depends heavily on stable retrieval head detection across different model architectures and sizes
- Performance gains are modest (1.4-3.5 percentage points) relative to the significant data reduction achieved
- Computational costs for processing large corpora with attention masking experiments are not fully disclosed

## Confidence

**High Confidence**: The observation that attention head activation patterns correlate with reasoning-intensive data is well-supported by the experimental results.

**Medium Confidence**: The claim that AttentionInfluence selects higher quality and more diverse data is supported but could benefit from more rigorous quality metrics beyond benchmark performance.

**Low Confidence**: The weak-to-strong scaling property's generalizability to different model sizes, architectures, and domains remains unproven.

## Next Checks
1. **Cross-Architecture Validation**: Test AttentionInfluence on different model architectures (e.g., transformer variants, sparse models) to verify head detection stability and selection effectiveness.

2. **Domain Generalization Study**: Apply the method to non-English corpora and specialized domains (e.g., biomedical, legal) to assess its effectiveness beyond general knowledge benchmarks.

3. **Cost-Benefit Analysis**: Conduct a detailed computational cost analysis comparing AttentionInfluence to alternative selection methods, including total processing time and resource requirements for large-scale corpus filtering.