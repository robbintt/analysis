---
ver: rpa2
title: Visual Language Models show widespread visual deficits on neuropsychological
  tests
arxiv_id: '2504.10786'
source_url: https://arxiv.org/abs/2504.10786
tags:
- visual
- tests
- arxiv
- task
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Three state-of-the-art VLMs were tested on 51 neuropsychological
  and psychological vision tests. Models performed near chance level on low- and mid-level
  visual tasks involving orientation, size, and occlusion judgments, but excelled
  on high-level object recognition tests.
---

# Visual Language Models show widespread visual deficits on neuropsychological tests

## Quick Facts
- **arXiv ID:** 2504.10786
- **Source URL:** https://arxiv.org/abs/2504.10786
- **Reference count:** 0
- **Primary result:** VLMs fail on low/mid-level visual tasks (orientation, size, occlusion) but excel at object recognition

## Executive Summary
This paper evaluates three state-of-the-art visual language models (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-pro) on 51 neuropsychological and experimental psychology vision tests. The models were tested using 1,586 images from clinical batteries (BORB, HVOT, DFPT) and psychological tests (MindSet:Vision, L-POST, L-EFT). Performance was measured against human normative data, with "clinically significant impairment" defined as >2 standard deviations below the human mean.

The results reveal a striking pattern: all three VLMs performed near chance level on low- and mid-level visual tasks involving orientation, size, and occlusion judgments, while excelling on high-level object recognition tests. GPT-4o and Claude-3.5-Sonnet showed clinically significant impairments on 18 tests each, and Gemini-1.5-pro on 16 tests. The highly correlated performance across tasks suggests these models achieve complex recognition without foundational visual concepts that humans learn implicitly.

## Method Summary
Three commercial VLMs were evaluated zero-shot on 51 neuropsychological and psychological vision tests using 1,586 images resized to 512x512px. The evaluation used four task formats (Naming, Match-to-sample, Same-different, Yes/No) with explicit instructions to provide "single definitive answers." Human baselines came from existing clinical norms for 6 batteries and a new N=60 online study for psychological tests. Performance was normalized to z-scores relative to human norms, with clinically significant impairment defined as >2 SD below the mean.

## Key Results
- VLMs performed at or near chance level on 18 clinical tests involving low- and mid-level visual processing
- All three models showed highly correlated performance patterns across tasks
- GPT-4o and Claude-3.5-Sonnet impaired on 18 tests each; Gemini-1.5-pro on 16 tests
- Models excelled at high-level object recognition while failing basic visual discrimination tasks

## Why This Works (Mechanism)
None provided in the paper.

## Foundational Learning

**Neuropsychological testing batteries**: Standardized assessments for visual perception and cognitive function. Needed to establish clinically validated benchmarks for visual deficits. Quick check: Verify test battery descriptions match published clinical manuals.

**Visual perception hierarchy**: Low-level (orientation, size, occlusion) → Mid-level (shape, texture) → High-level (object recognition). Needed to understand which visual processing stages VLMs can and cannot perform. Quick check: Map each test to its corresponding level in the perception hierarchy.

**Z-score normalization**: Statistical method to compare performance against population norms. Needed to quantify impairment relative to human performance. Quick check: Confirm z-score calculations use correct population mean and standard deviation.

**Zero-shot evaluation**: Testing without model fine-tuning or task-specific training. Needed to assess baseline visual capabilities of commercial VLMs. Quick check: Verify no task-specific prompts or examples were provided beyond standard instructions.

**Clinical significance threshold**: Definition of impairment as >2 standard deviations below human mean. Needed to establish objective criteria for visual deficits. Quick check: Confirm threshold calculation uses correct two-tailed critical values.

## Architecture Onboarding

**Component map**: Visual encoder → Text decoder → API interface. The visual encoder processes input images, the text decoder generates responses, and the API interface handles prompt formatting and response parsing.

**Critical path**: Image input → Visual feature extraction → Multimodal fusion → Language generation → Response output. Performance bottlenecks occur at the visual feature extraction stage for low-level visual properties.

**Design tradeoffs**: Commercial VLMs prioritize high-level recognition over low-level visual processing to optimize for common use cases. This creates systematic deficits in basic visual discrimination while maintaining strong object recognition capabilities.

**Failure signatures**: Chance-level performance on orientation, size, and occlusion tasks; consistent patterns across all three models; excellent performance on object naming despite failures in basic visual processing. These suggest architectural limitations rather than random errors.

**First experiments**:
1. Test VLMs on simple geometric discrimination tasks (same-different for basic shapes) to establish baseline visual processing capability.
2. Evaluate performance on degraded or occluded versions of familiar objects to probe occlusion handling.
3. Compare responses to mirror-reversed versus normally oriented objects to assess orientation sensitivity.

## Open Questions the Paper Calls Out

**Intervention efficacy**: Can specialized training datasets or alternative visual encoders remediate the observed low- and mid-level visual deficits? The authors suggest engineering solutions like adapters or specialized datasets could address shortcomings, but this remains untested.

**Chain-of-thought reasoning**: Does CoT enable VLMs to overcome low-level visual processing deficits? The authors note CoTs may reveal response strategies but were not tested with the current models, which used direct output only.

**Component isolation**: Do deficits stem from visual encoder failures or text decoder limitations? The closed-source nature of the models prevents isolating whether geometric features are present in embeddings but lost during text generation.

**Prompt engineering robustness**: Are deficits consistent across different prompt strategies? The authors used fixed prompts for clinical validity but acknowledge that alternative phrasing or grounding techniques might improve performance.

## Limitations

- **Proprietary stimuli**: Exact BORB and HVOT test materials are not public, requiring approximated re-renders that may not capture original test nuances
- **Single-pass evaluation**: No measure of within-model consistency or test-retest reliability to distinguish stochastic variation from true deficits
- **Human baseline uncertainty**: Performance gaps depend on accurate human normative data that cannot be fully verified without original clinical materials

## Confidence

- **High confidence**: VLMs perform at chance or near-chance on low- and mid-level visual discrimination tasks
- **Medium confidence**: Specific magnitude of impairment scores (z-score values) due to dependence on accurate human baselines and stimulus fidelity
- **Medium confidence**: Interpretation that deficits indicate missing implicit visual concepts, as evaluation cannot fully disentangle perceptual limitations from language-model guessing strategies

## Next Checks

1. **Stimulus fidelity validation**: Digitally recreate a subset of BORB and HVOT stimuli based on published descriptions, run them with all three VLMs, and compare accuracy profiles to reported results.

2. **Test-retest reliability check**: For 10 randomly selected tasks, run each trial twice with the same model and compute inter-run agreement to measure consistency.

3. **Human-machine gap replication**: Re-run the MindSet:Vision tests on a fresh sample of N=30 human participants matching original demographics to verify VLM-human performance gaps.