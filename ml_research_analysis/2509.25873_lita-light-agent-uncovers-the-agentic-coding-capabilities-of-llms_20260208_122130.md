---
ver: rpa2
title: 'Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs'
arxiv_id: '2509.25873'
source_url: https://arxiv.org/abs/2509.25873
tags:
- lita
- agent
- arxiv
- openhands
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lita, a lightweight autonomous agent for
  evaluating and extending LLMs in coding tasks. It addresses the problem of complex,
  task-specific scaffolding in current LLM-based coding agents, which introduces fairness
  issues, obscures true model capabilities, and creates high overhead.
---

# Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs

## Quick Facts
- **arXiv ID**: 2509.25873
- **Source URL**: https://arxiv.org/abs/2509.25873
- **Reference count**: 27
- **Primary result**: Lita achieves competitive or superior performance compared to workflow-based agents while consuming fewer tokens and requiring significantly less design effort.

## Executive Summary
Lita introduces a lightweight autonomous agent for evaluating and extending LLMs in coding tasks, addressing the problem of complex, task-specific scaffolding that obscures true model capabilities. The paper argues that heavy human intervention in current LLM-based coding agents introduces fairness issues, creates high overhead, and makes it difficult to assess intrinsic model abilities. Lita minimizes manual design by decoupling the agent from specific models and tasks, using only essential tools (Editor, Terminal, Search, Finish) and supporting structured reasoning and memory management. The authors transform popular benchmarks into multi-turn agentic formats for unified evaluation and demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines while consuming fewer tokens and requiring significantly less design effort.

## Method Summary
Lita is implemented as an LLM-powered agent that interacts with coding environments through six tools: Editor (file operations), Terminal (command execution), Search (code search), Finish (task completion), Think (self-reflection), and Plan (next-step outlining). The agent operates on transformed benchmark datasets reformatted into a unified prompt structure containing Initial State, Task Description, Output State, and Validation Steps. The system supports both linear memory (full history) and summarized memory (LLM-triggered condensation) strategies. Lita uses string replacement for file editing by default and runs with Temperature=0.0, Top_p=1.0, limited to 100 iterations per task. The evaluation measures pass@1/resolution rates, token consumption, and agent intrinsic complexity across various model capabilities.

## Key Results
- Lita achieves competitive or superior performance compared to workflow-based and agentic baselines across HumanEval, Aider Polyglot, and SWE-Bench Verified benchmarks
- The system consumes fewer tokens than complex scaffolding-based approaches while maintaining or improving success rates
- Performance gaps between simple (Lita-mini) and complex agents shrink as model capability increases, supporting the Agent Complexity Law hypothesis
- String replacement editing strategy significantly improves edit success rates for weaker models compared to diff-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling the agent from specific LLMs and tasks reveals intrinsic model capabilities by removing hidden optimizations
- Mechanism: When prompts and tools are not optimized for specific architectures, the model must rely on its own reasoning rather than benchmark-specific hints embedded in scaffolding
- Core assumption: Complex scaffolding obscures true capabilities rather than enhancing them
- Evidence anchors: Abstract states "heavy human intervention obscures a model's true underlying capabilities"; core principle listed as "Decoupling the agent from specific LLMs and Tasks"

### Mechanism 2
- Claim: Minimal tool sets shift token budget from wasted actions to explicit reasoning
- Mechanism: With fewer tools, stronger models allocate more function calls to Think and Plan operations rather than repetitive failed edits
- Core assumption: Token allocation reflects cognitive effort; more Think/Plan calls indicate genuine reasoning
- Evidence anchors: Tool call logs show Lita allocates more function calls to Think and Plan; Table 5 shows higher Think percentages than OpenHands

### Mechanism 3
- Claim: The Agent Complexity Lawâ€”performance gaps between simple and complex agents shrink as model capability increases
- Mechanism: Stronger models possess better autonomous planning, error recovery, and instruction-following, reducing reliance on external workflow guidance
- Core assumption: Model improvement trajectories will continue; convergence is not task-specific
- Evidence anchors: Abstract states "performance gap between agents of varying complexity... will shrink as the core model improves"; Figure 3 shows visual evidence of declining gaps

## Foundational Learning

- **Function calling / tool use APIs**
  - Why needed here: Lita relies on native function calling to interact with Editor, Terminal, Search, Finish tools
  - Quick check question: Can you explain how an LLM decides which tool to invoke given a user request and a tool schema?

- **Agentic vs. workflow paradigms**
  - Why needed here: The paper explicitly contrasts autonomous agents (trial-and-error, self-directed) with predefined workflows (human-designed procedures)
  - Quick check question: What is the key difference between SWE-Agent's approach and Agentless's approach to solving coding tasks?

- **Multi-turn conversation memory management**
  - Why needed here: Lita implements both linear and summarized memory strategies; understanding context window constraints and summarization tradeoffs is necessary
  - Quick check question: Why might linear memory fail on long-horizon tasks, and how does summarized memory address this?

## Architecture Onboarding

- **Component map**: Tools layer (Editor, Terminal, Search, Finish) -> Reasoning layer (Think, Plan) -> Memory layer (Linear or Summarized) -> Environment layer (transformed benchmark prompts)

- **Critical path**: 1. Receive transformed benchmark prompt 2. Model generates tool call via function calling 3. Execute tool in environment 4. Append result to memory 5. Repeat until Finish called or iteration limit reached 6. Validate solution against external test cases

- **Design tradeoffs**:
  - Diff-based vs. string-replace editing: String-replace improves edit success for weaker models (GPT-4o jumps from 16.1% to 56.5% edit success)
  - Linear vs. summarized memory: Linear reveals long-context management capability; summarized reduces token cost but may lose detail
  - Terminal-only vs. full tool set: Terminal-only works for strong models (GPT-5 achieves 89.5% on Polyglot with Lita-mini) but fails for weaker ones

- **Failure signatures**: Early-step failures on strong models; high edit failure rates indicate instruction-following weaknesses; repetitive tool calls without progress suggest planning deficits

- **First 3 experiments**:
  1. Run Lita on Aider Polyglot with GPT-4o-mini to establish baseline for weaker models; compare Lita vs. Lita-mini to validate tool necessity
  2. Test string-replace vs. diff-based editing on a medium-capability model (GPT-4.1-mini) to confirm editing strategy impact
  3. Run ablation removing Think/Plan tools (Lita-reason variant) on a strong model (Claude 3.7 Sonnet) to determine if explicit reasoning tools remain necessary at higher capability levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "Agent Complexity Law" hold for multi-repository or collaborative software engineering tasks?
- Basis in paper: [explicit] Authors state current benchmarks represent a "narrow slice" and suggest "Future benchmarks could expand to multi-repository projects, collaborative development, or longer-term maintenance tasks."
- Why unresolved: Lita was evaluated exclusively on single-repository benchmarks, leaving generalizability to complex ecosystems unproven
- What evidence would resolve it: Evaluation of Lita against complex baselines on a multi-repository or collaborative benchmark

### Open Question 2
- Question: Can the "liteness" philosophy be maintained when integrating advanced capabilities like retrieval, web search, or multi-agent collaboration?
- Basis in paper: [explicit] Authors note they "didn't include advanced features such as retrieval, web search and multi-agent... which may be necessary for scaling to more complex tasks"
- Why unresolved: The paper focuses on minimal tools; the trade-off between adding necessary complexity for advanced features and retaining "lite" benefits is untested
- What evidence would resolve it: Experiments measuring performance and token efficiency of Lita variants equipped with retrieval or multi-agent tools

### Open Question 3
- Question: Will the performance gap between minimal and complex agents eventually converge to zero, or does a permanent capability floor exist for minimal scaffolding?
- Basis in paper: [inferred] "Agent Complexity Law" predicts convergence to a "negligible difference," but Figure 3 shows shrinking yet persistent gaps even for strongest models
- Why unresolved: Current frontier models may not be capable enough to test ultimate limits; outliers exist where complex scaffolding still aids stronger models
- What evidence would resolve it: Empirical results from future, more capable models showing statistically negligible differences between Lita and workflow-based agents

## Limitations

- Model version dependency: Results rely on proprietary or unreleased model versions (GPT-5, GPT-4.1, Claude Sonnet 4) that cannot be reproduced without access to internal APIs
- Tool schema refinement ambiguity: Exact refinement process and final schema definitions are not specified in the main text
- Convergence law extrapolation: The Agent Complexity Law extends beyond evaluated model capabilities, making the claimed convergence speculative

## Confidence

**High confidence**: Lita's competitive performance against workflow-based baselines on standard benchmarks; the mechanism by which minimal scaffolding reveals true capabilities through reduced overfitting; practical advantage of reduced token consumption and design overhead

**Medium confidence**: The specific performance rankings across different model families; the magnitude of the Agent Complexity Law effect (requires extrapolation); the generalizability of results beyond tested benchmark suite

**Low confidence**: Exact numbers for proprietary model versions that cannot be reproduced; the optimal balance between linear and summarized memory strategies across all task types; whether observed convergence represents fundamental capability limits or implementation artifacts

## Next Checks

1. **Reproduce with accessible models**: Implement Lita using publicly available models (GPT-4o, Claude 3.5 Sonnet) and verify that performance advantages over workflow-based agents persist when using verifiable model versions

2. **Validate convergence law**: Test Lita-mini (terminal-only) against Lita (full tool set) across a broader range of model capabilities including both weaker and stronger models than those evaluated, to empirically measure the convergence rate predicted by the Agent Complexity Law

3. **Schema sensitivity analysis**: Systematically vary the tool schemas (particularly Think/Plan tool prompts and Editor interface) while keeping core Lita architecture constant to determine how much performance depends on these specific design choices versus the fundamental "light agent" approach