---
ver: rpa2
title: Weights-Rotated Preference Optimization for Large Language Models
arxiv_id: '2508.17637'
source_url: https://arxiv.org/abs/2508.17637
tags:
- ropo
- matrix
- wang
- preference
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking in Direct Preference Optimization
  (DPO), where language models overly suppress rejected completions to maximize rewards,
  leading to verbose generations and knowledge forgetting. The authors propose Weights-Rotated
  Preference Optimization (RoPO), which combines implicit KL divergence regularization
  on output logits with explicit orthogonal regularization on intermediate hidden
  states using a multi-granularity rotation matrix.
---

# Weights-Rotated Preference Optimization for Large Language Models

## Quick Facts
- arXiv ID: 2508.17637
- Source URL: https://arxiv.org/abs/2508.17637
- Reference count: 40
- Primary result: RoPO achieves up to 3.27-point improvement on AlpacaEval 2 and surpasses best baseline by 6.2-7.5 points on MT-Bench while using only 0.015% of trainable parameters

## Executive Summary
This paper addresses reward hacking in Direct Preference Optimization (DPO), where language models overly suppress rejected completions to maximize rewards, leading to verbose generations and knowledge forgetting. The authors propose Weights-Rotated Preference Optimization (RoPO), which combines implicit KL divergence regularization on output logits with explicit orthogonal regularization on intermediate hidden states using a multi-granularity rotation matrix. This design preserves angle-encoded knowledge while preventing excessive deviation from the reference model. Experiments show RoPO achieves up to 3.27-point improvement on AlpacaEval 2 and surpasses the best baseline by 6.2-7.5 points on MT-Bench while using only 0.015% of trainable parameters, effectively mitigating reward hacking while maintaining strong alignment performance.

## Method Summary
RoPO decomposes weight matrices into magnitude vectors and directional components, then applies multi-granularity orthogonal rotations to preserve neuron arrangement patterns. The rotation matrix combines global Householder reflections with fine-grained Givens rotations, parameterized by trainable unit vectors and angles. Applied to attention layer weights, RoPO constrains parameter space changes while maintaining KL divergence regularization. The method uses a 100x higher learning rate (1e-3 vs 2e-5) compared to LoRA baselines, with only 0.015% of parameters trainable. Experiments on UltraChat-200k (SFT) and UltraFeedback (preference optimization) show significant performance gains while mitigating reward hacking artifacts like excessive generation length.

## Key Results
- Up to 3.27-point improvement on AlpacaEval 2 over baseline methods
- 6.2-7.5 point MT-Bench advantage over best baseline while using only 0.015% trainable parameters
- Effective mitigation of reward hacking: reduced generation length and maintained knowledge diversity
- Preserved neuron arrangement patterns validated through hyperspherical energy analysis

## Why This Works (Mechanism)
RoPO addresses reward hacking by preventing parameter collapse through weight rotation while maintaining KL divergence constraints. The key insight is that traditional preference optimization methods like DPO can cause excessive deviation from reference models, leading to loss of encoded knowledge. By decomposing weights into magnitude and directional components and applying orthogonal rotations, RoPO preserves the angular relationships between neurons that encode semantic knowledge. The multi-granularity approach combines global rotations (via Householder reflections) with fine-grained adjustments (via Givens rotations), allowing controlled parameter updates that prevent the model from overly suppressing rejected completions. This maintains the knowledge learned during pre-training while improving alignment with human preferences.

## Foundational Learning
- **Householder reflections**: Orthogonal transformations that reflect vectors across hyperplanes; needed to implement global rotation component of R matrix. Quick check: Verify R = I - 2uu^T where u is unit vector.
- **Givens rotations**: Orthogonal matrices that rotate vectors in 2D subspaces; used for fine-grained rotation adjustments. Quick check: Confirm θ initialization to 0 preserves initial weights.
- **Hyperspherical energy**: Metric measuring uniformity of neuron arrangements on high-dimensional spheres; used to validate preservation of neuron patterns. Quick check: HE should remain stable during RoPO training.
- **Weight magnitude-direction decomposition**: Separating parameter norms from directional components to enable orthogonal transformations. Quick check: ||W||_c should match m during initialization.
- **KL divergence regularization**: Implicit constraint on output distribution changes; prevents excessive deviation from reference model. Quick check: Monitor KL divergence during training.
- **Orthogonal matrix multiplication**: Preserves vector lengths and angles; critical for maintaining encoded knowledge. Quick check: Verify R^T R ≈ I throughout training.

## Architecture Onboarding

Component map: SFT → Reference model → RoPO rotation → Preference optimization → Aligned model

Critical path: Weight decomposition → Orthogonal rotation application → Magnitude adaptation → KL regularization → Preference loss

Design tradeoffs: RoPO uses 100x higher learning rate (1e-3) vs LoRA (2e-5) but only 0.015% trainable parameters, trading computational efficiency for precise parameter control. The multi-granularity rotation provides both global and fine-grained adjustments but increases implementation complexity.

Failure signatures: Reward hacking persistence (excessive generation length), neuron arrangement disruption (HE increase), performance degradation (accuracy drop), gradient instability (nans or explosions in rotation parameters).

First experiments: 1) Verify weight decomposition preserves initial values (W' = W when θ=0, m=||W||_c), 2) Test rotation matrix orthogonality preservation during training (||R^T R - I||_F < tolerance), 3) Validate hyperspherical energy stability across training epochs.

## Open Questions the Paper Calls Out
None

## Limitations
- Layer selection for RoPO application remains underspecified (exact layers receiving rotation constraints unclear)
- Numerical stability of multi-granularity rotation matrix during optimization requires careful handling
- Hyperspherical energy calculation details for neuron arrangement validation not fully specified

## Confidence

High confidence: Core algorithmic innovation (weight rotation + magnitude adaptation) and theoretical motivation are sound

Medium confidence: Hyperparameter choices (learning rates, β values, initialization schemes) based on experimental evidence

Medium confidence: Reproducibility given some architectural details require inference from context

## Next Checks
1. Verify rotation matrix implementation preserves orthogonality throughout training by monitoring ||R^T R - I||_F
2. Test neuron arrangement preservation by comparing hyperspherical energy distributions before and after RoPO training
3. Conduct ablation study isolating contributions of Householder vs Givens components to performance gains