---
ver: rpa2
title: Extend Adversarial Policy Against Neural Machine Translation via Unknown Token
arxiv_id: '2501.12183'
source_url: https://arxiv.org/abs/2501.12183
tags:
- adversarial
- perturbations
- policy
- character
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the DexChar policy, which extends mainstream
  adversarial attacks against neural machine translation by incorporating dexterous
  character-level perturbations. The key innovation is using the "unknown token" (UNK)
  as an entry point to enable character-based adversarial examples while maintaining
  semantic constraints.
---

# Extend Adversarial Policy Against Neural Machine Translation via Unknown Token

## Quick Facts
- **arXiv ID:** 2501.12183
- **Source URL:** https://arxiv.org/abs/2501.12183
- **Reference count:** 29
- **Primary result:** DexChar achieves better semantic constraints (higher PA scores) and more efficient adversarial examples (higher DPE) compared to baseline methods, while retaining efficiency.

## Executive Summary
This paper introduces DexChar, a novel adversarial attack policy for neural machine translation (NMT) that leverages the "unknown token" (UNK) to enable character-level perturbations. By incorporating dexterous character-level perturbations and using noisy data augmentation for the discriminator in a reinforcement learning framework, DexChar addresses limitations of existing adversarial attacks, particularly for models with shared vocabularies. Experiments demonstrate improved semantic preservation and attack efficiency on WMT14 English-German and CWMT17 English-Chinese datasets.

## Method Summary
The DexChar method extends adversarial attacks against NMT by incorporating dexterous character-level perturbations via the unknown token (UNK). The authors use a reinforcement learning framework where the discriminator is trained with noisy data augmentation to better handle false-negative discrimination from character perturbations. This approach enables more effective adversarial examples while maintaining semantic constraints, particularly for models with shared vocabularies where traditional substitution-based attacks fail.

## Key Results
- DexChar achieves higher PA (Perturbation Accuracy) scores, indicating better semantic constraints compared to baseline methods.
- The method shows higher DPE (Discriminative Perturbation Efficiency), demonstrating more efficient adversarial examples.
- Fine-tuning with DexChar-generated adversarial examples improves robustness across different perturbation types.

## Why This Works (Mechanism)
The DexChar method works by exploiting the unknown token (UNK) as an entry point for character-level perturbations in NMT models. By incorporating dexterous character-level perturbations and addressing false-negative discrimination through noisy data augmentation for the discriminator, the method achieves more effective adversarial examples while maintaining semantic constraints. This is particularly useful for models with shared vocabularies where traditional substitution-based attacks are less effective.

## Foundational Learning

1. **Unknown Token (UNK) Usage in NMT**
   - *Why needed:* UNK tokens represent out-of-vocabulary words, providing a natural entry point for adversarial perturbations.
   - *Quick check:* Verify that the NMT model uses UNK tokens and understand how they are handled during translation.

2. **Reinforcement Learning Framework for Adversarial Attacks**
   - *Why needed:* RL allows the adversarial policy to learn effective perturbation strategies through interaction with the discriminator.
   - *Quick check:* Ensure the RL environment is properly set up with the NMT model as the environment and the discriminator as the reward function.

3. **Noisy Data Augmentation for Discriminator Training**
   - *Why needed:* Helps the discriminator better handle false-negatives from character-level perturbations, improving attack effectiveness.
   - *Quick check:* Confirm that the discriminator is trained with both clean and perturbed examples, including character-level variations.

## Architecture Onboarding

**Component Map:**
Adversarial Policy -> UNK-based Character Perturbations -> NMT Model -> Discriminator -> RL Reward

**Critical Path:**
1. Adversarial policy generates perturbations using UNK tokens
2. Perturbations are applied to source text
3. NMT model translates perturbed text
4. Discriminator evaluates semantic preservation
5. RL updates policy based on discriminator feedback

**Design Tradeoffs:**
- Character-level perturbations offer finer control but may be harder to detect semantically
- Noisy data augmentation improves discriminator robustness but increases training complexity
- UNK-based attacks are effective for shared vocabulary models but may be less impactful for others

**Failure Signatures:**
- Low PA scores indicating poor semantic preservation
- High computational cost without proportional improvement in attack effectiveness
- Discriminator unable to distinguish between benign and adversarial examples

**First Experiments to Run:**
1. Generate DexChar adversarial examples on a small subset of the dataset and manually verify semantic preservation
2. Compare PA and DPE scores of DexChar against a baseline character-level attack method
3. Fine-tune the NMT model with DexChar-generated examples and test robustness against various perturbation types

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to WMT14 English-German and CWMT17 English-Chinese datasets, raising questions about generalizability to other language pairs and domains.
- Lack of thorough ablation studies isolating the impact of noisy data augmentation and UNK-based attack vector on performance.
- Comparative analysis against state-of-the-art methods in various settings is not exhaustive.

## Confidence
- **Major claims:** Medium
- **Rationale:** While the method introduces a novel perspective on adversarial attacks using the unknown token and shows improvements in semantic constraints and efficiency, claims about superiority over existing methods and robustness of fine-tuned models are not fully validated across broader scenarios or with rigorous statistical analysis.

## Next Checks
1. Conduct ablation studies to isolate the impact of noisy data augmentation and the unknown token usage on attack performance and semantic preservation.
2. Evaluate DexChar across a wider variety of language pairs, domains, and tokenization strategies to assess generalizability.
3. Perform statistical significance testing and cross-adversarial-strategy transfer studies to rigorously validate robustness improvements and generalization claims.