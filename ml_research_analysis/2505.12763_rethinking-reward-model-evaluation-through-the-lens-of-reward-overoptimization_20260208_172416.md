---
ver: rpa2
title: Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization
arxiv_id: '2505.12763'
source_url: https://arxiv.org/abs/2505.12763
tags:
- reward
- responses
- evaluation
- overoptimization
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines reward model evaluation through the lens of
  reward overoptimization. We introduce a metric to quantify the degree of overoptimization
  and analyze how various evaluation design choices impact its estimation.
---

# Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization

## Quick Facts
- arXiv ID: 2505.12763
- Source URL: https://arxiv.org/abs/2505.12763
- Reference count: 40
- Key outcome: This work examines reward model evaluation through the lens of reward overoptimization, introducing a metric to quantify overoptimization and analyzing how evaluation design choices impact its estimation.

## Executive Summary
This paper proposes a novel framework for evaluating reward models (RMs) by quantifying their tendency to overoptimize against proxy rewards rather than true human preferences. The authors introduce the degree of overoptimization (γ) as a metric that measures the area between proxy and gold reward curves during policy optimization. Through extensive experiments across 16 evaluation designs, they demonstrate that benchmarks minimizing distributional differences between chosen and rejected responses, using multi-pairwise comparisons, and sourcing responses from diverse models yield more reliable evaluations that better capture overoptimization dynamics. However, they caution that excessively optimizing for correlation with γ can actually harm downstream performance, highlighting the need to use overoptimization as a tool rather than an end goal.

## Method Summary
The study evaluates reward models on mathematical reasoning tasks using the MATH500 dataset. The core methodology involves: (1) generating 1024 responses per problem from policy models (MetaMATH-Mistral-7B, Llama3-8B-Instruct), (2) scoring with proxy RMs to select the highest-scoring response, (3) running BoN sampling to generate optimization trajectories comparing proxy and gold rewards, (4) computing γ as the area difference between normalized proxy and gold curves, and (5) correlating γ with downstream performance (BoN n=64, PPO). The authors test 16 evaluation designs varying in response sources (human vs AI), comparison types (random vs style variations), and pairwise configurations (1:9, 3:3). PPO experiments use specific hyperparameters (LR 1e-7/2e-7, KL penalty 0.05/0.08, batch size 64).

## Key Results
- Evaluation designs minimizing distributional differences (length, style) between chosen and rejected responses show stronger correlation with overoptimization
- Multi-pairwise comparisons using responses from diverse models significantly enhance benchmark reliability
- Design O (random-3 vs random-3 3:3 acc) achieves r² > 0.85 correlation with γ, while Designs A/E show r² < 0.2
- Extremely high correlation with γ (r² > 0.9) can paradoxically result in lower downstream PPO performance
- The Skywork-o1-Open-PRM-Qwen2.5-7B serves as the gold RM, with accuracy as the oracle metric

## Why This Works (Mechanism)

### Mechanism 1: Overoptimization as a Proxy for Utility
Measuring the degree of overoptimization (γ) provides a more reliable signal of RM quality than static benchmark accuracy because it captures the dynamics of the learning signal provided to the policy. As a policy optimizes against a proxy RM, KL divergence from the initial policy increases. While the proxy reward often increases indefinitely, the true reward eventually declines. The metric γ quantifies the area between these curves, with lower γ indicating better tracking of true reward and stronger downstream performance correlation.

### Mechanism 2: Mitigating Spurious Correlations via Distribution Matching
Minimizing distributional differences between chosen and rejected responses forces RMs to learn semantic correctness rather than surface-level heuristics. Existing benchmarks often pair short human-written responses with verbose AI-generated ones, creating length bias. Normalizing these distributions removes this shortcut, revealing the model's true ability to distinguish reasoning quality from formatting artifacts.

### Mechanism 3: Robustness via Multi-Pairwise Diversity
Sourcing responses from diverse models and using multi-pairwise comparisons increases evaluation reliability by smoothing out model-specific idiosyncrasies. Single-pair comparisons are noisy and sensitive to individual model failure modes. Aggregating across multiple models forces RMs to generalize their preference signal across a wider distribution of reasoning styles and error types.

## Foundational Learning

- **Concept: Reward Overoptimization (Goodhart's Law)**
  - Why needed here: This is the central phenomenon the paper analyzes; without understanding that "when a measure becomes a target, it ceases to be a good measure," the motivation for the new metric γ is lost.
  - Quick check question: Why does proxy RM performance continue to rise during PPO while the actual gold reward eventually declines?

- **Concept: Best-of-N (BoN) vs. PPO**
  - Why needed here: The paper relies on BoN to analytically measure KL divergence and PPO to validate downstream performance.
  - Quick check question: How does the learning signal differ between selecting the best of 64 samples (BoN) and updating weights via gradient descent (PPO)?

- **Concept: KL Divergence**
  - Why needed here: It serves as the x-axis for calculating the degree of overoptimization (γ), quantifying how far the optimized policy has shifted from the initial policy.
  - Quick check question: In the context of this paper, what does a high KL divergence imply about the relationship between the proxy RM and the gold RM?

## Architecture Onboarding

- **Component map:** Proxy RM -> Gold/Oracle RM -> Policy Models -> Evaluation Designs -> γ calculation -> Downstream performance
- **Critical path:** 1) Select design configuration (e.g., Design O: Random, 3 chosen vs. 3 rejected) 2) Generate responses using Policy Model set 3) Run BoN sampling to generate optimization trajectory (Proxy Reward vs. Gold Reward vs. KL) 4) Calculate γ (area difference between normalized Proxy and Gold curves) 5) Correlate γ with downstream task performance (e.g., MATH500 accuracy)
- **Design tradeoffs:**
  - Accuracy vs. Matrix Metric: "Accuracy" captures overoptimization better, while "Matrix" offers more granular signals for PPO stability
  - Cost vs. Reliability: Calculating γ directly requires >500K inferences per RM (expensive), while proposed benchmark designs offer cheaper proxy for reliability
  - Correlation Target: Designing for perfect γ correlation may result in lower correlation with specific downstream tasks (Goodhart's Law in benchmark design)
- **Failure signatures:**
  - Length Hacking: RM scores high on Design A (Human vs Unaligned GPT) but γ is high (bad), indicating reliance on length/style rather than reasoning
  - Overfitting to γ: A design shows r² > 0.9 with γ but low correlation with PPO performance
- **First 3 experiments:**
  1. Reproduce Length Bias: Evaluate your specific RM on Design A (Human/Unaligned) vs. Design H (GPT-4o*/Random). Large score drop on H indicates reliance on length/style heuristics
  2. Calculate γ: Run BoN (n=64) with your RM as proxy and fixed Gold RM. Plot Gold Reward vs. KL Divergence curve to visualize overoptimization peak
  3. Stress Test Diversity: Compare Design K (Style variations) vs. Design L (Random models) for your RM. Performance drop on L indicates lack of robustness to distributional shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can benchmark designers navigate the trade-off where optimizing for extremely high correlation with the degree of overoptimization (γ) leads to a lower correlation with downstream performance?
- Basis in paper: [explicit] The authors state in Section 5.3 and the Conclusion that "excessively optimization results in a decline" in correlation with downstream tasks, and explicitly warn against using the degree of overoptimization as an "end goal."
- Why unresolved: The paper identifies the phenomenon (referencing Goodhart's Law) where r² > 0.9 with γ results in a drop in PPO correlation, but does not propose a specific calibration method to maximize both simultaneously.
- What evidence would resolve it: A study identifying a "sweet spot" or a composite metric that weights overoptimization correlation against downstream performance decay across various policy optimization algorithms.

### Open Question 2
- Question: Do the findings regarding response diversity and multi-pairwise comparisons generalize to significantly larger policy and reward models (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The "Limitations" section states: "Due to resource constraints, we were unable to experiment with larger reward models or policy models. Exploring a wider range of models could offer further insights into the scalability..."
- Why unresolved: The study relies primarily on 7B and 8B models; it is unconfirmed if the correlation trends observed in smaller models persist or degrade as model capacity and in-context learning abilities increase.
- What evidence would resolve it: Replicating the 16 evaluation designs using 70B or 405B parameter models as both the reward models and policy models to verify if the r² trends remain consistent.

### Open Question 3
- Question: What is the most appropriate evaluation metric for reward models that balances the need for strict ranking precision (captured by Accuracy) and stable learning signals (captured by Matrix)?
- Basis in paper: [explicit] Section 5.3 asks "Is accuracy a good metric?" and concludes that "Accuracy more effectively captures overoptimization," but implies a trade-off exists as its correlation with PPO is relatively lower than the Matrix metric in some contexts.
- Why unresolved: The paper demonstrates a divergence where Accuracy correlates better with γ (and BoN), while Matrix offers different characteristics, leaving the definition of a unified "best" metric open.
- What evidence would resolve it: The formulation of a new metric that weights pairwise comparisons dynamically, tested to show high correlation with both the degree of overoptimization (γ) and final PPO downstream scores.

## Limitations

- The proposed metric γ depends heavily on the quality of the gold RM and oracle (accuracy), which may not perfectly capture true human preferences
- The correlation between γ and downstream performance, while strong (r² > 0.85 for Design O), may not generalize to domains outside mathematical reasoning
- The BoN n=64 trajectory assumes specific optimization dynamics that might differ under alternative RL algorithms or hyperparameter settings

## Confidence

- **High:** The observation that distributional differences (length, style) between chosen/rejected responses create spurious correlations in RM evaluation
- **Medium:** The claim that multi-pairwise comparisons from diverse models improve benchmark reliability
- **Medium:** The assertion that extremely high correlation with γ doesn't guarantee better downstream performance

## Next Checks

1. Test γ correlation with downstream performance on non-mathematical domains (e.g., code generation, general knowledge QA) to assess domain generalizability
2. Evaluate the impact of different optimization algorithms (PPO, DPO) on the relationship between γ and actual policy performance
3. Investigate whether the proposed design choices (Design O: random-3 vs random-3 3:3 acc) maintain their advantages when scaled to larger, more diverse RM pools and real-world preference datasets