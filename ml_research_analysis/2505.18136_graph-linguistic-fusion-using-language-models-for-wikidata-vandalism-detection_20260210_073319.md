---
ver: rpa2
title: 'Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection'
arxiv_id: '2505.18136'
source_url: https://arxiv.org/abs/2505.18136
tags:
- wikidata
- vandalism
- content
- system
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new vandalism detection system for Wikidata,
  a large multilingual knowledge base. The main challenge is to detect vandalism in
  both structured data (knowledge triples) and unstructured text across multiple languages.
---

# Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection

## Quick Facts
- arXiv ID: 2505.18136
- Source URL: https://arxiv.org/abs/2505.18136
- Reference count: 16
- Outperforms ORES on AUC (0.924 vs 0.859) for Wikidata vandalism detection

## Executive Summary
This paper introduces Graph2Text, a novel vandalism detection system for Wikidata that converts all structured and unstructured edits into textual representations for unified processing by a multilingual language model. The system addresses the challenge of detecting vandalism across multiple languages and edit types (textual and structured) in the world's largest multilingual knowledge base. By leveraging semantic content rather than user metadata, Graph2Text achieves superior performance compared to the current production system (ORES) while reducing bias against anonymous and new editors.

## Method Summary
The Graph2Text approach converts structured Wikidata triples into natural language by mapping entity/property IDs to English labels, then prepends action prefixes (insert/remove/change) to distinguish edit types. Each content change is independently classified by a fine-tuned multilingual BERT model, with outputs mean-pooled per revision. A CatBoost classifier then combines these pooled embeddings with revision metadata to produce the final revert probability. The system uses bert-base-multilingual-cased for content processing and trains on 80% of human-created edits (Sept 2021–Sept 2023), reserving 20% for final classifier training.

## Key Results
- Graph2Text achieves AUC of 0.924 versus ORES's 0.859
- Reduces workload for patrollers with FR@99 of 22.5% versus ORES's 49.1%
- Demonstrates improved fairness with DIR_anon of 2.07 versus ORES's 5.69

## Why This Works (Mechanism)

### Mechanism 1: Graph-to-Text Unification
Converting structured Wikidata triples into textual representations enables a single language model to process all edit types uniformly. The system maps Wikidata entity/property IDs to their English labels, transforming {entity, property, value} triples into natural language sequences. Action-specific prefixes ("insert", "remove", "change") are prepended to distinguish edit types, inspired by T5's text-to-text approach. The semantic knowledge in pretrained multilingual LMs transfers effectively to vandalism detection when structured edits are presented as text.

### Mechanism 2: Content-Based Fairness Improvement
Rich content features from language models reduce reliance on user metadata, decreasing bias against anonymous and new editors. By shifting predictive power toward content semantics rather than user characteristics, the model reduces disparate treatment. The content-only baseline achieves the best fairness scores, demonstrating content features can substitute for metadata while maintaining detection capability.

### Mechanism 3: Hierarchical Two-Stage Classification
Separating content processing from final aggregation enables independent optimization while combining complementary signals. LMC processes each content change independently; outputs are mean-pooled per revision; CatBoost aggregates pooled content scores with metadata features for final prediction. This architecture allows specialized optimization at each stage while capturing both semantic content and contextual metadata.

## Foundational Learning

- **Concept**: Semantic Triples and Knowledge Graphs
  - Why needed here: Wikidata stores information as {entity, property, value} tuples that must be converted for LM processing
  - Quick check question: Given the Bulgaria vandalism example {Q219, anthem, "Despacito"}, what would the Graph2Text representation look like?

- **Concept**: Multilingual BERT Architecture
  - Why needed here: System uses bert-base-multilingual-cased (~178M params) covering ~100 languages for unified processing
  - Quick check question: What is the trade-off between a single multilingual model versus language-specific models for cross-lingual vandalism patterns?

- **Concept**: Fairness Metrics (DIR, DAUC)
  - Why needed here: System evaluates bias using Disparate Impact Ratio and AUC difference between privileged/unprivileged groups
  - Quick check question: If anonymous users have 30% positive rate vs. 10% for registered users, what is DIR and what does it signal?

## Architecture Onboarding

- **Component map**: Revision → JSON diff extraction → Graph2Text → LMC inference → Mean pooling → CatBoost → score
- **Critical path**: The pipeline processes each content change independently through LMC, aggregates via mean pooling, then combines with metadata in CatBoost for final prediction
- **Design tradeoffs**: Single multilingual model (simpler maintenance) vs. specialized models; English-only label mapping (simplicity) vs. multilingual mapping (coverage); CPU-only inference constraint ruled out larger LLMs
- **Failure signatures**: ~9% of IDs map to "unknown" (lost semantic signal); smaller performance gains on non-English content; self-reverts/edit wars filtered; mean pooling may dilute strong signals in multi-change revisions
- **First 3 experiments**:
  1. Reproduce baseline comparison on holdout set: verify AUC scores (ORES: 0.859, Graph2Text: 0.924) and confidence intervals match Table 1
  2. Ablate by content type: measure AUC separately for textual vs. non-textual edits, English vs. non-English, to locate performance gains
  3. Fairness-performance Pareto analysis: plot DIR_anon vs. AUC for MbC/CbC/Graph2Text variants to validate hybrid design choice

## Open Questions the Paper Calls Out

- **Open Question 1**: How would the utilization of alternative language model architectures impact the system's vandalism detection performance compared to the currently used BERT-base model?
- **Open Question 2**: Does mapping Wikidata IDs to labels in non-English languages during the Graph2Text conversion process enhance model performance and coverage?
- **Open Question 3**: To what extent does the inclusion of structural changes to Wikidata qualifiers and rankings improve the detection of vandalism?

## Limitations

- Approximately 9% of Wikidata IDs map to "unknown" labels, potentially losing semantic information
- English-only label mapping may create coverage gaps for non-English content with smaller performance gains observed on non-English edits
- Hierarchical two-stage architecture's mean pooling aggregation may dilute strong vandalism signals when revisions contain multiple content changes with varying risk levels

## Confidence

- **High Confidence**: The AUC comparison results (0.924 vs. 0.859) and the fairness metric improvements (DIR_anon reduction from 5.69 to 2.07) are well-supported by reported experiments and statistical significance testing
- **Medium Confidence**: The claim that content-based features alone can adequately detect vandalism is supported but may have untested edge cases
- **Low Confidence**: The long-term maintenance viability of the single multilingual model approach versus specialized models remains unproven

## Next Checks

1. **Ablation Study**: Systematically remove each component (metadata, English label mapping, Graph2Text conversion) and measure performance degradation to quantify individual contribution values
2. **Cross-Lingual Transfer Test**: Evaluate the model on a multilingual vandalism detection benchmark with parallel edits across languages to measure true cross-lingual generalization capability
3. **Edge Case Analysis**: Create a curated test set of sophisticated vandalism examples including vandalism targeting rare properties, vandalism requiring cultural context, and vandalism that historically correlated with user experience to stress-test the fairness claims