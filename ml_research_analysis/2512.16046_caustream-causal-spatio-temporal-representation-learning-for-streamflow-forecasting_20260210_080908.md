---
ver: rpa2
title: 'CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting'
arxiv_id: '2512.16046'
source_url: https://arxiv.org/abs/2512.16046
tags:
- causal
- forecasting
- streamflow
- learning
- caustream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CauSTream introduces a unified deep learning framework for causal\
  \ streamflow forecasting that jointly learns meteorological forcing dependencies\
  \ and river routing structures. The method models runoff generation and streamflow\
  \ propagation using two distinct causal graphs\u2014an instantaneous forcing DAG\
  \ and an \u2113-windowed routing DAG\u2014learned directly from observational data."
---

# CauSTream: Causal Spatio-Temporal Representation Learning for Streamflow Forecasting

## Quick Facts
- arXiv ID: 2512.16046
- Source URL: https://arxiv.org/abs/2512.16046
- Reference count: 35
- Key outcome: Jointly learns meteorological forcing and river routing causal structures for improved multi-step streamflow forecasting

## Executive Summary
CauSTream introduces a unified deep learning framework for causal streamflow forecasting that jointly learns meteorological forcing dependencies and river routing structures. The method models runoff generation and streamflow propagation using two distinct causal graphs—an instantaneous forcing DAG and an ℓ-windowed routing DAG—learned directly from observational data. Two variants are proposed: CauSTream-Shared, using a single runoff function, and CauSTream-Local, employing a hypernetwork for station-specific adaptation. Evaluated on three major U.S. river basins (Brazos, Colorado, and Upper Mississippi) across three forecast horizons, the model consistently outperforms state-of-the-art baselines, with larger gains at longer horizons (e.g., NSE improvements from 0.77 to 0.89 in the Brazos basin for long-range forecasts).

## Method Summary
CauSTream decomposes streamflow forecasting into two causal processes: runoff generation from meteorological forcings and streamflow routing across river networks. The framework learns two causal graphs—an instantaneous forcing DAG capturing dependencies among meteorological variables to generate runoff embeddings, and an ℓ-windowed routing DAG governing how runoff and upstream flows propagate downstream over time. A VAE models the forcing DAG, while an STGCN with learned adjacency matrix handles the routing. The method employs curriculum learning, starting with standard forecasting losses before gradually introducing structural penalties to ensure sparse, acyclic graphs. Two variants are proposed: Shared uses a single runoff function across all stations, while Local employs a hypernetwork for station-specific parameterization.

## Key Results
- CauSTream consistently outperforms state-of-the-art baselines across all three basins and forecast horizons
- Performance gains are most pronounced at longer horizons (e.g., 0.77→0.89 NSE improvement in Brazos basin for long-range forecasts)
- Learned causal graphs align with physical hydrology and runoff embeddings closely match VIC-simulated runoff (MCC=0.92, R²=0.96 for local variant)
- Station-specific adaptation via hypernetwork (CauSTream-Local) provides significant improvements over shared parameterization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing streamflow forecasting into separate forcing-runoff and routing processes improves generalization, especially at longer horizons.
- **Mechanism:** The architecture learns two distinct causal graphs: (1) an instantaneous forcing DAG (G_F) capturing dependencies among meteorological forcings to generate runoff embeddings, and (2) an ℓ-windowed routing DAG (G_Q) governing how runoff and upstream flows propagate downstream over time. This mirrors the VIC-CaMa-Flood physical modeling paradigm but learns structures from data.
- **Core assumption:** Runoff generation and streamflow routing are separable processes with distinct causal timescales—forcing effects are approximately instantaneous at daily resolution, while routing involves multi-day lagged dependencies.
- **Evidence anchors:**
  - [abstract] "CauSTream jointly learns (i) a runoff causal graph among meteorological forcings and (ii) a routing graph capturing dynamic dependencies across stations."
  - [Section III-B] "G_F = ⟨(F,r),E_F⟩: an instantaneous DAG that captures causal dependencies among meteorological forcings... G_Q = ⟨(r,Q),E_Q, ℓ⟩: an ℓ-windowed DAG, capturing both instantaneous and lagged causal influences"
  - [corpus] Weak direct support; HydroDiffusion (arXiv:2512.12183) addresses probabilistic streamflow but uses LSTM backbones without causal decomposition.
- **Break condition:** If meteorological forcings exhibit significant lagged effects on runoff (e.g., long groundwater residence times) that an instantaneous DAG cannot capture, the forcing representation will be misspecified.

### Mechanism 2
- **Claim:** Causal graph structures can be identified from observational hydrological data through Jacobian-based regularization constrained by physical masks.
- **Mechanism:** The framework applies two structural penalties to the Jacobians of generative functions: (1) L_sparse (L1 penalty) encouraging sparse graphs, and (2) L_DAG (acyclicity penalty) ensuring valid DAGs. A DEM-derived adjacency mask M constrains routing edges to hydrologically feasible upstream-downstream connections, reducing search space and enforcing physical consistency.
- **Core assumption:** Assumptions 1–3 hold: functional faithfulness (Jacobian support reflects true DAG), independent exogenous noise across processes, and non-Gaussian noise distributions (required for nonlinear ICA identifiability).
- **Evidence anchors:**
  - [Section IV-A] "Under Assumptions 2–3, Q_t admits an equivalent nonlinear ICA representation... The causal adjacency Jacobian is given by J_g(Q_t) = I − D'_m(S_t)(J'_m)^{-1}(S_t)"
  - [Section IV-C] "Sparsity Loss (L_sparse): an L1 penalty on the masked Jacobians... Acyclicity Loss (L_DAG): a smooth penalty to ensure the learned graphs are valid DAGs"
  - [corpus] No direct corpus evidence for Jacobian-based causal discovery in spatiotemporal forecasting; related work (TCDF, Streams) uses attention or RL-based discovery.
- **Break condition:** If noise terms are Gaussian or significantly correlated across stations (violating Assumptions 2–3), causal structures may not be identifiable, and learned graphs could be spurious.

### Mechanism 3
- **Claim:** Station-specific hypernetwork parameterization improves forecasting by capturing local hydrological heterogeneity analogous to calibration in process-based models.
- **Mechanism:** CauSTream-Local uses a hypernetwork to generate station-specific parameters for the runoff function f_r, enabling adaptation to local conditions (soil, vegetation, topography) without requiring explicit calibration data. This contrasts with CauSTream-Shared, which assumes a homogeneous runoff process.
- **Core assumption:** Local hydrological heterogeneity is systematic and learnable from station identity, rather than requiring explicit geophysical features as input.
- **Evidence anchors:**
  - [abstract] "CauSTream-Local, employing a hypernetwork for station-specific adaptation"
  - [Table IV] CauSTream-Local achieves MCC=0.92 and R²=0.96 for runoff embedding alignment with VIC-simulated runoff vs. MCC=0.77 and R²=0.67 for Shared variant
  - [corpus] Damba-ST (arXiv:2506.18939) addresses domain adaptation in spatiotemporal models but uses Mamba architecture, not hypernetworks.
- **Break condition:** If station count is very large (e.g., continental scale) or new stations lack training data, the hypernetwork may overfit or fail to generalize to unseen locations.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Directed Acyclic Graphs (DAGs)**
  - **Why needed here:** The entire framework represents hydrological processes as SCMs with two DAGs. Understanding parent-child relationships, exogenous noise, and acyclicity constraints is essential to interpret the learned structures.
  - **Quick check question:** Can you explain why a DAG cannot have cycles and what constraint L_DAG enforces mathematically?

- **Concept: Variational Autoencoders (VAEs) and the ELBO**
  - **Why needed here:** The forcing DAG is learned via a VAE that models F_t = f_F(ε^(F)_t). The ELBO objective balances reconstruction accuracy against KL divergence regularization of inferred noise distributions.
  - **Quick check question:** Why does the paper adopt a Laplace prior for p(ε^(F)_t) rather than a Gaussian prior?

- **Concept: Spatiotemporal Graph Convolutional Networks (STGCNs)**
  - **Why needed here:** The forecasting component uses an STGCN where the learned adjacency matrix A_Q guides message passing between stations. Understanding graph convolutions and temporal modules is required to modify the architecture.
  - **Quick check question:** How does the learned adjacency A_Q differ from a distance-based or connectivity-based adjacency used in standard STGCNs?

## Architecture Onboarding

- **Component map:** Forcing VAE (Encoder E_F + Decoder D_F) → Runoff embedding (r_t) → Routing DAG discovery (A_Q) → STGCN forecasting → Multi-step rollout
- **Critical path:** Forcing VAE → Runoff embedding (r_t) → Routing DAG discovery (A_Q) → STGCN forecasting → Multi-step rollout. Loss flows backward through all components with curriculum annealing of structural penalties.
- **Design tradeoffs:**
  - **Shared vs. Local:** Shared is computationally cheaper and works for homogeneous basins; Local captures heterogeneity but requires more parameters and data per station.
  - **Lag window (L):** Larger L captures longer travel times (up to 9 days in Brazos) but increases graph complexity; paper uses L=1 for efficiency.
  - **Curriculum learning:** Training forecast/ELBO losses first before annealing structural penalties stabilizes learning but requires careful scheduling.
- **Failure signatures:**
  - **Dense/uninterpretable learned graphs:** L_sparse weight too low or mask M not applied correctly.
  - **Cycles in routing graph:** L_DAG weight insufficient or optimization not converged.
  - **Poor long-horizon performance:** Runoff embedding dimension (d_r=2) may be too small, or autoregressive error accumulation.
  - **Misalignment with VIC runoff:** Hypernetwork not learning station-specific dynamics; check if station embeddings are effectively used.
- **First 3 experiments:**
  1. **Reproduce Brazos basin short-range results** with CauSTream-Shared; verify NSE ≈ 0.87 against Table I. Check that learned forcing DAG shows expected precipitation → runoff relationships.
  2. **Ablate causal learning losses** (set λ_sparse = λ_DAG = 0); confirm performance drops to CSF baseline levels per Table III ("CauSTream (Local) - CL").
  3. **Visualize learned routing graph A_Q** against ground-truth river network (Fig. 5); verify upstream-downstream edges align with DEM-derived mask and that long-distance connections may be missed due to L=1 lag constraint.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CauSTream framework be extended to maintain causal identifiability when latent confounding variables are present?
- Basis in paper: [explicit] The authors identify the inability to account for latent variables as a "strong" assumption and a key limitation.
- Why unresolved: The current theoretical guarantees (Corollaries 1 and 2) rely on the assumption of independent exogenous noise, which excludes unobserved confounders.
- What evidence would resolve it: A theoretical extension of the identifiability proofs and empirical validation showing robust performance on datasets with simulated latent confounders.

### Open Question 2
- Question: How does the computational efficiency and forecasting accuracy of CauSTream scale when applied to continental-sized domains?
- Basis in paper: [explicit] The paper lists enhancing scalability to continental domains (e.g., the contiguous United States) as an "immediate direction" for future work.
- Why unresolved: The current evaluation is restricted to basin-level applications with relatively few stations (8 to 73), leaving the model's behavior on thousands of stations untested.
- What evidence would resolve it: Benchmarks on a continental-scale dataset, demonstrating training times and memory usage that remain feasible alongside predictive performance.

### Open Question 3
- Question: Can the learned causal graphs be utilized to perform reliable counterfactual reasoning for water resource management?
- Basis in paper: [explicit] The conclusion suggests the learned streamflow graph provides a foundation for "what-if" analyses to support decision-making.
- Why unresolved: The current study focuses solely on predictive forecasting accuracy and structural alignment; it does not implement or validate a counterfactual inference mechanism.
- What evidence would resolve it: A module enabling interventional queries (e.g., simulating the effect of altered precipitation) validated against physical model simulations.

## Limitations
- The framework assumes instantaneous meteorological forcing effects on runoff, which may not hold for all catchments with long groundwater residence times
- Causal graph identifiability relies on non-Gaussian noise and independent exogenous processes, assumptions that may not hold in all hydrological systems
- The ℓ-windowed routing DAG with L=1 may miss long-distance routing connections in larger basins with travel times exceeding one day
- Hyperparameters for structural learning penalties are not fully specified, making exact replication difficult

## Confidence

- **High confidence**: Predictive performance claims (NSE/KGE improvements), alignment with VIC-simulated runoff (MCC/R²), and overall superiority over baselines
- **Medium confidence**: Physical interpretability of learned causal graphs and the separability assumption between forcing-runoff and routing processes
- **Low confidence**: Claims about causal discovery from observational data without experimental validation of identifiability conditions

## Next Checks

1. Test CauSTream performance when forcing Jacobians violate non-Gaussianity assumption by adding correlated noise to input variables
2. Evaluate CauSTream with larger ℓ-window (L=3-5) on Upper Mississippi basin to capture longer routing times and compare learned graph structures
3. Implement ablation study where forcing DAG is fixed to physical knowledge rather than learned, measuring performance trade-offs and computational benefits