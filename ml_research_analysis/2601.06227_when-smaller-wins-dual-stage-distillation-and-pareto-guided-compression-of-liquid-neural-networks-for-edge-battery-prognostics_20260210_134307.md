---
ver: rpa2
title: 'When Smaller Wins: Dual-Stage Distillation and Pareto-Guided Compression of
  Liquid Neural Networks for Edge Battery Prognostics'
arxiv_id: '2601.06227'
source_url: https://arxiv.org/abs/2601.06227
tags:
- teacher
- students
- student
- distillation
- battery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying accurate battery
  health prognostics models on edge devices with strict memory and compute constraints.
  DLNet is a deployment-aware framework that compresses a high-capacity liquid neural
  network (LNN) teacher into compact, edge-deployable students through dual-stage
  knowledge distillation and Pareto-guided model selection.
---

# When Smaller Wins: Dual-Stage Distillation and Pareto-Guided Compression of Liquid Neural Networks for Edge Battery Prognostics

## Quick Facts
- **arXiv ID**: 2601.06227
- **Source URL**: https://arxiv.org/abs/2601.06227
- **Reference count**: 11
- **Primary result**: 0.0066 MAE for 100-cycle SoH forecasting with 94 kB model deployed on Arduino Nano 33 BLE Sense

## Executive Summary
This paper addresses the challenge of deploying accurate battery health prognostics models on edge devices with strict memory and compute constraints. DLNet is a deployment-aware framework that compresses a high-capacity liquid neural network (LNN) teacher into compact, edge-deployable students through dual-stage knowledge distillation and Pareto-guided model selection. The framework first discretizes LNN dynamics via Euler approximation for LiteRT compatibility, then distills teacher knowledge into diverse students, and applies structured pruning followed by second-stage distillation. Pareto optimization selects models balancing prediction accuracy and deployment cost. Experiments on the MIT-Stanford battery dataset show the final deployed student achieves 0.0066 MAE for 100-cycle SoH forecasting—15.4% lower than the teacher—while reducing model size from 616 kB to 94 kB (84.7% reduction) and enabling 21 ms inference on Arduino Nano 33 BLE Sense. The results demonstrate that compact models can outperform larger teachers when properly supervised and selected for edge deployment.

## Method Summary
DLNet compresses a high-capacity liquid neural network teacher into edge-deployable students through a multi-stage pipeline. First, Euler discretization reformulates continuous-time ODE dynamics into discrete solver-free tensor updates for LiteRT compatibility. The teacher then guides diverse students via first-stage knowledge distillation using time-varying λ scheduling. Selected students undergo structured pruning at multiple sparsity levels, followed by second-stage distillation to recover knowledge after compression. Pareto-guided selection balances prediction accuracy against deployment cost metrics (size, inference time, energy, CO2) to identify optimal models. The final models are quantized to int8 and exported to LiteRT for edge deployment on microcontrollers.

## Key Results
- Deployed student achieves 0.0066 MAE for 100-cycle SoH forecasting
- Model size reduced from 616 kB to 94 kB (84.7% reduction)
- Inference time: 21 ms on Arduino Nano 33 BLE Sense
- Student outperforms teacher by 15.4% in accuracy while being 6.5× smaller
- Pareto-optimal models balance accuracy (MAE, RMSE, MAPE, uncertainty, confidence) with deployment costs (size, time, energy, CO2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Euler discretization enables deployment of liquid neural network dynamics on edge devices lacking ODE solver support.
- Mechanism: The continuous-time ODE governing hidden state evolution (dh/dt = -αh + βtanh(Wh + ū)) is approximated via explicit Euler step: h(t+Δt) = h(t) + Δt · dh/dt. This replaces iterative numerical integration with a single solver-free tensor update.
- Core assumption: The Euler approximation sufficiently preserves the learned temporal dynamics for the prediction task.
- Evidence anchors:
  - [abstract] "DLNet first applies Euler discretization to reformulate liquid dynamics for embedded compatibility."
  - [section 2.2] "LiteRT does not support general ODE solvers or iterative numerical integration operators... Explicit Euler-based discretization reformulates the hidden-state evolution into a single solver-free tensor update."
  - [corpus] No direct corpus validation of Euler discretization for LNNs; related EntroLnn paper uses LNNs but does not address discretization.
- Break condition: If forecast horizon or Δt is too large relative to dynamics timescale, discretization error accumulates and degrades predictions.

### Mechanism 2
- Claim: Dual-stage distillation enables knowledge recovery after aggressive compression.
- Mechanism: Stage 1 transfers teacher knowledge to diverse Euler-based students using joint loss L_total = λL_true + (1-λ)L_distill with time-varying λ. Stage 2 applies structured pruning, then re-distills to recover shifted predictions using the same teacher guidance.
- Core assumption: The frozen teacher remains a stable knowledge target; pruning-induced errors are recoverable via re-distillation.
- Evidence anchors:
  - [abstract] "performs dual-stage knowledge distillation to transfer the teacher model's temporal behavior and recover it after further compression."
  - [section 2.3] "After pruning, student models may have shifted prediction behaviors... we introduce another round of distillation to strengthen students by recovering and stabilizing their useful knowledge."
  - [corpus] Corpus papers on knowledge distillation (FastWhisper, AfroXLMR-Comet) support single-stage distillation effectiveness but lack evidence for dual-stage recovery mechanisms.
- Break condition: If pruning sparsity exceeds model capacity to represent the teacher's function, second-stage distillation cannot recover performance.

### Mechanism 3
- Claim: Pareto-guided selection identifies models that balance prediction accuracy and deployment cost better than single-metric optimization.
- Mechanism: Students are evaluated in bi-objective space (error f_err, cost f_cst). Pareto-optimal models are retained where no other student dominates on both objectives. Error aggregates MAE, RMSE, MAPE, uncertainty, confidence; cost aggregates size, inference time, energy, CO2.
- Core assumption: Deployment constraints are multi-dimensional and cannot be reduced to a single metric without losing practical relevance.
- Evidence anchors:
  - [abstract] "Pareto-guided selection under joint error-cost objectives retains student models that balance accuracy and efficiency."
  - [section 2.2] Eq. 7-9 define the multi-objective formulation and Pareto dominance criterion.
  - [corpus] Activation-Informed Pareto-Guided Low-Rank Compression paper uses Pareto optimization for LLM compression, supporting the general approach.
- Break condition: If threshold constraints (f_max_err, f_max_cst) are too loose or too tight, the Pareto front may include impractical models or exclude all viable candidates.

## Foundational Learning

- Concept: **Knowledge Distillation**
  - Why needed here: Understanding how soft targets from a teacher guide student learning, and why λ scheduling shifts from teacher-guided to ground-truth learning.
  - Quick check question: Can you explain why distillation loss (matching teacher outputs) might help more than ground-truth loss early in training?

- Concept: **Continuous-Time Neural Networks / Neural ODEs**
  - Why needed here: The teacher LNN uses learned ODE dynamics; understanding how hidden states evolve continuously versus discretely is essential for grasping the Euler approximation.
  - Quick check question: What is the difference between solving an ODE with numerical integration versus Euler discretization?

- Concept: **Pareto Optimality**
  - Why needed here: Model selection uses multi-objective optimization; understanding non-dominated solutions is critical for interpreting why certain students are selected.
  - Quick check question: Given two models where A has lower error but higher cost than B, which is Pareto-optimal?

## Architecture Onboarding

- Component map:
  - Teacher LNN: Encoder (linear→LayerNorm→ReLU→dropout→linear→LayerNorm→Tanh) → Dynamics block (ODE integration) → Decoder (linear→LayerNorm→ReLU→dropout→linear→LayerNorm→ReLU→linear)
  - Euler Student Generator: Replaces ODE integration with explicit Euler step; low-rank dynamics matrix W' = diag(w_diag) + (1/r)UV^T
  - Distillation Pipeline: Stage 1 (teacher→students) → Pareto selection → Pruning → Stage 2 (re-distillation) → Final Pareto selection
  - Edge Export: int8 quantization → LiteRT conversion → C header file generation

- Critical path:
  1. Train high-capacity LNN teacher (accuracy-first, frozen after training)
  2. Generate diverse Euler students with varying hidden dimensions (2–128)
  3. First-stage distillation with MSE or Cosine loss, λ from 0.1→0.9
  4. Pareto selection (filter by thresholds, retain non-dominated)
  5. Prune selected students at multiple sparsity levels (0.1–0.9)
  6. Second-stage distillation to recover from pruning
  7. Final Pareto selection → int8 quantization → edge deployment

- Design tradeoffs:
  - Hidden dimension vs. model capacity: dimensions <8 show sharp error increases
  - Sparsity vs. recoverability: sparsity >0.5 often yields high uncertainty or instability
  - Distillation loss choice: MSE vs. Cosine produces different Pareto fronts; both are needed for diversity
  - λ scheduling: early teacher guidance stabilizes learning; late ground-truth focus enables specialization

- Failure signatures:
  - Model compilation errors or NaN outputs at high sparsity (>0.8) → prune capacity exceeded
  - MAE >0.01 after int8 quantization → quantization-aware training may be needed
  - Student pool collapses to single point in error-cost space → insufficient diversity in hidden dimensions or loss types
  - Inference crashes on microcontroller → tensor arena too small or unsupported operators remain

- First 3 experiments:
  1. **Baseline teacher validation**: Train LNN teacher, export to LiteRT (float16, int8), confirm MAE <0.01 on test set to verify teacher eligibility.
  2. **First-stage distillation sweep**: Generate students with hidden dimensions {2, 4, 8, 16, 32, 64, 128}, train with both MSE and Cosine distillation loss, plot error-cost scatter to identify Pareto front.
  3. **Pruning sensitivity test**: Take one Pareto-optimal student (e.g., M-4 or C-16), apply sparsity levels {0.1, 0.3, 0.5, 0.7, 0.9}, re-distill, and measure MAE/uncertainty vs. model size to identify stable sparsity range.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DLNet framework generalize to non-battery industrial analytics tasks that face similar embedded constraints?
- Basis in paper: [explicit] The conclusion states that "Beyond battery health prognostics, the framework can be customized and extended to other battery tasks... and also to non-battery applications."
- Why unresolved: The experimental validation is restricted to the MIT-Stanford battery dataset and SoH forecasting, leaving the framework's efficacy on other time-series domains unproven.
- What evidence would resolve it: Application of DLNet to diverse industrial datasets (e.g., vibration analysis for predictive maintenance) with corresponding edge deployment results.

### Open Question 2
- Question: Does the use of explicit Euler discretization limit the model's ability to capture highly non-linear battery degradation compared to the teacher's ODE solver?
- Basis in paper: [inferred] The methodology replaces the teacher's ODE solver with explicit Euler discretization to ensure LiteRT compatibility, a necessary engineering trade-off noted in Section 2.2.
- Why unresolved: Euler methods are first-order approximations; the paper does not quantify potential accuracy losses on chaotic or stiff degradation dynamics that higher-order solvers might handle better.
- What evidence would resolve it: A comparative analysis of student performance on battery cells exhibiting rapid phase transitions or highly non-linear aging trajectories.

### Open Question 3
- Question: To what extent is the "smaller wins" phenomenon (student outperforming teacher) dependent on the specific noise characteristics of the training data?
- Basis in paper: [explicit] The conclusion hypothesizes that the student outperforms the teacher because "compression alleviates noise sensitivity," but this mechanism is not isolated in the experiments.
- Why unresolved: While the result is demonstrated, the specific cause (noise filtering vs. regularization vs. overfitting reduction) remains an observational inference rather than a proven mechanism.
- What evidence would resolve it: Ablation studies using synthetic datasets with controlled noise levels to correlate model compression ratios with noise robustness.

### Open Question 4
- Question: How sensitive is the Pareto-optimal model selection to the specific heuristic weights assigned to the error and cost objectives?
- Basis in paper: [inferred] Section 3.1 defines specific weights (e.g., 50% to MAE/RMSE/MAPE) for the objective functions, but does not analyze how sensitive the final model selection is to these arbitrary choices.
- Why unresolved: Small changes in objective weights could shift the Pareto frontier, potentially selecting different "optimal" students, yet this robustness is not discussed.
- What evidence would resolve it: Sensitivity analysis varying the weights ($w_{err}, w_{cst}$) to determine if the selected student models remain stable across different weighting configurations.

## Limitations
- Critical hyperparameters including teacher hidden dimension, low-rank size r, dropout rates, and optimizer settings are unspecified, preventing faithful reproduction
- Pareto optimization weighting scheme for combining multiple error and cost components is described but not fully specified
- Structured pruning methodology lacks implementation details, making it unclear whether temporal dynamics are preserved

## Confidence
- **High Confidence**: The core claim that Euler discretization enables edge deployment of LNN dynamics is well-supported by explicit formulation (Eqs. 3-5) and the successful deployment on Arduino Nano 33 BLE Sense with measurable inference latency (21 ms).
- **Medium Confidence**: The dual-stage distillation mechanism is logically sound and supported by the framework description, but the effectiveness of knowledge recovery after aggressive pruning remains theoretical without ablation studies showing stage 2's specific contribution.
- **Low Confidence**: The Pareto-guided selection's practical superiority over simpler single-metric optimization is asserted but not empirically validated against alternative selection strategies, and the multi-component weighting scheme lacks implementation details.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary teacher hidden dimension and low-rank size r to determine their impact on student pool diversity and final Pareto-optimal performance.
2. **Stage 2 Distillation Ablation**: Compare final deployed models with and without second-stage distillation to quantify the knowledge recovery contribution from pruning-induced prediction shifts.
3. **Pareto vs. Single-Metric Comparison**: Implement alternative selection strategies (e.g., accuracy-only, size-only, weighted sum) and compare against Pareto selection to validate the multi-objective approach's practical benefits.