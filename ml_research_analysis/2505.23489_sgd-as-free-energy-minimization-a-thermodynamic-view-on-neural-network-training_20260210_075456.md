---
ver: rpa2
title: 'SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training'
arxiv_id: '2505.23489'
source_url: https://arxiv.org/abs/2505.23489
tags:
- loss
- training
- learning
- gradient
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a thermodynamic perspective on neural network
  training, framing stochastic gradient descent (SGD) with fixed learning rates as
  minimizing a free energy function \( F = U - TS \), where \( U \) is training loss,
  \( S \) is the entropy of the weight distribution, and \( T \) is an effective temperature
  controlled by the learning rate. This view explains why high learning rates prevent
  convergence to the loss minimum and why different learning rates yield different
  final loss levels.
---

# SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training

## Quick Facts
- arXiv ID: 2505.23489
- Source URL: https://arxiv.org/abs/2505.23489
- Reference count: 40
- One-line primary result: This paper frames SGD as minimizing a free energy function, explaining learning rate effects through thermodynamic temperature and revealing key differences between underparameterized and overparameterized regimes.

## Executive Summary
This paper introduces a novel thermodynamic perspective on neural network training, casting stochastic gradient descent with fixed learning rates as a process of minimizing free energy F = U - TS, where U is training loss, S is weight distribution entropy, and T is an effective temperature controlled by the learning rate. This framework elegantly explains why high learning rates prevent convergence to the loss minimum and why different learning rates yield different final loss levels. The authors validate their approach empirically on both underparameterized and overparameterized models, discovering that the key distinction lies in the signal-to-noise ratio of stochastic gradients near optima, which manifests differently in each regime.

## Method Summary
The authors develop a thermodynamic framework by reinterpreting SGD as minimizing a free energy function that combines training loss (U) and entropy of the weight distribution (S) weighted by an effective temperature (T) controlled by the learning rate. They empirically validate this framework by comparing underparameterized (UP) and overparameterized (OP) models, measuring how temperature varies with learning rate in each regime. The experiments include toy examples and neural network training to demonstrate how the signal-to-noise ratio (SNR) of stochastic gradients near optima determines whether temperature remains finite (UP) or drops to zero (OP) at low learning rates, enabling convergence to the optimum.

## Key Results
- High learning rates prevent convergence to the loss minimum due to increased temperature, while different learning rates yield different final loss levels
- In underparameterized models, temperature increases smoothly with learning rate, while in overparameterized models, temperature drops to zero at low learning rates
- The key difference between regimes is attributed to variations in the signal-to-noise ratio (SNR) of stochastic gradients near optima
- The framework provides a unified thermodynamic explanation for observed optimization dynamics across different learning rates

## Why This Works (Mechanism)
The thermodynamic perspective works because it captures the inherent trade-off in SGD between minimizing training loss and maintaining exploration through weight distribution entropy. The effective temperature parameterizes this trade-off, with higher temperatures (learning rates) favoring exploration over exploitation, preventing convergence to sharp minima. The signal-to-noise ratio of stochastic gradients near optima determines whether the system can reach low-temperature states (OP models) or remains at elevated temperatures (UP models), explaining the fundamental difference in convergence behavior between the two regimes.

## Foundational Learning

1. **Free Energy (F = U - TS)**
   - Why needed: Provides the mathematical foundation for connecting thermodynamics to optimization
   - Quick check: Verify that minimizing F balances loss reduction against entropy preservation

2. **Effective Temperature in Optimization**
   - Why needed: Maps learning rate to a physical quantity that controls exploration-exploitation trade-off
   - Quick check: Confirm that increasing learning rate monotonically increases effective temperature

3. **Signal-to-Noise Ratio (SNR) of Stochastic Gradients**
   - Why needed: Explains the fundamental difference in convergence behavior between under/overparameterized regimes
   - Quick check: Measure SNR near optima and correlate with temperature behavior

## Architecture Onboarding

**Component Map:** SGD Optimization -> Free Energy Minimization -> Temperature Control -> Convergence Analysis

**Critical Path:** Learning Rate → Effective Temperature → Free Energy Minimization → Final Loss/Convergence

**Design Tradeoffs:** Higher learning rates increase exploration but prevent convergence; lower rates enable convergence but may get stuck in suboptimal minima

**Failure Signatures:** High learning rates yielding poor final loss; premature convergence to suboptimal solutions

**3 First Experiments:**
1. Plot temperature vs learning rate for a simple underparameterized linear model
2. Compare SNR of stochastic gradients near optima for under/overparameterized models
3. Measure final loss as a function of learning rate for both regimes

## Open Questions the Paper Calls Out
None

## Limitations
- The framework primarily focuses on fixed learning rate SGD and may not generalize to adaptive learning rate methods or momentum-based optimizers
- Empirical validation relies on specific architectures and datasets without exploring the full breadth of model families and training conditions
- The mathematical analysis is limited to specific forms of free energy and may not capture all complexities of non-convex optimization landscapes

## Confidence

**Major Claims Confidence Assessment:**
- **High confidence**: The basic thermodynamic formulation (F = U - TS) and its mathematical consistency
- **Medium confidence**: The explanation of learning rate effects on convergence through temperature interpretation
- **Medium confidence**: The SNR-based explanation for differences between UP and OP models
- **Low confidence**: The generalizability of the framework to complex real-world training scenarios and modern architectures

## Next Checks

1. Test the framework's predictions across diverse architectures (CNNs, Transformers, ResNets) and datasets to assess generalizability

2. Extend the analysis to adaptive optimizers (Adam, RMSprop) and momentum-based methods to evaluate framework robustness

3. Conduct controlled experiments varying SNR characteristics in different loss landscapes to isolate its causal effect on temperature behavior and convergence properties