---
ver: rpa2
title: Generalization of RLVR Using Causal Reasoning as a Testbed
arxiv_id: '2512.20760'
source_url: https://arxiv.org/abs/2512.20760
tags:
- init
- rlvr
- reasoning
- n10v2
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies RLVR\u2019s generalization behavior on causal\
  \ inference over graphical models. The authors construct a synthetic dataset varying\
  \ query type (association/intervention/counterfactual) and structural complexity,\
  \ then fine-tune Qwen-2.5-Instruct models (3B-32B) using RLVR or SFT."
---

# Generalization of RLVR Using Causal Reasoning as a Testbed

## Quick Facts
- arXiv ID: 2512.20760
- Source URL: https://arxiv.org/abs/2512.20760
- Reference count: 40
- Key outcome: RLVR improves causal reasoning generalization but only for model sizes ≥7B and specific query types; 3B models fail to learn reasoning strategies.

## Executive Summary
This paper studies RLVR's generalization behavior on causal inference over graphical models, constructing a synthetic dataset varying query type (association/intervention/counterfactual) and structural complexity. The authors fine-tune Qwen-2.5-Instruct models (3B-32B) using RLVR or SFT, finding RLVR outperforms SFT on within-level and across-level generalization for larger models, but only when the base model has sufficient initial reasoning competence. The study reveals that RLVR improves specific reasoning subskills like marginalization strategy (toward incremental marginalization) and reduces derivation and calculation errors, with benefits emerging only when the base model has sufficient initial reasoning competence. Counterfactual queries remain particularly difficult, with even 32B models achieving near-chance performance.

## Method Summary
The authors construct the RLCausal dataset with 10-node binary DAGs and three query levels: association, intervention, and counterfactual. They implement a variable elimination solver for ground truth computation and fine-tune Qwen-2.5-Instruct models (3B-32B) using either RLVR (GRPO/DAPO) with a 0.8 accuracy + 0.2 format reward, or SFT for direct prediction. Training uses 32 rollouts per example with batch size 8 and learning rate 1e-6. Evaluation measures accuracy using TVD<0.01 threshold plus LLM-judge annotations for strategy and error analysis.

## Key Results
- RLVR outperforms SFT on within-level generalization for models ≥7B, with larger models showing better performance
- RLVR improves across-level generalization for 7B and 32B models but not for 3B models
- 3B models fail to learn reasoning strategies and regress to direct prediction after RLVR
- RLVR shifts marginalization strategy from brute-force summation to incremental marginalization
- Counterfactual queries remain near-chance performance even for 32B models

## Why This Works (Mechanism)

### Mechanism 1: Initial Reasoning Competence Threshold
RLVR effectiveness is gated by the base model's pre-existing reasoning success rate; below this threshold, models regress to direct prediction rather than improving. When initial marginalization attempts have high failure rates, reward sparsity prevents learning of improved strategies. Models instead learn to bypass reasoning entirely, predicting answers directly without computation chains.

### Mechanism 2: Strategy Refinement via Error-Driven Selection
RLVR preferentially reinforces incremental marginalization strategies over brute-force summation by penalizing the higher error rates of the latter. Brute-force strategies produce long summation formulas with many terms, creating more opportunities for copy and arithmetic errors. Incremental strategies decompose marginalization into smaller steps with fewer terms per operation.

### Mechanism 3: Cross-Level Generalization via Shared Subskill Composition
RLVR trained on one query level generalizes to other levels when they share common subskills (graph modification, marginalization, arithmetic), but fails when required subskills are absent from training distribution. Association, intervention, and counterfactual queries decompose into shared primitives: graph interpretation, conditional probability lookup, summation/substitution, and graph modification.

## Foundational Learning

- **Variable Elimination in Graphical Models**: The core computation for all three query levels is marginalizing over irrelevant variables; understanding elimination order and computational complexity is essential for interpreting the "incremental vs brute-force" strategy distinction.
  - Quick check: Given a 5-node DAG with edges A→B, A→C, B→D, C→D, D→E, what is the optimal elimination order for computing P(E|A=1)?

- **Pearl's Causal Hierarchy (Association/Intervention/Counterfactual)**: The paper's generalization axes are defined by query level; understanding the structural differences (observation vs do-intervention vs counterfactual) is prerequisite for interpreting cross-level results and counterfactual failure modes.
  - Quick check: In a causal graph X→Y→Z, explain why P(Z|do(X=1)) ≠ P(Z|X=1) in general, and what additional information is needed to compute P(Z_X=1 | X=0, Y=1).

- **RL Policy Gradient with Verifiable Rewards**: RLVR uses GRPO/DAPO variants; understanding how sparse binary rewards (correctness + format) shape policy updates is necessary for diagnosing why 3B models fail to improve.
  - Quick check: If only 3 of 32 rollouts per example are correct (accuracy ~9%), what happens to gradient variance and how might this affect learning stability?

## Architecture Onboarding

- **Component map**: Graph Sampler (DAG topology, 10 nodes) -> Mechanism Sampler (CPTs) -> Query Sampler (level-specific) -> Solver (variable elimination for ground truth) -> Base model (Qwen2.5-Instruct) -> RLVR (GRPO/DAPO) or SFT -> Checkpoint selection via dev loss/accuracy -> Evaluation (Correctness metric + LLM-judge annotations) -> Analysis (Manual trace review + o4-mini automated annotation)

- **Critical path**: Understand the three query levels and their graph modification semantics (Figure 2) -> Implement or verify the solver module (variable elimination with graph surgery) -> Reproduce baseline accuracy for each (model size, query level) combination -> Run RLVR training and compare against SFT on within-level and across-level splits -> Perform trace analysis to confirm mechanism hypotheses

- **Design tradeoffs**: Exact-match vs relaxed threshold (TV<0.01 is strict); Reasoning vs direct prediction (RLVR produces interpretable traces but fails for low-competence models); Graph size (10 nodes) balances complexity with tractability

- **Failure signatures**: 3B regression to direct prediction (accuracy plateaus early; traces show "no marginalization" strategy dominates); Counterfactual near-chance performance (even 32B models achieve <20% accuracy); Brute-force high error rate (traces with large summation formulas show elevated derivation and copy errors); Reward hacking potential (format reward could incentivize syntactically correct but logically invalid traces)

- **First 3 experiments**: 
  1. Baseline competence audit: Evaluate base Qwen2.5-3B/7B/32B-Instruct zero-shot on all three query levels with reasoning prompt; record accuracy and manually inspect 20 traces per level for strategy type and error patterns
  2. Ablate training query level: Train RLVR on intervention only, evaluate on association and counterfactual; compare against training on all levels
  3. Error budget analysis: For 7B RLVR model, annotate 100 incorrect traces with error type (derivation/copy/arithmetic) and step count; correlate with query complexity (|V_rel|)

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the minimum threshold of initial reasoning competence required for RLVR to be effective, and can this threshold be quantified across different domains? The paper demonstrates the phenomenon but does not identify the precise success rate threshold or whether it is domain-specific.

- **Open Question 2**: Can explicit instruction or curriculum design induce LLMs to learn twin-network construction for counterfactual reasoning? The authors report that models never attempted to build twin-networks or perform inference over exogenous variables, and providing hints did not significantly improve RLVR's performance on counterfactual level.

- **Open Question 3**: Does RLVR improvements on formal causal reasoning transfer to natural language causal reasoning tasks? The paper discusses bridging this gap but evaluates only synthetic formal tasks, leaving transfer to benchmarks like CLadder's natural language scenarios untested.

## Limitations

- RLVR effectiveness depends critically on the base model's initial reasoning competence, with 3B models failing to improve and potentially regressing to direct prediction
- Counterfactual reasoning remains near-chance performance even for 32B models, suggesting current RLVR formulation cannot induce twin-network reasoning without explicit architectural support
- The shift toward incremental marginalization strategy may be more about avoiding error-prone brute-force approaches than genuinely learning better reasoning

## Confidence

- **High**: RLVR improves within-level generalization for models ≥7B when initial competence is sufficient; strategy shift from brute-force to incremental marginalization occurs post-RLVR
- **Medium**: RLVR improves across-level generalization via shared subskill composition; 3B models regress to direct prediction due to insufficient initial reasoning success rate
- **Low**: RLVR cannot improve counterfactual reasoning because models lack twin-network construction ability; error rate per term is constant across strategies

## Next Checks

1. Systematically vary initial model competence by fine-tuning base models with SFT on reasoning traces, then evaluate RLVR effectiveness at different competence levels to empirically determine the competence threshold
2. Measure actual derivation and arithmetic error rates in brute-force vs incremental traces (with and without external calculators) to validate the error opportunity hypothesis
3. Implement twin-network construction in the reasoning prompt and evaluate whether RLVR can then learn counterfactual reasoning, or whether architectural scaffolding is required