---
ver: rpa2
title: ACE and Diverse Generalization via Selective Disagreement
arxiv_id: '2509.07955'
source_url: https://arxiv.org/abs/2509.07955
tags:
- loss
- rate
- distribution
- learning
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning models that generalize
  under complete spurious correlations, where ground-truth and spurious features are
  perfectly correlated in training data but diverge in target distributions. The authors
  propose ACE (Algorithm for Concept Extrapolation), which trains an ensemble of classifiers
  to confidently disagree on instances where concepts are most likely to differ.
---

# ACE and Diverse Generalization via Selective Disagreement

## Quick Facts
- **arXiv ID**: 2509.07955
- **Source URL**: https://arxiv.org/abs/2509.07955
- **Reference count**: 40
- **Primary result**: ACE outperforms existing methods (DivDis and D-BAT) on complete-spurious correlation benchmarks, achieving higher accuracy while being more configurable

## Executive Summary
This paper addresses the problem of learning models that generalize under complete spurious correlations, where ground-truth and spurious features are perfectly correlated in training data but diverge in target distributions. The authors propose ACE (Algorithm for Concept Extrapolation), which trains an ensemble of classifiers to confidently disagree on instances where concepts are most likely to differ. The method uses a self-training approach with a top-K pseudo-label loss that encourages selective, confident disagreement among ensemble members. ACE outperforms existing methods (DivDis and D-BAT) on complete-spurious correlation benchmarks across image and language datasets, achieving higher accuracy while being more configurable. The method also demonstrates competitive performance on measurement tampering detection without requiring untrusted measurements. A key advantage is ACE's principled unsupervised model selection via validation loss, avoiding reliance on oracle target data.

## Method Summary
ACE trains an ensemble of F classifiers on source data with ground-truth labels, then applies a selective disagreement loss on target data. The algorithm computes disagreement group probabilities for each instance, selects the top-k instances most likely to belong to each disagreement group, and applies pseudo-label loss to push ensemble members toward confident but opposing predictions on these instances. This bootstraps initial diversity (from random initialization) into semantically distinct concepts. The method uses a mix rate schedule that ramps the lower bound on group probabilities from 0 to a configured value over training. ACE is a proper scoring rule when mix rate lower bounds hold, unlike DivDis and D-BAT which are only proper at specific mix rates.

## Key Results
- ACE achieves 99.1% target accuracy on CelebA-Gender when source correlation is 0.8, outperforming DivDis (89.7%) and D-BAT (93.5%)
- ACE attains 93.4% accuracy on measurement tampering detection without requiring untrusted measurements, comparable to methods needing them
- Validation loss provides signal for mix rate inference; selecting highest lower bound within 20th percentile of validation loss density works reasonably across datasets

## Why This Works (Mechanism)

### Mechanism 1: Selective Disagreement via Top-K Pseudo-Labeling
ACE learns diverse generalizations by training ensemble members to confidently disagree on the subset of target instances where they already disagree most. The algorithm computes disagreement group probabilities for each instance, selects the top-k instances most likely to belong to each disagreement group, and applies pseudo-label loss to push ensemble members toward confident but opposing predictions on these instances. This bootstraps initial diversity (from random initialization) into semantically distinct concepts. The method assumes some fraction of target instances belong to each disagreement group at a rate at or above a known lower bound (mix rate).

### Mechanism 2: Low Density Separation via Hardened Pseudo-Labels
Training against hardened pseudo-labels implicitly implements entropy regularization, pushing decision boundaries toward low-density regions of representation space. By maximizing confidence on selected instances (pushing predictions toward 0 or 1), the loss creates margin pressure that naturally separates concepts along sparsely populated regions, rather than intersecting high-density clusters where ambiguous or interpolated concepts would reside. This appeals to the natural abstractions hypothesis that human-interpretable concepts correspond to decision boundaries in low-density regions.

### Mechanism 3: Proper Scoring under Configurable Mix Rate Bounds
ACE's loss function is a proper scoring rule (true distribution achieves global minimum) whenever mix rate lower bounds do not exceed true mix rates, unlike DivDis and D-BAT which are proper only at specific mix rates (0.5 and 1.0 respectively). The mean validation loss across groups remains comparable across configurations until the lower bound exceeds the true mix rate, at which point loss degrades. This allows principled model selection and mix rate inference without oracle target labels.

## Foundational Learning

- **Spurious Correlations (Complete vs. Incomplete)**
  - **Why needed here**: The paper distinguishes complete correlations (perfect on source, diverge on target) from incomplete (source contains some correlation-breaking examples). ACE targets the former where generalization is fundamentally underspecified.
  - **Quick check question**: Can you explain why standard ERM fails under complete spurious correlation but may partially succeed under incomplete correlation?

- **Pseudo-Labeling and Semi-Supervised Learning**
  - **Why needed here**: ACE's target loss uses hardened pseudo-labels on selected instances; understanding confidence thresholding and entropy minimization is essential.
  - **Quick check question**: How does pseudo-labeling differ from standard supervised learning, and what failure modes does it introduce?

- **Ensemble Diversity and Proper Scoring Rules**
  - **Why needed here**: ACE trains multiple heads with disagreement incentives; knowing why ensembles typically converge and how proper scoring constrains loss design is critical.
  - **Quick check question**: Why do deep ensembles often fail to produce diverse predictions despite different initializations, and what property does a proper scoring rule guarantee?

## Architecture Onboarding

- **Component map**: Shared backbone (ResNet50 for images, BERT for text) -> F classification heads -> Source loss (NLL) + Target loss (top-k pseudo-label loss) -> Masks updated via top-k selection

- **Critical path**:
  1. Initialize ensemble heads (different random seeds)
  2. Forward pass on source batch → compute source loss for all heads
  3. Forward pass on target batch → compute disagreement group probabilities
  4. Update masks via top-k selection based on current probabilities
  5. Backward pass with combined loss, update shared backbone and heads
  6. Periodically evaluate validation loss for early stopping / hyperparameter tuning

- **Design tradeoffs**:
  - **Tighter vs. looser mix rate lower bound**: Tighter bounds improve concept separation when correct but risk excluding true disagreement instances if underestimated; looser bounds are more robust but may include noise
  - **Group-specific vs. aggregated top-k**: Separate group losses work better for balanced datasets; aggregated loss handles class imbalance but may favor majority groups
  - **Joint vs. iterative head training**: ACE trains heads jointly; D-BAT requires iterative training (less stable without strong simplicity bias)

- **Failure signatures**:
  - **Collapsed agreement**: Heads converge to identical predictions (insufficient disagreement incentive or mix rate bound too low)
  - **Spurious disagreement**: Heads disagree on random or irrelevant features while maintaining source accuracy (mix rate bound too high, overfitting to noise)
  - **Validation loss spike**: Sharp increase suggests mix rate lower bound exceeded true rate
  - **Poor target accuracy despite low validation loss**: May indicate concepts learned are not human-aligned (failure of low-density separation assumption)

- **First 3 experiments**:
  1. **Sanity check on synthetic dataset**: Run ACE on Toy Grid or FMNIST-MNIST with known mix rate (e.g., 0.5); verify both heads achieve high multi-label accuracy and validation loss correlates with test accuracy
  2. **Mix rate sensitivity sweep**: Fix ground-truth mix rate, vary configured lower bound (0.1, 0.3, 0.5, 0.7); plot accuracy degradation when bound exceeds true rate to establish calibration
  3. **Comparison on incomplete correlation**: Evaluate ACE on Waterbirds (original) with and without group labels; compare to DivDis to understand behavior when correlation is not complete

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can out-of-distribution (OOD) detection methods or model uncertainty metrics successfully replace the current ad-hoc thresholding approach to dynamically identify the mix rate lower bound?
- **Basis in paper**: The authors explicitly state in Section 6.1 that future work should address the sensitivity to the mix rate lower bound by avoiding the "somewhat ad-hoc" validation loss thresholding method, potentially using OOD detection or uncertainty metrics instead.
- **Why unresolved**: The current method for inferring the mix rate relies on a manually selected threshold (20% of the range in validation loss) determined using oracle information, which requires extensive trials and may not be robust.
- **What evidence would resolve it**: Demonstrating that an automated, principled selection method achieves comparable or superior target accuracy without relying on oracle-validated thresholds or multiple training runs.

### Open Question 2
- **Question**: Does combining ACE with domain adaptation techniques (such as penalizing the distance between source and target distributions) ensure that ensemble members disagree on semantically meaningful features rather than unrelated noise?
- **Basis in paper**: Section 6.1 notes that disagreement alone is insufficient because hypotheses may learn to disagree on unrelated features. The authors suggest future work could use domain adaptation techniques to address this limitation.
- **Why unresolved**: While ACE enforces confident disagreement, the current loss function does not explicitly constrain why the models disagree, risking the learning of non-semantic artifacts.
- **What evidence would resolve it**: Empirical results showing that adding distribution-matching penalties improves the semantic alignment of learned concepts or increases robustness on datasets with high feature noise.

### Open Question 3
- **Question**: Do the validation loss density thresholds used for model selection generalize effectively to diverse datasets and model architectures without requiring dataset-specific tuning?
- **Basis in paper**: In Section 4.3, the authors note that their selection heuristic (picking the highest lower bound within the 20th percentile of validation loss) was chosen in an ad-hoc manner and state that "future work should evaluate how well thresholds generalize to other datasets and models."
- **Why unresolved**: The threshold was derived from specific experimental conditions; its applicability to broader contexts (e.g., large language models or different visual domains) remains unverified.
- **What evidence would resolve it**: A comprehensive ablation study across varied domains showing that a fixed threshold maintains strong correlation with test performance.

## Limitations
- **Complete correlation assumption**: ACE specifically targets complete spurious correlations and may underperform on incomplete correlations where standard ERM partially succeeds
- **Mix rate sensitivity**: Performance depends critically on correctly specifying or inferring the mix rate lower bound, with incorrect bounds leading to concept collapse or spurious disagreement
- **Binary concept focus**: The method is primarily demonstrated on binary spurious correlations, with unclear extension to multi-concept hierarchies or continuous concept spaces

## Confidence

- **High**: ACE's empirical performance advantage on complete spurious correlation benchmarks, the validity of the top-k pseudo-label loss formulation, and the proper scoring property under mix rate constraints
- **Medium**: The effectiveness of low-density separation for concept disentanglement in real-world datasets, and the generalizability of the mix rate inference heuristic
- **Low**: Theoretical guarantees for ACE's behavior under incomplete correlations, and the robustness of the method to more complex concept relationships beyond binary spurious correlations

## Next Checks

1. **Incomplete Correlation Testing**: Evaluate ACE on Waterbirds with varying degrees of spurious correlation (by introducing some counter-examples) to determine the method's performance degradation as the correlation becomes incomplete, and compare against DivDis and D-BAT in these intermediate regimes
2. **Mix Rate Calibration Study**: Systematically vary the true mix rate across multiple datasets while keeping the configured lower bound fixed, then plot the relationship between validation loss, accuracy, and mix rate to empirically validate the proper scoring property and establish calibration curves for mix rate inference
3. **Concept Complexity Extension**: Test ACE on multi-concept datasets (e.g., CelebA with multiple binary attributes) where spurious correlations exist between different attribute pairs, evaluating whether the method can learn multiple disentangled concepts simultaneously or if it degrades to learning only the most dominant spurious correlation