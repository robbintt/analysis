---
ver: rpa2
title: Modeling Contextual Passage Utility for Multihop Question Answering
arxiv_id: '2512.06464'
source_url: https://arxiv.org/abs/2512.06464
tags:
- utility
- passage
- passages
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to modeling contextual passage
  utility for multihop question answering. The key insight is that a passage's usefulness
  depends on its position within a reasoning chain and its relationship to other passages,
  rather than just its standalone relevance.
---

# Modeling Contextual Passage Utility for Multihop Question Answering

## Quick Facts
- arXiv ID: 2512.06464
- Source URL: https://arxiv.org/abs/2512.06464
- Authors: Akriti Jain; Aparna Garimella
- Reference count: 5
- Primary result: Novel contextual passage utility model achieves 98.33 R@5 and 89.57 NDCG@5 on HotpotQA

## Executive Summary
This paper introduces a novel approach to modeling contextual passage utility for multihop question answering. The key insight is that a passage's usefulness depends on its position within a reasoning chain and its relationship to other passages, rather than just its standalone relevance. The authors propose a lightweight RoBERTa-based regression model that predicts utility scores from 1 to 5 for passages, capturing inter-passage dependencies. To train this model, they leverage reasoning traces from an advanced reasoning model (o1) to capture the order in which passages are used, then use GPT-4o to annotate synthetic training data with context-aware utility scores. The method is evaluated on three multihop QA datasets (HotpotQA, MuSiQue, and 2WikiMultiHopQA), where it consistently outperforms relevance-based reranking methods like BM25, Contriever, MonoT5, MDR, and LLM-based rerankers.

## Method Summary
The approach involves generating synthetic training data using reasoning traces from OpenAI's o1 model to understand passage ordering in reasoning chains. GPT-4o then annotates these passages with utility scores from 1 to 5 based on their contextual importance. A lightweight RoBERTa regression model is trained to predict these utility scores. During inference, the trained model re-ranks passages from initial retrieval (using methods like BM25 or Contriever) by assigning context-aware utility scores, improving both retrieval metrics and downstream QA performance. The model operates in a listwise manner, considering all passages simultaneously to capture inter-passage dependencies.

## Key Results
- Achieves 98.33 R@5 and 89.57 NDCG@5 on HotpotQA, compared to best baseline at 86.10 and 79.24
- Demonstrates superior downstream QA performance with Exact Match score of 87.12 on HotpotQA
- Consistently outperforms relevance-based rerankers (BM25, Contriever, MonoT5, MDR) and LLM-based rerankers across HotpotQA, MuSiQue, and 2WikiMultiHopQA datasets

## Why This Works (Mechanism)
The approach works by explicitly modeling the contextual utility of passages rather than treating them as independent relevance signals. By leveraging reasoning traces from advanced models like o1, the method captures the sequential nature of how humans approach multihop reasoning tasks. The utility scoring system (1-5) allows the model to distinguish between passages that are merely relevant and those that are strategically important for bridging the reasoning gap. This context-aware ranking addresses the "lost-in-the-middle" problem that affects listwise scoring methods, where passages in the middle of a list receive insufficient attention.

## Foundational Learning

**Multi-hop Question Answering** - Why needed: Understanding the core problem of answering questions requiring multiple reasoning steps. Quick check: Can you explain why simple retrieval fails for multihop questions?

**Passage Utility vs. Relevance** - Why needed: Distinguishing between standalone relevance and contextual importance in reasoning chains. Quick check: Can you identify when a relevant passage might have low utility?

**Listwise Ranking** - Why needed: Processing multiple passages simultaneously to capture dependencies. Quick check: Can you explain how listwise differs from pairwise ranking approaches?

**Synthetic Data Generation** - Why needed: Creating labeled training data without expensive human annotation. Quick check: Can you describe how o1 traces are used to generate training data?

**Utility Scoring (1-5 scale)** - Why needed: Quantifying the relative importance of passages in reasoning contexts. Quick check: Can you explain what distinguishes a utility score of 5 from 4?

## Architecture Onboarding

**Component Map**: Input passages -> Initial retrieval (BM25/Contriever) -> Utility prediction model (RoBERTa) -> Re-ranked passages -> Reader model

**Critical Path**: The most important sequence is: question + passage context → utility prediction → re-ranking → reader QA model. The utility model must accurately capture inter-passage dependencies for optimal performance.

**Design Tradeoffs**: The lightweight RoBERTa encoder offers speed and efficiency but may lack the comprehensive context understanding of larger decoders. The synthetic data approach avoids expensive annotation but introduces potential bias from the o1 traces and GPT-4o annotations.

**Failure Signatures**: Poor performance may manifest as: (1) over-reliance on initial retrieval quality, (2) failure to capture complex reasoning patterns beyond o1's capabilities, (3) sensitivity to synthetic data noise, (4) degradation on abstract reasoning tasks.

**First Experiments**:
1. Run the utility model on a small subset of HotpotQA to verify utility score distribution matches expectations
2. Compare utility scores against human annotations on a validation set to assess alignment
3. Evaluate the impact of different initial retrieval methods (BM25 vs. Contriever) on final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can contextual utility models generalize to questions requiring high-level inferential reasoning rather than just factual synthesis?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that current datasets are "predominantly fact-based" and the model may not generalize to tasks involving "abstract semantic properties," such as explaining why a book was provocative.
- Why unresolved: The model was trained and evaluated exclusively on fact-based benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA).
- What evidence would resolve it: Evaluation on datasets requiring qualitative or abstract multi-hop reasoning (e.g., "Why" questions with non-factual answers).

### Open Question 2
- Question: How can architectures effectively combine lightweight encoder regression with the comprehensive context understanding of large decoders?
- Basis in paper: [explicit] The authors note that "further research" is needed to leverage encoder utility regression capabilities while simultaneously benefiting from the "comprehensive contextual understanding offered by large-context decoders."
- Why unresolved: The auxiliary experiment showed decoders struggle with "lost-in-the-middle" issues in listwise scoring, while the encoder model (RoBERTa) scored passages individually without full list context.
- What evidence would resolve it: A hybrid model design that outperforms the standalone RoBERTa and LLaMA baselines on passage ranking metrics.

### Open Question 3
- Question: Is synthetic training data generated by specific LLMs (o1 and GPT-4o) generalizable across different reasoning models?
- Basis in paper: [inferred] The method relies on distilling reasoning traces from OpenAI's o1 and utility scores from GPT-4o.
- Why unresolved: The paper does not analyze if the utility model learns general reasoning dependencies or if it overfits to the specific reasoning patterns (traces) of the teacher model.
- What evidence would resolve it: Cross-model evaluation where the trained utility scorer improves performance for reader models with architectures differing from the teacher.

## Limitations
- Reliance on o1 traces introduces potential bias toward specific reasoning patterns that may not generalize to human reasoning
- Synthetic data generation depends heavily on GPT-4o annotations, which may introduce systematic biases or noise
- Performance gains primarily demonstrated on English-language datasets, with untested effectiveness for other languages

## Confidence

**High confidence**: Utility-based reranking framework and core methodology
**Medium confidence**: Synthetic data generation pipeline's robustness and generalizability across different reasoning models
**Medium confidence**: Cross-lingual applicability and generalization to abstract reasoning tasks

## Next Checks
1. Evaluate the model's performance using reasoning traces from multiple advanced reasoning models (beyond o1) to test generalizability
2. Test the approach on non-English multihop QA datasets to assess cross-lingual applicability
3. Conduct a detailed error analysis comparing the model's predictions with human annotations on a subset of the data to better understand potential systematic biases