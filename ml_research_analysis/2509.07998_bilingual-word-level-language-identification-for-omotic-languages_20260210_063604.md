---
ver: rpa2
title: Bilingual Word Level Language Identification for Omotic Languages
arxiv_id: '2509.07998'
source_url: https://arxiv.org/abs/2509.07998
tags:
- language
- languages
- text
- data
- omotic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses bilingual word-level language identification
  for Wolaita and Gofa, two closely related Omotic languages spoken in southern Ethiopia.
  The challenge lies in distinguishing between the languages due to their shared linguistic
  features and geographical proximity.
---

# Bilingual Word Level Language Identification for Omotic Languages

## Quick Facts
- arXiv ID: 2509.07998
- Source URL: https://arxiv.org/abs/2509.07998
- Reference count: 40
- BERT-base-uncased + LSTM achieved F1-score of 0.72 on 3-class word-level classification

## Executive Summary
This study tackles the challenging problem of word-level language identification for Wolaita and Gofa, two closely related Omotic languages spoken in southern Ethiopia. The languages share many linguistic features and geographical proximity, making them difficult to distinguish at the word level. The authors developed a word-level dataset annotated for Wolayta, Gofa, and their code-switched form (wal-gof) from religious, educational, and social media sources. They experimented with machine learning, deep learning, and transformer-based models. The best-performing model combined BERT-base-uncased with LSTM, achieving an F1-score of 0.72. This approach outperformed other models, including CNN-LSTM and BiLSTM variants. The work provides a foundation for further research on low-resource Ethiopian languages and offers potential applications in social media monitoring and language processing tasks.

## Method Summary
The study employs a BERT-base-uncased model to generate contextualized embeddings for each word, followed by an LSTM layer for sequential processing and classification. The input text undergoes preprocessing including lowercase conversion, removal of URLs, numbers, non-alphabetic characters, and HTML tags. Three native-speaker annotators manually label each word as wal (Wolayta), gof (Gofa), or wal-gof (shared/cognate), with majority voting resolving disagreements. The BERT embeddings (768 dimensions) are passed through a 128-unit LSTM, batch normalization, two dense layers (768 units each with ReLU activation), dropout (0.1 rate), and a final dense output layer (3 units with softmax). The model is trained using Adam optimizer with learning rate 0.001, batch size 128, and 10-30 epochs.

## Key Results
- BERT-base-uncased + LSTM achieved the highest F1-score of 0.72 on 3-class word-level classification
- CNN model achieved F1-score of 0.64, outperforming LSTM-attention (F1=0.55) and BiLSTM (F1=0.59)
- The dedicated wal-gof class for shared vocabulary improved classification precision on exclusive language classes

## Why This Works (Mechanism)

### Mechanism 1: Contextualized embeddings from pretrained models capture subtle orthographic distinctions
- Claim: BERT's token-level representations encode character patterns that help distinguish similar words across closely related languages, even without target-language pretraining
- Mechanism: BERT-base-uncased produces 768-dimensional dense vectors from subword tokenization. For similar words like "Haasayaappe" (Wolaitta) versus "Hayssafe" (Gofa), the tokenizer breaks these into character-level subword units, whose combined embeddings capture orthographic patterns that LSTM then sequences into language-discriminative signals
- Core assumption: The orthographic differences between Wolaitta and Gofa words (arising from spoken accent variations) produce consistent subword patterns that BERT can encode distinctively
- Evidence anchors:
  - [abstract]: "the combination of the BERT-based pretrained language model and LSTM approach performed better, with an F1-score of 0.72"
  - [section 6]: "The model computes the embeddings of the input text using a pre-trained model. The embeddings are a dense representation of the input text that can capture semantic and syntactic information"
  - [corpus]: Weak direct evidence—related work on cross-lingual phoneme recognition (arxiv:2508.19270) and bilingual lexicon induction (arxiv:2505.23146) suggests transfer learning benefits, but none specifically validate subword embeddings for Omotic languages
- Break condition: If shared cognates exceed ~60–70% of vocabulary and orthographic differences become sporadic rather than systematic, subword embeddings alone may fail without language-specific pretraining

### Mechanism 2: LSTM sequential processing captures morphological and suffix patterns
- Claim: LSTM's recurrent hidden states learn language-specific morphological regularities (suffixes, phonological patterns) from the sequence of word embeddings
- Mechanism: The 128-unit LSTM processes each word's BERT embedding as a single-step input. The dense layer transformations (768→768→3) learn to map embedding patterns to language labels based on morphological cues—for example, Wolaitta suffix "-aappe" versus Gofa "-afe" in the paper's examples
- Core assumption: Wolaitta and Gofa exhibit consistent, learnable morphological differences in suffixation and character sequences that survive the tokenization process
- Evidence anchors:
  - [section 3.4, Table 1]: Shows systematic orthographic differences—"Haasayaappe" vs "Hayssafe" (from his/her talk), "Xaafayida dayiis" vs "Xaafettidayssi" (while I'm writing)—attributed to accent variations
  - [section 6]: "The model is based on the LSTM architecture, which is a type of RNN that can capture long-term dependencies in sequential data"
  - [corpus]: arxiv:2211.14459 (referenced in [23]) on transformer-based word-level LID for code-mixed Kannada-English showed similar benefits from combining pretrained embeddings with LSTM
- Break condition: If word-level classification prevents capturing cross-word contextual patterns, the model may struggle with short, morphologically ambiguous tokens

### Mechanism 3: Three-way classification with shared-label category reduces boundary confusion
- Claim: The dedicated "wal-gof" class for cognates and genuinely ambiguous words prevents forced binary decisions on inherently unclear cases
- Mechanism: Rather than forcing every word into "wal" or "gof," the wal-gof label absorbs shared vocabulary (e.g., "Kaallidi," "Biittaa," "Iita," "Daro" in Table 1). This soft boundary reduces false positives where the model would incorrectly assign a language label to cognates, improving precision on the exclusive classes
- Core assumption: A substantial portion of vocabulary is genuinely shared between Wolaitta and Gofa due to geographical proximity and genetic relatedness, justifying a dedicated class
- Evidence anchors:
  - [section 4.3]: "wal-gof: is a LID term utilized when both the Wolayta and Gofa people communicate, and it indicates the presence of shared words or linguistic elements"
  - [section 3.4, Table 1]: Lists multiple words marked "Use common words for both"—e.g., "Kaallidi" (while following), "Biittaa" (country), "Iita" (bad), "Daro" (many)
  - [corpus]: Limited direct corpus evidence on multi-class LID with shared-category labels; related work focuses on binary language discrimination
- Break condition: If the wal-gof class becomes a low-confidence dumping ground (class imbalance), the model may over-predict wal-gof and underperform on the primary language classes

## Foundational Learning

- **Concept: Word-level vs. document-level language identification**
  - Why needed here: This paper operates at word level, where each token must be classified independently with minimal context—fundamentally harder than document-level LID. Understanding this constraint explains why F1 scores plateau at 0.72 despite transformer embeddings
  - Quick check question: Why would the word "Biittaa" (shared between Wolaitta and Gofa) be easier to classify correctly in a full sentence versus in isolation?

- **Concept: Code-switching and cognates in closely related languages**
  - Why needed here: The wal-gof label addresses both code-switching (alternating languages in discourse) and cognates (words inherited from a common ancestor). Distinguishing these phenomena is crucial for annotation consistency
  - Quick check question: What is the difference between a word being "code-switched" (speakers alternating languages) versus being a "cognate" (genetically shared vocabulary)?

- **Concept: Transfer learning from non-target-language pretrained models**
  - Why needed here: BERT-base-uncased has no Wolaitta or Gofa in its pretraining corpus, yet it outperformed models trained from scratch. Understanding why character-level subword patterns transfer across scripts helps explain the results
  - Quick check question: Why might a model pretrained only on English text still provide useful embeddings for Ethiopian languages written in Latin script?

## Architecture Onboarding

- **Component map:**
  Input word → Text preprocessing (lowercase, remove non-alphabetic) → BERT-base-uncased tokenizer (subword tokenization) → BERT-base-uncased encoder (768-dim contextual embeddings) → LSTM layer (128 units, processes embedding sequence) → Batch normalization → Dense layer (768 units, ReLU activation) → Dense layer (768 units) → Dropout (rate=0.1) → Dense output (3 units, softmax) → Predicted label: {wal, gof, wal-gof}

- **Critical path:**
  1. **Data preprocessing**: Remove URLs, numbers, HTML tags, non-alphabetic characters; convert to lowercase
  2. **Annotation pipeline**: Three native-speaker annotators per language; majority vote resolves disagreements
  3. **Embedding extraction**: BERT produces one 768-dim vector per subword token; for single-word inputs, pool or use [CLS] representation
  4. **Sequential classification**: LSTM processes the embedding; dense layers project to 3-class output
  5. **Training configuration**: Adam optimizer (lr=0.001), batch size 128, 10–30 epochs, LeakyReLU activations

- **Design tradeoffs:**
  - **BERT-base-uncased vs. RoBERTa-base**: BERT achieved 0.72 F1 versus RoBERTa's 0.69—counterintuitive given RoBERTa's generally stronger pretraining, suggesting tokenizer differences may matter for these languages
  - **Word-level vs. character-level input**: Word-level classification misses sub-character patterns; character-level CNNs might capture finer distinctions but require more data
  - **Three-class vs. binary**: The wal-gof class adds label complexity but prevents forced decisions on genuinely ambiguous cognates

- **Failure signatures:**
  - **High precision, very low recall** (e.g., LSTM with attention: 0.78 precision / 0.43 recall, F1=0.55): Model is over-conservative, predicting only high-confidence cases; threshold calibration needed
  - **Low precision, high recall** (e.g., Logistic Regression: 0.55 precision / 0.76 recall, F1=0.47): Model over-predicts, generating many false positives; class imbalance or insufficient feature capacity
  - **F1 plateau at 0.64–0.69 with CNN/LSTM combinations**: Suggests architectural changes alone insufficient; likely need more data or language-specific pretraining

- **First 3 experiments:**
  1. **Establish ML baseline with Logistic Regression**: Confirm task difficulty; expected F1 ~0.47 validates that simple bag-of-features approaches fail on closely related languages
  2. **Ablate DL architectures (LSTM vs. BiLSTM vs. CNN)**: Test whether sequential (LSTM/BiLSTM) or local pattern (CNN) processing better captures morphological distinctions. Paper shows CNN (0.64) outperformed LSTM-attention (0.55), suggesting local character patterns dominate
  3. **Frozen BERT embeddings with trainable LSTM head**: Extract BERT embeddings, freeze backbone, train only LSTM and dense layers. Compare against end-to-end fine-tuning to diagnose whether embedding quality or classifier capacity is the bottleneck. If frozen performs comparably, the pretrained representations are already sufficient; if not, fine-tuning may help but risks overfitting on 144K words

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does increasing the size and diversity of the training dataset improve the F1-score of bilingual LID models for Omotic languages?
- Basis in paper: [explicit] The conclusion states that "further research can focus on... increasing the size and diversity of the training data" to improve model performance
- Why unresolved: The current study utilized a novel dataset of 144K words, but the authors explicitly identify data volume and diversity as ongoing challenges for low-resource languages
- What evidence would resolve it: Experiments comparing the current baseline against models trained on expanded corpora including diverse domains (e.g., news, casual speech) to measure the delta in macro F1-scores

### Open Question 2
- Question: How can the developed word-level LID system be effectively integrated into Machine Translation (MT) pipelines to enhance translation accuracy for Wolaita and Gofa?
- Basis in paper: [explicit] The authors note that "an important application of the bilingual LID model lies in machine translation tasks," suggesting the selection of appropriate language models based on detected source language
- Why unresolved: While the authors propose this application, the paper focuses solely on the classification task and does not implement or evaluate the downstream impact on translation quality
- What evidence would resolve it: A comparative study of MT systems with and without the LID preprocessing module, evaluating metrics such as BLEU scores for code-switched text

### Open Question 3
- Question: Would transformer models pre-trained on multilingual or morphologically rich languages outperform the current English-centric `bert-base-uncased` model for Omotic language identification?
- Basis in paper: [inferred] The study utilizes `bert-base-uncased` (an English model) for embeddings on Omotic languages. The authors suggest experimenting with "different feature representations" to improve performance
- Why unresolved: Using an English-based model for Omotic languages presents a linguistic mismatch; it is untested whether multilingual models (e.g., mBERT, XLM-R) would capture the morphological nuances of Wolaita and Gofa better
- What evidence would resolve it: Benchmarking the current BERT-LSTM architecture against variants using multilingual BERT or AfriBERTa to determine if cross-lingual transfer learning improves upon the 0.72 F1-score

## Limitations

- The dataset size of 144K words remains relatively small for transformer-based models, potentially limiting performance
- BERT-base-uncased was pretrained exclusively on English text without language-specific pretraining on Ethiopian languages
- The wal-gof class may artificially inflate performance metrics if it absorbs genuinely ambiguous cases
- Lack of ablation studies prevents definitive attribution of performance gains to transformer architecture versus LSTM classifier
- No error analysis or confusion matrices provided to identify specific failure patterns

## Confidence

**High Confidence (90%+):** The experimental setup is well-specified and reproducible: 3-class word-level classification, BERT-base-uncased + LSTM architecture, Adam optimizer (lr=0.001), batch size 128, and the reported F1 scores for each model variant. The annotation methodology (3 native speakers, majority vote) is standard and transparent.

**Medium Confidence (70-89%):** The claim that BERT-base-uncased + LSTM achieves "best performance" with F1=0.72 is supported by the experimental results, but this confidence is tempered by the lack of comparison to alternative embedding strategies (e.g., language-specific pretraining, character-level embeddings) and the absence of statistical significance testing between models. The assertion that the wal-gof class meaningfully improves classification by reducing boundary confusion is plausible but not empirically validated.

**Low Confidence (below 70%):** The broader claims about "potential applications in social media monitoring and language processing tasks" lack supporting evidence or quantitative projections. The paper's assertion that this work "provides a foundation for further research on low-resource Ethiopian languages" is aspirational but not substantiated with concrete next steps or benchmarks.

## Next Checks

1. **Ablation study on BERT component:** Run experiments comparing (a) frozen BERT embeddings with trainable LSTM head, (b) trainable BERT with frozen LSTM head, and (c) from-scratch embeddings with full model training. This will isolate whether the performance gains stem from the pretrained representations or the classifier architecture.

2. **Language-specific pretraining evaluation:** Pretrain a BERT-like model on a corpus of related Ethiopian languages (e.g., Amharic, Tigrinya) or even multilingual Wikipedia data, then fine-tune on the Wolaitta-Gofa dataset. Compare F1 scores against BERT-base-uncased to quantify the benefit of language-specific initialization.

3. **Error analysis and confusion matrix generation:** Generate per-class precision, recall, and F1 scores, along with a full confusion matrix. Analyze misclassifications to identify whether errors cluster around specific word types (e.g., short words, cognates, morphologically complex words) or specific language pairs, informing targeted architectural improvements.