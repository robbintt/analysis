---
ver: rpa2
title: The Resurgence of GCG Adversarial Attacks on Large Language Models
arxiv_id: '2509.00391'
source_url: https://arxiv.org/abs/2509.00391
tags:
- arxiv
- adversarial
- prompts
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the scalability of gradient-based adversarial\
  \ attacks on large language models using the Greedy Coordinate Gradient (GCG) algorithm\
  \ and its annealing-augmented variant, T-GCG. The study systematically tests GCG\
  \ attacks on three open-source LLMs\u2014Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B\u2014\
  using both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts."
---

# The Resurgence of GCG Adversarial Attacks on Large Language Models
## Quick Facts
- arXiv ID: 2509.00391
- Source URL: https://arxiv.org/abs/2509.00391
- Reference count: 40
- Key result: GCG adversarial attack success rates drop with model size, with reasoning tasks more vulnerable than safety prompts

## Executive Summary
This paper evaluates the scalability of gradient-based adversarial attacks on large language models using the Greedy Coordinate Gradient (GCG) algorithm and its annealing-augmented variant, T-GCG. The study systematically tests GCG attacks on three open-source LLMs—Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B—using both safety-oriented prompts (AdvBench) and reasoning-intensive coding prompts. Key findings include: (1) attack success rates decrease with model size, reflecting greater robustness of larger models due to more complex loss landscapes; (2) prefix-based heuristic evaluations substantially overestimate attack effectiveness compared to GPT-4o semantic judgments, which provide stricter and more realistic assessments; and (3) coding-related prompts are significantly more vulnerable than safety prompts, suggesting reasoning tasks expose alignment weaknesses. Preliminary T-GCG results show annealing-inspired exploration can improve prefix-based ASR, though benefits under semantic judgment remain limited. Overall, the work highlights the need for task-specific robustness testing and more rigorous evaluation protocols when assessing adversarial vulnerabilities in LLMs.

## Method Summary
The study employs the Greedy Coordinate Gradient (GCG) algorithm to generate adversarial examples for large language models. GCG iteratively optimizes input tokens by computing gradients of a loss function with respect to model parameters, selecting and modifying tokens that most reduce the model's compliance with safety or reasoning constraints. The variant T-GCG introduces annealing-inspired exploration to potentially escape local minima during optimization. Attacks are tested on three open-source models (Qwen2.5-0.5B, LLaMA-3.2-1B, GPT-OSS-20B) using safety prompts from AdvBench and reasoning-intensive coding tasks. Evaluation uses both heuristic prefix-based checks and stricter GPT-4o semantic judgments to measure attack success rates.

## Key Results
- Attack success rates decrease with increasing model size, indicating greater robustness in larger models
- Prefix-based evaluations overestimate attack effectiveness compared to GPT-4o semantic judgments
- Coding-related prompts are significantly more vulnerable than safety prompts
- T-GCG's annealing exploration shows limited improvement under semantic evaluation

## Why This Works (Mechanism)
The mechanism behind GCG's effectiveness lies in its gradient-based token optimization, which iteratively modifies input tokens to maximize model misalignment. Larger models exhibit more complex loss landscapes with numerous local minima, making gradient-based attacks less effective as they struggle to find globally optimal adversarial examples. The vulnerability difference between safety and reasoning prompts suggests that models may have stronger alignment mechanisms for safety-related outputs but weaker defenses against reasoning manipulation. T-GCG's annealing approach attempts to address local minimum trapping by introducing controlled exploration, though its practical benefits remain limited under stricter semantic evaluation.

## Foundational Learning
**Gradient-based adversarial attacks**: Iterative optimization methods that modify inputs using model gradients to induce misclassification or misbehavior. Why needed: Core attack mechanism for testing model robustness. Quick check: Verify gradient computation and token selection logic.

**Loss landscape complexity**: The geometric structure of loss functions in high-dimensional parameter space, characterized by local minima, saddle points, and global optima. Why needed: Explains why attack effectiveness varies with model size. Quick check: Compare loss surface visualizations across model scales.

**Prefix-based vs semantic evaluation**: Two evaluation paradigms where prefix-based uses simple heuristic checks while semantic uses model-based understanding of output meaning. Why needed: Highlights evaluation rigor differences. Quick check: Test same adversarial examples under both evaluation methods.

**Annealing in optimization**: Temperature-based exploration strategy that gradually reduces randomness to escape local minima. Why needed: T-GCG's key modification for improved attack performance. Quick check: Monitor optimization trajectory with varying temperature schedules.

## Architecture Onboarding
**Component map**: Input prompt -> GCG optimization -> Adversarial example generation -> Model evaluation -> Success rate calculation
**Critical path**: Token gradient computation → Token selection → Token modification → Model output evaluation → Success determination
**Design tradeoffs**: GCG balances attack effectiveness with computational efficiency; prefix-based evaluation trades rigor for speed; larger models gain robustness at cost of attack feasibility
**Failure signatures**: Low ASR on large models indicates gradient-based attacks struggle with complex loss landscapes; high prefix-based ASR with low semantic ASR indicates evaluation inconsistency
**First experiments**: 1) Replicate ASR scaling with model size, 2) Compare prefix vs semantic evaluation on identical examples, 3) Test T-GCG annealing parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to models up to 20B parameters, leaving scalability to frontier systems untested
- Heavy reliance on GPT-4o for evaluation may introduce evaluator bias and limit generalizability
- Mechanisms behind robustness-size relationship remain incompletely characterized
- T-GCG benefits are preliminary and may not generalize across different attack scenarios

## Confidence
- High confidence in observed ASR trends across model sizes and prompt types
- Medium confidence in the relative overestimation by prefix-based evaluations, given reliance on a single judge (GPT-4o)
- Medium confidence in the robustness-size relationship, pending replication on larger or proprietary models

## Next Checks
1. Replicate key ASR findings on larger (30B–70B) or proprietary models to test scalability of robustness trends
2. Cross-validate semantic judgments with multiple human annotators and alternative models to reduce evaluator bias
3. Test T-GCG under more varied annealing schedules and extended optimization budgets to clarify its practical gains