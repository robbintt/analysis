---
ver: rpa2
title: 'AddressVLM: Cross-view Alignment Tuning for Image Address Localization using
  Large Vision-Language Models'
arxiv_id: '2508.10667'
source_url: https://arxiv.org/abs/2508.10667
tags:
- street
- image
- images
- address
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-grained street-level
  address localization within urban areas using large vision-language models (LVLMs).
  The proposed method, AddressVLM, integrates city-wide address localization capabilities
  into LVLMs by incorporating cross-view alignment tuning.
---

# AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models

## Quick Facts
- **arXiv ID**: 2508.10667
- **Source URL**: https://arxiv.org/abs/2508.10667
- **Reference count**: 39
- **Primary result**: Proposed method outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on Pittsburgh and San Francisco datasets, respectively.

## Executive Summary
This paper addresses the challenge of fine-grained street-level address localization within urban areas using large vision-language models (LVLMs). The proposed method, AddressVLM, integrates city-wide address localization capabilities into LVLMs by incorporating cross-view alignment tuning. This approach uses perspective-invariant satellite images as macro cues to establish connections between sparse street-view images, enhancing the LVLM's global understanding of street distribution through cross-view matching. AddressVLM consists of two-stage training protocols: cross-view alignment tuning and address localization tuning.

## Method Summary
AddressVLM employs a two-stage training approach to fine-tune a large vision-language model for city-wide address localization. In Stage 1, the model learns cross-view alignment by grafting street-view images into geographically corresponding satellite images, teaching it to associate micro-scale visual cues with macro-scale spatial layout. In Stage 2, the model performs address localization tuning on street-view images only, building on the spatial prior learned in Stage 1. The method uses LoRA (Low-Rank Adaptation) for efficient fine-tuning and evaluates performance across three question types (generation, judgment, multiple-choice) at two granularities (district, street).

## Key Results
- AddressVLM achieves 90.02% average accuracy on the Pitts-VQA dataset, outperforming counterpart LVLMs by over 9%
- The method achieves 86.65% average accuracy on the SF-Base-VQA dataset, outperforming counterpart LVLMs by over 12%
- Cross-view alignment tuning contributes approximately 3% improvement in joint street-district accuracy compared to training without alignment

## Why This Works (Mechanism)

### Mechanism 1: Cross-View Grafting for Spatial Context Injection
Embedding street-view images onto satellite maps enables LVLMs to learn city-wide street distributions that purely local street-view training cannot provide. Street-view images are scaled and grafted into the upper-right corner of geographically corresponding satellite images (with street labels visible). The LVLM learns to associate micro-scale visual cues with macro-scale spatial layout through the combined visual input.

### Mechanism 2: Automatic Label Generation for Alignment Reasoning
LVLM-generated explanatory alignment labels outperform template-based labels for cross-view tuning by providing diverse, natural language reasoning. Rather than using rigid template labels, the system provides a hint to an off-the-shelf LVLM, which generates detailed reasoning about visual matching cues. These generated explanations serve as training targets for the alignment task.

### Mechanism 3: Two-Stage Sequential Specialization
Separating global spatial alignment (Stage 1) from local address prediction (Stage 2) prevents the narrow street-view task from overwhelming the broader spatial understanding. Stage 1 uses grafted images to teach cross-view correspondence and street layout. Stage 2 uses only street-view images for VQA, building on the spatial prior. LoRA adaptation is applied in both stages.

## Foundational Learning

- **Cross-view Geo-localization**
  - **Why needed**: The entire method builds on the premise that satellite and ground views can be matched.
  - **Quick check**: Can you explain why a bird's-eye view helps locate a street-level photo when the two views look nothing alike?

- **LoRA (Low-Rank Adaptation)**
  - **Why needed**: The paper fine-tunes a 4B parameter LVLM efficiently using LoRA with rank 128.
  - **Quick check**: Why does LoRA allow training on consumer GPUs when full fine-tuning would not fit, and what does the rank parameter control?

- **Visual Question Answering (VQA) Formats**
  - **Why needed**: The paper evaluates three question types at two granularities.
  - **Quick check**: Given a street-view image, how would you formulate a judgment-type versus generation-type question for address localization?

## Architecture Onboarding

- **Component map**: Input (336×336 image + text) → Vision Encoder (CLIP ViT-L/14): extracts Z_v → VL Adapter (2-layer MLP): maps Z_v → H_v (language token space) → Pre-trained LLM (Phi-3.1-mini, 4B): fuses H_v with text tokens T_v → generates answer

- **Critical path**:
  1. Data prep: For each street-view image, retrieve corresponding satellite image (GPS-matched, ~50m radius), render street name labels on satellite, graft street-view at δ=0.5 overlap.
  2. Stage 1 training: Cross-view alignment on grafted images with auto-generated reasoning labels. Unfreeze LLM + adapter.
  3. Stage 2 training: Address localization VQA on street-view only. Unfreeze vision encoder + adapter + LLM.
  4. Inference: Street-view only, flexible prompts (generation/judgment/multi-choice).

- **Design tradeoffs**:
  - Grafting vs. separate images: Grafting preserves more visual tokens within 336×336 constraint but slightly obscures satellite context.
  - Automatic vs. manual labels: Auto-generation scales but may introduce noise.
  - Single-city vs. multi-city training: Mixed training slightly improves both cities.

- **Failure signatures**:
  - Low street-level accuracy but high district-level: Model learned coarse patterns but lacks fine-grained discrimination.
  - Hallucinated addresses not in training set: Likely general LVLM prior overriding fine-tuned knowledge.
  - Inconsistent outputs across repeated inference: Temperature too high.

- **First 3 experiments**:
  1. Baseline sanity check: Train Stage 2 only on a subset of Pitts-VQA; expect ~87% average accuracy vs. 90% with full method.
  2. Grafting ablation: Compare δ=0.3, 0.5, 0.7 on validation set; expect 0.5 optimal.
  3. Cross-city transfer: Train on Pittsburgh, evaluate zero-shot on San Francisco.

## Open Questions the Paper Calls Out

- **Cross-view alignment methods**: Can more sophisticated cross-view image alignment methods outperform the current image grafting mechanism, particularly in handling low-resolution street-view inputs?
- **Generative vs discriminative gap**: How can generative LVLMs close the performance gap with discriminative models on closed-set address classification tasks without losing the flexibility of generative outputs?
- **Global generalization**: Does the cross-view alignment tuning strategy generalize effectively to cities on different continents with distinct urban layouts and non-Western address systems?

## Limitations
- Cross-view generalization gap: Method not validated on zero-shot transfer to new cities with different layouts and satellite imagery quality.
- Label generation reliability: Automatic label generation quality not evaluated with human annotators.
- Dataset specificity: Constructed datasets are not publicly available, requiring significant effort to reproduce.

## Confidence
- **High**: Cross-view grafting mechanism well-supported by quantitative results showing grafting outperforms separate or no cross-view input.
- **Medium**: Two-stage sequential training shows consistent improvements, but ablation on joint training or skipping Stage 1 is limited.
- **Low**: Automatic label generation mechanism lacks direct evaluation of label quality.

## Next Checks
1. Zero-shot city transfer: Train AddressVLM on Pittsburgh only, then evaluate on San Francisco without any SF-specific fine-tuning.
2. Label quality audit: Generate 100 alignment labels for a held-out validation set, then have human annotators rate their relevance and correctness.
3. Joint training baseline: Implement a single-stage model that trains on both grafted and street-view images simultaneously. Compare against the two-stage approach.