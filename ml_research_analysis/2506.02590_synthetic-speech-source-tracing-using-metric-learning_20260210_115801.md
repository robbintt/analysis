---
ver: rpa2
title: Synthetic Speech Source Tracing using Metric Learning
arxiv_id: '2506.02590'
source_url: https://arxiv.org/abs/2506.02590
tags:
- loss
- embeddings
- resnet
- audio
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates source tracing in synthetic speech, aiming
  to identify the generative TTS system responsible for manipulated audio. It compares
  classification-based and metric-learning approaches using ResNet and SSL backbones
  on the MLAADv5 benchmark.
---

# Synthetic Speech Source Tracing using Metric Learning

## Quick Facts
- arXiv ID: 2506.02590
- Source URL: https://arxiv.org/abs/2506.02590
- Reference count: 0
- Primary result: ResNet with GE2E loss achieves 4.63% EER on MLAADv5 test set, competitive with SSL-based systems

## Executive Summary
This paper investigates source tracing in synthetic speech, aiming to identify which text-to-speech system generated a given audio sample. The authors compare classification-based and metric-learning approaches using both ResNet and SSL backbones on the MLAADv5 benchmark. They demonstrate that metric learning methods (GE2E, Angular Prototypical) significantly outperform classification losses, and that ResNet with Mel filterbanks can match or exceed SSL-based systems despite having far fewer parameters. The study shows that embeddings from 10-50 dimensions are sufficient for this task, with 50-dimensional embeddings achieving the best test performance.

## Method Summary
The study evaluates source tracing using two architectures: ResNet-34 with Mel filterbank inputs and Wav2Vec2 with AASIST modules. Both architectures employ self-attentive pooling to aggregate frame-level features into utterance embeddings of varying dimensions (10-512). Four loss functions are compared: Softmax, AMSoftmax, AAMSoftmax (classification-based), and GE2E/Angular Prototypical (metric learning). Models are trained on MLAADv5, a subset containing 25 TTS systems. Evaluation uses Equal Error Rate (EER) and includes linear probing to assess embedding generalizability to unseen classes.

## Key Results
- Metric learning losses (GE2E, Angular Prototypical) outperform classification losses for source tracing
- ResNet with GE2E loss and 50-dimensional embeddings achieves 4.63% EER on test set
- Linear probing shows embeddings are separable even for unseen TTS classes
- Mel filterbank features demonstrate better generalization to unseen classes than SSL representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metric learning losses outperform classification losses because they learn a generalizable embedding space rather than memorizing training classes.
- Mechanism: GE2E computes per-class centroids from support samples in each mini-batch, then trains the embedding extractor to pull same-class samples closer to their centroid while pushing different-class samples away.
- Core assumption: TTS system artifacts form coherent clusters in embedding space that persist across languages and architectures.
- Evidence anchors: Abstract states "Metric learning methods outperform classification losses"; section 5 shows GE2E achieving sub-5% EER; neighbor paper uses similar metric learning approach.
- Break condition: If TTS systems share identical synthesis components, their artifacts may be indistinguishable.

### Mechanism 2
- Claim: Lower-dimensional embeddings (10-50 dimensions) are sufficient and sometimes superior to higher dimensions for source tracing.
- Mechanism: With only 24 training TTS classes, high-dimensional embeddings risk overfitting. Smaller embeddings act as a regularizer, forcing the model to capture only the most salient discriminative features.
- Core assumption: The intrinsic dimensionality of TTS system artifacts is low.
- Evidence anchors: Section 5 shows most models don't benefit from larger output embedding sizes; best test EER achieved with 50-dim embeddings.
- Break condition: If the number of TTS systems to distinguish grows significantly, embedding capacity may become insufficient.

### Mechanism 3
- Claim: ResNet with Mel filterbank inputs can match SSL-based systems despite SSL models having orders of magnitude more parameters.
- Mechanism: Mel filterbanks capture spectral artifacts consistent across seen and unseen classes. SSL representations may overfit to training distribution features that don't transfer as well.
- Core assumption: TTS system artifacts manifest in spectral patterns that Mel filterbanks can capture without requiring learned representations.
- Evidence anchors: Abstract states "ResNet achieves competitive performance"; section 5 notes SSL models showed weaker generalization.
- Break condition: If TTS systems leave subtler artifacts in higher-level linguistic features rather than spectral patterns.

## Foundational Learning

- **Equal Error Rate (EER)**: Primary evaluation metric where false acceptance rate equals false rejection rate. Essential for interpreting all results.
  - Quick check: If a model has EER of 4.63%, what does that mean about its error tradeoff at the decision threshold?

- **Metric Learning vs Classification**: Central comparison. Classification learns class boundaries in original feature space; metric learning learns an embedding space where distance correlates with similarity. Metric learning generalizes better to open-set problems.
  - Quick check: Why would a model trained with triplet loss potentially handle unseen TTS systems better than one trained with cross-entropy?

- **Centroid-based Classification**: GE2E and Angular Prototypical losses use centroids (mean embeddings per class) for training and inference. Understanding how centroids are computed and used is essential.
  - Quick check: In GE2E, why is the query sample excluded when computing its own class centroid during training?

## Architecture Onboarding

- Component map: Input Audio (2s segment) -> [ResNet-34 Backbone] or [Wav2Vec2 + AASIST] -> Frame-level features -> [Self-Attentive Pooling (SAP)] -> Utterance-level embedding (10/50/200/512 dim) -> [Loss Layer: Softmax/AMSoftmax/AAMSoftmax/GE2E/AngularProto]

- Critical path:
  1. Extract random 2-second audio segment during training
  2. Compute 40-dim Mel filterbanks (ResNet) or 1024-dim Wav2Vec2 embeddings (SSL)
  3. Apply instance normalization
  4. Forward through backbone + SAP pooling â†’ utterance embedding
  5. Apply loss function (balanced sampler required for metric learning)
  6. Inference: compute cosine similarity between embeddings; evaluate with EER

- Design tradeoffs:
  - ResNet (1.4M params, ~2h training) vs AASIST+Wav2Vec2 (315M params, ~4h training)
  - Random sampler works with classification losses; balanced sampler required for metric learning
  - 10-50 dims sufficient; 512 provides no benefit and may harm generalization

- Failure signatures:
  - Softmax with random sampler produces ~45% dev EER (near random for 25 classes)
  - SSL models showing dev EER < test EER (generalization issues)
  - Confusion between mms-tts variants (classes 1-3) due to shared architecture

- First 3 experiments:
  1. Train ResNet-34 with GE2E loss, 50-dim embeddings, balanced sampler on MLAADv5. Target: dev EER <5%, test EER ~5%.
  2. Compare 10, 50, 200, 512 dims with GE2E. Expect 50-dim to be optimal.
  3. Train linear classifier on dev set embeddings from best ResNet model. Generate confusion matrix.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the metric learning pipeline maintain robustness when applied to replayed TTS audio?
- Basis in paper: Conclusion states "Further experimentation may include... experiments with replayed TTS."
- Why unresolved: Current study evaluates direct synthesized audio but doesn't assess performance degradation from playback and re-recording.
- What evidence would resolve it: Evaluation on dataset containing replayed synthetic speech samples to measure performance gaps against clean synthetic inputs.

### Open Question 2
- Question: Can SSL-based backbones be optimized to outperform ResNet in generalizing to unseen TTS systems?
- Basis in paper: Abstract and conclusion underscore "need to optimize SSL representations for this task" because AASIST models exhibited weaker generalization.
- Why unresolved: SSL models performed well on development sets but struggled with unseen classes.
- What evidence would resolve it: Ablation studies applying domain adaptation techniques or alternative SSL pre-training objectives.

### Open Question 3
- Question: Do alternative architectures or advanced training algorithms improve upon the Thin-ResNet and AASIST baselines?
- Basis in paper: Authors list "other architectures and training algorithms" as primary direction for further experimentation.
- Why unresolved: Resource constraints limited scope to two specific backbones and restricted set of loss functions.
- What evidence would resolve it: Benchmarking additional architectures (e.g., ECAPA-TDNN, RawNet3) or complex data augmentation strategies.

## Limitations

- SSL model results may reflect implementation issues rather than architectural limitations, as AASIST consistently showed worse test EER than dev EER
- Evaluation only on MLAADv5 with 25 TTS systems, many sharing similar architectures (mms-tts variants 1-3)
- No comparison with properly tuned Wav2Vec2 baseline to establish true architectural differences

## Confidence

- **High confidence**: Metric learning losses outperforming classification losses for source tracing in controlled settings with balanced sampling
- **Medium confidence**: ResNet achieving competitive performance with SSL despite parameter disparity, requiring validation with properly tuned SSL baselines
- **Low confidence**: Claims about Wav2Vec2 embeddings learning "specific features that may not always aid with unseen classes" without systematic ablation studies

## Next Checks

1. **SSL Baseline Tuning**: Replicate experiments with properly tuned Wav2Vec2 + linear classifier and Wav2Vec2 + fine-tuning configurations on MLAADv5 to establish whether AASIST results reflect architectural limitations or implementation issues.

2. **Cross-Corpus Generalization**: Test the best ResNet model (GE2E, 50-dim) on LJ-Spoof or other TTS source tracing datasets to evaluate real-world generalization beyond MLAADv5's limited TTS system diversity.

3. **Feature Attribution Analysis**: Conduct Grad-CAM or similar visualization studies to determine whether ResNet with Mel filterbanks and SSL representations focus on different spectral/temporal regions when distinguishing TTS systems, validating the hypothesis about consistent vs. learned features.