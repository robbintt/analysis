---
ver: rpa2
title: RL-Guided Data Selection for Language Model Finetuning
arxiv_id: '2509.25850'
source_url: https://arxiv.org/abs/2509.25850
tags:
- data
- training
- reward
- selection
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient data selection for
  fine-tuning large language models (LLMs) under a strict data budget. It reformulates
  this as a tractable Markov Decision Process (MDP) over semantic clusters of the
  training data and trains reinforcement learning (RL) agents to learn optimal selection
  policies guided by a proxy-model-based reward signal.
---

# RL-Guided Data Selection for Language Model Finetuning

## Quick Facts
- **arXiv ID:** 2509.25850
- **Source URL:** https://arxiv.org/abs/2509.25850
- **Reference count:** 40
- **Primary result:** 5% data subset selection matches or exceeds full-data performance across benchmarks

## Executive Summary
This paper introduces an RL-guided approach for efficient data selection in LLM fine-tuning, addressing the challenge of limited compute budgets and noisy training data. The method reformulates data selection as a Markov Decision Process over semantic clusters and trains RL agents to learn optimal selection policies guided by proxy-model-based rewards. By training on carefully selected subsets (as small as 5% of original data), the approach achieves performance matching or exceeding full-data fine-tuning while reducing training time by up to 2x across multiple tasks.

## Method Summary
The approach clusters training data into semantic groups and frames data selection as a sequential decision-making problem. An RL agent learns to select optimal subsets of clusters through interaction with a proxy model that provides reward signals based on validation performance. The method explores both DQN and PPO algorithms with different reward modeling strategies (DynaDQN and CLIMB-Disc). The learned policies effectively identify high-quality data while filtering noisy examples, enabling efficient fine-tuning with minimal performance loss.

## Key Results
- 5% data subset selection matches or exceeds full-data performance by up to 10.8 accuracy points
- Up to 2x reduction in wall-clock training time across four benchmark tasks
- Effective filtering of noisy data, particularly improving performance on MetaHate dataset
- Consistent performance gains across diverse tasks including ANLI, GooglePlay, MetaHate, and MMLU

## Why This Works (Mechanism)
The method works by transforming data selection from a static filtering problem into a dynamic sequential decision process. By clustering data and learning selection policies over these clusters, the RL agent can capture semantic relationships between data points that simple filtering would miss. The proxy model reward signal provides a computationally efficient way to evaluate selection quality without requiring full fine-tuning on each candidate subset. This allows the agent to learn nuanced selection strategies that balance data diversity with quality, leading to more effective training subsets.

## Foundational Learning
**Markov Decision Process (MDP):** A mathematical framework for modeling sequential decision-making where actions influence future states and rewards. Needed because data selection is inherently sequential - each choice affects subsequent options. Quick check: Verify state transitions follow Markov property.

**Reinforcement Learning:** A learning paradigm where agents learn optimal behaviors through interaction with an environment and reward signals. Needed to learn data selection policies without explicit supervision. Quick check: Ensure reward signal is properly shaped and informative.

**Semantic Clustering:** Grouping data points based on meaning rather than surface features. Needed to create meaningful states in the MDP and enable cluster-level selection. Quick check: Validate clusters capture task-relevant semantic relationships.

**Proxy Model Evaluation:** Using a smaller, faster model to estimate the quality of data subsets before full fine-tuning. Needed to make RL training computationally feasible. Quick check: Confirm proxy model correlates well with full model performance.

**Dyna-style Planning:** Combining model-free RL with learned environment models to improve sample efficiency. Needed to accelerate RL learning in the data selection MDP. Quick check: Verify planning updates improve policy convergence.

## Architecture Onboarding

**Component Map:** Data Clusters -> MDP State Space -> RL Agent (DQN/PPO) -> Selection Policy -> Proxy Model Evaluation -> Reward Signal -> Agent Update

**Critical Path:** Cluster training → MDP formulation → RL policy learning → Subset selection → Proxy model validation → Fine-tuning

**Design Tradeoffs:** The method trades upfront computational cost (clustering, proxy training, RL optimization) for reduced downstream fine-tuning cost and improved data quality. This makes sense when fine-tuning budget is constrained but compute for selection is available.

**Failure Signatures:** Poor clustering leads to ineffective selection policies; inadequate proxy model performance causes suboptimal reward signals; RL algorithm choice affects convergence speed and final policy quality.

**First Experiments:**
1. Validate clustering quality on a small held-out set to ensure semantic coherence
2. Test proxy model correlation with full model performance across data subsets
3. Run RL training with simplified state/action spaces to verify MDP formulation

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computationally expensive upfront investment in clustering and RL training
- Performance highly dependent on clustering quality and proxy model selection
- MDP formulation may not capture all relevant data relationships
- Requires significant compute resources for multiple training cycles

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Matching/exceeding full-data performance with 5% subset | High |
| 2x training time reduction | Medium |
| Noisy data filtering effectiveness | Medium |

## Next Checks
1. Test method generalization on out-of-domain datasets with different data characteristics
2. Conduct ablation studies on clustering algorithms and proxy model architectures
3. Measure sensitivity of results to different reward model strategies across tasks