---
ver: rpa2
title: 'ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages'
arxiv_id: '2512.01077'
source_url: https://arxiv.org/abs/2512.01077
tags:
- cultural
- translation
- languages
- language
- recipes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELR-1000, a dataset of 1,060 traditional
  recipes in 10 endangered Indic languages collected from rural communities using
  a mobile interface designed for low digital literacy. The dataset includes multimodal
  data (text, images, audio) and parallel English translations for evaluation.
---

# ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages

## Quick Facts
- arXiv ID: 2512.01077
- Source URL: https://arxiv.org/abs/2512.01077
- Reference count: 37
- Dataset of 1,060 traditional recipes in 10 endangered Indic languages with multimodal data and English translations

## Executive Summary
This paper introduces ELR-1000, a dataset of 1,060 traditional recipes in 10 endangered Indic languages collected from rural communities using a mobile interface designed for low digital literacy. The dataset includes multimodal data (text, images, audio) and parallel English translations for evaluation. Experiments with six state-of-the-art LLMs show that without context, translations are often unusable, but providing cultural and linguistic context significantly improves translation quality. Even with context, models struggle with cultural nuances and often substitute indigenous concepts with Western equivalents. The work highlights the need for culturally-grounded datasets and evaluation methods to support low-resource and endangered language technologies.

## Method Summary
The dataset was collected using the Karya mobile app, which supports offline data collection with audio-visual interfaces for low-literacy contributors. Native speakers from 10 endangered Indic languages documented traditional recipes, resulting in multimodal data (text, images, audio) with parallel English translations. Six state-of-the-art LLMs were evaluated under two conditions: context-free prompting and context-aware prompting that included language background, few-shot examples, cultural preservation guidelines, and structural instructions. Human evaluators assessed translations across four dimensions: Adequacy, Fluency, Comprehensibility, and Cultural Appropriateness using Likert scales.

## Key Results
- Context-aware prompting significantly improved translation quality across all models and evaluation dimensions
- Fluency and comprehensibility scores consistently exceeded adequacy and cultural appropriateness scores, revealing "fluent falsehoods"
- Models frequently substituted indigenous concepts with Western equivalents (e.g., replacing traditional tools with "chopping board")
- Even with context, models struggled with cultural nuances, particularly for languages with minimal web presence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing targeted cultural and linguistic context to LLMs significantly improves translation quality for endangered languages.
- Mechanism: Contextual prompts supply language background, few-shot translation examples, cultural preservation guidelines, and structural instructions—enabling models to retrieve relevant latent knowledge and reduce hallucination.
- Core assumption: Models contain some latent knowledge about these languages from pre-training that can be activated with appropriate prompting cues.
- Evidence anchors:
  - [abstract] "providing targeted context—including background information about the languages, translation examples, and guidelines for cultural preservation—leads to significant improvements in translation quality"
  - [section 5.2.2] "results showed significantly improved performance across all models in the contextual condition, with particularly dramatic improvements for models like Mistral that exhibited hallucination behaviors without context"
  - [corpus] Limited corpus support; neighbor papers (LakotaBERT, ultra-low-resource translators) suggest context/fine-tuning helps but mechanisms differ across languages
- Break condition: Context may fail when models have near-zero pre-training exposure to a language (e.g., Kaman Mishmi), or when cultural concepts have no approximate representation in training data.

### Mechanism 2
- Claim: Community-generated, domain-specific data captures cultural nuance that web-scraped or translated benchmarks miss.
- Mechanism: Native speakers documenting lived practices (recipes, tools, seasonality) encode tacit knowledge—ingredient naming, traditional implements, oral narration—that high-resource benchmarks derived from Wikipedia cannot replicate.
- Core assumption: Domain-specific, community-authored text preserves epistemic content better than translation-from-English approaches.
- Evidence anchors:
  - [section 1] "most datasets focus on high-resource or officially recognized languages, overlooking India's tribal and indigenous diversity"
  - [section 3.5] "54% of recipe steps included all three modalities, 29% used image-text, and 83.4% featured text. Notably, 64.5% included audio narration, underscoring the cultural significance of oral knowledge"
  - [corpus] Neighbor papers (Indigenous Languages in Argentina, Australian Aboriginal English) similarly emphasize community-sourced data for under-documented languages
- Break condition: Community data may be heterogeneous, non-standardized, and require significant preprocessing; not all domains lend themselves to crowdsourcing.

### Mechanism 3
- Claim: Fluency scores systematically exceed accuracy/cultural-appropriateness scores, creating "fluent falsehoods" that mask translation failure.
- Mechanism: Models optimize for grammatical correctness and fluency in the target language (English), defaulting to Western concepts (e.g., "chopping board") even when inappropriate, because cultural adequacy is not explicitly penalized during training or evaluation.
- Core assumption: Standard training objectives and benchmarks do not weight cultural fidelity.
- Evidence anchors:
  - [section 6.1] "'Fluency' and 'Comprehensibility' scores consistently exceeded 'Adequacy' and 'Cultural Appropriateness' scores... models excel at generating grammatically correct, readable English output that often bears minimal resemblance to the source text's actual meaning"
  - [table 3] Gemini substitutes "bamboo shoot" for "star fruit" and introduces "chopping board"—a tool rarely used in indigenous kitchens
  - [corpus] Weak corpus support; neighbor papers on cultural expressiveness in LLMs hint at similar phenomena but do not confirm mechanism
- Break condition: Mitigation requires explicit cultural-appropriateness metrics in evaluation and potentially in training objectives.

## Foundational Learning

- Concept: Low-resource vs. zero-resource language distinction
  - Why needed here: The paper evaluates languages with minimal digital presence; knowing where a language sits on this spectrum determines whether few-shot prompting can work or if models will hallucinate entirely
  - Quick check question: Can you identify which languages in ELR-1000 have some web presence versus those with near-zero pre-training signal?

- Concept: Cultural grounding in NLP
  - Why needed here: Standard translation metrics (BLEU, chrF) do not capture cultural fidelity; understanding this gap explains why the paper introduces a 4-dimension Likert evaluation including cultural appropriateness
  - Quick check question: What specific cultural losses occurred in model outputs (e.g., ingredient substitution, tool normalization)?

- Concept: Multimodal data heterogeneity
  - Why needed here: Community contributions vary in completeness across modalities (text, image, audio); preprocessing must handle nulls and variable structure
  - Quick check question: What percentage of recipe steps included all three modalities, and what does this imply for evaluation design?

## Architecture Onboarding

- Component map:
  Data collection layer: Karya mobile app (offline-capable, audio-visual UI for low literacy) → local coordinator validation → contributor payment pipeline
  Data storage: Modular array-based structure with parallel directories for text, image, audio per recipe step
  Translation layer: Six LLMs evaluated under context-free vs. context-aware prompting
  Evaluation layer: Human translations → LLM-generated translations → hybrid LLM-judge ensemble (Gemini 2.5 Pro + GPT-5) with human oversight on discrepancies

- Critical path:
  1. Define target languages and recruit native-speaking local coordinators
  2. Deploy mobile app with in-person training; collect multimodal recipe data
  3. Validate and preprocess (handle nulls, normalize structure)
  4. Generate translations under both prompt conditions
  5. Run 4-dimension Likert evaluation (Adequacy, Fluency, Comprehensibility, Cultural Appropriateness)

- Design tradeoffs:
  - Authenticity vs. consistency: Allowing free-form contributions captures cultural richness but increases heterogeneity
  - Context length vs. cost: Detailed cultural prompts improve quality but increase inference cost and latency
  - Proprietary vs. open models: Proprietary models (Gemini, GPT-4o) performed best; open models may be necessary for community deployment

- Failure signatures:
  - Hallucinated recipes: Model generates entirely different dishes (e.g., silkworm recipe becomes mushroom curry)
  - Western concept insertion: Inappropriate tools/ingredients (chopping board, gas stove) appear in output
  - Near-zero performance without context: Some languages (Kaman Mishmi, Bodo) approach complete failure

- First 3 experiments:
  1. Replicate the context-free vs. context-aware comparison on a held-out subset of recipes; measure score deltas across all four dimensions
  2. Ablate context components (remove language background, remove few-shot examples, remove cultural guidelines) to identify which contributes most to improvement
  3. Test whether few-shot examples from one language transfer to related languages in the dataset (e.g., Sadri examples for Ho)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can internal probing methods or question-answering tasks effectively measure how cultural concepts are represented in LLM embeddings for endangered languages?
- Basis in paper: [explicit] "Future research could therefore move beyond surface-level evaluation and explicitly probe LLMs' cultural awareness through question–answering tasks or internal probing methods that examine how cultural concepts are represented in model embeddings."
- Why unresolved: Current evaluation focuses on output fluency and adequacy, not on whether models internally encode cultural knowledge or merely generate plausible surface text.
- What evidence would resolve it: Development of probing benchmarks that correlate with downstream cultural appropriateness, or layer-wise analysis showing where cultural information is (or isn't) represented.

### Open Question 2
- Question: Can fine-tuning or in-context learning with culturally grounded datasets like ELR-1000 reduce the "fluent falsehood" phenomenon where models produce grammatically correct but culturally inaccurate translations?
- Basis in paper: [inferred] The paper documents "fluent falsehood" as a critical failure mode but only evaluates zero-shot/few-shot translation without examining whether model training on such datasets improves cultural fidelity.
- Why unresolved: The dataset is released as a benchmark for evaluation, not yet tested for model improvement; the gap between syntactic fluency and cultural accuracy remains unbridged.
- What evidence would resolve it: Pre/post fine-tuning comparisons showing reduced Western concept substitution rates and improved preservation of indigenous tools, ingredients, and methods.

### Open Question 3
- Question: What is the minimum scale of community-generated parallel data needed to meaningfully improve LLM translation quality for severely low-resource endangered languages?
- Basis in paper: [explicit] "We collect approximately 100 recipes per language... This may be sufficient for benchmarking existing LLMs but may not enough to improve the LLMs to work better in this domain."
- Why unresolved: The current dataset size (106 recipes/language average) was designed for evaluation, not training; the threshold for practical improvement remains unknown.
- What evidence would resolve it: Systematic experiments varying dataset size per language and measuring translation quality improvements on held-out cultural content.

### Open Question 4
- Question: Do culturally grounded datasets from one domain (e.g., cuisine) transfer to improve LLM performance on other culturally-specific domains (e.g., agriculture, medicine) in the same endangered languages?
- Basis in paper: [explicit] "Cuisine is not the only topic that could help us collect culturally relevant data. Some other such topics could be the agricultural and livestock farming practices... Covering many diverse topics like this could make the collected benchmark more valuable."
- Why unresolved: It is unknown whether cultural knowledge learned from recipe language generalizes to other domains sharing similar ecological vocabulary and conceptual frameworks.
- What evidence would resolve it: Cross-domain transfer experiments showing whether recipe-trained models better handle agricultural or medicinal texts compared to baselines.

## Limitations
- Evaluation relies on human judges but lacks reported inter-annotator agreement metrics
- Only English target language tested; benefits of contextual prompting for other languages unknown
- Dataset size (1,060 recipes) may limit generalizability to other domains or larger corpora
- No quantification of Western concept substitution frequency or impact on downstream tasks

## Confidence
- **High confidence**: Context-aware prompting significantly improves translation quality over context-free prompting (supported by large, consistent score differences across all models and dimensions)
- **Medium confidence**: Community-generated, multimodal data better captures cultural nuance than standard benchmarks (supported by qualitative observations and prior literature, but lacks direct comparative quantitative evidence)
- **Medium confidence**: Fluency scores systematically exceed cultural-appropriateness scores, masking translation failures (supported by score patterns but not explicitly validated as a widespread phenomenon beyond this dataset)

## Next Checks
1. **Annotator Agreement Analysis**: Compute and report inter-annotator agreement (e.g., Krippendorff's alpha) for all four Likert dimensions to establish rater reliability
2. **Cross-Lingual Generalization Test**: Replicate the context-free vs. context-aware comparison for German and Spanish target languages to test whether contextual prompting benefits extend beyond English
3. **Cultural Substitution Quantification**: Annotate a sample of model outputs to measure the frequency and types of Western concept substitutions (e.g., tool, ingredient, or practice substitutions) and assess their impact on downstream cultural preservation tasks