---
ver: rpa2
title: 'When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service
  under Token-Level Behavioral Instability in LLMs'
arxiv_id: '2506.10095'
source_url: https://arxiv.org/abs/2506.10095
tags:
- pbss
- prompt
- drift
- behavioral
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic
  framework that measures how large language models respond to semantically equivalent
  prompts that differ only in surface-level token form. The authors construct controlled
  prompt sets across ten tasks, generate model outputs under two decoding temperatures,
  and use sentence embeddings to compute pairwise behavioral drift.
---

# When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs

## Quick Facts
- arXiv ID: 2506.10095
- Source URL: https://arxiv.org/abs/2506.10095
- Reference count: 40
- This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic framework that measures how large language models respond to semantically equivalent prompts that differ only in surface-level token form.

## Executive Summary
This paper introduces Prompt-Based Semantic Shift (PBSS), a diagnostic framework that measures how large language models respond to semantically equivalent prompts that differ only in surface-level token form. The authors construct controlled prompt sets across ten tasks, generate model outputs under two decoding temperatures, and use sentence embeddings to compute pairwise behavioral drift. Their analysis reveals consistent, model-specific drift patterns that persist across embedding architectures, showing that tokenization and decoding dynamics contribute to post-training quality-of-service instability. Higher-capacity instruction-tuned models exhibit lower drift than smaller or pre-alignment models, with Kruskal-Wallis tests confirming statistically significant differences between model tiers. PBSS offers a lightweight, encoder-agnostic diagnostic for detecting prompt-induced behavioral variance, with implications for reliability in high-stakes domains such as medicine and legal applications.

## Method Summary
The authors construct 10 tasks with 5 prompt sets each, generating 15 paraphrases per task (750 prompts total). They validate paraphrases using SBERT similarity (>0.5 threshold) and syntax checks, then generate outputs from 5 models at two temperatures (0.2 and 1.3). Outputs are embedded using three SBERT encoders, and PBSS scores are computed as 1 - cosine similarity between output pairs. The analysis includes CDF plots, t-SNE projections, z-score heatmaps, and Kruskal-Wallis significance testing to identify drift patterns and model differences.

## Key Results
- Instruction-tuned models (GPT-3.5, LLaMA-2) show significantly lower drift than legacy models (GPT-2, GPT-Neo) under semantically equivalent prompt variations
- Behavioral drift patterns are consistent across three different SBERT encoders, suggesting the metric captures model-internal regularities rather than embedding artifacts
- Higher temperature (1.3) amplifies drift in legacy models but has minimal effect on instruction-tuned models, indicating different stability regimes
- Kruskal-Wallis tests confirm statistically significant divergence between model tiers (p < 0.01)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantically equivalent prompts with different token-level realizations activate divergent internal computation paths in LLMs, producing systematically different outputs.
- **Mechanism:** Tokenization maps surface-form text to discrete token IDs. Paraphrases with identical intent but different phrasing produce different token sequences, which query different regions of the embedding space and attend to different positional patterns during transformer forward passes. This propagates through layers as divergent activation patterns, yielding output distributions that differ beyond sampling noise.
- **Core assumption:** The embedding model (SBERT) captures rhetorical and stylistic variation that reflects meaningful behavioral differences, not just surface noise.
- **Evidence anchors:**
  - [abstract] "These results highlight an overlooked dimension of model evaluation stability under rephrasing and suggest that tokenization strategies and decoding dynamics may contribute to post-training quality-of-service instability."
  - [Section 3.1] "Such variance often arises from different token sequences—variations that standard accuracy metrics ignore."
  - [corpus] "Training Language Models with homotokens" (arXiv:2601.02867) documents that many distinct token sequences decode to identical surface forms yet induce different internal computations.
- **Break condition:** If SBERT similarity between outputs from paraphrased prompts is consistently high (>0.9) across all models and tasks, the tokenization-driven divergence hypothesis weakens.

### Mechanism 2
- **Claim:** Instruction tuning creates a behavioral phase shift that stabilizes prompt-to-output mappings under surface variation.
- **Mechanism:** Instruction-tuned models undergo supervised fine-tuning on diverse prompt formulations paired with consistent task completions. This training explicitly teaches the model to map varied tokenizations to stable intent representations, reducing sensitivity to surface form. Legacy models lack this regularization and retain stronger priors toward specific token patterns.
- **Core assumption:** The observed stability difference between model families reflects training procedure differences, not just parameter scale.
- **Evidence anchors:**
  - [Section 5.5] "CDF plots reveal a sharp divide: instruction-tuned models (e.g., GPT-3.5, LLaMA-2) cluster tightly under prompt variation, while older models (GPT-2, GPT-Neo) show broad, unstable dispersion. This is not a gradual trend but a behavioral phase shift."
  - [Section 5.8] "Kruskal–Wallis tests confirm statistically significant divergence between these groups."
  - [corpus] Evidence for instruction tuning mechanisms is indirect; no corpus paper directly validates this training-pathway hypothesis.
- **Break condition:** If scaling model parameters without instruction tuning produces equivalent stability gains, the instruction-tuning mechanism is confounded with scale.

### Mechanism 3
- **Claim:** SBERT embedding distance serves as a proxy for behavioral drift because it captures stylistic and rhetorical variation invisible to task-accuracy metrics.
- **Mechanism:** SBERT encodes sentences into dense vectors via siamese BERT networks trained on semantic similarity tasks. When model outputs to paraphrased prompts are embedded, cosine distance captures divergence in tone, structure, hedging, and framing—not just factual content. This makes it sensitive to prompt-induced behavioral shifts that accuracy metrics miss.
- **Core assumption:** SBERT's semantic similarity training transfers to detecting rhetorical inconsistency in LLM outputs.
- **Evidence anchors:**
  - [Section 3.3] "PBSS quantifies output consistency by projecting model responses into a semantic embedding space using S-BERT, which captures sentence-level meaning beyond surface variation."
  - [Section 5.4] "Across encoders, we observe consistent model rankings and curve shapes... This consistency—across encoder architectures and temperature regimes—suggests that PBSS captures model-internal regularities."
  - [corpus] "Mapping from Meaning" (arXiv:2510.17028) addresses prompt sensitivity but does not validate SBERT specifically for behavioral drift detection.
- **Break condition:** If different SBERT variants produce inconsistent model rankings or uncorrelated CDF shapes, the embedding-proxy mechanism lacks robustness.

## Foundational Learning

- **Concept: Cosine distance in embedding space**
  - **Why needed here:** PBSS measures drift as 1 - cosine_similarity between output embeddings; understanding this metric is essential to interpret heatmaps and CDFs.
  - **Quick check question:** If two outputs have cosine similarity 0.85, what is their PBSS drift score?

- **Concept: Tokenization as non-injective mapping**
  - **Why needed here:** The paper's core premise is that different token sequences can preserve meaning but induce different model behaviors.
  - **Quick check question:** Why might "don't" and "do not" produce different token sequences in BPE tokenization?

- **Concept: Kruskal-Wallis H-test**
  - **Why needed here:** Statistical validation that drift differences between model families are significant, not sampling artifacts.
  - **Quick check question:** What does a p-value < 0.01 in Kruskal-Wallis tell you about group distributions?

## Architecture Onboarding

- **Component map:** Prompt Generator -> Semantic Validator -> LLM Inference Layer -> Embedding Module -> Drift Calculator -> Visualization/Analysis
- **Critical path:** Prompt semantic validation → controlled paraphrase generation → multi-temperature inference → embedding projection → pairwise drift computation → statistical testing
- **Design tradeoffs:**
  - SBERT encoder choice: Deeper encoders (MPNet) capture more semantic nuance but may conflate stylistic variation with meaning drift; lighter encoders (MiniLM-L6) are faster but may miss subtle differences.
  - Temperature settings: Low T (0.2) isolates tokenization effects; high T (1.3) reveals drift under stochastic decoding but adds noise.
  - Prompt set size: 15 variants balances coverage with computational cost; may miss edge-case paraphrases.
- **Failure signatures:**
  - Flat CDF curves: Model produces near-identical outputs regardless of prompt—may indicate template collapse or over-regularization.
  - Encoder disagreement: Different SBERT variants produce inconsistent model rankings—suggests drift signal is weak or embedding-dependent.
  - High variance within task clusters: t-SNE shows fragmented origin-space clusters—indicates task framing doesn't stabilize behavior.
- **First 3 experiments:**
  1. **Baseline validation:** Run PBSS on GPT-2 and GPT-3.5-Turbo with 3 tasks, MiniLM-L6 encoder, T=0.2. Confirm CDF separation matches paper's Figure 5-6 pattern.
  2. **Encoder sensitivity test:** Re-run with MPNet encoder on same models/tasks. Compare model rankings—if consistent, proceed; if divergent, investigate embedding granularity effects.
  3. **Temperature robustness check:** Compare drift distributions at T=0.2 vs T=1.3 for a single model. If high-T distributions show similar relative rankings with added variance, the tokenization mechanism is decoupled from sampling noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does high behavioral drift under benign paraphrasing correlate with increased vulnerability to adversarial jailbreak attacks?
- **Basis in paper:** [explicit] The authors state in Section 7.2 that "future work could explore whether high PBSS drift correlates with jailbreak susceptibility," suggesting that high-drift regions might overlap with security weaknesses.
- **Why unresolved:** The current study deliberately focused on non-adversarial, semantically equivalent prompts to establish a baseline for QoS consistency without triggering safety refusals.
- **What evidence would resolve it:** A study measuring PBSS scores on standard instruction sets versus success rates of known jailbreak techniques across the same model architectures.

### Open Question 2
- **Question:** How does behavioral drift manifest in multi-turn or role-adaptive conversational contexts compared to the single-turn settings tested?
- **Basis in paper:** [explicit] Section 8 notes that "Studying [drift] in multi-turn or role-adaptive settings could expose how models encode rhetorical context over time," implying the current single-turn analysis may miss longitudinal effects.
- **Why unresolved:** The experimental setup was restricted to constrained, single-turn tasks to isolate the immediate effects of token-level realization on output stability.
- **What evidence would resolve it:** Extending the PBSS framework to track semantic drift across multi-turn dialogues where the prompt is rephrased or the persona shifts midway through the interaction.

### Open Question 3
- **Question:** Can the observed "phase boundary" between legacy and instruction-tuned models be attributed to specific architectural differences or training objectives?
- **Basis in paper:** [inferred] While the paper observes a "behavioral phase shift" (Section 5.5) separating GPT-2/Neo from LLaMA/GPT-3.5, it stops short of isolating the specific mechanisms (e.g., RLHF, tokenization, attention patterns) that cause this stability.
- **Why unresolved:** The paper functions as a diagnostic framework rather than a mechanistic interpretability study, focusing on quantifying the *existence* of drift rather than its internal *cause*.
- **What evidence would resolve it:** Ablation studies or internal activation probing (logit lens) comparing how different training regimes (e.g., SFT vs. RLHF) respond to identical token-level perturbations.

## Limitations
- The paper relies on semantic embedding distance as a proxy for behavioral drift without validating whether cosine similarity between SBERT embeddings correlates with human judgments of output quality or task correctness.
- Temperature manipulation introduces confounding variables; low temperature (0.2) may amplify model biases and trigger repetitive patterns unrelated to prompt variation.
- The paper doesn't systematically separate tokenization-driven drift from temperature-induced artifacts or validate that PBSS detects drift that other metrics miss.

## Confidence
- **High Confidence:** The empirical observation that instruction-tuned models show lower drift than pre-alignment models is well-supported by the data and statistically significant.
- **Medium Confidence:** The mechanism linking tokenization to behavioral divergence is plausible but untested against human judgment; the SBERT embedding proxy is reasonable but unverified.
- **Low Confidence:** The claim that PBSS provides a "lightweight diagnostic for detecting prompt-induced behavioral variance" overstates the evidence; the paper shows PBSS varies but doesn't validate it against ground-truth reliability metrics.

## Next Checks
1. **Human validation study:** Recruit 3-5 domain experts to rate output pairs from paraphrased prompts on a 5-point scale for task completion quality and stylistic consistency. Correlate these ratings with PBSS scores to validate that embedding distance predicts meaningful behavioral differences rather than superficial variation.

2. **Ablation with task accuracy:** For each model and task, compute the variance in task-specific metrics (e.g., exact match for QA, code execution success for coding tasks) across paraphrased prompts. Compare whether PBSS variance correlates with performance variance beyond what random sampling would produce.

3. **Tokenization pathway analysis:** Using a transformer interpretability tool, visualize attention patterns for paraphrased prompts that produce divergent outputs. Identify whether specific attention heads or layer depths show systematic differences that correlate with PBSS scores, providing mechanistic evidence for the tokenization-drift hypothesis.