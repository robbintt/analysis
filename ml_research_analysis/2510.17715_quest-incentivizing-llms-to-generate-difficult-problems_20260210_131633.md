---
ver: rpa2
title: 'QueST: Incentivizing LLMs to Generate Difficult Problems'
arxiv_id: '2510.17715'
source_url: https://arxiv.org/abs/2510.17715
tags:
- problems
- problem
- data
- reasoning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QueST, a novel framework for generating challenging
  coding problems to advance reasoning in large language models. QueST combines difficulty-aware
  graph sampling with difficulty-aware rejection fine-tuning to train specialized
  generators that create problems more difficult than those produced by GPT-4o.
---

# QueST: Incentivizing LLMs to Generate Difficult Problems

## Quick Facts
- **arXiv ID**: 2510.17715
- **Source URL**: https://arxiv.org/abs/2510.17715
- **Reference count**: 40
- **Primary result**: 8B model trained on QueST data achieves SOTA among similarly sized models on LiveCodeBench

## Executive Summary
QueST introduces a novel framework for generating challenging coding problems to advance reasoning in large language models. The framework combines difficulty-aware graph sampling with difficulty-aware rejection fine-tuning to train specialized generators that create problems more difficult than those produced by GPT-4o. By generating over 100K challenging coding problems with long chain-of-thought solutions, QueST enables smaller models to achieve state-of-the-art performance on competitive coding benchmarks, approaching the capabilities of much larger models. The approach demonstrates that generating complex problems at scale offers an effective method for advancing coding and reasoning capabilities in LLMs.

## Method Summary
QueST generates challenging coding problems through a three-stage process: first, extracting concepts (topics and knowledge points) from seed problems and building a difficulty-aware concept graph; second, sampling concept combinations via random walks and generating candidate problems with difficulty estimation using solution disagreement (δ); third, rejection fine-tuning the generator on the hardest problems from each batch. The framework uses GPT-4o to generate test cases and solutions, computes difficulty through majority voting disagreement, and filters problems with excessive invalid test cases. The final dataset of 100K problems trains smaller models through supervised fine-tuning and reinforcement learning, achieving strong performance on LiveCodeBench and USACO benchmarks.

## Key Results
- 8B model trained on QueST data achieves SOTA among similarly sized models on LiveCodeBench
- QueST generators outperform GPT-4o at creating challenging problems
- Difficulty-aware sampling and rejection fine-tuning significantly improve generation quality
- Student models approach performance of much larger DeepSeek-R1-671B models

## Why This Works (Mechanism)

### Mechanism 1: Difficulty Estimation via Solution Disagreement
QueST uses disagreement among multiple LLM solutions as a proxy for problem difficulty. For each problem, M candidate solutions are sampled and executed on T test cases. The difficulty metric δ(q) = 1 - (1/T)Σ majority_voting_rate measures disagreement - higher δ indicates more divergent solutions and thus harder problems. This works because self-consistency correlates inversely with difficulty, though it may fail if solutions diverge due to ambiguity rather than genuine challenge.

### Mechanism 2: Rejection Fine-Tuning with Difficulty Selection
The generator is trained on only the hardest problem from each K-sample batch, where hardness is determined by maximum δ. This selective retention forces the generator to learn what makes problems difficult rather than overfitting to specific problem types. The approach successfully creates generators that outperform general-purpose models at generating challenging problems, though it may lead to gaming the difficulty metric rather than creating genuinely hard problems.

### Mechanism 3: Difficulty-Aware Concept Graph Sampling
Concept graph edges are weighted by both co-occurrence frequency and problem difficulty, with α=0.2 prioritizing difficulty over frequency. Random walks over this graph prefer paths through difficult concept pairs, leading to better training prompts. This upweights rare, hard concepts like "knapsack problem" and "prime factorization" while downweighting basic I/O handling. The approach may be biased if difficulty labels in seed data don't reflect true algorithmic challenge.

## Foundational Learning

- **Concept: Self-Consistency and Majority Voting**
  - **Why needed here**: The difficulty metric δ depends on understanding why disagreement among sampled solutions correlates with problem hardness.
  - **Quick check question**: If you sample 8 solutions and 6 agree on the same output, is δ high or low? What does that suggest about difficulty?

- **Concept: Rejection Sampling for Training Data Curation**
  - **Why needed here**: QueST doesn't use all generated problems - understanding selective retention based on quality signals is core to the method.
  - **Quick check question**: Why might training on only the hardest samples be better than training on all samples, even if you lose data volume?

- **Concept: Knowledge Graph Construction and Random Walk Sampling**
  - **Why needed here**: The scaffolding layer uses concept graphs to generate prompts; understanding how edge weights influence traversal is essential.
  - **Quick check question**: If you increase α from 0.2 to 0.8 in Equation 10, does the graph favor more common concepts or more difficult concepts?

## Architecture Onboarding

**Component map:**
Concept Extractor -> Difficulty-Aware Graph Builder -> Prompt Sampler -> Problem Generator -> Test Case Generator -> Solution Sampler -> Difficulty Estimator -> Rejection Filter -> Fine-tuning Loop

**Critical path:**
Seed problems (TACO, 25K) → Concept extraction → Graph construction → Sample concepts → Generate 8 problems/prompt → Generate 20 test cases/problem → Generate 8 solutions/problem → Execute + compute δ → Select hardest → Fine-tune generator → Generate 100K final problems

**Design tradeoffs:**
- Computational cost vs signal quality: Difficulty estimation requires 8 solutions × 20 test cases per problem. Paper uses GPT-4o for both - expensive but reliable.
- α hyperparameter (0.2): Low value prioritizes difficulty over frequency. May under-sample common but useful concept combinations.
- Generator size: Paper uses Qwen2.5-14B-Instruct as generator. Smaller models may struggle with instruction following for problem generation.

**Failure signatures:**
- δ gaming: Generated problems become ambiguous rather than difficult (solutions diverge for wrong reasons)
- Concept drift: Generator overfits to specific concept combinations, losing diversity
- Test case validity: If >50% of test cases return None, problem is filtered (Section 2.2). High filter rate suggests test case generator issues
- Contamination: Paper checks 50-gram Jaccard similarity = 0. Implement similar check.

**First 3 experiments:**
1. **Validate δ as difficulty proxy**: Generate 50 problems, manually annotate difficulty, correlate with δ. Table 2 provides weak validation via downstream performance, but direct correlation analysis is missing.
2. **Ablate generator backbone**: Compare Qwen2.5-14B vs Qwen2.5-7B vs GPT-4o as generators with identical QueST training. Table 5 shows partial comparison but not systematic.
3. **Scale analysis**: Train student models on 10K, 50K, 100K QueST problems to verify scaling benefits. Paper shows 100K results but not intermediate checkpoints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can QueST's generator be trained using reinforcement learning with real-time difficulty rewards instead of rejection fine-tuning?
- Basis in paper: [explicit] The authors state in the Limitations section: "our current difficulty calculation is computationally expensive and challenging to implement in real-time to provide difficulty rewards in an RL pipeline." They suggest "directly training a reward model to predict difficulty" as future work.
- Why unresolved: Current difficulty estimation requires generating 8 responses and 20 test cases per problem, executing them, and computing majority voting - too slow for online RL.
- What evidence would resolve it: A learned difficulty predictor that correlates strongly with δ and enables stable RL training of the generator within comparable compute budgets.

### Open Question 2
- Question: How well does the majority-voting difficulty metric δ correlate with actual human-judged problem difficulty?
- Basis in paper: [inferred] The paper uses δ as a proxy for difficulty, motivated by the link between self-consistency and accuracy from Wang et al. (2023a). However, the correlation between δ and human difficulty ratings is not directly evaluated.
- Why unresolved: While δ successfully selects beneficial training data, the paper does not include a human evaluation or comparison against human difficulty labels on generated problems.
- What evidence would resolve it: A correlation study between δ scores and human expert difficulty ratings (e.g., using a 1-5 scale) on a sample of generated problems.

### Open Question 3
- Question: Does the QueST framework generalize effectively to mathematical reasoning and other non-coding domains?
- Basis in paper: [explicit] The authors claim "other forms of reasoning tasks (e.g., mathematical reasoning) can be regarded as special cases of coding tasks" but only evaluate on coding benchmarks (LiveCodeBench, USACO).
- Why unresolved: The scaffolding is adapted from MathScale (a math-focused method), but the trained generator and difficulty-aware sampling are only validated on code; performance on AIME, MATH, or similar benchmarks remains untested.
- What evidence would resolve it: Experiments applying QueST-generated math problems to train models evaluated on MATH, AIME, or GSM8K, compared against math-specific baselines.

### Open Question 4
- Question: What is the optimal trade-off between synthetic challenging problems and human-written problems in training data composition?
- Basis in paper: [inferred] The best results (QueST-8B) use a combination of 100K synthetic and 112K human-derived examples. However, the paper does not systematically vary the ratio to determine whether more synthetic data could further improve performance or if human data remains essential.
- Why unresolved: Only one combined configuration is reported; the contribution of each data source in the merged setting is partially ablated but not fully explored across different ratios.
- What evidence would resolve it: A sweep of training mixtures (e.g., 0%/100%, 25%/75%, 50%/50%, 75%/25%, 100%/0% synthetic/human) with controlled total sample counts, reporting benchmark performance.

## Limitations
- Difficulty estimation is computationally expensive and may not correlate directly with human-perceived difficulty
- Generator may learn to game the difficulty metric rather than create genuinely challenging problems
- Test case generation quality is not systematically evaluated, with high filtering rates suggesting potential issues

## Confidence

**High Confidence Claims:**
- QueST framework successfully generates 100K coding problems with long chain-of-thought solutions
- Student models trained on QueST data show strong performance on LiveCodeBench and USACO
- Difficulty-aware components (graph sampling, rejection fine-tuning) improve generation quality

**Medium Confidence Claims:**
- δ(q) serves as reliable difficulty estimator
- Rejection fine-tuning generalizes beyond specific concept combinations
- Test case generation via GPT-4o produces sufficient coverage

**Low Confidence Claims:**
- Generated problems are universally "challenging" (not just harder than baselines)
- 100K problems are sufficient for saturating model capabilities
- Downstream performance gains are primarily due to problem difficulty vs. data volume

## Next Checks

1. **Direct Difficulty Validation Study**: Generate 100 QueST problems and have human experts rate difficulty (1-5 scale). Compute Pearson correlation between human ratings and δ to validate whether the disagreement metric actually captures algorithmic complexity rather than ambiguity or poor test case design.

2. **Generator Diversity Analysis**: Track concept combination diversity during rejection fine-tuning. Plot the number of unique concept pairs vs. training steps. If diversity drops sharply, the generator may be overfitting to specific patterns rather than learning to create challenging problems generally.

3. **Scale-Aware Performance Evaluation**: Train identical student models on 10K, 25K, 50K, and 100K QueST problems. Plot performance vs. data volume on LiveCodeBench to reveal whether the 100K dataset is optimal or if performance continues scaling, and whether smaller datasets provide reasonable alternatives given computational constraints.