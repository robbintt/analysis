---
ver: rpa2
title: Controlling Large Language Model with Latent Actions
arxiv_id: '2503.21383'
source_url: https://arxiv.org/abs/2503.21383
tags:
- action
- drawer
- latent
- cola
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Controlling Large Language Models with Latent
  Actions (CoLA), a framework that addresses the inefficiency of reinforcement learning
  in large language models by replacing token-level actions with a compact latent
  action space. CoLA learns an inverse dynamics model to extract latent actions from
  token sequences, integrates them into a pre-trained LLM via a merge module, and
  uses a policy model to generate actions.
---

# Controlling Large Language Model with Latent Actions

## Quick Facts
- **arXiv ID:** 2503.21383
- **Source URL:** https://arxiv.org/abs/2503.21383
- **Reference count:** 40
- **Primary result:** Replaces token-level RL actions with compact latent actions, improving efficiency and semantic diversity in LLM adaptation

## Executive Summary
This paper introduces CoLA, a framework that replaces inefficient token-level actions in LLM reinforcement learning with a compact latent action space. By learning an inverse dynamics model to extract discrete actions from token sequences and conditioning the LLM via a merge module, CoLA enables more efficient exploration, maintains alignment better than baselines, and demonstrates improved downstream task performance while reducing reward hacking.

## Method Summary
CoLA trains an Inverse Dynamics Model (IDM) to extract discrete latent actions from token transitions, then conditions a pre-trained LLM (World Model) on these actions through a merge module. The policy model generates actions, and during RL only the policy is updated while the world model remains frozen. This approach combines pre-training on large corpora, behavior cloning of the IDM, fine-tuning under action guidance, and RL optimization of the policy model.

## Key Results
- Math500 score of 42.4 vs. 38.2 for baseline
- 68.2 accuracy with MCTS-Q on math tasks
- Halved computation time in enhanced thinking prompt tasks
- Reduced reward hacking compared to standard RL approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Compressing the action space into discrete latent codes improves RL exploration efficiency and semantic diversity compared to token-level actions.
- **Mechanism:** CoLA decouples high-level decision-making from low-level token prediction. Instead of searching a vocabulary of 128k+ tokens, the policy model searches a compact codebook (size N=64 in the paper). The Inverse Dynamics Model (IDM) extracts these actions by observing the transition from history x_{1:t} to future x_{t+1}, effectively clustering semantic transitions into discrete codes.
- **Core assumption:** The assumption is that meaningful semantic shifts in text can be compressed into a small discrete space without critical loss of expressiveness, and that these "latent actions" generalize across different contexts.
- **Evidence anchors:**
  - [Abstract] "CoLA's latent actions enable greater semantic diversity... halve computation time."
  - [Section 4.2] Shows "Random Action Sampling" achieves higher diversity scores than random token sampling.
  - [Corpus] Neighbor "Co-Evolving Latent Action World Models" supports the general validity of latent action spaces for world models, though primarily in video/robotics domains.

### Mechanism 2
- **Claim:** Conditioning the language model (World Model) on latent actions reduces prediction uncertainty and mitigates "forgetting" during fine-tuning.
- **Mechanism:** The "Merge Module" injects the latent action vector into the LLM embeddings. During Fine-Tuning under Action Guidance (FTA), the model is conditioned on the optimal action for a target response. This theoretically reduces the entropy of the prediction task—instead of guessing the next token from the full distribution, the model narrows its prediction conditioned on the high-level intent provided by the action.
- **Core assumption:** The paper assumes that conditioning on the latent action acts as a "guide" that preserves pre-trained knowledge better than standard Supervised Fine-Tuning (SFT), which relies solely on the token context.
- **Evidence anchors:**
  - [Section 3.2] "FTA-I can effectively retain the knowledge of the pre-trained model."
  - [Appendix D.4] "Prediction loss [uncertainty] is 0.45... much lower than... 1.77" for the base model.

### Mechanism 3
- **Claim:** Freezing the World Model during RL updates prevents "alignment tax" (capability degradation) and reward hacking.
- **Mechanism:** CoLA freezes the parameters of the large base model (θ_{base}) and merge module during the RL phase, updating only the lightweight Policy Model (θ_{policy}). Because the generative engine (the World Model) is immutable, the policy cannot "hack" the reward model by distorting the underlying language mechanics.
- **Core assumption:** This relies on the assumption that the pre-trained World Model is sufficiently robust to realize the "intent" of any valid latent action provided by the Policy Model.
- **Evidence anchors:**
  - [Section 4.5] "Baseline completely failed [with KL=0], implying reward hacking... CoLA is more robust."
  - [Figure 5a] Shows CoLA maintaining capabilities where baseline degrades.

## Foundational Learning

- **Concept:** Inverse Dynamics Modeling (IDM)
  - **Why needed here:** Unlike standard RL where actions are known, LLM text datasets contain only observations (tokens). You must learn the function f(s_t, s_{t+1}) → a_t to label the data for training.
  - **Quick check question:** Given two consecutive frames of text, can you predict the vector difference that connects them?

- **Concept:** Gumbel-Softmax & Straight-Through Estimators
  - **Why needed here:** The paper uses a discrete codebook. To backpropagate gradients through a discrete sampling step (choosing an action), you need a differentiable approximation.
  - **Quick check question:** How can you calculate the gradient of a function that outputs integers (0 or 1)?

- **Concept:** KL Divergence Constraints in RLHF
  - **Why needed here:** To understand the baseline problem. Standard RLHF penalizes deviation from the initial model to prevent "reward hacking." CoLA claims to reduce the need for this constraint.
  - **Quick check question:** What happens to a language model if you maximize reward without a KL penalty? (Answer: It often outputs repetitive nonsense or collapses to high-reward sequences).

## Architecture Onboarding

- **Component map:**
  - Base Model (Llama-3.1-8B) -> Merge Module (MLPs) -> World Model (LLM) -> Output
  - Policy Model (2B params) -> Codebook (64 codes) -> Merge Module
  - Inverse Dynamics Model (IDM) (1B params) -> Codebook (64 codes) -> Policy Model (training only)

- **Critical path:**
  1. Pre-training: Train IDM + Merge Module to reconstruct tokens from latent codes (Algorithm 1)
  2. Behavior Cloning: Train Policy to mimic IDM (Algorithm 1)
  3. RL: Freeze World Model; train Policy Model only to maximize reward via Algorithm 3 & 4

- **Design tradeoffs:**
  - VQ-VAE vs. Direct Assignment: The paper notes standard VQ-VAE "suffers from codebook collapse." They switch to "Direct Action Assignment" (Gumbel-Softmax) to ensure all codes are used.
  - FTA-I vs. FTA-P: Use FTA-I (Action from IDM) for diverse data to prevent overfitting; use FTA-P (Action from Policy) for narrow data (like math) to ensure the policy stays in sync.

- **Failure signatures:**
  - Codebook Collapse: Monitoring "Alive Action" count; if it drops to <10, the model is not using the latent space effectively
  - Sycophancy/Reward Hacking: If the model generates queries like "I would like to clarify..." inappropriately
  - Latency: The merge module adds overhead; ensure it is profiled

- **First 3 experiments:**
  1. Codebook Health Check: Train the IDM on a small subset and plot the histogram of action usage. Verify "Direct Assignment" prevents collapse better than standard VQ-VAE.
  2. Diversity Scaling: Replicate Figure 2. Compare semantic diversity of random latent action sampling vs. random token sampling to verify the space is expressive.
  3. Countdown Game (Efficiency): Implement the "thinking format" RL task. Verify that CoLA learns the format in ~10 steps while the baseline takes ~20 steps.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the CoLA framework generalize effectively to diverse LLM architectures beyond Llama-3.1-8B?
- **Basis in paper:** [Explicit] The conclusion states that the current work "requires broader comparisons due to the limitation of computation resources, such as the effectiveness across multiple base models."
- **Why unresolved:** Experiments primarily focused on Llama-3.1-8B, with only a small-scale validation on Qwen-2.5-Math-1.5B shown in the appendix.
- **What evidence would resolve it:** Applying CoLA to distinct architectures (e.g., Mamba, encoder-decoder models) and scales (70B+ parameters) to verify if efficiency gains persist.

### Open Question 2
- **Question:** What is the optimal size N for the discrete latent action codebook?
- **Basis in paper:** [Inferred] The paper fixes N=64 to compress the action space but does not ablate this hyperparameter to determine the trade-off point between compression and expressiveness.
- **Why unresolved:** While N=64 works, it is unclear if this size optimally balances the need for semantic diversity against the risk of vocabulary collapse.
- **What evidence would resolve it:** Ablation studies analyzing downstream task performance and "alive action" counts across a range of codebook sizes (e.g., N={16, 128, 256}).

### Open Question 3
- **Question:** Can the choice between Fine-Tuning under Action guidance (FTA-I vs. FTA-P) be automated?
- **Basis in paper:** [Inferred] Appendix B.1.2 heuristically states FTA-I is for diverse data and FTA-P for narrow data, but provides no quantitative threshold.
- **Why unresolved:** The selection relies on manual judgment regarding data distribution rather than a defined metric.
- **What evidence would resolve it:** Analysis correlating dataset entropy or distribution metrics with the performance delta between the two FTA methods.

## Limitations

- The framework requires significant pre-training computation (200B tokens) before RL can begin
- Generalization to architectures beyond Llama-3.1-8B remains untested
- The optimal codebook size (64) was empirically chosen without systematic ablation
- Efficiency gains are primarily in RL exploration steps, not total training cost

## Confidence

- **Latent Action Efficiency:** Medium Confidence - The theoretical mechanism is sound and supported by diversity metrics and computation time reductions, but absolute performance gains are modest
- **Preventing Reward Hacking:** High Confidence - Strong empirical evidence from ablation study showing baseline failure while CoLA succeeds
- **Maintaining Pre-trained Knowledge:** Medium Confidence - Supported by prediction loss comparison but would benefit from direct comparison with other fine-tuning methods

## Next Checks

1. **Architecture Generalization Test:** Replicate the CoLA framework with a different base LLM (e.g., Mistral 7B or a smaller Llama variant) to verify the method is not specific to Llama-3.1-8B's internal representations. Monitor codebook health and downstream task performance.

2. **Codebook Size Sensitivity Analysis:** Systematically vary the codebook size (e.g., 16, 32, 64, 128) on the same base model and task. Measure semantic diversity, task performance, and "alive action" count to identify if there's an optimal size range or if the space is robust to scaling.

3. **Reward Model Robustness Test:** Intentionally introduce noise or bias into the reward model (e.g., by corrupting a fraction of the labels or using an underspecified reward function). Compare CoLA's performance and output quality against a standard RL baseline to quantify the claimed robustness to suboptimal reward models.