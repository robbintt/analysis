---
ver: rpa2
title: Tensor Network Based Feature Learning Model
arxiv_id: '2512.02547'
source_url: https://arxiv.org/abs/2512.02547
tags:
- learning
- feature
- features
- data
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new tensor network based Feature Learning
  (FL) model for efficient large-scale supervised learning. The key idea is to represent
  tensor-product structured Fourier features as a learnable Canonical Polyadic Decomposition
  (CPD), enabling joint learning of feature hyperparameters and model parameters via
  Alternating Least Squares (ALS) optimization.
---

# Tensor Network Based Feature Learning Model

## Quick Facts
- arXiv ID: 2512.02547
- Source URL: https://arxiv.org/abs/2512.02547
- Authors: Albert Saiapin; Kim Batselier
- Reference count: 12
- Primary result: New tensor network feature learning model that trains 3-5x faster than cross-validated CPD kernel machines while achieving comparable prediction performance on large-scale regression tasks.

## Executive Summary
This paper introduces a tensor network based Feature Learning (FL) model for efficient large-scale supervised learning. The key innovation is representing tensor-product structured Fourier features as a learnable Canonical Polyadic Decomposition (CPD), enabling joint optimization of feature hyperparameters and model parameters via Alternating Least Squares (ALS). This approach eliminates expensive cross-validation for hyperparameter selection while exploiting tensor network structure for computational efficiency. The FL model is evaluated on multiple UCI regression datasets and one large-scale airline dataset, demonstrating significant speedups (3-5×) compared to cross-validated CPD kernel machines while maintaining comparable or better prediction accuracy.

## Method Summary
The FL model uses quantized Fourier features with CPD rank-R decomposition, where feature hyperparameters θ and model parameters are jointly learned via ALS optimization. The method alternates between updating the W^(d) cores (each core representing feature weights for dimension d) and the λ coefficients (weighting different feature combinations). The ALS update for W^(d) involves solving a regularized least squares problem using matrix A_d constructed from tensor contractions, while λ is updated using either L1 proximal gradient or L2 regularization. The model handles both small and large-scale problems effectively, with computational complexity of O(EDN IR(P+IR)) and peak memory complexity of O(N R(P+I)).

## Key Results
- FL achieves 3-5× faster training than cross-validated CPD kernel machines while maintaining comparable MSE
- On Airline dataset (5.9M samples): FL achieves test MSE of 0.804 in 15,159 seconds vs. 0.779 in 56,590 seconds for cross-validation
- Model handles datasets ranging from small (Yacht N=308) to large-scale (Airline N=5.9M) effectively
- Different regularization strategies (L1 and L2) for feature weights are explored and validated

## Why This Works (Mechanism)
The method works by replacing expensive cross-validation with joint optimization of feature hyperparameters and model parameters. By representing Fourier features in CPD form, the model can learn optimal frequency parameters θ directly from data rather than selecting them through grid search. The ALS optimization exploits the tensor network structure to compute updates efficiently without explicitly forming large feature matrices, enabling scalability to millions of samples. The quantized Fourier basis with tensor product structure allows capturing complex feature interactions while maintaining computational tractability.

## Foundational Learning
- **Alternating Least Squares (ALS)**: Optimization technique that alternates between fixing subsets of variables and solving for others. Needed for efficient joint optimization of CPD components. Quick check: Verify ALS updates converge monotonically.
- **Canonical Polyadic Decomposition (CPD)**: Tensor decomposition representing a tensor as sum of rank-1 components. Needed to represent feature weights in tensor form. Quick check: Ensure rank-R decomposition captures sufficient variance.
- **Fourier Features**: Mapping input data to high-dimensional feature space using sinusoidal functions. Needed to capture non-linear relationships. Quick check: Verify frequency parameters θ are learned effectively.
- **Tensor Networks**: Data structures exploiting multi-dimensional relationships for efficient computation. Needed to scale to large feature spaces. Quick check: Monitor memory usage vs. naive implementation.
- **Quantized Fourier Features**: Discretized frequency representation using tensor products. Needed for computational efficiency. Quick check: Verify I_d=2^K_d mapping is correct.

## Architecture Onboarding
- **Component Map**: Data → Quantized Fourier Features → Tensor Product → CPD Decomposition → ALS Optimization → Model Parameters
- **Critical Path**: ALS iteration → Update W^(d) cores → Update λ coefficients → Check convergence → Output model
- **Design Tradeoffs**: Joint optimization vs. cross-validation speed, tensor rank R vs. model capacity, L1 vs. L2 regularization for sparsity
- **Failure Signatures**: Non-converging ALS (check initialization), memory overflow (use streaming computation), poor accuracy (verify frequency learning)
- **First Experiments**:
  1. Yacht dataset (D=6, I=2, R=6, P=8): 10 epochs, 10 restarts, 80/20 split. Expected MSE ≈ 0.112, time ≈ 0.15s
  2. Airfoil dataset (N=1502, D=5): Standard parameters, verify convergence behavior
  3. Concrete dataset (N=1030, D=8): Test different regularization strategies and compare L1 vs. L2 performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited comparison to modern scalable kernel methods and deep learning approaches
- Critical implementation details (column normalization, stopping criteria, quantization mapping) not fully specified
- No theoretical guarantees on convergence rates or generalization bounds provided
- Performance sensitivity to initialization and hyperparameters not thoroughly explored

## Confidence
- Methodological claims: **High** confidence - ALS framework is mathematically sound with well-supported performance improvements
- Comparative analysis: **Medium** confidence - Limited baseline comparisons, only benchmarks against CPD kernel machines
- Reproducibility: **Low** confidence - Critical implementation details missing (normalization method, stopping criteria, quantization mapping)

## Next Checks
1. Implement Yacht dataset experiment (D=6, I=2, R=6, P=8) with 10 epochs and 10 restarts to verify MSE ≈ 0.112 and training time ≈ 0.15 seconds matches reported values.

2. Test Airline dataset with specified parameters (α=0.01, β=0.1, R=20) to verify test MSE ≈ 0.804 and training time ≈ 15,159 seconds on full 5.9M samples.

3. Validate ALS convergence behavior by monitoring loss decay across epochs for Concrete dataset (N=1030, D=8), ensuring monotonic decrease and checking numerical stability in matrix inversions.