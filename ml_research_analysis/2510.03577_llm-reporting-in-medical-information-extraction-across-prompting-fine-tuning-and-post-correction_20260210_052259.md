---
ver: rpa2
title: LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning
  and Post-correction
arxiv_id: '2510.03577'
source_url: https://arxiv.org/abs/2510.03577
tags:
- dans
- pour
- donn
- entit
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses biomedical named entity recognition (NER)
  and health event extraction in French using large language models (LLMs) under a
  few-shot learning setting. Three approaches are explored: (1) GPT-4.1 with in-context
  learning and annotation guidelines, (2) GLiNER fine-tuned on synthetic data with
  LLM-based post-correction, and (3) LLaMA-3.1-8B-Instruct fine-tuned on synthetic
  data.'
---

# LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction

## Quick Facts
- arXiv ID: 2510.03577
- Source URL: https://arxiv.org/abs/2510.03577
- Reference count: 0
- Primary result: GPT-4.1 achieved highest macro-F1 of 61.53% for NER and 15.02% for event extraction in French biomedical domain using few-shot learning

## Executive Summary
This study investigates medical information extraction from French clinical text using large language models under few-shot learning conditions. The research addresses biomedical named entity recognition and health event extraction tasks in a low-resource language setting where training data is limited. Three distinct approaches are evaluated: in-context learning with GPT-4.1 using annotation guidelines and few-shot examples, fine-tuning GLiNER on synthetic data with LLM-based post-correction, and fine-tuning LLaMA-3.1-8B-Instruct on synthetic data. Synthetic data augmentation plays a crucial role in expanding the limited training corpus. The study demonstrates that well-crafted prompting strategies with few-shot examples can achieve competitive performance even in resource-constrained scenarios.

## Method Summary
The study employs three complementary approaches to tackle biomedical NER and health event extraction in French clinical text. The first approach leverages GPT-4.1's in-context learning capabilities, providing annotation guidelines and few-shot examples directly in prompts to guide entity recognition and event extraction. The second approach involves fine-tuning GLiNER on synthetic data generated through LLM prompting, followed by post-correction using another LLM to refine outputs. The third approach fine-tunes LLaMA-3.1-8B-Instruct directly on synthetic data without additional post-processing. All methods utilize synthetic data augmentation to compensate for the limited size of the original training corpus, with the synthetic data generation process involving LLM-based prompting strategies to create diverse and representative training examples.

## Key Results
- GPT-4.1 achieved the highest macro-F1 score of 61.53% for biomedical named entity recognition in French clinical text
- GPT-4.1 achieved a macro-F1 score of 15.02% for health event extraction task
- Fine-tuned LLaMA-3.1-8B-Instruct achieved a macro-F1 of 51.46% for NER without post-correction
- GLiNER fine-tuned on synthetic data with LLM post-correction represents an intermediate approach between pure prompting and full fine-tuning

## Why This Works (Mechanism)
The effectiveness of the prompting approach stems from LLMs' ability to generalize from few examples when provided with clear annotation guidelines and well-structured prompts. GPT-4.1's strong few-shot learning capabilities allow it to adapt to the biomedical domain without extensive fine-tuning, leveraging its pre-existing knowledge encoded during training. The synthetic data augmentation strategy works by expanding the limited training corpus through LLM-generated examples, creating a more diverse training distribution that helps models generalize better. The post-correction mechanism in the GLiNER approach helps refine outputs by applying additional LLM reasoning to correct errors introduced during initial processing. The fine-tuning approaches benefit from task-specific adaptation while synthetic data provides sufficient volume to avoid overfitting on limited real data.

## Foundational Learning
- Few-shot learning: Why needed - enables adaptation to new tasks with minimal examples; Quick check - evaluate performance with varying numbers of shot examples
- Synthetic data augmentation: Why needed - addresses limited training data in low-resource languages; Quick check - measure domain alignment between synthetic and real data
- In-context learning: Why needed - allows model adaptation without parameter updates; Quick check - test different prompt structures and ordering
- Post-correction mechanisms: Why needed - improves model outputs through iterative refinement; Quick check - compare raw vs. post-corrected outputs
- Fine-tuning vs prompting trade-offs: Why needed - different approaches suit different resource constraints; Quick check - measure performance vs. computational cost
- Low-resource NLP challenges: Why needed - French biomedical domain lacks extensive annotated corpora; Quick check - evaluate domain adaptation effectiveness

## Architecture Onboarding

Component map: User Input -> Prompt Engineering -> LLM Processing -> Post-correction/Fine-tuning -> Output Generation

Critical path: Prompt creation and few-shot example selection represent the most critical components, as they directly determine model performance. The quality of synthetic data generation and post-correction steps also significantly impacts final results.

Design tradeoffs: The study balances between computational efficiency (prompting requires no fine-tuning) and potential performance gains from fine-tuning. Using synthetic data addresses data scarcity but introduces potential distribution shifts. The post-correction approach adds computational overhead but may improve accuracy.

Failure signatures: Poor prompt design leads to inconsistent entity recognition. Low-quality synthetic data results in poor generalization. Inadequate post-correction fails to address systematic errors. Over-reliance on few-shot examples without proper guidelines causes annotation drift.

First experiments: 1) Test different prompt structures with varying numbers of few-shot examples; 2) Evaluate synthetic data quality through human annotation comparison; 3) Measure the impact of post-correction on raw model outputs.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance metrics remain relatively low, indicating the inherent difficulty of the task
- Synthetic data augmentation may introduce distributional shifts and hallucination effects
- Limited size of original training corpus affects model generalization
- Absence of extensive ablation studies makes it difficult to isolate methodological contributions

## Confidence
- Performance metrics reliability: Medium - results show moderate effectiveness but limited by task difficulty
- Methodology validity: Medium - innovative approaches but lack of comprehensive validation framework
- Generalizability claims: Medium - French language specificity limits broader applicability
- Synthetic data quality: Medium - potential issues with domain alignment and hallucination

## Next Checks
1. Conduct a human evaluation study to assess the quality and clinical relevance of synthetic data generated for fine-tuning, measuring hallucination rates and domain accuracy.
2. Perform cross-linguistic validation by testing the prompting strategy on an English biomedical NER dataset to evaluate language-specific effects and generalizability.
3. Implement an ablation study comparing the performance of each component (few-shot examples, annotation guidelines, post-correction) in isolation to quantify their individual contributions to overall performance.