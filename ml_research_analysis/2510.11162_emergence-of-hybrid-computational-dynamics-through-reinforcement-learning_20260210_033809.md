---
ver: rpa2
title: Emergence of hybrid computational dynamics through reinforcement learning
arxiv_id: '2510.11162'
source_url: https://arxiv.org/abs/2510.11162
tags:
- learning
- neural
- dynamics
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different learning paradigms shape
  emergent computational strategies in recurrent neural networks (RNNs). The authors
  systematically compare reinforcement learning (RL) and supervised learning (SL)
  on identical context-dependent decision-making tasks, revealing that RL discovers
  fundamentally different dynamical solutions.
---

# Emergence of hybrid computational dynamics through reinforcement learning

## Quick Facts
- arXiv ID: 2510.11162
- Source URL: https://arxiv.org/abs/2510.11162
- Reference count: 40
- RL discovers hybrid attractor architectures combining fixed points and quasi-periodic dynamics for flexible evidence integration

## Executive Summary
This study systematically compares reinforcement learning (RL) and supervised learning (SL) in recurrent neural networks performing context-dependent decision-making tasks. The key finding is that RL spontaneously discovers fundamentally different computational strategies, favoring hybrid attractor architectures that combine stable fixed points for decision maintenance with quasi-periodic attractors for flexible evidence integration. This contrasts sharply with SL, which almost exclusively converges to stable fixed-point attractors. The RL-discovered hybrid dynamics are particularly advantageous for ambiguous, low-coherence stimuli where flexible evidence integration is critical.

The authors demonstrate that these differences are not merely implementation details but reflect the fundamental nature of the learning paradigms themselves. RL's reward-driven exploration autonomously discovers sophisticated dynamical mechanisms that are less accessible to direct gradient-based optimization. Additionally, RL promotes functionally balanced neural populations through implicit regularization, whereas SL produces more heterogeneous solutions. The prevalence of complex dynamics in RL can be controllably modulated by weight initialization and correlates strongly with performance gains, especially as task complexity increases.

## Method Summary
The study trains identical RNN architectures on context-dependent decision-making tasks using either reinforcement learning or supervised learning approaches. The networks process noisy stimulus inputs over time and must make decisions based on both stimulus evidence and contextual cues. Task difficulty is systematically varied through stimulus coherence levels, ranging from high-coherence (clear evidence) to low-coherence (ambiguous) stimuli. The authors analyze the emergent attractor landscapes by characterizing network dynamics through principal component analysis and examining fixed-point stability. Population-level analysis quantifies functional balance and heterogeneity across neural responses. Multiple random initializations and network instantiations ensure robust conclusions about learning paradigm effects.

## Key Results
- RL discovers hybrid attractor architectures combining stable fixed points with quasi-periodic dynamics, while SL converges almost exclusively to stable fixed points
- Complex dynamics emerge more frequently in RL, especially for ambiguous, low-coherence stimuli where flexible evidence integration is advantageous
- RL implicitly promotes functionally balanced neural populations through regularization, contrasting with SL's more heterogeneous solutions

## Why This Works (Mechanism)
The fundamental difference stems from the distinct optimization landscapes and exploration strategies of RL versus SL. RL's reward-driven learning creates an optimization surface that encourages diverse dynamical solutions, including those with complex, quasi-periodic components. This emerges because the reward signal provides global feedback about task performance rather than local gradient information about prediction errors. The exploration inherent in RL's learning process allows the network to discover attractor landscapes that combine the stability of fixed points for decision maintenance with the flexibility of quasi-periodic dynamics for evidence integration.

In contrast, SL's gradient-based optimization strongly biases toward local minima corresponding to stable fixed-point attractors. This occurs because SL directly minimizes prediction error through gradient descent, which naturally favors solutions with minimal dynamical complexity. The implicit regularization in RL, arising from the exploration of the reward landscape, promotes balanced neural populations where units contribute complementary computational roles. SL lacks this implicit regularization, allowing for more heterogeneous and potentially redundant representations to develop.

## Foundational Learning

**Recurrent Neural Networks (RNNs)** - Neural architectures with cyclic connections enabling memory and temporal processing. Essential for modeling sequential decision-making where past information influences current choices.

**Attractor Dynamics** - Stable states or patterns toward which network dynamics evolve over time. Critical for understanding how neural systems maintain information and make decisions through self-organizing computation.

**Reinforcement Learning (RL)** - Learning paradigm based on reward maximization through trial-and-error interaction with an environment. Provides global feedback signals that can discover diverse computational strategies.

**Supervised Learning (SL)** - Learning paradigm based on minimizing prediction error through gradient descent on labeled examples. Tends to find local minima corresponding to simple, stable solutions.

**Quasi-periodic Attractors** - Complex dynamical states that exhibit approximate periodicity with sensitive dependence on initial conditions. Enable flexible, context-dependent processing of ambiguous information.

**Functional Population Balance** - Distribution of computational roles across neural units where different populations contribute complementary functions. Important for efficient, robust information processing.

## Architecture Onboarding

**Component Map**: Input Layer -> Recurrent Hidden Layer -> Output Layer -> Reward Signal (RL) or Error Signal (SL)

**Critical Path**: Stimulus input flows through recurrent dynamics where attractor states form, ultimately producing decision outputs that generate either reward or error signals for learning

**Design Tradeoffs**: RL prioritizes exploration and reward maximization, potentially discovering complex dynamics at the cost of training efficiency; SL prioritizes error minimization, finding simpler solutions more quickly but potentially missing sophisticated computational strategies

**Failure Signatures**: SL networks get stuck in stable fixed points even when flexible evidence integration would improve performance; RL networks may exhibit unstable or overly complex dynamics that reduce generalization

**First Experiments**: 1) Train identical networks on high-coherence stimuli to establish baseline performance differences, 2) Systematically vary stimulus coherence to test performance on ambiguous inputs, 3) Analyze attractor landscapes through PCA to characterize dynamical differences between learning paradigms

## Open Questions the Paper Calls Out
None specified in the provided analysis.

## Limitations
- Findings primarily demonstrated in relatively small-scale RNNs, raising questions about scalability to larger networks and more complex cognitive domains
- Analysis focuses on attractor dynamics and may not capture other computational strategies like transient coding schemes
- Correlation between complex dynamics and performance gains established, but causal mechanisms remain incompletely characterized
- Conclusions about functional population balance based on specific metrics that may not encompass all relevant aspects of neural representation

## Confidence
- High confidence in systematic differences between RL and SL attractor architectures
- Medium confidence in correlation between complex dynamics and performance gains
- Medium confidence in characterization of functional population balance differences

## Next Checks
1. Test scalability of hybrid attractor architectures to larger networks (100+ units) and more complex cognitive tasks to assess whether RL advantage persists
2. Conduct ablation studies removing quasi-periodic components to establish causal links between complex dynamics and improved performance on ambiguous stimuli
3. Investigate alternative regularization strategies in SL to determine if complex dynamics can be artificially induced, clarifying whether RL's advantage is algorithmic or implementational