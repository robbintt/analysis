---
ver: rpa2
title: 'AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations'
arxiv_id: '2511.17747'
source_url: https://arxiv.org/abs/2511.17747
tags:
- identity
- avatar
- aegis
- masking
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AEGIS, the first viewpoint-consistent identity
  masking method for 3D Gaussian avatars. Unlike 2D masking methods, AEGIS applies
  adversarial perturbations directly to the DC color coefficients of 3D Gaussian primitives,
  preserving geometry and view-dependent appearance while ensuring consistent protection
  across all viewpoints.
---

# AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations

## Quick Facts
- arXiv ID: 2511.17747
- Source URL: https://arxiv.org/abs/2511.17747
- Reference count: 40
- Primary result: First viewpoint-consistent identity masking for 3D Gaussian avatars using DC coefficient perturbations

## Executive Summary
AEGIS introduces a novel approach for viewpoint-consistent identity masking of 3D facial avatars using adversarial perturbations on DC color coefficients of Gaussian primitives. The method achieves complete de-identification (0% verification accuracy) while preserving geometric structure and view-dependent appearance. By restricting perturbations to the DC (zeroth-order) spherical harmonics coefficients and leaving geometry, opacity, and higher-order SH coefficients untouched, AEGIS maintains high visual fidelity (SSIM = 0.9555, PSNR = 35.52 dB) while ensuring privacy protection across all viewing angles.

## Method Summary
AEGIS applies adversarial optimization to the DC coefficients of 3D Gaussian primitives using Projected Gradient Descent (PGD) with l∞ constraints. The method renders the avatar from multiple viewpoints, extracts face embeddings via verification networks (ArcFace and AdaFace), and optimizes DC coefficients to minimize similarity to the reference identity. A cross-entropy loss pushes the masked avatar's embedding away from the reference in the embedding space. The ℓ∞ constraint ε bounds per-channel color changes, preventing visual artifacts. Expectation Over Transformation (EOT) over 5 viewpoints ensures consistent protection across camera angles. The approach preserves geometry and higher-order SH coefficients encoding view-dependent effects while achieving complete de-identification.

## Key Results
- Achieves complete de-identification with 0% face retrieval and verification accuracy
- Maintains high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB)
- Preserves key facial attributes including age, race, gender, and emotion
- Provides consistent protection across all viewpoints without per-view re-application

## Why This Works (Mechanism)

### Mechanism 1: DC Coefficient Perturbation Preserves Geometry While Disrupting Identity
Restricting adversarial perturbations to DC coefficients encodes view-independent base color while leaving Gaussian positions, scale, rotation, opacity, and higher-order SH coefficients untouched. This preserves the avatar's 3D structure and view-dependent effects while disrupting identity-relevant color patterns. Face verification systems primarily rely on color/texture rather than pure geometry for identity matching.

### Mechanism 2: Differentiable Rendering Enables Gradient Flow Through 3D-to-2D Projection
The differentiable rendering pipeline allows face verification gradients to propagate back through the 3D representation, enabling targeted optimization in parameter space. The rendering function produces a 2D image from 3D Gaussians, which passes through alignment and verification networks. Gradients from the cross-entropy loss flow back through this chain, indicating which DC coefficients most affect identity similarity.

### Mechanism 3: EOT Over Viewpoints Ensures Viewpoint-Consistent Protection
Sampling multiple viewpoints during optimization via Expectation Over Transformation ensures the mask generalizes across camera angles. K=5 camera viewpoints are uniformly sampled from pitch/yaw ranges [-0.5, 0.5] radians. The loss is estimated over these samples, producing gradients that improve protection across the distribution rather than overfitting to a single view.

## Foundational Learning

- **Concept: 3D Gaussian Splatting primitives**
  - Why needed here: The method operates on 3DGS representations where each primitive is defined by position μ, covariance Σ, opacity σ, and spherical harmonics color coefficients C. Understanding this parameterization is essential for knowing what can and cannot be perturbed.
  - Quick check question: What five parameters define each Gaussian primitive, and which does AEGIS modify?

- **Concept: Spherical Harmonics (DC vs AC coefficients)**
  - Why needed here: AEGIS targets only DC (zeroth-order) coefficients while preserving AC (higher-order) coefficients. DC encodes view-independent base color; AC encodes view-dependent appearance (specular highlights, shading variations).
  - Quick check question: Why would perturbing AC coefficients degrade visual quality more than perturbing DC coefficients?

- **Concept: Constrained optimization with l∞ norm**
  - Why needed here: The privacy budget ε constrains the maximum per-channel color change via ||C - C₀||∞ ≤ ε, preventing unrealistic color artifacts. The projection operator φ clips values back into this bound after each update.
  - Quick check question: After a PGD update step C(t) + α·sign(∇L), what does the projection operator do?

## Architecture Onboarding

- **Component map:**
  Input: Trained 3D Gaussian Avatar (GaussianAvatars format with FLAME binding)
  ↓
  Reference embedding extraction: Render frontal view → RetinaFace alignment → ArcFace/AdaFace → e_r
  ↓
  PGD Optimization Loop (300 iterations):
     Sample K=5 viewpoints ~ Uniform[-0.5, 0.5] rad (pitch/yaw)
     For each viewpoint: Render → Align → Extract embedding e_k
     Compute average cosine similarity s̄ with reference e_r
     Compute cross-entropy loss with logits [-s̄λ, s̄λ] targeting "no match"
     Backpropagate to DC coefficients: C(t+1) = φ(C(t) + α·sign(∇L))
     Project to l∞ ball: clip to [C₀-ε, C₀+ε]
  ↓
  Output: Masked avatar with optimized DC coefficients C* replacing C₀

- **Critical path:**
  1. Differentiable rendering correctness—gradients must accurately reflect how DC coefficient changes affect rendered pixels
  2. Face alignment module—must be differentiable or pre-computed; RetinaFace landmarks feed into alignment transform
  3. l∞ constraint enforcement—projection after each step is essential to maintain visual fidelity bounds

- **Design tradeoffs:**
  - ε=0.1: Complete de-identification against AdaFace, high fidelity (SSIM 0.9555), but requires higher ε for ArcFace
  - ε=0.2: Stronger cross-system transfer and consistent protection across extreme poses, but lower SSIM (0.88)
  - Full-face masking vs. region-specific (EFN): Region-specific requires larger ε for equivalent privacy, suggesting identity information is distributed

- **Failure signatures:**
  - Identity leakage at extreme poses: Indicates ε too low or training viewpoint distribution too narrow
  - Visible color artifacts: Indicates ε too high or projection not being applied correctly
  - High verification rate despite low Rank-1: Mask disrupts unique identification but verification threshold is too permissive
  - Poor transfer to unseen verification model: Suggests overfitting to surrogate model's embedding geometry

- **First 3 experiments:**
  1. Reproduce the ε sweep (0.05, 0.1, 0.2, 0.3) on a single avatar, plotting Rank-1, Match Rate, and SSIM to validate the privacy-utility tradeoff curve.
  2. Test cross-system transfer: Optimize mask against ArcFace, evaluate against AdaFace (and vice versa) to characterize generalization asymmetry.
  3. Ablate viewpoint sampling: Reduce K from 5 to 1, then increase to 10, measuring pose-consistency of protection (variance of similarity across held-out poses).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the method ensure robust, symmetric transferability between different face verification models (e.g., ArcFace and AdaFace)? The authors note "cross-system transfer remains asymmetric" and call for "future validation across diverse avatar systems... to improve transferability." Masks trained on AdaFace require larger perturbations to fool ArcFace, likely due to differing margin formulations, which degrades visual quality.

- **Open Question 2:** Can the restriction to DC coefficients be relaxed to eliminate residual identity leakage at extreme poses? The Limitations section states that masking only DC coefficients "leaves minor residual identity cues at extreme poses." Perturbing geometry or higher-order harmonics was avoided to preserve the avatar's structural integrity and view-dependent appearance.

- **Open Question 3:** Is AEGIS applicable to non-Gaussian avatar representations like NeRFs or meshes? The problem scope includes "photorealistic 3D facial avatars" generally, but the implementation is tied to Gaussian primitives. The optimization targets specific DC color tensors of Gaussian primitives, which do not exist in implicit (NeRF) or mesh-based rendering pipelines.

## Limitations

- Privacy guarantees evaluated only on limited test set (10 subjects from NeRSemble), raising questions about robustness to broader identity distributions
- Cross-verification between ArcFace and AdaFace demonstrates some transfer, but lack of testing against black-box or unknown models leaves true de-identification boundary unclear
- ℓ∞ constraint prevents visible artifacts but may limit the perturbation's ability to disrupt subtle identity cues encoded in higher-order SH coefficients or geometric features

## Confidence

- **High Confidence**: The mechanism of perturbing DC coefficients while preserving geometry and AC coefficients is clearly specified and technically sound. The PGD optimization procedure with EOT over viewpoints is well-defined.
- **Medium Confidence**: The privacy-utility tradeoff results (SSIM, PSNR, de-identification rates) are well-documented on the test set, but generalization to broader populations and unseen verification models is uncertain.
- **Low Confidence**: The assumption that identity is primarily encoded in color/texture rather than geometry is stated but not empirically validated. The method's robustness against sophisticated reconstruction attacks is not addressed.

## Next Checks

1. **Cross-Verification Generalization**: Evaluate optimized masks against a third, unseen face verification network (e.g., FaceNet or VGGFace) to assess true black-box transferability and potential model-specific overfitting.

2. **Geometric Vulnerability Analysis**: Measure face verification performance when perturbing geometric parameters (positions, scales) alongside color to determine if DC-only perturbations are sufficient or if geometric features contribute significantly to identity.

3. **Adversarial Robustness**: Apply reconstruction attacks (e.g., optimization-based identity recovery) to masked avatars to assess whether the perturbations withstand attempts to invert or weaken the privacy protection.