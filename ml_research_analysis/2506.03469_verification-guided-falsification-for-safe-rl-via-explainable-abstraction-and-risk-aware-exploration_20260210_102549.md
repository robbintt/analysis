---
ver: rpa2
title: Verification-Guided Falsification for Safe RL via Explainable Abstraction and
  Risk-Aware Exploration
arxiv_id: '2506.03469'
source_url: https://arxiv.org/abs/2506.03469
tags:
- safety
- falsification
- policy
- violations
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring the safety of reinforcement
  learning (RL) policies in high-stakes environments by integrating formal verification
  with interpretable policy abstraction and risk-aware falsification. The core method,
  CAPS (Comprehensible Abstract Policy Summarization), generates a human-interpretable
  graph from offline trajectories, which is then verified using Storm probabilistic
  model checking against PCTL safety specifications.
---

# Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration

## Quick Facts
- arXiv ID: 2506.03469
- Source URL: https://arxiv.org/abs/2506.03469
- Authors: Tuan Le; Risal Shefin; Debashis Gupta; Thai Le; Sarra Alqahtani
- Reference count: 34
- Primary result: Integration of formal verification with interpretable policy abstraction and risk-aware falsification to detect safety violations in RL policies

## Executive Summary
This paper addresses the challenge of ensuring safety in reinforcement learning (RL) policies for high-stakes environments by integrating formal verification with interpretable policy abstraction and risk-aware falsification. The proposed framework, CAPS (Comprehensible Abstract Policy Summarization), generates a human-interpretable graph from offline trajectories, which is verified using Storm probabilistic model checking against PCTL safety specifications. When no violations are found, the framework performs risk- and uncertainty-guided falsification to explore high-risk and underrepresented states, using a risk critic and ensemble-based epistemic uncertainty to detect novel safety violations. The approach also includes a runtime safety shield that switches to a fallback policy when risk exceeds a threshold. Experiments across navigation, maze, and medical (insulin dosing) domains demonstrate superior safety violation detection compared to uncertainty-based and fuzzy-based search methods.

## Method Summary
The framework operates through a verification-guided falsification loop that combines formal methods with RL safety analysis. First, offline trajectories are processed to generate an interpretable graph abstraction (CAPS) representing the policy's behavior. This abstraction is formally verified using Storm probabilistic model checking against PCTL safety specifications to identify potential violations. If no violations are found, the framework performs risk-aware falsification using a risk critic and ensemble-based epistemic uncertainty to explore high-risk and novel states. The approach includes a runtime safety shield that monitors risk levels and switches to a fallback policy when thresholds are exceeded. The method provides PAC-style completeness guarantees under abstraction and data limitations, ensuring that safety violations are detected with high probability when they exist.

## Key Results
- The framework detects significantly more safety violations than uncertainty-based and fuzzy-based search methods across multiple domains
- Discovered counterexamples show higher diversity and novelty in safety-critical states
- Maintains efficiency and interpretability while providing formal verification guarantees
- Runtime safety shield effectively monitors and responds to risk threshold violations

## Why This Works (Mechanism)
The framework works by combining formal verification's mathematical rigor with falsification's exploration capabilities. The interpretable graph abstraction enables human-understandable policy analysis while supporting formal verification through Storm model checking. Risk-aware falsification fills gaps in the abstraction by exploring high-risk and underrepresented states using both risk critics and epistemic uncertainty measures. This dual approach ensures both verification completeness under abstraction limitations and discovery of novel violations through targeted exploration. The runtime shield provides practical safety guarantees during deployment by monitoring risk and enabling policy switching.

## Foundational Learning
- **Formal verification using PCTL**: Mathematical framework for specifying and checking safety properties; needed for rigorous safety guarantees; quick check: verify simple properties on known safe/unsafe policies
- **Probabilistic model checking with Storm**: Tool for analyzing Markov decision processes; needed for scalable verification of policy abstractions; quick check: verify small MDP examples with Storm
- **Abstraction generation from trajectories**: Process of creating interpretable policy graphs; needed to bridge raw policy behavior and formal verification; quick check: visualize abstractions from simple trajectory datasets
- **Risk critic and epistemic uncertainty**: Methods for identifying high-risk and novel states; needed for effective falsification exploration; quick check: compare uncertainty estimates on known safe/unsafe states
- **PAC-style completeness guarantees**: Probabilistic guarantees for verification under limitations; needed to quantify confidence in safety claims; quick check: verify theoretical bounds under varying abstraction fidelity

## Architecture Onboarding

Component map: Offline trajectories -> CAPS abstraction -> Storm verification -> Risk-aware falsification -> Runtime safety shield

Critical path: Abstraction generation → Formal verification → Falsification exploration → Safety monitoring

Design tradeoffs: The framework trades some verification completeness (due to abstraction limitations) for interpretability and scalability. Using offline trajectories limits exploration of the full state space but enables formal analysis. The risk-aware falsification partially addresses this limitation but may still miss violations in poorly represented regions.

Failure signatures: Incomplete abstraction coverage may miss safety violations. Poor trajectory quality or insufficient diversity can lead to false safety guarantees. Runtime shield failures may occur if risk estimation is inaccurate or fallback policies are inadequate.

Three first experiments:
1. Verify simple grid-world policies with known safety violations to test framework detection capability
2. Compare falsification effectiveness between risk critic and uncertainty-based approaches on benchmark domains
3. Evaluate runtime shield performance by simulating dynamic risk conditions and measuring policy switching accuracy

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Reliance on offline trajectory datasets may not capture full state space or high-risk scenarios
- Effectiveness depends heavily on trajectory coverage and quality, not explicitly quantified
- Runtime safety shield assumes availability of well-defined fallback policies
- PAC-style guarantees depend on abstraction and dataset quality without rigorous analysis of their relationship

## Confidence
- Verification-guided falsification framework: Medium - Core method is novel but depends on abstraction quality
- Safety violation detection superiority: Medium - Experimental results show improvement but limited to specific domains
- Runtime safety shielding effectiveness: Low - Shield mechanism described but runtime performance not demonstrated

## Next Checks
1. Test framework on high-dimensional safety-critical domains (e.g., autonomous driving with image inputs) to evaluate scalability
2. Conduct ablation studies varying trajectory dataset quality and size to quantify impact on abstraction completeness
3. Implement end-to-end evaluation of runtime safety shield performance in dynamic environments with changing risk conditions