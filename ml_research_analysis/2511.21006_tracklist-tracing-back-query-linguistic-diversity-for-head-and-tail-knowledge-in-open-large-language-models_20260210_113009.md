---
ver: rpa2
title: 'TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge
  in Open Large Language Models'
arxiv_id: '2511.21006'
source_url: https://arxiv.org/abs/2511.21006
tags:
- language
- arxiv
- knowledge
- medical
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how the frequency of medical concepts in
  large language models' (LLMs) pre-training data impacts their performance on different
  types of questions. It introduces TrackList, a fine-grained linguistic and statistical
  analysis pipeline, and RefoMed-EN, a medical QA dataset with 6,170 human-annotated
  terms across five query types.
---

# TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models

## Quick Facts
- **arXiv ID**: 2511.21006
- **Source URL**: https://arxiv.org/abs/2511.21006
- **Reference count**: 0
- **Primary result**: OLMo-1b achieves 0.27 BERTScore correlation with term frequency for definition queries; best model with 33% hallucination rate

## Executive Summary
This paper investigates how term frequency in pre-training data affects large language model (LLM) performance on different types of medical questions. The authors introduce TrackList, a pipeline for analyzing linguistic diversity in LLM outputs, and RefoMed-EN, a medical QA dataset with 6,170 terms across five query types. They find that LLMs perform best on definition queries and worst on exemplification queries, with performance correlating with term frequency in pre-training corpora. The study reveals that LLMs tend to paraphrase more on frequent (head) knowledge and less on rare (tail) knowledge, particularly in expert texts. OLMo-1b emerged as the best-performing model, achieving a BERTScore correlation of 0.27 with term frequency for definition queries and the lowest hallucination rate (33%) on manually evaluated answers.

## Method Summary
The study employs a zero-shot prompting approach using RefoMed-EN, a medical QA dataset containing 6,170 human-annotated medical terms with five query types: definition, exemplification, denomination, paraphrase, and explanation. The authors use open LLMs (OLMo-1b, OLMo-7b, OLMo-7b-instruct, Pythia-1b) and measure document frequency of terms in pre-training corpora via WIMBD API and infini-gram. They evaluate outputs using BERTScore against gold answers, compute Pearson correlations between BERTScore and log document frequency, and analyze CLS embedding similarities for n-grams (2-5) to trace paraphrasing versus memorization behavior. Manual hallucination evaluation was performed on 400 answers.

## Key Results
- LLMs perform best on definition-type queries (BERTScore correlation 0.27 with term frequency for OLMo-1b) and worst on exemplification-type queries (correlation 0.05)
- OLMo-1b achieves the lowest hallucination rate (33%) compared to Pythia-1b (55%) on manually evaluated answers
- LLMs tend to paraphrase more on frequent (head) knowledge and less on rare (tail) knowledge, with CLS embedding correlations showing negative values (-0.44 for OLMo-1b)
- Small LLMs (1B-7B parameters) struggle to distinguish between different pragmatic query types, defaulting to definition-style responses

## Why This Works (Mechanism)

### Mechanism 1: Distributional Memorization
Term frequency in pre-training data moderately correlates with answer quality for definition-type queries through distributional memorization. Frequent concepts appear across more documents in pre-training corpora (DOLMA, The Pile), creating stronger statistical representations that improve semantic alignment with gold answers. This mechanism breaks entirely for very long technical multi-word terms with zero frequency.

### Mechanism 2: Query Type Underspecification
Small LLMs fail to distinguish between different pragmatic query types, defaulting to definition-style responses. This occurs because definition patterns dominate training data, causing models to ignore exemplification, denomination, or paraphrase instructions even when explicitly prompted. Instruction-tuned models may partially mitigate this issue.

### Mechanism 3: Semantic Diversity and Co-occurrence
LLMs paraphrase more on head knowledge and reproduce/near-copy on tail knowledge due to semantic diversity inversely correlating with co-occurrence probability. Frequent terms have richer contextual variation in pre-training, enabling paraphrasing; rare terms force reliance on limited memorized passages. This breaks when tail terms have zero document frequency.

## Foundational Learning

- **Head vs. Tail Knowledge in Zipfian Distributions**
  - Why needed here: All mechanisms depend on frequency-based categorization (e.g., "disease" at 75M docs vs. "testosterone-inhibiting" at 115 docs)
  - Quick check question: Given a term appearing in 50 documents out of 3 trillion tokens, would you classify it as head, torso, or tail knowledge?

- **Distributional Memorization vs. Generalization**
  - Why needed here: Explains why frequency helps definitions but not exemplification—memorization aids knowledge retrieval but not reasoning-intensive reformulation
  - Quick check question: If n-gram search in pre-training data yields exact matches for tail terms but paraphrased content for head terms, which behavior indicates memorization?

- **Semantic Similarity Metrics (BERTScore, CLS Embeddings)**
  - Why needed here: Paper relies on these proxies for answer quality; understanding their limitations is critical for interpretation
  - Quick check question: Why would BERTScore be misleadingly high for a denomination query where the LLM simply repeats the query term?

## Architecture Onboarding

- **Component map**: RefoMed-EN Dataset -> Frequency Lookup (WIMBD/infini-gram) -> Zero-Shot QA Inference -> Evaluation Layer (BERTScore + Pearson correlation + CLS cosine similarity) -> Tracing Layer (Co-occurrence probability)

- **Critical path**: Query by type → Zero-shot inference → BERTScore vs. gold → Correlate with log(document frequency) → CLS embedding analysis on top-3 n-grams

- **Design tradeoffs**:
  - Open models only (OLMo, Pythia): Enables corpus tracing but limits scale (1B-7B); larger proprietary models may behave differently
  - BERTScore over human eval: Scalable but conflates repetition with quality for DEN queries
  - Translated dataset (French→English): Ensures non-contamination but may introduce translation artifacts

- **Failure signatures**:
  - Exemplification failure: "Give examples of severe motor disorders → The answer is severe motor disorders"
  - Denomination failure: Model repeats query term instead of providing synonyms
  - Hallucination spike: Pythia-1b shows +22% hallucination rate over OLMo-1b
  - Definition bias: Model outputs definition when asked for paraphrase/examples

- **First 3 experiments**:
  1. Baseline frequency correlation: Run all 5 query types on OLMo-1b, compute Pearson correlation between BERTScore and log(Cdf); expect DEF≈0.27, EX≈0.05
  2. CLS embedding divergence: For 50 head + 50 tail terms, compute cosine similarity between term embedding and top-3 generated n-gram embeddings; correlate with co-occurrence probability (expect negative correlation ~-0.44)
  3. Manual hallucination audit: Sample 100 answers per model across head/tail splits; categorize as accurate/hallucinated/off-topic to replicate 33% (OLMo-1b) vs. 55% (Pythia-1b) hallucination rates

## Open Questions the Paper Calls Out

- **Open Question 1**: Would retrieval-augmented generation (RAG) systems significantly improve LLM performance on exemplification and explanation queries for tail medical knowledge?
  - Basis in paper: The authors note: "Our study focused on small size language models to investigate the inner working of LLMs without domain knowledge finetuning, DPO finetuning or RAG systems... We are aware that using the methods listed above will improve vanilla LLM's performance for the QA task."
  - Why unresolved: The study deliberately excluded these enhancement methods to isolate the effects of pretraining data frequency on different query types, leaving their specific impact on linguistically diverse queries untested.
  - What evidence would resolve it: Experiments applying RAG systems to the same RefoMed-EN benchmark, particularly measuring performance gains on exemplification queries for tail knowledge.

- **Open Question 2**: What evaluation metrics beyond BERTScore would better capture the correctness of LLM responses to different query types?
  - Basis in paper: The authors state: "BERTscore might not be the best fit for all types of queries we analyzed. For example, for denomination-types queries where the LLM only repeats the medical term in the query, the BERTscore will be very high, but not relevant."
  - Why unresolved: Current metrics fail to distinguish between semantically appropriate answers and superficially similar but pragmatically incorrect responses, particularly for non-definition query types.
  - What evidence would resolve it: Development and validation of new metrics that account for pragmatic function alignment, tested across the five query types defined in RefoMed-EN.

- **Open Question 3**: Does the pattern of LLMs favoring definition-style answers across all query types persist in larger models (70B+ parameters) and instruction-tuned variants?
  - Basis in paper: The study was limited to small models (1B and 7B parameters) due to computational constraints, and found that "language models tend to give definition-type answers to different queries, even when asked to give examples or paraphrases."
  - Why unresolved: The authors acknowledge this limitation but don't test whether larger models overcome this linguistic understanding deficit.
  - What evidence would resolve it: Application of the TrackList pipeline to larger open models (70B+) and comparison of query type adherence rates.

## Limitations

- The study relies heavily on semantic similarity metrics (BERTScore) which may not fully capture answer quality, particularly for denomination queries where repetition inflates scores
- The translated nature of RefoMed-EN introduces potential artifacts that could affect model performance
- The focus on open models (OLMo, Pythia) limits generalizability to proprietary systems that may behave differently
- Manual hallucination evaluation was performed on a limited sample (400 answers) without detailed inter-annotator agreement metrics
- Tail terms with zero document frequency are excluded, potentially underestimating the true scope of LLM limitations

## Confidence

- **High Confidence**: The correlation between term frequency and BERTScore for definition-type queries (0.27 for OLMo-1b), and the general trend of definition queries outperforming other types
- **Medium Confidence**: The claim that LLMs paraphrase more on head knowledge versus tail knowledge, based on CLS embedding correlations
- **Low Confidence**: The broader claim about query type underspecification in LLMs, as this is supported by qualitative observations rather than quantitative comparisons with instruction-tuned or RAG-enhanced models

## Next Checks

1. **Replication with Larger Models**: Test the same pipeline on proprietary LLMs (GPT-4, Claude) to determine if the frequency correlation and query type biases persist at scale.

2. **Manual Evaluation Expansion**: Conduct comprehensive human evaluation (beyond 400 answers) with explicit quality rubrics for each query type to validate BERTScore correlations and hallucination rates.

3. **Zero-Frequency Term Analysis**: Explicitly test model behavior on medical terms with zero document frequency in pre-training corpora to quantify hallucination versus fallback strategies.