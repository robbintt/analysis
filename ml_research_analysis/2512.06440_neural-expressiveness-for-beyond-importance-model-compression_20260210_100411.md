---
ver: rpa2
title: Neural expressiveness for beyond importance model compression
arxiv_id: '2512.06440'
source_url: https://arxiv.org/abs/2512.06440
tags:
- pruning
- expressiveness
- network
- nexp
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "Expressiveness" as a novel criterion for
  neural network pruning, which measures a neuron's ability to redistribute informational
  resources based on activation overlap, contrasting with traditional importance-based
  methods that rely on weight magnitude or gradients. The approach demonstrates that
  expressiveness can be effectively estimated using minimal or arbitrary data, enabling
  data-agnostic pruning strategies.
---

# Neural expressiveness for beyond importance model compression

## Quick Facts
- arXiv ID: 2512.06440
- Source URL: https://arxiv.org/abs/2512.06440
- Reference count: 40
- This paper introduces "Expressiveness" as a novel criterion for neural network pruning, which measures a neuron's ability to redistribute informational resources based on activation overlap, contrasting with traditional importance-based methods that rely on weight magnitude or gradients.

## Executive Summary
This paper introduces Neural Expressiveness (NEXP) as a novel pruning criterion that measures a neuron's ability to redistribute informational resources based on activation overlap rather than weight importance. The approach demonstrates that expressiveness can be effectively estimated using minimal or arbitrary data, enabling data-agnostic pruning strategies. Experiments show expressiveness-based pruning achieves up to 10x greater parameter compression than weight-based methods with only 1% average performance degradation across CIFAR-10, ImageNet, and COCO benchmarks.

## Method Summary
The method computes NEXP scores by measuring pairwise Hamming distances between binarized activation patterns across a mini-batch of samples. For each filter, activation maps are binarized (ReLU > 0 → 1, else 0), and the average pairwise dissimilarity forms the NEXP score. Filters are globally ranked and the bottom-κ least expressive are removed. The approach supports both post-training (PaT) and pruning-at-initialization (PaI) scenarios, with the latter showing promise due to correlation between initialization and converged NEXP rankings. The method is implemented as an extension to the DepGraph framework for structured pruning.

## Key Results
- Expressiveness-based pruning achieves up to 10x greater parameter compression than weight-based methods with only 1% average performance degradation
- On YOLOv8 for object detection on COCO, expressiveness achieves a 46.1% MACs reduction by removing 55.4% of parameters while improving mAP50-95 by 3%
- NEXP can be effectively estimated using ~60 random samples (0.4% of dataset), yielding consistent similarity scores above 99% compared to full-dataset computation

## Why This Works (Mechanism)

### Mechanism 1: Activation Overlap as Discriminative Capacity Proxy
- Claim: Expressiveness approximates a neuron's contribution to feature discrimination by measuring how differently it activates across input samples.
- Mechanism: For each filter, the method constructs an N×N pairwise dissimilarity matrix using Hamming distance on binarized activations. The average dissimilarity becomes the NEXP score—higher overlap (lower distance) indicates lower expressiveness.
- Core assumption: Neurons producing similar activation patterns across diverse inputs carry less discriminative information than those with high inter-sample variance.

### Mechanism 2: Data-Agnostic Approximation Stability
- Claim: The relative ranking of NEXP scores remains stable regardless of input data composition, enabling estimation with ~60 random samples.
- Mechanism: Discriminative capacity manifests as a structural property captured in activation pattern rankings; random sampling preserves ordinal relationships even if absolute scores shift.
- Core assumption: Expressiveness is an intrinsic network property that generalizes across input distributions, not tied to task-specific feature correlations.

### Mechanism 3: Initialization-to-Convergence Pattern Persistence
- Claim: NEXP rankings at initialization partially correlate with post-training rankings, enabling effective pruning before training (PaI).
- Mechanism: Early training critical paths form based on initial weight configurations; these paths shape subsequent expressiveness distribution, especially in early layers.
- Core assumption: Training hyperparameters remain congruent with initialization—aggressive learning rates or large weight decay can break this correlation.

## Foundational Learning

- Concept: Hamming Distance on Binarized Activations
  - Why needed here: The dissimilarity function uses Hamming distance on thresholded ReLU outputs (>0 → 1, else 0).
  - Quick check question: Given two activation maps of shape (H, W), can you compute the Hamming distance between their binarized versions?

- Concept: Structured Pruning vs. Weight Pruning
  - Why needed here: NEXP operates at filter/channel granularity (removing entire convolutional filters), not individual weights.
  - Quick check question: Why does structured pruning require global layer-wise sparsity constraints to avoid irregular memory access patterns?

- Concept: Global vs. Local Pruning Scope
  - Why needed here: The paper empirically finds global (network-wide) NEXP ranking outperforms local (layer-wise) ranking.
  - Quick check question: If layer A has 64 filters and layer B has 128 filters, how would you normalize NEXP scores for global comparison?

## Architecture Onboarding

- Component map: Mini-batch X (N samples) → Forward Pass → Activation Maps C^(l) → Binarization → Pairwise Hamming Matrix (N×N per filter) → NEXP Score (Eq. 11) → Global Ranking → Bottom-κ Removal → Fine-tuning

- Critical path: Forward pass must capture all intermediate activations; Hamming computation is O(N² × H × W × K) per layer where K = filter count.

- Design tradeoffs:
  - Batch size: 60–64 samples sufficient; larger batches yield diminishing returns (>99% cosine similarity achieved)
  - One-shot vs. iterative: One-shot is faster for evaluation; iterative with intermediate fine-tuning improves YOLOv8 recovery (46.1% MACs reduction, +3% mAP)
  - Hybrid weight α: Higher α (more NEXP, less importance) → higher compression but more accuracy loss (Fig. 3)

- Failure signatures:
  - Layer collapse: Entire layer pruned when all filters have low NEXP → check per-layer sparsity before commit
  - Zero-variance activations: All NEXP scores near zero → investigate dead ReLU or extreme batchnorm
  - PaI divergence: Poor fine-tuning recovery from initialization pruning → verify hyperparameter consistency or switch to PaT

- First 3 experiments:
  1. Reproduce Table 1 (ResNet-56, CIFAR-10): Implement NEXP one-shot pruning at 2× FLOPs target; verify ~2.87× params compression with <1% accuracy drop
  2. Data ablation: Run NEXP with batch sizes [16, 32, 64, 256] of random vs. stratified samples; plot score correlation vs. full-dataset baseline
  3. Hybrid sweep: On VGG-16, test α ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} for score = (1-α)×L1 + α×NEXP; plot FLOPs reduction vs. accuracy tradeoff surface

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the "hybrid" compression solution space be optimized beyond simple linear combinations of expressiveness and importance scores?
- Basis in paper: [explicit] The Conclusion states the authors aim to "investigate optimization solutions for exploring the hybrid compression solution space."
- Why unresolved: The current study utilizes a linear combination (weighted by $\alpha$) and notes that exhaustive search is impractical, leaving the optimal method of exploiting the complementary nature of these criteria undefined.
- What evidence would resolve it: An adaptive algorithm that dynamically weights expressiveness and importance per layer or epoch, demonstrating higher compression ratios than static linear combinations.

### Open Question 2
- Question: Does the effectiveness of pruning at initialization imply that neural network training is about revealing pre-existing knowledge rather than learning from scratch?
- Basis in paper: [explicit] The Conclusion explicitly asks: "Is neural network training essentially about learning new knowledge from scratch, or is it about revealing the knowledge that the model already possesses?"
- Why unresolved: While the paper shows NEXP is consistent between initialization and convergence, the theoretical causal link between the initial "expressiveness" state and the final learned "importance" remains a hypothesis.
- What evidence would resolve it: Theoretical analysis or empirical tracking showing that high-NEXP neurons at initialization consistently correspond to the high-importance features learned after convergence across diverse architectures.

### Open Question 3
- Question: Can the calculation of the Neural Expressiveness (NEXP) score be accelerated to overcome the computational bottleneck of pairwise activation comparisons?
- Basis in paper: [inferred] Appendix B.4 notes that computing the NEXP score involves comparing activation patterns for all samples in the batch for every filter, admitting this accounts for the "bulk of the computational complexity."
- Why unresolved: The method relies on calculating an $N \times N$ matrix of dissimilarities (Eq. 10), which scales poorly with batch size compared to simple weight-based metrics.
- What evidence would resolve it: A sampling or approximation theorem showing that a subset of pairwise comparisons (e.g., $O(N)$ instead of $O(N^2)$) can estimate the NEXP score within acceptable error margins for pruning.

## Limitations
- The claim that expressiveness is an intrinsic network property stable across diverse datasets remains primarily theoretical, with limited cross-domain validation
- The method's effectiveness on emerging architectures (vision transformers, MLPs) or unconventional tasks is unexplored
- The pruning criterion's sensitivity to activation function choice (e.g., ReLU variants, GELU) is untested

## Confidence
- **High Confidence**: The core NEXP computation mechanism (pairwise Hamming distance on binarized activations) is well-defined and reproducible. Empirical results on standard architectures are robust.
- **Medium Confidence**: The data-agnostic approximation claim (60 samples suffice) is supported by internal validation but lacks external corpus validation. The initialization-to-convergence pattern persistence is plausible but not fully characterized.
- **Low Confidence**: The generalizability of NEXP to non-CNN architectures and the method's behavior under extreme compression are open questions.

## Next Checks
1. **Cross-Domain Stability**: Validate NEXP score stability on a dataset shift (e.g., train on CIFAR-10, compute NEXP on ImageNet samples). Quantify degradation in pruning effectiveness.
2. **Architecture Transfer**: Apply NEXP to a vision transformer (e.g., ViT-B/16) and an MLP-based model (e.g., MLP-Mixer). Compare compression-accuracy tradeoff to established methods.
3. **Activation Function Sensitivity**: Replace ReLU with GELU and Swish in ResNet-56. Measure changes in NEXP score distribution and pruning recovery, isolating the impact of activation non-linearity.