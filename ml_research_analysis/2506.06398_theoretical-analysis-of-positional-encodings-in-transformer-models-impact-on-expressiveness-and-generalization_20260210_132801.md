---
ver: rpa2
title: 'Theoretical Analysis of Positional Encodings in Transformer Models: Impact
  on Expressiveness and Generalization'
arxiv_id: '2506.06398'
source_url: https://arxiv.org/abs/2506.06398
tags:
- encodings
- nmax
- extrapolation
- positional
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization

## Quick Facts
- arXiv ID: 2506.06398
- Source URL: https://arxiv.org/abs/2506.06398
- Authors: Yin Li
- Reference count: 4
- Primary result: Novel theoretical framework analyzing how different positional encoding schemes affect transformer expressiveness and generalization, with novel proposals using orthogonal transforms

## Executive Summary
This paper provides a comprehensive theoretical analysis of how different positional encoding schemes impact transformer model expressiveness and generalization. The author derives Rademacher complexity bounds showing how PE choices control model capacity, proves universal approximation theorems for various encoding schemes, and introduces novel wavelet and Legendre polynomial-based positional encodings. Through a unified framework, the paper connects the mathematical properties of positional encodings to their practical effects on extrapolation and generalization performance.

## Method Summary
The paper analyzes transformer models with different positional encoding schemes through three theoretical lenses: generalization bounds via Rademacher complexity, universal approximation capabilities, and extrapolation behavior. Experiments use a synthetic running-sum task with 2-layer transformer encoders (d_model=64, single-head attention) trained on sequences of length 50 and tested on lengths 50, 100, and 200. Four positional encoding schemes are compared: sinusoidal, ALiBi, wavelet (Daubechies-4 basis), and Legendre polynomials. The theoretical analysis is validated through controlled experiments measuring MSE across interpolation and extrapolation scenarios.

## Key Results
- Wavelet-based positional encodings achieve the lowest extrapolation error (MSE 0.0108 at N=200) among tested schemes
- ALiBi's extrapolation capability comes with a linear error growth bound: |A(μ,-αd) - A(μ,-αNmax)| ≤ L_A·α(d-Nmax)
- Learned positional encodings provide zero extrapolation capability beyond training length
- Universal approximation for fixed N does not guarantee extrapolation to longer sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALiBi enables extrapolation to longer sequences because its linear bias term remains well-defined and monotonic for any token distance.
- Mechanism: The attention logit includes b(i,j) = -α|i-j|. When |i-j| exceeds training maximum, this formula still applies uniformly. The bias dominates query-key dot products for large distances, creating a "soft locality" prior that scales with token distance regardless of training data.
- Core assumption: The attention function A is Lipschitz continuous in its bias argument.
- Evidence anchors:
  - [Section 5.3]: "Because b(i,j) remains monotonic in distance, the model's learned attention function at train time can be extended to test time by continuity."
  - [Section 5.3, Eq. 9]: Extrapolation error bound |A(μ,-αd) - A(μ,-αNmax)| ≤ L_A·α(d-Nmax) shows linear error growth.
  - [corpus]: Related work (HoPE paper) confirms Alibi exhibits "performance degradation on extrapolation" for very long contexts—consistent with paper's theoretical limits.
- Break condition: Extrapolation error grows linearly with (d-Nmax). For extremely long sequences where d >> Nmax, attention weights degrade unless α is reduced.

### Mechanism 2
- Claim: Wavelet-based positional encodings preserve positional distinctions beyond training length through exponential decay of high-frequency components.
- Mechanism: Wavelet basis functions have compact support. For pos > Nmax, fine-scale (high-frequency) terms vanish exponentially, while coarse-scale terms contribute small but nonzero values. The encoding difference ||PE_wavelet(pos) - PE_wavelet(Nmax)||_2 decays as O(exp(-β(pos-Nmax))).
- Core assumption: Wavelet basis functions satisfy compact support and exponential decay properties outside their main support region.
- Evidence anchors:
  - [Section 6.1.4]: "|ψ_j,k(pos)| ≤ C·2^{-j/2}·exp(-β|pos-k·2^{-j}|/2^{-j}) for j ≤ j_max"
  - [Section 7.2, Table 5]: Wavelet achieves lowest extrapolation MSE (0.0108 at N=200 vs sinusoidal 0.0423).
  - [corpus]: No direct corpus validation for wavelet-based PE specifically—this is a novel proposal from the paper.
- Break condition: For very large pos-Nmax where only the coarsest scale contributes, all positions may converge toward similar encodings, losing fine-grained distinction.

### Mechanism 3
- Claim: Generalization bounds for transformers depend on input norm bounds induced by positional encodings, with bounded PEs yielding tighter Rademacher complexity.
- Mechanism: For Lipschitz-continuous transformers, Rademacher complexity R_m(F) scales as O(L_trans·B_0/√m) where B_0 bounds ||z_i^(0)||_2. Sinusoidal PEs have bounded norm (√d_model), while learned PEs require regularization to control B_p.
- Core assumption: Weight matrices satisfy ||W||_2 ≤ C_W, making each layer L_layer-Lipschitz.
- Evidence anchors:
  - [Section 4.2]: "R_m(F^{L,H}_{sinusoidal}) scales as O(L_trans·B_0/√m) up to logarithmic factors"
  - [Table 2]: Compares norm bounds across encoding schemes—sinusoidal B_x+√d_model vs learned B_x+B_p.
  - [corpus]: Weak corpus connection—related papers focus on empirical benchmarking rather than theoretical generalization bounds.
- Break condition: If learned PE norms (B_p) grow unchecked, or if ALiBi's α(N'-1) becomes large for very long inference sequences, capacity inflation degrades generalization.

## Foundational Learning

- Concept: Rademacher Complexity
  - Why needed here: The paper uses this to derive generalization bounds showing how PE schemes control model capacity.
  - Quick check question: If a function class F has Rademacher complexity 0.1 vs 0.5, which generalizes better from limited data?

- Concept: Universal Approximation
  - Why needed here: The paper analyzes whether different PE schemes allow transformers to approximate any sequence-to-sequence function as depth/width increase.
  - Quick check question: Why does universal approximation for fixed N not guarantee extrapolation to longer sequences?

- Concept: Orthogonal Function Bases (Wavelets, Legendre Polynomials)
  - Why needed here: The paper proposes novel PEs using these bases, claiming better extrapolation through controlled decay properties.
  - Quick check question: Why might wavelet bases extrapolate better than sinusoidal bases despite both being orthogonal?

## Architecture Onboarding

- Component map: Input Embeddings (X) → + Positional Encodings (P) → Z^(0) → Multi-Head Self-Attention → Feed-Forward Network → [Repeat L layers] → Z^(L)
- Critical path: Positional encoding choice → attention logit computation → softmax normalization → output representation. For extrapolation tasks, the PE's behavior beyond Nmax directly determines whether the attention mechanism can maintain meaningful position-dependent attention patterns.
- Design tradeoffs:
  - Sinusoidal: Simple, bounded norm, but periodic ambiguity beyond ~10000 positions
  - Learned: Most expressive within training range, zero extrapolation capability
  - ALiBi: Strong extrapolation, but α must be tuned; weak absolute position encoding
  - Wavelet (proposed): Strong extrapolation with exponential decay, requires normalization to control norm bounds
  - Legendre (proposed): Bounded norm, but saturates to constant vector for very large positions
- Failure signatures:
  - Sinusoidal on long sequences: Positional ambiguity causes attention patterns to repeat incorrectly
  - Learned beyond Nmax: No embedding exists—reusing p_Nmax causes all positions to collapse
  - ALiBi with large α on very long sequences: Attention collapses to purely local, ignoring useful long-range dependencies
  - Unnormalized wavelet: Input norm inflation (O(Nmax·√log Nmax)) destabilizes training
- First 3 experiments:
  1. Replicate the running-sum synthetic task (Section 7.1) with N_train=50, testing extrapolation to N_test=100,200. Compare sinusoidal vs ALiBi vs wavelet to validate theoretical predictions.
  2. Measure ||PE(pos)||_2 across positions 0 to 2·Nmax for each encoding scheme to verify norm bound claims from Section 4.2 and Table 2.
  3. Visualize attention patterns for positions beyond Nmax using ALiBi and wavelet encodings to confirm that attention weights decay gracefully rather than becoming erratic (testing Section 5.3's Lipschitz assumption).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the expressiveness and generalization bounds change when extending the theoretical analysis to multi-head transformers with layer normalization?
- **Basis in paper:** [explicit] The authors list "Multi-Head and Full Transformer Analysis" as a future direction, noting the current theory focuses on single-head attention without standard components like dropout.
- **Why unresolved:** The mathematical interaction between multiple attention heads and layer normalization complicates the Lipschitz continuity and Rademacher complexity derivations used for the single-head model.
- **What evidence would resolve it:** Derivation of new generalization bounds that explicitly account for head interactions and empirical validation on standard multi-head architectures.

### Open Question 2
- **Question:** Do orthogonal transform-based encodings (wavelet, Legendre) retain their theoretical advantages in extrapolation and generalization on large-scale, real-world datasets?
- **Basis in paper:** [explicit] The "Future Directions" section calls for "Empirical Validation on Real Data," acknowledging that current results rely solely on a lightweight synthetic running-sum task.
- **Why unresolved:** Synthetic tasks allow for controlled isolation of variables but do not reflect the noise, complexity, or feature distributions of natural language or vision data.
- **What evidence would resolve it:** Benchmarking the proposed encodings against ALiBi and sinusoidal baselines on long-context tasks like document summarization or long-range language modeling.

### Open Question 3
- **Question:** Can adaptive schedules for ALiBi's bias slope (α) optimize the trade-off between generalization on training lengths and error rates during extrapolation?
- **Basis in paper:** [explicit] The paper suggests investigating "Adaptive Bias Schedules" to manage the trade-off where a fixed small α weakens locality on short sequences while a large α harms stability on long ones.
- **Why unresolved:** The current theoretical bounds rely on a fixed α, leaving the dynamic adjustment of this parameter as an unexplored heuristic.
- **What evidence would resolve it:** Formulating α as a function of sequence length and demonstrating reduced extrapolation error compared to static α configurations.

## Limitations

- Theoretical framework validated only on synthetic running-sum task, lacking real-world dataset validation
- Analysis assumes Lipschitz continuity and bounded weight norms that may not hold in practice
- Novel wavelet positional encoding lacks empirical validation beyond the controlled synthetic setting

## Confidence

**High Confidence:**
- ALiBi's extrapolation mechanism via monotonic bias terms
- Sinusoidal PE's bounded norm properties and their impact on Rademacher complexity
- General relationship between input norm bounds and generalization capacity

**Medium Confidence:**
- Wavelet PE's extrapolation capabilities through exponential decay
- Legendre PE's bounded norm properties and controlled saturation
- Specific extrapolation error bounds for different PE schemes

**Low Confidence:**
- Universal approximation capabilities with novel PE schemes beyond the synthetic task
- Real-world performance implications of the capacity inflation analysis
- Comparative extrapolation performance across all PE schemes on practical benchmarks

## Next Checks

1. Apply the four PE schemes (sinusoidal, ALiBi, wavelet, Legendre) to established benchmarks like LRA (Long Range Arena) or WikiText-103 to validate whether theoretical advantages translate to practical gains.

2. Systematically vary weight initialization scales, learning rates, and regularization strength to measure how these affect the empirical Lipschitz constants of transformer layers and whether they correlate with predicted generalization bounds.

3. Extend experiments beyond H=1 to multi-head attention (H>1) to analyze whether the theoretical properties of each PE scheme scale with increased model capacity, or if head competition creates new failure modes not captured in the current analysis.