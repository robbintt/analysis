---
ver: rpa2
title: Fair Graph Machine Learning under Adversarial Missingness Processes
arxiv_id: '2311.01591'
source_url: https://arxiv.org/abs/2311.01591
tags:
- sensitive
- fairness
- bias
- missing
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how adversarial missingness processes\
  \ can undermine fairness in graph machine learning by causing imputed sensitive\
  \ attributes to underestimate true bias. The authors propose BFtS, a 3-player adversarial\
  \ framework that jointly trains a GNN classifier with two adversaries\u2014one predicting\
  \ sensitive attributes from embeddings, another imputing missing values to maximize\
  \ bias\u2014thereby ensuring worst-case imputation scenarios."
---

# Fair Graph Machine Learning under Adversarial Missingness Processes

## Quick Facts
- arXiv ID: 2311.01591
- Source URL: https://arxiv.org/abs/2311.01591
- Reference count: 40
- One-line primary result: BFtS framework improves fairness-accuracy trade-offs under adversarial missingness via worst-case imputation

## Executive Summary
This paper addresses the vulnerability of fair graph neural networks to adversarial missingness processes, where strategic removal of sensitive attribute labels can cause fair models to underestimate true bias. The authors propose BFtS, a three-player adversarial framework that jointly trains a GNN classifier with two adversaries—one predicting sensitive attributes and another imputing missing values to maximize bias. This approach ensures the model is robust to worst-case imputation scenarios by minimizing the maximum possible demographic parity violation.

The key insight is that standard imputation methods trained to maximize accuracy can inadvertently disguise underlying bias when sensitive attributes are missing non-randomly. BFtS overcomes this by framing the problem as distributionally robust optimization, where the imputation adversary actively generates imputations that maximize fairness violations, forcing the classifier to be fair even under the worst-case scenario. Experiments on synthetic and real datasets demonstrate that BFtS achieves better fairness-accuracy trade-offs than baselines like FairGNN, RNF, and FairSIN, particularly under adversarial missingness conditions.

## Method Summary
BFtS is a three-player adversarial framework for fair graph machine learning under adversarial missingness. It consists of a GNN classifier (f_class), a DNN adversary predicting sensitive attributes (f_bias), and a GNN imputer for missing sensitive values (f_imp). The method uses a minimax objective where f_imp generates worst-case imputations that maximize demographic parity violations, f_bias predicts sensitive attributes from embeddings, and f_class learns to be fair against these imputations. Training alternates between updating each component's parameters using alternating gradient updates, with the classifier minimizing a weighted combination of classification and fairness losses while the adversaries maximize fairness violations. The framework is specifically designed to handle cases where up to 70% of sensitive attributes are missing.

## Key Results
- BFtS achieves better fairness-accuracy trade-offs than baselines like FairGNN, RNF, and FairSIN under adversarial missingness
- The framework maintains effectiveness even when only 10-20% of sensitive attributes are available, outperforming baselines that require full sensitive attribute labels
- BFtS improves imputation accuracy and produces more balanced representations compared to standard independent imputation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial missingness processes cause standard imputation to underestimate true bias, leading fair models to overestimate their fairness
- Mechanism: When sensitive attributes are missing non-randomly (e.g., strategically selecting low-degree nodes), independent imputation methods trained to maximize accuracy produce imputed values that reduce the apparent demographic parity violation relative to the complete data. The fair classifier then optimizes against a misleadingly "fair" imputation and inherits the true underlying bias
- Core assumption: The missingness process can be influenced by an adversary who exploits graph structure (specifically degree bias) to maximize imputation error on cross-group nodes
- Evidence anchors:
  - [abstract] "an adversarial missingness process can inadvertently disguise a fair model through the imputation, leading the model to overestimate the fairness of its predictions"
  - [Section 3] Theorem 1 proves AMADB is NP-hard; Figure 2 shows degree-based adversarial missingness most effectively causes GCN imputation to underestimate bias (up to 433% discrepancy on NBA dataset)
  - [corpus] Limited direct support; related work focuses on bias mitigation rather than adversarial missingness specifically
- Break condition: If missingness is truly MCAR or if the imputation model has access to perfect predictors of sensitive attributes that are independent of degree, adversarial manipulation becomes ineffective

### Mechanism 2
- Claim: Optimizing for worst-case imputation scenarios yields fairer models under adversarial missingness than optimizing for expected imputation accuracy
- Mechanism: BFtS frames the problem as distributionally robust optimization: rather than minimizing E_{s ~ P_s}[L_bias] where P_s is estimated from incomplete data, it minimizes max_{u in U} E_{s ~ u}[L_bias] over an uncertainty set U of plausible sensitive attribute distributions. The imputation adversary f_imp generates imputations s that maximize Delta DP, and the classifier f_class learns to be fair against these worst-case imputations
- Core assumption: The true sensitive attribute distribution lies within the uncertainty set U reachable by the imputation adversary
- Evidence anchors:
  - [abstract] "imputations should approximate the worst-case scenario for fairness—i.e., when optimizing fairness is the hardest"
  - [Section 4.1] "fimp plays the role of an adversary to fclass by predicting values that maximize the accuracy of fbias"
  - [Section 4.2] Theorem 2 proves f_imp maximizes |p(y=1|s=1) - p(y=1|s=0)|; Theorem 3 proves f_class minimizes the supremum over imputations
  - [corpus] Weak direct support; related papers address fairness via counterfactuals or debiasing, not distributional robustness
- Break condition: If the true distribution P_s is far outside U (e.g., imputation adversary is too constrained), the worst-case optimization may over-regularize and sacrifice accuracy unnecessarily

### Mechanism 3
- Claim: The three-player adversarial scheme prevents convergence failures that plague two-player approaches with independent imputation
- Mechanism: With independent imputation s', if p(h|s'=1) ≈ p(h|s'=0), the JS divergence vanishes, the gradient of L_bias becomes constant, and the classifier receives no useful fairness signal. In BFtS, f_imp actively maximizes JS(p(h|s=1); p(h|s=0)), ensuring non-vanishing gradients and more stable adversarial training
- Core assumption: The adversarial dynamics reach a meaningful equilibrium rather than cycling or diverging
- Evidence anchors:
  - [Section 4.2] Corollary 1 proves JS(p(h|s'=1); p(h|s'=0)) ≤ JS(p(h|s=1); p(h|s=0)) for independent imputation s'
  - [Appendix, Convergence analysis] Discusses three failure modes of two-player approaches and how BFtS mitigates them
  - [corpus] No direct corpus support for this specific convergence mechanism
- Break condition: If the three-player game fails to converge (e.g., due to oscillation between imputation strategies), training may be unstable or require careful hyperparameter tuning

## Foundational Learning

- Concept: Demographic Parity (Delta DP) and its limitations
  - Why needed here: The entire BFtS framework optimizes against Delta DP; understanding that Delta DP = |p(y=1|s=0) - p(y=1|s=1)| measures prediction rate disparity across groups is essential to interpret the minimax objective
  - Quick check question: If Delta DP = 0.15, what does this mean about the difference in positive prediction rates between groups?

- Concept: Message-passing GNNs and degree bias
  - Why needed here: The adversarial missingness heuristic exploits that low-degree nodes are harder to impute due to fewer neighbor aggregations. Without understanding GNN aggregation (h_v^(k) = COMBINE(h_v^(k-1), AGGREGATE({h_u^(k-1): u in N(v)})), the attack mechanism is opaque
  - Quick check question: Why would a node with degree 2 be harder to classify fairly than a node with degree 50?

- Concept: Adversarial training and minimax optimization
  - Why needed here: BFtS is fundamentally a three-player game with objective min_theta_class max_theta_imp, theta_bias L_bias. Understanding gradient reversal, alternating optimization, and why maximization over adversaries promotes robustness is prerequisite
  - Quick check question: In the BFtS formulation, why does f_imp minimize L_imp - beta*L_bias rather than just maximizing L_bias?

## Architecture Onboarding

- Component map:
  - f_class (GNN classifier): 2-layer GNN producing node representations h_v and predictions y_v; minimizes L_class + alpha*L_bias
  - f_bias (sensitivity adversary): DNN taking h_v as input, predicting sensitive attributes; maximizes L_bias (cross-entropy on observed V_S plus imputed s for missing nodes)
  - f_imp (imputation adversary): GNN producing s_v for missing nodes; minimizes L_imp - beta*L_bias using LDAM loss for class imbalance
  - Observed sensitive set V_S: ~30% of nodes with ground-truth sensitive attributes; rest are imputed

- Critical path: Input (G, X, y, V_L, V_S) -> f_imp generates s -> combine s with observed s -> f_class produces h_v -> f_bias predicts s_a from h_v -> compute L_class, L_bias, L_imp -> alternate parameter updates per Algorithm 1

- Design tradeoffs:
  - alpha (fairness-accuracy tradeoff): Higher alpha enforces stricter fairness but reduces accuracy; paper shows alpha has larger impact than beta
  - beta (imputation-worst-case tradeoff): Higher beta pushes toward worst-case imputation but may hurt imputation accuracy
  - LDAM vs. cross-entropy for L_imp: LDAM provides better fairness-accuracy tradeoff by handling class imbalance in sensitive attributes

- Failure signatures:
  - If Delta DP remains high despite training: check if f_imp is collapsing to constant predictions; verify |V_S| is sufficient
  - If accuracy collapses: alpha may be too large; reduce fairness regularization
  - If training oscillates: reduce learning rate for adversaries or increase alternating update frequency
  - If f_imp imputation accuracy is very low: verify LDAM loss implementation; check degree distribution of missing nodes

- First 3 experiments:
  1. Reproduce Figure 2: Train a GCN imputation model under degree-based adversarial missingness vs. MCAR; confirm that independent imputation underestimates bias (compare Delta DP of imputed vs. true sensitive attributes)
  2. Ablation on worst-case assumption: Run BFtS with beta = 0 (pure accuracy imputation) vs. beta > 0; measure fairness-accuracy tradeoff curves as in Figure 5
  3. Vary |V_S|: Test BFtS with 10%-80% observed sensitive attributes (as in Figure 8) on a single dataset; confirm robustness to limited sensitive data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance and stability of BFtS change when extended to non-binary or continuous sensitive attributes?
- Basis in paper: [explicit] Section 4.1.2 states that while BFtS considers binary attributes for simplicity, it theoretically extends to non-binary settings "by appropriately generalizing the imputation loss and the bias loss"
- Why unresolved: The paper lacks empirical validation or theoretical analysis on how the minimax objective (specifically the JS divergence bounds) behaves with multi-class sensitive attributes or high-dimensional continuous attributes
- What evidence would resolve it: Experimental results on benchmark datasets with multi-class sensitive attributes (e.g., race with multiple categories) showing convergence behavior and fairness-accuracy trade-offs compared to binary baselines

### Open Question 2
- Question: Is BFtS robust against adversarial missingness strategies that exploit feature correlations rather than graph topology?
- Basis in paper: [inferred] Section 3 relies on the "degree bias assumption," defining the adversarial missingness process based on node degree (topology) because access to true labels is a "strong assumption"
- Why unresolved: The proposed defense is tailored to a specific "degree" heuristic. It is unclear if the 3-player framework successfully defends against adversaries that select missing values based on feature correlations or latent subgroups rather than node degree
- What evidence would resolve it: Evaluation of BFtS under a feature-based or content-aware adversarial missingness strategy (e.g., masking attributes for specific feature subspaces) to test if the framework generalizes beyond topological attacks

### Open Question 3
- Question: Can the trade-off between worst-case fairness and classification accuracy be theoretically characterized to allow for automated hyperparameter selection?
- Basis in paper: [explicit] Appendix Figure 7 and 10 highlight sensitivity to hyperparameters alpha and beta, noting that the "worst-case assumption... may overestimate the bias," creating a utility trade-off that must be tuned via cross-validation
- Why unresolved: The paper treats the trade-off as a manual hyperparameter search, leaving open the question of whether an optimal or adaptive balance can be determined dynamically during training without costly validation
- What evidence would resolve it: A theoretical bound on the utility loss relative to the fairness gain, or an algorithm that adaptively adjusts beta to maintain fairness guarantees while minimizing unnecessary accuracy degradation

## Limitations

- Limited experimental validation on large-scale graphs (>10K nodes) raises questions about scalability of the 3-player adversarial framework
- No ablation studies on the impact of varying |V_S| below 30% or on different sensitive attribute distributions
- The adversarial missingness heuristic assumes degree bias is the primary attack vector; real-world adversaries may exploit different structural patterns

## Confidence

- High confidence in the theoretical framework (Theorems 2-3, Corollary 1) and mechanism linking adversarial missingness to underestimated bias
- Medium confidence in empirical claims due to limited dataset diversity and absence of robustness tests against alternative missingness patterns
- Medium confidence in the three-player architecture's convergence benefits, as no direct comparison with two-player alternatives is provided

## Next Checks

1. Test BFtS on graphs with 50K+ nodes to evaluate computational scalability and memory constraints
2. Implement alternative adversarial missingness strategies (e.g., based on clustering coefficient or betweenness centrality) to assess framework robustness
3. Conduct ablation studies varying |V_S| from 10%-90% to identify minimum required sensitive attribute coverage