---
ver: rpa2
title: Unifying Linear-Time Attention via Latent Probabilistic Modelling
arxiv_id: '2402.17512'
source_url: https://arxiv.org/abs/2402.17512
tags:
- attention
- linear
- standard
- latte
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits linear-time attention mechanisms through the
  lens of probabilistic graphical models, introducing a novel directed parameterisation
  called Latte. The authors show that standard linear attention can be interpreted
  as an undirected latent variable model, and then propose a directed variant that
  introduces asymmetric dependencies, better reflecting the causal nature of language.
---

# Unifying Linear-Time Attention via Latent Probabilistic Modelling

## Quick Facts
- arXiv ID: 2402.17512
- Source URL: https://arxiv.org/abs/2402.17512
- Authors: Rares Dolga; Lucas Maystre; Marius Cobzarenco; David Barber
- Reference count: 15
- Primary result: Introduces Latte, a directed latent-variable attention mechanism achieving competitive perplexity on language modeling benchmarks

## Executive Summary
This paper presents Latte, a novel approach to linear-time attention based on directed probabilistic graphical models. The authors show that standard linear attention can be interpreted as an undirected latent variable model, and propose a directed variant that better captures the causal nature of language. Latte integrates global latent-variable attention with local standard attention in a fully normalized framework, eliminating the need for relative positional encodings through recurrent parameterisation. Experiments demonstrate competitive performance with standard attention on language modeling tasks while maintaining linear computational complexity.

## Method Summary
Latte reformulates linear attention as inference in a directed probabilistic graphical model, introducing latent states that represent "concepts" in the sequence. The model uses a parallel scan operation to maintain cumulative state representations, with queries and keys generated through recurrent layers (RGLRU or Conv1D) that implicitly handle positional information. A special latent state $l=0$ is dedicated to local sliding-window attention, while global attention is computed through $L$ latent states. The framework is trained end-to-end using standard cross-entropy loss, with normalization achieved through mixture weights across all latent states.

## Key Results
- Achieves perplexity of 17.6 on 153M-parameter models for language modeling
- Outperforms existing linear attention variants on LRA benchmark classification tasks
- Maintains linear computational complexity while integrating local and global attention mechanisms
- Eliminates need for explicit relative positional encodings through recurrent parameterization

## Why This Works (Mechanism)

### Mechanism 1: Directed Latent Variable Parameterization
- **Claim:** Re-interpreting linear attention as a directed graphical model introduces asymmetry required for causal sequence modeling
- **Mechanism:** Standard linear attention uses symmetric potentials; Latte explicitly models conditional dependencies $p(s|l,t)$ and $p(l|t)$, forcing the model to learn a generative flow where latent concepts influence tokens
- **Core assumption:** Sequential dependencies in language are better captured by directed causal graphs than undirected associative fields
- **Evidence anchors:** Abstract states "novel directed parameterisation... aligned with the causal and sequential nature of language"; Section 4 defines $p(s|t)$ via directed conditionals
- **Break condition:** If attention weights exhibit symmetry (e.g., in bidirectional retrieval), directed assumption offers no advantage

### Mechanism 2: Unified Mixture of Local and Global Attention
- **Claim:** Combining local and global attention requires unified probabilistic mixture to prevent normalization bias
- **Mechanism:** Defines special latent state $l=0$ for standard sliding-window attention, creating weighted mixture: $\tilde{x}_t = p(l=0|t)\text{Local} + \sum p(l|t)\text{Global}$
- **Core assumption:** Global context can be compressed into discrete latent states without losing fine-grained local precision
- **Evidence anchors:** Section 4.1 describes "correctly normalised attention by defining a special latent state"; abstract mentions "integrates global latent-variable attention with local standard attention"
- **Break condition:** If optimal pattern is purely sparse (random access), dedicated local state may constrain arbitrary distant attention

### Mechanism 3: Recurrent Positional Encoding
- **Claim:** Explicit relative positional encodings are incompatible with linear attention's low-rank state compression
- **Mechanism:** Uses 1D convolution or RGLRU to generate position-aware $y_t$ before projecting to $Q, K$, making query/key generation dependent on local temporal context
- **Core assumption:** Sequence state can carry positional information implicitly through recurrence dynamics
- **Evidence anchors:** Section 4.1 states "convolution and recurrent layers break positional invariance, eliminating need for positional encodings"
- **Break condition:** In tasks requiring precise arithmetic on token indices, implicit positioning may fail OOD compared to explicit encodings

## Foundational Learning

- **Concept: Probabilistic Graphical Models (PGMs)**
  - **Why needed here:** Formulates attention as inference on graph with latent nodes rather than matrix operation
  - **Quick check question:** Can you distinguish between undirected Markov Random Field (symmetric potentials) and directed Bayesian Network (conditional probabilities)?

- **Concept: Associative Matrix Property**
  - **Why needed here:** Mathematical trick allowing linear complexity: $(QK^T)V = Q(K^TV)$
  - **Quick check question:** If $Q, K, V$ are $(T, D)$ matrices, which multiplication order computes result in $O(T)$ time?

- **Concept: State Space Models (SSMs) / RGLRU**
  - **Why needed here:** Paper uses RGLRU to handle positional dynamics for Q/K generation
  - **Quick check question:** How does "gated" recurrence differ from simple linear recurrence $h_t = W h_{t-1} + x_t$?

## Architecture Onboarding

- **Component map:** Input Processor (RGLRU/Conv1D) -> Projectors ($y_t \to Q,K$ and $x_t \to V$) -> Latte Core (Scan maintaining $(\tilde{v}_{t,l}, \alpha_{t,l})$) -> Hybrid Mixer (Softmax over $L+1$ states)

- **Critical path:** Parallel scan operation (Algorithm 1) is bottleneck - compute-bound on reduction over $L$ latent states vs memory-bandwidth bound on $T \times T$ matrix in standard attention

- **Design tradeoffs:**
  - **Latent Dimension ($L$):** Higher $L$ increases expressivity but slows scan operation; paper uses $L=256$ for $D=512$
  - **Unroll Factor:** Affects compilation time vs runtime speed in scan implementation

- **Failure signatures:**
  - **Latent Collapse:** Model ignores global latent states, relies entirely on $l=0$ (local) state; check $p(l|t)$ distribution during training
  - **Normalization Instability:** If $\alpha_{t,l}$ grows unbounded, gradients may vanish; ensure exponential max-shifting trick is implemented correctly

- **First 3 experiments:**
  1. **Synthetic MQAR:** Train on MQAR dataset to verify basic recall capabilities against baselines like Mamba/GLA (verify implementation works)
  2. **Ablation on Positioning:** Train small Latte model on OpenWebText with vs without RGLRU/Conv layer to confirm implicit positioning works (validate mechanism)
  3. **Check Latent Usage:** Visualize $p(l|t)$ on held-out sequence; if distribution is flat or concentrated on $l=0$, global mechanism is failing (debug latent collapse)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Latte framework be effectively extended to multimodal modeling and sequence-to-sequence generation tasks?
  - **Basis:** Conclusion states "Future work will explore extensions to tasks such as question answering, sequence-to-sequence generation, and multimodal modeling"
  - **Why unresolved:** Experiments restricted to autoregressive language modeling and classification
  - **What evidence would resolve it:** Empirical results on sequence-to-sequence or multimodal benchmarks showing Transformer-comparable performance

- **Open Question 2:** Can the probabilistic graphical model interpretation be adapted to include non-normalized recurrent architectures like Mamba?
  - **Basis:** Limitations section notes "framework does not cover all linear-time architectures, in particular... non-normalized recurrent sequence mixers (e.g., Mamba) fall outside probabilistic assumptions"
  - **Why unresolved:** Current framework relies on valid probability distribution for attention weights, which Mamba explicitly lacks
  - **What evidence would resolve it:** Theoretical extension accounting for unnormalized mixing, or hybrid architecture validating approach against Mamba benchmarks

- **Open Question 3:** Why does the model underperform on continuous data tasks compared to discrete language tasks?
  - **Basis:** Section 5.3 observes "performance on image-based (continuous) tasks remains weaker" relative to state-space models
  - **Why unresolved:** Paper highlights performance gap on LRA image tasks but doesn't isolate cause within Latte architecture
  - **What evidence would resolve it:** Ablation studies determining if directed parameterization or normalization constraints hinder continuous signal modeling

## Limitations

- Numerical stability concerns with exponential term in directed parameterization, acknowledged but not fully addressed
- Limited ablation studies - doesn't provide complete ablation over all architectural components
- Claims about avoiding positional encodings rely on implicit positioning through recurrence, not rigorously tested against explicit encodings
- Directed PGM interpretation doesn't demonstrate clear advantages over undirected approaches in bidirectional tasks

## Confidence

- **High confidence:** Core mathematical framework (Equations 10-14) and integration with local attention via latent state $l=0$ are well-specified and reproducible; computational complexity claims are straightforward to verify
- **Medium confidence:** Performance claims on OpenWebText and LRA benchmarks; methodology sound but exact baseline comparisons depend on implementation details not fully specified
- **Low confidence:** Directed PGM interpretation as causal advantage; argues this better reflects language's sequential nature but lacks strong empirical evidence compared to undirected alternatives

## Next Checks

1. **Latent State Distribution Analysis:** During training, log and visualize distribution $p(l|t)$ across all latent states for multiple sequences. Verify distribution is neither collapsed to $l=0$ nor uniformly flat, but shows meaningful differentiation between local and global attention patterns.

2. **Controlled Ablation on Positioning:** Train two identical Latte models on OpenWebText - one with RGLRU/Conv layer for implicit positioning, one without but with standard RoPE positional encodings added to queries and keys. Compare perplexity and analyze attention patterns to determine whether implicit positioning is truly necessary.

3. **Numerical Stability Stress Test:** Implement controlled experiment with deliberately extreme input values to trigger potential overflow in exp($x^T w$) term. Monitor $\alpha_{t,l}$ for NaN/Inf values and measure effectiveness of cummax trick. Document input magnitude thresholds where instability occurs.