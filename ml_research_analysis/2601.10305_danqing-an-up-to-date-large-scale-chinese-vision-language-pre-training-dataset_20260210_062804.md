---
ver: rpa2
title: 'DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset'
arxiv_id: '2601.10305'
source_url: https://arxiv.org/abs/2601.10305
tags:
- danqing
- chinese
- dataset
- data
- image-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DanQing is a large-scale Chinese vision-language dataset containing
  100 million high-quality image-text pairs collected from 2024-2025 Common Crawl
  data. The authors developed a systematic filtering pipeline that reduced 1 billion
  raw pairs by 90.46% through four stages: data source selection, text refinement,
  visual diversification, and cross-modal cross-batch filtering.'
---

# DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset

## Quick Facts
- **arXiv ID**: 2601.10305
- **Source URL**: https://arxiv.org/abs/2601.10305
- **Reference count**: 40
- **Primary result**: 100M high-quality Chinese image-text pairs from 2024-2025 Common Crawl data

## Executive Summary
DanQing addresses the critical bottleneck in Chinese vision-language pre-training by providing a large-scale dataset of 100 million high-quality image-text pairs. The dataset was constructed through a systematic four-stage filtering pipeline that reduced 1 billion raw pairs by 90.46%, addressing the scarcity of Chinese vision-language data that has limited VLP development. Through continued pretraining of SigLIP2 models, DanQing consistently outperformed existing Chinese datasets across zero-shot classification, cross-modal retrieval, and Chinese-centric large multimodal model tasks.

## Method Summary
The authors developed a systematic filtering pipeline that reduced 1 billion raw pairs through four stages: data source selection, text refinement, visual diversification, and cross-modal cross-batch filtering. This process addressed the scarcity of high-quality Chinese vision-language data that has limited Chinese VLP development. The dataset was constructed from 2024-2025 Common Crawl data and underwent extensive quality control to ensure high-quality image-text pairs. The filtering pipeline included automated quality assessment, duplicate removal, and semantic diversity enhancement to create a balanced and comprehensive dataset.

## Key Results
- DanQing consistently outperformed existing Chinese datasets across zero-shot classification, cross-modal retrieval, and Chinese-centric large multimodal model tasks
- The dataset shows superior scaling capability and more balanced semantic distribution compared to existing datasets
- Better understanding of novel concepts was demonstrated compared to existing datasets
- SigLIP2 models pretrained on DanQing showed consistent performance improvements across all evaluated tasks

## Why This Works (Mechanism)
The systematic four-stage filtering pipeline addresses the core challenge of data quality in Chinese vision-language datasets. By starting with Common Crawl data and applying rigorous quality control through text refinement and visual diversification, the pipeline ensures that only high-quality, semantically diverse pairs are retained. The cross-modal cross-batch filtering specifically targets alignment quality between images and text, which is crucial for effective VLP. The use of continued pretraining with SigLIP2 models demonstrates the dataset's effectiveness in improving model performance across various downstream tasks.

## Foundational Learning

**Chinese Vision-Language Pre-training (VLP)**: Why needed - Chinese VLP has been limited by lack of high-quality training data, hindering development of Chinese multimodal models. Quick check - Compare Chinese and English VLP model performance on cross-lingual tasks.

**Cross-Modal Alignment**: Why needed - Effective VLP requires strong alignment between visual and textual representations for tasks like image-text retrieval. Quick check - Measure retrieval performance on benchmark datasets.

**Data Filtering Pipelines**: Why needed - Raw web data contains significant noise and low-quality pairs that can degrade model performance. Quick check - Analyze the impact of each filtering stage on final dataset quality.

**Semantic Diversity**: Why needed - Balanced representation across concepts ensures models can generalize to diverse real-world scenarios. Quick check - Evaluate model performance across different semantic categories.

**Scaling Laws**: Why needed - Understanding how dataset size affects model performance is crucial for optimal resource allocation. Quick check - Plot performance vs. dataset size to identify scaling breakpoints.

## Architecture Onboarding

**Component Map**: Common Crawl Data -> Filtering Pipeline (4 stages) -> 100M High-Quality Pairs -> SigLIP2 Pretraining -> Downstream Tasks

**Critical Path**: The cross-modal cross-batch filtering stage is most critical as it directly ensures alignment quality between images and text, which is fundamental for VLP effectiveness.

**Design Tradeoffs**: The 90.46% reduction rate represents a significant quality improvement at the cost of data quantity. This aggressive filtering prioritizes quality over scale, which may limit the dataset's applicability for tasks requiring extremely large training sets.

**Failure Signatures**: 
- Low cross-modal alignment scores indicating poor image-text matching
- Semantic clustering suggesting lack of diversity in the dataset
- High duplicate rates revealing insufficient filtering
- Domain-specific performance gaps indicating coverage limitations

**First Experiments**:
1. Evaluate zero-shot classification performance on standard benchmarks
2. Test cross-modal retrieval accuracy using image-text matching
3. Assess novel concept understanding through out-of-distribution testing

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but implicit questions include: How does DanQing perform with other VLP architectures beyond SigLIP2? What is the optimal balance between filtering aggressiveness and dataset size? How well does the dataset generalize to specialized Chinese domains like traditional medicine or historical documents?

## Limitations
- Reliance on Common Crawl data may introduce inherent biases despite extensive filtering
- Performance comparisons focus primarily on SigLIP2 models, limiting generalizability to other VLP architectures
- The systematic filtering pipeline relies on heuristic thresholds that may not generalize to other languages or domains
- Claims of superiority should be interpreted cautiously without direct comparisons to the most recent vision-language datasets

## Confidence

**High confidence**: The dataset construction methodology and filtering pipeline are well-documented and reproducible. The claim of 100 million high-quality pairs after filtering is supported by the systematic four-stage process.

**Medium confidence**: Performance improvements over existing datasets are demonstrated, but the evaluation focuses primarily on SigLIP2 models, limiting generalizability to other VLP architectures.

**Low confidence**: The assertion that DanQing addresses a critical bottleneck in Chinese VLP development would benefit from more extensive comparisons with emerging datasets and broader model architectures.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of each filtering stage to final dataset quality and model performance.

2. Evaluate DanQing-pretrained models on Chinese-specific downstream tasks not mentioned in the paper, such as traditional Chinese medicine image analysis or Chinese historical document understanding.

3. Test the dataset's performance when used for pretraining different VLP architectures (e.g., CLIP, BLIP, or Flamingo) to assess its general applicability beyond SigLIP2 models.