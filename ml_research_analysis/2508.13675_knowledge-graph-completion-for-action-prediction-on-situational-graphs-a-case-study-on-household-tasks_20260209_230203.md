---
ver: rpa2
title: Knowledge Graph Completion for Action Prediction on Situational Graphs -- A
  Case Study on Household Tasks
arxiv_id: '2508.13675'
source_url: https://arxiv.org/abs/2508.13675
tags:
- knowledge
- action
- prediction
- graph
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates knowledge graph completion methods for predicting
  human actions in household tasks using the KIT Bimanual Actions Dataset. The dataset
  provides structured, frame-level annotations of bimanual activities like cooking
  and assembling, enabling the construction of situational knowledge graphs that model
  action sequences and object interactions.
---

# Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks

## Quick Facts
- arXiv ID: 2508.13675
- Source URL: https://arxiv.org/abs/2508.13675
- Reference count: 40
- Key outcome: KG embedding models achieved 3-6% Hits@5 for parent action prediction while frequency baselines reached 76-100% Hits@3

## Executive Summary
This paper evaluates knowledge graph completion methods for predicting human actions in household tasks using the KIT Bimanual Actions Dataset. The study compares embedding-based models, statistical baselines, and GPT-4o-mini on two tasks: identifying overall household tasks from action sequences and forecasting the next sub-action. Standard KG embedding models struggle with disconnected, sequential data, while simple heuristics and LLMs excel at coarse task recognition but fail on fine-grained predictions. The results suggest that KG completion in robotics requires task-specific graph structures and evaluation protocols to capture temporal and compositional dependencies.

## Method Summary
The study constructs situational knowledge graphs from the KIT Bimanual Actions Dataset, which provides structured, frame-level annotations of bimanual activities. The dataset contains 540 RGB-D recordings across 9 tasks, resulting in 13,928 nodes and 32,577 edges with 540 disconnected components. Three approaches are evaluated: frequency-based baselines (action-only and action+object), KG embedding models (TransE, TransR, ComplEx, DistMult, RotatE plus literal variants) via PyKeen, and GPT-4o-mini with few-shot prompting. The evaluation uses Hits@1, 3, and 5 metrics on a fixed test split (2 of 10 recordings per task).

## Key Results
- Frequency-based baselines achieved 76-100% Hits@3 for parent action prediction, while KG models reached only 3-6% Hits@5
- For sub-action prediction, statistical baselines led with ~82% Hits@1, graph models peaked at 52% Hits@1, and GPT-4o-mini dropped to 13%
- RotatE embeddings captured sequential "has next" relations more effectively than translational models (TransE) for fine-grained action forecasting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-based heuristics outperform standard embedding models for high-level task recognition in fragmented graphs.
- **Mechanism:** Statistical baselines map observed sub-sequences to goals via majority voting and co-occurrence counts, bypassing the need for dense graph topology.
- **Core assumption:** Strong deterministic correlation exists between specific object-action pairs and overall tasks.
- **Evidence anchors:** Abstract shows 76-100% Hits@3 for baselines vs 3-6% for graph models; section 4 details Baseline2's object context improvement.
- **Break condition:** Fails if tasks are highly ambiguous or training distribution differs significantly.

### Mechanism 2
- **Claim:** RotatE embeddings capture sequential "has next" relations more effectively than translational models.
- **Mechanism:** RotatE models relations as rotations in complex space, accommodating compositional sequential steps.
- **Core assumption:** Sequential dependencies follow patterns learnable via relational rotation.
- **Evidence anchors:** Section 4 shows RotatE at 52.16% Hits@1 vs TransE/TransR struggling with multistep dependencies.
- **Break condition:** Performance degrades with highly stochastic or non-Markovian action sequences.

### Mechanism 3
- **Claim:** LLMs provide strong semantic priors for coarse classification but fail on precise sequential forecasting.
- **Mechanism:** GPT-4o-mini uses few-shot prompting for semantic task recognition but lacks specific procedural memory for dataset execution order.
- **Core assumption:** LLM's pre-training includes sufficient household task knowledge for few-shot generalization.
- **Evidence anchors:** Abstract notes LLMs excel at coarse recognition but fail on fine-grained predictions; section 4 shows 13% Hits@1 for sub-action prediction.
- **Break condition:** Fails with exceeded context window, novel tasks, or sequence-order contradictions.

## Foundational Learning

- **Concept: Disconnected Knowledge Graph Topology**
  - **Why needed here:** The KIT dataset results in 540 weakly connected components, violating dense connectivity assumptions of standard KGC models.
  - **Quick check question:** Does the link prediction task require traversing between separate video recordings, or does it only operate within a single session's subgraph?

- **Concept: Hits@k Metric in Temporal Contexts**
  - **Why needed here:** High Hits@1 for sub-actions (~82% baseline) vs low Hits@5 for parent actions (3-6% graph models) indicates difference between deterministic sequences and ambiguous goal recognition.
  - **Quick check question:** If a model predicts the next action with 50% Hits@1, is it learning the task or just guessing the most frequent pause?

- **Concept: Semantic vs. Structural Link Prediction**
  - **Why needed here:** Parent action is largely semantic classification (LLMs/baselines win) while next action is structural transition (RotatE favors).
  - **Quick check question:** Is the relationship "has element" (sub-action to parent) defined by logical inference or graph topology?

## Architecture Onboarding

- **Component map:** JSON annotations (KIT Dataset) → Preprocessor: Bounding box intersection → Graph Builder: Creates 540 independent subgraphs → Models: Baseline counters/Majority Voting, PyKeen embeddings, GPT-4o-mini few-shot → Evaluator: Hits@k calculation

- **Critical path:** The Bounding Box Intersection logic is the primary data bottleneck; errors here break the has_object relation critical for high-performing Baseline2.

- **Design tradeoffs:**
  - **Baseline vs. KG:** Baselines are cheap and highly accurate for this dataset but lack generalization; KG models are theoretically flexible but failed due to fragmentation.
  - **LLM vs. Statistical:** LLMs offer easy reasoning without training but require API costs and prompt engineering; they failed on temporal precision.

- **Failure signatures:**
  - **TransE/TransR (0% Hits@1 for sub-action):** Indicates inability to learn sequential transitions due to disconnected components.
  - **LLM (13% Hits@1 for sub-action):** Hallucinating plausible but incorrect sequences (semantic plausibility ≠ ground truth).
  - **Graph Models (3-6% Parent Action):** Cannot aggregate local sub-actions into global parent concept without cross-component connectivity.

- **First 3 experiments:**
  1. **Reproduce Baseline2:** Implement the object-action co-occurrence counter to establish the upper bound for parent action prediction.
  2. **Test RotatE on has_next:** Train RotatE specifically on sub-sequence extraction to verify the 52% Hits@1 claim.
  3. **Ablate Context:** Run GPT-4o-mini with only action labels (no objects) to quantify how much object semantics contribute to its 78% parent prediction score.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can evaluation protocols be redefined to effectively handle disconnected graph components in link prediction tasks?
  - **Basis in paper:** Authors state future research should "redefine evaluation protocols for LP in disconnected graphs, moving beyond random link masking."
  - **Why unresolved:** Standard benchmarks assume dense connectivity, but situational graphs often consist of weakly connected components (540 in this study).
  - **What evidence would resolve it:** A novel evaluation framework that accounts for component boundaries and yields performance metrics correlating with downstream robotic task success.

- **Open Question 2:** Can hybrid architectures combining statistical baselines with KG relational reasoning outperform current methods on fine-grained action prediction?
  - **Basis in paper:** Conclusion suggests the need to "develop hybrid architectures that integrate baseline robustness with KG relational reasoning."
  - **Why unresolved:** Statistical baselines currently dominate sub-action prediction (82% Hits@1), while KG models struggle with sequential dependencies.
  - **What evidence would resolve it:** A model that combines frequency-based heuristics with embedding-based reasoning to surpass the current 81.7% baseline.

- **Open Question 3:** Do dynamic graph embeddings capture temporal action progression more effectively than the static models evaluated?
  - **Basis in paper:** Paper proposes to "explore dynamic graph embeddings to capture action progression."
  - **Why unresolved:** Static embedding models failed to model hierarchical and temporal nature of actions.
  - **What evidence would resolve it:** Results showing temporal embedding techniques significantly improve Hits@k on parent action prediction compared to static baselines (≤5.72%).

## Limitations

- Evaluation relies on a fixed train-test split of 2/10 recordings per task, which may not fully represent dataset variability.
- KG embedding hyperparameters (embedding dimension, learning rate, epochs, batch size, negative sampling) are not specified.
- GPT-4o-mini few-shot prompt examples and count are not detailed, making exact replication challenging.

## Confidence

- **High confidence:** Frequency-based baselines achieving 76-100% Hits@3 for parent action prediction (directly supported by results)
- **Medium confidence:** RotatE's superior performance for sub-action prediction (52% Hits@1) due to limited ablation studies
- **Low confidence:** Claims about disconnected components being the primary cause of poor embedding model performance (inferred rather than empirically validated)

## Next Checks

1. **Ablate Context:** Run GPT-4o-mini with only action labels (no objects) to quantify how much object semantics contribute to its 78% parent prediction score.
2. **Test Connectivity Impact:** Train a baseline KG model on a merged, connected version of the dataset (synthetic) versus the original disconnected components to isolate topology effects.
3. **Vary Hyperparameters:** Systematically test KG embedding models with different embedding dimensions and negative sampling rates to establish if poor performance is model-architecture or hyperparameter-related.