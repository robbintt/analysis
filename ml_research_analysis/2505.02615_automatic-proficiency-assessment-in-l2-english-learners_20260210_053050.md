---
ver: rpa2
title: Automatic Proficiency Assessment in L2 English Learners
arxiv_id: '2505.02615'
source_url: https://arxiv.org/abs/2505.02615
tags:
- proficiency
- assessment
- speech
- language
- wav2vec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep learning framework for automated assessment
  of L2 English proficiency using both speech and text data. It evaluates multiple
  architectures including 2D CNNs, ResNet, wav2vec 2.0 for speech and BERT for text,
  across structured speech, written essays, and spontaneous dialogues.
---

# Automatic Proficiency Assessment in L2 English Learners

## Quick Facts
- **arXiv ID**: 2505.02615
- **Source URL**: https://arxiv.org/abs/2505.02615
- **Reference count**: 40
- **Primary result**: wav2vec 2.0 achieves 75% accuracy on ANGLISH speech classification and up to 62% on spontaneous dialogue

## Executive Summary
This paper presents a deep learning framework for automated assessment of L2 English proficiency using both speech and text data. The authors evaluate multiple architectures including 2D CNNs, ResNet, wav2vec 2.0 for speech and BERT for text across structured speech, written essays, and spontaneous dialogues. Experiments are conducted under speaker-independent conditions on the ANGLISH, EFCamDat, and a private dataset. wav2vec 2.0 achieves the highest performance, with 75% accuracy on ANGLISH speech classification and up to 62% on spontaneous dialogue, outperforming traditional CNN and BERT-based models. Text-based models improve with fine-tuning and sequence optimization, but speech-based models generally perform better. Results demonstrate the effectiveness of deep learning for automated L2 assessment, particularly in capturing fluency and interactional competence, with multimodal approaches recommended for future work.

## Method Summary
The study evaluates deep learning models for L2 English proficiency assessment across three data types: structured speech, written essays, and spontaneous dialogues. Speech models include 2D CNNs, ResNet, and wav2vec 2.0, while text models employ BERT. Experiments use speaker-independent evaluation on three datasets: ANGLISH (speech), EFCamDat (essays), and a private dataset (spontaneous dialogue). Models are trained to classify proficiency levels, with performance measured by accuracy. The wav2vec 2.0 model is fine-tuned for speech tasks, while BERT is fine-tuned for text tasks. Results show speech-based models generally outperform text-based models, with wav2vec 2.0 achieving the highest accuracy across tasks.

## Key Results
- wav2vec 2.0 achieves 75% accuracy on ANGLISH speech classification
- Text-based BERT models improve from 59% to 62% accuracy with fine-tuning
- Speech-based models outperform text-based models across all tasks

## Why This Works (Mechanism)
The paper demonstrates that deep learning models can effectively capture linguistic patterns associated with L2 proficiency levels. Speech-based models, particularly wav2vec 2.0, excel at capturing prosodic features, fluency markers, and pronunciation patterns that correlate with proficiency. Text-based BERT models capture lexical complexity, grammatical accuracy, and discourse coherence. The speaker-independent evaluation setup ensures models generalize across different speakers rather than memorizing individual speech patterns. The combination of structured tasks (read speech, essays) and spontaneous dialogue provides a comprehensive assessment of both controlled language production and natural interactional competence.

## Foundational Learning
- **wav2vec 2.0**: Self-supervised speech representation learning that captures hierarchical acoustic features. Needed for robust speech feature extraction without manual feature engineering. Quick check: Verify pre-training on large speech corpus.
- **BERT**: Transformer-based language model for contextual text representation. Needed for capturing complex linguistic dependencies in written text. Quick check: Confirm pre-training on large text corpus.
- **Speaker-independent evaluation**: Testing models on speakers not seen during training. Needed to ensure generalization across different speakers. Quick check: Verify train/test speaker separation.
- **Multimodal assessment**: Combining speech and text features for comprehensive proficiency evaluation. Needed since human assessment relies on multiple modalities. Quick check: Examine feature fusion strategies.

## Architecture Onboarding

**Component Map**: wav2vec 2.0 -> Classification Head -> Proficiency Level; BERT -> Classification Head -> Proficiency Level; CNN/ResNet -> Classification Head -> Proficiency Level

**Critical Path**: Input data -> Feature extraction (wav2vec 2.0/BERT/CNN) -> Classification head -> Output probability distribution over proficiency levels

**Design Tradeoffs**: Speech models capture prosody and fluency but require audio data; text models capture lexical complexity but miss pronunciation; CNNs are simpler but less powerful than wav2vec 2.0; BERT requires more computational resources but captures deeper linguistic patterns.

**Failure Signatures**: Low accuracy on spontaneous dialogue indicates models struggle with natural conversation variability; poor performance on new speakers suggests overfitting to speaker-specific patterns; modest text model improvements suggest limited benefit from fine-tuning on limited L2 data.

**First Experiments**: 1) Compare wav2vec 2.0 with and without fine-tuning on ANGLISH dataset; 2) Test BERT model performance with different sequence lengths on EFCamDat; 3) Evaluate CNN baseline performance on private spontaneous dialogue dataset.

## Open Questions the Paper Calls Out
The paper identifies the need for multimodal fusion approaches that combine speech and text features for more comprehensive proficiency assessment. The authors also note that future work should explore attention mechanisms for combining modalities and investigate how different proficiency aspects (grammar, vocabulary, pronunciation, fluency) are captured by different model architectures.

## Limitations
- Absence of multimodal fusion experiments limits the assessment to single-modality approaches
- Use of private datasets restricts reproducibility and independent verification
- Modest improvements in text-based models suggest limitations in BERT fine-tuning for L2 assessment

## Confidence
- High confidence in wav2vec 2.0 speech model performance on structured speech tasks (75% accuracy)
- Medium confidence in text-based BERT model results due to limited fine-tuning scope and modest improvement (59% to 62% accuracy)
- Medium confidence in spontaneous dialogue assessment results (up to 62% accuracy) due to inherent data complexity

## Next Checks
1. Implement and evaluate multimodal fusion architectures combining wav2vec 2.0 speech features with BERT text embeddings using attention-based mechanisms
2. Conduct ablation studies removing specific model components (CNN layers, transformer heads) to quantify their individual contributions to performance
3. Test the trained models on additional L2 English datasets with different demographic distributions to assess cross-population generalizability