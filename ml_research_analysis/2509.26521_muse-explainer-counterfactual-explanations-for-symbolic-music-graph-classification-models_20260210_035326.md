---
ver: rpa2
title: 'MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification
  Models'
arxiv_id: '2509.26521'
source_url: https://arxiv.org/abs/2509.26521
tags:
- graph
- music
- explanations
- counterfactual
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUSE-Explainer, a counterfactual explanation
  method for graph neural networks in symbolic music analysis. The core idea is to
  generate musically coherent counterfactual explanations by applying small, interpretable
  edits (e.g., updating pitch, onset, duration, or adding/removing notes) to musical
  score graphs represented as heterogeneous directed graphs.
---

# MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models

## Quick Facts
- **arXiv ID**: 2509.26521
- **Source URL**: https://arxiv.org/abs/2509.26521
- **Reference count**: 22
- **Primary result**: Flip cadence detection predictions (PAC↔NC) with 1.4-5.0 operations and 92%+ accuracy using musically constrained graph edits

## Executive Summary
MUSE-Explainer generates counterfactual explanations for graph neural networks classifying symbolic music by applying minimal, musically coherent edits to heterogeneous directed score graphs. The method addresses out-of-distribution issues common in music graph modifications by restricting changes to five interpretable operations: updating pitch, onset, duration, adding notes, or removing notes. Each operation is constrained to preserve musical validity through domain-specific rules. The approach is validated on a cadence detection model using Mozart piano sonatas, successfully flipping predictions with sparse edits while maintaining interpretability.

## Method Summary
MUSE-Explainer takes a music score represented as a heterogeneous directed graph and a target label, then trains an inner model to learn which of five musically meaningful operations to apply. The inner model optimizes a composite loss balancing prediction flip (cross-entropy) against edit distance minimization (node differences and graph edit distance). This generates a chain of counterfactual graphs where each step applies a single edit, producing interpretable explanation sequences. The method is trained iteratively for 50-100 epochs with configurable hyperparameters controlling the trade-off between flip success and explanation sparsity.

## Key Results
- Successfully flips cadence detection predictions with 1.4-5.0 operations on average
- Achieves 92%+ flip accuracy for PAC→NC conversions, 8-32% for NC→PAC (reflecting class imbalance)
- Maintains musical validity through constrained operations while minimizing edit distance
- Generation time of ~67-135 seconds for 10 counterfactuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining perturbations to musically meaningful edit operations prevents out-of-distribution artifacts that would otherwise destabilize counterfactual generation.
- Mechanism: The system restricts all graph modifications to five operation types (update pitch, onset, duration, add note, remove note), each with domain-specific constraints—for example, onset updates must align with existing note onsets, and durations snap to standard musical values. This ensures the counterfactual remains within the training distribution.
- Core assumption: Musical validity corresponds to distributional validity; the original model's training data contained structurally similar edits.
- Evidence anchors:
  - [Page 6]: "To address this, we first considered which kinds of modifications are sensible for a musical score. We propose five musically meaningful edit operations... By restricting changes to these carefully crafted operations, we ensure that the resulting graphs remain within the valid distribution of music graphs."
  - [Page 3]: "Out-of-distribution issues typically occur when an input graph is modified by an explainer... In the context of music graphs, out-of-distribution problems are particularly pronounced due to the fundamental role of features and edges in encoding musical information."
  - [corpus]: MASCOTS (arXiv:2503.22389) addresses similar temporal-constraint challenges for time series counterfactuals, suggesting domain-constrained operations are a general pattern for structured data.
- Break condition: If the target model was trained on music with fundamentally different structural conventions (e.g., contemporary graphic notation), the predefined operations may not span the valid distribution.

### Mechanism 2
- Claim: A learnable inner model can discover which operation-parameter combinations flip predictions while minimizing edit distance.
- Mechanism: The inner model is trained via gradient descent on a composite loss: λ_ent for prediction flip (cross-entropy toward target label) plus λ_nd·D_nd + λ_gp·D_gp for distance minimization (node feature differences and graph edit distance). This jointly optimizes for validity and sparsity.
- Core assumption: The loss landscape is sufficiently smooth that gradient-based optimization can discover meaningful operation-parameter pairs; the inner model architecture can represent the necessary operation selection.
- Evidence anchors:
  - [Page 5]: "L(G, Ḡ, y, λ_gp, λ_nd, λ) = λ_ent(f(Ḡ), y) + (λ_nd D_nd(Ḡ, G) + λ_gp D_gp(Ḡ, G))"
  - [Page 4]: "This component then enters a training phase, using the inner model to learn both the type of operation to apply and its associated parameters."
  - [corpus]: No direct corpus comparison found for learnable operation selection in music counterfactuals; this appears novel to the domain.
- Break condition: If the black-box model has highly discontinuous decision boundaries, gradient-based inner model training may fail to converge or produce high-variance explanations.

### Mechanism 3
- Claim: Iterative application of single edits produces interpretable explanation sequences where each step is human-readable.
- Mechanism: Inspired by diffusion processes, the algorithm generates a chain of counterfactuals G₁, G₂, ..., G_n where each G_i = InnerModel(G_{i-1}). This yields progressive sparsification rather than one-shot generation.
- Core assumption: A small number of operations (observed: 1.4–5.0 on average) suffices to flip predictions; users can mentally track sequential modifications.
- Evidence anchors:
  - [Page 4]: "Similar to noise diffusion, we start with an input graph and progressively create noisier versions. However, instead of introducing random noise, we apply musically coherent modifications."
  - [Page 7, Table 1]: PAC→NC requires 1.4–2.3 operations; NC→PAC requires 1.6–5.0 operations across configurations.
  - [corpus]: D4Explainer (cited as [3]) uses discrete denoising diffusion for GNN explanations, confirming diffusion-inspired counterfactual generation as a viable strategy.
- Break condition: If a task requires coordinated multi-note changes (e.g., chord substitutions), single-operation iterations may be inefficient or fail to find valid paths.

## Foundational Learning

- Concept: **Heterogeneous Directed Graphs for Music**
  - Why needed here: The input representation uses typed nodes (note, beat, measure) and typed edges (onset, consecutive, during, rest). Understanding this schema is prerequisite to modifying it correctly.
  - Quick check question: Given two notes u and v where on(u) + dur(u) = on(v), what edge type connects them?

- Concept: **Counterfactual Explanation Theory**
  - Why needed here: The method's goal is to find minimal edits that change model output. You must distinguish counterfactuals from adversarial examples (the former prioritizes interpretability over imperceptibility).
  - Quick check question: What two properties does the MUSE-Explainer loss function jointly optimize?

- Concept: **Graph Neural Network Message Passing**
  - Why needed here: The black-box model being explained is a GNN; understanding how information propagates through the graph helps interpret why certain edits flip predictions.
  - Quick check question: If a note's prediction depends on its 2-hop neighborhood, what graph radius must the inner model potentially modify?

## Architecture Onboarding

- Component map:
  - Outer Explainer -> Inner Model -> Operation Suite -> Apply constrained edit -> Compute loss -> Backprop to inner model -> Repeat for N epochs -> Output counterfactual chain

- Critical path: Input graph → Inner model forward pass → Operation selection + parameterization → Apply constrained edit → Compute loss → Backprop to inner model → Repeat for N epochs → Output counterfactual chain

- Design tradeoffs:
  - λ balance: Higher λ_ent accelerates flip success but may produce larger edits; higher distance weights yield sparser counterfactuals but risk non-convergence.
  - Epochs per explanation: Paper shows 50 vs. 100 epochs had minimal accuracy impact, suggesting diminishing returns; prefer fewer epochs for faster iteration.
  - Operation constraints: Stricter constraints (e.g., snapping duration to quarter notes only) improve validity but reduce search space flexibility.

- Failure signatures:
  - Accuracy <50% with high operation counts: Inner model likely stuck in local minima; reduce distance weights or increase λ_ent.
  - Generated graphs with orphaned nodes (no edges): Note-removal operation not properly reconnecting neighbors; check operation implementation.
  - All counterfactuals use same operation type: Inner model collapsed to trivial solution; add operation diversity regularization or check gradient flow.

- First 3 experiments:
  1. Reproduce PAC→NC flip on a single Mozart measure with default λ settings (λ=2.0, λ_nd=λ_gp=0.1); log operation types selected and final distance. Confirms baseline implementation.
  2. Ablate the onset constraint (allow arbitrary onset values instead of aligning to existing notes); measure counterfactual validity via SMUG-Explain playback. Quantifies out-of-distribution degradation.
  3. Sweep λ_nd ∈ {0.01, 0.05, 0.1, 0.5} with fixed λ_ent=2.0; plot accuracy vs. average operation count. Identifies the sparsity-accuracy frontier for your target task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MUSE-Explainer be adapted for edge-level and graph-level classification tasks where modifying the input graph requires larger structural changes?
- Basis in paper: [explicit] The authors state, "we plan to extend our experiments to a broader range of graph neural network architectures, including edge-level and graph-level classification tasks" which may pose challenges regarding the magnitude of necessary modifications.
- Why unresolved: The current method is evaluated solely on node-level cadence detection; graph-level tasks require global decision boundaries which might demand different optimization strategies or loss functions to flip a prediction meaningfully.
- What evidence would resolve it: Successful application and evaluation of MUSE-Explainer on a graph-level dataset (e.g., composer classification) demonstrating that musically coherent counterfactuals can be generated efficiently for whole-score predictions.

### Open Question 2
- Question: How can the semantic interpretability of explanations be improved when operations (like adding a note) yield ambiguous musical insights?
- Basis in paper: [inferred] The discussion notes that while some edits (like removing a suspension) are clear, "others, such as adding a new note, are harder to interpret consistently, since their effect depends on broader harmonic and structural context."
- Why unresolved: The current framework identifies *what* to change to flip a label but does not fully bridge the gap to explaining *why* musically in complex contexts, risking explanations that are valid but opaque to musicologists.
- What evidence would resolve it: An extension of the framework that maps low-level graph edits to high-level musicological concepts, validated by user studies where experts consistently agree on the semantic meaning of the generated counterfactuals.

### Open Question 3
- Question: Are the five defined musically meaningful edit operations sufficient for diverse symbolic music domains outside of Classical piano sonatas?
- Basis in paper: [inferred] The methodology restricts edits to pitch, onset, duration, and note addition/removal to ensure validity. However, the experiments were conducted exclusively on Mozart piano sonatas.
- Why unresolved: Other musical forms or instruments (e.g., polyphonic choral works or orchestral scores) may require different types of modifications (e.g., changing voice leading, articulation, or instrument assignment) to generate valid counterfactuals.
- What evidence would resolve it: Experiments on diverse datasets showing that the current operation set achieves high validity and explanation success rates without needing additional domain-specific edit operations.

## Limitations
- Inner model architecture unspecified, creating reproducibility barriers
- Assumes musical validity equals distributional validity, which may not hold across genres
- Depends on gradient-based optimization which may fail on discontinuous decision boundaries

## Confidence
- **High Confidence**: The method's fundamental approach is logically sound and well-motivated
- **Medium Confidence**: Empirical results appear reasonable but lack detailed specification for verification
- **Low Confidence**: Generalizability to different music genres and more complex tasks remains untested

## Next Checks
1. **Architecture Sweep**: Systematically vary inner model depth (1-3 layers) and width (16-128 hidden units) to determine minimum viable architecture that achieves baseline accuracy on the cadence detection task.
2. **Out-of-Distribution Stress Test**: Generate counterfactuals for a model trained on contemporary piano music, then evaluate whether MUSE-Explainer's predefined operations remain valid or produce distribution-shifted graphs.
3. **Decision Boundary Analysis**: Measure the loss landscape smoothness around counterfactuals by generating multiple explanations for the same input-label pair; high variance indicates sensitivity to initialization or optimization instability.