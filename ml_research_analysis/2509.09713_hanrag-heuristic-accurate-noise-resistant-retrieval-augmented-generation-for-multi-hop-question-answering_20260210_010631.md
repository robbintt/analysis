---
ver: rpa2
title: 'HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation
  for Multi-hop Question Answering'
arxiv_id: '2509.09713'
source_url: https://arxiv.org/abs/2509.09713
tags:
- question
- query
- retrieval
- answer
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenges in Retrieval-Augmented Generation
  (RAG) for multi-hop question answering, specifically the over-reliance on iterative
  retrieval for compound queries, ineffective querying for complex queries, and noise
  accumulation during retrieval. The proposed HANRAG framework introduces a "Revelator"
  master agent that routes queries, decomposes compound problems, refines complex
  questions, and filters noise from retrieved documents.
---

# HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering

## Quick Facts
- **arXiv ID**: 2509.09713
- **Source URL**: https://arxiv.org/abs/2509.09713
- **Reference count**: 21
- **Primary result**: HANRAG improves multi-hop QA accuracy (EM, F1, accuracy) by adaptively routing queries and filtering retrieval noise, reducing retrieval steps compared to iterative methods.

## Executive Summary
HANRAG addresses key challenges in multi-hop question answering within retrieval-augmented generation systems. It introduces a "Revelator" master agent that intelligently routes different query types (straightforward, single-step, compound, complex) to appropriate processing strategies. The framework uses parallel retrieval for compound queries and iterative refinement for complex queries, while filtering noise from retrieved documents before generation. Experimental results show HANRAG outperforms existing methods on both single-hop and multi-hop benchmarks, achieving better accuracy with fewer retrieval steps.

## Method Summary
HANRAG employs a multi-agent architecture centered around a fine-tuned Llama-3.1-8B model called the "Revelator" that serves as Router, Decomposer, Refiner, and Relevance Discriminator. The system classifies incoming queries into four types and applies appropriate strategies: straightforward questions are answered directly, single-step queries use efficient retrieval, compound queries leverage parallel asynchronous retrieval, and complex queries use iterative synchronous refinement with seed question generation. The Revelator is trained via LoRA fine-tuning on synthetic datasets for routing, decomposition, and relevance discrimination tasks. A BM25 retriever fetches documents, which the Revelator filters for relevance before passing to a frozen Llama-3.1-8B generator for final answers.

## Key Results
- HANRAG achieves improvements in Exact Match (EM), F1, and accuracy scores across single-hop and multi-hop benchmarks
- The framework reduces retrieval steps compared to standard iterative retrieval methods
- Demonstrates superior adaptability, noise resistance, and efficiency across diverse query types

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Query Routing and Decomposition
The Revelator agent classifies queries as compound (parallel sub-queries) or complex (sequential reasoning), routing compound queries to parallel asynchronous retrieval and complex queries to sequential synchronous retrieval. This reduces wasteful iterative retrieval steps on compound queries. Core assumption: accurate classification between logical dependence and independence. Break condition: misclassification of complex queries as compound leads to irrelevant parallel retrieval.

### Mechanism 2: Iterative Seed Question Refinement
The Refiner module generates specific "seed questions" for each retrieval step based on previously gathered information, rather than using the original complex query repeatedly. This targets specific missing information for multi-hop reasoning. Core assumption: the LLM can identify knowledge gaps at each reasoning step. Break condition: hallucinatory or drifted seed questions break the reasoning chain.

### Mechanism 3: Post-Retrieval Noise Filtering
The Relevance Discriminator evaluates semantic relationships between queries and retrieved documents, filtering out irrelevant documents before generation. This mitigates noise accumulation in iterative loops. Core assumption: discriminator has lower false-positive rate than base retriever. Break condition: overly strict filtering causes "lost-in-retrieval" scenarios where the LLM must rely on parametric memory.

## Foundational Learning

- **Concept: Query Topology (Compound vs. Complex)**
  - **Why needed here**: HANRAG's primary optimization is routing based on this distinction
  - **Quick check**: Does the answer to sub-query A change the phrasing or retrieval target of sub-query B? (Yes = Complex, No = Compound)

- **Concept: Iterative Retrieval (IRCoT)**
  - **Why needed here**: Paper positions against standard iterative retrieval; understanding baseline cost is necessary
  - **Quick check**: In standard iterative RAG, what serves as the query for the 2nd retrieval step? (Usually the original query or a generated follow-up)

- **Concept: Fine-grained vs. Coarse-grained Filtering**
  - **Why needed here**: HANRAG filters entire documents rather than sentences to maintain context integrity
  - **Quick check**: Does the system filter out irrelevant sentences within a document, or entire documents? (HANRAG filters whole documents)

## Architecture Onboarding

- **Component map**: Revelator (Router/Decomposer/Refiner/Relevance Discriminator) -> BM25 Retriever -> ANRAG Module (Filter -> Generator) -> LLM Generator
- **Critical path**: The data construction pipeline for the Revelator - constructing `<Q, CLS>` for routing, `<Q, q1...>` for decomposition, and `<Q, D, Rel>` pairs for relevance discrimination before system can function
- **Design tradeoffs**: High training overhead requiring synthetic datasets; introduces extra LLM inference step (Revelator filtering) increasing per-step latency but potentially reducing total steps
- **Failure signatures**: Infinite loops (Ending Discriminator failure), early termination (gold document filtered out), router confusion (misclassifying complex as compound)
- **First 3 experiments**:
  1. Router Accuracy Audit: Measure classification accuracy against synthetic test set (83.93% mentioned in Appendix C)
  2. Ablate Relevance Filtering: Run without Revelator_rel step on noisy dataset to quantify performance drop
  3. Compound vs. Complex Latency: Measure wall-clock time for asynchronous vs synchronous paths to verify efficiency claims

## Open Questions the Paper Calls Out
- How can the data construction overhead required for training the Revelator agent be reduced or eliminated?
- To what extent does the sequential "seed question" generation strategy limit performance on complex queries requiring non-linear reasoning?
- How does the noise-resistance mechanism generalize to semantic dense retrieval methods compared to the lexical BM25 baseline?

## Limitations
- High training overhead due to requirement for extensive synthetic datasets for multi-task Revelator training
- Document-level noise filtering may be too coarse-grained for some tasks requiring sentence-level precision
- Additional LLM inference steps for Revelator filtering may offset latency improvements in resource-constrained environments

## Confidence
- **High Confidence**: Core architectural innovation of adaptive query routing and demonstrated improvements in EM/F1 scores on benchmark datasets
- **Medium Confidence**: Efficiency claims regarding reduced retrieval steps (lacks detailed latency measurements under identical hardware)
- **Low Confidence**: Generalizability of Revelator's routing accuracy to out-of-distribution queries (no out-of-distribution testing reported)

## Next Checks
1. Evaluate Revelator's routing accuracy on held-out multi-hop queries from different domains (e.g., biomedical) to assess generalization beyond Wikipedia training data
2. Measure wall-clock time for HANRAG versus standard iterative retrieval (IRCoT) on fixed hardware, including all LLM inference steps
3. Systematically vary noise levels in retrieved documents and measure performance degradation versus baseline RAG system to quantify relevance filtering benefit