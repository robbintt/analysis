---
ver: rpa2
title: 'Prompt Sentiment: The Catalyst for LLM Change'
arxiv_id: '2503.13510'
source_url: https://arxiv.org/abs/2503.13510
tags:
- sentiment
- prompt
- prompts
- language
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how prompt sentiment influences LLM-generated
  outputs across six domains, revealing significant effects on coherence, factuality,
  and bias. Using both lexicon-based and transformer-based sentiment analysis, 500
  prompts were categorized and evaluated with five leading LLMs (Claude, DeepSeek,
  GPT-4, Gemini, LLaMA).
---

# Prompt Sentiment: The Catalyst for LLM Change

## Quick Facts
- **arXiv ID**: 2503.13510
- **Source URL**: https://arxiv.org/abs/2503.13510
- **Reference count**: 17
- **Primary result**: Negative prompts reduce LLM factual accuracy by 8.4% and amplify bias, while neutral prompts yield most reliable outputs

## Executive Summary
This study systematically investigates how prompt sentiment influences LLM-generated outputs across six domains using 500 prompts transformed into positive, neutral, and negative variants. The research reveals that negative sentiment significantly reduces factual accuracy by 8.4% and amplifies bias in sensitive topics, while positive prompts increase verbosity by 8.1% and sentiment propagation. Across all domains, neutral prompts produced the most reliable and balanced outputs. The findings demonstrate that sentiment-aware prompt engineering is crucial for ensuring fair, factual, and unbiased AI-generated content, particularly in high-stakes applications like journalism, healthcare, and legal analysis.

## Method Summary
The study employed 500 prompts across six domains, each transformed into three sentiment variants (positive, neutral, negative) for a total of 1,500 prompts. Sentiment classification utilized both lexicon-based methods (VADER, TextBlob) and transformer-based approaches (BERT/RoBERTa fine-tuned on SST-2). Five leading LLMs (ChatGPT, Claude, DeepSeek, Gemini, LLaMA) were evaluated with controlled generation parameters. Outputs were assessed using multiple metrics: coherence (perplexity + semantic similarity), factuality (FEVER API, Google FactCheck), bias (sentiment drift analysis), and sentiment propagation (output vs. input sentiment strength). A composite quality score Q = λ₁C + λ₂F − λ₃B − λ₄Sₚ was used for ranking.

## Key Results
- Negative prompts reduce factual accuracy by 8.4% compared to neutral prompts
- Positive prompts increase output verbosity by 8.1% and amplify sentiment propagation
- Domain-dependent sentiment amplification: strongest in creative writing, weakest in technical documentation
- Negative sentiment increases bias in sensitive topics despite models having access to identical background knowledge

## Why This Works (Mechanism)

### Mechanism 1
LLMs amplify prompt sentiment in outputs with magnitude dependent on domain subjectivity. Models trained on human language statistically approximate sentiment patterns in training data. Subjective domains (creative writing, journalism) have richer affective patterns in training corpora, leading to stronger amplification. Objective domains (legal, technical) have reinforcement from professional-tone fine-tuning that suppresses emotional expression. Core assumption: Training data distribution and fine-tuning procedures modulate affective response strength.

### Mechanism 2
Negative prompt sentiment reduces factual accuracy by inducing speculative or alarmist language generation. Negative framing shifts model sampling toward lower-probability tokens associated with concern, urgency, or speculation. This departs from the high-probability factual patterns learned during pre-training, increasing hallucination risk. Core assumption: Emotional content interferes with precise information retrieval by biasing token selection away from neutral, factually-grounded patterns.

### Mechanism 3
Sentiment-driven prompts amplify bias in outputs, especially for socially sensitive topics. Sentiment acts as a framing cue that activates specific latent associations in model weights. Negative sentiment preferentially retrieves stereotypical or emotionally charged patterns from training data, even when neutral alternatives exist. Core assumption: Models store both neutral and biased associations; sentiment operates as a retrieval cue that preferentially activates biased patterns.

## Foundational Learning

- **Sentiment Analysis Methods (Lexicon vs. Transformer-based)**: Understanding how sentiment is classified (VADER/TextBlob vs. BERT/RoBERTa) is necessary to interpret the prompt categorization methodology and potential measurement bias. Quick check: Would a lexicon-based method capture sarcasm or context-dependent sentiment as accurately as a transformer-based classifier?

- **Perplexity and Semantic Similarity Metrics**: Coherence (CC) evaluation relies on perplexity and semantic similarity. Without understanding these, you cannot validate or reproduce the coherence findings. Quick check: Does lower perplexity always indicate higher-quality output, or can it reflect repetitive but low-information text?

- **Fact-Checking Benchmarks (FEVER)**: Factuality (FF) scores derive from FEVER and Google FactCheck API. Understanding their limitations is critical for interpreting the 8.4% factual degradation claim. Quick check: What types of claims are FEVER-style benchmarks most and least reliable for evaluating?

## Architecture Onboarding

- **Component map**: Prompt generation → sentiment transformation → sentiment validation → model inference → metric computation → quality aggregation

- **Critical path**: 1) Prompt generation → sentiment transformation, 2) Sentiment validation (92% inter-rater threshold), 3) Model inference across all sentiment variants, 4) Metric computation and comparison vs. neutral baseline

- **Design tradeoffs**: Lexicon-based sentiment analysis is interpretable but context-insensitive; transformer-based is more accurate but less transparent. Automated fact-checking scales but has coverage gaps; human annotation is reliable but expensive. Single composite score (Q) enables ranking but obscures individual metric tradeoffs.

- **Failure signatures**: Sentiment classifier disagrees with human raters >8% of time → invalid prompt categorization. Factual accuracy variance across sentiment variants <2% → sentiment effect may be domain-specific or model-specific. Sentiment propagation ratio near 1.0 for all domains → suggests neutralization fine-tuning or insufficient prompt sentiment intensity.

- **First 3 experiments**: 1) Replicate factual accuracy finding with a single model (e.g., GPT-4) on a 50-prompt subset, comparing neutral vs. negative sentiment on fact-checkable claims. 2) Test sentiment propagation in a single domain (e.g., creative writing) to validate amplification before expanding to all six domains. 3) Introduce a neutralization post-processing step (e.g., sentiment rewriting) on negative-prompt outputs to measure factual accuracy recovery.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does prompt sentiment interact with advanced prompt engineering techniques like chain-of-thought (CoT) reasoning? Basis: The authors state in Section 4.4 that "exploring the interaction between prompt sentiment and other prompt engineering techniques, such as chain-of-thought prompting [11], may yield deeper insights."

- **Open Question 2**: Do the observed patterns of sentiment propagation and factual degradation hold true across different languages and cultural contexts? Basis: Section 4.4 notes that the dataset represents a fraction of the potential space and suggests "future work should consider larger, more varied datasets, including cross-lingual and multicultural contexts."

- **Open Question 3**: Can real-time dynamic sentiment adaptation mechanisms mitigate the factual errors caused by negative prompting? Basis: Section 5 proposes "future research on... real-time sentiment-aware model adaptations to ensure more robust factual responses."

- **Open Question 4**: What specific training data characteristics cause domain-dependent sentiment amplification? Basis: The paper observes that models amplify sentiment in creative domains but neutralize it in technical ones, suggesting "underlying training regimes... play a role" (Section 4.1), but does not isolate the cause.

## Limitations

- Data coverage may not fully represent real-world LLM applications across all domains and use cases
- Model-specific effects are not fully explored, potentially limiting generalizability of findings
- Automated fact-checking APIs have known coverage gaps, particularly for domain-specific claims in legal and healthcare contexts

## Confidence

- **High Confidence**: Negative prompts increase sentiment propagation and verbosity compared to neutral prompts; sentiment effects are domain-dependent
- **Medium Confidence**: Negative prompts reduce factual accuracy by 8.4% compared to neutral prompts; sentiment-driven prompts amplify bias in sensitive topics
- **Low Confidence**: The composite quality score Q effectively captures overall output quality across all domains; the specific 8.4% factual degradation figure is robust across different evaluation methodologies

## Next Checks

1. **Replication with Human Fact-Checking**: Conduct human evaluation of factual accuracy on a 100-prompt subset, comparing neutral and negative sentiment variants to validate whether the 8.4% degradation holds with expert human fact-checkers.

2. **Cross-Model Sentiment Sensitivity Analysis**: Test the same 100 prompts across additional model families to determine whether sentiment sensitivity is a universal LLM property or varies significantly by architecture.

3. **Sentiment-Neutralization Intervention**: Apply post-generation sentiment rewriting to negative-prompt outputs and measure factual accuracy recovery to test whether observed degradation is reversible.