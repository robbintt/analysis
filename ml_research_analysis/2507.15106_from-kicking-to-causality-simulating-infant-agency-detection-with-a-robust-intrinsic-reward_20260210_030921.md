---
ver: rpa2
title: 'From Kicking to Causality: Simulating Infant Agency Detection with a Robust
  Intrinsic Reward'
arxiv_id: '2507.15106'
source_url: https://arxiv.org/abs/2507.15106
tags:
- agent
- reward
- cais
- action
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of robust agency detection in
  noisy environments, where standard reinforcement learning methods fail due to their
  reliance on correlational rewards. To overcome this, the authors introduce the Causal
  Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference
  that quantifies an action's influence by measuring the 1-Wasserstein distance between
  the learned distribution of sensory outcomes conditional on that action and the
  baseline outcome distribution.
---

# From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward

## Quick Facts
- **arXiv ID:** 2507.15106
- **Source URL:** https://arxiv.org/abs/2507.15106
- **Authors:** Xia Xu; Jochen Triesch
- **Reference count:** 33
- **Primary result:** CAIS enables robust agency detection in noisy environments where correlation-based methods fail

## Executive Summary
This work addresses the challenge of robust agency detection in noisy environments, where standard reinforcement learning methods fail due to their reliance on correlational rewards. To overcome this, the authors introduce the Causal Action Influence Score (CAIS), a novel intrinsic reward rooted in causal inference that quantifies an action's influence by measuring the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on that action and the baseline outcome distribution. Tested in a simulated infant-mobile environment with and without external noise, CAIS enabled the agent to reliably discover its causal efficacy in the noisy condition where correlation-based methods failed completely. Furthermore, the high-quality predictive model learned for CAIS, when augmented with a surprise signal, successfully reproduced the "extinction burst" phenomenon, demonstrating psychological plausibility and the synergy between causal inference and expectation violation.

## Method Summary
The authors developed CAIS (Causal Action Influence Score) as an intrinsic reward mechanism based on causal inference. The approach measures the 1-Wasserstein distance between the learned distribution of sensory outcomes conditional on an action and the baseline outcome distribution. They implemented this in a simulated infant-mobile environment using the MuJoCo physics engine, where an agent's leg movements cause a mobile to rotate. The framework includes a pre-trained visual encoder, a conditional distribution learner that predicts sensory outcomes given actions, and a policy trained to maximize CAIS. The method was tested against standard correlation-based approaches in both noise-free and noisy conditions.

## Key Results
- CAIS successfully detected agency in noisy environments where correlation-based methods completely failed
- The predictive model learned for CAIS, when combined with a surprise signal, reproduced the extinction burst phenomenon
- CAIS demonstrated psychological plausibility and synergy between causal inference and expectation violation

## Why This Works (Mechanism)
CAIS works by directly measuring causal influence rather than relying on correlation between actions and outcomes. In noisy environments, correlation-based methods fail because external disturbances create spurious correlations between actions and outcomes. CAIS overcomes this by learning the conditional distribution of sensory outcomes given each action and comparing it to the baseline distribution using the 1-Wasserstein distance metric. This approach isolates the causal effect of the action from external noise, enabling robust agency detection even when the environment is heavily disturbed.

## Foundational Learning
- **Causal inference in RL**: Why needed - To distinguish true causal relationships from spurious correlations in noisy environments; Quick check - Can the agent detect its own influence when external noise is present?
- **1-Wasserstein distance**: Why needed - To quantify the difference between outcome distributions in a way that's robust to noise; Quick check - Does the metric correctly identify causal influence versus correlation?
- **Predictive modeling for intrinsic rewards**: Why needed - To anticipate outcomes and detect violations that signal causal efficacy; Quick check - Does the model accurately predict outcomes and generate appropriate surprise signals?
- **Agency detection**: Why needed - To understand how agents (including infants) discover their ability to influence the world; Quick check - Can the agent consistently identify when its actions cause changes?

## Architecture Onboarding

**Component map:** Visual encoder -> Distribution learner -> CAIS calculator -> Policy optimizer -> Environment

**Critical path:** Action selection -> Environment response -> Visual encoding -> Distribution update -> CAIS calculation -> Policy update

**Design tradeoffs:** The use of pre-trained visual encoder sacrifices end-to-end learning capability for stability and faster convergence, while the discrete action space simplifies the distribution learning problem but limits generalizability to continuous control tasks.

**Failure signatures:** When CAIS fails to detect agency in noisy conditions, it manifests as the policy failing to learn any meaningful behavior, with actions appearing random and no convergence to high-reward states.

**First experiments:** 1) Test CAIS in noise-free environment to establish baseline performance; 2) Gradually increase noise levels to find the threshold where correlation-based methods fail but CAIS succeeds; 3) Compare CAIS with alternative distributional distance metrics to validate the choice of 1-Wasserstein distance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CAIS framework be computationally extended to model the developmental "contingency switch," where an agent's preference transitions from perfect self-contingency to high-but-imperfect social contingency?
- **Basis in paper:** [explicit] The Conclusion states, "The next steps include computationally modeling the 'contingency switch' itself, likely by introducing probabilistic action-outcome links..."
- **Why unresolved:** The current study successfully simulates only the "pre-switch" phase (detecting self-efficacy), whereas the switch to preferring imperfect social interaction requires a mechanism for valuing specific levels of probabilistic responsiveness.
- **What evidence would resolve it:** A modification of the CAIS reward or policy that causes the agent to eventually prefer an interacting partner with probabilistic delays over the perfect deterministic feedback of the self-controlled mobile.

### Open Question 2
- **Question:** Does the CAIS mechanism remain robust when integrated into an end-to-end learning framework that handles continuous motor control and online representation learning?
- **Basis in paper:** [explicit] The Conclusion notes: "Integrating the CAIS mechanism into agents that learn representations and continuous actions end-to-end remains a key long-term objective."
- **Why unresolved:** The current implementation relies on a pre-trained, frozen visual encoder and a discrete action space (binary joint activation), which abstracts away the noise and complexity of learning representations and fine-grained motor control simultaneously.
- **What evidence would resolve it:** Successful policy acquisition in the noisy mobile environment using a randomly initialized visual encoder trained concurrently with the CAIS model and a continuous action space.

### Open Question 3
- **Question:** Can the noise-filtering capabilities of the CAIS reward generalize to physical robotic platforms operating in the real world?
- **Basis in paper:** [explicit] The Conclusion lists "validating the agent's robustness by transferring it to a physical robotic platform" as a specific next step for future work.
- **Why unresolved:** While the MuJoCo simulation provides high fidelity, it still lacks the full complexity of real-world physics, sensor noise distributions, and actuation latencies that a physical robot would encounter.
- **What evidence would resolve it:** Successful transfer of the learning algorithm to a physical robot (e.g., an infant-like humanoid) that learns to identify its causal influence on a physical mobile despite real-world environmental noise.

## Limitations
- The approach relies on a pre-trained visual encoder, limiting end-to-end learning capabilities
- Results are validated only in a controlled simulation environment, not in real-world conditions
- The method uses discrete actions, which may not generalize well to continuous control tasks

## Confidence
- **CAIS effectiveness in noisy environments**: High - Demonstrated clear superiority over correlation-based methods in the simulated environment
- **Psychological plausibility of extinction burst**: Medium - Initial evidence provided but requires more rigorous validation
- **Generalizability to real-world scenarios**: Low - Limited to simulation environment with controlled conditions
- **End-to-end learning capability**: Low - Current implementation relies on pre-trained components and discrete actions

## Next Checks
1. Test CAIS in environments with varying levels of complexity, including multi-agent scenarios and non-stationary dynamics, to assess robustness beyond the controlled simulation
2. Conduct ablation studies to determine the sensitivity of CAIS performance to different distributional distance metrics (e.g., KL divergence, total variation) and evaluate whether 1-Wasserstein distance is indeed optimal
3. Design experiments to more rigorously validate the extinction burst phenomenon, potentially through comparison with empirical data from developmental psychology or through controlled experiments with human subjects interacting with similar systems