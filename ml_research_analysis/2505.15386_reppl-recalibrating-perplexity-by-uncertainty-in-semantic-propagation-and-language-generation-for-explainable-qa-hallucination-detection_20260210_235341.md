---
ver: rpa2
title: 'RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and
  Language Generation for Explainable QA Hallucination Detection'
arxiv_id: '2505.15386'
source_url: https://arxiv.org/abs/2505.15386
tags:
- uncertainty
- hallucination
- these
- methods
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RePPL addresses the challenge of detecting and explaining hallucinations
  in large language models (LLMs) by introducing a novel uncertainty recalibration
  framework. The method combines two key sources of uncertainty: semantic propagation
  (measured via token attribution discrepancies across sampled generations) and language
  generation (measured via confidence scores).'
---

# RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection

## Quick Facts
- arXiv ID: 2505.15386
- Source URL: https://arxiv.org/abs/2505.15386
- Reference count: 40
- Primary result: Achieves 0.833 average AUC across four QA datasets and three models

## Executive Summary
RePPL introduces a novel uncertainty recalibration framework for detecting hallucinations in large language models by combining two sources of uncertainty: semantic propagation (measured via token attribution discrepancies across sampled generations) and language generation (measured via confidence scores). The method produces both a total hallucination score and token-level explanations by aggregating these uncertainties in a Perplexity-style log-average form. Experiments demonstrate strong detection performance across multiple QA datasets, with RePPL outperforming baseline methods while providing explainable uncertainty scores for each token.

## Method Summary
RePPL works by first generating multiple sampled outputs and one greedy output for each input. It then extracts attention maps from all layers and heads during inference, converting these to attribution matrices using either AvgPool or Rollout methods. The InnerPPL component computes the coefficient of variation of attribution scores across samples to measure semantic propagation uncertainty. The OuterPPL component calculates log-probabilities of the greedy output normalized by the average length of sampled generations. These two uncertainty measures are combined multiplicatively to produce the final RePPL score, with token-level attribution variance providing explanations.

## Key Results
- Achieves 0.833 average AUC across four datasets (TriviaQA, Natural Questions, CoQA, SQuAD) and three models
- Outperforms baseline methods including EigenScore, G-Eval, and RoFT
- Provides token-level explanations that highlight which input and output tokens contribute to hallucinations
- Ablation studies show the importance of both InnerPPL and OuterPPL components

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hallucinations correlate with instability in how input tokens propagate semantics to output tokens across different generations.
- **Mechanism:** The method computes attention-based attribution matrices for multiple sampled responses. It measures the coefficient of variation of these attribution scores across samples. High variance indicates unstable "reasoning paths" from input to output.
- **Core assumption:** Assumption: Discrepancies in attention attribution across stochastic samples reflect epistemic uncertainty or lack of knowledge retrieval, rather than just linguistic diversity.
- **Evidence anchors:**
  - [abstract] "...uncertainty in semantic propagation, where attention mechanisms gradually fuse local token information..."
  - [section 3.2] "The coefficient of variation vector r quantifies the uncertainty of different input tokens."
  - [corpus] Related work "The Map of Misbelief" supports the link between attention patterns and hallucination types.

### Mechanism 2
- **Claim:** Standard perplexity fails to distinguish hallucinations because greedy generations are often shorter than sampled ones when the model is confident.
- **Mechanism:** RePPL re-calibrates language generation uncertainty by normalizing the log-probability sum by the average length of sampled generations rather than greedy path length. This ratio acts as a penalty factor.
- **Core assumption:** Assumption: Hallucinating models produce generations of similar length in greedy and sampling modes, whereas confident models "get to the point" faster in greedy mode.
- **Evidence anchors:**
  - [section 3.2] "Figure 3: Length of greedy outputs... basically shorter than sampling generations when LLM is certain."
  - [section 4.3] Ablation shows OuterPPL outperforms vanilla Perplexity on specific datasets like TriviaQA.

### Mechanism 3
- **Claim:** Multiplying semantic propagation uncertainty and generation uncertainty provides "hierarchical distinctness" superior to additive or single-metric approaches.
- **Mechanism:** RePPL combines InnerPPL and OuterPPL via multiplication. This ensures a hallucination score is high only if both the internal path is unstable and the external generation probability suggests risk.
- **Core assumption:** Hallucinations are not just low-probability events or just unstable internal states, but a failure in the consistency between state and output.
- **Evidence anchors:**
  - [abstract] "...dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form..."
  - [section 3.2] "This trait equipped our method more layered distinctness for better detection."

## Foundational Learning

- **Concept: Attention Attribution (Rollout/Average Pooling)**
  - **Why needed here:** InnerPPL relies on extracting a token-to-token contribution matrix from raw attention weights to see how input tokens influence outputs.
  - **Quick check question:** Does `AvgPool` capture the semantic flow better or worse than `MaxPool` for detecting unstable token contributions in this specific paper? (Answer: AvgPool performed best in ablation).

- **Concept: Coefficient of Variation (CV)**
  - **Why needed here:** The paper uses CV (Std/Mean) to measure uncertainty across samples because it is scale-independent, allowing comparison of attribution stability regardless of absolute attention weight magnitude.
  - **Quick check question:** Why is Standard Deviation alone insufficient for measuring attribution uncertainty across different generation lengths? (Answer: Magnitude varies; CV normalizes by the mean).

- **Concept: Sampling-based Uncertainty**
  - **Why needed here:** Unlike logits-based methods that look at one generation, RePPL requires multiple samples to derive statistics about the model's internal state variance.
  - **Quick check question:** What inference hyperparameter is critical to ensure enough randomness for RePPL to function? (Answer: Temperature $T=1.0$ and Top-K/P settings).

## Architecture Onboarding

- **Component map:** Sampler -> Attention Hook -> Attribution Pooling -> InnerPPL Calculator -> OuterPPL Calculator -> Aggregator
- **Critical path:** The extraction and pooling of attention maps. This is the most computationally intensive step.
- **Design tradeoffs:** Speed vs. Explainability (2.6s per example), Attribution Method (simple pooling vs. complex gradient-based methods)
- **Failure signatures:** High-confidence Hallucinations, Short Generations
- **First 3 experiments:**
  1. Sanity Check (Attribution): Run RePPL on factual vs. fictional prompts. Verify InnerPPL attribution heatmaps highlight proper nouns as high-uncertainty in fictional prompts.
  2. Ablation (Length Divisor): Compare AUC using standard length divisor vs. $\bar{S}$ divisor on TriviaQA. Confirm ~2-3% performance gain.
  3. Hyperparameter Stability: Vary number of samples N (5 vs 10 vs 20) to observe stability of Coefficient of Variation vector.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to detect high-confidence hallucinations that do not exhibit uncertainty signals?
- Basis in paper: [explicit] The Limitations section notes the method is inherently restricted to "uncertainty-based hallucinations" and struggles with "high-confidence hallucinations."
- Why unresolved: RePPL relies on variance across sampled generations and entropy; if a model confidently hallucinates consistently, these metrics will incorrectly indicate low risk.
- What evidence would resolve it: A modification incorporating internal state probes validated on datasets curated for confident but incorrect model outputs.

### Open Question 2
- Question: Does replacing simple attention pooling with state-of-the-art attribution methods improve the faithfulness of explanations without compromising efficiency?
- Basis in paper: [explicit] The authors note their explanations are "relatively coarse" due to a "trade-off in time complexity" that favored simple pooling over "computationally intensive state-of-the-art methods."
- Why unresolved: It is unclear if the performance gap between simple pooling and complex methods is significant enough to justify the computational cost.
- What evidence would resolve it: An ablation study measuring explanation faithfulness and runtime when using advanced attribution methods versus the simple AvgPool baseline.

### Open Question 3
- Question: Does the observed negative correlation between attribution importance and uncertainty generalize across non-QA tasks?
- Basis in paper: [explicit] The authors state their macro-level findings regarding the correlation between importance and uncertainty are "preliminary" and require "more comprehensive statistical investigations at a larger scale."
- Why unresolved: The current analysis is restricted to specific QA datasets; it is untested whether this correlation holds for summarization, translation, or reasoning tasks.
- What evidence would resolve it: A large-scale statistical analysis across diverse NLP tasks to confirm if high uncertainty consistently correlates with disrupted attention importance patterns.

## Limitations
- Struggles with high-confidence hallucinations that don't exhibit uncertainty signals
- Computationally intensive, requiring attention map extraction and multiple generations
- Explanations are relatively coarse due to simple pooling methods used for efficiency

## Confidence
- Attention Attribution Instability: **Medium** confidence
- Length-Based Calibration Assumption: **Medium** confidence  
- Combined Signal Effectiveness: **High** confidence for experimental setup, **Medium** for generalizability
- Computational Cost vs. Accuracy Tradeoff: **High** confidence

## Next Checks
1. **Sample Size Sensitivity**: Perform controlled experiment varying number of samples (N=5, 10, 20) to determine minimum sample size needed for stable InnerPPL variance calculations.
2. **Attribution Method Comparison**: Implement and compare Rollout method against AvgPool on same datasets to quantify performance difference.
3. **Length Constraint Robustness**: Test RePPL on tasks with fixed-length outputs to evaluate whether OuterPPL length normalization mechanism breaks down.