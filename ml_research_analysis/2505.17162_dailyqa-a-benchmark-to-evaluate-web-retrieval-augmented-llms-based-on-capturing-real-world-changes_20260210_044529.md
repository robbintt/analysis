---
ver: rpa2
title: 'DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing
  Real-World Changes'
arxiv_id: '2505.17162'
source_url: https://arxiv.org/abs/2505.17162
tags:
- queries
- query
- llms
- answer
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DailyQA, a benchmark for evaluating large language
  models (LLMs) on time-sensitive question answering using web retrieval. It constructs
  a weekly-updated query set paired with daily-updated answers derived from Wikipedia
  revision logs.
---

# DailyQA: A Benchmark to Evaluate Web Retrieval Augmented LLMs Based on Capturing Real-World Changes

## Quick Facts
- **arXiv ID**: 2505.17162
- **Source URL**: https://arxiv.org/abs/2505.17162
- **Reference count**: 8
- **Primary result**: DailyQA benchmark evaluates LLMs on time-sensitive QA using Wikipedia infobox revisions and web retrieval

## Executive Summary
This paper introduces DailyQA, a dynamic benchmark for evaluating large language models' ability to handle time-sensitive question answering with web retrieval. The benchmark constructs queries from Wikipedia infobox changes and answers from daily revision logs, creating a continuously updating test set that captures real-world factual changes. Experiments show that while reranking retrieved documents improves performance, handling temporal constraints remains challenging, particularly for frequently updated information where models must distinguish between multiple versions of the same fact.

## Method Summary
DailyQA is constructed by monitoring Wikipedia revision logs, extracting infobox changes, and using an LLM to generate natural language questions where the changed value is the answer. The benchmark is updated weekly with new queries and daily with answers. Evaluation uses retrieval-augmented generation pipelines with multiple models and conditions including basic search, snippet retrieval, document retrieval, reranking, and time-aware reranking. The task requires models to retrieve relevant web documents and synthesize current answers while avoiding outdated information.

## Key Results
- Reranking retrieved documents with bge-v2-m3 improved subset match from 0.356 to 0.392
- Frequent-update queries are systematically harder than infrequent-update queries, with ~30% lower performance
- Time-aware reranking (Rerank-T) degraded performance from 0.502 to 0.311, likely due to unreliable modification timestamps on web pages
- Increasing model scale helps, but reasoning-specialized models like DSRD-Qwen-32B underperformed standard instruct models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wikipedia infobox revisions provide a structured, low-redundancy signal for detecting factual changes that can be automatically converted into time-sensitive QA pairs.
- Mechanism: The pipeline monitors daily Wikipedia revision logs, filters to infobox changes only, converts these to Python dictionaries, and uses an LLM to generate natural language queries where the changed value is the answer.
- Core assumption: Infobox fields contain concise factual information with clear temporal validity boundaries, and changes represent meaningful real-world updates.
- Evidence anchors: [section] "wiki infoboxes tend to be well structured and purify factual information with little redundancy compared to the main text"
- Break condition: If infobox changes are primarily formatting/style edits rather than factual updates, query-answer pairs will be noisy or misleading.

### Mechanism 2
- Claim: Reranking retrieved documents by semantic relevance improves LLM performance on time-sensitive queries, but naive temporal reranking does not.
- Mechanism: Web search returns documents in search-engine order. Extracting full HTML text and reranking with a neural reranker (bge-v2-m3) improved subset match from 0.356 to 0.392.
- Core assumption: Document modification timestamps do not reliably indicate when the factual content became valid; many pages lack this metadata entirely.
- Evidence anchors: [abstract] "rerank of web retrieval results is critical"
- Break condition: If temporal metadata on web pages becomes more reliable and standardized, time-aware reranking could become viable.

### Mechanism 3
- Claim: Frequent-update queries are systematically harder than infrequent-update queries because retrieved documents contain more conflicting temporal information.
- Mechanism: Queries classified as "frequent update" showed ~30% lower subset match than "infrequent update" queries across all models.
- Core assumption: Web search surfaces documents from multiple timepoints; without effective temporal filtering, LLMs must reason about which version matches the query date.
- Evidence anchors: [section] "All models perform better on infrequent update queries than on frequent update queries"
- Break condition: If retrieval systems could accurately filter by content validity dates (not page modification dates), the frequent-update penalty would decrease.

## Foundational Learning

- Concept: **Temporal validity of retrieved information**
  - Why needed here: Understanding that a document's publication/modification date ≠ the validity period of its factual claims.
  - Quick check question: If you retrieve a sports statistics page today, how do you determine what date range its "current" statistics actually represent?

- Concept: **Reranking in retrieval pipelines**
  - Why needed here: Neural rerankers re-score query-document pairs after initial retrieval, improving precision but adding latency.
  - Quick check question: What's the difference between a search engine's first-stage ranking and a neural reranker's scoring? What signal does each optimize for?

- Concept: **Dynamic benchmark construction**
  - Why needed here: Static benchmarks become stale; DailyQA uses an automated pipeline to refresh queries weekly and answers daily.
  - Quick check question: How does a dynamic benchmark's evaluation signal differ from a static benchmark as model capabilities evolve over time?

## Architecture Onboarding

- Component map: Wiki Revision Monitor -> Query Generator -> Quality Checker -> Answer Extractor -> Evaluation Harness
- Critical path: 1. Query generation depends on infobox revision detection (weekly) 2. Answer extraction depends on daily revision log parsing 3. RAG evaluation depends on web search → HTML extraction → chunking → reranking → LLM generation
- Design tradeoffs: **Infobox-only vs. full-page**: Structured infoboxes reduce noise but miss changes in body text; paper argues infoboxes are "well structured and purify factual information"
- Failure signatures: **Low subset match with high retrieval**: Retrieved documents contain correct entity but wrong temporal version
- First 3 experiments:
  1. Replicate the "Search w/o Time + Rerank" pipeline on one week of DailyQA data with a smaller model (Qwen-7B-Instruct); verify subset match is in the 0.35-0.45 range
  2. Ablate the reranker: compare Doc (no rerank) vs. Rerank to quantify the ~10-15% SM improvement; analyze which query types benefit most
  3. Inspect failure cases on frequent-update queries: manually review 20 examples where the model retrieved relevant documents but selected the wrong temporal version; categorize error patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agentic RAG approaches that dynamically reformulate queries based on intermediate retrieval results outperform the static time-integrated retrieval methods?
- Basis in paper: [explicit] The authors state that "Precise retrieval through agentic RAG may be a promising approach in the future"
- Why unresolved: The paper only tested rule-based temporal integration, not adaptive query reformulation strategies
- What evidence would resolve it: A systematic comparison of agentic RAG systems against the established baselines on DailyQA

### Open Question 2
- Question: Can content-based temporal inference methods that extract effective information dates from document text improve performance on frequently-updated queries?
- Basis in paper: [explicit] The authors note that "the modification time of a web page is not equivalent to the effective time of the information"
- Why unresolved: The Rerank-T pipeline failed partly because webpage modification times are unreliable proxies for information currency
- What evidence would resolve it: Development and evaluation of a temporal extraction module that identifies effective dates from document content

### Open Question 3
- Question: Does the trade-off between reasoning capability and factual extraction accuracy persist across other reasoning-enhanced models on time-sensitive retrieval tasks?
- Basis in paper: [explicit] The authors found that "DSRD-Qwen-32B, which has been validated to have stronger inference, does not perform as well as the same-sized Qwen2.5-32B-Instruct"
- Why unresolved: Resource limitations prevented evaluation of state-of-the-art reasoning models
- What evidence would resolve it: Evaluation of GPT-o1 and DeepSeek-R1 on DailyQA with analysis separating extraction errors from reasoning errors

## Limitations

- The benchmark construction relies on Wikipedia infobox changes as a proxy for real-world events, potentially missing significant updates in article body text
- Automated quality control may miss subtle query-answer pairs where the answer is technically correct but misleading in context
- The assumption that infobox changes represent meaningful factual updates could be violated if Wikipedia's editing patterns shift

## Confidence

- **High confidence**: The core finding that reranking retrieved documents improves performance over raw search results
- **Medium confidence**: The claim that frequent-update queries are systematically harder than infrequent-update queries
- **Medium confidence**: The conclusion that time-aware reranking does not help and may hurt performance

## Next Checks

1. **Manual error analysis on frequent-update failures**: Select 50 frequent-update queries where models retrieved relevant documents but returned incorrect answers; categorize whether failures stem from outdated information in retrieved documents or missing date context

2. **Temporal metadata reliability study**: For a random sample of 100 web pages retrieved in the benchmark, manually verify whether modification dates actually correspond to when the factual content became valid

3. **Infobox vs. full-text comparison**: Run the same benchmark pipeline on a subset of queries using full Wikipedia article text instead of infoboxes only; compare answer accuracy and retrieval precision