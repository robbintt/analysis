---
ver: rpa2
title: 'Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks'
arxiv_id: '2504.01241'
source_url: https://arxiv.org/abs/2504.01241
tags:
- tasks
- forgetting
- learning
- performance
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses catastrophic forgetting in Large Language
  Models (LLMs) during sequential fine-tuning on multiple Natural Language Understanding
  (NLU) tasks. The research evaluates various open-source LLMs under 10 billion parameters
  on GLUE benchmark tasks (SST-2, MRPC, CoLA, MNLI) using continual fine-tuning with
  prompt engineering.
---

# Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks

## Quick Facts
- arXiv ID: 2504.01241
- Source URL: https://arxiv.org/abs/2504.01241
- Authors: Naimul Haque
- Reference count: 2
- One-line primary result: Phi-3.5-mini shows minimal forgetting (0.02) while maintaining strong learning, with Orca-2-7b achieving best overall performance (0.81 average accuracy) after fine-tuning

## Executive Summary
This study systematically evaluates catastrophic forgetting in Large Language Models during sequential fine-tuning on multiple Natural Language Understanding tasks. The research focuses on open-source LLMs under 10 billion parameters, examining their ability to learn new tasks while retaining knowledge of previously learned ones. Through continual fine-tuning with prompt engineering on GLUE benchmark tasks, the study reveals a fundamental trade-off between learning capacity and retention, with smaller models like Phi-3.5-mini demonstrating superior stability while larger models achieve higher learning rates at the expense of increased forgetting.

## Method Summary
The study employs continual instruction fine-tuning on GLUE benchmark tasks (SST-2, MRPC, CoLA, MNLI) using prompt engineering to transform datasets into instructional prompts. Base models under 10 billion parameters are sequentially fine-tuned on each task, with evaluation performed on all previously learned tasks after each fine-tuning episode. The methodology tracks two key metrics: "Forgetting" (maximum accuracy during training minus final accuracy) and "Learning" (maximum accuracy minus base accuracy), providing a quantitative framework for assessing the trade-off between plasticity and stability in sequential learning scenarios.

## Key Results
- Phi-3.5-mini exhibits minimal forgetting (0.02) while maintaining strong learning capabilities
- Orca-2-7b achieves the best overall performance (0.81 average accuracy) after fine-tuning
- Larger models show higher learning rates but increased forgetting compared to smaller models
- Smaller models can effectively balance learning and retention in sequential fine-tuning scenarios

## Why This Works (Mechanism)
Catastrophic forgetting occurs when neural networks overwrite prior knowledge during sequential training on new tasks. This study demonstrates that prompt engineering and model architecture choices significantly influence this phenomenon. By transforming datasets into instructional prompts, the models receive clearer learning objectives that help maintain task-specific knowledge boundaries. The observed trade-off between plasticity (ability to learn new tasks) and stability (ability to retain old knowledge) emerges from how weight updates during fine-tuning affect task-specific representations, with smaller models showing more localized updates that preserve existing knowledge structures.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - Why needed here: This is the central problem the paper addresses. Understanding that neural networks tend to overwrite prior knowledge when sequentially trained on new tasks is essential.
  - Quick check question: What would happen to model accuracy on Task A immediately after it is fine-tuned on Task B?

- **Concept: Instruction Fine-Tuning & Prompt Engineering**
  - Why needed here: The study's methodology relies on transforming datasets into specific instructional prompts. Understanding how prompts guide model behavior is key to replicating the experiment.
  - Quick check question: How does formatting a dataset sample as an instruction (e.g., "Analyze the sentiment...") change the model's learning objective compared to raw text completion?

- **Concept: Trade-off: Plasticity vs. Stability**
  - Why needed here: The paper identifies a core trade-off where models good at learning new tasks (plasticity) are bad at remembering old ones (stability). This frame is needed to interpret the results.
  - Quick check question: In the context of this paper, does a "high learning rate" metric typically correspond to low or high forgetting for larger models?

## Architecture Onboarding

- **Component map:**
  Base Model (M0) -> Prompt Engine (PE(X)) -> Fine-tuning Loop (M0 -> M1 -> ... -> Mn) -> Evaluation Harness (test Mi on all tasks {T1...Ti}) -> Metrics (Forgetting, Learning)

- **Critical path:**
  1. Select base model under 10B parameters
  2. Prepare GLUE tasks using defined prompt templates (Table 1)
  3. Execute fine-tuning sequentially
  4. **Crucial Step:** After training on Task Ti, immediately evaluate on all prior tasks {T1...Ti} to quantify degradation

- **Design tradeoffs:**
  - Model Selection: Choosing smaller model (Phi-3.5-mini) for stability (min forgetting) vs. larger one (Qwen2.5-7B) for peak performance (max learning)
  - Metrics: Using accuracy as sole metric, which may not capture nuances in model confidence or calibration

- **Failure signatures:**
  - Runaway Forgetting: Significant drop in accuracy on Task T1 after fine-tuning on Task T2 (e.g., >10-20% drop), particularly in larger models (>7B)
  - Zero-shot Failure: Base model performs very poorly on a task, meaning "learning" metric has little room to improve or may be learning from noise

- **First 3 experiments:**
  1. Baseline Run: Take stable model (Phi-3.5-mini) and replicate 4-task GLUE sequence. Verify observed forgetting is near reported ~0.02
  2. Ablation on Scale: Run same 4-task sequence with larger model (Llama-3.1-8B) and smaller one (Llama-3.2-1B) to compare stability-plasticity trade-off curve
  3. Prompt Robustness Check: Alter prompt templates and re-run fine-tuning on single task pair to test assumption that prompt engineering is key driver of retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do regularization-based or memory-replay strategies compare to standard sequential fine-tuning in mitigating forgetting for sub-10B parameter models?
- Basis in paper: [explicit] The conclusion states, "Future work should explore more advanced continual learning techniques to mitigate catastrophic forgetting."
- Why unresolved: The study isolated sequential fine-tuning to establish a baseline, deliberately excluding methods like Elastic Weight Consolidation (EWC) or Gradient Episodic Memory (GEM) mentioned in literature review.
- What evidence would resolve it: Comparative study applying EWC or replay mechanisms to specific model set (Phi-3.5-mini, Orca-2-7b) used in this paper.

### Open Question 2
- Question: Does observed trade-off—where higher learning rates correlate with increased forgetting—persist in models exceeding 10 billion parameters?
- Basis in paper: [inferred] Limitations section notes study "focused on models under 10 billion parameters," acknowledging results "may not generalize to larger models."
- Why unresolved: Experimental scope restricted to smaller architectures suitable for agentic workflows, leaving scaling laws of forgetting in larger models untested.
- What evidence would resolve it: Applying same continual fine-tuning protocol to larger variants (70B parameter models) to check if forgetting metrics scale linearly or change qualitatively.

### Open Question 3
- Question: To what extent does specific phrasing of task instructions (prompt engineering) mask or exaggerate true degree of catastrophic forgetting?
- Basis in paper: [inferred] Authors note that "Relying on prompt engineering may introduce biases affecting performance comparisons."
- Why unresolved: Study relied on fixed prompt templates (Table 1) to stabilize training, without testing how sensitive "forgetting" metric is to variations in these prompts.
- What evidence would resolve it: Ablation studies measuring performance variance when prompt format for Task A is altered after model has already been fine-tuned on Task B.

## Limitations
- Task sequence order is not explicitly specified, which could significantly affect forgetting patterns
- Accuracy is used as sole metric, potentially missing important aspects like calibration or confidence
- Reliance on prompt engineering introduces uncertainty as effectiveness may vary based on implementation details
- Focus on models under 10 billion parameters may limit generalizability to larger commercial models

## Confidence

- **High Confidence:** Phi-3.5-mini exhibits minimal forgetting (0.02) while maintaining strong learning capabilities
- **Medium Confidence:** Smaller models can effectively balance learning and retention
- **Low Confidence:** Larger models show higher learning rates at cost of increased forgetting

## Next Checks

1. **Reproduce with Specified Task Order:** Implement sequential fine-tuning with exact task order (SST-2 → MRPC → COLA → MNLI) to verify reported forgetting metrics, tracking per-task accuracy after each episode.

2. **Ablation Study on Model Scale:** Systematically fine-tune same 4-task GLUE sequence with models of varying scales (Phi-3.5-mini, Llama-3.1-8B, Llama-3.2-1B) to validate claimed stability-plasticity trade-off curve with consistent hyperparameters.

3. **Prompt Robustness Evaluation:** Modify prompt templates to be less explicit or use different prompt engineering strategy, then re-run fine-tuning on single task pair (SST-2 followed by MRPC) to assess whether prompt engineering is key driver of retention.