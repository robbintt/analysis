---
ver: rpa2
title: Deep Reinforcement Learning-Based User Association in Hybrid LiFi/WiFi Indoor
  Networks
arxiv_id: '2503.01803'
source_url: https://arxiv.org/abs/2503.01803
tags:
- user
- lifi
- network
- networks
- s-ppo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the user-access point (AP) association problem
  in hybrid indoor LiFi/WiFi networks, aiming to maximize the sum throughput while
  considering user mobility and practical capacity limitations. The problem is formulated
  as a nonconvex binary integer programming problem.
---

# Deep Reinforcement Learning-Based User Association in Hybrid LiFi/WiFi Indoor Networks

## Quick Facts
- arXiv ID: 2503.01803
- Source URL: https://arxiv.org/abs/2503.01803
- Authors: Peijun Hou; Nan Cen
- Reference count: 40
- Primary result: Proposed S-PPO method achieves 32.25% higher sum throughput and 19.09% better fairness than SSS, and 10.34% and 10.23% improvements over TRPO respectively

## Executive Summary
This paper addresses the user-AP association problem in hybrid indoor LiFi/WiFi networks, formulating it as a nonconvex binary integer programming problem to maximize sum throughput while considering user mobility and capacity constraints. The authors propose a sequential-proximal policy optimization (S-PPO) method that decomposes the action space into sequential user-AP association decisions to reduce computational complexity. Through extensive simulations, the S-PPO approach demonstrates significant performance improvements over traditional signal strength strategies and other deep RL baselines across various network topologies and capacity constraints.

## Method Summary
The authors formulate the user-AP association problem as a nonconvex binary integer programming problem that maximizes sum throughput while accounting for user mobility and practical capacity limitations. To solve this complex optimization problem, they propose a sequential-proximal policy optimization (S-PPO) method that leverages deep reinforcement learning. The S-PPO approach decomposes the action space into sequential decisions for user-AP associations, which reduces computational complexity compared to traditional methods. The method is evaluated through extensive simulations across various network topologies and capacity constraints, demonstrating superior performance compared to signal strength strategies and trust region policy optimization baselines.

## Key Results
- S-PPO outperforms signal strength strategy (SSS) by 32.25% in sum throughput and 19.09% in fairness
- S-PPO achieves 10.34% and 10.23% improvements over TRPO in sum throughput and fairness respectively
- Performance gains are demonstrated across various network topologies and capacity constraints

## Why This Works (Mechanism)
The S-PPO method works by decomposing the complex user-AP association problem into sequential decisions, which reduces the computational burden of exploring the full action space. By leveraging deep reinforcement learning, the algorithm can learn optimal association policies that adapt to user mobility patterns and capacity constraints. The sequential decomposition allows the agent to make decisions one user at a time, making the problem more tractable while still finding near-optimal solutions that maximize overall network throughput.

## Foundational Learning
- **Nonconvex binary integer programming**: Needed to accurately model the discrete nature of user-AP associations while capturing the complex relationship between user placement and network performance. Quick check: Verify that the problem formulation correctly represents all constraints and objectives.
- **Deep reinforcement learning**: Required to learn optimal association policies in dynamic environments with user mobility and capacity constraints. Quick check: Ensure the reward function properly reflects the optimization objectives.
- **Sequential action decomposition**: Used to reduce computational complexity by breaking down the joint association problem into sequential decisions. Quick check: Confirm that the sequential approach maintains solution quality while improving tractability.
- **Proximal policy optimization**: Employed to ensure stable learning and prevent large policy updates that could destabilize training. Quick check: Monitor KL divergence between policy updates to ensure stability.
- **Hybrid LiFi/WiFi networks**: The specific context requires understanding the complementary characteristics of both technologies and their impact on user association. Quick check: Validate that the model captures the key differences between LiFi and WiFi propagation and capacity characteristics.

## Architecture Onboarding

**Component Map:**
User Mobility -> State Representation -> S-PPO Agent -> Action Selection -> AP Association -> Network Throughput

**Critical Path:**
State observation (user positions, channel conditions) → Sequential action decomposition → PPO policy update → Action execution → Reward calculation (throughput) → Policy improvement

**Design Tradeoffs:**
The sequential decomposition reduces computational complexity but may sacrifice some optimality compared to joint optimization approaches. The choice of PPO over other RL algorithms balances stability and performance, though it may converge slower than some alternatives. The focus on throughput maximization comes at the cost of not considering other metrics like latency or energy efficiency.

**Failure Signatures:**
- Poor performance in highly dynamic mobility scenarios where sequential decisions cannot adapt quickly enough
- Suboptimal associations when capacity constraints create complex interdependencies between user assignments
- Degraded fairness when the algorithm prioritizes throughput over equitable resource allocation

**3 First Experiments:**
1. Test S-PPO performance in a simplified network with only two APs to validate basic functionality
2. Evaluate convergence behavior with varying learning rates and batch sizes
3. Compare performance against random association baseline to establish minimum expected gains

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on throughput maximization without considering other performance metrics like latency or energy efficiency
- Binary integer programming formulation assumes perfect channel state information, which may not reflect real-world conditions with measurement noise and delays
- Evaluation limited to controlled simulations without real-world testbed validation

## Confidence
- 32.25% throughput improvement over SSS: High
- 19.09% fairness improvement over SSS: High
- 10.34% and 10.23% improvements over TRPO: High
- Generalizability across different network topologies: Medium
- Performance under real-world measurement noise: Medium

## Next Checks
1. Test the S-PPO algorithm in a real-world hybrid LiFi/WiFi testbed to validate simulation results
2. Evaluate the algorithm's performance under varying channel measurement noise levels to assess robustness
3. Compare the proposed approach against additional baseline methods including traditional optimization techniques and other deep RL algorithms to establish its relative performance more comprehensively