---
ver: rpa2
title: 'Emergent Lexical Semantics in Neural Language Models: Testing Martin''s Law
  on LLM-Generated Text'
arxiv_id: '2511.21334'
source_url: https://arxiv.org/abs/2511.21334
tags:
- text
- training
- semantic
- martin
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically tests Martin's Law - the relationship
  between word frequency and polysemy - in neural language models across training.
  Using DBSCAN clustering of contextualized embeddings, the study analyzes four Pythia
  models (70M-1B parameters) at 30 training checkpoints.
---

# Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text

## Quick Facts
- **arXiv ID**: 2511.21334
- **Source URL**: https://arxiv.org/abs/2511.21334
- **Reference count**: 14
- **Primary result**: Martin's Law emerges non-monotonically during training, peaking at intermediate checkpoints before degrading

## Executive Summary
This study systematically tests Martin's Law - the relationship between word frequency and polysemy - in neural language models across training progression. Using DBSCAN clustering of contextualized embeddings from four Pythia models (70M-1B parameters) at 30 checkpoints, the research reveals a surprising non-monotonic developmental trajectory. Martin's Law compliance emerges around checkpoint 100, peaks at checkpoint 104 (r > 0.6), then degrades by checkpoint 105. Smaller models experience catastrophic semantic collapse at late checkpoints, while larger models show graceful degradation. The frequency-specificity tradeoff remains stable (r ≈ -0.3) across all models.

## Method Summary
The study analyzes Pythia models at 30 logarithmically spaced checkpoints, generating 100 samples of 512 tokens each per checkpoint with temperature=1.0. For each model, final-layer hidden states are extracted and filtered to retain alphabetic tokens ≥3 characters. DBSCAN clustering (ε=0.3, min_samples=2, cosine metric) is applied to contextualized embeddings for each word type, with polysemy counted as the number of clusters excluding noise points. Spearman correlation is computed between log(frequency) and polysemy count for the top 500 words with at least 5 occurrences. The frequency-specificity tradeoff is measured by correlating word frequency with the inverse of the mean cosine distance to cluster centroids.

## Key Results
- Martin's Law compliance emerges non-monotonically: appears around checkpoint 100, peaks at checkpoint 104 (r > 0.6), then degrades by checkpoint 105
- Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation
- Frequency-specificity tradeoff remains stable across all models (r ≈ -0.3)
- The optimal semantic window occurs at intermediate training stages rather than at convergence

## Why This Works (Mechanism)
The observed non-monotonic trajectory suggests that neural networks initially learn to associate high-frequency words with multiple senses during early training, reaching optimal semantic representation at intermediate stages. As training progresses beyond this point, the models appear to overfit or collapse semantic distinctions, particularly in smaller architectures. The frequency-specificity tradeoff remaining stable indicates that the inverse relationship between word frequency and contextual specificity is a more fundamental property that emerges earlier and persists throughout training.

## Foundational Learning
- **Martin's Law**: Empirical linguistic observation that word frequency correlates with polysemy - why needed to establish baseline linguistic regularity, quick check: verify correlation in human text corpora
- **DBSCAN clustering**: Density-based algorithm for identifying clusters in embedding space - why needed to operationalize "sense" from contextualized representations, quick check: test clustering on synthetic data with known clusters
- **Contextualized embeddings**: Token representations that vary based on surrounding context - why needed to capture word meaning variation across contexts, quick check: visualize embeddings for polysemous words
- **Spearman correlation**: Non-parametric measure of monotonic relationship - why needed to assess frequency-polysemy relationship without assuming linearity, quick check: compare with Pearson correlation
- **Polysemy detection**: Identifying distinct meanings of words - why needed as operational definition of semantic complexity, quick check: validate against human judgments
- **Training checkpoint analysis**: Evaluating model properties across training progression - why needed to understand developmental trajectory, quick check: verify checkpoint ordering matches training steps

## Architecture Onboarding

### Component Map
Data Generation -> Embedding Extraction -> Token Filtering -> DBSCAN Clustering -> Polysemy Counting -> Correlation Analysis

### Critical Path
The critical path flows from data generation through embedding extraction to clustering and analysis. The DBSCAN clustering step is most sensitive to parameter choice (ε, min_samples) and directly determines polysemy counts. Embedding extraction quality and sample size significantly impact downstream clustering stability.

### Design Tradeoffs
- Sample size (100×512) vs. computational cost: Larger samples would provide more stable frequency estimates but increase processing time quadratically
- ε parameter in DBSCAN: Smaller values capture finer distinctions but risk splitting true senses; larger values merge senses but may miss genuine polysemy
- Token filtering threshold: Stricter filtering (e.g., ≥5 characters) reduces noise but may exclude meaningful short words
- Minimum occurrence threshold: Higher thresholds (e.g., ≥10) ensure frequency stability but reduce vocabulary size

### Failure Signatures
- All words returning 0 clusters indicates ε too small or embeddings collapsed
- Extremely high cluster counts suggests ε too large or embeddings too dispersed
- Unstable correlations across runs indicates insufficient sample size
- Catastrophic collapse in small models suggests training instability rather than semantic degradation

### First Experiments
1. Vary ε from 0.2 to 0.5 in DBSCAN to test robustness of polysemy detection
2. Increase sample size from 100 to 500 sequences per checkpoint to assess stability
3. Compare results using alternative clustering methods (e.g., hierarchical clustering) against DBSCAN

## Open Questions the Paper Calls Out
- Does the peak Martin's Law correlation reflect human-like semantic structure without a baseline from human-written text?
- Do training interventions like learning rate scheduling causally affect the semantic peak location?
- Is the non-monotonic trajectory robust to variations in clustering algorithms and hyperparameters?

## Limitations
- DBSCAN parameter ε=0.3 is empirically chosen without theoretical justification
- Sample size of 51,200 tokens per checkpoint may be insufficient for stable frequency estimates
- Interpretation of cluster counts as polysemy assumes DBSCAN accurately captures semantic distinctions
- No comparison with human-authored text to establish ground truth for Martin's Law

## Confidence
- **High confidence**: Non-monotonic relationship between training and Martin's Law compliance is robust across models
- **Medium confidence**: Optimal semantic window around checkpoint 104 may be model-specific to Pythia
- **Low confidence**: Claim that non-compliance indicates "less human-like" representation lacks direct testing

## Next Checks
1. Systematically vary ε in DBSCAN from 0.2 to 0.5 across representative checkpoints to test robustness
2. Increase sample size to 500 sequences per checkpoint (256,000 tokens) to assess stability
3. Implement alternative polysemy metrics (e.g., entropy-based contextual diversity) and compare against DBSCAN results