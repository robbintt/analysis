---
ver: rpa2
title: 'The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold'
arxiv_id: '2511.01938'
source_url: https://arxiv.org/abs/2511.01938
tags:
- learning
- dynamics
- loss
- zero-loss
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a formal understanding of grokking by showing
  that post-memorization learning dynamics minimize weight norm constrained to the
  zero-loss manifold. The key insight is that after perfect memorization, gradient
  descent is effectively driven by weight decay alone, pushing parameters toward the
  minimum-norm solution on the zero-loss set.
---

# The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold

## Quick Facts
- arXiv ID: 2511.01938
- Source URL: https://arxiv.org/abs/2511.01938
- Reference count: 17
- Primary result: Proves grokking occurs through weight decay-driven norm minimization on the zero-loss manifold after perfect memorization

## Executive Summary
This paper provides a formal understanding of grokking by showing that post-memorization learning dynamics minimize weight norm constrained to the zero-loss manifold. The key insight is that after perfect memorization, gradient descent is effectively driven by weight decay alone, pushing parameters toward the minimum-norm solution on the zero-loss set. The paper proves that loss gradients become orthogonal to the zero-loss manifold as models approach it, leaving weight decay as the sole learning driver. To isolate component dynamics, the authors introduce an approximation where one parameter subset is optimized while treating others as optimal for the current state. Applied to a two-layer network, this yields a closed-form expression for the first layer's post-memorization dynamics. Experiments on modular addition confirm that simulated dynamics reproduce both delayed generalization and circular representation learning characteristic of grokking.

## Method Summary
The authors develop a theoretical framework for understanding grokking through the geometry of the zero-loss manifold. They prove that loss gradients become orthogonal to this manifold as models approach perfect memorization, leaving weight decay as the sole optimization driver. To analyze the dynamics, they introduce an approximation scheme that isolates parameter subset updates by treating other parameters as fixed at their current optimal values. For a two-layer network, this yields a closed-form solution for post-memorization dynamics based on ridge regression. The approach is validated through experiments on modular addition tasks, showing that the theoretical predictions match observed grokking behavior including delayed generalization and circular representation development.

## Key Results
- Proves loss gradients become orthogonal to the zero-loss manifold as models approach perfect memorization
- Shows weight decay drives dynamics on the zero-loss manifold after memorization
- Derives closed-form expression for two-layer network dynamics post-memorization
- Experiments on modular addition reproduce delayed generalization and circular representations
- Validates approximation method against full optimization trajectories

## Why This Works (Mechanism)
The mechanism works because after perfect memorization, the loss landscape becomes flat along directions tangent to the zero-loss manifold. As gradients become orthogonal to this manifold, weight decay (L2 regularization) becomes the dominant force shaping the optimization trajectory. This drives parameters toward the minimum-norm solution on the zero-loss set, which corresponds to solutions with better generalization properties. The approximation scheme isolates these dynamics by treating one parameter subset as fixed while optimizing the other, enabling tractable analysis of the post-memorization phase.

## Foundational Learning
- **Zero-loss manifold**: The set of all parameters achieving perfect training accuracy. Why needed: Central geometric object where post-memorization dynamics occur. Quick check: Verify models reach zero training loss before generalization.
- **Gradient orthogonality**: Loss gradients becoming perpendicular to the zero-loss manifold. Why needed: Explains why weight decay dominates post-memorization. Quick check: Measure angle between gradients and manifold tangents.
- **Ridge regression**: The optimization problem that emerges for post-memorization dynamics. Why needed: Provides closed-form solution for parameter updates. Quick check: Compare theoretical predictions with actual parameter evolution.
- **Isolated dynamics approximation**: Treating parameter subsets independently during optimization. Why needed: Enables tractable analysis of complex multi-parameter systems. Quick check: Validate approximation accuracy against full optimization.
- **Weight decay as sole driver**: L2 regularization becomes the only effective learning signal. Why needed: Explains delayed generalization mechanism. Quick check: Remove weight decay and observe loss of generalization.
- **Manifold confinement**: Training dynamics remain on the zero-loss manifold after memorization. Why needed: Ensures stability of post-memorization learning. Quick check: Monitor training loss during generalization phase.

## Architecture Onboarding

**Component Map**: Input -> First Layer -> Second Layer -> Output

**Critical Path**: The post-memorization dynamics where weight decay drives parameters toward minimum-norm solution on zero-loss manifold.

**Design Tradeoffs**: The two-layer architecture enables closed-form analysis but may not capture all dynamics present in deeper networks. The modular addition task provides clean experimental conditions but may not generalize to all grokking scenarios.

**Failure Signatures**: 
- Loss gradients not becoming orthogonal to zero-loss manifold
- Weight decay failing to drive parameters toward minimum-norm solution
- Training dynamics leaving the zero-loss manifold after memorization
- Lack of delayed generalization despite perfect memorization

**First Experiments**:
1. Verify orthogonality of loss gradients to zero-loss manifold during training
2. Confirm weight decay drives parameters toward minimum-norm solution
3. Test approximation scheme accuracy against full optimization trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the constrained norm minimization framework hold for cross-entropy loss, or is it specific to mean-squared error?
- Basis in paper: [explicit] The "Limitations" section states, "This work does not cover cross-entropy loss, which is commonly used in practice."
- Why unresolved: The theoretical proofs rely on the geometric properties of the MSE zero-loss manifold; cross-entropy landscapes may behave differently.
- What evidence would resolve it: Extending the gradient orthogonality proofs (Theorem 4.14) to cross-entropy or empirically verifying norm minimization dynamics in classification tasks.

### Open Question 2
- Question: Can the isolated dynamics approximation be generalized to networks with more than two layers?
- Basis in paper: [explicit] The authors note their work is "limited to the case of two-layer networks" and cite "exciting challenges" in understanding more complex architectures.
- Why unresolved: The closed-form solution for isolated dynamics relies on ridge regression (Eq. 15), which treats the second layer as a linear readout; deeper non-linear stacks likely lack such tractable optimal solutions.
- What evidence would resolve it: Deriving a tractable approximate cost function for a hidden layer in a 3+ layer network or showing the breakdown of the 2-layer approximation.

### Open Question 3
- Question: How do finite learning rates and weight decay coefficients distort the training trajectory compared to the infinitesimal limit?
- Basis in paper: [inferred] The proofs rely on Assumption 4.3 (Gradient Flow) and Assumption 4.5 (Vanishing Weight Decay, λ → 0).
- Why unresolved: The paper only formally proves stability and orthogonality in the continuous limit; discrete updates with large step sizes could destabilize the confinement to the zero-loss manifold.
- What evidence would resolve it: A theoretical bound on the maximum learning rate/weight decay ratio that maintains the orthogonality and stability properties observed in the limit.

## Limitations
- Analysis limited to two-layer networks and MSE loss
- Relies on gradient flow and vanishing weight decay assumptions
- Approximation scheme may not capture all dynamics in deeper architectures
- Theoretical framework not extended to cross-entropy loss used in practice

## Confidence

**Major Claim Clusters and Confidence:**
- **High Confidence**: The orthogonality of loss gradients to the zero-loss manifold as models approach perfect memorization is well-supported by both theory and experimental evidence.
- **Medium Confidence**: The weight decay-driven dynamics on the zero-loss manifold as the primary mechanism for delayed generalization, based on theoretical arguments and toy model experiments.
- **Low Confidence**: The generality of the approximation scheme for decomposing parameter dynamics to more complex architectures beyond the studied two-layer case.

## Next Checks
1. Test the theoretical predictions on diverse grokking-prone tasks (e.g., other arithmetic operations, small-scale image classification) to assess the framework's generality.
2. Experimentally verify the orthogonal gradient hypothesis in deeper networks by measuring gradient angles relative to the zero-loss manifold throughout training.
3. Evaluate whether the approximation method extends to multi-layer networks by comparing predicted dynamics against full optimization trajectories on controlled synthetic tasks.