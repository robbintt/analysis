---
ver: rpa2
title: Analysis of Robustness of a Large Game Corpus
arxiv_id: '2504.03940'
source_url: https://arxiv.org/abs/2504.03940
tags:
- levels
- game
- level
- games
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal measure of robustness for structured
  discrete data, applied to game levels, which are highly sensitive to small changes
  due to hard constraints like solvability. The authors define non-robustness as the
  probability that small input perturbations cause output label changes, and compare
  this across game datasets and standard ML datasets.
---

# Analysis of Robustness of a Large Game Corpus

## Quick Facts
- arXiv ID: 2504.03940
- Source URL: https://arxiv.org/abs/2504.03940
- Reference count: 40
- Primary result: Single-tile changes cause solvability changes in up to 79% of cases, far exceeding sensitivity in standard ML datasets like MNIST or CIFAR-10

## Executive Summary
This paper introduces a formal measure of robustness for structured discrete data, applied to game levels which are highly sensitive to small changes due to hard constraints like solvability. The authors define non-robustness as the probability that small input perturbations cause output label changes, and compare this across game datasets and standard ML datasets. They generate a large corpus (GGLC) of over 80,000 levels across four 2D tile-based games, with solutions included. Experiments show that single-tile changes cause solvability changes in up to 79% of cases, far exceeding sensitivity in datasets like MNIST or CIFAR-10. The GGLC is intended to address data sparsity in procedural content generation and enable more robust ML models for structured domains.

## Method Summary
The paper defines non-robustness as the probability that points within distance r share different labels, adapting adversarial robustness metrics from classifiers to data distributions. For discrete game levels, this uses Hamming distance at r=1 to measure single-tile perturbations. The Generated Game Level Corpus (GGLC) was created using a constraint-based level generator called Sturgeon, which encodes tile placements as Boolean variables with pathfinding constraints and tile-rewrite rules. A solver portfolio (RC2, clingo, HiGHS) races in parallel to generate solvable levels, while unreachability constraints generate unsolvable variants. The corpus contains over 80,000 levels across four games with both image and text representations plus solutions.

## Key Results
- Single-tile changes cause solvability changes in 17-79% of cases across games (crates: 78.9%, cave: 43.1%, platform: 17.1%, vertical: 17.5%)
- Non-robustness values for game datasets are substantially higher than standard ML datasets (MNIST, CIFAR-10, CLIP-based embeddings)
- The GGLC corpus contains 80,000+ levels with complete solutions, enabling new research in robust PCGML

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-tile perturbations in game levels cause output label changes (solvability/acceptability) at substantially higher rates than single-pixel changes in standard ML datasets.
- Mechanism: Game levels encode hard constraints (global: solvability; local: structural coherence) where individual tiles carry functional rather than merely visual significance. A tile representing a key, door, or platform edge is a discrete decision point that can break the entire constraint graph.
- Core assumption: The solvability label reflects an underlying ground truth about functional validity, not just classifier prediction error.
- Evidence anchors:
  - [abstract] "single-tile changes cause solvability changes in up to 79% of cases, far exceeding sensitivity in datasets like MNIST or CIFAR-10"
  - [section 6.1 Table 2] crates: 78.9% changed solvability; cave: 43.1%; platform: 17.1%; vertical: 17.5%
  - [corpus] Related work "Evolutionary Level Repair" addresses similar functional repair problems, suggesting this is a recognized domain challenge.
- Break condition: If game mechanics were redesigned to make all tiles functionally redundant (no single point of failure), non-robustness would decrease toward image-dataset levels.

### Mechanism 2
- Claim: Non-robustness can be measured formally as the probability that points within distance r share different labels, adapting adversarial robustness metrics from classifiers to data distributions.
- Mechanism: The formulation \(ND_r(D) = P_{x \sim D}[Label(x) \neq Label(x') | \exists x', d(x,x') \leq r]\) quantifies label instability directly in the data manifold, independent of any classifier. For discrete data, Hamming distance at r=1 captures single-tile perturbations.
- Core assumption: The distance metric (Hamming for discrete, Euclidean in CLIP embedding for continuous comparison) meaningfully captures "small" perturbations relevant to the domain.
- Evidence anchors:
  - [section 3] Full mathematical derivation adapting classifier robustness (Equation 1) to data robustness (Equation 2-3)
  - [section 5.2] Discrete form uses Hamming distance r=1; continuous form uses CLIP embeddings with UMAP reduction
  - [corpus] No direct corpus evidence for this specific formalization; appears novel to this work.
- Break condition: If the embedding space poorly aligns with functional similarity (e.g., CLIP prioritizes visual over functional similarity), continuous-form measurements may misrepresent true non-robustness.

### Mechanism 3
- Claim: Constraint-based level generation with solver portfolios can produce large corpora where solvable and unsolvable variants exist with controlled properties.
- Mechanism: Sturgeon encodes tile placements as Boolean variables with pathfinding constraints (reachability from start to goal) and tile-rewrite rules (playthrough generation). Solver portfolios (RC2, clingo, HiGHS) race in parallel; unreachability constraints generate unsolvable variants.
- Core assumption: Constraint satisfaction with shuffled variable ordering produces sufficiently diverse levels without excessive duplication.
- Evidence anchors:
  - [section 5.1] "Sturgeon utilizes a concise, mid-level API to define constraints on Boolean variables, converting these into low-level constraint satisfaction problems"
  - [section 4] GGLC contains >80,000 levels: cave (60,000+ across variants), platform (10,000+), crates (10,000), vertical (10,000+)
  - [corpus] "The Procedural Content Generation Benchmark" offers complementary evaluation infrastructure but focuses on generation tasks rather than robustness analysis.
- Break condition: If scalability demands exceed solver capacity (e.g., larger levels, more complex mechanics), generation throughput may bottleneck.

## Foundational Learning

- Concept: **Constraint Satisfaction Problems (CSP)**
  - Why needed here: Understanding how Sturgeon encodes level generation as Boolean variable assignments under hard constraints (reachability, tile counts, local patterns) is essential for modifying or extending the corpus.
  - Quick check question: Can you explain why a "hard" constraint cannot be relaxed into a penalty term without changing the problem semantics?

- Concept: **Hamming Distance for Discrete Data**
  - Why needed here: The non-robustness metric operates on Hamming distance at radius 1, meaning all single-element substitutions are considered equivalent perturbations regardless of tile semantics.
  - Quick check question: What information does Hamming distance discard that might matter for functional similarity in game levels?

- Concept: **Adversarial Robustness vs. Data Robustness**
  - Why needed here: The paper distinguishes classifier robustness (predictions stable under perturbation) from data robustness (labels themselves may legitimately change under perturbation). This reframes when "sensitivity" is a bug vs. a feature.
  - Quick check question: In CIFAR-10, why should a classifier be robust to pixel noise? In a key-door puzzle game, why should a classifier *not* necessarily be robust to key removal?

## Architecture Onboarding

- Component map:
  - Sturgeon Generator -> Solver Portfolio -> GGLC Corpus
  - CLIP Embedding -> UMAP Reduction -> Non-robustness Pipeline

- Critical path:
  1. Define game mechanics (tile types, movement rules, win conditions)
  2. Encode constraints in Sturgeon (local patterns from examples, global reachability)
  3. Run solver portfolio to generate solvable levels; use unreachability constraints for unsolvable
  4. Post-process to extract solutions (paths or playthroughs)
  5. For robustness analysis: embed with CLIP, reduce with UMAP, compute non-robustness at varying radii

- Design tradeoffs:
  - Custom games vs. existing titles: Custom enables full licensing control (CC-BY 4.0) but lacks the ecological validity of recognizable domains
  - Discrete vs. continuous non-robustness: Discrete (Hamming r=1) directly measures tile-level sensitivity; continuous (CLIP embedding) enables comparison with image datasets but may conflate visual and functional similarity
  - Solver portfolio vs. single solver: Portfolio reduces variance in generation time across level difficulty but adds infrastructure complexity

- Failure signatures:
  - High duplicate rate in generated levels (solver converging to same solutions)
  - Generated unsolvable levels that are trivially distinguishable (not useful for training classifiers)
  - CLIP embedding collapses solvable/unsolvable into overlapping clusters even at large radii
  - Constraint solver timeouts on larger or more constrained level specifications

- First 3 experiments:
  1. **Baseline robustness replication**: Take 100 random levels per game, apply all single-tile substitutions, measure solvability flip rate; verify Table 2 results (crates ~79%, cave ~43%)
  2. **Embedding sensitivity check**: Visualize CLIP+UMAP projections for each game; confirm solvable/unsolvable overlap patterns match Figure 5 before computing continuous non-robustness
  3. **Cross-game comparison**: Train a simple classifier (e.g., CNN on level images) to predict solvability; measure its adversarial robustness and compare to data non-robustness—hypothesis: classifier robustness should be *lower* for high-non-robustness games if the classifier correctly models the true label dynamics

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content and methodology, several important unresolved issues emerge:

### Open Question 1
- Question: Can machine learning models trained on the GGLC dataset generalize to human-authored game levels from existing commercial games?
- Basis in paper: [inferred] The paper notes that levels "are not directly sourced from existing games" and "are fairly simple and short" compared to datasets like VGLC which are "taken from well-known and popular games."
- Why unresolved: No experiments were conducted testing transfer learning between GGLC and human-authored corpora.
- What evidence would resolve it: Benchmarking PCGML models trained on GGLC against those trained on VGLC, measuring generation quality on held-out human-designed levels.

### Open Question 2
- Question: What is the relationship between data non-robustness and model performance in generating valid game content?
- Basis in paper: [inferred] The paper formalizes non-robustness but does not experimentally link higher non-robustness values to training difficulties or generation failure rates in PCGML models.
- Why unresolved: The dataset is presented as a resource, but no models were trained on it to validate whether non-robustness predicts training challenges.
- What evidence would resolve it: Training generative models (GANs, VAEs, transformers) on each game dataset and correlating non-robustness scores with rates of unsolvable or unacceptable generated levels.

### Open Question 3
- Question: How does non-robustness scale with simultaneous multi-tile perturbations versus single-tile changes?
- Basis in paper: [explicit] The methodology fixes radius r=1 (Hamming distance) for the discrete case, and notes "the choice of radius r may need adjustment." The authors measure only single-tile random replacement.
- Why unresolved: The paper does not explore whether sensitivity compounds with multiple changes or saturates, limiting understanding of robustness dynamics.
- What evidence would resolve it: Measuring NDᵣ(D) for r=2,3,5 across all game types and analyzing whether non-robustness increases linearly, sublinearly, or exhibits phase transitions.

## Limitations

- The continuous-form non-robustness comparison with standard ML datasets relies on CLIP embeddings that may not capture functional similarity in game levels
- The corpus consists of custom-designed games rather than levels from existing popular titles, limiting ecological validity
- Scalability of the constraint-based generation approach to more complex games or larger levels remains untested

## Confidence

**High Confidence**: The discrete non-robustness measurements showing single-tile changes cause solvability flips in 17-79% of cases across games. This is directly measurable and reproducible.

**Medium Confidence**: The formal definition of non-robustness as a data property independent of classifiers. While mathematically sound, its practical interpretation depends on embedding quality.

**Low Confidence**: The continuous-form non-robustness comparison with MNIST/CIFAR-10. This relies on CLIP+UMAP capturing relevant functional similarities, which may not hold for all games or tile types.

## Next Checks

1. **Embedding Validation**: For each game, visualize CLIP+UMAP projections and verify that solvable/unsolvable levels form overlapping clusters consistent with Figure 5 before computing continuous non-robustness values.

2. **Solver Portfolio Reliability**: Run the solver portfolio on identical level specifications multiple times to measure variance in generation time and solution diversity, confirming that parallel execution meaningfully reduces bottlenecks.

3. **Constraint Template Replication**: Attempt to reproduce the constraint templates for one game (e.g., crates) using only the provided tile semantics and mechanics, then verify that generated levels satisfy both solvability and local structure constraints.