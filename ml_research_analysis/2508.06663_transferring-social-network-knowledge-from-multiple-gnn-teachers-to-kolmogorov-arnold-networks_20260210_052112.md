---
ver: rpa2
title: Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold
  Networks
arxiv_id: '2508.06663'
source_url: https://arxiv.org/abs/2508.06663
tags:
- knowledge
- graph
- networks
- student
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the scalability and efficiency limitations
  of Graph Neural Networks (GNNs) by integrating Kolmogorov-Arnold Networks (KANs)
  into three popular GNN architectures: GAT, SGC, and APPNP, resulting in KGAT, KSGC,
  and KAPPNP. The study proposes a multi-teacher knowledge amalgamation framework
  that distills knowledge from multiple KAN-based GNN teachers into a graph-independent
  KAN student model.'
---

# Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks

## Quick Facts
- **arXiv ID:** 2508.06663
- **Source URL:** https://arxiv.org/abs/2508.06663
- **Reference count:** 26
- **Primary result:** KAN-based GNNs (KGAT, KSGC, KAPPNP) improve node classification accuracy compared to traditional counterparts; heterogeneous teacher distillation yields best student performance.

## Executive Summary
This paper proposes integrating Kolmogorov-Arnold Networks (KANs) into three popular GNN architectures (GAT, SGC, APPNP) to enhance their non-linear expressiveness for node classification tasks. The authors introduce a multi-teacher knowledge amalgamation framework that distills knowledge from multiple KAN-based GNN teachers into a graph-independent KAN student model, enabling efficient graph-free inference. Experiments on Cora, Citeseer, and Amazon Photo datasets demonstrate that KGAT achieves the highest accuracy across all datasets, while heterogeneous teacher combinations (one KAN-based GNN paired with a traditional GNN) provide superior student performance compared to homogeneous pairs.

## Method Summary
The study integrates KAN layers (learnable spline-based univariate functions) into three GNN architectures: KGAT (replaces attention mechanism), KSGC (replaces final linear classifier), and KAPPNP (replaces initial MLP). These KAN-based GNNs are trained as teachers on graph data. A graph-independent KAN student model is then trained via multi-teacher knowledge amalgamation, where it learns to mimic the softened output distributions of heterogeneous teacher pairs through KL-divergence minimization. The student can perform inference without accessing the graph structure, achieving both accuracy improvements and computational efficiency.

## Key Results
- KGAT achieves the highest node classification accuracy on all three benchmark datasets (Cora, Citeseer, Amazon Photo).
- Knowledge amalgamation significantly boosts student model performance compared to single-teacher training.
- Heterogeneous teacher pairs (one KAN-based GNN + one traditional GNN) yield the best student performance, with attention weights correctly balancing their contributions.

## Why This Works (Mechanism)

### Mechanism 1
Replacing fixed activation functions with learnable spline-based univariate functions (KAN layers) increases non-linear expressiveness of graph aggregation steps. KAN layers parameterize weights as learnable functions φ(x) rather than static values, allowing dynamic adaptation of transformations applied to neighbor features during propagation. This captures complex data manifolds that fixed ReLU or linear layers might miss. Break condition: If the graph task is primarily linear or the dataset is extremely noisy, the added capacity may lead to overfitting without accuracy gains.

### Mechanism 2
Multi-teacher knowledge amalgamation transfers structural graph knowledge to a graph-independent student model by forcing it to mimic softened output distributions of heterogeneous teachers. The student (standard KAN without graph layers) is trained to minimize KL-divergence against a weighted average of teacher outputs. By learning from teachers that use graph adjacency, the student learns a functional approximation of the graph structure encoded in teachers' logits, enabling inference without explicit graph data. Break condition: If teachers rely heavily on structural features not present in node attributes (low homophily), the feature-only student will fail to converge to teacher performance.

### Mechanism 3
Heterogeneous teacher pairs (one KAN-based GNN + one traditional GNN) provide superior supervision signals due to diverse inductive biases. Different architectures capture different aspects of the graph (e.g., GAT focuses on attention, SGC on efficient propagation). Amalgamating a KAN-architecture with a traditional architecture averages out architectural bias, providing a more robust "super-teacher" signal. Break condition: If teachers have widely divergent accuracy levels, the amalgamation might confuse the student rather than aid it.

## Foundational Learning

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - **Why needed here:** This is the mathematical basis for KANs. The paper replaces "sum of non-linearities" (MLP) with "non-linearities of sums" (KAN) via learnable splines.
  - **Quick check question:** Can you explain why a learnable spline function φ is theoretically more expressive than a fixed ReLU activation on a weight matrix?

- **Concept: Graph Message Passing (Aggregation vs. Transformation)**
  - **Why needed here:** To understand where KAN layers fit. The paper proposes placing KAN layers before aggregation (APPNP), after (SGC), or replacing the attention mechanism (GAT).
  - **Quick check question:** In the KSGC architecture, does the KAN layer perform the neighborhood aggregation or the feature transformation?

- **Concept: Knowledge Distillation (Soft Targets & Temperature)**
  - **Why needed here:** The core training loop for the student relies on minimizing KL divergence against "soft" teacher outputs (logits), not just hard labels.
  - **Quick check question:** Why does the loss function use the weighted output of teachers (Zt) rather than just using the ground truth labels to train the student?

## Architecture Onboarding

- **Component map:**
  - Node features X → KAN layer (learnable splines) → (GAT: attention mechanism / SGC: aggregation / APPNP: propagation) → Softmax logits Zt
  - Node features X → KAN student (graph-independent) → Softmax logits Zs
  - KAN student + multiple teacher logits → Attention weights αt → Weighted average Zt → KL-divergence loss

- **Critical path:**
  1. Pre-train all Teacher models (GCN, GAT, SGC, APPNP, and K-counterparts) on the full graph.
  2. Inference (Teachers): Generate soft predictions (logits) Zt for all nodes using the teachers.
  3. Distillation (Student): Train the student KAN using X as input, optimizing Ltotal (combination of Hard Label Loss and Soft Teacher Amalgamation Loss).
  4. Deployment: Use the student KAN for fast inference without loading the graph adjacency matrix.

- **Design tradeoffs:**
  - Replacing attention vector vs. linear matrix in KGAT: Arch 1 (attention) showed slightly better results.
  - Inference Speed vs. Accuracy: Student KAN is faster (no graph lookup) but may lose a few percentage points in accuracy compared to the best teacher.
  - Spline Grid Size: Larger grids capture more detail but increase computational cost (paper uses grid=32).

- **Failure signatures:**
  - NaN Loss: Instability in spline computations if learning rates are too high or input features are unnormalized.
  - Student Collapse: Student accuracy drops significantly (<50%)—often caused by mismatched teacher weights αt or excessive temperature T in softmax.
  - Overfitting Teachers: If teachers overfit the training set, the distilled student will also overfit; early-stopping teachers is critical.

- **First 3 experiments:**
  1. Baseline Verification: Reproduce KGCN and KSGC accuracy on Cora/Citeseer using specified hyperparameters (grid=32, spline order=3).
  2. Heterogeneity Test: Train student with (Teacher A: GAT + Teacher B: KSGC) vs. (Teacher A: GAT + Teacher B: GAT) to verify heterogeneous pairs yield higher accuracy.
  3. Inference Benchmark: Measure average inference time of KAN Student vs. KGAT Teacher on test set to quantify "graph-free" efficiency gains.

## Open Questions the Paper Calls Out

- **Open Question 1:** Why did the contrastive learning component fail to improve performance in the knowledge amalgamation framework, and can it be adapted for KAN student models? The authors removed contrastive learning to streamline training without analyzing why transferability failed specifically for KAN students.

- **Open Question 2:** Does the KAN-based GNN integration retain the "avoidance of catastrophic forgetting" property attributed to KANs in continual learning scenarios? The paper does not evaluate these models in dynamic or sequential graph learning setups where forgetting typically occurs.

- **Open Question 3:** Does the efficiency and accuracy of the heterogeneous teacher approach scale to large-scale industrial graphs? Experiments are limited to small benchmark datasets (max ~7,500 nodes), leaving unclear if improvements persist with denser or significantly larger graphs.

## Limitations

- **Unknown Hyperparameters:** The paper lacks critical implementation details including optimizer settings, learning rates, weight decay, and specific values for loss balancing coefficient λ and temperature T used in knowledge amalgamation.
- **Scalability Uncertainty:** The practical efficiency gains and heterogeneous teacher effectiveness are only validated on small benchmark datasets (Cora, Citeseer, Amazon Photo) and may not generalize to large-scale industrial graphs.
- **Weak Corpus Evidence:** The specific mechanism of heterogeneous teacher pairs providing superior supervision is only partially supported, with limited corpus evidence for this particular pairing strategy.

## Confidence

- **High Confidence:** The core architectural modifications (integrating KAN layers into GAT, SGC, and APPNP) are well-specified and verifiable. The improvement in accuracy for KGAT over GAT on all three datasets is clearly demonstrated.
- **Medium Confidence:** The knowledge amalgamation framework is conceptually sound, but the effectiveness of heterogeneous teacher pairs is only partially supported. The corpus evidence for this specific mechanism is weak, and the optimal attention-weighting strategy is not fully explored.
- **Low Confidence:** The practical efficiency gains (inference speed) and the precise conditions for successful graph-free inference are not quantified or validated in the paper.

## Next Checks

1. **Reproduce Baseline Results:** Implement and train KGAT, KSGC, and KAPPNP on Cora and Citeseer with specified hyperparameters (grid=32, spline order=3) to verify claimed accuracy improvements over GAT, SGC, and APPNP.

2. **Test Heterogeneous Distillation:** Conduct an ablation study comparing student accuracy when trained with homogeneous teacher pairs (GAT+GAT) versus heterogeneous pairs (GAT+KSGC) to validate claimed superiority of heterogeneous combinations.

3. **Benchmark Inference Efficiency:** Measure and compare average inference time per node for the KGAT Teacher (with graph lookup) versus the KAN Student (graph-free) on the test sets to quantify practical efficiency gains.