---
ver: rpa2
title: 'GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with
  Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning'
arxiv_id: '2601.06795'
source_url: https://arxiv.org/abs/2601.06795
tags:
- reward
- advantage
- sampling
- training
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GDEPO tackles inefficiencies in reinforcement learning for automated
  theorem proving, where Group Relative Policy Optimization struggles with composite
  rewards, data wastage, and uniform training iterations. The authors propose three
  core mechanisms: dynamic additional sampling to retry invalid batches until valid
  proofs are found, equal-right advantage to decouple correctness-based advantage
  signs from auxiliary reward magnitudes ensuring correct gradient directions, and
  dynamic additional iterations to apply more updates to challenging samples that
  eventually succeed.'
---

# GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.06795
- Source URL: https://arxiv.org/abs/2601.06795
- Reference count: 39
- Primary result: Achieves 84% improvement on challenging PutnamBench theorem proving benchmark over standard GRPO.

## Executive Summary
GDEPO addresses fundamental inefficiencies in reinforcement learning for automated theorem proving where Group Relative Policy Optimization struggles with composite rewards, data wastage, and uniform training iterations. The authors propose three core mechanisms: dynamic additional sampling to retry invalid batches until valid proofs are found, equal-right advantage to decouple correctness-based advantage signs from auxiliary reward magnitudes ensuring correct gradient directions, and dynamic additional iterations to apply more updates to challenging samples that eventually succeed. Experiments on MinF2F-test, MathOlympiadBench, and PutnamBench demonstrate significant performance gains, especially an 84% improvement on the most difficult PutnamBench, with ablation studies confirming the necessity of each component.

## Method Summary
GDEPO implements three mechanisms to improve reinforcement learning efficiency for automated theorem proving. Dynamic Additional Sampling resamples queries until at least one correct proof emerges, preventing data wastage on low-probability correct solutions. Equal-Right Advantage computes advantages using binary correctness labels for sign and normalized auxiliary rewards for magnitude, activated when pass rate falls below threshold τ. Dynamic Additional Iterations applies extra gradient steps to samples requiring multiple sampling rounds, accelerating learning on challenging but solvable problems. The method uses Godel-Prover-V2-8B with Open-R1 framework, training on 30,000 Lean 4 formalized theorems with 2 epochs, batch size 32, and asymmetric clipping.

## Key Results
- Achieves 84% improvement on PutnamBench, 26% on MathOlympiadBench, and 8% on MiniF2F-test over standard GRPO
- Ablation study confirms each component's necessity: removing Dynamic Additional Sampling reduces PutnamBench improvement from +84% to +11%
- Improves data utilization by finding correct proofs in batches that would otherwise be discarded
- Demonstrates particular effectiveness on challenging problems requiring multiple sampling attempts

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Additional Sampling
Resampling queries until at least one correct proof emerges increases data utilization when correct solutions exist at low probability. The system performs up to n additional sampling rounds rather than discarding batches with no correct outputs, terminating when a valid proof is found or max attempts are reached.

### Mechanism 2: Equal-Right Advantage
Decoupling the advantage function's sign (determined by verifier correctness) from its magnitude (determined by auxiliary rewards) prevents correct trajectories from receiving negative advantages under composite rewards. The advantage is computed as Â*_i,t = l*_i · L(l*_i, Ŝ*_i), where l*_i is the binary correctness label and Ŝ*_i is the normalized auxiliary reward score.

### Mechanism 3: Dynamic Additional Iterations
Allocating extra gradient steps to samples that required multiple sampling rounds accelerates learning on challenging but solvable problems. For queries where k > 1 (not solved in first round) and at least one correct trajectory exists, the system applies m additional policy updates using the same trajectory group.

## Foundational Learning

- **Concept: Advantage Functions in Policy Gradient**
  - Why needed: Understanding that advantage sign determines whether trajectory probability increases (positive) or decreases (negative) is essential for grasping why equal-right advantage matters.
  - Quick check: If a correct proof receives a negative advantage, what happens to its generation probability during training?

- **Concept: Formal Verification vs. Learned Reward Models**
  - Why needed: GDEPO's design hinges on the formal verifier providing binary, authoritative feedback—fundamentally different from learned scalar rewards.
  - Quick check: Why can't we simply train a reward model to predict proof correctness like we do for chat preferences?

- **Concept: Composite Reward Interference**
  - Why needed: The paper's core theoretical contribution is proving that auxiliary rewards can dominate correctness signals in standard GRPO.
  - Quick check: Given Eq. 14-17, if α (correctness weight) is too low relative to auxiliary reward variance, can a correct trajectory average below the group mean?

## Architecture Onboarding

- **Component map**: Query q → Initial Sampling (G trajectories) → Lean4 Verification → Binary labels L_q → All incorrect? → Dynamic Additional Sampling (up to n rounds) → Equal-Right Advantage Computation → Dynamic Iterations Decision → Policy Update via Clipped Objective

- **Critical path**: Lean4 verification output determines all downstream advantage signs; pass rate threshold τ switches between equal-right and standard GRPO modes; sampling round count k triggers dynamic additional iterations.

- **Design tradeoffs**: Max sampling attempts (n) balances discovery probability against compute; pass rate threshold τ controls correctness priority; asymmetric clipping (ε_low=0.2, ε_high=0.28) amplifies updates for low-probability correct solutions; additional iterations (m) require tuning per dataset.

- **Failure signatures**: Correct proofs being suppressed during training indicates equal-right advantage disabled or threshold τ too high; no learning signal for hard batches suggests n too low or model fundamentally unable to solve the problem class; output length/repetition increasing indicates auxiliary rewards not being incorporated properly.

- **First 3 experiments**: (1) Reproduce ablation study by disabling each component sequentially to verify individual contributions, expecting D.A.S. removal to hurt PutnamBench most; (2) Log percentage of correct trajectories receiving negative advantages under standard GRPO vs. equal-right on held-out batch; (3) Train with 10%, 30%, 50% challenging samples to verify GDEPO's advantage widens as difficulty increases.

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of dynamic additional sampling scale with problem difficulty and does it remain tractable for real-world training regimes? The paper mentions resampling "until either a valid output is obtained or a preset maximum number of attempts is reached" but provides no analysis of computational cost trade-offs, particularly for the challenging 50% dataset configuration.

### Open Question 2
How sensitive is GDEPO's performance to the choice of the pass-rate threshold τ and the maximum resampling count n? The threshold τ controls when equal-right advantage activates versus standard GRPO, and n limits resampling, but both are treated as fixed hyperparameters (τ and n values not systematically varied in experiments).

### Open Question 3
Does GDEPO generalize to other formal verification systems beyond Lean4, such as Isabelle or Coq? All experiments use Lean4 exclusively; the conclusion claims the method offers "a novel perspective on designing RL algorithms for tasks equipped with precise verification mechanisms" but no cross-system validation is provided.

## Limitations
- Missing critical hyperparameter values including pass rate threshold τ, maximum sampling attempts n, additional iterations m, and group size G
- Dataset construction ambiguity regarding specific filtering criteria for "challenging samples" beyond failing pass@32 baseline
- Implementation dependencies on Lean 4 server configuration, timeout settings, and memory limits not documented

## Confidence

**High Confidence**: The core theoretical insight that composite rewards can cause correct trajectories to receive negative advantages in GRPO is mathematically sound and well-proven.

**Medium Confidence**: Experimental results showing GDEPO's performance improvements are convincing, but missing hyperparameter details and specific implementation of dynamic components introduce uncertainty about exact replication.

**Low Confidence**: Claims about relative importance of each component based on ablation studies are difficult to fully validate without complete experimental setup and understanding of challenging sample selection.

## Next Checks

1. **Ablation Validation**: Reproduce the ablation study by systematically disabling each GDEPO component and measuring performance impact on PutnamBench and MiniF2F-test, verifying that removing Dynamic Additional Sampling causes the largest degradation on challenging problems.

2. **Advantage Sign Analysis**: Implement a diagnostic to measure the percentage of correct trajectories receiving negative advantages under standard GRPO with composite rewards versus GDEPO's equal-right advantage on a held-out validation set, with the difference should be statistically significant.

3. **Difficulty Scaling Experiment**: Replicate the challenging data scaling analysis by training with varying proportions (10%, 30%, 50%) of challenging samples and measuring GDEPO's advantage over standard GRPO as problem difficulty increases.