---
ver: rpa2
title: Augmenting Researchy Questions with Sub-question Judgments
arxiv_id: '2510.21733'
source_url: https://arxiv.org/abs/2510.21733
tags:
- documents
- researchy
- questions
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces automatic relevance judgments for sub-questions
  in the Researchy Questions dataset, a collection of complex queries requiring multi-faceted
  information retrieval. The authors use Llama3.3 70B to generate graded (0-5 scale)
  relevance labels for each sub-question based on retrieved documents from ClueWeb22.
---

# Augmenting Researchy Questions with Sub-question Judgments

## Quick Facts
- arXiv ID: 2510.21733
- Source URL: https://arxiv.org/abs/2510.21733
- Reference count: 3
- Primary result: Dataset of 24.3M automatic graded relevance judgments for 1.29M sub-questions in Researchy Questions collection

## Executive Summary
This paper addresses the challenge of training retrieval models for complex information needs by introducing automatic relevance judgments for sub-questions in the Researchy Questions dataset. The authors generate graded (0-5 scale) relevance labels using Llama3.3 70B, enabling models to learn fine-grained document-query relationships. The resulting dataset provides 18.87 judgments per sub-question on average, creating a valuable resource for factoid retrieval research.

## Method Summary
The authors augment Researchy Questions by retrieving candidate documents using a hybrid approach combining BM25 and neural reranking, then applying Llama3.3 70B to generate graded relevance judgments. The pipeline retrieves top-100 documents via BM25, reranks them with Qwen3 0.6B, and selects top-20 from each source plus clicked documents. Llama3.3 70B then rates each document-question pair using a rubric-based prompt adapted from prior work, producing structured training signals across 24.3M judgments.

## Key Results
- 24.3M relevance judgments across 1.29M sub-questions (18.87 judgments per sub-question)
- Hybrid retrieval balances lexical (BM25) and semantic (Qwen3) signals
- Graded rubric captures nuanced relevance distinctions beyond binary labels
- Dataset enables training models for complex, multi-aspect information needs

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Candidate Retrieval Balancing Lexical and Semantic Signals
- Claim: Combining BM25 lexical retrieval with neural reranking produces a more diverse and balanced candidate set for judgment than either method alone.
- Mechanism: The pipeline retrieves top-100 documents via BM25, reranks them with Qwen3 0.6B, then selects top-20 from each source plus clicked documents. This ensures candidates include both exact keyword matches and semantically related passages that may use different terminology.
- Core assumption: Neither lexical nor semantic retrieval alone captures all relevant documents for complex, multi-aspect questions.
- Evidence anchors: [section 2] "we collect the top 20 documents from the initial BM25 retrieval results and the top 20 documents from the reranking output to balance the lexical and semantic matching in the candidate set"

### Mechanism 2: Graded Answerability Judgments Capture Relevance Granularity
- Claim: A 0-5 rubric-based scale captures finer distinctions in document-question relevance than binary labels, enabling models to learn nuanced relevance signals.
- Mechanism: The Llama3.3 70B model rates each document-question pair against a 6-level rubric describing completeness, accuracy, and gap severity. This produces structured training signals distinguishing "highly relevant and complete" (5) from "mostly relevant with minor gaps" (4) through to "not relevant" (0).
- Core assumption: LLMs can reliably distinguish between adjacent relevance levels when given explicit rubric criteria.
- Evidence anchors: [section 3] "Judgments were on a 0-5 scale using the prompts from Ju et al. (2025), which are intended to assess the answerability of a question given a document"

### Mechanism 3: Sub-Question Decomposition Enables Fine-Grained Document Matching
- Claim: Decomposing complex queries into sub-questions allows retrieval models to learn document relevance at the aspect level rather than only at the whole-query level.
- Mechanism: Each Researchy Question has associated GPT-4-generated sub-questions addressing different aspects of the complex information need. By judging document relevance to each sub-question independently, the dataset provides fine-grained supervision for models to identify which documents address which aspects.
- Core assumption: Sub-questions produced by GPT-4 meaningfully decompose the original query's information need into addressable components.
- Evidence anchors: [abstract] "Each query in ResearchyQuestions is associated with sub-questions that were produced by prompting GPT-4"

## Foundational Learning

- **Graded Relevance vs Binary Labels**
  - Why needed here: Understanding why a 0-5 scale matters for training—it allows models to learn that some documents partially satisfy information needs, which is critical for complex queries where no single document may be complete.
  - Quick check question: Given a question about "climate change effects on agriculture," would a document discussing only temperature impacts (not precipitation) be irrelevant, or partially relevant?

- **LLM-as-Judge Paradigms**
  - Why needed here: The entire labeling pipeline depends on trusting Llama3.3 70B's judgments as proxies for human relevance assessments. Understanding known failure modes (length bias, verbosity bias, positional bias) helps validate the approach.
  - Quick check question: What validation would you run to check if Llama3.3 70B's relevance judgments correlate with human judgments on a sample?

- **Query Decomposition for Multi-Aspect Retrieval**
  - Why needed here: The sub-question approach assumes complex queries can be productively broken into simpler sub-queries. Understanding decomposition quality affects how you interpret and use this dataset.
  - Quick check question: For "What are the pros and cons of electric vehicles?", what sub-questions would capture distinct aspects that might require different documents?

## Architecture Onboarding

- **Component map:**
  ```
  Original Query (Researchy Questions)
         ↓
  GPT-4 Sub-question Generation (pre-existing)
         ↓
  ClueWeb22 Set B Index → BM25 Retrieval (top 100)
         ↓                      ↓
  Clicked Documents    Qwen3 0.6B Reranker (top 100)
         ↓                      ↓
         └──────────┬───────────┘
                    ↓
         Candidate Set (top 20 BM25 + top 20 reranked + clicked)
                    ↓
         Llama3.3 70B Graded Judgment (0-5 scale)
                    ↓
         Labeled Dataset (24.3M judgments)
  ```

- **Critical path:** The quality of downstream retrieval models depends on (1) candidate coverage—if relevant documents aren't in the candidate set, they can't be labeled relevant; (2) judgment accuracy—systematic errors in LLM judgments propagate to trained models.

- **Design tradeoffs:**
  - **Efficiency vs coverage:** Only using ClueWeb22 Set B (48% of clicked docs) reduces indexing cost but may miss relevant documents in other segments.
  - **Candidate set size:** 18.87 judgments per sub-question balances annotation cost against coverage; increasing candidates would find more relevant docs but multiply LLM inference costs.
  - **LLM judge size:** Llama3.3 70B provides presumably higher-quality judgments than smaller models but at significant inference cost for 24M judgments.

- **Failure signatures:**
  - High proportion of level 0-2 judgments (2.70 + 2.06 + 6.14 = 10.90 per sub-question) suggests many candidates are non-relevant—consider tightening candidate selection.
  - If trained models struggle with queries having few level 4-5 judgments, the dataset may lack sufficient positive examples for those query types.
  - Disagreement between LLM judgments and human evaluation on held-out samples would indicate rubric interpretation issues.

- **First 3 experiments:**
  1. Validate LLM judgments against human annotations on a stratified sample (e.g., 500 sub-question/document pairs across all relevance levels) to measure correlation and identify systematic biases.
  2. Analyze candidate coverage by checking what proportion of human-judged relevant documents (if available from original Researchy Questions click data) appear in the candidate set.
  3. Train a baseline retrieval model (e.g., fine-tune a bi-encoder) using the level 4-5 judgments as positive labels and compare against a model trained only on original query-level clicks to measure the value of sub-question granularity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training retrieval models on the augmented sub-question judgments improve performance on complex information needs compared to training on click data alone?
- Basis in paper: [explicit] The authors state the dataset is intended as a resource for "training retrieval models that better support complex information needs" and encourage "modeling exploration."
- Why unresolved: The paper describes the dataset construction and statistics but includes no experiments evaluating downstream retrieval performance.
- What evidence would resolve it: Benchmarks comparing neural rankers fine-tuned on this dataset against baselines trained on the original Researchy Questions click data.

### Open Question 2
- Question: What is the alignment between the Llama3.3 70B automatic judgments and human relevance assessments?
- Basis in paper: [inferred] The methodology relies entirely on an "automatic judgment model" using a prompt-based rubric without validating the LLM's output against human annotators.
- Why unresolved: While LLMs can grade relevance, they are known to hallucinate or exhibit bias; without a human agreement study, the label quality remains unverified.
- What evidence would resolve it: A gold-standard set of human judgments for a sample of sub-question/document pairs to calculate correlation metrics (e.g., Cohen's Kappa) with the LLM labels.

### Open Question 3
- Question: To what extent does the candidate document generation pipeline (BM25 + Qwen3 Reranker top-20) suffer from false negatives?
- Basis in paper: [inferred] The authors limit candidates to top-k results "for efficiency," acknowledging they did not index the full corpus, which creates a potential retrieval bottleneck.
- Why unresolved: If the initial retrieval stage misses relevant documents, the LLM will label them as non-relevant (or they will be absent), potentially misguiding models trained on this data.
- What evidence would resolve it: An analysis measuring the recall of the candidate generation pipeline against a set of known relevant documents for a sample of queries.

## Limitations

- The dataset relies entirely on LLM-generated judgments without human validation, leaving label quality unverified
- Restriction to ClueWeb22 Set B (48% of clicked documents) creates potential blind spots for relevant documents in other segments
- High proportion of level 0-2 judgments suggests candidate selection may be too broad, including many non-relevant documents

## Confidence

- **High Confidence**: The dataset creation pipeline (BM25 → reranking → LLM judgment) is technically sound and follows established IR practices.
- **Medium Confidence**: The 0-5 rubric provides meaningful granularity for training, but validation against human judgments is needed to confirm this.
- **Low Confidence**: That sub-question decomposition meaningfully improves retrieval performance without direct empirical comparison to whole-query approaches.

## Next Checks

1. **Human judgment validation**: Annotate a stratified random sample (n=500) of sub-question/document pairs across all relevance levels and compute inter-annotator agreement and correlation with LLM judgments to establish reliability.

2. **Coverage analysis**: For a subset of queries with known relevant documents (from original click data), measure what proportion of human-identified relevant documents appear in the candidate set versus what the LLM actually judged as relevant.

3. **Downstream impact study**: Fine-tune a retrieval model using only level 4-5 judgments versus one using original query-level clicks, measuring improvements on a held-out test set of complex queries to quantify the benefit of sub-question granularity.