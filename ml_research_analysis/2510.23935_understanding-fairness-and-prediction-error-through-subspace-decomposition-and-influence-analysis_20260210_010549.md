---
ver: rpa2
title: Understanding Fairness and Prediction Error through Subspace Decomposition
  and Influence Analysis
arxiv_id: '2510.23935'
source_url: https://arxiv.org/abs/2510.23935
tags:
- fairness
- sensitive
- learning
- information
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in machine learning by proposing
  a principled framework that adjusts data representations to balance predictive utility
  and fairness. The core idea uses sufficient dimension reduction to decompose the
  feature space into target-relevant, sensitive, and shared components, enabling controlled
  removal of sensitive information.
---

# Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis

## Quick Facts
- arXiv ID: 2510.23935
- Source URL: https://arxiv.org/abs/2510.23935
- Reference count: 40
- One-line primary result: Sequential Fair Projection achieves 76.88% accuracy with 3.78% demographic parity gap on Adult dataset

## Executive Summary
This paper addresses fairness in machine learning by proposing a principled framework that adjusts data representations to balance predictive utility and fairness. The core idea uses sufficient dimension reduction to decompose the feature space into target-relevant, sensitive, and shared components, enabling controlled removal of sensitive information. The method selectively removes directions informative about the sensitive attribute while preserving task-relevant information, offering flexible control over the fairness-utility trade-off.

Theoretical analysis shows how prediction error and fairness gaps evolve as shared subspaces are added, while influence functions characterize the asymptotic behavior of parameter estimates. Experiments on synthetic and real-world datasets demonstrate that the proposed Sequential Fair Projection (SFP) method achieves strong fairness improvements while preserving predictive accuracy. On the Adult dataset, SFP attains 76.88% accuracy with the lowest demographic parity gap (3.78%), outperforming existing fairness-aware methods. The framework provides a practical and interpretable approach to fairness-aware learning through direct manipulation of data representations.

## Method Summary
The Sequential Fair Projection (SFP) framework uses sufficient dimension reduction to decompose the feature space into orthogonal subspaces for target information, sensitive information, and their intersection. It constructs a sequence of projection matrices that start with maximum fairness (minimal utility) and incrementally add shared directions to restore predictive power. The method estimates central subspaces using MSAVE, identifies shared directions through generalized eigenvalue decomposition, and projects data onto the orthogonal complement of sensitive-only information. A post-SDR training approach with logistic regression is used, selecting the optimal number of shared dimensions that maximizes accuracy while maintaining at least 95% of original performance.

## Key Results
- On Adult dataset: 76.88% accuracy with 3.78% demographic parity gap (lowest among compared methods)
- Sequential addition of shared dimensions allows granular control over fairness-utility trade-off
- Theoretical bounds characterize prediction error and fairness gaps as shared subspaces are added
- Influence function analysis provides asymptotic error bounds for parameter estimates

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Subspace Decomposition
The feature space contains distinct linear subspaces for target information ($Y$), sensitive information ($Z$), and their intersection, which can be isolated to remove bias without destroying all predictive signal. The method utilizes Sufficient Dimension Reduction (SDR) to estimate central subspaces $S_{Y|X}$ and $S_{Z|X}$. It identifies the intersection $S_{Y|X} \cap S_{Z|X}$ (shared info) and the orthogonal complement $S_{Y|X} \cap S_{Z|X}^\perp$ (fair predictive info). By projecting data onto the orthogonal complement, sensitive information is geometrically removed.

### Mechanism 2: Sequential Fair Projection (SFP)
The trade-off between fairness and utility can be controlled granularly by sequentially adding "shared" directions back into the representation. The algorithm constructs a sequence of projection matrices $P^{(m)}$. It starts with $m=0$ (pure fairness, maximum utility loss) and iteratively adds shared directions $\phi_1, \dots, \phi_m$ from the intersection subspace. This gradually restores predictive power (utility) at the cost of reintroducing bias.

### Mechanism 3: Influence Function Regularization
The stability and asymptotic variance of the fair estimator can be characterized by tracing the learning path back to the input projection. The paper employs influence functions to analyze how small perturbations in the data distribution affect the estimated parameters in the reduced subspace. This provides a theoretical bound on the estimation error $\tilde{\theta}^{(m)}$ compared to the original $\theta$.

## Foundational Learning

- **Concept: Sufficient Dimension Reduction (SDR)**
  - Why needed here: This is the mathematical engine of the paper. Without understanding how SDR reduces data to its "central subspace" while preserving conditional independence, the decomposition of fair vs. unfair components is opaque.
  - Quick check question: Can you explain why SIR (Sliced Inverse Regression) or SAVE might estimate different central subspaces, and which one is used here?

- **Concept: Demographic Parity (DP) & Distance Covariance (dCov)**
  - Why needed here: The paper optimizes for DP (independence of prediction and sensitive attribute) and uses dCov as a theoretical fairness measure. Understanding the difference between independence and conditional independence is crucial.
  - Quick check question: Does Theorem 3.3 imply that zero distance covariance guarantees Demographic Parity, or just statistical independence?

- **Concept: Influence Functions**
  - Why needed here: Used in Section 5 to validate the estimator's robustness. Understanding how influence functions measure the sensitivity of an estimator to a single data point helps in interpreting the asymptotic variance bounds.
  - Quick check question: How does the influence function of the projection $P^{(m)}$ differ from the influence function of the downstream model parameter $\theta$?

## Architecture Onboarding

- **Component map:** Raw features $X$, Target $Y$, Sensitive $Z$ -> SDR Estimator (computes M_Y, M_Z) -> Intersection Solver (generalized eigenvalue decomposition) -> Projector (constructs P(m)) -> Downstream Model (trains on transformed features)

- **Critical path:** Estimating the rank $s$ of the intersection subspace $S_{Y|X} \cap S_{Z|X}$. If the Ladle estimator (step 4 in Algo 1) fails to identify the correct dimensionality of the shared bias, the sequential projection will either remove too much signal or leave too much bias.

- **Design tradeoffs:**
  - Linear vs. Non-linear: The core implementation uses Linear SDR. While faster and interpretable, it may fail on complex, non-linear biases (addressed briefly in Section D/Limitations).
  - Accuracy Threshold ($\tau$): Algorithm 2 sets a hard threshold (e.g., 95% of original accuracy). Lowering this increases fairness; raising it increases utility.

- **Failure signatures:**
  - Model Misspecification: If the linear SDR assumption is violated (Section 6.3), the method still runs but the trade-off curve flattensâ€”you lose significant accuracy for minor fairness gains.
  - Rank Overestimation: If the intersection rank $\hat{s}$ is overestimated, the algorithm wastes dimensions on "shared" noise.

- **First 3 experiments:**
  1. Synthetic Validation: Generate data with a known intersection dimension $s=6$. Verify if the Ladle estimator recovers $s=6$ and if the RMSE gap tracks the theoretical curve (Fig 1).
  2. Ablation on Shared Dimensions: On the Adult dataset, plot Accuracy vs. DP Gap for $m=0$ to $m=s$. Identify the "knee" of the curve where adding dimensions yields diminishing fairness returns.
  3. Robustness Check: Run SFP on the Bank dataset. Compare the "Post-SDR" approach (Algo 2) against a standard "In-Processing" constraint method to verify if SFP preserves accuracy better under strict fairness constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Sequential Fair Projection (SFP) framework be extended to capture and mitigate nonlinear dependencies using nonlinear sufficient dimension reduction (SDR)?
- Basis in paper: The Limitations section states that the current linear SDR assumption limits the ability to handle nonlinear dependencies and proposes extending the framework using kernel methods as a "promising direction for future research."
- Why unresolved: The theoretical derivations and algorithms currently rely on linear projection matrices, which cannot represent complex, nonlinear relationships between features, targets, and sensitive attributes.
- What evidence would resolve it: A derivation of the projection operator within a kernel-induced feature space and empirical validation on datasets with known nonlinear bias structures.

### Open Question 2
- Question: Is there a theoretically optimal, data-driven criterion for selecting the number of shared dimensions ($m$) to strictly balance the fairness-utility trade-off without relying on arbitrary accuracy thresholds?
- Basis in paper: Algorithm 2 selects the optimal feature set by maximizing accuracy subject to a fixed threshold (e.g., 95% of original accuracy), implying a lack of a theoretical loss function to autonomously determine the ideal trade-off point.
- Why unresolved: The paper characterizes the evolution of error and fairness gaps but does not propose an objective function that mathematically optimizes the intersection of these two metrics.
- What evidence would resolve it: An information criterion or heuristic that provably identifies the "knee" of the fairness-utility curve, along with convergence guarantees for the selected $m$.

### Open Question 3
- Question: Can theoretical guarantees for prediction error and fairness be established when the linear SDR assumption is strongly violated (model misspecification)?
- Basis in paper: Section 6.3 notes that under misspecification, the method uses a "practical remedy" (retaining subsets of directions) rather than providing theoretical guarantees on sufficiency or fairness.
- Why unresolved: While experiments show empirical robustness, the theoretical bounds derived in Section 5 assume Hadamard differentiability and specific subspace structures that may not hold under misspecification.
- What evidence would resolve it: Theoretical bounds on the utility loss and residual bias when the central subspaces cannot be fully captured by low-rank linear estimates.

## Limitations
- The linear SDR assumption may not hold for complex real-world datasets where sensitive attributes interact non-linearly with target variables, potentially limiting the effectiveness of the subspace decomposition
- The method's performance depends critically on accurate rank estimation of the shared subspace, which may be unstable with high-dimensional or noisy data
- The influence function analysis assumes asymptotic normality, which may not hold in small sample regimes typical of some fairness applications

## Confidence
- **High Confidence:** The core theoretical framework of subspace decomposition and the sequential projection mechanism (SFP) is well-specified and mathematically sound
- **Medium Confidence:** The experimental results demonstrate strong performance on benchmark datasets, though results may vary with different data characteristics
- **Low Confidence:** The influence function regularization component has limited empirical validation and its practical benefits remain to be demonstrated

## Next Checks
1. **Non-linear Extension Validation:** Test the kernel-based non-linear SDR approach on datasets known to have non-linear sensitive attribute relationships to verify if the method maintains its fairness-utility trade-off
2. **Rank Estimation Robustness:** Systematically evaluate the Ladle estimator's performance across varying noise levels and dimensionality to quantify its stability
3. **Cross-dataset Generalization:** Apply SFP to additional fairness datasets with different characteristics (e.g., image data with demographic attributes) to assess broader applicability beyond tabular data