---
ver: rpa2
title: 'From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement
  Learning'
arxiv_id: '2507.22028'
source_url: https://arxiv.org/abs/2507.22028
tags:
- navigation
- learning
- arxiv
- environments
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a framework that combines large-scale video
  pretraining with reinforcement learning to scale navigation foundation models. It
  proposes two innovations: an Anchor-Guided Distribution Matching strategy for pretraining,
  which models diverse motion patterns through anchor-based supervision, and a Residual-Attention
  Module for RL finetuning, which enables reactive behaviors while preserving pretrained
  knowledge.'
---

# From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.22028
- Source URL: https://arxiv.org/abs/2507.22028
- Reference count: 40
- Primary result: RL improves navigation success rates by 21% over pretraining alone

## Executive Summary
This paper presents a framework that combines large-scale video pretraining with reinforcement learning to scale navigation foundation models. The authors introduce Anchor-Guided Distribution Matching for multimodal behavior modeling from offline data, and a Residual-Attention Module that enables RL fine-tuning while preserving pretrained knowledge. They also introduce NavBench-GS, a comprehensive end-to-end evaluation benchmark built on 3D Gaussian Splatting reconstructions. Experiments demonstrate that RL significantly outperforms supervised learning scaling, achieving 21% higher success rates while maintaining superior obstacle avoidance and generalization across different robotic embodiments.

## Method Summary
The framework uses a two-stage approach: first, pretraining on 35 hours of real-world navigation videos using Anchor-Guided Distribution Matching where anchor points serve as high-level intentions for multimodal action prediction. Second, RL fine-tuning with Proximal Policy Optimization using a Residual-Attention Module that freezes pretrained cross-attention layers while training a parallel residual branch. The model architecture consists of an EfficientNet-B0 encoder, 4-layer transformer encoder, 4-layer decoder with RAM, and three MLP heads for trajectory, score, and velocity scale prediction. Training occurs in simulation (URBAN-SIM) with 256 parallel environments for approximately 15 hours.

## Key Results
- RL fine-tuning achieves 21% higher success rates compared to pretraining alone
- Model demonstrates superior obstacle avoidance and generalizes across different robotic embodiments
- Anchor-guided distribution matching with 8 anchors provides optimal balance between multimodality and redundancy
- Residual-Attention Module enables effective sim-to-real transfer without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1: Anchor-Guided Distribution Matching for Multimodal Behavior Modeling
This formulation structures action prediction as an anchor-guided Gaussian Mixture Model, enabling stable learning of diverse navigation behaviors from offline video data. Uniformly sampled anchor points in the robot's forward direction serve as interpretable high-level intentions, with each anchor corresponding to a Gaussian mode. The model predicts trajectories by selecting among these modes rather than fitting a single unimodal distribution. This works because navigation trajectories are inherently multimodal—multiple valid actions exist under the same observation due to partial observability and prediction error accumulation.

### Mechanism 2: Residual-Attention Module for Knowledge Preservation During RL Fine-Tuning
This approach freezes pretrained cross-attention layers while training a parallel residual branch, enabling interactive skill acquisition without catastrophic forgetting of visual representations. RAM creates a trainable copy of cross-attention layers wrapped by zero-initialized linear layers, preserving original visuomotor representations while selectively capturing interaction-related features. This works because cross-attention layers model agent-environment interactions that transfer across domains, while visual encoders and self-attention layers are sensitive to sim-to-real distribution shifts.

### Mechanism 3: Reinforcement Learning Overcoming Diminishing Returns of Offline Scaling
RL post-training provides performance gains that offline data scaling cannot achieve because embodied interaction teaches counterfactual reasoning absent in passive video. Offline pretraining captures statistical correlations but lacks grounded causality; RL enables agents to close the gap through trial-and-error feedback, learning to adapt, recover, and reason about obstacle avoidance in ways imitation cannot. This works because the action space is sufficiently low-dimensional that supervised learning quickly saturates model capacity, while interactive experience provides orthogonal signal.

## Foundational Learning

- **Gaussian Mixture Models (GMMs)**
  - Why needed here: The policy represents actions as mixture distributions q(w|o) = Σ qm · N(μm, σm). Understanding how modes are assigned and optimized via negative log-likelihood is essential for both pretraining and entropy-regularized RL.
  - Quick check question: Can you explain why the assignment strategy selects the mode whose anchor is closest to ground truth rather than using soft assignment across all modes?

- **Proximal Policy Optimization (PPO)**
  - Why needed here: RL fine-tuning uses PPO with clipped objective and entropy regularization. The GMM policy requires a custom entropy approximation (Equation 9) since closed-form entropy doesn't exist.
  - Quick check question: What happens to exploration if the logstd head is not re-initialized before RL fine-tuning?

- **Transformer Cross-Attention**
  - Why needed here: The architecture uses anchors as queries (Q) and observation-goal embeddings as keys/values (K, V). RAM operates specifically on cross-attention layers, not self-attention.
  - Quick check question: Why would freezing self-attention but fine-tuning cross-attention preserve sim-to-real transferability?

## Architecture Onboarding

- **Component map**: EfficientNet-B0 encoder → 768-dim observation tokens (5 frames context) → Goal + anchor MLPs → 768-dim tokens → 4-layer Transformer encoder → spatiotemporal context → 4-layer Transformer decoder with RAM → cross-attention using anchors as Q → 3 MLP heads → trajectory (μ, σ, ρ), score (softmax over anchors), velocity scale

- **Critical path**: 1. Pretrain end-to-end on 35h video data with NLL + L2 regression loss (Equation 2, 3) 2. Freeze encoder, self-attention, and original cross-attention weights 3. Initialize RAM with zero-initialized linear layers around copied cross-attention 4. Fine-tune with PPO in URBAN-SIM (256 parallel environments, 15 hours)

- **Design tradeoffs**: 8 anchors balances mode diversity vs. redundancy (Table 7 shows 16 anchors degrades mAP); EfficientNet-B0 over larger encoders yields smoother trajectories on limited navigation data; RAM over full fine-tuning preserves real-world generalization at cost of limited adaptation capacity

- **Failure signatures**: Excessive collisions in dynamic environments → pretrained model lacking RL fine-tuning; Trajectory drift or open-space crashes → sim-to-real gap from overfitting to synthetic visuals; Mode collapse to single anchor → insufficient anchor count or improper NLL assignment

- **First 3 experiments**: 1. Ablate anchor count (1, 4, 8, 16) on held-out RECON trajectories; verify 8 anchors minimize ADE without degrading mAP 2. Compare RAM vs. full fine-tuning vs. head-only fine-tuning on NavBench-GS obstacle scenarios; confirm RAM achieves highest SR with lowest CT 3. Deploy frozen pretrained model vs. S2E-Full on real quadruped in pedestrian scenario; validate zero-shot transfer of collision avoidance learned in simulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explicit 3D perception, such as depth estimation or occupancy prediction, be effectively integrated into the S2E framework to resolve collision avoidance failures inherent to vision-only approaches?
- Basis in paper: [explicit] Section 7 states, "current systems lack 3D perception... One potential solution is integrating depth estimation or occupancy (OCC) prediction tasks to infer 3D structural cues."
- Why unresolved: The current S2E model relies exclusively on RGB observations, which the authors acknowledge leads to persistent collision challenges.
- What evidence would resolve it: An ablation study showing that augmenting the visual encoder with depth/occupancy heads significantly reduces collision rates in NavBench-GS without compromising generalization.

### Open Question 2
- Question: Can the Seeing-to-Experiencing framework be successfully extended to complex interactive tasks beyond navigation, such as mobile manipulation?
- Basis in paper: [explicit] Section 7 explicitly lists this as a goal: "In the future, we also plan to extend the proposed framework to other robotic applications such as mobile manipulation."
- Why unresolved: The paper validates the method only on navigation (waypoint following); it is unclear if the Anchor-Guided Distribution Matching and Residual-Attention Module can handle the action spaces and physics of manipulation.
- What evidence would resolve it: Successful zero-shot transfer of an S2E-trained policy to a mobile manipulation task in NavBench-GS or a similar simulation environment.

### Open Question 3
- Question: Can higher-fidelity simulation modeling and extensive data augmentation effectively mitigate the sim-to-real performance degradation caused by locomotion control inaccuracies?
- Basis in paper: [explicit] Section 7 identifies "performance degradation induced by locomotion control inaccuracies" and suggests "Implementing higher-fidelity simulation modeling coupled with extensive data augmentation strategies may effectively mitigate these issues."
- Why unresolved: The current framework abstracts locomotion via a velocity interface, potentially hiding low-level mechanical discrepancies that cause failures in real-world deployment.
- What evidence would resolve it: Real-world deployment results showing improved success rates when S2E is fine-tuned in higher-fidelity simulations compared to the current URBAN-SIM setup.

## Limitations

- Anchor sampling distribution specifics (forward direction range/spacing) are underspecified, potentially affecting multimodal behavior modeling
- Reward coefficient values for RL training are not provided, making it difficult to reproduce the exact training dynamics
- The exact curriculum schedule for obstacle and pedestrian density during RL finetuning is unclear

## Confidence

- **High Confidence**: Anchor-Guided Distribution Matching improves trajectory prediction multimodality and the 8-anchor configuration's optimality (supported by ablation Table 7)
- **Medium Confidence**: RAM effectively preserves pretrained knowledge while enabling RL adaptation, as evidenced by superior sim-to-real transfer (requires replication to fully verify)
- **Medium Confidence**: RL provides substantial gains over supervised scaling alone, with 21% SR improvement (limited by single RL training run reported)

## Next Checks

1. Replicate the anchor ablation study (1, 4, 8, 16 anchors) on a held-out RECON dataset to verify mAP/ADE trends and confirm 8 anchors as optimal
2. Compare RAM vs. full fine-tuning vs. head-only fine-tuning on NavBench-GS, measuring SR, CT, and OOD generalization to confirm RAM's knowledge preservation benefit
3. Deploy the pretrained-only and S2E-Full models in a real-world pedestrian navigation scenario to validate zero-shot sim-to-real transfer claims