---
ver: rpa2
title: 'Beyond the Surface: Measuring Self-Preference in LLM Judgments'
arxiv_id: '2506.02592'
source_url: https://arxiv.org/abs/2506.02592
tags:
- bias
- response
- llama-3
- self-preference
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of self-preference bias in large
  language models (LLMs) used as judges, where models tend to favor their own responses
  over those generated by others. The core method introduces the DBG score, which
  measures bias as the difference between a judge model's scores for its own responses
  and gold judgments (proxies for true response quality), rather than comparing to
  scores from other models.
---

# Beyond the Surface: Measuring Self-Preference in LLM Judgments

## Quick Facts
- arXiv ID: 2506.02592
- Source URL: https://arxiv.org/abs/2506.02592
- Reference count: 40
- Key outcome: All tested LLMs exhibit self-preference bias, with larger models showing less bias than smaller ones

## Executive Summary
This paper introduces a novel metric called DBG score to measure self-preference bias in large language models used as judges, where models systematically favor their own responses over others. Unlike previous approaches that compare scores between different models, DBG isolates bias by measuring the difference between a judge model's scores for its own responses versus gold judgments (aggregated from multiple strong LLMs). Experiments across various model sizes, versions, and reasoning abilities demonstrate that self-preference bias is universal among LLMs, with mitigation strategies including style alignment and training on identical data showing promise.

## Method Summary
The DBG score measures self-preference bias as the difference between scores assigned by a judge model to its own responses versus corresponding gold judgments. The methodology uses pairwise comparisons with position swapping to control for presentation order, temperature=0 for determinism, and gold judgments aggregated from three strong LLMs (GPT-4o-mini, Gemini-1.5-Flash, DeepSeek-V3). Models are tested on three datasets (AlpacaEval, WMT19, TruthfulQA) with responses constrained to 200 words. Pre-trained models use 2-shot in-context learning while post-trained models use zero-shot. Attention analysis examines per-layer attention patterns when models judge self versus other responses.

## Key Results
- All LLMs exhibit self-preference bias, preferring their own responses regardless of actual quality
- Larger models (Llama-3.1-70B, Qwen2.5-72B) show significantly less bias than smaller models (Llama-3.1-8B, Qwen2.5-7B)
- Both pre-trained and post-trained models display self-preference bias
- Reasoning models are not immune to self-preference bias
- Style alignment and fine-tuning on identical data can reduce self-preference bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The DBG score isolates self-preference bias from response quality confounds.
- **Mechanism:** By comparing judge model scores against gold judgments rather than against other models' scores, the metric subtracts the quality component δ from the bias component bA, yielding: ŵA = E_x[σ(δ + bA) - σ(δ)].
- **Core assumption:** Gold judgments approximate true response quality sufficiently well; individual model biases in the gold judge ensemble cancel out or remain small.
- **Evidence anchors:**
  - [abstract] "The DBG score measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments."
  - [Section 2] Formal derivation shows ŵA ≈ E_x[σ'(δ)] · E_x[bA] under weak correlation assumption.
  - [corpus] Related work "Do LLM Evaluators Prefer Themselves for a Reason?" confirms self-preference correlates with capability but uses different methodology.
- **Break condition:** If gold judges themselves exhibit systematic bias toward certain response styles, DBG may misattribute style effects to self-preference.

### Mechanism 2
- **Claim:** Models allocate higher attention to their own generated responses during judgment.
- **Mechanism:** Attention analysis reveals that when a model evaluates response pairs, it naturally assigns higher attention scores to tokens from its own outputs compared to tokens from other models—potentially priming higher preference scores.
- **Core assumption:** Higher attention allocation causally influences judgment outcomes rather than merely correlating with familiarity.
- **Evidence anchors:**
  - [Section 4.4] "Each model tends to assign more attention to its own responses than the other model does... contributing to the emergence of self-preference."
  - [Figure 7] Shows attention score differences across layers, with models attending more to self-generated responses.
  - [corpus] Weak/no direct corpus evidence on attention mechanisms for self-preference.
- **Break condition:** If attention differences reflect genuine quality recognition rather than self-familiarity, the causal claim fails.

### Mechanism 3
- **Claim:** Aligning response styles across models reduces self-preference bias.
- **Mechanism:** Rewriting responses into unified styles (attractive/humorous) removes superficial stylistic markers that models may use for self-recognition, thereby reducing the self-other distinction during judgment.
- **Core assumption:** Style serves as a primary cue for self-recognition; content-based self-preference persists but is smaller.
- **Evidence anchors:**
  - [Section 4.2] DBG scores decreased after style transfer (e.g., Llama-3.1-8B: 18.7% → 7.2% for attractive style).
  - [Section 4.3] Fine-tuning different models on identical data (UltraChat-200k) reduced DBG scores.
  - [corpus] No direct corpus evidence on style-based mitigation.
- **Break condition:** If models recognize self-generated content through semantic fingerprints beyond style, style alignment will not eliminate bias.

## Foundational Learning

- **Concept: Bradley-Terry preference modeling**
  - **Why needed here:** The paper formalizes judgment as P(rA ≻ rB | x) = σ(S_A(rA) - S_A(rB)), building directly on this framework.
  - **Quick check question:** Can you explain why the sigmoid function maps score differences to probabilities?

- **Concept: Position and length bias in LLM judges**
  - **Why needed here:** The methodology explicitly swaps response positions and constrains length to control these confounds.
  - **Quick check question:** Why would a model systematically prefer the first-presented response regardless of quality?

- **Concept: Gold standard construction via ensemble aggregation**
  - **Why needed here:** The DBG score depends on gold judgments constructed by aggregating three strong LLMs.
  - **Quick check question:** What assumptions must hold for ensemble aggregation to reduce individual model bias?

## Architecture Onboarding

- **Component map:** Generate responses -> Collect judge model token probabilities (with position swap) -> Obtain gold judgments -> Compute DBG = Judge_Score - Gold_Score
- **Critical path:** Generate responses → Collect judge model token probabilities (with position swap) → Obtain gold judgments → Compute DBG = Judge_Score - Gold_Score
- **Design tradeoffs:**
  - Using 3-model ensemble vs. single strongest model: ensemble reduces individual bias but increases cost
  - Binary judgment (A/B) vs. continuous scores: binary is more stable but loses granularity
  - Temperature 0 ensures determinism but may underrepresent model uncertainty
- **Failure signatures:**
  - DBG ≈ 0 but gold-human alignment low → gold judges unreliable
  - DBG varies dramatically across datasets → bias is task-specific
  - Large position swap discrepancies → position bias not fully mitigated
- **First 3 experiments:**
  1. Replicate DBG scoring on a new dataset (e.g., 100 samples from your domain) to validate gold judge alignment with human annotations.
  2. Test attention analysis: extract attention weights when your model judges self vs. other responses to verify the self-attention mechanism.
  3. Apply style transfer to your model's responses and measure DBG change to assess whether style-based mitigation generalizes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does self-preference bias manifest differently in complex interactive scenarios, such as agent-based tasks or multi-turn dialogues, compared to the single-turn instruction-following tasks tested?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they limited the scope to instruction-following and translation tasks, noting that "Further investigation is needed to explore the self-preference bias of LLMs in other tasks, such as agent tasks and dialogue tasks."
- **Why unresolved:** The current experimental design relies on single-turn datasets (AlpacaEval, WMT19, TruthfulQA) and does not assess how context retention or tool usage in longer interactions affects the bias.
- **What evidence would resolve it:** Applying the DBG score metric to multi-turn dialogue benchmarks and agent task trajectories to compare bias severity against the single-turn baselines established in this paper.

### Open Question 2
- **Question:** Does utilizing significantly more capable models (e.g., GPT-4o or Gemini-1.5-Pro) as gold judges substantially alter the calculated DBG scores compared to the lighter models used in this study?
- **Basis in paper:** [explicit] The paper notes: "Due to cost constraints, we do not utilize more powerful models, such as GPT-4o or Gemini-1.5-Pro. Using these more capable models could potentially provide more reliable gold-standard judgments."
- **Why unresolved:** The reliability of the DBG score depends on the "gold judgment" being an unbiased proxy for truth. If the lighter models used for gold judgments (GPT-4o-mini, Gemini-1.5-Flash) have subtle biases, the resulting DBG measurements may be skewed.
- **What evidence would resolve it:** A comparative analysis re-evaluating a subset of the data using state-of-the-art proprietary models as the gold standard to measure the variance in the resulting DBG scores.

### Open Question 3
- **Question:** Is the higher attention allocation to self-generated responses the *cause* of self-preference bias, or is it merely a correlation with no causal impact on the final judgment?
- **Basis in paper:** [inferred] The paper observes that "models naturally allocate more attention to their own responses" and suggests this "may partly explain" the bias. However, correlation in attention patterns does not prove that this mechanism drives the judgment decision.
- **Why unresolved:** The analysis is observational; it identifies a pattern in the attention layers but does not perform an interventional study to test if manipulating this attention changes the bias.
- **What evidence would resolve it:** Mechanistic interpretability experiments where attention weights on self-generated tokens are dampened or masked during inference to observe if the self-preference bias decreases or disappears.

## Limitations

- DBG score validity depends on gold judgments being unbiased proxies for true response quality, but ensemble aggregation may still harbor systematic biases
- Attention analysis showing differential allocation to self vs. other responses remains correlational without proving causation
- Style alignment mitigation only addresses superficial stylistic markers; deeper semantic self-recognition mechanisms may persist

## Confidence

- DBG score methodology and correlation with model size/capability: **High**
- Attention analysis showing differential allocation to self vs. other responses: **Medium**
- Style alignment as effective mitigation for self-preference bias: **Medium**
- Claim that reasoning models are not immune to self-preference: **Low** (based on single dataset)

## Next Checks

1. Test DBG score reliability by comparing against human annotations on a held-out subset to validate gold judge alignment
2. Conduct ablation study removing attention mechanism to determine if it's necessary for self-preference emergence
3. Apply DBG scoring to out-of-distribution domains (e.g., code generation, mathematical reasoning) to assess generalizability