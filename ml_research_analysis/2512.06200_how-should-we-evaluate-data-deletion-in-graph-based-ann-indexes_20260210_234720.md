---
ver: rpa2
title: How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?
arxiv_id: '2512.06200'
source_url: https://arxiv.org/abs/2512.06200
tags:
- uni00000013
- deletion
- uni00000014
- uni00000048
- uni0000004f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a comprehensive evaluation framework for data\
  \ deletion in graph-based ANNS indexes, addressing the lack of standardized assessment\
  \ methods. The authors formalize three deletion approaches\u2014logical deletion,\
  \ physical deletion, and rebuilding\u2014and implement them in HNSW."
---

# How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?

## Quick Facts
- **arXiv ID:** 2512.06200
- **Source URL:** https://arxiv.org/abs/2512.06200
- **Reference count:** 39
- **Primary result:** Proposed comprehensive evaluation framework for data deletion in graph-based ANNS indexes, demonstrating dynamic Deletion Control strategy reduces total deletion time while maintaining accuracy requirements.

## Executive Summary
This paper addresses the critical gap in evaluating data deletion capabilities for graph-based Approximate Nearest Neighbor Search (ANNS) indexes, which are essential for privacy compliance and dynamic datasets. The authors formalize three deletion approaches—logical deletion (flag-based), physical deletion (edge removal), and rebuilding (full reconstruction)—and implement them in HNSW. Through extensive experiments across five datasets, they demonstrate that each method has distinct trade-offs in accuracy, speed, and memory usage. Based on these findings, they propose Deletion Control, a dynamic strategy that selects the optimal deletion method based on accuracy requirements, achieving significant performance improvements over single-method approaches.

## Method Summary
The evaluation framework implements three deletion methods in HNSW: logical deletion marks nodes as deleted via flags without removing them (fastest at ~10⁹ QPS-delete but unbounded memory growth), physical deletion removes nodes and prunes edges from neighbors (moderate speed at ~10³ QPS-delete with stabilized accuracy), and rebuilding constructs a new graph from remaining vectors (slowest but preserves accuracy). The framework conducts experiments with batch sizes of 10⁵ and 10³ across multiple datasets, measuring 1-Recall@10, query throughput, memory usage, and deletion throughput over 5-15 update steps. Deletion Control calibrates parameters θ (minimum achievable accuracy with physical deletion) and π (maximum logical deletion steps before accuracy drops below threshold) from training queries, then dynamically switches between methods to minimize total deletion time while maintaining accuracy requirements.

## Key Results
- Logical deletion achieves fastest deletion (10⁹ QPS-delete) but causes unbounded memory growth and accuracy degradation over time
- Physical deletion shows accuracy stabilization to dataset-dependent θ values after sufficient update cycles, with moderate deletion speed (10³ QPS-delete)
- Deletion Control reduces total deletion time by 45% compared to physical-only approach while maintaining accuracy requirements
- Larger batch sizes (10⁵ vs 10³) yield higher stabilized accuracy values under physical deletion

## Why This Works (Mechanism)

### Mechanism 1: Physical Deletion Accuracy Stabilization
- Claim: Physical deletion in graph-based ANNS indexes causes search accuracy to converge to a stable value after repeated update cycles.
- Mechanism: When edges connected to deleted nodes are removed, the graph structure initially degrades. However, after sufficient insert/delete iterations, accuracy stops declining and stabilizes at a dataset-dependent value θ. The insertion process partially repairs graph connectivity during subsequent batch insertions, preventing indefinite degradation.
- Core assumption: The insertion process partially repairs graph connectivity during subsequent batch insertions, preventing indefinite degradation.
- Evidence anchors:
  - [Section 4] "in physical deletion, search accuracy stabilizes to a constant value after multiple updates"
  - [Section D.2] "This indicates that the structural properties of the graph become stable after a sufficient number of operations"
  - [corpus] Corpus papers (CleANN, FreshDiskANN) address dynamism but do not specifically validate or refute this stabilization phenomenon
- Break condition: Very small batch sizes (b=10³ vs b=10⁵) may not provide sufficient insertion volume to repair structure, potentially preventing stabilization or yielding lower converged accuracy.

### Mechanism 2: Deletion Control Hybrid Strategy
- Claim: Dynamically switching between deletion methods based on accuracy requirements can reduce total deletion time while maintaining accuracy thresholds.
- Mechanism: The Deletion Control algorithm estimates two parameters from a training query set: θ (minimum accuracy achievable with physical deletion alone) and π (maximum steps logical deletion can run before accuracy drops below threshold α). If α < θ, physical deletion alone suffices. Otherwise, alternate logical deletion (fast, ~10⁹ QPS-delete) for π steps with occasional rebuilding (slow, restores accuracy).
- Core assumption: Accuracy degradation during logical deletion is approximately linear, and rebuilding fully restores initial accuracy R₀.
- Evidence anchors:
  - [Section 5] "the proposed method has the smallest total deletion time among the deletion strategies that meet the accuracy requirement"
  - [Figure 4] Shows proposed method at 25 min total time vs 46 min for physical-only at α=0.84
  - [corpus] No direct validation in corpus papers; related work focuses on incremental maintenance rather than adaptive strategy selection
- Break condition: Non-linear degradation or incomplete rebuilding would cause π estimates to fail, potentially violating accuracy constraints.

### Mechanism 3: Logical Deletion Memory Accumulation
- Claim: Logical deletion causes unbounded memory growth proportional to cumulative deletions.
- Mechanism: Logical deletion only marks nodes as deleted via flag set F without removing them from node set P or neighbor sets N. Deleted vectors and their edges remain in memory indefinitely; only search-time filtering excludes them from results.
- Core assumption: Flag-checking overhead during search is negligible relative to distance computations.
- Evidence anchors:
  - [Section 3, Figure 1a] Illustrates flag array parallel to data array with deleted nodes persisting
  - [Section D.3] "memory consumption in logical deletion increases linearly with each step across all datasets"
  - [corpus] Corpus papers do not explicitly address memory accumulation patterns
- Break condition: When deleted-to-active ratio becomes extreme, search may return empty candidate sets after filtering, causing functional failure before memory exhaustion.

## Foundational Learning

- Concept: **Graph-based Approximate Nearest Neighbor Search (ANNS)**
  - Why needed here: The entire evaluation framework addresses deletion in graph-based indexes; understanding why graphs work for ANNS is prerequisite.
  - Quick check question: Can you explain why navigating a proximity graph can find approximate nearest neighbors faster than linear scan?

- Concept: **Hierarchical Navigable Small World (HNSW) graphs**
  - Why needed here: All experiments implement deletion methods in HNSW, the de facto standard graph-based ANNS index.
  - Quick check question: How does the multi-layer hierarchy in HNSW enable logarithmic search complexity?

- Concept: **Recall@k metric**
  - Why needed here: 1-Recall@10 is the primary accuracy metric used throughout all experiments and Deletion Control calibration.
  - Quick check question: Given ground truth nearest neighbor g and approximate result set R̂ with |R̂|=10, how is 1-Recall@10 computed?

## Architecture Onboarding

- Component map:
  - Index Structure: Node set P (vectors), neighbor sets N (graph edges), optional flag set F (logical deletion)
  - Search Module: SEARCH(q, P, N) → R, with post-filtering for logical deletion
  - Deletion Module: Three implementations—logical (O(1) flag update), physical (edge removal from neighbors), rebuild (full reconstruction via CONSTRUCT)
  - Deletion Control: Calibration phase (estimate θ, π from training queries) + runtime policy selector
  - Construction Module: CONSTRUCT(P) → N invoked for rebuilding

- Critical path:
  1. Define accuracy requirement α for your deployment
  2. Run calibration: execute physical deletion to convergence for θ; execute logical deletion to estimate degradation rate Δ for π calculation
  3. Deploy policy: if α < θ → physical-only; else → logical for π steps, then rebuild, repeat
  4. Monitor actual recall periodically to validate estimates

- Design tradeoffs:
  - **Speed vs. memory**: Logical deletion fastest (10⁹ QPS-delete) but unbounded memory; physical deletion slower (10³ QPS-delete) but stable memory
  - **Accuracy vs. maintenance cost**: Rebuilding preserves accuracy but blocks queries during reconstruction; physical deletion trades some accuracy for lower overhead
  - **Batch size**: Larger batches (10⁵) yield higher stabilized accuracy than small batches (10³) under physical deletion

- Failure signatures:
  - Empty search results (logical deletion with excessive flagged nodes)
  - Disconnected graph components causing unreachable regions (physical deletion without repair)
  - Accuracy falling below α (incorrect π estimation or non-linear degradation)
  - Memory exhaustion (long-running logical deletion without rebuild trigger)

- First 3 experiments:
  1. Replicate baseline comparison on SIFT1M: implement all three deletion methods in HNSW, measure QPS-delete, 1-Recall@10 trajectory, and memory across 5 steps with b=10⁵
  2. Validate batch size effect: run physical deletion on SIFT1B subset with b=10³ vs b=10⁵, plot accuracy convergence to confirm larger batches yield higher θ
  3. End-to-end Deletion Control test: set α=0.84, calibrate on 10% training queries, run full workload on remaining queries, verify total deletion time reduction while maintaining recall ≥ α

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed evaluation framework and Deletion Control strategy be effectively generalized to non-HNSW indexes, such as tree-based or IVF-based ANNS methods?
- Basis in paper: [explicit] The authors limit their implementation to one structure, stating, "We apply the proposed evaluation framework to Hierarchical Navigable Small World."
- Why unresolved: Different index structures manage connectivity and partitioning differently; the trade-offs observed in graph-based HNSW may not translate directly to partition-based or tree-based systems.
- What evidence would resolve it: Experimental results applying the logical, physical, and rebuilding protocols to diverse index architectures like IVF or KD-trees.

### Open Question 2
- Question: How can Deletion Control be adapted for dynamic environments where query distributions shift or a labeled training set is unavailable?
- Basis in paper: [explicit] The method relies on the assumption that "a small query training set (query data with known ground truth)" is available to estimate parameters θ and π.
- Why unresolved: Real-world streaming applications often feature non-stationary query distributions, which could render the pre-estimated thresholds inaccurate over time.
- What evidence would resolve it: A study evaluating the robustness of the policy under distribution drift or an adaptation using online parameter estimation without labeled data.

### Open Question 3
- Question: Does the search accuracy of physical deletion stabilize permanently, or does it eventually suffer from graph fragmentation or unreachable nodes over extended update cycles?
- Basis in paper: [inferred] The authors observe that accuracy "stabilizes to a constant value," but the experiments are limited to a small number of steps (up to 15), while physical deletion inherently alters graph topology.
- Why unresolved: A short-term plateau in accuracy might precede long-term structural degradation (e.g., disconnected components) that is not visible in the provided experimental window.
- What evidence would resolve it: Experiments extending the update steps to thousands of iterations to analyze long-term structural integrity and recall trends.

## Limitations
- Physical deletion accuracy stabilization mechanism lacks theoretical grounding for why specific θ values emerge
- Deletion Control's linear degradation assumption for logical deletion may not hold under different workloads
- Memory accumulation in logical deletion is unbounded in theory but practical failure points occur at lower thresholds than documented

## Confidence
- **High**: Logical deletion QPS measurements (10⁹ QPS-delete is straightforward timing); memory growth patterns under logical deletion; accuracy stabilization observation across multiple datasets
- **Medium**: Physical deletion accuracy stabilization mechanism (empirical observation but lacks theoretical explanation); Deletion Control strategy effectiveness (simulation-based validation only); batch size effects on θ
- **Low**: Exact functional form of accuracy degradation during logical deletion; completeness of rebuilding in restoring initial accuracy; edge case behaviors in physical deletion

## Next Checks
1. Test Deletion Control under adversarial query distributions (e.g., focused on recently deleted regions) to verify π estimates remain valid when logical deletion degradation becomes non-linear
2. Measure physical deletion performance on extremely sparse graphs where edge removal causes significant fragmentation to validate whether stabilization occurs or accuracy continues degrading
3. Implement memory pressure monitoring during long-running logical deletion to identify the practical point where search begins returning empty results rather than waiting for theoretical memory exhaustion