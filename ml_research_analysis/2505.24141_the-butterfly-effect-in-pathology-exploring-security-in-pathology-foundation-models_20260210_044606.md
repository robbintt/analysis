---
ver: rpa2
title: 'The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation
  Models'
arxiv_id: '2505.24141'
source_url: https://arxiv.org/abs/2505.24141
tags:
- attack
- adversarial
- pathology
- perturbation
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic investigation into the
  security of whole slide image-level pathology foundation models (WSI-FMs) under
  adversarial attacks. The authors propose a label-free adversarial attack framework
  based on the principle of "local perturbation with global impact," which targets
  only a small subset of patches (0.1% per slide) with imperceptible noise.
---

# The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models

## Quick Facts
- arXiv ID: 2505.24141
- Source URL: https://arxiv.org/abs/2505.24141
- Reference count: 40
- Primary result: Imperceptible perturbations to <0.1% of patches can cause up to 20% accuracy degradation in pathology foundation models

## Executive Summary
This study presents the first systematic investigation into the security vulnerabilities of whole slide image-level pathology foundation models (WSI-FMs) under adversarial attacks. The authors develop a label-free adversarial attack framework targeting only a small subset of patches (0.1% per slide) with imperceptible noise. Comprehensive experiments across three representative WSI-FMs (CHIEF, PRISM, and TITAN) demonstrate that these models remain vulnerable to attacks that can degrade accuracy by up to 20%. The research identifies key factors influencing attack success and proposes a lightweight defense strategy using uniform noise that shows promise in mitigating attacks while maintaining model performance.

## Method Summary
The authors propose a label-free adversarial attack framework based on the principle of "local perturbation with global impact," adapting four classical white-box attack methods (FGSM, BIM, MIM, and C&W) to this setting. They redefine the perturbation budget specifically for whole slide images and conduct experiments across five datasets and six downstream tasks. The framework targets only a small subset of patches (0.1% per slide) with imperceptible noise, evaluating the adversarial robustness of three representative WSI-FMs: CHIEF, PRISM, and TITAN. A lightweight defense strategy using small-scale uniform noise is also proposed and evaluated.

## Key Results
- Imperceptible perturbations to fewer than 0.1% of patches can cause up to 20% accuracy degradation in WSI-FMs
- CHIEF exhibits the strongest adversarial robustness among the tested models, though all remain vulnerable
- Perturbation scope has greater impact on attack success than magnitude
- Normal patches within tumor WSIs are more vulnerable to attacks than tumor patches

## Why This Works (Mechanism)
The effectiveness of these attacks stems from the foundation models' reliance on global contextual information across entire whole slide images. By introducing small, strategically placed perturbations to a minimal number of patches, the attacks can significantly alter the model's overall interpretation of the slide. The label-free approach is particularly effective because it exploits the model's learned representations without requiring knowledge of specific downstream task labels, making the attacks more general and harder to defend against through traditional supervised methods.

## Foundational Learning
- Adversarial machine learning: Understanding how small, carefully crafted perturbations can cause significant model errors is crucial for developing secure AI systems
- Whole slide image processing: WSIs contain billions of pixels, requiring specialized patch-based processing and global context integration
- Foundation model vulnerabilities: Large pre-trained models can have unexpected weak points that become attack surfaces
- Transfer learning in pathology: Models trained on diverse datasets can be fine-tuned for specific diagnostic tasks, but may inherit vulnerabilities
- Perturbation optimization: Mathematical techniques for generating minimal changes that maximize model error
- Clinical AI safety: Ensuring diagnostic AI systems are robust against intentional manipulation is essential for clinical deployment

## Architecture Onboarding
**Component Map:** Input WSI -> Patch Extraction -> Feature Encoding -> Global Context Pooling -> Classification Head
**Critical Path:** The attack primarily targets the feature encoding and global context pooling stages, where local patch perturbations can influence the overall slide representation
**Design Tradeoffs:** The models balance between local detail capture and global context integration, creating vulnerabilities where local manipulations affect global interpretation
**Failure Signatures:** Accuracy degradation of up to 20% with imperceptible changes to <0.1% of patches, with normal regions being unexpectedly more vulnerable than tumor regions
**3 First Experiments:** 1) Baseline accuracy measurement across all models and tasks, 2) Attack success rate with varying perturbation scopes, 3) Defense effectiveness of uniform noise across different attack methods

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus exclusively on white-box attack scenarios with full model access
- Practical implications in clinical workflows and real-world deployment conditions remain unclear
- Defense strategy using uniform noise is relatively simplistic and may not withstand adaptive attacks
- Difficulty in isolating which specific architectural choices contribute to observed robustness differences

## Confidence
- High confidence: Imperceptible perturbations to <0.1% of patches can cause up to 20% accuracy degradation
- Medium confidence: CHIEF exhibits superior adversarial robustness compared to PRISM and TITAN
- Medium confidence: Normal patches within tumor WSIs are more vulnerable than tumor patches

## Next Checks
1. Test the proposed attack framework under black-box and transfer attack scenarios to assess practical attack feasibility
2. Evaluate the uniform noise defense strategy against adaptive attacks where adversaries specifically target the defense mechanism
3. Conduct clinical workflow simulations to understand the practical impact of successful adversarial attacks on diagnostic accuracy and patient outcomes