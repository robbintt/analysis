---
ver: rpa2
title: Cognitive bias in LLM reasoning compromises interpretation of clinical oncology
  notes
arxiv_id: '2511.20680'
source_url: https://arxiv.org/abs/2511.20680
tags:
- clinical
- reasoning
- error
- errors
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GPT-4 reasoning errors, particularly confirmation bias and anchoring\
  \ bias, were associated with guideline-discordant and potentially harmful clinical\
  \ recommendations in oncology note interpretation. A hierarchical error taxonomy\
  \ was developed and validated across two cancer types (breast, pancreatic, and prostate),\
  \ with high inter-rater agreement (\u03BA\u22650.85) and an overall error rate of\
  \ 23.1% (\xB10.8%)."
---

# Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes

## Quick Facts
- arXiv ID: 2511.20680
- Source URL: https://arxiv.org/abs/2511.20680
- Reference count: 0
- GPT-4 reasoning errors led to guideline-discordant, potentially harmful oncology recommendations

## Executive Summary
Large language models (LLMs) exhibit significant reasoning errors when interpreting clinical oncology notes, with cognitive biases manifesting as harmful clinical recommendations. The study evaluated GPT-4's performance across breast, pancreatic, and prostate cancer cases, finding an overall error rate of 23.1% (±0.8%). These errors were predominantly reasoning failures (85.4%) rather than comprehension issues, with the highest frequency occurring in management-phase tasks and advanced disease states. Recommendations containing reasoning errors showed significantly lower clinical impact scores and were more likely to deviate from NCCN guidelines.

## Method Summary
The study developed a three-tier hierarchical error taxonomy mapping computational failures to cognitive bias frameworks, validated across 844 LLM responses to 60 oncology queries. GPT-4-32k generated responses using zero-shot chain-of-thought prompts with temperature=0. Two independent annotators applied the taxonomy to identify first reasoning failures per response, achieving high inter-rater reliability (κ≥0.85). The research used the CORAL dataset (40 notes: 20 breast, 20 pancreatic cancer from PhysioNet) and Mayo Clinic prostate cancer cohort (24 consult notes across disease stages). Automated evaluators including GPT-4-Turbo, Claude 3.5-Sonnet, and Gemini 1.5-Pro were tested but failed to reliably classify error subtypes.

## Key Results
- GPT-4 error rate of 23.1% (±0.8%) across oncology note interpretation tasks
- Reasoning failures dominated (85.4%) over comprehension errors, most common in management-phase tasks and advanced disease
- Recommendations with reasoning errors had significantly lower clinical impact scores (p < 0.0001) and higher guideline discordance

## Why This Works (Mechanism)
LLMs struggle with clinical oncology reasoning due to inherent cognitive biases that emerge during chain-of-thought processing. The study demonstrates that confirmation bias and anchoring bias manifest as systematic errors in medical decision-making, particularly when handling complex management decisions in advanced disease states. These biases compromise the model's ability to properly weigh clinical evidence and follow established guidelines.

## Foundational Learning

1. **Three-tier error taxonomy** - Hierarchical classification system mapping computational failures to cognitive biases; needed for systematic error analysis and achieving high inter-rater agreement.
2. **Chain-of-thought prompting** - Step-by-step reasoning approach that paradoxically exposes and amplifies cognitive biases in LLM outputs.
3. **Clinical impact scoring** - Five-point Likert scale assessment of recommendation severity; needed to quantify harm potential of reasoning errors.
4. **NCCN guideline concordance** - Standardized oncology treatment protocols; used as ground truth for evaluating recommendation quality.
5. **Inter-rater reliability (κ statistic)** - Statistical measure of annotator agreement; κ≥0.85 achieved indicates robust taxonomy design.
6. **Error localization** - Identifying first reasoning failure per response; critical for understanding where cognitive biases emerge in the reasoning chain.

## Architecture Onboarding

Component map: Oncology notes → GPT-4-32k (CoT) → LLM responses → Human annotation (3-tier taxonomy) → Error classification → Clinical impact assessment

Critical path: Note interpretation → Chain-of-thought reasoning → First reasoning failure identification → Taxonomy classification → Clinical impact evaluation

Design tradeoffs: Zero-shot prompting (simplicity, no fine-tuning) vs. potentially higher error rates compared to few-shot approaches; human annotation (accuracy) vs. automated evaluation (scalability).

Failure signatures: Low inter-rater agreement (κ<0.85) indicates ambiguous taxonomy criteria; automated evaluators detecting errors but misclassifying subtypes; variable error rates across cancer types suggesting disease-specific factors.

Three first experiments:
1. Pilot annotation on 20-30 responses to measure initial inter-rater agreement and refine taxonomy rubric
2. Compare error rates between breast, pancreatic, and prostate cancer cohorts to identify disease-specific patterns
3. Test automated evaluator performance on a subset of responses to quantify human vs. machine classification agreement

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown taxonomy rubric details in Table S1 essential for consistent error classification and achieving reported high inter-rater agreement (κ≥0.85)
- Lack of access to Mayo Clinic prostate cancer consult notes, representing substantial portion of validation cohort
- Reliance on zero-shot chain-of-thought prompting without few-shot examples may limit generalizability

## Confidence
- High confidence: Error rate (23.1% ±0.8%) and inter-rater reliability (κ≥0.85) claims
- Medium confidence: Clinical impact scores and NCCN guideline concordance findings
- Low confidence: Automated evaluator performance claims

## Next Checks
1. Obtain and review Table S1 taxonomy rubric with decision boundaries and examples to establish consistent error classification criteria
2. Pilot the annotation process on a small subset of 20-30 responses to measure initial inter-rater agreement and refine the taxonomy rubric
3. Compare error rates and distributions across cancer types in the CORAL dataset to identify disease-specific factors influencing reasoning failures