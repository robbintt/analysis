---
ver: rpa2
title: 'OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph'
arxiv_id: '2511.18622'
source_url: https://arxiv.org/abs/2511.18622
tags:
- opengloss
- semantic
- wordnet
- generation
- sense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OpenGloss is a synthetic English lexical resource that integrates\
  \ definitions, encyclopedic content, etymologies, and semantic relationships. Generated\
  \ through a multi-agent LLM pipeline with schema validation, it contains 537K senses\
  \ across 150K lexemes\u20144.6\xD7more than WordNet 3.1\u2014with 9.1M semantic\
  \ edges, 1M usage examples, and 60M words of encyclopedic content."
---

# OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph

## Quick Facts
- arXiv ID: 2511.18622
- Source URL: https://arxiv.org/abs/2511.18622
- Authors: Michael J. Bommarito
- Reference count: 40
- Contains 537K senses across 150K lexemes with 9.1M semantic edges

## Executive Summary
OpenGloss is a synthetic English lexical resource that integrates definitions, encyclopedic content, etymologies, and semantic relationships. Generated through a multi-agent LLM pipeline with schema validation, it contains 537K senses across 150K lexemes—4.6× more than WordNet 3.1—with 9.1M semantic edges, 1M usage examples, and 60M words of encyclopedic content. The entire dataset was produced in under one week for under $1,000, demonstrating that structured generation can create comprehensive lexical resources at practical cost and time scales. OpenGloss complements existing resources by providing integrated pedagogical content absent from traditional computational lexicons, while maintaining broad semantic coverage suitable for vocabulary learning and NLP applications. The resource is publicly available under CC-BY 4.0.

## Method Summary
OpenGloss was generated using a four-stage multi-agent LLM pipeline with schema-validated outputs. The process began with lexeme selection from the `wamerican` dictionary (150K words, filtered for length 3-15) plus snowball sampling to add 76K additional lexemes. Stage 2 used two-agent LLM calls to generate senses (1-4 per POS) with definitions, synonyms, hypernyms, and examples. Stage 3 deterministically extracted 9.1M semantic edges across 13 relationship types from the sense data. Stage 4 enriched entries with etymologies (97.5% coverage) and 200-400 word encyclopedic entries (99.7% coverage). The entire pipeline used `pydantic-ai` with strict Pydantic V2 schemas for validation, with malformed outputs (2-4%) triggering automatic retry. Quality assurance was conducted using Claude Sonnet 4.5 to evaluate 1K samples against lexicographic standards.

## Key Results
- 537K senses across 150K lexemes (4.6× WordNet 3.1)
- 9.1M semantic edges across 13 relationship types
- 60M words of encyclopedic content generated in under one week for under $1,000
- 97.5% etymology coverage, 99.7% encyclopedia coverage

## Why This Works (Mechanism)

### Mechanism 1
Schema-validated structured generation enables reliable, reproducible lexical resource creation at scale. Pydantic V2 schemas constrain LLM outputs to typed, validated structures. Malformed outputs (2-4% estimated) trigger automatic retry with enhanced prompts. Each of the four pipeline stages validates before proceeding, enabling modular fault isolation.

### Mechanism 2
Snowball sampling from semantic relationships expands vocabulary coverage beyond static seed dictionaries. Stage 3 (graph construction) extracts edges from sense data deterministically. Discovered relationships (hypernyms, hyponyms, synonyms) reveal lexemes absent from initial wamerican seed, feeding back into lexeme selection for targeted expansion.

### Mechanism 3
Coarser sense granularity (1-4 senses per POS) produces computationally tractable graphs while maintaining pedagogical utility. Constrained sense generation limits polysemy representation, reducing graph density. This yields lower time-space complexity for traversal algorithms compared to WordNet's fine-grained distinctions.

## Foundational Learning

- **Pydantic schema validation for LLM outputs**: Why needed here - The entire pipeline depends on structured outputs. Without understanding how Pydantic constrains LLM responses into typed objects, you cannot debug generation failures or extend the schema. Quick check question: If an LLM returns `{"senses": [{"definition": 123}]}` where definition should be a string, what happens?

- **Semantic relationship types in lexical databases (hypernymy, hyponymy, synonymy, meronymy)**: Why needed here - Stage 3 (graph construction) extracts and validates these relationships. Understanding the distinction between paradigmatic (hypernym/hyponym) and syntagmatic (collocation) relations is essential for interpreting the 9.1M edges. Quick check question: Is "dog" → "animal" a hypernym or hyponym relationship from the perspective of "dog"?

- **Sense inventory design and polysemy**: Why needed here - The paper explicitly constrains to 1-4 senses per POS, trading off granularity for tractability. Understanding why WordNet has 57 senses for "run" helps contextualize this design choice. Quick check question: What is the risk of over-splitting senses for a word sense disambiguation task?

## Architecture Onboarding

- **Component map**: Lexeme Selection -> Sense Generation -> Graph Construction -> Enrichment
- **Critical path**: Stage 2 (sense generation) is the bottleneck—it requires the most LLM calls and produces the core content that subsequent stages depend on. Schema validation failures here cascade.
- **Design tradeoffs**:
  - Granularity vs. tractability: 1-4 senses per POS limits polysemy but improves graph algorithm performance
  - Breadth vs. depth: 150K lexemes with 3.58 avg senses vs. WordNet's finer distinctions
  - Generation cost vs. quality: gpt-5-nano selected for cost efficiency; Claude Sonnet 4.5 used for QA (higher quality, higher cost)
  - Pedagogical focus vs. comprehensive coverage: K-12 vocabulary prioritized over specialized technical terms
- **Failure signatures**:
  - Schema validation loops: If >5% of outputs retry, prompts may be underspecified
  - Semantic relationship drift: If hypernym chains contain cycles or cross-POS errors (noun hypernym of verb), validation failed
  - Encyclopedia hallucination: Factual claims in encyclopedia entries lack verification; 60M words of generated content will contain errors
  - Edge target validity: Invalid edges referencing non-existent lexemes should be caught; if not, graph integrity compromised
- **First 3 experiments**:
  1. Reproduce a 1K lexeme subset through the full pipeline with a different LLM backend (e.g., open-weights model) to validate methodology portability and cost claims.
  2. Sample 100 entries with high semantic relationship flag rates from QA; manually annotate to distinguish genuine errors from conservative QA standards.
  3. Benchmark graph traversal algorithms (shortest path, community detection) on OpenGloss vs. WordNet to validate claimed performance improvements from coarser granularity.

## Open Questions the Paper Calls Out

1. How does OpenGloss perform on standard NLP benchmarks compared to WordNet and other lexical resources? The paper focused on resource construction and automated QA; no downstream task evaluation was conducted. Systematic evaluation on SemEval/Senseval WSD tasks, SimLex-999, WordSim-353, and lexical substitution benchmarks with quantitative comparisons to WordNet baselines would resolve this.

2. What proportion of semantic relationship flags reflect genuine errors versus overly conservative QA standards? Only automated QA was conducted; no human expert validation distinguished true errors from acceptable variations. Professional lexicographer evaluation of flagged entries with inter-annotator agreement metrics comparing human judgments against automated QA verdicts would resolve this.

3. Can the multi-agent generation pipeline successfully produce comparable lexical resources for non-English languages? OpenGloss covers only English; cross-lingual applicability of the schema-validated generation approach remains untested. Applying the pipeline to generate lexical resources in typologically diverse languages and evaluating coverage, consistency, and quality against existing multilingual resources like BabelNet would resolve this.

## Limitations
- Quality validation relies entirely on automated QA rather than human expert review, leaving uncertainty about the accuracy of 60M words of generated encyclopedic content
- Semantic relationship accuracy remains unverified—the 9.1M edges may not reflect genuine linguistic patterns despite passing deterministic validation
- Cost and time claims depend on proprietary `gpt-5-nano` model access that may not be reproducible with more accessible LLM backends

## Confidence
- **High Confidence**: Schema validation mechanism works as described; basic pipeline architecture is clearly specified and technically feasible
- **Medium Confidence**: Quantitative claims about coverage and performance are internally consistent; computational tractability improvements from coarser granularity are logically sound
- **Low Confidence**: Quality of generated encyclopedic content, accuracy of semantic relationships, and pedagogical effectiveness remain largely unverified; cost claims depend on proprietary model access

## Next Checks
1. Commission lexicographers to manually evaluate 200 randomly sampled entries (100 high-confidence, 100 low-confidence per QA) to assess whether conservative QA thresholds correspond to actual quality issues versus false positives, particularly for encyclopedic content and semantic relationships.
2. Map OpenGloss semantic relationships to WordNet synsets and evaluate precision/recall of hypernym/hyponym pairs. Additionally, test graph traversal algorithms (shortest path, community detection) on identical subgraphs from both resources to quantify claimed performance improvements from coarser granularity.
3. Reproduce the pipeline for a 10K lexeme subset using publicly available models (`gpt-4o-mini`, `Claude Haiku`) to validate whether the <$1,000 claim holds under different backend configurations and to establish a baseline for cost-effectiveness comparisons.