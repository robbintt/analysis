---
ver: rpa2
title: 'Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning
  in Large Language Models'
arxiv_id: '2506.05850'
source_url: https://arxiv.org/abs/2506.05850
tags:
- language
- reasoning
- grpo
- training
- gsm8k
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual Collapse occurs when multilingual reasoning models,
  trained via reinforcement learning, systematically revert to their dominant pre-training
  language (usually English) during chain-of-thought generation, even when prompts
  are in other languages. This study trains Llama-3.2-3B and Qwen-2.5-1.5B with Group-Relative
  Policy Optimization on translated GSM8K and SimpleRL-Zoo datasets in Chinese, Korean,
  and Ukrainian, monitoring task accuracy and language consistency.
---

# Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2506.05850
- **Source URL**: https://arxiv.org/abs/2506.05850
- **Reference count**: 13
- **Key outcome**: Cross-lingual Collapse occurs when multilingual reasoning models, trained via reinforcement learning, systematically revert to their dominant pre-training language (usually English) during chain-of-thought generation, even when prompts are in other languages

## Executive Summary
Cross-lingual Collapse reveals a critical vulnerability in multilingual reasoning systems trained with reinforcement learning. When models trained primarily in English undergo GRPO fine-tuning on translated reasoning tasks, they systematically abandon their target language in favor of English during chain-of-thought reasoning, despite maintaining or improving task accuracy. This phenomenon affects low-resource languages most severely but extends to mid-resource languages when harder curriculum data is introduced. The study demonstrates that this language drift is largely irreversible through post-hoc fine-tuning, raising fundamental questions about the viability of current RL approaches for developing truly multilingual reasoning capabilities.

## Method Summary
The study trained Llama-3.2-3B and Qwen-2.5-1.5B models using Group-Relative Policy Optimization on translated GSM8K and SimpleRL-Zoo datasets in Chinese, Korean, and Ukrainian. Researchers monitored task accuracy and language consistency metrics throughout training, comparing standard GRPO against a language-consistency reward variant. Post-hoc recovery attempts were conducted using distilled Language Reasoning Models. The experimental design systematically varied dataset difficulty and language resource levels to map the boundaries of collapse across different training conditions.

## Key Results
- Ukrainian word ratio dropped from 98% to 0.3% during GRPO training while accuracy rose by +17.1%
- Language-consistency reward mitigated collapse but reduced accuracy gains by 5–10 percentage points
- Post-hoc fine-tuning on distilled LRMs failed to restore target-language reasoning, indicating bias is largely irreversible

## Why This Works (Mechanism)
Cross-lingual Collapse emerges from the interaction between pre-training language imbalances and reinforcement learning optimization dynamics. When models trained predominantly in English undergo GRPO fine-tuning, the reward structure inadvertently favors patterns learned during pre-training. The chain-of-thought generation process, being a sequential decision-making task, allows the model to "shortcut" by reverting to familiar English reasoning patterns that were reinforced during initial training. The GRPO algorithm, optimizing for task completion rather than language consistency, amplifies these pre-existing biases. As the model discovers that English reasoning patterns yield higher rewards regardless of prompt language, it increasingly defaults to these patterns, creating a self-reinforcing cycle that accelerates language drift.

## Foundational Learning
**Reinforcement Learning with Language Models** - Understanding how RL algorithms like GRPO optimize language model outputs through reward signals. *Why needed*: The entire collapse phenomenon stems from how RL training interacts with pre-existing language biases. *Quick check*: Can you explain how GRPO differs from standard supervised fine-tuning in terms of optimization objectives?

**Chain-of-Thought Reasoning** - The process by which models generate intermediate reasoning steps before producing final answers. *Why needed*: Collapse manifests specifically in the reasoning chain generation, not in final answer selection. *Quick check*: How does chain-of-thought differ from direct answer generation in terms of model behavior and training requirements?

**Language Resource Hierarchies** - The distribution of training data across different languages, typically heavily skewed toward high-resource languages like English. *Why needed*: The severity of collapse correlates directly with the resource level of the target language. *Quick check*: What factors determine whether a language is considered high, mid, or low resource in the context of foundation model training?

## Architecture Onboarding
**Component Map**: Pre-trained Foundation Model -> GRPO Fine-tuning Pipeline -> Reward Function -> Output Monitor -> Language Consistency Checker
**Critical Path**: Input Prompt → Language Detection → Chain-of-Thought Generation → Reward Calculation → Policy Update → Next Generation
**Design Tradeoffs**: The tension between maximizing task accuracy and maintaining language consistency, where optimizing for one typically degrades the other
**Failure Signatures**: Rapid decline in target language word ratios coupled with accuracy improvements, complete abandonment of target language in reasoning chains despite multilingual prompts
**First 3 Experiments**:
1. Baseline GRPO training on monolingual English dataset to establish performance ceiling
2. GRPO training on translated dataset with standard reward function to observe collapse
3. GRPO training with language-consistency reward to test mitigation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to reasoning tasks, leaving open whether collapse occurs similarly in other domains
- Focus on smaller models (3B parameters) raises questions about behavior in larger frontier models
- Single post-hoc recovery method tested for irreversibility claims
- Fixed language-consistency reward parameters without exploring optimal reward shaping strategies

## Confidence
- **High Confidence**: The empirical observation that GRPO training amplifies pre-training language imbalances and causes systematic language drift toward dominant languages
- **Medium Confidence**: The characterization of collapse as "largely irreversible" based on post-hoc fine-tuning attempts
- **Medium Confidence**: The claim that adding harder curriculum data induces collapse in mid-resource languages

## Next Checks
1. Test whether the language-consistency reward's accuracy trade-off can be optimized through hyperparameter tuning or alternative reward formulations
2. Replicate the collapse phenomenon on larger foundation models to determine if model scale mitigates or exacerbates the effect
3. Investigate whether alternative RL algorithms beyond GRPO exhibit the same cross-lingual collapse tendencies