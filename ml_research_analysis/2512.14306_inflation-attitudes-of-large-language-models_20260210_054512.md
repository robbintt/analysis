---
ver: rpa2
title: Inflation Attitudes of Large Language Models
arxiv_id: '2512.14306'
source_url: https://arxiv.org/abs/2512.14306
tags:
- inflation
- survey
- expectations
- information
- economic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of Large Language Models (LLMs)
  to form inflation perceptions and expectations based on macroeconomic price signals.
  Using a quasi-experimental design around GPT's 2021 training cut-off, the authors
  replicate the Bank of England's Inflation Attitudes Survey and find that GPT tracks
  aggregate survey projections and official statistics at short horizons.
---

# Inflation Attitudes of Large Language Models

## Quick Facts
- arXiv ID: 2512.14306
- Source URL: https://arxiv.org/abs/2512.14306
- Reference count: 11
- Primary result: GPT-3.5-turbo replicates aggregate inflation attitudes but lacks micro-level correspondence with human survey respondents

## Executive Summary
This study investigates whether Large Language Models can simulate human inflation perceptions and expectations using macroeconomic price signals. The authors conduct a quasi-experimental survey using GPT-3.5-turbo to replicate the Bank of England's Inflation Attitudes Survey, conditioning responses on demographic profiles and economic information. GPT successfully tracks aggregate survey projections and official statistics at short horizons while replicating key empirical regularities across demographic groups. A novel Shapley value decomposition reveals GPT's heightened sensitivity to food inflation information similar to human respondents, though the model lacks a consistent internal model of consumer price inflation.

## Method Summary
The researchers designed a synthetic survey replicating the Bank of England's Inflation Attitudes Survey methodology. They used GPT-3.5-turbo via OpenAI's API, prompting the model with demographic profiles and economic conditioning information about inflation subcomponents. The experiment employed temperature calibration across multiple settings to optimize response distribution alignment with human survey data. Responses were mapped to numeric scales and validated against 2022Q4 (cross-validation) and 2023Q1 (main test) IAS samples. A novel Shapley value decomposition was applied to quantify the marginal and interactive contributions of different inflation components to model outputs.

## Key Results
- GPT replicates aggregate inflation perception distributions matching IAS survey means and standard deviations
- Model demonstrates demographic sensitivity patterns aligning with human respondents for income, housing tenure, and social class
- Shapley decomposition reveals GPT's over-sensitivity to food inflation information (slope-to-weight ratio of 4.01) compared to other components
- GPT lacks consistent internal model of consumer price inflation, showing non-monotonic sensitivity and weak micro-level correlation with individual human responses

## Why This Works (Mechanism)

### Mechanism 1: Demographic-to-Attitude Mapping via Synthetic Personas
- Claim: Conditioning LLMs on demographic profiles generates response distributions that replicate meso-level empirical regularities in human survey data.
- Mechanism: The model encodes statistical associations between demographic characteristics and economic attitudes from training corpus; prompting with persona descriptions activates these latent patterns without explicit fine-tuning.
- Core assumption: Training data contains sufficient signal linking demographic profiles to inflation attitudes that survives model compression.
- Evidence anchors:
  - [abstract] "GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class"
  - [section 4.3.1] Table 5 shows GPT demographic regression coefficients aligning with ONS reference values for housing tenure (owner w/mortgage: -1.23 p.p. vs ONS -1.54) and income
  - [corpus] Weak direct support; related work on LLMs predicting human attitudes (arxiv 2503.21011) suggests similar capability but in different domains
- Break condition: Demographic groups underrepresented in training data; novel demographic intersections not seen during pre-training

### Mechanism 2: Economic Conditioning as Treatment Effect via Shapley Decomposition
- Claim: Discrete economic information treatments (inflation subcomponents) can be decomposed to quantify their marginal and interactive contributions to response outputs.
- Mechanism: Prompt engineering structures economic information as separable treatment bits; Shapley values (Eq. 3) enumerate all coalitions of treatment variables to isolate contribution of each component while accounting for interactions.
- Core assumption: LLM response function behaves consistently enough across treatment permutations that Shapley decomposition yields meaningful attribution.
- Evidence anchors:
  - [abstract] "A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs"
  - [section 4.4.2] Energy treatment Shapley value (2.79) exceeds naïve decomposition (1.74) by >50%, indicating "GPT's inflation perceptions are particularly elevated due to jointly high values of food and energy price inflation"
  - [corpus] No direct corpus support for this specific Shapley adaptation to LLM treatment settings
- Break condition: Non-linear response thresholds; inconsistent model behavior across permutation orders; treatment interactions that violate Shapley assumptions

### Mechanism 3: Human-like Salience Bias to Frequent Price Signals
- Claim: GPT exhibits overweighting of salient inflation components (food, energy) similar to documented human cognitive biases, despite no explicit training for this behavior.
- Mechanism: Training corpus reflects human discourse patterns where grocery/energy prices receive disproportionate attention; this propagates into model's implicit weighting of price signals.
- Core assumption: Linguistic salience in training text correlates with cognitive salience in human economic judgment.
- Evidence anchors:
  - [abstract] "GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents"
  - [section 4.4.1] Slope-to-weight ratio for food_rest = 4.01 (vs. 0.98 for "other" component), indicating 4x over-reaction relative to basket weight
  - [corpus] D'Acunto et al. (2021) cited in paper documents grocery price bias in human expectations
- Break condition: Contexts where salience patterns differ from training distribution; counterfactual scenarios with novel price component relationships

## Foundational Learning

- Concept: **Shapley Value Decomposition**
  - Why needed here: Core explainability tool for isolating treatment effects when multiple conditioning variables interact; required to move beyond naïve marginal analysis
  - Quick check question: Given 3 binary treatments, how many coalition subsets must be evaluated for one variable's Shapley value?

- Concept: **Temperature Parameter in LLM Sampling**
  - Why needed here: Controls trade-off between response diversity (matching human distribution variance) and consistency (micro-level correlation stability); central to calibration
  - Quick check question: At T→0, what happens to softmax output distribution? At T→∞?

- Concept: **Quasi-Experimental Design with Knowledge Cut-offs**
  - Why needed here: Enables causal inference about treatment effects by exploiting temporal boundaries in model knowledge; validates that responses derive from conditioning, not memorization
  - Quick check question: If GPT recalled Feb 2023 inflation without conditioning, what would this imply about the experimental validity?

## Architecture Onboarding

- Component map: System prompt (persona simulation) + User prompt (demographics + economic conditioning) → GPT-3.5-turbo API → Response extraction → Numeric mapping → Shapley decomposition → Three-way validation (GPT vs. IAS vs. ONS)
- Critical path: Temperature calibration (cross-validation sample) → Baseline response distribution → Treatment effect computation → Shapley decomposition → Three-way validation (GPT vs. IAS vs. ONS)
- Design tradeoffs:
  - High temperature (T=1.5): Better aggregate distribution match, but higher invalid response rate (13-102 missing values at longer horizons) and weaker micro-level correlations
  - Low temperature (T=0): Stronger cross-horizon correlations matching human patterns, but poorer mean/SD alignment with IAS
  - Conditioning detail: More economic information improves face validity but introduces interaction complexity requiring Shapley analysis
- Failure signatures:
  - Non-monotonic sensitivity: GPT fails to extrapolate monotonically for food & restaurants beyond historical range (Fig 8)
  - Kink-type non-linearities: Insensitivity to "other" component 0-2%, then linear scaling beyond (no theoretical justification)
  - Model time drift: Later GPT releases show ~3 p.p./year increase in unconditioned perceptions despite same stated cut-off (Fig 5), indicating RLHF information leakage
- First 3 experiments:
  1. **Knowledge cut-off validation**: Query model about events/inflation values post-Sep 2021 without conditioning; confirm inability to recall (see Appendix validation prompts)
  2. **Temperature sweep on cross-validation sample**: Generate responses at T ∈ {0, 0.5, 1.0, 1.5} for 2022Q4; compute loss function (Eq. 9) against IAS mean and SD
  3. **Single-component sensitivity analysis**: Vary one inflation component across [0, max observed + buffer] while holding others at baseline; plot response curve to identify linear range and break points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What additional conditioning information or prompting strategies would improve micro-level alignment between LLM responses and individual human survey responses?
- Basis in paper: [explicit] "The micro-level correspondence between LLM and human responses is rather weak and partially unstable. This could be caused by the rudimentary economic conditioning environment and leaves plenty of scope for future research." (p. 3); "Achieving better alignment on the micro level is an interesting research problem in itself" (p. 30)
- Why unresolved: The study used limited conditioning (demographics plus four inflation components), which was insufficient to capture idiosyncratic states of human respondents. Pearson correlations between GPT and IAS remained weak (0.03–0.07) across temperature settings.
- What evidence would resolve it: Experiments varying the richness of personalization (e.g., consumption basket details, recent purchase history, media exposure) showing statistically significant improvements in individual-level correlations.

### Open Question 2
- Question: Do the findings on inflation attitude simulation generalize across different LLMs, or does each model require separate validation?
- Basis in paper: [explicit] "Finally, most of our results where obtained for a single model without a guarantee that these generalise to other models or settings. Given the complexity and diversity of LLMs, it is therefore essential to validate a model whenever either the model or setting changes" (p. 31)
- Why unresolved: Only GPT-3.5-turbo was tested. Different architectures, training data, and fine-tuning approaches may produce different response patterns, biases, and sensitivities to conditioning.
- What evidence would resolve it: Replication of the experimental design across multiple LLMs (e.g., Claude, Llama, Gemini) with systematic comparison of demographic alignment, treatment sensitivity, and Shapley decomposition patterns.

### Open Question 3
- Question: Can LLMs be engineered or fine-tuned to develop consistent internal models of economic concepts such as consumer price inflation?
- Basis in paper: [explicit] "However, we also find that it lacks a consistent model of consumer price inflation" (Abstract); GPT showed non-monotonic sensitivity to food inflation and kink-type non-linearities "for no apparent reason" (p. 30)
- Why unresolved: GPT's sensitivity to inflation components exhibited puzzling behavior—insensitive to the "other" component below 2%, then extrapolating far beyond historical ranges. Responses did not sum consistently across Shapley decompositions.
- What evidence would resolve it: Testing whether domain-specific fine-tuning on economic data produces monotonic, consistent responses across input ranges and internally coherent decompositions.

### Open Question 4
- Question: When LLM outputs align with official statistics but not human survey responses (or vice versa), which benchmark should guide model use and validation?
- Basis in paper: [explicit] "The situation can arise where LLM outputs match either the human benchmark or official statistics, but not both... a decision on the usefulness of LLM outputs will most likely face trade-offs" (p. 3, 31)
- Why unresolved: GPT matched ONS out-turns for housing tenure demographics better than it matched IAS responses, raising normative questions about alignment targets.
- What evidence would resolve it: Framework development linking use-case objectives (policy simulation vs. human behavior modeling) to appropriate validation benchmarks, with empirical tests of downstream decision quality under each alignment strategy.

## Limitations

- Weak micro-level correspondence between GPT and human responses, with only 1 of 6 demographic correlations exceeding 0.1 in absolute value
- High temperature settings produce higher invalid response rates (13-102 missing values) while sacrificing micro-level correlation strength
- Shapley decomposition methodology lacks validation against known benchmarks and assumes consistent LLM response behavior across treatment permutations
- GPT exhibits non-monotonic sensitivity to inflation components and kink-type non-linearities without theoretical justification

## Confidence

**High Confidence**: Aggregate distribution matching (mean/SD alignment with IAS), validation of knowledge cut-off (GPT cannot recall post-2021 events), monotonic long-horizon correlation patterns.

**Medium Confidence**: Demographic coefficient replication at meso-level, Shapley decomposition attribution of treatment effects, comparison to official CPIH statistics.

**Low Confidence**: Micro-level correlation strength, individual-level response correspondence, extrapolation behavior beyond historical inflation ranges, causal interpretation of treatment effects.

## Next Checks

1. **Shapley Value Validation**: Apply the same decomposition framework to a synthetic benchmark where ground truth treatment contributions are known (e.g., linear model with controlled interactions) to verify that Shapley values correctly recover the underlying attribution structure before applying to LLM outputs.

2. **Cross-Model Generalization**: Repeat the experimental protocol with a different LLM architecture (e.g., Claude or Llama) to test whether the observed demographic patterns and treatment sensitivities are model-specific artifacts or more general phenomena in language models trained on economic discourse.

3. **Robustness to Corpus Variation**: Generate synthetic survey responses using training corpora with manipulated salience patterns (e.g., amplified food price discussion) to quantify how linguistic representation in training data propagates to implicit weighting of inflation components in model outputs.