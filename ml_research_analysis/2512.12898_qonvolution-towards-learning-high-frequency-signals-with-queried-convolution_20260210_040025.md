---
ver: rpa2
title: 'Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution'
arxiv_id: '2512.12898'
source_url: https://arxiv.org/abs/2512.12898
tags:
- image
- high-frequency
- queries
- neural
- psnr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning high-frequency signals
  in computer vision and graphics, where neural networks often struggle due to spectral
  bias or optimization difficulties. The proposed solution, Queried-Convolutions (Qonvolutions),
  is a simple yet powerful modification that uses the neighborhood properties of convolution
  to convolve a low-frequency signal with queries (such as coordinates) to enhance
  the learning of intricate high-frequency signals.
---

# Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution

## Quick Facts
- arXiv ID: 2512.12898
- Source URL: https://arxiv.org/abs/2512.12898
- Authors: Abhinav Kumar, Tristan Aumentado-Armstrong, Lazar Valkov, Gopal Sharma, Alex Levinshtein, Radek Grzeszczuk, Suren Kumar
- Reference count: 40
- Primary result: Introduces Qonvolutions to learn high-frequency signals by convolving low-frequency features with coordinate queries; achieves state-of-the-art NVS results on Tanks & Temples

## Executive Summary
Qonvolutions address the challenge of learning high-frequency details in neural networks by leveraging convolution's neighborhood structure. The method combines low-frequency signal features with coordinate queries through concatenation before convolution, enabling the network to learn high-frequency residuals effectively. Evaluated across 1D regression, 2D super-resolution, 2D residual image regression, and novel view synthesis, Qonvolutions consistently improve reconstruction quality. Notably, when integrated with Gaussian splatting for NVS, the approach achieves state-of-the-art performance on complex real-world scenes, outperforming models like Zip-NeRF in image quality while training faster.

## Method Summary
Qonvolutions replace standard linear layers with convolutional layers that process concatenated low-frequency signal features and coordinate queries. The architecture typically uses 4 convolutional layers with 3x3 kernels and 64 channels, processing inputs formed by concatenating low-frequency approximations (e.g., 3DGS splatted images or downsampled images) with 2D coordinate grids. The network is trained with AdamW (lr=1e-4), and for NVS tasks, the QNN is added to the baseline and trained only during the final 25% of iterations to avoid destabilizing the low-frequency backbone.

## Key Results
- Achieves state-of-the-art NVS performance on Tanks & Temples when combined with Gaussian splatting
- Outperforms Zip-NeRF in image quality while training faster
- Demonstrates consistent improvements across 1D regression, 2D super-resolution, and 2D residual image tasks
- SIREN activations cause severe performance collapse (PSNR drops to ~14.5), validating the ReLU design choice

## Why This Works (Mechanism)
Qonvolutions exploit the spatial correlation structure inherent in convolution operations to better capture high-frequency details. By concatenating coordinate queries with low-frequency features before convolution, the network can learn how spatial position modulates the high-frequency residual. This approach addresses spectral bias by explicitly encoding positional information into the convolution's receptive field, allowing the network to learn position-dependent high-frequency patterns more effectively than standard MLPs.

## Foundational Learning
- **Convolutional Neural Networks**: Understanding how spatial locality and parameter sharing enable efficient feature extraction from images
  - *Why needed*: Qonvolutions are fundamentally convolutional architectures that process spatial information
  - *Quick check*: Can explain how convolution kernels detect edges, textures, and patterns
- **Spectral Bias in Neural Networks**: Recognizing that standard networks learn low-frequency functions first and struggle with high-frequency details
  - *Why needed*: Qonvolutions specifically address this limitation in learning high-frequency signals
  - *Quick check*: Can describe why neural networks prefer smooth, low-frequency solutions
- **Coordinate-Based Neural Representations**: Familiarity with using spatial coordinates as network inputs for tasks like NeRF and image regression
  - *Why needed*: Qonvolutions build on this concept by incorporating coordinates into convolutional processing
  - *Quick check*: Can explain how positional encoding helps neural networks learn spatial patterns

## Architecture Onboarding

**Component map**: Low-frequency features + Coordinate queries → Concatenation → Convolutional layers → High-frequency residuals → Output

**Critical path**: Input preparation (low-frequency signal + coordinate grid) → QNN forward pass (concatenate + convolve) → Add residuals to low-frequency output → Compute reconstruction loss

**Design tradeoffs**: Uses convolution for spatial coherence vs. MLPs' flexibility; requires coordinate normalization; trades parameter efficiency for better high-frequency learning

**Failure signatures**: 
- Performance collapse with SIREN activations (PSNR ~14.5)
- No improvement if coordinates aren't properly normalized or concatenated
- Instability if QNN trains from the start rather than after backbone convergence

**First experiments**:
1. Implement Qonvolutions on 1D signal regression (e.g., f(x) = sin(30x)) to verify basic functionality
2. Apply to 2D image super-resolution using a simple LR-HR image pair to test spatial processing
3. Integrate with 3DGS for NVS on a synthetic scene to validate the full pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Missing synthetic data generation details for 1D/2D experiments, hindering reproducibility
- Limited comparison to recent high-frequency MLPs (Fourier Features, MLA) in non-NVS tasks
- Evaluation restricted to Tanks & Temples for NVS, without testing on other complex scene datasets

## Confidence

**High confidence**: The core Qonvolution architecture (coordinate+feature concatenation → convolution) is clearly described and validated across multiple tasks

**Medium confidence**: The empirical improvements on NVS tasks are demonstrated, but comparisons are limited to specific baselines (Gaussian splatting, Zip-NeRF)

**Low confidence**: Reproducibility of 1D/2D experiments is hindered by missing synthetic data generation details and unclear "fair comparison" setups

## Next Checks

1. Implement and validate the Qonvolution architecture on a simple 1D signal regression task (e.g., f(x) = sin(30x)) to confirm the method's effectiveness in isolation

2. Conduct a direct ablation study comparing Qonvolutions to Fourier Features Networks and SIREN on a standard 2D image super-resolution benchmark (e.g., DIV2K)

3. Recreate the NVS experiment pipeline on a different complex scene dataset (e.g., DTU or BlendedMVS) to test generalizability beyond Tanks & Temples