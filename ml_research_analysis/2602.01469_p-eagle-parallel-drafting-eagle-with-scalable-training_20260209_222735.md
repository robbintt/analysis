---
ver: rpa2
title: 'P-EAGLE: Parallel-Drafting EAGLE with Scalable Training'
arxiv_id: '2602.01469'
source_url: https://arxiv.org/abs/2602.01469
tags:
- training
- p-eagle
- positions
- depth
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: P-EAGLE addresses the scalability challenge of training parallel-drafting
  speculative decoding models for long-context inference. It transforms EAGLE from
  autoregressive to parallel multi-token prediction using a learnable shared hidden
  state, eliminating the need for position-specific hidden states.
---

# P-EAGLE: Parallel-Drafting EAGLE with Scalable Training

## Quick Facts
- arXiv ID: 2602.01469
- Source URL: https://arxiv.org/abs/2602.01469
- Reference count: 24
- Primary result: Achieves 1.10×–1.36× speedup over autoregressive EAGLE-3 across GPT-OSS 120B, GPT-OSS 20B, and Qwen3-Coder 30B while maintaining comparable draft quality

## Executive Summary
P-EAGLE addresses the scalability challenge of training parallel-drafting speculative decoding models for long-context inference. It transforms EAGLE from autoregressive to parallel multi-token prediction using a learnable shared hidden state, eliminating the need for position-specific hidden states. The key innovation is a scalable training framework featuring amortized mask construction and sequence partitioning techniques, enabling gradient accumulation within individual sequences while preserving attention dependencies. The approach solves the quadratic attention scaling problem that limits existing parallel drafting methods.

## Method Summary
P-EAGLE transforms EAGLE into a parallel multi-token predictor through a learnable shared hidden state and mask token embedding. The training framework features amortized attention mask pre-computation (pre-compute once at max length, slice per-example in O(1) time) and sequence partitioning with dependency-aware assignment (Algorithm 1 ensures positions at depth d inherit the same segment as their dependency at depth d-1). Implemented in vLLM, P-EAGLE achieves significant speedups across GPT-OSS 120B, GPT-OSS 20B, and Qwen3-Coder 30B models.

## Key Results
- Achieves 1.10×–1.36× speedup over autoregressive EAGLE-3 across tested models
- Amortized mask construction reduces data loading time from 700s to 17s per 128 examples at 2048 tokens
- Sequence partitioning reduces peak attention memory from O(L²) to O(L²/S²) with S segments
- Maintains comparable draft quality while enabling efficient long-context training

## Why This Works (Mechanism)

### Mechanism 1: Shared Hidden State with Mask Token Embedding
- Claim: A single learnable shared hidden state enables parallel multi-token prediction without position-specific states.
- Mechanism: Multi-Token Prediction positions lack preceding tokens and hidden states. P-EAGLE substitutes both with: (1) a learnable `h_shared` vector, and (2) a mask token embedding. The attention mechanism + RoPE already encode positional information, making explicit depth encodings redundant.
- Core assumption: RoPE-based attention uniquely encodes absolute position, allowing the model to infer prediction depth from attention patterns alone.
- Evidence anchors:
  - [abstract] "transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state"
  - [Section 4.1, Table 3] All four position-aware alternatives underperformed the simple shared approach by 7–15%
  - [Appendix B, Theorem B.3] Proves attention score function is injective in relative position for almost all query-key pairs

### Mechanism 2: Amortized Attention Mask Pre-computation
- Claim: Pre-computing one maximum-length attention mask and slicing per example eliminates quadratic per-batch mask construction overhead.
- Mechanism: Causal attention structure across prediction depths is position-invariant—the mask for any sequence length is the top-left submatrix of a longer mask. Pre-compute once at training initialization; retrieve via constant-time tensor slicing.
- Core assumption: Training sequences never exceed the pre-computed maximum length.
- Evidence anchors:
  - [abstract] "featuring attention mask pre-computation and sequence partitioning techniques"
  - [Section 3.1, Table 2] At 2048 tokens, PARD's per-example mask construction causes 48× data loading slowdown vs. 17.5s with amortized approach
  - [corpus] PARD (arXiv:2504.18583) uses per-batch mask construction, which becomes prohibitive beyond 4K contexts

### Mechanism 3: Sequence Partitioning with Dependency-Aware Assignment
- Claim: Partitioning sequences into segments while tracking cross-depth dependencies enables within-sequence gradient accumulation.
- Mechanism: COD sampling creates different position sets per prediction depth. Algorithm 1 assigns positions to segments iteratively: depth d positions inherit segment from their dependency at depth d−1. Each segment includes cumulative depth-0 positions for causal attention.
- Core assumption: Dependencies flow only from position p at depth d−1 to position p at depth d (local dependency structure).
- Evidence anchors:
  - [Section 3.2] "With S segments, peak attention memory reduces from O(L²) to O(L²/S²)"
  - [Figure 4, Algorithm 1] Explicit pseudocode for dependency-preserving partitioning
  - [corpus] No direct corpus comparison; this is a novel technique not explored in prior parallel drafting work

## Foundational Learning

- Concept: Speculative Decoding
  - Why needed here: P-EAGLE is a drafter; understanding draft-then-verify paradigm is prerequisite.
  - Quick check question: Can you explain why verification accepts/rejects tokens without quality loss?

- Concept: Rotary Position Embeddings (RoPE)
  - Why needed here: Justification for shared hidden state relies on RoPE encoding position via attention.
  - Quick check question: Given query q and key k, how does RoPE encode relative position δ in the attention score?

- Concept: Gradient Accumulation
  - Why needed here: Sequence partitioning extends gradient accumulation to within-sequence segments.
  - Quick check question: Standard gradient accumulation splits batches—what's different about within-sequence accumulation?

## Architecture Onboarding

- Component map:
  - Target Model (frozen) -> Extract h_prompt and h_context from layers 2, L/2, L−1
  - Input Construction -> Concatenates token embedding with projected hidden state (2d per position)
  - P-EAGLE Drafter -> N transformer layers (recommended: 4) with RoPE
  - LM Head -> Borrowed from target model
  - Training Framework -> Amortized mask buffer + sequence partitioning logic

- Critical path:
  1. Target model forward → extract hidden states at specified layers
  2. Construct input: NTP position gets actual token + target hidden state; MTP positions get mask token + `h_shared`
  3. Drafter forward (single pass for all K predictions)
  4. LM head → logits for all positions
  5. Loss: NTP cross-entropy at depth 0; MTP cross-entropy at depths 1–K−1 with COD sampling

- Design tradeoffs:
  - **Layer count**: 2 layers = lower latency but −12% avg acceptance; 4 layers = higher acceptance but more compute
  - **K_train vs K_infer**: Train at K=8, infer at K=5 yields +4% over matched settings
  - **Frozen vs trainable embeddings**: Frozen loses mask token semantics; trainable adds +5%

- Failure signatures:
  - OOM at 8K+ context → mask not pre-computed or partitioning disabled
  - 48× data loading slowdown → per-example mask construction active (not amortized)
  - Low acceptance on long sequences → trained on short contexts (distribution mismatch)
  - MTP accuracy degrades → embeddings frozen or insufficient training duration

- First 3 experiments:
  1. **Baseline replication**: Train 1-layer P-EAGLE with frozen embeddings at K=5, measure acceptance length on MT-Bench. Expect ~2.4; validates setup.
  2. **Capacity ablation**: Compare 1-layer vs 2-layer vs 4-layer on HumanEval. Expect +33% from 1→2 layers, +9% from 2→4.
  3. **Mask overhead measurement**: Profile data loading time with PARD-style per-batch masks vs. amortized masks at 2048 tokens. Expect 48× slowdown.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can P-EAGLE's training framework be combined with KV-cache optimization techniques (quantization, attention sinks) to further extend context length support?
- Basis in paper: [explicit] Related work section states these techniques "are orthogonal to our approach and could be combined to further increase context lengths."
- Why unresolved: P-EAGLE focuses on parallel drafting efficiency; integration with memory-reduction techniques was not explored.
- What evidence would resolve it: Experiments combining P-EAGLE with KV-cache quantization or streaming attention, measuring both acceptance length and memory footprint at 32K+ contexts.

### Open Question 2
- Question: What is the optimal speculation depth K for different model sizes and concurrency levels?
- Basis in paper: [inferred] Table 10 shows speedups vary non-monotonically with K: P-EAGLE underperforms at K=3 for Qwen 30B but excels at K=5–7. The paper attributes this to "deeper architecture" overhead but provides no systematic analysis.
- Why unresolved: The interaction between drafter depth, speculation depth, and batch size creates tradeoffs not fully characterized.
- What evidence would resolve it: Ablation study varying K across model sizes with controlled drafter depths, measuring both drafting latency and acceptance length.

### Open Question 3
- Question: How does P-EAGLE scale to MoE architectures with expert routing overhead?
- Basis in paper: [explicit] Section 5.3 notes "for MoE models, expert routing overhead scales with batch size, shifting the bottleneck from drafting to verification."
- Why unresolved: Only dense models (GPT-OSS) were evaluated; the MoE routing bottleneck was identified but not addressed.
- What evidence would resolve it: Evaluation on MoE models (e.g., Mixtral, DeepSeek-MoE) with profiling of verification vs. drafting latency at varying concurrency levels.

## Limitations
- Generalization uncertainty across diverse target model architectures using non-RoPE positional encodings
- Scalability claims assume memory constraints dominate over compute, which may not hold for smaller models
- End-to-end speedups are sensitive to vLLM implementation details, batch sizes, and hardware configurations

## Confidence
- **High Confidence (90%+)**: Amortized mask construction and sequence partitioning are algorithmically sound with measurable improvements (48× speedup, O(L²/S²) scaling)
- **Medium Confidence (70-80%)**: Shared hidden state effectiveness depends on RoPE properties; ablation data supports approach but theoretical justification requires specific attention score properties
- **Low Confidence (50-60%)**: End-to-end speedups combine multiple factors and are sensitive to implementation details across different serving frameworks and GPU architectures

## Next Checks
1. **Architectural Transfer Test**: Implement P-EAGLE with a target model using learned positional embeddings (e.g., Llama-style) rather than RoPE. Measure whether shared hidden state performance degrades compared to position-aware alternatives.

2. **Memory-Compute Tradeoff Analysis**: Profile P-EAGLE across a range of sequence lengths (1K, 4K, 16K, 64K) on both H200 and A100 GPUs. Quantify the break-even point where sequence partitioning overhead exceeds memory savings.

3. **Alternative Position Encoding Validation**: Replace RoPE with relative positional bias (FlashAttention-2 style) in the drafter while maintaining shared hidden state. Test whether attention still uniquely encodes position for the shared state approach.