---
ver: rpa2
title: Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power
  System Optimizers
arxiv_id: '2502.05727'
source_url: https://arxiv.org/abs/2502.05727
tags:
- data
- poisoning
- power
- attacks
- feasibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the impact of data poisoning attacks on
  machine learning-based optimization proxies for solving the DC Optimal Power Flow
  (DC-OPF) problem in power systems. Three optimization proxy methods were tested:
  a penalty-based method, the DC3 method (a post-repair approach), and the LOOP-LC
  method (a direct mapping approach).'
---

# Impact of Data Poisoning Attacks on Feasibility and Optimality of Neural Power System Optimizers

## Quick Facts
- **arXiv ID**: 2502.05727
- **Source URL**: https://arxiv.org/abs/2502.05727
- **Reference count**: 22
- **Primary result**: Data poisoning attacks significantly degrade both feasibility and optimality of neural optimization proxies for DC-OPF problems

## Executive Summary
This paper investigates the vulnerability of machine learning-based optimization proxies to data poisoning attacks in power system applications. Three optimization proxy methods - penalty-based, DC3 (post-repair), and LOOP-LC (direct mapping) - were tested against a white-box attack that maximizes loss by perturbing input data within bounded limits. The attack successfully degraded both feasibility and optimality across all methods, with the LOOP-LC method showing the best resilience by maintaining feasibility but experiencing a threefold increase in optimality error. The penalty and DC3 methods suffered severe feasibility violations alongside large optimality errors. These findings demonstrate that current neural optimization proxies are vulnerable to adversarial attacks and require robust defense mechanisms, particularly hard feasibility enforcement.

## Method Summary
The study employs a white-box data poisoning attack framework where an attacker perturbs input data to maximize loss within bounded constraints. Three neural optimization proxy methods are evaluated: a penalty-based method that incorporates constraint violations into the loss function, the DC3 method which applies post-repair corrections to infeasible solutions, and the LOOP-LC method that directly maps inputs to feasible solutions. The attack is designed to find worst-case input perturbations that degrade both feasibility (constraint violations) and optimality (objective function value deviation from optimal). Performance is measured across multiple IEEE test cases for the DC Optimal Power Flow problem.

## Key Results
- All three optimization proxy methods experienced significant degradation in both feasibility and optimality under data poisoning attacks
- LOOP-LC method maintained feasibility but showed a threefold increase in optimality error
- Penalty and DC3 methods suffered severe feasibility violations alongside large optimality errors
- The attack successfully maximized loss by finding worst-case input perturbations within bounded constraints

## Why This Works (Mechanism)
The attack works by exploiting the sensitivity of neural optimization proxies to input perturbations. In a white-box setting, the attacker has knowledge of the model architecture and parameters, allowing them to craft adversarial examples that maximize the loss function. The bounded perturbation constraint ensures the attack remains realistic while still being effective. The vulnerability stems from the fact that neural networks can be sensitive to small input changes, which can lead to large deviations in both feasibility (constraint satisfaction) and optimality (objective value) when applied to optimization problems.

## Foundational Learning
- **DC Optimal Power Flow (DC-OPF)**: Simplified power flow problem focusing on active power dispatch - needed for understanding the optimization context and constraints
- **Data Poisoning Attacks**: Adversarial attacks where training or input data is manipulated to degrade model performance - needed to understand the threat model
- **Neural Optimization Proxies**: Machine learning models that approximate solutions to optimization problems - needed to understand the target systems being attacked
- **Feasibility vs Optimality**: Feasibility refers to constraint satisfaction while optimality refers to objective function value - needed to evaluate attack impact
- **White-box vs Black-box Attacks**: White-box attacks have full knowledge of the target model while black-box attacks do not - needed to understand attack capabilities
- **Bounded Perturbations**: Attack constraints that limit the magnitude of input modifications - needed to assess attack realism

## Architecture Onboarding

**Component Map**: Input Data -> Neural Network -> Solution Output -> Feasibility Check -> Optimality Evaluation

**Critical Path**: Data Poisoning Attack -> Input Perturbation -> Neural Network Forward Pass -> Solution Generation -> Feasibility and Optimality Assessment

**Design Tradeoffs**: The study balances attack effectiveness against realism through bounded perturbations, but this may underestimate real-world attack capabilities. The choice of DC-OPF over AC-OPF simplifies the problem but reduces practical applicability.

**Failure Signatures**: Large feasibility violations indicate severe constraint violations, while optimality error increases show degraded solution quality. The LOOP-LC method shows resilience to feasibility violations but remains vulnerable to optimality degradation.

**First Experiments**: 1) Test attack transferability to other power system topologies, 2) Evaluate gray-box attack performance with limited model knowledge, 3) Validate findings on actual operational power system data

## Open Questions the Paper Calls Out
None

## Limitations
- Attack scenario assumes white-box threat model with bounded perturbations, potentially underestimating real-world attack capabilities
- Focus on DC-OPF rather than AC-OPF limits practical applicability to real power system optimization
- Results based on synthetic test cases without validation on actual power system data or under operational conditions

## Confidence
- **Feasibility degradation under attack**: High confidence - Multiple methods consistently showed feasibility violations with clear quantitative metrics
- **LOOP-LC method resilience**: Medium confidence - Maintained feasibility better but still experienced significant optimality degradation
- **Need for robust defenses**: High confidence - Systematic degradation across all methods provides strong evidence for improved resilience mechanisms

## Next Checks
1. Test attack transferability across different power system topologies and scales beyond the IEEE test cases used
2. Evaluate performance under gray-box and black-box attack scenarios where attacker knowledge is limited
3. Assess real-world impact by validating findings with operational power system data and constraints