---
ver: rpa2
title: 'Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM
  Inference: Algorithms and Numerical Analysis'
arxiv_id: '2503.01873'
source_url: https://arxiv.org/abs/2503.01873
tags:
- attention
- pasa
- matrix
- precision
- overflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the numerical instability of low-precision
  attention computation in large language models, specifically focusing on overflow
  issues in half-precision (FP16) calculations. The authors propose PASA (Pseudo-average
  Shifting Attention), an algorithm that introduces online pseudo-average shifting
  and global recovering techniques to eliminate overflow origins while maintaining
  computational efficiency.
---

# Online Pseudo-average Shifting Attention(PASA) for Robust Low-precision LLM Inference: Algorithms and Numerical Analysis

## Quick Facts
- **arXiv ID:** 2503.01873
- **Source URL:** https://arxiv.org/abs/2503.01873
- **Reference count:** 40
- **Key outcome:** PASA eliminates FP16 overflow in attention computation while maintaining accuracy comparable to FP32 baseline

## Executive Summary
This paper addresses the critical numerical overflow issues that arise when computing attention mechanisms in large language models using half-precision (FP16) arithmetic, particularly for long sequences. The authors identify two primary sources of overflow: large bias in the sequence dimension and resonance between query and key matrices in the head dimension. To solve this, they propose PASA (Pseudo-average Shifting Attention), an algorithm that introduces online pseudo-average shifting and global recovering techniques to eliminate overflow origins while maintaining computational efficiency. The method enables full FP16 computation throughout the attention process by shifting local block biases close to zero and recovering global bias information online.

## Method Summary
PASA introduces a shifting matrix M = I/α - βJ/(αs₂) where α=√d and β is optimized for the target hardware (recommended 0.984497). The algorithm operates in four main steps: (1) preprocessing the key matrix K to K' = KM using batched GEMM, (2) computing the attention matrix S' = QK' in FP16, (3) online tracking of pseudo-averages F_j for each block, and (4) applying global average recovery with correction terms Δm and softmax normalization. This approach shifts local block biases close to zero to prevent overflow while maintaining the global average information through online recovery, allowing the entire attention computation to be performed in FP16 without sacrificing accuracy.

## Key Results
- PASA successfully eliminates overflow in FP16 attention computation for both synthetic benchmarks and real models (Qwen2-7B and Stable-Video-Diffusion)
- RMSE remains below 1e-4 compared to FP32 baseline across all tested configurations
- No overflow instances detected in PASA implementations while standard FP16 Flash Attention fails on inputs with bias ≥30 or amplitude ≥15
- Visual quality of generated outputs matches FP32 baseline for both language and video generation tasks

## Why This Works (Mechanism)
The overflow in standard FP16 attention occurs when attention score matrices exceed FP16's maximum value (65504), primarily due to large bias in the sequence dimension and resonance between query and key matrices in the head dimension. PASA works by introducing a shifting matrix that moves local block biases close to zero, preventing overflow at the source. The global average recovery mechanism ensures that the overall attention distribution remains accurate by tracking and correcting for the pseudo-averages of each block online. This combination of local shifting and global recovery allows the entire attention computation to be performed in FP16 without losing critical information.

## Foundational Learning
- **Overflow threshold**: FP16 can represent values up to 65504; overflow occurs when attention scores exceed this limit
  - *Why needed*: Understanding the numerical limits that cause computation failure
  - *Quick check*: Verify attention scores stay below 65504 in FP16 implementation
- **Pseudo-average tracking**: Online computation of block averages F_j to maintain global information
  - *Why needed*: Preserves global attention distribution while preventing local overflow
  - *Quick check*: Confirm F_j values track correctly across block boundaries
- **Resonance mechanism**: Phase coincidence or 180-degree shift between Q and K matrices amplifying overflow risk
  - *Why needed*: Identifies specific conditions that make overflow more likely
  - *Quick check*: Measure correlation between Q and K matrix phases in overflow cases
- **Fixed-point iteration for β**: Iterative solver to find optimal β ensuring zero invariance error
  - *Why needed*: Guarantees numerical stability of the shifting operation
  - *Quick check*: Verify convergence of Eq. 22 solver for different input distributions
- **Block partitioning**: Division of attention computation into manageable blocks (s₁, s₂ typical 128-256)
  - *Why needed*: Enables localized bias shifting while maintaining computational efficiency
  - *Quick check*: Confirm block sizes prevent overflow while minimizing overhead
- **Correction terms**: Δm and Δm' adjustments for accurate softmax computation
  - *Why needed*: Restores correct attention probabilities after shifting operations
  - *Quick check*: Verify softmax outputs match expected distributions after correction

## Architecture Onboarding

**Component map**: Input → K preprocessing (KM) → Attention matrix (QK') → Online pseudo-average tracking → Global recovery → Output

**Critical path**: The attention computation path is critical - any overflow or numerical error here directly impacts model output quality. The online pseudo-average tracking and global recovery steps must operate in real-time to maintain performance.

**Design tradeoffs**: PASA trades additional computational overhead for numerical stability. The shifting operations add FLOPs but eliminate the need for higher-precision fallback modes. The algorithm prioritizes overflow prevention over computational efficiency, though it claims to maintain FP16 speed.

**Failure signatures**: Standard overflow manifests as INF or NAN values in attention outputs. PASA failure would show as degraded RMSE or visual artifacts in generated content, indicating incorrect pseudo-average tracking or global recovery.

**First experiments**:
1. Implement shifting matrix construction (Eq. 10) and β optimization solver on uniform random data with x₀=30, Aₘ=0.5 to verify overflow elimination
2. Run full Algorithm 1 on hybrid random data with varying Aₘ and compare RMSE against FP32 Flash Attention baseline
3. Validate on real models using LongBench prompts for Qwen2-7B and HuggingFace SVD-IMG2VID pipeline, checking for overflow and output quality

## Open Questions the Paper Calls Out
### Open Question 1
Is the "resonance mechanism" (phase coincidence or 180-degree shift between query and key matrices) a universal phenomenon causing overflow across diverse Large Language Models, or is it specific to the tested architectures? The current study validates the mechanism only on Qwen2-7B and Stable-Video-Diffusion models. A broad statistical analysis of attention patterns and overflow instances across a wider variety of open-source LLMs would resolve this.

### Open Question 2
Can an adaptive mechanism be designed to trigger PASA only when overflow is imminent, rather than applying it universally? Since overflow does not always appear, a low-cost heuristic or detection method for large bias/amplitude that predicts overflow could provide performance improvements from the adaptive approach.

### Open Question 3
What are the specific performance (latency/throughput) improvements of PASA on different hardware architectures, particularly GPUs versus NPUs? The paper focuses on numerical accuracy and algorithm validation in eager mode, lacking concrete wall-clock time benchmarks for optimized kernels on target hardware.

### Open Question 4
Can the PASA algorithm be effectively combined with FP8 or int8 quantization methods to address both overflow and underflow issues? It is unclear if PASA's shifting logic is compatible with the significantly reduced dynamic range of FP8 without introducing new errors.

## Limitations
- Implementation details remain unspecified, including exact block sizes s₁ and s₂ used in experiments
- Hardware-specific optimizations for NPU CUBE and GPU Tensor Cores lack detailed specifications
- β optimization requires solving a fixed-point equation that may not always converge numerically
- Evaluation focuses on specific models (Qwen2-7B and Stable-Video-Diffusion) without broader testing across different LLM architectures

## Confidence
- **High confidence**: PASA algorithm design and theoretical foundations, identification of overflow sources, basic overflow elimination capability on synthetic benchmarks
- **Medium confidence**: Claims about negligible accuracy degradation (RMSE < 1e-4) and computational efficiency parity with standard FP16 attention
- **Low confidence**: Claims about hardware-specific kernel optimizations without implementation details, generalization across diverse LLM architectures beyond the two tested models

## Next Checks
1. Implement β optimization solver and verify numerical convergence across different input distributions, particularly testing edge cases where fixed-point iteration might fail
2. Test PASA's overflow elimination capability on a third distinct model architecture (e.g., Llama, Mistral) to validate generalizability beyond Qwen2-7B and SVD-IMG2VID
3. Benchmark actual computational overhead of PASA against standard FP16 attention on target hardware (NPU/GPU) to verify claimed efficiency parity