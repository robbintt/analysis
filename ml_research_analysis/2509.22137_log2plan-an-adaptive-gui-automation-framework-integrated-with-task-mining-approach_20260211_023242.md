---
ver: rpa2
title: 'Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining
  Approach'
arxiv_id: '2509.22137'
source_url: https://arxiv.org/abs/2509.22137
tags:
- task
- user
- log2plan
- automation
- mining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Log2Plan addresses the brittleness and latency issues of existing
  GUI automation agents by integrating a structured two-level planning framework with
  task mining from user behavior logs. It mines repetitive GUI interaction patterns
  into reusable task groups and retrieves semantically similar past workflows to generate
  robust high-level plans, which are then grounded into low-level actions via a lightweight
  LocalPlanner.
---

# Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach

## Quick Facts
- arXiv ID: 2509.22137
- Source URL: https://arxiv.org/abs/2509.22137
- Reference count: 40
- 80% success rate on 200 real-world desktop automation tasks, outperforming baselines UFO (46.5%) and ReAct (18.0%)

## Executive Summary
Log2Plan addresses the brittleness and latency issues of existing GUI automation agents by integrating a structured two-level planning framework with task mining from user behavior logs. It mines repetitive GUI interaction patterns into reusable task groups and retrieves semantically similar past workflows to generate robust high-level plans, which are then grounded into low-level actions via a lightweight LocalPlanner. Evaluated on 200 real-world desktop automation tasks, Log2Plan achieves an 80.0% success rate and maintains over 60% success on long-horizon sequences, outperforming baselines like UFO (46.5%) and ReAct (18.0%). Its task retrieval and hierarchical planning architecture enable scalable, adaptable, and efficient automation across diverse GUI environments.

## Method Summary
Log2Plan combines task mining from user logs with hierarchical planning to automate GUI tasks. The system pre-processes raw interaction logs into 19 high-level events, uses GPT-4o to label these into hierarchical task groups, and embeds them for retrieval. At runtime, it retrieves semantically similar past workflows for a user command, uses a GlobalPlanner to generate high-level plans, and employs a LocalPlanner to ground these into low-level actions based on the current GUI state. The framework was evaluated on 200 tasks across various applications, achieving 80.0% success rate.

## Key Results
- 80.0% success rate on 200 real-world desktop automation tasks
- Maintains over 60% success on long-horizon sequences (exceeding 30 low-level actions)
- Outperforms baselines: UFO (46.5%) and ReAct (18.0%)

## Why This Works (Mechanism)

### Mechanism 1: Log-Derived Structured Task Mining
Converting raw interaction logs into a hierarchical "Task Dictionary" creates reusable planning primitives that reduce the LLM's search space. The system segments raw mouse/keyboard events into 19 predefined high-level GUI events, then clusters these into Task Groups labeled with Environment and Action metadata. This transforms variable user behaviors into structured embeddings. Core assumption: User intent can be reliably reverse-engineered from interaction patterns, and past behaviors are predictive of future automation needs.

### Mechanism 2: Retrieval-Augmented Global Planning
Retrieving semantically similar past workflows allows the LLM to decompose complex user commands into high-level plans without fine-tuning, effectively bypassing the "brittle generalization" of zero-shot prompting. When a user issues a command, the system embeds the query and retrieves the top-k relevant Task Groups via cosine similarity. The GlobalPlanner uses these retrieved demonstrations to construct a step-by-step plan, grounding the reasoning in actual historical success rather than hallucinated logic. Core assumption: The semantic similarity between a natural language command and past log descriptions correlates strongly with the procedural similarity of the required GUI actions.

### Mechanism 3: Context-Grounded Local Execution
Decoupling high-level planning from low-level execution allows a lightweight LocalPlanner to adapt abstract plans to specific GUI states, solving the "latency" and "brittleness" issues of monolithic vision-based agents. The LocalPlanner receives a high-level task and inspects the current GUI state via PyWinAuto (a component dictionary). It maps the task to a specific "Task Block" (low-level event sequence) by matching object names and types, effectively grounding the abstract plan in real-time UI elements. Core assumption: The GUI elements are inspectable via accessibility APIs (text labels/control types) and do not rely solely on pixel-perfect coordinates.

## Foundational Learning

- **Concept: Hierarchical Task Planning**
  - **Why needed here:** Log2Plan explicitly separates intent (GlobalPlanner) from execution (LocalPlanner). Understanding how to decompose a goal into a DAG of sub-tasks is essential to debugging why a plan might fail at specific horizons.
  - **Quick check question:** Can you explain the difference between a "Task Group" (semantic abstraction) and a "Task Block" (executable sequence)?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The system does not rely on the LLM's internal weights for procedural knowledge. Instead, it relies on vector similarity search over logs. Understanding embedding spaces is critical for optimizing the "Semantic Retrieval" module.
  - **Quick check question:** How does the diversity selection strategy (excluding similar candidates to force diversity) improve the GlobalPlanner's output?

- **Concept: GUI Accessibility Trees vs. Pixel-Based Vision**
  - **Why needed here:** Log2Plan uses PyWinAuto to read the GUI structure rather than just looking at screenshots. This distinction determines the system's limitations (e.g., it struggles with unlabeled visual elements).
  - **Quick check question:** Why would a text-based component dictionary be faster but potentially more brittle than a pure Vision-Language Model (VLM) approach for UI interaction?

## Architecture Onboarding

- **Component map:** User Command + User Logs -> Log Pre-processing -> Pattern Matching -> Hierarchical Labeling (GPT-4o) -> Task Dictionary (Vector Store) -> Retrieval -> GlobalPlanner -> LocalPlanner -> Executor
- **Critical path:** The Retrieval-to-Planning interface. If the vector store returns irrelevant "Task Groups," the GlobalPlanner will construct a plan based on wrong precedents. The quality of the "Task Dictionary" embeddings is the primary determinant of system performance.
- **Design tradeoffs:**
  - **Structure vs. Flexibility:** The system relies on 19 predefined high-level events, constraining the action space (reducing hallucinations) but limiting expressiveness for novel interaction types.
  - **Text vs. Vision:** The LocalPlanner uses accessibility APIs (text/structure) rather than raw pixels. This reduces latency significantly compared to VLMs but fails on purely visual UI elements.
- **Failure signatures:**
  - Hallucinated Dependencies: The planner might assume a "Task Group" applies because of semantic similarity, even if the UI environment differs.
  - Accumulator Noise: Over time, the log database accumulates redundant or low-significance task files, diluting retrieval quality.
  - Label Ambiguity: The LocalPlanner fails if multiple UI elements have similar names but different functions.
- **First 3 experiments:**
  1. **Retrieval Ablation:** Run the GlobalPlanner with retrieval disabled vs. enabled on a standardized set of 50 tasks to quantify the exact delta provided by the log memory.
  2. **Log Pollution Test:** Intentionally inject noise into the Task Dictionary to measure the robustness of the retrieval ranking and identify the breaking point of the "diversity selection" strategy.
  3. **Long-Horizon Stress Test:** Execute tasks exceeding 30 low-level actions to verify if the hierarchical decomposition maintains coherence better than baseline ReAct/UFO agents.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can automated log filtering techniques be designed to mitigate the accumulation of redundant or low-significance task files to preserve retrieval quality in long-term deployments? The authors state that as user logs accumulate, the system retains redundant files which dilute retrieval quality and introduce planning overhead, but the current implementation does not include a mechanism to prune or prioritize logs.

- **Open Question 2:** Can lightweight visual recognition within component viewports be effectively integrated to disambiguate GUI elements that share ambiguous or non-semantic textual labels? The Conclusion identifies that the current reliance on textual labels makes it difficult to distinguish visually similar elements (e.g., image files) and proposes visual recognition as future work.

- **Open Question 3:** To what extent does the reliance on a fixed dictionary of 19 predefined high-level GUI events limit the framework's adaptability to applications with non-standard or complex interaction paradigms? The paper does not discuss how the system handles useful interactions that fall outside this fixed taxonomy, such as complex gesture controls or custom application widgets.

## Limitations
- System's dependence on text-based GUI component identification is explicitly acknowledged as a limitation, with failures on unlabeled visual elements (e.g., "dog.png" scenario).
- Log accumulation can introduce noise over time, potentially degrading retrieval quality, but the degradation rate is not quantified.
- Evaluation relies heavily on proprietary "in-house" tasks and logs, making external validation challenging.

## Confidence

- **High Confidence:** The core architectural claims (hierarchical planning, retrieval-augmented generation, log-derived task mining) are well-supported by the evaluation results showing 80% success rate versus 46.5% (UFO) and 18.0% (ReAct).
- **Medium Confidence:** The claimed efficiency gains (61.7s average execution time) are plausible given the lightweight LocalPlanner, but comparative baseline execution times aren't provided for direct validation.
- **Low Confidence:** The long-term scalability claim (maintaining >60% success on long-horizon sequences) lacks supporting data on how performance degrades with log database growth or across diverse GUI environments beyond the tested scope.

## Next Checks

1. **Log Pollution Stress Test:** Systematically inject varying levels of noise (random clicks, irrelevant tasks) into the Task Dictionary to measure degradation in retrieval accuracy and identify the breaking point where performance drops below baseline levels.

2. **Visual Element Benchmarking:** Create a standardized test suite of tasks requiring interaction with unlabeled icons, images, and visual-only UI elements to quantify the system's limitations compared to vision-based alternatives.

3. **Long-Term Log Accumulation Analysis:** Simulate 6+ months of continuous log collection and task execution to measure how retrieval quality and planning accuracy change over time, testing the system's claim of maintaining performance despite growing log databases.