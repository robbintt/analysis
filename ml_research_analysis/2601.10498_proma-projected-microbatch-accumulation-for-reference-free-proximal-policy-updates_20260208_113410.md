---
ver: rpa2
title: 'PROMA: Projected Microbatch Accumulation for Reference-Free Proximal Policy
  Updates'
arxiv_id: '2601.10498'
source_url: https://arxiv.org/abs/2601.10498
tags:
- policy
- gradient
- proma
- microbatch
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROMA introduces a reference-free proximal policy method that projects
  accumulated gradients during backpropagation to be orthogonal to sequence-wise gradients,
  avoiding reliance on likelihood ratios to a reference policy. This projection is
  applied layer-wise and efficiently implemented during the backward pass.
---

# PROMA: Projected Microbatch Accumulation for Reference-Free Proximal Policy Updates

## Quick Facts
- arXiv ID: 2601.10498
- Source URL: https://arxiv.org/abs/2601.10498
- Reference count: 15
- PROMA achieves proximal updates without entropy collapse, with lower KL divergence between successive policies compared to both GRPO and REINFORCE baselines on GSM8K with Qwen-3 0.6B.

## Executive Summary
PROMA introduces a reference-free proximal policy method that projects accumulated gradients during backpropagation to be orthogonal to sequence-wise gradients, avoiding reliance on likelihood ratios to a reference policy. This projection is applied layer-wise and efficiently implemented during the backward pass. The method maintains comparable or improved validation performance relative to GRPO while providing tighter local KL control and higher entropy throughout training, indicating smoother updates and sustained exploration.

## Method Summary
PROMA modifies the standard gradient accumulation pattern during RL fine-tuning by projecting the partially accumulated gradient to be orthogonal to sequence-wise log-probability gradients from the current microbatch. This projection, implemented layer-wise during the backward pass, constrains parameter updates to directions that minimize probability-shift contributions, thereby controlling KL divergence without requiring a reference policy. The method uses REINFORCE-style updates without PPO clipping or KL penalties, and experiments employed an iterative Gram-Schmidt approximation for computational efficiency.

## Key Results
- PROMA maintains higher entropy throughout training compared to GRPO, indicating sustained exploration
- PROMA achieves lower KL divergence between successive policies than both GRPO and REINFORCE baselines
- Validation performance on GSM8K with Qwen-3 0.6B is comparable or improved relative to GRPO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting gradients orthogonal to sequence-wise log-probability gradients reduces local KL divergence between successive policy updates.
- Mechanism: The direction ∇log π(s) represents the parameter direction where a sequence's probability changes fastest. KL divergence grows with squared overlaps E_s[(dθ · ∇_θ log π(s))²]. By projecting out these components from the accumulated gradient before each microbatch addition, PROMA removes the parameter directions that would most rapidly shift sequence probabilities.
- Core assumption: The sequence-wise gradients from the current microbatch provide a sufficiently representative sample of the directions that contribute to KL growth.
- Evidence anchors:
  - [abstract] "projects the partially accumulated gradient to be orthogonal to the sequence-wise gradients of the current microbatch"
  - [section 2] "PROMA aims to avoid this contribution to the KL divergence by projecting away these components of the gradient update"
  - [corpus] ISOPO (arXiv:2512.23353) similarly explores reference-free proximal updates via isometric rescaling, suggesting alternative geometric approaches to the same problem.
- Break condition: If microbatch size is too small or sequences are unrepresentative, the projected subspace may not capture true KL-contributing directions, leading to underprojection and potential instability.

### Mechanism 2
- Claim: Reference-free proximal control can be achieved through gradient geometry rather than likelihood-ratio clipping.
- Mechanism: Traditional PPO/GRPO bound policy deviation by clipping the importance ratio π/π_old. PROMA instead constrains updates geometrically by ensuring gradient steps lie in a subspace that minimizes probability-shift contributions. This eliminates the need to maintain and query a reference policy during training.
- Core assumption: Orthogonal projection provides sufficient constraint without the adaptive thresholds that clipping provides.
- Evidence anchors:
  - [abstract] "reference-free proximal policy method...avoiding reliance on likelihood ratios to a reference policy"
  - [section 4] "PROMA configuration is based on REINFORCE without any PPO clipping...to isolate the effect of the projected accumulation"
  - [corpus] No direct corpus comparison to other reference-free methods; ISOPO is mentioned as related work but empirical comparison not provided.
- Break condition: If the optimization landscape requires different constraints at different training phases, the fixed geometric constraint may be too rigid compared to adaptive clipping.

### Mechanism 3
- Claim: Layer-wise projection during backward pass enables efficient implementation with negligible overhead relative to gradient computation.
- Mechanism: Projection uses QR decomposition (O(k²d) for exact) or iterative Gram-Schmidt (O(kd) for approximate), where k is microbatch sequence count and d is parameter dimension. Since gradient computation requires O(Td) where T is token count, and typically k ~ 8 while T >> k², the overhead is minimal.
- Core assumption: The approximate iterative projection (project_to_complement_v2) provides sufficient orthogonality for practical purposes.
- Evidence anchors:
  - [section 2] "The cost 2k²d of PROMA is then negligible compared to the T×d operations required to form the gradient"
  - [section 4] "The results in this section used the iterative projection project_to_complement_v2 which takes O(kd) FLOPS"
  - [corpus] No corpus evidence on projection efficiency comparisons.
- Break condition: If exact orthogonality is critical for theoretical guarantees, the approximate method may accumulate projection errors across microbatches.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) objective**
  - Why needed here: PROMA is positioned as an alternative to PPO-style clipping. Understanding what PPO constrains (the likelihood ratio) clarifies what PROMA constrains instead (gradient geometry).
  - Quick check question: Can you explain why PPO clips the ratio π(a|s)/π_old(a|s) rather than directly penalizing KL divergence?

- Concept: **KL divergence and its gradient relationship**
  - Why needed here: The paper's core motivation is that KL divergence correlates with squared dot products between update direction and log-probability gradients. Understanding this connection is essential.
  - Quick check question: Why does D_KL(π || π') scale quadratically with the overlap between the parameter update and ∇log π(s)?

- Concept: **Gradient accumulation across microbatches**
  - Why needed here: PROMA modifies the standard gradient accumulation pattern. Without understanding how microbatch gradients are typically summed, the modification won't make sense.
  - Quick check question: In standard gradient accumulation, how are gradients from multiple microbatches combined before the optimizer step?

## Architecture Onboarding

- Component map:
  accumulate_PROMA() -> project_to_complement_v2() -> optimizer step
  (layer-wise projection during backward pass)

- Critical path:
  1. Compute sequence-wise log-probability gradients (mcb_seq_grads) during policy gradient evaluation
  2. Call accumulate_PROMA before adding microbatch gradient to accumulator
  3. Project accumulated gradient to complement of sequence gradient subspace
  4. Add current microbatch policy gradient to projected accumulator
  5. Continue to next microbatch or apply optimizer step

- Design tradeoffs:
  - **Exact vs. approximate projection**: QR decomposition (Algorithm 2) vs. iterative projection (Algorithm 4). Exact costs O(k²d), approximate costs O(kd). Paper used approximate without speed comparison.
  - **PROMA vs. Intra-PROMA**: PROMA operates across microbatches (sequential dependency), Intra-PROMA operates within each microbatch (embarrassingly parallel). Figure 3c shows PROMA maintains higher entropy longer than Intra-PROMA.
  - **Projection strength**: Paper clamps subtracted component to at most 0.5 × ||mcb_policy_grad|| to prevent overprojection.

- Failure signatures:
  - **Entropy collapse**: If projection is too weak, policy may still collapse. Monitor Figure 3c-style entropy curves.
  - **Training stall**: If projection is too aggressive (removes too much), validation score may plateau early. Paper notes sandwich projection variant (Equation 2) caused lower plateau.
  - **Gradient norm explosion**: Without the norm clamping, accumulated gradient could grow unboundedly in the complement subspace.

- First 3 experiments:
  1. **Ablation on projection strength**: Vary the clamping multiplier (0.5 in paper) to find the boundary between stability and flexibility on a held-out task.
  2. **Comparison of exact vs. approximate projection**: Measure wall-clock overhead and training dynamics difference between QR-based and iterative projection on the same GSM8K setup.
  3. **Microbatch size sensitivity**: Test k ∈ {4, 8, 16, 32} to characterize how subspace dimension affects KL control and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the approximate iterative projection (Algorithm 4) provide measurable wall-clock speedup over the exact QR-based projection in practice?
- Basis in paper: [explicit] The author states: "I did not test if this was faster in practice" regarding the O(kd) approximate projection versus the exact method.
- Why unresolved: Only FLOP counts were analyzed theoretically; no timing benchmarks were reported.
- What evidence would resolve it: Wall-clock timing comparisons between exact and approximate projection methods across varying microbatch sizes and model scales.

### Open Question 2
- Question: How does PROMA scale to larger language models (e.g., 7B+ parameters) compared to GRPO?
- Basis in paper: [inferred] All experiments used only Qwen-3 0.6B; scalability to larger models is unstated.
- Why unresolved: Memory and computational characteristics of layer-wise orthogonal projection may change non-trivially with model scale.
- What evidence would resolve it: Empirical comparison of PROMA vs. GRPO on models ranging from 1B to 70B parameters, measuring performance, KL control, and training throughput.

### Open Question 3
- Question: Is PROMA effective beyond mathematical reasoning tasks, such as in open-ended dialogue, code generation, or general instruction-following?
- Basis in paper: [inferred] Evaluation was limited to GSM8K; generalization to other RLHF domains is not demonstrated.
- Why unresolved: The KL-projection mechanism may interact differently with reward landscapes in other tasks.
- What evidence would resolve it: Benchmarks on diverse RLHF datasets (e.g., HH-RLHF, MBPP, IFEval) comparing PROMA against GRPO and REINFORCE baselines.

## Limitations
- Limited empirical validation beyond GSM8K mathematical reasoning task
- No direct comparison between exact and approximate projection methods
- Scalability to larger models and generalization to other RLHF domains remains untested

## Confidence
**High Confidence**: The geometric interpretation of KL control through gradient projection is mathematically sound and well-explained. The empirical results on GSM8K show clear improvements in entropy maintenance and KL control compared to baselines.

**Medium Confidence**: The efficiency claims regarding layer-wise projection overhead are reasonable given the complexity analysis, but lack direct empirical validation. The method's generalization to other tasks and model scales remains uncertain.

**Low Confidence**: The long-term stability of the reference-free approach and its performance in scenarios requiring more aggressive exploration or different reward structures has not been demonstrated.

## Next Checks
1. **Exact vs. Approximate Projection Comparison**: Implement both QR-based exact projection and iterative approximation, measuring training dynamics and wall-clock overhead on GSM8K to quantify the trade-off.

2. **Cross-Task Generalization**: Test PROMA on additional RLHF tasks (e.g., instruction following, summarization) with different model scales to evaluate robustness beyond the single GSM8K experiment.

3. **Projection Strength Sensitivity**: Systematically vary the clamping multiplier (currently 0.5) and measure the impact on final validation performance, entropy collapse timing, and KL divergence stability across multiple seeds.