---
ver: rpa2
title: Investigating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable,
  Realistic, Multi-hop Queries
arxiv_id: '2510.11956'
source_url: https://arxiv.org/abs/2510.11956
tags:
- https
- queries
- crumqs
- multi-hop
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CRUMQs, a pipeline for generating unanswerable,
  uncheatable, realistic, and multi-hop queries for RAG evaluation. Existing RAG benchmarks
  fail to capture realistic task complexity, often allowing disconnected reasoning
  or focusing on simple factual recall.
---

# Investigating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries

## Quick Facts
- arXiv ID: 2510.11956
- Source URL: https://arxiv.org/abs/2510.11956
- Reference count: 40
- Primary result: CRUMQs pipeline generates unanswerable, uncheatable, realistic, multi-hop queries that expose significant weaknesses in RAG systems

## Executive Summary
This paper introduces CRUMQs, a novel pipeline for generating unanswerable, uncheatable, realistic, and multi-hop queries to evaluate Retrieval-Augmented Generation (RAG) systems. Existing RAG benchmarks often fail to capture the complexity of real-world tasks, allowing disconnected reasoning or focusing on simple factual recall. CRUMQs addresses these limitations by extracting topic keyphrases from document collections, sourcing recent external articles, and generating multi-hop QA pairs that are either fully or partially unanswerable. The pipeline leverages LLM-based verification and synthetic chain-of-thought annotations to ensure query quality and complexity.

Experiments demonstrate that CRUMQs are significantly more challenging than prior benchmarks, with RAG systems achieving lower acceptable ratios and up to 81.0% reduction in cheatability compared to existing datasets like MultiHop-RAG. The pipeline enables difficulty-controlled query generation adaptable to any corpus, advancing RAG system evaluation and development.

## Method Summary
CRUMQs is a pipeline for generating unanswerable, uncheatable, realistic, and multi-hop queries for RAG evaluation. It extracts topic keyphrases from a document collection, sources recent external articles, and generates multi-hop QA pairs that are either fully or partially unanswerable. Questions are verified for unanswerability and multi-hop reasoning through LLM judgments and synthetic chain-of-thought annotations. The pipeline allows for difficulty-controlled generation of queries adaptable to any corpus, addressing limitations in existing RAG benchmarks by focusing on realistic task complexity and resistance to disconnected reasoning.

## Key Results
- RAG systems achieve significantly lower acceptable ratios on CRUMQs compared to UAEval4RAG queries (e.g., 18% vs. 34% acceptable ratio for a leading system)
- CRUMQs demonstrate up to 81.0% reduction in cheatability compared to MultiHop-RAG
- The pipeline enables difficulty-controlled generation of queries adaptable to any corpus, advancing RAG system evaluation

## Why This Works (Mechanism)
CRUMQs works by constructing queries that require integration of information from multiple sources while deliberately including unanswerable or misleading elements. The pipeline extracts keyphrases from a corpus, retrieves relevant external articles, and generates multi-hop QA pairs that are verified for unanswerability and reasoning complexity using LLM-based judgments and synthetic chain-of-thought annotations. This approach ensures that queries are both realistic and resistant to simple disconnected reasoning, exposing weaknesses in RAG systems that simpler benchmarks miss.

## Foundational Learning
- **Topic Keyphrase Extraction**: Identifying central themes from document collections to ground query generation
  - Why needed: Ensures queries are contextually relevant to the corpus
  - Quick check: Verify extracted keyphrases accurately represent document topics

- **External Article Sourcing**: Retrieving recent, relevant articles to introduce unanswerable elements
  - Why needed: Creates realistic scenarios where complete answers require external knowledge
  - Quick check: Confirm retrieved articles are topically related but not contained in the corpus

- **Multi-hop Reasoning Generation**: Constructing questions requiring integration across multiple information sources
  - Why needed: Tests RAG systems' ability to perform complex reasoning beyond simple retrieval
  - Quick check: Validate that answers require synthesizing information from at least two sources

- **LLM-based Verification**: Using language models to confirm unanswerability and reasoning complexity
  - Why needed: Ensures generated queries meet quality standards automatically
  - Quick check: Compare LLM judgments across different model versions and prompts

## Architecture Onboarding

**Component Map**
Corpus -> Keyphrase Extraction -> External Article Retrieval -> Query Generation -> LLM Verification -> Synthetic Chain-of-Thought Annotation -> Final CRUMQs Dataset

**Critical Path**
Keyphrase Extraction → External Article Retrieval → Query Generation → LLM Verification → Final Dataset

**Design Tradeoffs**
- Balancing unanswerability with realistic information needs
- Trade-off between generation difficulty and query quality
- LLM-based verification vs. human annotation costs

**Failure Signatures**
- Queries that can be answered from single documents (indicating failed multi-hop generation)
- LLM verification inconsistencies across different model versions
- Over-generation of certain topic types due to corpus bias

**First Experiments**
1. Generate a small set of CRUMQs and manually verify unanswerability and multi-hop reasoning
2. Compare RAG system performance on CRUMQs vs. existing benchmarks using identical configurations
3. Test sensitivity of LLM verification by varying model versions and prompts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit challenges include the robustness of LLM-based verification steps and the generalizability of CRUMQs across different domains and RAG architectures.

## Limitations
- Robustness of LLM-based verification steps is uncertain across different model versions or prompts
- Limited empirical validation of "realism" beyond derivation from actual news articles
- Experimental scope is constrained to a specific corpus and small set of RAG system variants

## Confidence
- **Medium**: The core innovation is well-justified, but empirical evidence is constrained by evaluation setup and lack of external validation

## Next Checks
1. Conduct user studies or expert reviews to confirm that CRUMQs reflect realistic information needs and query distributions
2. Perform ablation studies on the LLM verification steps to assess sensitivity to model choice, prompting, and annotation consistency
3. Test CRUMQs across multiple, diverse corpora and with a broader set of RAG architectures to evaluate generalizability of the difficulty results