---
ver: rpa2
title: 'Beyond surface form: A pipeline for semantic analysis in Alzheimer''s Disease
  detection from spontaneous speech'
arxiv_id: '2512.13685'
source_url: https://arxiv.org/abs/2512.13685
tags:
- semantic
- language
- similarity
- original
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a novel pipeline for Alzheimer\u2019s disease\
  \ detection from spontaneous speech by transforming text surface forms while preserving\
  \ semantic content. The method employs large language models to restructure and\
  \ summarize transcripts, isolating semantic features from syntactic and lexical\
  \ cues."
---

# Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech

## Quick Facts
- arXiv ID: 2512.13685
- Source URL: https://arxiv.org/abs/2512.13685
- Reference count: 40
- Primary result: Semantic impairment detection remains feasible after surface standardization, supporting improved early AD detection systems focused on underlying meaning rather than surface patterns.

## Executive Summary
This study introduces a novel pipeline for Alzheimer's disease detection from spontaneous speech by transforming text surface forms while preserving semantic content. The method employs large language models to restructure and summarize transcripts, isolating semantic features from syntactic and lexical cues. Evaluations on two datasets (English and Portuguese) show high semantic similarity (>0.6) and low surface form similarity (BLEU < 0.1, chrF < 0.5) across transformations. Classification using BERT achieves comparable macro-F1 scores to original text (differences within ±0.1), demonstrating robustness to surface form changes. Text-to-image-to-text transformations degrade semantic preservation and performance, suggesting modality shifts introduce noise. The findings confirm that semantic impairment detection remains feasible after surface standardization, supporting the potential for improved early AD detection systems focused on underlying meaning rather than surface patterns.

## Method Summary
The pipeline transforms spontaneous speech transcripts using large language models to restructure text while preserving semantic content. Two datasets are used: Dog Story (Brazilian Portuguese, 139 texts) and ADReSS (English, 156 texts). Dog Story is translated to English using NLLB-200. GPT-4o or Llama-3.3 70B apply four transformations: Short/Medium/Long Summaries and Storyboard conversion. Classification is performed using BERT (bert-base-uncased for English, BERTimbau for Portuguese) with 10 epochs and early stopping. Surface form similarity is measured with BLEU and chrF, while semantic similarity uses SentenceBERT cosine similarity. The method compares classification performance between original and transformed texts to assess semantic feature isolation.

## Key Results
- Text transformations achieve high semantic similarity (>0.6) while dramatically reducing surface form similarity (BLEU < 0.1, chrF < 0.5)
- BERT classification on transformed texts achieves macro-F1 scores within ±0.1 of original text baselines
- Text-to-image-to-text transformations show pronounced semantic degradation (cosine ~0.39-0.45) and near-chance classification performance (macro-F1 ~0.52-0.53)
- Statistical analysis shows surface metrics (pronoun-noun ratio, adverb ratio) lose significance after transformation while classification performance remains stable

## Why This Works (Mechanism)

### Mechanism 1: Semantic Content Preservation Under Surface Form Transformation
LLM-based text transformations remove lexico-syntactic surface features while preserving semantic content, allowing AD classifiers to rely primarily on meaning rather than superficial patterns. Generative LLMs restructure text via summarization and storyboard conversion, altering surface form (low BLEU < 0.1, low chrF < 0.5) but retaining high semantic similarity (cosine similarity > 0.6). A downstream BERT classifier trained on these transformed texts achieves macro-F1 scores within ±0.1 of the original text baseline, demonstrating that semantic signals alone are sufficient for detection.

### Mechanism 2: Standardization Removes Spurious Lexico-Syntactic Correlations
The transformation pipeline acts as a filter, removing known lexico-syntactic AD markers and forcing the classifier to base decisions on more fundamental semantic integrity. By rewriting all inputs into a standardized format, the pipeline normalizes superficial differences. Post-transformation analysis shows that measures like pronoun-noun ratio and adverb ratio, which were significantly different between AD and Control groups in original text, lose their statistical significance (p-values increase from p=0.001 to p=0.252 for Short Summaries). The BERT classifier must therefore rely on residual signals, which the authors argue are semantic.

### Mechanism 3: Modality Fidelity Limits Semantic Transfer
Cross-modal transformation (text-to-image-to-text) introduces noise that disrupts the precise semantic details required for AD detection, leading to performance degradation. Converting text to an image and back to text attempts to strip all syntax, but current generative models hallucinate visual details or fail to render full textual context. The regenerated text has low semantic similarity (Cosine ~0.39-0.45) and the classifier trained on it performs near random baseline (macro-F1 ~0.52-0.53). The information bottleneck and generative errors destroy the signal.

## Foundational Learning

**Concept: Semantic vs. Surface Form in NLP**
- Why needed here: The entire paper hinges on separating "meaning" (semantics) from "wording" (syntax/lexis). Understanding that BERT learns both, and this work tries to isolate the semantic component for interpretability.
- Quick check question: Does a sentence's "surface form" include its vocabulary choice and grammatical structure?

**Concept: Embeddings and Cosine Similarity**
- Why needed here: The authors use SentenceBERT embeddings to quantify how much "meaning" is preserved after transformation. Understanding that similar meanings are mapped to proximate vectors in high-dimensional space is crucial for interpreting their "high semantic similarity" claim.
- Quick check question: If two sentences mean the same thing but use different words, would you expect their cosine similarity to be high (>0.6) or low (<0.1)?

**Concept: BLEU and chrF Scores**
- Why needed here: These metrics quantify the success of the *surface form disruption*. Low scores here are good for the authors' goal because they prove the text has been rewritten, not just copied.
- Quick check question: A BLEU score of 0.03 suggests what about the lexical overlap between the original and transformed text?

## Architecture Onboarding

**Component map:**
Translation -> Transformation (LLM) -> Validation Metrics -> Downstream Classification

**Critical path:**
The Translation -> Transformation -> Validation loop. The success of the entire system relies on the Transformation LLM's ability to follow instructions that rewrite the text without hallucinating new facts or losing the core narrative required for AD detection.

**Design tradeoffs:**
- **LLM Selection**: The authors use GPT-4o for one dataset (Dog Story) and a local Llama model for the other (ADReSS) due to data privacy rules. This introduces a confounding variable (model capability) that makes direct comparison between datasets more difficult.
- **Transformation Type**: Summaries reduce token count (efficiency), but may remove "low-level" semantic details (e.g., specific word-finding pauses encoded in text) that could be informative. Storyboards standardize structure but may impose an external narrative logic onto disordered speech.

**Failure signatures:**
- **Semantic Drift**: High BLEU/chrF (indicating low transformation) or very low Cosine Similarity (<0.5). The former means the surface form wasn't stripped; the latter means too much meaning was lost.
- **Classifier Collapse**: Macro-F1 drops >0.15 from baseline. This suggests the transformation removed the signal (semantic or otherwise) entirely.
- **Positive Class Failure**: High negative class accuracy (>0.9) but near-zero positive class accuracy, indicating the model is defaulting to the majority class (often Control).

**First 3 experiments:**
1. **Baseline Replication**: Run the BERT classifier on the raw, original transcripts of the ADReSS dataset to establish your own baseline macro-F1.
2. **Transformation Fidelity Check**: Apply the "Long Summary" prompt to a sample of 10 transcripts. Manually inspect the outputs to confirm semantic preservation and surface form change. Calculate BLEU and Cosine Similarity for this sample.
3. **Ablation on Transformation Length**: Train three separate BERT classifiers on the Short, Medium, and Long summaries. Plot the macro-F1 against the summary length to verify the authors' finding that Long Summaries and Storyboards perform best.

## Open Questions the Paper Calls Out
The paper explicitly states that whether the poor performance of text-to-image-to-text transformations is due to modality changes or differences in text/image model quality "remains to be investigated." Additionally, the authors note that "further work is needed to verify" whether translation improves AD detection by removing language-specific surface-form confounds or merely by enabling access to higher-resource English models.

## Limitations
- The semantic similarity preservation claim relies on SentenceBERT's ability to capture clinically relevant AD-specific semantic impairments, which may not be fully validated
- Different LLMs (GPT-4o vs. Llama-3.3) are used for different datasets, introducing a confounding variable in cross-dataset comparisons
- The transformation pipeline may introduce new spurious correlations that BERT learns instead of genuine semantic signals
- The study only evaluates picture description tasks, limiting generalizability to other linguistic tasks like narrative recall or conversational speech

## Confidence
- **High Confidence**: The surface form disruption is effective (low BLEU/chrF scores confirm successful rewriting) and the classification performance remains robust to these transformations (macro-F1 differences within ±0.1). The text-to-image-to-text failure mode is clearly demonstrated.
- **Medium Confidence**: The semantic similarity preservation claim, while supported by high cosine scores, may not fully capture AD-specific semantic impairments. The interpretation that performance retention proves semantic feature isolation is plausible but depends on the quality of the semantic embedding model.
- **Medium Confidence**: The claim that standardization removes spurious lexico-syntactic correlations is supported by the loss of statistical significance in surface metrics, but the direct causal link between this and the classifier's reliance on semantics is inferred rather than directly measured.

## Next Checks
1. **Semantic Embedding Validation**: Conduct a qualitative analysis comparing original and transformed texts for AD-specific semantic markers (e.g., concept loss, coherence breakdowns) to verify that high cosine similarity actually reflects preservation of clinically relevant semantic impairment signals.

2. **Transformation Artifact Analysis**: Examine the transformed texts for systematic rephrasing patterns or artifacts introduced by the LLMs, and test whether a classifier trained on these artifacts alone can achieve comparable performance, which would challenge the semantic isolation claim.

3. **Cross-Modality Fidelity Test**: Replicate the text-to-image-to-text transformation using a more sophisticated multi-modal model (if available) to determine whether the performance degradation is due to fundamental limitations of the image modality in representing abstract semantic deficits, or simply current model limitations.