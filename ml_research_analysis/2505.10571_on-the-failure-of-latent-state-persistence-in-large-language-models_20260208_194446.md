---
ver: rpa2
title: On the Failure of Latent State Persistence in Large Language Models
arxiv_id: '2505.10571'
source_url: https://arxiv.org/abs/2505.10571
tags:
- state
- llms
- memory
- latent
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes and quantifies "Latent State Persistence"
  (LSP) in large language models (LLMs) by designing three novel experiments that
  test whether models can maintain internal, unexpressed representations across multiple
  steps. The Number Guessing Game demonstrates that LLMs fail to allocate probability
  mass to a singular hidden choice, violating a fundamental probabilistic principle.
---

# On the Failure of Latent State Persistence in Large Language Models

## Quick Facts
- arXiv ID: 2505.10571
- Source URL: https://arxiv.org/abs/2505.10571
- Reference count: 40
- Key outcome: 17 frontier LLMs fail to maintain latent states across multiple steps, functioning as reactive post-hoc solvers rather than proactive planners with true working memory.

## Executive Summary
This paper investigates whether large language models can maintain internal, unexpressed representations (latent state persistence) across multiple steps. Through three novel experiments—Number Guessing Game, Yes-No Game, and Mathematical Mentalism—the authors demonstrate that current LLMs fail to maintain consistent probabilistic commitments to hidden states. The results show that models instead rely on reactive, context-based inference rather than proactive planning, with scaling laws failing to bridge this fundamental gap. The study provides a framework for evaluating internal representation fidelity and highlights a critical architectural divergence between transformer-based LLMs and human-like cognition.

## Method Summary
The study evaluates 17 frontier LLMs (GPT-4o variants, o-series, LLaMA 3.1/3.3, Qwen2.5, DeepSeek V3/R1) across three experiments. Number Guessing tests whether models allocate probability mass to a hidden number when queried repeatedly. Yes-No Game measures how long models maintain consistency while answering binary questions about hidden attributes. Mathematical Mentalism evaluates variable binding and state evolution in multi-step transformations. All experiments use controlled prompts with fixed temperature and top-p parameters, measuring metrics like Empirical State Mass, Mean Steps to Contradiction, and Invariant Success Rate.

## Key Results
- ESM deviates from theoretical target of 1.0 across all tested state space sizes, showing models don't commit to latent states
- Models exhibit finite Mean Steps to Contradiction in Yes-No Game, with pass rates varying significantly across model families
- Chain-of-Thought prompting and Long Reasoning Models substantially improve performance by externalizing latent states
- Scaling laws fail to bridge the Latent State Persistence gap—8B models sometimes outperform 405B models on LSP metrics

## Why This Works (Mechanism)

### Mechanism 1: Stateless Post-Hoc Inference
- Claim: LLMs simulate stateful behavior based on statistical plausibility rather than maintaining internal states
- Mechanism: When prompted to "think of a number," models generate acknowledgement tokens without instantiating an internal state x. Subsequent queries are processed as independent frequentist inferences rather than comparisons against a fixed x, causing ESM to collapse toward zero
- Core assumption: Autoregressive transformers lack persistent hidden state between token generations
- Evidence: ESM varies non-monotonically and decays for large n instead of converging to 1.0
- Break condition: If models exhibited true LSP, ESM would converge to 1.0 across varying state space sizes

### Mechanism 2: Constraint Accumulation and Recency Bias
- Claim: As dialogue length increases, models override earlier commitments to satisfy local consistency with proximate tokens
- Mechanism: Without persistent latent anchor, maintaining global consistency requires solving an O(t) Dynamic Constraint Satisfaction Problem. Attention mechanism prioritizes recent tokens, causing earlier constraints to be "forgotten" or contradicted
- Core assumption: Transformer attention distributions favor proximate positions
- Evidence: GPT-4o achieves 13.5% pass rate vs. 0% for GPT-4o-Mini; if LSP existed, pass rate would be independent of t
- Break condition: Pass rate should be O(1) if LSP existed

### Mechanism 3: Externalization Bypass via Tokenized Reasoning
- Claim: Chain-of-Thought and Long Reasoning Models succeed by converting latent-state problems into explicit sequence-to-sequence tasks
- Mechanism: CoT and hidden reasoning tokens serve as "scratchpad" that writes intermediate states into context window, transforming LSP failure into tractable autoregressive prediction
- Core assumption: Context window functions as external working memory
- Evidence: Zero-shot ISR near 0% vs. CoT ISR 10-30% vs. LRM ISR up to 100%
- Break condition: Improvement from externalization confirms latent persistence is bottleneck

## Foundational Learning

- **Autoregressive generation without hidden state**
  - Why needed here: Understanding transformers only pass information via context tokens (attention) and weights—not through persistent hidden variables like RNNs—is essential to grasping why LSP fails
  - Quick check: If you pause generation mid-sequence and resume with same prompt but new random seed, will the model "remember" its prior outputs?

- **Probabilistic consistency constraints**
  - Why needed here: Sum-of-Probability Identity (Σ P(Yes|i) = 1) provides mathematical null hypothesis for state persistence; violating it proves model never committed to state
  - Quick check: An agent claims to have chosen a number 1-10. If P(Yes|i=1) = 0.05 for all i, what is ESM and what does it imply?

- **Working memory vs. context window**
  - Why needed here: This paper distinguishes "true" working memory (internal, unexpressed representations) from mere context retrieval; most prior "memory" benchmarks conflate these
  - Quick check: In an N-back task where all stimuli remain visible in context, what cognitive ability is actually being tested?

## Architecture Onboarding

- **Component map**: Input prompt (P_inst, R_ack, Q_i) -> Autoregressive transformer with attention over context window -> Token probabilities P(y|Q_i, context) -> Missing: Persistent latent state buffer

- **Critical path**: 1) Model receives "think of a number" → generates acknowledgement without instantiating x 2) Query arrives → model estimates P(Yes) via pattern matching, not comparison to x 3) As queries accumulate → context grows, earlier constraints diluted by attention 4) Contradiction emerges → St = ∅

- **Design tradeoffs**: CoT/scratchpads: Reliable but token-costly, requires explicit prompting. LRMs (o-series, R1): High accuracy but opaque reasoning traces, still no true LSP. Architectural alternatives (SSMs, RNNs): Potential LSP but less explored at scale

- **Failure signatures**: ESM ≈ 0 or >> 1 (should be exactly 1), MSC finite and model-size dependent, "Blue-Seven" bias: disproportionate predictions of 7, Non-monotonic scaling (8B outperforms 405B)

- **First 3 experiments**: 1) Number Guessing baseline: Run 200 trials per query across n ∈ {3, 10, 20, 40}, compute ESM; expect deviation from 1.0 2) Yes-No contradiction detection: Track feasible set St across 250 queries; record first contradiction step; compare MSC across models 3) CoT ablation: Run Mathematical Mentalism zero-shot vs. CoT; measure ISR delta to confirm externalization benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled experimental settings may not capture real-world LLM usage complexity where contextual cues could influence latent state maintenance
- Analysis focuses on English-language models and prompts, potentially limiting generalizability to multilingual or code-generation scenarios
- Evaluation relies on binary response formats (Yes/No), which may artificially constrain model behavior compared to free-form responses

## Confidence
- **High Confidence**: Empirical demonstrations of ESM ≠ 1.0, finite MSC, and ISR improvements with CoT are directly observable
- **Medium Confidence**: Mechanistic explanations linking failures to transformer architecture are plausible but not definitively proven
- **Low Confidence**: Broader claims about fundamental cognitive divergence and "architectural" vs "training" limitations are speculative extrapolations

## Next Checks
1. Cross-linguistic validation: Replicate Number Guessing experiments with non-English prompts and models to determine if LSP failure is language-dependent or universal
2. Continuous response analysis: Modify Yes-No Game to accept probabilistic responses rather than binary answers, and analyze whether ESM patterns persist in continuous space
3. Architectural ablation study: Compare transformer variants with modified attention mechanisms to isolate whether attention recency or lack of recurrence drives LSP failure