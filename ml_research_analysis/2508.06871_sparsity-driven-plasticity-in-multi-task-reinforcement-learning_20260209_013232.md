---
ver: rpa2
title: Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning
arxiv_id: '2508.06871'
source_url: https://arxiv.org/abs/2508.06871
tags:
- learning
- arxiv
- plasticity
- dense
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether sparsification methods, particularly
  Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), can mitigate
  plasticity loss and enhance performance in multi-task reinforcement learning (MTRL)
  agents. The study evaluates these methods across three MTRL architectures (shared
  backbone, Mixture of Experts, and Mixture of Orthogonal Experts) on standard MTRL
  benchmarks, comparing against dense baselines and various plasticity-inducing or
  regularization techniques.
---

# Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.06871
- Source URL: https://arxiv.org/abs/2508.06871
- Reference count: 40
- One-line primary result: Gradual Magnitude Pruning and Sparse Evolutionary Training mitigate plasticity loss in multi-task RL, often outperforming dense baselines and matching explicit plasticity interventions.

## Executive Summary
This paper investigates whether sparsification methods can mitigate plasticity loss and enhance performance in multi-task reinforcement learning agents. The study evaluates Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET) across three MTRL architectures on standard benchmarks, comparing against dense baselines and various plasticity-inducing or regularization techniques. Results show that both GMP and SET effectively reduce neuron dormancy and prevent representational collapse, common indicators of plasticity degradation. These improvements often correlate with enhanced multi-task performance, with sparse agents frequently outperforming dense counterparts and achieving competitive results against explicit plasticity interventions.

## Method Summary
The study applies Gradual Magnitude Pruning (GMP) with a cubic sparsity schedule (targeting 95% sparsity, pruning window [5%, 80%] of training) and Sparse Evolutionary Training (SET) with constant 95% sparsity and random rewiring to three MTRL architectures: MTPPO (shared backbone), Mixture of Experts (MoE), and MOORE. These methods are evaluated on MiniGrid benchmarks (MT3, MT5, MT7) and MetaWorld MT10, comparing against dense baselines, LayerNorm, and weight decay. The primary metrics include normalized Interquantile Mean (IQM) of episodic return, neuron dormancy percentage, effective rank, and Fisher Information Matrix trace. Training uses PPO for discrete control and MTMH-SAC for continuous control with standard hyperparameters.

## Key Results
- Both GMP and SET effectively reduce neuron dormancy and prevent representational collapse across all tested architectures.
- Sparse agents frequently outperform dense baselines on IQM metrics, particularly MTPPO and MoE architectures.
- Performance benefits are architecture-dependent, with MOORE showing minimal gains due to its orthogonality constraints.
- SET's random rewiring mechanism proves particularly effective at minimizing actor dormancy to near-zero levels.

## Why This Works (Mechanism)

### Mechanism 1: Capacity Constraint as Plasticity Preserver
Constraining network capacity through progressive sparsification may preserve adaptability by limiting premature commitment to suboptimal representations. GMP incrementally removes low-magnitude weights (up to 95% sparsity) following a cubic schedule, which can guide optimization toward flatter minima with lower curvature, indicated by the peak-and-decline pattern in Fisher Trace metrics. Core assumption: Reducing degrees of freedom does not eliminate critical task-specific pathways before they are discovered. Evidence: GMP often displayed a characteristic peak-and-decline pattern in Fisher Trace, whereas SET maintained a low and stable FIM trace. Break condition: Aggressive sparsity (>97%) on architectures with orthogonality constraints can cause effective rank collapse and performance degradation.

### Mechanism 2: Topological Exploration via Dynamic Rewiring
Continuous restructuring of network connectivity maintains plasticity by preventing prolonged weight dormancy and encouraging exploration of alternative pathways. SET maintains fixed sparsity (95%) while periodically pruning the lowest-magnitude connections and randomly regrowing new ones at previously zero locations. This evolutionary rewiring reduces actor dormancy to near-zero levels in MTPPO and MoE. Core assumption: Random regrowth provides sufficient exploration signal without requiring task-specific heuristics. Evidence: SET proved particularly effective at minimizing neuron dormancy to very low levels. Break condition: Architecture-specific; SET reduced dormancy in MOORE but did not translate to performance gains, suggesting sophisticated architectures may interact differently with random rewiring.

### Mechanism 3: Implicit Regularization Without Representational Collapse
Sparsity-induced plasticity differs fundamentally from explicit normalization techniques by maintaining representational diversity while reducing dormancy. Unlike LayerNorm, which caused severe effective rank reduction across all architectures, sparse methods maintained or improved effective rank while reducing dormancy. Weight decay showed plasticity dynamics nearly identical to dense baselines. Core assumption: Effective rank correlates with the network's capacity to represent diverse task-specific features. Evidence: LayerNorm is accompanied by a severe and persistent reduction in the effective rank, while sparse agents typically maintain lower percentages of dormant neurons and a higher or more stable mean effective rank. Break condition: Very high sparsity (99%) on MOORE architecture leads to rank collapse, indicating architectural limits.

## Foundational Learning

- **Plasticity Loss in Deep RL**
  - Why needed here: The entire paper frames sparsification as a solution to plasticity degradation. Without understanding that neural networks lose adaptability during training through gradient interference, representational collapse, and neuron dormancy, the motivation for sparsity is unclear.
  - Quick check question: Can you explain why a network that has trained for 100K steps might learn more slowly from new data than a freshly initialized network?

- **Magnitude-Based Pruning Schedules**
  - Why needed here: GMP uses a cubic sparsity schedule with specific start/end windows. Implementers must understand that pruning too early destabilizes learning, while pruning too late provides no plasticity benefit.
  - Quick check question: Given a 400K timestep training run, what would happen if you set pruning start at 50% and end at 90% of training?

- **Multi-Task Interference**
  - Why needed here: MTRL agents face conflicting gradient directions across tasks. The paper argues sparsity helps by implicitly regularizing, but understanding why MTRL is harder than single-task RL is prerequisite knowledge.
  - Quick check question: Why might gradients from Task A harm performance on Task B in a shared-backbone architecture?

## Architecture Onboarding

- **Component map:**
  ```
  [Input: Observation + Task Context]
           ↓
  [Shared Feature Extractor: Conv2D layers (16→32→64 channels)]
           ↓
  [Sparsified Linear Layers: GMP mask or SET topology]
           ↓
  [Task-Specific Heads: One head per task]
           ↓
  [Output: Actor logits + Critic value]
  
  Optional: Task Router (MoE) → Expert Selection → Sparse Expert Networks
  ```

- **Critical path:**
  1. Initialize dense network with standard PPO/MTPPO setup
  2. For GMP: Apply pruning mask every `fp` timesteps, increasing sparsity per cubic schedule
  3. For SET: At evolution intervals, prune fraction ε of lowest weights, regrow randomly at zero locations
  4. Monitor plasticity metrics (dormancy, effective rank, Fisher trace) from plasticity replay buffer

- **Design tradeoffs:**
  - GMP: Simpler to implement; no training speedup (operates on dense matrices with masks); inference benefits only
  - SET: Maintains true sparsity; requires sparse data structures; random regrowth may be suboptimal without heuristics
  - Architecture selection: MTPPO/MoE show consistent gains; MOORE's orthogonality constraints may conflict with extreme sparsity

- **Failure signatures:**
  - Performance plateau early + high dormancy → Increase pruning frequency or try SET
  - Effective rank collapses late in training → Sparsity too aggressive; reduce target to 90-95%
  - High variance across seeds → Pruning window starts too early; delay `t_start` to 5-10%
  - MOORE-specific: Performance degrades at >95% sparsity → Architecture orthogonality requirements incompatible with capacity reduction

- **First 3 experiments:**
  1. Replicate MT5 benchmark with MTPPO: Compare dense baseline vs. GMP (95% sparsity, schedule [0.05, 0.80]) vs. SET (95% sparsity, ε=11). Verify dormancy reduction correlates with IQM improvement.
  2. Ablate pruning schedule: On MTPPO/MT5, test schedule windows [0.0, 0.80], [0.05, 0.80], [0.10, 0.80]. Confirm [0.05, 0.80] provides best balance of early adaptation and late stabilization.
  3. Architecture sensitivity test: Apply identical GMP config to MTPPO, MoE, and MOORE on MT3. Confirm MOORE shows minimal or negative gains, validating architecture-dependence claim before scaling to larger benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do sparse MTRL subnetworks align with specific task features to offer interpretability advantages over dense models?
- Basis in paper: The authors state, "As future work, we plan on investigating the theoretical underpinnings and potential interpretability advantages of sparse MTRL models."
- Why unresolved: The current study focuses on empirical performance correlations and plasticity metrics without analyzing the semantic meaning or modularity of the preserved connections.
- Evidence: Visualizations of sparse connectivity patterns mapped to specific task attributes; analysis of whether distinct expert subnetworks emerge for distinct tasks.

### Open Question 2
- Question: Why does Sparse Evolutionary Training (SET) lead to performance degradation in the Mixture of Orthogonal Experts (MOORE) architecture?
- Basis in paper: The paper notes SET "led to a slight decline across all benchmarks" for MOORE, hypothesizing its "sophisticated structure might be less responsive to the typical benefits derived from these plasticity changes."
- Why unresolved: The paper identifies the performance drop but does not isolate the mechanism, such as whether dynamic rewiring conflicts with the architecture's orthogonality constraints.
- Evidence: Ablation studies on MOORE varying orthogonality regularization strength alongside sparsity levels to identify the specific interference point.

### Open Question 3
- Question: Does the superior performance of actor-only pruning in continuous control imply that critic networks require higher density to maintain value function accuracy?
- Basis in paper: The MetaWorld results show "selectively pruning the actor... outperforms... the globally pruned model," leading to the conclusion that "benefits of sparsity are both role and context-dependent."
- Why unresolved: While the experiments show actor-only pruning works better, the paper does not fully verify if this is strictly due to critic capacity needs or the specific dynamics of SAC value functions.
- Evidence: Comparative analysis of critic approximation error and effective rank in the critic network under varying sparsity levels.

## Limitations
- The observed plasticity benefits from sparsity may be architecture-specific rather than universally applicable across all MTRL designs.
- The random regrowth mechanism in SET lacks theoretical grounding for why it should maintain plasticity in MTRL contexts.
- The study focuses primarily on MiniGrid and MetaWorld benchmarks; generalization to other domains remains untested.

## Confidence
- **High confidence:** Plasticity metrics (dormancy, effective rank, Fisher trace) show consistent improvement with sparse methods across all tested architectures.
- **Medium confidence:** Performance improvements correlating with plasticity gains, particularly for GMP on MTPPO and MoE.
- **Low confidence:** The mechanism explanation for why random rewiring in SET maintains plasticity without explicit task heuristics is speculative and lacks theoretical foundation.

## Next Checks
1. Systematically test GMP and SET across additional MTRL architectures (e.g., PopArt, Task-Agnostic Multi-Task RL) to determine whether sparsity benefits generalize beyond the three tested designs.
2. Apply sparse methods to non-grid-world benchmarks (e.g., Atari, DMLab) to validate whether plasticity preservation translates across diverse observation spaces and task types.
3. Investigate whether the random regrowth mechanism in SET can be replaced with task-informed exploration strategies while maintaining or improving plasticity benefits.