---
ver: rpa2
title: Missing-Modality-Aware Graph Neural Network for Cancer Classification
arxiv_id: '2506.22901'
source_url: https://arxiv.org/abs/2506.22901
tags:
- modalities
- missing
- data
- magnet
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAGNET addresses the challenge of missing modalities in multimodal
  biological data for cancer classification. It introduces a patient-modality multi-head
  attention mechanism to fuse modality embeddings based on their importance and missingness,
  and constructs a patient interaction graph with fused embeddings as node features.
---

# Missing-Modality-Aware Graph Neural Network for Cancer Classification

## Quick Facts
- arXiv ID: 2506.22901
- Source URL: https://arxiv.org/abs/2506.22901
- Reference count: 40
- Method achieves up to 3.88% improvement in MCC over state-of-the-art multimodal fusion methods

## Executive Summary
MAGNET addresses missing modalities in multimodal biological data for cancer classification through a novel patient-modality multi-head attention mechanism and patient interaction graph. The method fuses modality embeddings based on learned importance and missingness patterns, then constructs a graph where patients are connected if they share available modalities. Experiments on three public multiomics datasets with real-world missingness show consistent improvements over state-of-the-art fusion methods, with accuracy gains up to 3.88% in MCC. The approach handles diverse missing-modality patterns while maintaining linear scalability with the number of modalities.

## Method Summary
MAGNET processes multimodal cancer data through modality-specific MLPs that encode each modality into a common embedding space. A patient-modality multi-head attention mechanism with binary masking fuses these embeddings, excluding missing modalities from the fusion process. The fused patient embeddings become node features in a patient interaction graph, where edges connect patients sharing at least one modality and are weighted by cosine similarity on shared features. A GraphSAGE GNN propagates information across this graph, and an MLP decoder produces classification predictions. The model is trained with cross-entropy loss plus a KL divergence term that preserves patient similarity from the original data space.

## Key Results
- MAGNET consistently outperforms state-of-the-art fusion methods (M3Care, MUSE) across three multiomics datasets
- Achieves accuracy improvements of up to 3.88% in MCC compared to existing methods
- Handles diverse missing-modality patterns found in real-world clinical data (0-66% missingness rates)
- Scales linearly with the number of modalities, avoiding combinatorial explosion of missingness patterns

## Why This Works (Mechanism)

### Mechanism 1: Patient-Modality Multi-Head Attention with Masking
The PMMHA mechanism enables adaptive fusion of available modalities by learning per-patient, per-modality importance while excluding missing modalities through element-wise multiplication with a binary mask before softmax normalization. This forces attention weights for missing modalities to zero, ensuring only available data contributes to the fused representation. The mechanism assumes attention can effectively capture modality importance and that hard masking is sufficient to exclude missing data without corrupting the attention distribution over remaining modalities.

### Mechanism 2: Patient Interaction Graph with Missingness-Aware Connectivity
MAGNET constructs a graph where edges connect patients sharing at least one modality, allowing information to propagate from patients with complete data to those with partial data. The graph uses fused patient embeddings as node features and cosine similarities on shared modalities as edge weights. This design assumes that patients who share available modalities are likely to be similar in the missing modalities as well, and that propagating information through such connections improves representation quality for partially-observed patients.

### Mechanism 3: Linear Scalability via Modular Attention Expansion
The architecture scales linearly with the number of modalities by requiring only a new modality-specific encoder and an expanded attention matrix column for each additional modality. The fusion and GNN stages remain unchanged as they operate on the fused embedding dimension, not on modality count. This modular design assumes that modality-specific encoders can be trained independently or with minimal joint tuning, and that the attention mechanism's capacity scales appropriately with additional columns.

## Foundational Learning

- **Multi-head Attention Mechanisms**: Core to PMMHA; allows model to jointly attend to information from different representation subspaces. Essential for understanding how MAGNET weights modalities.
  - Quick check: Given attention scores [0.1, 0.3, 0.6] for modalities A, B, C, and a mask [1, 0, 1], what are the effective attention weights after masking and re-normalization?

- **Graph Neural Networks and Message Passing**: The GNN module aggregates information from neighboring patients in the constructed graph. Understanding GraphSAGE's sampling and aggregation is critical.
  - Quick check: In a 2-layer GraphSAGE, how does a patient's final embedding incorporate information from 2-hop neighbors?

- **Missing Data Patterns in Multimodal Learning**: MAGNET specifically targets "structured missingness" where entire modalities are missing for some patients. Distinguishing this from random missing values is essential.
  - Quick check: Why does the exponential growth of missingness patterns (2^M - 1 patterns for M modalities) challenge approaches that build separate models per pattern?

## Architecture Onboarding

- **Component map**: Raw modality data → per-modality encoding → PMMHA fusion → graph node features → edge construction → GNN message passing → classification head → predictions
- **Critical path**: The patient-modality attention mechanism is the critical innovation, as it enables modality-specific fusion while completely excluding missing data. The graph construction follows naturally from the fused representations, and the GNN provides the final classification capability.
- **Design tradeoffs**: The method evaluates on real-world missingness rather than artificial patterns, which is more realistic but makes controlled ablation harder. GraphSAGE was chosen over GAT/GCN/GIN based on ablation results, likely due to its inductive nature and edge feature handling. The binary mask represents a design choice between hard masking and soft attention approaches.
- **Failure signatures**: Performance collapse on datasets with missingness patterns not represented in training (e.g., a modality missing for >90% of patients), degraded performance when the number of modalities grows beyond the attended dimensions, or poor generalization if test patients have no overlapping modalities with any training patient.
- **First 3 experiments**:
  1. Reproduce main results on BRCA/BLCA/OV datasets using provided code; verify MCC improvements and run ablation without PMMHA or GNN to quantify component contributions.
  2. Analyze missingness sensitivity by taking a complete subset and systematically introducing controlled missingness patterns (vary from 10% to 80% per modality); compare MAGNET vs. M3Care and MUSE as in Figure 3.
  3. Test modality extensibility by training on a subset of modalities (e.g., 2 of 3) and adding the third; measure performance gain and training time increase to validate linear scaling claim.

## Open Questions the Paper Calls Out
- Can MAGNET effectively integrate fundamentally different modalities, such as medical imaging or clinical text, in addition to omics data? (The authors explicitly state this as future work to explore broader applicability.)
- Does MAGNET generalize to non-biomedical domains with different data distributions and missingness patterns? (The authors note the method is general but experiments were limited to cancer-related applications.)
- How can the framework be adapted to handle partial feature missingness (missing values) within a modality? (The problem formulation explicitly states addressing missing values is beyond the current scope.)

## Limitations
- Performance on datasets with systematic or extreme missingness (e.g., a modality missing for >90% of patients) is unknown, as the method relies on shared modalities for graph connectivity
- Linear scalability claim is based on architectural design but not empirically validated beyond the tested number of modalities (up to 3 in experiments)
- Key hyperparameters (edge threshold β, KL loss weight λ, attention head count) are tuned per dataset, but sensitivity to these choices and their interaction is not fully characterized

## Confidence
- **High confidence**: The core mechanism of PMMHA with masking to exclude missing modalities is well-specified and theoretically sound, with strong ablation evidence from Figure 3
- **Medium confidence**: The patient interaction graph with shared-modality connectivity is a reasonable approach, but its effectiveness depends heavily on non-random, overlapping missingness patterns
- **Medium confidence**: Linear scalability is a design claim supported by modular architecture, but not empirically validated beyond tested modality counts

## Next Checks
1. **Controlled missingness ablation**: Take a complete subset of a dataset, systematically introduce missingness at rates from 10% to 80% per modality, and measure MAGNET's performance drop versus baselines to isolate the impact of missingness patterns
2. **Modality extensibility test**: Train MAGNET on a 2-modality subset, then add the third modality mid-training or in a separate run to empirically validate the linear scalability claim through performance gain and training time measurements
3. **Extreme missingness stress test**: Create synthetic datasets where one modality is missing for 90%+ of patients, and another where missingness patterns are disjoint (no shared modalities between groups), to evaluate if MAGNET's graph construction and attention mechanism still function or fail catastrophically