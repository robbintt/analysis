---
ver: rpa2
title: 'Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation
  Industrial Automation'
arxiv_id: '2507.07115'
source_url: https://arxiv.org/abs/2507.07115
tags:
- heater
- control
- agent
- temperature
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a modular agentic framework integrating large\
  \ language models (LLMs) for dual autonomy in industrial control: discrete fault-recovery\
  \ planning via finite state machines (FSMs) and continuous temperature regulation\
  \ under disturbances. In the FSM case study, GPT-4o and GPT-4o-mini achieved 100\
  \ % valid-path success across 180 randomly generated FSMs (4\u201325 states, 4\u2013\
  300 transitions) within five reprompts, outperforming open-source LLMs in accuracy\
  \ and latency."
---

# Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation

## Quick Facts
- **arXiv ID:** 2507.07115
- **Source URL:** https://arxiv.org/abs/2507.07115
- **Reference count:** 22
- **Primary result:** GPT-4o achieved 100% valid-path success in 180 FSM benchmarks; LLM-based continuous control matched PID performance on TCLab platform.

## Executive Summary
This paper introduces a modular agentic framework that integrates large language models (LLMs) for dual autonomy in industrial control systems. The framework combines symbolic reasoning for discrete fault recovery planning via finite state machines (FSMs) with continuous temperature regulation under disturbances. In case studies, GPT-4o demonstrated perfect valid-path success across diverse FSMs within five reprompts, while LLM-based controllers achieved PID-comparable performance on a dual-heater testbed. The work highlights how structured feedback loops and modular agents can enable LLMs to unify symbolic and continuous control in autonomous industrial systems.

## Method Summary
The framework employs a modular architecture where LLMs serve as reasoning agents within a closed-loop system. For discrete control, an Action Agent proposes state transitions that are validated against FSMs by a Simulation Agent, with a Reprompting Agent providing corrective feedback on failures. For continuous control, the LLM proposes power settings that are executed by a digital twin to predict outcomes, with validation ensuring movement toward setpoints. The approach leverages structured prompt engineering and JSON-formatted outputs to enable programmatic integration with industrial systems.

## Key Results
- GPT-4o and GPT-4o-mini achieved 100% valid-path success across 180 randomly generated FSMs (4-25 states, 4-300 transitions) within five reprompts
- LLM-based continuous controller maintained TCLab dual-heater temperature setpoint under asymmetric disturbances, matching PID performance
- Ablation experiments demonstrated the critical role of the reprompting loop for handling nonlinear dynamics
- Open-source LLMs (LLaMA-3) showed significantly lower accuracy and struggled with iterative correction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative validator-reprompting loops significantly improve LLM reliability in symbolic planning tasks compared to single-pass inference.
- **Mechanism:** An Action Agent proposes a candidate path, which a deterministic Simulation Agent tests against a graph (FSM). If validation fails, a Reprompting Agent injects the specific error context back into the prompt, allowing the LLM to correct hallucinated transitions or invalid logic without human intervention.
- **Core assumption:** The LLM possesses sufficient inherent reasoning capability to correct its errors when given explicit feedback; it does not require gradient updates, only in-context refinement.
- **Evidence anchors:** [abstract] mentions "100% valid-path success within five reprompts" for GPT-4o models. [section 4.1.3] shows LLaMA struggles with reprompts (repeating invalid paths), while GPT-4o maintains low reprompt counts, suggesting model capability is a boundary condition. [corpus] "Agentic AI for Intent-Based Industrial Automation" (arXiv 2506.04980) supports the general feasibility of intent-based automation, though the specific reprompting loop efficiency is unique to this paper's findings.
- **Break condition:** If the model lacks the capacity to follow the corrective instructions (as seen with LLaMA-3 in the results), the reprompting loop may oscillate or fail to converge within the iteration budget.

### Mechanism 2
- **Claim:** Constrained symbolic abstractions, specifically Finite State Machines (FSMs), act as effective "operating envelopes" to ground LLM reasoning in discrete control tasks.
- **Mechanism:** Instead of allowing the LLM to free-form plan, the system encodes valid operational states and transitions in a dictionary format. The LLM is restricted to traversing these pre-defined edges, reducing the search space from "all possible actions" to "valid graph traversals."
- **Core assumption:** The system dynamics can be adequately modeled or abstracted into discrete states; the complexity arises from the combinatorics of the graph, not the continuous physics.
- **Evidence anchors:** [abstract] explicitly adopts FSMs as "interpretable operating envelopes." [section 4.1] describes encoding FSMs as Python dictionaries found to be "more interpretable and reliably processed by LLMs than adjacency matrices." [corpus] "Graph-Symbolic Policy Enforcement and Control" (arXiv 2512.20275) aligns with this, suggesting neuro-symbolic frameworks help manage topology hallucination risks.
- **Break condition:** If the process cannot be discretized (e.g., highly fluid continuous reactions) or if the state space explodes in complexity, the token limit or reasoning fidelity of the LLM may be exceeded.

### Mechanism 3
- **Claim:** Hybridizing LLM proposals with deterministic digital twins enables safe handling of continuous dynamics despite LLM limitations in physics calculation.
- **Mechanism:** The LLM acts as a high-level controller proposing power settings. A digital twin (simulation) executes these settings to predict the outcome. A Validation Agent checks if the predicted state moves closer to the setpoint. This separates "strategy generation" (LLM) from "physics execution" (Twin).
- **Core assumption:** The digital twin accurately reflects the physical plant dynamics, and the latency introduced by the twin simulation is acceptable within the control loop.
- **Evidence anchors:** [abstract] notes the digital twin was used "to maintain a target average temperature," achieving comparable performance to PID. [section 4.2.2] highlights that LLMs produce "physically implausible approximations" (e.g., bad ODE solving), which the validation layer must catch. [corpus] Related works like "Leveraging LLM agents and digital twins for fault handling" (cited in text but not top neighbor) support this; "AssetOpsBench" (arXiv 2506.03828) further validates the benchmarking of agents in operational workflows.
- **Break condition:** If the system dynamics are faster than the LLM + Twin inference latency (observed as 12-34s in the paper), the control action arrives too late, causing instability or overshoot.

## Foundational Learning

- **Concept:** **Finite State Machines (FSM) & Graph Traversal**
  - **Why needed here:** The paper relies on FSMs to define the "rules of the game" for the AI. Without understanding states, transitions, and adjacency lists, one cannot interpret the success metrics of the symbolic planning case study.
  - **Quick check question:** Given a dictionary of states `{'A': ['B'], 'B': ['C', 'A']}`, is a path from 'A' to 'C' valid if the proposed sequence is `['A', 'C']`? (Answer: No, 'A' only connects to 'B').

- **Concept:** **Closed-Loop Control & Disturbance Rejection**
  - **Why needed here:** Case Study 2 involves regulating temperature against a cooling fan. Understanding error signals (setpoint vs. measured) and how PID vs. LLM handles "asymmetric disturbances" is central to the paper's contribution.
  - **Quick check question:** If a fan cools Heater 1, should the controller increase or decrease power to Heater 1 to maintain the average temperature? (Answer: Increase).

- **Concept:** **Prompt Engineering & Structured Output (JSON)**
  - **Why needed here:** The framework depends on the LLM outputting machine-parsable JSON (e.g., `[q1, q2]`) rather than conversational text. The "reprompting" mechanism also relies on engineering specific error messages back into the prompt.
  - **Quick check question:** Why would a developer enforce a JSON schema in the LLM output for a control system? (Answer: To programmatically extract values (q1, q2) to send to the PLC/actuator without human parsing).

## Architecture Onboarding

- **Component map:** Monitoring Agent -> Action Agent (LLM) -> Simulation/Digital Twin -> Validation Agents -> Reprompting Agent (LLM) -> Safety Override
- **Critical path:** The control loop is `Monitoring -> Action -> Simulation -> Validation`. If `Validation` returns `False`, the path diverts to `Reprompting -> Action` (looping up to 5 times). Latency in the `Simulation` or `LLM Inference` steps is the bottleneck for real-time viability.
- **Design tradeoffs:**
  - **Cloud vs. Local:** Cloud models (GPT-4o) offer superior reasoning but suffer network latency (up to 34s) and privacy risks. Local models (LLaMA) are faster/secure but fail complex reasoning tasks (lower accuracy).
  - **Token vs. Physics:** Relying on LLMs to solve ODEs internally (Example 4.3) is unreliable; the architecture prefers using the LLM for *selection* of inputs and the Digital Twin for *simulation* of physics.
- **Failure signatures:**
  - **Instruction Lapses:** The LLM repeats the exact same invalid path in subsequent reprompting turns despite explicit negative feedback (common in smaller models).
  - **State Mismatch:** The physical plant changes state during the long inference time, rendering the LLM's calculated action obsolete by the time it is applied.
  - **Physics Hallucination:** The LLM proposes equal power distribution for unequal thermal loads, ignoring asymmetry.
- **First 3 experiments:**
  1. **FSM Pathfinding Benchmark:** Generate 20 random FSMs (4-6 nodes). Run the Action Agent without reprompting. Measure "First Pass Accuracy" to establish a baseline for the specific LLM being used.
  2. **Digital Twin Latency Test:** Deploy the continuous control loop on the TCLab digital twin. Measure the time taken for one full `Action -> Validation -> Reprompt` cycle. Ensure it is < 10% of the thermal time constant.
  3. **Ablation on Reprompting:** Run the dual-heater control with the Reprompting Agent disabled. Compare the RMSE against the PID baseline to quantify the value add of the corrective loop.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the instruction-following deficiencies in open-source LLMs (e.g., LLaMA) be mitigated to prevent them from repeating invalid paths during iterative reprompting cycles?
- **Basis in paper:** [explicit] The authors note in Section 4.1.3 that LLaMA models frequently failed to adhere to instructions to avoid previously explored paths, often returning the same invalid trajectory despite augmented prompts.
- **Why unresolved:** The current reprompting strategy successfully corrects closed-source models (GPT-4o) but fails to prevent repetitive errors in open-source alternatives, limiting their deployability.
- **What evidence would resolve it:** Demonstration of a modified prompting strategy or fine-tuning approach that significantly improves the valid-path accuracy and reduces the average reprompts for LLaMA models on the FSM traversal benchmark.

### Open Question 2
- **Question:** Can retrieval-augmented generation (RAG) or physics-informed fine-tuning effectively correct the coarse ODE approximations and physical reasoning errors observed in continuous control tasks?
- **Basis in paper:** [explicit] Section 5 suggests future systems should incorporate domain-specific knowledge via lightweight fine-tuning or RAG to address the failure modes documented in Examples 4.1-4.3, such as ignoring heat losses or miscalculating temperature dynamics.
- **Why unresolved:** The current study relied on general-purpose models which produced physically implausible predictions (e.g., predicting linear temperature increases without thermal loss), leading to control inaccuracies.
- **What evidence would resolve it:** Comparative results showing that a RAG-enabled or physics-tuned LLM agent reduces the Root Mean Square Error (RMSE) in the TCLab case study by resolving the specific approximation errors listed.

### Open Question 3
- **Question:** Can the agentic framework be integrated into safety-critical SCADA/DCS architectures while satisfying functional safety standards (e.g., IEC 61511) and deterministic latency constraints?
- **Basis in paper:** [explicit] Section 5 identifies cloud latency, data sovereignty, and the lack of formal verification as barriers to deployment, explicitly stating that adherence to standards like ISA-95 and IEC 61511 is required for production environments.
- **Why unresolved:** The experiments were conducted on lab-scale systems with slow dynamics and cloud-based inference; the framework has not been validated under the strict timing and safety interlocks of industrial control systems.
- **What evidence would resolve it:** A successful case study where the LLM agent operates as a supervisor within a simulated DCS environment, meeting hard real-time deadlines and passing formal verification of output constraints.

## Limitations
- **Latency constraints:** LLM-based control loops (12-34s) may be too slow for fast dynamic processes requiring millisecond-level responses
- **Model dependency:** Performance heavily depends on LLM instruction-following capability, with open-source models showing significant degradation
- **Physics approximation:** Digital twin approximations produce "physically implausible" results requiring heavy validation

## Confidence

| Claim | Confidence |
|-------|------------|
| FSM Planning Claims | High |
| Continuous Control Claims | Medium |
| Reprompting Mechanism Claims | Medium |

## Next Checks
1. **Real-Time Viability Test:** Deploy the continuous control framework on a process with a thermal time constant under 10 seconds and measure control performance degradation as latency increases from 1s to 30s. This will establish the practical speed limits of the approach.

2. **Cross-Domain Transfer Test:** Apply the FSM-based fault recovery to a discrete process from a different domain (e.g., discrete-part manufacturing or packaging) with no prior prompt engineering. Measure the number of reprompts needed to achieve first success compared to the original process.

3. **Safety-Critical Validation:** Implement a simulated failure mode where the digital twin becomes temporarily desynchronized from the physical plant (e.g., sensor drift or actuator delay). Test whether the framework's validation layer and safety overrides prevent unsafe control actions within the 5-rep prompt limit.