---
ver: rpa2
title: 'Thinking Like a Doctor: Conversational Diagnosis through the Exploration of
  Diagnostic Knowledge Graphs'
arxiv_id: '2602.01995'
source_url: https://arxiv.org/abs/2602.01995
tags:
- patient
- diagnosis
- diagnostic
- knowledge
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of building conversational
  diagnosis systems that can operate effectively under incomplete and underspecified
  patient information, a common scenario in real-world clinical history-taking. The
  authors propose a system that uses a diagnostic knowledge graph to ground its reasoning
  in two iterative steps: generating plausible diagnostic hypotheses from dialogue
  context, and verifying these hypotheses through targeted clarifying questions.'
---

# Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs

## Quick Facts
- arXiv ID: 2602.01995
- Source URL: https://arxiv.org/abs/2602.01995
- Reference count: 36
- This paper addresses the challenge of building conversational diagnosis systems that can operate effectively under incomplete and underspecified patient information, a common scenario in real-world clinical history-taking.

## Executive Summary
This paper addresses the challenge of building conversational diagnosis systems that can operate effectively under incomplete and underspecified patient information, a common scenario in real-world clinical history-taking. The authors propose a system that uses a diagnostic knowledge graph to ground its reasoning in two iterative steps: generating plausible diagnostic hypotheses from dialogue context, and verifying these hypotheses through targeted clarifying questions. The system is evaluated using a realistic patient simulator that reflects low-specificity symptom descriptions. Results show that graph grounding substantially improves diagnostic accuracy (Recall@1 of 0.250, Recall@4 of 0.418) and efficiency (average 6.9 turns), outperforming larger foundation models and baselines that do not use the graph. Qualitative feedback from physicians confirms that the generated questions and simulator responses are clinically realistic and useful. The approach demonstrates that structured diagnostic knowledge can guide more effective and interpretable conversational diagnosis.

## Method Summary
The proposed method uses a diagnostic knowledge graph to guide conversational diagnosis through two iterative steps. First, a Hypothesis Generator (HG) predicts top-n disease candidates from dialogue history via multi-label classification. These anchor nodes undergo 3-hop expansion in the diagnostic knowledge graph to retrieve discriminative attributes and competing diseases. Second, a Hypothesis Verifier (HV) generates clarifying questions or final diagnoses based on the extracted subgraph and dialogue context. The system is trained on synthetic dialogues generated using GPT-4o-mini with oracle subgraph grounding, and evaluated using a patient simulator augmented to produce vague symptom descriptions. The knowledge graph is constructed from a Korean clinical textbook with 338 diseases, 1,733 nodes, and 3,935 edges, mapped to MIMIC-IV patient profiles.

## Key Results
- Graph grounding improves diagnostic accuracy (Recall@1 up to 0.250) and efficiency (average dialogue turns reduced to 6.9) over strong baselines
- The system outperforms larger foundation models and baselines that do not use the knowledge graph
- Physicians preferred the augmented simulator in 46.3% of cases vs 24.4% for baseline, citing realistic vagueness in symptom descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypothesis-driven graph grounding improves diagnostic accuracy and efficiency over parametric knowledge alone.
- Mechanism: The Hypothesis Generator (HG) predicts top-n disease candidates from dialogue history via multi-label classification. These anchor nodes undergo 3-hop expansion in the diagnostic knowledge graph: (1) to shared attributes (symptom, cause, risk_factor), (2) to competing diseases sharing those attributes, and (3) to those diseases' attributes. This exposes discriminative cues for clarifying questions (CQs).
- Core assumption: Ground-truth disease appears in the extracted subgraph; patient responses align with profile data.
- Evidence anchors:
  - [abstract]: "improved diagnostic accuracy (Recall@1 up to 0.250) and efficiency (average dialogue turns reduced to 6.9) over strong baselines"
  - [section 5.1, Table 4]: "Sub Recall of 0.559" for n=2, τ=0.005; Diagnosis Recall@1 (0.250) surpasses HG Recall@1 (0.142)
  - [corpus]: Related work on multi-round diagnostic RAG and proactive dialogue supports the inquiry-diagnosis relationship (neighbor titles align), but no direct replication of this specific 3-hop hypothesis mechanism.
- Break condition: If ground-truth disease is not in subgraph (low Sub Recall), CQs cannot verify correct hypothesis; performance degrades.

### Mechanism 2
- Claim: Structured reasoning traces before outputs improve diagnostic decisions.
- Mechanism: Hypothesis Verifier (HV) generates a reasoning trace within `<think/>` tokens, then outputs either a CQ or final diagnosis. The linearized subgraph (e.g., "Disease A causes Symptom B") is injected into the prompt, enabling explicit comparison across candidate diseases.
- Core assumption: Model follows structured reasoning format post-SFT; reasoning trace reflects actual decision process.
- Evidence anchors:
  - [abstract]: "verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached"
  - [section 3.3.2]: "generates an explicit reasoning trace r_t and a response u_D^t"
  - [corpus]: Weak direct evidence; related systems use multi-turn reasoning but without this specific trace+output structure.
- Break condition: If model ignores reasoning prompt or generates incoherent traces, CQ quality drops; diagnosis becomes premature or unfocused.

### Mechanism 3
- Claim: Low-specificity patient simulation creates realistic training and evaluation conditions.
- Mechanism: PatientSim backbone is augmented with prompts enforcing vague symptom descriptions across 5 attributes (location, character, duration, onset, aggravating/relieving factors). Persona traits (language proficiency, recall, etc.) modulate response style.
- Core assumption: Real patients in early encounters describe symptoms vaguely; physician preferences validate realism.
- Evidence anchors:
  - [abstract]: "model vague symptom descriptions"
  - [section 5.3]: Physicians preferred augmented simulator in 46.3% vs 24.4% for baseline; vagueness identified as pivotal factor.
  - [corpus]: Related simulators exist (e.g., MedAgentSim), but specific low-specificity augmentation is novel here.
- Break condition: If vagueness is too extreme (uninformative) or persona conflicts with specificity prompts, patient responses become incoherent or diagnostically useless.

## Foundational Learning

- **Multi-label classification with sigmoid outputs**
  - Why needed here: HG predicts probabilities over 338 diseases; multi-label handles co-occurring conditions.
  - Quick check question: Can you explain why sigmoid (not softmax) is appropriate when a patient may have multiple diseases?

- **Graph expansion and subgraph extraction**
  - Why needed here: 3-hop expansion from anchor diseases retrieves discriminative attributes.
  - Quick check question: What does a 3-hop expansion from a disease node return in this schema?

- **Supervised fine-tuning (SFT) on synthetic dialogues**
  - Why needed here: HV is trained on GPT-4o-mini-generated dialogues conditioned on patient profiles and oracle subgraphs.
  - Quick check question: Why is synthetic data used instead of real clinical dialogues?

## Architecture Onboarding

- **Component map:**
  - Dialogue history → Hypothesis Generator (HG) → top-n diseases → 3-hop expansion → Hypothesis Verifier (HV) → Clarifying Question or Diagnosis

- **Critical path:**
  1. Dialogue history → HG → top-n diseases.
  2. Top-n diseases → 3-hop expansion → G_sub.
  3. G_sub + history → HV → CQ or diagnosis.
  4. If CQ: simulator responds → append to history → repeat from step 1.

- **Design tradeoffs:**
  - n=2 vs higher: Fewer anchors concentrate attention but may miss correct disease if HG Recall is low.
  - τ threshold: Lower retains more competing diseases (larger subgraph) but increases noise.
  - Max turns (50): Prevents infinite loops but may truncate complex cases.
  - Synthetic vs real data: Synthetic enables scalable training but may not capture all real-world variability.

- **Failure signatures:**
  - Low Sub Recall: Ground-truth disease excluded from subgraph; CQs cannot verify correct hypothesis.
  - Premature termination: HV outputs diagnosis before sufficient evidence (e.g., CoD baseline with high confidence threshold).
  - Vague patient responses: If simulator produces uninformative answers, dialogue stalls or misdiagnoses.

- **First 3 experiments:**
  1. **Ablate KG**: Run GPT-4.1-mini without knowledge graph; compare Recall@1 and turns to baseline (expected drop from 0.243 to ~0.142).
  2. **Vary HG hyperparameters**: Test n∈{1,2,3,4} and τ∈{0.003,0.005,0.01,0.015}; plot Diagnosis Recall vs Sub Recall to find optimal balance.
  3. **Simulator realism check**: Generate dialogues with baseline vs specificity-augmented simulator; run physician preference evaluation on 20+ scenarios to replicate 46.3% vs 24.4% preference finding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the system's diagnostic accuracy and efficiency compare when evaluated in real-world clinical encounters versus the specific-augmented patient simulator and physician role-plays?
- Basis in paper: [explicit] Section 7 states the authors used a patient simulator rather than real-world clinical encounters due to ethical considerations, relying instead on physician role-plays for validation.
- Why unresolved: The authors explicitly list the use of a simulator rather than real patients as a primary limitation inherent to the experimental scope.
- What evidence would resolve it: Results from clinical trials where the system interacts with actual patients in outpatient settings, compared against the simulator-based benchmarks.

### Open Question 2
- Question: Can the framework maintain its hypothesis verification efficiency when the diagnostic knowledge graph is expanded to include rare diseases and conditions currently outside the "standard medical textbook" schemas?
- Basis in paper: [explicit] Section 7 notes the diagnostic scope is bounded by the current knowledge graph, which excludes rare diseases, though the authors suggest the framework can be adopted "as-is" with an expanded graph.
- Why unresolved: It is unclear if the hypothesis-driven subgraph extraction will remain efficient or become noisy when the node count increases significantly to cover rare pathologies.
- What evidence would resolve it: Experiments running the current framework on an expanded knowledge graph containing rare disease nodes, measuring Recall@k and average turns.

### Open Question 3
- Question: How can the framework be adapted to handle hierarchical relationships between diagnoses to prevent penalization of correct but generalized predictions (e.g., predicting "Arrhythmia" when the ground truth is "Atrial Flutter")?
- Basis in paper: [explicit] Appendix I identifies "Label Granularity" as a primary error pattern where pathophysiologically sound reasoning is penalized because the model identifies a parent category while the ground truth is a specific subtype.
- Why unresolved: The current setup treats distinct nodes as separate classes, failing to reward diagnostic reasoning that narrows down to the correct family of diseases.
- What evidence would resolve it: Performance evaluation using hierarchical metrics (e.g., hierarchical F1-score) or a modification of the KG to allow "soft" matching between superclasses and subclasses.

## Limitations
- The knowledge graph is constructed from a Korean clinical textbook, limiting generalizability to other clinical contexts or languages
- Semantic mapping from MIMIC-IV to the knowledge graph achieves only 28% retention, potentially excluding relevant patient profiles
- The synthetic training data may not capture the full complexity and variability of real clinical dialogues

## Confidence
- **High Confidence**: The mechanism of using structured knowledge graphs to ground diagnostic hypotheses is well-supported by results showing improved Recall@1 (0.250) and reduced dialogue turns (6.9) compared to baselines. The physician preference for the augmented simulator (46.3% vs 24.4%) provides strong external validation.
- **Medium Confidence**: The effectiveness of the 3-hop subgraph expansion mechanism is supported by quantitative results (Sub Recall of 0.559 for n=2, τ=0.005) but relies on assumptions about the quality of the underlying knowledge graph and the HG's ability to include the ground-truth disease.
- **Low Confidence**: The generalization of results to different clinical domains or languages is uncertain due to the specific knowledge graph construction and the limited diversity in physician evaluation.

## Next Checks
1. **Generalization Test**: Evaluate the system on a different clinical dataset (e.g., MedQA or PubMedQA) with a distinct knowledge graph to assess cross-domain performance and identify potential overfitting to the Korean clinical context.

2. **HG Hyperparameter Sensitivity**: Systematically vary HG parameters (n and τ) across a wider range and measure the trade-off between Sub Recall and Diagnosis Recall to optimize the balance between coverage and precision.

3. **Real-World Dialogue Integration**: Incorporate a small set of real clinical dialogues into the training data to assess whether synthetic-to-real transfer improves diagnostic accuracy and reduces the gap between synthetic and real-world performance.