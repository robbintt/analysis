---
ver: rpa2
title: Optimal sparse phase retrieval via a quasi-Bayesian approach
arxiv_id: '2504.09509'
source_url: https://arxiv.org/abs/2504.09509
tags:
- phase
- sparse
- retrieval
- sparsity
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sparse phase retrieval, where
  a signal must be reconstructed using only the magnitude of its transformation without
  phase information. The method introduces a novel sparse quasi-Bayesian approach
  using a scaled Student's t-distribution as a continuous shrinkage prior to enforce
  sparsity, and analyzes it using the PAC-Bayesian inequality framework.
---

# Optimal sparse phase retrieval via a quasi-Bayesian approach

## Quick Facts
- arXiv ID: 2504.09509
- Source URL: https://arxiv.org/abs/2504.09509
- Authors: The Tien Mai
- Reference count: 40
- Primary result: Introduces a sparse quasi-Bayesian approach using scaled Student's t-distribution prior achieving minimax-optimal convergence rates under sub-exponential noise

## Executive Summary
This paper introduces a sparse quasi-Bayesian approach for phase retrieval that uses a scaled Student's t-distribution as a continuous shrinkage prior to enforce sparsity. The method employs PAC-Bayesian analysis to establish non-asymptotic convergence guarantees and demonstrates minimax-optimal rates matching state-of-the-art frequentist methods. The approach uses an efficient Langevin Monte Carlo sampling algorithm for computational feasibility and shows comparable performance to existing methods in numerical experiments.

## Method Summary
The method reconstructs a sparse signal from magnitude-only measurements using a quasi-Bayesian framework. It employs a scaled Student's t-distribution prior to enforce sparsity through continuous shrinkage, and constructs a Gibbs posterior using the empirical risk instead of a true likelihood. The algorithm samples from this posterior using unadjusted Langevin Monte Carlo, which requires computing gradients of the log-posterior. The final estimate is obtained as the posterior mean, with sign ambiguity resolved by selecting the minimum distance to the true signal.

## Key Results
- The quasi-Bayesian estimator achieves minimax-optimal convergence rate O(σ²s* log(p/s*)/m) under sub-exponential noise
- The scaled Student's t-distribution prior effectively enforces sparsity through continuous shrinkage without discrete variable selection
- PAC-Bayesian analysis provides non-asymptotic upper bounds on estimation error with explicit constants
- Numerical experiments demonstrate comparable performance to existing MD baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The scaled Student's t-distribution prior enforces sparsity by concentrating most probability mass near zero while maintaining heavy tails that permit large coefficients for true signal components.
- Mechanism: The prior π(θ) ∝ ∏(ς² + θᵢ²)⁻² approximates a scaled Student's t with 3 degrees of freedom. When ς is small, most components concentrate near zero, but the heavy-tailed nature ensures that truly non-zero components are not over-shrunk. This creates a continuous shrinkage effect without discrete variable selection.
- Core assumption: The true signal θ* is sparse (s* ≪ p), and signal components have a clear separation between zero and non-zero magnitudes.
- Evidence anchors:
  - [Section 2.2]: "By choosing a very small value for ς, most entries of ςS are concentrated near zero. However, the heavy-tailed nature of the distribution ensures that a few components deviate significantly from zero."
  - [Section 4.3.4]: Experiments show ς < 1 yields good performance, with ς = 0.1 optimal.
  - [corpus]: Related shrinkage priors (Horseshoe) in corpus confirm continuous shrinkage as established sparsity mechanism.
- Break condition: Fails when signal is not truly sparse (s* approaches p) or when coefficients have similar magnitudes without clear zero/non-zero separation.

### Mechanism 2
- Claim: The Gibbs posterior construction enables quasi-Bayesian inference under general sub-exponential noise, reducing to standard Bayesian posterior when noise is Gaussian.
- Mechanism: The quasi-posterior ρ̂λ(θ) ∝ exp[-λr(θ)]π(θ) uses negative empirical risk scaled by λ instead of a true likelihood. When noise is N(0,σ²) and λ = 2m/σ², this recovers standard Bayesian posterior. The framework extends to sub-exponential noise, which is practically motivated for phase retrieval applications like optics.
- Core assumption: Noise is independent, centered, and sub-exponential (satisfies E|εⱼ|ᵏ ≤ σ²k!2ξᵏ⁻² for k ≥ 3).
- Evidence anchors:
  - [Section 2.2]: "When the noise variable εⱼ follows a Gaussian distribution N(0,σ²), setting λ = 2m/σ² transforms our Gibbs posterior into a standard Bayesian posterior."
  - [Section 3.1]: Assumption 2 specifies sub-exponential noise conditions encompassing Gaussian and bounded noise.
  - [corpus]: Weak direct corpus support for quasi-Bayesian phase retrieval; this appears novel to this paper.
- Break condition: Fails if noise has infinite higher moments or exhibits strong correlation across samples.

### Mechanism 3
- Claim: PAC-Bayesian bounds provide non-asymptotic guarantees that the quasi-posterior mean achieves minimax-optimal convergence rate O(σ²s* log(p/s*)/m).
- Mechanism: The analysis combines Bernstein-type concentration inequalities with Donsker-Varadhan variational representation to bound estimation error. The key inequality balances empirical risk against KL divergence from prior, yielding error bounds that adapt to unknown sparsity s* without requiring it as input.
- Core assumption: Sensing vectors Aⱼ ∼ N(0,Ip) satisfy the restricted eigenvalue-like condition E[(A⊤s)(A⊤t)] ≥ κ₀‖s‖‖t‖.
- Evidence anchors:
  - [Theorem 1]: "Eθ∼ρ̂λ[‖θ-θ*‖₂‖θ+θ*‖₂] ≤ C σ²s* log(mp/s*) + log(2/δ)/m" with probability ≥ 1-δ.
  - [Remark 1]: "This rate is known to be minimax-optimal as a lower bound of the same order was established in [27]."
  - [corpus]: No direct corpus evidence for PAC-Bayesian phase retrieval; this is the first such guarantee per the paper.
- Break condition: Violated if sensing vectors are highly correlated or if the restricted eigenvalue condition fails (κ₀ approaches zero).

## Foundational Learning

- Concept: **Gibbs posteriors / quasi-Bayesian inference**
  - Why needed here: This is the core probabilistic framework bridging Bayesian and frequentist approaches. Understanding why the exponential of negative risk serves as a generalized likelihood is essential.
  - Quick check question: Can you explain why setting λ = 2m/σ² recovers standard Bayesian inference under Gaussian noise?

- Concept: **Sub-exponential distributions and concentration inequalities**
  - Why needed here: The theoretical guarantees rely on Bernstein-type bounds for sub-exponential random variables. Understanding moment conditions is critical for assessing when results apply.
  - Quick check question: Does a Pareto distribution with infinite variance satisfy the sub-exponential condition in Assumption 2?

- Concept: **Langevin Monte Carlo (LMC) sampling**
  - Why needed here: The computational implementation uses gradient-based MCMC. Understanding the unadjusted LMC update θₖ₊₁ = θₖ - γ∇log ρ̂λ(θₖ) + √(2γ)Nₖ is necessary for implementation.
  - Quick check question: Why does LMC require computing ∇log ρ̂λ, and what role does the step-size γ play in convergence?

## Architecture Onboarding

- Component map:
  - Prior module -> Gibbs posterior module -> Gradient oracle -> Sampling engine -> Point estimator
  - Data {(Aⱼ, yⱼ)}ₘ -> Initialize θ₀ -> Set λ, ς -> Iterate LMC updates -> Discard burn-in -> Compute posterior mean -> Resolve sign ambiguity via min{‖θ̂-θ*‖, ‖θ̂+θ*‖}

- Critical path: Data {(Aⱼ, yⱼ)}ₘ → Initialize θ₀ → Set λ, ς → Iterate LMC updates → Discard burn-in → Compute posterior mean → Resolve sign ambiguity via min{‖θ̂-θ*‖, ‖θ̂+θ*‖}

- Design tradeoffs:
  - ς: Smaller values enforce stronger sparsity but may over-shrink true signals; paper recommends ς = 0.1
  - λ: Controls influence of empirical risk; λ = 4m worked well, but λ too large degrades performance
  - LMC vs MALA: LMC is faster per iteration but less accurate; MALA has acceptance step requiring 0.5 acceptance rate tuning
  - Sample size m vs dimension p: Theory requires m ≫ s* log(p/s*); experiments used m ∈ [100, 2000] with p ∈ [100, 500]

- Failure signatures:
  - Estimator stuck near zero: ς too small or λ too small; prior overwhelms likelihood
  - High variance across runs: γ too large in LMC; reduce step-size or use MALA
  - No convergence to true signal: m insufficient for (p, s*) combination; or sensing vectors violate Assumption 3
  - Sign flip instability: Inherent to problem; always report min{‖θ̂-θ*‖, ‖θ̂+θ*‖}

- First 3 experiments:
  1. **Validation on synthetic data with known ground truth**: Generate θ* with s* = 10, p = 100, m = 500, σ = 1. Compute minimum relative error across 100 trials. Compare LMC, MALA, and MD baseline.
  2. **Parameter sensitivity sweep**: Vary ς ∈ {0.0001, 0.01, 0.1, 1, 10} and λ ∈ {m/25, 2m/25, 4m, 100m, 400m} with fixed m = 200, p = 100. Identify stable operating regions.
  3. **Noise robustness test**: Vary noise-to-signal ratio σ/‖θ*‖ ∈ {0.5, 1, 2, 5, 10} with m = 500, p = 100, s* = 10. Compare performance degradation curves against MD baseline to verify theoretical sub-exponential noise handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the quasi-Bayesian framework be extended to enforce structured sparsity or hierarchical network structures while preserving minimax-optimal convergence rates?
- Basis in paper: [explicit] The conclusion states that extending the framework to "handle more general structured sparsity or hierarchical network structure" is a future research avenue.
- Why unresolved: The current theoretical analysis and scaled Student's t-distribution prior are designed exclusively for standard coordinate-wise sparsity.
- What evidence would resolve it: Derivation of theoretical guarantees for structured models and empirical demonstration of improved recovery on group-sparse or graph-structured datasets.

### Open Question 2
- Question: Can adaptive techniques, such as empirical Bayes, be utilized to select hyper-parameters λ and ς automatically without sacrificing theoretical guarantees?
- Basis in paper: [explicit] The conclusion identifies "the development of adaptive or data-driven choices for hyper-parameters" as a key open question.
- Why unresolved: The current method relies on fixed theoretical values or manual tuning for the inverse temperature and prior scale parameters.
- What evidence would resolve it: An algorithm that jointly estimates these parameters and a theoretical proof showing the estimator still achieves the minimax rate.

### Open Question 3
- Question: How does the method perform on real-world phase retrieval tasks, such as optical imaging, compared to the synthetic simulations presented?
- Basis in paper: [explicit] The conclusion notes that applying the methodology to "real-world tasks... would further validate its practical utility."
- Why unresolved: Experiments are currently restricted to simulated Gaussian measurements and the MNIST dataset, which lack the specific physical constraints of real optical systems.
- What evidence would resolve it: Successful image reconstruction from experimental optical data, demonstrating the method's robustness to model-specific noise and distortions.

## Limitations

- Hyperparameter specification gaps: The paper omits explicit values for critical LMC step size γ and initialization strategy, preventing faithful reproduction without extensive parameter tuning
- Computational complexity analysis: While demonstrating performance at m=2000, the paper lacks analysis of scaling with very large p (e.g., p > 10,000) or high sparsity levels
- Theoretical extension limitations: The sub-exponential noise assumption and restricted eigenvalue conditions may not hold for all practical sensing matrix distributions

## Confidence

**High confidence** in the theoretical convergence rate O(σ²s*log(p/s*)/m) and its minimax-optimality. The PAC-Bayesian framework and proof techniques are well-established, and the paper provides rigorous non-asymptotic bounds with explicit constants.

**Medium confidence** in the practical superiority claims. While experiments show comparable performance to MD baselines, the limited comparison set (only MD) and restricted parameter ranges (p ≤ 500) warrant caution about generalizability.

**Low confidence** in reproducibility without additional specification. Critical algorithmic parameters (γ, initialization) are underspecified, and the paper lacks ablation studies showing robustness to hyperparameter choices.

## Next Checks

1. **Sensitivity analysis on LMC step size**: Systematically vary γ across orders of magnitude (10⁻⁵ to 10⁻²) for fixed (p=100, m=500, s*=10) to identify stable operating regions and quantify performance degradation when γ is poorly chosen.

2. **Scalability benchmark**: Test the method on high-dimensional problems (p = 10,000) with varying sparsity levels (s* = 10, 100, 1000) to empirically verify computational scaling and identify practical limits of the approach.

3. **Noise robustness evaluation**: Generate synthetic datasets with heavy-tailed noise distributions (Cauchy, Pareto) that violate the sub-exponential assumption to assess whether the method maintains reasonable performance or breaks down completely.