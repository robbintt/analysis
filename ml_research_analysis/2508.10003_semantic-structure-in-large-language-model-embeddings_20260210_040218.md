---
ver: rpa2
title: Semantic Structure in Large Language Model Embeddings
arxiv_id: '2508.10003'
source_url: https://arxiv.org/abs/2508.10003
tags:
- semantic
- feature
- vectors
- features
- survey
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that semantic features in LLM embedding
  matrices are highly correlated, forming a low-dimensional structure that closely
  mirrors human semantic ratings. By projecting word vectors onto feature directions
  defined by antonym pairs, the authors show that these projections correlate strongly
  with human survey responses.
---

# Semantic Structure in Large Language Model Embeddings

## Quick Facts
- arXiv ID: 2508.10003
- Source URL: https://arxiv.org/abs/2508.10003
- Reference count: 40
- One-line primary result: Semantic features in LLM embeddings are highly correlated, forming a low-dimensional structure that closely mirrors human semantic ratings.

## Executive Summary
This paper demonstrates that semantic features in LLM embedding matrices are not randomly distributed but form a highly correlated, low-dimensional structure that mirrors human semantic judgments. By projecting word vectors onto feature directions defined by antonym pairs, the authors show strong correlations (0.3-0.7) with human survey responses. Principal components analysis reveals these 28 semantic features effectively reduce to a 3-dimensional subspace resembling the Evaluation, Potency, and Activity dimensions from psychological research. The study also shows that interventions on one semantic feature cause predictable off-target effects on other features proportional to their cosine similarity, indicating that feature alignment reflects meaningful semantic relations rather than random superposition.

## Method Summary
The paper extracts semantic feature directions from LLM embedding matrices by averaging normalized differences between antonym pairs. For each of 28 semantic axes, 10 antonym pairs are used to compute a feature direction vector. Token associations are measured via cosine similarity between token embeddings and feature directions, then validated against human ratings from a survey of 1,750 respondents rating 360 words (filtered to 301 single-token words). PCA is applied to the 28-feature projection matrix to analyze dimensionality reduction. Token-level interventions are performed by shifting embeddings along feature directions (±0.35 × ||w||) and measuring off-target effects on other semantic features.

## Key Results
- Semantic feature projections correlate 0.3-0.7 with human ratings across Gemma-3, Llama-3, and Qwen-2.5 models
- PCA reveals 28 features reduce to 3 dimensions capturing 40-55% of variance, resembling Evaluation-Potency-Activity structure
- Off-target intervention effects correlate strongly (p < 0.001) with feature pair cosine similarity across model sizes
- Whitening embeddings reduces off-target effects but decreases human correlation by ~20%, suggesting removal of meaningful structure

## Why This Works (Mechanism)

### Mechanism 1: Feature Directions as Antonym Vector Differences
Semantic features are extracted by averaging normalized differences between antonym pairs. This produces a direction vector capturing the semantic axis, with token associations measured via cosine similarity. Antonyms must consistently occupy opposite positions along a single direction for this to work effectively.

### Mechanism 2: Low-Dimensional Semantic Subspace (EPA Structure)
PCA on the 28-feature projection matrix reveals the first 3 components capture 40-55% of variance (vs. ~3.6% per component under orthogonality). Component loadings show Component 1 corresponds to Evaluation (good-bad), Component 2 to "vibrancy" related to Potency, and Component 3 to Activity.

### Mechanism 3: Cosine Similarity Predicts Off-Target Intervention Effects
When a token embedding is shifted along feature direction d_f by adding c·d_f, the shift component along any other feature direction d_g is proportional to cos(d_f, d_g). This geometrically aligned component produces predictable changes in model outputs on the off-target feature.

## Foundational Learning

- **Distributed Representations**: Features are directions (not individual dimensions) in embedding space, with semantic information encoded in geometric relationships between these directions. Quick check: If all features were orthogonal in a 4096-dim space, how many could theoretically be represented? (Answer: Up to 4096 exactly orthogonal directions.)

- **Cosine Similarity**: Used to quantify both (a) how aligned a token is with a feature direction and (b) how aligned two feature directions are. Quick check: Two feature vectors have cosine similarity 0.3. If you shift a token by 1.0 units along feature A, how much does it move along feature B's direction? (Answer: 0.3 units.)

- **Principal Component Analysis (PCA)**: Used to demonstrate that 28 semantic features reduce to a low-dimensional subspace. Quick check: If 28 features were perfectly uncorrelated, what fraction of variance would each of the first 3 PCs explain? (Answer: ~10.7% total, or ~3.6% each.)

## Architecture Onboarding

- **Component map**: Embedding Matrix -> Transformer Stack -> Unembedding Matrix
- **Critical path**: 1. Define semantic feature using 10 antonym pairs, 2. Compute feature direction via averaged antonym differences, 3. Validate with human rating correlation, 4. Analyze structure with PCA, 5. Intervene by shifting token embeddings, 6. Measure off-target effects
- **Design tradeoffs**: Embeddings are token-specific and easier to interpret vs. activations; whitening reduces off-target effects but decreases human correlation; intervention magnitude (±0.35 × ||w||) balances effect size and coherence
- **Failure signatures**: Low correlation (< 0.2) with human ratings suggests feature extraction failed; PCA showing uniform variance (~3.6% per component) suggests features are near-orthogonal; off-target effects uncorrelated with cosine similarity suggests geometric structure doesn't causally influence behavior
- **First 3 experiments**: 1. Extract good-bad direction using 10 antonym pairs, correlate with human ratings (target r > 0.4), 2. Shift "chair" embedding along good-bad direction, verify directional sentiment effect, 3. For "chair" → "beautiful" shift, measure off-target effects across 5 features and compute correlation with cosine similarity

## Open Questions the Paper Calls Out
None

## Limitations
- Feature extraction methodology assumes antonyms consistently occupy opposite positions along a single axis, which may not hold for polysemous words
- Human rating dataset availability is unclear, blocking exact reproduction of correlation results
- Cross-lingual generalization is not addressed, leaving open whether findings apply to multilingual models

## Confidence
- **High Confidence**: Feature extraction via antonym pairs is theoretically sound and supported by prior work; off-target effects proportional to cosine similarity is geometrically inevitable; PCA reduction to 3 dimensions is a standard technique
- **Medium Confidence**: EPA structure resemblance relies on single survey dataset comparison; strong correlation between feature cosine similarity and off-target effects may depend on intervention scale
- **Low Confidence**: Potential confounding factors (tokenization, polysemy, embedding dimensionality) are not addressed; absence of activation analysis limits claims about dynamic semantic processing

## Next Checks
1. **Antonym Pair Robustness Test**: Vary number of antonym pairs (1, 3, 5, 10) for 3-5 features and measure stability of extracted directions via cosine similarity across different pair subsets
2. **Whitening Impact Validation**: Compare PCA analysis on raw vs. whitened embeddings, measure change in human correlation (expected ~20% drop) and off-target effect magnitude
3. **Intervention Scale Sensitivity**: Apply interventions at multiple scales (±0.1, ±0.35, ±0.7 × ||w||) for one feature, measure linearity of off-target effects vs. cosine similarity and degradation in model output coherence