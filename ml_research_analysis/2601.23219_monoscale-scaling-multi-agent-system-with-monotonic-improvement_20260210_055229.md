---
ver: rpa2
title: 'MonoScale: Scaling Multi-Agent System with Monotonic Improvement'
arxiv_id: '2601.23219'
source_url: https://arxiv.org/abs/2601.23219
tags:
- agent
- agents
- system
- router
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MonoScale tackles the cold-start misrouting problem in multi-agent
  systems during agent expansion. It proactively synthesizes agent-conditioned warm-up
  tasks to probe new agents' capabilities and failure modes, then distills evidence
  into natural-language routing memories.
---

# MonoScale: Scaling Multi-Agent System with Monotonic Improvement

## Quick Facts
- arXiv ID: 2601.23219
- Source URL: https://arxiv.org/abs/2601.23219
- Authors: Shuai Shao; Yixiang Liu; Bingwei Lu; Weinan Zhang
- Reference count: 40
- Key outcome: MonoScale enables smaller open-weight routers to outperform proprietary baselines during multi-agent system expansion by proactively familiarizing routers with new agents through warm-up tasks and memory-guided routing.

## Executive Summary
MonoScale addresses the cold-start misrouting problem in multi-agent systems during agent expansion. It proactively synthesizes agent-conditioned warm-up tasks to probe new agents' capabilities and failure modes, then distills evidence into natural-language routing memories. These memories guide future routing decisions while preventing performance collapse. Experiments on GAIA and HLE benchmarks show that MonoScale enables stable improvements as agent pools grow from 3 to 10 agents, while naive scale-up exhibits performance degradation.

## Method Summary
MonoScale introduces a three-phase approach to multi-agent system expansion. First, it synthesizes agent-conditioned warm-up tasks using a planner–executor–validator loop that generates tasks based on new agent cards. Second, it executes these tasks in parallel, capturing both successful and failed interactions. Third, it distills these interactions into structured natural-language memory entries containing task patterns, actionable rules, and agent-specific guidance. The router uses trust-region memory updates with conservative fallback to ensure monotonic non-decreasing performance across expansion stages, even in noisy agent pools with malfunctioning workers.

## Key Results
- Stable performance improvement from 3 to 10 agents on GAIA and HLE benchmarks
- Smaller open-weight router outperforms proprietary baselines with larger model sizes
- Robustness to malfunctioning agents that break tools or provide incorrect outputs
- Monotonic performance guarantee under non-interfering expansion assumption

## Why This Works (Mechanism)

### Mechanism 1
Agent-conditioned warm-up tasks reduce cold-start misrouting by proactively exposing capability boundaries and failure patterns. The task synthesizer uses agent cards to condition a planner–executor–validator loop that generates representative tasks. The core assumption is that warm-up task distribution sufficiently approximates deployment distribution for new-agent-relevant contexts. Break condition: if warm-up tasks under-cover the true deployment distribution, the router may learn incomplete boundaries.

### Mechanism 2
Distilling both successful and failed interactions into structured natural-language memory enables auditable, rollbackable routing improvements. The system abstracts execution traces into JSON memory entries with task patterns, actionable rules, and agent-specific guidance. Core assumption: natural-language memory can be reliably retrieved and applied at inference time by the frozen router LLM. Break condition: if memory entries become contradictory or too large for the token budget, retrieval quality degrades.

### Mechanism 3
Trust-region memory updates with conservative fallback yield monotonic non-decreasing performance across expansion stages. The policy induced by memory is evaluated via TRPO-style surrogate objective with KL constraint. Updates are constrained by expected KL divergence ≤ δ, with conservative fallback ensuring feasibility. Core assumption: non-interfering expansion where adding agent a_k doesn't change rewards for plans using only existing agents. Break condition: if the non-interfering assumption is violated, the guarantee doesn't hold.

## Foundational Learning

- Concept: Contextual Bandits
  - Why needed here: Sequential augmentation is formalized as a sequence of contextual bandit problems indexed by expansion stage k, with expanding action spaces Y_k.
  - Quick check question: Can you explain why the conservative lift π↑ preserves performance when the action space expands?

- Concept: Trust-Region Policy Optimization (TRPO)
  - Why needed here: The surrogate objective L_m_t(m) and KL constraint δ directly adapt TRPO to memory-space updates.
  - Quick check question: What is the role of the KL constraint in preventing performance collapse during memory updates?

- Concept: Multi-Agent System Routing
  - Why needed here: Understanding the router's three phases (planning, coordinating, answering) is prerequisite to debugging routing failures and designing memory schemas.
  - Quick check question: In the coordinating phase, how does the router select among worker agents given their descriptions and tools?

## Architecture Onboarding

- Component map:
  Router (frozen LLM) -> Task Synthesizer -> Worker Agents -> Memory System

- Critical path:
  1. New agent a_k arrives → agent card created
  2. Task synthesizer generates ~50 warm-up tasks conditioned on agent cards
  3. Router executes tasks (4 parallel samples per task); failed tasks filtered
  4. Success/failure traces distilled into memory candidates
  5. Trust-region optimization selects memory update m_{t+1} (or falls back to conservative m^↑_t)
  6. Updated memory injected into router prompts for deployment

- Design tradeoffs:
  - Warm-up task quantity (~50) vs. compute cost: More tasks improve coverage but increase latency
  - Memory token budget K vs. retention: Larger K allows more principles but risks retrieval noise
  - KL threshold δ vs. adaptivity: Tighter δ ensures stability but may block beneficial updates

- Failure signatures:
  - Cold-start misjudgment: Router assigns tasks to new agent based on incomplete card description; agent fails at capability boundary
  - Brittle workflow links: Specialist agent fails on interface constraint; context lost during handover due to schema mismatch
  - Confirmation bias in verification: Router defaults to text-centric search instead of multimodal analysis; repeats incorrect entity from snippets

- First 3 experiments:
  1. Replicate the clean pool GAIA scaling experiment (N=3→5→7→10) with Qwen3-30B-A3B router; compare naive scale-up vs. MonoScale with memory
  2. Inject malfunctioning agents (broken tools, exaggerated prompts) and measure whether MonoScale maintains stable performance while strong-router baselines collapse
  3. Ablate the failure-trace distillation component: train with only success traces and quantify the performance gap on tasks requiring negative constraints

## Open Questions the Paper Calls Out
None

## Limitations

- The theoretical guarantee of monotonic improvement depends on the non-interfering expansion assumption, which may not hold in real-world MAS with shared state or cross-agent dependencies.
- Warm-up task synthesis coverage and diversity haven't been quantitatively analyzed to verify the claimed sufficiency of ~50 tasks.
- The router's frozen LLM architecture trades potential performance gains from fine-tuning for deployment stability and interpretability, but this tradeoff hasn't been benchmarked.

## Confidence

**High Confidence**: The experimental results showing MonoScale's stable performance across agent pool expansions (N=3→5→7→10) are well-supported by the GAIA and HLE benchmark data.

**Medium Confidence**: The theoretical proof of monotonic improvement under stated assumptions is mathematically sound, but practical relevance depends heavily on Assumption B.2 holding in deployment scenarios.

**Low Confidence**: The scalability claims beyond 10 agents are speculative, as experiments only test up to N=10.

## Next Checks

1. **Non-Interfering Assumption Stress Test**: Design experiments where new agents create shared state dependencies or modify system-wide reward structures, then measure whether MonoScale's monotonic guarantee breaks down compared to baseline approaches.

2. **Warm-up Task Coverage Analysis**: Implement task diversity metrics and coverage analysis to quantify whether the ~50 synthesized tasks adequately span the deployment distribution. Test whether increasing warm-up task count improves routing accuracy.

3. **Memory Scaling Boundary**: Systematically scale the agent pool beyond 10 agents while monitoring memory retrieval latency and accuracy. Identify the point where memory bloat or contradictory entries degrade routing performance, and test whether pruning or hierarchical memory structures can extend the scaling boundary.