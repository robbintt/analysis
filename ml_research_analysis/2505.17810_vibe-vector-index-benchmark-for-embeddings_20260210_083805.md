---
ver: rpa2
title: 'VIBE: Vector Index Benchmark for Embeddings'
arxiv_id: '2505.17810'
source_url: https://arxiv.org/abs/2505.17810
tags:
- datasets
- search
- algorithms
- methods
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VIBE, a comprehensive benchmark suite for
  evaluating approximate nearest neighbor (ANN) search algorithms on modern embedding
  datasets. VIBE addresses the need for up-to-date benchmarks that reflect current
  applications like retrieval-augmented generation (RAG) and multimodal search.
---

# VIBE: Vector Index Benchmark for Embeddings

## Quick Facts
- arXiv ID: 2505.17810
- Source URL: https://arxiv.org/abs/2505.17810
- Authors: Elias Jääsaari; Ville Hyvönen; Matteo Ceccarello; Teemu Roos; Martin Aumüller
- Reference count: 40
- Primary result: Introduces VIBE, a comprehensive benchmark suite for evaluating ANN search algorithms on modern embedding datasets

## Executive Summary
VIBE addresses the critical need for modern benchmarks that reflect current applications like retrieval-augmented generation (RAG) and multimodal search. The benchmark suite evaluates 21 state-of-the-art ANN implementations across 18 datasets, including both in-distribution and out-of-distribution scenarios. Key findings reveal that graph-based methods (SymphonyQG, Glass, NGT-QG) and clustering-based methods (LoRANN, ScaNN, IVF-PQ) outperform other approaches, with specialized OOD methods excelling in approximate attention computation tasks.

## Method Summary
VIBE creates 12 in-distribution datasets using popular embedding models including Nomic, DistilRoBERTa, and CLIP, plus 6 out-of-distribution datasets for multimodal search and approximate attention computation. The benchmark evaluates algorithms across four categories: graph-based, clustering-based, tree-based, and hashing-based methods. Performance metrics include throughput at 95% recall, GPU support capabilities, and binary embedding support, with throughputs reaching up to 10^5 QPS for optimized implementations.

## Key Results
- Graph-based methods (SymphonyQG, Glass, NGT-QG) achieve highest throughput at 95% recall on in-distribution datasets
- Clustering-based methods (LoRANN, ScaNN, IVF-PQ) show strong performance across multiple scenarios
- Regular ANN methods degrade significantly on out-of-distribution queries, while specialized OOD methods (RoarGraph, MLANN, LoRANN) excel in approximate attention computation tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of modern embedding scenarios and rigorous evaluation methodology. By including both in-distribution and out-of-distribution datasets, VIBE captures real-world performance variations that single-distribution benchmarks miss. The evaluation of 21 implementations across multiple algorithmic approaches provides robust comparative insights, while the inclusion of GPU support and binary embeddings ensures relevance to current deployment requirements.

## Foundational Learning
- **Approximate Nearest Neighbor Search**: Why needed - Enables efficient similarity search in high-dimensional spaces; Quick check - Verify understanding of trade-offs between recall and performance
- **Embedding Models**: Why needed - Modern applications rely on learned vector representations; Quick check - Understand differences between text, image, and multimodal embeddings
- **Graph-based ANN Methods**: Why needed - Provide strong performance for many embedding scenarios; Quick check - Know how proximity graphs enable efficient search
- **Clustering-based ANN Methods**: Why needed - Offer different trade-offs for different dataset characteristics; Quick check - Understand IVF and product quantization concepts
- **Out-of-distribution Evaluation**: Why needed - Real-world queries often differ from training data; Quick check - Recognize importance of testing algorithm robustness

## Architecture Onboarding

**Component Map**: Embedding Generation -> Dataset Creation -> Algorithm Implementation -> Performance Evaluation -> Results Analysis

**Critical Path**: Embedding generation creates datasets → ANN algorithms process queries → Throughput and recall metrics computed → Comparative analysis across methods

**Design Tradeoffs**: VIBE prioritizes comprehensive coverage over specialization, including both in-distribution and out-of-distribution datasets. This broad approach may miss niche optimizations but provides more generalizable insights. The inclusion of GPU support adds complexity but reflects modern deployment requirements.

**Failure Signatures**: Algorithm performance degradation on out-of-distribution queries indicates poor generalization. Low throughput despite high recall suggests implementation inefficiencies. Binary embedding performance issues may reveal algorithm limitations with quantized data.

**Three First Experiments**:
1. Run SymphonyQG and Glass algorithms on in-distribution text embedding datasets to verify claimed high throughput
2. Test LoRANN performance on out-of-distribution multimodal datasets to confirm specialized OOD capabilities
3. Compare GPU vs CPU implementations for the same algorithm to quantify performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on specific embedding modalities and datasets, potentially limiting generalizability to other domains
- Relies on embeddings from specific models (Nomic, DistilRoBERTa, CLIP) which may not represent all embedding spaces
- Out-of-distribution datasets are limited in number (6 datasets) and may not capture full diversity of real-world multimodal search scenarios

## Confidence

**High confidence**: Comparative performance rankings of ANN algorithms across tested datasets given systematic evaluation of 21 implementations

**Medium confidence**: Generalization of findings to broader embedding spaces and applications beyond those tested, particularly for specialized domains

**Medium confidence**: Scalability claims regarding throughput (up to 10^5 QPS) and GPU support, which would benefit from additional real-world deployment testing

## Next Checks

1. Validate VIBE findings on additional embedding models and domains (e.g., biomedical embeddings, specialized scientific embeddings) to assess generalizability beyond current model coverage

2. Conduct long-term performance evaluations of top-ranked algorithms (SymphonyQG, Glass, LoRANN) under production workloads with dynamic data insertion and deletion patterns

3. Test the benchmark's GPU implementations across different hardware configurations and memory constraints to verify claimed throughput scalability across diverse infrastructure environments