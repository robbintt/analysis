---
ver: rpa2
title: 'Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon
  Planning'
arxiv_id: '2508.03018'
source_url: https://arxiv.org/abs/2508.03018
tags:
- reasoning
- planning
- action
- agent
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training reasoning language
  models for long-horizon, sparse-reward planning tasks in interactive environments.
  The authors propose BPO, a three-stage framework (bootstrapping, extrapolation,
  refinement) that uses a data curation flywheel instead of conventional reinforcement
  learning.
---

# Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning

## Quick Facts
- arXiv ID: 2508.03018
- Source URL: https://arxiv.org/abs/2508.03018
- Reference count: 21
- Primary result: Achieves 88.16% average success rate vs 81.83% for next best method, using 112 tokens vs 620-763 for baselines

## Executive Summary
This paper introduces BPO (Beyond Policy Optimization), a three-stage framework that addresses the challenge of training reasoning language models for long-horizon, sparse-reward planning tasks. Unlike conventional reinforcement learning approaches, BPO employs a data curation flywheel that separates verbose reasoning from concise planning intent through planning quaternions. The method achieves state-of-the-art performance across ALFWorld, ScienceWorld, and WebShop environments while using significantly fewer tokens than baseline reasoning models.

## Method Summary
BPO is a three-stage framework consisting of bootstrapping, extrapolation, and refinement phases. It uses planning quaternions to separate verbose reasoning from concise planning intent, employs curriculum synthesis to generate out-of-distribution tasks, and implements reward-gated rejection sampling for self-refinement using only successful trajectories. The approach is model-agnostic and particularly effective as task complexity increases, achieving superior performance through data curation rather than policy optimization.

## Key Results
- Achieves 88.16% average success rate across tested environments
- Uses only 112 tokens compared to 620-763 for baseline reasoning models
- Outperforms next best method (RTL) by 6.33 percentage points
- Shows increasing advantage as task complexity grows

## Why This Works (Mechanism)
The paper demonstrates that separating verbose reasoning from concise planning intent (planning quaternions) allows the model to focus on essential planning information rather than lengthy reasoning chains. The curriculum synthesis approach generates diverse, out-of-distribution tasks that improve generalization, while the reward-gated rejection sampling ensures the model learns primarily from successful trajectories, avoiding reinforcement learning's sample inefficiency issues.

## Foundational Learning

**Planning Quaternions**: Mathematical representation separating reasoning from planning intent. Why needed: Reduces token overhead while preserving planning capability. Quick check: Compare performance with and without quaternion separation.

**Curriculum Synthesis**: Automated generation of increasingly complex tasks. Why needed: Prevents overfitting to training distribution and improves generalization. Quick check: Measure performance degradation when testing on out-of-distribution tasks.

**Reward-Gated Rejection Sampling**: Filtering training data to retain only successful trajectories. Why needed: Ensures learning from high-quality examples in sparse-reward environments. Quick check: Analyze correlation between training data success rate and final model performance.

## Architecture Onboarding

**Component Map**: Data Generator -> Planning Quaternions -> Curriculum Synthesizer -> Reward Filter -> Self-Refinement Module -> Trained Model

**Critical Path**: Task generation → Quaternion extraction → Curriculum synthesis → Success filtering → Model fine-tuning → Evaluation

**Design Tradeoffs**: 
- Token efficiency vs. reasoning depth: Quaternions reduce tokens but may lose some contextual nuance
- Data quality vs. quantity: Success-gated sampling ensures quality but limits training data volume
- Computational cost vs. performance: Higher quality data requires more computation per sample

**Failure Signatures**:
- Poor performance on novel task variations suggests curriculum synthesis limitations
- Degradation with increased task complexity indicates insufficient reasoning capture in quaternions
- High variance across runs suggests instability in the data curation flywheel

**First Experiments**:
1. Ablation study removing quaternion separation to measure its contribution
2. Testing with different base model sizes to assess scalability
3. Evaluating on environments with sparser rewards than tested domains

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Requires substantial computation with 40K-50K data points generated per experiment
- Performance depends heavily on quality of initial reasoning model
- May struggle in truly sparse-reward environments where successes are extremely rare
- Curriculum synthesis may not always produce meaningful task variations

## Confidence

**High**: Empirical results showing improved success rates and token efficiency across three distinct domains with multiple baselines
**Medium**: Scalability and generalizability claims limited by specific domains and model sizes tested
**Low**: Theoretical justification for why planning quaternion separation works effectively

## Next Checks

1. Test BPO's performance when initialized with weaker base models to determine if improvements are framework-driven
2. Evaluate on environments with even sparser rewards and longer horizons than tested domains
3. Conduct ablation studies removing the planning quaternion separation to quantify its specific contribution