---
ver: rpa2
title: Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based
  Multi-Agent System
arxiv_id: '2509.17240'
source_url: https://arxiv.org/abs/2509.17240
tags:
- system
- reviews
- prisma
- systematic
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent system (MAS) that automates
  the evaluation of systematic literature reviews (SLRs) by assessing them against
  PRISMA guidelines. The system uses 27 specialized agents organized into six PRISMA-aligned
  societies, each handling specific evaluation tasks.
---

# Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System

## Quick Facts
- arXiv ID: 2509.17240
- Source URL: https://arxiv.org/abs/2509.17240
- Reference count: 12
- Key outcome: Multi-agent system achieves 84% agreement with human experts in evaluating SLRs against PRISMA guidelines

## Executive Summary
This paper presents a multi-agent system (MAS) for automated evaluation of systematic literature reviews (SLRs) against PRISMA guidelines. The system employs 27 specialized agents organized into six PRISMA-aligned societies, each handling specific evaluation tasks. Evaluated on five SLRs across diverse domains, the MAS achieved 84% agreement with expert human reviewers on PRISMA scores. The highest alignment occurred in Introduction (97%), Discussion (94%), and Methods (93%) sections, with lower but still strong agreement in Results (84%) and Other Information (81%). This demonstrates the system's potential to streamline SLR evaluation while maintaining consistency with expert judgment, though results are based on a small sample and further validation is needed.

## Method Summary
The MAS uses GPT-4.1 with a 1M token context window to evaluate SLRs against 27 PRISMA checklist items through 27 specialized agents organized into six societies. Each agent handles one PRISMA item using one-shot detailed prompting. The system processes PDFs via OCR (Unstructured), decomposes PRISMA checklists through Coordinator and Task agents, and retrieves external evidence via arXiv Toolkit. Agents score each item 0-5 and provide qualitative feedback. The architecture aggregates scores and outputs unified evaluations through a web interface. Five SLRs from diverse domains (Medical, E-commerce, AI, Metaverse, IoT) were evaluated against three human expert reviewers using ICC, Krippendorff's α, and MAE-based agreement metrics.

## Key Results
- 84% overall agreement between MAS and human expert reviewers
- Highest agreement in Introduction (97%), Discussion (94%), and Methods (93%) sections
- Lower but still strong agreement in Results (84%) and Other Information (81%) sections
- ICC = 0.924, Krippendorff's α = 0.889, Pearson ρ = 0.921 indicate strong inter-rater reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-item agent specialization improves evaluation stability compared to multi-item agents.
- Mechanism: Assigning one agent per PRISMA checklist item reduces cognitive load on individual agents, allowing focused prompting with domain-specific examples. The paper reports that early multi-item agent experiments produced "unstable behavior—overloaded agents, degraded performance, and unpredictable agent spawning."
- Core assumption: GPT-4.1's 1M token context window is sufficient for single-item evaluation tasks without cross-item reasoning degradation.
- Evidence anchors:
  - [section] "Early experiments with multi-item agents resulted in unstable behavior... so we adopted a one-agent-per-item design with one-shot detailed prompting"
  - [corpus] Weak direct corpus validation; neighboring papers (LGAR, prompting strategies evaluation) focus on single-stage screening tasks, not multi-agent decomposition.
- Break condition: If checklist items require cross-referencing (e.g., Methods-Results consistency), single-item agents may miss inter-sectional inconsistencies.

### Mechanism 2
- Claim: Society-based grouping enables domain-appropriate scoring alignment with human experts.
- Mechanism: Six PRISMA-aligned societies (Abstract/Title, Introduction, Methods, Results, Discussion, Other) map evaluation tasks to sections with varying complexity. Methods requires 11 agents due to detailed protocol requirements; Discussion requires 1. This mirrors PRISMA's item density distribution.
- Core assumption: PRISMA checklist structure directly correlates with evaluation difficulty and required agent capacity.
- Evidence anchors:
  - [section] Table 1 shows agent distribution: Methods (11), Results (7), Discussion (1), with agreement varying by section complexity
  - [abstract] "27 specialized agents organized into six PRISMA-aligned societies"
  - [corpus] No corpus papers validate society-based grouping specifically.
- Break condition: If PRISMA guidelines are updated with restructured sections, society mapping requires manual reconfiguration.

### Mechanism 3
- Claim: External retrieval (arXiv Toolkit) supports evidence-grounded evaluation beyond document content.
- Mechanism: Agents access arXiv for topic relevance verification and citation checking. The SLR-GPT follow-up agent uses the same toolkit for cross-checking literature and suggesting new papers.
- Core assumption: arXiv coverage is sufficient for interdisciplinary SLR validation; missing databases (PubMed, Scopus) do not critically undermine evaluation.
- Evidence anchors:
  - [section] "Each agent uses the arXiv Toolkit to retrieve relevant research as needed"
  - [section] Limitations: "excludes other key databases like PubMed or Scopus, creating potential gaps"
  - [corpus] Guide-RAG paper demonstrates evidence-driven corpus curation benefits, indirectly supporting retrieval-augmented evaluation.
- Break condition: For medical SLRs heavily indexed in PubMed but not arXiv, relevance checking may produce false negatives.

## Foundational Learning

- Concept: PRISMA Guidelines (Preferred Reporting Items for Systematic Reviews and Meta-Analyses)
  - Why needed here: The entire MAS architecture is structured around PRISMA checklist items. Understanding the 27-item framework (across 6 sections) is prerequisite to interpreting agent outputs and scoring logic.
  - Quick check question: Can you explain why Methods requires more PRISMA items than Discussion?

- Concept: Multi-Agent System (MAS) Coordination Patterns
  - Why needed here: The system uses Coordinator and Task agents for decomposition and dispatch. Understanding hierarchical vs. peer-to-peer agent communication is essential for debugging agent spawning behavior.
  - Quick check question: What happens when an agent's output falls below threshold—how does the Coordinator respond?

- Concept: Inter-rater Reliability Metrics (ICC, Krippendorff's α, MAE)
  - Why needed here: The paper validates system performance against three expert reviewers using ICC (0.924), α (0.889), and MAE-derived agreement (84%). Understanding these metrics is necessary to interpret whether 84% agreement is meaningful.
  - Quick check question: Why is inter-expert variability (Fig. 5) important for validating the human benchmark?

## Architecture Onboarding

- Component map: PDF upload -> OCR parsing (Unstructured) -> Task decomposition (Coordinator + Task Agent) -> Parallel agent evaluation (27 agents + 2 utilities) -> Score aggregation -> Web UI display
- Critical path: PDF upload → OCR parsing → Task decomposition → Parallel agent evaluation (0-5 scoring + qualitative feedback) → Score aggregation → UI display. Bottleneck is GPT-4.1 API calls for 27 agents per document.
- Design tradeoffs:
  - One-agent-per-item vs. multi-item agents: Chose specialization for stability, sacrificed potential cross-item reasoning efficiency.
  - arXiv-only vs. multi-database retrieval: Chose simplicity and open access, accepted coverage gaps in medical/biological domains.
  - 5-paper evaluation vs. large-scale validation: Chose rapid prototyping, acknowledged preliminary status.
- Failure signatures:
  - Low agreement on "Other Information" (81%) suggests registration/funding detection needs improved prompting or external database access.
  - Inter-expert variability highest in Methods (Fig. 5) indicates this section may require human-in-the-loop validation even with MAS support.
  - Domain-specific SLRs (medical) may underperform due to arXiv coverage gaps.
- First 3 experiments:
  1. Reproduce the 5-SLR evaluation with the same papers to validate reported agreement levels and identify per-section error patterns.
  2. Ablation study: Replace single-item agents with multi-item agents on 2 SLRs to quantify performance degradation (MAE increase, instability frequency).
  3. Extend retrieval to PubMed API for the medical SLR in the dataset; compare Methods agreement with vs. without PubMed access to test the database-gap hypothesis.

## Open Questions the Paper Calls Out
- Will the 84% agreement rate between the MAS and human experts hold or improve when evaluated on a larger, more diverse sample of SLRs? (Basis: Authors state "The current evaluation spans five SLRs from distinct domains. In subsequent studies, we are looking towards increasing the number of papers and hence the credibility of our results.")
- How does the exclusion of major databases (PubMed, Scopus, Web of Science) affect the system's ability to verify citations and assess literature coverage? (Basis: "The system's integration with arXiv enhances open-access coverage but excludes other key databases like PubMed or Scopus, creating potential gaps.")
- How do real-world users (systematic reviewers, authors) assess the system's clarity, usefulness, and trustworthiness in actual workflows? (Basis: "We will test the system with real-world users, systematic reviewers, and authors to assess its collaborative effectiveness. Structured feedback will be collected using Likert scales.")

## Limitations
- Small sample size (5 SLRs) limits generalizability across disciplines
- arXiv-only retrieval creates coverage gaps for medical/biological literature
- No user experience validation with practicing systematic reviewers

## Confidence

High confidence:
- The MAS architecture and agent society design are clearly specified
- The 84% agreement metric and reliability measures are well-documented
- The five SLR domains are explicitly stated

Medium confidence:
- Exact one-shot prompts and exemplars are not disclosed
- The five evaluation SLRs and human expert annotation protocol are not provided
- Code and interface details are not released

## Next Checks

1. Reproduce the 5-SLR evaluation with the same papers to validate reported agreement levels and identify per-section error patterns.

2. Conduct ablation study replacing single-item agents with multi-item agents on 2 SLRs to quantify performance degradation.

3. Extend retrieval to PubMed API for the medical SLR in the dataset; compare Methods agreement with vs. without PubMed access.