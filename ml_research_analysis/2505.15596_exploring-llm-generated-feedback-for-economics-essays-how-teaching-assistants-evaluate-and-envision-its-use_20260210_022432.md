---
ver: rpa2
title: 'Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants
  Evaluate and Envision Its Use'
arxiv_id: '2505.15596'
source_url: https://arxiv.org/abs/2505.15596
tags:
- feedback
- they
- rubric
- rubrics
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study explored whether AI-generated feedback can serve as
  effective suggestions to assist human instructors in grading economics essays. A
  feedback engine was developed that decomposes the feedback generation process into
  three steps: identifying relevant sentences, making rubric-based judgments, and
  generating targeted feedback.'
---

# Exploring LLM-Generated Feedback for Economics Essays: How Teaching Assistants Evaluate and Envision Its Use

## Quick Facts
- **arXiv ID**: 2505.15596
- **Source URL**: https://arxiv.org/abs/2505.15596
- **Reference count**: 33
- **Key outcome**: TAs found AI-generated feedback more aligned with rubrics, better at providing praise and guiding questions, and potentially useful for expediting grading and improving consistency, while maintaining the ability to detect and correct AI errors through visible intermediate outputs.

## Executive Summary
This study investigates whether AI-generated feedback can assist human instructors in grading economics essays by developing a feedback engine that decomposes the process into three steps: identifying relevant sentences, making rubric-based judgments, and generating targeted feedback. Five TAs participated in 20 think-aloud sessions comparing their handwritten feedback with AI-generated suggestions on 4 writing assignments. TAs found the AI feedback valuable for its rubric alignment, praise, and guiding questions, while also appreciating the intermediate outputs that enabled them to understand and correct AI judgments. The study suggests a promising human-AI collaborative approach to feedback provision that could expedite grading and improve consistency.

## Method Summary
The study developed a three-step feedback generation pipeline using GPT-4o that decomposes the process into: (1) sentence retrieval to identify relevant text, (2) judgment classification to determine rubric satisfaction with binary outcomes and rationales, and (3) feedback generation to create targeted comments. The system was evaluated through 20 think-aloud sessions with 5 TAs who compared their handwritten feedback against AI suggestions on 4 economics essays. TAs first completed their grading independently, then reviewed AI outputs, allowing researchers to capture their evaluation of AI feedback quality and envisioned usage patterns. The interface displayed highlighted sentences, rubric labels, AI judgments, and feedback options within a Word plugin.

## Key Results
- TAs found AI feedback more aligned with rubrics than their own handwritten feedback
- AI feedback better incorporated praise and guiding questions compared to human feedback
- TAs valued intermediate outputs and sentence highlights for understanding and correcting AI judgments
- AI feedback tended to be overly rigid in rubric application and lacked the holistic perspective TAs provided

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing feedback generation into modular subtasks with visible intermediate outputs enables human instructors to detect and correct AI errors efficiently.
- Mechanism: The pipeline separates (1) sentence identification, (2) rubric judgment, and (3) feedback generation. Each step produces inspectable outputs (highlights, binary judgments, text). TAs can validate or override at any stage without reading opaque AI reasoning.
- Core assumption: Users will engage with intermediate outputs rather than accept end-to-end suggestions uncritically.
- Evidence anchors:
  - [abstract] "TAs found the intermediate outputs and sentence highlights valuable for understanding and correcting AI judgments"
  - [section 4.2] "even when AI makes mistakes in determining whether a rubric item is satisfied, the highlighted sentences in the essay made it easy for them to understand and correct AI's judgment"
  - [corpus] Related work on LLM grading (e.g., LLM-as-a-Grader) emphasizes alignment metrics but does not systematically test intermediate-output visibility for error correction.
- Break condition: When intermediate outputs become too voluminous or fragmented for users to efficiently review (e.g., >15 rubric items per essay with independent highlights).

### Mechanism 2
- Claim: Detailed, explicitly-worded rubrics with acceptable alternatives and depth specifications are necessary for LLMs to generate accurate feedback on knowledge-intensive essays.
- Mechanism: The LLM pattern-matches against rubric language. When rubrics are vague ("correctly use the terms quantity demanded vs. demand"), the LLM lacks the domain knowledge to judge correctness. When rubrics include definitions, alternatives, and expected depth, the LLM's pattern-matching becomes more reliable.
- Core assumption: Instructors can and will invest effort in writing comprehensive rubrics that explicitly define correct answers, acceptable alternatives, and common misconceptions.
- Evidence anchors:
  - [section 4.1] "AI frequently made mistakes on assessing students' definitions of specialized economic terms, when the definitions were not provided in the rubric"
  - [section 5, Table 1] Provides concrete examples of improved rubric phrasing with domain-specific knowledge and acceptable alternatives
  - [corpus] CoachGPT and related writing assistants rely on scaffolding prompts but do not systematically address rubric specificity for knowledge-intensive domains.
- Break condition: When rubrics cannot feasibly enumerate all valid formulations (e.g., highly creative or open-ended responses).

### Mechanism 3
- Claim: Presenting AI feedback as suggestions rather than replacements preserves instructor agency and encourages selective adoption.
- Mechanism: TAs first form their own judgments, then consult AI outputs as a "second set of eyes." They adopt AI feedback that meets their standards (praise, guiding questions) and modify or reject the rest. This prevents over-reliance while still capturing efficiency gains.
- Core assumption: Instructors will maintain a critical stance toward AI suggestions and not default to acceptance under time pressure.
- Evidence anchors:
  - [abstract] "TAs found AI feedback...potentially useful for expediting grading and improving consistency"
  - [section 4.2] "Almost all participants mentioned that to ensure all the important points were covered...they would read the essay and make their own judgment first"
  - [section 4.2] "You don't want to like over-rely on it (AI), in case something is inconsistent" (P5)
  - [corpus] Guardrails in AI TAs (Exploring Student Behaviors...) address student over-reliance but parallel concerns apply to instructors.
- Break condition: Under extreme time pressure, instructors may shift from "review-then-accept" to "accept-then-spot-check," increasing error adoption.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The feedback engine explicitly uses CoT-style decomposition to improve LLM reasoning about rubric satisfaction before generating feedback.
  - Quick check question: Can you explain why asking an LLM to "identify relevant sentences first" before making a judgment might reduce errors compared to end-to-end feedback generation?

- Concept: **Rubric-Based Assessment**
  - Why needed here: The entire pipeline depends on well-structured rubrics. Understanding how rubrics function—and their limitations—is essential for interpreting AI feedback quality.
  - Quick check question: What types of student responses might satisfy a rubric's literal wording but fail to demonstrate genuine understanding?

- Concept: **Human-AI Collaboration Patterns**
  - Why needed here: This system is designed as a suggestion tool, not an autonomous grader. Understanding collaborative workflows (e.g., AI-as-draft, human-as-editor) clarifies the intended use model.
  - Quick check question: In what scenarios might suggestion-based collaboration degrade into over-reliance, and what interface features could mitigate this?

## Architecture Onboarding

- Component map:
  Input layer: Student essay (text) + course rubrics (structured text) + instructor-authored example feedback (few-shot)
  Processing pipeline (per rubric item): (1) Sentence retriever → identifies relevant sentences; (2) Judgment classifier → binary satisfied/not satisfied; (3) Feedback generator → produces targeted comment; (4) Historic feedback retriever → optional fallback
  Output layer: Word plugin rendering in-text comments with highlights, rubric labels, AI judgment, AI feedback, and historic feedback
  LLM backend: GPT-4o (temperature 0.05 for consistency)

- Critical path:
  1. Rubric ingestion and prompt template construction
  2. Per-rubric sentence identification (retrieval accuracy is rate-limiting)
  3. Judgment generation (primary source of errors per findings)
  4. Feedback text generation (highest variance in quality)
  5. UI rendering with highlights and attribution

- Design tradeoffs:
  - Granularity vs. coherence: One feedback item per rubric ensures coverage but produces fragmented feedback; TAs sometimes prefer holistic comments.
  - Rigidity vs. flexibility: Strict rubric alignment improves consistency but risks penalizing valid alternative phrasings.
  - Transparency vs. cognitive load: Showing all intermediate outputs aids verification but may overwhelm users; the current design shows highlights + judgment + two feedback options.

- Failure signatures:
  - Consistent misjudgments on specific rubric types (e.g., specialized terminology not defined in rubric)
  - Over-generation of feedback for essays that already meet rubrics (unnecessary comments)
  - Highlight drift: AI highlights sentences that do not actually support its judgment
  - Hallucinated domain knowledge: AI generates plausible-sounding but incorrect economic explanations

- First 3 experiments:
  1. **Rubric-ablation test**: Measure judgment accuracy and feedback quality across three conditions—minimal rubric (current course defaults), elaborated rubric (Table 1 style), and rubric-with-examples (including student response exemplars). Hypothesis: elaboration improves judgment accuracy; examples reduce false negatives on alternative phrasings.
  2. **Highlight necessity test**: Compare TA error-detection speed and accuracy in two interfaces—one with highlights visible, one without. Hypothesis: highlights reduce time-to-error-detection by >30% without increasing miss rates.
  3. **Adoption-pattern analysis**: Log which AI feedback components (judgments, AI text, historic text) TAs accept, modify, or reject across 50+ sessions. Identify predictors of adoption (e.g., rubric type, feedback length, presence of praise). Hypothesis: adoption rates will vary significantly by rubric category and feedback personalization level.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does feedback generated with AI assistance lead to improved student learning outcomes compared to feedback written solely by humans?
  - Basis in paper: [explicit] The authors state that "future work could further investigate how feedback created with AI feedback as suggestions would impact students’ learning outcomes."
  - Why unresolved: This study focused entirely on the instructor (TA) perspective regarding efficiency and quality, without measuring the actual impact on the students who received the feedback.
  - What evidence would resolve it: A controlled experiment comparing student revision quality and performance scores between groups receiving human-only feedback versus AI-assisted feedback.

- **Open Question 2**: Do students prefer the "point-to-point" rubric-aligned feedback generated by the AI over the holistic feedback typically provided by TAs?
  - Basis in paper: [explicit] The authors note that while TAs adopt a holistic approach, the AI produces fragmented feedback, and suggest "future work could explore whether such point-to-point feedback is preferable."
  - Why unresolved: The study only evaluated how TAs perceived the fragmented nature of the feedback, not how students received or utilized this structure.
  - What evidence would resolve it: Student surveys or interviews assessing their comprehension and preference when receiving localized, rubric-specific comments versus broader holistic feedback.

- **Open Question 3**: Does the use of AI-generated suggestions in a live workflow actually expedite grading time and improve consistency?
  - Basis in paper: [inferred] While TAs "envisioned" the tool would save time and improve consistency, the study methodology had TAs complete their grading *before* viewing the AI suggestions. The actual efficiency of a real-time, AI-assisted workflow was not measured.
  - Why unresolved: The think-aloud protocol measured retrospective evaluation and hypothetical usage, rather than the actual time-on-task or consistency in a live deployment.
  - What evidence would resolve it: A field study tracking grading duration and inter-rater reliability scores for TAs using the tool during their actual grading shifts.

## Limitations
- The study's qualitative findings depend heavily on a small sample of 5 TAs across only 20 think-aloud sessions.
- The system's reliance on detailed rubrics with explicit domain knowledge may not generalize to all economics courses or instructors who lack resources to create such comprehensive rubrics.
- The current design may overwhelm users with intermediate outputs when processing essays with numerous rubric items.

## Confidence
- **High confidence**: The finding that intermediate outputs (highlights, judgments) help TAs detect and correct AI errors is well-supported by direct quotes and consistent across participants.
- **Medium confidence**: The claim that rubric elaboration improves AI accuracy is based on qualitative observations and specific examples but lacks systematic comparison of rubric versions.
- **Low confidence**: The system's ability to reduce grading time and improve consistency under real-world conditions cannot be assessed from this exploratory study.

## Next Checks
1. **Rubric-ablation test**: Compare judgment accuracy and feedback quality across three rubric conditions (minimal, elaborated, with examples) to quantify the impact of rubric specificity.
2. **Highlight necessity test**: Measure TA error-detection speed and accuracy with and without sentence highlights to determine the cognitive overhead of transparency features.
3. **Adoption-pattern analysis**: Log TA acceptance rates of AI feedback components across 50+ sessions to identify predictors of adoption and potential over-reliance patterns.