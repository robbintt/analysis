---
ver: rpa2
title: 'HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding'
arxiv_id: '2505.22906'
source_url: https://arxiv.org/abs/2505.22906
tags:
- code
- participants
- they
- completion
- alternatives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of programmers over-relying on
  AI-generated code without understanding or controlling the underlying decisions,
  which can lead to security vulnerabilities. The core method is Human-in-the-Loop
  Decoding, implemented in a tool called HILDE that highlights critical decision points
  in code completions and provides local alternatives with explanations, allowing
  programmers to directly influence LLM decisions during code generation.
---

# HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding

## Quick Facts
- arXiv ID: 2505.22906
- Source URL: https://arxiv.org/abs/2505.22906
- Reference count: 40
- Primary result: 31% reduction in security vulnerabilities with HiLDe vs baseline AI assistant

## Executive Summary
This paper addresses the problem of programmers over-relying on AI-generated code without understanding or controlling the underlying decisions, which can lead to security vulnerabilities. The core method is Human-in-the-Loop Decoding, implemented in a tool called HiLDe that highlights critical decision points in code completions and provides local alternatives with explanations, allowing programmers to directly influence LLM decisions during code generation. In a user study with 18 participants on security-related tasks, HiLDe led to significantly fewer vulnerabilities in generated code (31% reduction) and enabled participants to make more intentional security repairs compared to a baseline AI assistant, despite participants having limited security training.

## Method Summary
HiLDe uses a two-LLM architecture: Qwen2.5-Coder-32B generates code completions with top-k token probabilities, while gpt4.1-nano analyzes alternatives for security implications. The system computes "corrected entropy" by weighting token probabilities with semantic importance scores, highlighting only decision points with meaningful alternatives. When users select alternatives, HiLDe regenerates suffixes while preserving similarity to the original completion using Levenshtein distance. The VSCode extension displays inline highlighting, local alternatives panel, explanation tooltips, and navigation codelenses.

## Key Results
- 31% reduction in security vulnerabilities compared to baseline AI assistant
- 71% of security repairs were intentional with HiLDe vs 29% with baseline
- Participants spent significantly more time evaluating suggestions (76.2s vs 35.5s average acceptance time)

## Why This Works (Mechanism)

### Mechanism 1: Semantically-Weighted Uncertainty Highlighting
Highlighting critical decision points reduces over-reliance by drawing attention to meaningful choice points rather than all uncertain tokens. HiLDe combines model entropy with semantic importance scores from an analysis LLM to compute "corrected entropy." Tokens with at least one significant alternative (affecting security, efficiency, robustness) are highlighted, while stylistic variations are suppressed.

### Mechanism 2: Local Alternatives with Contextual Explanations
Providing token-level alternatives with natural language explanations enables users to discover implementation strategies they weren't explicitly seeking. For each alternative token, HiLDE generates a preview completion and queries an analysis LLM for a structured explanation (detailed, summary, category, importance score). Users see truncated summaries inline and full explanations on hover.

### Mechanism 3: Regeneration with Similarity Preservation
When users select an alternative, regenerating the suffix while preserving similarity to the original completion maintains user mental models of the code. After token replacement, HiLDE generates 10 candidate suffixes and selects the one with highest similarity (Levenshtein distance) to the original base completion.

## Foundational Learning

- **Token-level decoding in LLMs**: Why needed here - HiLDe operates at the token level, exposing the probability distribution at each generation step. Understanding that LLMs assign probabilities to all possible next tokens—not just the selected one—is essential to grasp what "local alternatives" means. Quick check question: At a given generation step, can you explain why multiple tokens might be "valid" continuations with different trade-offs?

- **Fill-in-the-middle code completion**: Why needed here - HiLDe uses Qwen2.5-Coder-32B specifically for its fill-in-the-middle capability, generating code that fits between existing context (prefix and suffix), not just left-to-right completion. Quick check question: How does fill-in-the-middle differ from standard autoregressive completion, and why does it matter for code editing?

- **Corrected entropy vs. raw model uncertainty**: Why needed here - The paper explicitly rejects raw entropy highlighting as inadequate (citing Vasconcelos et al.). Understanding why semantic weighting corrects for "unimportant but uncertain" tokens is central to the approach. Quick check question: Why might a model have high entropy on a variable name but low entropy on an insecure cryptographic function call?

## Architecture Onboarding

- Component map: Completion LLM -> Analysis LLM -> Highlighting Engine -> VSCode Extension -> User
- Critical path: 1) User requests completion → Completion LLM returns base completion + top-k tokens with probabilities at each step. 2) For each token, generate previews for each alternative token using Completion LLM. 3) For each preview, Analysis LLM returns explanation + category + importance score. 4) Highlighting Engine computes corrected entropy → highlights significant decision points. 5) User selects alternative → Completion LLM generates 10 suffix candidates → select most similar to original. 6) UI updates with blue border at choice point; codelenses enable navigation.
- Design tradeoffs: Latency vs. richness (using smaller gpt4.1-nano for analysis minimizes delay but may reduce explanation quality); Highlighting precision vs. recall (aggressive filtering reduces overwhelm but may miss important decisions); Exploration vs. efficiency (participants spent more time evaluating suggestions, improving security but reducing speed).
- Failure signatures: Downstream code loss (selecting a low-probability alternative regenerates a suffix that doesn't preserve prior edits); Overwhelming highlighting (too many highlighted tokens cause cognitive overhead); Irrelevant alternatives (some participants found alternatives that were not useful, adding noise).
- First 3 experiments: 1) Baseline comparison: Replicate the user study setup with a simple code completion assistant vs. HiLDe on security-related tasks. 2) Highlighting ablation: Disable semantic weighting and use only raw entropy for highlighting. 3) Regeneration fidelity test: Systematically select low-probability alternatives and measure how often the regenerated suffix preserves the original code's structure vs. diverges.

## Open Questions the Paper Calls Out

- Can HiLDe be generalized for non-security goals like efficiency, maintainability, or personal coding style? The current analysis prompt is tuned specifically for security-related decision points; other goals require different importance scoring and explanation logic.
- How can AI assistants preserve incremental user modifications when selecting local alternatives triggers downstream code regeneration? HiLDe's mitigation (generating 10 suffixes, selecting most similar) fails for low-likelihood tokens, causing prior edits to be lost.
- Do HiLDe's benefits persist for complex, multi-file programming tasks beyond the isolated exercises tested? Real-world development involves larger codebases, dependencies, and more complex decision-making; the 31% vulnerability reduction may not generalize.

## Limitations

- The study's participant pool of 18 users introduces significant sampling uncertainty, with unclear participant demographics and baseline coding competence.
- The evaluation methodology conflates two distinct effects: the quality of initial suggestions versus the impact of the decision-point interface.
- The "corrected entropy" mechanism relies heavily on the analysis LLM's judgment of semantic importance, which may misclassify security-critical changes.

## Confidence

- **High confidence**: The user study demonstrates that HiLDe participants spent significantly more time evaluating suggestions (76.2s vs 35.6s) and reported better alignment between perceived and actual code correctness.
- **Medium confidence**: The 31% reduction in vulnerabilities and the 71% vs 29% intentional repair rates are derived from a small sample (18 participants) and depend on the baseline comparison methodology.
- **Low confidence**: The mechanism claims about "corrected entropy" and semantic weighting are theoretically sound but lack empirical validation for user engagement and analysis LLM accuracy.

## Next Checks

1. **Isolate interface effect**: Conduct a controlled study where both groups receive identical base completions from the same model, then measure whether HiLDe's interface alone improves security outcomes compared to a simple highlighting system using only raw entropy.

2. **Analysis LLM accuracy audit**: Systematically evaluate the analysis LLM's classification of alternatives by having security experts independently label the same token alternatives for importance. Measure precision and recall of the automated importance scoring against human judgment.

3. **Regeneration fidelity testing**: Create a benchmark of low-probability alternative selections and measure the percentage of cases where the 10-candidate regeneration preserves the original code structure versus cases where it diverges or overwrites user work. Identify specific probability thresholds where the mechanism fails.