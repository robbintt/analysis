---
ver: rpa2
title: 'Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition
  Templates by Text Encoding'
arxiv_id: '2512.08981'
source_url: https://arxiv.org/abs/2512.08981
tags:
- demographic
- bias
- face
- embeddings
- utie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses demographic bias in face recognition (FR)
  systems, where face embeddings encode demographic-specific information that leads
  to performance disparities across groups. The authors propose UTIE, a novel approach
  that enriches face embeddings with features from other demographic groups to induce
  demographic ambiguity.
---

# Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding

## Quick Facts
- arXiv ID: 2512.08981
- Source URL: https://arxiv.org/abs/2512.08981
- Reference count: 40
- Primary result: UTIE reduces demographic bias in face recognition by adding mean text embeddings from other groups to face embeddings, achieving consistent STD/SER improvements across three VLMs

## Executive Summary
This paper addresses demographic bias in face recognition systems by introducing UTIE, a method that enriches face embeddings with features from other demographic groups to induce demographic ambiguity. The approach leverages Vision-Language Models (VLMs) to add the mean text embedding of all demographic groups (excluding the predicted one) to the image embedding, creating a more balanced representation across demographics. Evaluated on RFW and BFW datasets using three VLMs (CLIP, OpenCLIP, SigLIP), UTIE consistently reduces bias metrics while maintaining or improving verification accuracy.

The key insight is that by adding textual demographic features from underrepresented groups to face embeddings, the resulting template becomes less biased toward any single demographic. This approach works because the added text embeddings introduce information that helps the face recognition system better represent diverse populations. The method achieves this without requiring demographic labels during inference, instead using zero-shot prediction from VLMs to determine which demographic to exclude from the mean computation.

## Method Summary
UTIE operates by first extracting an image embedding from a face using a VLM's image encoder, then predicting the demographic class using cosine similarity to text embeddings generated from prompts like "A photo of a {label} person." The method computes the mean text embedding across all demographic groups except the predicted one, then adds this mean vector to the original image embedding. This enriched embedding is used for face verification instead of the original image embedding. The approach is evaluated on RFW (four racial subsets) and BFW (eight subgroups for intersectional bias) datasets using three different VLMs, measuring verification accuracy alongside bias metrics including Standard Deviation (STD) across demographic groups and Skewed Error Ratio (SER).

## Key Results
- On RFW with CLIP, UTIE reduces STD from 4.81 to 4.46 and SER from 1.50 to 1.45 while improving accuracy from 72.20% to 72.25%
- Consistent bias reduction trends observed across all three VLMs (CLIP, OpenCLIP, SigLIP) on both RFW and BFW datasets
- Verification accuracy is maintained or slightly improved while bias metrics show significant reduction
- The method demonstrates that inducing demographic ambiguity through text-derived features is an effective strategy for bias mitigation

## Why This Works (Mechanism)
The mechanism works by deliberately introducing demographic information from multiple groups into face embeddings, creating representations that are less strongly associated with any single demographic category. When a face embedding is combined with the mean text embedding of other demographics, it effectively dilutes the demographic-specific features that typically cause bias in face recognition systems. This "demographic ambiguity" makes the embedding more representative of the overall population rather than being optimized for a particular group. The zero-shot prediction step ensures that the method adapts to each image's predicted demographic without requiring explicit labels, while the exclusion of the predicted class prevents overcorrection for any single group.

## Foundational Learning

**Cosine similarity** - Measures the angle between two vectors, commonly used for comparing embeddings in face recognition. *Why needed*: Used to measure similarity between image embeddings and text embeddings for demographic prediction. *Quick check*: Compute cosine similarity between two random vectors and verify values between -1 and 1.

**Embedding space manipulation** - The practice of modifying vector representations by adding or subtracting other vectors to achieve desired properties. *Why needed*: UTIE relies on adding text embeddings to image embeddings to create demographic ambiguity. *Quick check*: Verify that vector addition produces meaningful changes in downstream task performance.

**Vision-Language Models (VLMs)** - Models that can process both images and text, such as CLIP, OpenCLIP, and SigLIP. *Why needed*: Used for both image embedding extraction and text embedding generation for demographic classes. *Quick check*: Confirm that VLM text embeddings for demographic prompts are consistent across runs.

**Bias metrics in face recognition** - Standard Deviation (STD) across demographic groups and Skewed Error Ratio (SER) measure performance disparities. *Why needed*: These metrics quantify the demographic bias that UTIE aims to reduce. *Quick check*: Calculate STD and SER manually on a small dataset to verify understanding.

**Zero-shot classification** - Making predictions without explicit training on the target task, using pre-trained knowledge. *Why needed*: Used to predict demographic classes from face embeddings without requiring labeled demographic data. *Quick check*: Test zero-shot classification accuracy on a held-out set.

## Architecture Onboarding

**Component map**: Face image -> VLM image encoder -> Image embedding I -> Demographic prediction -> Mean text embedding T̄ (excluding predicted class) -> I' = I + T̄ -> Face verification

**Critical path**: The most time-consuming steps are VLM embedding extraction (both image and text) and demographic prediction via cosine similarity computation. These operations must be performed for every face image in the dataset.

**Design tradeoffs**: The method trades slight computational overhead (adding text embedding computation) for significant bias reduction. The zero-shot approach avoids requiring demographic labels but introduces potential errors in demographic prediction that could affect the final result.

**Failure signatures**: If demographic prediction accuracy is low (<85%), UTIE may actually increase bias by adding incorrect mean embeddings. If vector addition is performed incorrectly (wrong normalization or dimensional mismatch), the method may fail to produce meaningful demographic ambiguity.

**First experiments**:
1. Test demographic prediction accuracy on a validation set using cosine similarity to text embeddings
2. Verify that adding mean text embeddings to image embeddings produces consistent changes in embedding space
3. Measure baseline STD and SER on the target dataset before applying UTIE

## Open Questions the Paper Calls Out

**Privacy leakage impact** - The paper explicitly calls for investigating whether UTIE's addition of text-derived demographic features to embeddings creates new privacy vulnerabilities, such as increased susceptibility to membership inference or attribute reconstruction attacks. This remains unexplored as the paper focuses solely on bias mitigation without analyzing privacy implications.

**Prompt engineering potential** - The authors identify prompt engineering as a future research direction, noting that their single simple prompt "A photo of a {label} person" could be enhanced through multiple prompts, sub-category descriptions, or ensemble approaches to potentially improve the bias-accuracy trade-off.

**Generalization to other attributes** - The paper suggests testing whether the demographic ambiguity induction principle transfers to other protected attributes beyond race and gender, such as age groups, skin tone variations, or more granular intersectional demographics that were not evaluated in the current study.

## Limitations

- Demographic prediction relies on zero-shot VLM classification without accuracy reporting, creating uncertainty about the quality of demographic estimates driving the UTIE process
- Evaluation is limited to only two benchmark datasets (RFW and BFW) with four racial groups and binary gender, raising questions about performance on more diverse populations
- The additive combination of image and text embeddings assumes linear separability in embedding space, which may not hold for all VLM architectures or demographic distributions

## Confidence

- High confidence: The methodological framework and experimental protocol are clearly specified and reproducible
- Medium confidence: The bias reduction claims are supported by consistent trends across multiple VLMs, though absolute improvements are modest
- Medium confidence: The verification accuracy maintenance is demonstrated, but the slight improvements could be within statistical variation

## Next Checks

1. Evaluate demographic prediction accuracy of the zero-shot VLM classification step to establish baseline quality of the demographic estimates
2. Test the approach on additional demographic categories beyond the four racial groups and binary gender used in RFW and BFW
3. Analyze embedding space geometry to verify that the additive combination of image and text embeddings produces meaningful demographic ambiguity rather than simple vector addition artifacts