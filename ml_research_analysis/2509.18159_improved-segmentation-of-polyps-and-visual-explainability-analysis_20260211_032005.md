---
ver: rpa2
title: Improved Segmentation of Polyps and Visual Explainability Analysis
arxiv_id: '2509.18159'
source_url: https://arxiv.org/abs/2509.18159
tags:
- segmentation
- polyp
- dice
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces PolypSeg-GradCAM, a deep learning framework
  that combines a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted
  Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. The model
  was rigorously evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset
  of 1,000 annotated endoscopic images.
---

# Improved Segmentation of Polyps and Visual Explainability Analysis

## Quick Facts
- arXiv ID: 2509.18159
- Source URL: https://arxiv.org/abs/2509.18159
- Authors: Akwasi Asare; Thanh-Huy Nguyen; Ulas Bagci
- Reference count: 40
- Primary result: PolypSeg-GradCAM achieves 0.8902 mean Dice coefficient on Kvasir-SEG dataset

## Executive Summary
This study introduces PolypSeg-GradCAM, a deep learning framework combining U-Net architecture with ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. The model was rigorously evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results demonstrate a mean Dice coefficient of 0.8902, mean Intersection-over-Union (IoU) of 0.8023, and Area Under the ROC Curve (AUC-ROC) of 0.9722. The integration of Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, providing insight into the model's decision-making process. This work highlights how combining segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools for early colorectal cancer prevention.

## Method Summary
PolypSeg-GradCAM is a U-Net architecture with ResNet-34 backbone pretrained on ImageNet for feature extraction. The model takes 256×256 RGB endoscopic images, processes them through 5-stage encoder-decoder with skip connections, and outputs per-pixel probability masks. Training uses binary Dice Loss with Adam optimizer (lr=1e-4) for 30 epochs per fold, augmented with random flips, rotations, and brightness/contrast adjustments. The model achieves 176 FPS inference speed and incorporates Grad-CAM visualization by backpropagating gradients from masked logits to decoder feature maps, producing clinically interpretable heatmaps highlighting polyp regions.

## Key Results
- Mean Dice coefficient: 0.8902 with standard deviation 0.0115 across 5 folds
- Mean Intersection-over-Union (IoU): 0.8023 with standard deviation 0.0147
- Area Under ROC Curve (AUC-ROC): 0.9722 with standard deviation 0.0031
- At optimal threshold 0.55: Sensitivity 0.9058, Precision 0.9083

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning from ImageNet
Transfer learning from ImageNet pre-training accelerates convergence and improves segmentation quality on limited medical datasets. The ResNet-34 encoder loads ImageNet weights, providing initialized convolutional filters that detect edges, textures, and shapes. Fine-tuning adapts these to polyp-specific features rather than learning from random initialization. Core assumption: Low-level visual features (edges, gradients, textures) transfer across natural and medical imaging domains.

### Mechanism 2: Skip Connections for Spatial Resolution
Skip connections between encoder and decoder preserve spatial resolution lost during downsampling, improving boundary delineation. U-Net concatenates high-resolution encoder features with upsampled decoder features at matching scales, reintroducing fine-grained spatial information during mask reconstruction. Core assumption: Encoder features contain localization information not fully recoverable from deep bottleneck representations alone.

### Mechanism 3: Grad-CAM for Segmentation Explainability
Grad-CAM visualizations adapted for segmentation reveal clinically relevant attention regions, supporting interpretability. Gradients of the segmentation output (spatial sum of masked logits) are backpropagated to final decoder feature maps. Channel-wise importance weights produce a heatmap highlighting regions influencing the prediction. Core assumption: Gradient flow correlates with feature importance for segmentation decisions.

## Foundational Learning

- **Semantic Segmentation vs. Classification**: Understanding that segmentation outputs per-pixel probabilities rather than a single class label is essential for interpreting Dice, IoU, and the Grad-CAM adaptation. Quick check: What is the shape of the model's output tensor, and how does it differ from a classification model's output?

- **Skip Connections in Encoder-Decoder Architectures**: Critical for understanding how U-Net preserves spatial information and why the decoder can produce sharp boundaries. Quick check: If you remove skip connections, what artifact would you expect in the segmentation masks?

- **Gradient-based Attribution Methods**: Grad-CAM relies on computing gradients of outputs w.r.t. feature maps; understanding gradient flow helps diagnose why heatmaps may be noisy or uninformative. Quick check: Why does Grad-CAM apply ReLU to the final weighted combination of feature maps?

## Architecture Onboarding

- **Component map**: Input 256×256×3 → ResNet-34 encoder (conv1, layer1-4) → 8×8×512 bottleneck → Decoder with 4-stage upsampling via nearest-neighbor ×2 + Conv2D→BatchNorm→ReLU → 1×1 Conv → Sigmoid → Output 256×256×1 mask → Grad-CAM module

- **Critical path**: 1) Image preprocessing (resize → 256×256, ImageNet normalization) 2) Forward pass through encoder → decoder with skip concatenations 3) Sigmoid activation → binary mask (threshold 0.55 optimized) 4) Dice loss backpropagation (training) or Grad-CAM computation (inference)

- **Design tradeoffs**: Nearest-neighbor upsampling vs. transposed convolution (avoids checkerboard artifacts but may produce smoother boundaries); ResNet-34 vs. deeper backbones (balances parameter count 24.4M with real-time inference 176 FPS); 5-Fold CV vs. single holdout (increases robustness but requires 5× training time)

- **Failure signatures**: High Dice but poor boundary accuracy (may indicate decoder upsampling too aggressive); Grad-CAM highlights background (check gradient flow for spurious correlations); large variance across folds (suggests dataset heterogeneity); inference latency >40ms (profile GPU utilization)

- **First 3 experiments**: 1) Ablate skip connections: Train U-Net without skip connections on Kvasir-SEG (single fold); compare Dice and boundary F1 2) Threshold sensitivity analysis: Sweep binarization threshold [0.3-0.7] on validation set; plot Dice, sensitivity, precision curves 3) External validation: Evaluate trained model on CVC-ClinicDB or ETIS without retraining; report Dice drop

## Open Questions the Paper Calls Out
1. Does PolypSeg-GradCAM generalize to external multi-center datasets with different imaging equipment and patient populations? (Basis: Authors state evaluation on single dataset requires external validation)
2. Does the model maintain temporal consistency when applied to video colonoscopy sequences? (Basis: Future work will focus on testing model on video sequences)
3. Are Grad-CAM heatmaps quantitatively aligned with clinically relevant regions, as validated by expert clinicians? (Basis: Claims visualizations confirm clinically relevant regions but lacks systematic expert evaluation)

## Limitations
- Single dataset evaluation limits generalizability to different imaging equipment and patient populations
- No ablation studies to isolate contribution of ResNet-34 pretraining versus other architectural choices
- Grad-CAM visualizations lack rigorous validation of interpretability quality through user studies or comparison with other explainability methods

## Confidence
- **High Confidence**: Quantitative metrics (Dice, IoU, AUC-ROC) are reliably computed and reported with proper cross-validation methodology
- **Medium Confidence**: Claims about Grad-CAM revealing clinically relevant regions are supported by visualizations but lack rigorous validation of interpretability quality
- **Medium Confidence**: Claims about transfer learning benefits are plausible given architectural choices but not empirically isolated

## Next Checks
1. External validation on at least one other polyp segmentation dataset (CVC-ClinicDB or ETIS) to assess cross-dataset performance drop
2. Ablation study comparing ResNet-34 pretrained vs randomly initialized encoder to quantify transfer learning contribution
3. User study with gastroenterologists evaluating whether Grad-CAM heatmaps correctly identify clinically important regions compared to ground truth annotations