---
ver: rpa2
title: Training Video Foundation Models with NVIDIA NeMo
arxiv_id: '2503.12964'
source_url: https://arxiv.org/abs/2503.12964
tags:
- training
- video
- diffusion
- nemo
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable, open-source training pipeline for
  Video Foundation Models (VFMs) using NVIDIA NeMo. The framework addresses the challenges
  of training large-scale VFMs by providing accelerated video dataset curation with
  NeMo Curator, efficient multimodal data loading via Megatron Energon, and parallelized
  video diffusion model training leveraging Megatron Core.
---

# Training Video Foundation Models with NVIDIA NeMo

## Quick Facts
- arXiv ID: 2503.12964
- Source URL: https://arxiv.org/abs/2503.12964
- Reference count: 37
- Primary result: Scalable open-source training pipeline for Video Foundation Models achieving up to 48.2% MFU across various configurations

## Executive Summary
This paper presents a comprehensive, open-source training pipeline for Video Foundation Models (VFMs) using NVIDIA NeMo. The framework addresses the challenges of training large-scale VFMs by providing accelerated video dataset curation with NeMo Curator, efficient multimodal data loading via Megatron Energon, and parallelized video diffusion model training leveraging Megatron Core. Key contributions include a comprehensive performance analysis demonstrating up to 48.2% Model FLOPs Utilization (MFU) across various model configurations, near-linear scaling efficiency exceeding 95% when scaling from 8 to 32 nodes of 8xH100 GPUs, and a novel hybrid parallelism approach for Spatial-Temporal DiT (ST-DiT) models that achieves up to 40% MFU. The framework enables fast video generation inference with near-linear scaling up to 32 H100 GPUs, and introduces algorithm-system co-design optimizations including AdaLN-LoRA architecture modifications that improve compute performance by up to 1.2x.

## Method Summary
The method involves a three-stage pipeline: (1) video dataset curation using NeMo Curator with auto-balanced distributed processing, hardware-accelerated decoding/transcoding, and WebDataset sharding; (2) multimodal data loading via Megatron Energon with sequence packing and cloud storage optimization; and (3) parallelized training using Megatron Core with 4D parallelism (tensor, context, pipeline, and FSDP) and AdaLN-LoRA optimizations. The framework supports both autoregressive and diffusion-based VFMs, with a novel hybrid parallelism approach for ST-DiT models that applies context parallelism to full attention and data parallelism to spatial/temporal attentions.

## Key Results
- Achieves up to 48.2% Model FLOPs Utilization (MFU) across various model configurations
- Near-linear scaling efficiency exceeding 95% when scaling from 8 to 32 nodes of 8xH100 GPUs
- AdaLN-LoRA architecture modifications improve compute performance by up to 1.2x

## Why This Works (Mechanism)

### Mechanism 1: Auto-Balanced Multi-Stage Curation Pipeline
The distributed video curation pipeline achieves high throughput by dynamically allocating workers to match the bottleneck stage's rate. Ray orchestrates GPU workers across curation stages (decoding, embedding, captioning), with auto-balancer assigning more workers to lower-throughput stages like vision-language captioning models. Hardware acceleration via NVDEC/NVENC provides 3x speedup on decode/transcode operations.

### Mechanism 2: 4D Parallelism with Algorithm-System Co-Design for DiT
Combining tensor parallelism (TP), context parallelism (CP), pipeline parallelism (PP), and FSDP with AdaLN-LoRA enables efficient training of billion-parameter video diffusion transformers. TP shards parameters across GPUs, CP shards long sequences via ring-attention with P2P communication overlapping compute, PP distributes transformer blocks, and FSDP shards optimizer states. AdaLN-LoRA decomposes large AdaLN projections into low-rank matrices, reducing parameter density and enabling larger hidden dimensions for fixed model size.

### Mechanism 3: Hybrid Parallelism for Spatial-Temporal DiT
The hybrid approach applies context parallelism to full attention (small batch, long sequence) and data parallelism to spatial/temporal attentions (large batch, short sequence), with strategic all-to-all reshaping between attention types. This uses only 2 all-to-alls versus 4 per attention in alternatives, achieving higher throughput by avoiding suboptimal parallelism application to each attention type.

## Foundational Learning

- **Diffusion models and denoising score matching**: Essential for understanding the EDM diffusion formulation and loss function used in training. Quick check: Can you explain why the model learns to predict noise Îµ_t rather than directly predicting clean data?
- **Transformer attention patterns and sequence parallelism**: Critical for understanding how context parallelism works with ring-attention and P2P communication. Quick check: Why does context parallelism require all-gather of KV but not Q during forward pass?
- **Memory hierarchy in distributed training**: Important for understanding tradeoffs between activation memory, parameter memory, and communication bandwidth. Quick check: Why might FSDP alone be sufficient for small models but not for models with long video sequences?

## Architecture Onboarding

- **Component map**: Raw video -> Curator clipping (NVDEC) -> VLM captioning (rate-limiting) -> Sharding to WebDataset -> Energon loading with sequence packing -> 3D tokenizer -> DiT training with 4D parallelism -> Parallelized inference
- **Critical path**: The data flows through video ingestion, clipping, annotation, sharding, optimized loading, tokenization, parallel training, and finally parallel inference
- **Design tradeoffs**: FSDP vs. TP+FSDP (simplicity vs. reduced shard size), CP vs. no CP (essential for long sequences but exposes overhead on short sequences), recompute vs. communicate conditioning embeddings (compute tradeoff for simpler implementation)
- **Failure signatures**: OOM during training (activation memory exceeded, enable CP or reduce micro-batch size), low MFU (<30%, suboptimal parallelism config), pipeline bubble overhead (PP stages unbalanced, adjust VPP interleaving)
- **First 3 experiments**:
  1. Baseline FSDP training: Run DiT-7B at 8k context length with FSDP only; measure MFU and identify bottleneck
  2. Add CP for long sequences: Same model at 74k context length; compare FSDP-only vs. FSDP+CP performance
  3. AdaLN-LoRA ablation: Compare standard AdaLN vs. AdaLN-LoRA on 7B model at both 8k and 74k; measure speedup

## Open Questions the Paper Calls Out

### Open Question 1
How do quantization, Classifier-Free Guidance (CFG) parallelism, and model distillation integrate with the proposed context-parallel inference pipeline to further accelerate video generation? The paper benchmarks context and tensor parallelism but explicitly excludes these specific algorithmic optimizations from the current performance analysis.

### Open Question 2
Can an automated quality assurance metric be integrated into the NeMo Curator pipeline to validate synthetic captions without manual human inspection? Manual inspection presents a scalability bottleneck for the "100PB+" data scale targeted by the framework, implying a gap in automated curation quality control.

### Open Question 3
What specific heuristic or auto-tuning algorithm can predict the optimal 4D parallelism configuration (TP, CP, PP, FSDP) for a given model size and context length? The paper provides rules of thumb but indicates that finding the "best performing parallelism strategy" currently requires experimentation, which is computationally expensive.

## Limitations

- Performance claims rely heavily on specific hardware configurations (H100 GPUs) and software versions within the NVIDIA ecosystem, potentially limiting generalizability
- Exact hyperparameters for benchmark runs (learning rate schedules, batch sizes, training durations) are not provided, making direct replication challenging
- The evaluation methodology assumes idealized conditions that may not translate directly to all production environments

## Confidence

**High Confidence**: The 4D parallelism framework effectively scales DiT training to large context lengths; AdaLN-LoRA provides 1.2x compute performance improvement; NeMo Curator pipeline achieves 3x speedup with NVDEC/NVENC acceleration

**Medium Confidence**: Near-linear scaling efficiency (>95%) from 8 to 32 nodes for standard DiT configurations; hybrid parallelism approach for ST-DiT achieving 40% MFU; sequence packing and blending strategies effectively utilize heterogeneous video datasets

**Low Confidence**: The claimed 48.2% maximum MFU across all configurations; inference scaling performance claims without detailed latency analysis; performance portability across different GPU architectures or cloud environments

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Reproduce the DiT-7B training at both 8k and 74k context lengths with varying batch sizes and learning rates to verify the claimed 1.2x speedup from AdaLN-LoRA is consistent across different optimization configurations.

2. **Cross-Platform Performance Validation**: Run the baseline FSDP-only configuration on alternative GPU architectures (A100, L40S) to assess the claimed scaling efficiency and identify hardware-specific bottlenecks not captured in the H100-focused evaluation.

3. **Dataset Generalization Study**: Test the video curation pipeline with heterogeneous video sources (different resolutions, frame rates, compression formats) to validate the robustness of the auto-balancing mechanism and identify failure modes when input characteristics deviate significantly from the benchmark datasets.