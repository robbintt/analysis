---
ver: rpa2
title: Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms
  and Hyperparameters on Shared Latent Space
arxiv_id: '2502.09329'
source_url: https://arxiv.org/abs/2502.09329
tags:
- space
- algorithm
- latent
- ranking
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the Combined Algorithm Selection and Hyperparameter
  optimization (CASH) problem, where the goal is to efficiently select the optimal
  combination of a machine learning algorithm and its hyperparameters. The main challenge
  lies in the fact that different algorithms have different hyperparameter spaces,
  making it difficult to share information across algorithms.
---

# Bayesian Optimization for Simultaneous Selection of Machine Learning Algorithms and Hyperparameters on Shared Latent Space

## Quick Facts
- arXiv ID: 2502.09329
- Source URL: https://arxiv.org/abs/2502.09329
- Reference count: 40
- This paper addresses the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem, proposing a Bayesian optimization framework that embeds different hyperparameter spaces into a shared latent space for efficient cross-algorithm information sharing.

## Executive Summary
This paper tackles the challenge of simultaneously selecting machine learning algorithms and their hyperparameters (CASH problem) by proposing a novel Bayesian optimization framework. The key innovation is embedding heterogeneous hyperparameter spaces into a shared latent space using multi-layer perceptrons, enabling a multi-task Gaussian process surrogate model to share information across different algorithms. The approach includes pre-training with adversarial regularization to encourage latent space overlap and a ranking model to select the most effective pre-trained embedding for a given target dataset. Experiments on OpenML datasets demonstrate superior performance compared to existing AutoML methods.

## Method Summary
The proposed method involves three main components: (1) Bayesian optimization with a multi-task Gaussian process surrogate model operating in a shared latent space where different algorithm hyperparameter spaces are embedded via MLPs; (2) pre-training of these embedding models on source datasets using quadratic surface fitting combined with adversarial regularization to encourage latent space overlap; and (3) a ranking model based on learning-to-rank (LambdaMART) that selects the best pre-trained embedding for a new target dataset based on meta-features. The framework addresses the challenge that different algorithms have different hyperparameter spaces, which typically prevents information sharing in standard Bayesian optimization approaches.

## Key Results
- The proposed method achieved superior rankings and validation accuracy throughout the optimization process compared to existing AutoML methods on OpenML datasets
- Pre-training with adversarial regularization and the selection of an appropriate pre-trained embedding model provided measurable benefits, as validated by ablation studies
- The ranking model based on meta-features successfully identified effective pre-trained embeddings, outperforming random selection

## Why This Works (Mechanism)

### Mechanism 1
Embedding heterogeneous hyperparameter spaces into a shared latent space enables cross-algorithm information transfer during Bayesian optimization. Each ML algorithm's HP space is mapped via MLP to a common latent space, and a multi-task GP is trained on all embedded observations jointly, allowing predictions for any (algorithm, hyperparameter) pair to leverage observations from all algorithms. The core assumption is that hyperparameters across different ML algorithms encode related notions of "model flexibility" that can be represented in a common geometric structure. Break condition: If algorithms have fundamentally unrelated hyperparameter semantics (e.g., categorical architecture choices vs. continuous regularization), the latent space may fail to align them meaningfully.

### Mechanism 2
Adversarial regularization during pre-training forces latent representations from different algorithms to occupy overlapping regions, enabling effective information sharing. During pre-training on source datasets, an adversarial classifier tries to predict which ML algorithm generated a given latent embedding. The embedding MLPs are trained to maximize this classification error, encouraging different algorithm spaces to become indistinguishable in the latent space. Core assumption: Overlapping latent regions imply that performance patterns learned from one algorithm transfer to others. Break condition: If algorithm-specific performance landscapes are genuinely disjoint, forcing overlap may create spurious correlations that mislead the surrogate.

### Mechanism 3
A ranking model trained via learning-to-rank can select the most effective pre-trained embedding for a new target dataset based on meta-features. Multiple pre-trained embedding models are created from different source datasets. For each pseudo-target, BO performance is measured with each PTEM to create ground-truth rankings. LambdaMART is trained on meta-feature differences to predict rankings for new datasets. Core assumption: Dataset similarity measured via meta-features correlates with transferability of pre-trained latent embeddings. Break condition: If meta-features fail to capture dataset properties relevant to embedding transfer (e.g., feature interactions, data distribution shape), ranking model predictions degrade to random.

## Foundational Learning

- **Multi-task Gaussian Process**: Core surrogate model that must handle multiple correlated tasks (algorithms) simultaneously with shared covariance structure. Quick check: Can you explain how the Linear Model of Coregionalization (LMC) kernel captures task dependencies?

- **Deep Kernel Learning**: The MLP embedding functions are integrated into the GP kernel, requiring joint optimization of neural network weights and GP hyperparameters. Quick check: How does backpropagation through the marginal likelihood work for deep kernel parameters?

- **Learning to Rank (LambdaMART)**: The ranking model uses LambdaMART to predict which pre-trained embedding will perform best, optimizing NDCG rather than raw score prediction. Quick check: Why optimize ranking loss (NDCG) rather than regression on performance scores for embedding selection?

## Architecture Onboarding

- **Component map**: Embedding MLPs -> MTGP surrogate -> Acquisition optimizer -> Pre-training pipeline -> Ranking model
- **Critical path**: 1) Pre-training phase: Train multiple PTEMs on source datasets with adversarial regularization; 2) Compute meta-features for target dataset; 3) Ranking model selects best PTEM; 4) Initialize BO with selected PTEM; 5) Iterate: optimize MTGP -> maximize EI -> evaluate -> update observations
- **Design tradeoffs**: Latent dimension (2-4 tested): Lower risks underfitting; higher risks overfitting with few observations. Pre-training regularization strength (α, β): Too strong anchors to source datasets; too weak loses transfer benefit. Fine-tuning strategy (paper freezes all but last MLP layer): Reduces overfitting but limits adaptation
- **Failure signatures**: Slow convergence with "w/o pre-train": MLPs unstable with few target observations. Random-selection-level performance: Ranking model failing; check meta-feature quality. Acquisition optimization stuck: Latent space may have poor geometry; check embedding visualization
- **First 3 experiments**: 1) Sanity check: Run proposed method vs. random search on 5 target datasets for 50 iterations; should show clear ranking advantage. 2) Ablation: Disable adversarial regularization during pre-training; visualize latent space with t-SNE colored by algorithm—expect more clustered/separated regions. 3) Transfer boundary: Test on target dataset from very different domain than source datasets (e.g., images vs. tabular); measure ranking model calibration

## Open Questions the Paper Calls Out
1. Can techniques mitigating the cycle consistency problem, such as latent data augmentation, be integrated to allow acquisition function maximization directly in the latent space? (Basis: Section 4 states this is a "possible future direction" because the authors currently maximize in the original space to avoid risks that decoded latent variables don't match their encoded values.)

2. How can variable computational costs of different ML algorithms be explicitly modeled within the acquisition function? (Basis: Section 5.4 notes that estimating cost from observations and incorporating it into the acquisition function is a "possible approach" excluded to maintain fair comparison, as the current evaluation assumes uniform cost for all algorithms.)

3. Is the assumption that a shared latent representation based on "model flexibility" valid for fundamentally different hyperparameter types (e.g., continuous regularization vs. discrete tree depth)? (Basis: Section 1 assumes common latent representation exists because "most of HPs control the model flexibility," while Section 3.2.1 enforces quadratic surface approximation, but it's unclear if this holds for highly heterogeneous algorithms.)

## Limitations
- Critical implementation details remain unspecified, including MLP architecture hyperparameters, categorical hyperparameter encoding scheme, and exact meta-feature set used by the ranking model
- The paper lacks ablation studies on latent space dimensionality and pre-training regularization strength, making it difficult to assess sensitivity to these design choices
- Without access to source code or clearer specification, faithful reproduction would require substantial engineering decisions

## Confidence
- **High confidence**: The core Bayesian optimization framework with multi-task GP surrogate operates as described, given standard GP implementations exist
- **Medium confidence**: The adversarial regularization mechanism for encouraging latent space overlap is theoretically sound, though empirical effectiveness depends heavily on implementation details and hyperparameter settings
- **Low confidence**: The ranking model's ability to select optimal pre-trained embeddings across diverse datasets relies on unproven assumptions about meta-feature transferability and the quality of the ground-truth ranking generation process

## Next Checks
1. **Latent space geometry verification**: Visualize embeddings from different algorithms in 2D/3D (via t-SNE or PCA) to confirm that adversarial regularization produces overlapping regions rather than algorithm-specific clusters

2. **Pre-training sensitivity analysis**: Systematically vary adversarial regularization strength β and measure its impact on both latent space overlap (classifier accuracy) and downstream BO performance to identify optimal trade-offs

3. **Meta-feature correlation study**: Compute pairwise similarity between datasets using the meta-features that feed the ranking model, then correlate these with actual transfer performance to validate whether the ranking model is learning meaningful patterns or memorizing source-target relationships