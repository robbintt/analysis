---
ver: rpa2
title: 'StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization
  with LLMs'
arxiv_id: '2505.22950'
source_url: https://arxiv.org/abs/2505.22950
tags:
- sentence
- summarization
- prompting
- sentences
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extractive summarization
  for long documents using large language models (LLMs) in a zero-shot setting. The
  authors propose StrucSum, a structure-aware prompting framework that leverages sentence-level
  graph structures to enhance LLM reasoning.
---

# StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs

## Quick Facts
- arXiv ID: 2505.22950
- Source URL: https://arxiv.org/abs/2505.22950
- Authors: Haohan Yuan; Sukhwa Hong; Haopeng Zhang
- Reference count: 16
- Key outcome: Structure-aware prompting framework improves zero-shot extractive summarization with LLMs by leveraging sentence-level graph structures

## Executive Summary
This paper addresses the challenge of extractive summarization for long documents using large language models (LLMs) in a zero-shot setting. The authors propose StrucSum, a structure-aware prompting framework that leverages sentence-level graph structures to enhance LLM reasoning. StrucSum incorporates three strategies: Neighbor-Aware Prompting (NAP) for local context, Centrality-Aware Prompting (CAP) for importance estimation, and Centrality-Guided Masking (CGM) for efficient input reduction. Experiments on ArXiv, PubMed, and Multi-News datasets demonstrate consistent improvements in both summary quality and factual consistency compared to vanilla prompting and unsupervised baselines.

## Method Summary
StrucSum introduces a graph-structured reasoning approach for zero-shot extractive summarization with LLMs. The framework constructs sentence-level graphs using TF-IDF weighted cosine similarity, then applies three complementary strategies. Neighbor-Aware Prompting provides local context by including neighboring sentences in prompts. Centrality-Aware Prompting incorporates sentence importance scores derived from graph centrality metrics. Centrality-Guided Masking reduces input size by filtering low-centrality sentences while preserving structural information. The approach is designed to work without training, making it directly applicable to any LLM capable of zero-shot reasoning.

## Key Results
- StrucSum achieves consistent improvements across ArXiv, PubMed, and Multi-News datasets
- On ArXiv dataset, StrucSum increases FactCC and SummaC scores by 19.2% and 8.0% points respectively
- Individual strategies outperform their combinations, with NAP being most effective for maintaining factual accuracy
- StrucSum surpasses vanilla prompting and unsupervised baselines in both summary quality and factual consistency

## Why This Works (Mechanism)
The framework works by integrating structural priors into LLM prompts through graph-based sentence relationships. The graph structure captures both local dependencies (through neighbor relationships) and global importance (through centrality measures). By explicitly incorporating these structural elements into prompts, LLMs can better reason about sentence relevance and coherence for summarization tasks.

## Foundational Learning
- **Graph Construction**: Sentence-level graphs built using TF-IDF weighted cosine similarity to capture semantic relationships
  - Why needed: Provides structured representation of document content beyond linear text
  - Quick check: Verify graph connectivity and density across different document types

- **Centrality Measures**: Graph centrality metrics used to estimate sentence importance for summarization
  - Why needed: Identifies key sentences that represent document content effectively
  - Quick check: Compare centrality rankings with human relevance judgments

- **Zero-shot Prompting**: Direct application of structural information without model training
  - Why needed: Enables immediate deployment across different LLMs and domains
  - Quick check: Test compatibility with multiple LLM architectures and sizes

## Architecture Onboarding

**Component Map**: Graph Construction -> Structural Analysis -> Prompt Generation -> LLM Reasoning

**Critical Path**: 
1. Document preprocessing and sentence tokenization
2. Graph construction using cosine similarity
3. Centrality computation for importance scoring
4. Prompt generation with structural information
5. LLM processing for extractive summarization

**Design Tradeoffs**: 
- Zero-shot approach sacrifices potential performance gains from fine-tuning
- Sentence-level graphs balance granularity with computational efficiency
- TF-IDF weighting provides simplicity but may miss semantic nuances

**Failure Signatures**: 
- Poor graph construction leading to disconnected components
- Centrality measures failing to identify truly important sentences
- Prompts becoming too complex for effective LLM reasoning

**3 First Experiments**:
1. Compare different graph construction methods (TF-IDF vs. semantic embeddings)
2. Test varying neighborhood sizes in Neighbor-Aware Prompting
3. Evaluate different centrality metrics (degree, betweenness, PageRank) for importance scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do combinations of structural strategies (NAP, CAP, CGM) fail to yield performance gains over individual strategies, and can they be modified to synergize effectively?
- Basis in paper: Section 4.5 shows that while individual strategies improve specific metrics, their combinations (e.g., NAP+CAP+CGM) perform worse or merely match individual components.
- Why unresolved: The paper identifies the lack of synergy but does not investigate the underlying feature interference or prompt confusion causing it.
- What evidence would resolve it: An analysis of attention patterns or feature importance when strategies are combined, or a dynamic prompting mechanism that selects strategies per document.

### Open Question 2
- Question: How effective is the StrucSum framework in multilingual contexts and with dynamic graph structures?
- Basis in paper: The conclusion states, "Future work may extend this framework to multilingual and dynamic graph reasoning."
- Why unresolved: The current experiments are restricted to English datasets (ArXiv, PubMed, Multi-News) and static graph constructions based on cosine similarity.
- What evidence would resolve it: Evaluation of StrucSum on multilingual summarization benchmarks and datasets where document structures evolve over time.

### Open Question 3
- Question: Can StrucSum be adapted for supervised fine-tuning to achieve higher task-specific performance, or does its strength lie exclusively in zero-shot settings?
- Basis in paper: The Limitations section notes, "Exploring fine-tuned variants remains an open direction," acknowledging the current design is training-free.
- Why unresolved: The paper isolates the zero-shot setting to prove the utility of graph priors without training, leaving the upper bound of performance untested.
- What evidence would resolve it: Comparative experiments showing whether fine-tuning LLMs with structural prompts yields significant gains over the zero-shot implementation.

## Limitations
- The approach is currently limited to English datasets and may not generalize to multilingual contexts
- Combinations of structural strategies fail to provide synergistic benefits, with individual strategies outperforming their combinations
- The framework does not explore alternative graph construction methods or compare different graph types beyond sentence-level structures

## Confidence

**High**: The general methodology and implementation of the three proposed strategies (NAP, CAP, CGM)

**Medium**: The comparative performance improvements across datasets and the effectiveness of individual vs. combined strategies

**Medium**: The conclusions about the importance of structure-aware prompting for zero-shot extractive summarization

## Next Checks

1. Conduct a more comprehensive ablation study to understand the interaction effects between NAP, CAP, and CGM, and test alternative orderings of strategy application

2. Evaluate the approach on additional long-document datasets with different characteristics (e.g., legal documents, technical reports) to assess generalizability

3. Perform a detailed computational efficiency analysis comparing StrucSum to baseline methods, including wall-clock time and token usage metrics