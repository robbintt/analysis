---
ver: rpa2
title: Moral Susceptibility and Robustness under Persona Role-Play in Large Language
  Models
arxiv_id: '2511.08565'
source_url: https://arxiv.org/abs/2511.08565
tags:
- moral
- persona
- robustness
- susceptibility
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a benchmark for evaluating moral judgment
  in large language models under persona role-play using the Moral Foundations Questionnaire
  (MFQ). The authors define two metrics: moral robustness (stability of MFQ scores
  across personas under repeated sampling) and moral susceptibility (sensitivity to
  persona variation).'
---

# Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models

## Quick Facts
- arXiv ID: 2511.08565
- Source URL: https://arxiv.org/abs/2511.08565
- Reference count: 40
- Primary result: Model family dominates moral robustness variance; larger models show higher susceptibility to persona variation.

## Executive Summary
This paper introduces a benchmark for evaluating moral judgment in LLMs under persona role-play using the Moral Foundations Questionnaire (MFQ). The authors define two metrics: moral robustness (stability of MFQ scores across personas under repeated sampling) and moral susceptibility (sensitivity to persona variation). Testing across multiple model families and sizes, they find that model family accounts for most variance in moral robustness, with the Claude family showing the highest robustness. In contrast, moral susceptibility shows a mild family effect but a clear size trend, with larger models being more susceptible. Notably, robustness and susceptibility are positively correlated, especially at the family level. The study provides a systematic framework for comparing moral profiles across models and offers insights into how persona conditioning affects moral judgments in LLMs.

## Method Summary
The benchmark evaluates 18 models across 6 families using 100 personas and 30 MFQ questions. For each persona-question pair, models are queried 10 times with constrained decoding (first token must be 0-5). Robustness measures within-persona stability by averaging standard deviations across repetitions, while susceptibility measures across-persona variation by computing standard deviations of persona means within balanced groups. The approach excludes 9 personas with complete parsing failures and normalizes both metrics to [0,1] for cross-model comparison.

## Key Results
- Model family accounts for most variance in moral robustness; Claude family shows highest robustness, followed by Gemini and GPT-4.
- Larger model variants within families show higher moral susceptibility to persona conditioning.
- Robustness and susceptibility are positively correlated (r = +0.27 at model level, r = +0.29 at family level), with correlation strengthening to r = +0.41/+0.48 when excluding outlier Grok family.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model family accounts for most variance in moral robustness, with no systematic size effect.
- Mechanism: Alignment procedures and training data curation create family-specific behavioral consistency patterns. Models from the same family inherit similar stability characteristics because they share post-training alignment pipelines (e.g., RLHF, constitutional AI) that constrain output variance under repeated sampling, regardless of parameter count.
- Core assumption: Family-level robustness differences reflect training methodology rather than architectural properties alone.
- Evidence anchors:
  - [abstract] "model family accounts for most of the variance, while model size shows no systematic effect"
  - [section 3] "Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness"
  - [corpus] Weak direct evidence; related work on persona-conditioned alignment (arxiv:2504.10886) suggests training methodology shapes moral behavior but does not isolate robustness specifically.
- Break condition: If robustness differences disappear when controlling for alignment procedure rather than family label, the mechanism would shift from family-inherent to alignment-method-specific.

### Mechanism 2
- Claim: Larger model variants within a family show higher moral susceptibility to persona conditioning.
- Mechanism: Increased model capacity enables richer representations of persona attributes, allowing more complete adoption of persona-driven perspectives. Larger models distribute moral reasoning across more parameters, making the moral latent space more malleable to contextual steering through persona prompts.
- Core assumption: Capacity gains translate to greater latent space plasticity for moral representations specifically.
- Evidence anchors:
  - [abstract] "moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible"
  - [section 3.2] Foundation-level susceptibility profiles show size gradients within GPT-5 family (Nano < Mini < full) and Gemini family (Flash-Lite < Flash)
  - [corpus] arxiv:2601.09833 finds persona-conditioned trait evaluation is sensitive to prompt variation, suggesting malleability, but does not test size scaling directly.
- Break condition: If susceptibility plateaus or reverses at larger scales (inverse scaling), the linear capacity-flexibility relationship would be falsified.

### Mechanism 3
- Claim: Robustness and susceptibility are positively correlated, especially at the family level.
- Mechanism: Both high robustness (within-persona stability) and high susceptibility (across-persona differentiation) require sophisticated persona processing. Models that can maintain consistent internal states for a given persona can also construct more distinct states for different personas. This coordinated flexibility emerges from the same underlying capability: precise persona-context binding.
- Core assumption: Correlation reflects shared capability rather than evaluation artifact or noise.
- Evidence anchors:
  - [abstract] "robustness and susceptibility are positively correlated, an association that is more pronounced at the family level"
  - [table 3] Pearson r = +0.27 (model-level), r = +0.29 (family-level); excluding outlier Grok family strengthens correlations to r = +0.41 and r = +0.48
  - [corpus] arxiv:2511.01205 reports persona effects in collaborative moral reasoning but does not examine robustness-susceptibility tradeoffs.
- Break condition: If correlation is driven entirely by outlier families (Grok has low robustness + high susceptibility), the coordinated flexibility hypothesis weakens. Excluding Grok strengthens correlations, suggesting the pattern holds but is masked by one anomalous family.

## Foundational Learning

- Concept: **Moral Foundations Questionnaire (MFQ)**
  - Why needed here: The entire benchmark depends on understanding that MFQ measures five foundations (Harm/Care, Fairness/Reciprocity, In-group/Loyalty, Authority/Respect, Purity/Sanctity) through 30 questions rated 0-5. Without this, the metric definitions in Equations 1-7 are opaque.
  - Quick check question: Given an MFQ score vector [4.2, 4.0, 2.8, 3.1, 2.0], which foundation shows highest sensitivity to harm considerations?

- Concept: **Within-Persona vs Across-Persona Variance**
  - Why needed here: Robustness (Eq. 4) averages standard deviations across repeated samples of the same persona-question pair. Susceptibility (Eq. 7) computes standard deviation of persona-means across different personas. Confusing these leads to misinterpreting the entire results.
  - Quick check question: If a model gives identical ratings for all 10 repetitions of Persona A but different ratings for Persona B vs Persona A, is it high or low on robustness? On susceptibility?

- Concept: **Persona Role-Play Prompting**
  - Why needed here: The method conditions model responses via "You are roleplaying as the following persona: <DESCRIPTION>" before each MFQ question. Understanding that persona adoption is prompt-driven—not fine-tuned—is essential for interpreting why susceptibility varies by model capability.
  - Quick check question: Would you expect susceptibility to change if personas were fine-tuned into model weights rather than prompted? Why or why not?

## Architecture Onboarding

- Component map:
  - Persona descriptions (100 personas from Ge et al.) + MFQ questions (30 questions, two sections: relevance and agreement) -> Prompt assembly: Persona role-play instruction + MFQ question + leading-integer response instruction -> Query execution: n=10 repetitions per persona-question pair via constrained decoding (first token must be 0-5) -> Score extraction: Parse leading integer; log failures; retry up to 4 times -> Aggregation: Compute mean (ȳpq) and std (upq) per persona-question pair (Eq. 1) -> Robustness computation: Average upq over all personas and questions -> inverse transform -> normalize by cross-model mean (Eq. 2-4) -> Susceptibility computation: Partition personas into G=7 groups of 13 -> compute std of persona-means within each group -> average across groups -> normalize (Eq. 5-7)

- Critical path:
  1. Obtain reliable mean/variance estimates per persona-question pair (Eq. 1) — this determines accuracy of both metrics
  2. Constrained decoding to ensure parseable integer responses — parsing failures cascade into missing data
  3. Group assignment for susceptibility — must be balanced (|Pg| = 13) for valid std estimation

- Design tradeoffs:
  - **Sampling-based (n=10) vs probability-based (Eq. 8)**: Paper uses sampling for cost efficiency but notes that direct next-token probability computation would be "more principled" and avoid parsing failures. Tradeoff: API cost + access to log-probabilities vs approximation quality.
  - **Single-question vs full-questionnaire prompting**: Paper queries one question at a time to "avoid sequence- and order-dependent effects." Tradeoff: 30x more API calls vs cleaner measurement.
  - **Group size G=7 for susceptibility**: Larger G gives more uncertainty estimates but smaller groups have higher variance. Paper uses G=7 after removing 9 personas with complete parsing failures, yielding 7 groups of 13.

- Failure signatures:
  - **Parsing failures**: Some models systematically ignore the "start with integer" instruction, opening with "As a..." instead. See Table 6: gemini-2.5-flash had 1,924 failures; some personas (IDs 66, 94) failed repeatedly across models.
  - **Complete persona rejection**: 9 personas (IDs 29, 42, 44, 51, 66, 75, 86, 90, 95) were excluded because models refused to rate all question repetitions. This creates missing data that breaks susceptibility computation.
  - **Family outliers**: Grok family shows low robustness + high susceptibility (opposite of general trend), which suppresses robustness-susceptibility correlations when included.

- First 3 experiments:
  1. **Reproduce with probability-based estimation**: Implement Eq. 8 using log-probability access. Compare robustness/susceptibility values to sampling-based estimates. Hypothesis: values converge but uncertainty decreases.
  2. **Test a new model family**: Run the full benchmark on a model not in the original set (e.g., Mistral, Cohere). Plot its robustness vs susceptibility position relative to existing families. Predict where it falls based on its known alignment procedure.
  3. **Ablate persona description length**: Truncate or expand persona descriptions to test whether susceptibility scales with persona information content. Hypothesis: longer descriptions increase susceptibility for high-capacity models but have diminishing returns for smaller models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using next-token probability distributions to compute MFQ scores directly yield equivalent robustness and susceptibility estimates compared to repeated sampling?
- Basis in paper: [explicit] Section 2.4 states "Rather than estimating these quantities via repeated sampling, a more principled alternative is to use the model's next-token distribution to directly compute this values" and provides Equations 8, but this approach was not implemented.
- Why unresolved: The paper used n=10 repeated samples per persona–question pair as an approximation, requiring parsing and incurring costs. The direct probability method was proposed but not tested.
- What evidence would resolve it: Run the benchmark on models with accessible token-level log-probabilities using both methods and compare the resulting robustness and susceptibility scores.

### Open Question 2
- Question: What mechanistic or training factors explain the positive correlation between moral robustness and moral susceptibility?
- Basis in paper: [explicit] The abstract and Section 3.3 report "robustness and susceptibility are positively correlated, an association that is more pronounced at the family level." The correlation coefficients are provided (r = +0.27 at model level, +0.29 at family level) but no causal explanation is offered.
- Why unresolved: Intuitively, robustness (stability) and susceptibility (sensitivity) might be expected to trade off negatively; the observed positive correlation is counterintuitive and unexplained.
- What evidence would resolve it: Controlled experiments varying training data diversity, instruction-tuning procedures, or model architecture to identify which factors drive both properties jointly.

### Open Question 3
- Question: Why does larger model size increase moral susceptibility without affecting robustness?
- Basis in paper: [explicit] The abstract states "moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible," while "model size shows no systematic effect" on robustness. This divergent pattern is reported but not explained.
- Why unresolved: The dissociation between size effects on robustness versus susceptibility suggests different underlying mechanisms, but the paper does not investigate what those mechanisms might be.
- What evidence would resolve it: Layer-wise or attention-head analyses tracking how persona information propagates through models of different sizes; ablation studies on scaling factors.

### Open Question 4
- Question: What persona or instruction characteristics cause certain personas to systematically elicit instruction-following failures across model families?
- Basis in paper: [explicit] Appendix C reports "some personas appeared repeatedly across models" with high parsing failures and "this behavior was unexpected as their descriptions do not obviously correlate with not following instructions, yet the pattern persists across architectures." Personas 66 and 94 are identified as worst offenders.
- Why unresolved: The paper documents the phenomenon but does not analyze what linguistic or semantic properties of these persona descriptions cause cross-model refusal or non-compliance.
- What evidence would resolve it: Systematic feature analysis of high-failure personas (length, sentiment, topic, syntax) combined with controlled persona modifications to identify causal factors.

## Limitations
- Parsing failures and persona exclusion: Nine personas were excluded due to complete parsing failures across all models, potentially biasing family-level comparisons.
- Grok family outlier: The Grok family shows the opposite pattern to other families (low robustness + high susceptibility), which suppresses overall robustness-susceptibility correlations.
- Robustness metric sensitivity: The robustness measure depends on sampling variance across n=10 repetitions rather than direct probability computation.

## Confidence
- **High confidence**: Model family accounts for most variance in moral robustness, with Claude family showing highest robustness.
- **Medium confidence**: Larger models within families show higher moral susceptibility to persona conditioning.
- **Medium confidence**: Robustness and susceptibility are positively correlated, especially at the family level.

## Next Checks
1. **Probability-based estimation validation**: Implement the probability-based robustness/susceptibility computation (Eq. 8) using log-probability access where available. Compare rankings and values to the sampling-based estimates to quantify the approximation error and determine if qualitative conclusions change.

2. **Family mechanism ablation**: Test a new model family with known different alignment procedure (e.g., Mistral, Cohere) to determine whether robustness differences reflect family-inherent characteristics or specific alignment methodologies. This would clarify whether "family" is the right explanatory variable.

3. **Prompt sensitivity analysis**: Systematically vary persona description length and detail for a subset of models to test whether susceptibility scales with persona information content. This would validate whether the observed size-susceptibility relationship reflects genuine latent space plasticity versus prompt sensitivity artifacts.