---
ver: rpa2
title: 'GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs'
arxiv_id: '2507.08107'
source_url: https://arxiv.org/abs/2507.08107
tags:
- knowledge
- dblp
- sparql
- query
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRASP is a zero-shot approach for generating SPARQL queries from
  natural language questions over arbitrary RDF knowledge graphs. It uses a large
  language model to explore the graph by executing SPARQL queries and searching for
  relevant IRIs and literals through a fixed set of functions.
---

# GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs

## Quick Facts
- arXiv ID: 2507.08107
- Source URL: https://arxiv.org/abs/2507.08107
- Reference count: 40
- Primary result: State-of-the-art zero-shot SPARQL generation from natural language questions across multiple RDF knowledge graphs

## Executive Summary
GRASP is a zero-shot approach for generating SPARQL queries from natural language questions over arbitrary RDF knowledge graphs. It leverages a large language model to explore the graph by executing SPARQL queries and searching for relevant IRIs and literals through a fixed set of functions. The method achieves state-of-the-art results on Wikidata benchmarks without requiring fine-tuning or few-shot examples, and demonstrates competitive performance using open-source models like Qwen2.5 72B.

## Method Summary
GRASP uses a large language model to explore RDF knowledge graphs by executing SPARQL queries and searching for relevant IRIs and literals through a fixed set of functions. The approach operates in a zero-shot manner, meaning it doesn't require fine-tuning or few-shot examples. The LLM iteratively generates and refines SPARQL queries by exploring the graph structure, using the fixed function set to navigate relationships between entities and retrieve relevant data. Performance can be further improved with feedback and few-shot examples, though the core method remains zero-shot.

## Key Results
- 72.5% F1 score on QALD-10 Wikidata benchmark
- 79.4% F1 score on QALD-7 Wikidata benchmark
- State-of-the-art performance without fine-tuning or few-shot examples
- Competitive results with open-source models like Qwen2.5 72B

## Why This Works (Mechanism)
GRASP works by combining the reasoning capabilities of large language models with structured graph exploration. The LLM acts as a reasoning engine that can understand natural language questions and translate them into structured SPARQL queries. The fixed set of exploration functions provides a controlled interface for the LLM to navigate the RDF graph without getting lost in the vast search space. This approach allows the model to discover relevant entities, relationships, and literals through iterative query execution and refinement.

## Foundational Learning

1. **RDF and SPARQL Basics** - Why needed: Understanding the data model and query language that GRASP operates on
   Quick check: Can you write a basic SPARQL query to retrieve all triples about a specific entity?

2. **Knowledge Graph Embeddings** - Why needed: Understanding how entities and relationships are represented in the graph
   Quick check: What is the difference between IRIs and literals in RDF?

3. **Large Language Model Prompting** - Why needed: Understanding how LLMs can be directed to perform structured tasks
   Quick check: How do zero-shot prompts differ from few-shot prompts in LLM applications?

4. **Graph Traversal Algorithms** - Why needed: Understanding how the fixed function set navigates the knowledge graph
   Quick check: What are the key considerations when designing graph traversal strategies for SPARQL generation?

## Architecture Onboarding

**Component Map:**
Natural Language Question -> LLM Reasoning Engine -> Fixed Function Set -> SPARQL Query Execution -> Graph Exploration -> Refined SPARQL Query -> Answer Extraction

**Critical Path:**
Question understanding → SPARQL generation → Query execution → Result evaluation → Iterative refinement

**Design Tradeoffs:**
- Zero-shot vs. fine-tuned approaches: GRASP prioritizes generalizability over potentially higher performance on specific KGs
- Fixed function set vs. open-ended exploration: Provides control but may limit complex query capabilities
- LLM-based reasoning vs. rule-based systems: Offers flexibility but depends on LLM quality

**Failure Signatures:**
- Poor question understanding leading to irrelevant queries
- Fixed function set limitations preventing complex relationship discovery
- LLM hallucinations generating syntactically invalid SPARQL
- Overfitting to specific KG structures during exploration

**First 3 Experiments to Run:**
1. Test GRASP on a simple Wikidata query with known ground truth to verify basic functionality
2. Run GRASP on a complex multi-hop query to identify fixed function set limitations
3. Compare performance between GPT-4.1 and Qwen2.5 72B on identical benchmark queries

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains primarily demonstrated on Wikidata, with limited cross-KG validation
- Use of feedback and few-shot examples somewhat contradicts the zero-shot framing
- Fixed function set may not scale well for highly complex or domain-specific queries
- Limited external validation shown by average FMR of 0.565 with zero citations in neighbor corpus analysis

## Confidence

**Wikidata benchmark results:** High (specific metrics provided, though limited to one KG)
**Cross-KG generalization:** Medium (claimed but under-specified)
**Open-source model performance:** Low (described as competitive without quantitative comparison)
**Zero-shot capability:** Medium (partially contradicted by feedback/few-shot improvements)

## Next Checks

1. Request detailed performance metrics for GRASP on DBLP, DBpedia, and ORKG benchmarks separately, including per-query analysis to identify failure patterns
2. Conduct ablation studies removing the feedback and few-shot components to quantify their contribution to the "zero-shot" performance
3. Perform head-to-head comparisons between GPT-4.1 and Qwen2.5 72B on identical queries across all benchmark KGs with statistical significance testing