---
ver: rpa2
title: 'Merlin: Multi-View Representation Learning for Robust Multivariate Time Series
  Forecasting with Unfixed Missing Rates'
arxiv_id: '2506.12459'
source_url: https://arxiv.org/abs/2506.12459
tags:
- missing
- forecasting
- time
- learning
- merlin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses robustness of multivariate time series forecasting
  models when data is incomplete, with varying missing rates over time. It proposes
  a method called Merlin that combines offline knowledge distillation and multi-view
  contrastive learning to align semantics between incomplete and complete observations,
  and across different missing rates.
---

# Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates

## Quick Facts
- **arXiv ID:** 2506.12459
- **Source URL:** https://arxiv.org/abs/2506.12459
- **Reference count:** 40
- **Primary result:** Merlin improves MTSF accuracy under varying missing rates by combining offline knowledge distillation with multi-view contrastive learning, achieving 19.84 MAE on PEMS04 at 90% missing rate versus 27.43 for best baseline.

## Executive Summary
Merlin addresses the challenge of multivariate time series forecasting (MTSF) when data is incomplete with varying missing rates over time. The method combines offline knowledge distillation with multi-view contrastive learning to align semantics between incomplete and complete observations, and across different missing rates. Experiments on four real-world datasets demonstrate significant improvements in forecasting accuracy compared to baselines, particularly at high missing rates, while enabling models to handle varying missing rates without retraining.

## Method Summary
Merlin trains a teacher model on complete observations, then uses knowledge distillation to transfer semantic understanding to a student model trained on incomplete data. The student learns from multiple views of the same data with different missing rates simultaneously. Multi-view contrastive learning aligns representations across these views, creating missing-rate-invariant embeddings. The approach uses an STID backbone with spatial-temporal identity embeddings to provide global context even when local information is missing. Only the student model is used at inference time, making it robust to unfixed missing rates.

## Key Results
- On PEMS04 dataset with 90% missing rate, Merlin achieves MAE of 19.84 versus 27.43 for best baseline
- Significant improvements across all tested missing rates (25%, 50%, 75%, 90%) on all four datasets
- Maintains performance when test missing rates differ from training rates, without requiring model retraining
- Ablation studies confirm both knowledge distillation and contrastive learning components are essential

## Why This Works (Mechanism)

### Mechanism 1: Representation Alignment via Offline Knowledge Distillation
If a teacher model is trained on complete data, it can transfer "semantic completeness" to a student model forced to learn from incomplete data. The framework pre-trains a teacher on complete observations (X). During student training on masked observations (X_M), the student is penalized via MSE if its hidden representations (H^L_E) or outputs (Y) deviate from the teacher's. This forces the student to hallucinate or infer the missing "semantics" (periodicity/trends) to match the teacher's state, rather than just fitting the sparse data.

### Mechanism 2: Robustness to "Unfixed" Rates via Multi-View Contrastive Learning
If observations from the same timestamp but with different missing rates are treated as positive pairs, the model learns a missing-rate-invariant representation. The student processes multiple views of the same data (e.g., X_{M,25%} and X_{M,90%}). Contrastive loss (L_{CL}) pulls these representations closer while pushing apart representations from different timestamps. This explicitly prevents the model from overfitting to a specific missing pattern (e.g., "90% missing means zero output").

### Mechanism 3: Semantic Preservation via Identity Embeddings
Explicit spatial and temporal identity embeddings allow the model to anchor predictions even when sequential local information is destroyed by missing values. The backbone (STID) injects Spatial Embeddings (SE) and Temporal Embeddings (TD_E, TW_E) into the representation. Even if the raw input X is zeroed out by masking, these embeddings retain the "global" context (where and when this is), helping the model reconstruct the "local" missing details.

## Foundational Learning

**Concept: Knowledge Distillation (KD)**
- Why needed here: You must understand how a "Teacher" network can supervise a "Student" network using soft labels (logits) or intermediate features, rather than just ground truth labels.
- Quick check question: Can you explain why minimizing MSE between the teacher's hidden state and the student's hidden state differs from standard supervised loss?

**Concept: Contrastive Learning (InfoNCE/NT-Xent)**
- Why needed here: This paper relies on creating "views" of data. You need to understand how temperature (τ) and positive/negative sampling shape the embedding space.
- Quick check question: In this paper, are "positive pairs" two different time steps with the same missing rate, or the same time step with different missing rates?

**Concept: Multivariate Time Series Embeddings**
- Why needed here: The solution depends on the model knowing "when" and "where" data is missing.
- Quick check question: How does the STID backbone incorporate the "identity" of a sensor separate from its time-series values?

## Architecture Onboarding

**Component map:**
- Data Preprocessing -> Random Masking (25%, 50%, 75%, 90%) -> STID Backbone (Student) -> Loss Calculation (L_Pre, L_HD, L_RD, L_CL) -> Student Model
- Complete Data -> STID Backbone (Teacher) -> Frozen Teacher Parameters -> Loss Calculation (L_HD, L_RD)

**Critical path:**
1. Pre-training: Train Teacher on raw complete data; freeze weights
2. View Generation: For a batch, generate m versions of input with varying mask rates (25% - 90%)
3. Forward Pass: Pass all views through Student; pass original clean data through Teacher
4. Loss Calculation: Compute L_KD (align student to teacher) and L_CL (align student views to each other)
5. Inference: Discard Teacher; run Student on test data (which may have any missing rate)

**Design tradeoffs:**
- Training Efficiency: Merlin requires training m forward passes per batch (for different missing rates), increasing memory usage vs. a single-pass model
- Teacher Dependency: The system relies on the assumption that "complete" training data exists. If the training set is also corrupted, Teacher quality limits Student potential
- Imputation vs. Robustness: The authors trade off explicit data recovery (imputation) for implicit semantic recovery (representation learning). You cannot visualize the "filled" data points, only the final forecast

**Failure signatures:**
- Collapse to Zero: If L_CL weight is too high or batch size too small, the model may learn to map all inputs to a single point
- Teacher Overfit: If the Teacher model overfits the training set, the Student learns to mimic noise
- Unfixed Rate Drift: If test missing rates (e.g., 95%) significantly exceed the max training mask (e.g., 90%), performance may degrade rapidly

**First 3 experiments:**
1. Teacher Sanity Check: Train the Teacher model on complete data and verify it achieves SOTA baseline metrics. If the Teacher is weak, distillation is useless.
2. Ablation on Views: Train the student with fixed missing rates (e.g., only 50%) vs. unfixed (multi-view). Verify that multi-view training is necessary for robustness to unfixed test rates.
3. Loss Weight Sensitivity: Tune β1, β2, β3. The paper suggests β1 (Forecast Loss) is dominant, but check if removing β3 (Contrastive) causes the specific failure mode of "unfixed rate" test errors.

## Open Questions the Paper Calls Out
- **Cross-architecture distillation:** In future work, the authors plan to investigate the effects of offline knowledge distillation when the teacher and student models utilize different neural network structures.
- **Block missing patterns:** The paper primarily evaluates random point missing scenarios, though real-world malfunctions often result in data loss patterns that differ from random distribution.
- **Teacher noise threshold:** While the paper shows Merlin is robust to slight noise in the teacher's training data, the "breaking point" where the teacher becomes an unreliable source of truth remains undefined.

## Limitations
- **Computational overhead:** Training requires multiple forward passes per batch, increasing memory and computational requirements compared to single-pass approaches.
- **Teacher quality dependency:** The method critically depends on having a reliable teacher model trained on complete data, limiting applicability when training data itself contains significant missingness.
- **Random missing assumption:** The method assumes missing data follows random point masking rather than structured patterns (like entire sensor failures), which may not capture all real-world scenarios.

## Confidence

**High Confidence:** Claims about improving forecasting accuracy on complete data through knowledge distillation, and the basic mechanism of contrastive learning for missing rate robustness are well-supported by the results.

**Medium Confidence:** Claims about "unfixed missing rates" being truly handled without retraining, as the method is only tested up to 90% missing rate and assumes training covers the test distribution of missing rates.

**Low Confidence:** Claims about performance on extreme missing rates (>90%) and generalization to fundamentally different data patterns (e.g., non-periodic, non-spatial data) lack empirical validation.

## Next Checks
1. **Teacher Quality Validation:** Systematically evaluate how teacher model quality (trained on varying degrees of complete data) impacts student performance across all missing rates.
2. **Extreme Missing Rate Test:** Evaluate Merlin on test data with missing rates exceeding the maximum training rate (e.g., 95-99%) to validate true robustness claims.
3. **Alternative Missing Patterns:** Test the method on structured missingness patterns (e.g., entire sensor failures, temporal blocks) rather than random point masking to assess real-world applicability.