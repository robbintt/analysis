---
ver: rpa2
title: 'Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval
  with Cross-Encoder Listwise Distillation and Synthetic Data'
arxiv_id: '2505.19274'
source_url: https://arxiv.org/abs/2505.19274
tags:
- queries
- training
- retrieval
- query
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that fine-tuning dense retrieval models with standard
  contrastive learning often reduces effectiveness, contrary to common assumptions.
  To address this, the authors propose combining contrastive learning with cross-encoder
  listwise distillation, which leverages richer relevance signals from a teacher model
  rather than treating relevance as binary.
---

# Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval with Cross-Encoder Listwise Distillation and Synthetic Data

## Quick Facts
- **arXiv ID**: 2505.19274
- **Source URL**: https://arxiv.org/abs/2505.19274
- **Reference count**: 22
- **Primary result**: Fine-tuning dense retrievers with standard contrastive learning often degrades performance; combining it with cross-encoder listwise distillation and synthetic queries achieves SOTA among BERT-based retrievers

## Executive Summary
This paper challenges the common assumption that contrastive learning fine-tuning improves dense retrievers, showing it often degrades SOTA models' effectiveness. The authors propose a solution combining contrastive learning with cross-encoder listwise distillation, which leverages richer relevance signals from a teacher model rather than treating relevance as binary. They also demonstrate that training with a diverse mix of synthetic query types (questions, claims, titles, keywords, and user search queries) outperforms single-query-type approaches and is as effective as human-written queries. Using this combined approach, they fine-tune a BERT-base model achieving state-of-the-art performance among BERT-based retrievers, trained solely on synthetic queries and general-purpose corpora.

## Method Summary
The authors address corpus-specific fine-tuning of dense retrievers using InfoNCE contrastive loss, which often degrades SOTA models. Their solution combines contrastive learning with cross-encoder listwise distillation from a teacher model. They generate synthetic queries using Llama-3.1 8B with six types (questions, claims, titles, keywords, zero-shot/few-shot natural queries) from MSMARCO or BEIR datasets. The training uses a combined loss: InfoNCE with hard-negative filtering (threshold 60% of positive score) plus KL-divergence listwise distillation from normalized cross-encoder scores. The model is trained with 4096 batch size, learning rate 2e-4, up to 30 epochs using GradCache, with a 90/10 train/dev split.

## Key Results
- Standard contrastive fine-tuning degrades effectiveness of SOTA retrievers (BGE, GTE, Arctic, E5-unsupervised)
- Combining contrastive learning with cross-encoder listwise distillation prevents degradation and improves performance
- Training with diverse synthetic query types (questions, claims, titles, keywords, user queries) outperforms single-query-type approaches
- Model trained on synthetic queries matches effectiveness of human-written queries on MSMARCO
- Achieves state-of-the-art performance among BERT-based retrievers

## Why This Works (Mechanism)
Conventional contrastive learning treats relevance as binary (relevant vs. not relevant), which is insufficient for fine-tuning already-trained retrievers that need nuanced ranking signals. Cross-encoder listwise distillation provides richer, graded relevance information from the teacher model, capturing relative rankings rather than just relevance judgments. This combination allows the model to learn both the fine-grained relevance distinctions needed for ranking and the semantic embeddings from contrastive learning. The diverse synthetic queries provide broader coverage of query types and reduce domain bias, making the retriever more robust across different retrieval scenarios.

## Foundational Learning
- **InfoNCE contrastive loss**: Contrastive objective that pulls positive pairs together and pushes negative pairs apart in embedding space. Why needed: Core mechanism for learning semantic representations. Quick check: Verify temperature τ=0.01 and hard-negative filtering at 60% threshold.
- **Cross-encoder listwise distillation**: Teacher model provides ranked lists with graded relevance scores rather than binary labels. Why needed: Supplies richer ranking signals than binary relevance for fine-tuning. Quick check: Ensure teacher reranker performance is adequate on target corpus.
- **Synthetic query generation**: Using LLM to create diverse query types from passages. Why needed: Enables training at scale without human annotation. Quick check: Verify <20 word limit and passage relevance filtering works correctly.
- **Hard-negative filtering**: Only using negatives that score above a threshold relative to positives. Why needed: Prevents uninformative negatives that don't contribute to learning. Quick check: Monitor negative count per batch to ensure sufficient hard negatives.
- **Min-max normalization with percentile clipping**: Normalizing teacher scores to [0,1] range with 1st/99th percentile clipping. Why needed: Stabilizes KL-divergence computation. Quick check: Verify normalized scores are in [0,1] range with minimal clipping.
- **GradCache**: Gradient caching optimization for large batch training. Why needed: Enables 4096 batch size with limited memory. Quick check: Monitor GPU memory usage and training speed.

## Architecture Onboarding
**Component map**: Passage corpus -> Synthetic query generator (Llama-3.1 8B) -> Retriever top-20 filter -> Cross-encoder reranker -> Query-passage pairs -> Combined contrastive + distillation trainer -> Fine-tuned retriever
**Critical path**: Query generation → Relevance filtering → Combined loss training → Model evaluation
**Design tradeoffs**: Using synthetic queries vs. human annotations (scale vs. quality), combined loss vs. single objective (complexity vs. performance), BERT-base vs. larger models (efficiency vs. capacity)
**Failure signatures**: Contrastive-only fine-tuning degrades performance; teacher reranker underperforms on certain domains; hard-negative threshold too aggressive/permissive; synthetic queries not properly filtered
**Three first experiments**:
1. Generate 10K synthetic queries and verify passage relevance filtering accuracy
2. Run contrastive-only fine-tuning baseline to confirm degradation effect
3. Test different hard-negative filtering thresholds (40%, 60%, 80%) on a small corpus

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends heavily on quality of synthetic query generation and teacher model performance
- Method requires significant computational resources (4096 batch size, 30 epochs, GradCache)
- Claims about synthetic query effectiveness are primarily validated on MSMARCO, with limited BEIR validation
- Hard-negative filtering threshold of 60% was empirically determined and may not generalize across domains

## Confidence
- **High confidence**: Empirical observation that conventional contrastive fine-tuning degrades SOTA retrievers' performance
- **Medium confidence**: Listwise distillation + contrastive learning combination is necessary for improvement
- **Medium confidence**: Diverse synthetic query types are "as effective as human-written queries" (primarily MSMARCO validated)
- **Medium confidence**: State-of-the-art performance among BERT-based retrievers (based on published comparisons)

## Next Checks
1. Re-run the contrastive-only fine-tuning baseline on the same synthetic data to verify degradation effect across different base retrievers and datasets
2. Test the hard-negative filtering threshold sensitivity by evaluating performance at 40%, 60%, and 80% thresholds
3. Validate synthetic query effectiveness claim by comparing model performance when training with human-written queries versus synthetic queries on the same corpus