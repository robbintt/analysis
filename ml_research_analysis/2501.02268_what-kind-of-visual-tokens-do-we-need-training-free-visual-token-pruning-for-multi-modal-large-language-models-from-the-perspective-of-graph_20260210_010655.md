---
ver: rpa2
title: What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for
  Multi-modal Large Language Models from the Perspective of Graph
arxiv_id: '2501.02268'
source_url: https://arxiv.org/abs/2501.02268
tags:
- tokens
- visual
- g-prune
- token
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visual redundancy in multimodal
  large language models (MLLMs), where excessive visual tokens lead to high computational
  costs and redundant information. The authors propose a graph-based method called
  G-Prune, which treats visual tokens as nodes in a graph and constructs connections
  based on semantic similarities.
---

# What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph

## Quick Facts
- arXiv ID: 2501.02268
- Source URL: https://arxiv.org/abs/2501.02268
- Reference count: 11
- Primary result: G-Prune reduces 63.57% FLOPs on VQA2.0 and TextVQA with only 0.95% and 2.34% accuracy drops respectively

## Executive Summary
This paper addresses the problem of visual redundancy in multimodal large language models (MLLMs), where excessive visual tokens lead to high computational costs and redundant information. The authors propose a graph-based method called G-Prune, which treats visual tokens as nodes in a graph and constructs connections based on semantic similarities. An iterative information propagation algorithm is then used to identify the most representative tokens for each object, which can be from either the foreground or background. Experiments on LLaVA-NeXT show that G-Prune can reduce 63.57% FLOPs on VQA2.0 and TextVQA with only 0.95% and 2.34% accuracy drops, respectively, outperforming existing pruning methods.

## Method Summary
G-Prune constructs a graph where visual tokens are nodes connected by edges representing semantic similarity. The method uses CLIP-based similarity scores to build the graph structure, then applies an iterative information propagation algorithm to identify representative tokens. The algorithm determines which tokens to keep by evaluating their contribution to object representation, considering both foreground and background tokens. The approach is training-free, operating solely on the input image to determine token importance without requiring fine-tuning of the MLLM.

## Key Results
- 63.57% FLOPs reduction on VQA2.0 benchmark
- 0.95% accuracy drop on VQA2.0 with G-Prune
- 2.34% accuracy drop on TextVQA with G-Prune
- Outperforms existing pruning methods on LLaVA-NeXT

## Why This Works (Mechanism)
The graph-based approach works by capturing semantic relationships between visual tokens rather than relying on spatial or positional heuristics. By treating tokens as nodes in a similarity graph, the method can identify which tokens are truly representative of objects regardless of their spatial location. The iterative information propagation algorithm effectively aggregates contextual information from neighboring tokens, allowing the model to recognize when background tokens contain critical information for answering questions. This object-centric approach overcomes limitations of previous methods that assumed foreground tokens are always more important than background tokens.

## Foundational Learning

**Graph-based token representation** - Why needed: Captures semantic relationships between tokens that spatial methods miss
Quick check: Verify edge weights reflect meaningful semantic similarity through qualitative inspection

**CLIP-based similarity scoring** - Why needed: Provides robust cross-modal semantic embeddings for token comparison
Quick check: Confirm similarity scores align with human semantic judgments on sample tokens

**Information propagation algorithms** - Why needed: Aggregates contextual information to identify representative tokens
Quick check: Test propagation convergence and stability across different graph structures

## Architecture Onboarding

**Component map**: Image -> Vision Transformer -> Visual Tokens -> Similarity Graph -> Information Propagation -> Pruned Tokens -> LLM
**Critical path**: Visual tokens → graph construction → information propagation → token selection
**Design tradeoffs**: Training-free approach vs. potential accuracy gains from fine-tuning; computational overhead of graph construction vs. FLOPs savings from pruning
**Failure signatures**: Over-aggressive pruning leading to loss of critical visual information; graph construction failures on complex scenes; propagation instability on large token sets
**First experiments**: 1) Run G-Prune on simple single-object images to verify basic functionality 2) Compare FLOPs reduction vs. accuracy on benchmark datasets 3) Visualize selected vs. pruned tokens to validate object-centric selection

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on CLIP-based similarity scores may not capture fine-grained object-level semantics in complex scenes
- Evaluation limited to LLaVA-NeXT and VQA benchmarks, uncertain generalization to other MLLM architectures
- Graph construction computational overhead not fully characterized relative to pruning benefits

## Confidence
- High confidence in computational efficiency claims (FLOPs reduction measured directly)
- Medium confidence in accuracy retention claims (limited benchmark diversity)
- Medium confidence in object-centric pruning mechanism (semantic similarity assumptions)

## Next Checks
1. Test G-Prune across diverse MLLM architectures beyond LLaVA-NeXT to assess architectural robustness
2. Evaluate on non-VQA visual reasoning tasks (e.g., visual grounding, image captioning) to validate broader applicability
3. Conduct ablation studies removing CLIP similarity constraints to isolate the contribution of graph-based representation from similarity metric choice