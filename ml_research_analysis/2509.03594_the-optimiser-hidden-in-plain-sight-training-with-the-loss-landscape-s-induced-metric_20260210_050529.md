---
ver: rpa2
title: 'The Optimiser Hidden in Plain Sight: Training with the Loss Landscape''s Induced
  Metric'
arxiv_id: '2509.03594'
source_url: https://arxiv.org/abs/2509.03594
tags:
- metric
- momentum
- optimiser
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new class of neural network optimisers\
  \ based on the Riemannian metric naturally induced when the loss landscape is embedded\
  \ in higher-dimensional space. This metric, which underlies common visualisations\
  \ of loss landscapes, automatically adapts the effective learning rate based on\
  \ local curvature\u2014decreasing it in highly curved regions and maintaining larger\
  \ updates in flatter areas."
---

# The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric

## Quick Facts
- **arXiv ID**: 2509.03594
- **Source URL**: https://arxiv.org/abs/2509.03594
- **Reference count**: 33
- **Primary result**: Novel optimizers based on the Riemannian metric induced by embedding loss landscapes in higher dimensions, achieving competitive performance with AdamW and slight improvements on average across diverse tasks

## Executive Summary
This paper introduces a new class of neural network optimizers derived from the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. The key insight is that this metric, which underlies common visualizations of loss landscapes, automatically adapts the effective learning rate based on local curvature—decreasing it in highly curved regions and maintaining larger updates in flatter areas. The resulting algorithms can be viewed as a smoothed form of gradient clipping and, in one variant, induce a scheduled learning rate with both warm-up and decay phases. Benchmarking across diverse tasks shows strong performance, with one RMSprop-based variant performing competitively with state-of-the-art methods like Adam, AdamW, and Muon.

## Method Summary
The optimizers are based on pulling back the Euclidean metric from an (N+1)-dimensional ambient space (parameters plus loss) to the parameter space, creating a preconditioner that adapts to local curvature. The core update uses the Sherman-Morrison formula to efficiently invert the rank-1 updated metric, yielding an O(N) complexity comparable to Adam. The method can be applied as a modification to any existing preconditioning method and naturally incorporates decoupled weight decay. Three variants are explored: a basic version using identity preconditioning, a log-loss embedding that induces implicit learning rate scheduling, and an RMSprop-based variant that showed the best average performance.

## Key Results
- In low-dimensional pathological functions, the optimizers achieved superior convergence and were the only methods to find global minima across all tested functions
- For neural network training on MNIST, CIFAR-10, and TinyShakespeare, the RMSprop-based variant performed competitively with state-of-the-art methods like Adam, AdamW, and Muon
- The log-loss embedding variant was highly effective in low dimensions but showed inconsistent performance in higher-dimensional tasks
- Computational efficiency is maintained at O(N) complexity, requiring only one additional dot product computation per iteration beyond baseline optimizers

## Why This Works (Mechanism)

### Mechanism 1: Pull-Back Metric Induces Curvature-Aware Scaling
Embedding the loss landscape in an (N+1)-dimensional ambient space and pulling back the metric to parameter space yields a preconditioner that automatically reduces step sizes in high-curvature regions. The ambient space has coordinates X^i = θ^i (parameters) and X^{N+1} = L (loss). The pull-back metric is g_{ij} = γ_{ij} + (∂L/∂θ^i)(∂L/∂θ^j). Applying the Sherman-Morrison formula to invert gives an update: δθ^i = -η × l^i / (1 + ξ Σ_k l_k l^k), where l^i = Σ_j γ^{ij} ∂L/∂θ^j. The denominator scales with gradient norm squared, reducing effective learning rate when gradients are large.

### Mechanism 2: Smoothed Gradient Clipping via Normalization Denominator
The normalization term 1/(1 + ξ Σ_k l_k l^k) functions as a smooth, continuous form of gradient clipping that prevents divergence while preserving gradient direction. Unlike hard clipping which truncates at a threshold, the denominator grows smoothly with gradient magnitude. For large gradients, the update approaches δθ^i ≈ -η/(ξ||l||²) × l^i, scaling down smoothly. For small gradients, δθ^i ≈ -η × l^i, preserving full step size.

### Mechanism 3: Log-Loss Embedding Induces Implicit Learning Rate Schedule
Using f(L(θ)) = ln(L(θ)) as the embedding function yields an effective learning rate with warm-up and decay phases without explicit scheduling. The update becomes δθ^i = -η × L(θ) / (L(θ)² + ξ Σ_k l_k l^k) × l^i. Under assumptions that L(θ) ∝ t^{-p} + C (power-law decay) and roughly constant gradients, the ratio L/(L² + const) creates early warm-up (L small relative to gradient term) then decay (L dominates denominator as it shrinks).

## Foundational Learning

- **Concept: Riemannian Metric and Pull-Back**
  - Why needed: The core innovation is treating the loss landscape as a Riemannian manifold embedded in ambient space. Understanding how metrics pull back from ambient to parameter space is essential to grasp why the denominator structure emerges.
  - Quick check: If you embed a 2D surface in 3D Euclidean space, does the induced metric on the surface measure distances along the surface or through the ambient space?

- **Concept: Preconditioning and Gradient Flow**
  - Why needed: The optimizer is a preconditioning method where g^{ij} transforms gradients. Recognizing how different metrics (diagonal vs. full, history-dependent vs. instantaneous) affect optimization helps compare to Adam, RMSprop, Muon.
  - Quick check: In Adam, is the implied preconditioning metric diagonal or does it have off-diagonal coupling between parameters?

- **Concept: Sherman-Morrison Formula**
  - Why needed: Efficient O(N) inversion of the rank-1 updated metric g_{ij} = γ_{ij} + v_i v_j relies on this formula. Without it, the method would require O(N²) or O(N³) matrix operations.
  - Quick check: Given (A + uv^T), what is the computational cost of computing (A + uv^T)^{-1}w for a vector w using Sherman-Morrison vs. direct inversion?

## Architecture Onboarding

- **Component map**: Gradient computation -> Preconditioning layer -> Metric scaling -> Normalization -> Update with momentum and weight decay
- **Critical path**:
  1. Compute gradients (standard)
  2. Apply γ^{-1} preconditioning (identity or RMSprop-style)
  3. Compute dot product s_t = ξ × Σ(g^i × g^i) ← **single additional O(N) operation beyond baseline**
  4. Update EMA v_t and compute scaling r_t
  5. Apply momentum, scaling, and weight decay
- **Design tradeoffs**:
  - ξ hyperparameter: Controls strength of curvature adaptation. Paper suggests ξ ~ 1/N as starting point. Too small → reduces to baseline optimizer; too large → excessive damping, slow convergence.
  - β (EMA decay): Paper uses smaller β than Adam (opposite regime) because instantaneous gradient norm is desired but must be approximated from batches.
  - Choice of γ^{-1}: Identity is simplest; RMSprop-based showed best average performance but adds β_rms hyperparameter; could substitute any preconditioner (Muon mentioned but not tested).
  - Log-loss embedding: Adds implicit scheduling but showed inconsistent results; may work better when loss dynamics match pedagogical assumptions.
- **Failure signatures**:
  - Training stalls with tiny updates: ξ too large, denominator dominates → reduce ξ by 10×
  - Divergence in early training: β too small for batched data, noisy gradient norm estimates → increase β
  - Log-loss variant oscillates or diverges: Loss may not be positive-definite (required) or loss scale mismatched → check L > 0, consider normalizing loss or switching to basic variant
  - No improvement over baseline: γ^{-1} = identity on well-conditioned problem may show minimal gain → try RMSprop-based variant or verify hyperparameter sweep includes appropriate ξ range
- **First 3 experiments**:
  1. **Sanity check on quadratic**: Optimize f(x) = x^T A x with A having high condition number. Compare SGD vs. basic induced-metric optimizer (γ = I). Expect induced metric to reduce oscillations. Verify effective learning rate adapts.
  2. **Ablation on ξ**: On CIFAR-10 with small CNN, sweep ξ ∈ {0, 0.1/N, 1/N, 10/N, 100/N} with fixed β = 0.8, η = 0.01. Plot validation accuracy vs. ξ to identify sweet spot. Confirm ξ = 0 matches baseline SGD-with-momentum.
  3. **RMS variant vs. AdamW head-to-head**: On TinyShakespeare with 4-layer transformer (paper's setup), run 50+ seeds each with Bayesian hyperparameter optimization. Compare best-50-run distributions for validation perplexity. Verify paper's claim that RMS variant shows "slight improvement on average." Check per-iteration wall-clock time overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the log-loss embedding variant succeed in low-dimensional tasks but exhibit inconsistent performance in higher-dimensional neural network training?
- Basis in paper: The author notes the log-loss optimiser was the only one to find global minima in low dimensions but performed poorly on regression and language tasks, stating, "Why this optimiser was successful for certain tasks only remains unclear."
- Why unresolved: The paper provides empirical benchmarks showing the divergence in performance but lacks a theoretical analysis of how the log-transformation interacts with the gradient variance or curvature in high-dimensional parameter spaces.
- What evidence would resolve it: A theoretical analysis linking the log-loss scaling factor $\frac{L(\theta)}{L(\theta)^2 + |\hat{v}_t|}$ to the dimensionality $N$ of the parameter space, or empirical ablation studies isolating the effect of dimensionality on the metric's stability.

### Open Question 2
- Question: Does adding $k$ extra dimensions to the ambient space (one for each of $k$ independent loss contributions) improve optimisation by capturing the geometry of composite loss functions?
- Basis in paper: The conclusion asks, "In cases where there are k independent contributions to the loss, is it beneficial to add k extra dimensions, and pull-back to the loss-landscape from that space?"
- Why unresolved: The current framework embeds the landscape into $N+1$ dimensions using a scalar loss, but it is unknown if the geometric benefits are preserved when pulling back from a higher-dimensional ambient space representing multiple loss components.
- What evidence would resolve it: Deriving the pull-back metric for a multi-output embedding and benchmarking the resulting optimiser on multi-task learning problems against the scalar loss version.

### Open Question 3
- Question: Are there alternative embedding functions $f(\mathcal{L}(\theta))$ that retain the desirable "warm-up" scheduling properties of the log-loss embedding while maintaining stability in high dimensions?
- Basis in paper: The author asks, "Are there other choices of the function $f(\mathcal{L}(\theta))$, that have interesting or useful properties?" and specifically questions if other functions might succeed where log-loss failed in high dimensions.
- Why unresolved: The paper tests linear and log embeddings, but the vast space of monotonic functions remains unexplored; the geometric interaction between $f$ and the effective learning rate $r_t$ is not fully characterized.
- What evidence would resolve it: A systematic search or theoretical derivation of embedding functions that normalize the gradient magnitude independent of parameter dimensionality.

### Open Question 4
- Question: Does turning on the off-diagonal elements of the ambient metric $\gamma$ provide optimisation benefits that outweigh the associated computational costs?
- Basis in paper: The conclusion queries, "Is there any benefit to turning on the off-diagonal elements of the metric on the ambient space...?"
- Why unresolved: The paper relies on diagonal metrics (Identity or RMSprop) for $O(N)$ efficiency; the theoretical benefits of capturing parameter correlations via off-diagonal terms are proposed but not tested.
- What evidence would resolve it: Implementation of the optimiser with a full or block-diagonal metric matrix $\gamma$ (rather than diagonal) and comparison of convergence speed against the computational overhead of matrix inversion.

## Limitations
- The log-loss embedding variant showed highly variable and task-dependent performance, with inconsistent results in high-dimensional neural network training despite theoretical appeal
- The RMS variant introduces an extra hyperparameter (β_rms) that wasn't fully explored in ablation studies
- Computational efficiency claims rely on γ being approximately diagonal; if this assumption fails, the O(N) complexity advantage disappears
- Comparison against specialized optimizers like Muon was limited to specific architectures and may not generalize to all deep learning scenarios

## Confidence
- **High confidence**: Basic induced-metric optimizer performance (consistently outperforms or matches baselines), computational complexity analysis (O(N) with Sherman-Morrison), and mechanism of curvature-adaptive scaling through the pull-back metric
- **Medium confidence**: RMS variant's slight improvement over AdamW (based on 50+ run averages but with limited hyperparameter tuning), and the smoothed gradient clipping interpretation
- **Low confidence**: Log-loss embedding variant's implicit scheduling mechanism (inconsistent high-dimensional results, relies on questionable pedagogical assumptions), and claims about generality across all deep learning architectures

## Next Checks
1. **Power-law loss validation**: For the log-loss variant, systematically measure whether validation loss actually follows t^{-p} + C during training across different tasks. If not, quantify the mismatch between assumed and observed loss dynamics to explain the variant's inconsistent performance.

2. **Diagonal γ assumption test**: Construct pathological cases where γ is not approximately diagonal (e.g., highly coupled parameters) and measure computational overhead and optimization performance degradation. This would validate the paper's claim about maintaining O(N) complexity.

3. **Muonic comparison replication**: Following the paper's TinyShakespeare setup, conduct a head-to-head comparison between the RMS variant and Muon with equal hyperparameter optimization budgets (50+ runs each). Measure both final performance distributions and wall-clock time to verify the claimed "slight improvement on average" while accounting for implementation differences.