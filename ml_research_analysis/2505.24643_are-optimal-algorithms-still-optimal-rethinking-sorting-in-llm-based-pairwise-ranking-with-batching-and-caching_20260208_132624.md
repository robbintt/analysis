---
ver: rpa2
title: Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise
  Ranking with Batching and Caching
arxiv_id: '2505.24643'
source_url: https://arxiv.org/abs/2505.24643
tags:
- quicksort
- inference
- ranking
- bubblesort
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of LLM-based
  pairwise ranking prompting (PRP) by proposing a novel framework that optimizes sorting
  algorithms through batching and caching. The core method idea is to shift the cost
  model from counting pairwise comparisons to counting LLM inference calls, recognizing
  that each comparison in PRP requires an expensive inference.
---

# Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching

## Quick Facts
- arXiv ID: 2505.24643
- Source URL: https://arxiv.org/abs/2505.24643
- Reference count: 4
- Primary result: Novel framework optimizes LLM-based pairwise ranking by shifting cost model to inference calls, achieving 44% fewer calls using Quicksort with batching vs. Heapsort

## Executive Summary
This paper addresses the computational inefficiency of LLM-based pairwise ranking prompting (PRP) by proposing a novel framework that optimizes sorting algorithms through batching and caching. The core method idea is to shift the cost model from counting pairwise comparisons to counting LLM inference calls, recognizing that each comparison in PRP requires an expensive inference. The authors demonstrate that classical sorting algorithms like Heapsort, optimal under traditional comparison counts, become less efficient when LLM inferences dominate the cost. They show that optimizations such as batching (grouping multiple comparisons into a single inference) and caching (reusing prior comparison results) can significantly reduce inference counts. Experiments on standard ranking benchmarks validate these findings, demonstrating that Quicksort with batching is 5.52× faster than Heapsort while maintaining similar ranking performance (nDCG@10).

## Method Summary
The paper proposes a framework that optimizes sorting algorithms for LLM-based pairwise ranking by recognizing that inference call cost, not comparison count, determines efficiency. The authors implement three sorting algorithms: Heapsort (baseline, no batching/caching), Bubblesort with caching of repeated adjacent comparisons, and Quicksort with batching during partition (multiple elements vs pivot in one inference). They use zero-shot PRP with Flan-T5-L/XL/XXL, Mistral-7B-Instruct, and Llama-3-8B-Instruct models on TREC DL 2019, TREC DL 2020, and BEIR datasets, measuring inference counts, latency, and nDCG@10 ranking quality.

## Key Results
- Quicksort with batch size 2 reduces inference calls by 44% compared to Heapsort baseline
- Quicksort with batching achieves 5.52× faster end-to-end PRP runtime than Heapsort
- Bubblesort with caching reduces inference calls by 46% on average
- All algorithms maintain similar ranking quality (nDCG@10 ≈ 0.605-0.608)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting the cost model from comparison counts to LLM inference calls inverts classical sorting algorithm efficiency rankings.
- Mechanism: Classical analysis treats each comparison as an atomic, uniform-cost operation. In LLM-based PRP, each comparison triggers an expensive inference call, making the *structure* of comparisons (not just count) determinative. Algorithms enabling parallel comparisons per inference gain advantage over those with inherently sequential structures.
- Core assumption: LLM inference latency dominates over all other computational costs in the ranking pipeline.
- Evidence anchors:
  - [abstract] "our analysis reveals that expensive LLM inferences overturn these predictions"
  - [section 1] "each pairwise comparison requires an expensive LLM inference, making a naive all-pairs approach prohibitively costly"
  - [corpus] Weak direct evidence—neighbor papers focus on algorithm-LLM integration broadly but not this specific cost model inversion.
- Break condition: If inference latency drops to near-zero (e.g., via extreme model compression or caching at inference-provider level), comparison count reverts as the dominant cost metric.

### Mechanism 2
- Claim: Batching independent comparisons into single inference calls yields ~44% reduction in inference calls for Quicksort vs. Heapsort at batch size 2.
- Mechanism: Quicksort's partition phase compares multiple elements against a single pivot *independently*—these can be issued as one batched prompt. Heapsort's binary heap structure enforces sequential, dependent comparisons that cannot be parallelized. The paper demonstrates this via the batching diagram (Figure 2) showing simultaneous pivot comparisons.
- Core assumption: The LLM serving infrastructure supports batch inference with near-linear throughput scaling (validated on A100 up to batch size 8).
- Evidence anchors:
  - [section 3] "Quicksort uniquely enables batching through its partition phase, where multiple elements can be evaluated simultaneously against a pivot"
  - [section 5] "with a batch size of 2, the average number of inference calls is reduced already by almost 45%"
  - [corpus] No direct corpus validation of batching-specific gains for sorting.
- Break condition: If GPU memory constraints limit batch sizes to 1, or if serving infrastructure doesn't support dynamic batching, Quicksort's advantage disappears.

### Mechanism 3
- Claim: Caching reduces Bubblesort inference calls by ~46% without altering final rankings.
- Mechanism: Bubblesort's repeated adjacent comparisons across passes create redundant queries (same document pairs compared multiple times). A dictionary cache stores prior LLM judgments, returning cached results instead of re-inference. Figure 1 illustrates this—dashed arrows show cached lookups replacing solid inference calls.
- Core assumption: Pairwise comparison outcomes are deterministic and stable (no temperature/sampling variation), making caching valid.
- Evidence anchors:
  - [section 3] "Bubblesort has been considered expensive due to its O(n²) complexity, but this can be adapted via caching from its repeated adjacent comparisons across passes"
  - [section 5] "reducing the total number of inferences by an average of 46%"
  - [corpus] No corpus papers address caching for LLM sorting specifically.
- Break condition: If LLM outputs are non-deterministic (sampling with temperature > 0), cached results may not reflect current inference behavior, breaking ranking consistency.

## Foundational Learning

- Concept: **Pairwise Ranking Prompting (PRP)**
  - Why needed here: The entire paper optimizes PRP's sorting phase; understanding that PRP reduces ranking to pairwise document comparisons is prerequisite.
  - Quick check question: Given 100 documents, why does PRP require sorting rather than direct scoring?

- Concept: **Comparison Complexity vs. Inference Complexity**
  - Why needed here: The paper's central thesis requires distinguishing between O(n log n) comparison count and actual inference call patterns under batching/caching.
  - Quick check question: If an algorithm makes 200 comparisons but can batch 4 per inference, how many calls are needed?

- Concept: **GPU Batch Inference Scaling**
  - Why needed here: The practical efficiency gains depend on hardware batch throughput; Figure 5 shows saturation points differ by GPU architecture.
  - Quick check question: At what batch size does RTX 3090 throughput saturate, and why does this matter for algorithm selection?

## Architecture Onboarding

- Component map: Query + Documents → BM25 Retriever (top-100) → Sorting Algorithm (Heapsort/Bubblesort/Quicksort) → Batch/Caching Layer ← LLM Inference Engine → Ranked Top-k Results

- Critical path:
  1. Implement PRP prompt template (Figure 2a shows baseline; 2b shows batched variant)
  2. Wrap LLM client with cache dictionary (key: sorted doc-pair tuple → value: comparison result)
  3. Modify sorting algorithm to collect independent comparisons before batch dispatch
  4. For Quicksort: during partition, gather all pivot comparisons → single batched inference
  5. For Bubblesort: check cache before each comparison; store result after inference

- Design tradeoffs:
  - **Quicksort + Batching**: Best for latency-sensitive apps with batch-capable GPUs; requires batch size ≥2 for advantage
  - **Bubblesort + Caching**: Competitive on some datasets (scifact, touche2020); better for intransitivity-prone LLMs (adjacent comparisons more stable)
  - **Heapsort**: Only optimal when batching unavailable (batch size = 1); natural top-k extraction via heap properties

- Failure signatures:
  - nDCG@10 drops significantly → check if cache invalidation needed (non-deterministic sampling)
  - No speedup despite batching → verify GPU supports batch inference; check for sequential API calls
  - Quicksort slower than expected → pivot selection may create unbalanced partitions; try median-of-three strategy
  - Memory overflow → batch size too large for GPU VRAM; reduce batch size

- First 3 experiments:
  1. **Baseline validation**: Run Heapsort vs. Quicksort (batch=1) on TREC DL 2019 subset; confirm Heapsort uses fewer inferences (per classical theory).
  2. **Batch scaling test**: Measure inference count and latency for Quicksort at batch sizes [1, 2, 4, 8, 16] on BEIR subset; identify saturation point for your GPU.
  3. **Cache effectiveness**: Implement Bubblesort with/without caching; verify 40-50% inference reduction on 2-3 datasets with varied document similarity distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ranking algorithms that do not assume transitivity offer practical advantages over standard sorting methods in LLM-based pairwise ranking?
- Basis in paper: [explicit] The authors state in the Limitations section that "Future work could examine how much performance is degraded and whether ranking algorithms that do not assume transitivity can actually offer any practical advantage."
- Why unresolved: LLMs frequently yield inconsistent or intransitive judgments for similar documents, but standard sorting algorithms (like Quicksort) used in the study strictly require transitivity to function correctly.
- What evidence would resolve it: Experiments comparing standard algorithms against non-transitive ranking methods (e.g., noisy sorting or graph-based aggregation) on datasets specifically designed to elicit inconsistent LLM preferences.

### Open Question 2
- Question: Can active ranking strategies or noisy sorting algorithms be integrated with batching to achieve fewer LLM queries than the proposed deterministic Quicksort approach?
- Basis in paper: [explicit] The Limitations section notes that "hybrid methods... as well as active ranking strategies or noisy sorting algorithms... are fully compatible with our approach: they rely on additional computations... enabling more informed—and thus fewer—LLM queries."
- Why unresolved: The current framework optimizes deterministic sorting, but it has not explored adaptive strategies that dynamically select the next comparison based on prior uncertainty, which could theoretically reduce the total inference count.
- What evidence would resolve it: Implementing an active ranking baseline within the batching framework and comparing its inference count and ranking quality (nDCG@10) against the batched Quicksort baseline.

### Open Question 3
- Question: Does the relative efficiency of batched Quicksort over Heapsort hold or improve when applied to significantly larger, state-of-the-art LLMs?
- Basis in paper: [explicit] The authors mention that "experiments were limited to medium-sized LLMs for budgetary and computational reasons," and suggest that "Future research should explore how our framework performs with these more powerful models."
- Why unresolved: The throughput and latency characteristics of batching and caching may interact differently with the architectures and memory constraints of much larger models (e.g., 70B+ parameters).
- What evidence would resolve it: Replicating the latency and inference count experiments using larger decoder-only models (e.g., Llama-3-70B) or frontier proprietary models to verify if the speedup ratios remain consistent.

## Limitations
- Limited to medium-sized LLMs due to computational constraints; efficiency gains may not generalize to larger models
- Assumes deterministic LLM outputs for caching; non-deterministic sampling would invalidate cached results
- Hardware-specific batching efficiency (RTX 3090, RTX 4090, A100) may not translate to all GPU architectures

## Confidence
- **High confidence**: The core claim that LLM inference cost inverts classical sorting algorithm efficiency rankings (Heapsort optimal under comparison count vs. Quicksort optimal under inference count) is well-supported by theoretical analysis and empirical validation across three datasets.
- **Medium confidence**: The specific 44% inference reduction for Quicksort with batch size 2 and 46% reduction for Bubblesort with caching are based on single experiments; results may vary with different document sets, LLM models, or ranking tasks.
- **Medium confidence**: The nDCG@10 equivalence across algorithms (0.605 vs 0.608 vs 0.606) suggests ranking quality is preserved, but this needs verification across more diverse query sets and document domains.

## Next Checks
1. **Batch saturation analysis**: Test Quicksort batching across a broader range of GPUs (including CPU-only and consumer GPUs) to identify the universal batch size that maximizes throughput without memory overflow.

2. **Caching robustness test**: Evaluate Bubblesort caching performance with stochastic LLMs (temperature > 0) to determine the maximum temperature at which cached results remain valid for consistent ranking.

3. **Algorithm cross-task validation**: Apply the optimized sorting framework to non-ranking tasks like multi-document summarization or feature selection to verify the generality of batching/caching optimizations beyond PRP.