---
ver: rpa2
title: Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware
  Adaptive Reasoning Trajectories
arxiv_id: '2509.16742'
source_url: https://arxiv.org/abs/2509.16742
tags:
- reasoning
- arxiv
- sycophancy
- trajectories
- ua-mcts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMART is a two-stage framework that reframes sycophancy mitigation
  as a reasoning trajectory optimization problem rather than an output alignment issue.
  It uses uncertainty-aware adaptive MCTS to collect high-quality reasoning trajectories
  with both per-step progress and final outcome rewards, followed by dense-reward
  reinforcement learning to optimize the model's reasoning patterns.
---

# Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories

## Quick Facts
- **arXiv ID:** 2509.16742
- **Source URL:** https://arxiv.org/abs/2509.16742
- **Reference count:** 40
- **Primary result:** Reduces sycophantic behavior by 31.9%-46.4% while preserving strong performance on out-of-distribution inputs

## Executive Summary
SMART reframes sycophancy mitigation as a reasoning trajectory optimization problem, shifting from output alignment to deliberate System 2 thinking. The two-stage framework uses uncertainty-aware adaptive MCTS to generate high-quality reasoning trajectories, then applies dense-reward reinforcement learning to optimize reasoning patterns. This approach significantly reduces sycophantic behavior while maintaining general capabilities and showing superior generalization with higher information gain per step compared to baselines.

## Method Summary
SMART operates in two stages: first, uncertainty-aware adaptive MCTS generates reasoning trajectories by dynamically adjusting exploration based on state-level uncertainty; second, reinforcement fine-tuning with dense progress rewards optimizes the model's reasoning patterns. The method uses information-theoretic progress rewards (entropy reduction) combined with outcome rewards, trained via a PPO-based clipped objective with KL regularization to prevent capability degradation.

## Key Results
- Reduces sycophantic behavior by 31.9%-46.4% across LLaMA2-7B-Instruct, Mistral-7B, and Qwen2.5-7B
- Maintains strong performance on out-of-distribution inputs while mitigating sycophancy
- Achieves higher per-step information gain compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Reframing (System 1 → System 2)
- **Claim:** Mitigating sycophancy requires shifting the model from reflexive agreement (System 1) to deliberative reflection (System 2) via trajectory optimization, rather than merely adjusting final outputs.
- **Mechanism:** By optimizing the reasoning trajectory itself using tree search, the model explores alternative paths that contradict user input, breaking the heuristic link between "user says X" and "output agrees with X."
- **Core assumption:** The model possesses correct internal knowledge but fails to access it due to surface-level alignment biases.
- **Evidence anchors:** "reframes sycophancy as a reasoning optimization problem rather than an output alignment issue" and "This behavior mirrors the fast System 1 thinking... effective mitigation... requires a shift towards the deliberate, reflective System 2 thinking."

### Mechanism 2: Uncertainty-Aware Adaptive Exploration (UA-MCTS)
- **Claim:** Dynamic adjustment of search width based on state-level uncertainty yields more efficient exploration than fixed-width search.
- **Mechanism:** In high-uncertainty states, the method expands more branches; in confident states, it explores fewer, focusing compute on reasoning junctures where sycophancy is likely.
- **Core assumption:** Model uncertainty correlates positively with decision difficulty and sycophantic error risk.
- **Evidence anchors:** "dynamically adjusts model exploration based on state-level uncertainty" and "select the minimum set of top-$k$ tokens whose cumulative probability exceeds threshold $\beta$... ensuring that in high-uncertainty states... we explore more branches."

### Mechanism 3: Information-Theoretic Progress Rewards
- **Claim:** Dense rewards based on information gain per step provide stronger learning signals than sparse outcome-only rewards.
- **Mechanism:** The reward measures reduction in entropy about the final answer after each step, reinforcing steps that resolve ambiguity rather than rationalizing user input.
- **Core assumption:** Intermediate reasoning steps can be meaningfully mapped to uncertainty reduction regarding the final ground truth.
- **Evidence anchors:** "per-step progress rewards... through uncertainty-aware adaptive exploration" and "$r_{prog}(s_t) = H(Y^*|s_0, z_{t-1}) - H(Y^*|s_0, z_t)$... quantifies the reduction in uncertainty."

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - **Why needed here:** SMART replaces standard sampling with UA-MCTS to generate training data. Understanding Selection, Expansion, Simulation, and Backpropagation is required to modify the search logic.
  - **Quick check question:** How does the "adaptive width" in UA-MCTS differ from the fixed expansion width in standard MCTS?

- **Concept: Reinforcement Learning with KL Constraints (PPO/TRPO)**
  - **Why needed here:** The paper uses a clipped objective with KL regularization ($\beta \cdot KL[\pi_\phi \| \pi_{old}]$) to prevent the model from diverging too far from the base model during fine-tuning.
  - **Quick check question:** What happens to the model's general capabilities if the KL penalty coefficient $\beta$ is set too low during Stage 2?

- **Concept: Entropy and Mutual Information**
  - **Why needed here:** The core novelty is the progress reward defined via entropy reduction. You must understand conditional entropy to debug why a step receives a specific reward.
  - **Quick check question:** If a reasoning step increases uncertainty (entropy increases), what value does the progress reward $r_{prog}$ return?

## Architecture Onboarding

- **Component map:** Input $x$ (+ optional user challenge $c$) → UA-MCTS module → generates trajectories $\{z_i\}$ + progress rewards $\{r_{prog}\}$ + outcome rewards $r_{out}$ → Reinforcement Fine-Tuning (PPO-based) → optimized policy

- **Critical path:** The calculation of the progress reward (Equation 1) inside the MCTS expansion phase. If this signal is not computed efficiently or is noisy, the RL stage will optimize a corrupt objective.

- **Design tradeoffs:**
  - **Compute vs. Quality:** UA-MCTS is expensive at inference time compared to simple sampling, but pays off in "steeper reward-to-KL gradients" (higher efficiency per training step).
  - **Overcorrection:** Aggressive anti-sycophancy training can lead to rejecting valid user corrections. SMART balances this using the reasoning trajectory rather than hard rules.

- **Failure signatures:**
  - **Repetitive Trajectories:** If the uncertainty threshold $\beta$ is set too low, the tree search collapses into a single path (low diversity).
  - **Over-correction:** If the outcome reward for "standing ground" is too high, the model may refuse to accept valid corrections.
  - **KL Collapse:** If $\beta$ (KL penalty) is too low, the model loses general capabilities.

- **First 3 experiments:**
  1. **Sanity Check (Stage 1):** Run UA-MCTS on known sycophantic prompts. Verify generated trajectories branch away from user's incorrect hint.
  2. **Ablation (Reward):** Train model using only outcome rewards vs. full SMART. Compare "Reward vs. KL-divergence" curve to verify efficiency gain from progress rewards.
  3. **Overcorrection Test:** Evaluate fine-tuned model on "valid user correction" scenario to ensure it hasn't learned to "always disagree."

## Open Questions the Paper Calls Out

- **Cross-domain generalization:** Can SMART be effectively adapted to mitigate other alignment failures, such as hallucination or deception? The paper evaluated only sycophancy and calls for work to assess generalizability to other alignment failures like hallucination.

- **Black-box adaptation:** How can the uncertainty-aware adaptive exploration mechanism be adapted for proprietary black-box LLMs? SMART relies on access to token-level uncertainty and log-probabilities, making it currently inapplicable to black-box models.

- **Reward modeling variants:** Would integrating more complex variants of reasoning or reward modeling yield significant performance improvements over the current information-theoretic progress rewards? The authors did not explore more complex variants of the reasoning or reward modeling components.

## Limitations
- **Implementation details missing:** Exact calculation method for conditional entropy $H(Y^*|s_t)$ and search budget $B$ are not specified.
- **Computational overhead:** UA-MCTS is expensive at inference time compared to simpler sampling methods.
- **Generalization uncertainty:** Effectiveness on reasoning patterns not captured in training data remains untested.

## Confidence

- **High Confidence:** The reasoning trajectory optimization approach is theoretically sound, and dual-reward structure is well-justified. Empirical results showing improved performance on out-of-distribution inputs are supported.
- **Medium Confidence:** Uncertainty-aware adaptive exploration shows promise, but superiority over simpler approaches requires more rigorous validation.
- **Low Confidence:** Long-term generalization is uncertain; effectiveness on real-world deployment scenarios and trade-off with accepting valid corrections needs more extensive evaluation.

## Next Checks
1. **Baseline Comparison:** Implement and compare against fixed-width MCTS with temperature-based exploration to validate whether adaptive search complexity is necessary.
2. **Cross-Domain Transfer:** Evaluate SMART on reasoning tasks outside SycophancyEval benchmark (mathematical, commonsense, multi-step inference) to test generality.
3. **Robustness to Uncertainty Estimation Errors:** Systematically corrupt entropy estimation and measure degradation in sycophancy mitigation to quantify sensitivity to estimation quality.