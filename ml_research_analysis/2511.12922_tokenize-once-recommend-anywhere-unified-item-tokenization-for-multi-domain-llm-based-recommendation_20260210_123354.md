---
ver: rpa2
title: 'Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain
  LLM-based Recommendation'
arxiv_id: '2511.12922'
source_url: https://arxiv.org/abs/2511.12922
tags:
- item
- domains
- across
- tokenization
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniTok, a unified item tokenization framework
  for multi-domain LLM-based recommendation systems. UniTok employs a mixture-of-experts
  (MoE) architecture with domain-specific and shared experts to capture both domain-specific
  and common knowledge, while using mutual information calibration to ensure semantic
  balance across domains.
---

# Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation

## Quick Facts
- arXiv ID: 2511.12922
- Source URL: https://arxiv.org/abs/2511.12922
- Reference count: 40
- Primary result: Up to 51.89% improvement in NDCG@10 and 9.63× parameter reduction

## Executive Summary
This paper introduces UniTok, a unified item tokenization framework for multi-domain LLM-based recommendation systems. UniTok employs a mixture-of-experts (MoE) architecture with domain-specific and shared experts to capture both domain-specific and common knowledge, while using mutual information calibration to ensure semantic balance across domains. Experiments on ten real-world datasets show that UniTok achieves up to 51.89% improvement in NDCG@10 compared to strong baselines, reduces model size by 9.63×, and generalizes well to unseen domains without retraining. Theoretical analysis supports its effectiveness in increasing entropy, reducing quantization error, and ensuring semantic consistency.

## Method Summary
UniTok tokenizes items across multiple domains using a shared autoencoder and TokenMoE architecture. Items are first projected into a unified latent space via a shared encoder, then routed to domain-specific experts using a learned softmax-based router. A shared expert captures cross-domain common knowledge. The final quantized embedding is a weighted combination of domain-specific and shared expert outputs. Mutual information calibration using HSIC ensures consistent semantic informativeness across domains. The framework is integrated with T5-based recommender (TIGER) for downstream evaluation.

## Key Results
- Up to 51.89% improvement in NDCG@10 compared to strong baselines
- 9.63× reduction in model parameters while maintaining performance
- Successful zero-shot generalization to unseen domains (Yelp, LastFM, Dianping)
- Token entropy increases from 9.63 to 10.42 compared to single codebook methods

## Why This Works (Mechanism)

### Mechanism 1: MoE-based Domain Disentanglement
The TokenMoE architecture enables unified tokenization across heterogeneous domains by disentangling domain-specific learning from shared representations. Items are routed via a learned softmax-based router to top-N domain-specific experts, while a shared expert captures cross-domain common knowledge. This prevents semantic mixing that occurs with naive shared codebooks.

### Mechanism 2: Mutual Information Calibration
Mutual information calibration reduces inter-domain performance variability by ensuring consistent semantic informativeness across domains. HSIC measures dependence between input and latent embeddings for each domain, and the MI calibration loss penalizes variance while encouraging high MI, preventing semantically similar items from being assigned inconsistent tokens.

### Mechanism 3: Entropy and Quantization Error Benefits
MoE-based tokenization induces higher entropy token space and lower expected quantization error compared to single codebook methods. The router's non-degenerate distribution over experts increases entropy, while expert specialization compensates for domain-specific inaccuracies, reducing overall quantization error.

## Foundational Learning

- **Concept: Residual Quantization (RQ)**
  - Why needed here: Core discretization mechanism that iteratively quantizes residuals to approximate item embeddings
  - Quick check: Given a 4-level RQ with 256 codes per level, how many unique item representations are possible? (Answer: 256^4 ≈ 4.3 billion)

- **Concept: Mixture-of-Experts (MoE) routing**
  - Why needed here: Enables conditional computation by routing items to relevant domain-specific experts while maintaining shared knowledge
  - Quick check: Why does sparse activation (top-N routing) reduce computational overhead while preserving model capacity?

- **Concept: Hilbert-Schmidt Independence Criterion (HSIC)**
  - Why needed here: Non-parametric proxy for mutual information to measure dependence between input and latent spaces
  - Quick check: Why might HSIC be preferred over MINE or InfoNCE for MI estimation in this context? (Hint: Consider training overhead)

## Architecture Onboarding

- **Component map:**
  Shared encoder -> Router -> (Top-N domain experts + Shared expert) -> Weighted combination -> Shared decoder

- **Critical path:**
  1. Pre-trained encoder produces semantic embeddings (2048-dim)
  2. Shared encoder projects to latent space (32-dim hidden)
  3. Router selects top-N experts; shared expert always included
  4. Each selected expert performs 4-level RQ (256 codes, 32-dim each)
  5. Weighted combination of quantized embeddings reconstructed by decoder
  6. Losses computed; all parameters jointly updated

- **Design tradeoffs:**
  - L (quantization levels): 4 optimal - higher L improves granularity but increases error accumulation
  - T (codebook size): 256 codes per level - larger T increases diversity but may overfit
  - N (top-experts): 1-2 optimal - promotes sparsity and efficiency but limits expert collaboration
  - λ values: λ_RQ=1, λ_MI=0.03 - higher λ_MI can amplify irrelevant variations

- **Failure signatures:**
  - Router collapse: All items route to single expert → entropy gain lost → check H(G) during training
  - Semantic imbalance: High MI variance across domains → inconsistent token quality → monitor Var[Î(k)]
  - Reconstruction failure: High L_Rec → latent space poorly captures semantics → check decoder convergence
  - Zero-shot failure: Poor generalization to unseen domains → shared expert undertrained or distributions too disparate

- **First 3 experiments:**
  1. Single-domain baseline: Run UniTok on one domain with TokenMoE disabled to establish quantization error baseline
  2. Router entropy analysis: Log router distribution G(z_i) per domain during training to verify non-degenerate distributions
  3. MI variance ablation: Train with λ_MI ∈ {0, 0.03, 0.1} and plot Var[Î(k)] vs. NDCG@10 across domains

## Open Questions the Paper Calls Out

### Open Question 1
How can UniTok be adapted as a general-purpose tokenization interface for a wider variety of recommendation foundation models?
Basis: The conclusion explicitly lists this as future work. Evidence needed: Demonstrating efficacy when integrated with diverse foundation backbones (e.g., LLaMA or multimodal models).

### Open Question 2
Can collaborative signals be integrated into UniTok without compromising its zero-shot generalization capabilities?
Basis: The method operates "independently of user data" because collaborative signals limit generalization. Evidence needed: A mechanism that fuses interaction graph data while maintaining performance on unseen domains.

### Open Question 3
Does the unified tokenization scale effectively to semantically distinct domains beyond the e-commerce categories tested?
Basis: The "unseen" domains used for zero-shot testing share high metadata similarity with source datasets. Evidence needed: Robust performance on heterogeneous datasets with significantly different feature distributions.

## Limitations
- Domain heterogeneity assumption may not hold for truly diverse domains beyond consumer products
- Zero-shot generalization claims lack ablation studies isolating key contributing factors
- Theoretical assumptions (Lipschitz continuity, convex squared norm) may not hold in practice
- Three-stage training pipeline introduces multiple points of potential error accumulation

## Confidence

**High confidence:**
- MoE architecture improves entropy and reduces quantization error
- TokenMoE successfully disentangles domain-specific from shared representations
- HSIC-based MI calibration effectively reduces performance variance across domains

**Medium confidence:**
- 51.89% NDCG@10 improvement represents true cross-domain learning
- Zero-shot generalization to truly heterogeneous domains will match reported performance
- 9.63× parameter reduction comes with minimal accuracy trade-offs

**Low confidence:**
- Theoretical bounds on MI variance translate to meaningful performance improvements
- Specific hyperparameter choices generalize across different domain distributions
- Framework handles extreme domain shifts (e.g., product recommendations vs. scientific papers)

## Next Checks

1. **Cross-domain ablation study**: Systematically remove components (shared expert, MI calibration, MoE routing) and measure performance degradation across all 10 domains to isolate key mechanisms.

2. **Extreme domain shift evaluation**: Test UniTok on truly heterogeneous domains beyond consumer products (e.g., scientific papers, code repositories, medical records) to validate shared expert transferability.

3. **Router behavior analysis**: Track expert activation patterns throughout training across all domains to quantify expert specialization, router entropy stability, and identify potential collapse scenarios.