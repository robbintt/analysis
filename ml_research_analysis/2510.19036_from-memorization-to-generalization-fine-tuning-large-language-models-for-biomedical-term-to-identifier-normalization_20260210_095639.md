---
ver: rpa2
title: 'From Memorization to Generalization: Fine-Tuning Large Language Models for
  Biomedical Term-to-Identifier Normalization'
arxiv_id: '2510.19036'
source_url: https://arxiv.org/abs/2510.19036
tags:
- gene
- term
- fine-tuning
- terms
- identifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated large language models (LLMs) for mapping
  biomedical terms to standardized identifiers across three ontologies: Human Phenotype
  Ontology (HPO), Gene Ontology (GO), and gene symbol-protein mappings (GENE). Fine-tuning
  Llama 3.1 8B showed strong memorization gains for GO (up to 77% accuracy improvement)
  and GENE (13.9% generalization to unseen pairs), but minimal gains for HPO.'
---

# From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization

## Quick Facts
- arXiv ID: 2510.19036
- Source URL: https://arxiv.org/abs/2510.19036
- Authors: Suswitha Pericharla; Daniel B. Hier; Tayo Obafemi-Ajayi
- Reference count: 40
- One-line primary result: Fine-tuning Llama 3.1 8B showed strong memorization gains for GO (up to 77% accuracy improvement) and GENE (13.9% generalization to unseen pairs), but minimal gains for HPO.

## Executive Summary
This study evaluates large language models for mapping biomedical terms to standardized identifiers across three ontologies. The research finds that fine-tuning improves memorization efficiency, with success dependent on identifier popularity and lexicalization. Lexicalized identifiers (like gene symbols) enable semantic generalization to unseen pairs, while arbitrary identifiers (like HPO/GO codes) restrict learning to rote memorization. The study reveals that autoregressive bias creates directional mapping asymmetries, with Term→ID mapping outperforming ID→Term.

## Method Summary
The study fine-tuned Llama 3.1 8B Instruct using LoRA (rank=64, alpha=128) on all linear layers. Researchers curated frequency-balanced datasets (200 pairs per ontology) from PMC counts, stratified into 20 frequency bins. Five prompt templates were used to create bidirectional training samples. Models were trained for 20 epochs with AdamW optimizer and cosine scheduler. Evaluation distinguished memorization (trained pairs) from generalization (unseen validation pairs) using top-1 accuracy metrics.

## Key Results
- Fine-tuning significantly improved memorization across all ontologies, with GO showing up to 77% accuracy improvement
- GENE identifiers demonstrated 13.9% generalization to unseen pairs due to lexicalization
- HPO showed minimal gains (0.7% improvement) due to arbitrary identifiers and low popularity
- Mapping terms to identifiers was more accurate than reverse mapping, reflecting autoregressive bias

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Memorization efficiency during fine-tuning is likely mediated by pretraining frequency (popularity) rather than model scale alone.
- **Mechanism:** Frequent exposure during pretraining establishes high "factual salience" (stronger weight associations). Fine-tuning then consolidates this latent knowledge with fewer epochs. Rare facts (long-tail) lack this salience, forcing the model into a prolonged "guessing phase" where optimization is inefficient.
- **Core assumption:** PMC citation counts serve as a valid proxy for term-identifier frequency in the model's proprietary pretraining corpus.
- **Evidence anchors:**
  - "Success depended on identifier popularity... popular identifiers were more likely encountered during pretraining, enhancing memorization."
  - "Fine-tuning is most effective for partially known or latent facts... whereas entirely unfamiliar facts are harder to integrate."
  - 'Latent Knowledge as a Predictor of Fact Acquisition...' supports the correlation between latent pretraining knowledge and fine-tuning success.
- **Break condition:** The mechanism may break for extremely rare "long-tail" terms (as seen in HPO), where fine-tuning yields negligible gains because the semantic representations were never formed during pretraining.

### Mechanism 2
- **Claim:** Generalization to unseen term-identifier pairs is conditional on the "lexicalization" of identifiers (semantic overlap in embedding space).
- **Mechanism:** Lexicalized identifiers (e.g., gene symbols like TP53) possess embeddings semantically aligned with their natural language terms. Fine-tuning shifts decision boundaries to exploit this pre-existing geometry, allowing the model to "unlock" related unseen pairs. Arbitrary identifiers (e.g., HP:0001251) have no semantic overlap, restricting learning to rote sequence memorization.
- **Core assumption:** The embedding geometry of the base model is largely preserved during LoRA fine-tuning, allowing semantic distances to dictate generalization.
- **Evidence anchors:**
  - "Embedding analyses revealed semantic alignment only for lexicalized GENE identifiers... arbitrary identifiers restricted learning to rote memorization."
  - Figure 5A shows significant cosine similarity for GENE pairs but zero alignment for GO/HPO.
  - Corpus papers primarily address entity recognition or memorization, lacking specific contradictory evidence on lexicalization-driven generalization.
- **Break condition:** This mechanism fails for arbitrary code-based identifiers (like HPO/GO), where the identifier string carries no semantic information about the term.

### Mechanism 3
- **Claim:** Mapping accuracy is directionally biased (Term→ID vs. ID→Term) due to the autoregressive training objective.
- **Mechanism:** Causal masking in LLMs creates directional token associations. Since text corpora usually present terms before identifiers (e.g., "ataxia (HP:0001251)"), the Term→ID direction aligns with pretraining statistical structures. The reverse direction violates this causal flow, triggering the "reversal curse."
- **Core assumption:** The training data distribution follows the standard linguistic convention of [Concept] followed by [Code].
- **Evidence anchors:**
  - "Mapping terms to identifiers was more accurate than mapping identifiers to terms... reflecting the autoregressive bias... [and] reversal curse."
  - "Baseline performance varied... success depended on identifier popularity..." (implies structure matters).
  - 'Predicting Failures of LLMs...' corroborates that specific structural properties of ontologies predict model failure.
- **Break condition:** If training data explicitly reverses the typical structure (Code→Term) with high frequency, or if bidirectional architectures are used, this asymmetry would likely diminish.

## Foundational Learning

- **Concept:** Zipf’s Law & Long-tail Distribution
  - **Why needed here:** The study relies on stratified sampling specifically to counter the long-tail distribution of biomedical terms. Understanding that most terms are rare (tail) while few are common (head) is essential to interpreting why HPO (long-tail heavy) failed while GENE (head-heavy) succeeded.
  - **Quick check question:** If a dataset is dominated by "tail" terms (low frequency), should you expect fine-tuning to efficiently memorize them?

- **Concept:** Latent vs. Accessible Knowledge
  - **Why needed here:** The paper argues fine-tuning primarily *surfaces* latent knowledge rather than *injecting* new knowledge. Distinguishing between what the model has "seen" (latent) vs. what it can "recall" (accessible) explains why popularity predicts success.
  - **Quick check question:** Does fine-tuning create new semantic connections for rare facts or merely strengthen weak pre-existing connections for popular facts?

- **Concept:** Semantic Embedding Alignment (Cosine Similarity)
  - **Why needed here:** The core differentiator between GENE (Generalization) and GO/HPO (Memorization only) is demonstrated via cosine similarity plots. You must understand vector similarity to grasp why lexicalized identifiers generalize.
  - **Quick check question:** If the cosine similarity between a term vector and its identifier vector is near 0, can the model likely "generalize" to unseen pairings of that type?

## Architecture Onboarding

- **Component map:** PMC Frequency Analysis -> Stratified Sampling (20 bins, 10 pairs/bin) -> Alignment Check (cosine similarity) -> LoRA Fine-Tuning (r=64, α=128) -> Evaluation (Memorization vs. Generalization)
- **Critical path:**
  1. **Frequency Analysis:** Calculate PMC frequencies for target ontology terms/IDs.
  2. **Stratified Sampling:** Select 200 pairs balanced across head/body/tail.
  3. **Alignment Check:** (Optional but recommended) Verify embedding cosine similarity of pairs to predict generalization potential.
  4. **LoRA Fine-Tuning:** Train on bidirectional prompts.
  5. **Evaluation:** Split results into "Memorization" (trained pairs) and "Generalization" (unseen pairs).
- **Design tradeoffs:**
  - **Memorization vs. Generalization:** You can optimize for rote memorization of arbitrary codes (GO) using brute-force epochs, but this yields no generalization. Optimizing for generalization requires lexicalized identifiers (GENE).
  - **Scale vs. Efficiency:** Larger models (GPT-4o) outperform fine-tuned small models (Llama 8B FT) on recall, suggesting RAG or larger baselines might be better than fine-tuning for purely arbitrary identifiers.
  - **Popularity:** High popularity aids memorization but may show diminishing returns (ceiling effects); low popularity creates data bottlenecks.
- **Failure signatures:**
  - **Zero Generalization:** If identifiers are arbitrary (non-lexicalized), validation accuracy will not improve.
  - **Directional Collapse:** Identifier→Term accuracy is significantly lower than Term→Identifier accuracy.
  - **HPO Pattern:** Low popularity + Arbitrary ID = Negligible gain from fine-tuning (worst case).
- **First 3 experiments:**
  1. **Corpus Frequency Audit:** Before fine-tuning, plot the frequency rank of your target ontology terms/IDs. If they cluster in the "long tail" (like HPO), expect poor fine-tuning efficiency.
  2. **Embedding Alignment Baseline:** Compute cosine similarity between term embeddings and identifier embeddings. If similarity is random/low, design the system for rote lookup (RAG), not semantic generalization.
  3. **Bidirectional Probe:** Fine-tune on Term→ID and test ID→Term. A large performance gap confirms strong autoregressive bias, suggesting you should structure retrieval prompts to follow the "Term then ID" flow.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the popularity–lexicalization continuum accurately predict fine-tuning success for term normalization in other major biomedical vocabularies?
- Basis in paper: [explicit] The authors state in the "Future Work" section that applying this framework to terminologies such as ICD-10, SNOMED CT, RxNorm, and LOINC is necessary to test if the observed continuum generalizes.
- Why unresolved: The current study only evaluated three specific terminologies (HPO, GO, GENE), leaving the broader applicability of the proposed framework unconfirmed.
- What evidence would resolve it: Replicating the study's methodology on SNOMED CT or RxNorm datasets to observe if lexicalized identifiers show generalization while arbitrary ones rely on memorization.

### Open Question 2
- Question: Does fine-tuning for term normalization primarily re-weight existing token probabilities, or does it reshape the semantic geometry of the model's embedding space?
- Basis in paper: [explicit] The "Future Work" section notes that direct analysis of embeddings in fine-tuned models is needed to determine if the process reshapes semantic structure.
- Why unresolved: The study analyzed embeddings in the base model but did not compare the geometry of embeddings before and after fine-tuning.
- What evidence would resolve it: Comparing the PCA projections and cosine similarity landscapes of the base Llama 3.1 8B model against the fine-tuned models to detect structural shifts.

### Open Question 3
- Question: Can specific fine-tuning configurations (adapter types, learning rates, data mixing) mitigate the knowledge degradation observed in high-performing lexicalized models?
- Basis in paper: [explicit] The authors suggest that systematic exploration of fine-tuning parameters could reveal methods to reduce the "Loser" (degradation) rates while preserving generalization.
- Why unresolved: The experiments used a fixed PEFT setup (LoRA, specific hyperparameters), so the effect of different configurations on the trade-off between gains and losses remains unknown.
- What evidence would resolve it: Ablation studies varying the LoRA rank, learning rate schedules, or training data composition specifically for the GENE mappings to minimize the "Degraded" metric.

## Limitations

- Generalizability to other ontologies beyond HPO, GO, and GENE is uncertain
- PMC citation frequency may not accurately represent pretraining corpus exposure
- Embedding stability during LoRA fine-tuning was not directly measured
- Sample size constraints (200 pairs per ontology) may miss edge cases

## Confidence

**High Confidence:**
- Fine-tuning improves memorization accuracy for all three ontologies, with gains proportional to baseline performance
- Identifier popularity predicts fine-tuning efficiency gains
- Directional bias exists in Term→ID vs. ID→Term mapping accuracy

**Medium Confidence:**
- Embedding cosine similarity differences explain the generalization gap between GENE and HPO/GO
- Pretraining frequency mediates fine-tuning efficiency through latent knowledge activation
- Lexicalization is the primary determinant of generalization potential

**Low Confidence:**
- The specific rank=64 and alpha=128 LoRA configuration is optimal for this task
- Results generalize to other model scales (beyond 8B parameters)
- Findings apply equally to non-biomedical normalization tasks

## Next Checks

1. **Cross-Ontology Validation:** Test the popularity-lexicalization framework on additional biomedical ontologies (e.g., SNOMED CT, ICD-10) and non-biomedical identifier systems to verify generalizability of the proposed mechanisms.

2. **Alternative Frequency Metrics:** Replicate the stratification and fine-tuning experiments using alternative frequency proxies (Google Scholar citations, clinical usage databases) to confirm whether PMC counts are the appropriate proxy for pretraining exposure.

3. **Embedding Stability Analysis:** Conduct ablation studies measuring embedding changes during LoRA fine-tuning across multiple random seeds, and test whether alternative embedding methods (e.g., sentence transformers) alter the observed semantic alignment patterns.