---
ver: rpa2
title: 'COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations'
arxiv_id: '2510.24810'
source_url: https://arxiv.org/abs/2510.24810
tags:
- reason
- notes
- helpfulness
- note
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task of predicting the helpfulness
  of fact-checking explanations and the reasons for their helpfulness. The authors
  create a large-scale dataset of over 104k social media posts with user-provided
  explanatory notes and helpfulness labels.
---

# COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations

## Quick Facts
- arXiv ID: 2510.24810
- Source URL: https://arxiv.org/abs/2510.24810
- Authors: Rui Xing; Preslav Nakov; Timothy Baldwin; Jey Han Lau
- Reference count: 40
- Primary result: Introduced a large-scale dataset (104k posts) with automated framework for predicting fact-checking explanation helpfulness and reasons via optimized textual definitions

## Executive Summary
This paper introduces a new task of predicting the helpfulness of fact-checking explanations and the reasons for their helpfulness. The authors create a large-scale dataset of over 104k social media posts with user-provided explanatory notes and helpfulness labels. They propose a framework that automatically generates and optimizes reason definitions using prompt optimization, and show that integrating these definitions into models improves both helpfulness and reason prediction performance. Experiments demonstrate that the helpfulness information also enhances automated fact-checking systems.

## Method Summary
The authors created the COMMUNITYNOTES dataset by processing raw X Community Notes data, applying a ranking algorithm to derive helpfulness and reason labels, and filtering for quality. They implemented a multi-task small language model baseline using DeBERTa with "[CLS] Claim: {claim} [SEP] Note: {note}" input format. For definition optimization, they used GPT-4o to generate seed definitions for each reason category, then applied PROMPTAGENT (a Monte Carlo Tree Search-based optimizer) to refine them. The optimized definitions were embedded using LLM2Vec and fused with note embeddings via Multi-Head Attention, where note embeddings serve as queries and definition embeddings as keys/values.

## Key Results
- Large-scale dataset of 104,966 posts with 58,569 English and 46,397 other language notes
- Automated prompt optimization improves reason prediction F1 by approximately 15% over seed definitions
- Helpfulness information enhances automated fact-checking systems, improving NEI label prediction by ~2%
- SLMs (DeBERTa) outperform LLMs (Llama3.1) on complex multi-label reason prediction tasks

## Why This Works (Mechanism)

### Mechanism 1: Semantic Signal Enhancement via Optimized Definitions
Integrating automatically optimized textual definitions of class labels into the model input appears to improve classification performance by reducing the semantic ambiguity of "reason" labels. The framework generates "seed" definitions using an LLM and iteratively refines them using PROMPTAGENT (a Monte Carlo Tree Search-based optimizer). By treating definitions as optimizable parameters rather than static text, the system aligns the latent space of the model with the specific, often subjective, criteria for helpfulness used in community notes. The underlying "ground truth" labels in the dataset are consistent enough that a single text definition can capture the necessary signal to distinguish between categories like `helpfulClear` and `helpfulImportantContext`.

### Mechanism 2: Attention-Guided Feature Fusion
Fusing note embeddings with definition embeddings using a Multi-Head Attention (MHA) layer provides superior representational power for helpfulness prediction compared to standard classification heads. Instead of treating definitions as static context, the architecture uses the note embedding as the Query (Q) and the definition embeddings as the Key/Value (K, V) pairs. This allows the model to dynamically "attend" to specific definition criteria (e.g., "unbiased language") based on the content of the specific note being analyzed. The encoder (LLM2Vec) can generate high-quality embeddings for the optimized definitions such that their vector space aligns with the vector space of the notes.

### Mechanism 3: Helpfulness as a Proxy for Evidence Sufficiency
A model trained to predict helpfulness reasons (specifically identifying "missing key points") may transfer effectively to the distinct task of evidence sufficiency in fact-checking. The paper posits that an explanation deemed unhelpful because it "misses key points" is functionally similar to evidence that is "insufficient" for verifying a claim. By mapping these semantic overlaps, the helpfulness predictor acts as a quality filter for downstream fact-checking systems. The concept of "missing key points" in a social media note is semantically isomorphic to "insufficient evidence" in a formal fact-checking context.

## Foundational Learning

- **Label Semantics & Textual Labels**: Standard classification treats labels as integer IDs (0, 1, 2). This paper treats labels as text objects ("helpfulClear") with explicit definitions. Understanding how to embed and fuse this text is crucial for the architecture. How does feeding a text definition into a model differ mathematically from using a one-hot encoded label vector?

- **Multi-Head Attention (MHA)**: The core architectural innovation uses MHA not for sequence processing, but for fusing two different modalities (the Note and the Definition). In the formula $Attention(Q, K, V)$, if $Q$ represents the Note and $K/V$ represent the Definitions, what does the resulting attention matrix represent semantically?

- **Monte Carlo Tree Search (MCTS) in Prompt Optimization**: The paper utilizes PROMPTAGENT, which uses MCTS to navigate the search space of possible prompt definitions. You need to understand that this is a discrete optimization strategy, distinct from backpropagation. Why is a "feedback agent" required in this loop, and how does MCTS decide which definition "moves" to explore?

## Architecture Onboarding

- **Component map**: Raw X Community Notes -> Ranking Algorithm -> Aggregated Labels -> GPT-4o (Seed Generator) -> PROMPTAGENT (MCTS + Feedback Agent) -> Optimized Definitions -> Shared encoder (e.g., DeBERTa) for Notes + LLM2Vec for Definitions -> Multi-Head Attention (Note → Query, Defs → Key/Value) -> Binary Helpfulness Head + Multi-Label Reason Head

- **Critical path**: The Prompt Optimization Loop. This is the most time-consuming and unstable component. It requires multiple calls to black-box LLMs to generate and refine definitions before the main classifier can even begin training.

- **Design tradeoffs**: LLMs (Llama3.1) excel at binary helpfulness (F1 0.92) but struggle with reason prediction compared to SLMs (DeBERTa). Use SLMs for the complex multi-label reasoning task; use LLMs if simple binary filtering is the goal. Zero-shot with optimized definitions is fast but lags behind the fine-tuned SLMs.

- **Failure signatures**: Long-tail collapse - the model frequently fails on rare classes like `helpfulEmpathetic` (F1 0.00). Monitor confusion matrices for these specific categories. High-severity confusion - confusion between `helpfulClear` and `helpfulImportantContext`. If these two are swapped frequently, the definitions are likely not distinct enough.

- **First 3 experiments**:
  1. Reproduce Zero-Shot Jump: Replicate Table 6 to verify that "Seed" and "Optimized" definitions actually improve Llama3.1-8B performance over the vanilla prompt. This validates the definition generation pipeline.
  2. Ablation on Fusion: Replace the MHA fusion module with a simple concatenation of [Note Embedding; Definition Embedding] to quantify the specific contribution of the attention mechanism (Table 7 baseline).
  3. Cross-Domain Stress Test: Train on CommunityNotes and test on the Evidence Sufficiency task (Section 5.1) using the `notHelpfulMissingKeyPoints` label specifically to validate the generalization hypothesis.

## Open Questions the Paper Calls Out

- How can the automated reason definition optimization pipeline be adapted to ensure cross-lingual alignment and robustness across non-English data? The authors state, "Future work should explore multilingual adaptation and cross-lingual alignment of reason definitions." Current optimization experiments focused exclusively on English, leaving the transferability of definitions to other languages unverified.

- Does the integration of optimized reason definitions improve the practical utility and interpretability of predictions for human fact-checkers? The authors note that "Further human-centered evaluation is necessary to assess whether these improvements translate to greater interpretability." Current evaluation relies solely on automated classification metrics (F1 score) rather than user studies.

- Can the helpfulness prediction framework generalize to other ecosystems with different norms, such as Meta or TikTok? The authors highlight that their data "may reflect platform-specific norms and biases" and suggest further investigation for broader generalization. The dataset is sourced exclusively from X, potentially encoding X-specific community guidelines.

- How can the framework be extended to effectively process and evaluate multimodal content? The conclusion lists plans to "extend our framework to ... multimodal settings." The current models rely on text-based transformers and do not account for images or videos often present in misleading posts.

## Limitations

- The prompt optimization loop is computationally expensive and dependent on black-box LLM behavior, with no reported stability across multiple optimization runs
- The dataset only includes notes with ≥5 ratings, potentially over-representing high-engagement, high-controversy posts rather than typical community note quality
- Severe performance collapse on rare categories (e.g., helpfulEmpathetic: 0.00 F1) indicates struggles with long-tail label distributions

## Confidence

- **High Confidence**: The binary helpfulness prediction task (F1 ~0.92 for SLMs) is well-established and reproducible with the provided architecture and data splits
- **Medium Confidence**: The multi-label reason prediction improvements from optimized definitions are internally validated but lack ablation studies isolating the contribution of the definition optimization versus the MHA fusion
- **Low Confidence**: The cross-domain generalization claim (helpfulness → evidence sufficiency) is supported by a single experiment and a narrow semantic mapping; broader validation across multiple fact-checking datasets is needed

## Next Checks

1. **Stability Test**: Run the PROMPTAGENT optimization loop three times with different random seeds and report variance in final reason F1 scores to assess reproducibility

2. **Ablation on Fusion**: Replace the MHA module with a simple concatenation baseline to quantify the specific contribution of attention-based definition fusion versus optimized definitions alone

3. **Cross-Domain Generalization**: Evaluate the helpfulness predictor on a second fact-checking dataset (e.g., FEVER or LIAR) to test whether the "missing key points" → "insufficient evidence" mapping holds beyond Climate-FEVER