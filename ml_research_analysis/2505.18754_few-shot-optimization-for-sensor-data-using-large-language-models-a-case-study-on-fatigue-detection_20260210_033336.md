---
ver: rpa2
title: 'Few-Shot Optimization for Sensor Data Using Large Language Models: A Case
  Study on Fatigue Detection'
arxiv_id: '2505.18754'
source_url: https://arxiv.org/abs/2505.18754
tags:
- hed-lm
- data
- fatigue
- distance
- user-id
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HED-LM, a hybrid few-shot optimization framework
  that improves example selection for sensor-based classification tasks. It combines
  Euclidean distance filtering with LLM-based contextual relevance scoring to address
  the limitations of purely numeric or random selection methods.
---

# Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection

## Quick Facts
- arXiv ID: 2505.18754
- Source URL: https://arxiv.org/abs/2505.18754
- Reference count: 40
- Primary result: HED-LM achieves 69.13% macro F1-score on fatigue detection, outperforming random selection by 16.6%

## Executive Summary
This paper introduces HED-LM, a hybrid few-shot optimization framework that improves example selection for sensor-based classification tasks. It combines Euclidean distance filtering with LLM-based contextual relevance scoring to address the limitations of purely numeric or random selection methods. Evaluated on fatigue detection using accelerometer data, HED-LM demonstrates significant performance gains over baseline approaches while maintaining computational efficiency through a constrained 2-shot prompting strategy.

## Method Summary
HED-LM processes 180-sample accelerometer signals through three 60-sample windows, extracting 30 features (10 per window including mean, RMS, skewness, kurtosis, etc.). The method employs a two-stage filtering approach: first using Euclidean distance to identify numerically similar candidates, then applying LLM scoring with domain knowledge to evaluate contextual relevance. The framework uses a constrained 2-shot prompt structure with exactly one example per class, achieving both high accuracy and efficiency through minimal token usage.

## Key Results
- HED-LM achieves 69.13% mean macro F1-score, outperforming random selection (59.30%) by 16.6%
- 2-shot configuration (67.70% F1) is 7.7× faster than full-shot variants (52.68% F1)
- Domain knowledge integration improves performance by 8.08% compared to knowledge-free LLM scoring

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Hybrid Filtering
Distance filtering narrows candidates by numerical similarity, then LLM scoring re-ranks by evaluating both feature similarity and label alignment with domain knowledge. This bridges the gap between geometric proximity and semantic relevance, addressing cases where numerically close examples lack contextual alignment.

### Mechanism 2: Domain Knowledge-Guided LLM Scoring
Explicit threshold rules (e.g., "RMS > 0.5 indicates fatigue") are embedded in LLM prompts to improve scoring precision. Domain knowledge is generated via GPT-4o analysis of feature distributions and significantly increases the precision of contextual evaluation.

### Mechanism 3: Constrained 2-Shot Prompt Structure
Using exactly 2 examples (one per class) with pre-computed relevance scores improves inference over larger-shot configurations. This balanced representation approach minimizes token overhead while providing clear class boundaries for binary classification.

## Foundational Learning

- **Concept: In-Context Learning / Few-Shot Prompting**
  - Why needed: HED-LM relies on LLMs making predictions from in-prompt examples without weight updates
  - Quick check: Why might more examples hurt performance in few-shot prompting?

- **Concept: Euclidean Distance in Feature Space**
  - Why needed: First filtering stage computes distance between 30-dim feature vectors
  - Quick check: Would Euclidean distance distinguish two signals with similar mean/RMS but different skewness patterns?

- **Concept: Time/Frequency Domain Feature Extraction**
  - Why needed: Raw signals → 30 features (10 features × 3 windows) that both distance and LLM evaluate
  - Quick check: Why might RMS alone be insufficient for fatigue detection with overlapping patterns?

## Architecture Onboarding

- **Component map**: Preprocessing (180-sample signal → 3 windows → Butterworth filter → min-max norm → 30 features) → Distance Filtering (Euclidean distance → top distance-K candidates) → LLM Scoring (Prompt with features + domain knowledge → relevance score + reason) → Re-Ranking (Sort by LLM score → top-K) → Prompt Construction (2 examples + features + scores + reasons) → Inference (GPT-4o-mini → binary label)

- **Critical path**: Feature quality → Domain knowledge accuracy → LLM scoring precision → Example balance → Inference. LLM scoring is most sensitive; misaligned thresholds degrade to random selection.

- **Design tradeoffs**: distance-K/top-K: (10/5) improves coverage but doubles API calls vs (5/3). 2-shot vs full-shot: 7.7× faster with higher accuracy. Fixed vs adaptive windows: Current 3-window split is simple but may split fatigue transitions.

- **Failure signatures**: User-ID 10 showed distance-only outperforming HED-LM (90.51% vs 89.88%) when signals fell in threshold gray zones. Label imbalance in top-K caused missing label representation. Ambiguous LLM output required fallback frequency counting.

- **First 3 experiments**:
  1. Reproduce User-ID 4 pipeline (k=5, top-K=3, with domain knowledge), verify ~67.70% F1
  2. Ablate domain knowledge: Expect ~8% F1 drop
  3. Sweep distance-K/top-K [(5/3), (10/5), (15/7)] on one user; plot F1 vs compute time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative similarity metrics such as Cosine or Mahalanobis distance outperform Euclidean distance in the HED-LM filtering stage?
- Basis: Section 5.2 states authors plan systematic ablation study of Euclidean, cosine, and Mahalanobis distances
- Why unresolved: Euclidean distance ignores feature correlations; Mahalanobis might capture class information better but has higher computational complexity
- What evidence would resolve it: Comparative analysis of macro F1-scores and inference latency when swapping Euclidean for Mahalanobis or Cosine distance

### Open Question 2
- Question: Does adaptive segmentation using change-point detection (CPD) improve HED-LM performance compared to fixed-window strategy?
- Basis: Section 5.2 notes fixed boundaries can dilute class-specific cues and proposes CPD-driven windowing
- Why unresolved: Current fixed segmentation (3 windows of 60 samples) may misalign with physiological transitions; unclear if adaptive segmentation maintains token efficiency
- What evidence would resolve it: Experimental results comparing accuracy of fixed windows versus CPD-based dynamic windows

### Open Question 3
- Question: Is the HED-LM framework generalizable to other sensor modalities and classification domains beyond fatigue detection?
- Basis: Section 5.2 lists "applying HED-LM across diverse domains and signal modalities" as future research direction
- Why unresolved: Study validated only on accelerometer data for fatigue; effectiveness on signals with different noise profiles is unverified
- What evidence would resolve it: Successful application and evaluation on distinct tasks such as stress detection or sleep stage recognition

## Limitations
- Dataset specificity: Performance gains tied to accelerometer magnitude signals from 19 runners; may not generalize to other sensor modalities
- Domain knowledge dependency: GPT-4o-generated thresholds are dataset-specific and may not transfer to different subjects or fatigue patterns
- Resource constraints: distance-K=10, top-K=5 configuration requires ~100 LLM API calls per user, making large-scale deployment expensive

## Confidence

**High confidence**: The 2-shot configuration consistently outperforming larger-shot variants (67.70% vs 52.68% F1) is well-supported by direct comparisons in section 4.2.2.

**Medium confidence**: The 16.6% improvement over random selection assumes the dataset's feature-label relationships are learnable through LLM reasoning. This may not hold for datasets with more ambiguous patterns.

**Medium confidence**: The claim that "domain knowledge significantly improves precision" is supported by the 8.08% drop without it, but this is based on aggregate results rather than per-user analysis across all subjects.

## Next Checks
1. Apply HED-LM to a different sensor dataset (e.g., human activity recognition) and measure if the 16.6% improvement over random selection persists
2. Analyze per-user F1 scores with and without domain knowledge to identify which subjects benefit most and under what signal characteristics
3. Systematically sweep distance-K and top-K parameters across users to quantify the relationship between API call volume and F1 score improvements, identifying the optimal cost-performance point