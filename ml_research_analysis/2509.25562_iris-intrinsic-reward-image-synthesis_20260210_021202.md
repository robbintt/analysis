---
ver: rpa2
title: 'IRIS: Intrinsic Reward Image Synthesis'
arxiv_id: '2509.25562'
source_url: https://arxiv.org/abs/2509.25562
tags:
- image
- reward
- iris
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IRIS, a reinforcement learning framework that
  improves text-to-image generation using only intrinsic rewards without external
  human-labeled data or domain-specific verifiers. The key insight is that autoregressive
  T2I models with higher self-certainty tend to generate simple, uniform images less
  aligned with human preferences, while lower self-certainty correlates with richer,
  more vivid images.
---

# IRIS: Intrinsic Reward Image Synthesis

## Quick Facts
- **arXiv ID**: 2509.25562
- **Source URL**: https://arxiv.org/abs/2509.25562
- **Reference count**: 40
- **Primary result**: Novel RL framework that improves text-to-image generation using only intrinsic rewards (no external data or verifiers), achieving state-of-the-art performance across multiple benchmarks.

## Executive Summary
This paper introduces IRIS (Intrinsic Reward Image Synthesis), a reinforcement learning framework that improves text-to-image generation by maximizing negative self-certainty (NSC) as an intrinsic reward signal. The key insight is that autoregressive T2I models with higher self-certainty tend to generate simple, uniform images less aligned with human preferences, while lower self-certainty correlates with richer, more vivid images. IRIS trains models to minimize their own output certainty, encouraging mode-covering behavior that produces more diverse and compositionally rich images. Empirical results show IRIS significantly improves Janus-Pro models across multiple benchmarks, surpassing individual external rewards and matching ensemble external reward performance while avoiding domain-specific constraints and reward hacking.

## Method Summary
IRIS is a reinforcement learning framework that trains autoregressive text-to-image models using only intrinsic rewards derived from the model's own uncertainty. The core mechanism maximizes Negative Self-Certainty (NSC), defined as the negative KL divergence between a uniform distribution and the model's output distribution. This encourages mode-covering behavior, producing more diverse and detailed images compared to mode-seeking approaches. The framework uses Group Relative Policy Optimization (GRPO) to update the model, applying the NSC reward to both text-based chain-of-thought reasoning and image generation. The method also promotes the emergence of descriptive chain-of-thought reasoning, enhancing instruction following capabilities. IRIS is trained on 553 GenEval instructions and evaluated on multiple benchmarks including T2I-CompBench, WISE, and TIIF-Bench.

## Key Results
- **Performance gains**: 13.3% improvement on T2I-CompBench, 28.8% on WISE, and 10.7%/4.2% on TIIF-short/long benchmarks
- **Superior to external rewards**: Outperforms models trained with individual external rewards and matches ensemble external reward performance
- **Better generalization**: Demonstrates superior performance on specialized categories and real-world design prompts where external rewards lose efficacy

## Why This Works (Mechanism)

### Mechanism 1: Mode-Covering via Negative Self-Certainty (NSC)
Maximizing NSC encourages the model's output distribution to be broad and cover multiple plausible outcomes rather than collapsing to a single high-probability mode. This is achieved by minimizing forward KL divergence from the uniform distribution to the model's distribution. The authors observe that high model certainty correlates with "simple and uniform" images, whereas lower certainty correlates with "vivid images rich in detail." This creates a mode-covering behavior that produces more diverse and compositionally rich outputs.

### Mechanism 2: Emergence of Descriptive Chain-of-Thought (CoT)
The IRIS training process incentivizes the model to spontaneously generate more detailed and descriptive semantic chain-of-thought reasoning prior to image synthesis. By maximizing NSC on text tokens, the model is encouraged to "expand" short prompts into richer, more explorative semantic representations. This richer CoT then conditions the image generation, leading to higher quality and better instruction following.

### Mechanism 3: Generalization via Internal Prior Over External Signals
An intrinsic reward signal derived from the model's own internal state enables better generalization across diverse domains compared to individual external rewards. External rewards are often narrow and prone to "reward hacking" (e.g., optimizing aesthetics at the expense of realism). IRIS exploits the model's intrinsic prior knowledge, encouraging "the model's inherent reasoning capabilities" without being constrained to specific reward model domains.

## Foundational Learning

- **Concept: KL Divergence (Forward vs. Backward)**
  - Why needed here: IRIS relies on minimizing *forward* KL divergence to encourage the model's output distribution to be broad and cover multiple modes. This is distinct from *backward* KL, which is mode-seeking.
  - Quick check question: Why does the paper use the forward KL divergence (`KL(U || π_θ)`) instead of the backward KL (which is related to entropy) for the NSC reward?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper uses GRPO, a variant of PPO, for its reinforcement learning loop. It samples a group of outputs per query and uses their relative rewards to compute advantages, providing an implicit baseline.
  - Quick check question: How is the advantage `Â_i,t` for a token calculated in GRPO, and why does this remove the need for a separate value model?

- **Concept: Autoregressive T2I with Chain-of-Thought (CoT)**
  - Why needed here: IRIS is designed for autoregressive models that first generate a text-based "chain-of-thought" before generating the image tokens. The intrinsic reward is applied to this entire sequence.
  - Quick check question: In the IRIS framework, what are the two components of the output sequence `o_i`, and over which parts is the intrinsic reward NSC computed?

## Architecture Onboarding

- **Component map**: Prompt Input -> CoT Generation -> Image Generation -> Reward Computation (NSC) -> GRPO Policy Update
- **Critical path**:
  1. **Prompt Input**: A text prompt `q` is fed into the MLLM
  2. **CoT Generation**: The model generates a semantic text description (Chain-of-Thought) `o_text` conditioned on `q`
  3. **Image Generation**: The model then generates image tokens `o_img` conditioned on `q` and `o_text`. The full output is `o = (o_text, o_img)`
  4. **Reward Computation**: For each token in `o`, the NSC reward is computed. The total reward for the sequence is the sum of these per-token rewards
  5. **Policy Update (GRPO)**: Multiple sequences are generated for each prompt. Their rewards are compared to calculate advantages, which are then used to update the model weights to maximize the objective in Eq. (1)

- **Design tradeoffs**:
  - Intrinsic vs. External Reward: IRIS uses only intrinsic signals, removing the need for costly or biased external data/models. The tradeoff is that the signal is less direct; the model must learn that higher internal uncertainty correlates with desired external qualities.
  - Text vs. Image Certainty: The paper finds that *minimizing* self-certainty on both text and image tokens is crucial for T2I. This is a key tradeoff/insight, as maximizing certainty is beneficial for reasoning tasks like math.
  - RL vs. Gradient Descent: Using GRPO (an RL algorithm) is essential. The paper shows that directly optimizing the NSC objective with gradient descent leads to model collapse, as it will simply maximize uncertainty towards a random output.

- **Failure signatures**:
  - Direct Optimization: If you treat NSC as a differentiable loss function and perform gradient descent to maximize it, the model will collapse to generating meaningless, high-entropy noise after a few hundred steps
  - Reward Hacking (External): If using an individual external reward like HPSv2, the model may "hack" the reward by improving aesthetics but degrading other qualities like realism
  - Inconsistent Chat Templates: Using the wrong chat template for the Janus-Pro model can lead to incorrect results

- **First 3 experiments**:
  1. **Sanity Check (RL vs. GD)**: Replicate the ablation from Sec 4.3, Fig 5. Train the model with direct gradient ascent on NSC and with GRPO on NSC. Confirm that the direct optimization fails (images become meaningless) while the GRPO method succeeds
  2. **Core Ablation (Text & Image Certainty)**: Conduct the ablation from Sec 4.3, Fig 7 & 8. Train three models: (a) minimizing text SC only, (b) minimizing both text and image SC (IRIS), and (c) minimizing text SC and maximizing image SC. Compare performance on external reward metrics to confirm the IRIS configuration is optimal
  3. **Baseline Comparison**: Train a baseline model with an individual external reward (e.g., HPSv2) and another with IRIS. Evaluate both on a diverse benchmark (like WISE) to confirm that IRIS provides more balanced improvements across Consistency, Realism, and Aesthetic

## Open Questions the Paper Calls Out
- **Adapting intrinsic rewards to non-autoregressive T2I architectures**: The authors explicitly identify adapting intrinsic rewards to continuous diffusion or masked modeling approaches as an "interesting future research direction"
- **Optimizing NSC via standard gradient descent with regularizers**: The paper shows direct gradient descent on NSC produces meaningless images, but doesn't test if adding explicit constraints (e.g., CLIP-score regularization) could prevent this collapse
- **Generalizing NSC to other generative domains**: The paper contrasts objective reasoning (math/code) where certainty should be maximized, with image generation where it should be minimized, but leaves other subjective domains (video, music) unexplored

## Limitations
- **Correlation vs. causation**: The link between lower model self-certainty and higher image quality remains primarily correlational without clear mechanistic justification
- **Architectural dependence**: The framework's dependence on autoregressive architecture with explicit CoT generation limits its applicability to non-autoregressive or encoder-only models
- **Implementation sensitivity**: The method is sensitive to implementation details - direct gradient optimization fails completely, suggesting careful RL implementation is critical

## Confidence

**High Confidence**: The empirical improvements over external rewards (13.3% on T2I-CompBench, 28.8% on WISE) are well-documented and reproducible given the described methodology. The failure of direct gradient optimization on NSC is a clear, observable phenomenon.

**Medium Confidence**: The mechanism explaining why NSC works (mode-covering behavior via forward KL) is theoretically sound but the connection to human perceptual preferences is primarily correlational. The emergence of descriptive CoT reasoning is plausible given the training setup but the causality is not definitively established.

**Low Confidence**: The claim that IRIS "matches ensemble external reward performance" requires careful scrutiny - the comparison may be influenced by implementation details, and the ensemble baseline may not represent state-of-the-art external reward combinations.

## Next Checks

1. **Mechanistic Validation**: Systematically vary the KL divergence direction and magnitude (e.g., test backward KL, entropy, different β values) on a held-out validation set to determine if the forward KL with uniform distribution is uniquely optimal, or if the benefits come from any form of uncertainty maximization.

2. **Cross-Architecture Generalization**: Implement IRIS on a non-autoregressive model (e.g., diffusion-based) or a model without explicit CoT generation to determine whether the core insight (negative self-certainty as reward) transfers beyond the specific architectural constraints.

3. **Reward Hacking Vulnerability**: Design targeted adversarial prompts that specifically exploit the NSC reward structure (e.g., prompts that reward high-entropy noise while appearing semantically valid) to test whether IRIS is truly immune to reward hacking or simply shifts the vulnerability to different attack vectors.