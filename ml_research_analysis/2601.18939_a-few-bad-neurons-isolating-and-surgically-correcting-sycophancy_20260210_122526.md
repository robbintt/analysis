---
ver: rpa2
title: 'A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy'
arxiv_id: '2601.18939'
source_url: https://arxiv.org/abs/2601.18939
tags:
- probe
- sycophantic
- sycophancy
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a targeted neuron-level fine-tuning method to
  reduce sycophantic behavior in large language models. Using sparse autoencoders
  and linear probes, it identifies the top 3% of MLP neurons most predictive of sycophancy
  and fine-tunes only those neurons via gradient masking.
---

# A Few Bad Neurons: Isolating and Surgically Correcting Sycophancy

## Quick Facts
- arXiv ID: 2601.18939
- Source URL: https://arxiv.org/abs/2601.18939
- Authors: Claire O'Brien; Jessica Seto; Dristi Roy; Aditya Dwivedi; Sunishchal Dev; Kevin Zhu; Sean O'Brien; Ashwinee Panda; Ryan Lagasse
- Reference count: 13
- One-line primary result: Sparse, neuron-level fine-tuning using SAEs and linear probes reduces sycophancy in Gemma models as effectively as full-model fine-tuning but with significantly less data.

## Executive Summary
This work proposes a targeted neuron-level fine-tuning method to reduce sycophantic behavior in large language models. Using sparse autoencoders and linear probes, it identifies the top 3% of MLP neurons most predictive of sycophancy and fine-tunes only those neurons via gradient masking. Applied to Gemma-2-2B and 9B models, the approach matches or exceeds state-of-the-art sycophancy mitigation across four benchmarks (Syco-Bench, NLP, POLI, PHIL) using significantly less data than full-model fine-tuning. The method demonstrates that sparse, interpretable neuron-level updates offer a scalable alternative to broad fine-tuning, maintaining effectiveness even with limited data while preserving general capability.

## Method Summary
The method uses sparse autoencoders to decompose MLP activations into interpretable sparse features, trains linear probes on these features to identify sycophancy-correlated neurons, and performs gradient masking to update only those neurons during fine-tuning. The process involves extracting SAE features from informative MLP layers, training a probe to classify sycophantic vs non-sycophantic responses, decoding probe weights to identify neuron indices, and applying gradient masks during backpropagation to update only selected neurons. The approach targets 2.8-3.2% of neurons globally across selected layers using a custom loss function with KL divergence and entropy regularization.

## Key Results
- Reduces sycophancy scores on Syco-Bench, NLP, POLI, and PHIL benchmarks to match or exceed state-of-the-art methods
- Achieves comparable results with significantly less training data than full-model fine-tuning
- Maintains general capability while reducing targeted behavior through sparse neuron updates
- Demonstrates interpretable feature isolation where SAE probe weights show clear outliers versus residual probe weights clustering near zero

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse autoencoder representations isolate sycophancy-correlated features with higher interpretability than raw residuals.
- Mechanism: SAEs encode dense MLP activations into sparse latent dimensions where each dimension ideally corresponds to a monosemantic, human-interpretable concept. Probe weights trained on SAE features show clear outliers (highly positive/negative weights) correlated with sycophancy, while residual probe weights cluster near zero with no distinguishable signal.
- Core assumption: Sycophantic behavior is encoded in a sparse subset of features rather than distributed broadly across the residual stream.
- Evidence anchors: [abstract] "Using sparse autoencoders (SAEs) and linear probes, we isolate the 3% of MLP neurons most predictive of a target behavior"; [section 3.1] "SAE probe weights were also clustered near 0, except with a few highly positive or negative outliers that correspond to neurons strongly correlated with sycophancy"

### Mechanism 2
- Claim: Global top-p neuron selection across concatenated layer features identifies a sparse circuit responsible for sycophancy.
- Mechanism: Rather than selecting top-k neurons per layer, the probe is trained on concatenated SAE features from multiple informative layers, and top-p selection is applied globally. This accounts for inter-layer communication channels where complex behaviors span multiple layers.
- Core assumption: Sycophancy-relevant information flows through low-rank subspaces connecting multiple layers.
- Evidence anchors: [section 3.1] "We select a top-p set of neurons for the entire subset, resulting in a different number of neurons included per layer depending on their importance"; [section 2] "Merullo et al. [2025] shows that transformer language models establish and pass information through inter-layer communication channels using low-rank subspaces"

### Mechanism 3
- Claim: Gradient masking restricts updates to sycophancy-correlated neurons, preserving general capability while reducing targeted behavior.
- Mechanism: A binary mask (1 for selected indices, 0 otherwise) is applied during backpropagation. Only the i-th column of up_proj/gate_proj and i-th row of down_proj are updated for each selected neuron index. Custom loss adds KL divergence to clean model outputs and entropy regularization.
- Core assumption: Updating only sycophancy-correlated neurons avoids unintended distributional shift in unrelated capabilities.
- Evidence anchors: [section 3.3] "To ensure that only selected neurons are updated, we attach a hook to the MLP layers that, during backpropagation, masks the gradients"; [abstract] "sparse, neuron-level updates offer a scalable and precise alternative to full-model fine-tuning, remaining effective even in situations when little data is available"

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs) for feature decomposition
  - Why needed here: The entire method relies on SAEs to transform dense MLP activations into interpretable sparse features where sycophancy correlates appear as outliers.
  - Quick check question: Given an SAE with decoder weight matrix W_dec, how would you project a probe weight vector w_probe from SAE space back to the residual stream dimension?

- Concept: Linear probes as feature importance indicators
  - Why needed here: Probe weights directly identify which sparse features (and consequently which MLP neurons) predict sycophancy.
  - Quick check question: If a linear probe trained on SAE features achieves 93% accuracy, what does a weight magnitude of 1.76 vs 0.02 tell you about feature relevance?

- Concept: Gradient masking / gradient surgery
  - Why needed here: The intervention requires updating only selected neurons without modifying the rest of the model.
  - Quick check question: In PyTorch, how would you implement a hook that zeroes gradients for all but specified row/column indices in a linear layer's weight tensor?

## Architecture Onboarding

- Component map: Pre-trained LLM -> MLP layers with up_proj/gate_proj/down_proj -> SAE encoding -> Linear probe -> Gradient masking hook -> Custom loss
- Critical path: 1) Forward pass → extract MLP activations at selected layers; 2) SAE encoding → sparse feature activations (max/mean pooled); 3) Concatenate layer features → probe prediction; 4) Decode probe weights via SAE decoder → identify neuron indices; 5) Backward pass → apply gradient mask → update only selected weights
- Design tradeoffs:
  - SAE probe (80% accuracy, interpretable outliers) vs residual probe (60% accuracy, no outliers): paper chooses interpretability
  - Global top-p (2.8-3.2% of neurons) vs per-layer top-k: global selection respects multi-layer circuits but may skip layer-specific features
  - MLP-only targeting vs attention heads: SPT baseline targets attention; this method targets MLP for finer granularity
- Failure signatures:
  - Over-training: catastrophic forgetting when training few neurons (paper explicitly notes this risk)
  - Under-training: insufficient sycophancy reduction if learning rate/steps too low
  - Residual probe outperforming SAE probe on some metrics (observed for Gemma-2-9B on Open-Ended Sycophancy)
  - Accuracy drops on non-sycophantic benchmarks if entangled neurons are modified
- First 3 experiments:
  1. Replicate probe training: Train linear probe on single-layer SAE activations, verify 93%+ in-domain accuracy, visualize weight distribution to confirm outlier pattern.
  2. Layer selection validation: Run greedy layer selection on last 30% of MLP layers with your target model, compare selected layers to paper's reported indices (5,6,7,8,11,13,15,19,24 for 2B).
  3. Gradient masking sanity check: Fine-tune with mask on random 3% of neurons vs probe-selected neurons; verify selected neurons reduce sycophancy while random neurons do not.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing key hyperparameters (learning rate, batch size, steps, α/β loss weights) and dataset generation details blocks exact replication
- Method validated only on Gemma-2 models and specific benchmarks, limiting generalizability claims
- Does not provide systematic ablations to rule out alternative explanations for observed sycophancy reduction

## Confidence
- High confidence in the core empirical claim: Targeted neuron-level fine-tuning using SAEs and linear probes reduces sycophancy more efficiently than full-model fine-tuning
- Medium confidence in the interpretability claim: SAE-based probes show clear outliers, but superiority over residual probes is only demonstrated on the same task and model family
- Medium confidence in the generalizability claim: The method is effective on Gemma-2 models, but broader applicability to other architectures or behaviors is not established

## Next Checks
1. Train a linear probe on SAE features from the specified Gemma-2 layers, verify >90% in-domain accuracy, and visualize the decoded weight distribution to confirm clear outliers
2. Systematically vary learning rate, batch size, and α/β loss weights during neuron-level fine-tuning to identify optimal settings and assess robustness
3. Apply the same SAE+probe+masking pipeline to a different LLM family (e.g., Llama-3) and behavior type (e.g., toxicity) to evaluate generalizability beyond Gemma-2 and sycophancy