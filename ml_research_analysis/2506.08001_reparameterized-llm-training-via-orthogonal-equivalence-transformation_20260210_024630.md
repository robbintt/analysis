---
ver: rpa2
title: Reparameterized LLM Training via Orthogonal Equivalence Transformation
arxiv_id: '2506.08001'
source_url: https://arxiv.org/abs/2506.08001
tags:
- layer
- poet
- training
- layers
- orthogonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes POET, a novel training framework for large
  language models that uses orthogonal equivalence transformation to reparameterize
  weight matrices, effectively preserving their spectral properties. POET reparameterizes
  each neuron as the product of two learnable orthogonal matrices and a fixed random
  weight matrix, enabling stable training and improved generalization.
---

# Reparameterized LLM Training via Orthogonal Equivalence Transformation

## Quick Facts
- **arXiv ID**: 2506.08001
- **Source URL**: https://arxiv.org/abs/2506.08001
- **Reference count**: 40
- **Primary result**: POET achieves better validation perplexity than AdamW on LLaMA models using fewer trainable parameters through orthogonal equivalence transformation

## Executive Summary
POET is a novel training framework for large language models that reparameterizes weight matrices using orthogonal equivalence transformation to preserve spectral properties. The method constrains weight matrices to maintain the singular value spectrum of their random initialization throughout training, which improves stability and generalization compared to standard unconstrained optimization. To scale to large models, POET introduces Stochastic Primitive Optimization (SPO) for efficient orthogonal matrix learning and Cayley-Neumann parameterization for numerical stability. Extensive experiments demonstrate that POET outperforms standard AdamW training and low-rank methods like GaLore and LoRA on LLaMA models, achieving better validation perplexity with fewer trainable parameters.

## Method Summary
POET reparameterizes each weight matrix $W$ as $RW_0P$, where $W_0$ is a fixed random matrix and $R, P$ are learnable orthogonal matrices. The method preserves the singular values of $W_0$ throughout training while learning the corresponding singular vectors. To scale this approach, POET employs two key approximations: SPO factorizes the orthogonal transformation into a product of small sparse matrices (primitives) that modify only subsets of dimensions, and Cayley-Neumann parameterization approximates the Cayley transform to maintain orthogonality without expensive matrix inversions. The merge-then-reinitialize strategy updates the weight matrix periodically and resets the orthogonal parameters, enabling memory-efficient training.

## Key Results
- On 1.3B LLaMA model, POET-FS (b=1/2) achieves 13.70 perplexity versus 14.73 for AdamW using only 406.88M trainable parameters
- POET maintains small hyperspherical energy, ensuring strong generalization across tasks
- POET provides flexible trade-off between parameter efficiency and convergence speed
- The method generalizes well to finetuning tasks beyond pretraining

## Why This Works (Mechanism)

### Mechanism 1: Spectrum Preservation via Orthogonal Equivalence
- **Claim:** Constraining the weight matrix to maintain the singular value spectrum of its random initialization throughout training improves stability and generalization compared to unconstrained optimization.
- **Mechanism:** POET reparameterizes the weight matrix $W$ as $RW_0P$, where $W_0$ is a fixed random matrix and $R, P$ are learnable orthogonal matrices. Since orthogonal transformations rotate vectors without changing their length, the singular values of $W$ remain identical to $W_0$ throughout training. This prevents the spectral explosion or decay often seen in standard training (AdamW), ensuring the network maintains a consistent "smoothness" bound.
- **Core assumption:** The generalization capacity of an LLM is linked to the spectral norm of its weights, and a well-conditioned random spectrum provides a sufficient basis for learning.
- **Evidence anchors:** [abstract] "POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties..."; [section 3.1] "It is also straightforward to verify that the spectral properties of $W_{RP}$ remain identical to those of the initial matrix $W_0$."

### Mechanism 2: Efficient Capacity via Stochastic Primitives
- **Claim:** A large orthogonal matrix transformation can be effectively approximated and learned by sequentially applying small, random "primitive" orthogonal matrices rather than optimizing the full matrix at once.
- **Mechanism:** Instead of learning a dense $m \times m$ matrix, Stochastic Primitive Optimization (SPO) learns a sequence of sparse matrices (primitives) that only modify a small subset of dimensions (block size $b$) at a time. This reduces the parameter count from $O(m^2)$ to $O(b^2)$ (Fully Stochastic) or $O(m \cdot b)$ (Block Stochastic), breaking the memory bottleneck of orthogonal parameterization.
- **Core assumption:** The optimization path for the orthogonal transformation can be decomposed into a product of sparse, low-rank updates without significant loss of representational power.
- **Evidence anchors:** [section 3.2.1] "SPO factorizes it into a product of primitive orthogonal matrices, each involving significantly fewer trainable parameters."; [figure 4] Visualizes how sparse primitives approximate the full transformation.

### Mechanism 3: Orthogonality Maintenance via Cayley-Neumann
- **Claim:** The Cayley parameterization of orthogonal matrices can be stabilized and accelerated by avoiding direct matrix inversion using a Neumann series approximation.
- **Mechanism:** The Cayley transform $(I+Q)(I-Q)^{-1}$ guarantees orthogonality for skew-symmetric $Q$, but the inverse is computationally expensive and unstable. POET approximates $(I-Q)^{-1}$ using a truncated Neumann series ($\sum Q^k$), avoiding the inverse calculation while approximately maintaining the orthogonal constraint.
- **Core assumption:** The skew-symmetric matrix $Q$ maintains a spectral norm less than 1 (ensured by merge-then-reinitialize), ensuring the Neumann series converges.
- **Evidence anchors:** [section 3.2.2] "To address this, we approximate the matrix inverse using a truncated Neumann series... By avoiding matrix inversion, the training stability of POET is improved."; [table 7] Shows that $k=0$ (no approximation terms) diverges, while $k \ge 1$ stabilizes training.

## Foundational Learning

- **Concept:** Spectral Norm & Singular Values
  - **Why needed here:** The core hypothesis of POET is that "spectral properties" control learning stability. You must understand that singular values represent the scaling factor of a linear transformation.
  - **Quick check question:** If an orthogonal matrix $R$ is applied to a vector $v$, what happens to the norm of the resulting vector $Rv$?

- **Concept:** Cayley Transform
  - **Why needed here:** This is the mathematical engine used to parameterize the learnable orthogonal matrices. It maps an unconstrained skew-symmetric matrix to a valid rotation matrix.
  - **Quick check question:** What property must the input matrix $Q$ satisfy for the Cayley transform to produce an orthogonal matrix?

- **Concept:** Merge-Then-Reinitialize Strategy
  - **Why needed here:** This is the practical "trick" that makes POET scalable. It prevents the accumulation of approximation errors and allows memory-efficient training.
  - **Quick check question:** After merging the learned rotation into the weights $W$, what happens to the orthogonal matrix parameters in the next step?

## Architecture Onboarding

- **Component map:** Fixed Backbone ($W_0$) -> Primitive Factories ($R, P$) -> Merger -> Current Weights
- **Critical path:** Input $\to W_{eff} = R W_{current} P \to \text{Output}$ -> Gradients to $R, P$ parameters -> Optimizer updates $Q$ -> Merge step (every $T_m$ steps): $W_{current} = R W_{current} P$; Reset $R, P \to I$
- **Design tradeoffs:**
  - **Block Size ($b$):** Small $b$ (e.g., 64) saves memory/params but slows convergence (Table 2). Large $b$ (e.g., 256) is faster but heavier.
  - **Merge Frequency ($T_m$):** Low $T_m$ reduces memory (don't store history) but limits the "range" of the rotation achievable before reset.
  - **Neumann Terms ($k$):** Higher $k$ = better orthogonality approximation but slower. Paper finds $k=3$ is a sweet spot (Table 7).
- **Failure signatures:**
  - **NaN Loss / Instability:** Often due to insufficient Neumann terms ($k$ too low) or learning rate too high for the $Q$ matrices.
  - **Stagnant Perplexity:** If $W_0$ spectrum is poor or block size $b$ is too small for the model scale.
  - **Memory OOM:** Occurs if you attempt to store all primitives sequentially instead of merging (i.e., skipping the "reinitialize" step).
- **First 3 experiments:**
  1. **Sanity Check (Replication):** Train a small Llama-60M using POET-FS ($b=1/2$) on C4 for 10k steps. Validate that perplexity drops and stays stable (verifies the merge-and-CNP loop).
  2. **Ablation ($k$-terms):** Run the 60M setup with $k=\{1, 2, 3\}$. Plot "Orthogonal Approximation Error" to see which $k$ keeps the error bounded without exploding (calibrating the Cayley-Neumann component).
  3. **Scaling Test:** Compare POET-BS ($b=256$) vs AdamW on Llama-350M. Monitor memory usage vs. validation perplexity to confirm the efficiency-performance trade-off before committing to 1B+ runs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the theoretical mechanism driving POET's three-phase training dynamics (conical shell searching, stable learning, and final adjusting)?
- **Basis in paper:** [explicit] The authors state on page 8 regarding the three phases: "The exact mechanism behind this phenomenon remains an open question, and understanding it could offer valuable insights into large-scale model training."
- **Why unresolved:** The paper empirically observes the dynamics but lacks a formal derivation explaining why the cosine similarity stabilizes at specific ranges or why validation perplexity improves linearly during the stable phase.
- **What evidence would resolve it:** Theoretical analysis of gradient flow in the orthogonal subspace or causal ablation studies isolating the factors that trigger transitions between phases.

### Open Question 2
- **Question:** What constitutes the optimal singular value structure for the fixed random weight matrix $W_0$ to maximize generalization?
- **Basis in paper:** [explicit] Section 5.3 notes that uniform spectrum initialization performs poorly compared to normalized Gaussian and concludes: "Finding the optimal singular value structure for weights remains an important open problem."
- **Why unresolved:** The paper tested standard, Xavier, uniform, and normalized initializations, but the performance variance suggests a sensitivity to spectral distribution that is not yet fully characterized.
- **What evidence would resolve it:** A systematic sweep over various spectral distributions (e.g., power-law vs. uniform) for $W_0$ to identify which structures yield the lowest perplexity for specific model scales.

### Open Question 3
- **Question:** Can POET maintain its efficiency and convergence properties when scaling to ultra-large models (e.g., 70B+)?
- **Basis in paper:** [inferred] Experiments validate POET on models up to 3B parameters. While the authors claim scalability, the interaction between the approximation error (CNP) and optimization stability in massive weight matrices remains unverified at the 70B scale.
- **Why unresolved:** The efficiency of Stochastic Primitive Optimization (SPO) may degrade or require hyperparameter retuning that negates efficiency gains as matrix dimensions grow significantly larger than tested.
- **What evidence would resolve it:** Training runs on 7B–70B models comparing wall-clock time, memory footprint, and convergence stability against baselines like AdamW or Muon.

## Limitations

- **Orthogonality Approximation Gap:** The Cayley-Neumann parameterization with finite Neumann terms ($k=3$) introduces approximation error, and the paper does not quantify the cumulative spectral drift over training epochs.
- **Task Generalization Scope:** The experiments focus exclusively on pretraining LLaMA models and finetuning on GLUE, lacking evaluation on diverse finetuning scenarios like instruction tuning or mathematical reasoning.
- **Computational Overhead:** While POET reduces trainable parameters, the orthogonal matrix computations add computational overhead, and the paper lacks wall-clock time comparisons against AdamW.

## Confidence

- **High Confidence:** The spectral preservation mechanism via orthogonal equivalence transformation is mathematically sound and well-supported by the theoretical framework.
- **Medium Confidence:** The Stochastic Primitive Optimization approximations (FS and BS variants) appear effective based on the experimental results, but lack ablation studies on different model scales.
- **Low Confidence:** The merge-and-reinitialize strategy's long-term stability is not thoroughly validated over the full training duration for very large models.

## Next Checks

1. **Spectral Drift Monitoring:** Implement continuous tracking of the Frobenius norm error ||RRᵀ-I||/||I|| during training to empirically validate that orthogonality approximation remains bounded throughout all training phases.

2. **Cross-Task Generalization:** Evaluate POET on instruction-tuned models (e.g., Alpaca, Vicuna) and mathematical reasoning datasets (GSM8K, MATH) to verify that the spectral preservation benefits extend beyond standard language modeling and GLUE-style tasks.

3. **Training Efficiency Benchmarking:** Conduct wall-clock time comparisons between POET and AdamW across different hardware configurations (A100 vs H100 GPUs) to determine whether the parameter efficiency translates to actual training speedups in practice.