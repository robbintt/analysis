---
ver: rpa2
title: Object-Level Verbalized Confidence Calibration in Vision-Language Models via
  Semantic Perturbation
arxiv_id: '2504.14848'
source_url: https://arxiv.org/abs/2504.14848
tags:
- confidence
- calibration
- probability
- verbalized
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object-level verbalized confidence
  miscalibration in vision-language models, where models often express high confidence
  in incorrect or hallucinated object detections. The proposed Confidence Calibration
  through Semantic Perturbation (CSP) framework addresses this by constructing a perturbed
  dataset where Gaussian noise is applied to key object regions at varying intensities
  to simulate visual uncertainty, with corresponding confidence labels.
---

# Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation

## Quick Facts
- arXiv ID: 2504.14848
- Source URL: https://arxiv.org/abs/2504.14848
- Reference count: 40
- Key outcome: Object-level verbalized confidence miscalibration addressed via semantic perturbation, improving accuracy from 0.25 to 0.67 and reducing ECE from 0.5699 to 0.4225

## Executive Summary
This paper addresses the problem of object-level verbalized confidence miscalibration in vision-language models (VLMs), where models often express high confidence in incorrect or hallucinated object detections. The proposed Confidence Calibration through Semantic Perturbation (CSP) framework addresses this by constructing a perturbed dataset where Gaussian noise is applied to key object regions at varying intensities to simulate visual uncertainty, with corresponding confidence labels. The model is then trained through a two-stage process combining supervised fine-tuning and preference optimization. Experiments across multiple benchmarks show significant improvements in calibration metrics while maintaining or improving overall task performance.

## Method Summary
The CSP framework constructs a perturbed dataset by extracting object regions from images using GroundingDINO and SAM, then applying Gaussian noise to these regions at intensities inversely proportional to target confidence levels via a diffusion schedule. The model is trained in two stages: first, supervised fine-tuning (SFT) establishes baseline mapping from perturbed images to confidence labels; second, preference optimization (SimPO) refines confidence calibration by learning pairwise preferences between correct and incorrect confidence expressions. The approach also reformats confidence queries to third-person perspective to reduce sycophantic overconfidence.

## Key Results
- Accuracy increases from 0.25 to 0.67, F1 score from 0.21 to 0.68, and Expected Calibration Error decreases from 0.5699 to 0.4225 on average
- The two-stage training (SFT + SimPO) outperforms either stage alone for confidence calibration
- Third-person perspective (TPP) framing of confidence queries reduces sycophantic overconfidence
- Local object-masked perturbation outperforms global noise injection for calibration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Localized semantic perturbation creates explicit training signal for mapping visual uncertainty to confidence expression.
- **Mechanism:** GroundingDINO + SAM extract object mask → Gaussian noise applied to masked region only → noise intensity inversely proportional to target confidence label (via diffusion schedule $T_c = T_{max} \times (1 - c/100)$) → model learns visual ambiguity → verbalized confidence mapping through supervised examples.
- **Core assumption:** Degrading object-region pixels simulates the visual uncertainty VLMs encounter in real deployment; the model can generalize from synthetic noise to natural ambiguity.
- **Evidence anchors:**
  - [abstract]: "establishing an explicit mapping between visual ambiguity and confidence levels"
  - [section 3.2]: Ablation shows global noise fails to improve calibration—mask-based perturbation required
  - [corpus]: Related work (Contamination Detection, arXiv:2511.03774) uses multi-modal semantic perturbation for VLM analysis, supporting perturbation-based approaches but not specifically this mechanism

### Mechanism 2
- **Claim:** Two-stage training (SFT → SimPO) outperforms either stage alone for confidence calibration.
- **Mechanism:** SFT establishes baseline mapping from perturbed images to confidence labels (cross-entropy loss on confidence tokens) → SimPO then refines by learning pairwise preferences: winning response = correct confidence $c$, rejected = $100\% - c$ → margin objective reinforces ranking.
- **Core assumption:** The model first needs exposure to uncertainty-conditioned examples (SFT) before preference optimization can effectively shape the confidence distribution.
- **Evidence anchors:**
  - [section 4.3.2]: "SimPO only" without SFT shows nearly no gains; "SFT only" yields moderate improvement but lags full method
  - [corpus]: Weak direct evidence; related work (ConfProBench, arXiv:2508.04576) evaluates confidence in MLLMs but does not address two-stage training

### Mechanism 3
- **Claim:** Third-person perspective (TPP) framing of confidence queries reduces sycophantic overconfidence.
- **Mechanism:** Instead of "How confident are you?" (first-person), prompt asks "A language model was asked... How certain are you about the model's answer?" → externalizes evaluation → reduces pressure to appear confident.
- **Core assumption:** First-person prompts induce social pressure toward confident self-presentation; third-party framing elicits more objective assessment.
- **Evidence anchors:**
  - [section 3.2]: "first-person confidence queries often resulted in inflated self-assessments. Framing queries in third-person promotes more objective calibration"
  - [corpus]: Assumption: Related work (arXiv:2506.03723) on verbalized confidence in LLMs suggests prompting strategy affects calibration, but does not directly test TPP framing in VLMs

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** Primary metric for evaluating whether verbalized confidence matches empirical correctness; lower ECE = better calibration.
  - **Quick check question:** If a model assigns 80% confidence to 100 predictions and 80 are correct, what is the calibration error for that bin? (Answer: 0—perfect calibration for that bin.)

- **Concept: Preference Optimization (SimPO)**
  - **Why needed here:** Core training component; preference optimization teaches models to rank correct confidence higher than incorrect alternatives without requiring a separate reward model.
  - **Quick check question:** In SimPO, what is the role of the margin parameter λ? (Answer: Ensures the winning response has sufficiently higher probability than the rejected response.)

- **Concept: Verbalized vs. Internal Confidence**
  - **Why needed here:** VLMs can express confidence through natural language ("I'm 80% sure") separately from token-level probabilities; this paper targets verbalized confidence explicitly.
  - **Quick check question:** Why might verbalized confidence be miscalibrated even if internal logits are accurate? (Answer: The model may not have learned to translate internal uncertainty into appropriate linguistic expression.)

## Architecture Onboarding

- **Component map:** Image + Query → GroundingDINO (bounding boxes) → SAM (segmentation mask) → Masked Gaussian noise injection (confidence-scaled) → Perturbed dataset D → Stage 1: SFT (cross-entropy on confidence tokens) → Stage 2: SimPO (preference pairs: c vs. 100%-c) → Calibrated VLM

- **Critical path:** Segmentation quality (GroundingDINO + SAM) determines noise localization → incorrect masks mean noise applied to wrong regions → calibration signal degrades. Validate segmentation on target domain before full training.

- **Design tradeoffs:**
  - **Local vs. global perturbation:** Local (object-masked) preserves context but requires accurate segmentation; global is simpler but ablation shows it fails to improve calibration.
  - **SFT-only vs. SFT+SimPO:** SFT-only faster but underperforms; SimPO requires preference pair construction but significantly improves results.
  - **Noise schedule (γ parameter):** Larger γ = faster degradation per step; affects granularity of confidence levels. Paper does not report sensitivity analysis.

- **Failure signatures:**
  - **Segmentation drift:** If target images contain objects poorly handled by GroundingDINO/SAM, noise misapplied → confidence calibration may not transfer.
  - **Overfitting to noise pattern:** Model learns to detect synthetic Gaussian noise rather than general visual uncertainty → fails on natural blur/occlusion.
  - **Confidence collapse:** Excessive SimPO margin may push model toward uniform low-confidence outputs.

- **First 3 experiments:**
  1. **Validate segmentation on target domain:** Run GroundingDINO + SAM pipeline on 100 sample images from your deployment distribution; manually inspect mask quality. If >20% masks are inaccurate, consider alternative detection/segmentation models.
  2. **Ablate noise schedule:** Train with γ ∈ {0.01, 0.05, 0.1} on a held-out validation set; compare ECE. This tests sensitivity to noise intensity granularity (not reported in paper).
  3. **Cross-domain transfer test:** Train CSP on COCO-style images, evaluate on domain-shifted data (e.g., medical imaging, satellite imagery). If ECE regresses significantly, domain-specific perturbation data may be required.

## Open Questions the Paper Calls Out

- **Question:** Does the CSP framework maintain its calibration efficacy when scaled to significantly larger, proprietary-scale vision-language models?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that experiments were conducted on mid-sized models and "assessing whether the benefits of CSP scale with larger models remains an open direction."
  - **Why unresolved:** Larger models may possess different emergent reasoning capabilities or starting calibration states that react unpredictably to the proposed semantic perturbation and preference optimization.
  - **What evidence would resolve it:** Empirical evaluation of calibration metrics (ECE, Brier Score) on VLMs with parameters exceeding 70B or proprietary architectures before and after CSP application.

- **Question:** How can the semantic perturbation strategy be adapted to handle non-object visual uncertainties, such as relational reasoning, background context, or temporal ambiguity?
  - **Basis in paper:** [explicit] The paper acknowledges that it centers on "object-centric perturbations," but "broader forms of uncertainty—such as those involving relational reasoning... or temporal ambiguity—may require extending this framework."
  - **Why unresolved:** The current method relies on masking specific object regions via GroundingDINO/SAM; uncertainty in spatial relations or video sequences involves dynamic or abstract dependencies that single-frame object masks cannot capture.
  - **What evidence would resolve it:** A modified perturbation mechanism targeting relational features or temporal frames, validated on benchmarks requiring reasoning beyond object identification.

- **Question:** Is the proposed linear mapping between Gaussian noise intensity and confidence labels effective for specialized domains like medical or scientific imaging?
  - **Basis in paper:** [explicit] The authors list "Task and Domain Generalization" as a limitation, noting that applicability to domains such as "medical or scientific imaging remains to be validated."
  - **Why unresolved:** The current training data (RLAIF) consists of natural images where noise simulates visual ambiguity; in specialized domains, "noise" may not correlate linearly with diagnostic uncertainty, potentially leading to misaligned confidence scores.
  - **What evidence would resolve it:** Experiments applying CSP to domain-specific VLMs (e.g., medical VQA) to determine if noise-based perturbations successfully translate to calibrated confidence in high-stakes scenarios.

## Limitations
- **Unknown parameters:** Diffusion noise parameters T_max and γ are not explicitly specified, creating reproducibility barriers.
- **Domain transfer uncertainty:** The linear mapping between Gaussian noise intensity and confidence labels may not hold for specialized domains like medical imaging.
- **Segmentation dependency:** Effectiveness depends heavily on object detection and segmentation quality, which is not validated across diverse domains.

## Confidence
- **High confidence**: The two-stage training architecture (SFT + SimPO) is clearly specified and supported by ablation results showing both stages are necessary for calibration gains.
- **Medium confidence**: The claim that third-person perspective reduces sycophantic overconfidence is supported by qualitative statements but lacks comparative experiments with first-person framing on the same models.
- **Medium confidence**: The assertion that local perturbation outperforms global perturbation is supported by ablation, but the ablation only tests one alternative (global noise), not other perturbation strategies.

## Next Checks
1. **Segmentation quality validation**: Run GroundingDINO + SAM on 100 sample images from your target domain and manually inspect mask quality. If >20% masks are inaccurate, consider alternative detection/segmentation models before proceeding with full CSP training.
2. **Noise schedule sensitivity**: Train CSP with γ ∈ {0.01, 0.05, 0.1} on a held-out validation set and compare ECE. This tests sensitivity to noise intensity granularity, which is not reported in the paper.
3. **Cross-domain transfer test**: Train CSP on COCO-style images, then evaluate on domain-shifted data (e.g., medical imaging, satellite imagery). If ECE regresses significantly, domain-specific perturbation data may be required for effective calibration.