---
ver: rpa2
title: Fixed-Confidence Best Arm Identification with Decreasing Variance
arxiv_id: '2502.07199'
source_url: https://arxiv.org/abs/2502.07199
tags:
- best
- arms
- cost
- sampling
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the fixed-confidence best arm identification
  problem in a multi-armed bandit setting where rewards are Gaussian with fixed means
  but time-varying variances that decrease as $1/t$. This setup models situations
  where feedback becomes more accurate over time, such as multiple reviewers improving
  their product evaluations with repeated use.
---

# Fixed-Confidence Best Arm Identification with Decreasing Variance

## Quick Facts
- arXiv ID: 2502.07199
- Source URL: https://arxiv.org/abs/2502.07199
- Authors: Tamojeet Roychowdhury; Kota Srinivas Reddy; Krishna P Jagannathan; Sharayu Moharir
- Reference count: 17
- Primary result: Proposes two policies (WTCS and PS-WSE) for best arm identification with decreasing variance rewards, achieving theoretical guarantees and significant cost improvements over classical algorithms

## Executive Summary
This paper addresses the fixed-confidence best arm identification problem in multi-armed bandits where rewards are Gaussian with time-varying variance that decreases as 1/t. The setup models scenarios where feedback becomes more accurate over time, such as product reviews improving with repeated use. The authors propose two novel policies: Wait-Then-Continuously-Sample (WTCS) for the case where the sub-optimality gap is known, and Periodic Sampling with Weighted Successive Elimination (PS-WSE) for the general case without prior knowledge. Both policies achieve the desired confidence level while minimizing a cost function that combines stopping time and weighted sampling cost. Simulations demonstrate significant performance improvements over classical algorithms like Successive Elimination and LUCB, particularly in terms of reduced sampling cost despite potentially longer identification times.

## Method Summary
The paper tackles the fixed-confidence best arm identification problem with Gaussian rewards having fixed means μ_k but time-varying variance σ²/t. The cost function C_π = τ_π + c·η_π combines stopping time and weighted sampling cost. Two policies are proposed: WTCS waits initially for t_W = (2σ/Δ)√(Kc·ln(K/δ)) rounds, then samples all arms continuously for t_W/(Kc) rounds, outputting the arm with highest empirical mean. PS-WSE samples active arms every λ=cK rounds with weighted averages μ̂_j(r) = Στ w_{τ,r}·X_{j,τλ} where weights w_{τ,r} = 2τ/(r(r+1)), eliminating arms where μ̂_j < μ̂_best - 2σ/t·√(λ·ln(2Kt²/(λ²δ))). Theoretical guarantees show both achieve desired confidence while minimizing cost, and simulations demonstrate significant improvements over baselines.

## Key Results
- WTCS and PS-WSE achieve desired confidence level while minimizing cost function combining stopping time and weighted sampling cost
- PS-WSE shows significant reduction in sampling cost compared to classical algorithms despite potentially longer identification times
- Both policies perform well across multiple experimental settings with different arm configurations and parameter values
- Theoretical bounds match empirical performance, validating the proposed approaches

## Why This Works (Mechanism)
The paper exploits the decreasing variance property of rewards over time to design more efficient sampling strategies. By recognizing that later samples have lower variance, the PS-WSE algorithm uses weighted averages that give more importance to recent samples, enabling more aggressive arm elimination decisions. The WTCS algorithm leverages the known sub-optimality gap to optimize the initial waiting period before continuous sampling. The cost function design encourages strategies that balance the trade-off between quick identification and expensive sampling, leading to policies that are both theoretically sound and practically effective.

## Foundational Learning
- **Multi-armed bandit problem**: Why needed - forms the fundamental framework for sequential decision making under uncertainty. Quick check - can you explain the exploration-exploitation trade-off?
- **Time-varying variance rewards**: Why needed - captures scenarios where feedback quality improves over time. Quick check - understand how 1/t variance affects confidence intervals.
- **Fixed-confidence setting**: Why needed - requires identifying best arm with high probability rather than minimizing regret. Quick check - distinguish between fixed-budget and fixed-confidence settings.
- **Weighted averages for sequential data**: Why needed - allows incorporating information from samples with different variances. Quick check - can you derive the optimal weights for combining samples?
- **Successive elimination strategy**: Why needed - provides baseline for comparing adaptive arm elimination. Quick check - understand how elimination thresholds scale with time.
- **Cost-aware bandit algorithms**: Why needed - real-world applications require considering sampling costs. Quick check - can you formulate the general cost function for this problem?

## Architecture Onboarding

**Component Map**
Environment -> WTCS/PS-WSE -> Arm Elimination -> Best Arm Output

**Critical Path**
Reward generation (N(μ_k, σ²/t)) -> Sampling decision (WTCS wait-then-sample or PS-WSE periodic sampling) -> Weighted mean calculation -> Elimination test -> Output best arm

**Design Tradeoffs**
- Known gap (WTCS) vs unknown gap (PS-WSE) affects algorithm complexity and performance
- Continuous sampling vs periodic sampling impacts computational overhead and responsiveness
- Fixed waiting period vs adaptive waiting affects initial exploration efficiency
- Weighted vs unweighted averaging determines how variance reduction over time is exploited

**Failure Signatures**
- If arms aren't being eliminated despite clear quality differences, check weighted mean calculation and elimination threshold scaling
- If cost exceeds theoretical bounds, verify wait time calculation in WTCS uses correct sub-optimality gap
- If PS-WSE performs worse than baselines, check that elimination isn't too aggressive due to incorrect weight computation

**3 First Experiments**
1. Implement basic environment with K=5 arms, arithmetic progression means, and verify reward generation follows N(μ_k, σ²/t)
2. Test WTCS with known Δ on simple instance, confirm wait time and cost match theoretical predictions
3. Validate PS-WSE weighted mean calculation with synthetic data, ensuring weights w_{τ,r} = 2τ/(r(r+1)) are computed correctly

## Open Questions the Paper Calls Out
None

## Limitations
- Exact confidence level δ used in simulations is not specified, which could affect reported performance
- Number of Monte Carlo runs for averaging results is unspecified, making statistical significance unclear
- LUCB implementation details are not provided, preventing exact baseline replication

## Confidence

**High Confidence**: Theoretical framework for time-varying variance rewards, cost function formulation, and basic WTCS and PS-WSE algorithms

**Medium Confidence**: Simulation results showing performance improvements over baselines, due to missing experimental details

**Low Confidence**: Exact numerical performance metrics, as they depend on unspecified parameters

## Next Checks

1. Implement PS-WSE with explicit verification of the weighted mean calculation w_{τ,r} = 2τ/(r(r+1)) and elimination threshold scaling with time t

2. Run controlled experiments with known Δ to verify WTCS achieves cost bounds matching the theoretical prediction ~6σ/Δ·√(Kc·log(K/δ))

3. Perform statistical significance testing on simulation results using multiple runs (at least 100) to confirm reported performance improvements are not due to random variation