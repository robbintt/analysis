---
ver: rpa2
title: Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time
  Recurrent RL
arxiv_id: '2602.02236'
source_url: https://arxiv.org/abs/2602.02236
tags:
- learning
- online
- fine-tuning
- recurrent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to online fine-tuning of pretrained
  driving policies using Real-Time Recurrent Reinforcement Learning (RTRRL), enabling
  continuous adaptation to changing environments without retraining from scratch.
  The method combines offline behavioral cloning with online RTRRL-based fine-tuning,
  leveraging biologically plausible gradient computation methods (RTRL/RFLO) for recurrent
  neural networks.
---

# Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL

## Quick Facts
- arXiv ID: 2602.02236
- Source URL: https://arxiv.org/abs/2602.02236
- Reference count: 37
- This paper presents an approach to online fine-tuning of pretrained driving policies using Real-Time Recurrent Reinforcement Learning (RTRRL), enabling continuous adaptation to changing environments without retraining from scratch.

## Executive Summary
This paper presents a hybrid approach for autonomous driving that combines offline behavioral cloning (BC) with online reinforcement learning (RL) via Real-Time Recurrent Reinforcement Learning (RTRRL). The method enables continuous adaptation of pretrained driving policies to changing environments without requiring full retraining from scratch. The approach leverages biologically plausible gradient computation methods (RTRL/RFLO) for recurrent neural networks and demonstrates significant performance improvements through fine-tuning in both simulated and real-world driving tasks.

## Method Summary
The method uses a hybrid offline-online approach: first, a CNN encoder and RNN policy are pretrained via behavioral cloning on expert demonstrations (human driving data). Then, online fine-tuning is performed using RTRRL, which computes gradients in real-time during the forward pass without backpropagation through time. The system employs TD(λ) with eligibility traces for credit assignment, separate RNNs for policy and critic, and includes a parameter change penalty to prevent excessive drift from the pretrained policy. Three recurrent architectures are tested: CT-RNN, LRU, and LRC (LrcSSM).

## Key Results
- In CarRacing simulation, LRC models showed the largest improvement in evaluation reward during online fine-tuning, with most consistent performance across seeds.
- Real-world line-following experiments with event camera input showed immediate improvements in the first lap of fine-tuning.
- LRC architecture demonstrated superior performance and consistency compared to CT-RNN and LRU models across both simulated and real-world tasks.
- The approach effectively addresses distribution shift challenges by enabling continuous adaptation of pretrained policies to changing environmental conditions.

## Why This Works (Mechanism)

### Mechanism 1: Forward-Only Gradient Computation Eliminates Backward Pass Latency
RTRRL enables per-timestep parameter updates by computing gradients online during the forward pass, avoiding memory-intensive BPTT. RTRL maintains an approximate Jacobian trace updated at each step via Ĵt+1 = Ĵt(I + ∇ht f(xt, ht)) + J̄t. For diagonal architectures (LRU, LrcSSM), this becomes computationally tractable. RFLO approximates this with fixed random feedback matrices B, reducing complexity from O(n⁴) to O(n²).

### Mechanism 2: TD(λ) Eligibility Traces Bridge Temporal Gaps in Sparse Reward Settings
Eligibility traces enable effective learning when rewards are delayed relative to causative actions. At each step, TD-error δt = rt + γ·v̂(ht+1) − v̂(ht) is computed. Eligibility traces eθ accumulate decaying gradient history (eθ,t = γλ·eθ,t−1 + ∇θ L). Parameters update as θt+1 ← θt + η·δt·eθ,t, distributing credit backward through time.

### Mechanism 3: Behavioral Cloning Initialization Provides Viable Starting Policy for Online Refinement
Pretraining on human demonstrations produces policies sufficiently competent to begin online fine-tuning without catastrophic exploration. CNN encoder + RNN policy trained via supervised learning (maximizing log-likelihood of expert actions). Parameter change penalty Lθ = β‖θpre − θt‖² prevents drift from viable behavior during online RL with batch-size 1.

## Foundational Learning

- **Concept: Temporal-Difference Learning with Eligibility Traces**
  - **Why needed here:** Core mechanism for online credit assignment without replay buffers. Understanding TD-error computation and trace decay is essential for debugging learning dynamics.
  - **Quick check question:** Given δt = 0.5, γ = 0.99, λ = 0.95, and previous trace eθ,t−1 = 2.0 with current gradient ∇θ L = 0.1, compute the new eligibility trace and parameter update with learning rate η = 10⁻⁵.

- **Concept: Recurrent Neural Network State Dynamics**
  - **Why needed here:** The policy must process temporal sequences; understanding how h_t evolves and how gradients flow through recurrence is critical for selecting appropriate architectures (CT-RNN vs. LRU vs. LRC).
  - **Quick check question:** For a CT-RNN with τ = 1.0 and φ = tanh, given h = 0.5, W = [0.8, 0.2], ξ = [1.0, 0.5], compute the time derivative ḣ and explain how this affects gradient flow.

- **Concept: Behavioral Cloning and Distribution Shift**
  - **Why needed here:** Understanding why BC alone fails (covariate shift) motivates the hybrid offline-online approach.
  - **Quick check question:** A BC policy trained on expert states achieves 95% accuracy on held-out expert trajectories but only 60% success when deployed. Explain why this occurs and how online fine-tuning addresses it.

## Architecture Onboarding

- **Component map:** [Observation ot] → [CNN Encoder] → [Encoded vector xt] → [RNN Policy] → Action distribution π(at|ht) → [RNN Critic] → Value estimate v̂(ht) → [RTRL/RFLO] → Jacobian Ĵt → [TD(λ) Update] → Parameter changes Δθ

- **Critical path:** Observation → CNN encoding → RNN forward pass → action sampling → environment step → TD-error computation → eligibility trace update → parameter update. All must complete within control frequency budget (~100 Hz for real-world deployment claimed).

- **Design tradeoffs:**
  - RTRL vs. RFLO: RTRL gives exact gradients but O(n⁴) cost; RFLO is O(n²) but uses approximate feedback. Paper uses RTRL for diagonal models (LRU, LrcSSM) and RFLO for full CT-RNN.
  - LRC vs. LRU vs. CT-RNN: LRC showed best fine-tuning results (most consistent improvement); LRU failed on real-world task (insufficient expressivity as single-layer model).
  - Shared vs. separate RNNs: Paper uses separate RNNs for policy and critic, contrary to prior RTRRL work—trades parameter efficiency for training stability.

- **Failure signatures:**
  - Pretrained policy immediately fails on real-world task: likely encoder mismatch or insufficient training data diversity.
  - Fine-tuning shows no improvement or instability: check learning rates (actor ~10⁻⁶, critic ~10⁻⁵), verify eligibility trace decay isn't too aggressive.
  - High variance across seeds: LRC most consistent; CT-RNN shows more variability—prefer LRC for deployment.
  - LRU models fail to complete laps: linear dynamics insufficient for steering behavior in single-layer configuration.

- **First 3 experiments:**
  1. Validate pretrained policy in target environment without fine-tuning: Establish baseline performance; if reward < 200 in CarRacing, revisit BC training (data quality, encoder reconstruction loss).
  2. Single-lap fine-tuning with monitoring: Track cumulative reward, TD-error magnitude, and parameter drift; verify eligibility traces are non-zero and TD-error decreases over time.
  3. Architecture ablation (LRC vs. CT-RNN vs. LRU): Run 5 seeds each for 10 laps of fine-tuning; compare final performance variance and convergence speed. Expect LRC to show lowest variance per Figure 6.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper lacks critical architectural specifications (RNN dimensions, CNN architecture, hyperparameter details) needed for direct reproduction.
- RTRRL-based RL has limited validation in the corpus; related work (e-prop, RFLO) is cited but not directly tested on control tasks.
- Claims about computational efficiency and biological plausibility are theoretical assertions not empirically validated in this work.

## Confidence

- **High Confidence:** Core mechanism of RTRRL with TD(λ) for online credit assignment, and the general hybrid BC + online RL approach for handling distribution shift.
- **Medium Confidence:** Architecture-specific results (LRC > CT-RNN > LRU) and real-world deployment feasibility, as these depend on unreported implementation details and limited experimental validation.
- **Low Confidence:** Claims about computational efficiency and biological plausibility, which are theoretical assertions not empirically validated in this work.

## Next Checks

1. **Architecture sensitivity:** Reproduce the CarRacing ablation (LRC vs CT-RNN vs LRU) with 5+ seeds each to verify the reported consistency differences and final performance ordering.

2. **Hyperparameter robustness:** Systematically vary λ (0.7-0.99) and learning rates (10× range) to test the claimed sensitivity and identify stable operating regions.

3. **Distribution shift quantification:** Measure the covariate shift between expert demonstrations and autonomous rollouts before and after fine-tuning using domain divergence metrics (e.g., MMD) to validate the core motivation.