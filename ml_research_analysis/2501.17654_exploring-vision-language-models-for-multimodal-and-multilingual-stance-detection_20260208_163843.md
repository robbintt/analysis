---
ver: rpa2
title: Exploring Vision Language Models for Multimodal and Multilingual Stance Detection
arxiv_id: '2501.17654'
source_url: https://arxiv.org/abs/2501.17654
tags:
- text
- image
- languages
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of vision-language models (VLMs)
  for multimodal and multilingual stance detection on social media data. It evaluates
  four state-of-the-art VLMs on a newly extended dataset covering seven languages
  and multimodal inputs.
---

# Exploring Vision Language Models for Multimodal and Multilingual Stance Detection

## Quick Facts
- **arXiv ID:** 2501.17654
- **Source URL:** https://arxiv.org/abs/2501.17654
- **Reference count:** 14
- **Primary result:** VLMs generally rely more on text than images for stance detection across seven languages, with Ovis showing the most consistent multilingual performance.

## Executive Summary
This paper investigates vision-language models (VLMs) for multimodal and multilingual stance detection on social media data. The study evaluates four state-of-the-art VLMs on a newly extended dataset covering seven languages and multimodal inputs. Results show that VLMs prioritize text over visual content for stance classification, with particular emphasis on text contained within images. The research finds that multilingual consistency is achieved through shared latent semantic spaces in the LLM backbone, with Ovis demonstrating notable performance across languages despite only officially supporting English.

## Method Summary
The study uses zero-shot inference with four VLMs (InternVL2, Qwen2-VL, Ovis 1.6, Llama-Vision) on a validation split of the Liang et al. (2024) dataset (1,778 tweet-image pairs). Tweet text was translated into six languages using Google Translate. Experiments include modality ablation (text-only vs. image-only), content ablation (text blackout vs. content blackout using GATE OCR), and cross-lingual consistency evaluation. Performance is measured using Macro F1 score and McNemar's test for statistical significance.

## Key Results
- VLMs rely significantly more on text than images for stance detection across all languages
- Ovis 1.6 showed the most consistent performance across languages despite only officially supporting English
- Text contained within images provides more signal than other visual content for stance detection
- Multimodal models generally outperform single-modality approaches when both text and images are available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs prioritize the text modality over visual cues for stance detection, even when both are available.
- **Mechanism:** The LLM backbone likely drives the final classification decision, treating image embeddings as auxiliary context rather than primary evidence, unless the image contains textual content.
- **Core assumption:** The semantic density of text in social media posts provides higher information gain for stance classification than the visual composition of the accompanying image.
- **Evidence anchors:**
  - [abstract] "results show that VLMs generally rely more on text than images for stance detection and this trend persists across languages."
  - [Page 3] "Three of the four models... perform significantly better in the Tweet Only... than the Image Only scenario."
  - [corpus] "Unified Multimodal and Multilingual Retrieval" notes VLMs often underperform specialized text models on text-only tasks, suggesting the text encoder capability is critical.
- **Break condition:** If a post contains purely visual sarcasm or symbolism (e.g., a meme with no text) where the text is neutral, this mechanism suggests the model will likely misclassify the stance.

### Mechanism 2
- **Claim:** Visual utility in VLMs is heavily mediated by Optical Character Recognition (OCR) capabilities rather than semantic visual understanding.
- **Mechanism:** When models process images, they extract significant signal from text embedded in the image (e.g., headlines, meme text). Performance drops significantly when this text is obscured, suggesting the vision encoder acts partially as a text-extraction tool.
- **Core assumption:** The vision encoder is trained or fine-tuned to prioritize textual regions in images over graphical or photographic content for information retrieval tasks.
- **Evidence anchors:**
  - [abstract] "VLMs rely significantly more on text contained within the images than other visual content."
  - [Page 4] "blacking out the text portion of the image has a greater negative effect on the F1 score than blacking out other content."
  - [corpus] Corpus evidence regarding specific OCR-robust VLMs is weak in the provided neighbors, though "TowerVision" and "Chitrarth" imply general VLM struggles with non-English visual text, reinforcing the difficulty of this mechanism in low-resource settings.
- **Break condition:** If the image text is decorative or irrelevant to the stance (noise), the model may over-rely on it, leading to false positives or incorrect stance assignments.

### Mechanism 3
- **Claim:** Multilingual consistency is achieved via shared latent semantic spaces in the LLM backbone, independent of explicit multilingual support declarations.
- **Mechanism:** Models translate or map input concepts into a dominant internal representation (likely English-centric) during processing. This allows consistent predictions across languages because the underlying reasoning is anchored in a high-resource representation space.
- **Core assumption:** The LLM components possess sufficient cross-lingual transfer capabilities to map diverse scripts into a unified semantic vector before classification.
- **Evidence anchors:**
  - [abstract] "models studied tend to generate consistent predictions across languages whether they are explicitly multilingual or not."
  - [Page 9] "Ovis, which officially only supports English, was notably the most consistent... suggesting that its LLM, Gemma2, is in practice multilingual."
  - [corpus] "TowerVision" confirms that VLMs often follow an "English-centric design," supporting the mechanism that internal processing may default to English representations.
- **Break condition:** This mechanism degrades for low-resource languages (e.g., Hindi in the study) where the mapping to the high-resource semantic space is weaker or noisier.

## Foundational Learning

- **Concept:** **Stance Detection (vs. Sentiment Analysis)**
  - **Why needed here:** Unlike sentiment (positive/negative emotion), stance is relational to a specific target (e.g., favoring "Ukraine" but opposing "Russia"). The paper evaluates how well VLMs resolve these relationships.
  - **Quick check question:** If a tweet says "I love how dedicated Trump is to his goals," is the sentiment positive, but the stance regarding "Biden" Neutral?

- **Concept:** **Zero-Shot Multimodal Evaluation**
  - **Why needed here:** The paper relies on 0-shot prompting (no fine-tuning) to test raw model capabilities. Understanding this helps distinguish between a model's inherent knowledge and its ability to follow specific task instructions.
  - **Quick check question:** Does the prompt provided in Figure 1 ask the model to generate a label based on training data it has already seen, or to infer the label solely from the provided instruction?

- **Concept:** **Modality Ablation (Text vs. Image)**
  - **Why needed here:** The study's core methodology involves removing one modality (e.g., "Text Blackout") to measure the other's contribution. This is critical for diagnosing whether a VLM is "cheating" by reading text in images rather than understanding the image content.
  - **Quick check question:** In the "Content Blackout" experiment (Page 4), is the model evaluated on the image's text or the image's graphics?

## Architecture Onboarding

- **Component map:** Vision Encoder (VM) -> Projector Layer -> LLM Backbone (LM) -> Input Pipeline -> Label Generation
- **Critical path:**
  1. Input Processing: Prompt (English) + Tweet (Translated) + Image (Raw/Blackout)
  2. Fusion: Vision tokens are concatenated with text tokens (Page 2)
  3. Inference: LLM generates "Favor", "Against", or "Neutral"
  4. Evaluation: Compare generated label against ground truth using Macro F1 and McNemar's test

- **Design tradeoffs:**
  - Model Size vs. Consistency: Llama-Vision (11B) had the worst performance and consistency, while Ovis (9B) was best. *Assumption:* Architecture alignment (projector training) and data quality outweigh raw parameter count
  - Text vs. Vision: Models with weaker vision encoders (InternVL2) may be hurt by image input (noise), while stronger ones (Ovis) benefit

- **Failure signatures:**
  - High Disagreement on Low-Resource Languages: Hindi showed significant prediction disagreement across models (Figure 5)
  - Negative Vision Contribution: InternVL2 performed worse when images were added (Figure 3), indicating vision noise
  - Script Sensitivity: Qwen2-VL showed high disagreement between Chinese and other languages, potentially due to over-fitting to specific script optimization

- **First 3 experiments:**
  1. Modality Ablation (Tweet Only vs. Image Only): Establish a baseline for how much signal comes from text vs. visuals. (Replicate Page 3 results)
  2. Text Blackout Test: Run inference on images where text is masked. If performance drops to random guessing, the model relies on OCR, not visual understanding. (Replicate Page 4 Table 3)
  3. Cross-Lingual Consistency Check: Translate a validation set into 2 high-resource (e.g., Spanish, German) and 1 low-resource (e.g., Hindi) language and check Cohen's Kappa between predictions. (Replicate Page 7 Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do attention distributions and feature attribution methods quantify the specific contribution of text versus visual features on individual instances?
- **Basis in paper:** [Explicit] The authors state that future explorations might obtain more instance-level insights by using attention distributions or feature attribution methods to investigate interactions between text and images.
- **Why unresolved:** The current study analyzed modality reliance through overall prediction distributions and ablation studies (blackout experiments) rather than examining internal model states on a per-example basis.
- **What evidence would resolve it:** Attention heatmaps (e.g., Grad-CAM) and token-level attribution scores mapping specific stance predictions to image regions versus text tokens.

### Open Question 2
- **Question:** Does the prediction consistency of VLMs degrade when applied to low-resource languages or distinct language families?
- **Basis in paper:** [Explicit] The authors note they avoided translating the dataset into low-resource languages due to potential Google Translate errors and suggest future work should introduce them to test for inconsistencies.
- **Why unresolved:** The study was limited to mid-to-high resource languages (Germanic, Romance, Indo-European, Sino-Tibetan), leaving the robustness of models like Ovis on low-resource languages unknown.
- **What evidence would resolve it:** Evaluation of Macro F1 scores and cross-lingual agreement (Cohen's Kappa) on human-translated datasets covering low-resource languages (e.g., distinct from the currently tested families).

### Open Question 3
- **Question:** To what extent does processing full temporal video inputs improve stance detection performance compared to static frames?
- **Basis in paper:** [Explicit] The authors acknowledge that using only the first frame of videos or GIFs resulted in a loss of information and suggest future work should leverage VLMs' ability to process different vision inputs.
- **Why unresolved:** The methodology required converting all video data to static images to fit the evaluated models' standard image processing capabilities, ignoring temporal dynamics.
- **What evidence would resolve it:** A comparative evaluation of VLMs using native video encoders versus static frame extraction on the video-containing subset of the dataset.

## Limitations

- The study relies on tweet IDs requiring API hydration, which is subject to rate limits and potential data loss from deleted or protected tweets
- The paper lacks explicit generation hyperparameters (temperature, top-k) for zero-shot inference, which could influence output distributions
- The research was limited to mid-to-high resource languages, leaving the robustness of VLMs on low-resource languages unexplored

## Confidence

- **High Confidence:** VLMs rely more on text than images for stance detection (supported by ablation studies and McNemar's test)
- **Medium Confidence:** VLMs rely heavily on text within images (supported by Text Blackout results, but OCR robustness not fully explored)
- **Medium Confidence:** Multilingual consistency across VLMs regardless of explicit support (strong evidence for Ovis, but mechanism inference requires validation)
- **Low Confidence:** LLM backbone is primary driver with vision encoder as auxiliary tool (mechanistic interpretation requiring more granular architectural analysis)

## Next Checks

1. **Hydrate and Verify Dataset:** Obtain the tweet IDs from the Liang et al. (2024) dataset and hydrate them using the X API. Compare the number of successfully retrieved tweets to the original study's statistics to quantify potential data loss. Log any tweets that fail to hydrate and analyze the reasons (e.g., deletion, privacy settings).

2. **Replicate Text Blackout Experiment:** Conduct the Text Blackout experiment (masking text in images using GATE OCR) and measure the drop in Macro F1 score. Compare this drop to the Content Blackout (masking non-text image content) to validate the claim that text within images is the primary visual signal for stance detection.

3. **Cross-Lingual Consistency with Low-Resource Languages:** Translate a validation set into a high-resource language (e.g., Spanish) and a low-resource language (e.g., Hindi). Calculate Cohen's Kappa for inter-model agreement between predictions in these two languages. This will validate the claim that multilingual consistency degrades for low-resource languages due to weaker semantic mapping.