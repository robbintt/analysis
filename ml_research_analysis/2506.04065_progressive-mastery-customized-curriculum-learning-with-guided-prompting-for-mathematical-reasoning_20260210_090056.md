---
ver: rpa2
title: 'Progressive Mastery: Customized Curriculum Learning with Guided Prompting
  for Mathematical Reasoning'
arxiv_id: '2506.04065'
source_url: https://arxiv.org/abs/2506.04065
tags:
- training
- samples
- learning
- difficulty
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of post-training large language
  models (LLMs) for mathematical reasoning, specifically inefficient sample utilization
  and inflexible processing of difficult samples. The authors propose Customized Curriculum
  Learning (CCL), a framework with two key innovations: model-adaptive difficulty
  definition that customizes curriculum datasets based on each model''s individual
  capabilities, and "Guided Prompting" that dynamically reduces sample difficulty
  through strategic hints.'
---

# Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2506.04065
- Source URL: https://arxiv.org/abs/2506.04065
- Reference count: 19
- Key outcome: CCL improves mathematical reasoning performance by 1.04-13.80% across 1.5B and 7B models on five benchmarks

## Executive Summary
This paper addresses the limitations of post-training large language models for mathematical reasoning, specifically inefficient sample utilization and inflexible processing of difficult samples. The authors propose Customized Curriculum Learning (CCL), a framework with two key innovations: model-adaptive difficulty definition that customizes curriculum datasets based on each model's individual capabilities, and "Guided Prompting" that dynamically reduces sample difficulty through strategic hints. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks.

## Method Summary
CCL is a three-phase framework for mathematical reasoning post-training. First, curriculum construction uses model inference accuracy (computed from 16 sampled responses per sample) to rank and partition training data into easy, medium, and hard tiers. Second, guided prompting transforms difficult samples by prepending partial solution steps as hints, converting unsolvable problems into learnable ones. Third, multi-stage training proceeds from easy to hard with optional curriculum review mixing, validated across both supervised fine-tuning and GRPO reinforcement learning paradigms.

## Key Results
- CCL improved SFT performance by 1.04% (1.5B) and 4.96% (7B) across five benchmarks
- CCL improved GRPO performance by 13.80% (1.5B) and 2.44% (7B) across five benchmarks
- Model-adaptive difficulty definition outperformed predefined difficulty metrics in ablation studies
- Curriculum Review mixing strategy prevented catastrophic forgetting in later training stages

## Why This Works (Mechanism)

### Mechanism 1: Model-Adaptive Difficulty Definition
Customizing difficulty labels based on each model's actual performance yields more effective curriculum learning than predefined metrics. For each training sample, the model generates 16 responses via sampling (temperature=0.7), accuracy is computed as correct/total, samples are ranked by accuracy, and the dataset is partitioned into stages from easy to hard. A sample's true learning difficulty is reflected by the specific model's current success rate, not external labels which may not correlate with model-specific capabilities.

### Mechanism 2: Guided Prompting for Difficult Sample Adaptation
Augmenting overly difficult samples with targeted hints converts unsolvable problems into learnable ones, improving data utilization without performance degradation. Reference solutions are decomposed into step-by-step components, a prefix of steps is prepended to the question as a hint, transforming answer generation into simpler answer completion. Partial solution steps reduce cognitive load and bridge the gap between current capability and target difficulty.

### Mechanism 3: Multi-Stage Training with Curriculum Review
Progressive training from easy to hard, combined with periodic review of earlier material, prevents catastrophic forgetting and builds solid foundations. Staged SFT or GRPO on easy→medium→hard tiers with "Curriculum Review" that mixes easier samples into later stages to reinforce prior learning. Models benefit from mastering foundational concepts before advancing; without review, later-stage learning overwrites earlier knowledge.

## Foundational Learning

### Concept: Curriculum Learning
Why needed: The entire CCL framework builds on training from simple to complex rather than uniform sampling.
Quick check: Why might easy-to-hard ordering outperform random-order training for a model with limited capacity?

### Concept: SFT vs. GRPO (Reinforcement Learning)
Why needed: Paper validates CCL across both post-training paradigms with different loss formulations.
Quick check: What signal does GRPO use that SFT does not?

### Concept: Sampling-Based Accuracy Estimation
Why needed: Difficulty is defined via multi-sample accuracy, not single-pass inference.
Quick check: If a model solves a problem 3/10 times, what accuracy and implied difficulty ranking results?

## Architecture Onboarding

### Component map:
Curriculum Construction -> Guided Prompting -> Multi-Stage Trainer

### Critical path:
1. Run inference (n=16, temp=0.7) on all training samples to compute accuracy
2. Sort and partition into 3 tiers (easy/medium/hard)
3. Apply Guided Prompting to near-zero-accuracy samples
4. Train stage-by-stage with Curriculum Review mixing

### Design tradeoffs:
- Sampling overhead: n=16 is expensive but stable; smaller n increases noise
- Hint ratio α: controls solution exposure
- Stage count p: 3 stages used; more stages add complexity

### Failure signatures:
- No gain over uniform: likely using predefined instead of model-adaptive difficulty
- Late-stage degradation: insufficient Guided Prompting or missing Curriculum Review
- High variance: unstable difficulty estimates; increase n

### First 3 experiments:
1. Replicate model-adaptive vs. predefined difficulty ablation (Figure 4)
2. Compare retain/discard/adapt strategies for hard samples (Table 2)
3. Pilot 2-stage training with/without Curriculum Review to observe forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CCL generalize effectively to domains beyond mathematical reasoning, such as logical reasoning, code generation, and natural language inference?
- Basis: The Limitations section states: "we see great potential in extending the CCL framework to other domains such as logical reasoning, code generation, and natural language inference, allowing us to further investigate its generalizability across diverse task types."
- Why unresolved: All experiments were conducted exclusively on mathematical reasoning benchmarks. Different domains may have different difficulty characteristics that affect how well model-adaptive difficulty definition and guided prompting transfer.

### Open Question 2
- Question: Can CCL be effectively combined with other post-training strategies beyond SFT and GRPO, such as Proximal Policy Optimization (PPO)?
- Basis: The Limitations section acknowledges: "combining CCL with other post-training strategies—like PPO and broader reinforcement learning techniques—remains an open direction."
- Why unresolved: The paper only validates CCL under supervised fine-tuning and GRPO. PPO has different optimization dynamics, including a separate critic model, which may interact differently with curriculum-based data ordering and difficulty adaptation.

### Open Question 3
- Question: How sensitive is CCL performance to the choice of accuracy threshold τ and hint ratio α in the Guided Prompting mechanism?
- Basis: Algorithm 1 specifies these as input parameters, but the paper does not report ablation studies on different values of these hyperparameters.
- Why unresolved: The threshold τ determines when a sample is considered "adapted" enough to include, and α controls how much of the solution to reveal as hints. Without ablation, it is unclear whether the reported gains are robust to different settings or highly tuned to specific values.

## Limitations

- The computational overhead of inference-based difficulty assessment scales with dataset size and model scale
- Critical hyperparameters (accuracy threshold τ, hint ratio α, review mixing proportions) are unspecified
- The framework's effectiveness beyond mathematical reasoning domains remains unproven

## Confidence

- Model-adaptive difficulty definition: **Medium** - Core concept is sound but effectiveness depends heavily on accurate accuracy estimation
- Guided Prompting mechanism: **Medium** - Intuitively plausible but lacks direct corpus support and depends on unspecified parameters
- Multi-stage training with Curriculum Review: **High** - Well-established principle with supporting evidence from ablation studies

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary hint ratio α (0%, 10%, 30%, 50%) and sampling count n (8, 16, 32) to quantify their impact on both computational efficiency and final accuracy.

2. **Generalization Across Difficulty Metrics**: Apply CCL to datasets with different difficulty definitions and compare whether model-adaptive difficulty consistently outperforms predefined metrics across multiple mathematical reasoning domains.

3. **Long-term Retention Study**: Extend training duration and monitor accuracy on early-stage samples throughout training to precisely measure catastrophic forgetting rates with and without Curriculum Review.