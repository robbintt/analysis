---
ver: rpa2
title: 'DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas'
arxiv_id: '2602.01326'
source_url: https://arxiv.org/abs/2602.01326
tags:
- mask
- diffusion
- length
- arxiv
- expand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the fixed-length bottleneck in diffusion language
  models (DLMs) that hampers their code infilling performance. It introduces DreamOn,
  a novel framework that enables dynamic variable-length generation by augmenting
  the diffusion process with two special states: [expand] and [delete].'
---

# DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas

## Quick Facts
- arXiv ID: 2602.01326
- Source URL: https://arxiv.org/abs/2602.01326
- Reference count: 20
- Key outcome: DreamOn enables diffusion language models to generate variable-length sequences for code infilling by introducing [expand] and [delete] states, achieving performance on par with state-of-the-art autoregressive models.

## Executive Summary
DreamOn addresses a fundamental limitation in diffusion language models: their inability to generate variable-length sequences during code infilling tasks. The paper introduces a novel framework that augments the diffusion process with two special states—[expand] and [delete]—allowing the model to autonomously adjust sequence length during generation without architectural changes. Built on top of existing diffusion models like Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance comparable to state-of-the-art autoregressive models on benchmarks like HumanEval-Infilling and SantaCoder-FIM, while also matching oracle performance achieved with ground-truth length information.

## Method Summary
DreamOn extends diffusion language models by adding two special tokens to the vocabulary: [expand] and [delete]. During training, these tokens are introduced through a merging scheduler that probabilistically replaces [mask] tokens with the special states. The [expand] token, when predicted during inference, is replaced with two [mask] tokens to grow the sequence, while [delete] removes the token and potentially trailing [mask] tokens via broadcasting. To handle class imbalance where [delete] would otherwise dominate the loss, the paper introduces a weighted cross-entropy loss that scales [delete] loss by 1/N_delete. The model is trained on code infilling tasks using augmented data and evaluated on standard benchmarks.

## Key Results
- Achieves average pass@1 score of 90.8% on HumanEval-Infilling with ground-truth lengths, matching autoregressive baselines
- Demonstrates 2.1× inference speedup through deletion broadcasting, reducing average steps from 122.8 to 52.4
- Shows average absolute improvement of 26.4% over diffusion baselines on variable-length infilling tasks
- Generalizes to creative natural language tasks beyond code, demonstrating versatility of the approach

## Why This Works (Mechanism)

### Mechanism 1: Special State Augmentation for Native Length Control
The core innovation is augmenting the diffusion process with [expand] and [delete] tokens that enable variable-length generation without architectural changes. During training, these special states are mapped to [mask] in the forward process, forcing the model to learn when to predict them. At inference, [expand] tokens are replaced with two [mask] tokens while [delete] removes its position, allowing the model to autonomously adjust sequence length based on context. The break condition occurs if the model over-predicts these states due to training imbalance, causing unstable generation.

### Mechanism 2: Loss Weighting to Correct Token Class Imbalance
The paper addresses asymmetric token counts where multiple [mask] tokens correspond to [expand] targets but only one [mask] corresponds to [delete]. Without correction, [delete] dominates the gradient. The weighted loss scheme (Eq. 3) scales [delete] loss by 1/N_delete so total weight matches a single regular token. This prevents the model from overfitting to deletion signals and maintains balanced gradient contributions. Over-correction could cause excessive truncation or failure to contract oversized masks.

### Mechanism 3: Deletion Broadcasting for Inference Efficiency
When [delete] is predicted and all tokens to its right are [mask], the method eliminates all trailing masks in a single operation. This transforms deletion from token-by-token iteration into a length-prediction action, reducing diffusion steps when initial mask length far exceeds target length. The approach is limited to broadcasting when trailing tokens are purely [mask] to prevent losing content that should later be expanded.

## Foundational Learning

- **Masked Discrete Diffusion**: DreamOn builds on standard masked diffusion where tokens are progressively corrupted to [mask] and denoised back. Understanding the forward/backward process is prerequisite to grasping how [expand]/[delete] integrate into it. *Quick check: Can you explain why the loss in Eq. (1) is only evaluated at [mask] positions?*

- **Any-Order Infilling vs. Autoregressive Generation**: The paper positions DLMs as alternatives to AR models for infilling. Understanding why AR models need workarounds (e.g., relocating target spans) clarifies the motivation. *Quick check: Why does left-to-right autoregressive generation struggle with bidirectional context in infilling tasks?*

- **Data Augmentation via Pseudo-Masking**: DreamOn uses augmentation to construct training targets containing [expand]/[delete]. Understanding how merging schedulers create these pseudo-labels is key to implementation. *Quick check: What is the difference between the static scheduler and dynamic inverse scheduler for mask merging?*

## Architecture Onboarding

- **Component map**: Base DLM (Dream-7B / DiffuCoder-7B / DreamCoder-7B) -> Vocabulary extension (add [expand] and [delete]) -> Training augmentation pipeline (merge scheduler + random [delete] insertion) -> Inference loop (standard diffusion + expand/delete handlers + broadcasting logic + L_max cap)

- **Critical path**: 1) Augment input sequences with [expand]/[delete] using merge schedulers during training 2) Apply weighted cross-entropy loss with per-token weighting for [delete] 3) At inference, handle predicted special tokens immediately; apply broadcasting when conditions met

- **Design tradeoffs**: Static scheduler better for large expansions (short initial masks), worse for long masks; Dynamic inverse scheduler better for long masks, struggles with large expansions; Paper uses 1:1 mixture; Single [expand]→2×[mask] keeps output space small but requires multiple expansions for long targets

- **Failure signatures**: Over-generation (model fills all [mask] tokens → likely missing [delete] training or loss imbalance); Truncation (output too short → check [expand] merge probability or disable broadcasting); Inference stall (excessive diffusion steps → verify broadcasting is enabled and L_max is reasonable)

- **First 3 experiments**: 1) Replicate ablation (Table 2) with fixed mask lengths 4, 16, 64 on small validation set 2) Profile inference step count with and without deletion broadcasting to confirm 2× efficiency gain 3) Test generalization beyond code: Apply DreamOn to natural language infilling (e.g., ROCStories as in Appendix B)

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on fixed [expand] and [delete] tokens rather than richer vocabulary of expansion factors, potentially limiting fine-grained length adjustments
- Performance claims primarily validated on code infilling tasks with limited rigorous benchmarking on natural language tasks
- Long-term stability of loss weighting approach across diverse distributions and training durations remains untested

## Confidence
**High Confidence**: Claims about achieving oracle-level performance with ground-truth lengths and effectiveness of loss weighting (ablation shows 6.2% absolute improvement).

**Medium Confidence**: Generalizability claims to creative natural language tasks and 2.1× inference speedup from deletion broadcasting based on limited experimentation.

**Low Confidence**: Assertion that no architectural changes are needed is somewhat misleading as vocabulary extension and specialized training pipeline constitute meaningful modifications.

## Next Checks
1. Evaluate DreamOn's performance across code datasets with varying length distributions to assess whether 1:1 scheduler mixture remains optimal
2. Conduct controlled experiments comparing DreamOn against modified autoregressive baselines with oracle length access to establish diffusion advantages beyond length prediction
3. Apply DreamOn to document-level infilling tasks to identify failure modes when making multiple sequential length decisions over extended contexts