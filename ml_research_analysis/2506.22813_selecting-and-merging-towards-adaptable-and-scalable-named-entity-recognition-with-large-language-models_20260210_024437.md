---
ver: rpa2
title: 'Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition
  with Large Language Models'
arxiv_id: '2506.22813'
source_url: https://arxiv.org/abs/2506.22813
tags:
- domain
- data
- merging
- expert
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the SaM framework to improve named entity
  recognition (NER) with large language models by dynamically selecting and merging
  expert models. Instead of training a single unified model, SaM first trains multiple
  domain-specific expert models and then selects relevant experts for a target domain
  based on domain similarity and sampled-instance evaluation.
---

# Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models

## Quick Facts
- arXiv ID: 2506.22813
- Source URL: https://arxiv.org/abs/2506.22813
- Authors: Zhuojun Ding; Wei Wei; Chenghao Fan
- Reference count: 28
- Primary result: SaM outperforms fully-trained unified models by 10% average F1, up to 20% on certain domains

## Executive Summary
This paper introduces SaM (Selecting and Merging), a framework for improving Named Entity Recognition with large language models by dynamically selecting and merging domain-specific expert models rather than training a single unified model. SaM first trains multiple expert models on different source domains using LoRA adapters, then selects relevant experts for a target domain based on domain similarity and sampled-instance evaluation. These selected experts are merged using parameter-efficient fine-tuning (specifically Ties-Merging) to create task-specific models. Experiments demonstrate consistent improvements over unified models across multiple benchmarks, with gains up to 20% on certain domains. The framework offers adaptability and scalability, allowing experts to be conveniently added or removed without retraining from scratch.

## Method Summary
SaM trains domain-specific expert models on curated NER datasets using LoRA adapters, then dynamically selects and merges relevant experts at inference time for target domains. The framework operates in two stages: (1) training 6 domain-specific LoRA adapters on 17 NER datasets across domains like Biomedical, Legal, News, Social Media, STEM, and Traffic; (2) at inference, selecting experts through two complementary strategies - domain similarity (using all-MiniLM-L6-v2 embeddings) and sampling evaluation (using pseudo-labels from ensemble predictions on 10 unlabeled instances). The selected experts are merged using Ties-Merging to address parameter interference, and the final output combines predictions from both merged models via union to favor recall. The entire process requires no labeled target domain data, operating in zero-shot settings.

## Key Results
- SaM outperforms a fully-trained unified model by an average of 10% F1 across multiple benchmarks
- Performance improvements reach up to 20% F1 on certain domains
- Optimal number of merged experts varies by domain (typically 2-4), with k=3 used as default
- The framework maintains adaptability - experts can be added or removed without retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain similarity correlates with expert model transferability for target domains.
- Mechanism: Text encoder produces embeddings for source and target domain data. Domain centroids are computed via averaging. Cosine similarity ranks experts; top-m are selected. The assumption is that embedding space proximity reflects data distribution similarity, which should transfer task capabilities.
- Core assumption: Embedding similarity captures task-relevant distributional properties; similar domains share entity structures and linguistic patterns.
- Evidence anchors:
  - [abstract]: "select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain"
  - [section 3.2]: "The domain embedding inherently captures the data distribution in the embedding space. Thus, in theory, the selected expert models exhibit a certain degree of similarity"
  - [corpus]: Weak direct evidence. Related work on domain adaptation for NER (MME-RAG, FiNERweb) focuses on retrieval-augmented or multilingual settings, not embedding-based domain matching.
- Break condition: When target domain has unique entity types or annotation schemas not represented in source domains; when embedding model fails to capture task-relevant features.

### Mechanism 2
- Claim: Performance on sampled instances predicts expert value for merging, even with pseudo-labels.
- Mechanism: Sample k instances from target domain. Each expert generates predictions. Ensemble via majority voting produces pseudo-labels. Expert performance against pseudo-labels ranks experts. This provides empirical, fine-grained assessment complementing domain similarity.
- Core assumption: Pseudo-labels from ensemble are sufficiently accurate for relative ranking; small sample size (k=10) captures enough domain characteristics.
- Evidence anchors:
  - [abstract]: "(ii) performance on sampled instances, respectively"
  - [section 3.2]: "sampling evaluation delivers a fine-grained, empirical quantification of expert performance"
  - [corpus]: No direct corpus support for pseudo-label-based expert selection specifically.
- Break condition: When all experts are equally poor (pseudo-labels become unreliable); when target domain requires knowledge none of the experts possess.

### Mechanism 3
- Claim: Ties-Merging reduces parameter interference when fusing multiple experts.
- Mechanism: Compute task vectors as δ_sft = θ_sft - θ_base. Ties-Merging addresses parameter redundancy and sign inconsistency across task vectors before averaging. Merged model retains capabilities from multiple domains without the gradient conflicts that arise in joint training.
- Core assumption: Task vectors encode relatively independent capabilities; parameter interference is the primary barrier to multi-domain fusion.
- Evidence anchors:
  - [section 3.2]: "We employ the Ties-Merging (Yadav et al., 2023) method, which addresses parameter redundancy and sign inconsistency to mitigate inter-model interference"
  - [section 4.10]: "Both dare and ties address the issue of parameter redundancy for merging...handling parameter redundancy is crucial for NER"
  - [corpus: Ties-Merging is established in Yadav et al. 2023; related model merging work (ESNERA) addresses dataset merging but not parameter-level fusion directly.]
- Break condition: When experts have fundamentally conflicting learned representations (e.g., different entity type schemas); when merging too many experts (k>4) dilutes individual strengths.

### Mechanism 4
- Claim: Union of predictions from two complementary selection strategies outperforms intersection.
- Mechanism: Domain similarity captures theoretical priors (coarse-grained); sampling evaluation captures empirical performance (fine-grained). Each produces a merged task model. Final output takes union of predictions, as the models capture different but complementary perspectives.
- Core assumption: False positives are less costly than false negatives in NER context; the two strategies have non-overlapping failure modes.
- Evidence anchors:
  - [section 3.3]: "Taking the intersection of two sets of predictions typically enhances reliability. However...we adopt their union as the final result."
  - [corpus: No direct corpus evidence for union vs. intersection in multi-model NER fusion.]
- Break condition: When both models produce high false positive rates on same entity types; when precision is more critical than recall for downstream task.

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: All expert training uses LoRA to make storage tractable (n experts stored as small adapters vs. n full models). Understanding rank selection and which layers to adapt is critical for reproduction.
  - Quick check question: If LoRA rank is too low, what capability degradation would you expect for fine-grained entity recognition?

- Concept: **Task Vectors / Model Arithmetic**
  - Why needed here: The entire merging mechanism depends on computing δ = θ_sft - θ_base and applying operations on these vectors. Without this conceptual foundation, the merging step is opaque.
  - Quick check question: Why might simple averaging of task vectors fail compared to Ties-Merging for models trained on heterogeneous domains?

- Concept: **Zero-Shot Cross-Domain NER Evaluation**
  - Why needed here: All experiments are zero-shot (no target domain labels). Understanding the distinction between zero-shot, few-shot, and in-domain evaluation clarifies why domain adaptation via merging is necessary.
  - Quick check question: Why would a unified model trained on all domains still underperform on a specific target domain in zero-shot settings?

## Architecture Onboarding

- Component map:
  1. Expert Training Pipeline -> Domain Similarity Selector -> Sampling Evaluator -> Merger -> Inference Engine
  2. 6 domain-specific LoRA adapters trained on curated NER datasets (10K-50K samples per domain)
  3. Text encoder (all-MiniLM-L6-v2) produces domain embeddings for similarity ranking
  4. k samples from target domain generate pseudo-labels for performance evaluation
  5. Ties-Merging on selected task vectors creates task-specific merged models
  6. Union of predictions from two merged models forms final output

- Critical path: Target domain arrives → (parallel: similarity computation + sampling inference) → two expert subsets selected → two merged models created → inference with both → union outputs. Total latency dominated by 2× inference passes.

- Design tradeoffs:
  - k (experts merged): Higher k adds diversity but increases interference. Paper finds k=2-4 optimal; fixed at k=3.
  - k samples: Higher k improves selection reliability but requires more unlabeled target data. Paper uses k=10.
  - Union vs. intersection: Union favors recall; intersection favors precision. Paper chooses union for NER.
  - Two-model vs. single-model inference: SaMeco (Section 4.4) reduces cost to 1× inference with ~1 point F1 drop.

- Failure signatures:
  - Performance drops when target domain entity types have no overlap with any source domain.
  - Excessive merging (k=6) degrades performance below best single expert.
  - Linear merging without redundancy handling causes structured output generation to fail entirely.
  - Very small sample pools (1 instance) destabilize domain similarity selection more than sampling evaluation.

- First 3 experiments:
  1. Reproduce expert training: Train a single domain expert (e.g., News) on the curated data. Verify LoRA adapter size (~18HrL parameters) and baseline F1 on in-domain test set matches paper's expert performance (~85 F1 for News).
  2. Validate selection strategies independently: For a held-out domain (e.g., AI), run domain similarity selection alone and sampling evaluation alone. Confirm each produces non-trivial improvements over random expert selection.
  3. Ablate merging technique: Compare Ties-Merging vs. simple averaging vs. dare on a fixed expert subset. Verify that averaging causes structured output failures while Ties-Merging preserves JSON format generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal number of experts (k) for merging be dynamically determined for each target domain, rather than using a fixed value?
- Basis in paper: [explicit] Section 4.5 shows "the optimal k varies across target domains, typically ranging from 2 to 4," and Section 5 explicitly states "dynamically determining the number of merged models, k" is a promising direction for improvement.
- Why unresolved: The paper fixes k=3 based on average performance across domains, but this is suboptimal for individual domains. Appendix B notes that when target and source domains coincide, k=1 (single expert) yields optimal results.
- What evidence would resolve it: A systematic method (e.g., validation-based selection or uncertainty estimation) that adaptively selects k per target domain and demonstrates consistent improvements over fixed-k baselines.

### Open Question 2
- Question: Can incorporating entity-type similarity alongside domain similarity improve expert selection, particularly for domains sharing similar entity schemas despite differing textual distributions?
- Basis in paper: [explicit] Section 5 explicitly proposes "alongside domain-level similarity, we could also leverage entity-type similarity when selecting source models" as a potential improvement.
- Why unresolved: Current selection relies on domain embeddings (textual similarity) and sampling evaluation, but ignores the semantic relatedness of entity types across domains, which could identify beneficial but textually dissimilar experts.
- What evidence would resolve it: Experiments comparing domain-only selection vs. hybrid selection incorporating entity-type overlap or ontology-based similarity metrics, showing gains on domains with mismatched text distributions but shared entity types.

### Open Question 3
- Question: How can the SaM framework be effectively extended to unified information extraction covering NER, relation extraction, and event extraction simultaneously?
- Basis in paper: [explicit] Section 5 states: "Our framework can be naturally extended to a broader IE setting by incorporating additional IE data to train IE experts" as explicit future work.
- Why unresolved: The current framework only addresses NER; cross-task interference during merging and different output formats for RE/EE introduce complexity not explored in this work.
- What evidence would resolve it: Implementation of SaM with task-specific IE experts (NER, RE, EE) demonstrating performance improvements over unified multi-task baselines on standard IE benchmarks.

## Limitations

- Missing implementation details (prompt templates, Ties-Merging hyperparameters) prevent direct replication and create uncertainty about exact reproducibility
- Performance gains demonstrated only in zero-shot settings; framework behavior in few-shot scenarios remains unexplored
- Claim that union of predictions is superior to intersection is asserted without empirical validation of precision-recall tradeoffs
- No ablation studies on domain similarity encoder choice to verify optimality of MiniLM for this task

## Confidence

- **High Confidence:** The core claim that dynamic expert selection and merging outperforms a single unified model is well-supported by experimental results showing consistent 10% average F1 improvements
- **Medium Confidence:** The specific mechanisms (domain similarity ranking, sampling evaluation, Ties-Merging) are described clearly and logically sound, but missing implementation details create uncertainty about exact reproducibility
- **Low Confidence:** The claim that the two selection strategies are "complementary" and that union is optimal is asserted without comparative analysis of false positive/negative distributions

## Next Checks

1. **Prompt Template Reconstruction:** Reconstruct the training prompt template from available examples and descriptions. Test whether minor variations in prompt structure significantly affect expert model performance on held-out domains.

2. **Hyperparameter Sensitivity Analysis:** Systematically vary Ties-Merging hyperparameters (scaling factors, merging coefficients) and measure impact on final F1 scores. Identify whether performance is robust to these choices or highly sensitive.

3. **Alternative Domain Encoder Evaluation:** Replace the MiniLM domain encoder with other sentence embedding models (e.g., Sentence-BERT variants) and measure changes in domain similarity selection accuracy and downstream NER performance.