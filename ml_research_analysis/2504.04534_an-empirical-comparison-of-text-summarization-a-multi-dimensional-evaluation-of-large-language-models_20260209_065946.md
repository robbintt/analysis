---
ver: rpa2
title: 'An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation
  of Large Language Models'
arxiv_id: '2504.04534'
source_url: https://arxiv.org/abs/2504.04534
tags:
- quality
- evaluation
- factual
- metrics
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comprehensive evaluation of 17 large language
  models across seven diverse summarization datasets using a multi-dimensional framework.
  The research examines factual consistency, semantic similarity, lexical overlap,
  and human-like quality at three summary lengths (50, 100, 150 tokens), while incorporating
  efficiency metrics like processing time and cost.
---

# An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2504.04534
- Source URL: https://arxiv.org/abs/2504.04534
- Reference count: 27
- Primary result: Multi-dimensional evaluation of 17 LLMs across 7 datasets reveals critical trade-offs between factual consistency and perceived quality at different summary lengths

## Executive Summary
This study presents a comprehensive evaluation framework for text summarization systems, comparing 17 large language models across seven diverse datasets using four metric families: factual consistency (SummaC), semantic similarity (BERTScore), lexical overlap (ROUGE), and human-like quality (LLM-as-judge). The research examines performance at three summary lengths (50, 100, 150 tokens) while incorporating efficiency metrics including processing time and cost. Key findings reveal that shorter summaries achieve higher factual consistency but lower perceived quality, with domain-specific variations showing technical datasets struggling more than conversational content. The study establishes a replicable methodology for evaluating text summarization systems and provides evidence-based recommendations for different use cases based on specific requirements for accuracy, quality, or efficiency.

## Method Summary
The study evaluates 17 LLMs using 30 random documents per dataset (CNN/DailyMail, XSum, BigPatent, BillSum, PubMed, SAMSum, WikiHow), generating summaries at 50/100/150 tokens with temperature=0.1. Evaluation employs four metric families: ROUGE (lexical overlap), BERTScore (semantic similarity), SummaC (factual consistency via NLI), and LLM-as-judge (human-like quality). Scores are normalized and weighted (Quality: 30%, Factual: 25%, Human-like: 20%, Efficiency: 15%, Cost: 10%) with open-source models running on NVIDIA A100 and API models via respective clients. The framework balances comprehensive assessment with practical constraints, using subset evaluations for LLM-as-judge to manage costs.

## Key Results
- DeepSeek-v3 achieves exceptional factual consistency (SummaC score of 0.6823) but lower efficiency
- Claude-3-5-Sonnet excels in human-like quality (4.75/5.0) across all summary lengths
- Gemini-1.5-Flash demonstrates superior efficiency (1.08 seconds processing time, $0.000124 per summary)
- Technical datasets (BigPatent, PubMed) show notably lower factual consistency than conversational content
- Critical trade-off exists between factual consistency (best at 50 tokens) and perceived quality (best at 150 tokens)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shorter summaries exhibit higher factual consistency with source documents than longer summaries.
- Mechanism: Reduced output token length constrains the model's opportunity to generate content that diverges from or contradicts the source material. With fewer tokens, the model must prioritize high-confidence information directly supported by the input.
- Core assumption: Factual errors accumulate probabilistically as generation length increases; models are more likely to introduce unsupported claims when given more generative "space."
- Evidence anchors:
  - [abstract]: "critical tension between factual consistency (best at 50 tokens) and perceived quality (best at 150 tokens)"
  - [section]: SummaC Conv scores: 0.486 at 50 tokens vs. 0.281 at 150 tokens (Table 8)
  - [corpus]: Agent-as-Judge paper notes "traditional metrics...do not adequately capture factual accuracy, particularly for long narratives (>100K tokens)"—consistent with length-faithfulness relationship
- Break condition: If factual consistency is measured via reference-based metrics rather than source-document NLI, this inverse relationship may not hold.

### Mechanism 2
- Claim: Technical domains (patents, biomedical literature) produce lower factual consistency scores than conversational content.
- Mechanism: Specialized terminology, dense information packing, and domain-specific reasoning requirements increase the cognitive load on the model, raising the probability of misrepresentation or hallucination when condensing complex material.
- Core assumption: General-purpose LLMs have weaker grounding in technical vocabularies; conversational data aligns more closely with training distributions.
- Evidence anchors:
  - [abstract]: "Technical datasets like BigPatent and PubMed show notably lower factual consistency compared to conversational content"
  - [section]: BigPatent SummaC = 0.093, PubMed = 0.102, vs. SAMSum = 0.531 (Table 9)
  - [corpus]: PlainQAFact paper addresses this directly—entailment/QA-based methods "struggle with plain language summarization" in biomedical contexts
- Break condition: Domain-adapted fine-tuning or retrieval-augmented generation may close this gap; the mechanism reflects current general-purpose model limitations.

### Mechanism 3
- Claim: Models optimized for factual consistency trade off processing efficiency.
- Mechanism: Higher factual reliability may require additional compute—either through model scale, internal verification steps, or more conservative decoding strategies—that increases latency.
- Core assumption: Factual consistency is not "free"; achieving it requires architectural or procedural investment.
- Evidence anchors:
  - [abstract]: "deepseek-v3 achieving exceptional factual consistency...gemini-1.5-flash demonstrating superior efficiency"
  - [section]: deepseek-v3: SummaC 0.6823, Time 21.03s (rank 13); gemini-1.5-flash: SummaC 0.289, Time 1.08s (rank 1)
  - [corpus]: Weak/missing—corpus neighbors do not directly address efficiency-factual consistency trade-offs
- Break condition: Future architectures may decouple these dimensions through more efficient verification mechanisms.

## Foundational Learning

- **Concept: ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**
  - Why needed here: Baseline lexical overlap metric; the paper uses ROUGE-1/2/L as 15% of quality weighting. Understanding its limitations (lexical focus, no semantic capture) explains why additional metrics are necessary.
  - Quick check question: Why would a summary with perfect meaning but different vocabulary score poorly on ROUGE?

- **Concept: SummaC (Summarization Consistency)**
  - Why needed here: Core factual consistency metric (35% weight). Uses NLI models to detect contradictions between source and summary—reference-free, captures hallucinations ROUGE/BERTScore miss.
  - Quick check question: How does SummaC differ from ROUGE in what it evaluates?

- **Concept: Abstractive vs. Extractive Summarization**
  - Why needed here: The paper evaluates abstractive LLM outputs; understanding that these generate novel text (vs. selecting source sentences) clarifies why factual consistency is a non-trivial challenge.
  - Quick check question: Which approach has lower risk of grammatical incoherence but higher risk of hallucination?

## Architecture Onboarding

- **Component map:** Input layer (7 datasets, 30 samples each) -> Model layer (17 LLMs via API or local inference) -> Evaluation layer (4 metric families) -> Aggregation layer (weighted scoring)

- **Critical path:** 1. Generate summaries at 50/100/150 tokens (temperature=0.1) 2. Run automated metrics (ROUGE, BERTScore, SummaC) on all samples 3. Run LLM-as-judge on 10-sample subset (different evaluator model) 4. Normalize and weight scores; produce ranked output

- **Design tradeoffs:** Sample size (30) vs. statistical robustness—authors acknowledge modest scale limitation; LLM-as-judge subset (10) vs. full evaluation—cost/quality proxy trade-off; Weighting scheme subjectivity—35/25/15/25 reflects prioritization but is not objectively "correct"

- **Failure signatures:** High ROUGE + low SummaC → summary is lexically similar but factually divergent (hallucination risk); Big performance drop on BigPatent/PubMed → general-purpose model struggling with technical domain; Large variance across token lengths → model sensitivity to output constraints

- **First 3 experiments:** 1. Replicate single-dataset evaluation (e.g., SAMSum) with 3 models to validate pipeline before full 17-model run 2. Test prompt sensitivity: minimal vs. instruction-rich prompts to isolate prompt engineering effects 3. Pilot LLM-as-judge calibration: compare GPT-3.5-Turbo vs. Claude-3.5-Haiku as evaluators on same summaries to assess evaluator bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How strongly do the LLM-based evaluation scores correlate with human expert judgments across the different technical and conversational domains?
- Basis in paper: [explicit] The authors state in Section 6 that human-like assessments "serve as proxies and might not precisely mirror actual human judgments."
- Why unresolved: The study relies on automated and LLM-based proxies for the primary evaluation without validating these scores against ground-truth human assessments.
- What evidence would resolve it: A correlation study comparing the LLM-generated ratings against blind human evaluations for the same summary subsets.

### Open Question 2
- Question: How sensitive are the final model rankings to the specific weighting scheme (e.g., 35% factual consistency vs. 25% human-like quality)?
- Basis in paper: [explicit] Section 6 notes that the adopted weighting scheme "offers a balanced but potentially subjective representation" of priorities.
- Why unresolved: It is unclear if the top-performing models retain their rankings if the weights are shifted to prioritize different metrics like efficiency or semantic similarity.
- What evidence would resolve it: A sensitivity analysis re-ranking all models across a spectrum of weight configurations to test ranking stability.

### Open Question 3
- Question: Does the truncation of source documents to 4,096 tokens significantly degrade the factual consistency scores for technical datasets like BigPatent and PubMed?
- Basis in paper: [inferred] The methodology truncates inputs to 4,096 tokens, and results show these domains had the lowest factual consistency; however, the specific impact of information loss was not isolated.
- Why unresolved: Poor performance may stem from lost context in truncated sections rather than the models' inherent inability to handle technical terminology.
- What evidence would resolve it: An ablation study comparing performance on truncated versus full-context inputs using models with larger context windows.

## Limitations
- Sample size of 30 documents per dataset may limit statistical power for detecting subtle performance differences
- Weighting scheme for quality metrics (35% factual consistency, 25% semantic similarity, 15% lexical overlap, 25% human-like quality) reflects subjective prioritization
- LLM-as-judge evaluation relies on a different model than the summarization model, potentially introducing evaluator bias
- Temperature=0.1 setting for deterministic generation may not reflect real-world usage patterns

## Confidence

- **High confidence**: Claims about efficiency differences between models (e.g., Gemini-1.5-Flash vs DeepSeek-v3 processing times) and relative ranking of models on individual metrics
- **Medium confidence**: Claims about trade-offs between factual consistency and perceived quality across summary lengths; claims about domain-specific performance variations
- **Low confidence**: The optimal weighting scheme for aggregating different quality dimensions is highly subjective and not empirically validated

## Next Checks

1. **Statistical significance testing**: Perform paired t-tests or bootstrap confidence intervals on model comparisons across the 30 samples to quantify whether observed differences are statistically significant or within sampling variation.

2. **Evaluator consistency validation**: Run duplicate LLM-as-judge evaluations on the same 10-sample subsets to measure inter-evaluator variance and assess whether the 0.1-0.2 score variance mentioned is typical or whether some model pairs show higher disagreement.

3. **Domain generalization test**: Select additional technical and conversational datasets beyond the current seven to test whether the observed domain-specific performance patterns (technical domains showing lower factual consistency) hold across different content types.