---
ver: rpa2
title: 'Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic
  and Transcriptomic Data'
arxiv_id: '2506.05542'
source_url: https://arxiv.org/abs/2506.05542
tags:
- datasets
- data
- agentomics-ml
- code
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Agentomics-ML addresses the challenge of automating machine learning\
  \ experimentation for heterogeneous -omics datasets, where traditional AutoML and\
  \ zero-shot LLM approaches struggle with generalization and success rates. The system\
  \ employs a fully autonomous agentic framework that iteratively executes a predefined\
  \ ML experimentation pipeline\u2014encompassing data exploration, representation\
  \ selection, model architecture design, training, and inference script generation\u2014\
  through bash-based file system interactions."
---

# Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data

## Quick Facts
- arXiv ID: 2506.05542
- Source URL: https://arxiv.org/abs/2506.05542
- Reference count: 16
- Agentomics-ML achieves 93.33% success rate in generating executable ML code and trained models on benchmark genomic datasets, significantly outperforming zero-shot LLMs and baseline agentic systems.

## Executive Summary
Agentomics-ML addresses the challenge of automating machine learning experimentation for heterogeneous -omics datasets, where traditional AutoML and zero-shot LLM approaches struggle with generalization and success rates. The system employs a fully autonomous agentic framework that iteratively executes a predefined ML experimentation pipeline—encompassing data exploration, representation selection, model architecture design, training, and inference script generation—through bash-based file system interactions. A key innovation is the reflection step, which leverages training and validation metrics to generate verbal feedback guiding subsequent iterations, enabling error correction and progressive model refinement. Evaluated on six genomic and transcriptomic benchmark datasets, Agentomics-ML achieves a 93.33% success rate in producing working ML code, significantly outperforming state-of-the-art agentic systems and zero-shot LLMs (which often failed entirely on complex datasets like AGO2_CLASH_Hejret). The method attains state-of-the-art automated performance on one dataset and narrows the gap with expert-built models on others, demonstrating robust generalization even for atypical sequence-interaction tasks. Agentomics-ML thus provides a reliable, reproducible, and autonomous solution for advancing data-driven discovery in molecular biology.

## Method Summary
Agentomics-ML is an autonomous agentic framework for ML experimentation on genomic and transcriptomic data. It follows a predefined, sequential workflow (data exploration → representation/architecture → script generation → training → reflection) using bash-based file system interactions. Each step must complete and pass validation before proceeding to the next. The system uses Pydantic schemas to validate outputs programmatically and executes inference scripts with dummy data to verify correctness. After training, validation metrics are fed to a reflection LLM that generates verbal feedback for subsequent iterations. The agent iteratively refines models across multiple runs, selecting the best validation performance. The framework was evaluated on six benchmark datasets using gpt-4.1, achieving a 93.33% success rate in generating executable ML code and trained models.

## Key Results
- 93.33% success rate in generating executable ML code and trained models on benchmark genomic datasets
- Achieved state-of-the-art automated performance on one dataset and narrowed performance gap with expert-built models on others
- Zero-shot LLMs and baseline agentic systems often failed entirely on complex datasets like AGO2_CLASH_Hejret, while Agentomics-ML succeeded
- Iterative feedback mechanism provided 3.8% average performance improvement over no-feedback models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A predefined, sequential workflow constrains agent behavior sufficiently to prevent planning failures while preserving enough autonomy to adapt to dataset specifics.
- **Mechanism:** Rather than allowing unconstrained exploration, the system enforces a fixed stage-gate process (data exploration → representation/architecture → script generation → training → reflection). Each stage must complete before the next begins. This reduces the search space of possible agent trajectories, limiting the combinatorial explosion of decision paths that causes other agentic systems to fail on novel datasets.
- **Core assumption:** The ML experimentation lifecycle is decomposable into stages with stable interfaces between them, and errors within a stage are recoverable through retry.
- **Evidence anchors:**
  - [abstract] "Our method follows predefined steps of an ML experimentation process, repeatedly interacting with the file system through Bash to complete individual steps."
  - [section 3.1] "We opted for a step-based approach instead of elaborate prompt engineering to avoid unnecessary agent planning as we consider these high-level steps essential in the machine learning experimentation process."
  - [corpus] Related work (Operand Quant, GenoMAS) similarly adopts structured stage decomposition, suggesting convergence toward this pattern for autonomous ML engineering.
- **Break condition:** If a dataset requires non-sequential processing (e.g., iterative feature engineering interleaved with model selection), the rigid stage ordering may prevent optimal solutions.

### Mechanism 2
- **Claim:** Programmatic output validation with forced retry creates a reliable execution guarantee that accumulates across pipeline stages.
- **Mechanism:** Each file output is validated against a schema using Pydantic. Inference scripts are executed with dummy data to verify argument handling and output format. The agent cannot proceed until validation passes. This creates a "type-safe" pipeline where each stage's output is guaranteed to meet the next stage's input requirements.
- **Core assumption:** Validation schemas can be defined a priori for all critical output types, and validation failures are fixable through agent retry rather than requiring human intervention.
- **Evidence anchors:**
  - [abstract] "Agentomics-ML achieved a 93.33% success rate in generating executable ML code and trained models."
  - [section 3.2] "The agent is triggered to retry and is not able to advance to the next step unless the output passes this programmatic validation."
  - [corpus] Weak direct evidence—corpus papers do not discuss Pydantic-style validation explicitly, though ML-Agent mentions "reinforcing" agents through experimental feedback.
- **Break condition:** If validation schemas are underspecified (e.g., logical errors that pass schema checks but produce semantically wrong predictions), bugs can propagate silently.

### Mechanism 3
- **Claim:** A reflection step that converts scalar metrics into verbal feedback enables iterative improvement by surfacing issues invisible to single-shot generation.
- **Mechanism:** After training completes, validation metrics (accuracy, loss curves, etc.) are fed to a reflection LLM call that generates natural language feedback. This feedback is injected into subsequent iterations, allowing the agent to adjust data representation, architecture, or hyperparameters. The system selects the best model across all iterations based on validation performance.
- **Core assumption:** The LLM can reliably diagnose ML pathologies (overfitting, underfitting, class imbalance) from scalar metrics alone, and suggested adjustments will transfer to improved test performance.
- **Evidence anchors:**
  - [abstract] "training and validation metrics provide scalar feedback to a reflection step to identify issues such as overfitting"
  - [section 4.2] "We find that models produced after iterative feedback obtain better test set metrics 80% of the time and outperform no-feedback models relatively on-average by 3.8%."
  - [corpus] ML-Agent and GenoMAS both incorporate feedback loops, with ML-Agent explicitly framing this as "reinforcing" based on experimental outcomes.
- **Break condition:** If feedback suggestions are contradictory across iterations or if validation metrics are misleading (e.g., data leakage), reflection can amplify rather than correct errors.

## Foundational Learning

- **Concept: Stage-gated ML pipelines**
  - **Why needed here:** Agentomics-ML's reliability stems from treating ML development as a sequence of validated stages. Understanding how AutoML systems (Auto-Sklearn, etc.) structure their search helps contextualize why agentic systems need similar constraints.
  - **Quick check question:** Can you name three stages in a typical ML pipeline where invalid output would cascade to downstream failures?

- **Concept: LLM tool use and environment interaction**
  - **Why needed here:** The agent operates through Bash commands and Python execution. Understanding the affordances and limitations of tool-calling (statelessness, error feedback loops) is prerequisite to diagnosing agent failures.
  - **Quick check question:** What information must an LLM receive after a tool call to decide its next action?

- **Concept: Genomic data pathologies**
  - **Why needed here:** The paper emphasizes high dimensionality, batch effects, class imbalance, and variable-length sequences as failure modes for generic systems. Domain awareness is needed to interpret why AGO2_CLASH_Hejret (two interacting variable-length sequences) broke all baseline methods.
  - **Quick check question:** Why would random train/validation splits overestimate generalization on genomic data with batch effects?

## Architecture Onboarding

- **Component map:** User Input (data file + description) → Docker Container (isolated execution environment) → Orchestrator (stage sequencer) → Agent Core (LLM with tools: Bash, Python write, Python execute) → Validation Layer (Pydantic schemas + dummy data execution) → Reflection Module (metrics → verbal feedback) → Model Selection (best validation score across iterations) → Output: training script + inference script + trained weights

- **Critical path:**
  1. Data exploration must successfully load and describe the dataset (breaks if file format is unsupported).
  2. Representation/architecture decisions must produce valid Python code.
  3. Training must complete without runtime errors.
  4. Validation metrics must be parseable for reflection.
  Any failure in this chain triggers stage retry.

- **Design tradeoffs:**
  - **Structured vs. autonomous:** Predefined stages sacrifice flexibility for reliability. Complex datasets requiring non-standard pipelines may underperform.
  - **Validation overhead:** Pydantic checks and dummy execution add latency but prevent downstream failures.
  - **Iteration budget:** More iterations improve results (3.8% average gain) but increase cost (~$2 per run noted in paper). Assumption: diminishing returns after ~3–5 iterations on most datasets.

- **Failure signatures:**
  - Stage retry loops exceeding threshold → likely schema mismatch or fundamental data incompatibility.
  - Training completes but validation accuracy ≈ random → agent failed to learn meaningful representation; check data loading and preprocessing.
  - Large train/validation gap flagged in reflection but no improvement → reflection suggestions not actionable; may need human review.

- **First 3 experiments:**
  1. **Baseline replication:** Run Agentomics-ML on `human_nontata_promoters` (the paper's prototype dataset) with 1 iteration, no reflection. Verify 100% success rate and ~0.89 accuracy.
  2. **Ablation study:** Run same dataset with reflection enabled across 3 iterations. Measure improvement delta. Confirm reflection mechanism contributes observable gains.
  3. **Stress test:** Apply to AGO2_CLASH_Hejret with 5 iterations. Monitor whether agent discovers interaction modeling between the two sequences. Compare final architecture to human SOTA approach.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can extending the number of reflection iterations or modifying the reflection mechanism further improve Agentomics-ML's performance?
- **Basis in paper:** [explicit] "It is possible that extending the number of iterations beyond what was tested, or adjusting the reflection mechanism, could push the benefits of the feedback loop even further."
- **Why unresolved:** The current experiments used a fixed iteration count; the optimal number and design of reflection steps was not systematically explored.
- **What evidence would resolve it:** Ablation studies varying iteration counts and comparing alternative reflection strategies across multiple datasets.

### Open Question 2
- **Question:** How well does Agentomics-ML generalize to biological tasks beyond nucleotide sequence classification?
- **Basis in paper:** [explicit] "Future work will focus on extending Agentomics-ML's applicability to a broader array of biological data modalities and computational tasks" and "A future direction for this field is the exploration of different types of datasets and tasks."
- **Why unresolved:** Current evaluation is limited to genomics/transcriptomics sequence classification; other modalities (proteomics, metabolomics) and tasks (structure prediction, clustering) remain untested.
- **What evidence would resolve it:** Benchmarking Agentomics-ML on diverse omics modalities and task types (regression, clustering, multi-omics integration).

### Open Question 3
- **Question:** Can Agentomics-ML maintain its performance advantage when evaluated on truly novel, never-before-published datasets?
- **Basis in paper:** [explicit] "It should be an important future task for an independent researcher to perform a benchmarking exercise in this field, ideally with never before published datasets."
- **Why unresolved:** Potential pre-training data leakage with closed-source LLMs cannot be ruled out; current benchmarks may have been exposed to LLM training corpora.
- **What evidence would resolve it:** Independent evaluation using newly collected datasets published after LLM training cutoff dates, with open-source model comparisons.

### Open Question 4
- **Question:** What modifications would enable Agentomics-ML to match or exceed human expert-built models on datasets where it currently underperforms?
- **Basis in paper:** [inferred] While Agentomics-ML achieves SOTA on one dataset, "state-of-the-art models built by domain experts still lead in absolute performance on the majority of the computational biology datasets used in this work."
- **Why unresolved:** The paper does not analyze what specific domain knowledge or techniques human experts apply that the agent lacks.
- **What evidence would resolve it:** Comparative analysis of human vs. agent design choices, followed by targeted improvements to the agent's domain knowledge integration.

## Limitations

- Controlled evaluation setting limits external validity; performance metrics reported only on curated benchmark datasets with pre-defined splits
- 93.33% success rate achieved in Docker sandbox with programmatic validation; real-world noisy datasets may encounter unencountered failure modes
- Reflection mechanism relies on scalar metrics alone, which may miss nuanced model pathologies (e.g., adversarial vulnerabilities or calibration issues)

## Confidence

- **High Confidence:** Success rate superiority over zero-shot LLMs and baseline agents on benchmark tasks. The stage-gated architecture and validation layer demonstrably reduce execution failures.
- **Medium Confidence:** State-of-the-art performance on one dataset and performance gap reduction on others. These claims depend on the specific dataset splits and preprocessing not fully detailed in the paper.
- **Low Confidence:** Generalization to arbitrary genomic/transcriptomic tasks beyond the six benchmarks. The reflection mechanism's ability to reliably diagnose and correct complex ML pathologies is inferred but not experimentally validated across diverse failure modes.

## Next Checks

1. **Cross-Dataset Robustness Test:** Apply Agentomics-ML to a held-out genomic dataset (e.g., GTEx RNA-seq) not seen during method development. Evaluate success rate and model performance relative to expert-tuned baselines.
2. **Reflection Mechanism Ablation:** Systematically disable the reflection step on AGO2_CLASH_Hejret and measure performance degradation. Confirm that iterative feedback contributes the claimed 3.8% average improvement.
3. **Schema Robustness Audit:** Intentionally corrupt Pydantic schemas (e.g., relax type constraints) and rerun on a simple dataset. Assess whether validation failures correctly prevent downstream model corruption.