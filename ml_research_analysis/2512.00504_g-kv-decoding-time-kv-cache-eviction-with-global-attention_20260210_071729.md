---
ver: rpa2
title: 'G-KV: Decoding-Time KV Cache Eviction with Global Attention'
arxiv_id: '2512.00504'
source_url: https://arxiv.org/abs/2512.00504
tags:
- score
- attention
- cache
- tokens
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: G-KV improves KV cache compression for long-chain-of-thought reasoning
  models by integrating local and historical attention scores into a global scoring
  mechanism, which more accurately captures long-term token importance. A memory decay
  factor and three score aggregation forms (max, mean, sum) are explored, with max
  and sum performing best.
---

# G-KV: Decoding-Time KV Cache Eviction with Global Attention

## Quick Facts
- arXiv ID: 2512.00504
- Source URL: https://arxiv.org/abs/2512.00504
- Reference count: 40
- Primary result: 5%-20% performance gains over prior methods under low KV cache budgets (e.g., 512 tokens) for long-chain-of-thought reasoning models

## Executive Summary
G-KV addresses the KV cache bottleneck in long-chain-of-thought reasoning by introducing a global attention-based eviction strategy that combines local and historical attention scores. The method uses a memory decay factor to maintain long-term token importance, achieving 5%-20% performance gains over prior methods under tight cache budgets. Through post-training techniques like reinforcement learning with sparse attention masks and distillation, G-KV adapts models to compressed KV cache settings while maintaining inference efficiency.

## Method Summary
G-KV implements a global scoring mechanism F_t = max(α·F_{t-1}, S_t/mean(S_t)) that recursively updates token importance scores by combining current attention S_t with decayed historical scores F_{t-1}. The method integrates with existing redundancy scoring (R-KV) using λ=0.7 for Qwen models. During inference, top-(b-w) tokens are retained per head every s=128 tokens using a w=16 observation window. Post-training adaptation employs RL with sparse attention masks (GRPO-style, 400 steps) or distillation (250 steps, lr=1e-6) to align training with inference constraints.

## Key Results
- 5%-20% pass@1 improvement over MorphKV on AMC-23 and AIME-24 benchmarks at 512-token budget
- 4×-12× higher throughput versus full KV cache while maintaining performance
- Token retention becomes more evenly distributed across sequence, preserving critical prompt context
- RL-Sparse training outperforms RL-Full, confirming reduced train-inference mismatch
- Compression time <1% of decoding time, maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1: Historical Attention Accumulation (Global Scoring)
G-KV replaces single-window scoring with recursive global score F_t that aggregates current attention S_t with historical scores F_{t-1} using decay rate α and max operator. This maintains memory of tokens that were previously critical even if ignored in current step, addressing the non-monotonic importance patterns in long CoT reasoning.

### Mechanism 2: Train-Inference Distribution Alignment (RL-Sparse)
The method uses Group Relative Policy Optimization where sampling policy π'_θ enforces KV eviction constraints during training. This forces models to learn reasoning paths that succeed despite cache eviction, eliminating train-test discrepancy present in standard LLMs trained on full attention.

### Mechanism 3: Positional Distribution Equalization
Global scoring redistributes KV cache budget to cover entire sequence history rather than recent tokens. By retaining tokens with high historical scores, G-KV preserves early prompt tokens and distant reasoning steps, resulting in uniform retention distribution that anchors complex reasoning to initial problem statements.

## Foundational Learning

- **Concept: KV Cache Bottleneck**
  - Why needed: G-KV directly responds to memory/compute explosion in long-output reasoning models where KV cache scales linearly with sequence length
  - Quick check: Why does KV cache size become primary bottleneck in long-chain-of-thought generation compared to model parameters?

- **Concept: Attention as Importance Proxy**
  - Why needed: G-KV's core logic relies on using attention weights as heuristic for "information value" to decide which tokens to evict
  - Quick check: How does "Observation Window" determine importance of cached token, and what is limitation of using only this window?

- **Concept: Policy Optimization (GRPO)**
  - Why needed: To understand RL-Sparse mechanism, one must grasp how reward signal updates model to prefer generation paths that succeed despite cache eviction
  - Quick check: Why is sampling from sparse policy π'_θ critical during training phase compared to standard full-attention training?

## Architecture Onboarding

- **Component map:** Global Scorer -> Evictor -> Sparse Attention Mask -> RL Trainer
- **Critical path:** Score update loop (Eq 3-5). Incorrect max/accumulation logic or decay α causes memory of importance to vanish too fast (forgetting) or saturate (retaining garbage)
- **Design tradeoffs:** High α (0.9) favors long-term memory but risks retaining obsolete tokens; max pooling preserves peak importance while sum captures cumulative utility
- **Failure signatures:** Recency collapse (forgetting initial prompt) suggests α too low; OOM during training requires CPU offloading for sparse masks
- **First 3 experiments:**
  1. Replicate overlap analysis (Figure 1) on target model to verify attention shifts exist
  2. Grid search α (0.6 to 0.95) on AMC-23 to find optimal decay rate
  3. Compare RL-Full vs RL-Sparse to isolate performance gain from global scoring vs train-inference alignment

## Open Questions the Paper Calls Out

- **Open Question 1:** How can advanced exploration techniques be integrated into RL-Sparse framework to mitigate overly deterministic policies during large-scale training?
- **Open Question 2:** Does training with explicit sparsity mechanisms induce better abstraction or generalization compared to full attention training?
- **Open Question 3:** Does global scoring effectiveness generalize to non-reasoning tasks with different attention patterns like RAG or long-document summarization?

## Limitations

- Implementation complexity requires careful integration of global scoring, redundancy filtering, and RL/distillation fine-tuning with incomplete specification details
- Generalizability limited to mathematical reasoning benchmarks; effectiveness on code generation, conversation, or creative writing not demonstrated
- Token retention trade-offs may lead to retaining less critical tokens in scenarios where recent context is paramount

## Confidence

- **High Confidence:** Core global scoring mechanism combining local and historical attention is well-supported; performance gains on specified benchmarks clearly demonstrated
- **Medium Confidence:** Train-inference alignment effectiveness supported by ablation but long-term impact on generalization not explored
- **Low Confidence:** Claim of "fundamentally addressing limitations" is strong assertion not fully validated across diverse task types

## Next Checks

1. Conduct alpha sweep (0.6-0.95) on diverse long-sequence tasks including code generation, conversation, and creative writing to validate generalizability beyond mathematical reasoning

2. Perform detailed ablation comparing RL-Sparse, RL-Full, and untrained local scoring on held-out validation set to isolate performance gains from global scoring mechanism itself

3. Measure actual memory consumption and throughput during inference and training across KV cache budgets (512, 1024, 2048) to verify practical efficiency claims and identify hidden computational costs