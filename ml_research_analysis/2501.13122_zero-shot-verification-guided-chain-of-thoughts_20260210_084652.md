---
ver: rpa2
title: Zero-Shot Verification-guided Chain of Thoughts
arxiv_id: '2501.13122'
source_url: https://arxiv.org/abs/2501.13122
tags:
- step
- reasoning
- prompt
- zero-shot
- verifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates zero-shot LLM-based self-verification of
  self-generated reasoning steps in Chain-of-Thought prompting. It introduces a new
  zero-shot prompt (COT STEP) for decomposing reasoning steps and two zero-shot verification
  prompts (R-prompt, COTR-prompt) for LLM-based verification.
---

# Zero-Shot Verification-guided Chain of Thoughts

## Quick Facts
- arXiv ID: 2501.13122
- Source URL: https://arxiv.org/abs/2501.13122
- Authors: Jishnu Ray Chowdhury; Cornelia Caragea
- Reference count: 32
- Primary result: Zero-shot LLM-based self-verification of self-generated reasoning steps in Chain-of-Thought prompting, with step-wise greedy search showing limited benefits.

## Executive Summary
This paper investigates zero-shot LLM-based self-verification of self-generated reasoning steps in Chain-of-Thought prompting. It introduces a new zero-shot prompt (COT STEP) for decomposing reasoning steps and two zero-shot verification prompts (R-prompt, COTR-prompt) for LLM-based verification. The authors evaluate the effectiveness of these approaches on mathematical and commonsense reasoning tasks, finding that while verification can classify correctness with reasonable accuracy on mathematical domains, it fails to improve upon self-consistency baselines and shows limited generalizability to commonsense reasoning.

## Method Summary
The method involves three main components: (1) COT STEP prompting, which appends "\n\nStep 1:" to standard zero-shot COT to enable automatic step decomposition; (2) R-prompt and COTR-prompt verification, which assess reasoning step correctness using binary or CoT-style verification approaches; and (3) a unified scoring mechanism that combines generation log-probability (C1) and verification log-probability (C2) via exponentiated averaging. The framework is evaluated through correctness classification tasks and step-wise greedy search experiments, comparing against standard COT and self-consistency baselines across mathematical and commonsense reasoning datasets.

## Key Results
- COT STEP performs competitively with existing zero-shot prompts while enabling automatic step decomposition
- The COTR-prompt variant performs better than the R-prompt for verification, especially in mathematical domains
- Zero-shot verifier scores do not significantly improve self-consistency performance
- Step-wise greedy search guided by verifier scores shows some benefit over plain COT but this advantage disappears when combined with self-consistency

## Why This Works (Mechanism)

### Mechanism 1: Structured Step Decomposition via Prompt Continuation
The COT STEP prompt appends "\n\nStep 1:" to the standard zero-shot COT prompt, exploiting the model's tendency to complete numbered list patterns. This provides a strong formatting constraint that creates consistent parse anchors for downstream verification. The core assumption is that models fine-tuned on instructional data will reliably continue numbered list patterns when primed, with "Step <number>:" serving as a consistent delimiter for automatic parsing.

### Mechanism 2: Dual-Score Verification Combining Fluency and Explicit Assessment
The verification framework combines generation log-probability (C1) and verification log-probability (C2) through exponentiated averaging to produce a unified correctness score. C1 captures how "natural" the step is to the model while C2 captures explicit self-assessment. The core assumption is that both fluency and explicit verification provide orthogonal, complementary correctness signals that together create a more robust assessment than either component alone.

### Mechanism 3: Step-wise Greedy Search with Local Verification
The approach generates multiple candidate steps per position (k=5) and greedily selects the highest-scored step using verifier scores. This allows local course-correction before error propagation by providing the opportunity to choose better reasoning steps at each turn. The core assumption is that verifier scores are sufficiently correlated with step-level correctness to guide selection better than random sampling, though this advantage disappears when combined with self-consistency.

## Foundational Learning

- **Zero-shot Chain-of-Thought Prompting**: Understanding why "Let's think step by step" works is prerequisite for extending it with verification. Quick check: Given "Q: What is 15% of 80? A: Let's think step by step.", what intermediate reasoning steps should a model generate?

- **Self-Consistency with Majority Voting**: Self-consistency is the primary baseline against which verification methods are compared. Quick check: If 20 sampled reasoning chains produce answers [42, 42, 42, 35, 42, 35, 42, ...] with frequencies 15 and 5, what is the self-consistency answer?

- **Token Log-Probability Extraction from LLMs**: Both generation and verification scores require extracting log-probabilities from model forward passes. Quick check: How would you compute the average token log-probability for a generated sequence "Step 1: Add 5 and 3" given model output logits?

## Architecture Onboarding

- **Component map**: COT STEP Prompt Template -> Step Decomposer -> R-prompt/COTR-prompt Verifier -> Score Aggregator -> Search Controller
- **Critical path**: 1) Format question with COT STEP template â†’ generate chain; 2) Decompose chain into N steps via "Step N:" delimiter; 3) For each step i: compute sC1, run verification prompt, compute sC2; 4) Aggregate: sf = exp((sC1 + sC2) / 2); 5) Chain-level score = sum of step scores
- **Design tradeoffs**: R-prompt vs COTR-prompt (R-prompt is ~256x faster but COTR-prompt shows 16-26 point accuracy gains on GSM8K); COT STEP vs TAB COT (COT STEP achieves 73.54 vs 65.2 on GSM8K); SG vs SB search (SG simpler and more effective)
- **Failure signatures**: R-prompt bias (near-100% TPR with <20% TNR on commonsense tasks); verification-SC incompatibility (max/weighted/filter policies don't beat majority voting); commonsense domain failure (all verifiers perform near random on CSQA-based datasets)
- **First 3 experiments**: 1) Replicate COT STEP vs COT comparison on GSM8K with SOLAR/Phi3 to verify step decomposition quality; 2) Build correctness classification dataset from generated chains; measure R-prompt vs COTR-prompt discrimination; 3) Run step-wise greedy search ablation: plain COT STEP vs COT STEP + SG vs COT STEP + SG + SC

## Open Questions the Paper Calls Out
- Can zero-shot verification be adapted to perform reliably on commonsense reasoning tasks where implicit reasoning is standard?
- Why does the performance gain from step-wise greedy search disappear when combined with self-consistency?
- How does the assumption that a correct final answer implies a correct reasoning chain affect the measured accuracy of verifiers?

## Limitations
- Verification effectiveness drops dramatically outside mathematical domains, with near-random performance on commonsense reasoning tasks
- The verification approach shows limited generalizability and fails to improve upon self-consistency baselines despite theoretical complementarity
- Evaluation framework may be confounded by systematic biases in the R-prompt verifier, particularly its severe bias toward "correct" predictions

## Confidence
- **High Confidence**: COT STEP performs competitively with existing zero-shot prompts while enabling automatic step decomposition
- **Medium Confidence**: The COTR-prompt variant performs better than the R-prompt for verification, especially in mathematical domains
- **Low Confidence**: Zero-shot verifier scores do not significantly improve self-consistency performance

## Next Checks
1. **Bias-Aware Verification Evaluation**: Construct a balanced correctness classification dataset (equal numbers of correct/incorrect chains) from multiple domains. Evaluate both R-prompt and COTR-prompt using standard classification metrics (Accuracy, TPR, TNR, F1) to quantify and compare their verification bias.

2. **Verification-Guided Search Ablation Study**: Implement and compare three variants on GSM8K: (a) plain COT STEP, (b) COT STEP + step-wise greedy search without verification (random selection), and (c) COT STEP + step-wise greedy search with verification. This isolates whether verification scores actually guide search better than random sampling.

3. **Cross-Domain Generalization Test**: Apply the complete verification framework (COT STEP + COTR-prompt verification + scoring) to commonsense reasoning datasets (StrategyQA, CommonsenseQA). Compare performance against baselines to verify the paper's finding that verification effectiveness drops dramatically outside mathematical domains.