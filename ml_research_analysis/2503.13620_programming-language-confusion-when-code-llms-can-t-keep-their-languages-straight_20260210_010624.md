---
ver: rpa2
title: 'Programming Language Confusion: When Code LLMs Can''t Keep their Languages
  Straight'
arxiv_id: '2503.13620'
source_url: https://arxiv.org/abs/2503.13620
tags:
- language
- code
- programming
- confusion
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Programming Language Confusion (PLC), where
  large language models (LLMs) generate code in unintended programming languages despite
  explicit instructions. Through systematic evaluation of 10 popular LLMs across six
  multilingual datasets, the study finds PLC is pervasive, with specialized models
  often exhibiting higher confusion rates than general-purpose ones.
---

# Programming Language Confusion: When Code LLMs Can't Keep their Languages Straight

## Quick Facts
- **arXiv ID**: 2503.13620
- **Source URL**: https://arxiv.org/abs/2503.13620
- **Reference count**: 40
- **Primary result**: Programming Language Confusion (PLC) is pervasive in code LLMs, with specialized models often showing higher confusion rates than general-purpose ones

## Executive Summary
This paper investigates Programming Language Confusion (PLC), where large language models generate code in unintended programming languages despite explicit instructions. Through systematic evaluation of 10 popular LLMs across six multilingual datasets, the study reveals that PLC is a widespread phenomenon affecting both general-purpose and specialized code models. The research demonstrates that explicit language keywords are more effective at mitigating confusion than natural language instructions, and that model quantization significantly amplifies PLC while degrading syntactic stability in complex tasks.

## Method Summary
The study systematically evaluates 10 popular LLMs across six multilingual datasets to assess Programming Language Confusion. The evaluation framework includes single-turn generation tasks with explicit language instructions, perplexity analysis to measure language preference, and comparative analysis between quantized and non-quantized model variants. The researchers examine both general-purpose and specialized code models, testing their ability to maintain language fidelity across different programming languages and task complexities.

## Key Results
- Programming Language Confusion is pervasive across all tested LLMs, with specialized models often exhibiting higher confusion rates than general-purpose ones
- Explicit language keywords are the most effective mitigation strategy, while natural language instructions have minimal impact on reducing confusion
- Model quantization significantly amplifies PLC and degrades syntactic stability in complex tasks, despite being crucial for practical deployment

## Why This Works (Mechanism)

Code LLMs learn statistical patterns from training data rather than understanding programming languages conceptually. When generating code, these models rely on learned associations between context and language patterns, leading to systematic migrations between syntactically similar languages. The preference for Python as a default output reflects its statistical dominance in training corpora. Quantization disrupts the fine-grained probability distributions that enable precise language selection, amplifying confusion by reducing the model's ability to distinguish between similar language patterns.

## Foundational Learning

1. **Statistical Language Modeling**: LLMs predict language patterns based on statistical associations from training data rather than understanding syntax rules. Needed to explain why models default to dominant languages like Python. Quick check: Compare model perplexity across different languages in training data.

2. **Quantization Effects**: Reducing model precision during quantization disrupts fine-grained probability distributions. Needed to explain amplification of PLC in deployed models. Quick check: Measure language confusion rates between full-precision and quantized versions of the same model.

3. **Multilingual Dataset Bias**: Training data distribution heavily influences language preference in generation. Needed to understand systematic biases toward certain languages. Quick check: Analyze language distribution in training corpora versus generation patterns.

## Architecture Onboarding

Component map: Input Context -> Language Detector -> Code Generator -> Output Filter
Critical path: Context processing → Language prediction → Token generation → Syntax validation
Design tradeoffs: General-purpose models offer better language flexibility but lower precision; specialized models provide domain expertise but higher confusion rates
Failure signatures: Defaulting to Python, systematic migration between syntactically similar languages, increased confusion with quantization
First experiments: 1) Compare language confusion across different model sizes, 2) Test mitigation effectiveness with various instruction formats, 3) Measure quantization impact on language fidelity

## Open Questions the Paper Calls Out
The study's controlled benchmark datasets may not fully capture real-world code generation complexity where context, documentation, and iterative refinement play crucial roles. The evaluation focuses on single-turn generation tasks, leaving questions about PLC behavior in multi-turn interactions or when models can access external resources. The mechanism behind quantization's amplification of PLC requires further investigation, as does the performance across different model architectures and training regimes.

## Limitations
- Controlled benchmark datasets may not reflect real-world coding scenarios with complex context and iterative refinement
- Single-turn generation focus leaves open questions about PLC behavior in multi-turn interactions
- Limited testing across different model architectures and training regimes restricts understanding of architectural contributions to PLC

## Confidence

**Major Claim Clusters Confidence Labels:**
- **PLC prevalence across model types (High)**: Systematic evaluation across multiple models and datasets provides strong evidence for widespread PLC
- **Python default behavior (Medium)**: Statistically supported but could reflect dataset bias rather than inherent model preference
- **Quantization impact on PLC (High)**: Consistent degradation in language fidelity well-documented across models

## Next Checks

1. Conduct multi-turn code generation experiments to assess whether PLC accumulates or self-corrects over conversational context
2. Test PLC behavior across different model architectures (e.g., decoder-only vs encoder-decoder) to isolate architectural contributions
3. Evaluate PLC mitigation strategies (explicit keywords, syntax highlighting) across domain-specific coding tasks beyond benchmark datasets