---
ver: rpa2
title: 'Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners
  With Verifiers'
arxiv_id: '2505.04842'
source_url: https://arxiv.org/abs/2505.04842
tags:
- verifier
- verification
- compute
- arxiv
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes RL^V, a method to integrate generative verification\
  \ into \u201Cvalue-free\u201D reinforcement learning (RL) for large language models\
  \ (LLMs) by jointly training the LLM as both a reasoner and verifier using RL-generated\
  \ data. The approach augments any value-free RL method (e.g., GRPO, Leave-One-Out\
  \ PPO, VinePPO) with a generative verification objective, enabling the same model\
  \ to generate solutions and assess their correctness."
---

# Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers

## Quick Facts
- arXiv ID: 2505.04842
- Source URL: https://arxiv.org/abs/2505.04842
- Authors: Kusha Sareen; Morgane M Moss; Alessandro Sordoni; Rishabh Agarwal; Arian Hosseini
- Reference count: 38
- One-line primary result: RL^V boosts MATH accuracy by over 20% with parallel sampling and enables 8–32× more efficient test-time compute scaling

## Executive Summary
This paper proposes RL^V, a method to integrate generative verification into "value-free" reinforcement learning for LLMs by jointly training the LLM as both a reasoner and verifier. The approach augments any value-free RL method (e.g., GRPO, Leave-One-Out PPO, VinePPO) with a generative verification objective, enabling the same model to generate solutions and assess their correctness. Empirically, RL^V achieves significant improvements in MATH accuracy and enables more efficient test-time scaling compared to base RL methods.

## Method Summary
RL^V combines an RL objective (GRPO, VinePPO, or Leave-One-Out PPO) with a generative verification objective, where the model learns to predict "Yes" or "No" tokens indicating solution correctness. The unified objective J_Unified(θ) = J_RL(θ; x) + λ·J_Verify(θ; x) is trained on RL-generated data using next-token prediction for verification rather than separate classification heads. At inference, the trained verifier scores solutions to enable efficient selection via weighted voting or Best-of-N strategies.

## Key Results
- RL^V boosts MATH accuracy by over 20% with parallel sampling compared to base RL methods
- Achieves 8–32× more efficient test-time compute scaling compared to base RL with LLM-as-a-Judge
- Generalizes well to harder and out-of-domain tasks like GPQA Physics and AIME'24
- Achieves 1.2–1.6× higher performance when scaling both parallel and sequential compute with long-chain-of-thought models

## Why This Works (Mechanism)

### Mechanism 1: Generative Verification via Next-Token Prediction
Framing verification as next-token prediction preserves reasoning capacity while enabling verification. The model learns to predict "Yes" or "No" tokens conditioned on (problem, solution, verification prompt), leveraging the LLM's existing generative capabilities rather than requiring a specialized verification architecture.

### Mechanism 2: Synergistic Transfer Between Reasoning and Verification Objectives
Jointly training verification and reasoning can improve Pass@1 (reasoning-only) performance, not just verification. The verification objective may act as implicit regularization or provide additional gradient signal that helps the model better distinguish correct reasoning patterns.

### Mechanism 3: Test-Time Compute Efficiency via Learned Verification Scores
A trained verifier enables more efficient test-time scaling than LLM-as-a-Judge prompting. The unified verifier provides task-specific correctness scores that better rank solutions, enabling Best-of-N or weighted voting to select correct answers with fewer samples.

## Foundational Learning

- **Concept: Value Functions in RL**
  - Why needed here: The paper's motivation hinges on understanding that traditional PPO learns a value function V(s) estimating expected future reward, which can serve as a verifier at test time.
  - Quick check question: Can you explain why GRPO, VinePPO, and Leave-One-Out PPO are called "value-free"?

- **Concept: Policy Gradient and Advantage Estimation**
  - Why needed here: Understanding how GRPO computes advantages from group rewards (Â = (r_i - mean) / std) versus PPO's learned value network is essential for understanding what RL^V adds back.
  - Quick check question: In GRPO, how is the baseline for advantage calculation derived without a value network?

- **Concept: Test-Time Compute Scaling Strategies**
  - Why needed here: The paper evaluates Best-of-N, weighted voting, and majority voting—understanding when each is optimal is critical for deployment.
  - Quick check question: Why might weighted voting outperform Best-of-N for short CoT models but not for long CoT models?

## Architecture Onboarding

**Component Map:**
Training Pipeline: Prompt x → LLM Policy π_θ → G solutions {y_1...y_G} → RL Objective (GRPO/etc.) + Verification Objective → J_Unified = J_RL + λ·J_Verify
Inference Pipeline: Prompt x → Sample N solutions → Score each with s(x,y)=P(Yes|xyI) → Select: Best-of-N / Weighted

**Critical Path:**
1. Implement base RL method (GRPO, VinePPO, or Leave-One-Out PPO)
2. During each training batch, collect (problem, solution, reward) tuples
3. Augment batch with verification examples using prompt "Is this solution correct? Answer Yes or No."
4. Combine losses with λ (start with 1.0, tune based on Figure 7b tradeoffs)
5. At inference, compute P(Yes | x, y, I) for each candidate solution

**Design Tradeoffs:**
- λ (verification coefficient): Higher λ improves verifier accuracy but can degrade reasoner accuracy (algorithm-dependent—GRPO^V shows stark tradeoff, Leave-One-Out PPO^V more forgiving)
- Inference strategy: Weighted voting generally best for short CoT; Best-of-N competitive for long CoT
- Ver. head type: Generative (next-token) outperforms BCE/regression heads

**Failure Signatures:**
- Reasoner Pass@1 drops significantly → λ too high, reduce verification weight
- Weighted voting underperforms majority voting → verifier not trained sufficiently or OOD miscalibration
- No efficiency gains vs. LLM-as-a-Judge → verifier accuracy too low

**First 3 Experiments:**
1. Baseline comparison: Train base GRPO vs. GRPO^V on MATH, evaluate verifier accuracy and Pass@1
2. Lambda sweep: Run λ ∈ {0.1, 0.5, 1.0, 1.5} for your chosen RL method to find optimal tradeoff
3. Inference strategy ablation: Compare Best-of-N, weighted voting, and majority voting at N ∈ {8, 16, 32, 64, 128, 256}

## Open Questions the Paper Calls Out

- Can the unified generative verifier be enhanced to produce explicit Chain-of-Thought (CoT) explanations without requiring separate verification-specific CoT datasets?
- Why does Leave-One-Out PPO^V achieve a better balance between reasoner and verifier accuracy compared to GRPO^V when varying the verification coefficient λ?
- Does RL^V maintain its 8–32× test-time compute efficiency gains when scaled to LLMs larger than 7B parameters?

## Limitations
- Verification mechanism constrained to binary Yes/No judgments, potentially insufficient for multi-step reasoning verification
- Scalability analysis limited to specific RL algorithms (GRPO, VinePPO, Leave-One-Out PPO)
- Compute efficiency claims based on comparison with LLM-as-a-Judge baselines, which have their own accuracy limitations
- Limited testing of out-of-domain generalization beyond AIME'24

## Confidence
- **High confidence (8-10/10):** Core mechanism of generative verification through next-token prediction well-supported by empirical results
- **Medium confidence (5-7/10):** Synergistic transfer claims supported but algorithm-dependent with significant tradeoffs
- **Low confidence (2-4/10):** Out-of-domain generalization claims based on limited testing and need more extensive validation

## Next Checks
1. Implement RL^V on a value-free RL algorithm not tested in the paper (e.g., SunPPO) to verify generalization beyond the three algorithms studied
2. Extend the verification mechanism to support multi-step Chain-of-Thought verification by predicting correctness at each reasoning step rather than binary final judgment
3. Test the trained verifier on mathematical problems from sources not used in training or standard benchmarks to assess true out-of-domain generalization