---
ver: rpa2
title: Grounding Foundational Vision Models with 3D Human Poses for Robust Action
  Recognition
arxiv_id: '2511.05622'
source_url: https://arxiv.org/abs/2511.05622
tags:
- action
- recognition
- human
- fusion
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel fusion architecture for human action
  recognition that combines contextual visual understanding from V-JEPA 2 with explicit
  3D human pose data from CoMotion. The model employs a cross-attention mechanism
  to integrate these complementary modalities, enabling robust recognition of human
  actions in complex, occluded environments.
---

# Grounding Foundational Vision Models with 3D Human Poses for Robust Action Recognition

## Quick Facts
- arXiv ID: 2511.05622
- Source URL: https://arxiv.org/abs/2511.05622
- Reference count: 40
- Primary result: Fusion of V-JEPA 2 visual understanding and CoMotion 3D pose data with cross-attention achieves state-of-the-art performance on occluded action recognition benchmarks

## Executive Summary
This paper introduces a novel fusion architecture for human action recognition that combines contextual visual understanding from V-JEPA 2 with explicit 3D human pose data from CoMotion. The model employs a cross-attention mechanism to integrate these complementary modalities, enabling robust recognition of human actions in complex, occluded environments. Experiments on the InHARD and UCF-19-Y-OCC benchmarks demonstrate that this fusion approach outperforms strong unimodal and state-of-the-art baselines, particularly in high-occlusion scenarios. An ablation study confirms the effectiveness of cross-attention over other fusion strategies. The findings highlight the importance of grounding action recognition in spatial and geometric understanding rather than relying solely on statistical pattern recognition.

## Method Summary
The proposed method fuses visual features from V-JEPA 2 with 3D pose features from CoMotion using a cross-attention mechanism. The architecture processes video frames through V-JEPA 2 to extract contextual visual understanding, while CoMotion generates 3D human pose sequences. These complementary representations are then combined through cross-attention layers, allowing the model to leverage both the spatial-temporal context from visual features and the geometric structure from pose data. This multimodal fusion enables robust action recognition even in challenging scenarios with significant occlusion or partial visibility of human subjects.

## Key Results
- Outperforms state-of-the-art methods on InHARD and UCF-19-Y-OCC benchmarks
- Cross-attention fusion strategy shows significant improvement over other fusion approaches in ablation studies
- Maintains robust performance in high-occlusion scenarios where unimodal approaches degrade significantly

## Why This Works (Mechanism)
The cross-attention mechanism effectively combines complementary information streams - V-JEPA 2 provides rich contextual visual understanding while CoMotion offers explicit 3D geometric structure. By allowing these modalities to interact through attention, the model can resolve ambiguities that arise from occlusion or partial visibility. The geometric constraints from pose data help disambiguate actions that might appear similar visually but differ in body movement patterns, while the contextual visual features provide scene understanding that pure pose data cannot capture.

## Foundational Learning

**Visual feature extraction** - Why needed: Provides contextual understanding of scenes and objects. Quick check: Verify V-JEPA 2 features capture relevant spatial-temporal patterns for action recognition.

**3D pose estimation** - Why needed: Offers explicit geometric structure independent of visual appearance. Quick check: Validate CoMotion produces accurate 3D poses across diverse action classes.

**Cross-attention fusion** - Why needed: Enables bidirectional information flow between modalities. Quick check: Ensure attention weights reflect meaningful interactions between visual and pose features.

**Occlusion handling** - Why needed: Real-world videos often contain partial visibility of subjects. Quick check: Test performance degradation as occlusion levels increase.

**Multimodal integration** - Why needed: Combines complementary strengths of different data representations. Quick check: Verify each modality contributes unique information not captured by the other.

## Architecture Onboarding

**Component map:** Video frames -> V-JEPA 2 -> Visual features -> Cross-attention -> Fusion features -> Classification

**Critical path:** Visual feature extraction and pose estimation feed into cross-attention layers, which produce the fused representation used for final classification.

**Design tradeoffs:** The architecture trades computational efficiency for robustness by incorporating two feature extractors and cross-attention layers, but this design choice enables superior performance in challenging scenarios.

**Failure signatures:** The model may struggle when both visual and pose features are corrupted or when the cross-attention mechanism fails to align complementary information effectively.

**First experiments:** 1) Test unimodal baselines with V-JEPA 2 only and CoMotion only. 2) Evaluate alternative fusion strategies (early fusion, late fusion, concatenation). 3) Assess performance degradation with increasing occlusion levels.

## Open Questions the Paper Calls Out

None provided in the source material.

## Limitations

- Heavy reliance on external feature extractors (V-JEPA 2 and CoMotion) that may have their own limitations
- Limited evaluation on available benchmarks may not reflect real-world performance
- Computational complexity and memory requirements not addressed for practical deployment

## Confidence

High confidence in the claim that the fusion approach outperforms unimodal baselines and state-of-the-art methods on the tested benchmarks.

Medium confidence in the assertion that grounding action recognition in spatial and geometric understanding is superior to statistical pattern recognition.

Low confidence in the scalability of the approach to large-scale, real-world deployments.

## Next Checks

1. Evaluate the model's performance on additional action recognition benchmarks with varying levels of complexity and occlusion, including real-world video datasets, to assess generalization capabilities.

2. Conduct an ablation study comparing the cross-attention fusion approach with alternative fusion mechanisms (e.g., early fusion, late fusion, and multimodal transformers) on a larger set of benchmarks to validate the superiority of the proposed method.

3. Implement and test the model in a real-world application scenario, such as human-robot interaction or video surveillance, to evaluate its practical utility and identify potential limitations not apparent in controlled benchmark environments.