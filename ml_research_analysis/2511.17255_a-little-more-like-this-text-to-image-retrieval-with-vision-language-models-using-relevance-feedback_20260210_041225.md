---
ver: rpa2
title: 'A Little More Like This: Text-to-Image Retrieval with Vision-Language Models
  Using Relevance Feedback'
arxiv_id: '2511.17255'
source_url: https://arxiv.org/abs/2511.17255
tags:
- feedback
- relevance
- retrieval
- image
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for enhancing text-to-image
  retrieval by refining query representations at inference time using relevance feedback.
  It proposes four strategies: pseudo-relevance feedback (PRF), generative relevance
  feedback (GRF) using synthetic captions, an attentive feedback summarizer (AFS)
  that integrates multimodal fine-grained features, and simulated explicit feedback
  using ground-truth captions.'
---

# A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback

## Quick Facts
- arXiv ID: 2511.17255
- Source URL: https://arxiv.org/abs/2511.17255
- Reference count: 40
- Text-to-image retrieval improved 3-5% MRR@5 for smaller VLMs, 1-3% for larger models using feedback

## Executive Summary
This paper introduces a framework for enhancing text-to-image retrieval by refining query representations at inference time using relevance feedback. The authors propose four strategies: pseudo-relevance feedback (PRF), generative relevance feedback (GRF) using synthetic captions, an attentive feedback summarizer (AFS) that integrates multimodal fine-grained features, and simulated explicit feedback using ground-truth captions. Experiments on Flickr30k and COCO with four vision-language models show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller models and 1-3% for larger ones compared to no feedback. AFS particularly mitigates query drift and is more robust in multi-turn retrieval settings.

## Method Summary
The framework enhances text-to-image retrieval by incorporating relevance feedback into the query refinement process. Four strategies are evaluated: PRF generates synthetic captions from top-ranked images, GRF uses a caption generator to create feedback captions, AFS employs an attention mechanism to summarize feedback into a weighted query, and explicit feedback uses ground-truth captions as simulated feedback. All methods operate at inference time without model fine-tuning, refining the initial text query based on retrieved results. The approach is tested across four VLMs (BLIP-2, GIT, CLIP, OWL-ViT) on Flickr30k and COCO datasets, measuring improvements in MRR@5 and retrieval accuracy.

## Key Results
- GRF, AFS, and explicit feedback improve retrieval by 3-5% MRR@5 for smaller VLMs
- AFS outperforms GRF and PRF, particularly in multi-turn retrieval settings
- Larger VLMs show more modest gains (1-3% MRR@5) but remain consistently improved
- PRF performs worse than baseline in several cases, indicating synthetic caption quality issues

## Why This Works (Mechanism)
The framework works by iteratively refining query representations based on feedback from retrieved results. GRF generates synthetic captions that capture visual aspects of top-ranked images, enriching the original query with relevant visual concepts. AFS uses attention mechanisms to weigh and combine fine-grained features from multiple feedback sources, creating a more focused and contextually appropriate query representation. This process reduces semantic gaps between text queries and visual content, particularly for smaller VLMs that lack the comprehensive world knowledge of larger models. The iterative refinement process also helps mitigate query drift by maintaining focus on the original intent while incorporating new relevant information.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Multimodal models that jointly process visual and textual information, needed for text-to-image retrieval tasks. Quick check: Verify model can generate relevant captions for given images.
- **Relevance Feedback**: User or system-generated feedback used to refine search queries, essential for iterative retrieval improvement. Quick check: Ensure feedback mechanism can process and integrate retrieved results.
- **Query Drift**: Phenomenon where iterative query refinement moves away from original intent, problematic for multi-turn retrieval. Quick check: Monitor query semantic similarity across refinement iterations.
- **Attention Mechanisms**: Neural network components that weigh input features based on relevance, critical for feature integration in AFS. Quick check: Verify attention weights meaningfully reflect feature importance.
- **MRR@5 (Mean Reciprocal Rank at 5)**: Retrieval evaluation metric measuring average reciprocal rank of relevant items within top-5 results. Quick check: Calculate baseline MRR@5 before applying feedback strategies.
- **Pseudo-Relevance Feedback**: Feedback generated from top retrieved results without explicit user input, useful for automated query refinement. Quick check: Validate synthetic captions align with visual content.

## Architecture Onboarding
**Component Map**: Text Query -> VLM Encoder -> Retrieval Engine -> Top Results -> Feedback Generator -> Query Refiner -> Enhanced Query
**Critical Path**: Initial query → VLM encoding → retrieval → feedback generation → query refinement → final retrieval
**Design Tradeoffs**: GRF trades caption generation quality for automation vs. explicit feedback; AFS adds complexity for better drift mitigation vs. simpler PRF
**Failure Signatures**: PRF shows performance degradation when synthetic captions misrepresent visual content; larger VLMs show diminishing returns suggesting capacity saturation
**First 3 Experiments**: 1) Compare MRR@5 improvement between GRF and PRF across all VLMs 2) Measure query drift using semantic similarity metrics in multi-turn settings 3) Ablation study removing attention mechanism from AFS to quantify contribution

## Open Questions the Paper Calls Out
The paper notes that PRF's poor performance suggests synthetic caption quality issues that warrant deeper analysis. It acknowledges that simulated explicit feedback assumes ground-truth captions, which is unrealistic in practice. The authors also suggest that capacity effects from different VLM pre-training regimes may influence feedback effectiveness but do not control for this systematically. Multi-turn retrieval improvements are demonstrated but the impact of query complexity and number of turns is not fully explored.

## Limitations
- Evaluation limited to Flickr30k and COCO datasets, constraining generalizability to other domains
- PRF strategy performs worse than baseline, indicating potential noise introduction from synthetic captions
- Simulated explicit feedback assumes ground-truth captions are available, unrealistic for real-world deployment
- No systematic control for VLM pre-training effects on feedback effectiveness

## Confidence
- Medium: Retrieval performance gains across different VLMs and datasets
- Medium: AFS superiority over GRF with limited ablation evidence
- High: General feasibility of feedback-enhanced VLMs without fine-tuning
- Medium: Claims about AFS mitigating query drift without qualitative analysis

## Next Checks
1. Test AFS and GRF on a third, out-of-domain dataset to assess robustness
2. Perform an ablation removing the attention mechanism in AFS to quantify its contribution
3. Compare feedback strategies across a wider range of VLM sizes and pre-training regimes to control for model capacity effects