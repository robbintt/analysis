---
ver: rpa2
title: 'NERCat: Fine-Tuning for Enhanced Named Entity Recognition in Catalan'
arxiv_id: '2503.14173'
source_url: https://arxiv.org/abs/2503.14173
tags:
- catalan
- entity
- dataset
- named
- gliner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Named Entity Recognition
  (NER) for the low-resource Catalan language by fine-tuning the GLiNER model using
  manually annotated Catalan television transcriptions. The methodology involved using
  Whisper Fast for speech-to-text transcription, followed by expert annotation to
  create a high-quality dataset covering domains like politics, sports, and culture.
---

# NERCat: Fine-Tuning for Enhanced Named Entity Recognition in Catalan

## Quick Facts
- arXiv ID: 2503.14173
- Source URL: https://arxiv.org/abs/2503.14173
- Authors: Guillem Cadevall Ferreres; Marc Serrano Sanz; Marc Bardeli Gámez; Pol Gerdt Basullas; Francesc Tarres Ruiz; Raul Quijada Ferrero
- Reference count: 5
- Primary result: Fine-tuned GLiNER model on manually annotated Catalan TV transcriptions, achieving up to 80% F1-score improvement in underrepresented categories.

## Executive Summary
This paper addresses Named Entity Recognition (NER) for the low-resource Catalan language by fine-tuning the GLiNER model using manually annotated television transcriptions. The methodology involved using Whisper Fast for speech-to-text transcription, followed by expert annotation to create a high-quality dataset covering domains like politics, sports, and culture. The NERCat model was then fine-tuned on this dataset, achieving significant performance improvements over the baseline GLiNER model, particularly in underrepresented categories. Results show overall F1-score increases of up to 80% in categories such as Law, with near-perfect precision and recall in Person, Organization, and Location entities.

## Method Summary
The approach fine-tunes `knowledgator/gliner-bi-large-v1.0` on a manually annotated dataset of 9,242 Catalan sentences from TV transcriptions, containing 13,732 entity instances across 8 types. Training uses batch size 8, focal loss (α=0.75, γ=2), dual learning rates (5×10⁻⁶ for entity layers, 1×10⁻⁵ for others), and a linear scheduler with warmup ratio 0.1. The model is evaluated on a held-out set of 100 sentences.

## Key Results
- Significant F1-score improvements across all categories, with Law showing an 80% increase
- Near-perfect precision and recall for high-frequency categories (Person, Organization, Location)
- Focal loss effectively addresses class imbalance, particularly benefiting rare entity types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific fine-tuning with manually annotated data substantially improves NER performance for underrepresented entity categories in low-resource languages.
- Mechanism: Manual annotation corrects the baseline model's systematic errors—particularly its tendency to misclassify common nouns as named entities—by providing gold-standard labels that recalibrate entity boundary detection and type classification for Catalan-specific patterns.
- Core assumption: High-quality manual annotations are more effective than automated approaches (LLMs, synthetic data) for correcting language-specific misclassification patterns in low-resource settings.
- Evidence anchors:
  - [abstract]: "evaluation results show significant improvements in precision, recall, and F1-score, particularly for underrepresented named entity categories such as Law, Product, and Facility"
  - [Section 3.1]: "A major issue with GLiNER Knowledgator is its tendency to misclassify common nouns as named entities, leading to a high false positive rate"
  - [Section 3.4]: "LLMs had significant difficulty distinguishing between proper nouns and common nouns, leading to erroneous classifications"
  - [corpus]: Related work (arXiv:2505.16814) explores synthetic data for low-resource NER, suggesting this remains an active comparison point; no direct citation evidence yet validates manual vs. synthetic trade-offs for Catalan specifically.
- Break condition: If the annotation quality is inconsistent or the domain coverage is too narrow (e.g., only television), improvements may not generalize to other Catalan text domains.

### Mechanism 2
- Claim: Focal loss addresses severe class imbalance, enabling meaningful learning on rare entity types.
- Mechanism: By down-weighting well-classified frequent classes (Person: 37.90%) and focusing optimization on hard, misclassified examples from rare classes (Law: 0.83%), focal loss allows the model to learn discriminative features for underrepresented categories rather than being dominated by majority classes.
- Core assumption: Class imbalance is a primary bottleneck for rare entity recognition, not merely insufficient data volume.
- Evidence anchors:
  - [Section 3.5]: "applying focal loss (α = 0.75, γ = 2) to address class imbalances"
  - [Section 4.1]: "greatest improvements are seen in categories with previously poor recognition, such as Facility, Product, and Law, where F1-scores increased by 57.75%, 66.71%, and 80.00% respectively"
  - [corpus]: No direct corpus evidence on focal loss for low-resource NER; this is a plausible but unverified transfer from computer vision literature.
- Break condition: If rare classes have fundamental ambiguity in their definitions (e.g., Law vs. Event boundary cases), focal loss alone cannot resolve labeling inconsistencies.

### Mechanism 3
- Claim: Real-world television transcriptions provide linguistically diverse, domain-relevant training data that better reflects actual Catalan usage than curated corpora.
- Mechanism: Television content spans politics, sports, and culture with natural code-switching (Catalan-Spanish), exposing the model to authentic entity distributions and linguistic variations absent from formal written corpora.
- Core assumption: Television transcriptions adequately represent the entity types and linguistic patterns needed for downstream Catalan NLP applications.
- Evidence anchors:
  - [abstract]: "focusing on domains such as politics, sports, and culture"
  - [Section 3.2]: "dataset primarily consists of Catalan-language texts, with occasional Spanish code-switching segments"
  - [corpus]: Weak corpus signal; no comparative studies on data source selection for Catalan NER specifically.
- Break condition: If downstream applications require formal written Catalan (e.g., legal documents), television transcriptions may underrepresent relevant vocabulary and entity types.

## Foundational Learning

- **Named Entity Recognition (NER)**:
  - Why needed here: This is the core task; understanding that NER involves both boundary detection (finding entity spans) and type classification (assigning categories) is essential for interpreting the results.
  - Quick check question: Given the sentence "The Barcelona court ruled on the Constitutional Law yesterday," can you identify all named entities and their types using the paper's taxonomy?

- **Class imbalance in classification**:
  - Why needed here: The dataset has extreme imbalance (Person: 37.90% vs. Law: 0.83%), which directly motivated the focal loss choice.
  - Quick check question: If you trained a classifier on this dataset using standard cross-entropy loss, what failure mode would you expect for the "Law" category?

- **Fine-tuning vs. from-scratch training**:
  - Why needed here: The approach relies on transferring knowledge from a multilingual pre-trained model rather than building a Catalan-specific model.
  - Quick check question: What are the trade-offs between fine-tuning GLiNER vs. training a BERT-based NER model from scratch using only the Catalan dataset?

## Architecture Onboarding

- **Component map**:
  - Whisper Fast → Raw transcriptions (speech-to-text)
  - NER Annotator Tool → Labeled dataset (8 entity types)
  - GLiNER Knowledgator (gliner-bi-large-v1.0) → Pre-trained backbone
  - Fine-tuning layer → Entity classification head with focal loss

- **Critical path**:
  1. Collect and transcribe Catalan TV audio using Whisper Fast
  2. Manually annotate 9,242 sentences with 13,732 entity instances
  3. Shuffle and split dataset (90% train / 10% test)
  4. Fine-tune with batch size 8, focal loss (α=0.75, γ=2), learning rates 5×10⁻⁶ (entity layers) and 1×10⁻⁵ (other parameters)

- **Design tradeoffs**:
  - Manual annotation vs. LLM-based labeling: Authors explicitly rejected LLM annotation due to poor noun/proper-noun distinction; trade-off is annotation cost vs. accuracy.
  - Television domain vs. broader corpora: Provides naturalistic code-switching but may miss formal/legal registers.
  - 90/10 split vs. cross-validation: Simple split may not capture variance; cross-validation would provide more robust estimates.

- **Failure signatures**:
  - Baseline GLiNER: High false positive rate from misclassifying common nouns as entities
  - LLM annotation: Cannot reliably distinguish proper nouns from common nouns in Catalan
  - Date category: Precision dropped from 1.00 to 0.88 after fine-tuning, but this reflects a recall increase (7% → 100%), not true degradation

- **First 3 experiments**:
  1. Establish baseline: Evaluate `knowledgator/gliner-bi-large-v1.0` on a held-out Catalan test set before any fine-tuning to quantify initial misclassification rates.
  2. Annotation quality check: Annotate a small subset (100-200 sentences) with multiple annotators to measure inter-annotator agreement and identify ambiguous entity boundaries.
  3. Ablation on loss function: Compare focal loss vs. standard cross-entropy on the same training data to isolate the contribution of class imbalance handling to rare entity performance.

## Open Questions the Paper Calls Out
The paper explicitly states that future work will focus on expanding the dataset and exploring additional linguistic variations to further refine entity recognition capabilities, indicating open questions about domain generalization and dataset completeness.

## Limitations
- Limited domain generalization: The model is trained exclusively on television transcriptions, raising questions about performance on other Catalan text domains like legal documents or social media.
- Unknown annotation quality: No inter-annotator agreement metrics are reported, making it difficult to assess the consistency of entity boundary definitions, particularly for ambiguous categories.
- Incomplete hyperparameter specification: Total training steps/epochs are not specified beyond evaluation frequency, limiting reproducibility.

## Confidence

- **High confidence**: The baseline characterization (GLiNER's tendency to misclassify common nouns as named entities) and the overall performance improvement pattern are well-supported by the evaluation results. The methodology of using manual annotation to correct systematic errors is sound and well-documented.

- **Medium confidence**: The mechanism by which focal loss addresses class imbalance is theoretically sound, but lacks direct empirical validation in this specific context. The choice of focal loss parameters and their impact on different entity categories warrants further investigation.

- **Low confidence**: Claims about television transcriptions providing optimal domain coverage and the superiority of manual annotation over LLM-based approaches are asserted but not empirically validated against alternatives. The generalizability to other Catalan domains remains speculative.

## Next Checks

1. **Inter-annotator agreement study**: Have 2-3 additional annotators label a 200-sentence subset of the dataset. Calculate Cohen's kappa for each entity category to establish annotation consistency, particularly for ambiguous categories like Law vs. Event. This will validate whether observed improvements reflect model learning or annotation quality.

2. **Cross-domain evaluation**: Test the fine-tuned NERCat model on a curated dataset from a different Catalan text domain (e.g., legal documents, news articles, or social media). Compare performance across domains to assess generalization and identify potential domain-specific weaknesses.

3. **Ablation on focal loss**: Retrain the model with standard cross-entropy loss while keeping all other conditions constant. Compare per-class F1 scores, particularly for rare categories, to quantify the specific contribution of focal loss to handling class imbalance in this low-resource setting.