---
ver: rpa2
title: From the Gradient-Step Denoiser to the Proximal Denoiser and their associated
  convergent Plug-and-Play algorithms
arxiv_id: '2509.09793'
source_url: https://arxiv.org/abs/2509.09793
tags:
- denoiser
- image
- algorithm
- operator
- plug-and-play
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies convergence of plug-and-play (PnP) algorithms
  that use denoisers to solve image inverse problems. The key innovation is the gradient-step
  denoiser (GS) and its extension, the proximal denoiser (Prox), which are trained
  to represent gradient or proximity operators of explicit regularization functionals
  while preserving state-of-the-art denoising performance.
---

# From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms

## Quick Facts
- arXiv ID: 2509.09793
- Source URL: https://arxiv.org/abs/2509.09793
- Reference count: 29
- Introduces gradient-step denoiser (GS) and proximal denoiser (Prox) that enable provable convergence of PnP algorithms

## Executive Summary
This paper addresses the convergence issues in Plug-and-Play (PnP) algorithms by designing denoisers that explicitly represent gradient or proximity operators of regularization functionals. The Gradient-Step Denoiser (GS) and Proximal Denoiser (Prox) are trained to be exactly the gradient descent operator or proximity operator of explicit functionals, respectively. This design allows provable convergence of PnP schemes such as Proximal Gradient Descent (PGD), Douglas-Rachford Splitting (DRS), and their variants under conditions like Lipschitz gradients and the Kurdyka-Lojasiewicz property.

## Method Summary
The paper introduces two convergent denoisers: the Gradient-Step Denoiser (GS) and the Proximal Denoiser (Prox). Both are implemented using the DRUNet architecture with smooth activation functions (ELU for GS, Softplus for Prox). The GS-denoiser is trained via MSE loss for ~1500 epochs, while the Prox-denoiser is fine-tuned from GS weights with spectral norm regularization for ~10 epochs. PnP algorithms (PGD, DRS, DRSdiff) are implemented with either fixed step sizes or backtracking procedures to ensure sufficient decrease. The methods are evaluated on denoising, super-resolution, deblurring, and inpainting tasks.

## Key Results
- GS and Prox denoisers achieve state-of-the-art denoising performance while enabling provable convergence of PnP algorithms
- Convergence is guaranteed under Lipschitz gradient conditions and the Kurdyka-Lojasiewicz property
- Performance depends critically on hyperparameter tuning (σ, λ, τ), with optimal results on moderately ill-posed problems
- Methods excel at denoising, super-resolution, and deblurring but struggle with large-hole inpainting where generative models are needed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gradient-step denoiser D_σ = Id - ∇g_σ enables provable PnP convergence by constructing an explicit regularization functional.
- Mechanism: Rather than using an implicit denoiser, the network N_σ parameterizes g_σ(x) = ½||x - N_σ(x)||², making D_σ a conservative vector field (gradient field). This explicit structure allows monitoring of the objective F = (1/λ)f + g_σ and verification of sufficient decrease during iterations.
- Core assumption: ∇g_σ is L-Lipschitz continuous; step size τ < λ/L.
- Evidence anchors:
  - [abstract] "the Gradient-Step Denoiser is trained to be exactly the gradient descent operator... of an explicit functional"
  - [Section 2.1] Defines D_σ = N_σ + J_Nσ^T(x - N_σ(x)) via auto-differentiation
  - [corpus] Related work "A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch" extends PnP-PGD convergence theory
- Break condition: If the Jacobian of N_σ is not L-Lipschitz, the step-size constraint τ < λ/L cannot be satisfied, and convergence guarantees fail.

### Mechanism 2
- Claim: The proximal denoiser enforces D_σ = Prox_{φ_σ} via Hessian spectral norm regularization, enabling DRS convergence even with non-differentiable data-fidelity terms.
- Mechanism: Proposition 3.1 shows that if ∇g_σ is L < 1 Lipschitz, D_σ equals the proximal operator of an explicit weakly convex potential φ_σ. Training penalizes |||∇²g_σ||| via power iteration, ensuring D_σ is injective with a well-defined inverse for computing φ_σ.
- Core assumption: L < 1 (strict contraction); for DRS with non-differentiable f, L < ½.
- Evidence anchors:
  - [Section 3.1] "it is sufficient to ask for ∇g_σ to be a contraction so that the denoiser can effectively be a proximity operator"
  - [Section 4.2.2] Power iteration estimates spectral norm during training with loss (21)
  - [corpus] "Nonasymptotic Convergence Rates for Plug-and-Play Methods With MMSE Denoisers" notes MMSE denoisers can be written as proximal operators but require additional structure for rates
- Break condition: If L ≥ 1, the proximal interpretation fails; if L ≥ ½, DRS with non-differentiable f loses guarantees.

### Mechanism 3
- Claim: Backtracking dynamically adjusts step-size to enforce sufficient decrease, removing need to estimate Lipschitz constant L a priori.
- Mechanism: At each iteration, while F(x^k) - F(T(x^k)) < (γ/τ)||x^k - T(x^k)||², reduce τ ← ητ. This ensures the Lyapunov decrease condition holds empirically even without knowing L.
- Core assumption: F is bounded below and coercive (enforced via projection to convex set C = [-1,2]^n).
- Evidence anchors:
  - [Section 2.4] "using a backtracking procedure allows to dynamically change the step-size so that the sufficient decrease property is verified"
  - [Algorithm 1] Implements backtracking check with γ ∈ (0, 1/2)
  - [corpus] Weak corpus evidence—backtracking for PnP is not extensively discussed in neighbors
- Break condition: If τ decays below numerical precision (ϵ = 10^-6) before convergence, algorithm stalls.

## Foundational Learning

- Concept: Proximity operator Prox_g(x) = argmin_y g(y) + ½||x-y||²
  - Why needed here: All PnP algorithms replace Prox_g or ∇g with denoisers; understanding proximal splitting is prerequisite.
  - Quick check question: Given g(x) = ½||x||², what is Prox_g(x)?

- Concept: Tweedie's identity: D_MMSE = Id + σ²∇log p_σ
  - Why needed here: Explains why MMSE denoisers are gradient fields; motivates GS-denoiser design.
  - Quick check question: If p_σ is Gaussian, what does the MMSE denoiser compute?

- Concept: Kurdyka-Łojasiewicz (KL) property
  - Why needed here: Required for global convergence (Theorem 2.2) beyond stationary-point guarantees; semi-algebraic functions satisfy KL.
  - Quick check question: Why does KL property ensure finite-length convergence rather than just cluster points?

## Architecture Onboarding

- Component map:
  DRUNet backbone (UNet with ResNet blocks, strided convolutions) -> Noise level map (additional channel encoding σ²) -> D_σ forward (N_σ(x) + J_Nσ^T(x - N_σ(x))) -> Power iteration (50 iterations per batch for spectral norm estimation)

- Critical path:
  1. Train GS-DRUNet with MSE loss (12) for ~1500 epochs
  2. Fine-tune Prox-DRUNet with loss (21) for ~10 epochs, regularizing spectral norm
  3. Verify L < 1 (Prox) or estimate L for GS; set λ > L_f and τ < λ/L
  4. Run fixed-point iterations with backtracking (GS) or fixed τ=1 (Prox)

- Design tradeoffs:
  - ELU (C¹) vs Softplus (C^∞): ELU for GS (sufficient), Softplus for Prox (C² required for φ_σ regularity)
  - Higher L → more expressive denoiser but stricter step-size constraints
  - λ tuning: larger λ = less regularization = sharper but potentially artifacted results

- Failure signatures:
  - PSNR decreases with iterations: τ too large or σ mismatched to noise level
  - Residuals stagnate above zero: L constraint violated or λ ≤ L_f
  - Large-hole inpainting produces only smooth color bleeding: PnP cannot hallucinate texture (generative models required)

- First 3 experiments:
  1. Denoising validation: Plot PSNR vs σ for both denoisers; verify GS slightly outperforms Prox in-distribution, Prox generalizes better out-of-distribution.
  2. Super-resolution with varying c = σ/ν: Identify optimal c ∈ [2,5] where PSNR peaks before declining.
  3. Deblurring with λ sweep: Confirm λ and σ have similar smoothing effects; verify residual convergence rate O(1/√K).

## Open Questions the Paper Calls Out
None

## Limitations
- Strict Lipschitz gradient constraint required for convergence, introducing uncertainty in parameter selection
- Methods fail to recover fine details in large-hole inpainting tasks, suggesting limitations in generative capability
- Performance depends heavily on hyperparameter tuning (σ, λ, τ) that varies significantly across tasks

## Confidence

- **High confidence**: The theoretical framework connecting denoisers to explicit regularization functionals (Mechanism 1 and 2) is well-established through Propositions 3.1 and 3.2, with clear mathematical proofs.
- **Medium confidence**: Empirical performance claims depend heavily on hyperparameter tuning (σ, λ, τ) that varies significantly across tasks. The reported PSNR improvements are task-dependent and may not generalize to all inverse problems.
- **Medium confidence**: The convergence guarantees rely on the Kurdyka-Łojasiewicz property, which holds for semi-algebraic functions like the DRUNet architecture, but the practical implications for convergence speed are not fully characterized.

## Next Checks

1. **Lipschitz constant verification**: Implement the power iteration method during training to monitor ||∇²g_σ|| and verify L < 1 for Prox and appropriate L for GS across different noise levels.

2. **Backtracking robustness test**: Systematically vary backtracking hyperparameters γ and η to determine their impact on convergence speed and final PSNR across multiple inverse problems.

3. **Cross-architecture generalization**: Train GS and Prox denoisers using alternative architectures (e.g., DnCNN, SwinIR) to assess whether the convergence guarantees extend beyond DRUNet and to identify architecture-specific failure modes.