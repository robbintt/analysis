---
ver: rpa2
title: Fractional Differential Equation Physics-Informed Neural Network and Its Application
  in Battery State Estimation
arxiv_id: '2512.12285'
source_url: https://arxiv.org/abs/2512.12285
tags:
- battery
- estimation
- fractional
- neural
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Fractional Differential Equation Physics-Informed
  Neural Network (FDIFF-PINN) for lithium-ion battery State of Charge (SOC) estimation.
  The method addresses the challenge of modeling complex nonlinearities and memory-dependent
  dynamics in battery electrochemical processes.
---

# Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation

## Quick Facts
- arXiv ID: 2512.12285
- Source URL: https://arxiv.org/abs/2512.12285
- Authors: Lujuan Dang; Zilai Wang
- Reference count: 0
- One-line result: FDIFF-PINN achieves SOC estimation with MSE stabilized below 3% under specific conditions

## Executive Summary
This paper introduces FDIFF-PINN, a novel approach for lithium-ion battery State of Charge (SOC) estimation that combines fractional calculus with deep learning. The method addresses the challenge of modeling complex nonlinearities and memory-dependent dynamics in battery electrochemical processes by embedding fractional-order differential equation residuals into the neural network loss function as physical regularization constraints. Experiments on Panasonic 18650PF batteries across temperatures (-10°C to 20°C) demonstrate superior performance over traditional data-driven approaches, particularly in low-temperature and steady-state conditions, while offering improved physical interpretability through mechanistic modeling principles.

## Method Summary
FDIFF-PINN integrates fractional calculus with deep learning by using a standard neural network (MLP, RNN, or LSTM) as the backbone and embedding fractional-order differential equation residuals into the loss function as physical regularization constraints. The total loss combines data fitting error with physics residuals from fractional-order SOC dynamics and polarization voltage equations, discretized using the Grünwald-Letnikov definition. The model takes Voltage, Current, and Temperature as inputs and predicts SOC and polarization voltage, with the physics constraints ensuring the predictions satisfy the fractional-order state-space equations of an equivalent circuit model.

## Key Results
- Achieves SOC estimation with Mean Squared Error stabilized below 3% under specific conditions
- Demonstrates superior performance over traditional data-driven approaches, particularly in low-temperature and steady-state conditions
- Reduces MAE from 0.108 (pure MLP) to 0.071 at -20°C under HWFET cycle
- MLP+Physics architecture often outperforms LSTM+Physics in low-temperature environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The fractional-order operator captures non-local memory dependencies in battery dynamics more effectively than integer-order models.
- **Mechanism**: By replacing integer-order capacitors with Constant Phase Elements (CPE) and utilizing the Grünwald-Letnikov (G-L) definition, the model computes a weighted sum of historical states, allowing retention of "heredity" information.
- **Core assumption**: Battery electrochemical processes exhibit power-law relaxation behaviors that integer-order calculus cannot approximate efficiently.
- **Evidence anchors**:
  - [abstract] "capturing the battery’s non-ideal polarization behavior and historical dependencies by incorporating the long-range temporal correlation prior knowledge embedded within the fractional-order differential operator."
  - [section 2.3.2] Defines CPE impedance $Z_{CPE}(s) = \frac{1}{Q s^\alpha}$, noting $\alpha$ characterizes the fractal nature of the electrode interface.

### Mechanism 2
- **Claim**: Embedding fractional differential equation residuals into the loss function acts as a physics-based regularizer, constraining the neural network to physically plausible solution spaces.
- **Mechanism**: The total loss $L_{total} = L_{data} + \lambda L_{phy}$ penalizes deviations from the fractional-order state-space equations, forcing the network to learn weights that satisfy both data error and physical constraints.
- **Core assumption**: The fractional-order equivalent circuit model (Thevenin model with CPE) is a sufficiently accurate approximation of true battery physics.
- **Evidence anchors**:
  - [abstract] "embedding the residual term of the fractional-order dynamic equation into the loss function... as physical regularization constraint."
  - [section 3.3] Explicitly defines $L_{phy} = L_{dyn} + L_{pol}$ derived from the G-L approximation.

### Mechanism 3
- **Claim**: The hybrid architecture provides superior generalization in low-temperature environments where data-driven models suffer from distribution shift.
- **Mechanism**: Pure data-driven models struggle when test conditions alter the voltage-current relationship significantly from training data. The FDIFF-PINN uses the fractional model structural prior to anchor predictions.
- **Core assumption**: The structure of the fractional-order dynamics remains valid across the temperature range, even if specific parameters change.
- **Evidence anchors**:
  - [abstract] "superior performance over traditional data-driven approaches, particularly in low-temperature... conditions."
  - [section 4.2.1] Tables 3 & 4 show FDIFF-PINN-MLP reducing MAE from 0.108 to 0.071 at -20°C under HWFET cycle.

## Foundational Learning

### Concept: Grünwald-Letnikov (G-L) Discretization
- **Why needed here**: This is the numerical engine that converts continuous fractional calculus into a computable weighted sum of past time steps (memory buffer).
- **Quick check question**: Can you explain why the G-L approximation requires storing a history buffer of states, unlike a standard Euler step?

### Concept: Constant Phase Element (CPE)
- **Why needed here**: Understanding CPEs is crucial to understanding why the "Physics" part of this PINN is fractional, not integer.
- **Quick check question**: If a CPE has an order $\alpha=0.5$, does it behave more like a resistor or a capacitor, and how does this relate to Warburg impedance?

### Concept: Loss Function Regularization ($\lambda$)
- **Why needed here**: The balance between data fitting and physics adherence is manually controlled here; tuning this is critical for onboarding.
- **Quick check question**: What happens to the model's ability to fit noisy data if $\lambda$ is set to 0 vs. 100?

## Architecture Onboarding

### Component map
Neural Network (MLP/RNN/LSTM) -> Fractional Derivative Calculator (G-L) -> Physics Loss Aggregator -> Total Loss (Data + Physics)

### Critical path
The implementation of the history buffer update (Eq. 11) is the most fragile component. You must ensure the weights $w_j^{(\alpha)}$ are applied correctly to the sliding window of past states during the forward pass to compute $L_{phy}$.

### Design tradeoffs
- **MLP vs. LSTM Backbone**: The paper shows MLP+Physics often outperforms LSTM+Physics in low temps. Assumption: The physics model handles the temporal memory, reducing the need for complex LSTM memory gates.
- **Memory Length ($M$)**: Longer memory ($M=15$) generally lowers error but increases computation (Table 17).

### Failure signatures
- **Exploding Gradients**: If the fractional order $\alpha$ is high and memory length is long without normalization, gradients can destabilize.
- **US06 Instability**: Table 10 shows FDIFF-PINN-MLP failing catastrophically (MAE 3.17) at -20°C on the US06 cycle, likely due to high-frequency dynamics mismatching the steady-state assumptions of the physics model.

### First 3 experiments
1. **Overfit Baseline**: Train a pure MLP (no physics, $\lambda=0$) on a single driving cycle (e.g., HWFET) to establish a baseline error floor.
2. **Lambda Sweep**: Introduce the physics term and sweep $\lambda$ (0.0 to 2.0). Plot the validation loss components ($L_{data}$ vs $L_{phy}$) to find the "elbow" where physics improves generalization without hurting data fit.
3. **Memory Ablation**: Fix $\lambda$ and vary the history length $M$ (e.g., 5, 10, 15) to verify that the fractional derivative is actually utilizing the history buffer to reduce error.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive mechanisms, such as reinforcement learning, be integrated to dynamically tune the physical weighting coefficient ($\lambda$) and fractional order ($\alpha$) during training or inference?
- Basis in paper: [explicit] The "Outlook" section explicitly identifies "Adaptive hyperparameter tuning using reinforcement learning" as a primary direction for future research.
- Why unresolved: The current study relies on manual tuning and ablation studies to determine optimal fixed values for $\lambda$ and $\alpha$.
- What evidence would resolve it: A study demonstrating an RL agent or adaptive controller adjusting these parameters in real-time, resulting in maintained or improved MSE without manual intervention.

### Open Question 2
- Question: What specific architectural modifications are required to stabilize the FDIFF-PINN-MLP model under high-frequency dynamic conditions where it currently exhibits instability?
- Basis in paper: [inferred] While the text states the model optimizes for "complex conditions," the results in Table 10 show that FDIFF-PINN-MLP suffers catastrophic failure (MAE 3.17) on the US06 cycle at -20°C.
- Why unresolved: The paper does not analyze the root cause of this divergence in the fractional-MLP branch under high-stress, low-temperature scenarios.
- What evidence would resolve it: Ablation studies on the US06 cycle identifying whether the instability stems from the fractional derivative approximation, the loss weighting, or the MLP's capacity to resolve the physics residual.

### Open Question 3
- Question: Can the FDIFF-PINN framework maintain its theoretical advantages in capturing memory-dependent dynamics when applied to battery chemistries with fundamentally different electrochemical relaxation behaviors?
- Basis in paper: [explicit] The "Outlook" section lists "Extending to other battery chemistries" as a key future work item.
- Why unresolved: The experiments were limited to a specific dataset (LG18650-HG2), leaving the universal applicability of the chosen fractional-order equivalent circuit model unproven.
- What evidence would resolve it: Benchmark results of the FDIFF-PINN model on Lithium Iron Phosphate (LFP) or Nickel-Manganese-Cobalt (NMC) datasets showing comparable or superior performance relative to the integer-order baselines.

## Limitations
- Neural network architecture specifications (layer counts, hidden units) are not provided, making exact replication challenging
- Assumed equivalent circuit model parameters (R₀, Rp, Cp, Cn) are treated as fixed rather than learned or optimized
- Catastrophic failure on the US06 cycle at -20°C (MAE 3.17) suggests physics constraints may not capture high-frequency dynamics effectively

## Confidence

### High Confidence
- The mechanism of using fractional calculus to capture memory effects in battery dynamics is well-established in the broader literature
- The physics-informed regularization approach (embedding PDE residuals into loss) is a proven PINN framework

### Medium Confidence
- The experimental results showing superior low-temperature performance are compelling, but the specific architecture details and parameter tuning remain unclear
- The comparison with baseline methods is reasonable but could benefit from more extensive ablation studies

### Low Confidence
- The claim of "superior performance over traditional data-driven approaches" across all conditions is qualified by the noted failure on US06 at extreme temperatures

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary the physical weight λ and fractional order α across their specified ranges to identify optimal settings and quantify sensitivity. Monitor both L_data and L_phy components separately to ensure balanced convergence.

2. **Architecture Ablation**: Implement and test the three different backbone architectures (MLP, RNN, LSTM) with identical physics constraints to verify the reported performance ordering (MLP > LSTM at low temperatures) and understand when complex temporal models are beneficial.

3. **Temperature Extrapolation Test**: Train the model on data from -10°C to 10°C, then test on -20°C data to assess the model's ability to generalize beyond its training temperature range, explicitly checking for systematic biases or failures in extreme conditions.