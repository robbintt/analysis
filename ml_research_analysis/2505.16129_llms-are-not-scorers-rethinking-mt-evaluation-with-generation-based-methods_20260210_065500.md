---
ver: rpa2
title: 'LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods'
arxiv_id: '2505.16129'
source_url: https://arxiv.org/abs/2505.16129
tags:
- evaluation
- translation
- language
- zhang
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the direct scoring approach for using large
  language models (LLMs) in machine translation quality estimation (MTQE). Instead
  of prompting LLMs to assign numeric scores, the authors propose a generation-based
  method: using decoder-only LLMs to generate high-quality reference translations,
  then measuring semantic similarity between these references and machine translations
  using sentence embeddings.'
---

# LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods

## Quick Facts
- **arXiv ID**: 2505.16129
- **Source URL**: https://arxiv.org/abs/2505.16129
- **Reference count**: 14
- **Primary result**: Generation-based LLM evaluation outperforms direct scoring and reference-free MTME metrics, especially on low-resource language pairs.

## Executive Summary
This paper challenges the direct scoring approach for using large language models (LLMs) in machine translation quality estimation (MTQE). Instead of prompting LLMs to assign numeric scores, the authors propose a generation-based method: using decoder-only LLMs to generate high-quality reference translations, then measuring semantic similarity between these references and machine translations using sentence embeddings. Evaluated across 8 LLMs and 8 language pairs, this method consistently outperforms both LLM-based direct scoring baselines and reference-free MTME metrics, especially on low-resource pairs. The approach is more stable, interpretable, and flexible, and supports a shift toward hybrid evaluation paradigms combining fluent generation with semantic assessment.

## Method Summary
The method generates high-quality reference translations from source sentences using decoder-only LLMs, then computes semantic similarity between these references and machine translations using sentence embeddings. The process involves: (1) generating a reference translation from the source using an LLM, (2) extracting sentence embeddings for both the generated reference and machine translation using all-mpnet-base-v2, and (3) computing cosine similarity as the quality score. The approach avoids the instability of direct LLM scoring while leveraging LLMs' generation strengths.

## Key Results
- Generation-based evaluation consistently outperforms LLM direct scoring baselines across 8 language pairs
- The method achieves higher correlation with human judgments than reference-free MTME metrics, particularly on low-resource pairs like UK-EN
- Decoder-only LLMs produce more reliable quality estimates when generating text than when assigning numeric scores

## Why This Works (Mechanism)

### Mechanism 1: Training Objective Alignment
Decoder-only LLMs produce more reliable outputs when generating text than when assigning numeric scores because they are trained via next-token prediction, making translation generation directly aligned with their optimization objective. Direct scoring requires regression-like behavior, which conflicts with the autoregressive training paradigm.

### Mechanism 2: Semantic Similarity as Quality Proxy
Similarity between LLM-generated reference and machine translation correlates better with human judgment than direct LLM scoring because sentence embeddings capture meaning overlap in a continuous space, avoiding the discretized integer clusters LLMs tend to output.

### Mechanism 3: Low-Resource Language Generalization
Generation-based evaluation shows stronger gains on low-resource language pairs compared to existing metrics because LLMs trained on multilingual corpora can transfer translation competence across languages, while reference-free metrics lack supervision signals for underrepresented pairs.

## Foundational Learning

- **Sentence Embeddings & Semantic Similarity**: Why needed here: The method relies on embedding models to convert text into vectors where similarity approximates semantic equivalence. Quick check: Can you explain why cosine similarity between two sentence embeddings might correlate with human judgments of meaning overlap?

- **Decoder-Only LLM Architecture**: Why needed here: Understanding why these models excel at generation but struggle with scoring informs the method's design rationale. Quick check: What is the training objective of decoder-only models, and how does it differ from regression tasks?

- **Segment-Level vs. System-Level Correlation**: Why needed here: The paper critiques prior work for low segment-level correlation; understanding this distinction is critical for interpreting results. Quick check: Why might a metric correlate well at the system level but poorly at the segment level?

## Architecture Onboarding

- **Component map**: Source sentence -> Decoder-only LLM (generator) -> Generated reference -> Sentence-BERT embedding model -> Cosine similarity comparator -> Quality score

- **Critical path**: 1. Prompt engineering for reference generation, 2. Reference translation quality, 3. Embedding extraction for both reference and MT output, 4. Similarity computation â†’ correlation evaluation against human DA scores

- **Design tradeoffs**: Larger models do not guarantee better results (Llama-3-8B outperformed Llama-2-13B with fewer failures); prompt specificity trades off against generality; English-only target language in experiments limits multilingual claims

- **Failure signatures**: Direct scoring baseline failures with invalid outputs and repeated integer scores; low correlation on specific pairs; MTME reference-free metrics showing near-zero correlations on UK-EN

- **First 3 experiments**: 1. Baseline reproduction: Implement GEMBA-style direct scoring on 2-3 language pairs to verify segment-level correlation, 2. Generation-based pipeline: Build three-step pipeline using smaller model (Gemma-7B or Llama-3-8B) on one high-resource and one low-resource pair, 3. Ablation on embedding model: Swap all-mpnet-base-v2 for alternative sentence encoder to test impact on correlation

## Open Questions the Paper Calls Out
- **Question**: How effective is the generation-based evaluation paradigm when the target language is non-English, particularly for low-resource or morphologically rich languages?
- **Basis in paper**: The authors explicitly state in the Limitation section that they encourage future research to apply the method to a broader range of target languages.
- **Why unresolved**: The study restricted all experimental targets to English to control for confounding variables, leaving the method's performance on other target languages unknown.

- **Question**: Can the generation-based approach surpass the performance of state-of-the-art reference-based metrics like COMET-22?
- **Basis in paper**: The paper focuses on outperforming reference-free metrics, but data suggests reference-based metrics often retain higher correlation scores.
- **Why unresolved**: The authors argue reference-based metrics belong to a "different paradigm" and focus their main results on reference-free comparisons.

- **Question**: Is the accuracy of the evaluation method upper-bounded by the LLM's own translation capability for a given language pair?
- **Basis in paper**: The methodology relies on the assumption that the LLM produces a "high-quality reference," but does not analyze failure cases where the LLM itself produces low-quality translations.
- **Why unresolved**: The paper does not isolate the impact of the generator's quality on the final evaluation score.

## Limitations
- Experiments restricted to English target language, limiting multilingual generalization claims
- True low-resource languages not tested; medium-resource pairs used instead
- Embedding model quality assumption untested in external validation
- Decoding parameters unspecified, potentially affecting reproducibility

## Confidence

- **High confidence**: Generation-based method outperforms LLM direct scoring baselines
- **Medium confidence**: Generation-based method outperforms reference-free MTME metrics
- **Low confidence**: Claims of low-resource generalization

## Next Checks

1. **Embedding Model Ablation**: Replace all-mpnet-base-v2 with an alternative multilingual sentence encoder (e.g., LaBSE) across all language pairs to determine if embedding choice drives performance differences.

2. **Target Language Generalization**: Extend the generation-based pipeline to produce non-English references (e.g., German, Russian) for the same MT outputs, measuring whether the correlation advantage persists when both reference and MT output share the same non-English target language.

3. **True Low-Resource Validation**: Apply the method to genuinely low-resource pairs (e.g., Nepali-English with fewer than 100k sentence pairs in pretraining data) to test whether the claimed generalization advantage holds beyond the medium-resource languages examined.