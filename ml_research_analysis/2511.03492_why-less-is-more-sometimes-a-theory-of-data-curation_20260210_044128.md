---
ver: rpa2
title: 'Why Less is More (Sometimes): A Theory of Data Curation'
arxiv_id: '2511.03492'
source_url: https://arxiv.org/abs/2511.03492
tags:
- data
- pruning
- where
- error
- curation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theoretical framework explaining when "less
  is more" in machine learning through strategic data curation. The authors analyze
  high-dimensional binary classification where an imperfect oracle selectively prunes
  training examples based on difficulty and correctness.
---

# Why Less is More (Sometimes): A Theory of Data Curation

## Quick Facts
- **arXiv ID**: 2511.03492
- **Source URL**: https://arxiv.org/abs/2511.03492
- **Reference count**: 40
- **Primary result**: Theoretical framework showing when "less is more" in ML through strategic data curation, with phase transitions tied to data abundance, generator quality, and oracle reliability.

## Executive Summary
This paper develops a theoretical framework explaining when "less is more" in machine learning through strategic data curation. The authors analyze high-dimensional binary classification where an imperfect oracle selectively prunes training examples based on difficulty and correctness. They derive exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing sharp phase transitions tied to dataset size, label quality, and oracle reliability. The core insight is that optimal pruning strategy depends on the interplay between generator quality (ρ), pruner quality (ρ*), and their alignment (ρg). When the generator is excellent (ρ→1) and data is abundant, aggressively pruning and keeping only hard examples outperforms using the full dataset. Conversely, when the generator is poor (ρ<1), keeping easy examples helps the model learn the basic data distribution. The authors validate these predictions through extensive experiments on ImageNet and show their framework explains contradictory findings in LLM mathematical reasoning tasks.

## Method Summary
The paper analyzes high-dimensional binary classification where an imperfect oracle prunes training examples based on difficulty and correctness. The theoretical framework operates in the proportionate scaling limit (n,d→∞ with d/n→ϕ∈(0,∞)), using isotropic Gaussian data with labels y = sign(x^T w_g). The authors derive exact scaling law curves for test error under label-agnostic (keeping examples based on margin difficulty) and label-aware (filtering both correctness and difficulty) curation rules. They prove sharp phase transitions occur based on the interplay between generator quality ρ, pruner quality ρ*, and their alignment ρg. The framework is validated through synthetic experiments and ImageNet-1K with ViT-B/16 architectures.

## Key Results
- Derives exact scaling laws for test error under strategic data curation, revealing sharp phase transitions tied to dataset size, label quality, and oracle reliability
- Proves "keep hard" strategy minimizes error when generator quality ρ→1 and data is abundant, while "keep easy" minimizes error when ρ<1
- Shows principled curation can mitigate model collapse in iterative self-training by preventing catastrophic degradation from noisy synthetic data
- Explains contradictory findings in LLM mathematical reasoning tasks through the lens of generator and pruner quality alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal pruning strategy depends critically on generator quality (ρ) relative to the target task distribution.
- Mechanism: When generator quality ρ→1 (strong generator) and pruner quality ρ*→1, keeping only hard examples (small-margin) uniquely minimizes test error because the model already captures the basic distribution and benefits from boundary refinement. When ρ<1 (weak generator), keeping easy examples (large-margin) helps the model learn foundational structure before tackling harder cases.
- Core assumption: High-dimensional binary classification in the proportionate scaling limit; symmetric pruning functions; isotropic covariance for simplified presentation.
- Evidence anchors:
  - [abstract] "When the generator is excellent (ρ→1) and data is abundant, aggressively pruning and keeping only hard examples outperforms using the full dataset. Conversely, when the generator is poor (ρ<1), keeping easy examples helps the model learn the basic data distribution."
  - [section] Theorem 2 (Optimal Pruning Strategy): Part (A) shows "keep hard" minimizes error when ρ→1, ρ*→1; Part (B) shows "keep easy" minimizes error when ρ<1, ρ*→1.
  - [corpus] Weak corpus signal on this specific mechanism; related work on synthetic data curation exists but lacks this theoretical formulation.
- Break condition: Pruner quality ρ* is poor (ρ*≪1), or data is severely limited (small-n regime), or asymmetric pruning functions violate Assumption 1.

### Mechanism 2
- Claim: "Less is more" only applies in a specific regime—when data is abundant AND the generator is strong. Otherwise, "more is more" remains optimal.
- Mechanism: The interaction between data scale (n) and generator quality (ρ) creates four regimes. Only the large-n + strong-generator quadrant shows optimal error at aggressive pruning fractions (p≪1). In the other three quadrants (small-n with any generator quality, or large-n with weak generator), test error is minimized at p=1 (full dataset).
- Core assumption: Sufficient data abundance exists for the strong generator's advantage to manifest; the pruning oracle has positive projection along the generator direction (ρg>0).
- Evidence anchors:
  - [abstract] "The authors derive exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing sharp phase transitions tied to dataset size, label quality, and oracle reliability."
  - [section] Figure 1 caption: "The bottom-left quadrant shows the crucial exception: only when data is abundant and the generator is strong does the 'less is more' principle apply."
  - [corpus] Neighbor "Escaping Collapse" (arxiv 2502.08924) discusses synthetic data quality without this regime-based framework.
- Break condition: Data scarcity dominates (small-n regime), or generator-Oracle alignment ρg approaches zero or becomes negative.

### Mechanism 3
- Claim: Principled curation mitigates model collapse by selectively retaining examples that preserve alignment with the ground-truth distribution.
- Mechanism: Under label shift (wg≠w*), iterative self-training on uncurated pseudo-labels causes performance degradation. "Keep hard" curation stabilizes performance by retaining examples where the model's predictions are least confident but correct (per the oracle), preventing error accumulation. Theoretical support comes from the regression analysis showing pruning provably reduces test error when ∥w*−w//g∥<∥w*−wg∥<∥w*−w⊥g∥.
- Core assumption: Oracle has access to ground-truth alignment (ρ*→1); label-aware curation can filter both correctness and difficulty.
- Evidence anchors:
  - [abstract] "A key practical outcome is that principled curation can mitigate model collapse by preventing iterative self-training on noisy synthetic data from catastrophic degradation."
  - [section] Figure 3: "Strategic pruning prevents model collapse. Over multiple rounds of pseudo-labeling, training on all examples leads to performance degradation. In contrast, selectively training on only hard, valid examples consistently preserves performance."
  - [corpus] Neighbor "Escaping Collapse" (arxiv 2502.08924) addresses similar collapse concerns without the theoretical phase-transition framework.
- Break condition: Oracle quality ρ* is poor (incorrect filtering), or no verification signal exists for label-aware curation.

## Foundational Learning

- Concept: **High-dimensional proportionate scaling limit** (d,n→∞ with d/n→ϕ)
  - Why needed here: All theoretical results (Theorems 1-5) are derived in this asymptotic regime where random matrix theory applies and exact error formulas become tractable.
  - Quick check question: Can you explain why the parametrization rate ϕ=d/n matters for generalization in overparameterized vs. underparameterized regimes?

- Concept: **Stieltjes transform and Marchenko-Pastur law**
  - Why needed here: Theorem 1 uses the Stieltjes transform of a "deformed" Marchenko-Pastur law to characterize the limiting spectral density of the pruned data covariance, which directly determines test error.
  - Quick check question: What is the Stieltjes transform of a probability measure, and how does it relate to resolvent matrices?

- Concept: **Bias-variance decomposition with label shift correction**
  - Why needed here: The regression analysis (Theorem 4, Proposition 3) extends classical bias-variance decomposition to handle label shift (wg≠w*), adding correction terms c²−2λE[wg⊤RΣϵ].
  - Quick check question: How does label shift modify the standard bias-variance tradeoff in ridge regression?

## Architecture Onboarding

- Component map:
  - **Generator** (wg, Cg): Produces training data with labels y=sign(x⊤wg); quality measured by ρ=cos(angle between wg and w*)
  - **Pruner/Oracle** (wo): Filters examples based on difficulty (via q) and correctness (label-aware); quality measured by ρ*=cos(angle between wo and w*)
  - **Learner**: Ridge-regularized linear classifier minimizing Eq. 2; outputs ŵ=RX⊤DY/n
  - **Curation rules**: Label-agnostic (Eq. 5: pi=q(xi⊤wo)) vs. label-aware (Eq. 6: pi=1 iff yi=yo_i AND q(xi⊤wo)=1)

- Critical path:
  1. Estimate generator quality ρ for your task slice (e.g., base model accuracy on representative samples)
  2. Estimate data abundance relative to problem dimensionality (effective ϕ)
  3. If ρ→1 AND large-n: apply "keep hard" curation (retain small-margin examples)
  4. If ρ<1 OR small-n: apply "keep easy" curation (retain large-margin examples) or use full dataset
  5. For iterative self-training: use label-aware "keep hard" at each round to prevent collapse

- Design tradeoffs:
  - **Aggressive pruning (small p)**: Lower compute cost, but only beneficial in strong-generator + large-n regime; risks losing diversity
  - **Keep-hard vs. keep-easy**: Keep-hard refines strong models; keep-easy stabilizes weak models—but both require oracle access to margin/difficulty scores
  - **Label-agnostic vs. label-aware**: Label-aware (filtering correctness) is more powerful but requires verification signal; label-agnostic is simpler but noisier

- Failure signatures:
  - Pruning aggressively in small-n regime: test error increases monotonically as p decreases
  - Applying "keep hard" with weak generator: model fails to learn basic distribution, error remains high
  - Model collapse in iterative training: performance degrades across rounds when using uncurated pseudo-labels (Figure 3, "all data" curve)
  - Poor oracle (ρ*≪1): curation removes correct examples or retains incorrect ones, degrading performance regardless of strategy

- First 3 experiments:
  1. **Regime validation on synthetic data**: Replicate Figure 1 by varying n (100 vs. 5000), ρ (0.3 vs. 1.0), and pruning strategy ("keep hard" vs. "random"); confirm phase transition occurs only in large-n + strong-generator quadrant
  2. **Ablation on pruning fraction p**: For a fixed task with known generator quality, sweep p∈[0.1, 1.0] and plot test error; verify optimal p shifts based on ρ as theory predicts
  3. **Collapse prevention test**: Simulate 3-5 rounds of self-training with pseudo-labels; compare "all data" vs. "keep hard + label-aware" curation; measure error trajectory across rounds to confirm stabilization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the theoretical framework generalize to non-linear models such as random-feature regimes, kernel methods, or infinite-width neural tangent kernels?
- Basis in paper: [explicit] The authors state: "Extending the theory to random-feature and kernel regimes—or to the infinite-width neural tangent kernel—would bridge the gap to practical deep learning architectures."
- Why unresolved: The current theory relies on exact analysis of linear models in the high-dimensional limit; extending to non-linear models introduces significant mathematical complexity and may require different analytical tools.
- What evidence would resolve it: A theoretical derivation of scaling laws and phase transitions for non-linear models under data curation, validated against experiments on realistic deep networks.

### Open Question 2
- Question: How do iterative, adaptive curation loops (with re-scoring and re-training) affect the phase transitions and stability guarantees derived for one-shot pruning?
- Basis in paper: [explicit] The authors note: "Incorporating iterative re-scoring and re-training would capture the feedback dynamics used in modern self-distillation and RLHF pipelines."
- Why unresolved: The current framework analyzes a single pruning step; iterative feedback may introduce complex dynamics not captured by static theory, potentially altering the conditions where "less is more" holds.
- What evidence would resolve it: Theoretical analysis of test error trajectories under iterative curation, with empirical validation on self-distillation or RLHF workflows showing whether the phase transitions shift or new equilibria emerge.

### Open Question 3
- Question: How robust are the derived phase transitions to non-isotropic covariances and realistic covariate shift between the generator and ground-truth distributions?
- Basis in paper: [inferred] The main results focus on the isotropic setting (Cg = Σ = Id), with general anisotropic cases deferred to the appendix. Real-world data often exhibits structured covariance.
- Why unresolved: The complexity of anisotropic settings may qualitatively change the optimal pruning strategy or the location of phase boundaries, but this is not fully characterized in the main analysis.
- What evidence would resolve it: Explicit theoretical and empirical analysis showing how the phase transition curves (e.g., optimal p vs. ρ) change under various non-isotropic covariance structures and covariate shifts.

### Open Question 4
- Question: Can the framework's parameters (generator quality ρ, pruner quality ρ*, alignment ρg) be reliably estimated in practice for LLM reasoning tasks to predict optimal curation strategies?
- Basis in paper: [inferred] The theory relies on knowing these quality parameters to determine when to use "keep hard" vs. "keep easy" strategies. However, the paper does not address how to estimate them from real data.
- Why unresolved: Without practical estimation methods, practitioners cannot apply the theoretical conditions directly to new tasks or models.
- What evidence would resolve it: A methodology for estimating these parameters from observable quantities (e.g., agreement rates, loss curves) and validation showing that estimated parameters correctly predict optimal pruning strategies across diverse tasks.

## Limitations

- The theoretical framework assumes isotropic Gaussian data and linear models, which may not fully capture the complexity of modern deep learning architectures and real-world data distributions
- The alignment parameters ρ, ρ*, and ρg are difficult to estimate precisely in practice, especially for complex tasks
- The "keep hard" strategy assumes access to a reliable oracle that can assess difficulty and correctness, which may not be available in all scenarios
- The phase transition predictions may be less sharp in finite-dimensional settings or with non-linear models

## Confidence

- **High confidence**: The core theoretical results (Theorems 1-5) and the synthetic experiments demonstrating phase transitions are well-established and reproducible. The framework's explanation of model collapse is also strongly supported.
- **Medium confidence**: The ImageNet validation experiments and the LLM reasoning task applications are promising but less thoroughly validated. The practical guidelines for estimating alignment parameters need further empirical testing.
- **Low confidence**: The exact behavior in intermediate regimes (between the four quadrants) and the performance with non-linear models remain open questions.

## Next Checks

1. **Real-world alignment estimation**: Develop and validate a practical protocol for estimating generator quality ρ and pruner quality ρ* on ImageNet or similar datasets without relying on synthetic alignment parameters
2. **Deep learning generalization**: Test whether the "less is more" predictions hold for deep neural networks (e.g., ResNet, ViT) beyond linear classifiers, examining if phase transitions persist or shift
3. **Non-isotropic data**: Validate the framework's predictions on non-Gaussian, structured data (e.g., natural images with spatial correlations) to assess robustness beyond the isotropic assumption