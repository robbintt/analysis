---
ver: rpa2
title: Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning
arxiv_id: '2601.15086'
source_url: https://arxiv.org/abs/2601.15086
tags:
- memory
- agents
- agent
- information
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of memory rewriting in reinforcement
  learning (RL) agents, focusing on how agents can selectively update their memory
  when environmental conditions change. The authors introduce two benchmark tasks,
  Endless T-Maze and Color-Cubes, which require agents to continually overwrite outdated
  memory information rather than just retain it.
---

# Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2601.15086
- **Source URL**: https://arxiv.org/abs/2601.15086
- **Reference count**: 40
- **Primary result**: LSTM with learnable forget gates outperforms other architectures in memory rewriting tasks

## Executive Summary
This paper challenges the assumption that memory retention alone is sufficient for reinforcement learning agents to master memory-dependent tasks. The authors demonstrate that successful performance requires not just remembering information, but actively rewriting outdated memory when environmental conditions change. Through two novel benchmark tasks (Endless T-Maze and Color-Cubes), they show that recurrent architectures with explicit forgetting mechanisms, particularly LSTM, significantly outperform transformer-based and structured memory systems in scenarios requiring continual memory updates.

## Method Summary
The study introduces two benchmark tasks designed to test memory rewriting capabilities: Endless T-Maze requires agents to follow cues that change over time, while Color-Cubes demands updating a spatial map when cube positions change. Five memory architectures are evaluated: PPO-LSTM, PPO-GRU, GTrXL (transformer), SHM, and FFM (structured memory). Agents are trained and tested under varying conditions including fixed vs. stochastic timing and partial observability. Performance is measured through success rates across different task configurations.

## Key Results
- PPO-LSTM achieves near-perfect performance across all Endless T-Maze configurations, while other architectures show significant degradation
- Transformer-based agents (GTrXL) struggle with sparse rewards and memory rewriting, performing poorly in uniform timing conditions
- Structured memory systems (SHM, FFM) show limited flexibility, failing completely in stochastic timing regimes
- Learnable forgetting gates are crucial for effective memory rewriting, as confirmed by ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Forgetting via Learnable Gates
- Claim: Agents with explicit, learnable forget gates (e.g., LSTM) appear better equipped to selectively discard obsolete cues than those with fixed decay rules or static caches.
- Mechanism: The forget gate in LSTM units allows the network to learn a context-dependent attenuation of the internal cell state. This facilitates "overwriting" by resetting specific memory dimensions when new observations signal a change in task phase, rather than relying on rigid temporal decay or accumulation.
- Core assumption: The agent's optimization process successfully learns to trigger the forget gate specifically when environmental cues invalidate prior instructions.
- Evidence anchors:
  - [abstract] "recurrent models with explicit forgetting mechanisms (e.g., PPO-LSTM) outperform others, demonstrating superior flexibility."
  - [section 7] "A comparison of PPO-RNN... with PPO-GRU and PPO-LSTM... indicates that gating mechanisms have a key influence on the success of rewriting tasks."
  - [corpus] The "FadeMem" paper (neighbor) supports the utility of biologically-inspired forgetting for mitigating information overload, aligning with the need for selective attenuation.
- Break condition: If the environment requires accumulating all historical cues rather than replacing them (e.g., keeping a running tally), a mechanism designed primarily for "overwriting" may discard critical long-term data.

### Mechanism 2: Generalization via Stochastic Timing
- Claim: Robustness in memory rewriting correlates with the ability to handle stochastic variations in event timing (when to rewrite), not just event detection.
- Mechanism: When corridor lengths are sampled uniformly (variable delay between cue and decision), the agent cannot rely on fixed-horizon heuristics. It must condition the memory update on the observation of the *new cue itself*, forcing a content-dependent rewrite strategy that generalizes across temporal delays.
- Core assumption: The agent learns to identify the *arrival* of a new cue as the trigger for rewriting, independent of the time-step count since the last action.
- Evidence anchors:
  - [section 6, RQ3] "PPO-LSTM... achieving complete success regardless of the number of corridors... FFM and SHM... significant performance degradation in uniform settings."
  - [table 1] Shows PPO-LSTM maintaining ~1.00 success in Uniform regimes where SHM/FFM drop to 0.00.
  - [corpus] "Continual Knowledge Adaptation" (neighbor) discusses non-stationary environments, supporting the need for mechanisms that handle changing conditions beyond fixed patterns.
- Break condition: If the training distribution is narrow (e.g., fixed lengths only), the agent may overfit to specific timing, failing to trigger the rewrite when delays vary at test time.

### Mechanism 3: State Inference under Partial Observability
- Claim: Complex memory rewriting requires maintaining a structured belief state (e.g., a map) that supports logical inference when observations are incomplete.
- Mechanism: In "Extreme" settings, agents receive position updates without color labels. To succeed, the agent must infer which cube moved by comparing the partial update against its internal map, updating the specific association without corrupting the rest.
- Core assumption: The memory architecture can encode and maintain relational data (color ↔ position) rather than just a single feature vector.
- Evidence anchors:
  - [section 4] "Extreme... incomplete teleportation updates (positions only, colors hidden). The agent must infer which cube moved... testing both rewriting and inference."
  - [section 6, RQ4] "All baselines showed zero success... indicating that their memory mechanisms are not sensitive to tasks where simply forgetting information does not help."
  - [corpus] Evidence is weak here; neighbor papers focus more on retention or general adaptation rather than the specific mechanism of logical inference for memory updating.
- Break condition: If the agent's memory capacity is too small to store the full state-space (e.g., all cube positions), or if the architecture lacks relational inductive bias, inference fails.

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper assumes the agent cannot see the full state (e.g., the current cue disappears after one step). Understanding POMDPs explains why "memory" is required at all—to reconstruct the hidden state from history.
  - Quick check question: Can an agent solve a POMDP using only the current observation $o_t$? (Answer: No, it requires history or a sufficient statistic $m_t$).

- **Concept**: LSTM Gating Dynamics (Input, Output, Forget)
  - Why needed here: The results hinge on the "forget gate" being superior to other update methods. You must understand how sigmoid gates selectively scale information flow to grasp why LSTMs handle "rewriting" better than RNNs or Transformers.
  - Quick check question: In an LSTM, if the forget gate outputs values near 0, what happens to the cell state? (Answer: It is erased/reset, facilitating the "overwriting" of old memory).

- **Concept**: Interpolation vs. Extrapolation in RL
  - Why needed here: The paper evaluates generalization by testing agents on corridor lengths/counts different from training. Distinguishing between interpolating (within training range) and extrapolating (beyond it) is critical for diagnosing the failure modes of structured memory agents like SHM.
  - Quick check question: If an agent is trained on corridors of length 5 and tested on length 10, is this interpolation or extrapolation?

## Architecture Onboarding

- **Component map**: Environment -> Memory Module (LSTM/Transformer/SHM/FFM) -> Policy/Critic (PPO) -> Action

- **Critical path**:
  1. Environment emits partial observation (e.g., new cue)
  2. Memory Module receives observation + previous state
  3. **Rewrite Step**: The mechanism (Gate/Attention) updates $m_t \to m_{t+1}$
  4. Policy uses $m_{t+1}$ to select action
  5. Reward signal updates the Memory Module's weights (learning *how* to rewrite)

- **Design tradeoffs**:
  - **LSTM**: High flexibility in *when* to forget (learnable gate), but potentially limited memory horizon (vanishing gradients over very long sequences)
  - **Transformer (GTrXL)**: Excellent at retaining long context (attention), but lacks an explicit "delete" operation, leading to interference from stale cached states (suffers in sparse reward/rewrite tasks)
  - **Structured Memory (FFM/SHM)**: Interpretable and stable for retention, but "rigid" update rules (e.g., fixed decay in FFM) fail when rewrite timing is unpredictable (Uniform regime)

- **Failure signatures**:
  - **Stuck in the Past**: Agent acts on old cues despite new observations (Transformer/SHM failure mode)
  - **Random Guessing**: Agent achieves ~50% success (random chance) even on simple retention tasks (GTrXL in sparse reward settings)
  - **Catastrophic Overwriting**: Agent updates memory too aggressively, losing necessary context (theoretical risk, though the paper highlights the opposite problem: failure to overwrite)

- **First 3 experiments**:
  1. **Baseline Validation**: Run PPO-MLP vs. PPO-LSTM on T-Maze $n=1$ (Retention) vs. $n=3$ (Rewriting) to confirm that MLP fails specifically when rewriting is required.
  2. **Timing Sensitivity Stress Test**: Train FFM and PPO-LSTM on Fixed corridor length $l=10$ and test immediately on Uniform $l \sim U[1, 10]$. Observe the performance gap to quantify "rigidity" vs. "flexibility."
  3. **Ablation on Gates**: Replace the LSTM's forget gate with a constant value (e.g., 1.0 = no forgetting) to verify if performance on Endless T-Maze drops, isolating the causal mechanism of the gate itself.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can current or novel memory architectures be adapted to solve the Color-Cubes Medium and Extreme modes, where all evaluated baselines currently achieve zero success?
- Basis in paper: [explicit] The authors note in the results (Table 2) and discussion that "all baselines showed zero success" on these modes, suggesting their mechanisms are "not sensitive to tasks where simply forgetting information does not help."
- Why unresolved: While agents mastered Endless T-Maze (single-cue rewriting), they failed completely when required to dynamically re-rank relevance and infer hidden state changes in multi-object environments.
- What evidence would resolve it: An architecture achieving a success rate significantly greater than 0% on the Color-Cubes Extreme benchmark, particularly one capable of logical inference alongside memory updates.

### Open Question 2
- Question: What specific architectural components are necessary to construct a dedicated "memory rewriting" mechanism that outperforms general-purpose recurrent gating?
- Basis in paper: [explicit] The Limitations & Future Work section states: "A promising direction for further work is the development of a proprietary mechanism that specifically addresses the task of memory rewriting."
- Why unresolved: The study evaluates general architectures (LSTMs, Transformers, etc.) and finds success correlates with implicit forgetting (LSTM gates), but no specialized "rewrite" module has been proposed or tested.
- What evidence would resolve it: The design and validation of a novel memory module with explicit read/write/forget operations that demonstrates superior sample efficiency or generalization compared to PPO-LSTM on the proposed benchmarks.

### Open Question 3
- Question: Can transformer-based agents be stabilized to handle memory rewriting under partial observability without relying on fixed-length context caching?
- Basis in paper: [inferred] The paper identifies that GTrXL fails in Endless T-Maze (uniform regime) and Color-Cubes due to a lack of explicit forgetting mechanisms and instability in sparse reward settings, exposing a fundamental limitation of the architecture.
- Why unresolved: The paper demonstrates the failure of the status quo (attention/caching) but does not propose a method to inject adaptive forgetting into transformer-based RL agents.
- What evidence would resolve it: Modifications to the GTrXL architecture (e.g., hybrid gating or adaptive attention decay) that allow it to match or exceed PPO-LSTM performance on the uniform Endless T-Maze benchmark.

## Limitations

- Results are primarily validated in two synthetic benchmarks; generalization to more complex, real-world tasks with richer state representations remains untested
- The sparse reward setting, while valuable for stress-testing, may not reflect typical RL deployment scenarios where rewards are denser
- The study focuses on episodic tasks without examining whether learned rewriting strategies transfer between episodes

## Confidence

- **High**: LSTM superiority in rewriting tasks, importance of learnable gates
- **Medium**: Generalization results across different corridor lengths
- **Low**: Claims about biological plausibility and cognitive mapping

## Next Checks

1. Test LSTM performance against Transformer variants with explicit deletion mechanisms (e.g., reversible layers with masking)
2. Evaluate transfer learning by training on one maze configuration and testing on structurally different layouts
3. Implement curriculum learning to gradually increase rewriting difficulty and measure learning curves for different architectures