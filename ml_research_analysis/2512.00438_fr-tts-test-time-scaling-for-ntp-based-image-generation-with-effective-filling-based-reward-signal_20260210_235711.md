---
ver: rpa2
title: 'FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based
  Reward Signal'
arxiv_id: '2512.00438'
source_url: https://arxiv.org/abs/2512.00438
tags:
- reward
- generation
- image
- samples
- filling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying test-time scaling
  (TTS) to next-token prediction (NTP)-based image generation models, where intermediate
  samples are incomplete in scale and semantic content, making reward evaluation unreliable.
  The authors propose a Filling-Based Reward (FR) mechanism that estimates the quality
  of intermediate samples by filling ungenerated tokens with randomly selected tokens
  from the already generated portion, then selecting the highest reward among multiple
  random fillings.
---

# FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal

## Quick Facts
- arXiv ID: 2512.00438
- Source URL: https://arxiv.org/abs/2512.00438
- Authors: Hang Xu; Linjiang Huang; Feng Zhao
- Reference count: 39
- Primary result: Introduces Filling-based Reward (FR) mechanism that estimates intermediate sample quality in NTP-based image generation, achieving up to 0.74 overall score on TIIF-Bench vs. 0.68 for Best-of-N baseline

## Executive Summary
This paper addresses the challenge of applying test-time scaling to next-token prediction (NTP) image generation models, where intermediate samples are incomplete and semantic content is sparse, making reward evaluation unreliable. The authors propose a Filling-Based Reward mechanism that fills ungenerated tokens with blocks from the already generated portion, then selects the highest reward among multiple random fillings. Building on this, FR-TTS combines FR with a diversity reward and dynamic weighting schedule, significantly outperforming baseline Best-of-N strategies across multiple reward models and benchmarks.

## Method Summary
FR-TTS introduces a Filling-Based Reward (FR) mechanism for NTP-based image generation, where intermediate samples are incomplete. The method fills ungenerated tokens by copying blocks from the generated portion, decodes to pixel space, and scores with reward models. Multiple filling trials are performed, selecting the highest reward as a proxy for trajectory quality. A coarse-to-fine search optimizes the filling scheme efficiently. A unified reward combines FR with a diversity reward, weighted dynamically based on generation progress. The system filters inferior samples at each step, maintaining diversity while improving quality.

## Key Results
- FR-TTS achieves 0.74 overall score on TIIF-Bench vs. 0.68 for Best-of-N baseline
- Spearman correlation between intermediate FR and final rewards reaches 0.93 vs. 0.55 for zero-padding
- 24-50% inference time overhead compared to standard generation
- Superior performance across multiple reward models (HPSv3, ImageReward, ClipScore, AestheticScore, HPSv2)

## Why This Works (Mechanism)

### Mechanism 1: Filling-Based Reward (FR) as a Trajectory Quality Proxy
Randomly filling ungenerated tokens with blocks from the generated portion creates complete image states that better correlate with final rewards than zero-padding. The maximum reward across multiple filling trials serves as a robust proxy for trajectory quality, assuming high-reward fillings indicate compatible semantic content in the existing tokens.

### Mechanism 2: Coarse-to-Fine Search for Efficiency
Finding optimal filling schemes via pure random sampling is computationally prohibitive. The hybrid search strategy uses Best-of-N for initial random sampling, then refines with ZeroOrder optimization by refilling small portions of the best scheme to improve the reward.

### Mechanism 3: Dynamic Reward Unification (Diversity vs. Quality)
Early generation steps lack semantic density, making FR unreliable. A unified reward combines FR with a diversity reward (VGG distance to previous samples), with linear decay from 1.0 to 0.0 as generation progresses. Variance-adjusted weighting emphasizes diversity when FR scores are undifferentiated.

## Foundational Learning

- **Concept: Next-Token Prediction (NTP) vs. Next-Scale Prediction**
  - Why needed: NTP generates sequential tokens creating incomplete spatial scale, unlike diffusion which maintains full spatial dimensions
  - Quick check: Why does a reward model struggle to evaluate a half-generated NTP image compared to a half-denoised Diffusion image? (Answer: Diffusion maintains full spatial scale; NTP does not.)

- **Concept: Test-Time Scaling (TTS) & Best-of-N (BoN)**
  - Why needed: FR-TTS is a sophisticated extension of the standard Best-of-N baseline
  - Quick check: What is the standard baseline that FR-TTS attempts to beat? (Answer: Best-of-N sampling)

- **Concept: Correlation Coefficient as a Metric for Intermediate Guidance**
  - Why needed: The paper validates its method by showing higher correlation between intermediate filled rewards and final complete rewards
  - Quick check: Why is "correlation" specifically used to validate the Filling-Based Reward? (Answer: If intermediate scores don't correlate with final scores, they cannot guide early pruning effectively)

## Architecture Onboarding

- **Component map:** Base Generator -> Block-Wise Filler -> VQGAN Decoder -> Reward Models -> Search Optimizer -> Diversity Module -> Scheduler
- **Critical path:** The inference loop involves fill -> decode -> score operations inside each generation step before the next token is predicted, adding latency to every inference step
- **Design tradeoffs:** Block size affects chaos vs. repetition; filling trials impact quality vs. compute; dynamic schedule timing affects when to switch from diversity to quality focus
- **Failure signatures:** Visual repetition from large blocks, high latency with low correlation gains, mode collapse from insufficient early diversity
- **First 3 experiments:**
  1. Reproduce Figure 3a correlation benchmark comparing FR vs. ZeroPadding
  2. Ablate block size on validation set (K=1, 6, 12, 24) to find optimal size
  3. Plot Quality vs. Inference Time for Best-of-N vs. FR-TTS to quantify efficiency

## Open Questions the Paper Calls Out

### Open Question 1
Can the filling-based reward search be replaced or enhanced by a learned policy to reduce computational overhead? The authors note random filling is inefficient and seek to minimize the performance-consumption trade-off through coarse-to-fine search.

### Open Question 2
Is the optimal block size content-dependent and can it be dynamically adjusted? The ablation study identifies block size as sensitive but uses fixed sizes, leaving open whether adaptive sizing would yield better correlation.

### Open Question 3
Can FR mechanism adapt to non-raster-scan generation orders? The current approach assumes contiguous generated tokens based on standard NTP raster scan, but doesn't address scattered generation patterns common in other AR methods.

## Limitations
- FR effectiveness drops significantly for very early generation steps (<10% complete) where copied blocks lack sufficient semantic coherence
- Optimal dynamic weighting parameters appear tuned for specific models and may not generalize across different NTP architectures
- Coarse-to-fine search assumes locally smooth reward landscape; non-linear reward surfaces may limit neighborhood refinement benefits

## Confidence

**High Confidence:** Correlation experiments showing FR's superiority over zero-padding (Spearman up to 0.93 vs. 0.55)

**Medium Confidence:** TIIF-Bench score claims (0.74 vs. 0.68) given controlled experimental setup, though advantage may vary with model scale

**Low Confidence:** "Reasonable" inference time overhead claim - while 5 trials are reported, actual compute cost scaling is not thoroughly characterized across hardware

## Next Checks

1. **Early-Step Failure Analysis:** Systematically evaluate FR-TTS performance at 10%, 30%, 50%, and 80% completion to identify when filling-based rewards become reliable and whether diversity compensation is sufficient

2. **Cross-Dataset Generalization:** Test FR-TTS on non-standard tasks (medical imaging, scientific visualization) where early filling quality correlation with final image quality may differ from natural image benchmarks

3. **Compute-Cost Tradeoff Profiling:** Measure marginal quality improvement per additional filling trial and neighborhood refinement step to determine optimal compute allocation point where diminishing returns set in