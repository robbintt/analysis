---
ver: rpa2
title: 'From Code to Field: Evaluating the Robustness of Convolutional Neural Networks
  for Disease Diagnosis in Mango Leaves'
arxiv_id: '2512.13641'
source_url: https://arxiv.org/abs/2512.13641
tags:
- robustness
- blur
- noise
- corruption
- lcnn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the robustness of convolutional neural networks
  (CNNs) for mango leaf disease diagnosis under image corruptions. The authors create
  MangoLeafDB-C, a corrupted version of the MangoLeafDB dataset, by applying 19 types
  of synthetic distortions at five severity levels.
---

# From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves

## Quick Facts
- arXiv ID: 2512.13641
- Source URL: https://arxiv.org/abs/2512.13641
- Reference count: 15
- Primary result: LCNN achieves best corruption robustness (mCE=48.9) and clean accuracy (99.5%) for mango leaf disease classification under synthetic image corruptions.

## Executive Summary
This study evaluates the robustness of five CNN architectures for mango leaf disease diagnosis under synthetic image corruptions. The authors create MangoLeafDB-C by applying 19 types of distortions at five severity levels to the original dataset. Testing ResNet-50, ResNet-101, VGG-16, Xception, and a custom LCNN architecture reveals that deeper models suffer significant performance degradation despite high clean accuracy. LCNN, a lightweight architecture designed specifically for this task, outperforms all others in both clean accuracy and corruption robustness, particularly excelling at blur-type corruptions while maintaining Pareto optimality.

## Method Summary
The study evaluates five CNN architectures on MangoLeafDB-C, a corrupted version of the MangoLeafDB dataset containing 19 corruption types at five severity levels. Models are trained on clean data only using transfer learning with ImageNet weights (except LCNN). Evaluation metrics include macro F1-score, corruption error (CE), and relative mean corruption error (relative mCE). The training protocol uses 50 epochs, batch size 32, learning rate 0.001, and Adam optimizer. Performance is compared against a reference ResNet-101 baseline.

## Key Results
- LCNN achieves 99.5% clean accuracy with mCE of 48.9, outperforming all other architectures
- ResNet-101 shows 68.1% clean accuracy but poor corruption robustness (mCE≈100)
- Xception demonstrates high noise sensitivity with relative CE above 130 for multiple noise types
- VGG-16 provides balanced performance with 99.4% clean accuracy and mCE of 63.4
- All models experience severe degradation on impulse/Shot/Speckle Noise at severity 5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Problem-specific lightweight architectures can simultaneously achieve superior clean accuracy and corruption robustness compared to deeper, general-purpose models.
- Mechanism: Domain-specialized architectures constrain the hypothesis space to learn task-relevant structural features rather than overfitting to dataset-specific textures. LCNN's design for mango leaf disease appears to prioritize shape-based features that persist under blur and digital distortions, whereas ImageNet-pretrained models inherit texture biases that are disrupted by corruption.
- Core assumption: The LCNN architecture was intentionally designed for mango leaf classification, constraining learned features to those relevant for the specific disease patterns rather than general visual features.
- Evidence anchors:
  - [abstract] "LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE"
  - [section 4.3] "LCNN exhibits distinct Pareto optimality, with a maximum clean accuracy of 99.5% and an mCE of 48.9, placing it at the forefront of the Pareto front. This shows that problem-specific architectures can achieve superior accuracy and robustness simultaneously."
  - [corpus] Related work (arxiv 2507.06332) on attention-guided repair for CNN robustness supports that architectural choices fundamentally determine corruption sensitivity.

### Mechanism 2
- Claim: Increased architectural depth alone does not improve corruption robustness and may amplify sensitivity to certain perturbation types.
- Mechanism: Deeper networks trained exclusively on clean data learn fine-grained texture and high-frequency features that are easily disrupted by corruption. Without explicit corruption-aware training, additional layers propagate and amplify input perturbations rather than learning invariance.
- Core assumption: Models were trained only on clean MangoLeafDB data using standard transfer learning, without corruption augmentation or adversarial training during training.
- Evidence anchors:
  - [abstract] "Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions"
  - [section 4.3] "Comparison of ResNet-101 with ResNet-50 shows that while the deeper model slightly improves clean accuracy (68.1% compared to 71.1%), it does not significantly enhance robustness. Both models have a similar mCE (approximately 100)"
  - [corpus] Work on noisy data improving robustness (arxiv 2601.08043) suggests clean-data-only training creates vulnerability, supporting this mechanism.

### Mechanism 3
- Claim: Architectural inductive biases create distinct, predictable sensitivity profiles to specific corruption types—no architecture is universally robust.
- Mechanism: Design choices (depth, filter sizes, skip connections, depthwise separable convolutions) create architecture-specific feature extraction patterns. Xception's depthwise separable convolutions appear particularly noise-sensitive, while LCNN's shallow structure preserves spatial information under blur.
- Core assumption: Architectural differences create inherent biases in which features are learned and how they respond to perturbations.
- Evidence anchors:
  - [section 4.1 Table 2] LCNN ranks Pixelate as least damaging (rank 1, F1=0.9930) while identifying Contrast as most damaging (rank 19, F1=0.2347); ResNet-101 shows inverse sensitivity
  - [section 4.2] "Xception, despite a lower absolute mCE than ResNets, is particularly vulnerable/brittle to noise-based corruptions, with relative CEs above 130 for several types of noise"
  - [corpus] Spatial robustness benchmark (arxiv 2504.01632) confirms DNN robustness varies significantly by corruption type, supporting architecture-specific sensitivity.

## Foundational Learning

- Concept: **Corruption Error (CE) and Relative Mean Corruption Error (mCE)**
  - Why needed here: These standardized metrics enable cross-architecture robustness comparison. CE normalizes performance against a reference model (ResNet-101) across severity levels; relative mCE accounts for baseline clean performance to isolate robustness degradation. Without understanding these metrics, Tables 3-4 are uninterpretable.
  - Quick check question: If Model A achieves mCE=48.9 and Model B achieves mCE=100, which demonstrates better corruption robustness? (Answer: Model A—lower mCE indicates superior robustness.)

- Concept: **Transfer learning with domain-specific fine-tuning**
  - Why needed here: ResNet-50, ResNet-101, VGG-16, and Xception use ImageNet pre-trained weights with final-layer replacement for 8-class classification. Understanding that pre-trained features may carry texture biases from natural images (not agricultural imagery under adverse conditions) explains their corruption vulnerability.
  - Quick check question: Why might ImageNet-pretrained features transfer poorly to weather corruptions (frost, fog) on leaf imagery? (Answer: ImageNet lacks agricultural imagery under adverse weather conditions; learned features don't encode weather-invariance for this domain.)

- Concept: **Pareto optimality in accuracy-robustness tradeoffs**
  - Why needed here: Figure 4 visualizes the tradeoff space between clean accuracy and corruption robustness. LCNN's Pareto optimality means no other evaluated model simultaneously achieves both higher clean accuracy AND lower mCE—critical for deployment decisions where both metrics matter.
  - Quick check question: If a model is on the Pareto front, what does that imply about potential improvements? (Answer: Any improvement in one metric must come at cost to the other; the model represents optimal tradeoff among evaluated options.)

## Architecture Onboarding

- Component map:
  MangoLeafDB → MangoLeafDB-C generator → 5 CNN architectures (LCNN, ResNet-50, ResNet-101, VGG-16, Xception) → Evaluation pipeline → CE, mCE metrics

- Critical path:
  1. Generate MangoLeafDB-C using corruption script (severity 1-5 for all 19 types) — **bottleneck: 95× dataset expansion**
  2. Train each model on clean MangoLeafDB only (50 epochs, batch 32, lr=0.001, Adam optimizer)
  3. Evaluate trained models on all 95 corrupted subsets without retraining
  4. Compute CE normalized to ResNet-101 reference; aggregate to mCE and relative mCE
  5. Visualize clean accuracy vs. robustness tradeoff for architecture selection

- Design tradeoffs:
  - **LCNN**: Best mCE (48.9) and clean accuracy (99.5%) but weaker on weather corruptions (Fog rel. CE=275); requires custom architecture effort
  - **VGG-16**: Strong balance (clean 99.4%, mCE 63.4); simpler architecture but larger parameter count than LCNN
  - **ResNet-101/50**: Well-documented but poor robustness (mCE~100) and lowest clean accuracy (68.1%/67%)
  - **Xception**: Good clean accuracy (97.9%) but extreme noise sensitivity (rel. CE >130 for multiple noise types)

- Failure signatures:
  - **Noise collapse**: All models fail severely on Impulse/Shot/Speckle Noise at severity 5 (F1 < 0.46)
  - **Weather brittleness**: Xception shows extreme weather vulnerability (rel. CE >140 for Snow, Frost, Fog)
  - **Contrast inversion**: LCNN ranks Contrast as most damaging (rank 19) while ResNet ranks it mid-tier—suggests LCNN relies heavily on intensity gradients
  - **Blur divide**: ResNet-50 shows Defocus Blur CE=127 vs. LCNN CE=34—architectural depth amplifies blur sensitivity

- First 3 experiments:
  1. **Corruption sensitivity profiling**: Evaluate your architecture on MangoLeafDB-C; rank corruption types by F1 degradation using Table 2 format. Identify whether sensitivity pattern matches LCNN (blur-robust, contrast-sensitive) or ResNet (noise-sensitive, blur-sensitive) to inform architecture selection.
  2. **Pareto frontier positioning**: Plot clean accuracy vs. mCE using Figure 4 template. If below the Pareto front, diagnose whether gap stems from clean accuracy loss (training issue) or corruption vulnerability (architecture issue) to guide targeted improvements.
  3. **Severity threshold detection**: Generate F1-vs-severity curves (Figure 3 format) for your model's top-3 damaging corruptions. Identify if degradation is gradual (graceful) or shows sharp threshold (brittle failure at severity 3-4)—critical for deployment safety margins.

## Open Questions the Paper Calls Out

- Do the robustness rankings derived from synthetic corruptions (MangoLeafDB-C) correlate with performance on actual field images containing natural variations in blur, noise, and weather?
  - Basis in paper: [explicit] The authors state: "Firstly, it did not include a comparison between field images and algorithmically generated ones."
  - Why unresolved: The study relies on synthetic distortions adapted from ImageNet-C; it remains unverified if these digital proxies accurately represent the complex, compound distortions found in real agricultural environments.
  - What evidence would resolve it: A comparative evaluation of model performance on a new dataset of naturally degraded field images versus the synthetic MangoLeafDB-C benchmark.

- Can adversarial training or noise-resistant loss functions significantly reduce the mean Corruption Error (mCE) for complex architectures like ResNet-101 that currently suffer high degradation?
  - Basis in paper: [explicit] The conclusion notes: "There is substantial scope to investigate sophisticated methods for boosting robustness, such as adversarial training and the creation of noise-resistant loss functions."
  - Why unresolved: The current study evaluates models trained via standard transfer learning protocols without assessing whether specific training interventions can mitigate the observed brittleness to corruption.
  - What evidence would resolve it: Experimental results comparing the mCE of standard models against models re-trained using these advanced robust training techniques on the same corruption types.

- Are the reported performance advantages of the lightweight LCNN over deeper models like ResNet-101 statistically significant across the various corruption types?
  - Basis in paper: [explicit] The paper lists: "The lack of formal statistical tests to rigorously validate differences between models and corruptions" as a limitation.
  - Why unresolved: The analysis relies on descriptive metrics (F1 score, mCE) and rankings without hypothesis testing, leaving uncertainty regarding the reliability of the performance differences cited.
  - What evidence would resolve it: The application of formal statistical tests (e.g., McNemar's test) to the classification outputs to provide p-values for the performance gaps between architectures.

## Limitations
- LCNN architecture details are incompletely specified, relying on external reference without full layer configurations
- Conclusions about lightweight models are constrained to mango leaf disease classification and may not generalize to other agricultural tasks
- Results depend heavily on the MangoLeafDB dataset characteristics, limiting applicability to datasets with different class distributions or image qualities

## Confidence
- High confidence: Corruption error metrics and relative mCE methodology; observed performance degradation of deep architectures under corruptions
- Medium confidence: LCNN's superiority claims (architecture details partially external); generalizability to other agricultural applications
- Low confidence: Absolute performance thresholds for deployment decisions (environmental conditions not tested)

## Next Checks
1. Implement and test LCNN architecture with full layer specifications from [8] to verify claimed performance metrics
2. Evaluate selected architectures on a different agricultural disease dataset to test generalizability of robustness findings
3. Conduct ablation study on corruption augmentation during training to determine if architecture advantages persist with robustness-aware training