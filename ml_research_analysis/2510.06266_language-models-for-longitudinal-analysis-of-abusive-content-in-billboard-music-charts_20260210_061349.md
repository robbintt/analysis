---
ver: rpa2
title: Language models for longitudinal analysis of abusive content in Billboard Music
  Charts
arxiv_id: '2510.06266'
source_url: https://arxiv.org/abs/2510.06266
tags:
- content
- lyrics
- songs
- music
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed abusive content in Billboard music charts using
  deep learning and language models over seven decades. The researchers extracted
  and processed lyrics from 10,600 unique songs released from 1990-2024, focusing
  on English-language tracks.
---

# Language models for longitudinal analysis of abusive content in Billboard Music Charts
## Quick Facts
- arXiv ID: 2510.06266
- Source URL: https://arxiv.org/abs/2510.06266
- Reference count: 40
- This study analyzed abusive content in Billboard music charts using deep learning and language models over seven decades.

## Executive Summary
This study analyzed abusive content in Billboard music charts using deep learning and language models over seven decades. The researchers extracted and processed lyrics from 10,600 unique songs released from 1990-2024, focusing on English-language tracks. They employed BERT and RoBERTa models fine-tuned with sentiment analysis and abuse detection datasets to identify explicit content. N-gram analysis revealed increasing explicit language, particularly from 2017-2024, with themes shifting from love to more profane content.

## Method Summary
The researchers collected Billboard chart data from Kaggle, then retrieved lyrics from Genius API and explicit labels from Spotify API. They filtered for English-language songs, cleaned promotional text and structural tags, and standardized artist names across sources. Using BERT and RoBERTa architectures, they fine-tuned models on SenWave sentiment datasets and RAL-E abuse detection corpora. A custom polarity score was derived from weighted sentiment outputs, and N-gram analysis tracked thematic shifts across segmented time periods. Models were evaluated using accuracy, F1 scores, and Hamming loss metrics.

## Key Results
- Models achieved high accuracy in detecting explicit content, with GloVe embeddings performing best
- Hip-hop had the highest proportion of negative sentiments, while country had the lowest
- N-gram analysis revealed increasing explicit language from 2017-2024, with themes shifting from love to profane content

## Why This Works (Mechanism)
### Mechanism 1
- Fine-tuning transformer-based models on domain-specific abuse corpora appears to improve detection of explicit content in lyrics compared to simple keyword matching.
- The architecture leverages bidirectional attention (via BERT/RoBERTa) to understand context, recognizing toxic linguistic structures and slang that static lexicons miss.
- Core assumption: Linguistic patterns of abuse in Reddit comments transfer effectively to song lyrics.
- Evidence anchors: Mentions leveraging "fine-tuned BERT and RoBERTa architectures" to detect "explicit language, profanity, and emotionally charged themes."

### Mechanism 2
- A weighted multi-label sentiment polarity score can detect inappropriate "tone" (e.g., nihilism, violence) even when explicit profanity is absent.
- Instead of binary classification, the system classifies lyrics into multiple emotions and assigns negative weights to emotions like "Anxious" or "Sad."
- Core assumption: Songs containing "abusive" or "harmful" themes map consistently to specific negative sentiment labels rather than just "Negative."
- Evidence anchors: Analysis of Michael Jackson's "Who is it" shows the model identifying dark/depressive content despite "abusive word count = 0."

### Mechanism 3
- N-gram frequency analysis over segmented time windows reveals macro-level shifts in lyrical themes that single-year analysis might miss.
- By chunking the dataset into eras and counting trigrams, the system filters out noise to show clear semantic shifts.
- Core assumption: The frequency of specific trigrams correlates directly with the prevalence of specific cultural themes.
- Evidence anchors: Figure 4 visualization identifies the shift from "love" trigrams to explicit trigrams in the 2017-2024 period.

## Foundational Learning
- **Transfer Learning (Domain Adaptation)**: Needed because you cannot train a robust abuse detector from scratch on just 10,600 songs. Quick check: If you fine-tune a model on COVID-19 tweets to analyze 1990s Hip Hop, what is the risk of "concept drift"?
- **Multi-Label Classification**: Needed because a song isn't just "Happy" or "Sad." Quick check: How does the evaluation metric differ when treating this as a multi-label problem versus a multi-class problem?
- **Token Limitation & Chunking**: Needed because lyrics often exceed the 512-token limit of standard BERT architectures. Quick check: If a verse is split into three chunks to fit into the model, how should you aggregate the predictions to form a single sentiment label for the verse?

## Architecture Onboarding
- **Component map**: Ingestion: Kaggle -> Spotify API -> Genius API -> Preprocessing: LangDetect -> Regex -> API Standardization -> Model Core: Sentiment Branch (BERT/RoBERTa fine-tuned on SenWave) -> Abuse Branch (RoBERTa/HateBERT fine-tuned on RAL-E) -> Analysis: N-gram Counter -> Longitudinal Trend Visualizer
- **Critical path**: The Genre Mapping and Lyrics Cleaning stage. The integrity of the longitudinal analysis depends on successfully mapping genres and cleaning promotional text from the lyrics.
- **Design tradeoffs**: Using Spotify's "Explicit" label as ground truth is convenient but biased by Spotify's internal review policies. Using SenWave (COVID tweets) for sentiment analysis is a mismatch for song lyrics.
- **Failure signatures**: "Joking" Overload (model classifies ~90% as "Joking"), Slang Blindness (model misses modern slang), Euphemism Trap (model relies too heavily on profanity detection).
- **First 3 experiments**: 1) Replicate the RoBERTa vs. BERT comparison to achieve ~91% validation accuracy. 2) Experiment with the "Polarity Score" threshold to reduce "Joking" classifications. 3) Train on 1990-2010 data and test on 2015-2024 data to test generalization to modern slang.

## Open Questions the Paper Calls Out
- To what extent does replacing automated Spotify API labels with human-validated annotations improve the accuracy of explicit content detection models?
- How do regional and cultural nuances affect the transferability and accuracy of these explicit content classification models when applied to non-US music charts?
- What is the measurable correlation between the detected longitudinal increase in abusive lyrical content and behavioral or mental health outcomes in young audiences?

## Limitations
- Temporal generalization risk from using social media-trained models on poetic lyrics
- Ground truth quality issues from using Spotify's platform-specific explicit labels
- Statistical power constraints from relatively recent data dominating temporal trend analysis

## Confidence
- **High Confidence**: Overall accuracy metrics and basic temporal trends in explicit language frequency
- **Medium Confidence**: Sentiment distribution across genres and N-gram analysis effectiveness
- **Low Confidence**: Transferability of social media-trained models to lyrical content and completeness of automated genre classification

## Next Checks
1. **Temporal Hold-out Validation**: Train models exclusively on 1990-2010 data, then test on 2015-2024 lyrics to assess generalization to modern slang
2. **Human Annotation Comparison**: Have human raters manually classify a stratified sample of 200 songs across decades for explicit content and sentiment
3. **Alternative Ground Truth Validation**: Cross-validate Spotify explicit labels against other platform labels and manual annotation to assess platform-specific bias