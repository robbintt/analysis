---
ver: rpa2
title: 'No Soundness in the Real World: On the Challenges of the Verification of Deployed
  Neural Networks'
arxiv_id: '2506.01054'
source_url: https://arxiv.org/abs/2506.01054
tags:
- sound
- verification
- network
- neural
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that existing neural network verification
  tools, despite being theoretically sound, are not practically sound when deployed
  in real-world environments. The authors show that floating-point arithmetic and
  deployment-specific factors can lead to significant discrepancies between theoretical
  and deployed network behavior.
---

# No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks

## Quick Facts
- arXiv ID: 2506.01054
- Source URL: https://arxiv.org/abs/2506.01054
- Reference count: 40
- The paper demonstrates that existing neural network verification tools, despite being theoretically sound, are not practically sound when deployed in real-world environments due to floating-point arithmetic and deployment-specific factors.

## Executive Summary
This paper reveals a critical gap between theoretical and practical soundness in neural network verification. The authors show that state-of-the-art verifiers, while mathematically sound in theory, fail to guarantee correctness when neural networks are deployed in real-world environments with floating-point arithmetic and stochastic operation ordering. Through carefully crafted adversarial networks with backdoors that exploit deployment-specific behaviors, they demonstrate that leading verification tools (based on interval analysis, symbolic bound propagation, and mixed-integer programming) consistently fail to detect malicious behavior. The results prove that theoretical soundness does not translate to practical soundness in deployed neural networks, challenging the reliability of current verification approaches for safety-critical applications.

## Method Summary
The authors created five adversarial neural networks by embedding detector neurons into a base convolutional network trained on MNIST. These detectors exploit floating-point arithmetic discrepancies and expression tree evaluation order variations to trigger backdoors during verification. The networks were tested against four state-of-the-art verifiers (ERAN, α-β-CROWN, MIPVerify, MN-BaB) using the first 100 MNIST test samples with l∞ robustness radius 0.1. The detector neurons were designed to produce specific outputs based on floating-point precision (2^24 for 32-bit, 2^53 for 64-bit) or expression tree ordering, causing the verifiers to miss the planted backdoors and report artificially high robust accuracy.

## Key Results
- All tested verifiers (ERAN, α-β-CROWN, MIPVerify, MN-BaB) failed to detect the planted backdoors in adversarial networks
- Verifiers reported high robust accuracy (~97%) even when backdoors were activated, showing 0% actual robustness
- The attack works by exploiting floating-point precision differences and verifier-specific expression tree evaluation orders
- Theoretical soundness guarantees do not extend to practical soundness in deployed environments with real floating-point arithmetic

## Why This Works (Mechanism)
The attack exploits the gap between idealized mathematical models used in verification and the reality of floating-point computation in deployed systems. Neural network verifiers typically assume exact arithmetic and deterministic operation ordering, but real hardware uses floating-point numbers with rounding errors and may evaluate operations in varying orders. The detector neurons are designed to trigger when these deployment-specific factors cause particular numerical behaviors, such as when summing floating-point numbers in different orders or when reaching specific precision thresholds. This causes the verifier's bounds to become extremely loose, allowing adversarial examples to bypass detection while maintaining high accuracy on benign inputs.

## Foundational Learning
- **Neural network verification theory**: Understanding soundness and completeness guarantees in verification; needed to grasp why theoretical guarantees fail practically; quick check: can you explain the difference between exact and approximate verifiers?
- **Floating-point arithmetic behavior**: Knowledge of IEEE 754 representation and rounding errors; needed to understand how numerical precision affects verification; quick check: can you predict when floating-point summation will lose precision?
- **Expression tree evaluation**: Understanding how operation ordering affects floating-point results; needed to comprehend the backdoor mechanisms; quick check: can you calculate how different summation orders affect floating-point results?
- **Adversarial examples and backdoors**: Familiarity with evasion attacks and trojan attacks; needed to understand the attack objectives; quick check: can you distinguish between evasion and backdoor attacks?
- **Robustness verification metrics**: Knowledge of verified robust accuracy and certification bounds; needed to interpret experimental results; quick check: can you explain why verified accuracy differs from standard accuracy?
- **Stochastic vs deterministic computation**: Understanding how hardware and software variations affect neural network execution; needed to grasp deployment challenges; quick check: can you list factors that make neural network execution non-deterministic?

## Architecture Onboarding

**Component Map**
Base CNN (Conv2D → Conv2D → FC → ReLU) -> Detector neurons (precision/order-based) -> Verification tools (ERAN/α-β-CROWN/MIPVerify/MN-BaB)

**Critical Path**
Forward pass through detector-embedded network -> Expression tree bound propagation -> Robustness certificate generation -> Comparison with ground truth accuracy

**Design Tradeoffs**
- Theoretical soundness vs computational tractability: Exact verification is NP-hard, forcing use of sound approximations
- Precision vs performance: Higher precision arithmetic would reduce discrepancies but increase computational cost
- Determinism vs efficiency: Stochastic operation ordering can improve performance but breaks verification guarantees
- Expressiveness vs analyzability: More complex networks are harder to verify formally

**Failure Signatures**
- Large discrepancies between theoretical and practical robustness bounds
- Sudden drops in verification accuracy when batch sizes change
- Extreme sensitivity to floating-point precision settings
- Inconsistent results across different hardware or software configurations

**Three First Experiments**
1. Run the base network through a verifier with varying floating-point precision (32-bit vs 64-bit) to observe changes in verification bounds
2. Modify the detector neuron implementation to use different summation orders and measure verifier response
3. Compare verifier outputs when processing single samples vs batched samples to identify batch-dependent vulnerabilities

## Open Questions the Paper Calls Out
**Open Question 1**: Can practically sound verifiers be constructed for stochastic environments without incurring prohibitive computational costs? The authors state that achieving practical soundness is "significantly harder computationally" but do not propose solutions.

**Open Question 2**: Is finding the specific expression tree that minimizes or maximizes the output of a floating-point summation an NP-hard problem? The authors hypothesize this but provide no formal proof.

**Open Question 3**: Can deployment-sensitive backdoors be designed to be input-dependent and statistically stealthy to evade detection? The current constructions use constant triggers and large weights, leaving room for more sophisticated attacks.

## Limitations
- The paper does not propose practical solutions for achieving practically sound verification
- The backdoors use very large weights and constant triggers, making them detectable by heuristic methods
- The study focuses on a specific network architecture and dataset (MNIST), limiting generalizability
- The exact numerical implementation details for detector neurons are not fully specified

## Confidence
- **High Confidence**: The theoretical soundness claim that floating-point discrepancies can break verification is well-supported by mathematical analysis
- **Medium Confidence**: The empirical results showing verifier failure, given the complexity of reproducing exact floating-point behavior
- **Medium Confidence**: The general vulnerability of sound verifiers to deployment-specific factors, since it depends on replicating intended conditions

## Next Checks
1. **Detector Output Verification**: Implement a unit test to compute detector neuron outputs under different floating-point precisions (32-bit vs 64-bit) and batch sizes, confirming the values match the expected 0 or integer patterns in Table 2.
2. **Cross-Verifier Backdoor Activation**: For each verifier, print the intermediate expression tree bounds during forward/backward passes to confirm that the planted backdoor causes the intended large bound increases (as described for precision and Order1/2/3 detectors).
3. **Minimal Reproduction**: Construct a minimal 1-layer network with a single detector neuron and verify that the verifier's robustness bound changes drastically (by orders of magnitude) when the detector is activated, demonstrating the core soundness gap in a controlled setting.