---
ver: rpa2
title: 'Using Contrastive Learning to Improve Two-Way Reasoning in Large Language
  Models: The Obfuscation Task as a Case Study'
arxiv_id: '2509.05553'
source_url: https://arxiv.org/abs/2509.05553
tags:
- code
- reasoning
- bidirectional
- semantic
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses whether large language models truly understand
  code semantics or merely replicate patterns through a bidirectional reasoning framework.
  The authors propose Contrastive Fine-Tuning (CFT) to overcome "cognitive specialization"
  - where standard fine-tuning degrades reverse reasoning capabilities while improving
  forward tasks.
---

# Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study

## Quick Facts
- arXiv ID: 2509.05553
- Source URL: https://arxiv.org/abs/2509.05553
- Reference count: 29
- Key outcome: Contrastive Fine-Tuning enables 39-52% reverse performance on code deobfuscation vs 0% with standard fine-tuning

## Executive Summary
This paper addresses whether large language models truly understand code semantics or merely replicate patterns through a bidirectional reasoning framework. The authors propose Contrastive Fine-Tuning (CFT) to overcome "cognitive specialization"—where standard fine-tuning degrades reverse reasoning capabilities while improving forward tasks. CFT trains models on semantic triplets: positive pairs (same meaning), negative pairs (different meaning), and forward obfuscation examples. Experimental results demonstrate a breakthrough: while standard fine-tuning achieves 0% success on deobfuscation tasks, CFT enables 39-52% reverse performance across multiple models and transformation types.

## Method Summary
The authors develop Contrastive Fine-Tuning (CFT) as a three-loss formulation that trains language models to understand semantic relationships in code transformations. The method combines positive pairs (original↔obfuscated code with same functionality), negative pairs (different functionality), and forward generation loss to enable bidirectional reasoning without explicit reverse training. CFT is evaluated across multiple transformation types including variable renaming, dead code insertion, and string encryption using both commercial (GPT-4.1-Mini) and open-source models (Qwen2.5-Coder-7B).

## Key Results
- Standard fine-tuning achieves 0% success on deobfuscation tasks, confirming cognitive specialization
- CFT enables 39-52% reverse performance across variable renaming, dead code insertion, and string encryption
- GPT-4.1-Mini achieved 52.03% semantic success in reverse variable renaming vs 0% with standard fine-tuning
- Forward task performance maintained while enabling bidirectional reasoning

## Why This Works (Mechanism)

### Mechanism 1: Triplet Contrastive Loss Composition
CFT optimizes L_total = L_pos + L_neg + L_gen, where L_pos learns semantic equivalence, L_neg learns semantic boundaries, and L_gen learns forward transformation. This multi-objective pressure prevents unidirectional pattern replication. Core assumption: Semantic understanding requires learning what makes code equivalent versus different, not just learning to apply transformations.

### Mechanism 2: Preventing Directional Neural Pathway固化
Standard fine-tuning creates asymmetric neural pathways (T: original → obfuscated) while eliminating reverse capability (T^-1). CFT's multi-task formulation forces the model to maintain semantic representations accessible from both directions. Core assumption: Cognitive specialization is analogous to catastrophic forgetting—forward task optimization overwrites reverse reasoning representations.

### Mechanism 3: Complexity-Dependent Emergence
Bidirectional reasoning emerges for semantically simple transformations but fails for complex ones. Variable renaming (52% success) requires mapping single tokens; dead code insertion requires distinguishing functional vs. non-functional statements; string encryption requires understanding cryptographic logic. CFT's benefit scales inversely with transformation complexity (Spearman's ρ = -0.91, p < 0.001).

## Foundational Learning

- **Concept: Semantic Equivalence vs. Syntactic Similarity**
  - Why needed here: CFT relies on distinguishing code that looks different but functions identically from code that looks similar but functions differently.
  - Quick check question: Can you explain why two programs with identical syntax except for variable names are semantically equivalent, while two programs with identical structure but different loop bounds are not?

- **Concept: Catastrophic Forgetting in Fine-Tuning**
  - Why needed here: Cognitive specialization is framed as a form of forgetting—forward training erases reverse capability.
  - Quick check question: Why does LoRA (Low-Rank Adaptation) help mitigate catastrophic forgetting compared to full fine-tuning?

- **Concept: Contrastive Learning Triplet Formulation**
  - Why needed here: CFT extends vision-based contrastive learning to code. Understanding anchor-positive-negative triplet structure is essential for constructing training data.
  - Quick check question: Given a code snippet A, what makes a good positive example vs. a hard negative example for semantic equivalence learning?

## Architecture Onboarding

- **Component map:** Input Code → [Tokenizer] → [Base LLM] → [LoRA Adapters] → Output

- **Critical path:**
  1. Construct triplet dataset (10K positive pairs, 10K negative pairs, 10K forward examples per transformation type)
  2. Initialize LoRA adapters (rank 8-16 typical for 7B models)
  3. Train with combined loss L_CFT = L_pos + L_neg + L_gen
  4. Evaluate forward (obfuscation) AND reverse (deobfuscation) performance on held-out set
  5. Verify semantic correctness via test case execution, not just syntactic similarity

- **Design tradeoffs:**
  - More negative examples → stronger semantic boundaries but risk of overfitting to specific difference types
  - Larger model (GPT-4 vs 7B) → higher bidirectional emergence (50-52% vs 39%) but 10x+ cost increase
  - Include reverse examples → guaranteed bidirectionality but violates the paper's core hypothesis test

- **Failure signatures:**
  - Forward success >80%, reverse success ≈0% → cognitive specialization (standard fine-tuning failed)
  - Both directions ≈0% → insufficient training or data quality issues
  - High CodeBLEU but low test pass rate → syntactic pattern matching without semantic understanding
  - Reverse outputs identical to inputs (similarity >0.65) → model learned to copy rather than transform

- **First 3 experiments:**
  1. Baseline replication: Standard fine-tune Qwen2.5-Coder-7B on variable renaming only; verify 0% reverse performance confirms cognitive specialization.
  2. Ablation study: Train three CFT variants—(L_pos + L_gen), (L_neg + L_gen), (L_pos + L_neg + L_gen)—to isolate which loss term enables bidirectionality.
  3. Complexity boundary: Apply CFT to increasingly complex transformations (variable renaming → dead code → control flow flattening) to map where bidirectional emergence fails.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Contrastive Fine-Tuning be extended to enable bidirectional reasoning for semantically complex transformations beyond variable renaming?
- Basis in paper: [explicit] The authors state that "Variable renaming success while dead code insertion and literals encryption fail confirms semantic complexity constraints... revealing current limitations in contrastive learning formulations."
- Why unresolved: CFT achieved 39–52% reverse performance on variable renaming but near-zero on dead code insertion and string encryption, suggesting the current formulation struggles with transformations requiring deeper algorithmic reasoning.
- What evidence would resolve it: Demonstrating CFT variants that achieve >30% reverse performance on dead code detection or encryption logic recovery, potentially via enriched negative examples or multi-stage contrastive objectives.

### Open Question 2
- Question: What model capacity thresholds are required for CFT to effectively overcome cognitive specialization?
- Basis in paper: [explicit] The paper notes "Commercial models demonstrate superior bidirectional emergence (GPT models: 50–52%, QwenCoder: 39%) while other open-source models show minimal improvement, revealing fundamental capacity constraints governing bidirectional reasoning emergence."
- Why unresolved: The correlation between model scale and CFT effectiveness remains unclear, with only a small set of models evaluated.
- What evidence would resolve it: Systematic evaluation of CFT across models spanning 1B–70B+ parameters to identify performance scaling curves and critical capacity thresholds.

### Open Question 3
- Question: Does the bidirectional reasoning enabled by CFT transfer to other code transformation tasks outside obfuscation?
- Basis in paper: [inferred] The conclusion states CFT is currently limited to simple code transformations and "lays the groundwork for future research into understanding-oriented AI systems."
- What evidence would resolve it: Applying CFT to distinct transformation families (e.g., refactoring, optimization, source-to-source translation) and measuring untrained reverse performance.
- Why unresolved: The study restricts evaluation to obfuscation; generalizability to broader software engineering tasks remains untested.

## Limitations
- Generalization beyond controlled obfuscation tasks remains untested
- Complexity-dependent emergence suggests fundamental limitations for algorithmic reasoning
- Semantic triplet formulation may not scale to scenarios requiring deeper structural understanding

## Confidence

**High Confidence (70-90%):**
- Standard fine-tuning creates unidirectional optimization pathways
- The 0% reverse success rate for standard fine-tuning on deobfuscation tasks
- Forward task performance maintenance during CFT

**Medium Confidence (40-70%):**
- CFT's specific loss composition is optimal for enabling bidirectional reasoning
- The 39-52% reverse success rates represent true semantic understanding
- Complexity-dependent emergence correlation reflects fundamental model limitations

**Low Confidence (0-40%):**
- CFT will scale to more complex transformations beyond the tested obfuscation types
- The approach generalizes to non-obfuscation bidirectional reasoning tasks
- The semantic triplet formulation is superior to alternative contrastive approaches

## Next Checks
1. Apply CFT to a non-obfuscation bidirectional task (e.g., code summarization↔generation) to verify the approach works beyond controlled transformation scenarios.

2. Systematically test CFT on transformations of increasing algorithmic complexity to identify the precise boundary where bidirectional emergence fails.

3. Compare CFT against ablations where L_pos, L_neg, or L_gen are trained individually or in different combinations to definitively establish which components are necessary versus sufficient for bidirectional reasoning.