---
ver: rpa2
title: Satisficing and Optimal Generalised Planning via Goal Regression (Extended
  Version)
arxiv_id: '2511.11095'
source_url: https://arxiv.org/abs/2511.11095
tags:
- planning
- problem
- plan
- goal
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MOOSE, a novel generalised planning approach\
  \ that leverages goal regression and problem relaxation to synthesize and execute\
  \ first-order Condition\u2192Actions rules. The method solves training problems\
  \ optimally for individual goals, performs goal regression, and lifts the resulting\
  \ macro-actions into reusable rules."
---

# Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)

## Quick Facts
- arXiv ID: 2511.11095
- Source URL: https://arxiv.org/abs/2511.11095
- Reference count: 40
- Solves 719.6/720 classical test problems with significantly less memory and time compared to baselines

## Executive Summary
This paper introduces MOOSE, a novel generalised planning approach that leverages goal regression and problem relaxation to synthesize and execute first-order Condition→Actions rules. The method solves training problems optimally for individual goals, performs goal regression, and lifts the resulting macro-actions into reusable rules. Theoretical results prove MOOSE's soundness and completeness under True Goal Independence (TGI) and its optimality under both TGI and Optimal Goal Independence (OGI) assumptions. Experiments on classical, numeric, and optimal planning domains show MOOSE outperforms state-of-the-art planners in synthesis cost, planning coverage, and solution quality.

## Method Summary
MOOSE operates in three phases: (1) for each training problem, compute optimal plans for each singleton goal atom using FAST-DOWNWARD with LM-cut heuristic, (2) apply goal regression to each plan to extract minimal preconditions, and (3) lift the regressed state-action pairs into first-order rules stored in an SQLite database. During test time, MOOSE queries this database to find applicable rules and executes the corresponding macro-actions. The approach assumes True Goal Independence (TGI) where goals can be achieved independently in any order, enabling a greedy decomposition strategy. For optimal planning, rules are encoded as PDDL axioms for SYMK symbolic planner.

## Key Results
- Synthesizes generalized plans that solve 719.6/720 classical test problems
- Reduces memory usage from 8GB to 1GB and time from 1800s to 25.5s compared to LAMA baseline
- Outperforms SLEARN in synthesis cost (10x faster) while achieving higher planning coverage
- Proves theoretical soundness and completeness under TGI assumption, optimality under OGI assumption

## Why This Works (Mechanism)

### Mechanism 1: Goal Regression for Precondition Extraction
Regressing goals through optimal plans produces minimal necessary preconditions for macro-actions, enabling rule generalization. For each optimal plan achieving a singleton goal, reverse-iterate through actions computing `regr(g, a) = (g \ add(a)) ∪ pre(a)`. Each regressed state captures exactly what must hold before executing the remaining plan suffix. These (partial-state, goal, action-sequence) tuples are lifted to first-order rules.

### Mechanism 2: Goal Independence Enables Greedy Subproblem Decomposition
Problems with True Goal Independence (TGI) can be solved by greedily achieving each goal atom optimally in sequence, making learning tractable. Decompose multi-goal problems into singleton-goal subproblems. For each goal atom in order: (1) compute optimal plan from current state, (2) progress state, (3) extract rules via regression. This relaxation reduces exponential search to polynomial-bounded subproblems under pTGI.

### Mechanism 3: Database Querying for Efficient Rule Instantiation
Treating states as databases and lifted rules as SQL queries enables fast instantiation on test problems. Encode MOOSE rules with state conditions and goal conditions. At runtime, query the current state database for variable bindings satisfying `stateCond(r)|_f ⊆ s` and `goalCond(r)|_f ⊆ (P[g] \ s)`. SQLite indexing accelerates grounding.

## Foundational Learning

- **Concept**: Goal Regression in STRIPS
  - Why needed here: Understanding how `regr(g, a)` computes minimal preconditions enables debugging why certain rules are synthesized.
  - Quick check question: Given action `move(?from, ?to)` with `add={atRobot(?to)}, del={atRobot(?from)}`, what is `regr({atRobot(kitchen)}, move(living, kitchen))`?

- **Concept**: Goal Independence Taxonomy (TGI/SGI/OGI)
  - Why needed here: Identifying which independence class your domain belongs to determines whether MOOSE guarantees soundness/completeness/optimality.
  - Quick check question: If achieving goal g1 optimally requires suboptimally achieving g2 first, which independence property fails?

- **Concept**: First-Order Lifting and Anti-Unification
  - Why needed here: Lifting ground instances to lifted rules is the generalization step; understanding variable substitution is essential for debugging rule applicability.
  - Quick check question: What is the lifted form of `at(cake, kitchen)` and `at(dog, backyard)` using variable `?obj` and `?loc`?

## Architecture Onboarding

- Component map:
  Training Problems → [A* with LM-cut] → Optimal Plans → [Goal Regression] → (State, Goal, Action-Seq) tuples → [Lifting] → First-Order Rules (MOOSE program) → [SQLite Grounding] → Applicable Rule → Execute Macro-Action → Loop until goal

- Critical path: Optimal plan generation (Algorithm 1, line 8) is the bottleneck; uses FAST-DOWNWARD with LM-cut heuristic. For optimal planning mode, MOOSE rules are encoded as PDDL axioms and fed to SYMK symbolic planner.

- Design tradeoffs:
  - TGI assumption vs. generality: MOOSE is provably sound/complete only under TGI_C; non-TGI domains may still work empirically but without guarantees.
  - Satisficing vs. optimal mode: Satisficing mode directly executes rules; optimal mode encodes rules as axioms for search pruning, trading synthesis cost for plan quality.
  - Singleton vs. conjunctive goal regression: Regressing over larger goal subsets (Appendix F) improves plan quality but increases synthesis and instantiation costs exponentially.

- Failure signatures:
  - No rule applicable: State/goal combination not covered by training problems; need more diverse training set.
  - Cycle detected during instantiation: Rules create loops; may indicate TGI violation or insufficient training coverage.
  - Invalid plan returned: Regression may ignore relevant atoms; verify domain has no conditional effects violating regressability.
  - Suboptimal plans in optimal mode: OGI assumption violated; achieving goals independently hurts global optimality.

- First 3 experiments:
  1. Validate TGI assumption on your domain: Run Algorithm 1 (TGI greedy algorithm) on test problems with random goal orderings; check if >50% produce valid plans.
  2. Ablate training problem coverage: Vary training set size and measure test coverage; identify minimum training examples needed for your domain's equivalence classes under `~_U`.
  3. Compare satisficing vs. optimal mode latency: Measure instantiation time (Algorithm 3) vs. axiom-guided SYMK search time; determine which mode suits your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
Can an algorithm be developed to automatically generate or select the minimal necessary training data required to guarantee completeness? Following the proof of Theorem 17, the authors suggest: "A fruitful next step is to develop a learning algorithm that learns to generate and select what training data is required." The theoretical upper bound for the number of training problems required to guarantee soundness and completeness is exponential relative to domain size, making exhaustive data collection intractable.

### Open Question 2
How can the MOOSE framework be extended to handle planning domains that require transitive closure computations? The conclusion explicitly lists "extending MOOSE to handle domains requiring transitive closure computations" as a direction for future work. The current method relies on lifting ground instances into first-order rules, which may struggle to represent the unbounded recursive nature of transitive relationships without infinite rulesets.

### Open Question 3
How can the method be adapted to maintain guarantees under weaker planning domain assumptions, specifically where goals are not independent? The conclusion lists "weaker planning domain assumptions" as a target for future work; additionally, the theoretical guarantees (Theorems 17 & 18) currently rely on strong Goal Independence (TGI/OGI) assumptions. The algorithm relies on a greedy strategy of solving singleton goals, which fails if achieving one goal deletes another (violating independence).

## Limitations

- The approach's performance on non-TGI domains remains unverified, and the relationship between domain structure and goal independence properties needs systematic study.
- The rule instantiation cost could become prohibitive for domains with complex first-order rules or large state spaces.
- The comparative performance claims against baselines assume fair hyperparameter tuning for SLEARN and proper domain modifications for all tested domains.

## Confidence

- **High Confidence**: The core mechanism of goal regression for precondition extraction is sound and well-established in classical planning literature. The database-based rule instantiation approach is technically feasible.
- **Medium Confidence**: The claim that TGI assumption holds for most benchmark domains is empirically supported but not formally proven. The optimality guarantees under OGI are theoretically sound but may not translate to practical optimality in non-TGI domains.
- **Low Confidence**: The comparative performance claims against baselines assume fair hyperparameter tuning for SLEARN and proper domain modifications for all tested domains. The scalability analysis is limited to the specific benchmark set used.

## Next Checks

1. **TGI Assumption Validation**: Systematically test MOOSE on domains known to violate TGI (e.g., Sussman anomaly problems) to verify the performance degradation and identify warning signs.
2. **Rule Instantiation Scaling**: Measure SQLite query performance as rule complexity and state space size increase, particularly for numeric domains with complex constraints.
3. **Baseline Parameter Sensitivity**: Conduct ablation studies on SLEARN hyperparameters and domain modification procedures to verify the fairness of comparative claims.