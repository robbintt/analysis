---
ver: rpa2
title: Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning
arxiv_id: '2602.00262'
source_url: https://arxiv.org/abs/2602.00262
tags:
- clustering
- subspace
- data
- contrastive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a contrastive self-supervised learning framework,
  CSC, for subspace clustering with incomplete data. The key idea is to use random
  masking to generate augmented views of incomplete samples, which are then passed
  through a shared deep network and trained using a SimCLR-style contrastive loss
  to learn invariant embeddings.
---

# Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2602.00262
- Source URL: https://arxiv.org/abs/2602.00262
- Reference count: 40
- Primary result: CSC consistently outperforms classical and deep baselines for subspace clustering on incomplete data, especially under high missing rates.

## Executive Summary
This paper introduces CSC, a contrastive self-supervised learning framework for subspace clustering with incomplete data. The key innovation is using disjoint random masking to generate augmented views of incomplete samples, which are then trained using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are clustered using sparse subspace clustering. Experiments on six benchmark datasets show CSC significantly outperforms classical methods like SSC, MSC, and GSSC, as well as deep learning baselines such as DSC and MAE, particularly under high missing rates.

## Method Summary
CSC processes incomplete vectors by generating two disjoint random masks that split observed entries into separate views. These masked views are passed through a shared deep network and trained using NT-Xent contrastive loss, forcing the encoder to map different partial views of the same underlying vector to similar latent representations. During inference, the embeddings from the backbone network are clustered using Sparse Subspace Clustering (SSC). The method leverages the self-expressiveness property of data lying in a union of subspaces, assuming the contrastive learning process preserves this structure in the embedding space.

## Key Results
- CSC achieves 65.95% mean clustering accuracy on MNIST across sampling rates versus 50.98% for MAE
- On hyperspectral datasets like IndianPines, CSC achieves 74.28% accuracy versus 65.70% for DSC-Conv10
- Training completes in under 3 hours on a single GPU
- CSC remains stable under high noise levels while reconstruction-based methods like MAE degrade sharply

## Why This Works (Mechanism)

### Mechanism 1
Disjoint random masking enforces invariance to missingness patterns by forcing the encoder to map different partial views of the same underlying vector to similar points in latent space, effectively learning to ignore the specific pattern of missing data.

### Mechanism 2
Contrastive objectives outperform reconstruction-based pre-training for clustering incomplete data by focusing on global semantic alignment rather than hallucinating missing values.

### Mechanism 3
Deep non-linearity separates representation learning from subspace clustering by projecting raw, non-linear data into an embedding space that preserves the "union of subspaces" structure better than the raw input.

## Foundational Learning

- **Concept: SimCLR-style Contrastive Learning**
  - Why needed here: This is the engine of the method, pulling positive pairs together and pushing negative pairs apart
  - Quick check question: In the loss function, do we push away embeddings of different masks of the *same* sample, or embeddings of *different* samples?

- **Concept: Sparse Subspace Clustering (SSC)**
  - Why needed here: This is the "head" of the architecture that clusters the embeddings using the self-expressiveness property
  - Quick check question: Why does the paper use SSC on the *embeddings* rather than a simple K-Means clustering algorithm?

- **Concept: Masking as Data Augmentation**
  - Why needed here: Masking is the only valid augmentation discussed for vector data where rotation/crop breaks spatial structure
  - Quick check question: Why is the "disjoint" constraint ($M_a \odot M_b = 0$) critical for the augmentation strategy in this specific context?

## Architecture Onboarding

- **Component map:** Input -> Augmenter (disjoint masks) -> Backbone (fθ) -> Projection Head (gϕ) -> Clustering Head (SSC on embeddings)
- **Critical path:** The generation of masks must enforce that $M_a + M_b \le M_{observed}$ to avoid noisy or invalid contrastive signals
- **Design tradeoffs:**
  - Depth vs. Noise: Increasing depth improves accuracy up to a point (L=5), but gains diminish
  - Residual Connections: No Residual model performed marginally better at large dataset sizes, suggesting potential overfitting with residuals in sparse regimes
- **Failure signatures:**
  - Collapse: Model outputs constant embeddings (check standard deviation of batch embeddings)
  - MAE-like failure: Model tries to reconstruct missing values rather than learning invariance
- **First 3 experiments:**
  1. Generate synthetic data with k=5 subspaces, run CSC at sampling rate ρ=0.5, verify >70% accuracy
  2. Train two MLP-4 models on MNIST with 50% missingness (with/without residuals), compare accuracy
  3. Add Gaussian noise (σ=0.3) to test set, compare CSC vs MAE performance

## Open Questions the Paper Calls Out
- How can the CSC framework be adapted to handle streaming data or multi-view settings with heterogeneous missingness patterns?
- What theoretical guarantees can be established for contrastive objectives when learning from incomplete data regarding subspace preservation?
- Does the requirement for disjoint random masks limit performance at extremely low sampling rates by providing insufficient overlap for meaningful contrastive learning?
- Is the application of linear SSC on learned embeddings sufficient to capture complex non-linear manifolds inherent in the raw data?

## Limitations
- Exact architectural details like projection head dimensions and SSC regularization parameters are unspecified
- Mask generation strategy (uniform, fixed split, or learned) is not clarified
- Hyperparameters including learning rate, weight decay, and training epochs require assumptions for reproduction
- Claims about SSC superiority over alternatives like K-Means are not experimentally validated

## Confidence
- **High:** The general contrastive learning pipeline (SimCLR-style, disjoint masking, SSC clustering) is clearly described and well-supported by experimental results
- **Medium:** The comparative advantage over MAE and DSC baselines is evident, but ablation on residuals and depth is less conclusive
- **Low:** Claims about SSC being superior to alternatives like K-Means are not experimentally validated

## Next Checks
1. Systematically vary learning rate and weight decay on MNIST to determine their impact on final clustering accuracy
2. Replace SSC with K-Means on the learned embeddings for a subset of datasets to directly test the necessity of the subspace clustering step
3. Implement alternative mask generation strategies (overlapping vs. disjoint) and compare their effect on stability and accuracy at high missingness rates