---
ver: rpa2
title: A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs
arxiv_id: '2512.17319'
source_url: https://arxiv.org/abs/2512.17319
tags:
- image
- object
- question
- answer
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RSHR-Bench, a large-scale ultra-high-resolution
  remote sensing benchmark for multimodal large language models (MLLMs). It addresses
  the gap between existing low-resolution benchmarks and real-world remote sensing
  imagery, which often contains up to hundreds of megapixels.
---

# A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs

## Quick Facts
- **arXiv ID**: 2512.17319
- **Source URL**: https://arxiv.org/abs/2512.17319
- **Reference count**: 40
- **Primary result**: Introduces RSHR-Bench, a large-scale ultra-high-resolution remote sensing benchmark for multimodal large language models (MLLMs).

## Executive Summary
This paper introduces RSHR-Bench, a large-scale ultra-high-resolution remote sensing benchmark for multimodal large language models (MLLMs). It addresses the gap between existing low-resolution benchmarks and real-world remote sensing imagery, which often contains up to hundreds of megapixels. RSHR-Bench features 5,329 full-scene images with long sides of at least 4,000 pixels and includes diverse tasks such as multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. The dataset is carefully curated from widely used remote sensing corpora and UAV collections, and all tasks undergo rigorous human and LLM verification to ensure visual grounding and minimize language priors. Evaluations on a broad suite of open-source, closed-source, and RS-specific VLMs reveal uniformly low performance, highlighting the benchmark's difficulty and the need for further progress in high-resolution visual understanding.

## Method Summary
RSHR-Bench is a zero-shot inference benchmark that evaluates MLLMs on ultra-high-resolution remote sensing imagery. The benchmark uses 5,329 full-scene images (long side ≥4,000px, up to ~3×10⁸ pixels) from multiple RS corpora. Tasks include multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. Questions are drafted by VLMs and filtered through text-only LLM adversarial filtering followed by human verification. Evaluation uses accuracy for MCQs, GPT-4o scoring (≥80 = correct) for open-ended VQA, and BLEU/METEOR/ROUGE-L for captioning.

## Key Results
- All fourteen evaluated models exhibit poor performance across all four task types
- Text-only LLMs achieve ~30/100 on open-ended VQA, indicating substantial hallucination
- Accuracy drops sharply at 100M/200M pixel resolutions across all model families
- Models score below 50 on perception tasks, indicating nearly no correct relevant responses

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Language-Prior Filtering
- **Claim**: Text-only LLM adversarial filtering appears to reduce questions solvable without visual inspection.
- **Mechanism**: The pipeline first drafts questions with VLMs, then tasks text-only LLMs (Llama3-8B, Qwen3-8B) to answer without image access. If text-only accuracy exceeds a threshold (~30%), items are revised or removed via human post-editing to eliminate lexical hints and commonsense shortcuts.
- **Core assumption**: If a text-only model can answer correctly without the image, the question relies on language priors rather than visual evidence.
- **Evidence anchors**:
  - [abstract] "adversarial filtering with strong LLMs followed by rigorous human verification"
  - [section 3.1] "If a model attains higher accuracy by relying solely on linguistic cues... we remove or revise the affected items"
  - [corpus] No direct corpus validation of this specific filtering mechanism for RS benchmarks.
- **Break condition**: If text-only models still achieve high accuracy after filtering, it suggests either (a) remaining questions still contain subtle linguistic cues, or (b) the task domain inherently overlaps with commonsense knowledge.

### Mechanism 2: Ultra-High-Resolution Native Preservation
- **Claim**: Preserving images at native ultra-high resolution (≥4K long side, up to ~3×10⁸ pixels) creates genuine visual demands that lower-resolution proxies cannot replicate.
- **Mechanism**: Unlike benchmarks that resize or tile images to fit standard ViT inputs (typically 336×336 to 4K), RSHR-Bench retains full scene context, requiring models to process long-range spatial relationships and small, densely distributed objects without fragmentation.
- **Core assumption**: Models' performance gaps on ultra-high-resolution imagery reflect true limitations in visual processing rather than artifacts of downscaling.
- **Evidence anchors**:
  - [abstract] "full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image"
  - [section 3.1] "Common tiling pipelines... fragment the global layout and object constellations"
  - [corpus] Related work on ultra-high-resolution segmentation (GLCAN, arXiv:2506.19406) shows similar computational and multi-scale fusion challenges.
- **Break condition**: If models perform comparably on intelligently downsampled versions, the resolution itself may not be the bottleneck.

### Mechanism 3: Multi-Task, Multi-Format Consistency Checks
- **Claim**: Combining multiple-choice VQA, open-ended VQA, captioning, and single-image evaluation provides cross-validation of visual grounding.
- **Mechanism**: Multiple-choice formats may inflate scores via option elimination; open-ended formats require free-form generation. Consistent low performance across both formats, especially when text-only models underperform in open-ended settings, suggests genuine visual limitations.
- **Core assumption**: A model that truly understands the image should perform reasonably across task formats, not just exploit format-specific shortcuts.
- **Evidence anchors**:
  - [abstract] "four task families—multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation"
  - [section 4] "all models score below 50 on perception tasks [open-ended]... indicating nearly no correct relevant responses"
  - [corpus] Multi-image reasoning benchmarks (e.g., MedFrameQA, arXiv:2505.16964) similarly use multi-format evaluation to probe robustness.
- **Break condition**: If a model excels in one format but fails others, the inconsistency may indicate overfitting to format rather than task content.

## Foundational Learning

- **Concept: Language Priors in VLM Benchmarks**
  - **Why needed here**: The paper's central finding is that existing RS benchmarks allow text-only models to achieve competitive accuracy (e.g., Qwen3-8B: 51.6% reasoning accuracy vs. GPT-4o: 45.2%), inflating perceived visual understanding.
  - **Quick check question**: Given a question like "What is the color of the airplane in the image?" could a model guess "white" correctly based on typical airplane colors without seeing the image?

- **Concept: Tiling vs. Native Resolution in Remote Sensing**
  - **Why needed here**: Remote sensing images often exceed 10,000×10,000 pixels; standard tiling to 512×512 or 336×336 fragments scene-level context. RSHR-Bench deliberately avoids this to test full-scene understanding.
  - **Quick check question**: If an image of a city is split into 64 non-overlapping 512×512 tiles, can a model reason about the overall city layout without a global view?

- **Concept: Multi-Turn and Multi-Image Evaluation**
  - **Why needed here**: Real-world RS analysis involves sequential queries and temporal comparisons (e.g., "Where is this complex?" → "What change might affect it?" → "What's the likely future state?"). Single-turn evaluation misses this.
  - **Quick check question**: How would you design a benchmark to test whether a model can track spatial references across a multi-turn dialogue about the same image?

## Architecture Onboarding

- **Component map**: Data corpus -> Task generation -> Verification pipeline -> Evaluation suite
- **Critical path**: Image selection → bounding box annotation → question drafting → Human verification (correctness, grounding, no hints) → Text-only adversarial filtering → revision loop → Model evaluation across four task types → Analysis: compare multimodal vs. text-only performance gaps
- **Design tradeoffs**:
  - Human verification vs. scale: Full human verification ensures quality but limits dataset size (~8,277 total tasks vs. millions in automated benchmarks)
  - Native resolution vs. computational cost: Ultra-high-resolution inputs dramatically increase inference cost; only GeoLLaVA-8K and Claude 3.7 explicitly support 8K inputs among tested models
  - Task diversity vs. evaluation consistency: 17 sub-tasks provide broad coverage but complicate cross-model comparison due to varying sample sizes
- **Failure signatures**:
  - Format bias: GeoLLaVA-8K achieves 0% on multi-region joint contrast (MRJC) because it heavily favors option "A" in multiple-choice
  - Hallucination in open-ended: Text-only LLMs score ~30/100 on open-ended VQA, indicating incorrect responses with substantial hallucinations
  - Resolution sensitivity: Accuracy drops sharply from 4K–8K to 100M/200M regimes across all model families
- **First 3 experiments**:
  1. **Baseline text-only vs. multimodal gap**: Run text-only LLMs (Llama3-8B, Qwen3-8B) and multimodal models (GPT-4o, Gemini-2.5-pro) on RSHR-Bench reasoning tasks. If text-only accuracy is >40% of multimodal, the benchmark may still contain language priors.
  2. **Resolution degradation study**: Evaluate a single model (e.g., InternVL3.5-8B) on the same tasks at 2K, 4K, 8K, and native resolution. Flat accuracy curves suggest resolution-invariant semantic cues (as seen on XLRS-Bench in Table 11).
  3. **Perception vs. reasoning breakdown**: Compare model performance on perception sub-tasks (color, shape, counting) vs. reasoning sub-tasks (anomaly detection, future prediction). Large gaps may indicate that models perceive but cannot integrate visual evidence for inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural innovations can enable VLMs to process ultra-high-resolution imagery (up to ~3×10⁸ pixels) while simultaneously preserving global context and fine-grained local detail?
- Basis in paper: [explicit] The paper states all fourteen models "exhibit poor performance across four types of tasks" and performance "drops markedly at 100M/200M" pixels, indicating "limited robustness to extreme pixel counts and large spatial contexts."
- Why unresolved: Current approaches (patching, tiling, token compression) fragment global layout or lose fine details; no evaluated model achieves satisfactory performance on the benchmark.
- What evidence would resolve it: A model architecture that achieves significantly higher accuracy on both perception and reasoning tasks at 100M+ pixel resolutions while maintaining single-pass inference.

### Open Question 2
- Question: How can reasoning-task benchmarks be designed to enforce genuine visual grounding rather than reliance on language priors or commonsense knowledge?
- Basis in paper: [explicit] The authors note text-only LLMs achieve ~77% accuracy on XLRS-Bench reasoning tasks and surpass GPT-4o (51.6% vs 45.2%), revealing that "models may answer many questions by exploiting textual cues and prior world knowledge."
- Why unresolved: Even after adversarial filtering, text-only LLMs still reach >30% accuracy on RSHR-Bench reasoning tasks, suggesting residual language bias.
- What evidence would resolve it: A task design methodology where text-only LLMs achieve near-random performance while multimodal models retain meaningful accuracy above human baseline.

### Open Question 3
- Question: How can faithful geospatial grounding and calibrated uncertainty be achieved in remote sensing MLLMs operating on gigapixel-scale imagery?
- Basis in paper: [explicit] The related work section identifies "persistent gaps include faithful geospatial grounding, multi-temporal reasoning, large-scale sensor fusion, and adjusted uncertainty" as unresolved challenges in RS-MLLMs.
- Why unresolved: Current models show systematic errors on small targets and counting tasks, with predictions skewing positive even when correct answers are "0"; detection recall drops sharply for ultra-high-resolution images.
- What evidence would resolve it: Models that provide well-calibrated confidence estimates and accurate grounding coordinates across the full resolution spectrum.

## Limitations
- The adversarial filtering threshold (30% accuracy) may not catch all subtle linguistic cues
- Computational infeasibility of processing full 100M+ pixel images means most models use tiling or resizing strategies
- Dataset size (5,329 images) is relatively modest compared to other VLM benchmarks

## Confidence
- **High confidence**: The core observation that existing RS benchmarks allow text-only models to achieve competitive accuracy
- **Medium confidence**: The claim that ultra-high resolution is necessary for genuine visual understanding
- **Medium confidence**: The adversarial filtering mechanism appears sound methodologically

## Next Checks
1. Conduct a controlled experiment comparing text-only LLM performance on RSHR-Bench vs existing RS benchmarks to quantify the actual reduction in language prior exploitation
2. Implement and test a model that can truly process native ultra-high resolution (e.g., using GLICAN-style cross-attention) to determine if resolution or architectural limitations drive current performance gaps
3. Perform inter-annotator agreement studies on the human verification process to assess consistency in identifying and removing language-prior-dependent questions