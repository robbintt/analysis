---
ver: rpa2
title: Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning Rate
  Scheduler
arxiv_id: '2508.01483'
source_url: https://arxiv.org/abs/2508.01483
tags:
- cooldown
- learning
- figure
- training
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the poorly understood cooldown phase of
  the Warmup-Stable-Decay (WSD) learning rate scheduler in transformer training. It
  introduces a bias-variance framework showing that different cooldown shapes create
  a fundamental trade-off between exploration (high learning rates) and exploitation
  (low learning rates).
---

# Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning Rate Scheduler

## Quick Facts
- arXiv ID: 2508.01483
- Source URL: https://arxiv.org/abs/2508.01483
- Authors: Aleksandr Dremov; Alexander Hägele; Atli Kosson; Martin Jaggi
- Reference count: 40
- Key outcome: Different cooldown shapes create bias-variance trade-offs, with sqrt and lowered linear 0.7 shapes consistently outperforming alternatives by balancing exploration and exploitation

## Executive Summary
This paper investigates the poorly understood cooldown phase of the Warmup-Stable-Decay (WSD) learning rate scheduler in transformer training. Through extensive experiments, the authors demonstrate that different cooldown shapes create a fundamental bias-variance trade-off, with shapes balancing exploration and exploitation consistently outperforming alternatives. They introduce a bias-variance framework showing that sqrt and lowered linear 0.7 shapes achieve minimum combined bias+variance. The research also reveals that tuning AdamW's β₂ hyperparameter during cooldown yields performance improvements comparable to shape selection, with higher values (approaching 0.999+) showing consistent benefits.

## Method Summary
The study uses a 210M parameter decoder-only transformer (d_model=768, 24 layers, 12 heads) trained on SlimPajama with AdamW optimizer. The WSD scheduler includes warmup (300 steps), stable phase (26,400 steps at max LR), and cooldown (20% of total steps with LR→0). Cooldown shapes tested include linear, cosine, mirror cosine, square, sqrt, and lowered_linear(α). Bias-variance analysis compares averaged model weights across different data shuffles against a reference model trained longer. The authors also systematically vary β₂ during cooldown while keeping β₁=0.9 fixed to measure its impact on final performance.

## Key Results
- Different cooldown shapes create a fundamental bias-variance trade-off, with sqrt and lowered linear 0.7 achieving minimum combined bias+variance
- Higher AdamW β₂ values during cooldown (up to 0.999+) consistently improve final performance by amounts comparable to shape selection
- Loss landscape visualizations support the "river valley" perspective, showing cooldown as descent into a basin orthogonal to the stable-phase valley
- Model souping (averaging multiple cooldown runs with different data orderings) can recover performance lost to high-variance shapes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different cooldown shapes create a fundamental bias-variance trade-off, with optimal shapes (sqrt, lowered linear 0.7) balancing exploration and exploitation.
- Mechanism: Shapes with sustained high learning rates (e.g., mirror cosine) explore more of the loss surface but produce higher variance across runs with different data orderings. Shapes with rapid decay (e.g., lowered linear 0.1) exploit the current region but yield solutions farther from better reference models (higher bias). The optimal shapes minimize bias + variance jointly.
- Core assumption: The reference model trained longer on the same data approximates a better solution achievable by the cooldown shape.
- Evidence anchors:
  - [abstract]: "different cooldown shapes reveal a fundamental bias-variance trade-off in the resulting models, with shapes that balance exploration and exploitation consistently outperforming alternatives"
  - [Section 4.1, Figure 6]: Empirical bias-variance plots show sqrt and lowered linear 0.7 achieve minimum bias+variance; high-variance low-bias shapes cluster separately from low-variance high-bias shapes
  - [corpus]: Related work on WSD dynamics exists (e.g., "Universal Dynamics of Warmup Stable Decay"), but the bias-variance framing for cooldown shapes is novel to this paper
- Break condition: If the number of data permutations N is too small, variance estimates become unreliable, destabilizing the trade-off analysis.

### Mechanism 2
- Claim: The cooldown phase corresponds to descent into a "river valley" in the loss landscape, transitioning from broad exploration to focused basin descent.
- Mechanism: During the stable phase, the optimizer moves along a valley (global optimization direction). During cooldown, it descends orthogonally into the basin. Local gradient steps are nearly orthogonal to the global direction, causing variance while making downstream progress.
- Core assumption: The global optimization direction (pre-cooldown checkpoint → final model) captures the primary descent trajectory.
- Evidence anchors:
  - [Section 7, Figure 12]: Loss landscape visualizations at cooldown start show a clear valley along the global optimization direction; by cooldown end, only the final basin is visible
  - [Section 4.1]: "the observation that a local optimization step (from a single example) is almost orthogonal to the best global direction is exactly what causes the variance"
  - [corpus]: The "river valley" concept was proposed by Wen et al. (2024); this paper provides the first direct visualizations supporting it
- Break condition: If the pre-cooldown model is already in a narrow basin (overtrained), the valley structure may not be visible.

### Mechanism 3
- Claim: Higher AdamW β₂ values during cooldown consistently improve final performance, with impact comparable to cooldown shape selection.
- Mechanism: Increasing β₂ extends the token half-life of the second-moment EMA, allowing the optimizer to retain longer-range gradient information. This stabilizes the descent during cooldown's rapid LR changes.
- Core assumption: The benefit derives from smoother second-moment estimates rather than from interaction with specific LR shapes.
- Evidence anchors:
  - [abstract]: "we observe consistent improvements with higher values of β₂ during cooldown"
  - [Section 6.2, Figure 11]: Varying only β₂ (with β₁=0.9 fixed) shows perplexity decreases as β₂ increases to near-1; the improvement range spans ~0.6 perplexity units, matching cooldown shape variation
  - [corpus]: No direct corpus evidence on β₂ tuning during cooldown specifically; this appears novel
- Break condition: If β₂ is too close to 1.0, the optimizer state barely updates, requiring higher learning rates to compensate.

## Foundational Learning

- Concept: **Warmup-Stable-Decay (WSD) scheduler structure**
  - Why needed here: The cooldown phase only makes sense as the final component of the three-phase schedule. Understanding its role requires knowing what precedes it.
  - Quick check question: Can you explain why WSD allows indefinite continuation during the stable phase while cosine scheduling does not?

- Concept: **AdamW optimizer mechanics (β₁, β₂, weight decay)**
  - Why needed here: Section 6 shows that tuning β₂ during cooldown yields gains comparable to shape selection. Understanding what these parameters control is essential for practical tuning.
  - Quick check question: What does β₂ control in AdamW, and what happens if it is set too close to 1.0?

- Concept: **Bias-variance trade-off in model ensembles**
  - Why needed here: The paper's core theoretical contribution frames cooldown shape selection as a bias-variance problem. Without this background, the trade-off plots are hard to interpret.
  - Quick check question: If averaging multiple cooldown runs from different data orders, would you prefer a high-variance or low-variance shape, and why?

## Architecture Onboarding

- Component map: Pre-cooldown checkpoint -> Cooldown scheduler -> AdamW state -> Final model weights
- Critical path:
  1. Train to stable-phase completion (typically 80% of total tokens)
  2. Select cooldown shape and duration (20% of steps is standard)
  3. Optionally tune β₂ higher (e.g., 0.999+) for cooldown only
  4. Run cooldown, decaying LR to zero
  5. If doing model souping, run multiple cooldowns with different data orders and average weights

- Design tradeoffs:
  - sqrt / lowered linear 0.7: Best single-run performance; balanced bias-variance
  - mirror cosine / square: Best when averaging multiple runs; high variance reduced by ensembling
  - Higher β₂: More stable cooldown descent; may require careful tuning to avoid stagnation
  - Resetting optimizer state: Slight degradation for balanced shapes; can help high-variance shapes by effectively increasing exploration

- Failure signatures:
  - Final perplexity much higher than expected: Check if cooldown shape is too aggressive (e.g., square with low β₂) or too conservative (lowered linear 0.1)
  - High run-to-run variance: Indicates exploration-heavy shape; consider switching to sqrt or averaging multiple runs
  - Cooldown loss plateaus early: LR may be decaying too fast or β₂ too high without compensating LR increase

- First 3 experiments:
  1. Baseline comparison: From the same pre-cooldown checkpoint, run sqrt vs. linear vs. mirror cosine cooldowns. Measure final perplexity and run-to-run variance (3+ seeds each).
  2. β₂ ablation: With sqrt shape, test β₂ ∈ {0.95, 0.99, 0.999, 0.9999} while keeping β₁=0.9 fixed. Plot perplexity vs. β₂.
  3. Model souping test: Run 4 cooldowns with different data shuffles using mirror cosine shape; average the final weights and compare to single sqrt run. Verify whether averaging recovers the performance gap.

## Open Questions the Paper Calls Out

- **Question**: What specific mechanistic changes occur within the model's internal representations during the cooldown stage?
  - Basis: [explicit] The Future Work section states that it "remains unclear what mechanistic changes occur in the model over the course of the cooldown stage."
  - Why unresolved: The authors' initial attempts to measure internal feature quality improvement via linear probing (Appendix I) did not yield useful conclusions.
  - What evidence would resolve it: Applying model diffing frameworks or detailed representational analysis to track how feature geometry shifts specifically during the learning rate decay.

- **Question**: Do the optimal cooldown shapes identified via validation loss (e.g., sqrt, lowered linear 0.7) transfer effectively to downstream task performance?
  - Basis: [explicit] The authors explicitly note that future work should "explore the relevance of observed effects for downstream tasks and model behavior" beyond validation loss.
  - Why unresolved: While lower perplexity generally correlates with better performance, the specific bias-variance trade-offs might impact downstream capabilities differently than they impact the loss landscape.
  - What evidence would resolve it: Evaluating models trained with various cooldown shapes on standardized benchmarks (e.g., reasoning or knowledge probes) rather than just perplexity.

- **Question**: Why do significantly higher values of β₂ in AdamW lead to consistent performance improvements during the cooldown phase?
  - Basis: [inferred] Section 6.2 notes the surprising empirical result that "surprisingly large values [of β₂] tend to yield the best results," but lacks a theoretical explanation.
  - Why unresolved: While the authors link this to token half-life, the exact optimization dynamics that make high β₂ beneficial specifically during decay, rather than stable training, remain undetermined.
  - What evidence would resolve it: A theoretical analysis connecting second-moment estimator dynamics to the annealing rate, or ablation studies isolating β₂ from learning rate interactions.

## Limitations

- The bias-variance framework assumes the reference model represents a better solution, which may not hold if longer training explores fundamentally different loss landscape regions
- Results are based on a single architecture (210M parameter decoder-only transformer) and dataset (SlimPajama subset), limiting generalizability to larger models and different domains
- The study focuses solely on perplexity as a performance metric, leaving open whether findings generalize to other tasks like classification or structured prediction

## Confidence

**High confidence**: The empirical bias-variance trade-off for cooldown shapes is well-supported by the data. The sqrt and lowered linear 0.7 shapes consistently achieve minimum combined bias+variance across multiple runs and data orderings. The visualizations of the loss landscape provide intuitive support for the river valley mechanism.

**Medium confidence**: The claim that β₂ tuning during cooldown yields improvements comparable to shape selection is supported by ablation studies, but the mechanism connecting longer EMA half-lives to smoother descent remains somewhat speculative. The interaction between β₂ values and specific cooldown shapes was not systematically explored.

**Low confidence**: The assertion that model souping (averaging multiple cooldown runs) consistently recovers performance lost to high-variance shapes is demonstrated only for mirror cosine shape. The general conditions under which averaging will or won't compensate for shape-induced variance remain unclear.

## Next Checks

1. **Architecture scaling test**: Replicate the bias-variance analysis with a 1B-2B parameter model on the same task. Measure whether sqrt/lowered linear 0.7 remain optimal or if larger models show different preferences for exploration vs exploitation.

2. **Dataset generalization**: Apply the cooldown shape selection framework to a non-language modeling task (e.g., image classification with Vision Transformer or masked image modeling). Verify whether the bias-variance trade-off manifests similarly across modalities.

3. **β₂ interaction study**: Systematically explore the interaction between cooldown shapes and β₂ values by testing combinations of shapes (sqrt, linear, mirror cosine) with multiple β₂ settings (0.95, 0.99, 0.999, 0.9999). Identify whether certain shapes benefit more from higher β₂ or if the improvement is shape-agnostic.