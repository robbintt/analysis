---
ver: rpa2
title: Expertise Is What We Want
arxiv_id: '2502.20335'
source_url: https://arxiv.org/abs/2502.20335
tags:
- clinical
- cancer
- guidelines
- patient
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the effectiveness of a Large Language Expert
  (LLE) architecture that combines LLMs with rule-based expert systems to automate
  clinical decision support. The LLE architecture was applied to identify pre-treatment
  workup gaps for cancer patients by translating clinical guidelines into structured
  knowledge bases and applying them to unstructured health records.
---

# Expertise Is What We Want

## Quick Facts
- arXiv ID: 2502.20335
- Source URL: https://arxiv.org/abs/2502.20335
- Reference count: 21
- This study demonstrates the effectiveness of a Large Language Expert (LLE) architecture that combines LLMs with rule-based expert systems to automate clinical decision support.

## Executive Summary
This paper introduces the Large Language Expert (LLE) architecture that combines large language models with deterministic rule-based expert systems to automate clinical decision support. The system identifies pre-treatment workup gaps for cancer patients by translating clinical guidelines into structured knowledge bases and applying them to unstructured health records. In a retrospective study of 100 breast and colon cancer cases, the system achieved high accuracy with only 2.1% of extracted clinical decision factors and 4.5% of workup recommendations requiring clinician adjustments. Clinicians were able to complete the review process in under 7.5 minutes per patient case.

## Method Summary
The LLE architecture uses a hybrid approach combining LLM-based extraction with deterministic rule evaluation. Clinical guidelines are translated into versioned knowledge bases containing first-order logic rules and associated clinical decision factors. For each patient, an LLM extracts decision factors from unstructured medical records with citations, then a deterministic logic evaluator applies the rules to generate recommendations. The system uses a two-step review process where clinicians first review the extracted decision factors with citations, then review the generated recommendations. Knowledge bases can be stacked with priority ordering to support institutional customization.

## Key Results
- 97.9% of clinical decision factors were not adjusted by clinicians during review
- 95.5% of workup recommendations were accepted without modification
- Non-specialists unfamiliar with patient cases completed review in under 7.5 minutes per case

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating LLM-based data extraction from deterministic rule evaluation reduces hallucination propagation while preserving reasoning flexibility.
- Mechanism: LLMs extract clinical decision factors as (variable_name, human_readable_question) tuples with citations; these are then evaluated against first-order logic rules deterministically. Errors are caught at the factor extraction step before compounding into downstream recommendations.
- Core assumption: LLMs can reliably extract discrete yes/no/unknown answers from unstructured text when prompted with specific clinical questions, but cannot reliably evaluate complex multi-step logic.
- Evidence anchors:
  - [abstract] "LLMs help address key challenges of Expert Systems, such as integrating and codifying knowledge, and data normalization. Conversely, an Expert System-like approach helps overcome challenges with LLMs, including hallucinations, atomic and inexpensive updates, and testability."
  - [section 3.3] "The rules are first-order logic formulas over the list of clinical decision factors we have at hand, and they can be evaluated deterministically."
  - [corpus] Related work on interpretable hybrid systems for dementia care (arXiv:2507.01282) similarly advocates separating pattern recognition from explicit reasoning, though without direct validation of this specific architecture.
- Break condition: If LLM extraction error rates exceed ~10%, the human-in-the-loop burden becomes impractical; if rules cannot capture guideline nuance, clinicians will override systematically.

### Mechanism 2
- Claim: Versioned, namespaced knowledge bases enable atomic updates and institutional customization without retraining models.
- Mechanism: Guidelines are translated into structured knowledge bases with explicit versioning. At inference, multiple knowledge bases are stacked with priority ordering—e.g., NCCN base → ASCO overlay → institution-specific overrides. Conflicts resolve by KB priority.
- Core assumption: Clinical guidelines can be faithfully represented as first-order logic rules over discrete decision factors; guidelines change incrementally rather than fundamentally.
- Evidence anchors:
  - [abstract] "evaluates them against versioned knowledge bases"
  - [section 3.2] "Once the update is made, the translated guidelines are snapshotted into a versioned artifact that the application can configuratively invoke... a hospital such as UCSF may have NCCN Guidelines at the base, overlay on top of these ASCO Guidelines, and have UCSF-specific Guidelines at the top level."
  - [corpus] No direct corpus validation of KB versioning mechanism; related work focuses on single-model approaches.
- Break condition: If higher-priority rules unintentionally override partial logic from lower-level rules (variable overloading problem), unexpected recommendations emerge; if guidelines require semantic rather than logical representation, translation fails.

### Mechanism 3
- Claim: Explicit exposure of decision factors with citations enables targeted clinician intervention, preventing error compounding.
- Mechanism: The UI shows each decision factor with its answer, explanation, and cited sentences. Clinicians correct individual factors before recommendations are generated. Corrections propagate deterministically through the logic layer.
- Core assumption: Clinicians will trust and engage with system outputs when reasoning is transparent; correcting factors is faster than generating recommendations from scratch.
- Evidence anchors:
  - [section 4.3.1] "97.9% of Clinical Decision Factors (Step 1) were not adjusted by clinician... 2.1% (172 for breast, 88 for colon)" changed.
  - [section 4.3.3] "Non-specialists unfamiliar with patient cases take less than 7.5 minutes to finalize recommendations."
  - [corpus] MATRIX framework (arXiv:2508.19163) emphasizes contextual clinical evaluation but doesn't validate this specific intervention pattern.
- Break condition: If citation retrieval is inaccurate or explanations are unfaithful, clinician trust degrades; if factor count explodes (hundreds per patient), review time becomes prohibitive.

## Foundational Learning

- Concept: **First-order logic (FOL) for clinical rules**
  - Why needed here: The system evaluates recommendations using FOL formulas over decision factors. You need to understand quantifiers (∀, ∃), predicates, and logical connectives to debug rule behavior.
  - Quick check question: Can you write a rule that recommends a test if (age > 65 OR has_family_history) AND NOT previously_completed?

- Concept: **RAG vs. Expert Systems tradeoffs**
  - Why needed here: LLE positions itself as an evolution of RAG toward structured expert systems. Understanding what each approach sacrifices helps you diagnose when the architecture is appropriate.
  - Quick check question: What does RAG sacrifice that expert systems preserve, and vice versa?

- Concept: **Clinical guideline structure (NCCN format)**
  - Why needed here: Knowledge base translation begins with real guidelines. You need to recognize how guidelines encode recommendations, decision points, and conditional logic to build robust extraction pipelines.
  - Quick check question: If a guideline says "consider X for patients with Y or Z," how would you translate this into a testable rule?

## Architecture Onboarding

- Component map:
  - Knowledge Base Pipeline -> Knowledge Base Server -> Extraction Layer -> Logic Evaluator -> Explanation Generator -> Clinical UI

- Critical path: Guideline document → KB translation quality → Decision factor extraction accuracy → Recommendation correctness. Errors compound downstream; the KB translation step is the highest-leverage for system quality.

- Design tradeoffs:
  - **Rigidity vs. flexibility**: KBs require structured format (costly upfront) but enable deterministic testing and atomic updates
  - **Transparency vs. coverage**: Explicit factors are reviewable but may miss edge cases a pure LLM would handle implicitly
  - **Customization depth vs. complexity**: KB stacking enables institutional adaptation but risks logic conflicts

- Failure signatures:
  - **Date calculation errors** (36.6% of Step 1 corrections in study): LLMs struggle with temporal math—delegate to deterministic functions
  - **Incorrect inference** (40.4% of Step 1 corrections): LLM makes unwarranted logical leaps—refine prompts or add few-shot examples
  - **KB ambiguity** (23.0% of Step 1 corrections): Question phrasing allows multiple interpretations—tighten factor definitions

- First 3 experiments:
  1. **Unit test a single KB rule**: Take one extracted rule, generate 20 synthetic patient scenarios with known ground truth, verify logic evaluator produces correct recommendations. This validates the translation pipeline in isolation.
  2. **Measure extraction accuracy by factor type**: Categorize decision factors (demographic, lab value, imaging finding, history) and measure LLM extraction accuracy per category. Identify which factor types require specialized handling.
  3. **Stress-test KB stacking**: Create a scenario where NCCN and institutional rules conflict; verify priority resolution and flag any cases where partial logic is unintentionally overridden.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the LLE architecture effectively detect and manage unintended side-effects when higher-level institutional knowledge bases override partial logic from lower-level guidelines?
- Basis in paper: [explicit] The authors note in Section 3.2 that a risk of the stacking approach is that higher-level rules may unintentionally override partial logic, similar to variable overloading, and suggest this could be part of future functionality.
- Why unresolved: The current implementation relies on keeping knowledge bases granular to minimize risk, but an automated detection mechanism for logical conflicts in stacked namespaces is not yet developed.
- What evidence would resolve it: A demonstration of an automated validation tool that identifies logical contradictions within stacked knowledge bases without requiring manual expert review of every rule interaction.

### Open Question 2
- Question: Does the inclusion of confidence scores for extracted clinical decision factors significantly improve the efficiency or accuracy of the human review process?
- Basis in paper: [explicit] Section 3.3 states that "Future versions may benefit from the inclusion of a confidence score," suggesting it could trigger specific UI behaviors like prompting experts to check low-confidence outputs.
- Why unresolved: The current implementation provides a single output for each factor without a confidence metric, so the utility of such scores in reducing the 2.1% adjustment rate remains theoretical.
- What evidence would resolve it: A comparative study measuring clinician review time and error detection rates between the current binary output interface and a confidence-weighted interface.

### Open Question 3
- Question: Can the LLE system maintain high accuracy (>95%) and low review times when applied to prospective clinical workflows or more complex, less standardized cancer types?
- Basis in paper: [inferred] The study relies on a retrospective design with a limited sample size (100 patients) focusing only on breast and colon cancer from a single academic center.
- Why unresolved: It is unclear if the 7.5-minute review time and low adjustment rates hold true in a live clinical setting where data may be messier or when applied to cancer types with more ambiguous guidelines.
- What evidence would resolve it: Results from a prospective deployment across multiple institutions involving a wider variety of cancer diagnoses, tracking time-to-treatment initiation as a primary outcome.

## Limitations

- Unknown KB schema and prompt engineering details prevent exact replication of the translation pipeline
- Study limited to breast and colon cancer from a single academic center, limiting generalizability
- Retrospective design without prospective clinical outcome measurement or time-to-treatment analysis

## Confidence

- **High confidence**: The hybrid LLM + deterministic rule evaluation mechanism works as described, with strong empirical support from the 97.9% factor accuracy and 7.5-minute review time.
- **Medium confidence**: The versioning and stacking mechanism is conceptually sound but lacks detailed validation; the study demonstrates institutional customization but doesn't stress-test conflict resolution.
- **Medium confidence**: The clinical validity of recommendations, while promising, is limited to retrospective analysis without prospective clinical outcome measurement.

## Next Checks

1. **KB rule verification**: Take 5 randomly selected rules from the KB, create 20 synthetic patient scenarios per rule with known ground truth, and verify the logic evaluator produces correct recommendations. This isolates the translation pipeline quality.
2. **Cross-cancer generalization**: Apply the LLE architecture to a different cancer type (e.g., lung or prostate cancer) with 20-30 patient cases to assess whether the approach generalizes beyond the validated breast/colon cancer domain.
3. **Conflict resolution stress test**: Design scenarios where NCCN, ASCO, and institutional rules conflict, then verify the priority resolution mechanism and identify cases where partial logic is unintentionally overridden.