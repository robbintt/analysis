---
ver: rpa2
title: Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine Translation
arxiv_id: '2502.19104'
source_url: https://arxiv.org/abs/2502.19104
tags:
- gender
- bias
- german
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces WinoMTDE, a new gender bias evaluation test
  set for German machine translation, extending the automatic evaluation method from
  Stanovsky et al. (2019) to German.
---

# Are All Spanish Doctors Male? Evaluating Gender Bias in German Machine Translation

## Quick Facts
- arXiv ID: 2502.19104
- Source URL: https://arxiv.org/abs/2502.19104
- Reference count: 40
- This study introduces WinoMTDE, a new gender bias evaluation test set for German machine translation, extending the automatic evaluation method from Stanovsky et al. (2019) to German.

## Executive Summary
This study introduces WinoMTDE, a new gender bias evaluation test set for German machine translation, extending the automatic evaluation method from Stanovsky et al. (2019) to German. The dataset comprises 288 balanced sentences with occupational stereotypes annotated using German labor statistics. Five commercial MT systems (Google Translate, Microsoft Translator, Amazon Translate, DeepL, SYSTRAN) and GPT-4o-mini were evaluated for their ability to correctly translate gendered German sentences into seven target languages. Results show persistent gender bias across most models, with GPT-4o-mini consistently outperforming traditional MT systems, particularly in accuracy (ranging from 37.0% to 95.8%) and lower gender-based F1-score gaps. The findings highlight the need for more inclusive and equitable MT systems.

## Method Summary
The study extends the WinoMT automatic evaluation method to German, creating a new test set (WinoMTDE) with 288 balanced sentences containing occupational stereotypes. German labor statistics were used to annotate occupations with gender-specific frequencies. Five commercial machine translation systems and GPT-4o-mini were evaluated on their ability to correctly translate gendered German sentences into seven target languages. The evaluation measured accuracy and gender-based F1-score gaps to quantify gender bias in translations.

## Key Results
- GPT-4o-mini consistently outperformed traditional MT systems across all evaluation metrics
- Accuracy ranged from 37.0% to 95.8% across different models and language pairs
- Traditional MT systems showed persistent gender bias, while GPT-4o-mini demonstrated lower gender-based F1-score gaps
- Gender bias patterns were consistent across occupational contexts but varied by target language

## Why This Works (Mechanism)
The evaluation method works by leveraging German labor statistics to create occupation-specific gender stereotypes, then measuring whether translation systems correctly maintain or appropriately alter gender markers when translating between languages. The approach extends automatic gender bias evaluation from English to German by accounting for German's grammatical gender system and occupational gender associations.

## Foundational Learning
- **German grammatical gender system**: Essential for understanding how occupations and professions are marked for gender in German; quick check: verify knowledge of masculine, feminine, and neuter noun classifications in German
- **Occupational gender stereotypes**: Understanding how certain professions are associated with specific genders in German labor statistics; quick check: review German labor data on gender distribution across professions
- **Automatic gender bias evaluation**: Methods for quantifying gender bias in machine translation output; quick check: understand precision, recall, and F1-score calculations in bias evaluation
- **Cross-linguistic gender marking**: How different languages handle gender in professions and pronouns; quick check: compare gender marking in German vs. target languages
- **Binary vs. non-binary gender frameworks**: Limitations of current evaluation methods; quick check: identify where binary gender assumptions might miss bias patterns
- **Statistical annotation of bias**: Using labor statistics to create ground truth for gender associations; quick check: verify statistical significance of occupational gender distributions

## Architecture Onboarding
- **Component map**: Labor statistics -> Occupational stereotypes -> Test sentences -> Translation systems -> Evaluation metrics -> Bias scores
- **Critical path**: Test sentence generation (using labor stats) → Translation by MT systems → Automatic evaluation → Bias quantification
- **Design tradeoffs**: Binary gender framework simplifies evaluation but misses non-binary representations; automatic evaluation is scalable but may miss nuanced bias patterns
- **Failure signatures**: High accuracy but large F1-score gaps indicate correct translation but biased gender selection; consistent errors across multiple systems suggest systematic bias in training data
- **3 first experiments**: 1) Test WinoMTDE on non-occupational contexts, 2) Evaluate with human annotators on ambiguous cases, 3) Test on languages with non-binary gender systems

## Open Questions the Paper Calls Out
None

## Limitations
- Binary gender framework may not capture the full complexity of gender bias across diverse linguistic contexts
- Focus on seven target languages limits generalizability to other language pairs
- Automatic evaluation metrics may not align with human perceptions of gender bias in all cases

## Confidence
- **High confidence**: Relative performance comparisons between GPT-4o-mini and traditional MT systems
- **Medium confidence**: Absolute bias magnitude estimates due to binary framework limitations
- **Medium confidence**: Generalizability to other language pairs beyond German-focused evaluation

## Next Checks
1. Test the WinoMTDE evaluation method on non-occupational sentence contexts (e.g., medical, educational, or domestic scenarios) to assess whether gender bias patterns extend beyond professional domains
2. Expand the evaluation to include non-binary gender representations and languages with more complex gender systems (e.g., Arabic, Hindi, or languages with genderless pronouns) to test the framework's cross-linguistic applicability
3. Conduct human evaluation studies to validate the automatic gender bias metrics, particularly for edge cases where statistical patterns may not align with human perceptions of gender bias or fairness