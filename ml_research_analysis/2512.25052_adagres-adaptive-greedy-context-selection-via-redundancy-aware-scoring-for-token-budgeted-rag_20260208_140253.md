---
ver: rpa2
title: AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for
  Token-Budgeted RAG
arxiv_id: '2512.25052'
source_url: https://arxiv.org/abs/2512.25052
tags:
- redundancy
- selection
- greedy
- context
- candidate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of redundancy in retrieval-augmented
  generation (RAG), where standard top-k retrieval often returns near-duplicate chunks,
  wasting token budget and degrading downstream performance. The core method, AdaGReS,
  introduces a redundancy-aware scoring function that balances query relevance with
  intra-set redundancy penalties, using a greedy selection algorithm under token budget
  constraints.
---

# AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG

## Quick Facts
- arXiv ID: 2512.25052
- Source URL: https://arxiv.org/abs/2512.25052
- Reference count: 8
- Core contribution: Instance-adaptive, greedy context selection algorithm that reduces redundancy in RAG systems under token budget constraints.

## Executive Summary
AdaGReS addresses redundancy in retrieval-augmented generation by introducing a redundancy-aware scoring function that balances query relevance with intra-set redundancy penalties. The method uses a greedy selection algorithm under token budget constraints and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter, eliminating manual tuning. Theoretical analysis shows the objective exhibits epsilon-approximate submodularity, ensuring near-optimal greedy selection. Experiments demonstrate improved redundancy control, context quality, and end-to-end answer quality compared to baselines on both open-domain and biomedical corpora.

## Method Summary
AdaGReS selects context chunks from a retrieved candidate pool using a scoring function that combines query relevance and redundancy penalties. The objective F(q,C) = α·Σsim(q,c) - β·Σ_{i<j}sim(c_i,c_j) is maximized via greedy selection using marginal gain ΔF(x|C_cur) = α·sim(q,x) - β·Σ_{c∈C_cur}sim(x,c). The key innovation is an adaptive β* calculated from pool statistics and token budget: β* = α·E[sim(q,x)] / [(k̄-1)·E[sim(x,y)] + ε], where k̄ ≈ T_max / L̄. The algorithm stops when no positive marginal gain remains or the token budget is exhausted.

## Key Results
- Improved redundancy control through explicit intra-set similarity penalties in the selection objective
- Instance-adaptive calibration eliminates manual tuning of the relevance-redundancy trade-off parameter
- Epsilon-approximate submodularity guarantees near-optimal greedy selection performance
- Demonstrated improvements in both open-domain (Natural Questions) and biomedical (drug) corpora

## Why This Works (Mechanism)

### Mechanism 1: Redundancy-Penalized Marginal Gain
- **Claim:** Selecting context based on marginal information gain rather than raw similarity reduces token waste.
- **Mechanism:** The algorithm calculates the marginal gain ΔF(x|C_cur) of adding a chunk x by subtracting the cumulative similarity of x to the already selected set C_cur from its query relevance.
- **Core assumption:** Embedding cosine similarity serves as a reliable proxy for semantic redundancy and information uniqueness.
- **Evidence anchors:**
  - [Section 3.1]: Defines the scoring function F(q, C) = α S_{qC} - β S_{CC}
  - [Section 3.2]: Details the marginal gain calculation ΔF
  - [Corpus]: Related work "Directed Information γ-covering" similarly uses predictiveness to manage redundancy

### Mechanism 2: Instance-Adaptive Calibration (β)
- **Claim:** A closed-form calibration of the trade-off parameter β eliminates the need for manual tuning across different datasets.
- **Mechanism:** The system calculates β* dynamically using the expected set size k̄ (derived from token budget) and the average pairwise similarity of the candidate pool.
- **Core assumption:** The statistics of the top-N candidate pool (mean similarity) accurately predict the redundancy distribution of the optimal selection set.
- **Evidence anchors:**
  - [Section 3.3]: Derives β* = α·E[sim(q,x)] / [(k̄-1)·E[sim(x,y)] + ε]
  - [Section 5.2.1]: Notes the dynamic β mechanism adjusts effectively when candidate pools have lower overall redundancy
  - [Corpus]: "Context-Picker" also emphasizes dynamic selection, validating the need for instance-level adaptation

### Mechanism 3: Approximate Submodularity Guarantees
- **Claim:** Greedy selection provides near-optimal results because the objective function satisfies ε-approximate submodularity.
- **Mechanism:** The objective function is "modular minus supermodular." While not strictly submodular, the paper proves the violation is bounded by ε = βkδ.
- **Core assumption:** Practical embedding distributions ensure that pairwise similarities δ are typically ≪ 1, keeping the error term ε small.
- **Evidence anchors:**
  - [Section 4.3]: Proves the objective is ε-approximately submodular
  - [Section 4.2]: Explicitly admits "Failure of Strict Submodularity" but bounds the error
  - [Corpus]: Weak direct evidence; "MUSS" discusses subset selection but lacks specific submodularity proofs

## Foundational Learning

- **Concept: Submodularity (Diminishing Returns)**
  - **Why needed here:** This is the theoretical bedrock allowing the use of a fast greedy algorithm.
  - **Quick check question:** If I add a chunk to a small set vs. a large set, does the "value" added decrease? If yes, the function is submodular.

- **Concept: Marginal Gain**
  - **Why needed here:** This is the specific quantity the algorithm maximizes at every step.
  - **Quick check question:** Given a query q and selected set C, if a new chunk x is identical to a chunk in C, what is its marginal gain (assuming β > 0)? (Answer: Near zero or negative).

- **Concept: Token Budgeting (Knapsack Problem)**
  - **Why needed here:** The selection is constrained not by the number of chunks (k), but by the total length (tokens).
  - **Quick check question:** Why is selecting chunks based on marginal gain per token potentially better than just marginal gain? (Relevant to efficient budget use).

## Architecture Onboarding

- **Component map:** Retriever -> Statistical Analyzer -> Calibrator -> Greedy Selector -> Generator
- **Critical path:** The Statistical Analyzer and Calibrator are the new critical steps.
- **Design tradeoffs:**
  - Exact vs. Sampled Stats: Calculating exact average pairwise similarity is expensive (O(N²)). The paper recommends sampling, which trades statistical precision for latency.
  - Fixed vs. Adaptive β: Adaptive β is robust but complex. Fixed β is faster/deterministic but requires per-dataset tuning.
- **Failure signatures:**
  - "The Empty Context": If β is calibrated too high, the marginal gain of all candidates drops below 0 before any are selected.
  - "The Echo Chamber": If the embedding model fails to distinguish nuanced differences, redundancy penalties may under-penalize semantic duplicates.
  - Latency Spike: If N (candidate pool size) is too large and sampling is not implemented for the β calculation.
- **First 3 experiments:**
  1. Baseline Comparison (IOU): Run AdaGReS vs. Top-k (similarity-only) on a validation set. Measure Intersection-over-Union (IOU) to verify if redundant chunks are effectively replaced by diverse ones.
  2. Sensitivity Analysis (β): Run the system with Fixed β (sweeping values) vs. Adaptive β. Plot performance to see if Adaptive β lands near the optimal manual setting without tuning.
  3. Ablation on Redundancy: Inject artificial near-duplicates into the retrieval candidate pool. Verify that AdaGReS performance remains stable while Top-k degrades.

## Open Questions the Paper Calls Out

- **Question 1:** Can reinforcement learning or meta-learning approaches for calibrating the trade-off parameters (β or λ) outperform the proposed closed-form adaptive solution in optimizing downstream task performance?
- **Question 2:** Do multi-pass selection strategies or more refined diversity modeling significantly improve performance over greedy selection in scenarios with extremely non-uniform redundancy distributions?
- **Question 3:** What are the precise computational latency and throughput costs of AdaGReS compared to standard top-k retrieval when processing industrial-scale candidate pools?

## Limitations

- **Calibration Parameter Ambiguity:** The empirical scaling β = λ·β* + β0 introduces an additional tuning parameter not specified in the paper.
- **Embedding Dependency:** The redundancy penalty relies entirely on cosine similarity in embedding space, which may fail in specialized domains without fine-tuning.
- **Theoretical Guarantees Under Budget Constraints:** The ε-approximate submodularity proof assumes selection by count, but the actual implementation uses token budget constraints.

## Confidence

- **High Confidence:** The greedy selection algorithm with marginal gain computation is clearly specified and implementable.
- **Medium Confidence:** The adaptive β mechanism is theoretically justified and empirically effective on tested datasets.
- **Low Confidence:** The claim of ε-approximate submodularity under token budget constraints lacks formal proof.

## Next Checks

1. **Calibration Sensitivity Test:** Implement AdaGReS with both the full adaptive β (including λ and β0) and a simplified version using only β*. Measure performance variance across datasets to quantify the impact of the unspecified scaling parameters.

2. **Embedding Granularity Analysis:** Inject controlled semantic near-duplicates into a test corpus. Run AdaGReS and measure whether the redundancy penalty correctly distinguishes them or incorrectly penalizes distinct information.

3. **Budget Constraint Impact:** Formalize the token budget as a knapsack constraint and empirically measure the gap between greedy selection and the optimal solution (computed via exhaustive search on small instances). Quantify how this gap scales with budget tightness and redundancy levels.