---
ver: rpa2
title: Language Models Predict Empathy Gaps Between Social In-groups and Out-groups
arxiv_id: '2503.01030'
source_url: https://arxiv.org/abs/2503.01030
tags:
- social
- prompt
- emotion
- group
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) replicate
  human-like empathy gaps between social in-groups and out-groups. The authors formulate
  an emotion intensity prediction task where an LLM, assigned a perceiver social identity,
  predicts the intensity of emotion felt by an experiencer with a given social identity.
---

# Language Models Predict Empathy Gaps Between Social In-groups and Out-groups

## Quick Facts
- arXiv ID: 2503.01030
- Source URL: https://arxiv.org/abs/2503.01030
- Reference count: 33
- Language models systematically assign higher emotion intensity scores to in-group members than out-group members across race/ethnicity, nationality, and religion

## Executive Summary
This study investigates whether large language models (LLMs) replicate human-like empathy gaps between social in-groups and out-groups. The authors developed an emotion intensity prediction task where LLMs, assigned a perceiver social identity, predict the intensity of emotion felt by an experiencer with a given social identity. By systematically manipulating perceiver and experiencer identities across three social dimensions (race/ethnicity, nationality, religion), the study measures how predicted emotion intensities differ between in-group and out-group settings. Results demonstrate that LLMs consistently exhibit empathy gaps, assigning higher emotion intensity scores to in-group members than out-group members, with the effect varying by model size and prompt design.

## Method Summary
The authors created an emotion intensity prediction task using 12 emotion words from the Geneva Emotion Wheel (amused, anxious, cheerful, disappointed, happy, nervous, relaxed, sad, scared, surprised, tense, worried). They constructed 6,120 sentences describing situations where an experiencer with a specific social identity feels an emotion. The experimental design manipulated perceiver and experiencer identities across three categories: race/ethnicity (Black, Hispanic, White, Asian), nationality (Americans, Chinese, English, Indians), and religion (Buddhist, Christian, Hindu, Muslim). Three prompt variations were tested: persona prompts (including explicit social identity), scale prompts (numerical emotion intensity scale), and narrative perspective (first vs third person). The primary metric δ measures the difference in predicted emotion intensity between in-group and out-group conditions.

## Key Results
- LLMs consistently assign higher emotion intensity scores to in-group members than out-group members across all three social grouping categories
- Llama-3.1-8B exhibits the strongest intergroup bias among tested models, while larger models show smaller but still significant gaps
- Prompt variations significantly influence empathy gaps: persona prompts increase gaps, scale prompts reduce them, and narrative perspective has mixed effects

## Why This Works (Mechanism)
The study demonstrates that LLMs can capture and reproduce human-like social biases through their training on human-generated text. The emotion intensity prediction task reveals that models have learned associations between social identities and expected emotional responses, likely from patterns in their training data that reflect societal biases and stereotypes. The systematic variation across different social dimensions and prompt designs suggests that these empathy gaps are not artifacts of a single experimental condition but represent a broader capability of LLMs to encode social group dynamics.

## Foundational Learning
- **Social identity theory**: Explains how individuals categorize themselves and others into social groups, leading to in-group favoritism and out-group bias - needed to understand the psychological basis for empathy gaps; quick check: does the model show consistent patterns across different social dimensions?
- **Emotion intensity prediction**: The task of quantifying emotional responses on a numerical scale - needed to operationalize empathy measurement; quick check: are emotion intensity predictions correlated with human ratings?
- **Intergroup bias measurement**: Quantifying differences in treatment or perception between social groups - needed to assess the magnitude of empathy gaps; quick check: what is the statistical significance of observed δ values?
- **Prompt engineering effects**: How different prompt formulations influence model outputs - needed to understand controllability of bias; quick check: which prompt variations minimize or maximize empathy gaps?
- **Model size effects**: Relationship between model parameters and bias manifestation - needed to assess whether larger models show reduced bias; quick check: do larger models show smaller δ values?
- **Cultural factor analysis**: How historical and cultural context affects model predictions - needed to explain variation between specific group pairs; quick check: can cultural factors predict which group pairs show larger gaps?

## Architecture Onboarding
- **Component map**: Emotion sentences -> LLM emotion intensity prediction -> δ calculation (in-group minus out-group) -> Statistical analysis
- **Critical path**: The sequence of creating experimental stimuli, generating LLM predictions, calculating empathy gaps, and performing statistical analysis
- **Design tradeoffs**: The study prioritizes systematic measurement of bias over ecological validity, using controlled prompts rather than natural language interactions
- **Failure signatures**: Inconsistent δ values across similar group pairs, lack of statistical significance, or results that contradict established social psychology findings
- **First experiments**: 1) Replicate δ calculations for a single model-group combination to verify methodology, 2) Test prompt variation effects on a subset of data, 3) Compare emotion intensity predictions with human ratings for validation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What does "perspective-taking" mean operationally for language models, and can it reduce intergroup empathy bias in LLM outputs as it does in humans?
- Basis in paper: The authors state: "it is not clear about the meaning of 'perspective-taking' when it comes to language models" when discussing how psychologists propose this approach to reduce interpersonal bias.
- Why unresolved: Perspective-taking in humans involves cognitive, emotional and behavioral capacities that LLMs may not possess; the translation of this psychological intervention to algorithmic systems remains undefined.
- What evidence would resolve it: Experiments testing whether instructing LLMs to adopt specific perspectives (e.g., via detailed prompts or fine-tuning) reduces the measured empathy gap δ across social groups.

### Open Question 2
- Question: What training data characteristics lead LLMs to acquire intergroup empathy biases, and can targeted data interventions mitigate these biases?
- Basis in paper: The authors explicitly state: "We need to study where they learn the intergroup bias so we can intervene the downstream decision-making tasks such as hiring."
- Why unresolved: The study measures bias at inference time but does not analyze pre-training or fine-tuning corpora; causal links between training data and intergroup bias remain unknown.
- What evidence would resolve it: Controlled pre-training experiments varying the distribution of intergroup narratives, or probing analyses correlating specific training corpus attributes with measured empathy gaps.

### Open Question 3
- Question: How does the intergroup empathy gap manifest for individuals with intersectional identities (e.g., Black Muslim, Hispanic American)?
- Basis in paper: In the Limitations section, the authors state: "social identities are complex... People may have multiple identities, such as Korean American or Chinese American, in addition to identifying as Asian. Groups involving multiple categories have also not been studied."
- Why unresolved: The current study constrains experiments to single-axis categories (race/ethnicity OR nationality OR religion), ignoring that real-world identities are multi-faceted.
- What evidence would resolve it: Extending the experimental paradigm to include perceiver and experiencer identities that span multiple categories, measuring whether empathy gaps are additive, interactive, or exhibit emergent patterns.

## Limitations
- The study uses LLM-generated emotion intensity predictions as proxies for human empathy gaps, which may not directly correspond to actual human empathic responses
- Reliance on a single prompt template across experiments may constrain generalizability to real-world contexts where empathy expression is more nuanced
- Focus on US-centric social identities limits cross-cultural generalizability of findings

## Confidence
- **Primary finding (LLMs exhibit empathy gaps)**: High confidence - consistent directional effects across multiple model architectures and all three social dimensions tested
- **Prompt variation effects**: Medium confidence - expected directional changes but with considerable variation across group pairs
- **Cultural factor analysis**: Medium confidence - methodologically sound but relies on qualitative coding that introduces interpretive uncertainty
- **Cross-cultural generalizability**: Low confidence - study focuses on US-centric social identities with limited exploration of non-Western contexts

## Next Checks
1. **Human validation study**: Conduct parallel experiments with human participants using identical stimuli to establish correlation between LLM predictions and actual human empathy ratings across in-group/out-group conditions
2. **Cross-cultural replication**: Test the same experimental design with LLM models fine-tuned on non-Western datasets and using social identity categories relevant to other cultural contexts (e.g., caste, tribal affiliations)
3. **Temporal stability analysis**: Evaluate whether empathy gap predictions remain stable across different versions of the same model families and over time, particularly for models that receive ongoing training updates