---
ver: rpa2
title: 'DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data
  Science Problems'
arxiv_id: '2601.13591'
source_url: https://arxiv.org/abs/2601.13591
tags:
- data
- text
- sentiment
- vader
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DSAEval introduces a comprehensive benchmark for evaluating data
  science agents on real-world workflows. It features 641 tasks across 285 datasets
  spanning structured and unstructured domains, incorporating multimodal perception,
  multi-query interactions, and multi-dimensional evaluation.
---

# DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems

## Quick Facts
- **arXiv ID:** 2601.13591
- **Source URL:** https://arxiv.org/abs/2601.13591
- **Reference count:** 40
- **Primary result:** Comprehensive benchmark evaluating data science agents on 641 tasks across 285 datasets, revealing strengths in structured data workflows and limitations in unstructured domains and complex tasks

## Executive Summary
DSAEval introduces a comprehensive benchmark for evaluating data science agents on real-world workflows, featuring 641 tasks across 285 datasets spanning structured and unstructured domains. The benchmark incorporates multimodal perception, multi-query interactions, and multi-dimensional evaluation. Assessment of 11 advanced LLMs revealed Claude-Sonnet-4.5 achieving the highest overall score of 8.164, GPT-5.2 demonstrating best efficiency, and MiMo-V2-Flash showing highest cost-effectiveness. Multimodal perception improved performance on vision-related tasks by 2.04% to 11.30%. Agents performed strongly on structured data and routine workflows but struggled with unstructured domains like computer vision and NLP, as well as complex tasks such as model training and optimization.

## Method Summary
DSAEval establishes a benchmark for evaluating data science agents through a comprehensive framework featuring 641 tasks across 285 real-world datasets. The evaluation methodology incorporates multimodal perception capabilities, multi-query interaction patterns, and multi-dimensional scoring metrics. The benchmark was applied to assess 11 advanced large language models across diverse data science domains including structured data analysis, computer vision, and natural language processing. Performance was measured across task completion rates, efficiency metrics, and cost-effectiveness, with particular attention to how multimodal capabilities affect outcomes on vision-related tasks.

## Key Results
- Claude-Sonnet-4.5 achieved the highest overall score of 8.164 among evaluated models
- GPT-5.2 demonstrated the best efficiency, while MiMo-V2-Flash showed highest cost-effectiveness
- Multimodal perception improved performance on vision-related tasks by 2.04% to 11.30%
- Agents showed strong performance on structured data and routine workflows but struggled with unstructured domains and complex tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of real-world data science workflows, incorporating both structured and unstructured data domains. The multi-dimensional evaluation approach captures different aspects of agent performance, from task completion to efficiency and cost considerations. The inclusion of multimodal perception capabilities specifically addresses the growing need for visual data processing in modern data science workflows.

## Foundational Learning
- **Multimodal perception in data science** - Why needed: Enables agents to process and integrate visual information from datasets; Quick check: Measure performance improvement on vision-related tasks with vs. without multimodal capabilities
- **Multi-query interaction patterns** - Why needed: Reflects realistic data science workflows where agents must handle multiple related queries; Quick check: Track query completion rates and task success correlation
- **Structured vs unstructured data handling** - Why needed: Different data types require distinct processing approaches; Quick check: Compare performance metrics across structured and unstructured task categories
- **Cost-effectiveness evaluation** - Why needed: Practical deployment requires balancing performance with resource consumption; Quick check: Calculate cost per task completion across different models
- **Workflow complexity assessment** - Why needed: Real-world data science involves varying task complexities; Quick check: Analyze performance degradation as task complexity increases

## Architecture Onboarding
**Component Map:** Task Generator -> Dataset Repository -> Agent Interface -> Evaluation Engine -> Performance Dashboard
**Critical Path:** Task definition → Dataset selection → Agent execution → Result evaluation → Performance scoring
**Design Tradeoffs:** Comprehensive coverage vs. benchmark complexity, multimodal capabilities vs. computational cost, structured evaluation vs. real-world flexibility
**Failure Signatures:** Performance drops on unstructured data, inability to handle multi-step workflows, inefficiency in complex task scenarios, cost overruns in resource-intensive operations
**First Experiments:** 1) Run baseline tests on structured data tasks to establish performance floors, 2) Test multimodal perception capabilities on vision-only datasets, 3) Evaluate agent performance on progressively complex workflow chains

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation metrics may not fully capture nuanced differences in agent capabilities despite structured framework
- Current multimodal perception implementations show limited effectiveness (2.04% to 11.30% improvement)
- Benchmark may not adequately assess agents' ability to handle truly open-ended or novel data science problems

## Confidence
- **High Confidence:** Benchmark structure and task categorization, relative model performance rankings, multimodal perception improvements
- **Medium Confidence:** Efficiency and cost-effectiveness rankings, sensitivity to implementation details
- **Low Confidence:** Generalizability to entirely novel data science workflows

## Next Checks
1. Conduct longitudinal studies tracking agent performance across extended, real-world data science projects spanning multiple months
2. Implement blind testing with domain experts to validate task completion quality and identify evaluation gaps
3. Test agent performance on synthetic but realistic "adversarial" datasets designed to probe specific weaknesses in multimodal perception and complex workflow handling