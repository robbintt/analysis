---
ver: rpa2
title: 'Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable
  inference'
arxiv_id: '2510.08303'
source_url: https://arxiv.org/abs/2510.08303
tags:
- feature
- shap
- importance
- features
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for AI models to adapt to continuously
  changing conditions in 6G networks, including new features, vendor-specific implementations,
  and evolving service requirements. The authors propose Adaptive Random Forests (ARFs)
  for dynamic feature adaptation and Drift-Aware Feature Importance (DAFI) for efficient
  explainability.
---

# Dynamic Features Adaptation in Networking: Toward Flexible training and Explainable inference

## Quick Facts
- arXiv ID: 2510.08303
- Source URL: https://arxiv.org/abs/2510.08303
- Reference count: 40
- Primary result: DAFI achieves up to 54.68% runtime reduction while improving feature importance accuracy compared to MDI alone

## Executive Summary
This paper addresses the challenge of adapting AI models to continuously changing conditions in 6G networks, where new features, vendor-specific implementations, and evolving service requirements require dynamic adaptation. The authors propose Adaptive Random Forests (ARFs) for dynamic feature adaptation and Drift-Aware Feature Importance (DAFI) for efficient explainability. ARFs demonstrate stable predictions that improve with incremental feature additions, while DAFI selectively invokes computationally expensive SHAP only when distributional drift is detected, defaulting to faster MDI otherwise. The approach combines ARFs' adaptability with DAFI's efficiency, providing a promising framework for flexible AI methods in 6G networks.

## Method Summary
The method combines Adaptive Random Forests (ARFs) for streaming classification with dynamic feature spaces and DAFI for efficient feature importance computation. ARFs update incrementally as new features arrive, maintaining stability without full retraining. DAFI uses Kolmogorov-Smirnov tests to detect distributional drift between consecutive training batches, triggering expensive SHAP computations only when drift exceeds threshold η, otherwise using faster MDI. Dynamic Top-k evaluation accumulates SHAP importance weights until a cumulative threshold (0.8) is reached, adapting to varying feature set sizes. The framework operates on prequential batches with 80:20 train/test splits, starting with 3 features and adding more at specific epochs.

## Key Results
- ARFs improve accuracy from ~0.64 to ~0.72 when two new features are added at epoch 20
- DAFI achieves 54.68% runtime savings on Network dataset while improving Top-k exact match from 0.09 (MDI) to 0.40
- Dynamic Top-k evaluation provides more reliable feature importance assessment than fixed Top-k when feature counts change

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARFs maintain predictive stability and improve accuracy when features are incrementally added during streaming training.
- Mechanism: ARFs use an ensemble of trees that can be updated iteratively without full retraining. New features are incorporated into the tree structure as they become available, allowing the model to leverage additional signal without discarding previously learned patterns.
- Core assumption: The underlying relationship between features and target remains consistent enough that incremental updates, rather than full retraining, are sufficient.
- Evidence anchors:
  - [abstract]: "iterative training of ARFs can effectively lead to stable predictions, with accuracy improving over time as more features are added"
  - [section 5]: "the introduction of two new features at epoch 20 improves the accuracy of the ARF from approximately 0.64 to 0.72"
  - [corpus]: Related work on agentic AI networking (SANNet, AgentNet) assumes dynamic environmental adaptation is feasible but does not specifically address feature-space evolution.
- Break condition: If feature semantics change fundamentally (e.g., same feature name but different measurement scale across vendors), ARF's incremental adaptation may fail without explicit alignment.

### Mechanism 2
- Claim: DAFI reduces feature importance computation time by selectively invoking SHAP only when distributional drift is detected, defaulting to faster MDI otherwise.
- Mechanism: The KS-Test compares empirical cumulative distribution functions between consecutive training batches for each feature. If the accumulated drift statistic exceeds threshold η, DAFI computes SHAP values; otherwise, it uses MDI. This avoids the exponential SHAP runtime growth with feature count and model size.
- Core assumption: Drift detection via KS-Test on training batches correlates with periods when MDI would disagree with SHAP (i.e., when the model is poorly adapted to current data).
- Evidence anchors:
  - [section 3]: "DAFI uses a distributional drift detector to signal when to apply computationally intensive FI methods instead of lighter alternatives"
  - [table 1]: DAFI achieves 54.68% runtime savings on Network dataset while improving Top-k exact match from 0.09 (MDI) to 0.40
  - [corpus]: No direct corpus validation of KS-Test for this specific XAI switching strategy; related XAI work (e.g., Explainable AI-aided Feature Selection for DRL-based V2X) focuses on feature selection, not runtime-adaptive explainers.
- Break condition: If drift occurs in the target variable or label distribution without corresponding feature distribution shifts, KS-Test may not trigger SHAP when needed, reducing FI accuracy.

### Mechanism 3
- Claim: Dynamic Top-k evaluation provides more reliable FI assessment than fixed Top-k when feature counts change over time.
- Mechanism: Instead of selecting a fixed number of top features, Dynamic Top-k accumulates SHAP importance weights in descending order until a cumulative threshold (0.8 in experiments) is reached. This adapts to the varying concentration of importance across different feature set sizes.
- Core assumption: SHAP-derived importance scores are sufficiently reliable ground truth for ranking, even though they are themselves approximations in high-dimensional spaces.
- Evidence anchors:
  - [section 4]: "Dynamic Top-k evaluation...determines the optimal number of top features to consider by accumulating SHAP feature importance weights in descending order"
  - [appendix A.8]: Visual illustration shows Set vs. Exact match definitions
  - [corpus]: Weak corpus signal; no neighboring papers explicitly address dynamic Top-k for evolving feature spaces.
- Break condition: If SHAP values are noisy or inconsistent across batches (possible with small sample sizes), the cumulative threshold may select too many or too few features.

## Foundational Learning

- Concept: **Stream Learning / Online Learning**
  - Why needed here: ARFs operate in a streaming setting where data arrives in batches and models must update incrementally without access to full historical data.
  - Quick check question: Can you explain why prequential evaluation (train-then-test on each batch in order) differs from traditional holdout validation?

- Concept: **SHAP Values and Additive Feature Attribution**
  - Why needed here: SHAP is the "ground truth" FI method in DAFI; understanding its cooperative game theory foundations helps interpret when and why it's computationally expensive.
  - Quick check question: Why does SHAP runtime grow exponentially with the number of features (Eq. 1 in Appendix A.1)?

- Concept: **Distributional Drift Detection (KS-Test, ADWIN)**
  - Why needed here: DAFI's core switching logic depends on detecting when data distributions have shifted enough to warrant expensive recomputation.
  - Quick check question: What is the difference between the KS-Test (batch-wise comparison) and methods like ADWIN (adaptive windowing) for drift detection?

## Architecture Onboarding

- Component map:
  Data Stream -> ARF Model -> Drift Detector (KS-Test) -> FI Engine (SHAP/MDI router) -> Evaluator (Dynamic Top-k)

- Critical path:
  1. Receive batch X_t, detect drift vs. X_{t-1} using KS-Test per feature
  2. If drift detected → train ARF update → compute SHAP on test samples
  3. If no drift → train ARF update → compute MDI on test samples
  4. Normalize FI scores, compute Dynamic Top-k, log runtime and accuracy

- Design tradeoffs:
  - **Threshold η**: Lower values increase SHAP frequency (higher accuracy, higher cost); paper uses η=1 for Electricity/Weather, η=0.125 for Network (larger batches, less pronounced drift)
  - **n_samples**: Number of test samples for FI computation; paper uses 50. Higher values improve stability but increase cost.
  - **n_trees**: More trees improve model accuracy but increase SHAP runtime (see Appendix A.4, Table 4: 500 trees → 1129s SHAP vs. 4.3s MDI)

- Failure signatures:
  - **Class imbalance in batches**: Network dataset reduced to 40 batches because smaller batches occasionally contained only one class
  - **MDI-SHAP disagreement during learning phases**: MDI underperforms when model hasn't adapted to drift yet; DAFI mitigates but doesn't eliminate this
  - **Feature semantics mismatch**: If "same" feature has different meanings across vendors, neither ARF nor DAFI will detect this automatically

- First 3 experiments:
  1. **Baseline FI comparison**: On your target dataset, measure runtime and Top-k agreement of SHAP vs. MDI vs. DAFI across multiple batches with static feature sets. Replicate Table 1 structure.
  2. **Drift sensitivity analysis**: Vary η threshold and plot SHAP invocation rate vs. FI accuracy (Spearman correlation with full SHAP). Identify the Pareto frontier for your data.
  3. **Feature addition stress test**: Start with 3 features, add 1-2 features at fixed epochs, measure ARF accuracy stability and DAFI's ability to detect drift around the addition points. Compare to non-adaptive Random Forest baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ARF and DAFI framework perform in dynamic scenarios involving feature removal or deletion, rather than just feature addition?
- Basis in paper: [explicit] The authors explicitly state in the Conclusion that "future research should focus on... adaptive scenarios (such as feature removal)."
- Why unresolved: The experimental evaluation (Section 4) only simulates the *addition* of features at specific epochs (e.g., adding one feature at epoch 10), leaving the handling of feature deletion or loss untested.
- What evidence would resolve it: Results from experiments where features are dynamically removed from the stream, measuring the model's recovery time and DAFI's accuracy during these removal phases.

### Open Question 2
- Question: Can the Drift-Aware Feature Importance (DAFI) method be effectively integrated with other ensemble models and Feature Importance (FI) methods?
- Basis in paper: [explicit] The Conclusion lists "exploring its integration with other types of ensemble models, and other FI methods" as a direction for future research.
- Why unresolved: The current implementation relies specifically on Adaptive Random Forests (ARFs) and the pairing of SHAP (model-agnostic) with MDI (tree-specific), and it is unclear if the drift-heuristic holds for other architectures.
- What evidence would resolve it: Benchmarks applying DAFI to non-tree-based ensembles (e.g., online boosting methods) or substituting MDI with other lightweight, model-specific importance metrics.

### Open Question 3
- Question: Is there an automated or adaptive mechanism for determining the optimal drift detection threshold (η) for the Kolmogorov-Smirnov test in DAFI?
- Basis in paper: [inferred] The paper notes in Section 3 that η was set manually to 1.0 or 0.125 "experimentally" based on the dataset, implying a lack of a theoretical or automated tuning method.
- Why unresolved: The efficacy of DAFI depends on this threshold to trigger the expensive SHAP calculations; a static or manually tuned threshold may not generalize well to unseen network conditions with different batch sizes.
- What evidence would resolve it: A sensitivity analysis showing the impact of different η values on runtime versus accuracy, or a proposed algorithm that adapts η dynamically based on stream statistics.

## Limitations
- Feature semantics mismatch across vendors is not addressed, potentially causing ARF-DAFI to fail when "same" features have different meanings
- KS-Test drift detection effectiveness for SHAP/MDI switching is empirically validated but theoretically ungrounded
- Runtime savings claims are dataset-specific and may not generalize to high-dimensional feature spaces

## Confidence
- Medium confidence in ARF-adaptation claims due to controlled synthetic feature schedules rather than unpredictable evolution
- Low confidence in vendor-specific feature adaptation claims due to lack of semantic drift handling
- Medium confidence in DAFI runtime savings based on experimental results but limited generalization

## Next Checks
1. **Cross-vendor feature alignment test**: Evaluate ARF-DAFI on datasets with known feature semantics shifts (e.g., simulated vendor-specific scaling differences) to verify robustness to feature meaning drift.

2. **Real-time constraint validation**: Implement DAFI in a simulated 6G network controller with latency budgets to confirm that runtime savings don't compromise real-time inference requirements.

3. **Dynamic feature semantics test**: Design an experiment where features are added with altered measurement scales or units to test whether ARF-DAFI can detect and adapt to semantic rather than just statistical drift.