---
ver: rpa2
title: Neural Conditional Transport Maps
arxiv_id: '2505.15808'
source_url: https://arxiv.org/abs/2505.15808
tags:
- uni00000013
- transport
- conditioning
- neural
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a neural framework for learning conditional
  optimal transport (OT) maps that can handle both categorical and continuous conditioning
  variables simultaneously. The core innovation is a hypernetwork architecture that
  generates transport layer parameters based on conditioning inputs, enabling adaptive
  mappings between probability distributions.
---

# Neural Conditional Transport Maps

## Quick Facts
- arXiv ID: 2505.15808
- Source URL: https://arxiv.org/abs/2505.15808
- Reference count: 24
- Primary result: Neural hypernetwork architecture learns conditional optimal transport maps that outperform simpler conditioning methods on climate and IAM applications

## Executive Summary
This paper introduces a neural framework for learning conditional optimal transport (OT) maps that can handle both categorical and continuous conditioning variables simultaneously. The core innovation is a hypernetwork architecture that generates transport layer parameters based on conditioning inputs, enabling adaptive mappings between probability distributions. This approach extends the Neural Optimal Transport framework with flexible conditioning mechanisms that outperform simpler methods like concatenation or feature modulation.

The method is evaluated on two real-world applications: climate economic impact distribution transport and global sensitivity analysis for integrated assessment models. For the climate application, the model successfully generates realistic distributions of GDP per capita with climate damages across different Shared Socioeconomic Pathways (SSPs) and time horizons, capturing scenario-specific patterns and uncertainty. In the sensitivity analysis application, the neural approach accurately reproduces traditional simplex-based OT costs while offering significantly higher computational efficiency.

## Method Summary
The framework learns conditional transport maps T(x,z,c) between source P and target Q distributions conditioned on c, which may include both discrete and continuous variables. The architecture uses a hypernetwork to generate the final encoder layer weights based on conditioning inputs, creating adaptive mappings. A two-phase training procedure first pre-trains the transport map toward identity and the critic toward smoothness and transport objectives, then runs alternating minimax optimization. The method employs sinusoidal positional encoding for continuous variables and learnable embeddings for discrete variables, unified through an MLP before conditioning the transport layers.

## Key Results
- Hypernetwork-based conditioning with positional encoding for continuous variables and pretraining achieves superior performance across benchmarks
- Successfully models climate economic impacts across SSPs and time horizons with realistic uncertainty quantification
- Accurately reproduces simplex-based OT costs in IAM sensitivity analysis with significantly higher computational efficiency
- Ablation studies demonstrate the necessity of each component for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hypernetwork-generated transport parameters enable condition-specific mappings that outperform simpler conditioning methods.
- Mechanism: A shallow MLP hypernetwork takes the conditioning vector c and generates the final encoder layer weights W and biases b directly, computing y = hW + b. This allows fundamentally different linear transformations per condition value rather than modulating shared features.
- Core assumption: Conditional transport problems require meaningfully different mapping geometries across conditions, not just shifted representations.
- Evidence anchors:
  - [abstract] "At the core of our method lies a hypernetwork that generates transport layer parameters based on these inputs, creating adaptive mappings that outperform simpler conditioning methods."
  - [section 3.4] "Unlike in feature modulation, generating weights allows fundamentally different transformations per condition... providing the expressiveness of separate networks per condition without its increased computational cost."
  - [corpus] Related work on conditional normalizing flows (arXiv:2505.22364) addresses similar conditional transport but via invertible transformations rather than hypernetworks—limited direct comparison available.

### Mechanism 2
- Claim: Unified conditioning on mixed categorical and continuous variables via embeddings and positional encoding preserves the structure of both variable types.
- Mechanism: Discrete variables map to learnable embeddings e_k ∈ R^{d_c}. Continuous variables pass through sinusoidal positional encoding PE(p, 2i) = sin(p/10000^{2i/d}) (and cos for odd indices). These are concatenated and processed through an MLP with SiLU activations to produce unified c.
- Core assumption: Positional encoding preserves sufficient continuous structure for the hypernetwork to interpolate meaningfully between nearby condition values.
- Evidence anchors:
  - [section 3.3] "For discrete variables... we use learnable embeddings. For continuous variables... we use sinusoidal positional encodings to preserve the continuous nature."
  - [table 1] Ablation shows positional encoding and Fourier features outperform scalar encoding on both datasets; scalar drops ρ from 0.942 to 0.305 on IAM data.
  - [corpus] No direct corpus comparison of encoding strategies for conditional OT—this appears novel to this work.

### Mechanism 3
- Claim: Two-phase pre-training stabilizes adversarial minimax optimization by initializing transport and critic networks in favorable basins.
- Mechanism: Phase 1 pre-trains T toward identity mapping (L^pre_T = E[||T(x,z,c) - x||²]) and f with multi-objective loss combining smoothness, transport objective, and magnitude control. Phase 2 runs alternating minimax optimization with KT=4-6 transport updates per critic update.
- Core assumption: Random initialization creates highly non-linear maps far from feasible transport, causing early-training instability that pre-training can cheaply correct.
- Evidence anchors:
  - [section 3.5] "Randomly initialized networks may implement highly non-linear transformations far from identity, leading to unstable optimization."
  - [table 1] Pre-training improves Wasserstein distance from 0.214 to 0.170 on climate data with negligible time overhead (466s vs 451s).
  - [corpus] Neural Hamilton-Jacobi flows (arXiv:2510.01153) addresses OT stability via characteristic methods rather than pre-training—orthogonal approach.

## Foundational Learning

- Concept: **Kantorovich dual formulation of optimal transport**
  - Why needed here: The entire framework builds on the dual form K(μ,ν) = sup_f[E_X[f^k(x)] + E_Y[f(y)]] and its minimax reformulation. Without this, the critic-transport adversarial structure is opaque.
  - Quick check question: Can you explain why the dual formulation enables neural network parametrization more naturally than the primal?

- Concept: **Hypernetworks (Ha et al., 2017)**
  - Why needed here: The core conditioning mechanism is a hypernetwork. Understanding that hypernetworks learn to generate weights for another network—trading compute for parameter sharing across conditions—is essential.
  - Quick check question: What is the memory overhead of generating d×n weights per forward pass versus storing K separate networks for K conditions?

- Concept: **Minimax optimization and adversarial balance**
  - Why needed here: Training alternates between maximizing critic loss and minimizing transport loss. Understanding why KT>1 transport steps per critic step helps balance learning is critical for debugging divergence.
  - Quick check question: Why might the critic learning "too fast" destabilize transport learning, and how does the asymmetric update schedule address this?

## Architecture Onboarding

- Component map: Input [x,z] -> Encoder -> h_L -> Hypernetwork(c) generates W,b -> y = h_L·W + b -> Decoder -> T(x,z,c)

- Critical path: Input [x,z] → Encoder → h_L → Hypernetwork(c) generates W,b → y = h_L·W + b → Decoder → T(x,z,c)

- Design tradeoffs:
  - Hypernetwork vs concatenation: +accuracy, +expressivity, modest +compute (466s vs 451s in ablation)
  - Separate vs shared embeddings for T and f: Separate improves accuracy (Table 1), doubles embedding parameters
  - Positional vs scalar encoding: Critical for continuous conditions (ρ: 0.942 vs 0.305), adds fixed computational overhead
  - Asymmetric decoder depth (8 vs 3): T needs more capacity for complex transformations; f only evaluates quality

- Failure signatures:
  - Gradient explosion early in training → Check pre-training ran for 500+ steps; increase smoothness term λ_smooth
  - Mode collapse (all conditions produce similar outputs) → Hypernetwork may be undercapacity; increase hidden size from 128
  - Poor interpolation between continuous condition values → Positional encoding dimension may be too low; increase from 64
  - Critic loss diverges → Reduce f learning rate (try 1e-5); increase weight decay (0.03 is baseline)

- First 3 experiments:
  1. Replicate unconditional ablation (Figure 3) on Black Hole or Swiss Roll with orthogonal init + residual + layer norm to validate architectural baseline before adding conditioning.
  2. Run conditional ablation on climate data with only hypernetwork vs only concatenation to isolate conditioning contribution—expect ~2x Wasserstein improvement.
  3. Test pre-training ablation: train with and without pre-training on IAM data, plot convergence curves for first 1000 epochs to verify stability claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be effectively extended to handle complex conditioning modalities such as CLIP text embeddings or pixel-wise semantic labels?
- Basis in paper: [explicit] The authors explicitly state in the "Limitations and Future Work" section that they "have not explored conditioning on complex modalities like CLIP... embeddings or pixel-wise semantic labels."
- Why unresolved: The current architecture uses learnable embeddings for discrete variables and positional encodings for scalars, which may not suffice for high-dimensional or dense input structures like images or text.
- What evidence would resolve it: Demonstration of NCTM successfully learning transport maps conditioned on text prompts or segmentation maps in a generative modeling task.

### Open Question 2
- Question: Do recurrent architectures improve the modeling of temporal dynamics in conditional optimal transport compared to the evaluated residual feedforward networks?
- Basis in paper: [explicit] The authors note in the Limitations section that they "primarily evaluated residual feedforward networks, not testing recurrent architectures."
- Why unresolved: The climate and economic data used involve temporal progressions (years), but the current architecture treats time as a static conditioning value rather than capturing sequential dependencies.
- What evidence would resolve it: A comparative study showing that RNN or Transformer-based transport maps outperform the ResNet baseline on datasets with strong temporal autocorrelation.

### Open Question 3
- Question: Can the computational overhead of the hypernetwork be reduced to match simpler conditioning methods without sacrificing mapping expressivity?
- Basis in paper: [explicit] The authors acknowledge that "our hypernetwork approach incurs higher computational costs than simpler alternatives."
- Why unresolved: While ablation studies show hypernetworks provide superior accuracy, the mechanism of generating layer weights dynamically is parameter-heavy compared to concatenation or feature modulation.
- What evidence would resolve it: The development of a lightweight or sparse hypernetwork architecture that retains the high Pearson's $\rho$ and low Wasserstein distance scores while reducing training time and parameter count to levels comparable with FiLM or AdaIN.

### Open Question 4
- Question: Can the NCTM framework theoretically and empirically connect to diffusion models or normalizing flows to improve generative tasks?
- Basis in paper: [explicit] The authors list "connections to diffusion models and normalising flows" as a specific avenue for future work.
- Why unresolved: The paper focuses on extending the Neural Optimal Transport (NOT) framework, but does not explore how these transport maps might integrate with other dominant generative modeling paradigms.
- What evidence would resolve it: A hybrid model that utilizes NCTM for conditional distribution alignment within a diffusion training objective, resulting in improved sample quality or faster convergence.

## Limitations
- Computational overhead of hypernetwork conditioning compared to simpler methods like concatenation or feature modulation
- Limited evaluation to residual feedforward networks without exploring recurrent or Transformer architectures
- No exploration of complex conditioning modalities like text embeddings or pixel-wise semantic labels
- Data preprocessing details for climate and IAM datasets not fully specified

## Confidence
- High confidence in hypernetwork conditioning mechanism (ablations show clear performance gains)
- Medium confidence in two-phase pre-training benefit (modest improvements, limited comparative analysis)
- Medium confidence in unified conditioning framework's necessity (alternative encodings degrade performance but don't fail catastrophically)

## Next Checks
1. Verify pre-training ablation: train with and without pre-training on IAM data, plot convergence curves for first 1000 epochs to confirm stability claims
2. Test encoding strategy sensitivity: run with only hypernetwork vs only concatenation on climate data to isolate conditioning contribution
3. Validate architectural baseline: replicate unconditional ablation on Swiss Roll with orthogonal init + residual + layer norm before adding conditioning