---
ver: rpa2
title: Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning
arxiv_id: '2509.15147'
source_url: https://arxiv.org/abs/2509.15147
tags:
- client
- learning
- federated
- clients
- aggregation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes logit-based federated learning to reduce communication
  costs by exchanging only model logits on a shared public dataset instead of gradients
  or weights. Three aggregation strategies are introduced: simple averaging, uncertainty-weighted
  averaging (UWA) using Gaussian mixture models to model client-specific logit distributions,
  and a learned meta-model aggregator.'
---

# Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning

## Quick Facts
- arXiv ID: 2509.15147
- Source URL: https://arxiv.org/abs/2509.15147
- Reference count: 22
- Primary result: Logit-based FL with UWA improves accuracy under high non-IID data heterogeneity

## Executive Summary
This paper introduces logit-based federated learning to reduce communication costs by exchanging model logits on a shared public dataset instead of gradients or weights. Three aggregation strategies are proposed: simple averaging, uncertainty-weighted averaging using Gaussian mixture models, and a learned meta-model aggregator. Evaluated on MNIST and CIFAR-10 with non-IID data partitions, the methods show improved accuracy over simple averaging, especially under high heterogeneity conditions.

## Method Summary
The approach uses a shared public dataset accessible to all clients. Instead of transmitting gradients or model weights, clients exchange their model's logits (pre-softmax outputs) computed on this public data. Three aggregation methods are introduced: simple averaging of logits, uncertainty-weighted averaging (UWA) where client weights are determined by modeling each client's logit distribution as a Gaussian mixture and computing its uncertainty, and a learned meta-model aggregator that predicts optimal aggregation weights. The server aggregates these logits and updates the global model, which is then sent back to clients.

## Key Results
- UWA improves accuracy significantly when clients have few classes (e.g., 0.9454 vs 0.8188 on MNIST with 2 classes per client)
- Meta-model aggregator consistently yields the best results across conditions
- All logit-based methods reduce communication overhead compared to traditional gradient-based FL

## Why This Works (Mechanism)
The mechanism works by leveraging the shared public dataset to create a common reference point for client model comparison. Since logits represent the model's confidence before normalization, they capture uncertainty information that can be used to weight contributions. The UWA method models each client's logit distribution as a Gaussian mixture to estimate uncertainty, giving less weight to unreliable clients. The meta-model learns to predict optimal aggregation weights based on client characteristics, adapting to different heterogeneity levels.

## Foundational Learning

1. **Federated Learning Basics** - Distributed ML where clients train locally and share updates with a central server. Why needed: Understanding the communication bottleneck and heterogeneity challenges. Quick check: Can explain difference between FedAvg and federated distillation.

2. **Logit Representation** - Raw model outputs before softmax activation. Why needed: Forms the basis of the communication-efficient aggregation. Quick check: Can compute logits from a trained model on sample data.

3. **Gaussian Mixture Models** - Probabilistic models representing data as mixture of multiple Gaussian distributions. Why needed: Used to model client-specific logit distributions and estimate uncertainty. Quick check: Can fit a GMM to sample data and extract uncertainty metrics.

4. **Non-IID Data Partitions** - Data distribution where clients have different class distributions. Why needed: Primary challenge being addressed, affects aggregation performance. Quick check: Can create Dirichlet-based non-IID partitions from a dataset.

## Architecture Onboarding

Component Map: Clients -> Logit Exchange -> Server Aggregation -> Global Model -> Clients

Critical Path: Client training → Logit computation on public data → Server aggregation → Model update → Client download

Design Tradeoffs:
- Communication efficiency vs aggregation accuracy
- GMM complexity vs UWA performance
- Meta-model training cost vs consistent improvement

Failure Signatures:
- Poor performance with highly diverse client models
- Increased uncertainty estimation errors with limited public data
- Meta-model overfitting to specific heterogeneity patterns

First Experiments:
1. Test simple averaging vs weight averaging on MNIST with 2-class non-IID partition
2. Implement UWA with GMM and compare against simple averaging under varying class distributions
3. Train and evaluate meta-model aggregator on CIFAR-10 with 5-class non-IID partition

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Focus on controlled synthetic non-IID partitions may not capture real-world heterogeneity
- GMM assumption for UWA may not hold for all client distributions
- Computational overhead of meta-model aggregator not thoroughly evaluated against communication savings

## Confidence

High:
- Experimental results demonstrate clear improvements over simple averaging in controlled settings
- Method reduces communication overhead as claimed

Medium:
- Generalizability to real-world scenarios with larger, more complex datasets
- Effectiveness under realistic client participation patterns and dropout
- Privacy implications of logit sharing not fully addressed

## Next Checks

1. Test aggregation strategies on larger-scale datasets (CIFAR-100, ImageNet subsets) with Dirichlet-based non-IID partitions

2. Evaluate computational overhead and wall-clock time of meta-model aggregator compared to communication savings under frequent update scenarios

3. Assess robustness to label noise, client dropout, and malicious clients sending poisoned logits