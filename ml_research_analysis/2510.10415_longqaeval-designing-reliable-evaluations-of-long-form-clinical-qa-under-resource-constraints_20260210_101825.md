---
ver: rpa2
title: 'LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under
  Resource Constraints'
arxiv_id: '2510.10415'
source_url: https://arxiv.org/abs/2510.10415
tags:
- fine-grained
- annotation
- coarse
- annotations
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LONGQAEVAL introduces a framework for evaluating long-form clinical
  question answering under resource constraints. It compares coarse answer-level and
  fine-grained sentence-level annotation designs across three dimensions: correctness,
  relevance, and safety.'
---

# LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints

## Quick Facts
- arXiv ID: 2510.10415
- Source URL: https://arxiv.org/abs/2510.10415
- Reference count: 16
- Key outcome: Annotation granularity effectiveness varies by evaluation dimension—fine-grained improves agreement for factual correctness while coarse works better for context-dependent relevance.

## Executive Summary
LONGQAEVAL introduces a framework for evaluating long-form clinical question answering under resource constraints by comparing coarse answer-level and fine-grained sentence-level annotation designs. Using 300 real patient questions answered by physicians and LLMs, the study systematically evaluates three dimensions—correctness, relevance, and safety—across different annotation granularities. The framework demonstrates that annotation design effectiveness varies by dimension, with fine-grained annotations improving inter-annotator agreement for factual correctness while coarse annotations work better for context-dependent relevance. Annotating just three sentences per answer achieves comparable reliability to full fine-grained annotation while reducing cost, and LLM-as-judge evaluation can be reliable when using fine-grained instructions and 3-point scales.

## Method Summary
The study evaluates 100 questions from the K-QA dataset, each answered by a physician, GPT-4, and Llama-3.1-Instruct-405B (300 QA pairs total). Six physician annotators were split into two groups, with each QA pair rated by three physicians using both coarse (whole-answer) and fine-grained (sentence-level) annotation designs. Annotations covered three dimensions—correctness, relevance, and safety—using 5-point Likert scales that were collapsed to 3-point for analysis. Fine-grained binary aggregation used dimension-specific logic: any incorrect sentence invalidates correctness, while at least one relevant sentence validates relevance. Partial fine-grained sampling evaluated three sentences per answer to assess cost-reliability trade-offs. LLM-as-judge experiments used GPT-4o with human codebooks for comparison.

## Key Results
- Annotation granularity effectiveness varies by dimension—fine-grained improves agreement for factual correctness (κ = 0.90 vs 0.74 coarse) while coarse works better for context-dependent relevance (κ = 0.71 vs 0.29-0.32 fine-grained).
- Annotating just three sentences per answer achieves comparable reliability to full fine-grained annotation while reducing cost, with correlation coefficient above 0.8 with six-sentence annotations.
- LLM-as-judge evaluation is reliable for correctness and relevance when using fine-grained instructions and 3-point scales, with GPT-4 and Llama-3.1-Instruct-405B performing comparably to physicians on these dimensions, though safety remains weak for all systems.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Annotation granularity effectiveness varies systematically by evaluation dimension—fine-grained improves agreement for factual correctness while coarse works better for context-dependent relevance.
- Mechanism: Factual dimensions (correctness) align with discrete sentence-level verification against medical knowledge. Context-dependent dimensions (relevance) require holistic judgment across multiple sentences, making sentence-level evaluation fragment the very signal needed for assessment.
- Core assumption: The cognitive task structure differs by dimension—fact-checking is localized, relevance-judgment is distributed.
- Evidence anchors: [abstract] "fine-grained annotation improves agreement on correctness, coarse improves agreement on relevance"; [section 4] "For correctness, agreement is substantially higher in the fine-grained setting (0.90, 0.88) compared to the coarse 3-point setting (0.74). In contrast, annotators reached higher agreement in the coarse setting (0.71) than in the fine-grained settings (0.32, 0.29) in the case of relevance."

### Mechanism 2
- Claim: Partial fine-grained annotation (3 sentences per answer) preserves reliability while halving annotation cost.
- Mechanism: Variance in fine-grained ratings decreases rapidly with additional sentences because clinical answers tend to be internally consistent. Three representative sentences capture the quality signal with diminishing returns beyond that point.
- Core assumption: Answer quality is not highly variable at the sentence level within a single response.
- Evidence anchors: [abstract] "Annotating just three sentences per answer achieves comparable reliability to full fine-grained annotation while reducing cost."; [section 4] "ratings based on assessing only three sentences per answer already correlate strongly (correlation coefficient above 0.8) with six-sentence annotations... variance is lower than coarse annotations for correctness and safety."

### Mechanism 3
- Claim: Fine-grained annotations mitigate length-related bias in system-level comparisons.
- Mechanism: Coarse evaluation conflate answer length with perceived quality—longer, elaborated LLM responses appear more complete even when sentence-level accuracy matches shorter physician answers. Sentence-by-sentence evaluation forces assessors to judge each unit independently, separating informational content from presentation style.
- Core assumption: Annotators implicitly associate length with thoroughness or accuracy during holistic assessment.
- Evidence anchors: [section 4] "physician answers receive much lower correctness ratings in the coarse setting (0.78) than in the fine-grained one (0.99)... We hypothesize that this biases annotators toward rating model-generated answers higher, despite comparable correctness"; [section 4] "Physician answers are consistently shorter and more to the point than model-generated answers."

## Foundational Learning

- Concept: Inter-annotator agreement (IAA) metrics (Randolph's κ, Fleiss' κ, Krippendorff's α)
  - Why needed here: The paper's central claim rests on comparing IAA across annotation designs. Randolph's κ is chosen because it better handles agreement among multiple expert raters than alternatives.
  - Quick check question: Why would Fleiss' κ understate agreement when three physicians evaluate each QA pair?

- Concept: Rating scale collapsing (5-point to 3-point)
  - Why needed here: The paper collapses fine distinctions to improve reliability, following prior work showing this reduces noise in subjective tasks. Understanding when to aggregate is critical for practical implementation.
  - Quick check question: What information is lost when collapsing "Partially Agree" and "Agree" into a single category?

- Concept: Binary aggregation schemes for sentence-level ratings
  - Why needed here: Fine-grained sentence ratings must be aggregated to compare against coarse answer-level scores. The paper uses dimension-specific logic (any incorrect sentence invalidates correctness; at least one relevant sentence ensures adequacy).
  - Quick check question: Why does the aggregation logic differ between correctness (any-failure) and relevance (any-success)?

## Architecture Onboarding

- Component map: Dataset layer (100 questions × 3 answer sources) -> Annotation layer (6 physicians in 2 groups, mixed coarse/fine-grained) -> Evaluation dimensions (Correctness, Relevance, Safety) -> Aggregation layer (5-point→3-point collapsing, sentence→answer binary) -> Judge layer (GPT-4o with human codebooks)

- Critical path: 1) Define evaluation dimensions with detailed codebooks; 2) Pilot annotation to validate granularity choice per dimension; 3) Deploy partial fine-grained sampling (3 sentences) for correctness/safety; 4) Use coarse annotation for relevance; 5) Collapse ratings and compute Randolph's κ for reliability assessment

- Design tradeoffs:
  - Fine-grained accuracy vs. annotation cost: Full sentence-level is expensive; 3-sentence sampling balances reliability and efficiency
  - Granularity vs. context: Sentence-level evaluation fragments context needed for relevance judgments
  - Rating detail vs. reliability: 5-point scales introduce noise; 3-point improves agreement but loses granularity
  - LLM-as-judge vs. human experts: GPT-4o achieves comparable agreement for correctness/relevance but safety agreement remains low

- Failure signatures:
  - Low IAA for safety across all designs (κ = 0.24–0.43) indicates fundamental subjectivity or underspecified codebook
  - Fine-grained relevance agreement dropping to 0.29 vs. 0.71 coarse signals broken granularity choice
  - Large rating variance between partial and full annotation indicates sampling insufficient
  - Physician ratings systematically lower than LLM in coarse but not fine-grained signals length bias

- First 3 experiments:
  1. Replicate granularity-dimension mapping on a different clinical dataset (e.g., specialized care) to test generalization beyond primary care questions.
  2. Vary partial sampling size (2, 3, 4, 5 sentences) to identify optimal cost-reliability frontier for each dimension.
  3. Test LLM-as-judge with different judge models and prompt variations to determine whether fine-grained instruction benefits extend beyond GPT-4o.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-grained annotation design improve intra-rater reliability (IRR) across evaluation dimensions in clinical QA?
- Basis in paper: [explicit] "Future work should investigate whether a fine-grained design improves IRR across dimensions... Our findings are based on a limited sample of question–answer pairs, and further work is needed to validate these observations."
- Why unresolved: The IRR analysis was based on only 6 repeated QA pairs per annotator, which the authors acknowledge was too small to achieve statistical significance or draw strong conclusions.
- What evidence would resolve it: A larger-scale study with more repeated annotations per annotator, powered to detect differences in IRR between coarse and fine-grained settings across correctness, relevance, and safety.

### Open Question 2
- Question: Can annotation design strategies or codebook refinements substantially improve inter-annotator agreement for safety evaluations in clinical QA?
- Basis in paper: [explicit] "Judgments on safety remain inconsistent... Safety shows low agreement in both settings" and "safety remains a persistent weakness for all models and physicians. This highlights the need for targeted evaluation and improvement."
- Why unresolved: Neither coarse nor fine-grained annotation achieved acceptable IAA for safety (κ = 0.24–0.43), suggesting fundamental challenges in operationalizing safety criteria even among medical experts.
- What evidence would resolve it: Experiments testing alternative safety definitions, more granular sub-criteria (e.g., separating severity levels), or structured checklists to determine if different annotation approaches can achieve κ > 0.6.

### Open Question 3
- Question: Do the observed annotation design effects generalize to clinical questions requiring specialized medical expertise?
- Basis in paper: [explicit] "Our dataset... consists of only questions on general primary care topics, which may limit the applicability of our findings to clinical questions about conditions that require specialized care" and "More work is needed to confirm... whether model performance generalizes to specialized care."
- Why unresolved: The study used only general primary care questions; specialized domains (e.g., oncology, cardiology) may have different inter-annotator agreement patterns or require different annotation granularity choices.
- What evidence would resolve it: Replication of the LONGQAEVAL framework on specialized clinical QA datasets with domain experts, comparing IAA patterns across annotation designs.

### Open Question 4
- Question: How robust are LLM-as-judge evaluations across different clinical QA models and prompt formulations?
- Basis in paper: [explicit] "Our evaluation focuses on only two widely used language models, GPT-4 and Llama-3.1-Instruct-405B... Future work should investigate... a broader range of models to validate and extend these findings."
- Why unresolved: Only one judge model (GPT-4o) was tested, and fine-grained instructions did not consistently improve LLM-expert agreement across dimensions; it remains unclear whether these patterns hold for other models or prompt variations.
- What evidence would resolve it: Systematic comparison of multiple LLM-as-judge models (e.g., Claude, Gemini, smaller open models) using both coarse and fine-grained prompting, measuring agreement with expert annotators.

## Limitations

- Safety dimension evaluation faces significant reliability challenges with inter-annotator agreement (κ = 0.24-0.43) substantially lower than correctness and relevance, suggesting fundamental subjectivity or underspecified codebook.
- The 3-sentence sampling strategy assumes uniform quality distribution within answers, a claim not empirically validated across diverse clinical domains or question types.
- The study uses only general primary care questions, limiting generalizability to specialized clinical domains that may have different inter-annotator agreement patterns or annotation requirements.

## Confidence

- **High Confidence:** The systematic variation in annotation granularity effectiveness by dimension (fine-grained for correctness, coarse for relevance) is well-supported by robust inter-annotator agreement metrics across 300 QA pairs.
- **Medium Confidence:** The 3-sentence sampling achieving comparable reliability to full fine-grained annotation assumes consistent quality within answers; this may not hold for heterogeneous clinical responses.
- **Medium Confidence:** LLM-as-judge reliability improvements with fine-grained instructions are demonstrated, but the limited judge model pool (only GPT-4o) and potential instruction-following variance across models temper generalizability.

## Next Checks

1. Test the granularity-dimension mapping on specialized clinical datasets (e.g., oncology, cardiology) to assess generalizability beyond primary care questions.
2. Experiment with varying partial sampling sizes (2-5 sentences) to identify optimal cost-reliability trade-offs for each evaluation dimension.
3. Evaluate multiple LLM judges (Claude, Gemini, Llama-3.1) with fine-grained instructions to determine if reliability improvements extend across model families.