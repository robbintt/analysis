---
ver: rpa2
title: Emergent Stack Representations in Modeling Counter Languages Using Transformers
arxiv_id: '2502.01432'
source_url: https://arxiv.org/abs/2502.01432
tags:
- languages
- counter
- probing
- these
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformer models trained on counter
  languages implicitly learn stack-like representations by probing their internal
  activations. The authors train transformer models on Dyck-1 and Shuffle-k languages
  (k=2,4,6), which can be modeled using counter variables or equivalently stacks.
---

# Emergent Stack Representations in Modeling Counter Languages Using Transformers

## Quick Facts
- **arXiv ID**: 2502.01432
- **Source URL**: https://arxiv.org/abs/2502.01432
- **Reference count**: 26
- **Primary result**: Transformer models trained on counter languages develop internal representations encoding stack depth information, extractable via probing classifiers.

## Executive Summary
This paper investigates whether transformer models trained on counter languages implicitly learn stack-like representations. The authors train transformer models on Dyck-1 and Shuffle-k languages (k=2,4,6), which can be modeled using counter variables or equivalently stacks. They then train probing classifiers to predict stack depths at each token position from the model's internal activations. Results show high probing accuracy (significantly above random baselines and control tasks) for all tested languages, with accuracy increasing with k values. This demonstrates that transformers learn stack-like structures when trained on counter languages, bringing insights into how these models process hierarchical patterns and supporting circuit discovery efforts in mechanistic interpretability.

## Method Summary
The authors train 1-layer encoder-only transformers on counter languages as next-token predictors, then probe internal representations for stack depths. Data generation uses Dyck-1 over bracket vocabulary and Shuffle-k with k disjoint bracket vocabularies. The model predicts valid next token sets using sigmoid outputs. Probing classifiers (linear to 6-layer MLPs) predict stack depth from final-layer embeddings. Selectivity measures task accuracy minus control task accuracy with randomized labels.

## Key Results
- Probing classifiers achieve high accuracy predicting stack depths from transformer representations
- Selectivity values are significantly above zero, validating genuine feature detection
- Shuffle-k models show higher probing accuracy than Dyck-1, with accuracy increasing as k increases
- Accuracy scales inversely with stack update frequency in Shuffle languages

## Why This Works (Mechanism)

### Mechanism 1: Stack-like Representation Emergence from Next-Token Prediction
The next-token prediction task on counter languages requires tracking hierarchical structure to determine valid continuations. The model learns to encode this as meaningful vectors where counter values map to stack depths. High probing accuracy with low control task accuracy suggests genuine feature encoding rather than probe artifacts.

### Mechanism 2: Multi-Counter Parallelization with Reduced Interference
Shuffle-k requires tracking k separate counters with disjoint vocabularies. Each stack updates less frequently as k increases (every kth token on average), reducing interference and improving probe accuracy. The model capacity scales to encode multiple independent counters.

### Mechanism 3: Control Task Selectivity Validation
Randomizing target values creates a control task. High main-task accuracy with near-random control accuracy indicates the model encodes the property; low selectivity suggests probe memorization. This validates genuine feature detection versus spurious correlations.

## Foundational Learning

- **Counter Languages and Stack Equivalence**: Dyck-1 and Shuffle-k are formal languages modelable by counter variables OR stacks (depth = counter value). This equivalence is essential to interpret probing targets.
  - Quick check: Why does Dyck-1 (balanced parentheses) require exactly one counter to recognize?

- **Probing Classifiers**: Probes extract properties from internal representations. High accuracy indicates information presence but not causality.
  - Quick check: What does high probing accuracy tell you? What does it NOT tell you?

- **Selectivity and Control Tasks**: Validates that probes detect learned features, not spurious correlations.
  - Quick check: If a probe achieves 90% on both real and randomized control tasks, what can you conclude?

## Architecture Onboarding

- **Component map**: Counter language data -> Encoder-only Transformer (1 layer, 4 heads, d_model=64) -> Linear decoder head -> Next-token predictions -> Probing classifier (1-6 layers, 128 hidden) -> Stack depth predictions

- **Critical path**: 1) Generate counter language data with stack depths 2) Train transformer on next-token prediction 3) Extract final-layer embeddings 4) Train probing classifier for stack depth 5) Compute selectivity vs control task

- **Design tradeoffs**: Small model (1 layer, 64 hidden) prevents overfitting but may limit capacity; linear vs deeper probes balance feature isolation vs memorization risk; multi-label prediction captures valid next token sets

- **Failure signatures**: Probing accuracy ≈ random baseline (no stack encoding), control accuracy ≈ task accuracy (probe memorization), language modeling non-convergence (baseline failure), probe fails on Tracr model (methodology issue)

- **First 3 experiments**: 1) Reproduce Dyck-1 probing with high selectivity 2) Compare linear vs 6-layer probe to confirm findings aren't probe-specific 3) Validate methodology using Tracr-compiled model with known stack structures

## Open Questions the Paper Calls Out

1. **Causal Role**: Do the emergent stack-like representations play a causal role in the model's computation, or are they merely correlated byproducts? [Section 5 states this is left to future work]

2. **Implementation Algorithms**: What specific algorithms do transformers learn to update and read out these stack representations during sequence processing? [Section 5 distinguishes between identifying representations and identifying algorithms]

3. **Architectural Sensitivity**: How sensitive is the emergence of stack representations to architectural variations such as positional embedding types or training objectives? [Section 5 notes ablating across multiple architecture choices might be useful]

## Limitations

- The minimal 1-layer transformer architecture may not generalize to standard transformer models
- Probing methodology cannot definitively prove active use of stack representations in computation
- Findings are limited to counter languages and may not extend to natural language syntax
- Single training run per condition limits robustness assessment

## Confidence

- **High Confidence**: Transformers trained on counter languages develop internal representations from which stack depths can be accurately predicted
- **Medium Confidence**: Shuffle-k models learn multiple independent stack representations with accuracy scaling inversely to update frequency
- **Low Confidence**: Broader claims about mechanistic interpretability implications for circuit discovery in larger models

## Next Checks

1. **Probe Transferability Test**: Train probing classifiers on representations from transformer models trained on different random seeds. High consistency in probing accuracy across seeds would strengthen confidence that stack representations are robust emergent features.

2. **Intervention Study**: Perform causal interventions by modifying internal representations at specific positions to have incorrect stack depth encodings, then observe whether this degrades next-token prediction accuracy.

3. **Architecture Scaling Analysis**: Repeat experiments with deeper transformers (2-6 layers) and larger hidden dimensions (128-512) while keeping same dataset and training procedure. Compare probing accuracy patterns to determine whether stack emergence is robust to architectural scaling.