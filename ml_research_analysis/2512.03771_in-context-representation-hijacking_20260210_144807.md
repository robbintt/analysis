---
ver: rpa2
title: In-Context Representation Hijacking
arxiv_id: '2512.03771'
source_url: https://arxiv.org/abs/2512.03771
tags:
- harmful
- attack
- carrot
- benign
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Doublespeak, a novel in-context representation hijacking
  attack against large language models. The attack works by systematically replacing
  harmful keywords with benign tokens across multiple in-context examples, causing
  the benign tokens' internal representations to converge toward harmful meanings.
---

# In-Context Representation Hijacking

## Quick Facts
- **arXiv ID**: 2512.03771
- **Source URL**: https://arxiv.org/abs/2512.03771
- **Reference count**: 40
- **Primary result**: Novel attack that replaces harmful keywords with benign tokens across in-context examples, causing semantic overwrite that bypasses model safety alignment

## Executive Summary
This paper introduces "Doublespeak," an in-context representation hijacking attack that systematically replaces harmful keywords with benign tokens across multiple in-context examples. This substitution causes the benign token's internal representation to converge toward the harmful meaning, effectively semantic overwriting. The attack bypasses model safety alignment because superficially innocuous prompts are internally interpreted as disallowed instructions. Using interpretability tools including logit lens and Patchscopes, the authors demonstrate how benign meanings in early layers progressively transform into harmful semantics in later ones.

The attack achieves 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override and maintains consistent effectiveness across diverse lexical categories. These findings reveal a critical blind spot in current safety paradigms, showing that static input-layer safety checks are insufficient and that robust alignment requires continuous semantic monitoring throughout the forward pass.

## Method Summary
The attack works by generating K in-context examples where harmful keywords are systematically replaced with benign substitutes, then appending a simplified harmful prompt with the same substitution. The authors use GPT-4o-mini for prompt simplification, GPT-4o for context generation, and evaluate success via StrongReject framework with GPT-4o-mini as judge. Interpretability is achieved through logit lens and Patchscopes, which track how benign tokens' representations shift from their original meaning to harmful semantics across transformer layers.

## Key Results
- 74% ASR on Llama-3.3-70B-Instruct with single-sentence context override
- Attack maintains effectiveness across nouns, verbs, adjectives, and pronouns
- Semantic hijacking emerges gradually across layers, not at input
- Early-layer refusal checks miss the attack because hijacking completes in later layers

## Why This Works (Mechanism)

### Mechanism 1: In-Context Semantic Remapping
Repeatedly substituting a harmful keyword with a benign token across in-context examples causes the model to temporarily remap the benign token's representation toward the harmful concept. Transformers update token representations contextually at each layer. When the context consistently pairs a benign word in syntactic/semantic slots normally occupied by a harmful word, the model's in-context learning mechanism treats this as a valid synonym mapping.

### Mechanism 2: Progressive Layer-by-Layer Semantic Drift
The semantic hijacking emerges gradually across layers, not at the input. Early layers retain the benign token's original meaning; middle-to-late layers progressively incorporate contextual evidence from the adversarial examples, shifting the representation toward the harmful concept.

### Mechanism 3: Time-of-Check vs. Time-of-Use Safety Bypass
Refusal mechanisms evaluate representations at early layers, before the semantic hijacking has completed—creating a temporal blind spot. Safety-critical refusal directions activate around layer 12 in Llama-3-8B, but the benign token still retains its original meaning at that layer; the harmful reinterpretation crystallizes later.

## Foundational Learning

- **Concept: In-Context Learning in Transformers** - Understanding that representations are dynamically constructed—not retrieved—is essential for grasping how the attack exploits context-dependent semantic updates.
  - *Quick check question*: Given "The zebra was delicious with frosting," what does the model likely interpret "zebra" as? (Answer: cake, via context.)

- **Concept: Residual Stream and Layerwise Representation Updates** - The attack's layer-by-layer semantic drift relies on how each transformer layer refines the residual stream.
  - *Quick check question*: If you patched a token's representation at layer 10, would it affect the model's output? Why or why not?

- **Concept: Refusal Directions and Activation-Space Safety** - Understanding that refusal is mediated by specific directions in activation space (not surface tokens) explains why the attack bypasses safety.
  - *Quick check question*: If you ablated the "refusal direction" from a model, what behavior change would you expect?

## Architecture Onboarding

- **Component map**: Embedding layer -> Transformer layers (1-L) -> Refusal direction (layer ~12) -> Unembedding/logit lens -> Patchscopes
- **Critical path**: 1) Adversarial context enters model 2) Early layers process mostly benign semantics 3) Middle layers shift substituted token toward harmful concept 4) Refusal direction check occurs before shift completes 5) Late layers finalize harmful interpretation
- **Design tradeoffs**: More in-context examples increase remapping reliability but also increase refusal risk; token choice affects semantic bleed; model scale affects optimal context length
- **Failure signatures**: Benign failure (literal interpretation), Refused failure (detected malicious intent), Noisy logit lens (small models)
- **First 3 experiments**: 1) Reproduce Patchscopes layerwise trajectory on Llama-3-8B-Instruct with K=10 examples, 2) Ablate context length K ∈ {1,4,7,10,15} on single prompt, 3) Test cross-lexical robustness by substituting across different lexical categories

## Open Questions the Paper Calls Out
- How can "representation-aware safety" be practically implemented to continuously monitor semantics throughout the forward pass without excessive computational overhead?
- Can representation hijacking effectively manipulate model behavior in non-refusal domains, such as biasing reasoning chains or forcing incorrect tool use?
- Is the bypass of refusal mechanisms primarily caused by a temporal mismatch or by the superposition of benign and harmful representations?

## Limitations
- Attack effectiveness validated primarily on specific AdvBench corpus (520 harmful prompts) without guarantee of generalization to arbitrary harmful prompts
- Interpretability tool reliability issues: logit lens fails on smaller models, Patchscopes requires careful baseline computation
- Temporal safety blind spot claims rely on model-specific assumptions about where refusal directions activate

## Confidence
- **High Confidence**: Core attack mechanism empirically demonstrated across multiple models and lexical categories
- **Medium Confidence**: Explanation for safety mechanism failure is plausible but relies on model-specific assumptions
- **Low Confidence**: Claims about attack robustness across arbitrary harmful prompts are overstated

## Next Checks
1. Apply the attack to a different harmful prompt dataset (e.g., RealToxicityPrompts) without automated simplification to assess prompt-specific vulnerabilities
2. Implement a mid-layer safety monitor that checks for semantic drift at layers 12-16 to measure whether this catches attacks that bypass early-layer refusal
3. Track representation stability across multiple attack executions with identical parameters to quantify reliability versus randomness