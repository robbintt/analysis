---
ver: rpa2
title: 'Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization
  for Deep Neural Network Training'
arxiv_id: '2510.14009'
source_url: https://arxiv.org/abs/2510.14009
tags:
- training
- learning
- lanton
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of geometry-aware optimizers
  like Muon when applied to deep neural networks with heterogeneous layer-wise curvature
  and gradient noise. These optimizers typically use fixed learning rates within layers
  of the same group, which can be suboptimal.
---

# Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training

## Quick Facts
- **arXiv ID:** 2510.14009
- **Source URL:** https://arxiv.org/abs/2510.14009
- **Reference count:** 40
- **Primary result:** Introduces LANTON, a method that estimates gradient variance in the dual norm induced by geometry-aware optimizers and dynamically assigns layerwise, noise-adaptive learning rates, achieving faster convergence and improved sample efficiency across multiple transformer architectures.

## Executive Summary
This paper addresses the inefficiency of geometry-aware optimizers like Muon when applied to deep neural networks with heterogeneous layer-wise curvature and gradient noise. The authors propose LANTON, a novel method that estimates gradient variance in the dual norm induced by the chosen linear minimization oracle and dynamically assigns layerwise, noise-adaptive learning rates. The method is compatible with geometry-aware optimizers and theoretically achieves a convergence rate of \(\tilde{O}(1/\sqrt{T} + P_\ell \bar{\sigma}_\ell / T^{1/4})\), improving upon prior work by accounting for layer-wise noise heterogeneity. Empirically, LANTON demonstrates faster convergence and improved sample efficiency across multiple transformer architectures (LLaMA, GPT2) and datasets (C4, MiniPile, OpenWebText), achieving lower training and validation losses compared to state-of-the-art optimizers. The method introduces only minimal computational overhead, approximately 4% additional training time.

## Method Summary
LANTON builds on geometry-aware optimization by assigning noise-adaptive learning rates to different layers within the same group. It maintains a variance tracker \(H_t^\ell\) using an exponential moving average of the dual norm of gradient differences, computes a noise-adaptive scaling factor \(\alpha_t^\ell = \alpha / \sqrt{\alpha^2 + H_t^\ell}\), and rescales each layer's learning rate proportionally. The algorithm uses different norms and Linear Minimization Oracles (LMOs) for different parameter groups: RMS→RMS norm with Newton-Schulz LMO for hidden layers, ℓ1→ℓ∞ norm with Signum LMO for embeddings and LM head, and RMS norm for normalization vectors. Training uses cosine learning rate decay with warmup, decoupled weight decay, and momentum.

## Key Results
- LANTON achieves faster convergence and lower final training/validation losses than D-Muon and AdamW across GPT2 and LLaMA architectures.
- The method introduces only ~4% additional training time overhead while improving sample efficiency.
- Theoretical convergence rate of \(\tilde{O}(1/\sqrt{T} + P_\ell \bar{\sigma}_\ell / T^{1/4})\) improves upon prior geometry-aware methods by accounting for layer-wise noise heterogeneity.

## Why This Works (Mechanism)

### Mechanism 1: Gradient noise heterogeneity drives layer-wise learning rate adaptation
Assigning smaller learning rates to layers with higher gradient variance and larger rates to low-noise layers accelerates convergence. The algorithm estimates per-layer gradient variance in the dual norm space via an exponential moving average (\(H_t^\ell\)), computes a noise-adaptive scaling factor \(\alpha_t^\ell = \alpha / \sqrt{\alpha^2 + H_t^\ell}\), and rescales each layer's learning rate proportionally. Layers with larger \(H_t^\ell\) receive smaller effective learning rates, aligning step sizes with local noise levels. This works because stochastic gradient noise varies substantially across layers and training stages, and bounded noise exists almost surely in the dual norm.

### Mechanism 2: Geometry-aware updates preserve parameter structure
Using norm-constrained Linear Minimization Oracles (LMOs) tailored to parameter types improves optimization stability. Different parameter groups (hidden matrices, embeddings, normalization vectors) use different norms and LMOs: hidden layers use RMS→RMS operator norm with nuclear dual norm, LMO via Newton-Schulz; embeddings/LM head use ℓ1→ℓ∞ norm with Signum LMO; normalization vectors use RMS norm with RMS normalization LMO. This respects matrix structure rather than flattening parameters, as treating parameters according to their geometric role yields better-conditioned updates than Euclidean treatment.

### Mechanism 3: Variance tracking via dual norm enables noise-adaptive scaling
Estimating gradient variance in the dual norm space (not Euclidean space) yields appropriate scaling for geometry-aware optimizers. The variance tracker \(H_t^\ell\) is updated as a moving average of \(\|G_t^\ell - \tilde{G}_t^\ell\|_*^2\) where \(\|\cdot\|_*\) is the dual norm induced by the LMO. This dual-norm variance is then used to scale learning rates, ensuring noise estimation respects the geometry. The core assumption is that dual norm variance correlates with effective noise in the geometry-aware update direction.

## Foundational Learning

- **Concept: Linear Minimization Oracle (LMO)**
  - **Why needed here:** LANTON builds on LMO-based optimizers; understanding how LMOs produce update directions is essential for grasping the algorithm's structure.
  - **Quick check question:** Can you explain how the LMO differs from a gradient descent step in terms of constraints and output?

- **Concept: Dual norm and operator norms**
  - **Why needed here:** The variance estimator operates in the dual norm space; comprehension of primal/dual norm relationships is required to understand the noise estimation logic.
  - **Quick check question:** Given a matrix \(A\), how does the nuclear norm relate to the spectral norm as its dual?

- **Concept: Stochastic gradient noise and variance**
  - **Why needed here:** The core motivation is noise heterogeneity; understanding how batch size, data, and architecture affect gradient variance clarifies why adaptive rates help.
  - **Quick check question:** Why would early training layers exhibit different noise profiles than later layers in a transformer?

## Architecture Onboarding

- **Component map:**
  - Hidden layers (QK, VO, MLP matrices) → RMS→RMS norm, nuclear dual, Newton-Schulz LMO
  - Embeddings/LM head → ℓ1→ℓ∞ norm, Signum LMO
  - Norm vectors → RMS norm, RMS normalization LMO
  - Variance tracker (\(H_t^\ell\)) → EMA of dual-norm squared gradient differences, momentum \(\beta_2\)
  - Noise scaling factor (\(\alpha_t^\ell\)) → Computed from \(H_t^\ell\) and base \(\alpha\); ratio \(\alpha_t^\ell / \alpha_t^m\) rescales per-layer learning rates
  - Base learning rate schedule → Cosine decay from \(\eta_{\max}\) to \(\eta_{\min}\)
  - Momentum buffer (\(B_t^\ell\)) → Standard momentum with \(\beta_1\) for gradient accumulation

- **Critical path:**
  1. Initialize per-layer \(H_0^\ell = 0\), momentum buffers
  2. Per iteration: compute stochastic gradient \(G_t^\ell\), update momentum \(B_t^\ell\), compute LMO direction \(O_t^\ell\)
  3. Update variance tracker \(H_t^\ell\) using dual norm of gradient difference
  4. Compute \(\alpha_t^\ell\) and per-layer effective learning rate \(\eta_t^\ell = \eta_t \sqrt{\alpha_t^\ell / \alpha_t^m}\)
  5. Update parameters: \(X_{t+1}^\ell = X_t^\ell + \eta_t^\ell O_t^\ell\)

- **Design tradeoffs:**
  - Option I vs Option II variance estimation: Option I reuses previous gradient (no extra compute), Option II uses two independent gradients (more accurate but doubles batch/bandwidth)
  - Frequency of variance estimation: Paper estimates every 10 iterations to reduce overhead (~4%); less frequent estimation reduces overhead but may lag noise changes
  - \(\beta_2\) choice: Higher \(\beta_2\) smooths estimates but may adapt slowly to noise shifts; lower \(\beta_2\) reacts faster but is noisier

- **Failure signatures:**
  - Unstable training/loss spikes: \(\beta_2\) too low or variance estimate diverges; dual norm computation may be numerically unstable for very large matrices
  - No speedup over baseline: Gradient noise is near-uniform across layers (e.g., very large batch), or base learning rate poorly tuned
  - Slow convergence early: \(\alpha\) or \(\beta_2\) misconfigured such that scaling factors saturate low for all layers

- **First 3 experiments:**
  1. **Sanity check:** Implement LANTON on a small transformer (GPT-2 small) with Option I, compare training/validation loss curves against AdamW and Muon under identical token budget and hyperparameters (tune base LR for each).
  2. **Ablation on variance estimation:** Compare Option I vs Option II vs fixed learning rates within layer groups; measure convergence speed and final loss.
  3. **Sensitivity analysis:** Vary \(\beta_2 \in \{0.7, 0.85, 0.95\}\) and estimation frequency (every 1, 10, 20 steps) on LLaMA-1.1B/C4; track overhead and convergence to identify robust settings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does LANTON maintain its sample efficiency gains and low overhead when scaling to models significantly larger than 2B parameters?
- **Basis in paper:** [explicit] The conclusion explicitly states that "experiments are conducted on moderately sized models; extending and validating the approach at larger scales is an important direction."
- **Why unresolved:** The 4% overhead was measured on a single node (4x H200); it is unclear if the noise estimation (requiring R-SVD) bottlenecks training or interacts poorly with distributed memory sharding in much larger settings.
- **What evidence would resolve it:** Training curves and runtime benchmarks for models like LLaMA-7B or 70B using tensor/pipeline parallelism compared against baselines like D-Muon.

### Open Question 2
- **Question:** Can the theoretical convergence bounds be tightened to remove the dependency on parameter dimension?
- **Basis in paper:** [explicit] The authors list the fact that "theoretical results may depend on the parameter dimension" as a primary limitation.
- **Why unresolved:** The constants \(C_1, C_2\) derived from norm equivalence (Lemma A.3) scale with matrix dimensions, which may render the theoretical guarantees loose for over-parameterized networks.
- **What evidence would resolve it:** A refined convergence analysis utilizing dimension-free concentration inequalities or empirical validation showing the convergence rate is stable across varying model widths.

### Open Question 3
- **Question:** Does the practical "Option I" variance estimator become unstable in training regimes with high gradient correlation?
- **Basis in paper:** [inferred] The theory assumes independent gradient samples (Option II), but experiments rely on "Option I" (using consecutive gradients \(G_t - G_{t-1}\)) to avoid doubling batch size.
- **Why unresolved:** In areas of high curvature or small batch sizes, consecutive gradients are highly correlated, potentially biasing the noise estimate \(\sqrt{H_t}\) and destabilizing the adaptive learning rate.
- **What evidence would resolve it:** An ablation study comparing the trajectory of Option I versus Option II in late-stage training or with small batch sizes.

## Limitations
- The empirical validation relies on a relatively small set of experiments with limited runs per setting, raising concerns about statistical significance.
- The theoretical convergence rate assumes bounded gradient noise in the dual norm almost surely, but this assumption is not empirically verified for the tested architectures.
- The choice of variance estimation method (Option I vs Option II) and update frequency (every 10 steps) are heuristic choices without systematic ablation.

## Confidence
- **High confidence:** The core mechanism of noise-adaptive layerwise learning rates and its compatibility with geometry-aware optimizers. The algorithmic description and implementation details are clearly specified.
- **Medium confidence:** The theoretical convergence rate, as it relies on assumptions about gradient noise that are not fully verified empirically. The computational overhead claim also falls into this category.
- **Low confidence:** The generality of the approach to other architectures and tasks, given the limited experimental scope. The optimal configuration of hyperparameters (β₂, update frequency) remains unclear.

## Next Checks
1. **Statistical validation:** Run each experiment 5-10 times with different random seeds to establish confidence intervals for training and validation loss curves. Compare the statistical significance of improvements over baselines.
2. **Ablation on variance estimation:** Systematically compare Option I vs Option II, and update frequencies of 1, 10, and 20 steps, on a held-out validation set. Measure both convergence speed and final loss to identify optimal settings.
3. **Architecture generalization:** Test LANTON on a non-transformer architecture (e.g., ResNet-50 on CIFAR-10 or ViT on ImageNet) to assess whether the noise-adaptive mechanism provides similar benefits outside the transformer domain.