---
ver: rpa2
title: Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals
  with High-Dimensional Data
arxiv_id: '2508.15676'
source_url: https://arxiv.org/abs/2508.15676
tags:
- data
- tensor
- where
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a tensorized multi-task learning framework
  to model heterogeneous subpopulations with high-dimensional data. The core idea
  is to decompose the collection of task model parameters into a low-rank tensor structure
  using Tucker decomposition, which captures shared patterns and subpopulation-specific
  variations.
---

# Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals with High-Dimensional Data

## Quick Facts
- arXiv ID: 2508.15676
- Source URL: https://arxiv.org/abs/2508.15676
- Reference count: 18
- Primary result: Tensorized multi-task learning framework with Tucker decomposition outperforms local and global models in personalized prediction tasks.

## Executive Summary
This paper introduces a tensorized multi-task learning (TenMTL) framework for modeling heterogeneous subpopulations with high-dimensional data. The core innovation is decomposing task parameters into a low-rank Tucker structure, enabling the model to capture both shared patterns and subpopulation-specific variations. By formulating this as an optimization problem with alternating minimization, the framework achieves superior prediction accuracy compared to traditional local and global modeling approaches while providing interpretability through its low-rank decomposition.

## Method Summary
The method constructs a parameter tensor by concatenating individual task models, then applies Tucker decomposition to capture shared and distinct patterns across tasks. An alternating minimization algorithm updates the core tensor and factor matrices using block coordinate descent with Lasso regularization. The framework handles both tensor and vector predictors through a unified optimization problem, balancing model fit with low-rank regularization to prevent overfitting while preserving personalization.

## Key Results
- TenMTL significantly outperforms Local, Global, LR-Tucker, and SVD-ASO methods across three simulation scenarios with varying heterogeneity levels.
- In Parkinson's disease severity prediction, TenMTL achieves 3.5% lower RMSE than competing methods, particularly effective when inter-patient variability is high.
- For ADHD classification, TenMTL demonstrates improved accuracy over benchmark methods while revealing interpretable patterns in brain connectivity features.

## Why This Works (Mechanism)

### Mechanism 1
Decomposing model parameters into a low-rank Tucker structure reduces overfitting by capturing shared latent patterns across heterogeneous individuals. Instead of learning independent high-dimensional parameter tensors for each individual (which overfits) or one global tensor (which ignores heterogeneity), the method concatenates all parameters into a tensor and enforces a low-rank structure that projects individual variations onto a shared low-dimensional subspace. This works under the assumption that true model parameters across the population lie on a low-dimensional manifold.

### Mechanism 2
Structuring factor matrices to have both shared and distinct components enables personalization while retaining global knowledge. The framework splits the individual-mode factor matrix into common columns capturing correlations shared across all tasks and distinct columns capturing specific variations. This allows the model to "finetune" the shared basis for specific individuals rather than forcing a one-size-fits-all representation, assuming heterogeneity is structured with individuals sharing a common basis of features but differing in their specific activation.

### Mechanism 3
Alternating minimization with Lasso regularization solves the non-convex tensor estimation problem while enforcing feature selection. The objective function is non-convex, so the algorithm fixes all variables except one and solves a convex sub-problem at each step. Lasso penalties drive sparsity, effectively performing feature selection within the tensor structure. This approach converges to a useful local optimum under the assumption that the block coordinate descent approach is effective for this problem structure.

## Foundational Learning

- **Concept: Tucker Decomposition (HOSVD)**
  - Why needed here: This is the structural core of the method. Understanding the difference between the "Core Tensor" (interactions) and "Factor Matrices" (loadings) is required to interpret the results.
  - Quick check question: In the equation $\tilde{B} = [\![G; U_0, U_1]\!]$, does $U_0$ represent the features or the individuals? (Answer: Individuals).

- **Concept: Multi-Task Learning (MTL) Trade-offs**
  - Why needed here: The paper positions itself between "Local" (high variance) and "Global" (high bias) models. You must understand negative transfer and knowledge sharing to evaluate the results.
  - Quick check question: Why might a "Global" model fail in the Parkinson's case study? (Answer: High inter-patient variability/heterogeneity violates the i.i.d. assumption).

- **Concept: Generalized Linear Models (GLM) & Link Functions**
  - Why needed here: The framework is built on GLM (logistic for ADHD, linear for Parkinson's). The optimization updates rely on solving GLM sub-problems.
  - Quick check question: If modeling binary classification (ADHD), which link function is likely used in the loss function? (Answer: Logit/Logistic).

## Architecture Onboarding

- **Component map:** Input Layer -> Parameter Tensor Constructor -> Tucker Engine -> Optimization Loop -> Output Layer
- **Critical path:** The initialization step (Algorithm 1, Line 2) is critical. The paper initializes using "locally estimated parameters." If this step is skipped or randomized, convergence may be significantly slower or converge to a poor local minimum.
- **Design tradeoffs:**
  - **Rank ($R$) vs. Complexity:** Higher rank $R_0$ captures more heterogeneity but reduces the "borrowing strength" effect and increases computation.
  - **Penalty ($\lambda$) vs. Sparsity:** High $\lambda$ ensures stability and sparsity but may miss subtle signals in high-dimensional features.
  - **Shared vs. Distinct Columns:** The split $W_0$ vs. $F_0$ requires deciding how much of the individual mode is "universal."
- **Failure signatures:**
  - **Model Collapse:** RMSE of TenMTL matches the Global model (Underfitting heterogeneity). *Remedy:* Increase rank $R_0$.
  - **Overfitting:** Test RMSE is significantly higher than training RMSE, or TenMTL performs worse than simple Local models. *Remedy:* Increase $\lambda$ or decrease rank.
- **First 3 experiments:**
  1. **Simulation Reproduction (Scenario I):** Run the simulation with $\beta_u = 1$ (high heterogeneity). Verify that TenMTL outperforms Global and Local models to ensure the implementation handles distinct clusters correctly.
  2. **Hyperparameter Sensitivity:** Run the Parkinson's case study varying $R_0$ (e.g., 1 to 10). Plot Test RMSE vs. Rank to find the "elbow" where heterogeneity is sufficiently captured without overfitting.
  3. **Ablation on Initialization:** Compare convergence speed of "Random Initialization" vs. "Local Estimation Initialization" (as suggested in the paper) to validate the efficiency of the proposed starting point.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the TenMTL framework be extended to a federated learning setting to handle decentralized tasks without sharing sensitive data? **Basis in paper:** [explicit] The conclusion states, "As future work, we plan to extend TenMTL to a federated learning setting, where tasks are distributed across decentralized nodes." **Why unresolved:** The current optimization algorithm assumes centralized access to all local datasets to compute global updates. **What evidence would resolve it:** A modified algorithm (e.g., using Federated Averaging) that allows nodes to update local parameters and transmit only gradients/parameters for global factors, along with convergence proofs ensuring privacy preservation.

### Open Question 2
**Question:** Can coupled tensor decomposition (CTD) be effectively integrated into TenMTL to model multi-modal data where the number of features varies significantly across modalities? **Basis in paper:** [explicit] Section 3.1.3 notes that for general multi-modal data, "The general TenMTL architecture can integrate other low-rank decompositions, such as coupled tensor decomposition (CTD)... In this scenario, the tensor of parameters associated with each modality shares a common... factor matrix." **Why unresolved:** The current implementation assumes inputs can be formed into a unified tensor or have fixed dimensions; CTD integration requires a new optimization formulation to handle heterogeneous parameter tensors linked by shared modes. **What evidence would resolve it:** An extension of the optimization problem incorporating CTD constraints and empirical validation on datasets with drastically different feature dimensions per modality.

### Open Question 3
**Question:** Is the proposed alternating minimization algorithm guaranteed to converge to a global optimum, or is it sensitive to initialization? **Basis in paper:** [inferred] The paper details an "alternating minimization algorithm" but does not provide theoretical convergence proofs regarding local minima. **Why unresolved:** Non-convex problems involving tensor decomposition often suffer from local minima, and the reliance on potentially noisy local estimates for initialization could impact the final solution quality. **What evidence would resolve it:** A theoretical analysis demonstrating convergence to a stationary point or global optimum, or a simulation study varying initialization strategies to test robustness.

## Limitations
- The alternating minimization algorithm lacks theoretical convergence guarantees to global optima.
- Initialization strategy using locally estimated parameters is critical but not fully specified.
- Hyperparameter sensitivity (particularly rank selection) is not thoroughly explored across diverse datasets.

## Confidence
- **High Confidence:** Superiority of TenMTL over Global models in heterogeneous populations (Parkinson's case study).
- **Medium Confidence:** Performance advantage over Local models is context-dependent, less pronounced in ADHD case study.
- **Medium Confidence:** Interpretability claim through low-rank decomposition is theoretically sound but lacks quantitative validation.

## Next Checks
1. **Convergence Validation:** Run extended simulations (100+ iterations) to verify that the alternating minimization algorithm consistently converges to the same objective value across multiple random seeds.
2. **Rank Sensitivity Analysis:** Systematically vary $R_0$ in the Parkinson's case study (e.g., 1-10) and plot test RMSE to identify the optimal rank that balances heterogeneity capture and overfitting prevention.
3. **Initialization Ablation:** Compare the proposed initialization (Tucker on local estimates) against random initialization in both simulation and real-data scenarios to quantify its impact on convergence speed and final performance.