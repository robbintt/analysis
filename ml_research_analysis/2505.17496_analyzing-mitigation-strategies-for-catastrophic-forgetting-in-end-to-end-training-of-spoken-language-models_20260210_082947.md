---
ver: rpa2
title: Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training
  of Spoken Language Models
arxiv_id: '2505.17496'
source_url: https://arxiv.org/abs/2505.17496
tags:
- speech
- training
- language
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates catastrophic forgetting in end-to-end
  training of spoken language models (SLMs) and evaluates three mitigation strategies:
  model merging, discounting LoRA scaling factor, and experience replay. The study
  finds that experience replay is the most effective strategy, significantly reducing
  knowledge loss while maintaining performance across both text and speech modalities.'
---

# Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models

## Quick Facts
- **arXiv ID**: 2505.17496
- **Source URL**: https://arxiv.org/abs/2505.17496
- **Reference count**: 0
- **Primary result**: Experience replay most effective at mitigating catastrophic forgetting in end-to-end SLM training

## Executive Summary
This paper investigates catastrophic forgetting in end-to-end training of spoken language models (SLMs) and evaluates three mitigation strategies: model merging, discounting LoRA scaling factor, and experience replay. The study finds that experience replay is the most effective strategy, significantly reducing knowledge loss while maintaining performance across both text and speech modalities. When combined with other methods, experience replay achieves further performance gains. The results demonstrate that catastrophic forgetting is a significant challenge in SLM training, particularly between ASR and TTS stages, and highlight experience replay as a robust solution for preserving knowledge across training stages.

## Method Summary
The study evaluates three mitigation strategies for catastrophic forgetting in SLM training. Experience replay uses a buffer to store past examples, randomly sampling from this buffer during training to maintain knowledge of previously learned tasks. Model merging involves combining models trained on different tasks by averaging their parameters, though this approach requires sufficient GPU memory. Discounting LoRA scaling factor adjusts the scaling factor to preserve information from the initial model during fine-tuning. The researchers tested these methods on an end-to-end SLM with ASR and TTS capabilities, measuring performance across both speech and text modalities.

## Key Results
- Experience replay significantly outperforms model merging and LoRA scaling factor adjustment in mitigating catastrophic forgetting
- Experience replay maintains performance across both speech and text modalities while reducing knowledge loss
- Combining experience replay with other mitigation strategies yields additional performance improvements

## Why This Works (Mechanism)
Experience replay works by maintaining a buffer of past examples that are periodically replayed during training, preventing the model from overwriting previously learned knowledge. This creates a form of regularization that balances learning new information while preserving old capabilities. The method is particularly effective because it addresses the core issue of catastrophic forgetting - the tendency of neural networks to rapidly forget previously learned information when trained on new tasks. By reintroducing past examples, the model is forced to maintain representations useful for both old and new tasks simultaneously.

## Foundational Learning
- **Catastrophic forgetting**: Neural networks rapidly lose previously learned information when trained on new tasks - needed to understand the core problem being addressed; quick check: observe performance degradation on old tasks after training on new tasks
- **Experience replay**: Technique storing and replaying past examples during training - needed to understand the primary mitigation strategy; quick check: verify buffer storage and sampling during training
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method using low-rank matrices - needed to understand one of the tested mitigation approaches; quick check: confirm LoRA matrices are properly initialized and scaled
- **Model merging**: Combining parameters from separately trained models - needed to understand alternative mitigation strategy; quick check: verify parameter averaging and memory requirements
- **End-to-end SLM architecture**: Unified model handling both speech and text processing - needed to understand the training context; quick check: confirm model handles both ASR and TTS tasks

## Architecture Onboarding

**Component Map:**
ASR Encoder -> SLM Backbone -> TTS Decoder

**Critical Path:**
Speech input → ASR encoding → shared representation → TTS decoding → speech output

**Design Tradeoffs:**
- Memory vs. performance: Model merging requires significant GPU memory but may offer better preservation
- Computational efficiency vs. effectiveness: Experience replay adds training overhead but provides superior mitigation
- Parameter efficiency vs. flexibility: LoRA offers efficient adaptation but may not fully prevent forgetting

**Failure Signatures:**
- Performance degradation on ASR tasks after TTS training
- Increased perplexity on text-only evaluation
- Loss of speech-specific features in representations

**First 3 Experiments to Run:**
1. Baseline training without any mitigation strategy to establish forgetting baseline
2. Experience replay with varying buffer sizes to optimize performance
3. Combined approach testing experience replay with discounted LoRA scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific Transformer-based architectures and relatively small-scale models
- Experience replay implementation uses fixed buffer size and uniform sampling, potentially suboptimal for diverse knowledge preservation
- Focus primarily on catastrophic forgetting between ASR and TTS stages, potentially missing other training scenarios

## Confidence
**High confidence**: Experience replay effectively mitigates catastrophic forgetting in SLM training
**Medium confidence**: Combining experience replay with other strategies yields additional performance gains
**Medium confidence**: Catastrophic forgetting is a significant challenge specifically between ASR and TTS stages

## Next Checks
1. Evaluate the proposed mitigation strategies on larger-scale SLM architectures with billions of parameters to assess scalability and effectiveness
2. Test alternative experience replay implementations including prioritized sampling and dynamic buffer management to optimize knowledge preservation
3. Investigate catastrophic forgetting patterns across additional training stages beyond ASR and TTS, including language understanding and generation tasks