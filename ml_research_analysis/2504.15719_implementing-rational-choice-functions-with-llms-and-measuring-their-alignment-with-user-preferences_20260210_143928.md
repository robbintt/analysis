---
ver: rpa2
title: Implementing Rational Choice Functions with LLMs and Measuring their Alignment
  with User Preferences
arxiv_id: '2504.15719'
source_url: https://arxiv.org/abs/2504.15719
tags:
- preferences
- alignment
- user
- choice
- alternatives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of implementing rational choice
  functions using large language models (LLMs) for intelligent user interfaces, focusing
  on alignment with user preferences. The authors propose design principles for using
  LLMs to implement rational choice functions, including Pairwise-Score, Pairwise-SCC,
  and Pairwise-Test methods.
---

# Implementing Rational Choice Functions with LLMs and Measuring their Alignment with User Preferences

## Quick Facts
- **arXiv ID**: 2504.15719
- **Source URL**: https://arxiv.org/abs/2504.15719
- **Reference count**: 23
- **Key result**: Pairwise-Score achieves 85% strict preference overlap (SPO) while Pairwise-SCC achieves best full alignment with K(p) score of 167.4 in automotive IUI domain

## Executive Summary
This paper addresses the challenge of implementing rational choice functions using large language models for intelligent user interfaces. The authors propose design principles for using LLMs to implement choice functions, focusing on two key evaluation metrics: strict preference overlap (SPO) for partial alignment and Kendall distance with penalty (K(p)) for full alignment. An empirical study in the automotive domain demonstrates that Pairwise-Score achieves the highest partial alignment (85% SPO) while Pairwise-SCC achieves the best full alignment (K(p) score of 167.4) when using GPT-4. The results highlight the importance of carefully designing choice functions with LLMs to accurately reflect user preferences.

## Method Summary
The authors propose five design principles for implementing rational choice functions using LLMs, focusing on pairwise comparison methods. The primary approaches are Pairwise-Score, which aggregates pairwise preferences into numerical scores to enforce transitivity, and Pairwise-SCC, which uses graph-based decomposition to identify indifference sets. The framework uses two evaluation metrics: SPO for partial alignment (measuring strict preference violations) and K(p) for full alignment (measuring indifference handling). The empirical study uses 40 personal objects and 23 contexts in an automotive domain, querying LLMs for pairwise preferences with prompt templates T1-T13, and comparing results against a "model user" ground truth.

## Key Results
- Pairwise-Score achieved highest partial alignment with 85% SPO (violates only 15% of strict preferences)
- Pairwise-SCC achieved best full alignment with K(p) score of 167.4, creating only 2.04 indifference sets
- GPT-4 consistently outperformed Gemini Pro and Llama-2-70b-chat across all methods and metrics
- Listwise and Pointwise methods failed to guarantee rational choice functions despite lower computational cost

## Why This Works (Mechanism)

### Mechanism 1: Scoring Enforces Transitivity
- **Claim:** Aggregating pairwise LLM comparisons into numerical scores enforces rationality (transitivity) even when raw model outputs are inconsistent
- **Core assumption:** The LLM has sufficient capability to reliably distinguish relevance in pairwise comparisons more often than not, and the scoring function accurately reflects preference intensity
- **Evidence anchors:** [section 4] "It is easy to see that ≿c is rational, because the score induces a ranking with ties"; [section 6] "Pairwise-Score emerged as the best-performing method [for partial alignment]"
- **Break condition:** If the LLM produces a high rate of invalid responses or refuses to discriminate, the score distribution flattens, rendering the choice function random

### Mechanism 2: SCCs Capture Indifference
- **Claim:** Graph-based decomposition (Strongly Connected Components) better captures user indifference than scalar scoring, improving full alignment
- **Core assumption:** Cycles in the LLM's queried preference graph correspond to user indifference rather than model confusion or error
- **Evidence anchors:** [section 4] "...the SCCs in G induce a relation on indifference sets representing a rational preference"; [section 6] "Pairwise-SCC achieved the closest alignment to user preferences [in full alignment]"
- **Break condition:** If the LLM is too "polite" or indecisive, the graph becomes overly connected, creating massive SCCs where the system is indifferent between highly distinct alternatives

### Mechanism 3: Dual Alignment Metrics Guide Optimization
- **Claim:** Distinguishing between partial alignment (strict preference violations) and full alignment (indifference handling) is required for correct architectural optimization
- **Core assumption:** Application requirements can be strictly categorized as either requiring strict adherence (partial) or nuance handling (full)
- **Evidence anchors:** [abstract] "...introduce evaluation metrics such as strict preference overlap (SPO) for partial alignment and Kendall distance with penalty (K(p)) for full alignment"
- **Break condition:** If the cost function (alignment metric) chosen during development does not match the downstream user utility, the "best" performing model will feel "wrong" to the user

## Foundational Learning

- **Concept: Rational Choice & Weak Orders**
  - **Why needed here:** The paper relies on the axioms of completeness and transitivity. Without understanding that a "rational" agent must not cycle preferences and must account for ties, the motivation for Pairwise-Score/SCC is lost
  - **Quick check question:** If a user prefers A to B and B to C, but the LLM outputs C over A, which axiom is violated and which design principle fixes this?

- **Concept: Position Bias / Order Effects**
  - **Why needed here:** The primary failure mode of naïve LLM ranking (Listwise) and raw pairwise testing is sensitivity to the order of items in the prompt
  - **Quick check question:** Why does the "Pairwise-Test" method fail to guarantee a rational choice function despite asking the correct questions?

- **Concept: Indifference Sets vs. Ties**
  - **Why needed here:** Standard ranking forces a strict order (1, 2, 3...). The paper's innovation is modeling equivalence classes (indifference sets) where x ~ y
  - **Quick check question:** In the Pairwise-SCC method, how does a cycle in the directed graph map to an indifference set?

## Architecture Onboarding

- **Component map:** Input Context -> Query Engine -> Response Parser -> Preference Resolver -> Choice Function
- **Critical path:** The **Preference Resolver** is the core logic. Raw LLM outputs are noisy. The choice between Path A (Score) and Path B (SCC) dictates the alignment profile. Path A is safer for strict utility; Path B is better for nuanced equivalence
- **Design tradeoffs:**
  - Computational Cost vs. Rationality: Both Pairwise methods require O(n²) LLM calls. Listwise/Pointwise are O(n) or O(1) but are irrational/unreliable
  - Granularity vs. Stability: Pairwise-Score produces many small indifference sets (fine-grained). Pairwise-SCC produces fewer, larger sets (stable but potentially blunt)
- **Failure signatures:**
  - The "Politeness" Collapse: The LLM refuses to choose, returning ~ (indifference) for everything. SCC collapses into one giant node
  - Order Bias bleed-through: Even with pairwise swapping, the model consistently prefers the first item presented, skewing scores
- **First 3 experiments:**
  1. Consistency Stress Test: Run Pairwise-Test on a small set of 5 items (10 pairs). Check for transitivity violations (cycles) without the resolver logic to establish a baseline error rate
  2. Metric Correlation Check: Implement Pairwise-Score and measure SPO on a labeled dataset. Verify that SPO > 0.8 (replicating paper results) before optimizing for K(p)
  3. Indifference Calibration: Compare the number of indifference sets generated by Pairwise-Score vs. Pairwise-SCC against the ground truth to determine if SCC is "over-lumping" items in your specific domain

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can design principles be modified to scale efficiently to contexts involving significantly larger sets of alternatives?
- **Basis in paper:** [explicit] The authors state in Section 7 that future research should focus on "developing design principles that scale more efficiently," noting that the robustness of Pairwise-Score and Pairwise-SCC comes at the cost of "added computational complexity (e.g., O(n²))."
- **Why unresolved:** The current quadratic complexity of pairwise prompting methods makes them computationally expensive or infeasible for applications where the set of alternatives (n) is very large
- **What evidence would resolve it:** The development and empirical validation of a new design principle that achieves comparable alignment performance (SPO and K(p)) with sub-quadratic complexity (e.g., O(n log n) or linear) on a dataset with thousands of alternatives

### Open Question 2
- **Question:** To what extent can prompt engineering reduce the sensitivity of LLMs to object ordering and instruction formulation in choice function implementation?
- **Basis in paper:** [explicit] The conclusion notes that "alignment quality is significantly influenced by model choice and prompt engineering," and Section 7 explicitly calls for "refining prompt templates to enhance alignment across a broader range of contexts."
- **Why unresolved:** The study was limited to a specific set of prompt templates, and the results demonstrated that LLMs are highly sensitive to these formulations, yet a systematic optimization remains undefined
- **What evidence would resolve it:** A comparative analysis of a wider variety of prompt templates across different LLMs, demonstrating a statistically significant reduction in variance and invalid responses while maintaining or improving alignment scores

### Open Question 3
- **Question:** How does the incorporation of weights for objects and contexts affect the alignment measurements (SPO and K(p)) and the resulting evaluation of different design principles?
- **Basis in paper:** [inferred] Section 7 notes a limitation where the dataset "does not assign weights on contexts or objects," implying that current metrics treat all preference violations equally, which may not reflect real-world usage where some contexts occur more frequently
- **Why unresolved:** It is currently unknown if Pairwise-Score or Pairwise-SCC would remain the superior methods if alignment metrics penalized violations involving high-frequency or high-importance objects more heavily
- **What evidence would resolve it:** An empirical study utilizing weighted versions of SPO and K(p) metrics, comparing whether the ranking of the proposed design principles shifts when frequency weights are applied to the automotive domain objects

## Limitations

- The empirical validation relies on a proprietary "model user" ground truth dataset that is not fully disclosed, limiting reproducibility
- The study focuses exclusively on automotive contexts (navigation, weather), which may not generalize to other domains
- The assumption that LLM cycles directly map to user indifference (Pairwise-SCC mechanism) remains theoretically plausible but lacks external validation

## Confidence

- **High confidence**: The mathematical framework for rational choice functions and the SPO/K(p) metrics are well-defined and internally consistent
- **Medium confidence**: The Pairwise-Score method's effectiveness in achieving partial alignment (85% SPO) is supported by results, though dependent on the specific automotive dataset
- **Low confidence**: The Pairwise-SCC mechanism's assumption about SCCs representing user indifference is plausible but untested against alternative interpretations of LLM cycles

## Next Checks

1. **Dataset replication**: Construct the 40-object, 23-context evaluation dataset from paper examples and verify if similar alignment metrics (SPO ~0.85, K(p) ~167) can be reproduced
2. **Domain generalization**: Apply the same framework to a non-automotive domain (e.g., productivity tools in work contexts) to test metric transferability
3. **Alternative cycle interpretation**: Test whether LLM cycles more likely represent model confusion versus true indifference by comparing SCC outputs against explicit user indifference annotations