---
ver: rpa2
title: 'From Continual Learning to SGD and Back: Better Rates for Continual Linear
  Models'
arxiv_id: '2504.04579'
source_url: https://arxiv.org/abs/2504.04579
tags:
- continual
- learning
- linear
- cited
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies continual learning where tasks are presented
  sequentially and the model is sequentially fitted. A key challenge is forgetting:
  the loss on previously seen tasks increases as the model adapts to new tasks.'
---

# From Continual Learning to SGD and Back: Better Rates for Continual Linear Models
## Quick Facts
- arXiv ID: 2504.04579
- Source URL: https://arxiv.org/abs/2504.04579
- Reference count: 40
- One-line primary result: New dimension-independent forgetting rates for continual linear regression under random task orderings

## Executive Summary
This paper establishes a reduction from continual linear regression to stochastic gradient descent (SGD) with a fixed, "stepwise-optimal" step size. By transforming the continual learning problem into an SGD framework, the authors develop novel last-iterate convergence bounds that yield the first informative rates for fixed step sizes large enough to support the reductions. The analysis shows that random task ordering alone—without task repetition—prevents catastrophic forgetting in sufficiently long task sequences, providing the first dimension-independent forgetting rates.

## Method Summary
The paper studies continual learning where tasks are presented sequentially and the model is sequentially fitted. It establishes a reduction from continual linear regression to SGD with a fixed, "stepwise-optimal" step size. This reduction enables new last-iterate SGD analysis for fixed step sizes large enough to support the continual learning setting. For random orderings over T tasks, the paper provides the first dimension-independent forgetting rates, previously uncovered by existing rates. The method uses Block Kaczmarz updates that can be rewritten as SGD steps on a modified objective function.

## Key Results
- For continual regression with replacement, improves best existing rate from O((d-¯r)/k) to O(min(1/⁴√k, √d-¯r/k, √T¯r/k))
- Establishes first rate for random task orderings without replacement: O(min(1/⁴√T, (d-¯r)/T))
- Shows randomization alone prevents catastrophic forgetting in sufficiently long task sequences
- Extends analysis to broader methods like block Kaczmarz and POCS with novel rates

## Why This Works (Mechanism)
### Mechanism 1: Reduction of Continual Learning to SGD
- **Claim:** In the joint realizable setting, minimizing the loss of a linear task to convergence is equivalent to taking a single SGD step on a modified objective.
- **Mechanism:** Block Kaczmarz update can be rewritten as an SGD step with step size η=1 on modified objective f_m(w) = ½‖X^+_m(X_m w - y_m)‖², where the gradient naturally incorporates the pseudo-inverse X^+.
- **Core assumption:** Joint Linear Realizability—there exists a single parameter vector w^★ that solves all tasks perfectly.
- **Evidence anchors:** Abstract states fitting a task is equivalent to a single SGD step; Section 3.2 shows iterates of Schemes 2 and 3 coincide.
- **Break condition:** Non-linear models or non-realizable data where W^★ is empty.

### Mechanism 2: Last-Iterate Convergence via Large Step Sizes
- **Claim:** Using fixed, "stepwise-optimal" step sizes (η=1) allows bounding forgetting rate at final iterate, whereas prior art required averaging iterates or smaller steps.
- **Mechanism:** Refines standard regret analysis to work for step sizes up to 2/β, showing denominator (2-ηβ) remains meaningful at η=1.
- **Core assumption:** Modified objective f_m is convex and 1-smooth (β=1).
- **Evidence anchors:** Abstract mentions novel last-iterate SGD upper bounds; Theorem 11 handles η < 2/β and shows convergence for η=1.
- **Break condition:** Step sizes η ≥ 2/β where gradient update may overshoot and diverge.

### Mechanism 3: Randomization Prevents Catastrophic Forgetting
- **Claim:** Random task ordering (with or without replacement) alone is sufficient to bound forgetting, independent of problem dimensionality.
- **Mechanism:** Random ordering induces averaging effect over projection operators; expected error contracts because random selection prevents persistent alignment of errors.
- **Core assumption:** Sufficiently long task sequences (k → ∞).
- **Evidence anchors:** Abstract states randomization alone prevents catastrophic forgetting; Theorem 10 provides first rate for random orderings without replacement.
- **Break condition:** Deterministic or adversarial task orderings where tasks maximize interference.

## Foundational Learning
- **Concept:** Block Kaczmarz Method (Randomized Kaczmarz)
  - **Why needed here:** This iterative method for solving linear systems is the mathematical proxy for the continual learning update rule used throughout the paper.
  - **Quick check question:** Can you explain how projecting onto the row space of a matrix solves Ax=b?

- **Concept:** Realizable / Interpolation Regime
  - **Why needed here:** The entire theoretical framework (Reduction 2) relies on the assumption that a perfect joint solution exists (interpolation).
  - **Quick check question:** Why does assuming the existence of a joint minimizer simplify the analysis of forgetting compared to a noisy setting?

- **Concept:** Moore-Penrose Pseudo-inverse (X^+)
  - **Why needed here:** The reduction to SGD constructs a modified loss function f_m using X^+, making this operator central to the equivalence.
  - **Quick check question:** How does the pseudo-inverse differ from the standard inverse, particularly regarding rank-deficient matrices?

## Architecture Onboarding
- **Component map:** Task Sampler -> Solver -> Evaluator
- **Critical path:** The calculation of the modified gradient term ∇f_m(w). Implementing this efficiently (approximating the pseudo-inverse or solving the linear system) is the main computational bottleneck.
- **Design tradeoffs:**
  - **Sampling Strategy:** "With replacement" (i.i.d.) vs. "Without replacement" (Shuffle). Paper shows "without replacement" achieves similar O(1/⁴√T) rates while ensuring coverage of all tasks.
  - **Step Size:** Analysis optimizes for η=1 (stepwise-optimal). Reducing η might stabilize training but could slow convergence rates.
- **Failure signatures:**
  - **Non-convergence:** If forgetting plateaus or grows, check the Realizability Assumption. If tasks are contradictory (no joint solution), O(1/k) or O(1/⁴√k) bounds do not apply.
  - **Dimensionality Scaling:** If error scales linearly with dimension d, you are likely observing older, looser bounds rather than dimension-independent rates enabled by random ordering.
- **First 3 experiments:**
  1. **Validate Reduction:** Replicate Scheme 2 (Block Kaczmarz) and Scheme 3 (SGD on modified objective) on small synthetic dataset to verify identical iterates w_t.
  2. **Ordering Ablation:** Run continual learning on permuted MNIST-like linear task. Compare random vs. adversarial ordering to observe "catastrophic" gap.
  3. **Dimension Independence:** Scale input dimension d while keeping task rank fixed. Plot forgetting F(k) to verify dimension-independent rate (O(1/⁴√k)) rather than prior O(d/k).

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What is the exact worst-case forgetting rate in continual linear regression, and does it lie strictly between Ω(1/k) and O(1/√k)?
- **Basis:** Discussion section notes while authors improved upper bounds, gap between lower and upper bounds persists.
- **Why unresolved:** Current proof technique introduces exponential dependence on step-size parameter ηβ, preventing tight characterization.
- **Evidence:** Refined last-iterate SGD analysis eliminating exponential sensitivity to ηβ, or specific construction of task sequence exhibiting rate slower than 1/k, would resolve this.

### Open Question 2
- **Question:** Can a tighter theoretical analysis distinguish convergence rates of random orderings with replacement versus without replacement?
- **Basis:** Appendix A states current universal rates for both orderings are similar up to constants and "do not indicate a clear advantage for either."
- **Why unresolved:** "Universal" nature of derived bounds absorbs specific structural differences between i.i.d. sampling and random permutations into constant factors.
- **Evidence:** Proof establishing strict separation (e.g., one order converges as O(1/k) while other is Ω(1/√k)) would resolve question.

### Open Question 3
- **Question:** How do forgetting rates and reduction to SGD change in non-realizable setting where label noise is present?
- **Basis:** Paper relies on Assumption 1 (Joint Linear Realizability) to establish equivalence between continual regression and block Kaczmarz method/SGD.
- **Why unresolved:** Reduction to "stepwise-optimal" SGD specifically utilizes property that w^★ perfectly solves all tasks; noise would break exact projection mapping.
- **Evidence:** Extending Theorem 11 and related reductions to noisy regression setting (where σ² > 0) would demonstrate if dimension-independent rates are achievable without realizability.

## Limitations
- Analysis critically depends on joint realizability assumption, which may not hold in practical continual learning scenarios with non-linear models or conflicting tasks.
- Focus on linear models and convex objectives limits direct applicability to deep learning settings.
- Dimension-independent rates rely heavily on random task ordering; unclear how robust results are to non-uniform or correlated task distributions.

## Confidence
- **High confidence:** Reduction of Block Kaczmarz to SGD and last-iterate convergence analysis (Section 5) - well-supported by rigorous proofs with clearly established mathematical framework.
- **Medium confidence:** Dimension-independent forgetting rates under random ordering - theoretical bounds are sound but empirical validation across diverse task distributions would strengthen claims.
- **Medium confidence:** Extension to block Kaczmarz and POCS methods - paper provides novel rates but practical implications and computational trade-offs warrant further investigation.

## Next Checks
1. **Empirical validation of reduction:** Implement both Scheme 2 (Block Kaczmarz) and Scheme 3 (SGD on modified objective) on the same synthetic dataset to verify they produce identical iterates w_t, confirming the reduction's practical validity.

2. **Robustness to task ordering:** Beyond uniform random sampling, test the forgetting bounds under correlated task orderings (e.g., tasks grouped by similarity) to assess the limits of randomization's protective effect against catastrophic forgetting.

3. **Non-realizable data extension:** Modify synthetic data generation to introduce small noise or conflicting tasks that violate joint realizability. Measure how quickly established rates break down and whether alternative regularization or replay mechanisms can restore performance.