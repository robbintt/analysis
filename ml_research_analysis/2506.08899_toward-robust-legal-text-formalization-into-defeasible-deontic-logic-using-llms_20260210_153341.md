---
ver: rpa2
title: Toward Robust Legal Text Formalization into Defeasible Deontic Logic using
  LLMs
arxiv_id: '2506.08899'
source_url: https://arxiv.org/abs/2506.08899
tags:
- rules
- legal
- rule
- atom
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the feasibility of using Large Language Models
  (LLMs) to transform legal texts into Defeasible Deontic Logic (DDL) representations.
  A structured pipeline segments legal norms into atomic snippets, extracts deontic
  rules, and assesses them for syntactic and semantic correctness.
---

# Toward Robust Legal Text Formalization into Defeasible Deontic Logic using LLMs

## Quick Facts
- arXiv ID: 2506.08899
- Source URL: https://arxiv.org/abs/2506.08899
- Reference count: 18
- Primary result: LLMs guided by structured Chain-of-Instructions prompting can transform legal texts into Defeasible Deontic Logic representations with quantifiable correctness and completeness

## Executive Summary
This study evaluates the feasibility of using Large Language Models (LLMs) to transform legal texts into Defeasible Deontic Logic (DDL) representations. A structured pipeline segments legal norms into atomic snippets, extracts deontic rules, and assesses them for syntactic and semantic correctness. A novel success metric measures completeness and correctness of formalizations, and a two-stage pipeline with a refinement step improves logical consistency. Experiments on the Australian Telecommunications Consumer Protections Code demonstrate that when guided effectively, LLMs can produce formalizations closely aligned with expert-crafted representations.

## Method Summary
The method employs a Chain-of-Instructions (CoI) prompting approach with 3-shot learning to guide LLMs through the complex task of converting legal text into DDL rules. The pipeline consists of: (1) segmentation of legal articles into atomic snippets using DeepSeek-R1, (2) single-snippet formalization using various LLMs with structured prompts, (3) evaluation against a gold standard using a 6-dimension checker (Q1-Q6), and (4) an optional refinement step using Claude Sonnet 4 to unify atom vocabulary across snippets. The evaluation metric S(l) ∈ [0,1] combines completeness (Q1) with syntactic validity (Q2), semantic correctness (Q3), deontic modality accuracy (Q4), precondition appropriateness (Q5), and atom naming quality (Q6).

## Key Results
- Chain-of-Instructions prompting with 3-shot examples significantly improves structured DDL formalization compared to unstructured approaches
- The refinement step using a separate LLM pass improves atom vocabulary consistency across snippets
- Reasoning LLMs (o3, DeepSeek-R1) and fine-tuned versions generally outperform traditional LLMs, though performance varies by model
- Segmentation of long legal articles into atomic snippets prevents information loss during formalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Instructions (CoI) prompting with few-shot examples guides LLMs to produce structured DDL formalizations more reliably than unstructured prompting.
- Mechanism: Explicit step-by-step instructions (define atoms → define if-then structure → identify deontic modalities → formalize) decompose the complex translation task into tractable subtasks, reducing cognitive load on the model and constraining output space.
- Core assumption: LLMs can follow multi-step procedural instructions when examples demonstrate the expected transformation pattern.
- Evidence anchors:
  - [Section 4.1]: "The prompt employed in our experiments was derived through a series of iterative refinements... The final version utilizes a three-shot learning approach."
  - [Section 4]: "In each of the following experiments, we provided LLMs with a prompt containing step-by-step instructions to guide the model through its task."
  - [corpus]: "A Neurosymbolic Approach to Natural Language Formalization and Verification" describes a two-stage neurosymbolic framework, suggesting decomposition strategies are broadly effective for formalization tasks.
- Break condition: If the legal text contains extensive cross-references requiring context beyond the current snippet, CoI alone fails to resolve semantic dependencies (Section 5.2 documents this limitation).

### Mechanism 2
- Claim: A post-hoc refinement step using a separate LLM pass consolidates inconsistent atom names across snippets, improving cross-snippet coherence.
- Mechanism: The refinement LLM receives all generated rules and normalizes synonymous atom names (e.g., unifying `resolvable15Days(X)` and `resolvedBy15Days(X)`), enforcing vocabulary consistency that single-snippet processing cannot achieve.
- Core assumption: Refinement models can identify semantic equivalence across atom names without introducing new errors.
- Evidence anchors:
  - [Section 4.4.2]: "For illustration, while the gold standard uniformly employs the atom resolvable15Days(X), the LLM-generated formalizations frequently produced a wide range of variants."
  - [Section 4.4.2]: "The results of this refinement step are presented in Figures 9 and 10" showing improved scores.
  - [corpus]: Weak direct corpus support; "Towards Trustworthy Legal AI through LLM Agents and Formal Reasoning" mentions multi-agent LLM approaches but doesn't specifically validate refinement stages.
- Break condition: Refinement models may generate excessively long atom names (100-150 characters) or output incomplete XML, as observed with OpenAI models (Section 4.4.2 notes failures with o3, GPT-4.1, GPT-4o in the refinement role).

### Mechanism 3
- Claim: Segmentation of long legal articles into atomic snippets prevents information loss during formalization.
- Mechanism: Splitting enumerations and complex paragraphs into smaller units ensures each snippet fits within the LLM's effective context window for detailed analysis, avoiding the compression that occurs when processing multi-page articles wholesale.
- Core assumption: Information in legal texts is sufficiently modular that segmentation does not destroy essential cross-sentence dependencies.
- Evidence anchors:
  - [Section 3.1]: "Overly long snippets risked losing critical information during formalization, whereas overly short ones hindered atom reuse."
  - [Section 3.1]: "In documents like the Australian Telecommunications Consumer Protections Code, where individual articles can span 4-5 pages, splitting the text into smaller segments helps the LLM systematically analyze each component."
  - [corpus]: No direct corpus evidence on segmentation effects for legal text; this mechanism is primarily validated within the paper.
- Break condition: Inter-paragraph references (e.g., "if clauses 8.2.1(c),(d) or (e) below have been complied with") require context that segmentation isolates, making generated rules semantically incomplete (Section 5.2).

## Foundational Learning

- **Defeasible Deontic Logic (DDL) rule structure**
  - Why needed here: The entire target formalism; rules have form `r: a₁, ..., aₙ ⇒ c` where elements are literals or deontic literals with operators [O] (obligation), [P] (permission), [F] (prohibition).
  - Quick check question: Given "Suppliers must inform consumers of complaint resolution," what DDL rule captures this? (Answer: `complaint(X), resolution(X) ⇒ [O] informResolution(X)`)

- **Superiority relations in DDL**
  - Why needed here: Legal norms frequently conflict; DDL uses a superiority relation to determine which rule prevails when conflicts occur.
  - Quick check question: If Rule A says "close complaints within 15 days" and Rule B says "urgent complaints close within 2 days," how does DDL resolve this? (Answer: Superiority relation specifies Rule B > Rule A for urgent complaints)

- **Short-circuit evaluation for rule assessment**
  - Why needed here: The paper's evaluation (Q1-Q6) uses sequential checking; early failure (e.g., syntactic invalidity) terminates assessment for a rule.
  - Quick check question: A rule has `closeComplaint(X)` in both antecedent and consequent. Which evaluation question fails first, and what happens to subsequent questions? (Answer: Q2 fails; Q3-Q6 are skipped and set to false)

## Architecture Onboarding

- **Component map:**
  Legal Text → [Segmentation LLM: DeepSeek-R1] → Law Snippets
       ↓
  [Formalization LLM: GPT-4.1/o3/DeepSeek-R1/etc. + CoI Prompt] → DDL Rules (XML)
       ↓
  [Evaluation Engine: 6-dimension checker (Q1-Q6)] → Success Score S(l)
       ↓
  [Optional: Refinement LLM: Claude Sonnet 4] → Consolidated Rules

- **Critical path:** Prompt engineering (CoI with 3-shot examples) → Single-snippet formalization → Q1-Q6 evaluation. The refinement step provides marginal gains but adds latency.

- **Design tradeoffs:**
  - Single-snippet processing maximizes detail but sacrifices atom reuse; multi-snippet batching improves reuse but causes over-merging of rules (Section 4.2.3).
  - Reasoning LLMs (o3, DeepSeek-R1) outperform traditional LLMs but may not respect output structure constraints (o3 failed single-interaction formatting in Section 4.2.3).
  - Fine-tuning showed limited benefit; GPT-4o improved after 1 epoch but degraded subsequently, suggesting overfitting risk with small datasets (Section 4.3).

- **Failure signatures:**
  - Syntactic redundancy: Antecedent contains consequence (Q2 failure)
  - Deontic modality swap: Permission formalized as obligation (Q4 failure)
  - Cross-reference gaps: Atoms like `clause8.2.1.c.complied(X)` generated but not reused in referenced paragraphs (Section 5.2)
  - Hallucinated atoms: `informResolution(X)` appearing without textual basis (Q3 failure)

- **First 3 experiments:**
  1. Run the three-shot CoI prompt (Listing 1) on 5 TCP Code snippets with GPT-4.1 and DeepSeek-R1; compare S(l) scores to establish baseline.
  2. Process the same snippets in a single interaction (Section 4.2.3) to quantify the detail-vs-reuse tradeoff for your specific legal domain.
  3. Apply the refinement step (Listing 4 prompt) using Claude Sonnet 4 to baseline outputs; measure atom vocabulary reduction and score improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the superiority relationship in DDL (used to resolve conflicts between applicable rules) be reliably formalized by LLMs via prompt engineering or a dedicated pipeline stage?
- Basis in paper: [explicit] "the formalization of the superiority relationship – currently omitted due to limited occurrences in the dataset – deserves further investigation, potentially via prompt engineering or a dedicated pipeline stage" (Conclusion).
- Why unresolved: The TCP Code dataset contained insufficient occurrences of superiority relations to evaluate this aspect of DDL.
- What evidence would resolve it: Evaluation on a legal corpus with frequent rule conflicts and explicit priority hierarchies, comparing LLM-extracted superiority relations against expert annotations.

### Open Question 2
- Question: How can cross-paragraph reference resolution be achieved to ensure that atoms generated for referenced clauses are properly reused in preconditions of dependent rules?
- Basis in paper: [explicit] "prompt engineering alone is insufficient to fully address the challenge of reference resolution. Instead, it requires the incorporation of additional procedural components into the methodology" (Section 5.2).
- Why unresolved: LLMs generated appropriate atoms (e.g., `clause8.2.1.c.complied(X)`) but did not reuse them in referenced paragraphs, rendering preconditions unmet. This persisted even with single-interaction formalization and refinement steps.
- What evidence would resolve it: A post-generation refinement phase that cross-checks references and unifies atoms across related clauses, evaluated on a dataset with dense inter-paragraph references.

### Open Question 3
- Question: Why does fine-tuning degrade performance for legal formalization tasks, and under what conditions (if any) could it become beneficial?
- Basis in paper: [inferred] Fine-tuning GPT-4o showed slight improvement after one epoch but then declined; fine-tuning GPT-4.1 performed worse than the base model across all configurations (Section 4.3).
- Why unresolved: The paper documents the failure but does not identify whether the issue stems from data scarcity, overfitting dynamics, or mismatch between fine-tuning objectives and the structured logic generation task.
- What evidence would resolve it: Systematic ablation studies varying training set size, learning rate schedules, and regularization strategies, coupled with analysis of attention patterns during rule generation.

## Limitations
- Gold-standard DDL formalization from a 2017 study is not publicly available, making independent validation difficult
- Evaluation relies on expert-annotated legal text from a single regulatory code (Australian Telecommunications Consumer Protections Code)
- The two-stage refinement pipeline introduces additional computational overhead and model dependency on Claude Sonnet 4

## Confidence
- **High confidence**: Structured CoI prompting improves DDL rule extraction compared to unstructured approaches
- **Medium confidence**: Specific performance rankings of LLMs (o3 > DeepSeek-R1 > GPT-4.1 > others) are domain-specific
- **Medium confidence**: Refinement step's effectiveness is demonstrated but may be vocabulary-pattern specific
- **Low confidence**: Generalizability to other legal domains based on single regulatory code testing

## Next Checks
1. Replicate the three-shot CoI prompting pipeline on a different legal corpus (e.g., GDPR clauses) using the same evaluation framework to test domain transferability
2. Implement the refinement step with alternative models (e.g., GPT-4o instead of Claude Sonnet 4) to assess whether improvements are model-specific or methodology-specific
3. Conduct ablation studies removing each evaluation dimension (Q1-Q6) to quantify their relative contribution to the overall success score S(l)