---
ver: rpa2
title: Argument-Based Comparative Question Answering Evaluation Benchmark
arxiv_id: '2502.14476'
source_url: https://arxiv.org/abs/2502.14476
tags:
- comparison
- evaluation
- human
- summary
- arguments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the CompQA evaluation framework to address
  the challenge of benchmarking comparative question answering systems. The framework
  defines 15 criteria for assessing comparative summaries and implements an automatic
  evaluation pipeline using LLMs as judges.
---

# Argument-Based Comparative Question Answering Evaluation Benchmark

## Quick Facts
- arXiv ID: 2502.14476
- Source URL: https://arxiv.org/abs/2502.14476
- Reference count: 40
- GPT-4 generates the highest quality comparative answers, while Llama-3 70B performs best as an automatic evaluator

## Executive Summary
This paper introduces the CompQA evaluation framework to address the challenge of benchmarking comparative question answering systems. The framework defines 15 criteria for assessing comparative summaries and implements an automatic evaluation pipeline using LLMs as judges. The study evaluates 6 contemporary LLMs (GPT-3.5, GPT-4, Llama3-8B, Llama3-70B, Perplexity, and Mixtral) on two datasets, along with human annotations. Results show that GPT-4 generates the highest quality comparative answers, while Llama-3 70B performs best as an automatic evaluator. The framework demonstrates strong correlation with human expert evaluations, validating its effectiveness for comparative QA assessment.

## Method Summary
The framework generates comparative summaries by prompting LLMs with four different scenarios of increasing structure constraints. These summaries are then evaluated using a separate LLM judge applying 15 criteria through few-shot prompting. The evaluation pipeline uses JSON output parsing and score aggregation (maximum 19 points). Human annotators evaluate a subset of 367 summaries to establish ground truth for validating LLM judges. The framework was tested on two datasets: one from CAM 2.0 and another with consumer product comparisons.

## Key Results
- GPT-4 generates the highest quality comparative answers across both datasets
- Llama-3 70B achieves the best evaluation performance with Spearman's correlation of 0.76 for individual criteria
- Structured prompts (Scenarios 3 and 4) consistently outperform unstructured approaches, scoring 17-19 points versus 11-16 points
- The framework demonstrates strong correlation with human expert evaluations (Spearman's ρ = 0.72, p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can evaluate comparative summaries with human expert-level quality when guided by structured criteria.
- Mechanism: The 15-criteria framework decomposes holistic quality assessment into discrete, scorable dimensions (structure, relevance, quality). This decomposition reduces cognitive load on human annotators and provides explicit rubrics for LLM judges, enabling consistent evaluation across both human and automatic assessors.
- Core assumption: Quality in comparative summaries can be meaningfully captured through additive scoring across independent criteria rather than holistic judgment alone.
- Evidence anchors:
  - [abstract] "The framework demonstrates strong correlation with human expert evaluations, validating its effectiveness for comparative QA assessment."
  - [section 3.2] "The results give a positive answer to the RQ-2: 'LLMs can reliably evaluate comparative summaries with human expert-level quality.'" Table 3 shows Spearman's correlation of 0.72 (p < 0.001) for LLaMA-3 70b.
  - [corpus] Weak external validation—neighbor papers focus on other QA domains (medical, legal, multimodal) but do not replicate this specific evaluation framework.
- Break condition: Correlation with human judgment drops significantly when evaluating summaries outside the training domain or when criteria are applied inconsistently (Krippendorf's α varies from 0.22-0.71 across models for final scores).

### Mechanism 2
- Claim: Prompt structure significantly impacts comparative answer quality, with structured role-based prompts outperforming simple prompts.
- Mechanism: Four prompt scenarios test increasing constraint levels. Scenarios 3 and 4 (analyst role, explicit structure requirements) consistently score higher (17-19 points) than Scenarios 1 and 2 (unstructured or argument-only, 11-16 points). The mechanism operates through explicit output schema specification reducing ambiguity in generation.
- Core assumption: Imposing structure constraints does not systematically bias the comparative content toward certain conclusions.
- Evidence anchors:
  - [abstract] "GPT-4 generates the highest quality comparative answers."
  - [section 4, Figure 6] Scenarios 3 and 4 score highest for both human and LLaMA-3 70b evaluation across all model families.
  - [corpus] No direct corpus validation for this prompt-engineering mechanism in comparative contexts.
- Break condition: When provided arguments are low-quality (CAM dataset), even structured prompts produce lower scores—Scenario 4 with CAM arguments underperforms Scenario 3 without them (Figure 6).

### Mechanism 3
- Claim: Larger open-weight models (Llama-3 70B) can match or exceed proprietary models (GPT-4) specifically in evaluation-as-judge tasks.
- Mechanism: The paper hypothesizes scale and instruction-tuning enable nuanced rubric application. Llama-3 70B achieves highest Spearman correlation (0.76 for individual criteria, 0.72 for total scores) against human judgments, outperforming GPT-4 (0.69/0.55).
- Core assumption: Correlation with human judgment is the appropriate proxy for evaluation quality, even though humans may themselves have biases.
- Evidence anchors:
  - [abstract] "Llama-3 70B performs best as an automatic evaluator."
  - [section 3.2, Table 3] LLaMA-3 70b shows α = 0.61 and Spearman's ρ = 0.76 for separate criteria scores.
  - [corpus] No external benchmark has validated this specific finding about open vs. proprietary model evaluation capabilities.
- Break condition: LLaMA-3 70B tends to overestimate performance compared to humans—assigning high scores (18-19) to some answers humans rate as mediocre (14-17), as shown in Figure 7 analysis.

## Foundational Learning

- Concept: **Comparative Question Answering (CQA) vs. Standard QA**
  - Why needed here: CQA requires weighing multiple objects against each other across aspects, not retrieving facts. The output is an argumentative summary, not a single answer.
  - Quick check question: Can you explain why token-overlap metrics (BLEU, ROUGE) would fail to capture quality in comparative summaries even if both outputs are high-quality?

- Concept: **LLM-as-a-Judge Paradigm**
  - Why needed here: The entire evaluation framework depends on substituting LLM scores for human annotation. Understanding the limitations of this substitution is critical.
  - Quick check question: What are two failure modes when using an LLM to evaluate outputs it could have generated itself?

- Concept: **Inter-Annotator Agreement Metrics (Krippendorf's α, Spearman's ρ)**
  - Why needed here: The paper validates its framework through correlation with human judgment. Readers must understand that α > 0.67 indicates substantial agreement, and the gap between per-criterion vs. total-score agreement matters.
  - Quick check question: Why might per-criterion agreement be higher than total-score agreement, and what does this imply about the scoring rubric?

## Architecture Onboarding

- Component map: Question → Generation (Scenario 3 or 4) → Evaluation (LLaMA-3 70B as judge) → Score comparison against human baseline

- Critical path: Question → Generation (Scenario 3 or 4) → Evaluation (LLaMA-3 70B as judge) → Score comparison against human baseline

- Design tradeoffs:
  - **Few-shot vs. zero-shot evaluation**: Paper uses 2-shot (one good, one bad example) for stability; zero-shot produced less coherent assessments.
  - **Criteria granularity**: 15 criteria enable fine-grained diagnosis but introduce annotation complexity; simpler rubrics trade interpretability for ease.
  - **Argument provision**: Providing CAM arguments (Scenarios 2, 4) enables grounding but introduces quality dependency on argument sources.

- Failure signatures:
  - **Model bias**: GPT-4 showed "clear bias" favoring Google over Yahoo even when given pro-Yahoo arguments, rewriting arguments to favor Google (Section 4).
  - **Aspect insensitivity**: LLaMA-3 70B scores summaries with/without specified aspects similarly, while humans penalize aspect-ignoring summaries (Table 4).
  - **Overestimation**: LLM judges systematically score higher than humans, particularly for high-quality outputs.

- First 3 experiments:
  1. **Baselining**: Generate 20 summaries using each scenario with GPT-4, evaluate with LLaMA-3 70B and human annotators on same subset. Calculate correlation.
  2. **Ablation**: Remove 5 criteria at random from the 15 and re-evaluate. Determine which criteria drive human-LLM agreement variance.
  3. **Bias probing**: Test the Google/Yahoo bias case with 5 model-evaluator pairs. Document whether this is isolated or systematic across popular/unpopular object comparisons.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CompQA evaluation criteria be effectively adapted to assess comparative summaries involving three or more objects?
- Basis in paper: [explicit] The authors state in the Introduction: "In this paper, we focus on comparing only two objects, leaving the cases with three or more for future research."
- Why unresolved: The current 15-criteria framework and scoring system (0–19 points) were designed specifically for binary comparisons (Object A vs. Object B).
- What evidence would resolve it: A modified set of criteria and scoring metrics that successfully evaluate multi-object comparisons with high correlation to human expert judgments.

### Open Question 2
- Question: Can smaller language models be trained to match the evaluation performance of the Llama-3 70B model on comparative summaries?
- Basis in paper: [explicit] The Conclusion notes: "As part of our future work, we plan to enlarge the comparative question dataset and train smaller language models for this task."
- Why unresolved: While Llama-3 70B was identified as the best evaluator, deploying models of this size is computationally expensive; the feasibility of distilling this capability is unknown.
- What evidence would resolve it: Benchmark results showing a fine-tuned smaller model (e.g., 7B or 8B parameters) achieving comparable Spearman correlation and Krippendorff's alpha scores against human evaluations.

### Open Question 3
- Question: How does the reliance on the CAM framework for argument retrieval impact the quality of generated summaries, and can human-annotated arguments improve results?
- Basis in paper: [explicit] The Limitations section states: "The arguments used in the strategies are solely derived from the CAM framework... future work could involve annotators in crafting or refining arguments."
- Why unresolved: The authors observed that Scenarios 2 and 4 (using CAM arguments) scored lower than Scenario 3 (no CAM arguments), suggesting the retrieved arguments may have been suboptimal or irrelevant.
- What evidence would resolve it: A comparative study evaluating summaries generated using CAM arguments versus those generated using high-quality, manually curated arguments.

### Open Question 4
- Question: What are the root causes of "popularity bias" in LLMs, where models ignore provided arguments to favor more popular entities?
- Basis in paper: [explicit] Section 4 discusses a specific case where GPT-4 ignored arguments favoring Yahoo to conclude Google was better, stating: "Further tests need to be conducted to determine the cause and possible solutions to this issue."
- Why unresolved: The paper identifies this as a potential "favoritism" or internal bias but does not quantify its prevalence across the dataset or propose a technical solution.
- What evidence would resolve it: A large-scale analysis of model outputs across various popular/unpopular entity pairs, followed by a method (e.g., constrained decoding) that forces adherence to provided evidence.

## Limitations
- The framework's reliance on LLM judges introduces systematic bias, particularly LLaMA-3 70B's tendency to overestimate summary quality
- The Google/Yahoo bias case demonstrates that LLMs may rewrite provided arguments to favor certain objects regardless of input
- Krippendorf's α of 0.22-0.71 across models for final scores indicates substantial variance in annotator agreement

## Confidence
- **High confidence**: GPT-4 produces higher-quality comparative answers than other LLMs
- **Medium confidence**: LLaMA-3 70B is the best evaluator (correlation metrics support this, but overestimation bias undermines claim of "human expert-level quality")
- **Low confidence**: The framework generalizes to domains beyond consumer products

## Next Checks
1. **Bias Replication Test**: Systematically test model bias across 10 pairs of popular/unpopular objects (brands, politicians, products) to determine if the Google/Yahoo bias is isolated or systematic.
2. **Cross-Domain Generalization**: Apply the framework to medical and legal comparative QA (where ground truth human annotations are more established) to validate performance outside consumer products.
3. **Evaluation Calibration**: Develop calibration techniques to adjust LLM judge scores to match human score distributions, particularly addressing the systematic overestimation by LLaMA-3 70B.