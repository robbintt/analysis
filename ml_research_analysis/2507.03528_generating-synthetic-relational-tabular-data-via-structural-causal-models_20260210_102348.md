---
ver: rpa2
title: Generating Synthetic Relational Tabular Data via Structural Causal Models
arxiv_id: '2507.03528'
source_url: https://arxiv.org/abs/2507.03528
tags:
- data
- relational
- node
- nodes
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel method to generate synthetic relational
  tabular data using structural causal models (SCMs). Unlike previous approaches that
  require real-world data to extract statistical patterns, this method independently
  constructs multiple interconnected tables with complex dependencies.
---

# Generating Synthetic Relational Tabular Data via Structural Causal Models

## Quick Facts
- arXiv ID: 2507.03528
- Source URL: https://arxiv.org/abs/2507.03528
- Reference count: 11
- One-line primary result: Novel method generates synthetic relational tabular data using structural causal models without requiring real data, demonstrated through 100K main table and 500-row auxiliary table with realistic inter-table dependencies

## Executive Summary
This paper introduces a method for generating synthetic relational tabular data using structural causal models (SCMs) that can create complex inter-table dependencies without requiring real-world data. Unlike previous approaches that extract statistical patterns from real data, this framework independently constructs multiple interconnected tables with realistic dependencies through directed acyclic graphs and shared key columns. The authors demonstrate their approach by generating a dataset where an auxiliary table contains information not present in the main table but influences its target columns, validating the realistic nature of the inter-table dependencies.

## Method Summary
The method uses Barabási-Albert model-generated DAGs as the structural foundation, with pre-sampling to estimate quantiles for noise calibration and k-means clustering for categorical nodes. During main generation, data propagates through one-layer neural networks with randomly assigned activations, with noise scaled by quantile ranges to preserve causal signal while adding realistic variability. The relational extension connects independent DAGs through coupling nodes and latent edges, creating dependencies where auxiliary tables influence main-table targets. Evaluation uses EmbDI row embeddings with k-NN weighted averaging for target prediction, comparing performance with and without auxiliary data across different embedding dimensions.

## Key Results
- Generated relational dataset with 100,000 rows in main table and 500 rows in auxiliary table
- Demonstrated that auxiliary table contains information not present in main table but influences its target columns
- Showed prediction performance improvement when using combined tables versus main table only, validating realistic inter-table dependencies
- Framework particularly suitable for training tabular foundation models and creating benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Quantile-Scaled Noise Integration in SCM Propagation
Pre-sampling without noise enables calibrated noise injection that preserves causal signal while adding realistic variability. A low-sample pre-run (1,000 samples) estimates the 10%- and 90%-quantiles (q₀.₁, q₀.₉) for each node. During main generation, noise is scaled by (q₀.₉ − q₀.₁), ensuring perturbation magnitude aligns with data distribution: xᵢ = gᵢ(xₚₐᵢ) + (q₀.₉⁽ⁱ⁾ − q₀.₁⁽ⁱ⁾)εᵢ. The quantile range captures the natural data spread, so proportional noise maintains parent information as the primary signal.

### Mechanism 2: Continuous Propagation with Post-Hoc Categorical Discretization
Delaying discretization until readout preserves information flow through child nodes that would otherwise be restricted by categorical bottlenecks. Data propagates as continuous n-dimensional vectors through all neural network layers. Categorical pooling (k-means cluster assignment) occurs only at readout via pᵢ(xᵢ) = argmin‖xᵢ − vₗ‖₂, where vₗ are cluster centroids from pre-sampling. Continuous intermediate representations enable richer downstream dependencies than early categorical encoding.

### Mechanism 3: Latent Inter-Table Causal Links via Coupling Nodes
Connecting independent DAGs through coupling nodes and latent edges creates realistic relational dependencies where auxiliary tables influence main-table targets. Two DAGs (Gₘₐᵢₙ, Gₐₐₐ) are linked via coupling node C (Gₐₐₐ → C → Gₘₐᵢₙ). Additional latent edges connect Gₐₐₐ features to Gₘₐᵢₙ targets, ensuring the auxiliary table carries predictive information absent from the main table. Latent causal edges create dependencies requiring multi-table integration for accurate target prediction.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for Causal Modeling**
  - Why needed: Core representation for SCMs—nodes are variables, edges define parent→child causal flow
  - Quick check question: Given nodes A→B→C, what happens if you reverse edge B→C to C→B while A→B remains?

- **Foreign Key Semantics in Relational Databases**
  - Why needed: Coupling node C mimics a foreign key column linking tables with different cardinalities
  - Quick check question: If the main table has 100,000 rows and the additional table has 500 rows with 175 unique C values, what join pattern emerges?

- **Embedding-Based Nearest Neighbor Prediction**
  - Why needed: Evaluation uses EmbDI row embeddings and k-NN weighted averaging for target prediction
  - Quick check question: Why average the 10 nearest neighbors' targets weighted by inverse distance rather than taking the single closest?

## Architecture Onboarding

- **Component map:**
  Structure Sampler → Root Initializer → Propagation Engine → Pre-Sampler → Noise Scaler → Pooling Layer → Relational Coupler

- **Critical path:**
  Pre-sample (1,000 rows) → Compute quantiles & clusters → Main generation (N rows) → Split into main/auxiliary tables by node membership

- **Design tradeoffs:**
  - Hidden dimension n: Larger n increases task difficulty but raises computational cost
  - Coupling node cardinality: Higher cardinality improves join specificity but risks sparse coverage
  - Latent edge density: More inter-table edges increase dependency complexity but may obscure primary signal
  - Assumption: Paper does not systematically ablate these parameters; sensitivity is claimed but not quantified.

- **Failure signatures:**
  - RMSE/AUC fails to improve when adding auxiliary table → latent edges may be disconnected or coupling node is uninformative
  - Categorical predictions cluster around single class → pre-sampling centroids are poorly separated
  - Training embeddings don't transfer to test rows → embedding dimension too low for combined table complexity

- **First 3 experiments:**
  1. Single-table baseline: Generate 10,000 rows × 10 nodes with n=2; predict each target via k-NN on row embeddings; record baseline RMSE/AUC.
  2. Relational validation: Add auxiliary table (500 rows) with coupling node (100 categories); compare target prediction with/without auxiliary data across embedding dimensions [16, 32, 64, 128].
  3. Ablation on hidden dimension: Fix graph structure; vary n ∈ {1, 2, 4, 8}; measure how prediction difficulty scales with n.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to incorporate multimodal data types such as images and text alongside numerical and categorical features? The current method only handles numerical and categorical features through continuous vector propagation and pooling functions; the SCM-based DAG structure does not account for high-dimensional, unstructured modalities. A modified architecture that integrates multimodal encoders into the SCM nodes and demonstrates successful generation of relational tables containing mixed data types with preserved inter-table dependencies would resolve this.

### Open Question 2
How does the method scale to relational databases with three or more interconnected tables including cross-connections between them? The current evaluation only demonstrates two coupled tables (Gmain and Gadd) connected via a single coupling node C; more complex relational structures remain untested. Experimental results showing successful generation and downstream task performance on datasets with multiple interconnected tables and cross-table relationships beyond simple pairwise coupling would resolve this.

### Open Question 3
How do key parameters (hidden dimension n, activation functions, graph depth) systematically affect data complexity and downstream task difficulty? Only one configuration (n=2) is demonstrated; no ablation study exists. Controlled experiments varying individual parameters while measuring downstream prediction quality (RMSE, AUC) to establish quantitative relationships between generation parameters and synthetic data utility would resolve this.

## Limitations

- The approach is primarily validated on a single synthetic experiment rather than real-world benchmarks, leaving questions about generalizability to complex, high-dimensional relational datasets
- The paper does not provide systematic ablation studies on key hyperparameters like hidden dimension size, coupling node cardinality, or latent edge density, making it difficult to assess robustness
- The evaluation relies on k-NN predictions using EmbDI embeddings without comparing against alternative relational modeling baselines

## Confidence

- **High confidence**: The core SCM-based generation framework and quantile-scaled noise integration mechanism
- **Medium confidence**: The relational extension through coupling nodes and latent edges, as validation is limited to synthetic experiments
- **Low confidence**: Claims about suitability for foundation model training without empirical demonstrations on actual foundation model benchmarks

## Next Checks

1. Conduct systematic ablation studies varying hidden dimension n, coupling node cardinality, and latent edge density to quantify their impact on generation quality and prediction difficulty
2. Implement evaluation against established relational generation benchmarks (RelDiff, TableDiffusion) to compare against state-of-the-art diffusion-based approaches
3. Test the generated data on actual foundation model training tasks (TabPFN, GraphPFN) to validate the claimed applicability for foundation model development