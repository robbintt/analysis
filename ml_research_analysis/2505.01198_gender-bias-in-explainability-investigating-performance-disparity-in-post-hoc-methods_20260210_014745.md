---
ver: rpa2
title: 'Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc
  Methods'
arxiv_id: '2505.01198'
source_url: https://arxiv.org/abs/2505.01198
tags:
- lime
- shap
- igxi
- grad
- femalemale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender disparities in post-hoc feature
  attribution methods used to explain language model predictions. The authors evaluate
  six widely used explainability methods (Gradient, Integrated Gradients, SHAP, LIME,
  and their input-multiplied variants) across three tasks and five language models
  (BERT, TinyBERT, GPT-2, RoBERTa, and FairBERTa) using seven evaluation metrics measuring
  faithfulness, robustness, and complexity.
---

# Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods

## Quick Facts
- **arXiv ID**: 2505.01198
- **Source URL**: https://arxiv.org/abs/2505.01198
- **Reference count**: 40
- **Primary result**: Post-hoc explainability methods exhibit significant gender disparities that persist even with unbiased training data

## Executive Summary
This study investigates gender disparities in post-hoc feature attribution methods used to explain language model predictions. The authors evaluate six widely used explainability methods (Gradient, Integrated Gradients, SHAP, LIME, and their input-multiplied variants) across three tasks and five language models (BERT, TinyBERT, GPT-2, RoBERTa, and FairBERTa) using seven evaluation metrics measuring faithfulness, robustness, and complexity. The results show that all six explanation methods exhibit significant gender disparities across the majority of experimental combinations, with more than 72% showing statistically significant differences and over 54% showing considerable effect sizes (Cohen's d ≥ 0.2). These disparities persist even when models are trained from scratch on unbiased datasets, indicating the bias stems primarily from the explanation methods themselves rather than the models or training data.

## Method Summary
The researchers conducted a comprehensive evaluation of six post-hoc explainability methods (Gradient, Integrated Gradients, SHAP, LIME, and their input-multiplied variants) across three distinct tasks and five language models. They used seven evaluation metrics measuring faithfulness, robustness, and complexity to assess explanation quality. To determine the source of bias, they trained models from scratch on unbiased datasets and compared results. The experimental design included systematic testing of different model architectures, tasks, and evaluation metrics to identify consistent patterns of gender disparity across the explanation methods.

## Key Results
- More than 72% of experimental combinations showed statistically significant gender disparities in explanation quality
- Over 54% of combinations exhibited considerable effect sizes (Cohen's d ≥ 0.2)
- Gender disparities persisted even when models were trained from scratch on unbiased datasets
- All six explanation methods showed significant disparities across majority of experimental conditions

## Why This Works (Mechanism)
The study reveals that post-hoc explainability methods introduce gender bias through their algorithmic design and computation processes, rather than inheriting it from the underlying language models or training data. The bias emerges from how these methods quantify feature importance and attribution, which appears to interact systematically with gender-related linguistic patterns in the input text. The persistence of disparities across different models and unbiased datasets suggests that the explanation methods themselves contain structural biases in their mathematical formulations that disproportionately affect certain gender representations.

## Foundational Learning
- **Post-hoc explainability methods**: Techniques that generate explanations after a model makes predictions, as opposed to inherently interpretable models; needed to understand the scope of bias investigation
- **Feature attribution**: The process of determining which input features contribute most to a model's prediction; central to understanding how explanations work
- **Cohen's d effect size**: A standardized measure of the difference between two groups; used to quantify the magnitude of gender disparities
- **Faithfulness metrics**: Evaluation measures that assess how well explanations reflect the actual decision-making process of the model
- **Robustness metrics**: Evaluation measures that assess the stability of explanations under input perturbations; important for understanding reliability
- **Input-multiplied variants**: Modified versions of explainability methods that incorporate input features into the explanation computation; tested to explore potential bias mitigation

## Architecture Onboarding
- **Component map**: Explanation methods (Gradient, IG, SHAP, LIME, Input-Multiplied variants) -> Language models (BERT, TinyBERT, GPT-2, RoBERTa, FairBERTa) -> Evaluation metrics (faithfulness, robustness, complexity) -> Gender disparity analysis
- **Critical path**: Input text -> Model prediction -> Explanation method computation -> Feature attribution scores -> Evaluation metrics -> Gender disparity assessment
- **Design tradeoffs**: The study balances comprehensive coverage of explanation methods against depth of analysis for each method; prioritizes statistical significance over detailed qualitative analysis of bias mechanisms
- **Failure signatures**: Systematic gender disparities across multiple evaluation metrics and model architectures; persistence of bias even with unbiased training data
- **3 first experiments**: 1) Test explanation methods on controlled synthetic data with explicit gender markers; 2) Compare explanation quality across gender-neutral versus gender-specific prompts; 3) Evaluate whether bias patterns differ for morphologically marked versus unmarked gender references

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis covers only three tasks and five language models, which may not capture all potential disparities across NLP applications
- Binary gender classification oversimplifies gender identity complexity and may miss intersectional effects
- The study does not explore whether disparities manifest differently across various demographic intersections or cultural contexts

## Confidence
- Gender disparities in explanation methods: High
- Bias origin in explanation methods rather than models: Medium
- Persistence of disparities across different training conditions: High
- Generalization across all NLP tasks and models: Low

## Next Checks
1. Replicate the study with a more diverse set of gender categories and intersectional identities to assess whether disparities persist or shift across different demographic groups
2. Conduct ablation studies testing individual components of explanation methods to isolate which specific algorithmic choices contribute most to gender disparities
3. Evaluate explanation methods on additional NLP tasks beyond the three studied to verify whether observed patterns generalize across different types of language understanding challenges