---
ver: rpa2
title: Causal Graph Recovery in Neuroimaging through Answer Set Programming
arxiv_id: '2506.09286'
source_url: https://arxiv.org/abs/2506.09286
tags:
- causal
- data
- graph
- graphs
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recovering causal graphs
  from fMRI time series data, where undersampling leads to loss of temporal information
  and ambiguity in identifying true causal relationships. The authors propose a novel
  method using Answer Set Programming (ASP) to incorporate undersampling effects explicitly,
  thereby improving causal graph recovery.
---

# Causal Graph Recovery in Neuroimaging through Answer Set Programming

## Quick Facts
- arXiv ID: 2506.09286
- Source URL: https://arxiv.org/abs/2506.09286
- Authors: Mohammadsajad Abavisani; Kseniya Solovyeva; David Danks; Vince Calhoun; Sergey Plis
- Reference count: 10
- Primary result: ASP-based method improves causal graph F1 score by 12% on undersampled fMRI data

## Executive Summary
This paper addresses the challenge of recovering causal graphs from fMRI time series data, where undersampling leads to loss of temporal information and ambiguity in identifying true causal relationships. The authors propose a novel method using Answer Set Programming (ASP) to incorporate undersampling effects explicitly, thereby improving causal graph recovery. Their approach, called RnR, retrieves multiple near-optimal solutions via OptN mode in Clingo, introduces density constraints to ensure biological realism, and prioritizes optimization by connection type (density, bidirectional, then directed edges). RnR is applied as a meta-solver on top of established causal inference methods (GIMME, MVGC, MV AR, FASK), enhancing their performance. Results show an average 12% improvement in F1 score over baseline methods. On realistic simulations, RnR demonstrates robustness to varying undersampling rates, maintaining high precision and recall even with aggressive subsampling (e.g., 3s intervals), where traditional methods degrade. The method achieves state-of-the-art performance in reconstructing causal graphs from undersampled fMRI data, offering a scalable and accurate solution for neuroimaging studies.

## Method Summary
The method uses Answer Set Programming to recover causal graphs from undersampled fMRI time series by explicitly modeling how temporal undersampling compresses causal structures. The approach takes an input graph H (from baseline methods or correlation analysis), enriches 2-cycles with bidirected edges, and applies adaptive weighting. Using Clingo's OptN mode, it retrieves multiple near-optimal solutions within a cost range. A three-stage prioritized optimization (density→bidirectional→directed edges) refines the graph structure under density constraints that enforce biological plausibility. The method functions as a meta-solver that enhances existing causal inference approaches by correcting for undersampling-induced distortions.

## Key Results
- 12% average improvement in F1 score over baseline methods
- Robust performance maintained at aggressive undersampling rates (u=3,4)
- Precision and recall remain high while traditional methods degrade with increased subsampling
- State-of-the-art performance in reconstructing causal graphs from undersampled fMRI data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling undersampling effects improves causal graph recovery from fMRI time series.
- Mechanism: The method accounts for how temporal undersampling compresses causal graphs—where an edge Vi → Vj in the observed undersampled graph Gu exists if and only if there is a directed path of length u in the true graph G1. Bidirected edges arise from common ancestors whose paths to both nodes complete within the undersampling window, effectively becoming unobserved confounders. By encoding these structural transformations as ASP constraints, the solver searches for source graphs that would produce the observed structure under undersampling.
- Core assumption: The true causal structure evolves at a faster timescale than measurement frequency, and the undersampling rate is unknown but finite.
- Evidence anchors:
  - [abstract] "incorporating the effects of sub-sampling in the derivation of causal graphs, resulting in more accurate and intuitive outcomes"
  - [section 2] "In the undersampled graph Gu, an edge Vi → Vj exists if and only if there is a directed path of length u from Vi to Vj in the fully observed graph G1"
  - [corpus] Weak direct support; neighbor papers address causal structure learning but not undersampling specifically.
- Break condition: If the true causal timescale matches or exceeds the measurement interval, undersampling effects vanish and the method offers no advantage.

### Mechanism 2
- Claim: Retrieving multiple near-optimal solutions rather than a single optimum improves robustness to noise in fMRI data.
- Mechanism: Using Clingo's OptN mode, the method retrieves all solutions within a specified cost range rather than only the minimum-cost solution. The cost function weights edge omissions and commissions based on confidence from the input graph. Multiple solutions form an equivalence class, enabling expert selection and reducing sensitivity to noise that might otherwise push a single solution away from the true structure.
- Core assumption: Noisy fMRI-derived input graphs produce multiple plausible solutions with comparable optimization costs.
- Evidence anchors:
  - [abstract] "ASP not only identifies the most probable underlying graph, but also provides an equivalence class of possible graphs for expert selection"
  - [section 3.2] "By setting the OptN flag in Clingo, we retrieve all solutions within a practical cost range, enabling a more robust exploration of near-optimal graphs"
  - [corpus] No direct corpus support for multi-solution ASP approaches in causal discovery.
- Break condition: If the input graph is highly accurate with low uncertainty, single-solution methods may be more efficient without performance loss.

### Mechanism 3
- Claim: RnR functions as a meta-solver that enhances existing causal inference methods by adding undersampling-aware constraints.
- Mechanism: Traditional methods (GIMME, MVGC, MVAR, FASK) produce graphs without undersampling correction. RnR receives their output, enriches 2-cycles with bidirected edges (which the paper observes correlate with true confounders), and applies adaptive weighting. The ASP solver then refines the graph structure subject to density constraints that enforce biological plausibility.
- Core assumption: First-order methods capture useful signal but systematically miss undersampling-induced structures like bidirected confounding.
- Evidence anchors:
  - [abstract] "our method can be used as a meta-approach on top of established methods to obtain, on average, 12% improvement in F1 score"
  - [section 3.3] "when methods return a length-2 loop between two nodes, then there is always (i) a bidirected edge between them in the correct model"
  - [corpus] Neighbor paper "Exact Graph Learning via Integer Programming" supports optimization-based graph learning but not the meta-solver pattern.
- Break condition: If the base method already accounts for undersampling or produces very low-quality input graphs, the meta-solver cannot recover useful structure.

## Foundational Learning

- Concept: Answer Set Programming (ASP)
  - Why needed here: ASP is the core constraint optimization engine; understanding declarative rule specification and the stable model semantics is required to modify encoding or constraints.
  - Quick check question: Can you explain how ASP differs from imperative optimization (e.g., gradient descent) in representing solution constraints?

- Concept: Markov Equivalence Classes in Causal Discovery
  - Why needed here: The method outputs equivalence classes rather than unique graphs; interpreting these requires understanding when different graphs imply the same conditional independencies.
  - Quick check question: Given two graphs with edges A→B→C versus A→B←C, which conditional independence statements differ between them?

- Concept: Undersampling and Compressed Graphs
  - Why needed here: The entire method revolves around how undersampling transforms temporal causal structure; understanding edge compression and confounder emergence is essential.
  - Quick check question: If the true causal graph has A→B→C with single-timestep edges, what edges appear in a graph undersampled at rate u=2?

## Architecture Onboarding

- Component map:
  Input module -> Graph enrichment -> ASP encoder (Clingo) -> OptN solver -> Prioritized optimizer -> Output

- Critical path:
  1. Generate input graph H from fMRI time series using a base method or correlation analysis
  2. Enrich H with bidirected edges where 2-cycles exist
  3. Encode undersampling constraints and density bounds in ASP
  4. Run Clingo in OptN mode to retrieve solution set
  5. Apply prioritized optimization stages sequentially
  6. Return top-k solutions ranked by cost for expert review

- Design tradeoffs:
  - OptN cost tolerance: Wider ranges produce more solutions but increase manual inspection burden (~50 solutions manageable per paper)
  - Density constraints: Tighter bounds enforce biological realism but may exclude valid sparse/dense structures
  - Prioritized ordering: Density-first ensures realistic structure but may prematurely eliminate valid low-density solutions

- Failure signatures:
  - Empty solution set: Constraints are over-restrictive; relax density bounds or cost tolerance
  - All solutions identical: Input graph has high certainty; single-solution mode may suffice
  - High variance in solution edges: Input graph is noisy; consider stronger adaptive weighting or better preprocessing
  - No improvement over base method: Undersampling rate may be minimal (1s TR); verify undersampling is actually present

- First 3 experiments:
  1. Replicate the edge-breaking experiment on synthetic graphs: Generate ground truth G1, undersample at rates 2-4, randomly delete edges, and measure omission/commission errors versus baseline sRASL to validate robustness.
  2. Apply RnR as meta-solver on Sanchez-Romero benchmark data: Run FASK and MVGC on simulated BOLD data, feed outputs to RnR, and compare F1 scores against uncorrected baselines.
  3. Test undersampling sensitivity with balloon model simulations: Generate VAR time series, convolve with BOLD balloon model, subsample at 1s, 2s, 3s intervals, and measure how precision/recall degrades for each method—RnR should maintain performance while baselines decline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the RnR framework be refined to weight specific estimation errors in the initial graph $H$ based on their varying impact on the quality of the final causal graph?
- Basis in paper: [explicit] The authors state in Section 6 that "not all errors in $H$ have the same effect on the quality of estimation" and minimizing overall error in $H$ "may not be the optimal strategy."
- Why unresolved: The current method minimizes a general cost function without distinguishing between errors that critically distort the causal structure versus those that have minimal downstream impact.
- What evidence would resolve it: A study identifying specific error types in $H$ that correlate strongly with final output failure, followed by a modified optimization scheme that penalizes these critical errors more heavily.

### Open Question 2
- Question: Can the ASP-based optimization be extended to handle causal graph recovery on high-dimensional datasets, such as voxel-level fMRI, where node counts increase by an order of magnitude?
- Basis in paper: [explicit] Section 6 notes that "extending the number of nodes by an order of magnitude could broaden the range of potential applications," highlighting current limitations in scalability.
- Why unresolved: Constraint optimization problems typically face combinatorial explosion as variables increase, and the current validation is limited to reasonably sized graphs (ROI-level).
- What evidence would resolve it: Demonstration of the method's computational tractability and memory efficiency on graphs with over 1,000 nodes while maintaining the reported F1 score improvements.

### Open Question 3
- Question: To what extent does the imposition of a density constraint bias the recovery of causal structures in systems where the true connectivity falls outside the "realistic" biological range?
- Basis in paper: [inferred] Section 3.4 introduces a constraint to keep connectivity density within a realistic range to ensure biological plausibility.
- Why unresolved: While this pruning aids biological realism, it creates an assumption that the true system must conform to standard density distributions, potentially masking novel or pathological connectivity patterns (e.g., hyper-connectivity in epilepsy).
- What evidence would resolve it: Experiments on simulated data specifically designed with extreme densities (very sparse or very dense) to test if the constraint causes the method to fail in recovering the true graph.

## Limitations
- The method assumes undersampling is the dominant source of error, potentially overlooking measurement noise and hemodynamic confounds
- Relies on expert selection from equivalence classes without automated scoring criteria
- Requires prior knowledge of plausible density ranges that may not generalize across brain regions or tasks

## Confidence
- Undersampling compression mechanism: High
- Meta-solver enhancement claim: Medium
- Multi-solution retrieval benefit: Low

## Next Checks
1. Test robustness to varying noise levels: Add Gaussian noise to simulated BOLD time series and measure how F1 degrades for RnR versus baselines.
2. Cross-method ablation: Run RnR with and without bidirectional edge enrichment to isolate the contribution of 2-cycle handling.
3. Real fMRI application: Apply RnR to open-task fMRI datasets with known ground truth (e.g., DCM-validated regions) to assess ecological validity.