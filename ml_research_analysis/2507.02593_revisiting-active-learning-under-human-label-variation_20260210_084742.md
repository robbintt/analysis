---
ver: rpa2
title: Revisiting Active Learning under (Human) Label Variation
arxiv_id: '2507.02593'
source_url: https://arxiv.org/abs/2507.02593
tags:
- label
- learning
- annotation
- variation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper examines how label variation\u2014especially human label\
  \ variation (HLV)\u2014affects active learning (AL). Traditional AL assumes a single\
  \ ground truth and a noiseless oracle, but these assumptions often fail in subjective\
  \ NLP tasks where HLV is present."
---

# Revisiting Active Learning under (Human) Label Variation

## Quick Facts
- arXiv ID: 2507.02593
- Source URL: https://arxiv.org/abs/2507.02593
- Reference count: 23
- The paper proposes decomposing label variation into signal (HLV) and noise, and adapting AL to handle distributional labels and annotator selection.

## Executive Summary
This paper examines how human label variation (HLV) affects active learning (AL) by challenging the standard assumption of a single ground truth. The authors propose treating labels as probability distributions rather than discrete classes, and decomposing observed label variation into informative signal (HLV) and noise (annotation error). They outline three key adaptations: using distributional loss measures, modifying acquisition functions to handle multiple annotators and HLV, and adding annotator selection strategies. The work establishes a conceptual framework for future empirical research in HLV-aware AL.

## Method Summary
The paper presents a conceptual framework for adapting active learning to handle human label variation. It proposes four key consequences: (C1) using distributional measures for classifier optimization, (C2) modifying acquisition functions to handle distributional labels and multiple annotator views, (C3) adding annotator selection functions to choose between humans and LLMs, and (C4) developing methods to assess label quality while distinguishing HLV from annotation noise. The framework references existing work by Baumler et al. (2023), Wang & Plank (2023), and van der Meer et al. (2024) but provides no concrete implementation details or empirical validation.

## Key Results
- Traditional AL assumptions fail when HLV is present in subjective NLP tasks
- Labels should be treated as distributions rather than discrete values
- Annotator selection strategies should optimize for capturing diverse perspectives
- LLMs may serve as cost-efficient annotators but raise questions about reliability

## Why This Works (Mechanism)

### Mechanism 1: Signal-Noise Decomposition
Treating all label variation as noise obscures informative signal. By decomposing LV into HLV (subjective but valid disagreement) and error (mistakes), the framework preserves uncertainty that represents genuine ambiguity. This works when subjectivity is non-zero and the task allows for legitimate multiple perspectives. The approach fails when tasks are purely objective or when decomposition cannot reliably distinguish error from valid variation.

### Mechanism 2: Distributional Label Representation
Replacing hard labels with probability distributions allows models to capture inherent non-determinism. Labels become distributions (e.g., Dirichlet) rather than classes, and models optimize distributional measures like KL divergence. This assumes the "truth" is continuous rather than discrete. The mechanism breaks when annotators provide deterministic labels despite subjectivity, or when distributional representations misrepresent true uncertainty.

### Mechanism 3: Annotator-Aware Acquisition
AL efficiency improves by optimizing who annotates alongside what to annotate. An "Annotator Selection Function" selects specific humans for diverse perspectives or LLMs for cost-efficiency. This assumes annotators have unique, valid perspectives rather than being interchangeable oracles. The approach provides no benefit when the annotation pool is homogenous or when all annotators provide identical perspectives.

## Foundational Learning

- **Human Label Variation (HLV)**: The central phenomenon where disagreement reflects subjective but valid perspectives rather than error. Why needed: Understanding that disagreement can be "plausible variability" rather than error is prerequisite for adopting soft-label architecture. Quick check: Does the annotation task rely on subjective interpretation (e.g., sentiment) or objective fact (e.g., OCR)?

- **The Active Learning (AL) Loop**: Standard cycle of Train → Select → Query → Repeat. Why needed: The paper modifies this loop by adding annotator selection and distributional decomposition. Quick check: In standard AL, what entity typically provides labels for selected instances?

- **Label Non-Determinism**: Statistical concept that a single input can legitimately map to multiple valid outputs. Why needed: Underpins the move to distributional labels and invalidates the "single ground truth" assumption. Quick check: If three experts label a borderline case, do you expect them to agree 100% of the time?

## Architecture Onboarding

- **Component map**: Unlabeled Pool + Annotator Pool → Model (Distributional Training) → Instance Acquisition → Annotator Selection → Updated Label Distributions
- **Critical path**: Implementing Label Distribution Storage - the system must store vectors of probabilities per instance rather than single classes to support distributional loss functions
- **Design tradeoffs**: Human vs. LLM annotators (authentic HLV vs. cost-efficient consistency); More annotators per instance vs. more instances covered
- **Failure signatures**: High disagreement on objective tasks (learning uncertainty about facts); Circularity in subjectivity (inferring subjectivity from variation that may be noise)
- **First 3 experiments**: 1) Baseline comparison of standard AL vs. HLV-AL on subjective datasets like hate speech detection; 2) Acquisition ablation testing if annotator selection yields more representative distributions than random assignment; 3) LLM verification comparing LLM-generated distributions against human-derived distributions

## Open Questions the Paper Calls Out

### Open Question 1
How can observed label variation be algorithmically decomposed into human label variation (signal) and annotation error (noise) within an active learning loop? The authors note this is a "complex endeavor" lacking methods to separate these factors dynamically. A validated method that successfully disentangles annotator error from subjective disagreement would resolve this.

### Open Question 2
How should acquisition functions be redesigned to balance querying new instances versus re-annotating existing ones to capture distributional labels? Standard functions assume single ground truth and ignore the value of multiple opinions. Empirical results showing novel acquisition strategies outperform standard uncertainty sampling on soft-label models would resolve this.

### Open Question 3
What criteria should drive annotator selection when choosing between LLM and human annotators? While LLMs provide cheap distributions, their reliability remains questionable. A comparison of model performance trained on LLM-generated versus human-annotated distributions across varying subjectivity tasks would resolve this.

## Limitations
- No empirical benchmarks provided to demonstrate performance gains from HLV-aware AL methods
- Missing concrete formulations for acquisition and annotator selection functions
- Unclear how to handle trade-off between capturing HLV signal and avoiding noise amplification

## Confidence
- **High confidence**: Problem identification (HLV invalidating standard AL assumptions) - well-supported by literature
- **Medium confidence**: Proposed solutions (distributional labels, HLV-aware acquisition) - conceptually sound but unverified empirically
- **Low confidence**: Implementation details - framework described at high level without concrete algorithms

## Next Checks
1. Implement and benchmark HLV-aware AL on a dataset with known subjectivity (e.g., GoEmotions) against standard AL baselines
2. Conduct controlled experiments to measure whether decomposing HLV from noise improves model robustness to out-of-distribution data
3. Test whether LLM-generated label distributions can effectively substitute for human annotations in capturing true HLV signals across multiple domains