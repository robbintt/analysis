---
ver: rpa2
title: 'SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for
  Automated Survey Generation'
arxiv_id: '2510.07733'
source_url: https://arxiv.org/abs/2510.07733
tags:
- survey
- papers
- citation
- generation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurveyG introduces a hierarchical citation graph framework to improve
  automated survey generation by modeling citation and semantic relationships among
  research papers. The graph, organized into Foundation, Development, and Frontier
  layers, enables multi-level summarization through horizontal clustering and vertical
  traversal, producing more structured and contextually rich surveys.
---

# SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation

## Quick Facts
- arXiv ID: 2510.07733
- Source URL: https://arxiv.org/abs/2510.07733
- Reference count: 39
- Primary result: Achieves up to 96.2 content quality and 95.6 structure scores on 10 CS topics, outperforming state-of-the-art methods

## Executive Summary
SurveyG introduces a hierarchical citation graph framework that organizes research papers into Foundation, Development, and Frontier layers to improve automated survey generation. The system combines horizontal clustering within layers and vertical traversal across layers to produce structured, multi-aspect summaries. Evaluated on 10 CS topics, SurveyG demonstrates superior content quality, citation recall, and structure compared to existing methods, with human and LLM-as-a-judge evaluations confirming its effectiveness in producing coherent, comprehensive surveys.

## Method Summary
SurveyG constructs a hierarchical citation graph from retrieved research papers, assigning them to three temporal layers based on trending scores derived from citations and publication years. The framework employs horizontal clustering (via Leiden algorithm) to identify research communities within each layer, and vertical traversal (via weighted BFS) to trace idea evolution from foundational works through development to frontier research. Multi-agent refinement iteratively improves survey quality through structured feedback between Writing and Evaluation agents, leveraging pre-built hierarchical summaries to manage long-context generation without concatenating all reference texts.

## Key Results
- Achieves up to 96.2 in content quality and 95.6 in structure scores
- Attains high citation recall (91.40) and F1 scores (83.49)
- Demonstrates superior coherence, coverage, and critical analysis compared to state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Organizing papers into Foundation/Development/Frontier layers enables structured summarization that captures research evolution.
- Mechanism: Papers are assigned to layers via a "trending score" (citations normalized by years since publication). Foundation = top-K by score; Development = pre-landmark year papers; Frontier = recent works. This temporal hierarchy allows separate summarization strategies per layer before synthesis.
- Core assumption: Citation counts and publication year correlate with a paper's conceptual role (foundational vs. incremental vs. emerging).
- Evidence anchors:
  - [abstract] "The graph is organized into three layers: Foundation, Development, and Frontier, to capture the evolution of research from seminal works to incremental advances and emerging directions."
  - [section 2.1.2] Defines formal layer assignment: `V_foundation`, `V_development`, `V_frontier` with trending score formula.
  - [corpus] Weak direct corpus support for three-layer partitioning; related work (SurveyForge, SciSage) uses different organizational schemes.
- Break condition: If citation data is sparse or field lacks clear foundational works, layer boundaries become arbitrary, degrading structure quality.

### Mechanism 2
- Claim: Combining horizontal clustering (within-layer) and vertical traversal (cross-layer) produces more diverse, multi-aspect summaries than flat retrieval.
- Mechanism: Horizontal uses Leiden algorithm to partition each layer into communities by citation+semantic proximity, then LLM summarizes each community. Vertical uses weighted BFS from each foundation paper through development to frontier, producing path-based summaries capturing idea evolution from each seed.
- Core assumption: Research communities cluster naturally via citation/semantic edges, and tracing citation paths reveals meaningful evolutionary narratives.
- Evidence anchors:
  - [section 2.1.3] "We propose a two-stage summarization framework designed to capture both the breadth and depth of the hierarchical citation graph."
  - [section 3.4] Ablation: Removing vertical traversal drops Synthesis by -7.4 points; removing horizontal clustering drops Synthesis by -8.5 points.
  - [corpus] No corpus papers validate this specific dual-traversal approach; mechanism is novel to this framework.
- Break condition: If citation graph is poorly connected or edges don't reflect true conceptual relationships, both clustering and traversal produce incoherent groups.

### Mechanism 3
- Claim: Pre-built hierarchical summaries injected as agent memory reduce long-context burden and improve factual grounding compared to concatenating raw papers.
- Mechanism: Instead of RAG-retrieving raw paper chunks at generation time, SurveyG pre-computes K+N summaries (K vertical + N horizontal). Writing Agent's memory is initialized with these; Evaluation Agent provides iterative feedback and suggests targeted retrieval queries. RAG supplements rather than replaces structured knowledge.
- Core assumption: Summaries preserve sufficient detail for survey writing, and iterative agent feedback converges on quality improvements.
- Evidence anchors:
  - [section 2.2] "The key innovation of SurveyG lies in its ability to manage long-context survey synthesis without concatenating all reference texts...Instead, it leverages hierarchical summarization from the citation graph G as structured knowledge injected into the Writing Agent."
  - [section 3.4] Removing multi-agent refinement (w/o MA) decreases Relevance (-3.9) and Structure (-2.9) points.
  - [corpus] Chain-of-Agents (Zhang et al.) uses multi-agent segmentation but not pre-built hierarchical memory.
- Break condition: If initial summaries are inaccurate or missing key papers, downstream generation inherits gaps; agent feedback may amplify errors if evaluation is uncalibrated.

## Foundational Learning

- Concept: **Graph clustering algorithms (Leiden/Louvain)**
  - Why needed here: Horizontal summarization depends on partitioning each layer into coherent research communities via citation/semantic edges.
  - Quick check question: Can you explain why Leiden guarantees well-connected communities better than Louvain?

- Concept: **Weighted breadth-first search (WBFS)**
  - Why needed here: Vertical traversal prioritizes semantically similar nodes via edge weights when expanding from foundation seeds.
  - Quick check question: How would you modify BFS to use cosine similarity weights for neighbor ordering?

- Concept: **Multi-agent conversation frameworks**
  - Why needed here: Writing/Evaluation agents iterate via structured feedback loops; understanding role separation and termination conditions is essential.
  - Quick check question: What failure modes occur if the Evaluation Agent is too permissive or too strict?

## Architecture Onboarding

- Component map:
  1. Paper Retrieval Module: Query expansion via LLM → keyword-based crawling → metadata + full-text extraction
  2. Graph Builder: Nodes = papers; edges = citation links + semantic similarity (cosine on embeddings); weights stored per edge
  3. Layer Assigner: Trending score calculation → Foundation (top-K), Development (<T, non-foundation), Frontier (≥T, non-foundation)
  4. Horizontal Summarizer: Leiden clustering per layer → LLM synthesizes each community
  5. Vertical Summarizer: WBFS from each foundation seed → hierarchical summarization along paths
  6. Multi-Agent Generator: Writing Agent (memory = pre-built summaries) + Evaluation Agent (feedback + retrieval queries) → iterative refinement (T_max loops)
  7. Output Assembler: Concatenates refined subsections into full survey

- Critical path: Paper retrieval → graph construction → layer assignment → horizontal clustering + vertical traversal → summary generation → outline construction → subsection writing with RAG augmentation → assembly. The traversal and summarization steps are rate-limiting.

- Design tradeoffs:
  - **K foundation papers**: Higher K improves coverage but increases vertical traversal cost and may dilute focus (optimal range 8-10 per ablation).
  - **T_max iterations**: More iterations improve quality with diminishing returns beyond 3; adds latency and token cost.
  - **Embedding type**: Summary-based embeddings richer but require full-text processing; abstract-based faster but may miss methodological detail.
  - **Memory vs. RAG**: Pre-built summaries reduce context load but require upfront computation; pure RAG is lazier but less structured.

- Failure signatures:
  - Low citation recall → likely graph construction missing key papers or retrieval keywords too narrow.
  - Poor synthesis scores → horizontal clustering producing incoherent communities or vertical paths not capturing evolution.
  - High variance across runs → evaluation agent feedback inconsistent or T_max insufficient for convergence.
  - Off-topic subsections → initial summaries drifted or retrieval queries ungrounded.

- First 3 experiments:
  1. **Validate layer assignment**: Run on a known field with clear seminal works; manually inspect Foundation layer for expected papers. If missing, tune trending score or K.
  2. **Ablate traversal strategies**: Generate surveys with (a) horizontal-only, (b) vertical-only, (c) both. Compare Synthesis/Critical Analysis scores to quantify contribution of each direction.
  3. **Stress-test agent loop**: Run T_max = 1, 3, 5 on same topic. Plot quality score vs. iteration to confirm convergence and identify when returns plateau.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SurveyG's hierarchical three-layer architecture generalize effectively to non-CS disciplines with different research evolution patterns?
- Basis in paper: [explicit] "Our evaluation focuses exclusively on English-language computer science publications; generalization to other disciplines or languages remains unexplored."
- Why unresolved: The Foundation/Development/Frontier taxonomy may not suit fields with different citation dynamics or publication cultures.
- What evidence would resolve it: Cross-domain evaluation on humanities, social sciences, or natural sciences benchmarks with comparative quality metrics.

### Open Question 2
- Question: How does the time landmark parameter T affect the quality and structure of generated surveys across fields with varying research velocities?
- Basis in paper: [inferred] The paper uses T=2025 for Development/Frontier layer split but does not ablate or discuss sensitivity to this parameter.
- Why unresolved: Different fields may require different temporal thresholds; rapid vs. slow-moving domains may need adaptive T selection.
- What evidence would resolve it: Systematic ablation study varying T across topics with known different publication rates, measuring resulting survey quality.

### Open Question 3
- Question: How can the framework mitigate biases inherent in citation-based layer assignment that may underrepresent work from marginalized researchers or non-prestigious institutions?
- Basis in paper: [explicit] "The citation graph inherently reflects existing biases in academic publishing, including systematic underrepresentation of work from marginalized researchers, non-prestigious institutions, and non-English publications."
- Why unresolved: No mitigation strategy is proposed or evaluated.
- What evidence would resolve it: Comparative analysis of citation-based vs. content-based layer assignment on diverse author/institution pools.

### Open Question 4
- Question: Can alternative text encoders improve semantic edge weighting quality without increasing computational overhead?
- Basis in paper: [inferred] DeBERTa-v3-large is used without comparison to domain-specific or newer encoders; encoder choice affects semantic relationship quality.
- What evidence would resolve it: Ablation comparing multiple encoders (domain-adapted, multilingual, etc.) on semantic clustering quality and downstream survey metrics.

## Limitations
- Citation graph inherently reflects existing biases in academic publishing, including systematic underrepresentation of work from marginalized researchers, non-prestigious institutions, and non-English publications.
- Evaluation on CS topics only; generalization to other disciplines remains unexplored.
- Full prompt templates are abbreviated in the paper, requiring reconstruction for exact reproduction.

## Confidence
- **High Confidence**: Content quality metrics (96.2 max) and structure scores (95.6 max) based on quantitative benchmarks.
- **Medium Confidence**: Layer assignment mechanism and multi-agent design, as core assumptions about citation significance and iterative refinement are plausible but not fully validated across domains.
- **Low Confidence**: Exact reproducibility of results without complete prompt templates and crawling module specifications.

## Next Checks
1. **Cross-domain robustness**: Apply SurveyG to fields with non-standard citation patterns (e.g., humanities, interdisciplinary areas) and compare layer coherence.
2. **Ablation on K and T_max**: Systematically vary foundation paper count (K=6,8,10) and max iterations (T_max=1,3,5) to confirm claimed optimal ranges.
3. **Long-term stability**: Evaluate SurveyG on topics with rapidly evolving research (e.g., AI/ML) over multiple time slices to test if layer assignments remain meaningful.