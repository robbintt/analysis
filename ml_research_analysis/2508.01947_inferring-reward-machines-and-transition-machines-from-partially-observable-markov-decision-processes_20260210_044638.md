---
ver: rpa2
title: Inferring Reward Machines and Transition Machines from Partially Observable
  Markov Decision Processes
arxiv_id: '2508.01947'
source_url: https://arxiv.org/abs/2508.01947
tags:
- state
- reward
- learning
- states
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of learning policies in Partially
  Observable Markov Decision Processes (POMDPs) where non-Markovian observations make
  standard reinforcement learning difficult. The authors identify that POMDPs exhibit
  two types of non-Markovian behavior: transition dependencies and reward dependencies,
  which current approaches like Reward Machines (RMs) only partially address.'
---

# Inferring Reward Machines and Transition Machines from Partially Observable Markov Decision Processes

## Quick Facts
- arXiv ID: 2508.01947
- Source URL: https://arxiv.org/abs/2508.01947
- Authors: Yuly Wu; Jiamou Liu; Libo Zhang
- Reference count: 40
- Primary result: Introduces Transition Machines to model transition dependencies in POMDPs, achieving 10x-1000x speedups over state-of-the-art baselines

## Executive Summary
This paper addresses the challenge of learning policies in Partially Observable Markov Decision Processes (POMDPs) where non-Markovian observations make standard reinforcement learning difficult. The authors identify that POMDPs exhibit two types of non-Markovian behavior: transition dependencies and reward dependencies, which current approaches like Reward Machines (RMs) only partially address. To tackle this, they introduce Transition Machines (TMs) to explicitly model transition dependencies, complementing RMs. They propose a unified framework called Dual Behavior Mealy Machine (DBMM) that subsumes both TMs and RMs, enabling a single inference algorithm.

The authors develop DB-RPNI, an efficient passive automata learning algorithm that avoids costly reductions used in prior work. They also introduce preprocessing optimizations and observation supplementation techniques. Experimental results show their method achieves speedups of up to three orders of magnitude over state-of-the-art baselines on benchmark environments. On a 25×25 grid-world, their approach successfully infers automata that enable a standard Q-learning agent to learn optimal policies. The method demonstrates both computational efficiency and scalability while producing compact, interpretable automata that effectively restore the Markov property to partially observable environments.

## Method Summary
The method addresses non-Markovianity in POMDPs by decomposing it into transition dependencies and reward dependencies. It introduces Transition Machines (TMs) to model transition dependencies and Reward Machines (RMs) to model reward dependencies. A unified Dual Behavior Mealy Machine (DBMM) framework is proposed that subsumes both TM and RM structures. The core inference algorithm is DB-RPNI, which adapts the RPNI algorithm to DBMMs using a red-blue state merging framework. The method includes preprocessing optimizations (redundant α-input removal and trivial β-input removal) and an observation supplement technique that augments observations with TM states before RM inference. This pipeline enables standard RL algorithms to operate in partially observable environments by restoring the Markov property.

## Key Results
- Achieved speedups of 10x to 1000x over state-of-the-art baselines on benchmark environments
- Successfully inferred automata in a 25×25 grid-world environment, demonstrating scalability
- Observation supplement technique prevented RM state explosion (reducing states from 218 to 2-15 in experiments)
- Enabled standard Q-learning agent to learn optimal policies in partially observable environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing non-Markovian behavior into transition dependencies and reward dependencies enables inference of minimal automata.
- Mechanism: The framework identifies that non-Markovianity in POMDPs arises from two distinct sources—transition dependencies (where next observations depend on history) and reward dependencies (where rewards depend on history). By modeling these separately with Transition Machines (TMs) and Reward Machines (RMs), each automaton need only capture one type of dependency, yielding smaller and more interpretable structures.
- Core assumption: The underlying Det-POMDP has deterministic transitions and rewards; the two dependency types are separable.
- Evidence anchors:
  - [abstract] "The authors identify two distinct types of non-Markovian behavior - transition dependencies and reward dependencies - and introduce Transition Machines (TMs) to explicitly model transition dependencies alongside existing Reward Machines (RMs)."
  - [Section 4.1] "This limitation arises because the minimal RM focuses exclusively on reward prediction accuracy... This problem reflects a fundamental characteristic of POMDPs from the agent's perspective."
  - [corpus] Weak direct support; neighboring papers focus on reward machines or automaton feedback but do not explicitly discuss transition/reward decomposition.
- Break condition: If transition and reward dependencies are strongly coupled (e.g., rewards depend on state transitions that themselves depend on reward history), the decoupled inference may produce non-minimal or incorrect automata.

### Mechanism 2
- Claim: Observation supplement using inferred TM states eliminates transition-related non-Markovianity for RM inference.
- Mechanism: After inferring a TM, each observation is augmented with the TM state reached before that observation, creating an augmented observation. This disambiguates histories where the same raw observation could lead to different outcomes, ensuring the RM only needs to model reward dependencies.
- Core assumption: The inferred TM is resolvent (accurately predicts environment transitions for all feasible histories).
- Evidence anchors:
  - [Section 5.3] "By decoupling transition-based non-Markovianity, we can infer more compact and interpretable RMs."
  - [Section 6.2, Table 1] Without observation supplement, RM state count increased from 2 to 15 (high data) or 218 (low data) in the 25×25 environment.
  - [corpus] No direct discussion of observation supplement in neighboring papers.
- Break condition: If the TM is not resolvent (e.g., training traces lack sufficient coverage), observation supplement may introduce spurious distinctions or fail to resolve true ambiguities.

### Mechanism 3
- Claim: Adapting RPNI to DBMMs provides polynomial-time inference with correctness guarantees under structure completeness.
- Mechanism: DB-RPNI constructs a prefix tree transducer from traces, then iteratively merges compatible states using a red-blue framework. The dual-input structure (α-inputs for outputs, β-inputs for transitions) allows direct inference without reducing to optimization problems like ILP or HMM parameter estimation.
- Core assumption: The sample set satisfies structure completeness (state coverage, transition coverage, and conflict convergence).
- Evidence anchors:
  - [Section 5.2] "When the input sample set satisfies the structure completeness condition, our algorithm is guaranteed to infer the minimal resolvent automaton."
  - [Section 5.2, Theorem 1] Complexity is O(|U| · |L| · T · F) under structure completeness, O(T³ · F) in general.
  - [corpus] RPNI is mentioned in [arxiv:2510.17386] as a passive automaton learning technique, supporting the algorithmic foundation.
- Break condition: If traces are insufficient for structure completeness, incorrect state merges may occur, producing non-resolvent automata.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The entire framework addresses learning in environments where agents receive incomplete state observations, breaking the Markov property.
  - Quick check question: Can you explain why the same observation-action pair might require different actions depending on history in a POMDP?

- Concept: **Reward Machines (finite-state automata encoding reward structures)**
  - Why needed here: RMs are the target representation for reward dependencies; understanding their structure (states, transitions, output functions) is essential for the inference task.
  - Quick check question: How does an RM's state-dependent reward-output function restore the Markov property?

- Concept: **Passive automaton learning (particularly the RPNI algorithm)**
  - Why needed here: DB-RPNI adapts RPNI's state-merging approach; understanding prefix tree construction and mergeability criteria is prerequisite to following the algorithm.
  - Quick check question: In RPNI, what determines whether two states in the prefix tree can be merged?

## Architecture Onboarding

- Component map: Preprocessing Module → DB-RPNI Core → Observation Supplement → Inference Pipeline → Downstream RL
- Critical path: Preprocessing → DB-RPNI for TM → Observation Supplement → DB-RPNI for RM → Standard RL with augmented state. Each step depends on the previous; failures cascade.
- Design tradeoffs:
  - Data requirements vs. automaton minimality: Structure completeness requires comprehensive trace coverage; insufficient data may produce non-resolvent automata.
  - Efficiency vs. generality: Current method is restricted to deterministic POMDPs (stochastic extensions not supported).
  - Observation supplement adds computational overhead but drastically reduces RM complexity.
- Failure signatures:
  - RM state explosion (>10x expected states): Likely missing observation supplement or TM is non-resolvent.
  - Timeout during inference: Check for missing preprocessing or overly long traces.
  - Non-convergent RL: Augmented state space may be non-Markovian due to incorrect automata; verify resolvent property on held-out traces.
- First 3 experiments:
  1. **Baseline efficiency comparison** (Section 6.1): Replicate 3×3, 4×4, 5×5 grid experiments comparing runtime against HMM-based and ILP-based methods. Validate 10x-1000x speedups.
  2. **Ablation study** (Section 6.2): On 25×25 environment, systematically disable each component (observation supplement, redundant α-input removal, trivial β-input removal) and measure RM state count and runtime. Confirm observation supplement prevents state explosion.
  3. **Downstream policy validation** (Section 6.3): Train Q-learning agent with augmented state (o, q, u) in 25×25 environment. Verify convergence to optimal policy and compare episode rewards against agent without automata augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DB-RPNI algorithm be extended to stochastic POMDPs while maintaining its efficiency over existing baselines?
- Basis in paper: [explicit] The conclusion identifies the restriction to deterministic POMDPs as a key limitation and suggests integrating probabilistic automata as a future direction.
- Why unresolved: The current theoretical guarantees and state-merging logic (local compatibility) rely on the deterministic nature of the transition and reward functions ($\delta_T$ and $\delta_R$), which do not hold in stochastic environments.
- What evidence would resolve it: A modified algorithm capable of inferring probabilistic automata from stochastic traces and empirical results showing convergence speeds comparable to the deterministic setting.

### Open Question 2
- Question: How can the inference framework be adapted to handle incomplete trace data where the "Structure Completeness" condition is not met?
- Basis in paper: [explicit] The conclusion notes that the algorithm relies on high-quality trace data and explicitly calls for future work on methods with "less stringent data assumptions," specifically citing incomplete data.
- Why unresolved: The algorithm's correctness guarantee (Theorem 1) depends on Structure Completeness; without full coverage, the state-merging process may produce incorrect or non-resolvent automata.
- What evidence would resolve it: Development of a robust inference mechanism (e.g., using statistical compatibility thresholds) that infers valid automata from sparse datasets, validated on ablation studies with reduced trace counts.

### Open Question 3
- Question: What is the precise necessary and sufficient condition for inferring a minimal correct automaton, beyond the sufficient "Structure Completeness" currently defined?
- Basis in paper: [inferred] Section 5.2 states that Structure Completeness is sufficient but not necessary, noting that the algorithm practically requires a "much weaker condition" (preventing incorrect merges) that remains mathematically undefined.
- Why unresolved: The paper provides a theoretical upper bound for data requirements but does not characterize the minimal information needed to distinguish states correctly.
- What evidence would resolve it: A formal characterization of the minimal conflict convergence properties required for convergence, supported by proofs of failure cases when this condition is violated.

## Limitations
- Restricted to deterministic POMDPs with deterministic rewards, limiting applicability to stochastic environments
- Efficiency gains depend critically on achieving structure completeness in trace collection, which may be challenging in large state spaces
- Trivial β-input removal relies on prior knowledge that may not generalize across domains

## Confidence

- **High confidence**: Computational efficiency improvements (empirical runtime reductions of 10x-1000x) and the basic algorithmic framework (DB-RPNI adaptation of RPNI)
- **Medium confidence**: The observation supplement technique's impact on RM compactness (limited ablation data in single environment)
- **Medium confidence**: The separability assumption between transition and reward dependencies (theoretical but not extensively validated across diverse POMDPs)

## Next Checks

1. Test scalability on stochastic POMDP variants by introducing controlled noise in transitions and rewards to quantify performance degradation
2. Conduct systematic coverage analysis to determine minimum trace requirements for structure completeness across different environment complexities
3. Perform cross-domain evaluation on non-grid environments (e.g., robotic manipulation, traffic control) to assess the generalizability of the transition/reward dependency separation