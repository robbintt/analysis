---
ver: rpa2
title: Representation Learning on a Random Lattice
arxiv_id: '2504.20197'
source_url: https://arxiv.org/abs/2504.20197
tags:
- features
- data
- cluster
- distribution
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes a random lattice model of data distributions
  using percolation theory to understand the geometric properties of learned features
  in neural networks. The model assumes a high-dimensional hypercubic lattice where
  input examples are connected if they yield the same outputs under the target function,
  with connections occurring randomly.
---

# Representation Learning on a Random Lattice

## Quick Facts
- arXiv ID: 2504.20197
- Source URL: https://arxiv.org/abs/2504.20197
- Reference count: 18
- Primary result: A random lattice model using percolation theory predicts feature geometry in neural networks, including fractal dimensions of 4, power-law distributions with τ=5/2, and hierarchical tree-like feature structures.

## Executive Summary
This paper introduces a random lattice model for understanding feature learning in neural networks through percolation theory. The model represents input examples as nodes in a high-dimensional hypercubic lattice, with connections formed when examples yield identical outputs under the target function. The analysis reveals a critical phase transition at occupation probability pc = 1/(z-1), leading to predictions about the geometric properties of learned features. The model suggests that natural features can be categorized into context features (identifying cluster membership), component features (providing coordinates for computation), and surface features (arising from imperfect learning). These clusters exhibit fractal geometry with dimension D = 4 in high dimensions and follow power-law size distributions.

## Method Summary
The paper employs percolation theory to analyze a random lattice model of data distributions. Input examples are placed on a high-dimensional hypercubic lattice, with edges connecting examples that produce the same outputs under the target function. The model assumes connections occur with a fixed probability p, and percolation theory is used to study the resulting cluster geometry. The analysis reveals a critical phase transition at pc = 1/(z-1), where z is the lattice degree. The model predicts three categories of features and characterizes their geometric properties, including fractal dimensions and power-law distributions.

## Key Results
- Percolation theory predicts a critical phase transition at pc = 1/(z-1), revealing geometric properties of learned features
- Natural features form hierarchical clusters with fractal geometry (D = 4) and power-law size distributions (τ = 5/2)
- Sparse autoencoders are most effective for datasets in the subcritical regime where feature sparsity predominates

## Why This Works (Mechanism)
The model works by abstracting neural network representations into a random lattice structure where connectivity represents functional equivalence. Percolation theory then provides a mathematical framework for analyzing the emergent geometric properties of these connected components. The critical phase transition captures the transition from sparse, disconnected features to dense, interconnected feature spaces. The fractal geometry emerges from the hierarchical nesting of clusters at criticality, while power-law distributions arise from the scale-invariant nature of the critical state. The model's predictions about feature categories (context, component, surface) reflect different computational roles within this geometric framework.

## Foundational Learning
- **Percolation Theory**: A mathematical framework for studying connectivity in random graphs. Needed to analyze the geometric properties of connected components in the random lattice model. Quick check: Can you explain the difference between subcritical, critical, and supercritical regimes?
- **Fractal Geometry**: Geometric objects with non-integer dimensions that exhibit self-similarity across scales. Needed to characterize the complex cluster structures that emerge in the model. Quick check: What is the box-counting dimension and how is it computed?
- **Power-Law Distributions**: Probability distributions where the probability of an event is proportional to a power of its size. Needed to describe the size distribution of clusters in the critical regime. Quick check: How does the exponent τ relate to the behavior of extreme events?

## Architecture Onboarding
- **Component Map**: Input space (hypercubic lattice) -> Target function -> Connected components (clusters) -> Feature categories (context, component, surface) -> Percolation analysis
- **Critical Path**: Lattice construction → Connection probability → Cluster formation → Percolation transition → Geometric characterization
- **Design Tradeoffs**: The model trades biological realism for mathematical tractability, using random lattices instead of actual network architectures. This enables rigorous analysis but may oversimplify real feature geometry.
- **Failure Signatures**: If predictions don't match empirical observations, potential failures include: incorrect mapping from real data to lattice model, breakdown of percolation assumptions, or missing higher-order interactions in feature formation.
- **First Experiments**: 
  1. Construct a simple lattice model with varying connection probabilities and measure cluster statistics
  2. Apply box-counting methods to measure fractal dimensions of simulated clusters
  3. Generate power-law distributed cluster sizes and fit to predicted τ = 5/2

## Open Questions the Paper Calls Out
None

## Limitations
- The random lattice abstraction may oversimplify the actual geometry of learned features in real neural networks
- Predictions about fractal dimensions and power-law distributions may not accurately capture real-world representations
- The claim that features form tree-like nested structures challenges established paradigms and requires empirical validation

## Confidence
- Theoretical framework mapping to neural networks: Medium
- SAE effectiveness in subcritical regime: Medium
- Four-dimensional component features and tree-like structure: Low

## Next Checks
1. **Empirical Dimension Analysis**: Measure the actual fractal dimension of feature clusters in trained neural networks using box-counting methods to test whether the predicted D=4 dimension emerges in practice.
2. **Component Feature Dimensionality**: Use SAEs with varying dimensionalities on real datasets to empirically determine whether four-dimensional representations consistently outperform lower-dimensional alternatives, as predicted.
3. **Power-Law Distribution Validation**: Analyze the size distribution of discovered features across multiple SAE runs and datasets to test whether the predicted τ=5/2 power-law scaling accurately describes the distribution of novel features.