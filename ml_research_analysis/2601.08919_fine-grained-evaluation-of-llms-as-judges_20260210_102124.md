---
ver: rpa2
title: Fine Grained Evaluation of LLMs-as-Judges
arxiv_id: '2601.08919'
source_url: https://arxiv.org/abs/2601.08919
tags:
- document
- inex
- relevant
- llms
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well large language models (LLMs) can
  serve as relevance assessors in information retrieval, going beyond document-level
  judgments to examine whether LLMs can accurately highlight relevant passages within
  documents. Using the INEX Wikipedia test collections, the authors prompt LLMs to
  generate rationales by highlighting relevant content in documents, and compare these
  outputs to human annotations.
---

# Fine Grained Evaluation of LLMs-as-Judges

## Quick Facts
- arXiv ID: 2601.08919
- Source URL: https://arxiv.org/abs/2601.08919
- Reference count: 36
- This study investigates whether LLMs can accurately highlight relevant passages within documents, not just judge document-level relevance.

## Executive Summary
This study investigates how well large language models (LLMs) can serve as relevance assessors in information retrieval, going beyond document-level judgments to examine whether LLMs can accurately highlight relevant passages within documents. Using the INEX Wikipedia test collections, the authors prompt LLMs to generate rationales by highlighting relevant content in documents, and compare these outputs to human annotations. Experiments with Llama-3.1-8B and GPT-4.1-mini reveal that while LLMs can identify relevant documents with reasonable accuracy, they often over-highlight content, inflating recall but lowering precision. GPT-4.1-mini generally outperforms the smaller Llama model in precision, though both struggle with documents containing sparse relevant passages. The study underscores that LLMs are promising but not yet reliable replacements for human assessors, especially for fine-grained relevance evaluation.

## Method Summary
The paper evaluates LLMs as fine-grained relevance judges by prompting them to highlight relevant passages within documents. Using INEX 2009 and 2010 Wikipedia collections with human-annotated passages, the authors employ 6-shot in-context learning with Llama-3.1-8B and GPT-4.1-mini. The LLM generates highlighted rationales for each query-document pair, which are then mapped to document spans via longest common subsequence matching. Performance is measured through macro and micro precision, recall, and F1 scores comparing LLM outputs to ground-truth human annotations. A two-stage pipeline is also tested where a zero-shot classifier first predicts document relevance before rationale extraction.

## Key Results
- GPT-4.1-mini consistently outperforms Llama-3.1-8B in precision metrics for passage highlighting
- Both models exhibit substantial over-highlighting, marking >50% of document content as relevant in 70-80% of cases
- Accuracy on non-relevant documents is very low (17-18% for GPT-4.1-mini), undermining the two-stage pipeline approach
- Models struggle particularly with "needle-in-haystack" documents where relevant content is sparse relative to document length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with k-shot exemplars enables LLMs to generate highlighted relevant passages (rationales) without parameter updates.
- Mechanism: The prompt includes task instructions followed by k query-document pairs (exemplars), where each exemplar concatenates all human-annotated relevant passages. The test instance is appended without the explanation, prompting the model to complete it.
- Core assumption: The model can generalize from few examples to identify and extract verbatim subsequences from documents that correspond to relevance, without hallucinating or paraphrasing substantially.
- Evidence anchors:
  - [abstract] "prompt LLMs to not only judge whether documents are relevant / non-relevant, but to highlight relevant passages"
  - [PAGE 2, Section 3.2] "our k-shot prompt includes exemplars to illustrate what we want (in the literature, this process is referred to as In-Context Learning (ICL)… and does not involve updating model parameters)"
  - [corpus] Related work on prompt sensitivity (2504.12408) underscores that ICL performance is highly prompt-dependent; not directly validating this paper's exemplar strategy.
- Break condition: If exemplars are poorly ordered or semantically distant from the test instance, the model's extraction accuracy degrades, and the rationale may diverge from ground truth.

### Mechanism 2
- Claim: Measuring token-level overlap between LLM-generated rationales and human highlights (precision/recall/F1) provides a fine-grained signal of whether the model is "right for the right reasons."
- Mechanism: Map LLM output to document subsequences via longest-common-subsequence matching at the word level, then compute overlap with gold-highlighted spans (offsets/lengths from INEX qrels) to derive macro/micro precision and recall.
- Core assumption: The LLM outputs verbatim text from the document rather than paraphrases, and that word-level LCS correctly aligns generated spans to document positions.
- Evidence anchors:
  - [abstract] "quantify how often these 'judges' are right for the right reasons"
  - [PAGE 4, Section 4.1] "We employ the pattern-matching algorithm proposed in [23]. This algorithm identifies the longest common subsequences (at the word level) between the document and the LLM-generated output."
  - [corpus] No direct corpus validation of this specific overlap metric; other LLM-as-judge papers focus on agreement rates rather than span-level rationales.
- Break condition: If models paraphrase, summarize, or hallucinate content not present verbatim in the document, LCS mapping fails or misaligns, undermining precision/recall validity.

### Mechanism 3
- Claim: Zero-shot, instruction-only prompts can separate document-level relevance prediction from rationale generation, enabling a two-stage pipeline.
- Mechanism: Stage 1 uses a zero-shot binary classification prompt (query + narrative + document → relevant/non-relevant). Stage 2, only if relevant, runs a few-shot rationale extraction to highlight spans.
- Core assumption: Binary classification can be learned from instructions alone without examples, and that separation reduces conflating relevance with extraction difficulty.
- Evidence anchors:
  - [PAGE 3, Section 3.3] "we use a zero-shot, instruction-only prompt… includes only a query, its detailed description and narrative, and a document, and asks the LLM to classify the document as relevant/non-relevant."
  - [PAGE 5, Section 5.1] Reports accuracy on relevant sets (Llama 90%/94%; GPT-4.1-mini 61%/72%) and very poor accuracy on non-relevant sets.
  - [corpus] Nearby work (2504.12408) reports high variability in LLM relevance judgments under prompt changes; not a direct confirmation but contextual caution.
- Break condition: High false positive rates on non-relevant documents (especially top-ranked) undermine the reliability of the first stage, leading to unnecessary or misleading rationale generation.

## Foundational Learning

- Concept: Relevance assessment in IR
  - Why needed here: The paper's core task is judging whether a document satisfies an information need, which underpins both the binary prediction and the passage highlighting.
  - Quick check question: Given a query "benefits of intermittent fasting" and a document that mentions fasting only in a historical-religious context with no health discussion, would you mark it relevant?

- Concept: In-Context Learning (ICL) / Few-shot prompting
  - Why needed here: The rationale extraction relies on providing exemplars in the prompt without model fine-tuning; understanding ICL is essential to interpret why exemplar selection/ordering matters.
  - Quick check question: If you swap the order of exemplars or replace one with a semantically unrelated example, what behavior change might you expect in the model's output?

- Concept: Token-level evaluation metrics (precision/recall/F1 at span level)
  - Why needed here: The paper evaluates not just binary decisions but how precisely the model identifies the exact relevant text spans; familiarity with these metrics is required to interpret macro vs micro scores.
  - Quick check question: If a model highlights an entire 10k-token document but only 500 tokens are truly relevant, what happens to precision and recall?

## Architecture Onboarding

- Component map:
  - Input Layer: Query + narrative + document text (plain text extracted from XML via libxml2)
  - Prompt Composer: Assembles task instruction, k exemplars (each with query/narrative/doc + concatenated gold highlights), and the test instance (blank explanation)
  - LLM Runner: Calls Llama-3.1-8B-Instruct or GPT-4.1-mini with specified decoding settings (temperature/top-k/top-p)
  - Output Parser: Uses regex to strip prefixes like "Explanation:", then applies LCS-based pattern matching to map generated text to document subsequences
  - Evaluation Engine: Computes character/word-level overlap with gold spans (from qrels), aggregates macro/micro precision, recall, F1
  - Optional Stage 1 Classifier: Zero-shot binary relevance predictor; if non-relevant, skip rationale extraction

- Critical path:
  1. Prepare dataset: Parse INEX XML, extract plain text, load qrels with offset/length annotations
  2. Build prompts: Select k exemplars (consider document length bins), format with instructions
  3. Run LLM: Generate rationales for each query-document pair (greedy decoding, max length aligned to doc length or budget)
  4. Post-process: Align output to document spans via LCS; compute overlap metrics
  5. Analyze: Compare macro vs micro scores, inspect over-highlighting patterns, examine failure modes (needle-in-haystack, discontiguous spans)

- Design tradeoffs:
  - Exemplar count vs prompt budget: More examples improve ICL but may exceed context window or dilute signal if poorly chosen
  - Determinism vs creativity: Low temperature (0) encourages verbatim extraction but may reduce adaptability; higher temperature risks paraphrasing/hallucination
  - Macro vs micro averaging: Macro emphasizes per-query fairness; micro emphasizes overall volume; choice affects interpretation of model quality

- Failure signatures:
  - Over-highlighting: Model marks >50% of document as relevant, inflating recall but tanking precision (observed in ~70–80% of cases for Llama)
  - False positives in Stage 1: Very low accuracy on non-relevant documents, especially those ranked highly by IR systems
  - Needle-in-haystack misses: Small relevant fractions in long documents yield lower precision
  - Discontiguous span struggles: Lower precision when gold highlights are scattered vs contiguous
  - Hallucination: Generation of extra examples (e.g., "Example 8") not in the prompt, requiring post-hoc filtering

- First 3 experiments:
  1. Reproduce baseline macro/micro precision/recall/F1 on INEX 2009/2010 using Exemplar 2 configuration with Llama-3.1-8B to validate pipeline correctness
  2. Ablate exemplar strategy: Compare Exemplar 1 (short docs), Exemplar 2 (random long docs), and Exemplar 3 (medium docs) to quantify sensitivity to exemplar length distribution
  3. Error analysis on needle-in-haystack bins: For document-length bins where gold fraction is <10%, manually inspect 20–30 cases to identify whether failures are due to over-highlighting, missed spans, or misaligned LCS mapping

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do different prompt variations and instruction phrasings impact the accuracy of LLMs in fine-grained relevance assessment and passage highlighting?
- **Basis in paper:** [explicit] The authors state in the Conclusion that "Prompt variations are known to significantly impact results produced by LLMs. This angle has not been studied in this paper, and needs careful consideration in future."
- **Why unresolved:** The study utilized a specific prompt structure (Figure 1) but did not perform ablation studies on the instruction text or format, leaving the sensitivity of the models to prompt engineering unknown.
- **What evidence would resolve it:** A comparative analysis of the same models across multiple prompt templates (e.g., varying the instruction style, order of examples, or output format) to measure variance in Precision/Recall.

### Open Question 2
- **Question:** Are the high rates of false positive relevance judgments produced by LLMs primarily driven by simple lexical cues (keyword matching) rather than semantic understanding?
- **Basis in paper:** [explicit] The authors note an "unexpectedly large number of false positives" and explicitly list the need to "look at whether these are also connected to lexical cues" as a future direction.
- **Why unresolved:** While the paper observes that LLMs struggle to identify non-relevant documents, it does not analyze the linguistic mechanisms (e.g., term overlap) causing these errors.
- **What evidence would resolve it:** Correlation analysis between false positive rates and the degree of lexical overlap (e.g., BM25 scores or term frequency) between the queries and the misjudged documents.

### Open Question 3
- **Question:** What are the computational, financial, and ecological trade-offs between meticulously curating few-shot examples versus relying on larger, more capable LLMs to compensate for prompt deficiencies?
- **Basis in paper:** [explicit] In the Conclusion, the authors state they "would like to study the computational, financial and ecological tradeoff between carefully choosing examples, and using large LLMs to make up for possible deficiencies in the exemplars."
- **Why unresolved:** The paper compares a smaller model (Llama 8B) with a larger one (GPT-4.1-mini) using fixed exemplars, but it does not quantify the cost efficiency of optimizing the input (exemplars) versus scaling the model.
- **What evidence would resolve it:** A study measuring the total resource cost (inference time, energy, API fees) required to achieve a target F1 score using: (a) a small model with optimized exemplars, and (b) a large model with random exemplars.

## Limitations

- The models exhibit significant over-highlighting behavior, marking more than half of document content as relevant in 80.7% of cases, which substantially reduces precision
- The zero-shot binary classifier shows very low accuracy on non-relevant documents (as low as 17% for GPT-4.1-mini), undermining the two-stage pipeline approach
- The LCS-based alignment method assumes verbatim extraction, but models may paraphrase or hallucinate content, potentially invalidating precision/recall metrics

## Confidence

**High Confidence**: The observation that GPT-4.1-mini outperforms Llama-3.1-8B in precision metrics is well-supported by the experimental results. The pattern of over-highlighting and its impact on precision-recall tradeoff is clearly demonstrated across multiple experiments.

**Medium Confidence**: The claim that LLMs can identify relevant documents with reasonable accuracy is supported, but the very low accuracy on non-relevant documents (particularly for GPT-4.1-mini at 17-18%) suggests this confidence should be tempered. The mechanism of in-context learning enabling passage extraction is theoretically sound but not extensively validated through ablation studies.

**Low Confidence**: The assertion that the two-stage pipeline (binary classification followed by rationale extraction) is effective is questionable given the poor performance of the first stage on non-relevant documents. The validity of LCS-based alignment for paraphrased outputs is uncertain without additional validation.

## Next Checks

1. **Exemplar Sensitivity Analysis**: Systematically vary the number of exemplars (k=2, 4, 6, 8) and their semantic similarity to test instances to quantify how exemplar selection impacts precision and recall. This would validate the robustness of the in-context learning mechanism.

2. **Paraphrase Detection Study**: Manually annotate a sample of LLM outputs to determine the frequency of paraphrased vs verbatim content, then measure how this affects LCS alignment accuracy and overlap metrics. This would validate whether the evaluation method assumes correct model behavior.

3. **Non-Relevant Document Classifier Improvement**: Test alternative prompt strategies or few-shot approaches for the binary classification stage, particularly focusing on improving accuracy on non-relevant documents which currently show very poor performance.