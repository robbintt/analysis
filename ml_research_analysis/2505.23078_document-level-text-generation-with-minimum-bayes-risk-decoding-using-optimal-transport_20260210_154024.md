---
ver: rpa2
title: Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal
  Transport
arxiv_id: '2505.23078'
source_url: https://arxiv.org/abs/2505.23078
tags:
- text
- machine
- utility
- computational
- document-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the effectiveness of Minimum Bayes Risk
  (MBR) decoding for document-level text generation tasks, where evaluating and generating
  longer texts presents unique challenges compared to sentence-level tasks. The authors
  propose MBR-OT, a variant of MBR decoding that leverages Wasserstein distance (optimal
  transport) to compute document-level utility using sentence-level utility functions.
---

# Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport

## Quick Facts
- arXiv ID: 2505.23078
- Source URL: https://arxiv.org/abs/2505.23078
- Reference count: 40
- Key outcome: MBR-OT improves document-level text generation by using Wasserstein distance to aggregate sentence-level utilities, achieving up to 3.72-point gains in MetricX-23 scores.

## Executive Summary
This paper addresses the challenge of document-level text generation by proposing MBR-OT, a Minimum Bayes Risk decoding variant that leverages optimal transport (Wasserstein distance) to compute document-level utility from sentence-level metrics. The method enables the use of established sentence-level metrics like MetricX, BERTScore, and BLEU for guiding the generation of longer documents with varying structures. Extensive experiments on machine translation (WMT24 En-De/En-Ja), text simplification (JADOS), and dense image captioning (PixelProse) demonstrate consistent improvements over standard MBR decoding, with the most significant gains observed in Japanese-English translation.

## Method Summary
The method generates 32 candidate documents using epsilon sampling, segments them into sentences, and computes pairwise sentence-level utilities. Optimal transport is then used to align sentence sets between candidate and reference documents, producing a document-level utility score. The hypothesis with highest expected utility across all candidates is selected. The approach supports three OT formulations (Linear Assignment, Wasserstein Distance, Entropic Regularized WD) and allows for uniform or length-based sentence weighting.

## Key Results
- MBR-OT achieves up to 3.72-point improvement in MetricX-23 scores for En-Ja translation and 2.33 points for En-De translation
- Method maintains effectiveness on tasks with fewer output segments (CNNDM summarization)
- Consistent outperformance across three diverse document-level tasks (MT, simplification, image captioning)
- Demonstrates robustness to structural variations like sentence reordering and merging

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Wasserstein Distance enables more accurate document-level utility by optimally matching sentence segments, accommodating structural variations.
- **Mechanism:** MBR-OT computes document-level utility by solving an optimal transport problem between sentence sets, allowing many-to-many matching that handles cases where one sentence in candidate corresponds to multiple in reference.
- **Core assumption:** Document utility can be effectively represented by optimal alignment and aggregation of segment utilities without strict sequential order enforcement.
- **Evidence anchors:** Abstract mentions leveraging Wasserstein distance to compute document-level utility using sentence-level functions. Section 3 formalizes utility as OT computation between sentence distributions.
- **Break condition:** Fails if OT formulation cannot handle structural changes like sentence merging, or if sentence segmentation quality is poor.

### Mechanism 2
- **Claim:** Method enables effective use of established sentence-level metrics for document-level generation guidance.
- **Mechanism:** MBR-OT acts as adapter that plugs sentence-level metrics into OT framework to produce document-level scores, bypassing need for scarce document-level metrics.
- **Core assumption:** Sentence-level metric good for segment judgment will serve as good proxy for document-level quality when optimally aggregated.
- **Evidence anchors:** Abstract states approach enables use of established sentence-level metrics like BLEU, BERTScore, MetricX for document-level evaluation.
- **Break condition:** Assumption breaks if sentence-level metric is inherently flawed for task or language pair.

### Mechanism 3
- **Claim:** MBR decoding improves quality by selecting hypothesis maximally similar to model's own distribution of plausible outputs.
- **Mechanism:** Algorithm samples hypotheses, computes utility of each against all others, selects one with highest average utility - the most "representative" among probable outputs.
- **Core assumption:** Candidate similar to many other probable candidates is more likely to be faithful and correct than outlier with high individual probability.
- **Evidence anchors:** Abstract defines MBR as using utility function to estimate output with highest expected utility from candidate set. Section 2.1 defines MBR selection rule.
- **Break condition:** Mechanism degrades if sample size too low to reliably estimate expected utility, or if model's distribution is systematically biased.

## Foundational Learning

- **Concept:** Minimum Bayes Risk (MBR) Decoding
  - **Why needed here:** Foundational algorithm that selects output based on consensus among samples rather than maximum probability.
  - **Quick check question:** How does MBR's selection criterion differ from standard beam search?

- **Concept:** Optimal Transport (Wasserstein Distance)
  - **Why needed here:** Core technical contribution that computes distance between sentence distributions by finding minimum-cost alignment, allowing flexible matching.
  - **Quick check question:** Why would Wasserstein Distance be superior to simple sentence-by-sentence alignment for documents with reordered sentences?

- **Concept:** Utility Functions (e.g., BLEU, BERTScore, MetricX)
  - **Why needed here:** Quality model that MBR optimizes; understanding their strengths is critical for knowing what MBR-OT optimizes for.
  - **Quick check question:** If using purely lexical utility like BLEU for MBR-OT, what high-quality but paraphrased outputs might it incorrectly penalize?

## Architecture Onboarding

- **Component map:** Hypothesis Sampler -> Sentencizer -> Sentence-Level Scorer -> Optimal Transport Solver -> MBR Aggregator
- **Critical path:** Generate Samples -> Sentencize -> Compute Pairwise Sentence Costs -> Compute Document Utility via OT -> Select Best Candidate. OT step is primary computational bottleneck.
- **Design tradeoffs:**
  - OT Formulation: Linear Assignment (faster, brittle) vs Wasserstein Distance (flexible, slower) vs Entropic Regularized WD (middle ground)
  - Probability Weighting: Uniform vs Length-weighted
  - Sample Size: Larger N improves selection but increases computation quadratically
- **Failure signatures:** Excessive runtime (>4x slower than standard MBR), pronoun/reference errors from ignoring sentence order, poor performance on short texts with 1-2 sentences
- **First 3 experiments:**
  1. Metric Validation: Correlate MBR-OT scores with human judgments on test set
  2. Formulation Ablation: Compare MBR-LA, MBR-WD, MBR-WDÎµ on document-level translation
  3. Sample Scaling: Measure performance and latency with N = 4, 8, 16, 32

## Open Questions the Paper Calls Out

- **Open Question 1:** Can structure-aware OT formulations (e.g., Fused Gromov-Wasserstein) resolve ordering blindness when segment order carries semantic meaning?
  - Basis: Authors suggest using more sophisticated OT formulations to account for segment structures
  - Why unresolved: Current implementation ignores segment order entirely, failing on discourse phenomena like coreference
  - What evidence would resolve it: Comparative experiments with Fused Gromov-Wasserstein on coreference-annotated datasets

- **Open Question 2:** Does MBR-OT's performance advantage persist under human evaluation, or does automated metric bias inflate observed gains?
  - Basis: Authors acknowledge dependence on automated metrics and desirability of human evaluation
  - Why unresolved: MBR decoding may overfit to utility metric, and using same metric family for evaluation may compound bias
  - What evidence would resolve it: Human evaluation comparing MBR-OT vs standard MBR on adequacy, fluency, and discourse coherence

- **Open Question 3:** Can alternative segment weighting schemes (probability mass, entropy, or learned informativeness) improve MBR-OT for informal texts with high segment variance?
  - Basis: Authors consider uniform or length-based weighting and suggest evaluating probability mass or entropy approaches
  - Why unresolved: Uniform/length weighting may underweight highly informative segments in informal texts
  - What evidence would resolve it: Experiments comparing weighting schemes on social media/conversational document tasks

## Limitations

- Effectiveness critically depends on sentence segmentation quality and appropriateness of sentence-level utility function
- Performance gain varies significantly by task and metric (3.72 points for En-Ja vs 2.33 points for En-De)
- Method shows weakness on tasks with few output segments (CNNDM summarization)
- Potential degradation on tasks requiring strict sequential consistency due to ignoring sentence order

## Confidence

- **High Confidence:** Core mechanism of using Wasserstein distance for document-level utility from sentence-level utilities is technically sound and well-supported by results across diverse tasks
- **Medium Confidence:** Claim of consistent outperformance over standard MBR is supported but relative gains vary significantly by task and metric
- **Low Confidence:** Assertion that MBR-OT is "robust and effective" for all document-level tasks is not fully validated, with acknowledged weaknesses on certain tasks

## Next Checks

1. **Failure Mode Analysis:** Systematically test MBR-OT on documents with challenging structures (extensive reordering, sentence merging/splitting) to quantify degradation and identify failure signatures
2. **Sample Size Sensitivity:** Conduct comprehensive study on impact of varying candidate samples (N) on performance and computational cost to establish optimal trade-offs
3. **Metric Ablation Study:** Evaluate MBR-OT using range of sentence-level metrics (BLEU, BERTScore, MetricX) on same tasks to isolate contribution of OT mechanism from utility function choice