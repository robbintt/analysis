---
ver: rpa2
title: 'R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging'
arxiv_id: '2510.13854'
source_url: https://arxiv.org/abs/2510.13854
tags:
- rules
- zarma
- loss
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R2T introduces a principled learning framework that embeds multi-tiered
  linguistic rules directly into neural network training objectives via an adaptive
  loss function, teaching models to handle out-of-vocabulary words with principled
  uncertainty. For Zarma POS tagging, the R2T-BiLSTM model trained without labeled
  data achieved 98.2% accuracy, outperforming AfriBERTa fine-tuned on 300 labeled
  sentences.
---

# R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging

## Quick Facts
- arXiv ID: 2510.13854
- Source URL: https://arxiv.org/abs/2510.13854
- Authors: Mamadou K. Keita; Christopher Homan; Sebastien Diarra
- Reference count: 18
- Key outcome: R2T-BiLSTM trained without labeled data achieved 98.2% accuracy on Zarma POS tagging, outperforming AfriBERTa fine-tuned on 300 labeled sentences.

## Executive Summary
R2T introduces a principled learning framework that embeds multi-tiered linguistic rules directly into neural network training objectives via an adaptive loss function, teaching models to handle out-of-vocabulary words with principled uncertainty. For Zarma POS tagging, the R2T-BiLSTM model trained without labeled data achieved 98.2% accuracy, outperforming AfriBERTa fine-tuned on 300 labeled sentences. For the more complex NER task, an R2T-Transformer pre-trained with rules and fine-tuned on only 50 labeled sentences surpassed a baseline trained on 300. The approach demonstrates that encoding explicit linguistic knowledge can be more data-efficient than traditional annotation-heavy methods, especially in low-resource settings.

## Method Summary
R2T integrates a four-tiered system of linguistic rules (unambiguous lexicon, ambiguous lexicon, morphology, syntax) into neural network training via a composite loss function. The framework uses FastText embeddings combined with character-level BiLSTM embeddings as input, processed by either a BiLSTM or Transformer encoder. The training objective combines four loss terms: lexical loss for rule-covered words, syntactic loss for valid tag transitions, distributional loss for encouraging diverse predictions, and adaptive OOV loss that forces uniform distributions for unknown words. The model is trained entirely on unlabeled data using rules as supervision, with optional fine-tuning on small labeled sets.

## Key Results
- R2T-BiLSTM trained without labeled data achieved 98.2% accuracy on Zarma POS tagging
- R2T-Transformer pre-trained with rules and fine-tuned on only 50 labeled sentences outperformed a baseline trained on 300 for Zarma NER
- Rule creation (~4 hours for Zarma) was faster than annotation of 300 gold sentences (~9-12 hours)

## Why This Works (Mechanism)

### Mechanism 1: Direct Rule-to-Gradient Integration
Embedding linguistic rules as differentiable loss components enables learning without labeled data. The multi-tiered rule system translates into four loss terms that directly shape gradients during backpropagation, making rules the primary learning signal rather than auxiliary constraints.

### Mechanism 2: Uncertainty Calibration for Out-of-Vocabulary Words
The adaptive OOV loss prevents confident incorrect predictions on unseen words by penalizing deviation from uniform distributions via KL divergence, forcing flat probability distributions on unknown words while maintaining confidence on rule-covered tokens.

### Mechanism 3: Architecture-Loss Interaction
BiLSTM's recurrent processing maintains strong local signals that align with per-token loss terms, while Transformer's global self-attention distributes signal across positions, diluting rule adherence but creating better representations for later fine-tuning.

## Foundational Learning

- **Concept: Sequence Tagging with Structured Output**
  - Why needed: R2T outputs per-token tag probabilities; understanding BIO tagging, CRF-style sequence constraints, and tag ambiguity is prerequisite
  - Quick check: Can you explain why "DET NOUN" is a valid tag sequence but "DET VERB" may be invalid?

- **Concept: Multi-Objective Loss Functions**
  - Why needed: R2T combines four weighted loss terms; understanding gradient interactions and hyperparameter balancing is essential
  - Quick check: If α dominates (lexical loss weight), what happens to the model's behavior on ambiguous words?

- **Concept: Linguistic Rule Formalization**
  - Why needed: The four-tier system requires basic linguistics knowledge to construct
  - Quick check: Given a new language, how would you distinguish Tier 1 from Tier 2 words?

## Architecture Onboarding

- **Component map:** FastText embeddings (300d) + Char-BiLSTM embeddings (50d) → 350d → BiLSTM (256d × 2) OR Transformer (768d, 10 layers, 6 heads) → Linear → Softmax → Tag probabilities

- **Critical path:**
  1. Prepare unlabeled corpus (tokenized sentences)
  2. Train FastText embeddings on corpus
  3. Create rule JSON files (lexicons, morphological patterns, syntactic constraints)
  4. Run unsupervised R2T training (30 epochs)
  5. Optionally fine-tune on small labeled set

- **Design tradeoffs:**
  - BiLSTM: Better unsupervised rule adherence, faster training, lower capacity
  - Transformer: Worse unsupervised, stronger after fine-tuning, higher compute cost
  - Rule coverage vs. annotation effort: ~4 hours for Zarma rules vs. ~9-12 hours for 300 gold sentences

- **Failure signatures:**
  - Systematic verb→noun misclassification: Transformer diluting token-level rules (add more Tier 1 entries)
  - Catastrophic tokenizer mismatch: XLM-RoBERTa-level failure (use word-level tokenization)
  - Over-confident OOV predictions: δ weight too low
  - All predictions same tag: γ (distributional loss) too low

- **First 3 experiments:**
  1. Replicate R2T-BiLSTM on provided Zarma data with default hyperparameters (α=0.85, β=0.08, γ=0.02, δ=0.05)
  2. Ablate one loss component at a time to understand contribution
  3. Test on new language with borrowed rules (start with Bambara rules from paper appendix)

## Open Questions the Paper Calls Out
None

## Limitations
- Rule quality assumption is critical but unexamined - errors in rules propagate directly into model predictions without ground truth to correct them
- OOV handling strategy (forcing uniform distributions) may be inappropriate for languages with large tag inventories
- Framework requires substantial linguistic expertise to create comprehensive multi-tiered rule systems

## Confidence
- **High confidence**: Core architecture and training procedure are clearly specified and reproducible
- **Medium confidence**: Empirical results on Zarma are promising but limited to one language pair
- **Low confidence**: Theoretical claims about architectural differences are speculative and lack rigorous analysis

## Next Checks
1. Systematically degrade rule quality and measure impact on model performance to test rule accuracy assumptions
2. Apply the exact same R2T framework to a different low-resource language with different linguistic properties
3. Replace the uniform distribution assumption with language-specific priors for OOV words and measure accuracy improvements