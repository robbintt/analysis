---
ver: rpa2
title: 'From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer
  Fusion'
arxiv_id: '2411.14507'
source_url: https://arxiv.org/abs/2411.14507
tags:
- fusegpt
- fusion
- block
- blocks
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FuseGPT introduces a prune-and-fuse compression paradigm for large
  language models, reframing structured pruning as iterative knowledge redistribution
  rather than simple removal. The method employs a dynamic Macro Influence metric
  to re-evaluate block redundancy as the network topology evolves and uses learnable
  low-rank fusion to adaptively graft pruned block knowledge onto surviving layers
  via lightweight local distillation.
---

# From Pruning to Grafting: Dynamic Knowledge Redistribution via Learnable Layer Fusion

## Quick Facts
- arXiv ID: 2411.14507
- Source URL: https://arxiv.org/abs/2411.14507
- Reference count: 20
- Achieves lower perplexity than prior methods at 25% sparsity, improving zero-shot reasoning by up to 4.5 points and delivering 1.33× inference speedup with 25% memory reduction.

## Executive Summary
FuseGPT reframes structured pruning as iterative knowledge redistribution rather than simple removal. The method employs a dynamic Macro Influence metric to re-evaluate block redundancy as the network topology evolves and uses learnable low-rank fusion to adaptively graft pruned block knowledge onto surviving layers via lightweight local distillation. Experimental results on LLaMA, Mistral, Qwen, and Phi families show that FuseGPT establishes a new state-of-the-art on the compression-accuracy Pareto frontier, achieving lower perplexity than prior methods at higher sparsity levels.

## Method Summary
FuseGPT introduces a prune-and-fuse compression paradigm that iteratively removes transformer blocks and redistributes their knowledge to neighbors. The method uses Macro Influence (MI) to dynamically re-evaluate block importance after each removal by measuring the cosine similarity between final hidden states before and after block removal. The pruned block's knowledge is then grafted to surviving neighbors within a fusion window using learnable low-rank fusion (C = AB^T), followed by local distillation to heal removal artifacts. The process repeats until reaching target sparsity, requiring only 1024 calibration samples and under one GPU-hour.

## Key Results
- At 25% sparsity, FuseGPT achieves lower perplexity than prior methods at 20% sparsity on multiple model families
- Improves zero-shot reasoning by up to 4.5 points compared to state-of-the-art compression methods
- Delivers 1.33× inference speedup with 25% memory reduction while maintaining quality
- Achieves 52.1% total compression with negligible quality loss when combined with 4-bit GPTQ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block importance is topology-dependent and must be re-evaluated after each removal
- Mechanism: Macro Influence (MI) computes the cosine similarity between final hidden states before and after removing each block. Lower MI indicates higher "absorbability" — the block's function can be implicitly reconstructed by remaining layers. This is recomputed iteratively rather than once.
- Core assumption: The paper assumes that hidden state perturbation at the final layer correlates with downstream task degradation. This is plausible but not proven for all tasks.
- Evidence anchors:
  - [abstract] "block importance fluctuates dynamically during pruning"
  - [Section 3, Figure 2(b)] Spearman correlation ρ=0.58 between pre- and post-removal rankings on LLaMA-2-7B; blocks shifted >10 ranks
  - [corpus] Weak corpus corroboration. Neighboring papers on structured pruning do not explicitly address dynamic re-ranking.

### Mechanism 2
- Claim: Linear weight averaging causes feature collapse because adjacent blocks occupy different functional manifolds
- Mechanism: Learnable low-rank fusion injects pruned-block knowledge via coefficient matrices C = AB^T (rank r≪min(d_out, d_in)). The fused weight becomes W_fused = W_neighbor + (AB^T) ⊙ W_pruned. A is zero-initialized so fusion starts as identity, then learns selective knowledge transfer through local distillation.
- Core assumption: Low-rank structure is sufficient to capture the transferable knowledge between blocks. This assumes redundancy is structured and compressible.
- Evidence anchors:
  - [Section 3, Figure 2(a)] Pearson r=0.23 between activation similarity and fusion MSE; high-similarity pairs (>0.85) still showed MSE >0.3
  - [Section 4.3] "Destructive Interference: even when two blocks produce similar activations, their weight matrices may lie on different functional manifolds"

### Mechanism 3
- Claim: Local distillation within a fusion window is sufficient to heal removal artifacts without full-model fine-tuning
- Mechanism: After fusing block B_p into neighbors within window G (default 7 blocks), the paper applies KL divergence loss between original and fused block outputs: L_distill = KL(σ(H_orig/τ) || σ(H_fused/τ)). Only parameters in the fusion window are updated via LoRA (rank 128).
- Core assumption: Local errors do not propagate significantly to distant layers; residual connections and layer-norm contain the damage locally.
- Evidence anchors:
  - [abstract] "lightweight local distillation"
  - [Section 5.1] "use 32 samples for calibration and 1024 samples for fine-tuning, which is extremely lightweight"
  - [Table 9] "Adding LoRA fine-tuning alone: 15.06 C4 PPL; +MI selection: 13.34; +learnable fusion: 11.58; +iterative: 11.17"

## Foundational Learning

- **Concept: Transformer Block Composition**
  - Why needed here: FuseGPT removes entire blocks and grafts knowledge to neighbors. You must understand residual connections (H_i = H_{i-1} + Block(H_{i-1})), layer normalization, and why depth matters for representation quality.
  - Quick check question: If you remove block B_5 and directly connect B_4 to B_6, what happens to the residual stream dimension and the expected activation distribution?

- **Concept: Low-Rank Matrix Factorization (LoRA-style)**
  - Why needed here: The fusion mechanism uses C = AB^T with r≪d. This is structurally identical to LoRA adapters but applied to weight injection rather than weight modification.
  - Quick check question: For a weight matrix W ∈ R^{4096×4096}, if r=128, how many parameters does AB^T require compared to full C? What is the compression ratio?

- **Concept: Knowledge Distillation with Soft Targets**
  - Why needed here: Local distillation uses KL divergence on softmaxed hidden states with temperature τ. You need to understand why soft targets preserve more information than hard labels.
  - Quick check question: Why does temperature τ > 1 help in distillation? What happens to gradient magnitude as τ increases?

## Architecture Onboarding

- **Component map:**
  [Calibration Data] → [MI Scoring] → [Block Selection (argmin MI)]
         ↓
  [Fusion Window (G=7)] ← [Pruned Block B_p]
         ↓
  [Low-Rank Fusion: A (zeros), B (Kaiming init)]
         ↓
  [Local Distillation: KL loss on hidden states]
         ↓
  [Weight Folding: W ← W + (AB^T)⊙W_p]
         ↓
  [Remove B_p, repeat until target sparsity]

- **Critical path:**
  1. MI computation is the gating factor — it requires one forward pass per block through the full model on calibration data. For N blocks and |D_cal| samples, this is O(N × |D_cal| × forward_cost).
  2. Local distillation is the compute bottleneck — default is 1024 samples, batch size 8, with LoRA updates on fusion window only.
  3. Weight folding is pure tensor arithmetic, trivially fast.

- **Design tradeoffs:**
  - **Fusion window G:** Larger G (more neighbors) increases recovery capacity but also parameter updates and distillation cost. Paper uses G=7 (~25% of 7B model params). No ablation provided.
  - **Low-rank r:** Higher r increases fusion capacity but risks overfitting on small calibration data. r=128 is a middle ground; r=64 or r=256 may be worth testing.
  - **Calibration samples:** 32 for MI, 1024 for distillation. Fewer samples → faster but potentially unstable MI rankings.
  - **Iterative vs. one-shot:** Iterative handles rank shift but costs ~3-4× more GPU time (45 min vs. 12-18 min for baselines in Table 7).

- **Failure signatures:**
  - **Perplexity spike after fusion:** Check if fusion window G is too small or low-rank r is insufficient. Symptom: C4 PPL diverges significantly from WikiText-2 PPL.
  - **MI scores flat across iterations:** May indicate calibration data is too small or unrepresentative. Try increasing from 32 to 128 samples.
  - **Zero-shot accuracy collapse on specific tasks:** MI optimizes for representation similarity, not task-specific heads. Some tasks (e.g., ARC-c) may be more sensitive to specific block removal.

- **First 3 experiments:**
  1. **Reproduce MI rank shift on your model:** Compute MI for all blocks, remove the lowest, re-compute MI, measure Spearman correlation. If ρ > 0.85, dynamic re-evaluation may not be necessary for your architecture.
  2. **Ablate fusion window G:** Test G ∈ {3, 5, 7, 9} at fixed 25% sparsity. Plot C4 PPL and zero-shot avg accuracy vs. G. Identify elbow point.
  3. **Stress test with quantization:** Apply FuseGPT at 25% sparsity, then GPTQ-4bit. Measure if perplexity degradation is multiplicative (independent) or super-additive (interacting). This validates orthogonality claim for your deployment stack.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the fusion mechanism be extended beyond local neighborhoods to enable knowledge grafting from pruned blocks to non-adjacent, semantically related blocks?
- **Open Question 2:** Can more adaptive methods for integrating fused weights improve performance over the current low-rank coefficient approach?
- **Open Question 3:** How does FuseGPT scale to models beyond 13B parameters, and can the iterative prune-and-fuse process be made more efficient for very high sparsity targets?
- **Open Question 4:** What is the theoretical relationship between Macro Influence scores and optimal fusion configurations, and can this inform better block selection without iterative re-computation?

## Limitations
- Iterative process is computationally intensive, especially for very large models or when targeting high sparsity levels
- Experiments only cover models up to 13B parameters, limiting scalability analysis
- Calibration and distillation sample sizes are fixed without systematic ablation studies

## Confidence

**High Confidence Claims:**
- FuseGPT achieves state-of-the-art compression-accuracy Pareto frontier on tested models
- Dynamic MI re-evaluation improves over static one-shot pruning (ρ=0.58 rank shift correlation)
- Learnable fusion with low-rank coefficients outperforms linear averaging (activation similarity r=0.23 vs fusion MSE)

**Medium Confidence Claims:**
- Local distillation is sufficient for healing removal artifacts (requires more extensive ablation on window size and rank)
- Orthogonal combination with quantization maintains quality (based on single 4-bit GPTQ experiment)
- 25% sparsity with 1.33× speedup is practical deployment threshold (not extensively tested across diverse workloads)

## Next Checks

1. **Validate MI rank shift:** Compute MI for all blocks on your target model, remove lowest-MI block, re-compute MI, and measure Spearman correlation. If ρ > 0.85, dynamic re-evaluation may not be necessary for your architecture.

2. **Test fusion window sensitivity:** Ablate fusion window size G ∈ {3, 5, 7, 9} at fixed 25% sparsity on your model. Plot perplexity and downstream task performance vs. G to identify optimal window size.

3. **Verify orthogonality with quantization:** Apply FuseGPT compression at 25% sparsity, then apply your quantization method (e.g., GPTQ-4bit). Measure if quality degradation is multiplicative (independent) or super-additive (interacting) to validate orthogonality claims for your deployment stack.