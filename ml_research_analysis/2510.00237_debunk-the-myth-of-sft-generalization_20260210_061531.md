---
ver: rpa2
title: Debunk the Myth of SFT Generalization
arxiv_id: '2510.00237'
source_url: https://arxiv.org/abs/2510.00237
tags:
- training
- arxiv
- data
- instruction
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the prevailing belief that supervised fine-tuning\
  \ (SFT) inherently memorizes training data and fails to generalize, while reinforcement\
  \ learning (RL) is more robust. The authors systematically evaluate SFT on two decision-making\
  \ benchmarks, Sokoban and General Points, and identify that much of SFT's perceived\
  \ generalization failure stems from \"frozen-prompt artifacts\" \u2014 when trained\
  \ on fixed instruction templates, SFT models overfit to training semantics rather\
  \ than adapting to new ones."
---

# Debunk the Myth of SFT Generalization

## Quick Facts
- arXiv ID: 2510.00237
- Source URL: https://arxiv.org/abs/2510.00237
- Authors: Xiaofeng Lin; Hejian Sang; Zhipeng Wang; Xuezhou Zhang
- Reference count: 40
- Primary result: Prompt diversity + chain-of-thought supervision enables SFT to match RL generalization on out-of-distribution instruction and difficulty variants.

## Executive Summary
This paper challenges the belief that supervised fine-tuning (SFT) inherently memorizes training data and fails to generalize, while reinforcement learning (RL) is more robust. Through systematic evaluation on Sokoban and General Points decision-making benchmarks, the authors identify that SFT's perceived generalization failure stems from "frozen-prompt artifacts" - overfitting to fixed instruction templates rather than learning to interpret instructions. The core insight is that introducing prompt diversity during training breaks this shortcut, yielding strong generalization to unseen instruction variants without harming in-distribution performance. Chain-of-thought supervision provides an algorithmic scaffold that markedly improves transfer to strictly harder tasks. The primary result is that combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines while retaining SFT's simplicity and stability.

## Method Summary
The study evaluates SFT generalization on Sokoban (multi-step puzzle) and General Points (arithmetic reasoning) using a systematic protocol that tests in-distribution, instruction-variant, difficulty-variant, and fake-environment performance. The authors implement prompt diversity by randomizing instruction components (action mappings, face-card values) during training and adding explicit mapping specifications to prompts. Chain-of-thought demonstrations are generated via RL-finetuned Qwen3-8B with rejection sampling. SFT training uses standard cross-entropy with AdamW, while RL baselines use GRPO. The key interventions are prompt diversity (breaking frozen-prompt shortcuts), CoT supervision (algorithmic scaffolding), and their combination.

## Key Results
- Prompt diversity improves instruction-variant performance (Sokoban Numerical: 0→0.89) while collapsing fake-environment performance (0.93→0), confirming it breaks frozen-prompt shortcuts
- Chain-of-thought supervision improves difficulty-variant performance across both tasks (Sokoban TwoBoxes: 0.35→0.57, General Points Large Numbers: 0.02→0.80)
- Combined Diversity + CoT achieves strong generalization matching or exceeding RL baselines while retaining SFT's stability and simplicity

## Why This Works (Mechanism)

### Mechanism 1: Frozen-Prompt Shortcut Breaking
When trained on fixed instruction templates, SFT models overfit to training semantics rather than adapting to new ones. This occurs because the model learns to ignore invariant prompt components and relies on memorized token-action mappings. The break condition is that if diversity improved variant performance but also maintained fake performance, the model would still be using shortcuts.

### Mechanism 2: Prompt Diversity as Distributional Coverage
Varying instruction components during training forces the model to treat instructions as input variables requiring interpretation. By sampling diverse action-word mappings and face-card interpretations per training example and explicitly specifying the mapping in each prompt, the model learns to attend to and apply the declared mapping rather than memorizing fixed associations.

### Mechanism 3: CoT as Algorithmic Scaffolding
Chain-of-thought supervision provides intermediate reasoning structure that enables transfer to harder task variants requiring longer-horizon planning or greater combinatorial depth. CoT demonstrations expose the reasoning process, allowing the model to internalize a generalizable procedure that scales to harder variants even though specific instances weren't seen during training.

## Foundational Learning

- **Concept: Cross-entropy loss and distributional drift**
  - Why needed here: SFT optimizes likelihood of reference outputs, which can overfit to training distribution if data is narrow
  - Quick check: Can you explain why cross-entropy on narrow data might produce different generalization than RL with reward signals?

- **Concept: Instruction following vs. task competence**
  - Why needed here: The paper disentangles these via validity metrics - a model may solve tasks correctly using wrong vocabulary
  - Quick check: If a model outputs "up" when instructed "1=up, 2=down...", does it lack task competence, instruction following, or both?

- **Concept: Procedural vs. instance learning**
  - Why needed here: CoT transfers to harder variants only if the model learns a procedure rather than memorizing instance-specific patterns
  - Quick check: Why would answer-only supervision fail to transfer to five-card compositions when four-card CoT succeeds?

## Architecture Onboarding

- **Component map:** BFS solver (Sokoban) / brute-force search (General Points) → state-action pairs or equation-formula pairs → Prompt constructor (templates with parameterized instruction components) → Diversity sampler (randomizes instruction components) → CoT generator (RL-finetuned Qwen3-8B with rejection sampling) → SFT trainer (standard cross-entropy) → Evaluator (multi-split protocol)

- **Critical path:** Implement diversity sampler before any SFT training - without this, you'll reproduce the "SFT memorizes" failure mode. If targeting difficulty generalization, generate CoT traces via rejection sampling from a capable model. Train with combined Diversity + CoT data; monitor both in-distribution and variant splits. Validate by checking that Fake performance collapses (confirming diversity worked) while difficulty-variant performance improves (confirming CoT worked).

- **Design tradeoffs:** Prompt diversity alone fixes instruction variants but insufficient for difficulty scaling. CoT alone helps difficulty but leaves instruction-variant gaps and may retain fake success. Proximity control improves instruction validity but limits task-specific adaptation and degrades difficulty generalization. RL warm-start is competitive but unstable, compute-intensive, and exploration-dependent.

- **Failure signatures:** High fake performance with low variant performance indicates frozen-prompt shortcut active. Strong instruction variants but poor difficulty variants means missing CoT scaffolding. Strong difficulty variants but poor instruction variants indicates CoT without diversity. Both variant types fail suggests likely data quality or base model capacity issue.

- **First 3 experiments:** 1) Train answer-only SFT without diversity to verify that fake performance tracks ID performance while instruction variants fail. 2) Add prompt diversity to answer-only SFT to confirm fake collapses and instruction variants improve while difficulty variants remain weak. 3) Add CoT to diversity training to verify instruction-variant and difficulty-variant performance both improve, matching or exceeding RL baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do prompt diversity and chain-of-thought supervision improve SFT generalization in open-ended or creative generation tasks?
- Basis: The authors state that "evaluation is limited to decision-making tasks, and further investigation is required to assess whether similar trends hold in more open-ended or creative generation settings."
- Why unresolved: The study restricted its scope to logic and planning benchmarks with deterministic ground truths.
- What evidence would resolve it: Applying the Diversity + CoT recipe to creative writing or dialogue benchmarks to test if instruction robustness transfers to semantic diversity.

### Open Question 2
- Question: Does the "Diversity + CoT" recipe transfer robustly to multi-modal tasks and longer-horizon interactive agents?
- Basis: The limitations section notes that "broader validation is warranted across modalities, longer-horizon interactive settings, and a wider range of base models."
- Why unresolved: The current experiments focus on single-modal text decision-making with relatively constrained horizons.
- What evidence would resolve it: Evaluating the method on vision-language benchmarks or multi-turn agentic workflows where visual tokens or extended context introduce new generalization pressures.

### Open Question 3
- Question: Can algorithmic modifications (e.g., objective shaping, proximal regularization) match the generalization performance of data-centric interventions without requiring massive data augmentation?
- Basis: The conclusion suggests future work should explore strengthening "SFT itself without data augmentation (e.g., objective shaping...)" and developing "compute-efficient hybrids."
- Why unresolved: This study focused on curating better demonstrations (data-centric) rather than modifying the loss function or optimization dynamics (algorithmic).
- What evidence would resolve it: Ablation studies comparing data-diverse vanilla SFT against algorithmically modified narrow-data SFT (e.g., using GEM or PPO-like clipping) on the same out-of-distribution variants.

## Limitations
- Conclusions drawn from only two domains (Sokoban and General Points) may not generalize to other task types
- Diversity intervention relies on synthetic random remapping that may not capture real-world instruction variation
- CoT demonstrations generated by RL-finetuned model introduce potential bias in reasoning patterns
- Study does not address catastrophic forgetting or scalability to larger models or more complex tasks

## Confidence
- **High confidence:** Prompt diversity breaking frozen-prompt shortcuts (strong quantitative evidence across both tasks with clear counterfactuals via Fake environment)
- **Medium confidence:** CoT scaffolding for difficulty generalization (consistent improvements but limited to two domains, mechanism is plausible but not definitively proven)
- **Medium confidence:** Combined Diversity + CoT superiority over RL (empirical but benchmark-limited, RL instability not fully characterized)

## Next Checks
1. **Domain Transfer Test:** Apply Diversity + CoT SFT to a third, structurally different domain (e.g., text-based game with inventory management) to verify the pattern holds beyond Sokoban and arithmetic reasoning.

2. **Natural Instruction Variation:** Replace synthetic random mappings with naturally occurring instruction variations (e.g., synonyms, paraphrasing) to test whether diversity effects persist under more realistic conditions.

3. **Architecture Ablation:** Repeat the main experiments with a smaller model (e.g., 3B parameters) and a larger one (e.g., 70B parameters) to assess whether the proposed interventions scale with model capacity or are specific to the 8B models used here.