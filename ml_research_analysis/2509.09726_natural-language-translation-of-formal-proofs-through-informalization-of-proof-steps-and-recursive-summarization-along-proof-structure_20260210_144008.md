---
ver: rpa2
title: Natural Language Translation of Formal Proofs through Informalization of Proof
  Steps and Recursive Summarization along Proof Structure
arxiv_id: '2509.09726'
source_url: https://arxiv.org/abs/2509.09726
tags:
- proof
- formal
- proofs
- natural
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for translating formal proofs into
  natural language proofs using large language models (LLMs) through step-wise informalization
  and recursive summarization. The method addresses the challenge of limited training
  data for autoformalization by combining rule-based templates with LLM-based slot-filling
  for each proof step, and recursively summarizing the results based on the proof
  structure.
---

# Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure

## Quick Facts
- arXiv ID: 2509.09726
- Source URL: https://arxiv.org/abs/2509.09726
- Reference count: 28
- Primary result: Method translates Lean 4 formal proofs to natural language with 86% accuracy on key proof points using template-constrained LLM slot-filling and recursive summarization

## Executive Summary
This paper presents a method for translating formal proofs from Lean 4 into natural language proofs using large language models (LLMs). The approach addresses the challenge of limited training data for autoformalization by combining rule-based templates with LLM-based slot-filling for each proof step, and recursively summarizing the results based on the proof structure. When evaluated on 17 university-level formal proofs, the method achieved 86% accuracy in capturing key proof points and successfully generated readable proofs from Mathlib's formal proofs without corresponding natural language versions.

## Method Summary
The method consists of a five-stage pipeline: (1) offline premise library generation that extracts and orders mathematical theorems/definitions by dependency level, then generates natural language explanations via LLM, (2) LeanDojo-extended extraction of tactic information, premise information, and AST dependencies from Lean 4/Mathlib4 proofs, (3) template-based slot-filling with GPT-4.1-mini where pre-defined natural language templates are filled by the LLM for each proof step, (4) dependency tree construction from AST to capture proof structure, and (5) recursive summarization along the tree structure where summaries are progressively synthesized from leaf nodes to the root using GPT-4.1-mini.

## Key Results
- Step-wise informalization accuracy of 89.05% when using templates with premise library
- Accuracy drops to 53.6% without templates, demonstrating their critical importance
- Summarization achieves 86% accuracy in capturing key proof points
- 6% improvement in accuracy from using premise library, primarily through reduced misinformation
- Successfully generated readable natural language proofs from Mathlib formal proofs lacking natural language counterparts

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Constrained generation via templates significantly reduces hallucination and omission errors compared to free-form LLM prompting.
**Mechanism:** The system selects pre-defined natural language templates based on Lean tactic types (e.g., `rw` vs. `exact`), restricting the LLM to filling specific slots like `[goalsBefore]` and `[theorems]` rather than generating entire sentences.
**Core assumption:** The logical intent of a formal tactic can be captured by a finite set of natural language sentence structures.
**Evidence anchors:** Section 4.3 describes template preparation per tactic; Table 1 shows accuracy dropping from ~89% to ~54% without templates.
**Break condition:** If a formal proof uses a tactic variant not covered by the manually curated template library, retrieval will fail or select a suboptimal template.

### Mechanism 2
**Claim:** Pre-emptive retrieval of natural language definitions (Premise Library) enables correct resolution of opaque formal identifiers.
**Mechanism:** The system queries a premise library containing human-readable descriptions of formal theorem labels and injects this context into the prompt, allowing the LLM to use provided semantic meaning rather than guessing.
**Core assumption:** The LLM cannot reliably translate formal identifiers without external context but can accurately integrate provided context.
**Evidence anchors:** Section 4.1 describes generating explanatory sentences for theorems/definitions; Section 5.2 reports ~6% accuracy improvement from premise library use.
**Break condition:** If the premise library is incomplete or dependency analysis misses a relevant theorem, the LLM may hallucinate meaning for the formal label.

### Mechanism 3
**Claim:** Recursive summarization aligned with proof structure preserves logical coherence better than end-to-end summarization.
**Mechanism:** The system builds a tree from the proof's AST and summarizes leaf nodes first, recursively feeding summaries into parent nodes, limiting context window size and mimicking hierarchical human proof structure.
**Core assumption:** The logical dependency graph of a formal proof maps effectively to the narrative structure of a natural language proof.
**Evidence anchors:** Section 4.5 describes progressive summarization reflecting proof structure; Section 5.3 notes 4/17 proofs contained reasoning absent from original without recursive summarization.
**Break condition:** If the proof structure is extremely flat or excessively deep, recursive summarization may over-compress details or lose the intended "sieve" effect.

## Foundational Learning

**Concept:** Tactics vs. Terms in Lean
**Why needed here:** The method relies on "tactic information" to drive templates; you must distinguish between what is being proved (term/type) and how it is manipulated (tactic).
**Quick check question:** In the command `rw [h1, h2]`, is `rw` the tactic or the term? (Answer: `rw` is the tactic; `h1`/`h2` are hypothesis terms).

**Concept:** Proof State (Hypotheses âŠ¢ Goal)
**Why needed here:** Templates have slots for `[goalsBefore]` and `[goalsAfter]`; understanding that a proof step transforms a state is required to design or debug templates.
**Quick check question:** If a tactic succeeds, what happens to the goal in the proof state? (Answer: It is either transformed into a new sub-goal or discharged/completed).

**Concept:** Abstract Syntax Tree (AST)
**Why needed here:** Dependency Analysis relies on AST to determine which steps belong to which sub-proof (e.g., a `have` block); without understanding tree structures, recursive summarization logic is opaque.
**Quick check question:** In the dependency tree, why are steps proving an intermediate lemma children of the step that declared that lemma? (Answer: Because the declaration creates a dependency that subsequent steps fulfill).

## Architecture Onboarding

**Component map:** Data Extractor -> Premise Library -> Informalizer -> Structure Analyzer -> Recursive Summarizer

**Critical path:** Template Matching (in Informalizer) and AST Parsing (in Structure Analyzer). If template matching fails, LLM gets unstructured input; if AST parsing fails, summarization order is wrong.

**Design tradeoffs:** Rule-based Templates vs. End-to-End LLM trades flexibility and maintenance cost (manually writing templates) for high precision and format control. Temperature settings differ (0.4 for translation, 1.0 for summarization) to balance precision and fluency.

**Failure signatures:** Untranslated Expressions (raw Lean syntax like `sInf`) indicates Premise Library lookup failure; Misinformation (text claims rewrite that didn't happen) indicates template-slot mismatch or LLM hallucination; Logical Deviation (summary contradicts formal proof) indicates recursive summarization failure due to context overload or "sieve" failure.

**First 3 experiments:**
1. Verify Extraction Fidelity: Run LeanDojo extension on 10 Mathlib proofs; manually check if `goalsBefore` and `goalsAfter` are correctly extracted for tactics like `rw` and `simp`.
2. Ablate Templates: Take 5 proof steps; run Informalizer with templates disabled; compare slot-filling accuracy and "Unnecessary Mention" rate against paper's Table 1 results.
3. Recursion Depth Test: Generate proof using recursive vs. flat concatenation summarization for a proof with deeply nested `have` structure; evaluate if flat version loses the main argument thread.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the process of creating templates and few-shot examples for tactic informalization be automated?
**Basis in paper:** [explicit] Appendix A.1 states identifying all possible tactic operations and manually creating examples is "extremely time-consuming" and suggests implementing tools or utilizing LLMs for automatic generation.
**Why unresolved:** Current method relies on manual curation, limiting scalability and range of applicable formal proofs.
**What evidence would resolve it:** An automated pipeline that generates accurate templates and few-shot examples for unseen tactics without human intervention.

**Open Question 2:** How can the summarization method be improved to filter trivial details in structurally simple proofs?
**Basis in paper:** [explicit] Appendix A.2 notes simple proofs containing trivial operations often output redundant details because recursive "sieving" mechanism depends on proof complexity.
**Why unresolved:** Current recursive algorithm does not explicitly mark important information, causing it to retain trivial steps when proof tree is shallow.
**What evidence would resolve it:** A method that explicitly marks essential reference expressions to be retained during summarization, validated on short proofs.

**Open Question 3:** How can translation of specific formal notations (e.g., `sInf`) be improved to eliminate untranslated expressions?
**Basis in paper:** [inferred] Section 5.2 reports 0.4% of outputs contained untranslated formal expressions (e.g., `sInf`, sequence notation) due to LLM's lack of internal knowledge, a problem premise library didn't significantly fix.
**Why unresolved:** Templates and premise libraries failed to fully bridge gap for specific formal notations unfamiliar to base LLM.
**What evidence would resolve it:** Reduction of "Untranslated Expression" errors to near zero through enhanced notation handling or specialized training data.

## Limitations
- Manual template creation is time-consuming and limits scalability across all Mathlib tactics
- Premise library effectiveness is limited for specific formal notations unfamiliar to the LLM
- Evaluation is restricted to 17 proofs from a single "Calculus I" textbook, raising generalization concerns
- Recursive summarization's "sieve" effect depends on proof complexity and may fail on simple proofs

## Confidence
- **High Confidence:** Template-constrained generation reducing hallucination is well-supported by ablation results (accuracy drop from ~89% to ~54% without templates)
- **Medium Confidence:** Premise library effectiveness demonstrated but implementation details sparse; 6% accuracy improvement claim relies on qualitative assessment
- **Low Confidence:** Recursive summarization preserving logical coherence supported by anecdotal evidence (4/17 failures without recursion) but lacks quantitative comparison of narrative quality or formal accuracy metrics

## Next Checks
1. **Template Library Completeness Audit:** Implement template retrieval algorithm and systematically test against all tactic types in representative Mathlib proofs to identify coverage gaps
2. **Cross-Domain Performance Evaluation:** Apply method to proofs from different mathematical domains (e.g., group theory, topology) and measure accuracy degradation patterns
3. **Ablation of Premise Library:** Run pipeline on proofs with known premise library gaps to quantify impact on "Misinformation" and "Untranslated Expressions" rates