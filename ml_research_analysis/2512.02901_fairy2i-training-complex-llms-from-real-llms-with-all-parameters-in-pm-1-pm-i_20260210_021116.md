---
ver: rpa2
title: 'Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\{\pm
  1, \pm i\}$'
arxiv_id: '2512.02901'
source_url: https://arxiv.org/abs/2512.02901
tags:
- quantization
- arxiv
- complex
- real
- real-valued
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fairy2i, a universal framework that enables
  the transformation of pre-trained real-valued large language models (LLMs) into
  complex-valued representations, allowing for extreme low-bit quantization while
  reusing existing checkpoints. The core innovation lies in a mathematical reparameterization
  that expresses any real-valued linear layer as an equivalent widely-linear complex
  form, preserving exact equivalence in forward computation.
---

# Fairy2i: Training Complex LLMs from Real LLMs with All Parameters in $\{\pm 1, \pm i\}$

## Quick Facts
- arXiv ID: 2512.02901
- Source URL: https://arxiv.org/abs/2512.02901
- Reference count: 40
- Achieves 7.85 perplexity on C4 with effective 2-bit quantization of LLaMA-2 7B

## Executive Summary
Fairy2i introduces a universal framework that transforms pre-trained real-valued large language models into complex-valued representations, enabling extreme low-bit quantization while reusing existing checkpoints. The core innovation lies in a mathematical reparameterization that expresses any real-valued linear layer as an equivalent widely-linear complex form, preserving exact equivalence in forward computation. Applied to LLaMA-2 7B, Fairy2i achieves a perplexity of 7.85 on C4 with an effective 2-bit precision, outperforming state-of-the-art real-valued binary and ternary quantization methods and approaching full-precision baseline performance.

## Method Summary
Fairy2i works by first converting each real-valued linear layer into an equivalent widely-linear complex form through a lossless mathematical transformation that preserves forward computation. The model is then quantized using a phase-aware scheme with the codebook $\{\pm 1, \pm i\}$, which fully utilizes the 2-bit representation budget. A recursive residual quantization mechanism iteratively quantizes remaining errors across multiple stages, with T=2 stages providing optimal accuracy-memory tradeoff. The framework maintains compatibility with existing real-valued models through checkpoint reuse, avoiding the need to train complex-valued models from scratch.

## Key Results
- Achieves 7.85 perplexity on C4 validation set with effective 2-bit quantization (W2)
- Outperforms state-of-the-art real-valued binary and ternary quantization methods
- Approaches full-precision baseline performance while maintaining compatibility with existing checkpoints
- T=2 recursive residual quantization provides optimal accuracy-memory tradeoff

## Why This Works (Mechanism)

### Mechanism 1: Widely-linear transformation (real-to-complex reparameterization)
Any real-valued linear layer can be losslessly converted to an equivalent complex-valued widely-linear form. A real matrix $R \in \mathbb{R}^{2n \times 2m}$ is partitioned into four blocks and decomposed into two complex matrices $U$ and $W$ via explicit formulas. The widely-linear map $y = Ux + \bar{W}x$ combines a complex-linear term and a conjugate-linear term, exactly reproducing the original real operation. This preserves forward computation before quantization.

### Mechanism 2: Phase-aware complex quantization with fourth roots of unity
The codebook $\{\pm 1, \pm i\}$ fully utilizes 2-bit encoding and provides better approximation than real ternary $\{+1, 0, -1\}$. Each complex weight is projected to the nearest codeword by phase angle. Two axis-wise scaling factors recover magnitude per tensor/group. This yields 4 discrete values (2 bits exactly) versus 3 for ternary (wastes encoding capacity).

### Mechanism 3: Recursive residual quantization
Iteratively quantizing the residual error substantially improves accuracy with minimal extra bit cost. After initial quantization $\hat{W}^{(0)}$, compute residual $R^{(1)} = W - \hat{W}^{(0)}$. Quantize $R^{(1)}$ to get $\hat{W}^{(1)}$. The deployed weight is $\sum_t \hat{W}^{(t)}$. Each stage adds 1 bit per parameter. T=2 is optimal; T=3 adds 50% storage for <5% PPL gain.

## Foundational Learning

- **Concept: Widely-linear algebra (augmented complex variables)**
  - Why needed here: Standard complex-linear maps $(y = Ux)$ cannot express all real linear transformations. Widely-linear form $y = Ux + \bar{W}x$ adds the conjugate-linear term needed for exact equivalence.
  - Quick check question: Why can't $y = Ux$ alone reproduce all real-valued linear maps?

- **Concept: Straight-through estimator (STE) for quantization-aware training**
  - Why needed here: Discrete quantization has zero gradients. STE treats the quantizer as identity in backward pass, allowing gradients to flow to full-precision master weights.
  - Quick check question: If $b(w)$ discretely projects to $\{\pm 1, \pm i\}$, how does $\partial L/\partial w$ reach the continuous master weight?

- **Concept: Codebook efficiency and bit utilization**
  - Why needed here: Real ternary $\{+1, 0, -1\}$ has 3 values but 2-bit encoding can represent 4. Complex $\{\pm 1, \pm i\}$ uses all 4 slots, eliminating wasted capacity.
  - Quick check question: What is the encoding efficiency of 3 values in a 2-bit representation versus 4 values?

## Architecture Onboarding

- **Component map:** Input transformation (stack real vectors) -> Parameter transformation (R to U, W) -> Phase-aware quantizer (angle projection + scaling) -> Recursive residual module (T-stage correction) -> Inference kernel (multiplication-free add/subtract)

- **Critical path:** Load pretrained real checkpoint -> Transform all linear layers to widely-linear form -> Initialize QAT with FP masters -> Forward pass uses quantized weights -> Train with WSD schedule on ~30B tokens -> Deploy with T=2 (2 effective bits)

- **Design tradeoffs:** T=1/2/3: T=2 optimal; T=3 adds 50% storage for <5% PPL gain. Per-tensor vs per-group scaling: Paper uses per-tensor; finer groups may help but increase metadata. Parallel inference: T stages are data-independent; latency is O(max_t) not O(T).

- **Failure signatures:** Odd dimensions without padding -> shape errors. Using full complex inner product for attention scores (must use Re(q^H k) for real softmax inputs). Learning rate too high or insufficient decay -> training instability at extreme low bits. Residual not shrinking across stages -> codebook mismatch to weight distribution.

- **First 3 experiments:** 1) Equivalence verification: Transform small real model to widely-linear; confirm identical outputs before quantization. 2) T-stage ablation: Compare W1/W2/W3 on validation PPL to find accuracy-memory sweet spot. 3) Learning rate sweep: Test WSD schedules with varying peak LR (3e-5 range) and decay timing to establish stable training before scaling.

## Open Questions the Paper Calls Out

- Can extended training on larger datasets enable Fairy2i to surpass rather than merely match full-precision baseline performance? The authors hypothesize complex-valued representation has superior capacity yet to be fully exploited with current 30B token training limit.

- What is the optimal scaling granularity (per-tensor, per-group, per-channel) for phase-aware complex quantization, and how does it affect accuracy-efficiency tradeoff? Current work uses only per-tensor scaling, leaving investigation of various group strategies as future work.

- How does Fairy2i's performance and training stability scale to significantly larger models (70B+ parameters) and multimodal architectures? Authors plan to scale to LLaMA-3 70B and multimodal architectures to verify universality.

- What are the theoretical properties of the complex-valued loss landscape that confer robustness in extremely low-bit regimes? Further theoretical investigation could provide deeper insights into observed robustness.

## Limitations

- Even-dimension requirement: Transformation requires even-dimensional inputs/outputs, necessitating zero-padding for odd dimensions which may introduce computational overhead.
- Codebook alignment sensitivity: Performance depends on weight phase distributions aligning with cardinal directions; no analysis of phase distributions across different architectures.
- Recursive residual diminishing returns: Marginal gains from additional residual stages decrease rapidly (T=2â†’T=3 yields only 4.11% PPL reduction).
- Training stability at extreme quantization: WSD schedule is tuned specifically for this setup without systematic ablation of learning rate sensitivity.

## Confidence

**High confidence**: Mathematical equivalence between real linear layers and widely-linear complex representations (Theorem 1) is rigorously proven. 2-bit encoding efficiency claim is mathematically sound (4 values in 2 bits vs 3 values in 2 bits for ternary).

**Medium confidence**: Experimental results (7.85 PPL on C4, 62.00% zero-shot average) are well-documented and reproducible given same training setup. Performance relative to baselines depends on specific quantization implementations compared against.

**Low confidence**: Optimality of $\{\pm 1, \pm i\}$ codebook for general weight distributions is assumed rather than empirically validated across different model architectures. Claim that recursive residual quantization is superior to alternative 2-bit schemes lacks comparative analysis.

## Next Checks

1. **Phase distribution analysis**: Measure and visualize phase angle distributions of weights across different layers in quantized model. Quantify alignment with cardinal directions and test whether alternative phase-aligned codebooks yield better performance.

2. **T-stage scaling study**: Systematically evaluate T=1,2,3,4 on held-out validation set to precisely characterize diminishing returns curve. Determine whether optimal T varies by layer type or model scale, and test whether alternative codebook expansion strategies provide better accuracy-per-bit scaling.

3. **Odd-dimension padding impact**: Create test cases with odd input/output dimensions and measure exact computational overhead and accuracy degradation from zero-padding. Compare against alternative padding strategies to quantify practical cost of even-dimension requirement.