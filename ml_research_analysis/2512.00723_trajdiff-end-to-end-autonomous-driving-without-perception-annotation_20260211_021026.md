---
ver: rpa2
title: 'TrajDiff: End-to-end Autonomous Driving without Perception Annotation'
arxiv_id: '2512.00723'
source_url: https://arxiv.org/abs/2512.00723
tags:
- driving
- trajdiff
- trajectory
- end-to-end
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TrajDiff, an end-to-end autonomous driving
  framework that eliminates the need for perception annotations by leveraging trajectory-oriented
  BEV conditioned diffusion. The method constructs Gaussian BEV heatmap targets from
  future trajectories to enable self-supervised learning of effective BEV features,
  and introduces a Trajectory-oriented BEV Diffusion Transformer (TB-DiT) that generates
  diverse and plausible trajectories without handcrafted motion priors.
---

# TrajDiff: End-to-end Autonomous Driving without Perception Annotation

## Quick Facts
- arXiv ID: 2512.00723
- Source URL: https://arxiv.org/abs/2512.00723
- Authors: Xingtai Gui; Jianbo Zhao; Wencheng Han; Jikai Wang; Jiahao Gong; Feiyang Tan; Cheng-zhong Xu; Jianbing Shen
- Reference count: 40
- Primary result: Achieves 87.5 PDMS on NAVSIM, state-of-the-art among annotation-free methods

## Executive Summary
This paper proposes TrajDiff, an end-to-end autonomous driving framework that eliminates the need for perception annotations by leveraging trajectory-oriented BEV conditioned diffusion. The method constructs Gaussian BEV heatmap targets from future trajectories to enable self-supervised learning of effective BEV features, and introduces a Trajectory-oriented BEV Diffusion Transformer (TB-DiT) that generates diverse and plausible trajectories without handcrafted motion priors. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among annotation-free methods. With trajectory data scaling, it further improves to 88.5 PDMS, matching advanced perception-based approaches. The results demonstrate the framework's effectiveness in learning driving policies directly from raw sensor inputs while enabling scalability through trajectory augmentation.

## Method Summary
TrajDiff consists of two main components: a Trajectory-oriented BEV Encoder that predicts Gaussian BEV heatmaps from future trajectories, and a TB-DiT diffusion transformer that generates diverse trajectories conditioned on BEV features and ego state. The BEV encoder learns to predict heatmap targets constructed from future trajectory waypoints, providing self-supervision without perception labels. These predicted heatmaps are fused with raw BEV features to create TrajBEV representations that substitute for perception-supervised features. The TB-DiT then performs diffusion-based trajectory generation, adding Gaussian noise to continuous trajectories and learning to denoise via a transformer that conditions on timestep embeddings, ego query refined through Ego-BEV interaction, and TrajBEV features via cross-attention. The framework is trained on NAVSIM with focal loss for heatmap prediction and diffusion MSE loss for trajectory generation.

## Key Results
- Achieves 87.5 PDMS on NAVSIM, establishing state-of-the-art among annotation-free methods
- With trajectory data scaling (resampling + full dataset), improves to 88.5 PDMS, matching advanced perception-based approaches
- Best-of-K sampling demonstrates trajectory diversity benefits: K=1 yields 88.5 PDMS, K=10 yields 92.0 PDMS
- Velocity-adaptive Gaussian targets significantly outperform fixed radius (87.5 vs 85.7 PDMS)

## Why This Works (Mechanism)

### Mechanism 1
Gaussian BEV heatmaps constructed from future trajectories provide effective self-supervision for learning driving-relevant scene representations without perception labels. Future trajectory waypoints are converted into Gaussian distributions in BEV space with velocity-adaptive standard deviations. The network predicts a BEV heatmap supervised by this target via focal loss. This forces the encoder to learn features encoding drivable areas and motion patterns without explicit object or map labels.

### Mechanism 2
Fusing raw BEV features with predicted heatmap produces TrajBEV features that substitute for perception-supervised representations. Raw BEV features from sensor backbone are concatenated with the predicted BEV heatmap and processed through a lightweight fusion network. The heatmap provides an explicit driving probability field that complements raw visual features. Ablation shows removing Gaussian supervision degrades performance to 85.3 PDMS, confirming TrajBEV provides critical planning information.

### Mechanism 3
Diffusion-based trajectory generation conditioned on TrajBEV and ego state produces diverse, plausible trajectories without predefined anchors. TB-DiT performs forward diffusion by adding Gaussian noise to continuous trajectories, then learns to denoise via a transformer that conditions on timestep embedding, ego query refined through Ego-BEV interaction, and TrajBEV via cross-attention. This enables anchor-free generation unlike anchor-based approaches.

## Foundational Learning

- **Bird's Eye View (BEV) Representations**
  - Why needed here: TrajDiff operates entirely in BEV space—sensors are encoded to BEV features, heatmaps are predicted in BEV, and trajectories are planned in BEV coordinates.
  - Quick check question: Given a multi-camera setup, how would you project image features into a top-down BEV grid? What spatial resolution trade-offs exist?

- **Diffusion Models (DDPM/DDIM)**
  - Why needed here: TB-DiT implements diffusion for trajectory generation; understanding forward noising, noise prediction networks, and sampling is essential.
  - Quick check question: For a 1D trajectory signal, sketch how forward diffusion adds noise over T steps and how DDIM accelerates reverse sampling vs. DDPM.

- **Transformer Cross-Attention for Conditioning**
  - Why needed here: TB-DiT uses cross-attention between trajectory queries and TrajBEV features; ego query interacts with BEV via attention.
  - Quick check question: In cross-attention, if query dimension is N×d and key/value dimension is M×d, what is the output dimension? How does this enable conditioning trajectory generation on scene features?

## Architecture Onboarding

- **Component map:**
  Sensor Backbone + TransFuser Encoder -> Raw BEV features (H×W×C)
  Trajectory-oriented BEV Encoder -> Predicted heatmap -> Fused TrajBEV
  Ego Status MLP -> Ego features (1×C)
  Ego-BEV Interaction -> Conditioned ego query
  TB-DiT Blocks -> Predicted noise

- **Critical path:**
  1. Raw BEV must capture scene structure (backbone quality matters)
  2. Gaussian heatmap supervision must teach drivable area patterns
  3. TrajBEV fusion must integrate heatmap with raw features
  4. Ego-BEV interaction must inject scene context into ego query
  5. TB-DiT cross-attention must align trajectory queries with TrajBEV

- **Design tradeoffs:**
  - Gaussian radius: Fixed vs. velocity-adaptive (Table 4: velocity-aware wins by 1.8 PDMS over fixed-25m)
  - Diffusion steps: 3 steps → 38.1 PDMS; 20 steps → 88.5 PDMS; 40 steps → 88.5 PDMS (diminishing returns)
  - Data scaling: Trajectory resampling (+0.4 PDMS) + full dataset (+0.6 PDMS) = 1.0 PDMS total gain (Table 3)

- **Failure signatures:**
  - No Gaussian supervision (diffusion-only): 85.3 PDMS (Table 2)
  - No TrajBEV at all: 81.6 PDMS (Table 2, first row)
  - Fixed Gaussian radius (overspecified at 25m): 85.7 PDMS (Table 4)
  - Too few diffusion steps (3): 38.1 PDMS (Table 6)

- **First 3 experiments:**
  1. Overfit TrajBEV encoder on 10 scenarios: Verify predicted heatmaps localize ground-truth trajectory positions visually; confirm training converges.
  2. Ablate Gaussian radius schedule: Compare fixed radii (5m, 10m, 25m) vs. velocity-adaptive on validation split; reproduce Table 4 curve.
  3. Profile inference latency vs. sampling steps: Measure FPS at 3, 12, 20, 40 steps on single GPU; identify optimal operating point for real-time constraint (target >15 FPS).

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the results and methodology that warrant further investigation.

## Limitations
- Performance still lags behind advanced perception-based methods (87.5-88.5 vs 88.8 PDMS), suggesting annotation-free approaches face limitations in complex scenarios
- Heavy reliance on trajectory data quality—if trajectories frequently traverse non-drivable areas or contain significant noise, the Gaussian heatmap supervision could encode erroneous driving patterns
- Requires high-quality trajectory annotations for training, even though it eliminates perception annotations during inference

## Confidence
- **High confidence**: The core mechanism of using trajectory-derived Gaussian heatmaps for self-supervision is well-supported by ablation studies (85.7 → 87.5 PDMS improvement)
- **Medium confidence**: The assumption that trajectory data alone provides sufficient scene understanding for complex driving scenarios
- **Low confidence**: The scalability claims—while trajectory data scaling shows improvement, the magnitude of real-world trajectory datasets and their quality characteristics are unclear

## Next Checks
1. **Trajectory Quality Sensitivity Analysis**: Systematically inject noise and bias into training trajectories and measure degradation in PDMS. This validates the assumption that trajectories provide reliable self-supervision and identifies failure modes.

2. **Cross-Domain Generalization Test**: Evaluate TrajDiff on a different autonomous driving benchmark or real-world dataset to assess generalization beyond NAVSIM. This addresses concerns about simulation-to-reality transfer and dataset-specific limitations.

3. **Perception-Annotation Hybrid Evaluation**: Compare performance when using partial perception annotations (e.g., road boundaries only) versus pure trajectory supervision to quantify the value of annotation-free learning versus minimal supervision.