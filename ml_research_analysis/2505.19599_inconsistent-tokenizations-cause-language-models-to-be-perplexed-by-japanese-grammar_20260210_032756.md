---
ver: rpa2
title: Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese
  Grammar
arxiv_id: '2505.19599'
source_url: https://arxiv.org/abs/2505.19599
tags:
- psych
- predicate
- which
- japanese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether pretrained language models understand
  nuanced Japanese grammar rules by measuring perplexity on minimal pairs of grammatical
  and ungrammatical sentences involving the "first person psych predicate restriction."
  Weblab is the only tested model that consistently assigns lower perplexity to grammatical
  sentences than ungrammatical ones, which the authors attribute to its uniformly
  poor tokenization forcing consistent handling of Japanese text. When restricted
  to well-tokenized sentences, Llama 3's performance improves by an order of magnitude
  (28x improvement).
---

# Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar

## Quick Facts
- arXiv ID: 2505.19599
- Source URL: https://arxiv.org/abs/2505.19599
- Reference count: 5
- Key outcome: Weblab model outperforms others on Japanese psych predicate grammar due to consistent tokenization

## Executive Summary
This paper investigates whether pretrained language models understand nuanced Japanese grammar rules by measuring perplexity on minimal pairs of grammatical and ungrammatical sentences involving the "first person psych predicate restriction." The authors find that Weblab, which has uniformly poor tokenization, is the only tested model that consistently assigns lower perplexity to grammatical sentences than ungrammatical ones. This unexpected result is attributed to Weblab's consistent handling of Japanese text despite poor tokenization quality. When restricted to well-tokenized sentences, Llama 3's performance improves dramatically by an order of magnitude (28x improvement). The study also includes machine translation experiments that confirm models without consistent tokenizers struggle to produce grammatically correct Japanese sentences with psych predicates in third person.

## Method Summary
The researchers evaluated language models using perplexity scores on minimal pairs of grammatical and ungrammatical Japanese sentences involving psych predicates. They tested two main models: Weblab and Llama 3. The evaluation focused on the "first person psych predicate restriction" phenomenon in Japanese grammar. The authors conducted both language modeling tasks (measuring perplexity) and machine translation experiments to assess grammaticality. They also analyzed the impact of tokenization quality by comparing performance on well-tokenized versus poorly-tokenized sentences.

## Key Results
- Weblab consistently assigns lower perplexity to grammatical sentences than ungrammatical ones
- Llama 3 shows 28x improvement in performance when restricted to well-tokenized sentences
- Models without consistent tokenizers produce ungrammatical or semantically incorrect Japanese translations involving psych predicates

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how inconsistent tokenizations create ambiguity in language models' understanding of Japanese grammatical structures. When tokenizers handle Japanese text inconsistently, models cannot develop reliable patterns for grammatical constructions. Weblab's uniformly poor tokenization paradoxically provides consistency, allowing the model to learn consistent (if imperfect) patterns for Japanese grammar. In contrast, models with variable tokenization quality struggle to maintain coherent grammatical understanding across different sentence structures.

## Foundational Learning

### Perplexity in Language Modeling
- **Why needed**: Core metric for evaluating language model performance on grammatical understanding
- **Quick check**: Lower perplexity indicates better prediction of grammatical sentence structure

### Japanese Psych Predicate Restriction
- **Why needed**: Complex grammatical phenomenon testing nuanced language understanding
- **Quick check**: Rules governing how psychological predicates interact with subject pronouns

### Tokenization Consistency
- **Why needed**: Affects how models process and learn language patterns
- **Quick check**: Consistent tokenization enables reliable pattern learning across sentence structures

## Architecture Onboarding

### Component Map
Tokenization Layer -> Embedding Layer -> Transformer Blocks -> Output Layer

### Critical Path
The critical path for this research is: Tokenization Quality → Model Training → Perplexity Measurement → Grammaticality Assessment

### Design Tradeoffs
The study highlights a fundamental tradeoff between tokenization quality and consistency. While good tokenization is generally desirable, inconsistent good tokenization may be worse than consistently poor tokenization for learning complex grammatical patterns.

### Failure Signatures
Models with inconsistent tokenizers show high perplexity variance across grammatical contrasts and produce ungrammatical translations of psych predicates.

### First Experiments
1. Test additional Japanese grammatical phenomena beyond psych predicates
2. Compare models with identical architectures but different tokenization strategies
3. Evaluate cross-linguistic generalization of tokenization consistency benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on one Japanese grammatical phenomenon (first person psych predicate restriction)
- Limited to two pretrained language models (Weblab and Llama 3)
- Results may not generalize to other languages or grammatical constructions

## Confidence

**High**: Weblab's consistent perplexity patterns across grammatical contrasts
**Medium**: Attribution of performance differences primarily to tokenization inconsistency
**Medium**: Translation experiment results focusing on narrow grammatical construction

## Next Checks

1. Test additional language models beyond Weblab and Llama 3 to determine if observed patterns generalize across different architectures and training regimes

2. Conduct ablation studies that systematically vary tokenization strategies while keeping model architecture constant to isolate the effect of tokenization consistency

3. Expand evaluation to include multiple Japanese grammatical phenomena and cross-linguistic validation to assess whether tokenization consistency benefits extend beyond the specific psych predicate construction studied