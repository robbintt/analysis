---
ver: rpa2
title: Gaze-supported Large Language Model Framework for Bi-directional Human-Robot
  Interaction
arxiv_id: '2507.15729'
source_url: https://arxiv.org/abs/2507.15729
tags:
- robot
- task
- user
- interaction
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multimodal human-robot interaction (HRI) framework
  that integrates gaze tracking, speech recognition, and distributed vision perception
  with large language model (LLM) reasoning to support bi-directional communication
  in dynamic environments. The system fuses inputs from multiple cameras, a mobile
  eye tracker, and voice transcription to generate context-aware responses and task
  plans using Chain-of-Thought reasoning.
---

# Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction

## Quick Facts
- **arXiv ID:** 2507.15729
- **Source URL:** https://arxiv.org/abs/2507.15729
- **Reference count:** 30
- **Primary result:** Multimodal HRI framework integrating gaze tracking, speech recognition, and vision perception with LLM reasoning; lab experiment showed LLM condition received marginally higher confidence ratings (M=5.6 vs 4.4, p=0.039) but consumed more energy (1972 Wh vs 1784 Wh).

## Executive Summary
This paper presents a multimodal human-robot interaction framework that combines gaze tracking, speech recognition, and distributed vision perception with large language model reasoning to enable bi-directional communication in dynamic environments. The system fuses inputs from multiple cameras, a mobile eye tracker, and voice transcription to generate context-aware responses and task plans using Chain-of-Thought reasoning. The framework is designed to be modular and transferable across different robot platforms with minimal implementation effort.

In a lab experiment comparing the LLM-based system to a traditional scripted pipeline, both conditions performed similarly in task efficiency and user engagement for well-defined tasks. The LLM condition received marginally higher confidence ratings but consumed more energy. Qualitative feedback indicated participants appreciated the vocal instructions and pointing gestures, though some noted the LLM's tendency toward redundant responses. The study suggests scripted interactions may be preferable for straightforward tasks, while LLM-based systems offer greater adaptability for complex, multi-step scenarios requiring dynamic context understanding.

## Method Summary
The framework integrates multimodal perception through a distributed vision pipeline (YOLO-World for object detection, BLIP-v2 for scene captioning), speech recognition (Whisper), and gaze tracking (Tobii eye tracker) with large language model reasoning. The system uses an event-driven loop triggered by speech completion, converting sensory inputs into a structured Python dictionary that grounds the LLM's reasoning in the immediate physical context. Chain-of-Thought prompting constrains the LLM to generate executable action plans using predefined SDK function calls, which the action module executes via robot gestures and speech. The architecture emphasizes modularity, allowing transfer to different robot platforms with minimal effort.

## Key Results
- LLM condition received marginally higher confidence ratings (M=5.6 vs 4.4, p=0.039) compared to scripted pipeline
- LLM system consumed 10% more energy (1972 Wh vs 1784 Wh) than scripted condition
- Both conditions performed similarly in task efficiency and user engagement for well-defined tasks
- Qualitative feedback showed appreciation for vocal instructions and pointing gestures, with some noting LLM redundancy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing gaze and environmental data into a structured text representation grounds the LLM's reasoning in the immediate physical context.
- **Mechanism:** The system converts distributed vision inputs (robot cameras, BLIP-v2 scene captions, YOLO-World objects) and user gaze vectors into a unified Python dictionary. This text-based state representation forces the reasoning module to attend to specific objects and spatial relationships identified by the user's gaze, reducing ambiguity in voice commands.
- **Core assumption:** The object detection and gaze mapping modules accurately reflect the physical world state within the latency window required for the interaction.
- **Evidence anchors:**
  - [section III-B.1] "The data gets collected, processed, and condensed into a Python dictionary... includes the vision inputs from the moment the transcription is done."
  - [section III-A] "gaze naturally connects vocal instructions to the dynamic 3D environment."
  - [corpus] *FAM-HRI* (arXiv:2503.16492) supports the efficacy of combining gaze and speech to disambiguate user intent in HRI.
- **Break condition:** Significant latency in the vision pipeline or errors in coordinate transformation between the eye-tracker and world frame results in the dictionary describing an outdated or incorrect scene, causing the LLM to "hallucinate" context.

### Mechanism 2
- **Claim:** Structured Chain-of-Thought (CoT) prompting constrains the LLM to generate executable action plans rather than open-ended text.
- **Mechanism:** The reasoning module uses a six-part prompt structure that explicitly defines available SDK function calls (e.g., `activity.talker()`) and forces a "step-by-step" reasoning process. This bridges the gap between high-level natural language intent and low-level robot API execution by generating Python code that the action module can interpret directly.
- **Core assumption:** The LLM (GPT-4o-mini) consistently adheres to the output format and function constraints provided in the system prompt without requiring fine-tuning.
- **Evidence anchors:**
  - [section III-B.2] "Part 4... lists available SDK function calls... Part 6 defines the output structure... generating a chain-of-thought action plan... without triggering direct robot actions."
  - [abstract] "structured plans are provided using Chain-of-Thought reasoning."
  - [corpus] *FAM-HRI* notes the difficulty in mapping foundation models to specific robot actions without structured grounding.
- **Break condition:** The "non-deterministic behavior" of the LLM results in syntax errors or calls to non-existent functions, triggering the `activity.error()` state.

### Mechanism 3
- **Claim:** Bi-directional feedback via the Action Module closes the interaction loop, maintaining user engagement even during processing delays.
- **Mechanism:** The Action Module provides continuous state feedback (listening, thinking, success, error) via robot gestures or lights. By signaling internal states—such as executing a distinct "thinking" behavior during LLM inference—the system manages user expectations and maintains the illusion of attentiveness.
- **Core assumption:** The robot's expressive modalities (gestures/lights) are perceptible to the user and are correctly interpreted as specific system states.
- **Evidence anchors:**
  - [section III-B.3] "The current state of the system is clearly indicated... using designated function calls... activity.thinking() (that is called during internal computational processes)."
  - [abstract] "Bi-directional... communication between the robot and the user."
  - [corpus] *ERR@HRI 2.0* highlights the necessity of error detection and state signaling in LLM-driven interactions to maintain trust.
- **Break condition:** If the "thinking" latency exceeds user patience (approx. 3-5 seconds based on qualitative feedback), the user may repeat commands, causing the speech module to queue conflicting inputs.

## Foundational Learning

- **Concept: Multimodal Sensor Fusion & Coordinate Transformation**
  - **Why needed here:** The system relies on mapping the user's gaze (eye-tracker coordinates) to objects in the world (robot camera coordinates). Without understanding how to transform these coordinate frames, the "input fusion" cannot accurately identify the object the user is looking at.
  - **Quick check question:** Can you explain how to map a 2D gaze point from a user's eye-glass camera to a 3D world coordinate using a depth map or pre-calibrated transformation matrix?

- **Concept: Prompt Engineering for Embodiment (Function Calling)**
  - **Why needed here:** The system does not use the LLM as a chatbot, but as a code generator. Understanding how to structure a prompt to output valid Python API calls (rather than conversational text) is critical for the Reasoning Module.
  - **Quick check question:** How would you structure a prompt to force an LLM to output strictly JSON or Python code that adheres to a specific robot SDK schema?

- **Concept: Real-time vs. Asynchronous Processing Loops**
  - **Why needed here:** The framework uses an event-driven loop (triggered by speech completion) rather than a purely continuous loop. Understanding this distinction is necessary to manage system latency and the "3-second wait" logic for phrase detection.
  - **Quick check question:** Why is the input fusion triggered only after the speech-to-text module detects a pause, rather than running continuously for every video frame?

## Architecture Onboarding

- **Component map:**
  1.  **Input Fusion (Purple):** Gathers Audio (Whisper), Vision (YOLO-World/BLIP), and Gaze (Tobii) -> outputs a `Python Dict`.
  2.  **Reasoning Module (Yellow):** Receives `Dict` + `System Prompt` -> calls GPT-4o-mini -> outputs `Python Code String`.
  3.  **Action Module (Green):** Executes `Python Code String` via Robot SDK -> triggers hardware (Speech/Gesture).

- **Critical path:** Speech registration -> Input Fusion (Dictionary creation) -> LLM Inference (Reasoning) -> Code Execution (Action). *Note: Latency accumulates primarily during LLM Inference and Vision Processing.*

- **Design tradeoffs:**
  - **Adaptability vs. Efficiency:** The LLM condition allows dynamic handling of ambiguity but consumes ~10% more energy (1972 Wh vs 1784 Wh) and is prone to redundant responses.
  - **Open-Vocabulary vs. Reliability:** The system uses YOLO-World for open-vocabulary detection, which trades the precision of trained object detectors for flexibility in novel environments.
  - **Local vs. Cloud:** The system runs perception locally but relies on an API for the LLM, balancing bandwidth needs against reasoning capability.

- **Failure signatures:**
  - **"Hallucinated Speech":** Background noise is interpreted as user input by Whisper. *Fix: Robust voice activity detection.*
  - **Redundant Output:** The LLM repeats instructions given in the prompt context. *Fix: Stronger negative constraints in the prompt.*
  - **Gaze Mismatch:** The user looks at an object from a skewed angle; detection fails. *Fix: Improve segmentation (SAM 2) or temporal filtering of gaze data.*

- **First 3 experiments:**
  1.  **Validation of Input Fusion Accuracy:** Place 5 objects; look at each while speaking. Verify the Python Dictionary correctly identifies the looked-at object as the target. Success criteria: >90% correct spatial association.
  2.  **Latency Profiling:** Measure time-to-first-token for the LLM and processing time for the Vision pipeline. Determine if the "thinking" gesture is sufficient to cover the delay (Target: <2.5s total loop time).
  3.  **Scripted vs. LLM Redundancy Test:** Execute a simple "pick up" task 10 times. Count the number of unnecessary clarifications or repetitive phrases in the LLM condition vs. the Scripted condition.

## Open Questions the Paper Calls Out
None

## Limitations
- **Latency and Energy Trade-offs:** The system incurs significant processing delays, with LLM inference and vision pipeline contributing to the ~10% higher energy consumption (1972 Wh vs 1784 Wh).
- **Reliability in Ambiguous Conditions:** The framework's dependence on accurate gaze-object mapping and speech recognition creates multiple failure points including background noise triggering "hallucinated speech" and gaze mismatches from skewed viewing angles.
- **Scalability of Multimodal Fusion:** The current implementation uses a static dictionary structure for state representation, which may not scale well to more complex environments with numerous objects or dynamic changes.

## Confidence
**High Confidence (90-100%):**
- The modular architecture design and its implementation details are well-documented and reproducible
- The energy consumption measurements and basic task efficiency comparisons are reliable
- The qualitative feedback patterns regarding user preferences are credible

**Medium Confidence (60-89%):**
- The statistical significance of the confidence rating difference (M=5.6 vs 4.4, p=0.039) is borderline and may not be practically meaningful
- The generalizability of findings to non-lab environments remains uncertain
- The long-term reliability of the system under varied conditions is unknown

**Low Confidence (Below 60%):**
- The system's performance on genuinely complex, multi-step tasks beyond the lab setting
- The effectiveness of the "thinking" gesture in managing user patience over extended interactions
- The framework's adaptability to robot platforms with different hardware capabilities

## Next Checks
1. **Error Rate Quantification:** Systematically measure and report false positive rates for speech detection, gaze-object mapping accuracy, and LLM hallucination frequency across 100+ interaction trials in varying lighting and noise conditions.

2. **Cross-Platform Transferability Test:** Implement the framework on two additional robot platforms with different sensor suites and computational capabilities to assess true modularity and required adaptation effort.

3. **Long-Duration Stress Test:** Conduct 4-hour continuous interaction sessions to evaluate system stability, memory management, and user fatigue, measuring degradation in response accuracy and timing over time.