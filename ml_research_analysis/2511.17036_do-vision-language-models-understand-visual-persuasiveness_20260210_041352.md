---
ver: rpa2
title: Do Vision-Language Models Understand Visual Persuasiveness?
arxiv_id: '2511.17036'
source_url: https://arxiv.org/abs/2511.17036
tags:
- visual
- message
- image
- persuasive
- persuasiveness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Vision-language models exhibit a recall-oriented bias in visual\
  \ persuasion judgment, over-predicting high persuasiveness while underperforming\
  \ on low-level and mid-level feature discrimination. Human-aligned high-level semantic\
  \ cues\u2014particularly the presence of message-relevant key objects\u2014are the\
  \ strongest predictors of persuasiveness, whereas models over-rely on generic human\
  \ presence."
---

# Do Vision-Language Models Understand Visual Persuasiveness?

## Quick Facts
- arXiv ID: 2511.17036
- Source URL: https://arxiv.org/abs/2511.17036
- Reference count: 40
- Primary result: VLMs exhibit recall-oriented bias in visual persuasion judgment, over-predicting high persuasiveness.

## Executive Summary
Vision-language models struggle to accurately judge visual persuasiveness, exhibiting a systematic over-prediction of high persuasiveness that humans do not share. The study reveals that while VLMs can leverage high-level semantic alignment between messages and key objects, they underweight this critical cue relative to generic human presence, which humans ignore entirely. Knowledge-injection strategies can partially correct this bias, but only when concise, object-grounded rationales are provided—instruction-based reasoning alone fails to improve performance. These findings highlight that VLMs' core limitation is not object recognition but linking visual cues to communicative intent, necessitating explicit, rationale-driven reasoning for accurate persuasive understanding.

## Method Summary
The study analyzes VLMs' ability to judge visual persuasiveness using the PVP dataset, focusing on a high-consensus subset of 562 images with "Almost perfect" inter-annotator agreement. Visual persuasive factors (VPFs) are extracted across three levels: low-level features (colorfulness, brightness), mid-level compositional features (saliency maps), and high-level semantic features (key objects and human presence). Rationales are generated using GPT-5 and injected into VLMs (GPT-5, Gemma3, Qwen2.5-VL) to improve persuasiveness judgment. Performance is evaluated using accuracy, precision, recall, and F1, with logistic regression odds ratios quantifying factor influence.

## Key Results
- VLMs exhibit recall-oriented bias, over-predicting high persuasiveness particularly when humans are present.
- High-level semantic alignment (message-object presence) is the strongest predictor for humans but is systematically underweighted by VLMs.
- Knowledge-injection improves performance only when concise, object-grounded rationales are provided; instruction-based reasoning alone fails to correct the bias.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: High-level semantic alignment—specifically, the presence of message-relevant key objects in an image—is the most powerful predictor of persuasiveness for both humans and VLMs, but VLMs systematically underweight this cue relative to generic human presence.
- **Mechanism**: Humans judge persuasiveness by verifying whether the image visually grounds the core message (e.g., a visible "stove" for a kitchen-safety message). VLMs also leverage this alignment but dilute its effect by over-attending to superficial features like human presence. This operates via the peripheral route of the Elaboration Likelihood Model, where semantic coherence signals credibility and relevance.
- **Core assumption**: The peripheral-route processing dominates in rapid persuasiveness judgments, making semantic alignment the primary signal over low-level perceptual cues.
- **Evidence anchors**:
  - [abstract] "High-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment."
  - [Table 1] For humans, key-object presence increases odds of "high" persuasiveness by >3× (OR≈3.28); for VLMs, effects are significant but weaker (OR≈2.0–2.9) and diluted by over-sensitivity to human presence.
  - [corpus] ColorBench (2504.10514) confirms VLMs struggle with low-level perception but can leverage higher-level semantic cues when properly grounded.
- **Break condition**: If key objects are present but visually ambiguous, contextually irrelevant, or obscured, alignment no longer predicts persuasiveness.

### Mechanism 2
- **Claim**: Knowledge-injection strategies improve VLM persuasiveness judgment only when concise, object-grounded rationales are provided; instruction-based or scaffolded reasoning alone fail to correct the recall-oriented bias.
- **Mechanism**: Rationales explicitly link detected objects to persuasive intent, forcing the model to reason *why* an object matters rather than defaulting to a "high" prediction. This bypasses the model's over-reliance on surface features (color, human presence) by providing a structured reasoning pathway that connects perception to communicative goals.
- **Core assumption**: VLMs' core limitation is not object recognition but linking recognized objects to persuasive intent—a reasoning gap that explicit rationales can bridge.
- **Evidence anchors**:
  - [abstract] "Knowledge-injection strategies improve performance only when concise, object-grounded rationales are provided; instruction-based or scaffolded reasoning alone fail to correct the bias."
  - [Table 2] Key-Object Rationale (Informed) improves F1 by +13–27 points across models, while Cognitive Injection yields ≈0 change and Knowledge-Conditioned Chain can degrade performance.
  - [corpus] Corpus evidence on pragmatic inference (2502.09120) shows VLMs benefit from explicit contextual cues that ground inference but struggle with purely linguistic scaffolding.
- **Break condition**: If rationales are verbose, irrelevant, or not grounded in specific detected objects, they add noise and may degrade performance.

### Mechanism 3
- **Claim**: VLMs exhibit a recall-oriented bias—systematically over-predicting "high" persuasiveness—driven by over-sensitivity to generic human presence, which is not a significant predictor for human judgments.
- **Mechanism**: Models treat human presence as a proxy for engagement or emotional relevance during pretraining, inflating recall. Humans, in contrast, weigh semantic relevance (message-object alignment) over mere presence, leading to more balanced precision-recall trade-offs.
- **Core assumption**: Human presence acts as a shortcut feature learned during VLM pretraining that correlates with "interesting" images but not with persuasive alignment.
- **Evidence anchors**:
  - [abstract] "Models over-rely on generic human presence."
  - [Table 1] Human presence has no significant effect on human judgments (OR≈1.22, ns), but VLMs show OR 1.56–2.48 with p<0.05, indicating systematic over-weighting.
  - [corpus] Corpus evidence on social inference (2506.11162) finds VLMs can detect social cues but misinterpret their relevance to higher-level tasks.
- **Break condition**: If human presence is explicitly down-weighted via prompt engineering or if the task frame emphasizes message-object alignment, the recall-oriented bias may reduce.

## Foundational Learning

### Concept: Elaboration Likelihood Model (ELM)
- **Why needed here**: The paper frames visual persuasion as a peripheral-route process where superficial cues (color, composition, human presence) provide background signals but are secondary to semantic alignment for judgment.
- **Quick check question**: Can you explain why low-level features like colorfulness have marginal effects on persuasiveness in this study, despite their known role in marketing?

### Concept: Logistic Regression and Odds Ratios
- **Why needed here**: The paper uses odds ratios to quantify how much key-object presence increases the odds of a "high" persuasiveness judgment, enabling comparison between human and model decision-making.
- **Quick check question**: If key-object presence has OR=3.28 for humans but only OR=2.0 for a VLM, what does this imply about the model's use of semantic alignment?

### Concept: Saliency Maps and Attention Modeling
- **Why needed here**: Mid-level compositional features are extracted from saliency maps (DeepGaze IIE), which approximate general human visual attention but may not capture persuasive-specific attention patterns.
- **Quick check question**: Why might a standard saliency model fail to predict which regions are persuasive for a specific message, even if it accurately models general attention?

## Architecture Onboarding

### Component map
Image-message pair -> Low-level features (color, brightness) -> Mid-level features (saliency maps) -> High-level features (key objects, humans) -> Rationale generator (GPT-5) -> VLM (GPT-5, Gemma3, Qwen2.5-VL) -> Binary persuasiveness label

### Critical path
1. Extract key nouns from message (spaCy POS tagging).
2. Detect objects in image (OWL-ViT for key objects, YOLO for humans).
3. Generate object-grounded rationale (GPT-5, label-agnostic or informed).
4. Inject rationale into VLM prompt for final binary judgment.

### Design tradeoffs
- Rationale generator: Label-informed provides upper-bound performance but requires oracle labels; label-agnostic is realistic but yields lower gains.
- Saliency model: DeepGaze IIE is general-purpose; a persuasion-oriented saliency model (future work) could improve mid-level feature relevance.
- Dataset scope: High-consensus subset (n=562) enables clean analysis but limits generalization to low-agreement, personality-dependent cases.

### Failure signatures
- Over-prediction of "high" when humans are present but message-object alignment is weak.
- Degraded F1 when rationales are verbose, irrelevant, or not grounded in detected objects.
- Low-level features (color, brightness) showing overlapping distributions across high/low labels, confirming limited discriminative power.

### First 3 experiments
1. **Baseline probe**: Run binary persuasiveness judgment on the high-consensus subset without knowledge injection. Quantify recall-oriented bias (near-perfect recall, inflated false positives).
2. **Feature ablation**: Compare performance with only low-level, only mid-level, and only high-level features injected. Expect high-level to dominate; low/mid-level to show minimal discriminative power.
3. **Rationale style test**: Compare label-agnostic vs. label-informed rationales on a held-out set. Measure F1, precision, recall, and rationale similarity (BERTScore, ROUGE-L, BLEU) to assess reasoning alignment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can VLMs generalize visual persuasion understanding to low-agreement scenarios where human interpretations vary widely?
- Basis in paper: [explicit] "Generalization to 'low-agreement' data—where human interpretations widely vary—remains a necessary next step."
- Why unresolved: The study deliberately filtered for high-consensus items to establish baseline failures; low-agreement cases introduce additional noise from subjective factors.
- What evidence would resolve it: Evaluation of VLMs on the full PVP dataset (not just the robust subset) with analysis of whether VPF factors retain predictive power when human agreement is low.

### Open Question 2
- Question: Can concise, label-agnostic rationales achieve performance comparable to oracle (label-informed) rationales for visual persuasion judgment?
- Basis in paper: [explicit] "A dataset and metrics for generating concise, object-grounded, label-agnostic rationales are needed." The gap between Agnostic (≈+3 F1) and Informed (+13–16 F1) rationale injection reveals a fundamental bottleneck.
- Why unresolved: Label-informed rationales require oracle access to ground truth; practical systems must generate useful rationales without knowing the answer.
- What evidence would resolve it: Development and benchmarking of rationale generators that achieve high BERTScore/ROUGE-L similarity to informed rationales without label access.

### Open Question 3
- Question: How do visual persuasive factors interact across levels (low/mid/high) to jointly influence persuasiveness judgments?
- Basis in paper: [explicit] "Future works can not only extend the level-wise analysis of VPFs but also progress toward merge-level inference. Collectively, these factors interact and merge."
- Why unresolved: The paper analyzes each VPF level independently; compositional effects (e.g., how color interacts with key-object salience) remain unexplored.
- What evidence would resolve it: Factorial experiments systematically varying VPFs across levels, or multivariate models capturing interaction effects on both human and model judgments.

### Open Question 4
- Question: Does persuasion-oriented saliency—trained on persuasive attention patterns—provide stronger explanatory power than general-purpose saliency models?
- Basis in paper: [inferred] "DeepGaze IIE-style saliency captures where attention concentrates, but it was trained on general visual attention, which may not be the same cognitive process as the attention used for judging persuasiveness."
- Why unresolved: Mid-level compositional features showed minimal discriminative power, but this may reflect a mismatch between general saliency and persuasion-specific attention.
- What evidence would resolve it: Training saliency models on eye-tracking data from persuasion judgment tasks and comparing their correlation with human persuasiveness ratings against general saliency baselines.

## Limitations

- The analysis is based on a high-consensus subset (n=562) rather than the full PVP dataset, limiting generalizability to low-agreement cases where persuasiveness is more subjective.
- The study does not provide detailed breakdowns of false-positive vs. false-negative rates for each VLM, making it difficult to assess which models are most prone to the recall-oriented bias.
- The reliance on generic saliency models (DeepGaze IIE) may underestimate the role of persuasion-specific attention patterns.

## Confidence

- **High confidence**: The core observation that VLMs systematically over-predict high persuasiveness, particularly in the presence of humans, is strongly supported by the odds ratio analysis showing significant ORs for human presence in VLMs but not in humans.
- **Medium confidence**: The claim that object-grounded rationales are the only knowledge-injection strategy that improves VLM performance is supported, but the lack of variance reporting and alternative prompt strategies means the robustness of this finding is uncertain.
- **Low confidence**: The assertion that low-level and mid-level features are inherently less useful for persuasion judgment is based on non-significant odds ratios, but without explicit performance comparisons for models using only these features, this claim is speculative.

## Next Checks

1. **Generalization test**: Evaluate the same VLM and rationale-injection pipeline on the full PVP dataset (including low-consensus images) to measure robustness to subjective, personality-dependent persuasiveness judgments.

2. **Low/mid-level only baseline**: Train or fine-tune a VLM to use only low-level (color, brightness) or mid-level (saliency) features, and compare its F1 to the high-level-only and full-feature models to quantify the marginal utility of semantic alignment.

3. **Prompt ablation study**: Systematically vary the format, length, and grounding of injected rationales (e.g., object-only vs. object+intent, short vs. long) and measure the impact on precision, recall, and reasoning alignment (BERTScore, ROUGE-L) to identify the most effective rationale style.