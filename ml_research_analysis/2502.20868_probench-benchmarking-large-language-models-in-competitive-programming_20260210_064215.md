---
ver: rpa2
title: 'ProBench: Benchmarking Large Language Models in Competitive Programming'
arxiv_id: '2502.20868'
source_url: https://arxiv.org/abs/2502.20868
tags:
- reasoning
- code
- programming
- problem
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ProBench, a comprehensive benchmark for\
  \ evaluating large language models (LLMs) in competitive programming, addressing\
  \ the inadequacy of existing benchmarks for advanced reasoning models. ProBench\
  \ collects real competition problems from Codeforces, Luogu, and Nowcoder platforms\
  \ (July\u2013December 2024) and employs an online submission strategy to leverage\
  \ original platforms\u2019 robust test suites, ensuring fair and accurate code robustness\
  \ assessment."
---

# ProBench: Benchmarking Large Language Models in Competitive Programming

## Quick Facts
- arXiv ID: 2502.20868
- Source URL: https://arxiv.org/abs/2502.20868
- Reference count: 13
- Large reasoning models (QwQ-32B-Preview) outperform larger general-purpose models on competitive programming tasks

## Executive Summary
This paper introduces ProBench, a comprehensive benchmark for evaluating large language models (LLMs) in competitive programming, addressing the inadequacy of existing benchmarks for advanced reasoning models. ProBench collects real competition problems from Codeforces, Luogu, and Nowcoder platforms (Julyâ€“December 2024) and employs an online submission strategy to leverage original platforms' robust test suites, ensuring fair and accurate code robustness assessment. The benchmark establishes a unified problem attribute system with difficulty grading and algorithm tagging.

## Method Summary
ProBench evaluates LLMs by generating C++ code solutions for competitive programming problems and submitting them to original online judge platforms (Codeforces, Luogu, Nowcoder) to leverage their comprehensive test suites. The benchmark uses a unified problem attribute system with difficulty grading and algorithm tagging. For each problem, 8 candidate solutions are generated using specified prompt templates. The evaluation measures pass@k metrics (k=1, 2, 4, 8) and analyzes error types, reasoning depth via Chain-of-Thought length, and performance across algorithm categories. Critical implementation details like automated submission scripts and exact sampling parameters are not fully disclosed.

## Key Results
- Reasoning-oriented models (QwQ-32B-Preview) significantly outperform larger general-purpose models, achieving a leading pass@1 score of 20.93
- Models exhibit "under-reasoning" for hard problems (flat CoT growth) and "over-reasoning" for easy problems (excessive CoT)
- Error distribution analysis shows stronger models fail on later test cases, while weaker models fail immediately
- Performance hierarchy: String < Graph < Data Structure < Dynamic Programming

## Why This Works (Mechanism)

### Mechanism 1
Online submission to original platforms significantly reduces false positive rates in code evaluation compared to offline local testing. Competitive programming platforms maintain proprietary, robust test suites including edge cases that are computationally expensive to replicate locally. By submitting directly via API or automation, the benchmark leverages these hidden suites to verify code correctness and efficiency rigorously.

### Mechanism 2
Specialized reasoning training (e.g., Chain-of-Thought + RL) improves performance on complex algorithmic tasks more effectively than parameter scaling alone. Reasoning-oriented models utilize extended "thinking" processes to decompose complex problems, allowing smaller models to outperform larger general-purpose models by allocating compute specifically to logical deduction.

### Mechanism 3
Error distribution across test cases serves as a proxy for "reasoning depth." Stronger models tend to fail on later test cases (deep in the test suite), whereas weaker models fail immediately. Early test cases usually check basic input/output handling, while later cases verify efficiency and edge-case logic.

## Foundational Learning

- **Pass@k Metric**: Measures probability of solving within k attempts. *Why needed:* Distinguishes between models that can eventually solve problems vs. those that solve consistently on first try. *Quick check:* If Model A has pass@1=20 and pass@8=40, and Model B has pass@1=18 and pass@8=50, which is more reliable for single try, and which has better coverage with retries?

- **False Positives in Offline Evaluation**: Local unit tests often miss edge cases like integer overflow or TLE on max constraints. *Why needed:* Explains why ProBench targets inadequacy of offline benchmarks. *Quick check:* Why might a Python solution pass local unit test but fail on Codeforces platform?

- **Chain-of-Thought (CoT) Length Analysis**: Diagnoses "under-reasoning" (CoT not growing with difficulty) and "over-reasoning" (long CoT for easy problems). *Why needed:* Central to ProBench's analysis of reasoning depth. *Quick check:* If a model maintains constant CoT length for both Easy and Hard problems, what specific failure mode occurs?

## Architecture Onboarding

- **Component map**: Problem Scraper (Codeforces/Luogu/Nowcoder) -> Attribute Normalizer (Difficulty/Tag mapping) -> LLM Inference Engine -> Code Extraction -> Online Submission Interface -> Result Parser -> Multi-dimensional Analyzer
- **Critical path**: The Online Submission Interface is the highest-risk component, requiring handling authentication, rate-limiting, and parsing non-standardized responses from three different platforms.
- **Design tradeoffs**: Online submission ensures validity but introduces high latency; focusing on C++ allows deeper efficiency testing but narrows Python-specific library reasoning evaluation.
- **Failure signatures**: High "Compilation Error" rates indicate code generation failures; high "WA/TLE" rates indicate reasoning failures; flat CoT growth curve indicates under-reasoning.
- **First 3 experiments**: 
  1. Sanity Check: Run 50 problems using offline vs. online tests to quantify false positive gap
  2. Reasoning Depth Calibration: Plot error interval distribution for baseline model to establish reference
  3. Tag Ablation: Evaluate model specifically on DP vs. String categories to verify performance hierarchy

## Open Questions the Paper Calls Out

1. **Performance of DeepSeek-R1 models**: How do state-of-the-art reasoning models like DeepSeek-R1 perform on ProBench compared to currently evaluated models? The authors explicitly state the number of evaluated reasoning models remains limited due to computational requirements.

2. **Operational convenience improvements**: Can novel solutions be developed to improve online submission benchmarks' operational convenience to match offline evaluation speed? The authors note current online submission lags behind offline systems in operational convenience.

3. **Dynamic reasoning scaling**: How can models be trained to dynamically scale reasoning length in proportion to problem difficulty? Analysis reveals under-reasoning phenomena where models show only modest growth in reasoning length for hard problems.

4. **Reasoning error mechanisms**: What specific mechanisms lead to persistent reasoning errors, and can algorithmic adaptability be disentangled from pure logical deduction capabilities? The paper identifies error types but doesn't determine if failures are due to incorrect algorithm selection vs. flawed implementation.

## Limitations
- Online submission reliability depends on third-party platforms with rate limits and API changes
- Reproducibility gap due to missing implementation details (automated scripts, exact parameters)
- Temporal validity concerns as platforms update test suites and new problems emerge
- Limited evaluation of reasoning models due to computational constraints

## Confidence
- **High**: Online submission methodology effectively reduces false positives
- **Medium**: Performance hierarchy (QwQ-32B-Preview > DeepSeek-V3) is likely valid
- **Low**: Reasoning depth diagnosis requires assumptions about test case ordering

## Next Checks
1. **Offline vs. Online Gap Quantification**: Run 100-problem subset using standard local testing vs. online submission to measure false positive reduction rate
2. **Submission Reliability Audit**: Test automated submission pipeline's success rate across all three platforms under typical conditions
3. **Temporal Stability Test**: Re-evaluate 50-problem subset after 3 months to assess platform changes' impact on results