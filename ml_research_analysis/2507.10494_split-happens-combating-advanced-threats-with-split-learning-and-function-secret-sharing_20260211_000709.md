---
ver: rpa2
title: 'Split Happens: Combating Advanced Threats with Split Learning and Function
  Secret Sharing'
arxiv_id: '2507.10494'
source_url: https://arxiv.org/abs/2507.10494
tags:
- splithappens
- training
- privacy
- public
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SplitHappens, a privacy-preserving machine
  learning protocol that combines Split Learning (SL) with Function Secret Sharing
  (FSS) to protect both data and labels during model training. The method uses U-shaped
  SL, where the model is split such that the final prediction layer remains on the
  client-side, preventing label disclosure to servers.
---

# Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing
## Quick Facts
- arXiv ID: 2507.10494
- Source URL: https://arxiv.org/abs/2507.10494
- Reference count: 33
- SplitHappens achieves 97.21% accuracy on MNIST, reducing training time and communication costs compared to fully FSS-based approaches while protecting against advanced inference attacks.

## Executive Summary
SplitHappens is a privacy-preserving machine learning protocol that integrates Split Learning (SL) with Function Secret Sharing (FSS) to secure both data and labels during model training. The protocol employs U-shaped SL, where the final prediction layer remains on the client-side, and uses FSS between two servers to compute intermediate layers securely. This approach mitigates Label Inference and Model Inversion attacks, as well as modern threats like PCAT, FORA, and Feature Sniffer, by masking activations and preventing disclosure of gradients and final predictions. Experimental results on benchmark datasets (MNIST, CIFAR, FMNIST) show competitive accuracy with state-of-the-art FSS methods and significant reductions in training time and communication costs.

## Method Summary
SplitHappens combines U-shaped Split Learning with Function Secret Sharing to protect privacy during model training. In U-shaped SL, the model is split so that the final prediction layer stays on the client, preventing label disclosure to servers. FSS is used between two servers to compute intermediate layers securely by sharing function keys and masking activations. This dual approach ensures that neither data nor labels are exposed during training, while maintaining model accuracy and efficiency.

## Key Results
- SplitHappens achieves 97.21% accuracy on MNIST, comparable to state-of-the-art FSS methods.
- Significant reductions in training time and communication costs compared to fully FSS-based approaches.
- Effective protection against Label Inference, Model Inversion, PCAT, FORA, and Feature Sniffer attacks through masking and non-disclosure of gradients and final predictions.

## Why This Works (Mechanism)
SplitHappens leverages the strengths of both Split Learning and Function Secret Sharing to create a robust privacy-preserving training protocol. By keeping the final prediction layer on the client-side (U-shaped SL), it prevents label leakage to servers. FSS between servers ensures that intermediate activations are masked and never fully exposed, thwarting inference attacks. The combination of these techniques provides strong privacy guarantees while maintaining high model accuracy and efficiency.

## Foundational Learning
- **Split Learning (SL)**: Splits neural network training between client and server to protect data privacy. Needed to prevent raw data exposure; quick check: client holds sensitive data, server processes only intermediate activations.
- **Function Secret Sharing (FSS)**: Enables secure computation of functions between parties without revealing inputs. Needed to mask intermediate layer computations; quick check: two servers jointly compute without exposing intermediate values.
- **U-shaped SL**: Variant where the final prediction layer remains on the client, protecting labels. Needed to prevent label inference attacks; quick check: labels never leave client device.
- **Masking in FSS**: Uses function keys to obscure intermediate values during computation. Needed to prevent reconstruction of activations; quick check: activations are split and masked across servers.
- **Label Inference Attacks**: Attempt to recover training labels from model gradients or outputs. Needed context for threat model; quick check: attacks target exposed labels or gradients.
- **Model Inversion Attacks**: Reconstruct training data from model outputs. Needed context for threat model; quick check: attacks exploit exposed activations or predictions.

## Architecture Onboarding
- **Component Map**: Client -> U-shaped SL (split model) -> Server 1 & Server 2 (FSS) -> Final prediction on client
- **Critical Path**: Client holds data and labels, sends masked activations to servers, servers compute intermediate layers using FSS, results returned to client for final prediction
- **Design Tradeoffs**: U-shaped SL preserves label privacy but requires more client-side computation; FSS reduces communication but adds server-side complexity
- **Failure Signatures**: If servers collude, FSS protection fails; if client is compromised, label privacy is lost; if network is unreliable, FSS communication may break
- **3 First Experiments**: 1) Verify label privacy by checking if servers can infer labels from gradients; 2) Test attack resistance by simulating Label Inference and Model Inversion; 3) Benchmark training time and communication costs versus baseline FSS

## Open Questions the Paper Calls Out
None provided in the source.

## Limitations
- Security assumptions rely on honest-but-curious servers, which may not hold in all operational contexts.
- Protocol's robustness against collusion or Byzantine server behavior is not explored.
- Experimental validation focuses on standard benchmark datasets and simple neural architectures, limiting generalizability.

## Confidence
- High: Correctness of proposed method and theoretical security properties under stated assumptions
- Medium: Empirical performance gains and generalizability due to limited scope of experiments and threat model coverage

## Next Checks
1. Conduct robustness testing against adaptive, model-aware adversaries and evaluate under collusion or Byzantine server behavior.
2. Benchmark against alternative hybrid SL-FSS approaches and quantify component contributions through ablation studies.
3. Evaluate scalability and communication efficiency under heterogeneous network conditions and on larger, more complex datasets.