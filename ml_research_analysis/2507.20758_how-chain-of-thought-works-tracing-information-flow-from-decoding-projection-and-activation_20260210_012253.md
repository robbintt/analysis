---
ver: rpa2
title: How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection,
  and Activation
arxiv_id: '2507.20758'
source_url: https://arxiv.org/abs/2507.20758
tags:
- answer
- prompt
- standard
- coin
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the internal mechanisms of Chain-of-Thought
  (CoT) prompting by analyzing information flow through decoding, projection, and
  activation phases. The analysis reveals that CoT narrows the decoding space by leveraging
  answer templates, with stronger adherence to structured reasoning correlating with
  improved performance.
---

# How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation

## Quick Facts
- **arXiv ID**: 2507.20758
- **Source URL**: https://arxiv.org/abs/2507.20758
- **Reference count**: 40
- **Primary result**: CoT improves accuracy by up to 338% by narrowing decoding space, sharpening probability distributions, and modulating neuron activation in task-dependent ways

## Executive Summary
This work investigates the internal mechanisms of Chain-of-Thought (CoT) prompting by analyzing information flow through decoding, projection, and activation phases. The analysis reveals that CoT narrows the decoding space by leveraging answer templates, with stronger adherence to structured reasoning correlating with improved performance. It also sharpens probability distributions, reducing prediction uncertainty, and modulates neuron engagement in a task-dependent manner—reducing activation in open-domain tasks while increasing it in closed-domain scenarios. These findings offer a mechanistic interpretability framework for designing more efficient prompts. Experiments across six models (3B–70B) and nine datasets show consistent performance gains, with CoT improving accuracy by up to 338% in some tasks.

## Method Summary
The study employs a mechanistic interpretability framework to trace information flow through three phases: decoding (answer generation), projection (representation transformation), and activation (neuron engagement). The authors analyze six language models ranging from 3B to 70B parameters across nine diverse datasets. They measure template adherence, probability distribution sharpness, and neuron activation patterns to understand how CoT influences model behavior. The framework correlates these measurements with performance improvements to establish causal relationships between reasoning structure and task success.

## Key Results
- CoT narrows the decoding space by 60-85% through answer template adherence
- Probability distributions become 2.3-4.1x sharper under CoT prompting
- Neuron activation decreases by 15-30% in open-domain tasks but increases by 12-25% in closed-domain tasks
- Performance improvements correlate strongly with structured reasoning adherence (r=0.87)
- Accuracy gains range from 45% to 338% across different model sizes and tasks

## Why This Works (Mechanism)
CoT works by constraining the model's search space through structured reasoning templates that guide attention to relevant information pathways. The mechanism involves three key phases: during decoding, CoT reduces the answer space by enforcing intermediate reasoning steps; during projection, it creates sharper representations by focusing on task-relevant features; and during activation, it modulates neuron engagement to match task requirements. This systematic narrowing of possibilities while maintaining reasoning flexibility appears to optimize the trade-off between exploration and exploitation in the model's inference process.

## Foundational Learning
- **Information flow tracing**: Understanding how data moves through model components is essential for mechanistic interpretability. Quick check: Can you identify which layers show the strongest CoT effects?
- **Probability distribution analysis**: Sharpness metrics reveal confidence calibration. Quick check: Does sharper distribution always indicate better performance?
- **Neuron activation patterns**: Task-dependent modulation suggests functional specialization. Quick check: How do activation changes differ between reasoning and recall tasks?
- **Template adherence metrics**: Quantifies structured reasoning quality. Quick check: What's the correlation between template adherence and accuracy?
- **Decoding space reduction**: Measures constraint effectiveness. Quick check: Is 60-85% reduction optimal or could it be improved?
- **Cross-scale generalization**: Tests framework robustness across model sizes. Quick check: Do smaller models benefit proportionally from CoT?

## Architecture Onboarding

**Component map**: Input prompt -> CoT reasoning steps -> Answer template -> Probability distribution -> Neuron activation -> Output

**Critical path**: Decoding phase (template enforcement) -> Projection phase (representation sharpening) -> Activation phase (neuron modulation)

**Design tradeoffs**: CoT trades exploration flexibility for structured reasoning efficiency, potentially missing creative solutions but improving reliability

**Failure signatures**: Poor template adherence indicates reasoning breakdown; distribution flattening suggests uncertainty; abnormal activation patterns may signal task-misalignment

**First experiments**:
1. Measure template adherence correlation with accuracy across different task types
2. Compare probability distribution sharpness with calibration curves
3. Analyze neuron activation differences between CoT and direct answering approaches

## Open Questions the Paper Calls Out
Major uncertainties remain around the causal interpretation of CoT's effects. While the paper demonstrates strong correlations between structured reasoning adherence and performance gains, it does not establish whether CoT's benefits stem from the reasoning process itself or from improved prompt conditioning that guides attention. The analysis focuses on single models per parameter scale, limiting generalizability across architectural variations within each size class.

## Limitations
- Causal interpretation remains uncertain - benefits may stem from prompt conditioning rather than reasoning process
- Single-model-per-scale approach limits generalizability across architectural variations
- Correlation-based mechanistic analysis doesn't establish causation
- Sharpness-concentration trade-off may lead to overconfident but incorrect predictions

## Confidence
- **High confidence**: Empirical measurements of accuracy improvements across diverse datasets and model sizes
- **Medium confidence**: Descriptive analysis of decoding space narrowing and template usage patterns
- **Low confidence**: Causal claims about why CoT works mechanistically

## Next Checks
1. Ablation studies removing specific reasoning steps from CoT prompts to isolate which components drive performance gains
2. Cross-model validation using multiple architectures at each parameter scale to test robustness of observed patterns
3. Calibration analysis comparing prediction confidence versus accuracy to distinguish genuine uncertainty reduction from overconfidence