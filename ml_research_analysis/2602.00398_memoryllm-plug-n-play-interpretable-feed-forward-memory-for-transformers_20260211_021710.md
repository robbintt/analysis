---
ver: rpa2
title: 'MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers'
arxiv_id: '2602.00398'
source_url: https://arxiv.org/abs/2602.00398
tags:
- memoryllm
- memory
- ffns
- layer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MemoryLLM, a novel transformer architecture
  that disentangles feed-forward networks (FFNs) from self-attention modules to enable
  interpretable token-wise neural memory. The key innovation is training FFNs in isolation
  using context-free token embeddings, allowing them to function as neural key-value
  retrieval memory accessible via a finite, human-interpretable vocabulary space.
---

# MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers

## Quick Facts
- arXiv ID: 2602.00398
- Source URL: https://arxiv.org/abs/2602.00398
- Reference count: 40
- Key outcome: MemoryLLM achieves ~40% reduction in active parameters while maintaining competitive perplexity through context-free FFN training and pre-computed token-wise lookups

## Executive Summary
MemoryLLM introduces a novel transformer architecture that disentangles feed-forward networks (FFNs) from self-attention modules by training FFNs on context-free token embeddings rather than contextualized residual streams. This enables interpretable token-indexed neural memory accessible via a finite vocabulary space, where semantically similar tokens activate similar memory locations within FFNs. The approach achieves significant efficiency gains through pre-computed token-wise lookups (ToLs) that can be transferred between VRAM and storage, and introduces Flex-MemoryLLM as a bridge variant that closely matches conventional LLM performance while reducing active parameters by ~40%.

## Method Summary
MemoryLLM modifies transformer architecture by replacing conventional FFNs with context-free variants that receive static token embeddings directly from the embedding layer rather than contextualized residual stream. Each FFN layer operates independently with its own LayerNorm on X₀, and the residual stream accumulates outputs from both self-attention (on evolving X_L) and all FFNs (on static X̂₀). The TKV framework interprets SwiGLU FFNs as neural key-value retrieval memory where token queries activate specific memory cells based on semantic similarity. ToLs are pre-computed by concatenating all FFN outputs per token, enabling plug-n-play memory transfer. Flex-MemoryLLM splits FFN parameters between context-aware (FFN-C) and context-free (FFN-M) modules with hyperparameter β controlling the split ratio.

## Key Results
- MemoryLLM achieves ~40% reduction in active parameters while maintaining competitive perplexity on C4 and Wikitext-2
- Flex-MemoryLLM closely matches conventional LLM performance with β=3 configuration (704M/1208M active params)
- FFNs act as dominant contributors to retrieval-based tasks versus reasoning tasks when contribution α is reduced
- Pre-computed ToLs enable efficient memory transfer with minimal perplexity degradation when dropping later layers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training FFNs on context-free token embeddings creates interpretable token-indexed memory.
- **Mechanism:** FFNs receive static token embeddings (X₀) directly from the embedding layer rather than contextualized residual stream (X̃L). Each token ID maps deterministically to a fixed FFN input, enabling discrete memory indexing over vocabulary space.
- **Core assumption:** Meaningful parametric knowledge can be encoded without contextual information from self-attention.
- **Evidence anchors:**
  - [abstract] "MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings."
  - [section 2.2.2] Equations 2.4-2.6 show FFN(X̂₀) where X̂₀ = LayerNorm(X₀) independent of layer L.
  - [corpus] Related work on FFN layerwise importance (arXiv:2508.17734) suggests FFNs have varying contributions across layers, but does not address context-free training.
- **Break condition:** If tasks require FFN access to contextualized information (e.g., coreference resolution where token meaning depends on antecedents), performance will degrade significantly compared to conventional architectures.

### Mechanism 2
- **Claim:** The TKV framework interprets SwiGLU FFNs as neural key-value retrieval memory with vocabulary-indexed queries.
- **Mechanism:** For query token q: (1) compute memory cell coefficients c_k = (q · W_Up) × g_k via gate-reweighting; (2) retrieve output as c_k-weighted combination of value vectors from W_Down. Semantically similar tokens activate similar key clusters.
- **Core assumption:** Keys in W_Up correspond to learnable memory cells that can be meaningfully clustered by semantic properties.
- **Evidence anchors:**
  - [abstract] "The TKV framework reveals that semantically similar tokens activate similar memory locations within FFNs."
  - [section 3.1] Figure 5 shows K-means clustering of c_k vectors yields interpretable clusters (punctuation, names, locations).
  - [corpus] Geva et al. (2021) established FFNs as key-value memories, but required reverse-engineering from non-interpretable latents; this work makes queries discrete.
- **Break condition:** If clustering coefficient drops sharply in middle layers or outlier key counts explode uniformly, the key-value interpretation loses utility.

### Mechanism 3
- **Claim:** Pre-computing FFN outputs as token-wise lookups (ToLs) enables plug-n-play memory transfer between VRAM and storage.
- **Mechanism:** Since FFN inputs are static (vocabulary size |V|), pre-compute ToL_{ti} = Concat(FFN_L0(x_ti), ..., FFN_LN-1(x_ti)) for all tokens. During inference, replace FFN computation with ToL lookup.
- **Core assumption:** Token distributions follow Zipf's law, allowing frequent-token caching; later-layer ToLs are less critical (Figure 4 shows minimal perplexity increase when dropping later layers).
- **Evidence anchors:**
  - [abstract] "FFNs can be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage."
  - [section 2.3.2] Equation 2.9-2.10 formalize ToL computation and inference usage.
  - [corpus] Weak corpus evidence for ToL-specific efficiency claims; related work on FFN skipping (arXiv:2508.17734) supports layerwise importance variations.
- **Break condition:** If storage latency exceeds computation savings, or if domain shift causes frequent cache misses on previously rare tokens.

## Foundational Learning

- **Concept: Residual Stream Architecture**
  - Why needed here: MemoryLLM's core innovation is decoupling FFNs from the residual stream; understanding how conventional transformers accumulate information is prerequisite.
  - Quick check question: Can you explain why Equation 2.6 (X_{L+1} = X_L + Attn(X_L) + FFN(X_0)) differs from conventional transformer residual flow?

- **Concept: SwiGLU Feed-Forward Networks**
  - Why needed here: The TKV framework interprets SwiGLU components (W_Up, W_Gate, W_Down) as keys, reweighting function, and values respectively.
  - Quick check question: Which matrix in SwiGLU would you modify to suppress activation of specific memory cells?

- **Concept: Key-Value Memory in Neural Networks**
  - Why needed here: MemoryLLM explicitly frames FFNs as neural retrieval memory; prior work (Geva et al.) provides theoretical foundation.
  - Quick check question: In the TKV framework, what determines which "keys" are activated for a given token query?

## Architecture Onboarding

- **Component map:**
  ```
  Input Tokens → Embedding Layer (X₀)
                    ↓
         ┌─────────┴─────────┐
         ↓                   ↓
  Self-Attention          LayerNorm_L
  (on X_L from            (on X₀, per-layer)
   residual)                   ↓
         ↓                   FFN_L
    Add to X_L                  ↓
         ↓              Add to X_L
         └─────────┬─────────┘
                   ↓
              X_{L+1} (Residual Stream)
  ```
  Note: All FFN_L modules receive the same X̂₀ input; attention receives layer-specific X_L.

- **Critical path:** Token ID → Embedding lookup → LayerNorm → FFN (or ToL lookup) → Add to residual. The parallel path (attention on evolving residual) remains unchanged from conventional transformers.

- **Design tradeoffs:**
  - **MemoryLLM vs Base:** ~40% reduction in active parameters, but perplexity gap (Table 2: MemoryLLM-750M PPL 22.08 vs Base-750M PPL 19.73 on C4).
  - **Flex-MemoryLLM:** Splits FFN into FFN-C (context-aware, βh² params) + FFN-M (context-free, pre-computable). β=3 closely matches base performance with 704M/1208M active params.
  - **Assumption:** Storage bandwidth and latency are acceptable for ToL retrieval; paper does not benchmark real-world inference latency with storage access.

- **Failure signatures:**
  - Retrieval-heavy tasks (LAMBDA, Wikitext-2) degrade more than reasoning tasks (PIQA, HellaSwag) when FFN contribution α is reduced (Table 1).
  - Performance collapse if middle-layer ToLs are dropped (Figure 12 shows asymmetric layer importance).
  - Training instability if per-FFN LayerNorm is removed (Section 2.2.2 notes this "significantly helps in convergence").

- **First 3 experiments:**
  1. **Validate TKV clustering:** Train MemoryLLM-250M on C4 subset (5B tokens), compute c_k vectors for vocabulary, run K-means clustering, verify semantic clusters emerge (compare to Figure 5).
  2. **Measure retrieval vs reasoning sensitivity:** Reduce FFN contribution (α=0.7, 0.5) and compare performance drops on LAMBDA (retrieval) vs PIQA (reasoning); expect larger drops on retrieval tasks.
  3. **Profile ToL storage/access:** For MemoryLLM-750M, pre-compute ToLs, implement 4-bit quantization (Table 7), measure PPL degradation and storage reduction; simulate cache-miss latency impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-uniform compression techniques significantly improve the storage efficiency of Token-wise Lookups (ToLs) compared to uniform SVD?
- Basis in paper: [explicit] Appendix D observes that ToLs exhibit non-uniform low-rank properties across layers and explicitly calls for "sophisticated studies to capitalize these redundancies as future work."
- Why unresolved: The authors only evaluated uniform SVD compression; the potential gains from adaptive rank reduction remain unquantified.
- What evidence would resolve it: Benchmarks comparing storage size versus perplexity using non-uniform rank reduction algorithms (e.g., different ranks per layer).

### Open Question 2
- Question: Does the interpretable clustering of keys in MemoryLLM enable precise knowledge editing or toxicity suppression?
- Basis in paper: [explicit] Section 2.3.1 states that the spatial clustering of semantically similar tokens "provides opportunities for researchers to explore knowledge editing and injection, or toxicity suppression."
- Why unresolved: The paper analyzes the spatial distribution of memory but does not conduct experiments to validate interventions on specific keys.
- What evidence would resolve it: Successful targeted modification of model behavior by editing specific key clusters without degrading general perplexity.

### Open Question 3
- Question: How does the ratio of context-free FFN memory to context-aware compute (FFN-C) impact training dynamics and convergence speed?
- Basis in paper: [explicit] Section 4.2 notes that the results "encourages future studies to closely investigate the over-parameterization ratio of FFNs... and its relationship with training dynamics."
- Why unresolved: The study focuses on final perplexity and downstream task performance rather than analyzing the underlying training curves or convergence stability.
- What evidence would resolve it: A detailed comparison of training loss trajectories and gradient statistics for Flex-MemoryLLM models with varying β configurations.

## Limitations

- The architecture assumes token-level knowledge is sufficient for most tasks, but has not been tested on complex reasoning or multi-step inference tasks
- Real-world inference latency with storage access is not benchmarked, leaving practical deployment uncertainty
- The TKV interpretability framework depends on SwiGLU architecture specifics that may not generalize to other FFN variants

## Confidence

**High Confidence (Well-supported by direct evidence):**
- FFNs can be trained independently of self-attention using context-free token embeddings while maintaining reasonable performance
- Pre-computed ToLs enable plug-n-play memory transfer and layer dropping with predictable perplexity impacts
- Flex-MemoryLLM can closely match conventional LLM performance while reducing active parameters by ~40%
- FFNs exhibit layerwise importance variations, with later layers being more dispensable

**Medium Confidence (Reasonable but requires additional validation):**
- The TKV framework provides meaningful interpretability of FFN memory through semantic clustering
- Retrieval-heavy tasks are more sensitive to FFN contribution reduction than reasoning tasks
- Storage efficiency gains from ToLs offset computational overhead in practical deployments

**Low Confidence (Limited evidence or unexplored):**
- MemoryLLM architecture generalizes to complex reasoning, mathematical, or multi-step inference tasks
- The TKV interpretability extends meaningfully to larger models (1B+) and different architectures
- Real-world inference latency benefits materialize with practical storage implementations

## Next Checks

1. **Extend TKV interpretability to 1B-scale models:** Replicate the K-means clustering analysis from Figure 5 on MemoryLLM-1B trained on full C4, examining whether semantic clusters remain interpretable at scale and whether key activation patterns change qualitatively.

2. **Benchmark real-world inference latency:** Implement ToL storage with practical caching strategies (e.g., LRU for frequent tokens) and measure end-to-end inference latency including storage access times, comparing against conventional transformer inference on equivalent hardware.

3. **Test on complex reasoning tasks:** Evaluate MemoryLLM and Flex-MemoryLLM on challenging reasoning benchmarks like GSM8K, MATH, and BigBench, analyzing whether the context-free FFN limitation becomes prohibitive for multi-step inference and identifying which reasoning patterns fail first.