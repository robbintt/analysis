---
ver: rpa2
title: Steerable Adversarial Scenario Generation through Test-Time Preference Alignment
arxiv_id: '2509.20102'
source_url: https://arxiv.org/abs/2509.20102
tags:
- adversarial
- reward
- real
- preference
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SAGE reframes adversarial scenario generation as a test-time preference
  alignment problem. Instead of training a single model for a fixed adversarial-realism
  trade-off, it fine-tunes two expert models on opposing preferences and interpolates
  their weights at inference.
---

# Steerable Adversarial Scenario Generation through Test-Time Preference Alignment

## Quick Facts
- arXiv ID: 2509.20102
- Source URL: https://arxiv.org/abs/2509.20102
- Reference count: 40
- Key outcome: Achieves higher attack success rates with lower realism penalties and map violations than state-of-the-art baselines

## Executive Summary
SAGE reframes adversarial scenario generation as a test-time preference alignment problem. Instead of training a single model for a fixed adversarial-realism trade-off, it fine-tunes two expert models on opposing preferences and interpolates their weights at inference. This enables continuous, steerable control over the balance between adversariality and realism without retraining. Experiments show SAGE achieves higher attack success rates while maintaining significantly lower realism penalties and map violations than state-of-the-art baselines, and it enables more effective closed-loop training of robust driving policies.

## Method Summary
SAGE employs hierarchical preference optimization (HGPO) to fine-tune two expert models from a pretrained motion predictor: one favoring adversariality (w_adv=0.9, w_real=0.1) and one favoring realism (w_adv=0.1, w_real=0.9). At inference, these experts are combined via linear weight interpolation θ(λ) = (1-λ)θ_real + λθ_adv, where λ ∈ [0,1] controls the adversariality-realism trade-off. HGPO uses a two-level hierarchy: first ensuring feasibility (map compliance) is strictly preferred over infeasibility, then ranking feasible trajectories by preference rewards. The framework also incorporates a dual-axis curriculum that progressively increases both adversarial intensity and frequency during closed-loop training of ego policies.

## Key Results
- Achieves higher attack success rates while maintaining significantly lower realism penalties than state-of-the-art baselines
- Enables continuous, steerable control over adversariality-realism trade-offs at inference without retraining
- Trained ego policies show improved robustness and generalization across diverse adversarial test sets

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decoupling of Hard Constraints from Soft Preferences
Separating feasibility constraints (map compliance) from preference trade-offs (adversariality vs realism) improves optimization stability and data efficiency compared to linear scalarization. HGPO uses a two-level hierarchy: (1) Feasibility First — any feasible trajectory is strictly preferred over infeasible ones; (2) Preference within Feasibility — among feasible trajectories, rank by preference reward with a margin δm. This is implemented via group sampling (N=32 candidates) to construct multiple preference pairs per scenario, maximizing information extraction.

### Mechanism 2: Linear Weight Interpolation for Test-Time Pareto Navigation
Interpolating between two fine-tuned expert models at inference enables continuous, steerable control over adversariality-realism trade-offs without retraining. Two experts are fine-tuned from the same pretrained π_ref with opposing mixed rewards. At test time, construct θ(λ) = (1-λ)θ_real + λθ_adv, then sample K candidates and rank by user's real-time R_μ. This traces the Pareto front by traversing a high-reward ridge in parameter space.

### Mechanism 3: Dual-Axis Curriculum for Closed-Loop Robustification
Progressively annealing both adversarial intensity (λ) and frequency (p_adv) during RL training produces more robust and generalizable driving policies while mitigating catastrophic forgetting. Start with λ=0.5, p_adv=0.1; linearly anneal to λ=1.0, p_adv=0.9 over training. This exposes the ego policy to an expanding spectrum of challenges, preventing overfitting to a fixed adversarial distribution.

## Foundational Learning

- **Concept: Linear Mode Connectivity (LMC)**
  - Why needed here: SAGE's core innovation rests on the premise that θ_adv and θ_real can be linearly connected without catastrophic performance degradation
  - Quick check question: Can you visualize the reward/loss landscape along the interpolation path between two fine-tuned models? Is it a flat basin, a convex ridge, or disconnected?

- **Concept: Direct Preference Optimization (DPO) and Bradley-Terry Preference Modeling**
  - Why needed here: HGPO extends DPO to hierarchical, group-based preferences; understanding the baseline helps diagnose why standard DPO is data-inefficient here
  - Quick check question: How does the Bradley-Terry model convert pairwise preferences into a reward function? What happens when preferences have inherent hierarchy (feasibility > preference)?

- **Concept: Pareto Fronts in Multi-Objective Optimization**
  - Why needed here: SAGE's goal is to navigate the Pareto front of adversariality vs realism at test time, not find a single optimal point
  - Quick check question: Can you sketch the Pareto front for a simple two-objective problem? Why does linear scalarization (weighted sum) fail to capture the full front?

## Architecture Onboarding

- **Component map:**
  Pretrained backbone (DenseTNT) -> HGPO fine-tuning module -> Weight interpolation layer -> Candidate sampling + ranking

- **Critical path:**
  1. Start with pretrained motion model (imitation learning on real data)
  2. Define R_adv, P_real, and binary feasibility F(τ,M) functions
  3. Fine-tune two experts with HGPO (opposing w_adv/w_real ratios)
  4. Validate LMC: plot reward landscape between experts (if disconnected, retrain with higher β regularization)
  5. Deploy with test-time λ control; validate monotonic transition in collision rate and realism penalty

- **Design tradeoffs:**
  - β (expert specialization): Higher β (→1.0) creates more specialized experts but narrows the optimal interpolation range [1-β, β]. Paper uses β∈(0.5, 1].
  - Group size N: Larger N improves data efficiency but increases computation. Paper uses N=32, K=8 pairs.
  - Margin δm: Prevents learning from noisy near-equal preferences. Paper uses δm=0.2.

- **Failure signatures:**
  - Non-monotonic transition curves (Fig. 4b) → experts may not be in same loss basin; check LMC
  - High map violation despite HGPO → feasibility function F(τ,M) may be too permissive
  - Unrealistic adversarial trajectories → P_real penalties underweighted; increase w_real during expert training

- **First 3 experiments:**
  1. **LMC validation:** Interpolate between θ_adv and θ_real in 0.1 increments; plot R_adv, R_real, and combined R_μ. Confirm concave reward curves (as in Fig. 6d) before proceeding.
  2. **Ablation on hierarchical decoupling:** Compare HGPO vs. (a) linear scalarization with map penalty, (b) standard DPO. Measure feasibility rate and preference reward convergence (Fig. 7a-b).
  3. **Steerability stress test:** For fixed scenario, vary λ∈[0,1] and measure collision rate vs. realism penalty. Confirm smooth Pareto frontier; compare against trajectory-space mixing and logit-space mixing baselines (Fig. 4a).

## Open Questions the Paper Calls Out

### Open Question 1
Can SAGE effectively scale to incorporate more than two competing objectives (e.g., scenario novelty, complexity) while maintaining test-time steerability through weight interpolation? The paper suggests this extension is possible but demonstrates only pairwise expert interpolation, which traces a 1D Pareto front. Higher-dimensional preference spaces would require interpolating among multiple experts, and it is unclear whether linear interpolation extends gracefully to trace Pareto surfaces or if non-linear merging strategies are needed.

### Open Question 2
Does the Linear Mode Connectivity assumption hold across more diverse or orthogonal fine-tuning objectives, or is it specific to the adversariality-realism trade-off studied here? LMC is known to depend on the similarity of fine-tuning tasks. If objectives induce gradients in substantially different directions or land in separate basins, linear interpolation may fail, breaking the suboptimality bound.

### Open Question 3
How does an automated curriculum that dynamically adapts adversarial scenarios based on real-time ego agent performance compare to the fixed dual-axis curriculum? The current curriculum anneals intensity and frequency on fixed schedules independent of actual ego performance. Adaptive curricula could accelerate learning but introduce instability if the ego agent is overfit to specific attack patterns.

## Limitations
- Relies on Linear Mode Connectivity assumption that may not hold across diverse architectures or training regimes
- Hierarchical formulation assumes binary feasibility constraints exist in the domain, which may not apply to tasks with purely continuous trade-offs
- Computational overhead of maintaining two full expert models at inference time

## Confidence
- **High**: The hierarchical decoupling of feasibility from preference improves optimization stability vs. linear scalarization (supported by Fig. 7c-d and ablation comparisons)
- **High**: Test-time weight interpolation enables steerable Pareto navigation without retraining (confirmed by concave reward landscapes in Fig. 6d)
- **Medium**: Dual-axis curriculum produces more robust policies (inferred from Table 4 results, but curriculum design choices appear somewhat heuristic)

## Next Checks
1. **LMC Robustness Test**: For 3 different backbone architectures (DenseTNT, vanilla Transformer, MLP), fine-tune two experts with opposing preferences and measure reward along interpolation path. Report % of architectures showing connected basins vs. disconnected.
2. **Feasibility Boundary Stress**: Systematically relax the binary feasibility constraint F(τ,M) by introducing soft margins. Measure at what point the hierarchical advantage disappears and performance matches linear scalarization.
3. **Curriculum Sensitivity Analysis**: Vary the annealing schedule (linear vs. exponential, different start/end points) and measure final policy robustness. Identify whether the specific dual curriculum is critical or if monotonic progression alone suffices.