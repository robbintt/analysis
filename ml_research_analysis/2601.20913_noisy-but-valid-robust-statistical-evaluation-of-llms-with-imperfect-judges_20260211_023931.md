---
ver: rpa2
title: 'Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges'
arxiv_id: '2601.20913'
source_url: https://arxiv.org/abs/2601.20913
tags:
- judge
- hypothesis
- testing
- noisy
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hypothesis testing framework for certifying
  LLM reliability using noisy LLM judges. By modeling judge error profiles (TPR/FPR)
  on a small human-labeled set and incorporating these estimates into a variance-corrected
  test statistic on a large judge-labeled dataset, the method guarantees finite-sample
  Type-I error control despite judge imperfections.
---

# Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges

## Quick Facts
- **arXiv ID**: 2601.20913
- **Source URL**: https://arxiv.org/abs/2601.20913
- **Reference count**: 40
- **Primary result**: Hypothesis testing framework that certifies LLM reliability using noisy LLM judges while guaranteeing Type-I error control

## Executive Summary
This paper addresses the challenge of evaluating large language models when using imperfect LLM judges instead of expensive human evaluation. The authors develop a statistical framework that accounts for judge error profiles (true positive rate and false positive rate) to enable robust hypothesis testing. By combining a small human-labeled dataset for error estimation with a larger judge-labeled dataset, the method achieves finite-sample Type-I error control guarantees despite judge imperfections. The approach effectively bridges the gap between automatic evaluation's scalability and human evaluation's reliability.

## Method Summary
The framework employs a two-stage approach: first estimating judge error profiles (TPR and FPR) on a small human-labeled dataset, then incorporating these estimates into a variance-corrected test statistic applied to a larger judge-labeled dataset. The method models judge behavior through an error profile matrix and uses this to adjust the statistical test, ensuring Type-I error control even with imperfect judges. The theoretical analysis provides conditions under which noisy testing outperforms direct human testing, depending on the relative sizes of the two datasets and the quality of the judges. The framework is validated across synthetic experiments, classification tasks, and generative evaluation tasks.

## Key Results
- Theoretical guarantees for finite-sample Type-I error control despite judge imperfections
- Mathematical conditions showing when noisy testing outperforms direct human testing based on judge quality and dataset sizes
- Experimental validation across synthetic, classification, and generative tasks demonstrating framework utility
- Quantitative assessment of the performance gap between practical methods and an idealized "Oracle" baseline

## Why This Works (Mechanism)
The method works by explicitly modeling and accounting for judge error characteristics rather than treating judge judgments as ground truth. By estimating TPR and FPR on a calibration set with human labels, the framework can correct for systematic biases in judge evaluations. The variance correction in the test statistic accounts for the uncertainty introduced by using noisy judges, allowing for valid statistical inference. This two-stage approach leverages the scalability of LLM judges while maintaining statistical rigor through explicit error modeling.

## Foundational Learning
- **Type-I error control**: The probability of falsely rejecting a true null hypothesis; critical for maintaining statistical validity when using noisy judges
- **TPR/FPR estimation**: True positive rate and false positive rate estimation from calibration data; needed to characterize judge behavior
- **Variance correction**: Adjusting statistical test statistics to account for noise in judge evaluations; essential for valid inference
- **Hypothesis testing with imperfect labels**: Statistical framework for drawing conclusions from noisy data; addresses fundamental challenge in automatic evaluation
- **Tradeoff analysis**: Mathematical conditions for when noisy evaluation outperforms human evaluation; guides practical deployment decisions
- **Error profile modeling**: Characterizing judge behavior through error matrices; enables systematic correction of judge biases

## Architecture Onboarding
**Component map**: Human-labeled dataset -> Error profile estimation -> Judge-labeled dataset -> Variance-corrected test statistic -> Statistical conclusion
**Critical path**: Small human-labeled set → TPR/FPR estimation → Large judge-labeled set → Adjusted hypothesis test → Result
**Design tradeoffs**: The framework trades off between the cost of human labeling (for calibration) and the accuracy of judge error estimates. Using a smaller human-labeled set reduces costs but increases uncertainty in error profile estimation, while larger judge-labeled sets improve statistical power but may amplify systematic judge biases if not properly corrected.
**Failure signatures**: Performance degrades when judge errors are highly correlated with the evaluation target, when the human-labeled calibration set is not representative of the judge-labeled data distribution, or when the iid assumption on judge errors is severely violated.
**First experiments to run**:
1. Synthetic experiments validating Type-I error control under known judge error profiles
2. Cross-task evaluation to assess framework robustness across different judge populations
3. Ablation study varying the size ratio between human-labeled and judge-labeled datasets

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the limitations section.

## Limitations
- Assumes independent and identically distributed judge errors, which may not hold with systematic biases or context-dependent performance variations
- Requires a separate labeled dataset for estimating TPR/FPR parameters, which may not always be available or representative
- Effectiveness depends on accurate judge error estimates, with the current approach assuming constant error rates across input types
- Potential violation of iid assumptions in real-world scenarios where judge behavior varies systematically

## Confidence
- **High confidence**: Type-I error control guarantees under stated assumptions
- **Medium confidence**: Practical performance due to potential violation of iid assumptions
- **Medium confidence**: Synthetic experiment results as they validate theoretical properties
- **Low confidence**: Real-world judge error estimation accuracy without extensive validation

## Next Checks
1. Conduct systematic evaluation of judge error dependency on input characteristics (e.g., length, complexity, domain) to test the iid assumption
2. Validate the framework across diverse judge populations and task types to assess generalizability
3. Implement cross-validation of judge error estimates using multiple small human-labeled datasets to assess stability of the error profile estimation process