---
ver: rpa2
title: The Impact of Role Design in In-Context Learning for Large Language Models
arxiv_id: '2509.23501'
source_url: https://arxiv.org/abs/2509.23501
tags:
- prompt
- user
- arxiv
- plot
- genre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of role design in in-context
  learning (ICL) for large language models (LLMs). It systematically evaluates five
  prompt configurations (ZeroU, ZeroSU, FewU, FewSU, FewSUA) across diverse NLP tasks
  and LLMs including GPT-3.5, GPT-4o, Llama2-7b, and Llama2-13b.
---

# The Impact of Role Design in In-Context Learning for Large Language Models

## Quick Facts
- arXiv ID: 2509.23501
- Source URL: https://arxiv.org/abs/2509.23501
- Authors: Hamidreza Rouzegar; Masoud Makrehchi
- Reference count: 4
- Primary result: Role-based prompt design (FewSUA) generally improves structural accuracy and performance, with task-specific strategies needed for complex reasoning.

## Executive Summary
This study investigates how different role-based prompt designs affect in-context learning performance across various large language models. The researchers evaluate five prompt configurations (ZeroU, ZeroSU, FewU, FewSU, FewSUA) on multiple NLP tasks using GPT-3.5, GPT-4o, Llama2-7b, and Llama2-13b. The results show that incorporating clear role distinctions with examples (FewSUA) generally enhances model performance and structural accuracy, though complex reasoning tasks like math often benefit more from explanation-based approaches than strict structural adherence.

## Method Summary
The study evaluates five prompt configurations (ZeroU, ZeroSU, FewU, FewSU, FewSUA) across five NLP datasets using four language models. For few-shot settings, three examples are randomly selected per dataset. The primary metric is F1 score, with structural accuracy measuring format compliance. Temperature is set to 0.0 for all experiments. The MATH dataset includes four additional prompt refinements (Basic, Specialized, Explanation, Reasoning-First) applied to each of the five role configurations. Post-processing extracts labels to compute F1 scores regardless of format adherence.

## Key Results
- FewSUA configuration consistently improves structural accuracy, particularly for smaller Llama models
- For complex reasoning tasks like math, prompts allowing explanations (Reasoning-First) often outperform strict structural adherence
- Task-specific and model-specific prompt engineering strategies are necessary, with optimal designs varying significantly between task types and model architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring prompts with explicit turn-taking (FewSUA) forces better output formatting and structural adherence, particularly in smaller models.
- **Mechanism:** The Few-shot System, User, and Assistant (FewSUA) configuration presents examples as a conversational history (User input → Assistant output). This leverages the model's next-token prediction auto-regressive nature; by establishing a pattern of "Question → Short Answer," the model statistically favors continuing this pattern over generating verbose, unstructured text.
- **Core assumption:** The model's instruction-tuning (RLHF) heavily weights the "Assistant" role as the signal to cease explanation and provide the final completion.
- **Evidence anchors:**
  - [abstract] "incorporating clear role distinctions with examples (FewSUA) generally enhances model performance and structural accuracy."
  - [section 5.1] "FewSUA prompt configuration helped the Llama models generate outputs in the desired structure... The structural accuracy of GPT models was generally high... However, the Llama models frequently generated more than one word... A standout finding is that the FewSUA prompt configuration helped the Llama models generate outputs in the desired structure."
  - [corpus] Weak/missing direct support; corpus neighbors focus on theoretical ICL generalization rather than role-specific formatting (e.g., "Towards Auto-Regressive Next-Token Prediction").
- **Break condition:** If the task requires open-ended generation rather than classification or short-form extraction, enforcing strict turn-taking may truncate necessary nuance.

### Mechanism 2
- **Claim:** Allowing "Reasoning-First" output generation (Chain-of-Thought) improves accuracy on complex tasks (Math) by creating intermediate computation steps.
- **Mechanism:** Complex reasoning requires "computational depth." By instructing the model to explain reasoning *before* giving the final answer (Reasoning-First), the model generates tokens that serve as scratchpad/working memory. This reduces the cognitive load of holding intermediate states in hidden layers, thereby reducing error rates compared to direct answer prediction.
- **Core assumption:** The model has sufficient internal knowledge to solve the step but fails to map it directly to a single token label without intermediate verbalization.
- **Evidence anchors:**
  - [abstract] "For complex reasoning tasks like math, prompts allowing for explanations often outperform those strictly adhering to structural accuracy."
  - [section 5.2] "configurations with lower structural accuracy often yielded better performance... better-performing answers often included explanations and reasoning rather than just a single-letter response."
  - [corpus] Consistent with "SkillGen" (neighbor) noting prompts should provide "step-level granularity," though this paper specifically links it to the "Reasoning-First" role design.
- **Break condition:** If the model is too small (e.g., Llama2-7b), it may hallucinate during the reasoning step or fail to ever converge on a final answer.

### Mechanism 3
- **Claim:** Separating high-level instructions into the "System" role improves stability and task focus compared to embedding everything in the "User" role.
- **Mechanism:** The "System" role acts as a persistent context or "state" for the interaction, whereas the "User" role is treated as transient input. By isolating the task instruction (System) from the data (User), the model is less likely to confuse the instructions with the content (e.g., classifying the instruction text itself).
- **Core assumption:** The model architecture processes the System token with higher priority or persistence than User tokens.
- **Evidence anchors:**
  - [section 3.2] "In role design, the 'system' provides high-level instructions that guide the overall task... This includes specifying the format, the rules for the responses..."
  - [table 3] ZeroSU (System+User) generally matches or exceeds ZeroU (User only) in F1 scores, particularly for GPT models (e.g., GPT-3.5 on commonsense_qa: ZeroU 68 vs ZeroSU 68; ai2_arc: ZeroU 76 vs ZeroSU 80).
  - [corpus] Weak support; corpus papers discuss ICL emergence broadly but do not specificy System vs User token attention weights.
- **Break condition:** In models without specific instruction tuning for chat formats (e.g., base models), the "System" role may be ignored or treated as regular text.

## Foundational Learning

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** The entire paper is predicated on the ability of LLMs to learn tasks from prompt demonstrations (Zero/Few-shot) without weight updates.
  - **Quick check question:** Can you explain why ICL requires no gradient descent during inference?
- **Concept:** **Structural Accuracy**
  - **Why needed here:** This is a novel metric introduced in the paper to measure *format compliance* (e.g., outputting just "A" vs "The answer is A"). It is distinct from F1 score (correctness) and crucial for evaluating Llama2's failures.
  - **Quick check question:** If a model outputs the correct answer but surrounds it with conversational filler, is the F1 score or Structural Accuracy affected?
- **Concept:** **Chain-of-Thought (CoT) / Reasoning-First**
  - **Why needed here:** The paper finds that for Math, the "Reasoning-First" approach (a variant of CoT) is superior to "Basic" prompting.
  - **Quick check question:** Why does forcing an explanation before the final answer improve performance on math problems?

## Architecture Onboarding

- **Component map:**
  - ZeroU / ZeroSU: No examples provided. Baseline for model "knowledge."
  - FewU / FewSU: 3 examples provided inline or in system context. Tests simple pattern matching.
  - FewSUA: 3 examples provided as conversational turns (User/Assistant). High structural enforcement.
  - Adaptive Math Layers: 4 variations (Basic, Specialized, Explanation, Reasoning-First) applied to the above 5 structures.

- **Critical path:**
  1. Identify task type (Classification/QA vs. Reasoning/Math).
  2. If Classification/QA: Route to **FewSUA** for highest Structural Accuracy + F1.
  3. If Reasoning/Math: Route to **Reasoning-First** configuration (sacrificing structural accuracy for F1 score).
  4. Check model scale: If using Llama2-7b/13b, strictly avoid FewU/FewSU (prone to hallucination) and stick to FewSUA or ZeroSU.

- **Design tradeoffs:**
  - **Format vs. Accuracy:** You generally cannot maximize both strict structural accuracy (single letter output) and reasoning accuracy (explanations) simultaneously for complex tasks.
  - **Model Scale vs. Role Complexity:** Smaller models (Llama2-7b) fail to follow roles in mixed prompts (FewU) but succeed when roles are strictly delineated (FewSUA).

- **Failure signatures:**
  - **The "Verbose Llama":** Llama2 models outputting full sentences ("The answer is...") instead of single tokens when not using FewSUA.
  - **The "Flip-Flop":** In Math "Explanation" modes, models giving an answer, explaining it, and then changing the answer in the final token (e.g., "a... [explanation]... actually b").
  - **Zero-shot Drift:** ZeroU prompts causing the model to drift into generating new questions rather than answering them.

- **First 3 experiments:**
  1. **Verify FewSUA dominance:** Run FewSUA vs. FewU on a classification task using Llama2-7b to measure the delta in Structural Accuracy (expect near 0% vs 60-100%).
  2. **Test Reasoning Tradeoff:** Run Basic Math vs. Reasoning-First on GPT-3.5. Confirm that Structural Accuracy drops (100% -> N/A) while F1 Score rises.
  3. **Role Isolation Test:** Compare ZeroU vs. ZeroSU on a sensitive QA task to quantify the "System" role's impact on instruction following.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do role-based prompt strategies transfer effectively to non-instruction-tuned base models?
- Basis in paper: [explicit] The authors state, "Investigating how these role-based strategies translate to non-instruction-tuned language models... could yield valuable insights."
- Why unresolved: This study exclusively evaluated instruction-tuned models (GPT-3.5, GPT-4o, Llama-Chat), leaving the impact on base models unknown.
- What evidence would resolve it: A comparison of FewSUA performance on base models (e.g., Llama Base) versus their instruction-tuned counterparts across the same datasets.

### Open Question 2
- Question: Can defining novel, task-specific roles beyond the standard system/user/assistant hierarchy further optimize performance?
- Basis in paper: [explicit] The paper suggests, "Future studies could explore creating new, task-specific roles to further optimize prompt designs."
- Why unresolved: The experiments were limited to pre-defined conversational roles (System, User, Assistant) provided by the model frameworks.
- What evidence would resolve it: Empirical results from prompts utilizing custom roles (e.g., "Analyst," "Critic") compared against the standard FewSUA baseline.

### Open Question 3
- Question: How does the trade-off between structural accuracy and reasoning performance manifest in generative tasks like summarization?
- Basis in paper: [inferred] The authors note that strict structural adherence reduced accuracy in math reasoning, and explicitly call for expanding experiments to "summarization and translation."
- Why unresolved: It is unclear if the negative correlation observed between structure and reasoning in math applies to complex generative NLP tasks.
- What evidence would resolve it: Experiments measuring F1 and structural accuracy on summarization tasks using strict FewSUA versus explanation-heavy prompt designs.

## Limitations

- Limited model scope: Only four LLMs tested (GPT and Llama2), leaving gaps in understanding performance across other model families
- Restricted task variety: Five specific NLP tasks may not generalize to broader domains or multimodal applications
- Novel metric validation: Structural accuracy lacks rigorous validation against human judgments of format compliance

## Confidence

- **High Confidence:** The observation that FewSUA configuration consistently improves structural accuracy for Llama models across tasks.
- **Medium Confidence:** The finding that Reasoning-First approaches improve math performance at the cost of structural accuracy.
- **Low Confidence:** The claim that task-specific prompt engineering is necessary for optimal performance.

## Next Checks

1. **Cross-Model Generalization:** Test the five prompt configurations on additional LLM families (Claude, Gemini, Mistral) to verify if the observed performance patterns hold across different architectural designs and training approaches.
2. **Structural Accuracy Validation:** Conduct human evaluation studies to validate the structural accuracy metric, ensuring that format compliance measurements align with human judgments of prompt-following behavior.
3. **Task Boundary Exploration:** Systematically test prompt performance across a broader range of task types (beyond the five studied) to establish more definitive guidelines for when to prioritize structural accuracy versus reasoning depth in prompt design.