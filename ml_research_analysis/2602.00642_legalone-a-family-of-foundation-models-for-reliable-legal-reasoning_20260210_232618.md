---
ver: rpa2
title: 'LegalOne: A Family of Foundation Models for Reliable Legal Reasoning'
arxiv_id: '2602.00642'
source_url: https://arxiv.org/abs/2602.00642
tags:
- legal
- reasoning
- data
- legalone
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LegalOne, a family of foundation models specifically
  designed for the Chinese legal domain. The authors address the challenge of applying
  general-purpose LLMs to legal tasks, which often lack precise domain knowledge and
  struggle with rigorous multi-step judicial reasoning.
---

# LegalOne: A Family of Foundation Models for Reliable Legal Reasoning

## Quick Facts
- arXiv ID: 2602.00642
- Source URL: https://arxiv.org/abs/2602.00642
- Reference count: 40
- Primary result: LegalOne achieves superior parameter efficiency, with LegalOne-8B surpassing significantly larger general-purpose models on Chinese legal benchmarks.

## Executive Summary
This paper introduces LegalOne, a family of foundation models specifically designed for the Chinese legal domain. The authors address the challenge of applying general-purpose LLMs to legal tasks, which often lack precise domain knowledge and struggle with rigorous multi-step judicial reasoning. LegalOne employs a three-phase training pipeline: mid-training with Plasticity-Adjusted Sampling (PAS) to inject domain knowledge while mitigating catastrophic forgetting, supervised fine-tuning with Legal Agentic CoT Distillation (LEAD) to internalize expert reasoning patterns, and curriculum-based reinforcement learning to enhance autonomous legal reasoning. LegalOne demonstrates state-of-the-art performance across legal benchmarks, achieving superior accuracy with fewer parameters compared to larger general-purpose models. The LegalOne-8B model, for example, achieves 71.19% average accuracy on LexEval, surpassing significantly larger models. The authors release the LegalOne weights and LegalKit evaluation framework to advance research in legal AI.

## Method Summary
LegalOne employs a three-phase training pipeline to develop specialized legal reasoning capabilities. First, mid-training uses Plasticity-Adjusted Sampling (PAS) with a perplexity-based scheduler to inject legal knowledge into pre-trained models while preserving general capabilities. Second, supervised fine-tuning applies the Legal Agentic CoT Distillation (LEAD) pipeline, which converts complex judicial processes into structured reasoning trajectories through an agentic workflow. Finally, curriculum-based reinforcement learning refines reasoning through progressive stages from memorization to complex document generation. The model is evaluated using the LegalKit framework on benchmarks like LexEval and JEC-QA.

## Key Results
- LegalOne-8B achieves 71.19% average accuracy on LexEval, surpassing significantly larger general-purpose models
- The model demonstrates superior parameter efficiency, achieving state-of-the-art performance with fewer parameters
- LegalOne shows effective mitigation of catastrophic forgetting during domain adaptation
- The curriculum-based RL approach successfully enhances multi-step legal reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1: Perplexity-Guided Data Scheduling for Catastrophic Forgetting Mitigation
- Claim: Coordinating data sampling with learning rate schedules stabilizes domain adaptation.
- Mechanism: Plasticity-Adjusted Sampling (PAS) uses perplexity to partition data into "familiar" (low-PPL) and "novel" (high-PPL) buckets. During learning rate re-warmup, sampling concentrates on low-PPL data to act as a numerical anchor, preventing optimization shock. As the learning rate stabilizes, the distribution smoothly transitions to include more novel legal data.
- Core assumption: Perplexity serves as a reliable proxy for the model's original training distribution when pre-training data is unavailable.
- Evidence anchors:
  - [abstract] "...Perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities..."
  - [section 2.2.1] "...low-PPL samples serve as a functional proxy for the unknown pre-training data..." & "...low-PPL data acts as numerical damper, stabilizing the training process..."
  - [corpus] Limited direct evidence; related work notes challenges in LLM robustness under domain shifts (arXiv:2503.18360).
- Break condition: Fails if low-perplexity data is not representative of the original knowledge manifold, or if the learning rate schedule changes abruptly without adjusting the data schedule.

### Mechanism 2: Agentic Workflow Distillation for Structured Reasoning
- Claim: Decomposing complex legal reasoning into an explicit, multi-stage agent workflow grounds model outputs in verifiable logic.
- Mechanism: The LEAD pipeline converts raw legal texts into structured reasoning trajectories (e.g., Fact Finding → Issue Identification → Rule Retrieval → Rule Deduction → Conclusion Derivation). This process enforces factual grounding via external knowledge base retrieval and logical rigor via theoretical templates (e.g., judicial syllogism). The resulting explicit chains-of-thought are then distilled into the student model.
- Core assumption: Implicit logic in judicial documents can be reliably reconstructed and formalized into structured, learnable steps.
- Evidence anchors:
  - [abstract] "...utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories..."
  - [section 3.2] "...every node is supported by a comprehensive Cognitive Scaffold... Expert Protocols, Legal Knowledge Base, Theoretical Templates."
  - [corpus] Related work (arXiv:2509.00783, arXiv:2508.12281) supports using structured chains to guide legal generation and reasoning.
- Break condition: Fails if the agentic workflow's decomposition is too rigid for diverse case types or if the teacher model lacks the legal expertise to generate accurate intermediate steps.

### Mechanism 3: Curriculum Reinforcement Learning for Autonomous Reasoning
- Claim: A progressive RL curriculum shifts model behavior from pattern matching to reliable, multi-step legal reasoning.
- Mechanism: A five-stage curriculum moves from verifiable tasks (e.g., statute memorization with ROUGE rewards) to complex generation (e.g., legal document drafting with rubric-based rewards). A token-level baseline reduces variance in advantage estimation, stabilizing policy updates. This progression is designed to foster an internalized "legal mentality."
- Core assumption: Skills learned in early, verifiable stages transfer to later, more complex and subjective legal reasoning tasks.
- Evidence anchors:
  - [abstract] "...progressive reinforcement process spanning memorization, understanding, and reasoning..."
  - [section 4.2.3] The variance reduction analysis proves a token-level baseline is provably lower-variance than a sequence-level baseline.
  - [corpus] Reinforcement learning is a noted method for enhancing LLM reasoning in legal contexts (arXiv:2508.12281).
- Break condition: Fails if the reward signals are misaligned with legal validity or if the curriculum progression is too fast, causing the model to get stuck in local optima.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: The primary challenge in mid-training is injecting new, dense legal knowledge into a pre-trained model without overwriting its general language abilities. PAS is explicitly designed to mitigate this.
  - Quick check question: Can you explain why training only on new legal data might make a model forget common words or general reasoning?

- Concept: **Knowledge Distillation**
  - Why needed here: The SFT stage uses the LEAD pipeline to distill reasoning patterns from a stronger teacher model (and structured workflows) into the smaller LegalOne student model.
  - Quick check question: How does the Knowledge Internalization step in LEAD differ from simply copying a teacher model's output?

- Concept: **Policy Gradient (RL) & Advantage Estimation**
  - Why needed here: The RL stage uses the DAPO algorithm and introduces a novel token-level baseline to optimize the model's generation policy, making its reasoning more reliable.
  - Quick check question: Why is reducing variance in the advantage function critical for stable reinforcement learning?

## Architecture Onboarding

- Component map:
  1. Mid-Training Stage: Pre-trains on a ~100B token corpus using Plasticity-Adjusted Sampling (PAS) to inject legal knowledge while preserving general capabilities.
  2. Supervised Fine-Tuning (SFT) Stage: Uses the Legal Agentic CoT Distillation (LEAD) pipeline to train on 500k high-quality, structured reasoning trajectories.
  3. Reinforcement Learning (RL) Stage: Employs a five-stage curriculum with the DAPO algorithm and a token-level baseline to refine reasoning.
  4. Evaluation: Assessed using the LegalKit framework on LexEval and JEC-QA benchmarks.

- Critical path: A new engineer should first understand the PAS scheduler's perplexity-based logic, then the LEAD pipeline's workflow decomposition, and finally the RL curriculum's reward design.

- Design tradeoffs:
  - **PAS Complexity vs. Stability**: Implementing PAS adds scheduling complexity but is claimed to be necessary to avoid optimization shock during learning rate re-warmup.
  - **Synthetic Data Quality vs. Scale**: The LEAD pipeline generates high-quality structured data, but it is computationally expensive compared to naive distillation.
  - **Legal vs. General Capability**: The paper explicitly deprioritizes some general skills (e.g., advanced coding) to maximize legal reasoning, accepting a slight performance trade-off in smaller models.

- Failure signatures:
  - Validation loss on general data increases during mid-training, indicating catastrophic forgetting (PAS may not be working).
  - The model generates fluent but legally nonsensical text, suggesting the agentic workflow or RL reward signals are misaligned.
  - RL training becomes unstable (high loss variance), potentially indicating an issue with the token-level baseline implementation.

- First 3 experiments:
  1. Ablation on PAS: Train a model variant with a static data mixture (no PAS) and compare its performance on both legal (LexEval) and general (C-Eval) benchmarks against the PAS-trained model.
  2. Evaluate LEAD Data Quality: Use the LegalKit evaluator to assess a model fine-tuned on raw agentic traces vs. one fine-tuned on the refined, converged trajectories from the LEAD pipeline.
  3. Token vs. Sequence Baseline: In a simplified RL setup, compare the training stability and final performance of a model trained with the proposed token-level baseline against one using a standard sequence-level baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the positive transfer from domain-specific legal reinforcement learning to general reasoning capabilities be systematically characterized and predicted across different domains?
- Basis in paper: [explicit] The authors state: "Despite using exclusively legal data for RL training, the model's general reasoning improved unexpectedly... This inspiring finding motivates future research into how domain-specific RL can enhance general intelligence by refining fundamental reasoning skills."
- Why unresolved: The mechanism behind this cross-domain transfer is hypothesized but not proven; the paper only observes the phenomenon without theoretical explanation.
- What evidence would resolve it: Controlled experiments mapping which legal reasoning patterns (e.g., syllogisms, multi-step analysis) transfer to which general tasks, combined with mechanistic interpretability analysis of shared neural circuits.

### Open Question 2
- Question: Does a distinct scaling law exist for legal reasoning during reinforcement learning, and how does it differ from general LLM scaling relationships?
- Basis in paper: [explicit] The ablation study notes: "We observed a significant trend where larger LLMs (e.g., 8B) achieved substantially higher performance gains during the RL phase... This finding suggests the existence of a scaling Law specific to legal reasoning within the RL stage."
- Why unresolved: Only three model sizes were tested; the relationship between model scale and RL gains for legal reasoning has not been formally characterized.
- What evidence would resolve it: Systematic experiments across a wider range of model sizes (e.g., 1B to 70B) with controlled RL training, fitting scaling law parameters specifically for legal reasoning performance.

### Open Question 3
- Question: What is the minimum effective parameter capacity required to accommodate both specialized legal knowledge injection and preservation of general capabilities without degradation?
- Basis in paper: [explicit] The paper observes: "We attribute this to the 'capacity bottleneck', where limited parameter space struggles to accommodate both new domain knowledge and existing general knowledge simultaneously."
- Why unresolved: The trade-off curve between domain specialization and general capability retention at different scales remains unquantified.
- What evidence would resolve it: Experiments varying both model size and domain data ratio, measuring the Pareto frontier of legal vs. general benchmark performance to identify critical capacity thresholds.

## Limitations
- The detailed specification of the agentic workflow within the LEAD pipeline is not fully provided, making exact replication challenging
- The model's performance is evaluated only on Chinese-language legal corpora and benchmarks
- The paper lacks extensive ablation studies for PAS scheduler parameters and comprehensive error analysis of failure modes

## Confidence
- **High confidence**: The overall three-stage training pipeline (PAS mid-training → LEAD SFT → RL fine-tuning) and the claimed performance gains on LexEval and JEC-QA are well-supported by the described methodology and results.
- **Medium confidence**: The specific implementation details of the PAS scheduler's perplexity-based logic and the exact prompts for the LLM-as-Judge in the RL stages are described but not fully specified, requiring some engineering assumptions.
- **Low confidence**: The exact impact of each individual component (e.g., PAS vs. a static mix, LEAD vs. standard SFT) on the final performance cannot be independently verified without the full implementation details and a comprehensive ablation study.

## Next Checks
1. Validate PAS Stability: Conduct an ablation study comparing a model trained with the full PAS scheduler against a model trained with a static 4:6 (General:Legal) data mixture throughout mid-training. Monitor validation loss on both a legal benchmark (LexEval) and a general benchmark (C-Eval) to confirm PAS prevents catastrophic forgetting.
2. Test Agentic Workflow Grounding: Using the LegalKit evaluation framework, compare the legal reasoning accuracy of a model fine-tuned on data generated by the full LEAD pipeline against a model fine-tuned on the same data after the "Knowledge Internalization" step (i.e., with explicit references removed). This will validate if the grounding mechanism is effective.
3. Analyze RL Reward Alignment: For the Judgment Prediction and Document Generation stages of the RL curriculum, perform an adversarial check by creating a small set of test cases where the model's output is fluent but legally incorrect. Use the same LLM-as-Judge prompts to evaluate these cases and assess if the reward signals are truly aligned with legal validity.