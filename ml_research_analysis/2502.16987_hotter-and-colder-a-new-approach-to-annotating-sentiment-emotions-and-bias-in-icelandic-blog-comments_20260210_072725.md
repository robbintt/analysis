---
ver: rpa2
title: 'Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias
  in Icelandic Blog Comments'
arxiv_id: '2502.16987'
source_url: https://arxiv.org/abs/2502.16987
tags:
- comments
- tasks
- task
- annotators
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hotter and Colder, a dataset of 12,232 Icelandic
  blog comments annotated for 25 tasks including sentiment, emotions, hate speech,
  and group generalizations. The authors developed a two-phase annotation methodology
  that combines GPT-4o mini silver labels with targeted human verification to address
  class imbalance and improve annotation agreement.
---

# Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and Bias in Icelandic Blog Comments

## Quick Facts
- arXiv ID: 2502.16987
- Source URL: https://arxiv.org/abs/2502.16987
- Reference count: 8
- 12,232 Icelandic blog comments annotated for 25 tasks including sentiment, emotions, hate speech, and group generalizations

## Executive Summary
This paper introduces Hotter and Colder, a dataset of 12,232 Icelandic blog comments annotated for 25 tasks including sentiment, emotions, hate speech, and group generalizations. The authors developed a two-phase annotation methodology that combines GPT-4o mini silver labels with targeted human verification to address class imbalance and improve annotation agreement. The dataset was created by first using GPT-4o mini to label approximately 800,000 comments on a 5-point Likert scale, followed by manual verification by crowdworkers focusing on extreme cases. The resulting dataset shows high agreement for tasks like disgust (Krippendorff's alpha 0.92) and politeness (Cohen's kappa 0.80), though some tasks like mansplaining showed lower agreement. The authors release both the annotated dataset and annotation platform to support research in content moderation for low-resource languages.

## Method Summary
The authors employ a two-phase annotation methodology: first using GPT-4o mini to label all 800,000 Icelandic blog comments on a 5-point Likert scale across 25 tasks, then having human annotators verify binary labels for comments at distribution extremes (ratings 1 or 5). The human annotation phase used 170 crowdworkers recruited via Facebook, with each task receiving 1,100 annotations (100 shared random + 600 rated "5" + 500 rated "1"). The platform provided context-aware annotation with gamification elements, and consensus was determined via majority vote. This targeted approach enables efficient identification of rare phenomena like hate speech that random sampling would miss.

## Key Results
- Dataset contains 12,232 unique comments with 19,301 total annotations
- Average Krippendorff's alpha inter-annotator agreement of 0.58 across tasks
- Average Cohen's kappa AI-human agreement of 0.56
- High agreement for disgust (α = 0.92), politeness (κ = 0.80), and negative sentiment (α = 0.87)
- Low agreement for some tasks: mansplaining (α = 0.07), sarcasm (α = 0.29), and fear (α = 0.24)

## Why This Works (Mechanism)

### Mechanism 1: Extreme-Case Sampling for Rare Phenomena
- Claim: Targeting LLM-rated extremes (1 or 5 on Likert scale) enables efficient identification of rare phenomena like hate speech that random sampling would miss.
- Mechanism: The LLM pre-screens 800K comments; humans verify only those at distribution extremes, concentrating effort where signal is strongest.
- Core assumption: LLM extreme ratings correlate with actual presence/absence of target phenomena.
- Evidence anchors: [abstract] "comments with high or low probabilities of containing each examined behavior were subjected to manual revision"; [section] "This targeted approach allows us to efficiently identify rare but important cases (the proverbial needles-in-a-haystack)"
- Break condition: If LLM extreme ratings don't correlate with ground truth, efficiency gain collapses.

### Mechanism 2: Scale Reduction for Agreement Improvement
- Claim: Converting LLM's 5-point scale to binary human annotations improves inter-annotator agreement by reducing cognitive complexity.
- Mechanism: Humans make yes/no decisions rather than nuanced scale judgments; Hick's Law predicts reduced decision time and error rate with fewer choices.
- Core assumption: Clear cases exist and are sufficient for foundational dataset building; edge cases can be deferred.
- Evidence anchors: [abstract] "improve annotation agreement"; [section] "Human annotators perform binary (yes/no) annotations... reflects our focus on identifying clear instances while acknowledging that intermediate cases may require more nuanced future investigation"
- Break condition: If critical signal exists primarily in intermediate cases (ratings 2-4), binary simplification loses essential nuance.

### Mechanism 3: LLM-Human Complementarity via Targeted Verification
- Claim: Human verification at LLM-predicted extremes corrects systematic errors while preserving scale benefits.
- Mechanism: LLM provides breadth (800K comments); humans provide precision at high-impact points (predicted positives/negatives).
- Core assumption: LLM errors are concentrated at extremes in predictable ways, making targeted correction efficient.
- Evidence anchors: [abstract] "By leveraging crowdworkers to refine these automatically labeled comments, we ensure the quality and accuracy"; [section] Table 1 shows Cohen's κ ranging 0.21–0.82 across tasks, indicating variable LLM-human alignment
- Break condition: If LLM error patterns vary significantly by task, uniform extreme-sampling strategy needs per-task calibration.

## Foundational Learning

- **Concept: Likert Scale vs. Categorical Annotation**
  - Why needed: The paper uses 5-point Likert scales for LLM but 3-class (sentiment) or binary (other tasks) for humans.
  - Quick check question: Why would researchers choose different scale granularities for LLM vs. human annotators?

- **Concept: Inter-Annotator Agreement Metrics (Krippendorff's α, Cohen's κ)**
  - Why needed: The paper reports K's α for human-human agreement and C's κ for AI-human agreement; interpreting these values determines dataset quality assessment.
  - Quick check question: Why might sympathy show high human agreement (K's α = 0.83) but low AI-human agreement (C's κ = 0.24)?

- **Concept: Class Imbalance in Annotation**
  - Why needed: The methodology explicitly addresses rare phenomena; understanding why random sampling fails for hate speech is essential.
  - Quick check question: If hate speech occurs in 0.5% of comments, how many random samples would you need to annotate to get 500 positive examples?

## Architecture Onboarding

- **Component map:**
  Source (~800K comments) -> GPT-4o mini (5-point Likert labels) -> Sampling (100 shared + 1,100 task-specific per task) -> Human annotation (binary yes/no) -> Gold output (12,232 comments)

- **Critical path:**
  1. Schema definition → 2. Prompt engineering for structured LLM output → 3. LLM inference on full corpus → 4. Stratified sampling from extremes → 5. Human annotation → 6. Consensus/agreement calculation

- **Design tradeoffs:**
  - Extreme-only sampling: Clear cases vs. missing edge-case robustness
  - Binary human labels: Higher agreement vs. lost granularity
  - Task-specific sampling: Deep per-task coverage vs. limited cross-task correlation analysis
  - Uncompensated crowdworkers: Ethical concern + participation imbalance

- **Failure signatures:**
  - Very low K's α (e.g., mansplaining 0.07): Unclear definition or inherently subjective concept
  - High K's α, low C's κ (e.g., sympathy, disgust): Cultural/linguistic mismatch between LLM and annotators
  - Near-zero extreme ratings (e.g., "surprise" only 27 at rating 5): LLM doesn't recognize concept or concept is genuinely rare
  - Participation skew across tasks: Annotator fatigue from sequential task presentation

- **First 3 experiments:**
  1. Sample from middle LLM ratings (2-4) to validate that signal degrades outside extremes—establishes boundary conditions.
  2. For low-agreement tasks (mansplaining, sarcasm), test alternative sampling thresholds or revised definitions to diagnose source of disagreement.
  3. Expand shared evaluation set to enable cross-task correlation analysis (e.g., does sarcasm correlate with negative sentiment in Icelandic?).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating sarcasm detection as an auxiliary task in a multi-task learning framework improve sentiment analysis performance for Icelandic text?
- Basis in paper: [explicit] "The introduction of a Multi-Task Learning framework as a future direction holds promise for improving the detection of complex phenomena, such as sarcasm... By integrating tasks such as sarcasm detection with sentiment analysis, future models may achieve greater accuracy."
- Why unresolved: The current study treats each annotation task independently; sarcasm detection showed low agreement (K's α = 0.29, C's κ = 0.26), and the authors hypothesize cultural factors (Icelandic humor relying on sarcasm) contribute to sentiment analysis difficulties.
- What evidence would resolve it: Train MTL models with and without sarcasm detection as an auxiliary task, compare sentiment classification accuracy on a held-out Icelandic test set.

### Open Question 2
- Question: What is the impact of task presentation order on annotation quality and annotator engagement in crowdsourced multi-task annotation projects?
- Basis in paper: [explicit] "Task participation decreased for tasks presented later in the annotation sequence... In future work, this issue could be mitigated by randomizing the order in which tasks are presented to each crowd worker."
- Why unresolved: All annotators saw tasks in the same sequence, creating confounding between task difficulty and presentation order; participation varied significantly across tasks.
- What evidence would resolve it: Run a controlled experiment with randomized task ordering across annotator groups and compare annotation counts, agreement scores, and completion rates per task.

### Open Question 3
- Question: Can smaller, fine-tuned Icelandic language models achieve comparable silver-label quality to GPT-4o mini for initial filtering in annotation pipelines, with reduced computational cost?
- Basis in paper: [explicit] "Future work should explore more environmentally sustainable approaches, such as using smaller, task-specific models for initial filtering or developing more efficient sampling strategies that could achieve similar results with less computational overhead."
- Why unresolved: GPT-4o mini was used to label all 800,000 comments despite the final gold dataset containing only ~12,000 annotations; no comparison to alternative models was conducted.
- What evidence would resolve it: Compare silver labels from smaller Icelandic-adapted models (e.g., IceBERT) against GPT-4o mini using human gold labels as ground truth, measuring both agreement scores and computational resource consumption.

## Limitations

- Variable inter-annotator agreement across tasks, with some categories showing very low agreement (mansplaining Krippendorff's α = 0.07, sarcasm α = 0.29)
- Two-phase annotation methodology may systematically miss nuanced cases that fall between extreme ratings targeted for human verification
- Use of uncompensated crowdworkers raises ethical concerns and may introduce participation bias

## Confidence

- **High confidence**: Basic dataset creation methodology, agreement metrics calculation, and dataset release procedures
- **Medium confidence**: Effectiveness of extreme-case sampling for rare phenomena detection, given limited validation of correlation between LLM extreme ratings and ground truth
- **Low confidence**: Claims about cross-task correlations and generalizability beyond the blog.is platform, due to limited shared evaluation set and single-source data collection

## Next Checks

1. Validate extreme-rating correlation by sampling 500 comments from middle LLM ratings (2-4) to test whether signal degrades outside extremes, establishing whether current sampling strategy misses meaningful cases.

2. Test cultural alignment by conducting a focused study comparing GPT-4o mini predictions with Icelandic cultural experts' judgments on a subset of comments, particularly for social acceptability tasks where AI-human agreement is low despite high human-human agreement.

3. Assess platform generalizability by applying the annotation methodology to a different Icelandic online platform (e.g., social media or news comments) using a small pilot sample to determine whether agreement patterns and rare phenomena detection effectiveness transfer beyond blog.is.