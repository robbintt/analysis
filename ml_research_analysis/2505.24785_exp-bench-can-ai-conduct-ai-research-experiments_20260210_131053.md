---
ver: rpa2
title: 'EXP-Bench: Can AI Conduct AI Research Experiments?'
arxiv_id: '2505.24785'
source_url: https://arxiv.org/abs/2505.24785
tags:
- setup
- missing
- research
- design
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EXP-Bench, a benchmark designed to evaluate
  AI agents on complete end-to-end AI research experiments. The benchmark extracts
  tasks from influential AI publications and their codebases, providing agents with
  a research question, high-level method description, and starter code.
---

# EXP-Bench: Can AI Conduct AI Research Experiments?

## Quick Facts
- arXiv ID: 2505.24785
- Source URL: https://arxiv.org/abs/2505.24785
- Authors: Patrick Tser Jern Kon; Jiachen Liu; Xinyi Zhu; Qiuyi Ding; Jingjia Peng; Jiarong Xing; Yibo Huang; Yiming Qiu; Jayanth Srinivasa; Myungjin Lee; Mosharaf Chowdhury; Matei Zaharia; Ang Chen
- Reference count: 40
- One-line primary result: AI agents successfully complete end-to-end AI research experiments only 0.5% of the time, with significant bottlenecks in experimental design and implementation phases.

## Executive Summary
This paper introduces EXP-Bench, a benchmark designed to evaluate AI agents on complete end-to-end AI research experiments. The benchmark extracts tasks from influential AI publications and their codebases, providing agents with a research question, high-level method description, and starter code. A semi-automated pipeline curates these tasks, ensuring high-fidelity and structured evaluation. The benchmark evaluates agents across four phases: experimental design, implementation, execution, and conclusion. Leading agents like OpenHands and IterativeAgent were tested, revealing significant challenges. While agents occasionally achieved 20-35% correctness on individual phases, complete, executable experiments succeeded only 0.5% of the time. Key bottlenecks included conceptualizing experimental designs, translating methodologies into correct code, and ensuring robust execution. The results highlight the need for targeted improvements in AI agents' ability to conduct rigorous AI research experiments. EXP-Bench serves as a valuable tool for guiding future agent development and advancing autonomous AI research capabilities.

## Method Summary
The paper introduces EXP-Bench, a benchmark that evaluates AI agents on conducting complete end-to-end AI research experiments. The benchmark extracts tasks from 51 influential papers across top venues, providing agents with research questions, high-level methods, and starter code. A semi-automated pipeline curates these tasks by extracting experimental details from papers and codebases. Agents are evaluated across four phases: experimental design, implementation, execution, and conclusion. The evaluation uses conjunctive metrics that require correctness across multiple phases simultaneously. Leading agents like OpenHands and IterativeAgent were tested in Ubuntu 24.04 Docker containers with 4x Nvidia A40 GPUs, with evaluation performed using an LLM judge (o3-mini) and a Code Execution Validator.

## Key Results
- AI agents achieve only 0.5% success rate on complete, executable experiments
- Individual phase correctness ranges from 20-35%, but conjunctive requirements drop scores dramatically
- Most prevalent failures: missing implementation components (39.7%), environment/dependency misconfigurations (29.4%), and misclassified design variables (16.1%)
- Agents struggle with conceptualizing experimental designs and translating methodologies into correct code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The semi-automated curation pipeline enables scalable creation of high-fidelity experimental tasks from research papers and codebases.
- Mechanism: A three-stage pipeline (source selection → experiment procedure extraction → verification) uses multimodal extraction to pull research questions and methods from papers, then goal-conditioned codebase search to locate corresponding implementations. AST tracing converts validated scripts into step-by-step requirements. Execution-based validation ensures functionality before human review.
- Core assumption: Papers with open-source code from top venues (NeurIPS, ICLR) contain sufficient signal to reconstruct experimental procedures.
- Evidence anchors:
  - [abstract] "To enable the creation of such intricate and authentic tasks with high-fidelity, we design a semi-autonomous pipeline to extract and structure crucial experimental details from these research papers and their associated open-source code."
  - [section 3.2, p.4-5] Describes Stage 2.1's multi-pass extraction (retrieval-augmented querying → semantic extraction at subsection level → targeted re-querying) and Stage 2.2's goal-conditioned codebase search with AST tracing.
  - [corpus] Weak. Neighbor papers focus on similar benchmarking (CSR-Bench, LMR-BENCH) but do not validate this specific curation mechanism.
- Break condition: If papers omit critical implementation details not recoverable from code (e.g., undocumented hyperparameter choices), the pipeline produces incomplete ground truth.

### Mechanism 2
- Claim: Conjunctive evaluation metrics prevent over-crediting agents by requiring simultaneous correctness across multiple phases.
- Mechanism: Rather than reporting single-metric scores, EXP-Bench combines metrics (e.g., I·E requires both implementation appropriateness AND executable code; M·C·D·I·E adds monitoring, conclusion, design, implementation, and execution). Each additional constraint sharply reduces scores—e.g., monitor alone yields 20.6%, but full conjunction drops to 0.2%.
- Core assumption: Valid research experiments require correctness across all phases; partial success is insufficient.
- Evidence anchors:
  - [abstract] "While agents occasionally achieved 20-35% correctness on individual phases, complete, executable experiments succeeded only 0.5% of the time."
  - [section 4.2, p.7] Fig. 6b shows progressive score collapse: M (20.6%) → M·C·D (3.7%) → M·C·D·I (0.4%) → M·C·D·I·E (0.2%).
  - [corpus] No direct corpus validation of conjunctive evaluation; related benchmarks (MLE-Bench, RE-Bench) focus on partial metrics.
- Break condition: If phase dependencies are weaker than assumed (e.g., correct conclusions from flawed designs), conjunctive metrics may undercount meaningful progress.

### Mechanism 3
- Claim: Fine-grained failure pattern extraction enables targeted agent improvement by isolating phase-specific weaknesses.
- Mechanism: Error traces from 3,238 raw insights are categorized into 361 failure types across four phases (design, implementation, execution, conclusion). Prevalence analysis identifies bottlenecks—e.g., 39.71% missing essential implementation components, 29.38% environment misconfigurations, 16.05% misclassified design variables.
- Core assumption: Failures are attributable to specific, addressable agent capabilities rather than irreproducible research artifacts.
- Evidence anchors:
  - [section 4.3, p.7-8] Table 2 shows failure prevalence; text describes specific examples (missing Mixup/CutMix techniques, missing STORM environment in jaxmarl).
  - [section 2, p.3] Notes that papers "often omit detailed intermediate steps," fragmenting critical details across papers, supplements, and codebases.
  - [corpus] Weak. No corpus papers validate failure pattern taxonomies for agent improvement.
- Break condition: If failures stem primarily from irreproducible original research (poor documentation, missing dependencies in source repos), agent improvements won't translate to higher scores.

## Foundational Learning

- Concept: Research experiment lifecycle (question → design → implementation → execution → analysis → conclusion)
  - Why needed here: EXP-Bench assumes agents must traverse this full pipeline; partial success on isolated phases (e.g., code execution alone) is insufficient.
  - Quick check question: Can you trace how a research question from a NeurIPS paper translates to dependent/independent variables, code modifications, and a falsifiable conclusion?

- Concept: Repository-level code navigation and modification
  - Why needed here: Agents receive masked repositories and must locate relevant scripts, understand dependencies, and implement modifications—39.71% of failures stem from missing implementation components.
  - Quick check question: Given a PyTorch repo with `train.py`, `model/`, and `configs/`, can you identify where to add a data augmentation technique specified in a method description?

- Concept: Execution environment reproducibility (dependencies, GPU requirements, containerization)
  - Why needed here: 29.38% of failures are environment/dependency misconfigurations; agents must infer requirements from README, config files, and import statements.
  - Quick check question: A repo requires `timm` but the agent uses `torchvision`—will the model load correctly? How would you detect this from error logs?

## Architecture Onboarding

- Component map:
  - Task input: Research question + high-level method + masked code repository
  - Ground truth: Experimental design (variables) + git diff (implementation) + conclusion
  - Evaluation pipeline: Monitor (integrity check) → LLM judge (design/conclusion/implementation scoring) → Code execution validator (E metric)
  - Metrics: D (design), I (implementation), E (execution), C (conclusion), plus conjunctive combinations (I·E, All✓, All·E✓)

- Critical path:
  1. Agent receives task → must NOT access original paper PDF (monitor violation)
  2. Agent produces design output → compared against ground-truth variables (D metric)
  3. Agent modifies code → git diff extracted and compared to ground-truth diff (I metric)
  4. Agent executes code → validator runs in clean container, checks for expected outputs (E metric)
  5. Agent produces conclusion → semantic match against ground-truth conclusion (C metric)

- Design tradeoffs:
  - **Conjunctive vs. per-phase scoring**: Conjunctive (stricter, surfaces brittleness) vs. per-phase (partial credit, 20-35% vs. 0.5%)
  - **Automated vs. human curation**: Pipeline reduces 2 hours → 20 minutes per paper, but may miss implicit domain knowledge
  - **Execution subset vs. full evaluation**: Only ~50-420 tasks execution-checked per model (time constraint), potentially missing execution failures in excluded tasks

- Failure signatures:
  - **Design**: Misclassified variables (16.05%), irrelevant procedural additions (7.62%)—agent confuses constants vs. independent variables or adds unrequested components
  - **Implementation**: Missing essential components (39.71%)—e.g., omitting semantic retrieval strategies or robustness techniques
  - **Execution**: Environment errors (29.38%), script-level errors (23.84%)—missing libraries, wrong model names, absent checkpoints
  - **Conclusion**: Missing content (26.18%), incorrect interpretation (19.66%)—plausible but unfounded claims, missing numerical comparisons

- First 3 experiments:
  1. **Baseline run**: Deploy OpenHands+Claude-3.7-Sonnet on a single RL task (e.g., MogaNet from ICLR 2024). Record D, I, E, C scores individually. Expect: I~40%, E~30%, D/C lower. Identify which phase fails first.
  2. **Ablate execution**: Skip the execution validator (E metric) and compare All✓ vs. All·E✓. Quantify how many "correct" implementations are actually non-executable—likely 10-20% gap based on paper's I·E vs. I disparities.
  3. **Failure pattern injection**: Intentionally introduce a common failure (e.g., omit Mixup augmentation in an image classification task) and verify the judge correctly penalizes I score. This validates the implementation evaluation prompt's granularity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning with verifiable rewards using the EXP-Bench dataset successfully enhance an agent's ability to automate research experimentation?
- Basis in paper: [explicit] Section 5 identifies this as a "promising direction" for enabling agents to autonomously navigate the research lifecycle.
- Why unresolved: The current paper only evaluates pre-existing agents like OpenHands rather than training new ones using the dataset.
- What evidence would resolve it: Performance metrics of an agent fine-tuned via RL on EXP-Bench compared to the baseline scores reported in the paper.

### Open Question 2
- Question: How can the benchmark be expanded to capture the "unstructured ideation" and literature review phases of the research lifecycle?
- Basis in paper: [explicit] Section 5 notes that these critical early stages are "not yet fully captured by the current task structures."
- Why unresolved: EXP-Bench currently focuses strictly on the experimentation procedure starting from a provided research question.
- What evidence would resolve it: The development of new task types within the benchmark that assess an agent's ability to identify research gaps and generate hypotheses.

### Open Question 3
- Question: What specific mechanisms are required to resolve the reproducibility challenges causing the high rate of environment and dependency configuration errors?
- Basis in paper: [inferred] Section 4.3 identifies environment/dependency misconfigurations as the most prevalent execution failure (29.4%), yet offers no solution.
- Why unresolved: Agents struggle with complex software stacks even when correct implementation logic is present.
- What evidence would resolve it: An ablation study showing that equipping agents with specific environment management tools significantly reduces this specific error rate.

## Limitations

- The semi-automated curation pipeline may miss critical implementation details not recoverable from code, potentially biasing results against agents
- Conjunctive evaluation metrics may undercount meaningful partial progress if research experiments have weaker phase dependencies than assumed
- The failure taxonomy assumes failures are addressable agent capabilities rather than irreproducible research artifacts

## Confidence

- **High Confidence**: The benchmark successfully evaluates agents across four distinct phases with automated scoring infrastructure. The pipeline creates structured tasks from real research papers, and the failure pattern extraction methodology is methodologically sound.
- **Medium Confidence**: The claim that agents rarely achieve complete, executable experiments (0.5%) is well-supported by conjunctive scoring, but the extent to which this reflects agent limitations vs. ground truth incompleteness remains uncertain.
- **Low Confidence**: The specific improvement trajectories for agents based on failure pattern analysis are speculative without longitudinal validation. The claim that this taxonomy enables "targeted agent improvement" requires empirical verification.

## Next Checks

1. **Ground Truth Validation**: Compare the semi-automated ground truth extraction against full human annotation for 10 randomly selected tasks. Quantify completeness gaps and assess whether missing details systematically bias results.

2. **Ablation of Conjunctive Scoring**: Run evaluation with per-phase scoring (allowing partial credit) and compare distributions. Determine whether conjunctive metrics reveal genuine brittleness or artificially penalize agents.

3. **Failure Source Attribution**: For the top 20 most frequent failure types, manually verify whether failures originate from agent capability gaps or irreproducible source materials. This determines whether the benchmark measures agent progress or research artifact quality.