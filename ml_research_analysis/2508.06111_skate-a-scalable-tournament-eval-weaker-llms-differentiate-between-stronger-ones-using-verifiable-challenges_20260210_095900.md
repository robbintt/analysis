---
ver: rpa2
title: 'SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger
  ones using verifiable challenges'
arxiv_id: '2508.06111'
source_url: https://arxiv.org/abs/2508.06111
tags:
- questions
- question
- which
- game
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SKATE, a scalable automated framework for
  evaluating large language models (LLMs) by having them compete in generating and
  solving verifiable tasks for each other. Instead of relying on human-designed benchmarks
  or LLM judges, SKATE uses a game-like setup where models act as both task-setters
  and solvers, incentivized to create questions that are both solvable by themselves
  and challenging for others.
---

# SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges

## Quick Facts
- **arXiv ID:** 2508.06111
- **Source URL:** https://arxiv.org/abs/2508.06111
- **Reference count:** 2
- **Key outcome:** Weaker LLMs can reliably differentiate and rank stronger ones using verifiable challenges.

## Executive Summary
SKATE is a scalable automated framework for evaluating large language models by having them compete in generating and solving verifiable tasks for each other. The framework uses a game-like setup where models act as both task-setters and solvers, incentivized to create questions that are both solvable by themselves and challenging for others. Using code-output-prediction challenges with a TrueSkill-based ranking system, SKATE enables weaker models to reliably differentiate stronger ones while automatically surfacing fine-grained capability differences.

## Method Summary
The method implements a tournament where LLMs act as both task-setters and solvers. Each round, every model attempts to generate a Code-Output-Prediction (COP) question—a Python code snippet with 9 unique distractors and verifiable output. All models then answer all questions via multiple-choice sampling. TrueSkill ranking updates occur based on pairwise performance comparisons. The system uses adaptive MCQ sampling until convergence, vector-based uniqueness constraints, and historical performance augmentation to guide question generation.

## Key Results
- Weaker models can reliably differentiate and rank stronger models
- Models exhibit self-preferencing behavior in question generation
- The framework automatically surfaces fine-grained capability differences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Competitive game dynamics incentivize models to generate tasks that maximize their own comparative advantage ("discriminatory niches").
- **Mechanism:** By rewarding a model only if it solves a challenge that competitors fail, the system forces models to probe the boundary between their capabilities and those of others.
- **Core assumption:** Models possess sufficient self-knowledge to gauge which tasks they can solve and sufficient theory-of-mind to guess where others might fail.
- **Evidence anchors:** Abstract and Section 4.2 show the game design rewards discriminatory questions. "Do LLMs Know When to Flip a Coin?" [40659] supports strategic game-theoretic reasoning.
- **Break condition:** If models fail to calibrate difficulty or fail to differentiate their capabilities, the adversarial search degrades into noise.

### Mechanism 2
- **Claim:** Code-Output-Prediction provides a scalable, objective verification substrate that bypasses LLM-judge bias.
- **Mechanism:** Instead of using a potentially biased LLM-as-judge, SKATE uses deterministic code execution where the sandbox determines correct output.
- **Core assumption:** Code execution environments are secure, deterministic, and generated code is executable.
- **Evidence anchors:** Abstract states "scoring is objective" and Section 3 details the code execution sandbox. "Graders should cheat" [17141] highlights the difficulty of using weaker models to evaluate stronger ones.
- **Break condition:** If models generate non-deterministic code or have access to tools that trivialize COP.

### Mechanism 3
- **Claim:** Bayesian ranking systems (TrueSkill) can aggregate noisy pairwise comparisons into stable global rankings, even when judges are weaker than subjects.
- **Mechanism:** TrueSkill models skill as Gaussian distributions and updates beliefs based on match outcomes, remaining robust to individual question noise.
- **Core assumption:** Transitivity holds generally and performance variance is consistent enough to be captured.
- **Evidence anchors:** Abstract and Section 6.1 show weaker models reliably differentiated stronger models. "Benchmark^2" [112961] emphasizes consistency in benchmarks.
- **Break condition:** If capability profiles are highly non-transitive, the single-dimensional ranking may become unstable.

## Foundational Learning

- **Concept:** **TrueSkill Rating System (μ and σ)**
  - **Why needed here:** Standard accuracy scores fail to capture uncertainty and relative strength in a tournament.
  - **Quick check question:** If a model has μ=25 and σ=8.33, what does the high σ imply about the system's confidence in its rating?

- **Concept:** **LLM-as-Judge Bias**
  - **Why needed here:** The paper positions itself explicitly against evaluation methods that use LLMs to grade other LLMs.
  - **Quick check question:** Why might an LLM judge prefer answers generated by itself or models with similar token distributions, even if those answers are factually incorrect?

- **Concept:** **Code-Output-Prediction (COP)**
  - **Why needed here:** This is the specific "verifiable task" instantiation used in the proof-of-concept.
  - **Quick check question:** How does the system handle a question if the generated code raises a runtime error?

## Architecture Onboarding

- **Component map:** Game Controller -> Task-Setter Agent -> Validation Layer (Sandbox) -> Solver Agent -> Ranking Engine (TrueSkill)
- **Critical path:** The Validation Layer is the bottleneck. If the Task-Setter generates invalid code or fails to generate unique questions, the round yields no data.
- **Design tradeoffs:**
  - COP vs. Math/Logic Puzzles: COP is "narrow" but general applicability claimed
  - Relative vs. Absolute Scoring: Relative scoring provided clearer separation
  - Context Augmentation: "Full Context" is expensive and noisy; "Historical Performance" balances cost and adaptive signal
- **Failure signatures:**
  1. Reward Hacking: Models generate semantically similar questions repeatedly
  2. Sandbagging: A model deliberately writes easy questions to ensure it gets points
  3. Tool-Augmented Triviality: If a solver has access to a code interpreter, COP becomes trivial
- **First 3 experiments:**
  1. Replicate Stability Test: Run a 4-model tournament and plot convergence of σ over 50 rounds
  2. Stress Test Uniqueness: Disable the uniqueness constraint and observe ranking system behavior
  3. Cross-Domain Validation: Replace COP with a different verifiable task to test mechanism holds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can SKATE effectively evaluate models using verifiable tasks outside of code execution, such as physical world simulations?
- **Basis in paper:** [explicit] Authors identify that code-execution capable models trivialize COP tasks and suggest future work could incorporate "other classes of verifiable tasks."
- **Why unresolved:** Current study only validates the framework using COP domain.
- **What evidence would resolve it:** Successful implementation and differentiation of models using non-code verifiable tasks like theorem proving or simulation control.

### Open Question 2
- **Question:** Will more capable future models effectively exploit full-context game history to construct maximally adversarial tasks?
- **Basis in paper:** [explicit] Authors note current augmentation strategies had minimal impact but "anticipate that as LLM capabilities mature, they will increasingly utilize such information."
- **Why unresolved:** Current frontier models tested did not demonstrate strong strategic reasoning or ability to leverage game-state information.
- **What evidence would resolve it:** Future experiments showing distinct rank shifts when advanced models are given access to full historical context.

### Open Question 3
- **Question:** Can the framework detect or control for strategic metagaming behaviors, such as "sandbagging"?
- **Basis in paper:** [explicit] Conclusion states this work lays groundwork for "visibility of strategic behaviours... such as self-preferencing, metagaming (e.g. early sandbagging)."
- **Why unresolved:** Current paper focuses on capability differentiation and self-preferencing but does not analyze deceptive strategies.
- **What evidence would resolve it:** Detection of specific patterns where models intentionally fail early tasks to lower their ranking.

## Limitations
- Framework's dependence on verifiable tasks like COP may limit applicability to domains without clear ground truth
- Sandbox's robustness to adversarial code generation remains untested at scale
- Claims about general applicability beyond COP are asserted rather than demonstrated

## Confidence
- **High confidence**: Models can reliably differentiate stronger models when acting as weaker evaluators
- **Medium confidence**: The adversarial game dynamics effectively probe capability boundaries
- **Low confidence**: Claims about general applicability beyond COP to arbitrary domains

## Next Checks
1. **Generalizability Test**: Replace COP with a non-code verifiable task and verify if weaker models still differentiate stronger ones
2. **Adversarial Robustness**: Systematically probe sandbox security by attempting to generate code that could break the verification environment
3. **Noise Sensitivity Analysis**: Vary the number of MCQ samples per question and measure impact on ranking stability