---
ver: rpa2
title: Scaling Agents via Continual Pre-training
arxiv_id: '2509.13310'
source_url: https://arxiv.org/abs/2509.13310
tags:
- agentic
- data
- tool
- research
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Agentic Continual Pre-training (Agentic CPT)
  to build agentic foundation models for deep research agents, addressing the limitation
  that current post-training approaches struggle due to lack of robust agentic foundation
  models. The authors introduce a two-stage training pipeline using First-order and
  Higher-order Action Synthesis to generate large-scale agentic data without external
  tool invocations, enabling models to learn diverse agentic behaviors and decision-making
  capabilities.
---

# Scaling Agents via Continual Pre-training

## Quick Facts
- arXiv ID: 2509.13310
- Source URL: https://arxiv.org/abs/2509.13310
- Reference count: 15
- Primary result: Agentic CPT achieves state-of-the-art performance on 10 benchmarks, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE

## Executive Summary
This paper addresses the fundamental challenge of building deep research agents by introducing Agentic Continual Pre-training (Agentic CPT), a two-stage training pipeline that creates agentic foundation models before post-training. The authors identify that current approaches struggle because general-purpose models lack agentic inductive biases, forcing post-training to simultaneously learn behaviors and align them to expert demonstrations. Their solution uses First-order Action Synthesis (FAS) to generate large-scale agentic data without external tool invocations, followed by Higher-order Action Synthesis (HAS) to convert trajectories into multi-step decision-making tasks. The resulting AgentFounder-30B model achieves state-of-the-art performance across multiple benchmarks while retaining strong general tool-use abilities.

## Method Summary
The method introduces a two-stage continual pre-training pipeline between standard pre-training and post-training. Stage 1 (200B tokens, 32K context) uses First-order Action Synthesis to generate synthetic agentic data without API calls, creating entity-anchored knowledge memory and synthesizing planning/reasoning steps via reject sampling. Stage 2 (100B tokens, 128K context) applies Higher-order Action Synthesis to convert existing trajectories into contrastive decision-action pairs, shifting from trajectory imitation to step-wise decision-making. The model starts from Qwen3-30B-A3B-Base and is followed by SFT post-training with React-style agent trajectories.

## Key Results
- AgentFounder-30B achieves 39.9% on BrowseComp-en and 43.3% on BrowseComp-zh, state-of-the-art performance
- GAIA benchmark score of 72.8%, significantly outperforming previous approaches
- HLE Pass@1 score of 31.5%, demonstrating strong performance on knowledge-intensive tasks
- Maintains general tool-use capabilities with 70.0% on ACEBench versus baseline 67.2%
- The two-stage CPT framework shows consistent improvements across all 10 evaluated benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Optimization Tension Resolution
General-purpose models lack agentic inductive biases, forcing post-training to simultaneously learn new capabilities and align them to expert demonstrations. Agentic CPT first installs foundational agentic behaviors via next-token prediction, allowing post-training to focus purely on alignment. This sequential isolation of capability acquisition and alignment resolves inherent optimization conflicts.

### Mechanism 2: Scalable Offline Synthesis
FAS generates massive agentic datasets without expensive API invocations by constructing entity-anchored knowledge memory and synthesizing planning steps internally. This enables broad exploration of the agentic policy space while preventing memorization of specific trajectories through diverse synthetic generation.

### Mechanism 3: Multi-step Decision Reframing
HAS converts trajectory learning from sequence reproduction to step-wise discrimination by expanding decision candidates at each step and creating contrastive decision-action formats. This prevents overfitting to single paths and improves decision robustness by teaching the model to identify correct vs. incorrect options at each decision point.

## Foundational Learning

**Concept: Inductive Bias**
*Why needed:* General models lack "agentic inductive biases," meaning they don't inherently understand tool use or multi-step planning structure. Understanding this explains why specific CPT is necessary rather than just more SFT.
*Quick check:* Can you explain why a model trained solely on static text might struggle to natively understand the "request-response" loop of a tool API?

**Concept: Optimization Tension**
*Why needed:* This is the core problem solved - the conflict between learning new capabilities (exploration) and aligning to experts (imitation) when trained jointly.
*Quick check:* If you try to teach a student a completely new sport while simultaneously grading them on professional-level form, what conflict arises?

**Concept: Policy Space & Exploration**
*Why needed:* Deep research agents operate in vast policy spaces (possible action sequences). FAS/HAS methods cover this space better than limited expert demos.
*Quick check:* Why is "memorizing" a successful trajectory insufficient for an agent facing dynamic environments (e.g., changing search results)?

## Architecture Onboarding

**Component map:**
Knowledge Memory -> FAS Module -> HAS Module -> Trainer (Stage 1: 32K context/FAS-heavy -> Stage 2: 128K context/HAS-heavy)

**Critical path:**
1. Data Quality: FAS "Reject Sampling" and HAS "Weakly-Supervised Filtering" are critical - garbage synthetic data will degrade the base model
2. Context Window: 128K context extension for Stage 2 is mandatory - truncated training fails to capture long-horizon planning

**Design tradeoffs:**
- Offline vs. Online Synthesis: FAS is scalable but relies on internal knowledge (hallucination risk); HAS uses real trajectories but harder to scale
- Imitation vs. Decision: HAS requires generating N candidates per step, increasing data prep compute but improving sample efficiency

**Failure signatures:**
- High API Costs: Skipping FAS and using real search APIs for 200B tokens will explode costs
- Catastrophic Forgetting: CPT data must be mixed with general corpora to prevent losing general capabilities
- Decision Collapse: If HAS candidates are too similar, the model learns to pick randomly rather than discerning quality

**First 3 experiments:**
1. Loss Convergence Check: Compare SFT loss curves of "Base + SFT" vs "Agentic CPT + SFT" - CPT should converge faster and lower
2. Ablation on Context: Train Stage 2 model with 32K vs 128K context on BrowseComp to validate long-context necessity
3. FAS Quality Audit: Run FAS generator on held-out questions and manually inspect if "first-step reasoning" aligns with ground truth tool usage

## Open Questions the Paper Calls Out
- Can enhancing the Agentic CPT data mixture with knowledge-dense reasoning corpora bridge the performance gap between information retrieval tasks (BrowseComp) and knowledge-intensive tasks (HLE)?
- To what extent does the performance lag in non-English benchmarks (BrowseComp-zh) stem from the underlying search tool's limitations versus lack of multilingual Agentic CPT data?
- Does the Agentic CPT framework generalize to agentic domains beyond "Deep Research" (e.g., coding) without requiring domain-specific trajectory synthesis?

## Limitations
- FAS scalability depends on internal knowledge quality - outdated or biased knowledge memory can generate incorrect tool invocation patterns
- HAS candidate generation quality is critical but not fully specified - low-quality alternatives create misleading contrastive signals
- 128K context requirement for Stage 2 may create computational bottlenecks limiting practical deployment

## Confidence
- High confidence: The two-stage CPT framework and its improvement over direct SFT
- Medium confidence: The mechanism of separating capability learning from alignment
- Medium confidence: FAS/HAS data synthesis methods (dependent on LLM-as-Judge quality)
- Low confidence: Long-term retention of general capabilities

## Next Checks
1. FAS Hallucination Risk Assessment: Generate 100 FAS samples and verify synthesized planning steps align with actual tool usage patterns
2. HAS Candidate Quality Audit: For 50 HAS-expanded steps, measure decision diversity and correctness rates to ensure meaningful contrastive learning signals
3. Long-context Degradation Test: Train Agentic CPT model with only 32K context in Stage 2 and compare BrowseComp performance to validate 128K context necessity