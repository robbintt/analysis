---
ver: rpa2
title: Low-rank Momentum Factorization for Memory Efficient Training
arxiv_id: '2507.08091'
source_url: https://arxiv.org/abs/2507.08091
tags:
- mofasgd
- galore
- low-rank
- memory
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses memory efficiency in fine-tuning large foundation
  models, which is a major bottleneck due to stateful optimizers like AdamW requiring
  significantly more GPU memory than inference. The core method, Momentum Factorized
  SGD (MoFaSGD), maintains a dynamically updated low-rank SVD representation of the
  first-order momentum.
---

# Low-rank Momentum Factorization for Memory Efficient Training

## Quick Facts
- arXiv ID: 2507.08091
- Source URL: https://arxiv.org/abs/2507.08091
- Authors: Pouria Mahdavinia; Mehrdad Mahdavi
- Reference count: 40
- One-line primary result: MoFaSGD achieves memory-efficient training by factorizing first-order momentum into low-rank SVD, enabling competitive performance with 41GB memory savings on LLaMA-8B

## Executive Summary
This paper addresses memory efficiency in fine-tuning large foundation models, which is a major bottleneck due to stateful optimizers like AdamW requiring significantly more GPU memory than inference. The core method, Momentum Factorized SGD (MoFaSGD), maintains a dynamically updated low-rank SVD representation of the first-order momentum. This factorization enables efficient low-rank gradient projections and spectral normalization updates while avoiding costly offline resampling. MoFaSGD adaptively updates the optimization subspace at each iteration and uses the computed momentum factors for spectrally normalized parameter updates.

## Method Summary
MoFaSGD factorizes the first-order momentum into low-rank SVD components, updating them via tangent space projections that minimize compression error. The algorithm projects gradients onto the tangent space of previous momentum factors, performs QR decomposition to expand the basis, constructs a small 2r×2r matrix, computes its SVD, and contracts back to rank-r factors. Parameter updates use spectral normalization via the momentum factors directly, avoiding accumulation errors from subspace changes. The method applies only to linear transformer layers, using AdamW-bf16 for embeddings and 1D layers.

## Key Results
- Achieves 41GB memory savings on LLaMA-8B fine-tuning (29GB vs 70GB with AdamW)
- Top-32 singular values capture ~80% of momentum energy, validating low-rank structure
- Competitive performance on GLUE, MMLU, and instruction tuning benchmarks with rank-32 factorization
- Convergence guarantees under nuclear-norm smoothness assumptions for non-convex optimization

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Momentum Factorization with Online Updates
Maintaining a dynamically updated low-rank SVD of first-order momentum enables memory-efficient training while preserving optimization quality. Rather than storing full-rank momentum buffers, MoFaSGD maintains factorized representation where U ∈ R^(m×r), V ∈ R^(n×r), Σ ∈ R^(r×r). The key insight is that gradient EMAs exhibit low-rank structure—the paper empirically shows top-32 singular values capture ~80% of momentum energy. The update combines projected gradient with decayed previous factors via efficient QR decomposition on expanded 2r-dimensional space, then truncates back to rank-r via small 2r×2r SVD.

### Mechanism 2: Tangent Space Projection Minimizes Compression Error
Projecting gradients onto the tangent space of previous momentum factors achieves optimal residual error among broad class of low-rank projections. The tangent space at current factorization includes directions: U_t·M·V_t^T + U_p·V_t^T + U_t·V_p^T (orthogonal corrections). Projection is: Ĝ_t = U_t·U_t^T·G_t + G_t·V_t·V_t^T - U_t·U_t^T·G_t·V_t·V_t^T. Theorem 4.3 proves this minimizes ||G_t - Ĝ_t||_F compared to left-only, right-only, or two-sided projections.

### Mechanism 3: Spectral Normalization Bypasses Subspace Accumulation Errors
Using momentum factors directly for spectrally normalized updates avoids error propagation from subspace moment accumulation. Update rule W_{t+1} = W_t - η·U_{t+1}·V_{t+1}^T implicitly performs spectral normalization (akin to Muon's orthogonalization). This contrasts with GaLore which accumulates Adam moments within subspace—when subspace changes, accumulated moments become misaligned. MoFaSGD's direct use of factors avoids this staleness problem.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) and Low-Rank Approximation**
  - Why needed here: Core representation; must understand how truncated SVD compresses matrices and what information is lost
  - Quick check question: Given matrix M with singular values [10, 5, 1, 0.1, 0.01], what fraction of Frobenius norm energy is captured by rank-2 approximation?

- **Concept: Momentum in Optimization**
  - Why needed here: MoFaSGD factorizes first-order momentum; need intuition for why EMA of gradients is useful and what β controls
  - Quick check question: If β=0.9, what is the effective window size of gradients contributing significantly to current momentum?

- **Concept: Matrix Norms (Frobenius, Spectral, Nuclear)**
  - Why needed here: Paper uses nuclear norm for convergence (Assumption 4.1), Frobenius for projection quality, spectral for updates
  - Quick check question: For rank-r matrix A = UΣV^T, express ||A||_* (nuclear), ||A||_2 (spectral), ||A||_F (Frobenius) in terms of singular values

## Architecture Onboarding

- **Component map:** Gradient computation -> Tangent space projection -> QR decomposition -> 2r×2r matrix construction -> SVD computation -> Factor contraction -> Spectral parameter update -> Memory buffer clearing

- **Critical path:**
  1. Backward pass produces gradient G_t
  2. Project G_t onto tangent space (3 matrix multiplications)
  3. QR decomposition expands basis to 2r dimensions
  4. Construct and decompose 2r×2r matrix S_t
  5. Contract back to rank-r factors (U_{t+1}, V_{t+1})
  6. Apply spectral update to parameters
  7. Clear gradient buffers (critical for memory savings)

- **Design tradeoffs:**
  - **Rank r:** Higher = better approximation, more memory, slower. Table 1 shows r=32 sweet spot for NanoGPT
  - **Momentum decay β:** Paper uses β=0.85-0.95. Lower β = faster subspace adaptation but noisier
  - **Layer selection:** Paper applies only to linear transformer layers (not embeddings/1D layers), uses AdamW elsewhere

- **Failure signatures:**
  - **Memory not decreasing:** Gradient buffers not being cleared—ensure backward hooks properly zero gradients
  - **Training instability at low rank:** Tangent space projection may amplify noise; try increasing r or β
  - **Slower than expected runtime:** QR decomposition cost scales with (m+n)r²; profile if r > 64
  - **Convergence plateau:** Low-rank bottleneck; gradient EMA may have higher effective rank than r permits

- **First 3 experiments:**
  1. **Validation on GLUE single task (e.g., MNLI):** Compare MoFaSGD vs. GaLore vs. LoRA at r=8. Quick sanity check (1-2 GPU hours). Expect MoFaSGD ≈ GaLore, both slightly below full AdamW
  2. **Ablation on rank sensitivity:** Train NanoGPT for 1000 steps with r ∈ {16, 32, 64}. Plot validation loss vs. step. Should see MoFaSGD curves smoother than GaLore at low rank (Figure 1 pattern)
  3. **Memory profiling:** Run LLaMA-8B fine-tuning with gradient accumulation. Compare peak memory: MoFaSGD should achieve ~29GB vs. AdamW ~70GB (Figure 4). Verify gradient buffers are actually being freed

## Open Questions the Paper Calls Out

- **Question:** How valid and consequential is the nuclear-norm smoothness assumption for deep learning optimization in practice?
  - Basis in paper: The authors state in Section 6 that "although our theoretical results establish convergence guarantees, they rely on assumptions such as nuclear-norm smoothness. The practical relevance and limitations of such assumptions in deep learning setups remain an open area for study."
  - Why unresolved: While the assumption is theoretically motivated and enables the convergence proof, its alignment with real loss landscapes during LLM training is unverified.
  - What evidence would resolve it: Empirical validation comparing observed gradient smoothness under nuclear vs. Frobenius norms, or ablations showing whether convergence behavior matches theoretical predictions.

- **Question:** Can adaptive or dynamic rank selection strategies close the performance gap between MoFaSGD and full-rank optimizers on complex tasks?
  - Basis in paper: Section 6 states "MoFaSGD may underperform compared to full-rank methods on more complex tasks. Exploring adaptive or dynamic rank selection strategies... could help mitigate this gap."
  - Why unresolved: All experiments use fixed rank; the paper suggests but does not implement or test adaptive rank allocation based on projection residuals or layer-wise error.
  - What evidence would resolve it: Results from a modified MoFaSGD that dynamically adjusts rank per layer or over time, showing whether it recovers performance while maintaining memory savings.

- **Question:** What is the impact of the discarded spectral energy (the ~20% not captured in top-r singular values) on final model quality and downstream task performance?
  - Basis in paper: Figure 6a shows top-32 singular values capture ~80% of momentum energy, leaving ~20% unmodeled. The convergence bound accounts for compression error, but the practical effect of this persistent loss on complex alignment tasks (e.g., Tulu3) is unclear.
  - Why unresolved: The paper demonstrates competitive performance but does not isolate the contribution of lost spectral components to the observed 4.2% gap versus AdamW on instruction tuning.
  - What evidence would resolve it: Layer-wise or task-specific analysis correlating residual spectral mass with performance degradation, or experiments incrementally increasing rank to identify where returns diminish.

## Limitations

- Empirical evaluation limited to synthetic gradients and specific architectures (GPT-2 style, RoBERTa)
- Memory efficiency claims depend on specific hardware configurations (A100-80GB)
- Convergence analysis relies on nuclear-norm smoothness assumption that may not universally hold
- Limited testing on diverse model families and extreme scale (70B+ parameters)

## Confidence

- **High Confidence:** Core algorithmic claims (tangent space projection optimality, convergence guarantees under stated assumptions), memory reduction measurements (29GB vs 70GB on LLaMA-8B), and qualitative patterns in empirical results (MoFaSGD vs GaLore trade-offs)
- **Medium Confidence:** Exact numerical comparisons between methods (Figure 3 MMLU scores), the 80% singular value capture claim (Figure 6a), and generalization to tasks beyond the evaluated benchmarks
- **Low Confidence:** Claims about superiority over all existing methods, the universality of low-rank momentum structure across diverse tasks, and the practical impact on extremely large models (70B+ parameters)

## Next Checks

1. **Independent replication of tangent space projection optimality:** Implement and test the three alternative projection schemes (left-only, right-only, two-sided) on the same synthetic gradient sequences to verify Theorem 4.3's claim about residual minimization.

2. **Memory profiling validation:** Use NVIDIA Nsight Systems or equivalent profiling tools to verify that full-rank gradient buffers are actually being freed during MoFaSGD training, confirming the ~41GB memory savings claim on LLaMA-8B.

3. **Rank sensitivity on diverse tasks:** Test MoFaSGD with varying ranks (r ∈ {4, 8, 16, 32}) on a broader set of tasks including code generation, mathematical reasoning, and multilingual benchmarks to validate the rank-r=32 sweet spot claim and assess performance degradation at lower ranks.