---
ver: rpa2
title: 'Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action
  Order'
arxiv_id: '2512.04277'
source_url: https://arxiv.org/abs/2512.04277
tags:
- order
- reward
- fine-tuned
- cell
- solver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether scalar rewards that hint at a canonical
  solving order can improve RL post-training performance on Sudoku, even when the
  base model is fine-tuned only on randomly ordered solution sequences. They combine
  cell accuracy with an ordering reward that encourages alignment with solver trajectories,
  using bootstrapped scaling to balance their magnitudes.
---

# Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order

## Quick Facts
- **arXiv ID**: 2512.04277
- **Source URL**: https://arxiv.org/abs/2512.04277
- **Reference count**: 7
- **Primary result**: Bootstrapped mixed rewards (0.75:0.25 cell-to-order) improve RL post-training Sudoku accuracy from 0.282 to 0.496.

## Executive Summary
The paper investigates whether scalar rewards that hint at a canonical solving order can improve RL post-training performance on Sudoku, even when the base model is fine-tuned only on randomly ordered solution sequences. They combine cell accuracy with an ordering reward that encourages alignment with solver trajectories, using bootstrapped scaling to balance their magnitudes. On 9×9 Sudoku, GRPO with mixed rewards outperforms cell-only optimization, with the best 0.75:0.25 cell-to-order mixture reaching 0.496 test accuracy—improving over the random-order fine-tuned baseline by 0.214 absolute points and recovering ~90% of the gap to the solver-order fine-tuned model. This shows that coarse ordering signals, supplied solely as scalar rewards during RL, can steer models toward solver-like reasoning without modifying supervised data or architecture.

## Method Summary
The approach uses a two-stage training pipeline: first fine-tuning a GPT-2 style Transformer (8 layers, 8 heads, 512 hidden dim) on randomly ordered Sudoku solution sequences, then post-training with GRPO using mixed rewards. The mixed reward combines R_cell (set intersection accuracy) and R_order (inverse distance to solver index for correctly predicted cells). Bootstrapped scaling computes mean rewards on a validation split to set global scalars so each component contributes in the target ratio α:(1-α) at initialization. GRPO optimizes the total reward with KL regularization, batch size 128, 8 rollouts, and weight decay 0.01. The method tests mixture ratios α ∈ {0, 0.25, 0.5, 0.75, 1} to find the optimal balance between correctness and ordering guidance.

## Key Results
- Mixed rewards outperform pure cell-only optimization: 0.496 accuracy with 0.75:0.25 mixture vs 0.282 baseline
- Bootstrapped scaling prevents implicit reward domination by equalizing initial magnitudes
- Pure order-only optimization (α=0) underperforms baseline at 0.262, showing ordering needs correctness signal
- Best mixture recovers ~90% of the gap to solver-order fine-tuned model (0.496 vs 0.520)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bootstrapped reward scaling enables controlled mixture optimization by equalizing initial reward magnitudes.
- Mechanism: Before GRPO training, the frozen fine-tuned model is evaluated on a validation split to compute empirical means (R̄_cell, R̄_order). Global scalars are set so that at initialization, each component contributes to R_total in the target ratio α:(1−α). This prevents implicit domination by whichever raw reward happens to be larger.
- Core assumption: The initial reward magnitudes are representative enough that fixed scaling remains appropriate throughout training.
- Evidence anchors:
  - [Section 4.3]: "This ensures that, at initialization, each component contributes to R_total in the target ratio regardless of absolute scale."
  - [Section 5.3]: "Without such normalization, order rewards can be many times smaller than cell-accuracy rewards, causing mixtures to be implicitly dominated by the cell term regardless of α."
  - [corpus]: No direct corpus evidence on this specific scaling technique; related work on multi-objective RL (Roijers et al., 2013) is cited but does not validate this particular bootstrap approach.
- Break condition: If reward distributions shift dramatically during training, fixed scaling may become mismatched; adaptive rescaling could be needed.

### Mechanism 2
- Claim: The ordering reward provides a structural hint that guides policy toward solver-like reasoning trajectories without explicit supervised ordering.
- Mechanism: The order reward computes, for each correctly predicted cell, r_i = 1/(1 + |π*(r,c) − π̂(r,c)|), where π* is the solver index and π̂ is the model's generation index. This rewards proximity to canonical order without requiring exact matching, acting as reward shaping (Ng et al., 1999).
- Core assumption: The model can learn an implicit state-transition model (which moves are valid next) when nudged toward solver-consistent emission orders.
- Evidence anchors:
  - [Abstract]: "ordering reward that increases when the model's emission order aligns with the solver order"
  - [Section 4.2]: "This yields a reward-shaping signal that guides toward solver-like rollouts without enforcing exact sequence reproduction."
  - [Section 6]: "The ordering reward nudges the policy toward trajectories that are consistent with an implicit state-transition model."
  - [corpus]: Related work (Shah et al., 2024) shows transformers trained on solver steps encode valid cell values in hidden states; corpus does not directly validate the reward-shaping mechanism here.
- Break condition: If the task lacks a meaningful canonical ordering, or if ordering is arbitrary, this reward would provide noise rather than signal.

### Mechanism 3
- Claim: Mixed rewards outperform pure cell-only optimization because ordering signals complement correctness by reducing the search space of valid trajectories.
- Mechanism: Cell accuracy alone provides sparse, outcome-level feedback. Adding ordering rewards gives denser intermediate signal that constrains the policy toward structured rollouts. The best mixture (0.75:0.25) balances these, while pure order-only (0:1) underperforms the baseline, indicating ordering must be paired with correctness.
- Core assumption: The ordering reward is complementary rather than redundant to cell accuracy; it captures orthogonal information about solution structure.
- Evidence anchors:
  - [Section 5.2]: "Mixtures that include a non-zero ordering component outperform pure cell-accuracy optimization in most cases... pure order-only (0:1) underperforms the fine-tuned (random order) baseline at 0.262."
  - [Section 5.2]: "These results on Sudoku suggest that a coarse scalar ordering hint is complementary to cell-level correctness."
  - [corpus]: Weak direct evidence; neighbor papers on learning-order autoregressive models and constraint reordering suggest order matters for structured tasks, but do not validate this specific mixed-reward interaction.
- Break condition: If the ordering reward conflicts with correctness (e.g., ordering emphasizes wrong paths), performance degrades; this may explain poor 0:1 results.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the RL algorithm used for post-training; it optimizes arbitrary reward functions over model rollouts without a separate value function.
  - Quick check question: Can you explain how GRPO differs from standard PPO in how it computes advantages?

- Concept: **Reward Shaping**
  - Why needed here: The ordering reward is explicitly framed as reward shaping; understanding potential-based shaping helps explain why it guides without changing optimal policy.
  - Quick check question: What condition must a shaping reward satisfy to preserve policy invariance?

- Concept: **Multi-Objective Reward Composition**
  - Why needed here: The paper combines two rewards with fixed weights; understanding scalarization tradeoffs is critical for reproducing and extending results.
  - Quick check question: If two reward components have different variance, how does fixed-weight scalarization affect gradient signal?

## Architecture Onboarding

- Component map:
  - GPT-2 style Transformer (8L/8H/512d) -> Causal LM fine-tuning (random-order) -> GRPO post-training with mixed rewards -> R_total = CELL_SCALE × R_cell + ORDER_SCALE × R_order

- Critical path:
  1. Prepare dataset with both solver-order and random-order sequences
  2. Fine-tune base model on random-order sequences (this is the RL initialization point)
  3. Run bootstrap pass on validation set to compute mean rewards and set scaling factors
  4. Run GRPO with R_total = CELL_SCALE × R_cell + ORDER_SCALE × R_order
  5. Evaluate on held-out test set with greedy decoding

- Design tradeoffs:
  - Fixed vs. adaptive scaling: Fixed is simpler and enables controlled study, but may mismatch if rewards shift during training
  - Mixture ratio α: Higher α emphasizes correctness; paper finds 0.75:0.25 optimal, but this is task-dependent
  - Order reward design: Current formulation uses inverse distance; stricter penalties could enforce closer matching but may reduce exploration

- Failure signatures:
  - Pure order-only reward (α=0) underperforms baseline: ordering alone provides no correctness signal
  - High order weight (e.g., 0.25:0.75) degrades performance: ordering dominates and correctness is underweighted
  - Without bootstrapped scaling, cell reward dominates regardless of α: implicit reweighting breaks mixture control

- First 3 experiments:
  1. **Reproduce baseline gap**: Fine-tune on random-order vs. solver-order; confirm ~0.24 accuracy gap (0.282 vs. 0.520) to validate data pipeline.
  2. **Ablate scaling**: Run GRPO with mixed rewards (α=0.75) with and without bootstrapped scaling; measure whether performance drops when raw rewards are used.
  3. **Sweep α values**: Test α ∈ {0, 0.25, 0.5, 0.75, 1} to confirm the 0.75:0.25 optimum replicates; plot accuracy curve against Figure 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bootstrapped mixed reward approach generalize to other structured reasoning tasks beyond Sudoku?
- Basis in paper: [explicit] The Limitations section states: "Our experiments are limited to a single task (9×9 Sudoku)... A natural next step is to test whether these findings generalize across additional tasks, scales, and architectures."
- Why unresolved: It remains unknown whether scalar ordering hints help for tasks with different structural properties (e.g., mathematical proofs, code generation, planning domains) where canonical orderings may be less well-defined.
- What evidence would resolve it: Experiments applying identical methodology to diverse reasoning benchmarks (theorem proving, program synthesis, other constraint satisfaction problems).

### Open Question 2
- Question: Why does pure order-only optimization (α=0) underperform even the baseline fine-tuned model?
- Basis in paper: [inferred] Results show pure order-only achieves 0.262 accuracy versus 0.282 for the random-order fine-tuned baseline—a 0.020 absolute drop. The paper notes order signal is "most effective when paired with a correctness objective" but does not explain the degradation mechanism.
- Why unresolved: The order reward should theoretically guide toward valid trajectories, yet without correctness signal it actively harms performance rather than remaining neutral.
- What evidence would resolve it: Analysis of generated sequences under pure order optimization; ablation studies on the order reward formulation to identify failure modes.

### Open Question 3
- Question: What internal representations emerge when training with mixed rewards—does the model develop implicit state-transition models?
- Basis in paper: [inferred] The paper hypothesizes that ordering rewards "nudge the policy toward trajectories that are consistent with an implicit state-transition model" but provides no mechanistic analysis of learned representations.
- Why unresolved: Prior work (Shah et al., 2024) showed solver-order models encode valid cell values in hidden states; whether mixed-reward RL produces similar representations without solver-order supervision is unexplored.
- What evidence would resolve it: Probing experiments on hidden states to test whether models trained with mixed rewards encode valid-next-move information similarly to solver-order fine-tuned models.

### Open Question 4
- Question: Is there a principled method to predict the optimal cell-to-order mixture ratio for a given task and model scale?
- Basis in paper: [inferred] The paper tests five discrete ratios and finds 0.75:0.25 optimal, but provides no theoretical justification or generalizable heuristic for this finding.
- Why unresolved: The bootstrapped scaling normalizes initial magnitudes, but the optimal training-time balance may depend on task properties, solution trajectory diversity, or model capacity.
- What evidence would resolve it: Systematic ablations across tasks with varying trajectory structures; adaptive or learned mixture methods compared against fixed ratios.

## Limitations

- The study focuses on a single structured task (Sudoku), limiting generalizability to other domains with different structural properties.
- The fixed bootstrapped scaling assumes reward distributions remain stable during training, which may not hold for tasks with shifting dynamics.
- The optimal 0.75:0.25 mixture ratio is presented as task-specific without theoretical justification for why this balance works best.

## Confidence

- **High confidence**: The experimental results on Sudoku (accuracy improvements from 0.282 to 0.496 with 0.75:0.25 mixture) are well-documented with clear metrics and controlled ablations. The mechanism of bootstrapped scaling preventing implicit reward domination is empirically validated.
- **Medium confidence**: The claim that ordering rewards are "complementary" to correctness is supported by ablation results but lacks deeper theoretical grounding. The mechanism by which ordering signals guide toward solver-like reasoning is plausible but not rigorously proven.
- **Low confidence**: The scalability of this approach to larger or more complex structured tasks is not addressed. The optimal mixture ratio appears arbitrary without systematic exploration of why 0.75:0.25 works best.

## Next Checks

1. **Ablate the bootstrap scaling**: Run GRPO with mixed rewards (α=0.75) but use raw reward values without normalization. Measure whether performance drops significantly, confirming that bootstrapped scaling is essential for controlled mixture optimization.

2. **Test mixture stability across runs**: Repeat the 0.75:0.25 mixture experiment across multiple random seeds. Report mean and standard deviation of final test accuracy to quantify result stability and determine if the claimed improvement is robust.

3. **Validate on a different structured task**: Apply the same methodology to a different domain (e.g., crossword puzzles or planning tasks) to test whether ordering rewards provide similar benefits outside Sudoku. This would validate the broader applicability of the approach.