---
ver: rpa2
title: Robust Training with Data Augmentation for Medical Imaging Classification
arxiv_id: '2506.17133'
source_url: https://arxiv.org/abs/2506.17133
tags:
- adversarial
- data
- training
- robustness
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a robust training algorithm with data augmentation
  (RTDA) to mitigate vulnerabilities of deep neural networks to adversarial attacks
  and distribution shifts in medical image classification. The method integrates adversarial
  training with data augmentations that simulate natural variations, aiming to achieve
  both adversarial robustness and improved generalization performance.
---

# Robust Training with Data Augmentation for Medical Imaging Classification

## Quick Facts
- **arXiv ID:** 2506.17133
- **Source URL:** https://arxiv.org/abs/2506.17133
- **Reference count:** 31
- **One-line primary result:** RTDA achieves superior adversarial robustness and improved generalization to distribution shifts while maintaining high clean accuracy across three medical imaging datasets.

## Executive Summary
This paper proposes Robust Training with Data Augmentation (RTDA), a method that integrates adversarial training with data augmentations to improve deep neural networks' robustness to adversarial attacks and distribution shifts in medical image classification. The approach applies cross-entropy loss directly to adversarial samples and enforces consistency between clean, augmented, and adversarial outputs using Jensen-Shannon Divergence. Evaluated across mammograms, X-rays, and ultrasound datasets, RTDA outperforms six baseline techniques in both adversarial accuracy and Brier Score metrics, demonstrating effective balance between adversarial robustness and distribution shift generalization.

## Method Summary
RTDA modifies the training objective by applying cross-entropy loss to adversarial samples rather than clean inputs, while simultaneously enforcing consistency between predictions on clean, augmented, and adversarial inputs using Jensen-Shannon Divergence. The method generates adversarial perturbations using Projected Gradient Descent (PGD) and applies stochastic augmentations that simulate natural variations like contrast changes. This dual approach aims to create decision boundaries that are both resistant to adversarial perturbations and invariant to natural distribution shifts, addressing the typical trade-off where adversarial training degrades clean accuracy.

## Key Results
- RTDA maintains high clean accuracy (>85% for X-ray) while achieving superior adversarial robustness compared to standard adversarial training
- The method demonstrates improved generalization performance under distribution shifts, particularly for low-contrast scenarios
- RTDA outperforms six baseline techniques across three medical imaging datasets using both adversarial accuracy and Brier Score metrics

## Why This Works (Mechanism)

### Mechanism 1
Applying cross-entropy loss directly to adversarial samples rather than clean inputs preserves adversarial robustness without severe clean accuracy degradation. By optimizing for $L_{CE}(f(x+\delta), y)$, the model learns a decision boundary less susceptible to attack perturbations while the consistency loss prevents excessive deviation from the clean data manifold. Core assumption: training perturbations represent potential test-time attacks. Evidence: Section 5.1 shows RTDA maintains high clean accuracy while achieving superior robustness. Break condition: epsilon too high causes underfitting.

### Mechanism 2
Enforcing consistency between clean, augmented, and adversarial outputs via Jensen-Shannon Divergence improves generalization to distribution shifts. The JSD term $L_{JSD}(f(x), f(g_{Aug}(x)), f(x+\delta))$ acts as a regularizer, smoothing the semantic landscape so natural variations don't drastically alter predictions. Core assumption: augmentations accurately simulate deployment distribution shifts. Evidence: Section 5.3 shows RTDA maintains stable performance under low-contrast shifts. Break condition: excessive augmentations wash out discriminative features.

### Mechanism 3
Integrating augmentations into the robust training loop mitigates the trade-off where standard adversarial training sacrifices clean accuracy. By exposing the model to augmented samples during robust optimization, RTDA prevents overfitting to specific adversarial noise patterns. Core assumption: model capacity sufficient for learning features invariant to both perturbations and augmentations. Evidence: Abstract claims effective balance between robustness and generalization. Break condition: batch size too small destabilizes gradient estimates.

## Foundational Learning

**Concept:** Projected Gradient Descent (PGD)
- **Why needed here:** RTDA relies on PGD to generate adversarial perturbations ($\delta^*$) before model updates. Understanding the computational cost of the attack loop is critical for implementation.
- **Quick check question:** Can you explain why the number of PGD steps (7 in training vs. 20 in testing) affects the trade-off between training speed and robustness?

**Concept:** Jensen-Shannon Divergence (JSD)
- **Why needed here:** This is the mathematical core of the consistency loss. Unlike KL divergence, JSD is symmetric and bounded, making it suitable for measuring distance between three probability distributions.
- **Quick check question:** Why is symmetry important when calculating divergence between a clean distribution and an augmented distribution?

**Concept:** Distribution Shift (Covariate Shift)
- **Why needed here:** The paper targets natural variations like contrast shifts common in medical imaging from different scanners/protocols. Distinguishing this from adversarial attack is key to understanding why data augmentation combines with adversarial training.
- **Quick check question:** In the context of this paper, is a "contrast shift" considered an adversarial attack or a distribution shift?

## Architecture Onboarding

**Component map:** Input Batch -> Augmentation Pipeline -> Adversarial Module -> Forward Pass -> Loss Aggregator

**Critical path:** The generation of adversarial samples via the PGD loop is the primary computational bottleneck. The forward pass is tripled compared to standard training (clean + aug + adv).

**Design tradeoffs:**
- **Epsilon ($\epsilon$) selection:** Dataset-specific epsilons (2.0 for X-ray, 0.15 for Mammogram) balance robustness and clean feature preservation
- **Lambda ($\lambda$):** Balances adversarial classification loss against consistency loss; specific value not specified in text

**Failure signatures:**
- **RobustAugMix behavior:** High clean accuracy but significant drop under adversarial attack indicates $L_{CE}$ term dominating
- **Over-regularization:** Poor performance on both clean and adversarial data suggests JSD weight too high or augmentations too aggressive

**First 3 experiments:**
1. **Baseline verification:** Implement RTDA loss (Eq. 10) on X-ray dataset (ResNet18) and plot Adversarial Accuracy curve to verify it lies between Standard Training and pure Adversarial Training
2. **Ablation on Loss Term:** Compare RTDA where $L_{CE}$ is applied to $x$ (RobustAugMix) vs. $x+\delta$ (RTDA) to quantify adversarial accuracy gains
3. **Distribution Shift Test:** Train on standard data, test on high-contrast shifted data to verify RTDA maintains lower Brier Score than standard Adversarial Training

## Open Questions the Paper Calls Out
- How does RTDA perform against adversarial attack types beyond PGD, such as AutoAttack, Square Attack, or query-based black-box attacks?
- Does RTDA maintain its robustness-generalization trade-off in real-world clinical deployment with multi-site data?
- Why does RTDA sacrifice clean accuracy on the POCUS ultrasound dataset but not on X-ray and mammogram datasets?
- How does RTDA generalize to distribution shifts beyond contrast variations, such as changes in imaging equipment manufacturers, scanning protocols, or patient demographics?

## Limitations
- Missing hyperparameter specifications (λ value, augmentation parameters) create reproducibility challenges
- Dataset accessibility issues, particularly with the non-public POCUS dataset
- Evaluation scope limited to ℓ₂-bounded attacks and contrast-based distribution shifts, potentially missing real-world clinical scenarios

## Confidence
- **High Confidence:** Claims about maintaining higher clean accuracy than standard adversarial training while achieving superior adversarial robustness
- **Medium Confidence:** Mechanism explanation that JSD consistency improves distribution shift generalization
- **Low Confidence:** Specific architectural choices due to missing specifications

## Next Checks
1. **Ablation Study on Loss Components:** Implement RTDA with controlled variations where L_CE is applied to x (clean) versus x+δ (adversarial) to quantitatively measure the contribution of this specific modification
2. **Cross-Dataset Generalization Test:** Train RTDA on X-ray data and evaluate on mammography data (and vice versa) to assess generalizability across imaging modalities
3. **Extended Attack Robustness Evaluation:** Test RTDA against ℓ∞-bounded attacks and spatial transformation attacks (rotation, translation) to verify robustness extends beyond ℓ₂ attacks