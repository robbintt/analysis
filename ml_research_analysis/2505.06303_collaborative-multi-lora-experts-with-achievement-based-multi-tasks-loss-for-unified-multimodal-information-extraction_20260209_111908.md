---
ver: rpa2
title: Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for
  Unified Multimodal Information Extraction
arxiv_id: '2505.06303'
source_url: https://arxiv.org/abs/2505.06303
tags:
- tasks
- lora
- task
- experts
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency and gradient
  conflicts in multi-task fine-tuning for multimodal information extraction (MIE).
  It proposes Collaborative Multi-LoRA Experts with Achievement-based Multi-Task Loss
  (C-LoRAE), a parameter-efficient fine-tuning method that uses a universal LoRA expert
  for shared knowledge and task-specific LoRA experts for specialized features, combined
  with mutual information maximization and an expert-motivated gate router.
---

# Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction

## Quick Facts
- arXiv ID: 2505.06303
- Source URL: https://arxiv.org/abs/2505.06303
- Authors: Li Yuan; Yi Cai; Xudong Shen; Qing Li; Qingbao Huang; Zikun Deng; Tao Wang
- Reference count: 12
- Key outcome: C-LoRAE achieves superior overall performance compared to full-parameter fine-tuning and vanilla LoRA methods while using comparable parameters to LoRA, setting new state-of-the-art results on four datasets.

## Executive Summary
This paper addresses the computational inefficiency and gradient conflicts in multi-task fine-tuning for multimodal information extraction (MIE). The proposed C-LoRAE method uses a universal LoRA expert for shared knowledge and task-specific LoRA experts for specialized features, combined with mutual information maximization and an expert-motivated gate router. An achievement-based multi-task loss balances training across tasks with varying sample sizes. Experiments on seven benchmark datasets show C-LoRAE achieves superior overall performance while maintaining parameter efficiency.

## Method Summary
C-LoRAE extends low-rank adaptation (LoRA) by incorporating a universal expert to learn shared multimodal knowledge from cross-MIE tasks and task-specific experts to learn specialized instructional task features. The method employs mutual information maximization to maximize the knowledge exchange between universal and task-specific expert representations, and introduces an experts-motivated gate router to adaptively blend their outputs. Additionally, an achievement-based multi-task loss balances training progress across instruction tasks with varying sample sizes.

## Key Results
- C-LoRAE Large achieves 521.3 overall score compared to 503.2 for vanilla LoRA Large on all datasets
- Sets new state-of-the-art results on four datasets (Twitter-15, Resume-14, Dialog2024, GUM)
- Outperforms full-parameter fine-tuning while using comparable parameters to LoRA (7.44M vs 7.44M trainable parameters)
- Ablation studies show mutual information maximization contributes 6.5 points and achievement-based loss contributes 10.6 points to overall performance

## Why This Works (Mechanism)

### Mechanism 1: Dual-Level LoRA Decomposition for Gradient Conflict Mitigation
Separating shared multimodal knowledge from task-specific instruction features reduces negative transfer in multi-task instruction fine-tuning. A universal LoRA expert (ULoRA) trains on all instruction tasks to capture cross-task multimodal patterns, while task-specific LoRA experts (TLoRA) optimize exclusively on their respective task datasets. The ULoRA uses standard LoRA rank ($r$), while each TLoRA uses reduced rank ($r/N$) to constrain capacity and prevent overfitting to limited task-specific data.

### Mechanism 2: Mutual Information Maximization for Expert Knowledge Transfer
Maximizing mutual information between universal and task-specific expert representations facilitates knowledge exchange without forcing direct parameter sharing. The variational information maximization method treats the universal expert as a teacher that distills essential multimodal knowledge to task-specific student experts. The MIM loss is computed between representations and jointly optimized with the multi-task loss.

### Mechanism 3: Achievement-Based Dynamic Loss Weighting
Normalizing task losses by their relative achievement (current F1 / SoTA F1) dynamically balances training progress across tasks with imbalanced sample sizes. Task weight decreases as task approaches SoTA performance, where margin prevents weight increase when exceeding SoTA. Weights are softmax-normalized to maintain gradient magnitude and prevent underfitting low-performing tasks.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: C-LoRAE extends vanilla LoRA; understanding the decomposition $h = W_0x + BAx$ and the role of rank $r \ll \min(d_{out}, d_{in})$ is essential for grasping how expert modules attach to frozen transformer weights.
  - Quick check question: Given a linear layer with $W_0 \in \mathbb{R}^{4096 \times 4096}$ and LoRA rank $r=8$, how many trainable parameters does one LoRA module add?

- **Mixture of Experts (MoE) Gating**
  - Why needed here: The experts-motivated gate router uses softmax-based gating to blend universal and task-specific outputs; understanding sparse vs. dense routing is critical.
  - Quick check question: If gate weights are $[0.7, 0.3]$ for [universal, task-specific] and outputs are $U=[1,2,3]$, $D=[4,5,6]$, what is the final blended output?

- **Multi-Task Gradient Conflicts (Negative Transfer)**
  - Why needed here: The paper's central problem is gradient conflicts from indiscriminately aggregating diverse instruction tasks; understanding why shared parameters can degrade when task gradients oppose each other motivates the dual-expert design.
  - Quick check question: If Task A's gradient for a shared parameter is $+0.5$ and Task B's gradient is $-0.3$, what is the combined gradient with equal weighting, and which task's objective is compromised?

## Architecture Onboarding

- **Component map**: Frozen Backbone -> Universal LoRA Expert -> Task-Specific LoRA Experts -> Gate Router -> Blended Output
- **Critical path**: Input token passes through frozen $W_0$ and both ULoRA/TLoRA in parallel. Gate router computes blending weights for this token. Blended LoRA output is added to $W_0 x$. Task loss is computed with achievement weights. MIM loss is computed between ULoRA and TLoRA representations. Total loss is backpropagated to LoRA parameters and gate router.
- **Design tradeoffs**: ULoRA rank vs. TLoRA rank (full vs. $r/N$), number of experts (current $N=3$), gate router complexity (simple linear vs. MLP), MIM coefficient $\beta$ (current 0.01).
- **Failure signatures**: Dominant task bias (disproportionate performance on one task), gate collapse (one expert unused), MIM divergence (oscillating or non-converging loss), underfitting small-sample tasks (MNER with 4,000 samples).
- **First 3 experiments**: 1) Single-task baseline: Train vanilla LoRA on each task individually to establish upper bounds. 2) Ablation: ULoRA vs. TLoRA only on target dataset mix. 3) Hyperparameter sweep: Rank and $\beta$ on validation split to identify robust configurations.

## Open Questions the Paper Calls Out
- How can the collaboration mechanism between the universal and task-specific LoRA experts be further optimized?
- Can the C-LoRAE framework be effectively generalized to other multimodal tasks beyond Information Extraction (IE)?
- How sensitive is the Achievement-based Multi-task Loss to the accuracy of the reference State-of-the-Art (SoTA) values ($P_m$)?

## Limitations
- Limited scope to MIE task set (MNER, MRE, MEE) without validation on other instruction task families
- No corpus validation of MIM approach for LoRA expert collaboration
- Performance depends critically on accurate SoTA benchmarks, which may be outdated or unavailable

## Confidence
- **High Confidence**: Superior overall performance compared to full-parameter fine-tuning and vanilla LoRA methods across seven benchmark datasets
- **Medium Confidence**: Gradient conflicts arise primarily from mixing diverse instruction formats and knowledge domains
- **Low Confidence**: MIM provides optimal knowledge transfer between universal and task-specific experts

## Next Checks
1. Apply C-LoRAE to a different multimodal instruction task family (e.g., visual reasoning or multimodal dialogue) and compare performance degradation relative to vanilla LoRA
2. Replace the MIM loss with alternative distillation objectives (e.g., KL divergence, cosine similarity) while keeping all other components constant
3. Systematically vary the SoTA performance values used in the achievement-based loss and measure the impact on task weighting distribution and overall performance