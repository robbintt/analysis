---
ver: rpa2
title: 'Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models'
arxiv_id: '2601.08955'
source_url: https://arxiv.org/abs/2601.08955
tags:
- uni00000013
- agent
- world
- lookahead
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Imagine-then-Plan (ITP), a unified framework
  that equips LLM-based agents with adaptive lookahead imagination via learned world
  models. By extending the standard POMDP to a Partially Observable and Imaginable
  MDP (POIMDP), ITP enables agents to explicitly reason over both the observable present
  and imagined future trajectories.
---

# Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models

## Quick Facts
- **arXiv ID**: 2601.08955
- **Source URL**: https://arxiv.org/abs/2601.08955
- **Reference count**: 38
- **Primary result**: Proposes ITP framework with adaptive lookahead; ITPI enhances zero-shot performance, ITPR achieves highest success rates on ALFWorld and ScienceWorld benchmarks.

## Executive Summary
Imagine-then-Plan (ITP) is a unified framework that equips LLM-based agents with adaptive lookahead imagination via learned world models. By extending POMDP to POIMDP, ITP enables explicit reasoning over observable present and imagined future trajectories. The core innovation is an adaptive lookahead mechanism that dynamically scales the imagination horizon by trading off ultimate goal and estimated task progress, balancing foresight reliability and computational efficiency. Extensive experiments demonstrate ITP significantly outperforms competitive baselines, with ITPI substantially enhancing zero-shot planning performance and ITPR achieving the highest success rates across all tested backbone models.

## Method Summary
The ITP framework trains an LLM world model to predict future states given current state and action, then uses this model to generate imagined trajectories of variable length. The framework consists of two variants: ITPI (training-free) which uses reflection on imagined trajectories for decision-making, and ITPR (reinforcement-trained) which jointly optimizes policy and imagination horizon via A2C. The adaptive lookahead mechanism uses a K-head predictor to select the optimal imagination depth at each timestep based on task progress and computational budget, with pseudo-labels derived from expert trajectories. The method is evaluated on ALFWorld and ScienceWorld benchmarks with multiple LLM backbones.

## Key Results
- ITPR achieves highest success rates across all backbones (e.g., 88.57% vs 82.14% IWM on ALFWorld with Qwen3-8B)
- Adaptive lookahead dominates random horizon selection with higher success rate and lower/stable budget
- Ablation study shows online reinforcement training significantly improves performance (ALFWORLD SR from 71.42% to 88.57% with online RT)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Horizon Selection
A learned K-head predictor selects imagination depth at each timestep by balancing expected task progress against rollout cost. The predictor is trained via pseudo-labels from expert trajectories and refined through online RL. Task complexity varies across timesteps, warranting deeper foresight for some decisions while others are trivial.

### Mechanism 2: Proactive Conflict Detection
Policy decisions are conditioned on imagined trajectories rather than just current observations, enabling proactive conflict detection and self-correction. The policy generates actions conditioned on both observable state and imagined trajectory from world model.

### Mechanism 3: Reinforcement-Trained Lookahead
ITPR outperforms training-free reflection because it explicitly learns when imagination is beneficial through joint optimization of policy and K-head via A2C with strategic imagination rewards.

## Foundational Learning

- **POMDP (Partially Observable Markov Decision Process)**: Understanding the base formulation is prerequisite to grasping how imagination augments the decision process. Quick check: Can you explain why the transition function T and observation space O are separate in POMDP, and what information the agent lacks access to?
- **World Models as Predictive Dynamics**: The entire ITP framework hinges on using an LLM as a world model that predicts next states. Quick check: Given a state "holding apple, near fridge" and action "open fridge," what should a world model predict, and what could go wrong if the prediction is inaccurate?
- **Actor-Critic (A2C) Reinforcement Learning**: ITPR's Stage 3 uses A2C to jointly optimize policy and K-head. Quick check: In the ITPR reward rt+1 = renv − λK·Kt − λstep, what behavior does each penalty term discourage, and what happens if λK is set too high?

## Architecture Onboarding

- **Component map**: Expert Demonstrations D_exp → SFT → Initial Policy π_θ0 → Rollout Trajectories D_roll → World Model M_ϕ ← D_exp ∪ D_roll. For ITP_R: M_ϕ + π_θ0 → Pseudo-Labels D_K → Warm-Up Training → Online A2C (joint optimization of K-head + Policy).
- **Critical path**: World model quality is foundational—M_ϕ errors propagate through all downstream stages. Pseudo-labeling determines K-head initialization; noisy labels cascade to poor RL warm-start. Online A2C is the performance differentiator.
- **Design tradeoffs**: ITPI vs ITP_R (plug-and-play vs. highest performance), K_max selection (too low misses dependencies, too high increases error), λK penalty (controls frugality), assumption that hyperparameters are tuned per-benchmark.
- **Failure signatures**: World model collapse (repetitive/incoherent states), K-head mode collapse (constant horizon), reward hacking (minimizing K_t without task completion), distribution shift (expert-trained M_ϕ fails on novel states).
- **First 3 experiments**: 1) Validate M_ϕ on held-out transitions—compute next-state prediction accuracy and inspect failure modes. 2) Train ITP_R with fixed K vs adaptive K, measure SR and normalized budget. 3) Run ITPI on small task subset with different reflection prompts to identify quality bottlenecks.

## Open Questions the Paper Calls Out

- **Multimodal environments**: Current evaluation focuses on text-based benchmarks and doesn't fully capture challenges of multimodal environments where visual or sensorimotor observations may introduce additional noise.
- **Computational overhead**: World models introduce higher inference-time overhead; further optimization via speculative decoding or distilled world models is needed for real-time applications.
- **Open-world robustness**: Framework relies on specific admissible action lists which may not exist in open-ended settings, limiting robustness in environments with dynamic or unconstrained action spaces.

## Limitations
- World model fidelity is untested—no quantitative validation of prediction error vs. rollout horizon provided
- Pseudo-labeling assumptions are unverified—no analysis of noise in expert-derived horizon labels
- Scalability to longer horizons is unproven—results only reported for K_max=5/8 with no evidence of compound error analysis

## Confidence
- **High confidence**: Core framework design and ITPR achieving higher SR than ITPI (given ablation study)
- **Medium confidence**: Adaptive lookahead mechanism effectiveness (better SR/budget trade-offs but K-head strategy unverified)
- **Low confidence**: World model quality claims (no empirical validation of imagination reliability)

## Next Checks
1. **World model rollout error analysis**: Measure next-state prediction accuracy for K=1,2,3,5,8 steps on held-out trajectories. If accuracy drops below 60% for K≥3, the adaptive lookahead mechanism is operating on unreliable signals.
2. **K-head strategy verification**: Log the K distribution selected by the adaptive predictor across different task types and difficulty levels. If the predictor consistently selects K=0 or K=K_max regardless of task, it indicates mode collapse rather than strategic adaptation.
3. **Imagination ablation**: Run experiments with noisy world models (e.g., randomly corrupt 10-30% of predicted states) and compare ITP performance to the clean model. If performance degrades significantly, it confirms dependence on imagination fidelity rather than just having lookahead capability.