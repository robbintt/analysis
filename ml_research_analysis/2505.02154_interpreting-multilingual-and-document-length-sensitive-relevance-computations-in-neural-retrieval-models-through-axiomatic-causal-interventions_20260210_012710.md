---
ver: rpa2
title: Interpreting Multilingual and Document-Length Sensitive Relevance Computations
  in Neural Retrieval Models through Axiomatic Causal Interventions
arxiv_id: '2505.02154'
source_url: https://arxiv.org/abs/2505.02154
tags:
- term
- query
- patching
- retrieval
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study validates and extends work on interpretability in neural
  retrieval models using axiomatic causal interventions. It reproduces key experiments
  on term frequency encoding and applies activation patching to Spanish, Chinese,
  and document-length focused tasks.
---

# Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions

## Quick Facts
- arXiv ID: 2505.02154
- Source URL: https://arxiv.org/abs/2505.02154
- Reference count: 18
- Key outcome: Validates activation patching for neural retrieval interpretability, confirming term frequency information migrates to CLS token across languages and layers, but finds no clear attention head specialization for multilingual or document-length tasks.

## Executive Summary
This study validates and extends causal intervention methods for interpreting neural retrieval models by applying activation patching to term frequency (TFC1) and document length (LNC1) axioms across English, Spanish, and Chinese. The research confirms that term frequency information is initially distributed across query term occurrences and later concentrated in the CLS token, with similar patterns across languages. While specific attention heads were identified for English term frequency tasks, no comparable heads emerged for multilingual or length-focused tasks. The findings support the effectiveness of activation patching for isolating task-dependent behavior but highlight challenges in generalization and reproducibility across languages and perturbation types.

## Method Summary
The study uses activation patching to perform causal interventions on neural retrieval models, specifically the TAS-B model. Researchers create diagnostic triples (query, baseline document, perturbed document) where perturbations change relevance according to IR axioms. Activations from one run are cached and patched into another, measuring how well the patched output recreates the perturbed result. Patching impact is calculated as (Score_patched - Score_baseline) / (Score_perturbed - Score_baseline). Experiments examine token-level and attention head-level behavior across layers for term frequency and document length information processing.

## Key Results
- Term frequency information is localized across query term occurrences and later concentrated in the CLS token, consistent across English, Spanish, and Chinese.
- Specific attention heads (L0H9, L1H6, L2H3, L3H8) were identified for English term frequency tasks but showed no similar patterns for multilingual or document-length tasks.
- Document-length information is also found in the CLS token, though with smaller magnitude than term frequency effects.
- The activation patching method effectively isolates task-dependent behavior in neural retrieval models, supporting its use for interpretability studies.

## Why This Works (Mechanism)

### Mechanism 1: Activation Patching Isolates Task-Dependent Behavior
Causal interventions via activation patching can localize where specific information is processed by creating diagnostic triples where perturbation changes relevance according to an IR axiom. High patching impact indicates the patched component carries targeted information. This works when the model consistently follows the axiom being tested.

### Mechanism 2: Term Frequency Information Migrates to CLS Token Across Layers
Query term frequency is initially distributed across term positions, then aggregated into the CLS token in later layers. Early layers show high patching impact at query term tokens, while later layers show strongest impact at the CLS token, indicating information consolidation for sequence-level output.

### Mechanism 3: Language-Dependent Attention Head Specialization
Specific attention heads encode term frequency information in English, but these heads do NOT generalize to other languages. Four attention heads show high patching impact for English append perturbations, but Spanish and Chinese experiments show no comparable head specialization.

## Foundational Learning

- **Concept: Activation Patching vs. Probing**
  - Why needed here: Probing shows correlation; patching shows causation. Claims depend on causal intervention logic.
  - Quick check question: If you patch an activation and the output changes toward the baseline, what does that prove about that activation's role?

- **Concept: IR Axioms (TFC1, LNC1)**
  - Why needed here: The diagnostic dataset and patching experiments are designed around these formal constraints on how relevance should behave.
  - Quick check question: TFC1 says more query term occurrences → higher score. How would you construct a (q, d_baseline, d_perturbed) triple to test this?

- **Concept: CLS Token Pooling in Bi-Encoders**
  - Why needed here: The relevance computation uses dot product of query and document CLS embeddings. All sequence-level information must flow through this bottleneck.
  - Quick check question: Why would document length information need to be encoded in the CLS token rather than distributed across token positions?

## Architecture Onboarding

- **Component map:** Document tokens → early layers (TF info at query term positions) → Attention heads aggregate information (layer-dependent groups) → Later layers → CLS token consolidation → CLS embedding → dot product with query embedding → relevance score

- **Critical path:** Document tokens processed through early layers where TF information appears at query term positions, attention heads aggregate information across layers, later layers consolidate all information into the CLS token, which is used for final relevance scoring via dot product with query embedding.

- **Design tradeoffs:** Residual stream vs. attention output patching (residual shows stronger late-layer CLS impact); prepend vs. append perturbation (append produces cleaner head identification; prepend shows stronger token impact but no head patterns); single-run reporting (faster but no variance estimates).

- **Failure signatures:** Axiom violation (perturbation doesn't change relevance as expected, making patching scores noisy); language mismatch (English heads don't transfer to other languages); metric limitations (formula assumes consistent perturbation direction).

- **First 3 experiments:**
  1. Reproduce block experiment (residual stream): Patch activations token-by-token for append perturbation; verify CLS token shows highest impact in Layer 5.
  2. Test a new language: Apply TFC1 patching to mMARCO German or Russian; check if term frequency still migrates to CLS (likely yes) and if any heads specialize (likely no).
  3. Ablate identified heads: Zero out L0H9, L1H6, L2H3, L3H8 and measure relevance score degradation on English data to confirm functional importance.

## Open Questions the Paper Calls Out

### Open Question 1
Do specific attention heads exist for processing term frequency in non-English languages, and do they function similarly to the English-specific heads identified in prior work? While the location of term frequency information (CLS token) generalizes across Spanish and Chinese, the specific attention heads responsible for moving this information could not be isolated using the current patching setup.

### Open Question 2
Can activation patching be combined with circuit analysis or sparse autoencoders to provide more robust causal validation of neural retrieval mechanisms? The current study focuses on singular module patching, which may miss complex interactions between components or fail to disentangle polysemantic neurons.

### Open Question 3
How can the activation patching metric be adapted to handle cases where models violate the underlying retrieval axioms? The current formula relies on a consistent difference between baseline and perturbed scores; when this relationship breaks, the "Patching Impact" becomes noisy or uninterpretable.

## Limitations
- Axiom compliance assumption: Results depend on the model consistently following the IR axioms being tested, which wasn't validated across the diagnostic dataset.
- Single-run design: No variance estimates from multiple runs makes it impossible to distinguish true signal from stochastic variation.
- Attention head brittleness: Head identification is fragile (append-only, top-ranked only) and doesn't generalize to other languages or perturbation types.

## Confidence
- **High confidence**: CLS token consolidation mechanism (term frequency and document length information flows to CLS token in later layers).
- **Medium confidence**: Term frequency distribution across query term occurrences (weighted toward early positions).
- **Low confidence**: Language-specific attention head specialization for term frequency encoding.

## Next Checks
1. **Axiom compliance filtering validation**: Measure how often the diagnostic dataset actually satisfies TFC1/LNC1 before running patching experiments, then verify patching impact scores become cleaner post-filtering.
2. **Multilingual CLS migration replication**: Apply the same residual stream patching experiments to German or Russian from mMARCO to confirm language-independent CLS consolidation.
3. **Head ablations for functional importance**: Zero out the four identified heads (L0H9, L1H6, L2H3, L3H8) and measure actual relevance score degradation on English data to confirm functional importance beyond patching correlation.