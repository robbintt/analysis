---
ver: rpa2
title: Constitutive Components for Human-Like Autonomous Artificial Intelligence
arxiv_id: '2506.12952'
source_url: https://arxiv.org/abs/2506.12952
tags:
- function
- autonomy
- arti
- cial
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study constructs a three-layer functional hierarchy for human-like
  autonomous AI, comprising Core Functions (perception, memory, state description,
  motor execution), the Integrative Evaluation Function (integrating perception and
  memory to select actions), and the Self Modification Function (dynamically restructuring
  goals and evaluation criteria). The framework distinguishes reactive, weak autonomous,
  and strong autonomous behavioral levels, with strong autonomy defined by the ability
  to internally set and revise goals.
---

# Constitutive Components for Human-Like Autonomous Artificial Intelligence

## Quick Facts
- arXiv ID: 2506.12952
- Source URL: https://arxiv.org/abs/2506.12952
- Reference count: 26
- Key outcome: Three-layer functional hierarchy for human-like autonomous AI with Core Functions, Integrative Evaluation, and Self Modification, where strong autonomy requires the ability to internally set and revise goals

## Executive Summary
This study proposes a three-layer functional hierarchy to construct human-like autonomous artificial intelligence, distinguishing between reactive, weak autonomous, and strong autonomous behavioral levels. The framework identifies that while reinforcement learning naturally realizes core functions (as policy), it lacks the Self Modification Function essential for strong autonomy. The authors argue that achieving intrinsic goal construction requires an additional design layer supporting self-referential processing beyond standard reward-based learning, necessitating further technical extensions for complete human-like autonomy.

## Method Summary
The study constructs a theoretical three-layer architecture for autonomous AI: Level 1 comprises Core Functions (perception, memory, state description, motor execution), Level 2 implements the Integrative Evaluation Function that integrates perception and memory to select actions, and Level 3 introduces the Self Modification Function that dynamically restructures goals and evaluation criteria. The framework maps Levels 1-2 to standard reinforcement learning (policy network and memory buffer), while Level 3 represents the critical missing component for strong autonomy. The method proposes that this hierarchical structure enables agents to transition from purely reactive behavior to internally driven goal generation and modification.

## Key Results
- Three-layer hierarchy clearly distinguishes reactive (Level 1), weak autonomous (Level 2), and strong autonomous (Level 3) behavioral capabilities
- Reinforcement learning naturally realizes Core and Integrative Evaluation functions but fundamentally lacks the Self Modification Function
- Strong autonomy is defined by the ability to internally set and revise goals, requiring self-referential processing beyond standard reward-based learning

## Why This Works (Mechanism)
The three-layer hierarchy works by progressively building complexity in autonomous behavior, starting from basic perception-action loops and advancing to self-directed goal modification. Each layer adds a critical capability: Level 1 provides the basic sensorimotor interface, Level 2 integrates these functions through evaluation and policy selection, and Level 3 enables the agent to restructure its own evaluation criteria and goals. This architectural progression mirrors cognitive development in humans, where increasingly sophisticated forms of autonomy emerge from the integration of simpler functions.

## Foundational Learning
- Core Functions (Level 1): Basic perception, memory, state description, and motor execution capabilities needed for environmental interaction
  - Why needed: Provides the fundamental sensorimotor interface without which higher-level autonomy is impossible
  - Quick check: Can the agent perceive its environment and execute basic actions?
- Integrative Evaluation (Level 2): Policy network that integrates perception and memory to select coherent actions
  - Why needed: Enables goal-directed behavior within externally defined reward structures
  - Quick check: Does the agent optimize for given reward functions effectively?
- Self Modification (Level 3): Mechanism for dynamically restructuring goals and evaluation criteria
  - Why needed: Enables true strong autonomy through internal goal generation and modification
  - Quick check: Can the agent modify its own objective function without external intervention?

## Architecture Onboarding

Component Map:
Perception/Memory/State Description/Motor Execution (Level 1) -> Policy Network (Level 2) -> Self-Modifying Architecture (Level 3)

Critical Path:
The critical path flows from Core Functions through the Integrative Evaluation to the Self Modification layer, with each level building on the previous one. Level 1 must be stable and functional before Level 2 can operate effectively, and Level 2 must be robust before Level 3 can safely modify evaluation criteria.

Design Tradeoffs:
- Stability vs. Adaptability: Self-modification introduces risk of catastrophic forgetting or instability
- Complexity vs. Efficiency: Additional layers increase computational overhead and implementation difficulty
- Safety vs. Autonomy: Greater autonomy through self-modification may reduce controllability and predictability

Failure Signatures:
- Collapsing to Weak Autonomy: Level 3 becomes static hyperparameter tuning rather than dynamic goal generation
- Catastrophic Forgetting: Unconstrained self-modification overwrites essential Core Functions
- Instability: Self-modification creates feedback loops that destabilize behavior

First Experiments:
1. Implement baseline RL agent (Level 2) and verify standard policy optimization works correctly
2. Test external reward modification to simulate human intervention and measure behavioral changes
3. Develop constrained self-modification mechanism with predefined bounds and monitor primitive task performance

## Open Questions the Paper Calls Out
The paper explicitly identifies several unresolved questions: How can current reinforcement learning frameworks be technically extended to include the Self Modification Function required for strong autonomy? What specific computational mechanisms constitute the "design layer" necessary for self-referential processing beyond standard reward-based learning? How can the theoretical three-layer hierarchy be validated through implementation in actual learning processes and social contexts? These questions remain open because the paper provides a functional framework without concrete algorithmic implementation for the critical Level 3 component.

## Limitations
- The Self Modification Function remains conceptually underspecified with no concrete algorithmic details provided
- No empirical validation or proof-of-concept implementation is presented in the current work
- Safety concerns and stability guarantees for self-modifying systems are not addressed
- The framework lacks specific technical mechanisms for goal generation and evaluation criteria modification

## Confidence
- Medium confidence in the three-layer hierarchical framework and behavioral level distinctions
- Low confidence in practical implementation of the Self Modification Function
- Medium confidence in the assertion that RL lacks strong autonomy due to missing self-modification capability

## Next Checks
1. Implement a baseline RL agent (Level 2) and test whether external reward modification can achieve equivalent behavior to an internally modifying system
2. Develop a constrained self-modification mechanism that can only adjust evaluation weights within predefined bounds, and measure stability of primitive task performance
3. Compare the time and sample efficiency of goal modification versus traditional meta-learning approaches in a multi-task environment to assess practical feasibility of the proposed architecture