---
ver: rpa2
title: Data-driven generative simulation of SDEs using diffusion models
arxiv_id: '2509.08731'
source_url: https://arxiv.org/abs/2509.08731
tags:
- paths
- diffusion
- sdes
- synthetic
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a data-driven approach to generate sample
  paths of unknown stochastic differential equations (SDEs) using conditional diffusion
  models, addressing the challenge of simulating SDEs when only finite sample paths
  are available and explicit drift/diffusion coefficients are unknown. Unlike traditional
  Monte Carlo methods or model-based approaches that estimate SDE parameters, this
  method directly learns the distribution of stochastic increments in an autoregressive
  manner using a sequence of conditional diffusion models.
---

# Data-driven generative simulation of SDEs using diffusion models

## Quick Facts
- **arXiv ID**: 2509.08731
- **Source URL**: https://arxiv.org/abs/2509.08731
- **Reference count**: 25
- **One-line primary result**: Achieves KL divergence of 0.1528 for 1D OU process and improves RL portfolio Sharpe ratio from 0.5438 to 0.5729

## Executive Summary
This paper addresses the challenge of simulating sample paths from unknown stochastic differential equations (SDEs) when only finite observational data is available. The authors propose a data-driven approach using conditional diffusion models that learn the distribution of stochastic increments in an autoregressive manner, avoiding explicit estimation of drift/diffusion coefficients. Unlike traditional Monte Carlo methods, this approach directly models the conditional distribution of increments given the current state and time, leveraging the Markov property of SDEs.

The method is demonstrated on both synthetic examples (Ornstein-Uhlenbeck process and high-dimensional Geometric Brownian Motion) and a practical application to continuous-time mean-variance portfolio selection. The results show significant improvements over benchmark methods in terms of KL divergence between real and synthetic path distributions, and demonstrate how synthetic paths can enrich market simulators to improve reinforcement learning policy performance through variance reduction in terminal wealth.

## Method Summary
The method trains a sequence of conditional diffusion models to generate sample paths of unknown SDEs. For each time step, a diffusion model learns to generate the stochastic increment conditioned on the current time and state. The forward process adds Gaussian noise to training increments until they become simple noise, while the reverse (denoising) process learns to convert this noise back into samples from the target increment distribution using a neural network score estimator. New paths are constructed autoregressively by starting at a known initial state and recursively adding the generated increments. The approach avoids explicit parameter estimation and scales to high dimensions through the use of neural network score estimators.

## Key Results
- Achieves KL divergence of 0.1528 for 1D Ornstein-Uhlenbeck process (vs benchmarks at 0.3578 and 0.6423)
- Maintains positivity in over 90% of trajectories for 100-dimensional Geometric Brownian Motion with KL divergence of 38.73
- Improves Sharpe ratio of RL portfolio policies from 0.5438 to 0.5729 for target wealth level of 1.10 through variance reduction
- Demonstrates scalability to high-dimensional problems where traditional methods fail (KL divergence > 1000 for alternatives)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conditional diffusion models can generate new SDE sample paths by learning the distribution of stochastic increments autoregressively.
- **Mechanism**: The method leverages the Markov property of SDEs. Instead of simulating the entire path at once, it trains a sequence of conditional diffusion models. Each model generates the next stochastic increment conditioned on the current time and state. The forward diffusion process gradually adds Gaussian noise to the training increments until they become simple noise. The reverse (denoising) process learns to convert this noise back into a sample from the target increment distribution, using a neural network to estimate the score function. New paths are constructed by starting at a known initial state and recursively adding the generated increments.
- **Core assumption**: The target SDE is a Markov process, allowing the next increment to depend only on the current state. The SDE has a unique strong solution.
- **Evidence anchors**:
  - [abstract] "...utilize conditional diffusion models to generate new, synthetic paths... learning the distribution of stochastic increments in an autoregressive manner."
  - [section 2] "To generate new sample paths, we train conditional diffusion models to simulate the stochastic increment of the target SDE... in an auto-regressive manner." and "Due to the Markov property of the SDE (1), we construct it in an auto-regressive manner."
  - [corpus] Evidence is weak for this specific autoregressive conditional diffusion mechanism for SDEs in the provided corpus neighbors. Related work mentions SDE matching and parameter-dependent systems but not this exact architecture.
- **Break condition**: If the underlying process is not Markovian (e.g., depends on long-term history or has a non-Markovian structure like fractional Brownian motion), the autoregressive generation conditioned only on the current state will fail to capture the true dynamics.

### Mechanism 2
- **Claim**: A score neural network provides a scalable method for estimating the score function within the diffusion model, enabling simulation of high-dimensional SDEs.
- **Mechanism**: The reverse process of a diffusion model requires estimating the score function (gradient of the log-probability density). This paper trains a neural network to parameterize the transition distribution in the denoising process, which is trained by maximizing an evidence lower bound. This approach is contrasted with a Monte Carlo empirical estimator (SDM-MC). The neural network's scalability is the key enabler for handling high-dimensional problems, such as the 100-dimensional Geometric Brownian Motion (GBM) in the experiments.
- **Core assumption**: A neural network is a sufficiently expressive function approximator to learn the score function for high-dimensional data.
- **Evidence anchors**:
  - [section 1] "...we opt for training a score neural network which offers significantly greater scalability as demonstrated in our experiments."
  - [section 3] "For a 100-dimensional geometric Brownian motion, it maintains positivity in over 90% of trajectories with KL divergence of 38.73..." (implying the network handled the high dimension).
  - [corpus] Corpus neighbor "SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations" addresses scalability in training latent SDEs, providing indirect, related support for the general challenge of scalable SDE learning.
- **Break condition**: Scalability may be lost if the dimensionality becomes so extreme that even large neural networks struggle, or if the training data is too sparse in high-dimensional space, making score estimation inaccurate.

### Mechanism 3
- **Claim**: Synthetic sample paths generated by diffusion models can enrich a market simulator, improving the performance and robustness of reinforcement learning (RL) policies for portfolio selection.
- **Mechanism**: RL agents learn by interacting with an environment. The paper uses the trained diffusion model to generate many synthetic but statistically plausible market paths. These paths are added to the set of historical data paths to create a richer environment for the RL agent to train on. This allows the agent to "experience" a wider variety of market scenarios, leading to a learned policy that is more robust and has lower variance in terminal wealth, as evidenced by a higher Sharpe ratio.
- **Core assumption**: The synthetic paths generated by the model are sufficiently realistic (distributionally similar to real data) to provide useful training signal, not just noise.
- **Evidence anchors**:
  - [abstract] "...synthetic paths enrich the market simulator, improving the Sharpe ratio of reinforcement learning policies..."
  - [section 4] "Intuitively, the synthetic paths enable the RL agent to explore more possible market scenarios and learn a more robust policy." and Table 2 shows improved Sharpe ratios and reduced variance.
  - [corpus] Corpus evidence is weak for this specific application of diffusion-generated SDE paths to RL portfolio selection. Related work focuses on learning SDEs or generating returns for static problems, not this dynamic RL enhancement.
- **Break condition**: If the generated paths fail to capture key "tail risks" or statistical properties of real markets, the RL agent may learn a policy that is brittle or overfits to the artifacts of the generative model, leading to poor real-world performance.

## Foundational Learning

- **Concept**: **Stochastic Differential Equations (SDEs) & Markov Property**
  - **Why needed here**: The core problem is simulating paths from an unknown SDE. The entire method is built on the assumption that the process is Markovian, allowing for autoregressive generation of increments.
  - **Quick check question**: Can you explain why the Markov property is essential for the autoregressive approach used in this paper?

- **Concept**: **Denoising Diffusion Probabilistic Models (DDPMs)**
  - **Why needed here**: This is the generative modeling framework used. Understanding the forward noising and reverse denoising processes is critical to grasping how the model learns to generate new increments.
  - **Quick check question**: Describe the forward and reverse processes in a DDPM and state what the neural network is trained to predict.

- **Concept**: **Reinforcement Learning (RL) from Simulated Data**
  - **Why needed here**: The paper's application relies on using generated paths to train an RL agent. Understanding how an agent learns from simulated experience is key to interpreting the empirical results.
  - **Quick check question**: Why might enriching a simulator with synthetic paths lead to a more robust RL policy?

## Architecture Onboarding

- **Component map**: Training Data -> Data Preparation -> Conditional Diffusion Models -> Autoregressive Sampler -> Downstream Application (RL)

- **Critical path**: Training the sequence of conditional diffusion models is the critical step. A failure in any single model for a specific time step/condition will corrupt all subsequent steps in the generated path.

- **Design tradeoffs**:
  - **Scalability vs. Simplicity**: The paper chooses a neural network score estimator over a Monte Carlo estimator (SDM-MC) for better scalability in high dimensions, at the cost of requiring more training.
  - **Model-free vs. Model-based**: This approach avoids explicitly estimating drift/diffusion coefficients. This is more flexible but may be less interpretable and could require more data than a well-specified parametric model.
  - **Conditional Design**: The paper conditions on current state and time. A more complex condition could capture non-Markovian effects but would increase model complexity.

- **Failure signatures**:
  - **Violation of Markov Property**: Generated paths will have incorrect long-term dynamics.
  - **Mode Collapse/Poor Training**: The diffusion model may generate unrealistic increments. For example, failing to maintain positivity for a GBM process, as seen in the benchmark models.
  - **Error Accumulation**: In the autoregressive loop, small errors in early increments can compound, leading generated paths to drift far from the true distribution over long time horizons.
  - **RL Overfitting**: The RL agent may overfit to artifacts in the synthetic data, performing well in simulation but poorly on real data.

- **First 3 experiments**:
  1.  **Replicate 1D OU Process**: Train on a small set of paths from a simple 1D Ornstein-Uhlenbeck process. Measure KL divergence between real and synthetic path distributions to validate basic functionality and compare with benchmarks.
  2.  **Test High-Dimensional GBM**: Train on a 100-dimensional Geometric Brownian Motion dataset. Assess the model's ability to maintain key properties (like positivity) and measure KL divergence. This tests the scalability claim against the neural network score estimator.
  3.  **Portfolio Optimization Task**: Train the model on historical market data (e.g., S&P 500). Generate synthetic paths and use them to enrich the training environment for an RL agent solving a mean-variance portfolio problem. Compare the resulting policy's Sharpe ratio and terminal wealth variance against agents trained only on historical or bootstrap data.

## Open Questions the Paper Calls Out

- **Non-uniform Time Grids**: The authors state they consider a uniform grid for "illustrative purposes" in this short version, implying the non-uniform case is an unexplored extension. The current formulation assumes a fixed time step $\Delta t$ for the transition kernel $p_\theta$, requiring structural changes to condition generation on variable time intervals.

- **Minimum Training Data Requirements**: The experiments use $H=100$ for 1D OU and $H=10,000$ for 100D GBM. The performance in data-scarce environments (e.g., $H < 50$) is not analyzed. Diffusion models are data-hungry; it is unclear if the method fails or requires architectural modifications when only a handful of real-world sample paths are available.

- **Path-Dependent Dynamics**: The method relies explicitly on the Markov property to condition the generation of the next increment solely on the current state $X(t_n)$. Real-world financial time series often exhibit memory or long-range dependencies (e.g., rough volatility) that violate the Markov assumption, potentially leading to model misspecification.

## Limitations

- Assumes underlying process is Markovian SDE with unique strong solution, limiting applicability to non-Markovian or weakly defined dynamics
- Autoregressive generation scheme may suffer from error accumulation over long time horizons, potentially degrading sample quality
- High-dimensional scalability demonstrated for 100D GBM but may face practical constraints with larger state spaces or sparse training data
- KL divergence metric may not fully capture tail risk behavior critical for financial applications

## Confidence

- **High Confidence**: The core mechanism of using conditional diffusion models to generate autoregressive increments for Markovian SDEs (Mechanism 1)
- **Medium Confidence**: The scalability of neural network score estimators for high-dimensional problems (Mechanism 2)
- **Medium Confidence**: The enrichment of RL training environments with synthetic paths improving policy performance (Mechanism 3)

## Next Checks

1. **Robustness to Non-Markovian Extensions**: Test the model on a stochastic process with weak dependence on history (e.g., fractional Brownian motion or an SDE with time-delayed drift). Verify whether conditioning on longer state histories improves performance.

2. **Long-Horizon Error Analysis**: Generate paths over extended time horizons (e.g., T=20 vs T=1) and quantify the divergence between real and synthetic path distributions at each time step. Plot the KL divergence or Wasserstein distance as a function of time to visualize error accumulation.

3. **Downstream Task Generalization**: Apply the framework to a different RL problem domain (e.g., robotic control with stochastic dynamics or resource allocation under uncertainty) to test whether synthetic path enrichment consistently improves policy robustness beyond the portfolio selection case.