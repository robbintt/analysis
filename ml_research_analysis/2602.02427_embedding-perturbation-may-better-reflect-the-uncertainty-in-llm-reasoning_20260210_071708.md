---
ver: rpa2
title: Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning
arxiv_id: '2602.02427'
source_url: https://arxiv.org/abs/2602.02427
tags:
- reasoning
- top3
- top5
- uncertainty
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a novel uncertainty quantification (UQ)\
  \ approach for large language models (LLMs) that uses embedding perturbations to\
  \ identify unreliable reasoning steps. Unlike prior methods relying on token probabilities,\
  \ entropy, or multiple sampling\u2014which struggle to capture long-term context\
  \ dependencies\u2014the proposed method measures how sensitive each generated token\
  \ is to small perturbations in its preceding embeddings."
---

# Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning

## Quick Facts
- arXiv ID: 2602.02427
- Source URL: https://arxiv.org/abs/2602.02427
- Reference count: 11
- Key result: Embedding perturbation achieves up to 66% detection rates for erroneous reasoning steps, outperforming token probability and entropy baselines.

## Executive Summary
This paper introduces a novel uncertainty quantification (UQ) approach for large language models (LLMs) that uses embedding perturbations to identify unreliable reasoning steps. Unlike prior methods relying on token probabilities, entropy, or multiple sampling—which struggle to capture long-term context dependencies—the proposed method measures how sensitive each generated token is to small perturbations in its preceding embeddings. Higher sensitivity suggests weaker grounding in the preceding context, indicating potential errors. Evaluated on two reasoning benchmarks—pure logical reasoning and mathematical reasoning—the method achieves up to 66% detection rates for erroneous steps, outperforming baseline UQ metrics. It is also computationally efficient, requiring only a single model run per token. Limitations include reduced effectiveness on factual hallucinations and tasks with high reasoning variability.

## Method Summary
The method applies small perturbations to the embeddings of all tokens preceding a given token, then measures how much the probability of that token changes. The core intuition is that tokens weakly grounded in their context will show larger probability shifts when the context is perturbed. The paper implements three variants: random Gaussian noise (requiring multiple samples), and two adversarial perturbations (signed and unsigned gradients) that require only a single backward pass. For each token in a generated sequence, the method computes a sensitivity score based on the variance (random) or log-probability drop (adversarial) after perturbation. These scores are then used to rank tokens and identify those most likely to be erroneous reasoning steps.

## Key Results
- Embedding perturbation achieves up to 66% detection rates for erroneous reasoning steps on logical and mathematical reasoning benchmarks
- l∞-adversarial perturbation is computationally efficient at 0.04 seconds per case versus 0.85 seconds for random perturbation
- The method outperforms token probability and entropy baselines but shows poor performance on factual hallucination detection tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokens generated with weak contextual grounding exhibit higher sensitivity to perturbations in preceding embeddings.
- Mechanism: When an LLM generates a token that is not well-supported by its reasoning context, small perturbations to the embeddings of preceding tokens cause larger variance in that token's generation probability. The perturbation tests whether the token's probability is stable (well-grounded) or unstable (uncertain).
- Core assumption: Reasoning errors arise from weak dependency on preceding context, not from word frequency effects.
- Evidence anchors:
  - [abstract] "an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings"
  - [Section 3.2] "The intuition is that the tokens whose generation is weakly grounded in the preceding context are more likely to be disrupted by such perturbations"
- Break condition: When models remain confident despite errors (paper notes "the LLM can remain highly confident even when making errors"), or when uncertainty stems from pathway selection variability rather than correctness uncertainty.

### Mechanism 2
- Claim: Adversarial perturbation (gradient-based) more efficiently identifies uncertain tokens than random sampling.
- Mechanism: Instead of multiple random noise samples, compute the gradient of total log-probability with respect to embeddings and perturb in the direction that decreases it. Tokens that drop most in probability after perturbation are flagged as uncertain.
- Core assumption: The gradient direction reveals vulnerability in the model's reasoning chain.
- Evidence anchors:
  - [Section 3.2] Eq. 5-7 define l2 and l∞ adversarial perturbations
  - [Section 4.4] "l∞-Adv. Pert. and l2-Adv. Pert. execute approximately 0.04 second per case" vs. 0.85s for random perturbation
- Break condition: When models have very large gradients (e.g., Mistral), fixed perturbation scale α may be inappropriate, degrading l2-adversarial performance.

### Mechanism 3
- Claim: Perturbation-based metrics capture long-range context dependencies better than token probability or entropy.
- Mechanism: Token probability and entropy are local statistics that can be confounded by word frequency (e.g., "we" or "$" have low probability but aren't errors). Perturbation measures how the token depends on the entire preceding context by perturbing all earlier embeddings.
- Core assumption: Reasoning errors often stem from incorrect deductions based on earlier context, not just local uncertainty.
- Evidence anchors:
  - [Section 1] "recent studies (Lu et al., 2025) demonstrate that such methods also have limited capacity to capture long-range contextual dependencies"
  - [Figure 2 Case 1] NLL flags "lying" and "Since" in addition to actual error token "telling"; perturbation pinpoints "telling"
- Break condition: For tasks with high reasoning variability (multiple valid pathways), perturbation may conflate pathway-selection uncertainty with correctness uncertainty—paper explicitly notes this limitation for Qwen.

## Foundational Learning

- Concept: Autoregressive generation
  - Why needed here: The method exploits that each token depends only on preceding tokens (not future ones), allowing single-pass perturbation analysis.
  - Quick check question: Can you explain why perturbing embeddings of later tokens doesn't affect earlier generation probabilities?

- Concept: Epistemic vs. aleatoric uncertainty
  - Why needed here: The paper frames reasoning uncertainty as epistemic (model limitation) rather than aleatoric (data ambiguity), which determines what perturbation can plausibly detect.
  - Quick check question: Would perturbation-based UQ help if the input question itself is ambiguous? Why or why not?

- Concept: Gradient-based adversarial perturbation
  - Why needed here: Understanding Eq. 5-6 (FGSM-style signed vs. unsigned gradients) is necessary to implement the efficient adversarial variants.
  - Quick check question: Why might l∞-adversarial (signed gradient) be more stable than l2-adversarial across models with different gradient magnitudes?

## Architecture Onboarding

- Component map: Embedding generation -> Perturbation application -> Probability comparison -> Sensitivity scoring -> Token ranking
- Critical path: Implement l∞-Adversarial Perturbation first—it's fastest (0.04s/case) and most stable across models (Table 1, 2). Requires only one backward pass for gradients + one forward pass with perturbed embeddings.
- Design tradeoffs:
  - Random vs. Adversarial: Random (r=20 samples) is more robust to gradient magnitude differences but 20× slower
  - Perturbation scale (σ, α): Paper finds σ ∈ [0.0001, 0.01] and α = 0.0001 work well, but large gradients (Mistral) require smaller α for l2
  - Detection threshold: Top-3, Top-5, or Top-1% of tokens—higher k increases recall but adds noise
- Failure signatures:
  - Factual hallucinations (FactScore benchmark): Perturbation performs poorly (Table 4, AUROC ~0.50-0.67) because factual errors stem from internal knowledge, not contextual reasoning
  - High pathway variability (Qwen): Multiple valid reasoning paths cause spurious uncertainty signals (Figure 2 Case 2)
  - Confident errors: Some models maintain high confidence when wrong—paper notes this as primary failure mode
- First 3 experiments:
  1. Replicate l∞-Adversarial Perturbation on GSM8K with Llama-3.1-8B, measuring top-5 detection rate for first-error-step. Expected: ~60% detection rate per Table 2.
  2. Ablate perturbation scale α ∈ {0.00001, 0.0001, 0.001} on a held-out subset to validate robustness per Figure 5.
  3. Test on FactScore-like factual generation task to confirm failure mode—expect AUROC drop compared to entropy baseline, establishing task boundaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can uncertainty quantification metrics distinguish between uncertainty regarding the correctness of a reasoning step and uncertainty arising from the selection among multiple plausible reasoning pathways?
- Basis in paper: [explicit] The Conclusion states the metrics "may not reliably disentangle uncertainty about the 'correctness of each reasoning step' from uncertainty arising from the 'selection among multiple plausible reasoning pathways'," a phenomenon observed in the Qwen model (Section 4.1).
- Why unresolved: The current method flags tokens with high sensitivity, but this sensitivity may reflect valid alternative reasoning choices rather than errors, leading to potential false positives in flexible models.
- What evidence would resolve it: A modified metric or post-processing technique that correlates high perturbation sensitivity with error rates only when pathway variance is low, or successfully decouples the two signals.

### Open Question 2
- Question: Can embedding perturbation methods be effectively adapted to detect factual hallucinations, or must they be fundamentally hybridized with other approaches?
- Basis in paper: [explicit] Section 5.1 and the Conclusion explicitly list the inability to detect factual hallucinations as a primary limitation, noting that these errors "often originate from the model's internal knowledge rather than from contextual reasoning."
- Why unresolved: The current mechanism relies on the dependency between a token and its preceding context; factual errors often lack this contextual dependency, rendering perturbations ineffective.
- What evidence would resolve it: Demonstrating a modified perturbation strategy that targets internal knowledge representations, or a hybrid system that combines perturbation with internal statistics to match baseline hallucination detection performance.

### Open Question 3
- Question: Does the localization of high-sensitivity tokens via embedding perturbation lead to improved success rates in downstream self-correction or backtracking mechanisms compared to standard probability-based uncertainty signals?
- Basis in paper: [inferred] The Introduction posits that measuring intermediate uncertainty can "enable more fine-grained and targeted interventions," and the Conclusion suggests future work should leverage these signals for "reliable backtracking," but the experiments only measure detection efficacy, not intervention success.
- Why unresolved: While the paper proves these metrics are better at *finding* errors, it does not validate the hypothesis that this improved detection translates into better automated error correction.
- What evidence would resolve it: An experiment where an LLM uses the perturbation-based uncertainty scores to trigger a reflection or correction step, showing a higher recovery rate than baselines.

## Limitations

- The method performs poorly on factual hallucination detection tasks because these errors stem from internal knowledge rather than contextual reasoning
- For models with high reasoning variability, the method may conflate legitimate alternative reasoning paths with uncertainty, leading to false positives
- The adversarial perturbation variants require careful tuning of perturbation scale α, particularly for models with large gradient magnitudes

## Confidence

**High Confidence**:
- Embedding perturbations detect reasoning errors better than token probability or entropy on the tested benchmarks
- The l∞-adversarial perturbation variant is computationally efficient (0.04s per case) and stable across models
- The method fails on factual hallucination tasks, confirming its limitation to reasoning uncertainty

**Medium Confidence**:
- The mechanism that reasoning errors arise from weak contextual grounding (not word frequency effects) is plausible but not directly tested
- The perturbation scale σ ∈ [0.0001, 0.01] and α = 0.0001 are optimal, though this appears to be based on empirical tuning rather than theoretical derivation
- The method generalizes across different LLM architectures (Llama, Qwen, Mistral) as claimed

**Low Confidence**:
- The claim that this method "may better reflect" uncertainty compared to all existing methods is not fully validated against the full spectrum of UQ approaches
- The specific detection thresholds (Top-3, Top-5, Top-1%) are presented as reasonable choices without systematic sensitivity analysis
- The method's performance on more complex reasoning tasks beyond the tested benchmarks remains unknown

## Next Checks

1. **Cross-task generalization test**: Apply the perturbation method to a diverse set of reasoning tasks including commonsense reasoning (HellaSwag, StrategyQA) and multi-step planning tasks. Compare detection rates to baseline methods to validate whether the 66% success rate generalizes beyond pure logical/mathematical reasoning.

2. **Adversarial robustness validation**: Systematically test the method against deliberately crafted adversarial examples where reasoning errors are introduced through subtle context manipulation rather than obvious logical flaws. Measure whether perturbation sensitivity still correlates with error presence under these conditions.

3. **Human annotation comparison**: Conduct a human study where annotators identify reasoning errors in LLM outputs, then compare the overlap between human-identified errors and those detected by the perturbation method. This would validate whether the method detects the same errors humans consider problematic, rather than artifacts of the model's internal representation.