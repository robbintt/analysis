---
ver: rpa2
title: 'Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large'
arxiv_id: '2505.17059'
source_url: https://arxiv.org/abs/2505.17059
tags:
- medical
- gpt-4
- text
- summarization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Medalyze is an AI-powered application designed to enhance understanding
  of complex medical texts through three specialized FLAN-T5-Large models: one for
  summarizing medical reports, one for extracting health issues from doctor-patient
  conversations, and one for identifying key questions in medical passages. The system
  was deployed across web and mobile platforms with real-time inference and data synchronization
  via YugabyteDB.'
---

# Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large

## Quick Facts
- **arXiv ID**: 2505.17059
- **Source URL**: https://arxiv.org/abs/2505.17059
- **Reference count**: 40
- **Primary result**: Medalyze outperforms GPT-4 on medical summarization with BERTScore 0.6533 vs 0.5453

## Executive Summary
Medalyze is an AI-powered application that uses three specialized FLAN-T5-Large models to enhance understanding of complex medical texts. The system includes models for summarizing medical reports, extracting health issues from doctor-patient conversations, and identifying key questions in medical passages. Deployed across web and mobile platforms with real-time inference and YugabyteDB synchronization, Medalyze achieved superior performance on domain-specific summarization tasks compared to GPT-4, with higher semantic similarity scores while maintaining low lexical overlap.

## Method Summary
The system uses three fine-tuned Flan-T5-Large models (~780M parameters each) on the Stanford medical dataset, split by content type. Training used Seq2Seq architecture with learning rate 1e-5, batch size 2, and gradient accumulation steps=2 on hardware with 16GB RAM and NVIDIA RTX 5000. Models were deployed via Flask API with YugabyteDB backend and evaluated using BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics.

## Key Results
- M-Passage achieved BERTScore 0.6533 vs GPT-4's 0.5453 on medical report summarization
- M-Question achieved SpaCy Similarity 0.8630 with near-zero BLEU (0.0084), demonstrating semantic preservation through paraphrasing
- M-Conversation showed moderate performance with BERTScore 0.4707 on lengthy conversational inputs (2992-3050 words)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sequence-to-sequence architecture outperforms autoregressive models for structured medical summarization when domain-adapted.
- **Core assumption**: Medical summarization benefits from bidirectional encoding of input before generating output, rather than left-to-right processing.
- **Evidence**: M-Passage achieved BLEU 0.0982 vs GPT-4's 0.0032; BERTScore 0.6533 vs 0.5453.

### Mechanism 2
- **Claim**: Task-specific fine-tuning on filtered medical datasets improves domain relevance more than general-purpose model scaling.
- **Core assumption**: Decomposing medical NLP into specialized tasks yields better results than a single general-purpose model.
- **Evidence**: Three separate models trained on distinct Stanford dataset subsets with aggressive hyperparameter tuning under hardware constraints.

### Mechanism 3
- **Claim**: Combining lexical and semantic evaluation metrics reveals abstractive behavior that single-metric evaluation would miss.
- **Core assumption**: Effective medical summarization should preserve meaning while simplifying language; exact word preservation is not the goal.
- **Evidence**: Consistent pattern of low lexical scores (BLEU near zero) combined with high semantic similarity (SpaCy >0.84) across all three models.

## Foundational Learning

- **Encoder-Decoder (Seq2Seq) Architecture**
  - *Why needed*: Understanding why FLAN-T5-Large was chosen over GPT-4 requires distinguishing bidirectional encoding from unidirectional autoregression.
  - *Quick check*: If you input "Patient has diabetes and hypertension" into both architectures, which sees "hypertension" while processing "diabetes"?

- **Lexical vs. Semantic Evaluation Metrics**
  - *Why needed*: Interpreting evaluation results requires knowing why BLEU could be near zero while BERTScore is high.
  - *Quick check*: A summary reads "The patient's blood sugar is elevated" for input "Patient has hyperglycemia"—would BLEU be high or low? What about BERTScore?

- **Fine-Tuning with Constrained Resources**
  - *Why needed*: The paper trains on 16GB RAM with aggressive techniques (gradient accumulation, FP16); reproducing this requires understanding these tradeoffs.
  - *Quick check*: If batch size is 2 and gradient accumulation steps are 2, what effective batch size does the optimizer see?

## Architecture Onboarding

- **Component map**:
User Input → Flask REST API → Model Selection Router → M-Passage/M-Conversation/M-Question → Summary → YugabyteDB → Response → Web/Mobile Frontend

- **Critical path**:
1. Text input validation (512-token limit for FLAN-T5-Large)
2. Tokenization via T5Tokenizer
3. Model inference (M-Passage/M-Conversation/M-Question)
4. Response storage with UUID
5. Frontend rendering

- **Design tradeoffs**:
  - **Lightweight vs. Context Length**: FLAN-T5-Large (780M params, 512 tokens) vs. GPT-4 (unknown params, 32K tokens)—chose lightweight for privacy-preserving local deployment, sacrificing long-context handling.
  - **Specialization vs. Generality**: Three task-specific models vs. one general model—increases maintenance but improves task performance.
  - **YugabyteDB in single-node mode**: Chose distributed-capable database for future scaling, though currently runs as standard PostgreSQL-compatible instance.

- **Failure signatures**:
  - Input >512 tokens: Truncation without warning; conversational inputs (2992-3050 words) show degraded performance
  - GPU memory overflow: Batch size must stay at 2 with gradient accumulation; FP16 required
  - Semantic drift: High SpaCy Similarity (0.88) with moderate BERTScore (0.47) on conversations suggests meaning partially lost
  - API timeout: Long conversations (3000+ words) may exceed response time limits

- **First 3 experiments**:
  1. **Token limit boundary test**: Feed M-Conversation progressively longer inputs (500, 1000, 2000, 3000 tokens) and plot BLEU/BERTScore degradation curve to quantify context-length sensitivity.
  2. **Metric sanity check**: Manually review 20 M-Question outputs where BLEU <0.01 but SpaCy Similarity >0.85 to verify semantic preservation vs. hallucination.
  3. **GPT-4 prompt comparison**: Test whether GPT-4's inferior performance stems from prompting—try 5 different prompt formulations on same M-Passage test set to isolate architectural vs. prompt-engineering effects.

## Open Questions the Paper Calls Out

- **How can the output quality stability of the M-Passage model be improved across varying text segment complexities?**
  - *Basis*: Section IX.C.2 states M-Passage shows "high variation" in scores and suggests "stabilizing M-Passage's performance across different text segments" as a potential improvement.
  - *Why unresolved*: The current fine-tuning approach leads to fluctuation in lexical and semantic scores depending on input length and complexity.
  - *What evidence would resolve it*: A modified training regime or architecture that yields lower variance in BERTScore and BLEU across long, medium, and short inputs.

- **How can the 512-token context window limitation be overcome for lengthy doctor-patient dialogues without losing critical medical history?**
  - *Basis*: Section IX.A notes Flan-T5-Large is limited to 512 tokens, whereas Section VII.B.2 describes M-Conversation inputs reaching up to 3,050 words, suggesting significant input truncation or data loss.
  - *Why unresolved*: The paper does not detail a specific chunking or hierarchical mechanism to process long conversational contexts within the model's fixed constraints.
  - *What evidence would resolve it*: Implementation of a sliding window or hierarchical summarization approach that maintains semantic accuracy comparable to full-context models.

- **What is the rate of clinical hallucination in Medalyze summaries compared to GPT-4 when assessed by medical experts?**
  - *Basis*: The evaluation relies on automated metrics (BLEU, BERTScore) and acknowledges "semantic drift" risks (Section VII.C), but lacks human clinical validation to verify factual accuracy.
  - *Why unresolved*: Automated similarity metrics do not capture medical factuality or safety; a model can score high on similarity yet generate clinically dangerous errors.
  - *What evidence would resolve it*: A clinical reader study where medical experts blind-review summaries for factual consistency and safety.

## Limitations

- **Clinical Accuracy**: The paper does not validate whether medical information remains clinically accurate after paraphrasing, risking semantic drift where terminology is simplified but clinical meaning is altered.
- **Long-Context Limitations**: The 512-token input limit severely constrains handling of comprehensive medical records, with conversational inputs (2992-3050 words) showing degraded performance.
- **Generalizability**: Models were evaluated only on the Stanford medical dataset and compared against GPT-4 using the same data, with unknown performance on other medical text corpora or real-world clinical data.

## Confidence

- **High Confidence**: The architectural advantage of Seq2Seq over autoregressive models for structured summarization is well-supported by the BLEU/BERTScore comparison; the combination of low lexical overlap with high semantic similarity indicates genuine abstractive behavior.
- **Medium Confidence**: Task-specific fine-tuning improves domain relevance compared to general-purpose models, though this is inferred from performance gaps rather than ablation studies.
- **Low Confidence**: Clinical accuracy and safety of the paraphrased summaries are not validated through medical expert review; the claim that lightweight models are sufficient for medical summarization lacks specification of context-length requirements for real clinical scenarios.

## Next Checks

1. **Clinical Expert Review**: Have 3 board-certified physicians independently review 50 M-Passage outputs to assess clinical accuracy and identify any semantic drift or missing critical information. Calculate inter-rater reliability and error rates.

2. **Long-Context Performance Test**: Systematically evaluate M-Conversation on inputs ranging from 512 to 4096 tokens in 256-token increments, measuring BLEU, BERTScore, and response time. Plot degradation curves to quantify context-length sensitivity and identify failure thresholds.

3. **Prompt Engineering Ablation**: Test 5 different GPT-4 prompt formulations (zero-shot, few-shot, chain-of-thought, role-based, and task-specific templates) on the same M-Passage test set. Compare results to isolate whether GPT-4's inferior performance stems from architecture or prompting.