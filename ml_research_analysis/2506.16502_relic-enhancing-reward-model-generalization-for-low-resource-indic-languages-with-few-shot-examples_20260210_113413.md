---
ver: rpa2
title: 'Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages
  with Few-Shot Examples'
arxiv_id: '2506.16502'
source_url: https://arxiv.org/abs/2506.16502
tags:
- reward
- languages
- arxiv
- relic
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RELIC addresses the challenge of reward model generalization for
  low-resource Indic languages, where models trained on high-resource languages fail
  to reliably distinguish response quality. The proposed framework trains a retriever
  with pairwise ranking loss to select discriminative in-context examples from auxiliary
  high-resource Indic languages, leveraging language similarity to enhance contextual
  relevance.
---

# Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples

## Quick Facts
- arXiv ID: 2506.16502
- Source URL: https://arxiv.org/abs/2506.16502
- Reference count: 34
- Key outcome: RELIC achieves 12.81% accuracy improvement over zero-shot prompting and 10.13% over state-of-the-art example selection for Bodo, demonstrating consistent gains across multiple low-resource languages

## Executive Summary
RELIC addresses the challenge of reward model generalization for low-resource Indic languages, where models trained on high-resource languages fail to reliably distinguish response quality. The proposed framework trains a retriever with pairwise ranking loss to select discriminative in-context examples from auxiliary high-resource Indic languages, leveraging language similarity to enhance contextual relevance. This approach significantly improves reward model accuracy without requiring preference data in target languages. On the PKU-SafeRLHF dataset using a LLaMA-3.1-8B reward model, RELIC achieves 12.81% accuracy improvement over zero-shot prompting and 10.13% over state-of-the-art example selection for Bodo, demonstrating consistent gains across multiple low-resource languages and datasets.

## Method Summary
RELIC is a framework that enhances reward model generalization for low-resource Indic languages through in-context learning. The method trains a dual-encoder retriever using pairwise ranking loss to select example pairs (one positive, one negative response) that maximize the frozen reward model's ability to discriminate between preferred and less-preferred responses. The retriever retrieves examples from auxiliary high-resource Indic languages identified through linguistic similarity, using multilingual BERT embeddings. During inference, the trained retriever provides in-context examples to the reward model, which then scores test queries with improved accuracy. The approach avoids fine-tuning the reward model directly while achieving significant performance gains across multiple low-resource languages.

## Key Results
- RELIC achieves 12.81% accuracy improvement over zero-shot prompting for Bodo on PKU-SafeRLHF dataset
- 10.13% improvement over state-of-the-art example selection methods for low-resource languages
- Consistent performance gains across multiple languages (Santali, Manipuri, Odia) and datasets (HH-RLHF, WebGPT)
- Pairwise ranking loss contributes 2-5% additional accuracy beyond relevance-based retrieval alone

## Why This Works (Mechanism)

### Mechanism 1
Training retrievers with pairwise ranking loss improves reward model discrimination more than relevance-based retrieval. The retriever scores candidate positive-negative example pairs by how well they help the reward model distinguish preferred from less-preferred responses, then learns via negative log-likelihood to rank these contrastive pairs higher. Core assumption: Examples that maximize reward model score separation transfer better to low-resource languages than merely semantically similar examples.

### Mechanism 2
In-context examples from linguistically similar high-resource languages provide more discriminative signal than examples from the target low-resource language itself. Cosine similarity between multilingual BERT embeddings identifies related high-resource Indic languages; their well-represented preference data compensates for scarcity in low-resource languages. Core assumption: Linguistic similarity correlates with shared preference structure that reward models can exploit.

### Mechanism 3
Contrastive paired examples (positive + negative) restructure reward model hidden representations to separate safe from unsafe responses. Paired examples create explicit contrast in context window, causing the reward model's final hidden layer to form more distinct representation clusters. Core assumption: In-context contrast generalizes to target language even when examples are in auxiliary languages.

## Foundational Learning

- **Bradley-Terry preference modeling**
  - Why needed here: RELIC's pairwise loss explicitly aligns with how reward models are trained (ranking preferred over dispreferred responses)
  - Quick check question: Can you explain why maximizing log-probability of correct ordering produces a ranking-capable model?

- **Dual-encoder retrieval architectures**
  - Why needed here: RELIC uses separate encoders (ϕ, ψ) for query encoding and example encoding with dot-product similarity
  - Quick check question: How does a dual encoder differ from cross-encoder reranking in computational cost and accuracy?

- **In-context learning without gradient updates**
  - Why needed here: The entire approach relies on conditioning the frozen reward model via retrieved examples at inference time
  - Quick check question: Why does ICL require careful example selection compared to fine-tuning?

## Architecture Onboarding

- Component map:
```
Training Pipeline:
Target language examples (DT) → Retriever encoder ϕp
Auxiliary high-resource bank (DHp) → Retriever encoder ψp
↓
Construct positive/negative candidate sets (F=25 each)
↓
Form all pairwise combinations (EHp)
↓
Score each pair via frozen reward model
↓
Select optimal pair (ê) and train retriever with NLL loss

Inference Pipeline:
Test query-response → Trained retriever → Top-K pairs from each auxiliary language
↓
Concatenate C=8 pairs as context
↓
Reward model scores augmented prompt
```

- Critical path:
1. Implement dual-encoder with mBERT initialization (must use multilingual encoder)
2. Build candidate pair construction (this determines training quality)
3. Integrate reward model scoring loop for pair evaluation (computationally expensive)
4. Implement inference-time retrieval and prompt construction

- Design tradeoffs:
- Candidate set size F: Larger F (50) improves accuracy but increases training time significantly; authors chose F=25 as balance
- Number of in-context examples C: Accuracy increases monotonically with C (tested 1-8), but limited by context window
- Number of auxiliary languages: More languages = more examples but potential noise; filtered by similarity threshold

- Failure signatures:
- Zero-shot accuracy near 50% indicates reward model has no signal for target language
- Random/Top-K baselines degrading performance suggests irrelevant examples harm discrimination
- Overlapping score distributions for safe/unsafe responses indicates retriever not providing discriminative context

- First 3 experiments:
1. Replicate zero-shot vs. RELIC comparison on one language (Bodo) with PKU-SafeRLHF to verify 12-13% gain
2. Ablate pairwise loss by training retriever with only relevance loss (EPR-style) on same data to isolate mechanism contribution
3. Vary auxiliary language selection threshold γ to understand sensitivity to linguistic similarity filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RELIC generalize effectively to low-resource languages outside the Indic language family?
- Basis in paper: [explicit] The authors state: "its effectiveness on other language families remains to be validated."
- Why unresolved: Experiments were limited to four Indic languages (Bodo, Santali, Manipuri, Odia); cross-lingual similarity thresholds and auxiliary language selection may not transfer.
- What evidence would resolve it: Evaluation on low-resource languages from African, Southeast Asian, or indigenous language families using the same framework.

### Open Question 2
- Question: Can RELIC be extended to implicit or generative reward models beyond classifier-based Bradley-Terry formulations?
- Basis in paper: [explicit] Authors note: "Exploring how RELIC could be extended to implicit or generative reward models is an interesting avenue for future work."
- Why unresolved: The pairwise ranking loss assumes explicit scalar rewards; implicit models (e.g., DPO-based) lack this structure.
- What evidence would resolve it: Adaptation of the retriever training objective for sequence-level preference optimization methods, with demonstrated accuracy gains.

### Open Question 3
- Question: How does RELIC impact downstream multilingual alignment outcomes in full RLHF pipelines?
- Basis in paper: [explicit] "The impact of incorporating in-context examples on multilingual alignment strategies is a potential direction for future research."
- Why unresolved: Evaluation focused solely on reward model pairwise accuracy, not on policy optimization or generated response quality.
- What evidence would resolve it: End-to-end RLHF experiments measuring alignment metrics (helpfulness, safety) in low-resource languages.

### Open Question 4
- Question: Are RELIC's reported improvements robust when evaluated on natively-written preference data rather than machine-translated datasets?
- Basis in paper: [inferred] All test datasets were created by translating English preference data using indictrans2, which may introduce translation artifacts affecting reward model behavior.
- Why unresolved: No validation against natively-curated preference judgments in low-resource Indic languages.
- What evidence would resolve it: Evaluation on authentic, natively-authored preference datasets in Bodo, Santali, or Manipuri.

## Limitations

- Performance depends heavily on quality of machine translations, which may introduce artifacts affecting reward model behavior
- Requires identification of linguistically similar high-resource languages, limiting applicability for isolated language families
- Pairwise ranking supervision relies on weak zero-shot reward model signals, potentially introducing noise during retriever training
- Limited validation on natively-authored preference data rather than machine-translated datasets

## Confidence

**High confidence**: The framework's core innovation (pairwise ranking loss for retriever training) is well-defined and reproducible. The computational architecture and training pipeline are clearly specified.

**Medium confidence**: The claimed 10-13% accuracy improvements across multiple languages and datasets appear consistent, but the sensitivity to data quality and similarity thresholds introduces variability. The ablation studies support the pairwise loss contribution but don't isolate all mechanisms.

**Low confidence**: The specific UMAP visualization showing representation separation relies on internal dataset processing that may not be fully reproducible. The claim that linguistic similarity directly correlates with preference structure transfer lacks extensive validation across diverse language pairs.

## Next Checks

1. **Cross-language robustness test**: Apply RELIC to a low-resource language with no linguistically similar high-resource relatives (e.g., a Dravidian language with minimal Indo-Aryan overlap) to test whether the similarity-based assumption holds.

2. **Signal reliability audit**: Measure the variance and confidence scores of reward model outputs during retriever training to quantify the quality of pairwise supervision. Compare performance when filtering low-confidence examples.

3. **Translation quality sensitivity**: Systematically vary the quality of translated training data (e.g., using different translation APIs or varying prompt engineering) to determine how sensitive the gains are to translation artifacts versus true cross-lingual preference structure.