---
ver: rpa2
title: 'RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large
  Language Models'
arxiv_id: '2508.01174'
source_url: https://arxiv.org/abs/2508.01174
tags:
- pass
- rspo
- responses
- policy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the mismatch between risk-neutral training
  objectives and risk-seeking evaluation metrics (Pass@k and Max@k) in large language
  model post-training. The authors propose Risk-Seeking Policy Optimization (RSPO),
  which directly optimizes these metrics by deriving closed-form expressions for the
  probability that a given response is the maximum among k samples.
---

# RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models

## Quick Facts
- arXiv ID: 2508.01174
- Source URL: https://arxiv.org/abs/2508.01174
- Authors: Kaichen Zhang; Shenghao Gao; Yuzhong Hong; Haipeng Sun; Junwei Bao; Hongfei Jiang; Yang Song; Hong Dingqian; Hui Xiong
- Reference count: 40
- Primary result: RSPO directly optimizes Pass@k and Max@k metrics through closed-form probability expressions, eliminating gradient contamination from low-reward responses.

## Executive Summary
This paper addresses the mismatch between risk-neutral training objectives and risk-seeking evaluation metrics (Pass@k and Max@k) in large language model post-training. The authors propose Risk-Seeking Policy Optimization (RSPO), which directly optimizes these metrics by deriving closed-form expressions for the probability that a given response is the maximum among k samples. This approach resolves the "hitchhiking" problem where low-reward responses are inadvertently reinforced when co-occurring with high-reward responses. RSPO produces efficient, unbiased gradient estimators for both metrics through combinatorial techniques.

## Method Summary
RSPO optimizes Pass@k and Max@k metrics by computing the probability that a specific response is the maximum among k independent samples. For Pass@k, the gradient weight is k(1-w_θ)^{k-1} where w_θ is the current success probability, implementing opportunity-cost awareness. For Max@k, the method uses a closed-form expression involving sorted rewards and hockey-stick identities. Both objectives yield unbiased gradient estimators via combinatorial subset enumeration without nested gradient computation. The approach requires sampling n≥k responses per prompt (recommended n≥2k) and computing combinatorial weights to determine each response's contribution to the policy gradient.

## Key Results
- RSPO consistently outperforms baseline algorithms across multiple math reasoning datasets, achieving optimal performance when training k matches evaluation metric
- The method effectively addresses the evaluation-training objective misalignment, showing improved scalability and robustness compared to existing approaches
- Extensive experiments on AIME2024, AMC, Math500, and other benchmarks demonstrate state-of-the-art performance for both Pass@k and Max@k optimization

## Why This Works (Mechanism)

### Mechanism 1
Decoupling individual responses from response groups eliminates gradient contamination from low-reward "hitchhikers." RSPO derives a closed-form probability that a specific response y achieves the maximum reward among k independent samples: P(y is max) = Σ_{i=1}^k P_{<,θ}(y)^{i-1}·π_θ(y|x)·P_{≤,θ}(y)^{k-i}. This transforms the joint distribution over k responses into a marginal distribution over a single response, allowing each response's gradient contribution to be computed independently based on its own merit rather than its group membership.

### Mechanism 2
Gradient weight k(1-w_θ)^{k-1} naturally implements opportunity-cost awareness, preventing over-reinforcement of already-solved problems. For Pass@k, this weight decreases monotonically as w_θ increases for k≥2. When Pass@k is already high, further reinforcing correct answers provides diminishing returns; remaining probability mass is implicitly allocated toward exploring alternative solutions that might improve the maximum.

### Mechanism 3
Unbiased gradient estimators can be constructed via combinatorial subset enumeration without nested gradient computation. For Pass@k, the unbiased estimator is C(n-c, k-1)/C(n-1, k-1) where c is the count of correct responses among n samples. This enumerates all C(n-1, k-1) subsets of size k-1 from the n-1 other responses and counts those consisting entirely of incorrect responses.

## Foundational Learning

- **Policy Gradient with REINFORCE Estimator**: RSPO builds on ∇_θ log π_θ(y|x) structure; understanding why this appears in the gradient is essential before analyzing the k(1-w_θ)^{k-1} weighting modification.
- **Order Statistics and Maximum of k Samples**: The core mathematical operation is computing the probability distribution of max{R(x,y_1), ..., R(x,y_k)}; requires understanding how order statistics transform a base distribution.
- **Unbiased vs. Consistent Estimators**: Theorem 4.3 claims unbiasedness; distinguishing this from consistency explains why the combinatorial estimator is preferred over the simpler biased estimator.

## Architecture Onboarding

- **Component map**: Prompt Batch D_b → Policy π_θ → n Responses per Prompt → Reward Model R(x,y) → Combinatorial Weight Computation ← Sort by Reward → Gradient Estimator J_RSPO(θ) → Policy Update (Adam/etc.)

- **Critical path**: Sampling efficiency requires n≥k, recommend n≥2k. Weight computation uses iterative product for combinatorial terms. Gradient aggregation filters zero-weight responses before backprop.

- **Design tradeoffs**: Higher n improves weight estimation variance but increases compute linearly. Training k vs. evaluation Pass@k alignment yields best results. Biased vs. unbiased estimator offers simplicity vs. systematic drift trade-off.

- **Failure signatures**: Gradient explosion from improper P_{≤} vs. P_{<} handling; entropy collapse indicating hitchhiking; data contamination from potentially seen benchmarks.

- **First 3 experiments**:
  1. Sanity check on k=1; should exactly match standard policy gradient
  2. Hitchhiking visualization comparing gradient weight distributions between baseline and RSPO
  3. k-alignment sweep training with k∈{2,4,8} and evaluating Pass@{1,2,4,8}

## Open Questions the Paper Calls Out

### Open Question 1
Can RSPO effectively optimize Pass@k and Max@k metrics in RLHF settings with proxy reward models, or is it limited to RLVR tasks with verifiable rewards? The authors tested only on mathematical reasoning with ground-truth verifiable rewards, leaving uncertainty about generalization to RLHF with learned proxy rewards.

### Open Question 2
What is the optimal relationship between training hyperparameter k and evaluation Pass@k/Max@k when they cannot be aligned? While alignment is beneficial, practical deployment scenarios may require optimizing for multiple k values simultaneously or may not know the target k during training.

### Open Question 3
How well does RSPO scale to much larger models (e.g., 70B+ parameters) and more diverse domains beyond mathematical reasoning? Experiments only tested Qwen2.5-Math-1.5B and 7B models on math benchmarks, leaving uncertainty about scalability to frontier models.

### Open Question 4
How sensitive is RSPO's Max@k objective to the choice of continuous reward function, particularly when rewards are sparse or have non-uniform distributions? The empirical validation uses specific reward formulations without systematic variation of reward properties.

## Limitations
- Relies heavily on independence assumption between responses, which may be violated by common decoding strategies
- Closed-form derivations assume binary or well-separated rewards; performance may degrade with continuous rewards having many ties
- Empirical validation limited to 1.5B and 7B parameter models, leaving uncertainty about scalability to frontier models

## Confidence

- **High Confidence**: The mathematical derivation of closed-form probability expressions for Pass@k and Max@k metrics is rigorous and verifiable through combinatorial identities.
- **Medium Confidence**: The empirical superiority of RSPO over baselines is well-demonstrated within tested model scales and datasets, but generalizability to larger models and non-math domains requires further validation.
- **Low Confidence**: The claim that RSPO eliminates "hitchhiking" effects entirely depends on implementation details of baseline algorithms not fully specified in the paper.

## Next Checks

1. **Independence Assumption Stress Test**: Implement RSPO with shared sampling strategies and measure gradient bias vs. fully independent sampling to quantify the independence assumption violation.

2. **Tie-Reward Sensitivity Analysis**: Create synthetic reward distributions with varying degrees of reward ties and measure RSPO Max@k performance degradation as tie frequency increases.

3. **Large-Scale Scalability Test**: Implement RSPO on a 70B+ parameter model with approximate gradient estimation to evaluate practical scalability limits and performance retention.