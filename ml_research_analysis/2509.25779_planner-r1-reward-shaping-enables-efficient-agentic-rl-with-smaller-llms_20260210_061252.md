---
ver: rpa2
title: 'Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs'
arxiv_id: '2509.25779'
source_url: https://arxiv.org/abs/2509.25779
tags:
- city
- transportation
- plan
- accommodation
- attraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Planner-R1 fine-tuned large language models for long-horizon travel\
  \ planning using agentic reinforcement learning. By leveraging dense, process-level\
  \ rewards, an 8B model achieved a 39.9% final pass rate with 3.5\xD7 higher compute\
  \ efficiency and 1.5\xD7 better memory efficiency than a 32B model, while still\
  \ matching or exceeding its planning performance."
---

# Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs

## Quick Facts
- arXiv ID: 2509.25779
- Source URL: https://arxiv.org/abs/2509.25779
- Reference count: 40
- 8B model achieves 39.9% final pass rate with 3.5× compute efficiency over 32B

## Executive Summary
Planner-R1 fine-tunes large language models for long-horizon travel planning using agentic reinforcement learning with dense reward shaping. An 8B model achieves competitive performance to 32B models while being 3.5× more compute-efficient and 1.5× better memory-efficient. The approach demonstrates that smaller models can match larger ones on planning tasks when provided with appropriate intermediate reward signals, challenging the assumption that larger models are always necessary for complex reasoning tasks.

## Method Summary
Planner-R1 trains Qwen3 models (8B and 32B) using GRPO (Group Relative Policy Optimization) via the VERL framework. The method introduces dense process-level reward shaping that decomposes sparse terminal rewards into intermediate signals based on schema compliance and constraint satisfaction. Training uses a multi-stage curriculum where rewards transition from dense to sparse. The system enforces structured JSON output through schema validation, coupling semantic correctness with format compliance. Models interact with a tool sandbox containing 7 APIs (flights, accommodations, restaurants, attractions, ground transportation, city information, calculator) to generate travel itineraries that satisfy both hard constraints and commonsense requirements.

## Key Results
- 8B Planner-R1 reaches 39.9% final pass rate, matching or exceeding 32B performance while using 3.5× less compute
- 32B Planner-R1 achieves 56.9% final pass rate, outperforming all baselines
- Smaller models show 3.5× higher compute efficiency and 1.5× better memory efficiency than larger models
- Training on 180 queries generalizes to unseen tasks (MULTI-IF, NATURALPLAN, τ-BENCH) without degradation

## Why This Works (Mechanism)

### Mechanism 1: Dense Process-Level Reward Shaping for Smaller Models
- Claim: 8B models require dense, process-level rewards to learn effectively in sparse-reward environments; without them, training collapses
- Core assumption: Shaped rewards preserve optimal policy per potential-based shaping theory
- Evidence: 8B models with Stage 2/3 rewards collapsed (3/5 and 5/5 failures respectively); dense Stage 1 rewards enabled learning
- Break condition: If shaping introduces policy bias or tasks lack intermediate structure to exploit

### Mechanism 2: JSON-Gated Output Coupling Semantics with Format
- Claim: Enforcing structured JSON output with schema validation reinforces tool-conditioned behaviors and improves generalization
- Core assumption: Schema captures task-relevant structure without being overly restrictive
- Evidence: Models show robustness to unseen tasks; schema validation couples format with semantic correctness
- Break condition: If schema is too restrictive or doesn't match task structure, models may game validation

### Mechanism 3: Compute-Efficiency Gap Between Model Scales Under Dense Rewards
- Claim: Dense rewards enable 8B models to achieve similar performance to 32B models with 3.5× less FLOPs
- Core assumption: Dense rewards reduce samples needed to reach competence, making smaller models Pareto-efficient
- Evidence: 32B reached 90% peak performance at 7.6×10^20 FLOPs vs 8B at 2.1×10^20 FLOPs
- Break condition: If tasks require reasoning depth beyond 8B capacity, larger models become preferable

## Foundational Learning

- **Markov Decision Process (MDP) Formulation for Tool-Use**
  - Why needed: Understanding how agents, tools, and environment states compose into a learnable system
  - Quick check: Can you articulate why γ=1 (no discounting) is appropriate for episodic planning tasks with terminal rewards?

- **Potential-Based Reward Shaping**
  - Why needed: Multi-stage rewards must preserve optimal policy per potential-based shaping theory
  - Quick check: If you add arbitrary intermediate reward for partial plan completion, how verify it doesn't change optimal policy?

- **GRPO (Group Relative Policy Optimization)**
  - Why needed: Understanding trajectory-level advantages and clipping mechanism for long-horizon planning
  - Quick check: Why might trajectory-level (vs token-level) advantages be preferred for long-horizon planning tasks?

## Architecture Onboarding

- **Component map**: User prompts → VERL Training Engine (FSDP) → SGLang Inference Engine → Tool Sandbox (7 APIs) → Reward Function (compute_score) → GRPO Update
- **Critical path**: 1) System prompts initialize episode → Agent generates tokens → Tool calls executed → Responses appended to state → Loop until `<answer>` → 2) Reward computed from final plan against schema + constraints → 3) GRPO update with group-normalized advantages
- **Design tradeoffs**: Dense vs sparse rewards (dense enables 8B learning but requires shaping effort); thinking mode disabled (found Qwen3's thinking tokens inflated context without metric gains); Multi-stage Awake memory (adds complexity but reduces peak GPU memory 20-23%)
- **Failure signatures**: Tool looping (base models repeatedly invoke calculator/restaurant tools until context limit); constraint hallucination (models output plans referencing non-existent accommodations); training collapse (8B models with sparse rewards converge to zero delivery rate); budget overflow (8B models satisfy constraints but exceed budget constraints)
- **First 3 experiments**: 1) Ablate reward density: Train 8B with Stage 1,2,3 rewards separately (500 steps each); expect collapse progression as sparsity increases; 2) Scale compute parity: Train 8B for 3000 steps vs 32B for 2000 steps; compare FLOPs-to-performance curves; 3) Generalization probe: Evaluate checkpoints on Multi-IF and NaturalPlan without fine-tuning; monitor for regression at high step counts (3000+)

## Open Questions the Paper Calls Out

- **Adaptive Curriculum Scheduling**: Can richer curriculum scheduling strategies (adaptive, performance-gated, or reward-annealing) improve upon the simple staged curriculum that showed no significant benefit?
- **Cross-Domain Efficiency**: Do the efficiency advantages of smaller, reward-shaped models persist on more complex or open-ended planning domains beyond TravelPlanner?
- **32B Variance Reduction**: What causes the higher run-to-run variance observed in 32B models, and can it be reduced without sacrificing their robustness to sparse rewards?

## Limitations

- Missing open-sourced reward function implementations and sandbox configurations blocks exact reproduction
- Generalization results to unseen tasks show parity with baselines but lack statistical significance reporting
- Mechanism linking JSON-gated output structure to improved generalization remains theoretically asserted rather than empirically validated

## Confidence

**High confidence**: 8B models require dense rewards to learn effectively (direct ablation experiments); 39.9% and 56.9% final pass rates are directly measurable
**Medium confidence**: 3.5× compute efficiency claim relies on FLOPs estimates with assumed scaling relationships
**Low confidence**: JSON-gated output coupling mechanism improving generalization lacks direct empirical support

## Next Checks

1. **Ablation of reward shaping components**: Systematically remove individual reward components (schema validation, constraint fractions, micro/macro rewards) to identify which elements are essential for 8B model convergence versus 32B robustness

2. **Compute efficiency under varying task complexity**: Evaluate Planner-R1 models on TRAVELPLANNER tasks with increasing constraint density and planning horizon length to determine whether the 3.5× efficiency advantage holds across the full task spectrum

3. **Open-ended planning capability test**: Remove JSON schema constraints and evaluate whether Planner-R1 models maintain planning competence when output structure is unconstrained, testing whether the structural coupling mechanism is beneficial or potentially limiting