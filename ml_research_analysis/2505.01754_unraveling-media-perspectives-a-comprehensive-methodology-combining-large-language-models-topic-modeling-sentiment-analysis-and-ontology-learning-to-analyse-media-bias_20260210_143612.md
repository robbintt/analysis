---
ver: rpa2
title: 'Unraveling Media Perspectives: A Comprehensive Methodology Combining Large
  Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse
  Media Bias'
arxiv_id: '2505.01754'
source_url: https://arxiv.org/abs/2505.01754
tags:
- bias
- sentiment
- media
- https
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel methodology for scalable, minimally
  biased analysis of media bias in political news, leveraging natural language processing
  techniques such as hierarchical topic modeling, sentiment analysis, and ontology
  learning with large language models. Through three case studies, the approach successfully
  identifies media bias across news sources at various levels of granularity, including
  event selection, labeling, word choice, and commission/omission biases.
---

# Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias

## Quick Facts
- **arXiv ID**: 2505.01754
- **Source URL**: https://arxiv.org/abs/2505.01754
- **Reference count**: 40
- **Primary result**: Introduces scalable, minimally biased methodology for multi-faceted media bias detection using hierarchical topic modeling, sentiment analysis, and LLM-generated ontologies

## Executive Summary
This study presents a novel methodology for scalable analysis of media bias in political news, leveraging natural language processing techniques to detect event selection, labeling/word choice, and commission/omission biases. Through three case studies analyzing 40,385 English news articles from 37 newspapers, the approach successfully identifies bias patterns across news sources at various granularities. The methodology combines hierarchical topic modeling with sentiment analysis and ontology learning using large language models, providing a comprehensive framework for assessing media bias while addressing challenges like semantic duplicates and computational costs.

## Method Summary
The methodology employs a multi-stage pipeline: minimal preprocessing removes non-English articles and noise using newspaper-specific regex patterns; BERTopic with SBERT embeddings, UMAP, and HDBSCAN generates hierarchical topics for event selection bias detection; sentiment analysis uses RoBERTa (NewsSentiment library) for titles and spaCy with spacytextblob for bodies, with target-dependent analysis for entity-level labeling bias; named entity recognition with bert-base-NER extracts entities for sentiment scoring; ontology learning via GPT-4 with JSON schema captures commission/omission bias through knowledge graphs; visualization uses Gephi and a custom Media Bias Spectrum plotting sentiment deviation against article/entity count deviation.

## Key Results
- Hierarchical topic modeling successfully identifies event selection bias by comparing article distribution across newspapers
- Target-dependent sentiment analysis reveals labeling/word choice bias invisible to document-level scoring
- LLM-generated ontologies expose commission/omission bias through included/excluded facts and sources
- The Media Bias Spectrum visualization effectively contrasts media bias exhibited by various newspapers
- Semantic duplicates in ontologies fragment networks and prevent meaningful comparison

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical topic modeling enables detection of event selection bias by comparing article distribution across newspapers. BERTopic with HDBSCAN clustering generates hierarchical topics from article corpora, calculating percentage of articles per newspaper and computing deviation from mean percentage. This relative comparison eliminates publication volume bias. Core assumption: Topic clusters accurately represent news events, and coverage differences reflect editorial choices rather than data collection artifacts.

### Mechanism 2
Target-dependent sentiment analysis at entity level reveals labeling/word choice bias invisible to document-level scoring. BERT-based NER extracts entities, then RoBERTa analyzes sentiment with contextual windows (150 characters left/right of target). Sentiment deviation from cross-newspaper mean identifies which outlets frame entities more positively/negatively. The Media Bias Spectrum visualization plots sentiment deviation against article/entity count deviation.

### Mechanism 3
LLM-generated ontologies expose commission/omission bias by representing what facts/sources are included versus excluded. GPT-4 prompted to extract classes, objects, and relationships in JSON format from each article. Three consistency checks (object-class, object-object, object-relation) filter hallucinations. Core reference ontology integrates all articles; filtering by newspaper reveals domain-specific knowledge gaps.

## Foundational Learning

- **Concept: Target-dependent sentiment analysis**
  - Why needed here: Standard document-level sentiment conflates multiple entities with opposing polarities. Target-dependent analysis isolates sentiment toward specific entities using contextual windows.
  - Quick check question: Given "Israel's blockade caused suffering, but their defense was justified," what sentiment does standard vs. target-dependent analysis assign to "Israel"?

- **Concept: Hierarchical clustering with HDBSCAN**
  - Why needed here: News topics exist at multiple granularities (specific event → regional conflict → foreign policy). Hierarchical clustering enables scope adjustment without reprocessing.
  - Quick check question: If a topic contains articles from only one newspaper, should it be merged with the noise cluster? Why?

- **Concept: Ontology consistency validation**
  - Why needed here: LLM outputs require automated quality gates. Three checks—object-class (is InstanceOf valid?), object-object (duplicates?), object-relation (do referenced objects exist?)—provide confidence scores.
  - Quick check question: If GPT-4 outputs "RelationshipTo": "Gaza Strip" but no object named "Gaza Strip" exists, which consistency check fails?

## Architecture Onboarding

- **Component map**: RSS Feeds → Preprocessing (language filter, noise removal) → BERTopic (SBERT embeddings → UMAP → HDBSCAN) → Sentiment Analysis (RoBERTa for titles, spaCy for bodies) → NER (bert-base-NER) → Target-dependent sentiment (RoBERTa) → Ontology Learning (GPT-4 with JSON schema) → Visualization (Media Bias Spectrum, Gephi for ontologies)

- **Critical path**: Topic modeling must complete before sentiment/entity analysis (to exclude noise cluster). Ontology learning is optional and cost-prohibitive for large corpora (~€3,000 for 24K articles per paper's estimate).

- **Design tradeoffs**: RoBERTa (512 token limit) for titles vs. spaCy for bodies: paper concludes spaCy is unsuited for news sentiment; consider longer-context models instead. BERTopic vs. LDA: BERTopic provides hierarchy but requires more compute. Single-newspaper topics merged to noise: prevents topic-specific bias but loses niche coverage data.

- **Failure signatures**: spaCy text sentiment shows near-zero variance across newspapers (Figure 10): model inappropriate for task. Ontology semantic duplicates fragment networks: manual or automated entity resolution required. High noise cluster (>40%): topic modeling parameters need tuning.

- **First 3 experiments**: 1) Replicate topic modeling on a subset (e.g., 5 newspapers, 1 week) to validate HDBSCAN hierarchy quality using "Red Flags for Degenerate Clustering Results" 2) Compare RoBERTa vs. spaCy sentiment on 50 manually labeled article titles to quantify performance gap 3) Run ontology extraction on 10 articles, compute all three consistency metrics, manually verify false positive/negative rates

## Open Questions the Paper Calls Out

- Can automated methods for consolidating semantically equivalent nodes in LLM-generated ontologies be developed that match or exceed manual merging accuracy?
- Do multimodal features (images, publication timing, author identity) significantly improve media bias detection compared to text-only analysis?
- What is the optimal context window for target-dependent sentiment analysis of entities in news articles?
- Can visualization techniques be developed that simultaneously display ontology node hierarchies, multiple edge types, and assortative communities without information loss?

## Limitations
- Semantic duplicate handling in ontology extraction remains an unresolved challenge, potentially fragmenting comparison datasets
- GPT-4-based ontology extraction costs are prohibitive for large-scale analysis (~€3,000 for 24K articles)
- Newspaper-specific regex patterns for noise removal were manually derived but not documented, creating reproducibility barriers
- Context window limitations (512 tokens for RoBERTa) may truncate important sentiment-bearing content

## Confidence
- **High Confidence**: Event selection bias detection through hierarchical topic modeling; sentiment analysis framework validation; Media Bias Spectrum visualization approach
- **Medium Confidence**: Ontology learning methodology for commission/omission bias; target-dependent sentiment analysis performance claims
- **Low Confidence**: Comparative performance claims between RoBERTa and spaCy without rigorous ablation studies

## Next Checks
1. Replicate topic modeling on a controlled subset (5 newspapers, 1 week) and verify HDBSCAN hierarchy quality using established clustering diagnostics
2. Conduct head-to-head comparison of RoBERTa vs. spaCy sentiment analysis on 50 manually labeled article titles to quantify performance differences
3. Run ontology extraction on 10 articles, compute all three consistency metrics, and manually verify false positive/negative rates for automated quality filtering