---
ver: rpa2
title: LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests
arxiv_id: '2510.08616'
source_url: https://arxiv.org/abs/2510.08616
tags:
- qwen2
- arxiv
- b-instruct
- accuracy
- paraphrase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the vulnerability of large language models
  to surface-form brittleness under paraphrase stress tests. The authors introduce
  a protocol to measure generalization by re-evaluating models on paraphrased versions
  of benchmark questions, using two 7B models (Mistral-7B-Instruct and Qwen2.5-7B-Instruct)
  on ARC-Easy and ARC-Challenge.
---

# LLMs Show Surface-Form Brittleness Under Paraphrase Stress Tests

## Quick Facts
- arXiv ID: 2510.08616
- Source URL: https://arxiv.org/abs/2510.08616
- Reference count: 3
- Models show 6-10 point accuracy drops when tested on paraphrased vs. original benchmark items

## Executive Summary
This study introduces a paraphrase stress test protocol to measure how large language models handle semantically equivalent but surface-form-altered questions. Testing two 7B models (Mistral-7B-Instruct and Qwen2.5-7B-Instruct) on ARC-Easy and ARC-Challenge, the authors find consistent accuracy drops of 6-10 points when models are evaluated on paraphrased items versus originals. The results suggest that high benchmark scores may reflect memorization of specific phrasings rather than robust reasoning, highlighting the importance of paraphrase-aware evaluation for assessing true model generalization.

## Method Summary
The authors develop a controlled evaluation protocol using cross-family paraphrasing (different model families for paraphrasing vs. answering) and deterministic decoding to isolate the effect of surface-form changes. They generate paraphrased versions of ARC benchmark questions using light sampling (temperature=0.7) and validate fidelity through automated checks. Models are evaluated on both original and paraphrased items with temperature=0 decoding to ensure deterministic outputs. The paired accuracy gap (Δ = Acc_orig − Acc_para) serves as the primary metric for measuring surface-form brittleness.

## Key Results
- Accuracy drops of 6-10 points observed consistently when models answer paraphrased vs. original items
- Cross-family paraphrasing reduces style-echo artifacts while introducing paraphraser quality as a potential confound
- Deterministic decoding ensures accuracy differences stem from input changes rather than sampling variance
- Surface-form brittleness suggests high benchmark scores may reflect memorization rather than robust reasoning

## Why This Works (Mechanism)

### Mechanism 1: Surface-Form Brittleness Exposure via Semantic Perturbation
- Claim: Paraphrasing benchmark items reveals when models rely on brittle surface patterns rather than robust semantic reasoning, with accuracy drops serving as a proxy for memorization or shortcut learning.
- Mechanism: By preserving semantic content while altering lexical/syntactic form, paraphrases disrupt pattern-matching shortcuts. If a model has internalized the underlying concept, semantically equivalent rephrasings should yield identical predictions; if it has memorized specific phrasings, paraphrases will degrade performance.
- Core assumption: True semantic understanding is surface-form invariant within the bounds of meaning-preserving paraphrases.
- Evidence anchors:
  - [abstract] "paraphrasing induces a non-trivial accuracy drop... consistent with prior concerns about contamination and brittle surface-form shortcuts"
  - [section 5] "paraphrasing disrupts such surface matches, exposing whether the model has internalized underlying concepts or simply memorized familiar strings"
  - [corpus] RoParQ paper confirms "LLMs often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding"
- Break condition: If paraphrases introduce semantic drift (adding/omitting information), measured drops may reflect paraphrase quality degradation rather than answerer brittleness.

### Mechanism 2: Cross-Model Paraphrasing to Disrupt Style Echo
- Claim: Using different model families for paraphrasing vs. answering reduces self-consistency artifacts that could artificially inflate robustness estimates.
- Mechanism: Models may perform better on self-generated paraphrases because instruction-tuned LLMs develop distinctive phrasing preferences. Cross-family separation prevents the paraphraser from unconsciously generating rewrites that match the answerer's preferred surface patterns.
- Core assumption: Same-family paraphrasing would produce artificially low accuracy drops due to stylistic alignment rather than semantic equivalence.
- Evidence anchors:
  - [section 3.2] "To discourage style echo and self-consistency artifacts, the paraphraser is always a different model family than the answerer"
  - [section 5] "results reflect the interaction of two models, not the answerer in isolation. If the paraphraser generates awkward or biased rewrites, measured brittleness may partially reflect its limitations"
  - [corpus] No direct corpus evidence on cross-model paraphrasing protocols; this is a methodological contribution of the paper
- Break condition: If the paraphraser model systematically produces lower-quality or biased rewrites, brittleness measurements conflate answerer sensitivity with paraphraser limitations.

### Mechanism 3: Deterministic Decoding for Isolated Comparison
- Claim: Fixed-seed, temperature=0 decoding for answering ensures that accuracy differences stem from input changes rather than sampling variance.
- Mechanism: By eliminating stochasticity in answer generation (temperature=0, do_sample=False), the protocol isolates the causal effect of paraphrasing. Paired comparison (same item, different surface form) controls for item-level difficulty.
- Core assumption: Temperature=0 produces stable, deterministic outputs that represent the model's "true" prediction for a given input.
- Evidence anchors:
  - [section 3.2] "Randomness is controlled via random.seed(1337) and torch.manual_seed(1337)... Determinism ensures fair comparisons across original vs. paraphrased conditions"
  - [section 3.2] "Answering (deterministic): max_new_tokens=32, temperature=0.0, top_p=1.0, do_sample=False"
  - [corpus] No direct corpus evidence on this specific design choice; standard practice in controlled evaluation
- Break condition: If instruction-tuned models exhibit inherent instability even at temperature=0 (e.g., due to numerical precision or tokenization edge cases), comparisons may include uncontrolled noise.

## Foundational Learning

- **Concept: Data Contamination vs. Generalization**
  - Why needed here: The paper's central claim—that accuracy drops indicate brittleness—requires distinguishing between contamination (memorization of leaked benchmark items) and genuine generalization failure. Without this distinction, you cannot interpret whether Δ reflects data leakage or reasoning fragility.
  - Quick check question: A model scores 92% on a benchmark's original items and 78% on paraphrased versions. Name two distinct explanations for this 14-point gap and what evidence would distinguish them.

- **Concept: Behavioral Robustness Testing (Perturbation-Based Evaluation)**
  - Why needed here: The paraphrase stress test is a specific instance of behavioral robustness testing. Understanding this paradigm—testing whether models maintain performance under meaning-preserving perturbations—is essential for interpreting results and extending the method.
  - Quick check question: If you wanted to test a model's robustness beyond paraphrasing, name two other perturbation types that could reveal surface-form dependence.

- **Concept: Instruction Tuning Effects on Output Stability**
  - Why needed here: The paper notes that "instruction tuning strongly conditions both the answerer and the paraphraser" and that "prompt design and instruction alignment are not neutral components." Understanding how instruction tuning shapes model behavior is critical for designing prompts and interpreting format-dependent errors.
  - Quick check question: Why might forcing a model to output only a single JSON letter ("answer": "B") affect its accuracy compared to allowing unconstrained text generation?

## Architecture Onboarding

- **Component map:**
  Benchmark Loader -> Paraphrase Generator -> Fidelity Validator -> Answer Generator -> Output Parser -> Scoring Module

- **Critical path:**
  1. Load fixed subset of benchmark items (deterministic seed)
  2. Generate paraphrased stem via cross-family paraphraser with light sampling (temp=0.7)
  3. Validate paraphrase fidelity; retry up to 3×, fallback to original if all fail
  4. Run answerer on original and paraphrased versions with deterministic decoding (temp=0)
  5. Parse outputs, compute paired accuracy gap Δ = Acc_orig − Acc_para

- **Design tradeoffs:**
  - **Paraphraser cross-family vs. same-family**: Cross-family avoids style echo but introduces paraphraser quality as confound; same-family would be methodologically cleaner but risks self-consistency artifacts
  - **Paraphrasing stems only vs. full items**: Preserving answer options verbatim maintains label mappings but may miss option-text brittleness
  - **4-bit quantization**: Enables single-A100 inference but could introduce subtle accuracy shifts vs. full-precision
  - **K=3 retry limit**: Prevents runaway paraphrase generation but may accept lower-quality outputs when paraphraser struggles

- **Failure signatures:**
  - **Semantic drift in paraphrases**: Manual inspection reveals information added/omitted; neither cleaning heuristics nor retries fully correct this
  - **Parser false positives**: Letter-extractor matches spurious capitals (e.g., "A" in "Answer: C"); mitigated by preferring JSON key
  - **Format rigidity errors**: Model knows answer but fails JSON formatting constraint; inspect rejected outputs to distinguish knowledge gaps from format failures
  - **Trivial paraphrases**: Temperature too low produces near-identical rewrites; check edit distance distribution

- **First 3 experiments:**
  1. **Baseline replication**: Run exact protocol (Mistral-7B-Instruct ↔ Qwen2.5-7B-Instruct cross-pairing, ARC-Easy 300 items, 4-bit) to confirm 6–10 point Δ range matches paper
  2. **Ablate cross-family constraint**: Test same-family paraphrasing (Mistral answering + Mistral paraphrasing) to measure style-echo effect size directly
  3. **Extend to new benchmark**: Apply identical protocol to a different MCQ benchmark (e.g., MMLU subset) to test whether surface-form brittleness generalizes beyond grade-school science domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed accuracy drop stem primarily from data contamination (memorization) or from genuine surface-form brittleness in reasoning?
- Basis in paper: [explicit] The authors state: "While our study does not perform explicit contamination auditing, methods such as n-gram overlap checks or retrieval-based similarity could strengthen causal attribution in future work."
- Why unresolved: The experimental design cannot disentangle whether models fail on paraphrases because they had memorized original phrasings, or because they rely on brittle surface patterns regardless of training data exposure.
- What evidence would resolve it: Contamination auditing via n-gram overlap or retrieval-based similarity searches against training corpora; testing on benchmarks confirmed to be uncontaminated.

### Open Question 2
- Question: How can evaluation protocols disentangle answerer brittleness from paraphrase quality limitations?
- Basis in paper: [explicit] The authors note: "paraphrase quality becomes a limiting factor for evaluation fidelity. Improving this component—for example by using human-in-the-loop filtering, specialized paraphrasing models, or multi-pass verification—would increase confidence that measured drops stem from answerer brittleness rather than paraphrase artifacts."
- Why unresolved: Using an LLM to generate paraphrases introduces a confound—measured drops could reflect paraphraser errors (semantic drift, awkward phrasing) rather than answerer fragility.
- What evidence would resolve it: Human evaluation of paraphrase fidelity; comparison against a fixed, human-verified paraphrase set; using specialized paraphrasing models with formal semantic equivalence guarantees.

### Open Question 3
- Question: Do paraphrase stress test findings generalize beyond grade-school science QA to other domains and task formats?
- Basis in paper: [explicit] The authors state: "Our study is limited to the ARC benchmark, which targets grade-school science... Extending paraphrase stress tests across domains and task formats is an important direction for future work."
- Why unresolved: ARC represents a narrow task type (multiple-choice science questions). It remains unknown whether similar brittleness appears in open-domain QA, multi-turn dialogue, mathematical reasoning, or code generation tasks.
- What evidence would resolve it: Applying the same protocol to diverse benchmarks (e.g., MMLU, GSM8K, HumanEval) and task formats (generation, ranking, dialogue).

### Open Question 4
- Question: Does model scale affect paraphrase brittleness, or is this vulnerability consistent across model sizes?
- Basis in paper: [inferred] The study evaluates only two 7B parameter models. The Discussion does not address whether larger models (70B, 175B+) or smaller models exhibit different magnitudes of paraphrase sensitivity.
- Why unresolved: Scaling may improve semantic robustness, or may simply amplify pattern-matching on more training data—the relationship between scale and brittleness is untested.
- What evidence would resolve it: Running the same protocol across model families at multiple scales (e.g., 1B, 7B, 70B, 175B) while controlling for training data and architecture.

## Limitations

- Paraphrase fidelity relies on automated checks rather than systematic semantic equivalence validation
- Cannot definitively distinguish between contamination-induced memorization and genuine reasoning fragility
- Limited to grade-school science domain and 7B-parameter models without testing generalizability
- Cross-family paraphrasing introduces paraphraser quality as a potential confound in measured accuracy drops

## Confidence

**High Confidence:** The observed accuracy drops under paraphrase stress tests are reproducible and methodologically sound. The controlled comparison protocol (deterministic decoding, paired items, cross-family paraphrasing) is well-designed and produces consistent results across multiple model pairs.

**Medium Confidence:** The interpretation that accuracy drops indicate surface-form brittleness rather than contamination is plausible but not definitively proven. The paper acknowledges this limitation and calls for future contamination audits, suggesting the authors recognize this uncertainty.

**Low Confidence:** Claims about the protocol's generalizability to other domains, model scales, or task types remain speculative. The 6-10 point accuracy drop may not represent a universal metric for surface-form brittleness across different benchmarks or model architectures.

## Next Checks

1. **Contamination Audit:** Compare accuracy drops between models trained on benchmark-contaminated data versus models with verified clean pretraining to isolate whether observed brittleness stems from memorization or genuine reasoning fragility.

2. **Cross-Domain Replication:** Apply the identical paraphrase stress test protocol to at least two different benchmark types (e.g., MMLU, GSM8K) to test whether surface-form brittleness generalizes beyond grade-school science questions.

3. **Option-Text Paraphrasing Extension:** Modify the protocol to paraphrase both question stems and answer options while maintaining consistent label mappings to test whether brittleness extends beyond stem recognition to full item comprehension.