---
ver: rpa2
title: 'Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees
  for Learning to Defer'
arxiv_id: '2506.20650'
source_url: https://arxiv.org/abs/2506.20650
tags:
- loss
- surrogate
- deferral
- learning
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning to defer with multiple
  experts, where input instances must be optimally assigned to experts to balance
  accuracy and computational cost. The authors introduce novel surrogate loss functions
  and efficient algorithms with strong theoretical guarantees for both single-stage
  (jointly learning predictor and deferral function) and two-stage (learning only
  the deferral function with a fixed expert) learning scenarios.
---

# Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer

## Quick Facts
- **arXiv ID**: 2506.20650
- **Source URL**: https://arxiv.org/abs/2506.20650
- **Reference count**: 40
- **Primary result**: Introduces novel surrogate losses with realizable H-consistency and Bayes-consistency for learning to defer with multiple experts, achieving superior performance in realizable settings while matching state-of-the-art in non-realizable settings.

## Executive Summary
This paper addresses the problem of learning to defer with multiple experts, where input instances must be optimally assigned to experts to balance accuracy and computational cost. The authors introduce novel surrogate loss functions and efficient algorithms with strong theoretical guarantees for both single-stage (jointly learning predictor and deferral function) and two-stage (learning only the deferral function with a fixed expert) learning scenarios. The framework achieves realizable H-consistency, H-consistency bounds, and Bayes-consistency under natural assumptions, with experimental validation on CIFAR-10, CIFAR-100, SVHN, and Tiny ImageNet datasets.

## Method Summary
The framework introduces surrogate losses for learning to defer with multiple experts. For single-stage deferral, it uses a family of realizable H-consistent surrogate losses based on an alternative decomposition of the deferral loss into classification and expert correction terms. For two-stage deferral, it derives new surrogate losses that achieve realizable H-consistency, H-consistency bounds, and Bayes-consistency. The method employs ResNet-16 architectures trained with Adam optimizer (weight decay 1e-3, batch size 1024) for 200 epochs, using synthetic experts defined by class ranges and instance-dependent cost functions based on classification error.

## Key Results
- Achieved system accuracy of 91.98-96.08% on CIFAR-10, CIFAR-100, SVHN, and Tiny ImageNet, comparable to baselines
- In realizable settings, achieved close to 100% system accuracy while baselines failed to find near-zero-error solutions
- Theoretical guarantees include realizable H-consistency, H-consistency bounds, and Bayes-consistency under natural assumptions
- Enhanced theoretical guarantees under low-noise assumptions for both single-stage and two-stage scenarios

## Why This Works (Mechanism)

### Mechanism 1: Alternative Deferral Loss Decomposition
The deferral loss $L_{def}(h, x, y)$ is algebraically rewritten into a weighted sum of classification loss and expert-specific correction terms. This decomposition permits substitution with smooth comp-sum surrogates (logistic, generalized cross-entropy, MAE) that preserve gradient information during optimization. The surrogate function Ψ must satisfy boundary conditions: $\lim_{u \to 1^-} \Psi(u) = 0$ and $\lim_{u \to 0^+} \Psi(u) = 1$ for multiple experts ($n_e \geq 2$).

### Mechanism 2: Realizable H-Consistency via Scaling Closure
When hypothesis set $H$ is closed under scaling, scaled versions of optimal hypotheses remain in the class, enabling convergence to zero deferral loss in realizable settings. For any optimal hypothesis $h^*$ with zero deferral loss, the scaled hypothesis $\alpha h^* \in H$ for any $\alpha > 0$. As $\alpha \to +\infty$, softmax outputs concentrate on correct labels/experts, driving surrogate loss to zero while maintaining zero target loss.

### Mechanism 3: H-Consistency Bounds via Conditional Regret Analysis
Bounding the conditional regret of the deferral loss by the conditional regret of the surrogate loss yields non-asymptotic estimation error bounds. The proof characterizes conditional error and conditional regret for both losses, showing that for $L_{mae}$ (Ψ = Ψ₁), the surrogate conditional regret is lower-bounded by a scaled version of the target conditional regret, yielding linear bounds.

## Foundational Learning

- **Surrogate Loss Functions**: Smooth, optimizable proxies for non-differentiable deferral loss with indicator functions. Why needed: Enables gradient-based optimization. Quick check: Can you explain why we use softmax-based surrogates instead of directly optimizing the 0-1 deferral loss?

- **Minimizability Gap ($M_L(H)$)**: Captures difference between best-in-class generalization error and expected best-in-class conditional error; non-zero for restricted hypothesis sets. Why needed: Important for understanding theoretical guarantees. Quick check: Under what conditions does the minimizability gap vanish?

- **Tsybakov Noise Assumption (Extended)**: Provides intermediate theoretical guarantees between deterministic and arbitrary distributions by bounding probability of small margins. Why needed: Enables convergence rate analysis. Quick check: How does the Tsybakov parameter α affect the convergence rate in Theorem 5.4?

## Architecture Onboarding

- **Component map**: Input layer -> Predictor head (outputs scores for each class) -> Deferral head (outputs scores for each expert) -> Expert modules -> Loss module

- **Critical path**: 
  1. Forward pass computes all class and expert scores
  2. Apply softmax over augmented label space $\bar{Y} = [n + n_e]$
  3. Compute surrogate loss using selected Ψ
  4. Backpropagate gradients through both heads jointly

- **Design tradeoffs**:
  - Ψ selection: Ψ₁ (MAE) gives linear H-consistency bounds but may be less robust to label noise; Ψ_q with $q \in (0, 1)$ gives square-root bounds with potential noise robustness
  - Single vs two-stage: Single-stage enables end-to-end training but requires expert predictions during training; two-stage decouples predictor from deferral function
  - Cost specification: Classification error costs require expert labels during training; alternative costs may relax this

- **Failure signatures**:
  - Deferral collapse: Model defers nearly everything → check cost scaling and expert quality
  - No deferral: Model never defers → expert costs may be too high or predictor overconfident
  - Inconsistent deferral in realizable settings: Verify Ψ satisfies boundary conditions ($q > 0$)

- **First 3 experiments**:
  1. Synthetic realizable validation using Mixture-of-Gaussians setup; verify system accuracy approaches 100% with sufficient samples
  2. Ablation on Ψ selection comparing Ψ₁ (MAE), Ψ₀.₇ (GCE), and Ψ₀ (logistic) on CIFAR-10 with controlled expert accuracy levels
  3. Low-noise regime test with varying margin distributions; validate convergence rates match Theorem 5.4 predictions as α approaches 1

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the proposed surrogate losses perform in real-world scenarios where expert domains overlap and data is heterogeneous? (Section 7 states future work on real-world empirical analysis with overlapping expert domains)

- **Open Question 2**: Can the proposed framework be adapted to handle previously unseen experts at test time? (Section 7 lists this as a promising direction; current formulation assumes fixed set of experts known during training)

- **Open Question 3**: Can the proposed methods be extended to scenarios where expert predictions are unavailable during training? (Section 7 acknowledges this relevance; current surrogate losses require expert predictions for cost calculation)

- **Open Question 4**: Can realizable H-consistency be proven for the two-stage multiple-expert surrogate loss without the assumption that at most one expert has zero error for a given instance? (Current proof strategy relies on uniqueness of perfect expert)

## Limitations

- Theoretical guarantees rely heavily on hypothesis sets being closed under scaling and symmetric, which may not hold for modern deep neural networks with bounded activations
- Empirical validation limited to image classification datasets with artificially constructed expert assignments rather than naturally occurring expert systems
- Choice of Ψ = Ψ₁ for single-stage and Ψ = Ψ₀ for two-stage is not systematically justified beyond their respective theoretical properties

## Confidence

- **High Confidence**: Alternative loss decomposition mechanism and its algebraic correctness; experimental results showing superior performance in realizable settings
- **Medium Confidence**: Realizable H-consistency proofs for single-stage deferral (depend on scaling closure assumptions); low-noise convergence rates
- **Low Confidence**: Practical significance of H-consistency bounds in non-realizable settings; transferability of results from synthetic to natural expert assignments

## Next Checks

1. **Hypothesis Class Validation**: Test realizable H-consistency on bounded neural networks (with weight decay or activation constraints) to verify scaling closure assumptions hold approximately in practice

2. **Real-World Expert Assignment**: Apply framework to dataset with naturally occurring expert assignments (e.g., medical diagnosis with human experts) to assess practical utility beyond synthetic scenarios

3. **Ψ Selection Sensitivity**: Conduct systematic ablation studies across different Ψ functions (Ψ₁, Ψ₀.₇, Ψ₀) across both single-stage and two-stage settings to validate theoretical recommendations empirically