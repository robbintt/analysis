---
ver: rpa2
title: 'One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking'
arxiv_id: '2601.11293'
source_url: https://arxiv.org/abs/2601.11293
tags:
- detection
- claim
- evidence
- stance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes multi-task learning (MTL) to fine-tune a single\
  \ small decoder-only LLM (Qwen3) for three core fact-checking tasks: claim detection,\
  \ evidence re-ranking, and stance detection. Using parameter-efficient QLoRA adapters,\
  \ three MTL configurations\u2014classification heads, causal language modeling heads,\
  \ and instruction tuning\u2014are evaluated against single-task baselines and zero-/few-shot\
  \ prompting."
---

# One LLM to Train Them All: Multi-Task Learning Framework for Fact-Checking

## Quick Facts
- arXiv ID: 2601.11293
- Source URL: https://arxiv.org/abs/2601.11293
- Reference count: 0
- A single small decoder-only LLM fine-tuned via multi-task learning achieves 44%, 54%, and 31% relative improvements over baselines in claim detection, evidence re-ranking, and stance detection respectively.

## Executive Summary
This paper proposes a multi-task learning (MTL) framework to fine-tune a single small decoder-only LLM (Qwen3-4B) for three core automated fact-checking tasks: claim detection, evidence re-ranking, and stance detection. Using parameter-efficient QLoRA adapters, three MTL configurations are evaluated against single-task baselines and zero-/few-shot prompting. The multi-task models consistently outperform isolated training, with classification heads (CLS) showing particular effectiveness for categorical outputs. The framework reduces redundancy, improves generalization, and offers a sustainable, resource-efficient alternative to large proprietary models for automated fact-checking.

## Method Summary
The method fine-tunes a frozen Qwen3-4B backbone with QLoRA adapters (r=64, α=16, 4-bit NF4) on three AFC tasks jointly. Three head variants are tested: classification heads (CLS), causal language modeling heads (CLM), and instruction tuning (IT). The model is trained with mixed batches from all tasks, optimizing a weighted sum of per-task losses. Datasets include CheckThat! 2024 English (claim detection), adapted AVeriTeC (evidence re-ranking), and a stance detection dataset from professional fact-checkers. Hyperparameters: 5 epochs, batch size 32, learning rate 2e-4, single 80GB A100 GPU.

## Key Results
- Multi-task learning with CLS heads achieves up to 44%, 54%, and 31% relative improvements in Macro-F1 over baselines for claim detection, evidence re-ranking, and stance detection respectively.
- MTL-CLS consistently outperforms single-task learning, zero-shot, and few-shot baselines across all three tasks.
- Classification heads outperform causal LM heads and instruction tuning for categorical fact-checking outputs, while also providing 3x faster inference throughput.

## Why This Works (Mechanism)

### Mechanism 1
Joint training on semantically dependent tasks creates shared representations that improve generalization over isolated training. The model resolves contradictions and consistencies between claims and evidence simultaneously, learning a common latent space encoding "factual consistency" rather than surface patterns.

### Mechanism 2
Classification heads outperform causal language modeling heads for categorical fact-checking tasks by providing more direct optimization signals. CLM heads treat labels as text generation problems, while CLS heads optimize direct probability distributions over specific class labels, resulting in faster convergence and higher precision for discrete outputs.

### Mechanism 3
Training schedule acts as a curriculum learning signal, influencing cross-task transfer. Optimizing tasks in a specific order (e.g., Ranking → Stance → Claim) allows the model to first establish retrieval/relevance grounding before learning higher-level classification logic, preventing simpler tasks from dominating gradients early in training.

## Foundational Learning

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed: The architecture freezes main model weights and only updates small adapter matrices. Understanding PEFT/LoRA is required to interpret parameter efficiency claims and backpropagation mechanics.
  - Quick check: Can you explain why the gradient ∂L_total/∂Θ = 0 in this architecture, and which parameters actually change during backprop?

- **Concept: Inductive Transfer / Negative Transfer**
  - Why needed: The core hypothesis is that knowledge from "evidence re-ranking" helps "stance detection." Engineers must understand that MTL can also hurt performance if task gradients conflict.
  - Quick check: If increasing the loss weight for Claim Detection causes Evidence Re-ranking performance to drop, what phenomenon is occurring?

- **Concept: Decoder-only Architecture (Causal LM)**
  - Why needed: The paper contrasts "Causal LM heads" with "Classification heads." Understanding the difference between generating the next token and pooling a hidden state for classification is required to interpret results.
  - Quick check: Why does a Classification Head require adding a specific linear layer (W_t) on top of the final hidden state h^(L), whereas a standard LLM prompt does not?

## Architecture Onboarding

- **Component map:** Raw text (Claim, Claim+Evidence) → Shared Embedding E(x) → Frozen Qwen3 Transformer blocks (Θ) with injected QLoRA adapters (A_r, B) → Three parallel lightweight heads (Claim Detection CLS, Evidence Re-ranking CLS, Stance Detection CLS) → Weighted sum of Cross-Entropy losses

- **Critical path:** Batch data loader mixes samples from all three tasks → Forward pass through shared backbone (adapters active) → Router sends representations to active head for specific sample → Loss calculation (masking inactive tasks) → Backpropagation updates only adapters and heads; backbone weights remain frozen

- **Design tradeoffs:** CLS offers 3x faster inference throughput and higher accuracy vs IT's flexibility for open-ended prompts. Model size: 4B parameters identified as efficiency "sweet spot"; 8B shows diminishing returns.

- **Failure signatures:** Task Dominance (one task dominates while others stagnate - remedy: adjust loss weights λ); Long-Context Drift (errors spike on inputs > 512 tokens or claims with high numerical density).

- **First 3 experiments:** 1) Sanity Check: Verify fine-tuning on Claim Detection improves over baseline to confirm training loop functionality. 2) Head Ablation: Train MTL model with CLM vs CLS heads, verify CLS achieves ~80 vs ~59 Macro-F1 for evidence re-ranking. 3) Loss Weight Tuning: Run sweep on loss weights (λ ∈ [1, 4]) to confirm weighting "Claim" and "Stance" higher than "Re-ranking" stabilizes training.

## Open Questions the Paper Calls Out

- **Open Question 1:** Why does evidence re-ranking performance not benefit from increased loss weights, unlike claim detection and stance detection which show clear improvements? The paper documents this asymmetry but does not investigate whether it stems from task complexity, dataset characteristics, or fundamental differences in gradient optimization across ranking versus classification objectives.

- **Open Question 2:** Does the MTL framework generalize to multilingual fact-checking settings and domains beyond political debates and journalist-verified claims? The paper evaluates exclusively on English datasets focusing on US political content, leaving unclear whether shared representations transfer across languages or to domains like scientific claims, health misinformation, or non-Western media.

- **Open Question 3:** Can implicit relevance detection in evidence re-ranking be improved through specialized training objectives or data augmentation? The current classification-based re-ranking head treats relevance as binary without explicitly modeling causal, temporal, or paraphrastic relationships between claims and evidence that constitute implicit support.

## Limitations

- Dataset preprocessing pipeline for evidence re-ranking (AVeriTeC snippet segmentation and positive/negative sampling) lacks full specification, creating potential variability in performance.
- Training methodology uncertainties include exact layer selection for adapter insertion, classification head dimensions, and mixed-batch training implementation details.
- Claims about handling "partial labels" in stance detection and model robustness to long-context inputs (>512 tokens) lack comprehensive empirical validation across diverse datasets.

## Confidence

**High Confidence:** The MTL framework's core architectural design and superiority of classification heads over causal language modeling variants are well-supported by experimental results.

**Medium Confidence:** Curriculum learning benefits (task ordering effects) show promise but rely on limited ablation studies. The 3x faster inference throughput claim needs independent verification.

**Low Confidence:** Claims about handling "partial labels" in stance detection and model robustness to long-context inputs lack comprehensive empirical validation.

## Next Checks

1. Implement the AVeriTeC snippet segmentation pipeline independently and verify that the 8% positive rate for evidence re-ranking is achievable through the described method.

2. Conduct controlled experiments training identical MTL models with CLS, CLM, and IT heads on a held-out subset of the CheckThat! dataset to independently verify the 3x throughput difference claim.

3. Evaluate the trained MTL model on claims with varying numerical density and token lengths beyond 512 tokens to quantify the performance degradation mentioned in Section 5.1.