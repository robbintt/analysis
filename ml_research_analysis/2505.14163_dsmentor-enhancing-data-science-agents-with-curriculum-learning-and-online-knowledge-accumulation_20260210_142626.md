---
ver: rpa2
title: 'DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online
  Knowledge Accumulation'
arxiv_id: '2505.14163'
source_url: https://arxiv.org/abs/2505.14163
tags:
- curriculum
- learning
- data
- difficulty
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DSMentor, a novel inference-time optimization
  framework for enhancing data science agents using curriculum learning and online
  knowledge accumulation. It organizes tasks from easier to harder and maintains a
  growing memory of past solutions to improve LLM agent performance on complex data
  science problems.
---

# DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation

## Quick Facts
- arXiv ID: 2505.14163
- Source URL: https://arxiv.org/abs/2505.14163
- Reference count: 7
- Primary result: 5.2% improvement in pass rate over baseline agents on DSEval and QRData benchmarks

## Executive Summary
DSMentor introduces a novel inference-time optimization framework that enhances LLM data science agents through curriculum learning and online knowledge accumulation. The framework organizes data science tasks from easier to harder, allowing agents to progressively build solutions from simpler components while maintaining a growing memory of past solutions. Tested on DSEval and QRData benchmarks, DSMentor demonstrates significant performance improvements, particularly showing an 8.8% gain in causal reasoning tasks compared to GPT-4 with Program-of-Thoughts prompts. The results validate the effectiveness of curriculum-based learning and knowledge accumulation during inference, effectively mirroring human learning processes to boost LLM performance.

## Method Summary
DSMentor employs a two-stage framework: a Mentor agent first assesses task difficulty using problem descriptions and orders problems easy-to-hard, then a Student agent iteratively solves problems while retrieving top-K similar examples from a growing long-term memory. The memory stores tuples of (problem description, generated code, evaluation tag) and is queried using cosine similarity of embeddings. As the Student agent works through the curriculum, it continuously updates the memory with new solutions, creating an adaptive knowledge base that improves performance on subsequent, more complex tasks. The approach uses models like Claude-3.5-Sonnet and Llama-3.1-70b with temperature=0 for deterministic generation.

## Key Results
- 5.2% improvement in pass rate compared to baseline agents on DSEval and QRData benchmarks
- 8.8% improvement in causal reasoning tasks compared to GPT-4 using Program-of-Thoughts prompts
- Easy-to-hard curriculum with examples ordered by increasing similarity outperforms alternative approaches

## Why This Works (Mechanism)

### Mechanism 1
Organizing tasks in an easy-to-hard sequence allows the agent to compose complex solutions from simpler, previously solved sub-problems. The Mentor agent assigns difficulty scores to structure the dataset, and by solving simpler tasks first, the agent populates long-term memory with foundational code patterns that can be reused when facing harder tasks.

### Mechanism 2
Retrieving relevant examples from a growing long-term memory at inference time enhances the agent's context, correcting for static knowledge limitations. As the Student agent solves tasks, it appends the problem, generated code, and evaluation tag to a memory buffer, then retrieves top-K similar examples using cosine similarity of embeddings to guide code generation.

### Mechanism 3
Ordering retrieved examples by increasing similarity improves the model's ability to utilize the context. When constructing the prompt for a hard problem, the system orders retrieved examples from easiest to hardest, providing a "curriculum within the context" that helps the model generalize from simpler analogies before tackling the specific target problem.

## Foundational Learning

- **Curriculum Learning**: Why needed - This is the core optimization strategy of DSMentor. Understanding that training/inference order matters is essential. Quick check - Can you explain why learning to add might help a model learn to multiply, and how that applies to code generation?

- **In-Context Learning (ICL) & Retrieval Augmented Generation (RAG)**: Why needed - DSMentor does not update weights; it relies on feeding retrieved examples (ICL) into the prompt to change behavior. Quick check - How does providing examples in a prompt differ from fine-tuning a model on those examples?

- **Embeddings & Cosine Similarity**: Why needed - The mechanism for retrieving "relevant" experience depends entirely on vectorizing text and finding nearest neighbors. Quick check - If two problems use different synonyms for the same statistical test, would a standard embedding model likely identify them as similar?

## Architecture Onboarding

- **Component map**: Mentor Agent (LLM) -> Sorter -> Student Agent (LLM) -> Memory Buffer (Vector Store) -> Retriever -> Executor

- **Critical path**: The synchronization between the Curriculum Sorter and the Memory Buffer. If the Mentor mis-ranks a very hard problem as "easy," the agent may fail early, polluting the memory with incorrect solutions or failing to build the necessary primitives for later tasks.

- **Design tradeoffs**: Problem-based vs. Code-based Difficulty (scalable but risks misjudging complexity), Memory Size (K) (more memory helps Claude but can hurt Llama-3.1-70b on multi-turn tasks)

- **Failure signatures**: Early Failure Cascade (incorrect early solutions pollute memory), Context Overflow (retrieving too many examples for complex tasks), Memory Retrieval Noise (incorrect examples degrading performance)

- **First 3 experiments**: 1) Difficulty Oracle Validation (correlate Mentor ratings with baseline pass rates), 2) Memory Ablation (run sorted curriculum without retrieval), 3) Error Analysis on Causal Reasoning (analyze what examples were retrieved for the 8.8% gain)

## Open Questions the Paper Calls Out
- Can an adaptive curriculum that dynamically adjusts task difficulty based on real-time agent performance outperform the current static, pre-determined ordering?
- How does the unbounded growth of the online long-term memory impact retrieval latency and accuracy in prolonged deployment?
- Is the LLM-based "problem-based" difficulty assessment robust enough, or does it fail to capture code complexity compared to execution-based metrics?

## Limitations
- Reliance on problem-based difficulty assessment rather than code-based difficulty, risking systematic misranking
- Potential for early incorrect solutions to pollute the memory buffer and cause cascading degradation
- Specific design decisions (embedding model, similarity metric) may not generalize across different problem domains

## Confidence
- High confidence: The core mechanism of curriculum learning improving LLM agent performance
- Medium confidence: The specific benefit of online knowledge accumulation via memory retrieval
- Medium confidence: The 8.8% causal reasoning improvement, though impressive, requires deeper error analysis

## Next Checks
1. Difficulty Assignment Validation: Have the Mentor agent rate difficulty on a held-out validation set and correlate these ratings with actual baseline model pass rates to ensure the "Easy-to-Hard" ordering is statistically valid.

2. Memory Ablation Study: Run the Student agent through the curriculum-ordered problems with retrieval disabled (curriculum-only baseline) and compare against full DSMentor to isolate whether gains come from ordering versus accumulation.

3. Memory Content Analysis: For the 8.8% causal reasoning improvement, perform detailed analysis of what examples were retrieved from memory to determine if they provided domain-specific knowledge or general reasoning scaffolding.