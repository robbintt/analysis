---
ver: rpa2
title: Detecting Silent Failures in Multi-Agentic AI Trajectories
arxiv_id: '2511.04032'
source_url: https://arxiv.org/abs/2511.04032
tags:
- anomaly
- detection
- traces
- systems
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of detecting silent failures\u2014\
  such as drift, cycles, and missing details\u2014in non-deterministic Multi-Agentic\
  \ AI systems driven by LLMs. The authors introduce anomaly detection in agentic\
  \ trajectories and develop a dataset curation pipeline that captures user behavior,\
  \ agent non-determinism, and LLM variation."
---

# Detecting Silent Failures in Multi-Agentic AI Trajectories

## Quick Facts
- arXiv ID: 2511.04032
- Source URL: https://arxiv.org/abs/2511.04032
- Authors: Divya Pathak; Harshit Kumar; Anuska Roy; Felix George; Mudit Verma; Pratibha Moogi
- Reference count: 5
- Primary result: Binary classification of agentic trajectories as normal or anomalous with accuracies up to 98% (XGBoost) and 96% (SVDD)

## Executive Summary
This paper addresses the challenge of detecting silent failures—such as drift, cycles, and missing details—in non-deterministic Multi-Agentic AI systems driven by LLMs. The authors introduce anomaly detection in agentic trajectories and develop a dataset curation pipeline that captures user behavior, agent non-determinism, and LLM variation. Using this pipeline, they curate and label two benchmark datasets with 4,275 and 894 trajectories from Multi-Agentic AI systems. They benchmark supervised (XGBoost), semi-supervised (SVDD), and unsupervised (K-Means) anomaly detection methods, showing that XGBoost and SVDD achieve comparable performance, with accuracies up to 98% and 96%, respectively. The study provides the first systematic evaluation of anomaly detection in such systems, along with datasets, benchmarks, and insights for future research.

## Method Summary
The authors curate two benchmark datasets (Stock Market: 4,275 traces; Research Writing: 894 traces) by instrumenting multi-agent systems with OpenTelemetry to capture agentic traces. They extract 16 features across five categories (token, latency, path, prompt/context, model features) from these traces. For labeling, they use a hybrid approach combining automated scripts with human-in-the-loop ground truth trajectory definitions. They benchmark supervised models (XGBoost, Random Forest, SVM, Logistic Regression, Naïve Bayes), semi-supervised methods (SVDD, Isolation Forest), and unsupervised clustering (K-Means) using a 70-15-15% train-val-test split, evaluating with accuracy, macro-F1, precision, and recall.

## Key Results
- XGBoost achieves accuracy up to 98% for detecting silent failures in multi-agent trajectories
- SVDD provides a practical semi-supervised alternative with accuracy up to 96% without requiring labeled anomalies
- Path-level features (tool_count, total_steps, unique_steps) are consistently ranked highest for anomaly detection
- Current approaches struggle with false negatives when anomalies closely resemble normal behavior

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised tree-based models (XGBoost) can detect silent failures in multi-agent trajectories with high accuracy by identifying deviations in structural and resource-usage features.
- **Mechanism:** The model learns to associate specific feature patterns—primarily path-level features like `tool_count`, `total_steps`, and `unique_steps`—with labeled anomalies. Silent failures like cycles or drift often manifest as statistically significant deviations in these trajectory features compared to normal executions.
- **Core assumption:** Silent failures consistently alter quantifiable trace attributes (e.g., path length, token usage, latency) in ways that distinguish them from successful executions.
- **Evidence anchors:**
  - [abstract]: "...benchmarking anomaly detection methods... supervised (XGBoost)... achieving accuracies up to 98%..."
  - [section 3.3]: "Path-level features such as tool_count, total_steps, unique_steps... are consistently ranked highest, highlighting their critical role in anomaly detection."
  - [corpus]: Related work (e.g., LumiMAS) confirms the general need for observability in Multi-Agent Systems but does not validate this specific feature-driven supervised mechanism.
- **Break condition:** The mechanism fails when anomalies are "silent" in terms of features—i.e., drift occurs without increasing path length or token usage, causing the anomaly to overlap with normal feature distributions (False Negatives).

### Mechanism 2
- **Claim:** Semi-supervised One-Class Classification (SVDD) provides a practical alternative to supervised learning by modeling the boundary of normal behavior without requiring labeled failure data.
- **Mechanism:** SVDD creates a hypersphere around the feature distribution of "normal" traces in a high-dimensional space. Traces falling outside this boundary are flagged as anomalies. This avoids the expensive process of labeling diverse failure modes.
- **Core assumption:** The training data contains predominantly "normal" trajectories, and these normal trajectories form a compact, modelable cluster distinct from potential anomalies.
- **Evidence anchors:**
  - [abstract]: "...semi-supervised (SVDD) approaches perform comparably, achieving accuracies up to... 96%."
  - [section 3.2]: "Given the relative ease of collecting normal traces, we include one-class classifiers (OCC) such as SVDD... which model the normal behavior and flag deviations as anomalies."
  - [corpus]: Weak/missing. Corpus neighbors focus on federated or graph-based methods, providing no direct validation for SVDD in this specific agentic context.
- **Break condition:** Performance degrades if the "normal" training data is contaminated with silent failures (label noise), causing the model to erroneously learn failure patterns as part of the normal boundary.

### Mechanism 3
- **Claim:** Deterministic labeling of non-deterministic agent behavior is feasible using a hybrid human-in-the-loop pipeline that compares execution traces against a "ground truth" trajectory.
- **Mechanism:** Domain experts define the expected "golden path" (sequence of tools/agents) for a user query. An automated script then compares the observed trace against this ground truth to detect drift, cycles, or tool errors, thereby generating binary labels (Normal/Anomaly) for ML training.
- **Core assumption:** For any given user prompt, a "correct" trajectory can be objectively defined, and deviations from this path constitute a failure.
- **Evidence anchors:**
  - [section 2.3]: "Drift is detected by domain experts who define the ground truth trajectory for each input prompt... The inter-annotator uses automatic labeling script... to automatically assign a label."
  - [abstract]: "...present a dataset curation pipeline that captures user behavior, agent non-determinism..."
  - [corpus]: No specific evidence from corpus neighbors regarding this specific labeling methodology for agents.
- **Break condition:** The mechanism cannot scale to open-ended or creative tasks where multiple valid trajectories exist, or where the definition of "success" is subjective.

## Foundational Learning

- **Concept: Distributed Tracing (OpenTelemetry)**
  - **Why needed here:** The entire detection approach relies on "Agentic AI traces" (spans) to capture the workflow. You must understand how parent-child spans represent the hierarchy of Agent $\rightarrow$ Tool $\rightarrow$ LLM calls.
  - **Quick check question:** If an agent calls a tool, which span is the parent and which is the child?

- **Concept: Silent Failures vs. Explicit Errors**
  - **Why needed here:** Standard monitoring detects errors (status codes). This paper targets "silent" issues like logical drift or missing details where the system returns a valid response but incorrect content.
  - **Quick check question:** If an agent returns a valid JSON response but ignores a specific constraint in the user prompt (e.g., "use only data from 2024"), is this an error or a silent failure?

- **Concept: Feature Engineering for Trajectories**
  - **Why needed here:** Raw traces must be converted into numerical features (tokens, latency, path length) for ML models. Understanding which features capture "behavior" vs. "noise" is critical.
  - **Quick check question:** Why might `total_tokens` be a good feature for detecting "cycles" (redundant loops)?

## Architecture Onboarding

- **Component map:** OpenTelemetry instrumentation -> Feature extraction pipeline -> Labeling engine (human-in-the-loop + automated script) -> ML detection models
- **Critical path:** Defining the Ground Truth Trajectories -> Generating & Labeling the Dataset -> Feature Extraction -> Model Training
- **Design tradeoffs:**
  - **Supervised (XGBoost):** Max Accuracy (~98%) but requires expensive, comprehensive labeling of both normal and anomaly data
  - **Semi-supervised (SVDD):** Lower data collection cost (needs only normal data) but slightly lower accuracy (~96%) and sensitivity to noise in the normal set
  - **Rule-based vs. ML:** The paper uses rule-based logic for *labeling* (curation) but ML for *runtime detection*
- **Failure signatures:**
  - **False Negatives (FNs):** Occur when an agent "drifts" (wrong path) but the feature values (path length, token count) remain statistically similar to normal traces. The paper notes these are the hardest to detect.
  - **Cycles:** Detected by checking if `unique_steps` < `total_steps`
- **First 3 experiments:**
  1. **Trace Collection:** Instrument a simple multi-agent system (e.g., a Research Assistant) with OpenTelemetry to generate raw spans
  2. **Feature Validation:** Run the trace collection for a fixed query 50 times. Visualize the distribution of `total_steps` and `total_tokens` to confirm non-determinism
  3. **Baseline Training:** Manually label the 50 traces from Experiment 2 as Normal/Anomaly. Train a simple XGBoost model using the paper's suggested features (specifically `tool_count` and `total_steps`) to test separation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can feature engineering be advanced to detect "silent drift" where anomaly feature distributions closely resemble normal behavior?
- **Basis in paper:** [explicit] The authors state in Future Plans: "plan to enhance anomaly detection... focusing on anomalies that mimics normal behavior" through "investigating additional feature engineering strategies."
- **Why unresolved:** Current path-level features (e.g., step count) fail to flag shorter, drifted paths lacking explicit errors, leading to false negatives where anomalies closely resemble normal traces.
- **What evidence would resolve it:** A model achieving significantly higher recall on the subtle drift subset of the benchmark datasets without reducing precision.

### Open Question 2
- **Question:** Can robust unsupervised or semi-supervised methods be developed for online, real-time anomaly detection in agentic systems?
- **Basis in paper:** [explicit] The Future Plans section highlights the need for "developing robust unsupervised or semi-supervised approaches suitable for both offline and online settings."
- **Why unresolved:** Current benchmarks evaluate static, offline datasets; real-time detection requires adapting to dynamic non-determinism without the high cost of labeling.
- **What evidence would resolve it:** An algorithm demonstrating high detection accuracy on streaming agentic data with low latency and minimal labeling requirements.

### Open Question 3
- **Question:** How can detection frameworks be extended to specifically identify tool failures and context propagation failures?
- **Basis in paper:** [explicit] The authors state: "Further analysis is needed to explore and address the other failure modes identified in our study (see Table 1)", listing tool and context failures.
- **Why unresolved:** The current datasets and binary classification benchmarks focus primarily on drift, cycles, and general errors, leaving tool and context failures as distinct, unaddressed categories.
- **What evidence would resolve it:** A dataset extension with explicit labels for tool and context failures, paired with a model capable of distinguishing these from other anomaly types.

## Limitations
- The labeling methodology assumes a single "ground truth" trajectory exists for each query, limiting scalability to creative tasks where multiple valid paths exist
- Performance may degrade if normal training data contains unlabeled silent failures, contaminating the anomaly boundary
- Current results may not generalize to larger, more diverse multi-agent systems or different agent architectures

## Confidence
- **High confidence:** Supervised XGBoost performance metrics (accuracy up to 98%) are well-supported by the experimental results and feature importance analysis
- **Medium confidence:** Semi-supervised SVDD's practical viability (accuracy up to 96%) is demonstrated but less robust across datasets
- **Medium confidence:** The labeling pipeline's scalability assumptions need further validation beyond the curated datasets

## Next Checks
1. Test model generalization by applying trained models to a held-out multi-agent system with different agent configurations and prompt distributions
2. Evaluate SVDD robustness by deliberately injecting mislabeled "normal" traces (actual anomalies) into the training set and measuring performance degradation
3. Validate the labeling methodology on a creative task (e.g., story generation) where multiple valid trajectories exist to assess subjective bias in anomaly definitions