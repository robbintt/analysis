---
ver: rpa2
title: Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval
  Augmented Generation at BioASQ 2025
arxiv_id: '2508.05366'
source_url: https://arxiv.org/abs/2508.05366
tags:
- batch
- test
- feedback
- ur-iw-5
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigated whether self-feedback mechanisms could\
  \ improve the performance of Large Language Models (LLMs) in retrieval-augmented\
  \ generation (RAG) for biomedical question answering. The authors evaluated current\
  \ reasoning and non-reasoning LLMs\u2014including Gemini-Flash 2.0, o3-mini, o4-mini,\
  \ and DeepSeek-R1\u2014on the BioASQ 2025 challenge tasks."
---

# Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025

## Quick Facts
- arXiv ID: 2508.05366
- Source URL: https://arxiv.org/abs/2508.05366
- Authors: Samy Ateia; Udo Kruschwitz
- Reference count: 33
- Primary result: Self-feedback mechanisms showed mixed performance gains in RAG for biomedical QA, with few-shot learning (10-shot) often outperforming feedback approaches

## Executive Summary
This study investigates whether Large Language Models can improve their own performance in retrieval-augmented generation for biomedical question answering through self-feedback mechanisms. The authors evaluated reasoning and non-reasoning LLMs (Gemini-Flash 2.0, o3-mini, o4-mini, DeepSeek-R1) on BioASQ 2025 tasks, implementing a self-critique loop where models generated, evaluated, and refined their own query expansions and answers. While the feedback approach showed promise in specific contexts, it did not consistently outperform zero-shot baselines, with few-shot learning (10-shot) achieving the strongest results particularly in retrieval and yes/no question tasks.

## Method Summary
The study employed a self-feedback mechanism where LLMs iteratively refined their outputs for query expansion and answer generation across four task types (yes/no, factoid, list, ideal summary). Five run configurations compared baseline, feedback, and 10-shot approaches using different models. The system used PubMed 2024 as the biomedical corpus, indexed in Elasticsearch for retrieval. For answer generation, models received retrieved documents as context and applied task-specific feedback prompts before final refinement. The methodology was evaluated across BioASQ's three phases, measuring performance using standard IR and QA metrics including MAP, F1, MRR, and precision/recall.

## Key Results
- Few-shot learning with Gemini Flash 2.0 consistently outperformed feedback approaches, particularly achieving best document retrieval MAP scores across all batches
- Self-feedback showed mixed performance: improved results in later batches for reasoning models but underperformed baselines overall
- Distilled models (Gemini Flash) consistently outperformed larger reasoning models (o3-mini, o4-mini) on retrieval and yes/no tasks
- Feedback mechanism did not provide consistent improvements across all task types and model combinations

## Why This Works (Mechanism)

### Mechanism 1: Iterative Self-Refinement via Structured Critique
- Claim: Models can identify and correct their own errors when provided retrieved context for evaluation
- Mechanism: Two-step pipeline where models generate draft outputs, receive their own work plus retrieved snippets, produce task-specific critique, then revise using a fixed refinement prompt
- Core assumption: Models can identify errors when prompted to evaluate against external evidence and apply corrections rather than re-affirming incorrect outputs
- Evidence anchors: UR-IW-4 improved in Task B Yes/No batches 3-4 (Macro F1 0.8706, 0.9097) but underperformed in Task A retrieval (MAP 0.1739 vs. 0.2865 for few-shot). Related work suggests refinement can help but effectiveness is task-dependent.
- Break condition: When feedback prompts lack specificity or retrieved context is irrelevant, models may hallucinate justifications or make unnecessary changes

### Mechanism 2: Few-Shot In-Context Learning for Domain Adaptation
- Claim: Providing task-aligned examples directly in prompts improves query formulation and answer formatting without parameter updates
- Mechanism: Ten expert-crafted BioASQ examples prepended to prompts enable models to infer output structure and domain terminology patterns
- Core assumption: Attention mechanism can generalize patterns from examples to novel queries without overfitting to example content
- Evidence anchors: UR-IW-5 (10-shot) achieved best document retrieval MAP among submitted systems across all batches. Standard finding across LLM literature.
- Break condition: When examples are misaligned with test distribution or contain outdated terminology, few-shot can misguide rather than help

### Mechanism 3: Distilled Model Efficiency-Quality Tradeoff
- Claim: Smaller, distilled models can match or exceed larger/reasoning models on domain-specific retrieval and QA under constrained prompting
- Mechanism: Distillation transfers capabilities from larger teachers while reducing inference overhead; Flash models retain strong baseline performance with lower latency and cost
- Core assumption: Distillation process preserved task-relevant reasoning patterns without introducing systematic blind spots for biomedical queries
- Evidence anchors: Gemini-2.0-flash achieved 0.954 Yes/No F1 and 0.684 Factoid MRR, outperforming gemini-2.0-pro-exp-02-05 and gpt-4.5-preview on multiple metrics
- Break condition: Distilled models may underperform on tasks requiring deep multi-step reasoning beyond training distribution

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Entire system architecture depends on grounding LLM outputs in retrieved PubMed documents rather than parametric knowledge alone
  - Quick check question: Can you explain why retrieving documents before generation reduces hallucination risk compared to pure generation?

- Concept: In-Context Learning (Few-Shot)
  - Why needed here: 10-shot baseline was most competitive approach; understanding how examples influence output structure is critical for debugging prompt design
  - Quick check question: What happens to output quality if few-shot examples are drawn from a different domain than test queries?

- Concept: Self-Critique / Self-Refinement
  - Why needed here: Paper's central investigation; engineers must understand why mechanism showed mixed results to iterate effectively
  - Quick check question: If a model's critique does not reference retrieved evidence, what failure mode should you suspect?

## Architecture Onboarding

- Component map: Question → Query Generation → Elasticsearch Retrieval → Snippet Extraction → Answer Generation (± Feedback Loop) → Final Answer
- Critical path: Question → Query Generation → Elasticsearch Retrieval → Snippet Extraction → Answer Generation (± Feedback Loop) → Final Answer. Latency dominated by multiple LLM calls when feedback enabled.
- Design tradeoffs:
  - Few-shot vs. Feedback: Few-shot adds prompt token overhead once; feedback doubles LLM calls. Few-shot showed more consistent gains.
  - Reasoning vs. Non-reasoning models: Reasoning models may self-critique better but add latency and cost; non-reasoning Flash models were more reliable overall.
  - Feedback granularity: Task-specific prompts (Yes/No vs. Factoid) were designed differently; generic prompts may reduce effectiveness.
- Failure signatures:
  - Feedback degrades performance: Check if critique ignores retrieved context or makes unnecessary changes. Compare Tables 3 and 6-11 for self-feedback vs. baseline gaps.
  - Few-shot copying: Model outputs closely mimic example answers rather than addressing the query.
  - Retrieval failure cascade: Poor initial queries → irrelevant snippets → wrong answers even with correct reasoning.
- First 3 experiments:
  1. Reproduce UR-IW-5 (10-shot Gemini Flash) baseline on held-out BioASQ batch to establish reference score for MAP and Yes/No F1.
  2. Ablate feedback loop: Run UR-IW-3 (Gemini + FB) and UR-IW-1 (Gemini baseline) side-by-side, then manually inspect 20 cases where feedback helped vs. hurt to identify critique patterns.
  3. Test cross-model feedback: Use stronger model (e.g., o3-mini) to generate critique for Flash model's draft, comparing against self-feedback to isolate whether critique quality or application is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of LLM-generated self-feedback compare to direct human expert feedback in professional biomedical search tasks?
- Basis in paper: Authors state in Discussion and Abstract that future work will focus on "comparing the effectiveness of LLM-generated feedback with direct human expert input."
- Why unresolved: Current study only evaluated self-critique loop without control group where human experts provided refinement cues
- What evidence would resolve it: Controlled experiment measuring task performance deltas when models are guided by human experts versus their own self-generated critiques

### Open Question 2
- Question: What specific error types (e.g., hallucination, format errors) are introduced or corrected by self-feedback mechanism across different answer formats?
- Basis in paper: Authors note future work requires "more granular analysis of generated answers and types of errors made by different models"
- Why unresolved: Current results report aggregate metrics (MAP, F1), which obscure specific reasons why feedback failed to yield consistent improvements
- What evidence would resolve it: Qualitative classification of failure modes (e.g., hallucination reinforcement vs. valid corrections) across different question batches

### Open Question 3
- Question: Why did smaller, distilled models (e.g., Gemini Flash) consistently outperform larger reasoning models (e.g., o3-mini) in query expansion and retrieval phases?
- Basis in paper: While paper explicitly reports distilled models achieved better results than larger counterparts, provides no theoretical or empirical explanation for this counter-intuitive finding
- Why unresolved: Authors observe performance gap but do not analyze if it stems from "overthinking" in reasoning models or specific tokenization issues in query generation step
- What evidence would resolve it: Ablation study analyzing query syntax and retrieval precision of distilled vs. reasoning models on same biomedical corpus

## Limitations
- Self-feedback mechanism effectiveness remains inconsistent across tasks and models, with mixed performance gains
- Comparative performance against other RAG systems limited to submitted runs only, without ablation studies on feedback components
- Query expansion via Elasticsearch query_string syntax may constrain semantic retrieval quality compared to neural retrieval methods

## Confidence

- Few-shot learning superiority over self-feedback (High): Clear evidence from UR-IW-5 achieving best MAP scores across batches
- Distilled models matching/retaining performance vs. larger reasoning models (High): Consistent results showing Gemini Flash 2.0 outperforming more expensive models
- Self-feedback potential for future refinement (Medium): Shows promise in specific contexts but requires additional engineering for consistent gains

## Next Checks

1. Conduct ablation studies isolating feedback components (critique quality vs. refinement application) to identify bottleneck sources
2. Test cross-model feedback (stronger model critiques weaker model drafts) to separate critique quality from self-application effects
3. Implement neural query expansion alternatives to compare against Elasticsearch query_string baseline performance