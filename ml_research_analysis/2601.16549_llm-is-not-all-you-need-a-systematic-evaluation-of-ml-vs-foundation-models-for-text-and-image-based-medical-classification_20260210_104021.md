---
ver: rpa2
title: 'LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models
  for text and image based Medical Classification'
arxiv_id: '2601.16549'
source_url: https://arxiv.org/abs/2601.16549
tags:
- classification
- medical
- text
- image
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically benchmarks traditional ML, zero-shot
  LLMs, and fine-tuned PEFT models across four medical classification tasks (binary/multiclass,
  text/image). It finds that classical ML models consistently achieve the best performance,
  especially on structured text datasets, while fine-tuned LoRA Gemma models underperform
  across all tasks, and zero-shot LLMs perform competitively only on multiclass image
  tasks.
---

# LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification

## Quick Facts
- arXiv ID: 2601.16549
- Source URL: https://arxiv.org/abs/2601.16549
- Reference count: 38
- Classical ML models consistently outperform fine-tuned LLMs on medical classification tasks

## Executive Summary
This study benchmarks traditional ML, zero-shot LLMs, and fine-tuned PEFT models across four medical classification tasks. Classical ML models achieve near-perfect accuracy on structured text datasets (0.9982 on Diabetes) while LoRA-tuned Gemma models underperform across all tasks. Zero-shot VLMs like Gemini 2.5 match classical ResNet-50 baselines on multiclass image tasks but fail on binary structured text. The results demonstrate that established ML methods remain the most reliable option for many medical classification scenarios, and that minimal fine-tuning can be detrimental, requiring significantly more epochs to close the performance gap.

## Method Summary
The study evaluates four medical classification tasks: Diabetes (binary text), Mental Health (multiclass text), Skin Cancer (binary image), and Respiratory Disease (multiclass image). Three model classes are compared: classical ML (Logistic Regression, LightGBM, custom CNN, ResNet-50), zero-shot LLMs (Gemini 2.5 Flash), and PEFT models (Gemma 1B/4B with LoRA rank=4, 1 epoch). Datasets are preprocessed with median imputation for zeros, class weighting for imbalance, and standardized image dimensions. Performance is measured using Accuracy, F1-Score, and AUC-ROC with consistent train/test splits.

## Key Results
- Classical ML models achieve near-perfect accuracy (0.9982) on structured text classification, far surpassing fine-tuned LLMs
- LoRA-tuned Gemma variants show worst performance across all tasks, with accuracy as low as 0.0150 on multiclass text
- Zero-shot VLMs match trained CNNs on multiclass image tasks (0.48 vs 0.48 accuracy) but fail on binary structured text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classical ML models outperform LLMs on structured text classification because they exploit linear separability in numerical feature spaces without tokenization overhead.
- Mechanism: LightGBM and Logistic Regression operate directly on normalized numerical features, preserving exact value relationships. When structured data is converted to natural language prompts for LLMs, numerical precision and feature interactions may be diluted through tokenization and the model's text comprehension pathway.
- Core assumption: The performance gap reflects fundamental processing differences rather than insufficient prompt engineering or hyperparameter tuning.
- Evidence anchors:
  - [abstract]: "This was especially true for structured text-based datasets, where the classical models performed exceptionally well."
  - [section V-A]: "LightGBM model achieved near-perfect classification (0.9982 Accuracy)... The fine-tuned Gemma model (0.6661 Accuracy) performed better than the zero-shot Gemini model (0.4224 Accuracy) but was still far surpassed."
  - [corpus]: Related work on transfer learning intuitions (arXiv:2510.00902) suggests source dataset selection impacts generalizability, but corpus lacks direct comparison of tabular-to-text conversion effects.
- Break condition: If tasks require semantic understanding of unstructured clinical narratives rather than structured features, this advantage may not hold.

### Mechanism 2
- Claim: Minimal LoRA fine-tuning (1 epoch, rank=4) degrades performance because the adaptation is insufficient to shift pretrained representations toward domain-specific decision boundaries.
- Mechanism: PEFT with limited epochs updates only low-rank adapters, potentially creating misaligned gradient updates that neither preserve useful pretrained knowledge nor adequately learn task-specific patterns. The paper shows increasing epochs improves performance, while increasing rank alone degrades it.
- Core assumption: The failure mode is undertraining rather than LoRA being fundamentally unsuitable for these tasks.
- Evidence anchors:
  - [abstract]: "LoRA-tuned Gemma variants consistently showed the worst performance across all text and image experiments, failing to generalize from the minimal fine-tuning provided."
  - [section V-D-2]: "Extending the training to 5 and 10 epochs resulted in substantial gains" (accuracy improved from 0.6661 to 0.8347).
  - [section V-D-1]: "Increasing the LoRA rank to r=8 resulted in performance degradation" (accuracy 0.6606).
  - [corpus]: No direct corpus evidence on epoch scaling for medical PEFT; this appears understudied.
- Break condition: If dataset size is too small relative to model capacity, extended training may cause overfitting rather than convergence.

### Mechanism 3
- Claim: Zero-shot VLMs match trained CNNs on multiclass image classification when pretrained visual features are sufficiently domain-adjacent and class distinctions align with semantic categories.
- Mechanism: VLMs like Gemini 2.5 encode visual information into semantic embedding spaces that may already capture disease-relevant features (texture, shape patterns). When class labels map cleanly to visual concepts (e.g., "bacterial pneumonia" vs "viral pneumonia"), zero-shot classification leverages this pre-aligned knowledge.
- Core assumption: The competitive performance reflects genuine transfer rather than dataset artifacts or class imbalance exploitation.
- Evidence anchors:
  - [abstract]: "Zero-shot LLM/VLM pipelines (Gemini 2.5)... demonstrated competitive performance on the multiclass image task, matching the classical ResNet-50 baseline."
  - [section V-B]: "ResNet-50 baseline and the zero-shot Gemini 2.5 Flash model showed nearly identical performance (0.4812 vs 0.4875 Accuracy)."
  - [corpus]: arXiv:2508.02281 suggests edge-enhanced pre-training benefits medical segmentation, implying visual pretraining quality matters for transfer.
- Break condition: Binary classification tasks may not benefit equally if decision boundaries require fine-grained discrimination beyond pretrained semantic knowledge.

## Foundational Learning

- Concept: **Feature Modality Mismatch**
  - Why needed here: The paper demonstrates that model choice must align with data structure. Structured numerical data → classical ML; unstructured text/images → consider foundation models.
  - Quick check question: Is your input data already in numerical/vector form, or does it require semantic interpretation of raw text/pixels?

- Concept: **PEFT Adaptation Inertia**
  - Why needed here: Standard PEFT guidance often suggests "few epochs for efficient adaptation," but this paper shows medical domains may require 10+ epochs to close performance gaps.
  - Quick check question: Have you budgeted for extended training epochs in your PEFT pipeline, or are you assuming 1-3 epochs will suffice?

- Concept: **Cross-Modal Benchmark Alignment**
  - Why needed here: Fair comparison requires identical data splits, preprocessing, and metrics. The paper explicitly controls for these across all model classes.
  - Quick check question: Are you comparing models on identical test sets with aligned preprocessing, or are you using each model's "standard" pipeline independently?

## Architecture Onboarding

- Component map:
  Classical ML Branch: StandardScaler → LightGBM/LogisticRegression (text) | Custom CNN/ResNet-50 (images)
  Zero-Shot LLM Branch: Raw input → Prompt engineering → Gemini 2.5 Flash (text) / Gemini 2.5 multimodal (images)
  PEFT Branch: Raw input → Prompt formatting → Gemma 1B/4B + LoRA adapters (rank=4, epochs=variable)

- Critical path:
  1. Data preprocessing (median imputation for zeros, class weighting for imbalance)
  2. Modality-specific pipeline selection
  3. Evaluation on held-out test set with consistent splits

- Design tradeoffs:
  - Speed vs. Accuracy: Classical ML offers deterministic inference with lower latency; LLMs add 10-100x inference cost without accuracy gains on structured tasks
  - Adaptability vs. Reliability: Zero-shot VLMs provide rapid deployment for novel tasks but show volatility; trained CNNs offer stable but task-specific performance
  - PEFT Efficiency vs. Effectiveness: Low-rank, few-epoch LoRA saves compute but underperforms; closing the gap requires extended training that erodes efficiency benefits

- Failure signatures:
  - LoRA accuracy near random (e.g., 0.0150 on multiclass text) → insufficient training epochs or rank mismatch
  - Zero-shot accuracy ~0.42 on binary structured data → modality mismatch; use classical ML
  - Classical ML achieving near-perfect scores → check for data leakage or trivial separability

- First 3 experiments:
  1. Establish classical ML baseline (LightGBM for structured text, ResNet-50 for images) on your dataset with stratified splits
  2. Test zero-shot Gemini 2.5 with identical test data to measure gap; if structured text, expect 30-50% accuracy drop
  3. If pursuing PEFT, start with 5-10 epochs (not 1) and monitor for overfitting; compare rank=4 vs. rank=8 on validation set before final selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific adaptations to the PEFT/LoRA training regimen (beyond increased epochs) are required for foundation models to successfully generalize on structured medical tabular data?
- Basis in paper: [explicit] The authors conclude that "minimal fine-tuning proved detrimental" and that while prolonged training (10 epochs) helped, it "reduced, but did not eliminate, the performance gap."
- Why unresolved: The study only tested limited configurations (LoRA ranks 4 and 8; max 10 epochs). It remains unclear if the failure is due to the LoRA architecture itself, insufficient model scale (1B/4B), or the fundamental mismatch between autoregressive objectives and tabular classification.
- What evidence would resolve it: Ablation studies testing full fine-tuning, higher LoRA ranks, or alternative PEFT methods (e.g., adapters, prefix-tuning) on the same structured datasets to isolate the adaptation bottleneck.

### Open Question 2
- Question: To what extent does the near-perfect performance of classical ML on the text datasets indicate data leakage or trivial linear separability rather than meaningful pattern recognition?
- Basis in paper: [explicit] The Conclusion states that the 0.9982 accuracy achieved by LightGBM "may indicate easy separability or possible data leaking," questioning the validity of the benchmark's ceiling.
- Why unresolved: The paper establishes the high performance but does not investigate the *cause* of the classical model's superior ceiling or if the dataset is artificially simple for LLMs to interpret via natural language prompts.
- What evidence would resolve it: Feature importance analysis (SHAP values) for the LightGBM model to check for artificial proxies, or evaluation on a more complex, noisier clinical text dataset where separability is lower.

### Open Question 3
- Question: Why do zero-shot VLMs match classical CNNs on multiclass respiratory imaging but significantly underperform on binary skin cancer classification?
- Basis in paper: [inferred] Results show the zero-shot Gemini matching ResNet-50 on multiclass (approx. 0.48 accuracy) but failing on binary (0.58 vs 0.82), suggesting specific task complexities or prompt sensitivities that were not analyzed.
- Why unresolved: The paper reports the "mixed results" but does not provide an error analysis explaining why VLMs handle the complexity of multiclass lung patterns better than the binary visual task of malignancy detection.
- What evidence would resolve it: A qualitative error analysis comparing VLM attention maps against CNN feature maps for both binary and multiclass tasks to determine if the failure is visual or semantic.

## Limitations

- Limited model and epoch diversity: Only tests LoRA rank=4 and 1 epoch before extending to 5-10 epochs, potentially missing optimal configurations
- Fixed zero-shot baseline: Using only Gemini 2.5 Flash limits generalizability to other VLM architectures
- Dataset scale constraints: Small datasets (max 2,768 rows, 2,637 images) may not reflect performance on larger clinical datasets

## Confidence

- High Confidence: Classical ML superiority on structured text classification, LoRA 1-epoch underperformance, and extended training improving PEFT results
- Medium Confidence: Zero-shot VLM competitiveness on multiclass image tasks, though may reflect dataset-specific properties
- Low Confidence: Generalizability to larger datasets, other medical domains, or different VLM architectures

## Next Checks

1. **Epoch Sensitivity Analysis**: Systematically test LoRA fine-tuning across 1, 3, 5, 10, and 20 epochs on the Mental Health dataset to identify the inflection point where performance plateaus versus overfitting begins.

2. **Cross-VLM Validation**: Replicate the zero-shot multiclass image classification results using at least two additional VLMs (e.g., GPT-4V, Claude 3) to determine if Gemini 2.5's competitive performance is architecture-specific or a general VLM property.

3. **Structured Data Prompt Engineering**: Test alternative prompt formats for structured text (tabular embedding, feature concatenation, natural language description) to quantify the performance cost of modality conversion versus inherent LLM limitations.