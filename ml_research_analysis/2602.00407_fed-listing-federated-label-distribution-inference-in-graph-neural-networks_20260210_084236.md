---
ver: rpa2
title: 'Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks'
arxiv_id: '2602.00407'
source_url: https://arxiv.org/abs/2602.00407
tags:
- attack
- distribution
- label
- data
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses label distribution inference attacks in federated
  graph neural networks (FedGNNs), where a malicious server attempts to infer the
  label proportions of target clients without access to raw data. The proposed Fed-Listing
  method leverages final-layer gradients exchanged during federated training to uncover
  statistical patterns revealing class proportions.
---

# Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks

## Quick Facts
- **arXiv ID:** 2602.00407
- **Source URL:** https://arxiv.org/abs/2602.00407
- **Reference count:** 40
- **Primary result:** Demonstrates a gradient-based label distribution inference attack on federated graph neural networks that outperforms existing baselines even under non-i.i.d. scenarios.

## Executive Summary
This paper introduces Fed-Listing, a novel attack method that exploits gradient information exchanged during federated graph neural network training to infer the label distribution of target clients. The attack leverages final-layer gradients to uncover statistical patterns revealing class proportions, using an auxiliary shadow dataset to train an attack model. Extensive experiments across multiple datasets and GNN architectures demonstrate that Fed-Listing significantly outperforms existing baselines like random guessing and Decaf, exposing a critical privacy vulnerability in federated graph neural networks.

## Method Summary
Fed-Listing is a gradient-based inference attack that targets label distributions in federated graph neural networks. The method exploits final-layer gradients exchanged during federated training to uncover statistical patterns that reveal class proportions. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions to train an attack model. The attack is evaluated across multiple benchmark datasets and GNN architectures, demonstrating superior performance compared to existing baselines even under challenging non-i.i.d. scenarios.

## Key Results
- Fed-Listing significantly outperforms random guessing and Decaf baselines in label distribution inference
- Attack achieves superior performance across various label distribution settings, particularly in single-class and one-class-dominant distributions
- Demonstrates consistent vulnerability across four benchmark datasets and three GNN architectures

## Why This Works (Mechanism)
Fed-Listing exploits the inherent privacy vulnerability in federated graph neural networks where final-layer gradients exchanged during training contain statistical patterns revealing class proportions. The attack model leverages these gradients combined with a shadow dataset to reconstruct label distributions without accessing raw data. The method works because gradient updates during federated learning preserve information about the underlying data distribution, particularly for the final classification layer where label information is most concentrated.

## Foundational Learning
- **Federated Learning:** Distributed machine learning framework where multiple clients train models collaboratively without sharing raw data. Needed because Fed-Listing operates within this architecture to exploit gradient exchanges.
- **Graph Neural Networks (GNNs):** Neural networks designed to operate on graph-structured data, learning node representations through message passing. Critical as Fed-Listing specifically targets GNN architectures in federated settings.
- **Gradient-based Inference Attacks:** Methods that extract information about training data by analyzing model gradients. Fundamental to Fed-Listing's approach of inferring label distributions from gradient patterns.
- **Non-i.i.d. Data Distributions:** Data heterogeneity across clients where distributions differ from client to client. Important because Fed-Listing's effectiveness is evaluated under these realistic federated learning conditions.
- **Shadow Dataset Technique:** Using auxiliary data to simulate various client distributions and train attack models. Key methodology enabling Fed-Listing to generalize across different label distribution scenarios.

## Architecture Onboarding

**Component Map:** Shadow Dataset -> Attack Model Training -> Gradient Analysis -> Label Distribution Inference

**Critical Path:** The attack requires access to final-layer gradients during federated training, a shadow dataset for training the attack model, and the ability to analyze gradient patterns to infer label distributions. The critical path flows from gradient collection through pattern analysis to distribution reconstruction.

**Design Tradeoffs:** The method balances attack effectiveness against computational overhead - using shadow datasets enables generalization but requires significant preprocessing. The choice to target final-layer gradients maximizes label information extraction while minimizing gradient dimensionality.

**Failure Signatures:** Attack performance degrades when gradients are heavily perturbed (through differential privacy or compression) or when the shadow dataset poorly represents target client distributions. Complete failure occurs when gradients are eliminated or when label distributions are perfectly uniform.

**First Experiments:** 1) Test Fed-Listing performance with varying shadow dataset sizes to understand sensitivity to auxiliary data quantity. 2) Evaluate attack effectiveness when applying different levels of gradient compression to identify the threshold where inference becomes impossible. 3) Assess transferability of the attack when the shadow dataset distribution significantly differs from target client distributions.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation of defense mechanisms beyond three tested strategies (differential privacy, noisy gradients, gradient compression)
- Potential impact of different graph structures or larger-scale federated settings not explored in the study
- Does not investigate the effect of varying client participation rates or communication rounds on attack feasibility

## Confidence
- **High:** Core attack methodology using gradient analysis and shadow dataset approach is well-justified and empirically validated
- **Medium:** Comparative advantage over Decaf is demonstrated but limited to one baseline
- **Low:** Practical impact in real-world federated learning deployments remains uncertain due to potential mitigating factors

## Next Checks
1) Test Fed-Listing against more diverse defense strategies including secure aggregation and differential privacy with varying privacy budgets
2) Evaluate attack performance under heterogeneous graph structures and larger client populations
3) Assess transferability of the attack when the shadow dataset differs significantly from target client data distributions