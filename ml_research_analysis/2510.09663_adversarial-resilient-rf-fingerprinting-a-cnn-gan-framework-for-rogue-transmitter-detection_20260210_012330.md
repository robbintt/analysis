---
ver: rpa2
title: 'Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter
  Detection'
arxiv_id: '2510.09663'
source_url: https://arxiv.org/abs/2510.09663
tags:
- devices
- genuine
- rogue
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a CNN-GAN framework for rogue device detection
  using RF fingerprinting. The approach addresses the problem of distinguishing genuine
  devices from both real rogue devices and synthetically generated ones that mimic
  genuine RF characteristics.
---

# Adversarial-Resilient RF Fingerprinting: A CNN-GAN Framework for Rogue Transmitter Detection

## Quick Facts
- arXiv ID: 2510.09663
- Source URL: https://arxiv.org/abs/2510.09663
- Authors: Raju Dhakal; Prashant Shekhar; Laxima Niure Kandel
- Reference count: 14
- Primary result: CNN-GAN framework achieves 96.7% rogue detection and 97.6% genuine device classification accuracy using I/Q fingerprinting

## Executive Summary
This paper proposes a CNN-GAN framework for detecting rogue transmitters in wireless networks using RF fingerprinting. The approach leverages unique hardware imperfections in SDRs to distinguish genuine devices from both real and synthetic rogue devices. The system employs a CNN classifier trained on genuine devices, combined with a softmax probability threshold to detect anomalies, while a GAN generates synthetic rogue samples for adversarial testing. The method was validated using ten ADALM-PLUTO SDRs, achieving high detection accuracy while maintaining robustness against both real and synthetic attacks.

## Method Summary
The framework processes I/Q samples from SDRs by aggregating 10 consecutive frames (72 samples each) into 720-sample sequences, then standardizing using training statistics. A CNN with one convolutional layer (32 filters, 5×1 kernel) classifies genuine devices (7 classes), while a threshold on the maximum softmax probability separates genuine from rogue devices. A GAN generates synthetic rogue samples using feature matching loss, trained on genuine device data. The threshold is calibrated using a validation set to maximize F1-score for rogue detection (0.1987). Performance is evaluated on real rogue devices and GAN-generated samples.

## Key Results
- CNN achieves 96.7% detection accuracy for rogue devices and 97.6% for genuine devices
- All ten genuine devices are correctly classified
- GAN-generated samples achieve Fréchet Distance score of 0.0545, indicating close match to real I/Q distributions
- Softmax probability thresholding effectively separates genuine devices (tightly clustered near 1.0) from rogue devices (broader distribution)

## Why This Works (Mechanism)

### Mechanism 1: Hardware Imperfections as Device Fingerprints
Each transmitter's analog components introduce minute distortions manifesting as consistent deviations in the I/Q constellation. The CNN learns to extract these fine-grained spatial patterns from stacked I/Q sequences, treating them as 2D "images." L2 regularization prevents overfitting to noise rather than true hardware signatures.

### Mechanism 2: Maximum Softmax Probability as Out-of-Distribution Detector
Rogue devices produce lower maximum softmax probabilities than genuine devices because they don't align closely with any learned class distribution. After temperature-scaled softmax (T=2.5), samples from known devices yield high-confidence predictions while rogue devices produce more uncertain distributions.

### Mechanism 3: GAN-Generated Adversarial Samples for Robustness Testing
The generator learns to map random noise vectors to I/Q sequences that match the statistical distribution of genuine device signals. Feature matching loss encourages the generator to produce samples whose intermediate discriminator features resemble real samples, creating realistic synthetic samples for stress-testing the detection system.

## Foundational Learning

- **I/Q Signal Representation**: I/Q samples represent orthogonal components of a complex baseband signal (magnitude and phase information). Why needed: The entire pipeline operates on I/Q samples. Quick check: Given I = 0.707 and Q = 0.707, what is the signal's phase angle?

- **Softmax Temperature Scaling**: The detection threshold depends critically on T=2.5 in the softmax function. Why needed: Higher temperatures flatten probability distributions, affecting threshold calibration. Quick check: If T increases from 1.0 to 3.0, will the maximum softmax probability for a given input increase or decrease?

- **Fréchet Distance for Distribution Comparison**: The paper uses FD = 0.0545 to claim GAN samples match real I/Q distributions. Why needed: FD measures similarity between multivariate Gaussians, helping interpret perceptual similarity. Quick check: What does an FD score approaching 0 indicate about two distributions?

## Architecture Onboarding

- **Component map**: Data preprocessing (frame merging, standardization, reshaping) -> CNN classifier (conv2d -> batch norm -> maxpool -> dense -> dropout -> output) -> Threshold calculator (validation set -> softmax -> grid search) -> GAN (generator + discriminator) -> Inference pipeline (input -> CNN -> softmax -> max probability -> threshold comparison)

- **Critical path**: 1) Collect I/Q data from genuine devices only, 2) Train CNN on genuine samples, 3) Train GAN on same genuine samples, 4) Use validation device + subset of genuine data to calibrate threshold, 5) Test on unseen genuine samples + real rogue devices + GAN-generated synthetic rogues

- **Design tradeoffs**: Frame aggregation increases temporal context but reduces dataset size and increases latency; temperature T=2.5 smooths confidence improving rogue detection but may increase false positives; GAN for testing only evaluates robustness without adversarial training

- **Failure signatures**: High false positive rate (threshold too high or temperature too low); rogue devices classified as genuine (feature space overlap); GAN samples detected as genuine (generator learned CNN-relevant features)

- **First 3 experiments**: 1) Reproduce threshold calibration by plotting max softmax probability distributions for validation set, 2) Ablate temperature scaling by testing T ∈ {1.0, 1.5, 2.0, 2.5, 3.0} and plotting rogue F1-score vs. threshold, 3) Train GAN and compute FD score by generating 1000 synthetic samples and visually comparing I/Q constellations

## Open Questions the Paper Calls Out

- **Question 1**: Does the framework maintain accuracy and computational efficiency when scaling to networks with hundreds of heterogeneous devices? Basis: Study limited to ten devices; real-world networks may involve hundreds. Why unresolved: Small controlled set may not reflect high-dimensional classification spaces. What evidence: Results with >100 authorized devices.

- **Question 2**: How does device mobility and varying Doppler shift affect softmax probability thresholding stability? Basis: All data from stationary devices; real-time systems often involve mobile nodes. Why unresolved: Rapid channel fluctuations could obscure hardware imperfections. What evidence: Performance metrics from varying velocities.

- **Question 3**: To what extent do environmental factors impact reliability of fixed softmax threshold over time? Basis: Data collected in specific indoor lab environment; threshold fixed based on validation set. Why unresolved: RF fingerprints can drift due to temperature changes or channel conditions. What evidence: Long-term temporal analysis under different environmental conditions.

## Limitations

- The controlled lab environment with ten identical SDRs may not reflect real-world conditions where environmental interference, mobility, and hardware aging affect RF signatures
- CNN's effectiveness depends entirely on hardware imperfections remaining stable across transmissions, which is not validated under varying temperature, power conditions, or time intervals
- GAN-generated samples, while statistically similar, may not capture the full complexity of sophisticated adversarial attacks that target the CNN's learned feature space

## Confidence

**High Confidence**: CNN achieves 96.7% detection accuracy for rogue devices and 97.6% for genuine devices on tested dataset. Softmax probability thresholding effectively separates known from unknown device distributions within controlled setup.

**Medium Confidence**: GAN successfully generates I/Q samples statistically similar to genuine device outputs, but visual similarity does not guarantee the synthetic samples would deceive the CNN's feature extractor.

**Low Confidence**: Claims about real-world applicability against sophisticated adversaries are not well-supported, as the paper only tests against two real rogue devices and synthetic samples from the same SDR model.

## Next Checks

1. **Environmental Robustness Test**: Collect I/Q samples from the same ten SDRs under varying temperature conditions (5°C to 35°C) and at 24-hour intervals to assess whether hardware fingerprints remain stable and detectable by the CNN.

2. **Feature Space Adversarial Attack**: Generate adversarial I/Q samples using white-box attacks targeting the CNN's feature extractor (not just I/Q statistics) to test whether the threshold-based detection system can identify samples that visually mimic genuine devices but fool the classifier.

3. **Cross-Hardware Generalization**: Replace two of the ten SDRs with different models (e.g., USRP X310) and retrain/validate the CNN to determine if the framework generalizes beyond identical hardware platforms.