---
ver: rpa2
title: Mechanistic Understanding of Language Models in Syntactic Code Completion
arxiv_id: '2502.18499'
source_url: https://arxiv.org/abs/2502.18499
tags:
- code
- closing
- parentheses
- logit
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal mechanisms of a Code Language
  Model (CodeLlama-7b) performing a syntax completion task, specifically closing parentheses.
  The authors conduct one of the first mechanistic interpretability studies on Code
  LMs, analyzing how the model uses its knowledge to complete code syntax.
---

# Mechanistic Understanding of Language Models in Syntactic Code Completion

## Quick Facts
- **arXiv ID**: 2502.18499
- **Source URL**: https://arxiv.org/abs/2502.18499
- **Reference count**: 12
- **Primary result**: First mechanistic interpretability study of a Code Language Model performing closing parentheses syntax completion, revealing late-layer crystallization, MHA dominance, and incorrect knowledge association in specialized heads.

## Executive Summary
This paper conducts a mechanistic interpretability analysis of CodeLlama-7b performing closing parenthesis syntax completion. Using a synthetic dataset of 168 prompts with 2-4 required closing parentheses, the authors employ logit lens analysis and attention visualization to understand how the model computes the correct output. They discover that the model requires middle-to-late layers to confidently predict the correct token, that multi-head attention sublayers contribute more critically than feed-forward layers, and that specific attention heads track closed parentheses but can promote incorrect outputs. This work provides foundational insights into how code language models perform syntactic reasoning tasks.

## Method Summary
The study analyzes CodeLlama-7b-hf (a 32-layer decoder-only transformer) on a synthetic dataset of 168 prompts requiring 2-4 closing parentheses. The authors use TransformerLens to perform logit lens analysis, projecting intermediate activations through the unembedding matrix to examine what the model "would predict" at each layer. They compute logit differences between correct and counterfactual tokens across layers and sublayers (MHA vs. FF), and visualize attention patterns of critical heads. The analysis focuses on identifying when correct tokens emerge in the ranking, which sublayers contribute most to distinguishing correct from incorrect predictions, and how specific attention heads behave across different sub-tasks.

## Key Results
- Code Llama requires middle-to-late layers (25-30) before confidently predicting the correct closing parenthesis token
- Multi-head attention sublayers contribute more than feed-forward layers to distinguishing correct from counterfactual tokens
- Two attention heads (L30H0 and L27H24) track closed parentheses but L27H24 promotes incorrect knowledge association for 3+ closing parentheses tasks

## Why This Works (Mechanism)

### Mechanism 1: Late-Layer Token Crystallization
Code Llama requires middle-to-late layers (25-30 of 32) before confidently predicting the correct closing parenthesis token. The residual stream accumulates syntactic evidence across layers, with early layers ranking the correct token outside the top 10, by layer ~18 it enters top 10, and by layer ~25 it becomes top-1 for simpler cases. Harder sub-tasks (3-4 closing parens) require even later layers. This assumes logit lens projections faithfully represent what the model "would predict" at each layer.

### Mechanism 2: MHA-Dominant Syntactic Contribution
Multi-head attention contributes more critically than feed-forward layers to distinguishing correct from counterfactual closing tokens. Both MHA and FF sub-layers contribute, but MHA sub-layers show larger positive logit differences between correct and counterfactual tokens. The most salient negative contributions come from MLP (FF) layers for some sub-tasks. This assumes logit difference between correct and counterfactual tokens measures task-specific contribution.

### Mechanism 3: Incorrect Knowledge Association in Specialized Heads
Some attention heads correctly track context (counting closed parentheses) but promote incorrect outputs regardless. L27H24 attends to the correct position but rigidly promotes "))" regardless of actual need. L30H0 uses the same attention pattern but dynamically promotes the correct count. This creates a positive-negative contribution split across sub-tasks. This assumes attention pattern indicates "understanding" of context while output promotion is semi-independent.

## Foundational Learning

- **Residual Stream and Logit Lens**: Understanding how information accumulates across layers and how to inspect intermediate predictions. Quick check: If you project layer 10's residual to vocabulary space and see the correct token ranked 50th, what does that tell you?
- **Attention Head Specialization**: The paper identifies specific heads (L30H0, L27H24) with distinct functional roles despite similar attention patterns. Quick check: Can two heads attend to identical positions but promote different tokens? How?
- **Counterfactual Token Analysis**: The paper contrasts correct vs. counterfactual tokens to isolate task-relevant contributions from generic promotion. Quick check: Why compare logit(correct) - logit(counterfactual) instead of just logit(correct)?

## Architecture Onboarding

- **Component map**: Input tokenization → embedding → layers 1-32 (MHA + FF per layer) → final residual → unembedding → softmax → token prediction. For closing-paren task, layers 25-31 are decision-critical.
- **Critical path**: For closing parenthesis completion, information flows through all layers but decision-critical computation occurs in layers 25-30 where correct token crystallizes in the top ranking.
- **Design tradeoffs**: Synthetic dataset (168 prompts) enables controlled analysis but limits generalization claims. Tokenization constraints affect which closing-paren counts appear naturally. Single model (CodeLlama-7b) studied; universality unknown.
- **Failure signatures**: Model fails on 3-closing-paren task when open-paren count exceeds ~9. L27H24 promotes wrong count for 3+ tasks. Performance degrades with nesting depth.
- **First 3 experiments**:
  1. Replicate logit lens analysis on 5 sample prompts per sub-task to verify layer timing (target: correct token enters top-10 by layer 18-20, top-1 by 25-30).
  2. Ablate L27H24 (zero out its output) and measure accuracy change on 3-closing-paren prompts (hypothesis: accuracy improves if this head causes incorrect knowledge association).
  3. Visualize attention patterns for L30H0 and L27H24 on a single prompt to confirm both track closed-paren positions despite different output behaviors.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can suppressing attention heads with "incorrect knowledge association" (like L27H24) improve Code LM accuracy on syntax completion tasks? The paper identifies L27H24 as promoting incorrect behavior but does not test intervention. Ablation experiments showing improved accuracy when suppressing L27H24 during inference on tasks requiring 3+ closing parentheses would resolve this.

- **Open Question 2**: Do Code LMs reuse the same internal components across programming languages for analogous syntactic tasks (e.g., brace matching in JavaScript, indentation in Python)? This study only examines Python parenthesis completion using heads L30H0 and L27H24. Cross-linguistic experiments mapping whether identical heads activate for analogous structures would resolve this.

- **Open Question 3**: Can a complete circuit mapping both MHA and MLP sub-layers explain how Code LMs perform syntax completion? Current analysis emphasizes MHA; MLP's role in knowledge lookup remains underexplored. Integrated circuit diagrams showing FF and attention layer interactions across all 32 layers would resolve this.

- **Open Question 4**: Why does the Three Closing Parentheses sub-task show lower accuracy (76.2%) than both Two (100%) and Four (100%) sub-tasks? The failure cases (9-11 open parentheses) suggest a threshold effect, but the internal cause is unidentified. Attention visualization and logit analysis on failing prompts revealing which heads malfunction at high parenthesis counts would resolve this.

## Limitations

- Synthetic dataset of 168 prompts limits generalization to natural code completion scenarios
- Single-model analysis (CodeLlama-7b) raises questions about universality of identified mechanisms
- Correlation vs. causation in mechanistic claims not validated through interventions

## Confidence

**High confidence**: The model requires middle-to-late layers (25-30) before confidently predicting correct closing parentheses; multi-head attention sublayers contribute more than feed-forward layers to distinguishing correct from counterfactual tokens; logit lens analysis correctly identifies when correct tokens emerge in the ranking.

**Medium confidence**: L30H0 and L27H24 specifically track the number of already-closed parentheses through their attention patterns; L27H24 promotes incorrect knowledge association for 3+ closing parenthesis tasks; FF sublayers provide the most salient negative contributions to logit differences for some sub-tasks.

**Low confidence**: The specific mechanisms identified are universal across code language models; L27H24's behavior represents a fundamental limitation in the model's syntactic reasoning rather than tokenization artifact; the synthetic dataset patterns generalize to real-world code completion scenarios.

## Next Checks

1. **Ablation experiment on L27H24**: Perform head ablation by zeroing out L27H24's output and measure accuracy changes specifically on 3-closing-paren prompts. This would test the causal claim that L27H24 has a "negative effect" on tasks requiring more than two closing parentheses. Compare accuracy on 3-paren tasks with and without L27H24 active across multiple random seeds.

2. **Cross-model validation**: Replicate the analysis on at least two other code language models (e.g., CodeT5-base, StarCoder) and one non-code LLM (e.g., LLaMA-7b). Compare whether similar late-layer crystallization, MHA dominance, and specialized head behaviors emerge. This would test the universality of the identified mechanisms beyond CodeLlama-7b.

3. **Natural code completion validation**: Create a dataset of actual code snippets requiring closing parenthesis completion (not synthetic prompts) and test whether the same layer timing and head behaviors predict correct completions in realistic scenarios. This would validate whether the synthetic task patterns generalize to practical code completion use cases.