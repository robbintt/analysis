---
ver: rpa2
title: 'IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language
  Understanding'
arxiv_id: '2501.15747'
source_url: https://arxiv.org/abs/2501.15747
tags:
- languages
- indic
- language
- across
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndicMMLU-Pro, a comprehensive benchmark
  for evaluating Large Language Models (LLMs) on multi-task language understanding
  across nine major Indic languages. The benchmark adapts the MMLU-Pro framework to
  address the unique linguistic diversity and cultural complexity of the Indian subcontinent,
  covering languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi,
  Tamil, Telugu, and Urdu.
---

# IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding

## Quick Facts
- arXiv ID: 2501.15747
- Source URL: https://arxiv.org/abs/2501.15747
- Reference count: 27
- This paper introduces IndicMMLU-Pro, a comprehensive benchmark for evaluating Large Language Models (LLMs) on multi-task language understanding across nine major Indic languages

## Executive Summary
This paper introduces IndicMMLU-Pro, a comprehensive benchmark for evaluating Large Language Models (LLMs) on multi-task language understanding across nine major Indic languages. The benchmark adapts the MMLU-Pro framework to address the unique linguistic diversity and cultural complexity of the Indian subcontinent, covering languages such as Hindi, Bengali, Gujarati, Marathi, Kannada, Punjabi, Tamil, Telugu, and Urdu. The dataset was created using IndicTrans2 for translation from English MMLU-Pro, followed by rigorous quality assurance including back-translation and expert proofreading. Baseline results show GPT-4o significantly outperforms other models, achieving accuracy scores between 38.46% and 44.80% across languages, while specialized Indic language models demonstrate competitive performance. The dataset and evaluation pipeline are publicly available on Hugging Face, providing a standardized framework for advancing research in Indic language AI.

## Method Summary
The IndicMMLU-Pro benchmark was created by translating the English MMLU-Pro dataset into nine major Indic languages using IndicTrans2, followed by rigorous quality assurance including back-translation and expert proofreading. The benchmark covers 15 diverse domains including mathematics, humanities, and social sciences, with each language receiving approximately 551 questions. The evaluation protocol employs exact string matching to maintain consistency across languages, with model responses processed through language-specific normalization steps. The dataset creation process emphasized cultural relevance by incorporating region-specific examples and addressing unique linguistic challenges such as code-mixing and script variations.

## Key Results
- GPT-4o achieved the highest accuracy scores ranging from 38.46% to 44.80% across all nine Indic languages
- Specialized Indic language models demonstrated competitive performance, with Sarvam-1 achieving 35.19% accuracy on the Hindi subset
- English MMLU-Pro models showed significant performance degradation when evaluated on Indic languages, with scores dropping from 44.66% to 28.84% on average

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its adaptation of the proven MMLU-Pro framework to Indic languages while addressing specific linguistic challenges. The translation-based approach enables rapid dataset creation while maintaining domain coverage consistency. The quality assurance process, including back-translation and expert review, helps preserve semantic integrity across languages. The exact string matching evaluation protocol ensures fair comparison across different scripts and writing systems. The inclusion of culturally relevant examples helps models better understand context-specific knowledge in the Indian subcontinent.

## Foundational Learning
- Multi-task language understanding - why needed: Enables comprehensive evaluation across diverse knowledge domains; quick check: Benchmark covers 15 distinct domains from STEM to humanities
- Translation-based dataset creation - why needed: Allows leveraging existing high-quality English benchmarks; quick check: Back-translation and expert review verify translation quality
- Exact string matching evaluation - why needed: Ensures consistent scoring across languages with different scripts; quick check: Language-specific normalization handles script variations

## Architecture Onboarding
- Component map: English MMLU-Pro -> IndicTrans2 translation -> Quality assurance -> Language-specific normalization -> Model evaluation -> Accuracy scoring
- Critical path: Translation -> Quality assurance -> Evaluation -> Scoring
- Design tradeoffs: Translation-based approach enables rapid dataset creation but may introduce subtle meaning shifts; exact string matching ensures consistency but may be overly strict
- Failure signatures: Poor performance on code-mixed content, script conversion errors, domain-specific terminology misinterpretation
- First experiments: 1) Evaluate model performance on individual domains to identify strengths/weaknesses, 2) Test code-mixing handling by evaluating mixed-language inputs, 3) Compare exact vs. fuzzy string matching evaluation protocols

## Open Questions the Paper Calls Out
- How to improve model performance on code-mixed Indic language content, which is prevalent in informal communication
- The impact of regional dialect variations on model understanding and performance across different Indian states
- Strategies for handling domain-specific terminology that may not have direct translations in certain Indic languages
- The role of cultural context in understanding questions and providing accurate responses in Indic languages

## Limitations
- Translation-based methodology may introduce quality issues despite back-translation verification, potentially affecting nuanced meanings in complex academic content
- Benchmark covers only 9 of India's 22 officially recognized languages, missing important linguistic communities despite representing ~75% of population
- Dataset size (4,961 questions total) may be insufficient for robust statistical analysis, particularly for languages with fewer questions
- Exact string matching evaluation may be overly strict and not account for semantically equivalent but syntactically different correct answers
- Limited coverage of regional dialects and variations within the nine major languages included in the benchmark

## Confidence
- Benchmark creation methodology: High confidence - The translation and quality assurance process is clearly documented and follows established practices
- Baseline results: Medium confidence - Results are presented clearly, but lack detailed statistical significance testing
- Language coverage representativeness: Low confidence - Limited explanation of language selection criteria and dialect considerations
- Cultural relevance of examples: Medium confidence - While examples were curated for cultural relevance, the extent of this curation is not fully detailed

## Next Checks
1. Conduct inter-annotator agreement studies on a subset of translated questions to quantify translation quality and consistency across languages
2. Perform statistical significance testing between model performances to determine if observed differences are meaningful
3. Extend the benchmark to include additional Indian languages and dialectal variants to improve linguistic coverage and cultural representativeness
4. Develop alternative evaluation protocols that account for semantic equivalence rather than exact string matching
5. Create specialized subsets focusing on code-mixed content and regional dialect variations to better assess model capabilities in real-world scenarios