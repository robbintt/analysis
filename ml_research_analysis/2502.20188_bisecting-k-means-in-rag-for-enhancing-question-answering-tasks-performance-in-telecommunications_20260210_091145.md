---
ver: rpa2
title: Bisecting K-Means in RAG for Enhancing Question-Answering Tasks Performance
  in Telecommunications
arxiv_id: '2502.20188'
source_url: https://arxiv.org/abs/2502.20188
tags:
- arxiv
- information
- query
- data
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a RAG framework for telecommunications domain
  question-answering using Bisecting K-Means clustering on embeddings to organize
  3GPP document chunks by content. A routing module pre-selects relevant clusters
  for each query, improving retrieval relevance.
---

# Bisecting K-Means in RAG for Enhancing Question-Answering Tasks Performance in Telecommunications

## Quick Facts
- arXiv ID: 2502.20188
- Source URL: https://arxiv.org/abs/2502.20188
- Reference count: 22
- Achieved 72.13% accuracy on TeleQnA using phi-3 with RAG + clustering + routing

## Executive Summary
This work introduces a RAG framework for telecommunications question-answering that employs Bisecting K-Means clustering on document embeddings to organize content into coherent clusters, coupled with a routing module that pre-selects relevant clusters for each query. The framework enhances retrieval relevance by grouping embedding vectors by semantic content rather than source document structure, and augments user queries with technical definitions and abbreviations extracted from 3GPP documents. Fine-tuning small language models (phi-2 and phi-3) on the TeleQnA dataset with RAG-generated context achieves high accuracy while maintaining computational feasibility and reduced training times.

## Method Summary
The framework processes 3GPP documents by extracting acronyms and technical terms, chunking at 500 characters, and generating embeddings with BAAI bge-large-en. Bisecting K-Means (18 clusters) organizes embeddings hierarchically by content, stored in SQLite3. Queries are enhanced by appending extracted definitions before routing via a feed-forward module (256-dim intermediate) that selects top-8 clusters using softmax probabilities. Retrieval uses FAISS on selected clusters, and fine-tuning employs HuggingFace AutoTrain with RAG-generated context in a chat template. Evaluation on TeleQnA shows 66.12% (phi-2) and 72.13% (phi-3) accuracy.

## Key Results
- Achieved 66.12% accuracy with phi-2 and 72.13% with phi-3 on TeleQnA
- Training completed in under 16 hours, significantly faster than larger models
- Improved retrieval relevance through content-based clustering and query enhancement
- Demonstrated computational feasibility for deployment in telecom QA systems

## Why This Works (Mechanism)

### Mechanism 1: Content-Based Clustering via Bisecting K-Means
- Organizes embedding vectors by semantic content rather than source document structure
- Uses hierarchical divisive clustering, splitting highest SSE cluster with K-Means K=2 until 18 clusters reached
- Assumes coherent clusters improve retrieval relevance for domain queries
- Evidence: Framework explicitly uses Bisecting K-Means for embedding organization; TopClustRAG employs K-Means clustering similarly
- Break condition: Poor cluster quality when documents lack semantic distinctiveness or chunks are too short

### Mechanism 2: Query Enhancement via Domain Glossary Injection
- Augments user queries with technical definitions and abbreviations from 3GPP documentation
- Extracts acronym-definition pairs into lookup dictionaries for inline expansion during query time
- Assumes general-purpose models lack telecom-specific terminology representation
- Evidence: Framework incorporates preprocessing step to extract definitions from 3GPP docs; Chat3GPP addresses document complexity but not glossary injection
- Break condition: Incomplete glossary or ambiguous definitions introduce retrieval noise

### Mechanism 3: Learned Routing Module for Cluster Pre-Selection
- Neural module trained to select relevant clusters, reducing search space and improving efficiency
- Projects cluster centroids and query embeddings through feed-forward layers to 256 dimensions, combines linearly, applies softmax classifier
- Assumes learnable mapping from query to relevant clusters preserves context while eliminating irrelevant space
- Evidence: Framework uses routing module with FF layers and softmax; no direct corpus comparison but probability-guided RAG uses different approach
- Break condition: Queries spanning multiple relevant clusters or undertrained classifier excluding necessary context

## Foundational Learning

- Concept: Bisecting K-Means Clustering
  - Why needed here: Core indexing mechanism using hierarchical divisive clustering and SSE-based split selection
  - Quick check question: Given a set of embeddings, how does Bisecting K-Means decide which cluster to split next, and when does it stop?

- Concept: Retrieval-Augmented Generation (RAG) Architecture
  - Why needed here: Framework extends standard RAG with clustering and routing; understanding retrieval models and context injection is prerequisite
  - Quick check question: In a standard RAG pipeline, what are the two core components and how do they interact during inference?

- Concept: Small Language Model Fine-Tuning
  - Why needed here: Framework fine-tunes phi-2/phi-3 with RAG context; understanding training data formatting and chat templates is necessary
  - Quick check question: Why might fine-tuning an SLM with retrieved context produce different behavior than fine-tuning on static domain text alone?

## Architecture Onboarding

- Component map:
  Pre-processing Block (offline): Source documents → Data extraction (acronyms/terms) → Document chunking (500 chars) → Embedding model (BAAI bge-large-en) → Bisecting K-Means (18 clusters) → Vector DB storage (SQLite3)
  Online Processing Block: User query → Query enhancement (glossary injection) → Embedding → Routing module (FF layers + softmax, top-8 clusters) → FAISS indexing → Context retrieval → Prompt engineering → Fine-tuned SLM → Response

- Critical path:
  1. Glossary extraction quality directly affects query enhancement effectiveness
  2. Embedding model choice determines cluster semantic coherence
  3. Router classifier accuracy gates retrieval precision; misrouting cascades to irrelevant context
  4. Fine-tuning data format (context + query + answer) shapes model's ability to use retrieved information

- Design tradeoffs:
  - Chunk size: 500 chars performed better than 250 for phi-2, balancing granularity vs context fragmentation
  - Number of clusters: 18 selected empirically; too few reduce discrimination, too many increase routing complexity
  - Top-K clusters: 8 used; higher K increases recall but reduces efficiency
  - Enhanced vs. standard training data: Enhanced responses with citations performed worse on accuracy but improved traceability

- Failure signatures:
  - Router consistently selecting wrong clusters → check classifier training data balance and centroid representation
  - Query enhancement introducing irrelevant definitions → review glossary extraction filters for precision
  - Retrieved context not improving answers → verify chunk-document alignment and embedding model domain compatibility
  - Fine-tuned model hallucinating despite context → inspect training prompt format and context-answer alignment in training data

- First 3 experiments:
  1. Reproduce baseline: Implement standard RAG (no clustering, no query enhancement) on TeleQnA with phi-2 to establish accuracy baseline
  2. Ablate clustering: Add Bisecting K-Means with 18 clusters but no routing module; compare retrieval relevance and accuracy to baseline
  3. Ablate query enhancement: Enable clustering + routing, test with and without glossary-based query expansion; measure delta in accuracy and inspect failure cases where expansion hurts performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating knowledge graphs as an alternative or complementary knowledge representation to Bisecting K-Means clustering further improve retrieval accuracy and reasoning in telecom RAG systems?
- Basis in paper: [explicit] "Future research could explore novel knowledge representation techniques, such as knowledge graphs, to improve information retrieval and reasoning capabilities."
- Why unresolved: Current framework only evaluates content-based clustering; no comparison to structured knowledge representations conducted
- What evidence would resolve it: Comparative study measuring accuracy, retrieval precision, and response quality between clustering-based and knowledge graph-augmented RAG pipelines on TeleQnA benchmark

### Open Question 2
- Question: Can refining the routing module to produce a more focused cluster selection (beyond current top-8 heuristic) consistently improve accuracy without sacrificing retrieval speed?
- Basis in paper: [explicit] "Refining the routing module to output a more focused selection of clusters would likely lead to more efficient and accurate responses."
- Why unresolved: Paper fixed 8 clusters and top-K chunks without systematic hyperparameter optimization or adaptive selection strategies
- What evidence would resolve it: Ablation experiments varying number of selected clusters and evaluating trade-off between accuracy and retrieval latency across multiple query types

### Open Question 3
- Question: Does rigorous preprocessing and cleaning of document chunks before embedding generation significantly enhance retrieval quality and final answer accuracy?
- Basis in paper: [explicit] "Greater emphasis on data preprocessing, particularly cleaning document chunks before converting them to embeddings, could significantly enhance the quality and relevance of the input data."
- Why unresolved: Current pipeline chunks raw 3GPP documents directly without deduplication, noise removal, or semantic boundary alignment
- What evidence would resolve it: Comparing RAG performance with and without chunk-cleaning steps (removing boilerplate, normalizing formatting) on same evaluation set

## Limitations

- Isolated ablation studies for routing, clustering, and query enhancement components are missing, making individual contribution quantification impossible
- Accuracy gains over baseline RAG are not reported, preventing direct performance comparison assessment
- Limited quantitative comparison with prior RAG clustering methods and no qualitative error analysis provided

## Confidence

- Routing module contribution: Medium - limited corpus evidence and lack of hyperparameter sensitivity analysis
- Overall framework efficacy: High - reported performance metrics align with standard SLM fine-tuning practices
- Clustering mechanism: Medium - SSE-based splitting effectiveness depends on document semantic coherence

## Next Checks

1. Implement and run a baseline RAG (no clustering, no query enhancement) on TeleQnA; compare accuracy, retrieval time, and context relevance to the proposed framework
2. Conduct ablation experiments: test each innovation (Bisecting K-Means clustering, query enhancement, routing module) in isolation to quantify individual contribution to accuracy gains
3. Perform a failure case analysis: identify and categorize queries where the framework underperforms or where query enhancement introduces noise, and evaluate router confidence calibration