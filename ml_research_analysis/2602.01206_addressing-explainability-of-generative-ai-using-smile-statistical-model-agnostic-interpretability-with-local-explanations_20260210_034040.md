---
ver: rpa2
title: Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic
  Interpretability with Local Explanations)
arxiv_id: '2602.01206'
source_url: https://arxiv.org/abs/2602.01206
tags:
- image
- arxiv
- explainability
- page
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces gSMILE, a unified framework for explaining
  generative AI models through statistical model-agnostic interpretability. By extending
  SMILE with Wasserstein distance metrics and weighted surrogate modeling, gSMILE
  provides fine-grained, token-level attributions for both text generation and instruction-based
  image editing models.
---

# Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)

## Quick Facts
- arXiv ID: 2602.01206
- Source URL: https://arxiv.org/abs/2602.01206
- Reference count: 0
- Introduces gSMILE, a unified framework for explaining generative AI models through statistical model-agnostic interpretability

## Executive Summary
This thesis introduces gSMILE, a unified framework for explaining generative AI models through statistical model-agnostic interpretability. By extending SMILE with Wasserstein distance metrics and weighted surrogate modeling, gSMILE provides fine-grained, token-level attributions for both text generation and instruction-based image editing models. The framework operates without internal model access, making it suitable for black-box systems. Extensive experiments demonstrate that gSMILE produces robust, human-aligned explanations with strong stability (Jaccard index ~0.85), fidelity (R² up to 0.94), and consistency across diverse generative architectures. These findings highlight gSMILE's potential to advance transparent, trustworthy deployment of generative AI technologies.

## Method Summary
gSMILE extends the SMILE framework to generative models by using Wasserstein distance metrics to quantify how specific prompt elements influence model outputs. The method generates perturbations of input prompts, computes Wasserstein distances between output distributions, and fits a weighted linear surrogate model to derive token-level attributions. The framework is model-agnostic, requiring only black-box access to the target model, and has been validated on both text generation (LLMs) and instruction-based image editing (diffusion models).

## Key Results
- gSMILE achieves high fidelity explanations with R² scores up to 0.94
- The framework demonstrates strong stability with Jaccard index of approximately 0.85
- Explanations show strong consistency across diverse generative architectures including LLMs and diffusion models

## Why This Works (Mechanism)

### Mechanism 1: Output Distributional Shift via Wasserstein Distance
- **Claim:** gSMILE determines token influence by quantifying the distributional shift in the model's output embedding space rather than comparing raw tokens or pixels directly.
- **Mechanism:** The framework computes the Wasserstein distance between the embedding of the original output and the output generated from a perturbed input. A larger distance implies the perturbed token had a higher causal influence on the result.
- **Core assumption:** The embedding models (Word Mover's Distance for text, DINOv2 for images) accurately capture semantic similarity such that distance correlates with semantic change.
- **Evidence anchors:** [abstract] "...Wasserstein distance metrics... to quantify how specific prompt elements influence model outputs." [section 3.1.2] "We quantify the semantic distance between the baseline output distribution... and the distribution for each perturbed prompt."
- **Break condition:** If the black-box model produces highly variable outputs for identical inputs (stochasticity), the distributional shift may reflect noise rather than semantic influence.

### Mechanism 2: Local Approximation via Weighted Surrogate Regression
- **Claim:** A simple linear model can approximate the behavior of a complex generative model within a constrained local region of the input space.
- **Mechanism:** gSMILE fits a weighted linear regression where the independent variables are binary indicators of token presence and the dependent variable is the Wasserstein distance. The resulting coefficients serve as attribution scores.
- **Core assumption:** The relationship between input tokens and the output embedding shift is locally linear (Lipschitz smoothness).
- **Evidence anchors:** [abstract] "...weighted surrogate modelling to quantify..." [section 3.1.4] "The learned coefficients θ quantify the influence of each token... providing intuitive and interpretable explanations."
- **Break condition:** If the generative model exhibits highly non-linear behavior that cannot be approximated locally, the linear surrogate will fail to capture true influence patterns.

### Mechanism 3: Semantic Similarity Kernel Weighting
- **Claim:** Not all perturbations are equally valid; weighting them by their semantic proximity to the original input improves explanation fidelity.
- **Mechanism:** Perturbations that drastically alter the meaning of the original prompt (high Word Mover's Distance) are down-weighted using a Gaussian kernel, ensuring the surrogate model focuses on the neighborhood closest to the original query.
- **Core assumption:** There is a meaningful "locality" in the semantic space where small changes in input should ideally lead to smooth changes in output.
- **Evidence anchors:** [section 3.1.3] "Perturbations that are more semantically similar to the original input are assigned higher importance... via a Gaussian kernel." [abstract] Mentions "controlled input perturbations" implying a filtering/weighting process.

## Foundational Learning

- **Concept:** **Optimal Transport (Wasserstein Distance)**
  - **Why needed here:** This is the core metric gSMILE uses to compare the generated output distributions. Unlike Euclidean distance, it handles the "transport" of probability mass, which is better suited for comparing semantic embeddings where dimensions don't align perfectly.
  - **Quick check question:** Can you explain why measuring the "cost" to move one pile of dirt (distribution) to another location is a better metaphor for semantic similarity than just measuring the height difference at specific points?

- **Concept:** **Model-Agnostic Perturbation (LIME Foundations)**
  - **Why needed here:** gSMILE extends the LIME philosophy. Understanding that the method treats the model as a black box and relies solely on input-output pairs (via perturbations) is essential to understanding its scope and limitations (no gradient access needed).
  - **Quick check question:** If you have a model you cannot open (API only), how would you determine which word in a 10-word prompt is most important using only 20 calls to the API?

- **Concept:** **Embedding Spaces (Text & Vision)**
  - **Why needed here:** The method relies on mapping unstructured outputs (text/images) to vector spaces (via WMD for text, DINOv2 for images) to calculate distances.
  - **Quick check question:** Why is it necessary to convert a generated image into a feature vector (like DINOv2) before calculating the "distance" to another image?

## Architecture Onboarding

- **Component map:** Perturbation Engine -> Black-Box Interface -> Embedding Encoder -> Metric Calculator -> Surrogate Trainer
- **Critical path:** The **Perturbation Generation -> Distance Calculation** loop. This is computationally expensive (O(Nd³) for Wasserstein) and must be optimized.
- **Design tradeoffs:**
  - **Fidelity vs. Speed:** The paper shows Wasserstein distance provides higher fidelity (R²) but is slower than Cosine distance.
  - **Surrogate Simplicity:** Linear regression is interpretable but may struggle with non-linear "reasoning" paths in complex models.
  - **Stability vs. Randomness:** Random perturbations can lead to instability; gSMILE attempts to mitigate this with statistical weighting, but variance remains a factor.
- **Failure signatures:**
  - **Low Fidelity (R² < 0.5):** The surrogate linear model fails to approximate the black-box behavior; likely happens with highly non-linear models or insufficient perturbations.
  - **High Variance in Attributions:** If running the explanation twice yields different heatmaps, the perturbation sampling is too sparse or the model is highly stochastic.
  - **Hallucination Noise:** If the generative model produces irrelevant output for perturbed prompts, the distance metric will capture noise.
- **First 3 experiments:**
  1. **Sanity Check (LLM):** Run gSMILE on a prompt like "What is the meaning of life?" on GPT-3.5/4. Verify that removing "meaning" or "life" results in high attribution scores (Red/High heatmap).
  2. **Visual Validation (Image):** Use Instruct-Pix2Pix with the prompt "Make it snow." Compare the heatmap: does "snow" have the highest weight? Does removing it change the output image significantly (verified by t-SNE cluster separation)?
  3. **Stress Test:** Add a neutral token (e.g., "###") to the end of a prompt. Re-run gSMILE. Check the **Jaccard Index** (Stability) to ensure attribution weights for the original tokens remain consistent (aiming for ≈ 0.85 as per paper results).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantically aware perturbation strategies (e.g., synonym substitution, paraphrasing) reduce semantic distortion and yield more informative attribution patterns than the current binary token removal approach?
- Basis in paper: [explicit] The authors state in Future Works that the current "binary removal–retention (0/1) of tokens" strategy is a limitation and propose exploring "richer perturbations, such as synonym substitution, paraphrasing... or adding Gaussian noise in the embedding space."
- Why unresolved: The current method can distort sentence meaning or violate syntax, potentially causing the model to operate outside its natural range and degrading explanation stability.
- What evidence would resolve it: A comparative study measuring stability and fidelity metrics (like WMSE and R²) between binary removal and context-preserving perturbation methods on the same LLMs and diffusion models.

### Open Question 2
- Question: Can faster optimal transport variants, such as Sinkhorn divergence, replace Wasserstein distance to reduce computational overhead while preserving explanation fidelity?
- Basis in paper: [explicit] The paper identifies "substantial computational requirements" as a key limitation due to the high complexity ($O(Nd^3)$) of Wasserstein distance. The authors explicitly call for investigating "faster optimal transport variants such as Sinkhorn divergence" in future work.
- Why unresolved: While gSMILE provides robust explanations, the cost of generating large perturbation sets and calculating Wasserstein distances limits accessibility for users without high-end hardware.
- What evidence would resolve it: Benchmarking the execution time of gSMILE using Sinkhorn divergence against the standard Wasserstein implementation, while ensuring that fidelity and accuracy metrics (like ATT AUROC) remain statistically equivalent.

### Open Question 3
- Question: Do more powerful, non-linear surrogate models (e.g., kernel regressors or neural local approximators) provide higher fidelity explanations for complex diffusion models compared to weighted linear regression?
- Basis in paper: [explicit] The authors note in Future Works that the current surrogate models (linear regression and Bayesian ridge) may be "insufficient for highly non-linear generative systems such as diffusion models" and suggest utilizing "kernel regressors, neural local approximators, or tree-based surrogates."
- Why unresolved: Highly non-linear generative architectures may have complex decision boundaries that linear surrogates oversimplify, potentially leading to a loss of fidelity.
- What evidence would resolve it: Experiments fitting non-linear surrogate models to the perturbation data of diffusion models (like Instruct-Pix2Pix) and comparing the fidelity metrics (R²) against the current linear baselines.

### Open Question 4
- Question: How does gSMILE perform when applied to temporal or higher-order multimodal generative systems, such as video generation or audio synthesis?
- Basis in paper: [explicit] The thesis focuses on text and static images. However, the authors explicitly list "Extension to additional generative modalities" as a future direction, stating gSMILE should be applied to "video generation, audio synthesis, and broader multimodal architectures" to evaluate interpretability under temporal constraints.
- Why unresolved: The current metrics and methodologies are validated for static outputs; temporal consistency and cross-modal alignment in video/audio generation introduce new interpretability challenges not yet addressed.
- What evidence would resolve it: Adapting the gSMILE pipeline to a video diffusion model to determine if the framework can effectively evaluate "temporal consistency" alongside current metrics like stability and accuracy.

## Limitations
- **Black-Box Assumption Strength**: Cannot access internal attention patterns or gradient information that might provide more granular explanations
- **Computational Cost**: Requires generating multiple outputs and computing Wasserstein distances, scaling poorly with embedding dimension
- **Locality Assumption**: Linear surrogate may fail to capture true influence patterns for models with complex non-linear behavior

## Confidence

**High Confidence Claims** (R² > 0.85, Jaccard > 0.8, validated across multiple model types):
- gSMILE provides more stable attributions than baseline methods (LIME) across different seeds
- The framework successfully extends SMILE's statistical approach to generative models using Wasserstein distance
- Token-level attributions from gSMILE align with human intuition for simple, well-defined tasks

**Medium Confidence Claims** (R² 0.7-0.85, some variance across model types):
- gSMILE provides higher fidelity explanations than Cosine distance baselines for generative models
- The framework's explanations are robust to random perturbations and maintain consistency across runs
- Attributions generalize across different generative architectures (LLMs, diffusion models)

**Low Confidence Claims** (Limited validation, single model type, or edge cases):
- gSMILE's explanations remain interpretable for highly abstract or creative generation tasks where human alignment is subjective
- The framework scales efficiently to production-grade models with billions of parameters
- Wasserstein distance is universally superior to all other distributional distance metrics for all generative model types

## Next Checks

**Validation Check 1: Stress Test with Stochasticity**
- **Goal**: Assess framework robustness to model stochasticity
- **Method**: Run gSMILE on a highly stochastic diffusion model (e.g., Stable Diffusion with high CFG scale) with 10 different random seeds. Calculate the mean and standard deviation of Jaccard stability across seeds. Compare against deterministic baselines.
- **Expected Outcome**: Jaccard index should remain above 0.75 on average, with variance below 0.1. If variance exceeds this, investigate averaging strategies or deterministic model variants.

**Validation Check 2: Ablation Study on Distance Metrics**
- **Goal**: Validate the necessity of Wasserstein distance over simpler metrics
- **Method**: Implement gSMILE with Euclidean and Cosine distances instead of Wasserstein. Run on the same LLM and diffusion model pairs from Tables 4.4 and 5.6. Compare R² fidelity scores. Use statistical tests (e.g., paired t-test) to determine if differences are significant.
- **Expected Outcome**: Wasserstein should outperform baselines by at least 10% in R². If not, investigate embedding space alignment or consider alternative optimal transport implementations.

**Validation Check 3: Scalability Benchmark**
- **Goal**: Assess computational feasibility for real-world deployment
- **Method**: Measure runtime of gSMILE on increasing input lengths (10, 50, 100 tokens for text; 64x64, 256x256, 1024x1024 pixels for images). Track memory usage and total explanation time. Compare against LIME runtime from Table 4.9.
- **Expected Outcome**: Runtime should scale linearly or better with input size. If runtime exceeds 30 seconds for 100-token prompts or 512x512 images, investigate dimensionality reduction or approximation techniques for Wasserstein distance.