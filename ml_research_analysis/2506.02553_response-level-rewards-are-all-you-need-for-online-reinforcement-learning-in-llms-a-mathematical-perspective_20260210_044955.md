---
ver: rpa2
title: 'Response-Level Rewards Are All You Need for Online Reinforcement Learning
  in LLMs: A Mathematical Perspective'
arxiv_id: '2506.02553'
source_url: https://arxiv.org/abs/2506.02553
tags:
- reward
- policy
- arxiv
- response-level
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a fundamental challenge in LLM alignment:
  the Zero-Reward Assumption, where only the final token in a response receives a
  reward while intermediate tokens receive zero rewards. The authors introduce the
  Trajectory Policy Gradient Theorem, proving that policy gradients can be unbiasedly
  estimated using only response-level rewards without requiring token-level rewards.'
---

# Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective

## Quick Facts
- **arXiv ID:** 2506.02553
- **Source URL:** https://arxiv.org/abs/2506.02553
- **Reference count:** 40
- **Primary result:** Proves that policy gradients can be unbiasedly estimated using only response-level rewards without requiring token-level rewards.

## Executive Summary
This paper addresses a fundamental challenge in LLM alignment: the Zero-Reward Assumption, where only the final token in a response receives a reward while intermediate tokens receive zero rewards. The authors introduce the Trajectory Policy Gradient Theorem, proving that policy gradients can be unbiasedly estimated using only response-level rewards without requiring token-level rewards. This theoretical result unifies various RL methods (PPO, GRPO, ReMax, RLOO) and shows they inherently model token-level information despite using response-level rewards. Based on this theorem, the authors propose Token-Reinforced Policy Optimization (TRePO), a simpler alternative to PPO that matches GRPO in memory efficiency. TRePO provides a more theoretically grounded approach to token-level modeling by sampling trajectories at intermediate steps to estimate token-level contributions.

## Method Summary
The paper introduces Token-Reinforced Policy Optimization (TRePO), which operationalizes the Trajectory Policy Gradient Theorem by explicitly sampling trajectories from intermediate steps to estimate value expectations. TRePO replaces the critic network used in PPO with Monte Carlo sampling, trading model complexity for sampling complexity. The algorithm samples full trajectories from the policy, selects a subset of time steps, and for each selected step generates multiple completions to estimate expected future rewards. This enables critic-free, token-level RL while maintaining memory efficiency comparable to GRPO. The method provides a theoretically grounded approach to token-level modeling using only response-level rewards.

## Key Results
- Proves the Trajectory Policy Gradient Theorem showing policy gradients can be unbiasedly estimated using only response-level rewards
- Demonstrates that widely used methods like PPO, GRPO, ReMax, and RLOO inherently model token-level rewards despite using response-level rewards
- Proposes TRePO as a simpler alternative to PPO that matches GRPO in memory efficiency while providing theoretical grounding for token-level modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Policy gradients based on true, unknown token-level rewards can be unbiasedly estimated using only a response-level reward model.
- **Mechanism:** The **Trajectory Policy Gradient Theorem** (Theorem 1) demonstrates that by introducing a state-dependent constant to the action-value function $Q$ at each time step, the policy gradient formula transforms into an expectation over future trajectories sampled from the current policy, evaluated by the response-level reward. This mathematical reformulation shows that the summation of response-level rewards over sampled trajectories is a sufficient statistic for the gradient direction.
- **Core assumption:** The response-level reward is a valid cumulative sum of discounted true token rewards (standard RL return definition).
- **Evidence anchors:**
  - [abstract] "...proves that policy gradients can be unbiasedly estimated using only response-level rewards without requiring token-level rewards."
  - [section 3.1, Theorem 1] The proof shows $\nabla J(\theta) = E_W[\sum E_{W^{(t+1)}}[R_M(W^{(t+1)})] \nabla \log \pi_\theta(w_t|W_{0,t-1})]$.
- **Break condition:** If the response-level reward model $R_M(W)$ is biased or correlates poorly with the true trajectory return, the gradient estimate will also be biased.

### Mechanism 2
- **Claim:** Standard RL algorithms (PPO, GRPO, ReMax, RLOO) inherently perform token-level modeling even when operating under the Zero-Reward Assumption.
- **Mechanism:** The paper classifies these algorithms as instances of REINFORCE or Actor-Critic families. According to the Trajectory Policy Gradient Theorem, the update rules used by these methods (e.g., PPO's advantage function, GRPO's group-normalized rewards) are mathematically valid estimators of the true token-level policy gradient. They implicitly approximate the expectation terms in Theorem 1 using single samples (REINFORCE) or learned functions (Critic).
- **Core assumption:** The implementation details (like PPO's GAE or GRPO's group baseline) serve as valid variance reduction techniques for the underlying estimator.
- **Evidence anchors:**
  - [abstract] "...reveals that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess the capacity to model token-level reward signals..."
  - [section 3.1, Corollary 1] "REINFORCE-based RL algorithms... have the capability of token-level modeling."
- **Break condition:** If the variance reduction methods (baselines/critics) are severely mis-specified or under-trained, practical convergence may fail despite theoretical unbiasedness.

### Mechanism 3
- **Claim:** TRePO (Token-Reinforced Policy Optimization) enables critic-free, token-level RL by explicitly sampling to estimate value expectations.
- **Mechanism:** TRePO operationalizes Theorem 2 by explicitly estimating the baseline term $E_{W^{(t)}}[R_M(W^{(t)})]$ (expected reward from state $t$) and the action-value term via Monte Carlo sampling of trajectory suffixes from intermediate steps. This removes the need to train a separate Critic network to approximate $V(s)$, trading model complexity (memory) for sampling complexity (compute).
- **Core assumption:** Sufficient sampling from intermediate states is feasible and provides a lower-variance estimate than a learned critic would, or the memory savings outweigh the compute cost.
- **Evidence anchors:**
  - [abstract] "...propose Token-Reinforced Policy Optimization (TRePO), a simpler alternative to PPO that matches GRPO in memory efficiency."
  - [section 3.2] Algorithm 1 describes the explicit sampling procedure (lines 3-4) to calculate advantage.
- **Break condition:** If the sampling budget is too low to accurately estimate $E_{W^{(t)}}[R_M(W^{(t)})]$, the "advantage" estimate will be noisy, leading to unstable training.

## Foundational Learning

- **Concept:** **Policy Gradient Theorem**
  - **Why needed here:** This paper extends the standard theorem. You must understand how $\nabla J(\theta)$ relates to $\log \pi(a|s)$ and $Q(s,a)$ to grasp why adding a state-dependent constant doesn't change the expectation.
  - **Quick check question:** Why does subtracting a baseline $b(s)$ from the return not introduce bias into the policy gradient?

- **Concept:** **The Zero-Reward Assumption**
  - **Why needed here:** The paper explicitly challenges the intuition that "only the last token matters." Understanding this assumption is key to appreciating why the theorem (which says response-level rewards *are* sufficient) is counter-intuitive but practically significant.
  - **Quick check question:** In a standard RLHF setup for math reasoning, why is the Zero-Reward Assumption typically made? (Hint: data availability).

- **Concept:** **Actor-Critic Architecture**
  - **Why needed here:** The paper contrasts TRePO with PPO. Understanding the role of the Critic (estimating $V(s)$ for variance reduction) highlights why removing it (as in TRePO/GRPO) saves memory but requires alternative estimation strategies (sampling).
  - **Quick check question:** What is the primary function of the Critic network in PPO?

## Architecture Onboarding

- **Component map:**
  - Policy Model ($\pi_\theta$) -> Reward Model ($R_M$) -> Sampler (TRePO specific) -> Baseline Estimator (TRePO specific) -> Update Mechanism

- **Critical path:**
  1.  **Forward Pass:** Generate a full trajectory $W$ for a prompt.
  2.  **Sampling:** For selected steps $t \in D$, generate multiple suffixes to estimate the expected future reward.
  3.  **Advantage Calc:** Compute $A_t = R_M(W) - E_{W^{(t)}}[R_M(W^{(t)})]$ (simplified view per Theorem 2).
  4.  **Update:** Apply policy gradient update using clipped surrogate loss (similar to PPO).

- **Design tradeoffs:**
  - **Memory vs. Compute:** TRePO eliminates the Critic model (saving ~50% memory on model weights) but introduces a sampling overhead at inference time (increasing compute/latency per training step).
  - **Variance:** A well-trained Critic (PPO) usually provides lower variance estimates than Monte Carlo sampling (TRePO), but TRePO avoids "self-referential bias" from a poorly initialized Critic.

- **Failure signatures:**
  - **High Gradient Variance:** If the sampling budget ($M$) is too low or the policy is stochastic, the advantage estimate will be noisy.
  - **Slow Convergence:** Excessive sampling makes each step slow; insufficient sampling makes learning unstable.
  - **Reward Hacking:** As with any RL method, if $R_M$ is flawed, the policy will exploit it.

- **First 3 experiments:**
  1.  **Toy Validation (Bias Check):** Implement TRePO on a simple environment where exact token rewards are known. Verify that the average gradient matches the exact gradient to confirm unbiasedness.
  2.  **Ablation on Sampling ($M$):** Test TRePO with varying numbers of samples ($M=1, 4, 16$) to plot the curve of training stability vs. compute cost.
  3.  **Benchmark vs. GRPO/PPO:** Run TRePO on a standard summarization or math reasoning task (e.g., GSM8K) to compare win-rate against GRPO (memory match) and PPO (performance target).

## Open Questions the Paper Calls Out

- **Open Question 1:** Does TRePO empirically outperform PPO, GRPO, ReMax, RLOO, and DPO across diverse LLM tasks including mathematical reasoning, multi-turn medical dialogues, and multi-agent RL?
- **Open Question 2:** Do improvements from token-level reward methods (RTO, ABC, PRIME) stem primarily from exact token position information or from providing a stronger aggregate response-level reward signal?
- **Open Question 3:** Can the theoretical advantages of PPO and TRePO (smaller approximation deviation, smoother policy gradients) translate into practical performance gains when implementation requirements are met?
- **Open Question 4:** How efficiently can TRePO's intermediate-step trajectory sampling be implemented using approximation techniques like PagedAttention, quantization, and temperature-adjusted sampling?

## Limitations

- Heavy reliance on the assumption that response-level reward models perfectly capture true cumulative token rewards, which may not hold in practice
- Computational overhead of TRePO's sampling mechanism could be prohibitively expensive for long sequences or high-throughput training
- Lack of empirical validation on standard benchmarks to demonstrate practical effectiveness
- Potential for overfitting to reward model idiosyncrasies when advantage estimation is directly tied to response-level rewards

## Confidence

- **High Confidence:** The Trajectory Policy Gradient Theorem is mathematically sound and correctly extends the standard Policy Gradient Theorem.
- **Medium Confidence:** The claim that TRePO matches GRPO in memory efficiency is plausible based on the elimination of the critic, but requires empirical validation.
- **Low Confidence:** The practical effectiveness of TRePO compared to PPO/GRPO is not empirically demonstrated.

## Next Checks

1. **Empirical Unbiasedness Test:** Implement TRePO on a synthetic environment (e.g., a gridworld or simple language task) where exact token-level rewards are known. Compare the average policy gradient estimated by TRePO against the exact gradient to verify unbiasedness in practice.

2. **Memory vs. Compute Trade-off Analysis:** Measure the memory savings and computational overhead of TRePO versus PPO/GRPO on a standard LLM (e.g., LLaMA-7B) for a fixed task. Profile GPU memory usage, wall-clock time per step, and convergence speed to quantify the trade-off.

3. **Benchmark Performance Comparison:** Evaluate TRePO, PPO, and GRPO on a standard LLM alignment task (e.g., GSM8K math reasoning or CNN/DailyMail summarization). Report win-rate, reward, and perplexity metrics to assess whether TRePO's theoretical advantages translate to practical gains.