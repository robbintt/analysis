---
ver: rpa2
title: DPO Learning with LLMs-Judge Signal for Computer Use Agents
arxiv_id: '2506.03095'
source_url: https://arxiv.org/abs/2506.03095
tags:
- agents
- arxiv
- preference
- these
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training privacy-preserving
  computer use agents (CUAs) that can operate on local hardware without requiring
  cloud-based vision-language models. The authors propose an LLM-as-Judge framework
  that uses GPT-4 to automatically evaluate and rank synthetic interaction trajectories
  generated by a lightweight vision-language model (UI-TARS-2B), creating preference-labeled
  training data without human annotation.
---

# DPO Learning with LLMs-Judge Signal for Computer Use Agents

## Quick Facts
- **arXiv ID**: 2506.03095
- **Source URL**: https://arxiv.org/abs/2506.03095
- **Reference count**: 27
- **Primary result**: DPO-trained models achieve 35.03% improvement on OS-World benchmark's 15-step setting using GPT-4 as judge for synthetic trajectory ranking

## Executive Summary
This paper addresses the challenge of training privacy-preserving computer use agents (CUAs) that can operate on local hardware without requiring cloud-based vision-language models. The authors propose an LLM-as-Judge framework that uses GPT-4 to automatically evaluate and rank synthetic interaction trajectories generated by a lightweight vision-language model (UI-TARS-2B), creating preference-labeled training data without human annotation. The ranked data is then used to fine-tune the model using Direct Preference Optimization (DPO). Experiments on the OS-World benchmark show that DPO-trained models consistently outperform the baseline, achieving a weighted average improvement of 35.03% (DPO-3) in the 15-step setting and 2.04% (DPO-1) in the 50-step setting. The 15-step DPO models even outperform the baseline in the 50-step setting, demonstrating greater efficiency and reliability. The authors also conduct qualitative analysis of failure cases, identifying three main categories: invalid coordination, wrong coordination, and repeat wrong action.

## Method Summary
The authors present a privacy-preserving approach to training computer use agents by leveraging GPT-4 as an automated judge to rank synthetic interaction trajectories. The process begins with generating synthetic trajectories using a lightweight vision-language model (UI-TARS-2B) on the OS-World benchmark. GPT-4 then evaluates and ranks these trajectories based on task completion quality, creating preference-labeled data without human annotation. This preference data is used to fine-tune the model using Direct Preference Optimization (DPO), which optimizes the policy to prefer higher-ranked trajectories. The approach enables local deployment of CUAs while maintaining performance, as the computationally expensive judgment process is offloaded to GPT-4 but the final model runs entirely on local hardware. The method addresses the privacy concerns associated with cloud-based vision-language models while avoiding the scalability challenges of human annotation.

## Key Results
- DPO-trained models achieve 35.03% weighted average improvement over baseline on OS-World 15-step setting
- DPO-1 model shows 2.04% improvement in 50-step setting, with DPO-3 model matching or exceeding baseline performance
- 15-step DPO models outperform baseline in 50-step setting, demonstrating greater efficiency and reliability
- Qualitative analysis identifies three failure modes: invalid coordination, wrong coordination, and repeat wrong action

## Why This Works (Mechanism)
The approach works by creating a scalable pipeline for preference learning without human annotation. GPT-4 serves as an effective judge because it can evaluate task completion quality based on visual and textual understanding, providing consistent and nuanced rankings of synthetic trajectories. The DPO algorithm then learns to optimize the policy toward trajectories that GPT-4 ranks higher, effectively distilling GPT-4's judgment capabilities into a model that can run locally. This creates a feedback loop where synthetic data generation, automated evaluation, and preference optimization work together to improve performance without the privacy risks of cloud-based models or the cost of human annotation.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: Why needed - Optimizes policies based on preference data without requiring complex reinforcement learning setups; Quick check - Verify the KL divergence constraint between policy updates
- **LLM-as-Judge Framework**: Why needed - Provides scalable, automated evaluation of task completion quality without human annotation; Quick check - Assess judge consistency across similar trajectories
- **Synthetic Data Generation**: Why needed - Creates diverse training examples without privacy risks of real user data; Quick check - Measure trajectory diversity and coverage of task space
- **Vision-Language Model Coordination**: Why needed - Enables interpretation of visual desktop states for action selection; Quick check - Validate action accuracy on unseen desktop configurations
- **OS-World Benchmark**: Why needed - Provides standardized evaluation environment for computer use agents; Quick check - Confirm benchmark task diversity and difficulty progression
- **Privacy-Preserving Local Deployment**: Why needed - Addresses data protection concerns in enterprise and personal computing contexts; Quick check - Verify model inference runs without external API calls

## Architecture Onboarding

**Component Map**
UI-TARS-2B (synthetic data generation) -> GPT-4 (trajectory ranking) -> DPO fine-tuning -> Local CUA deployment

**Critical Path**
1. Generate synthetic trajectories on OS-World tasks using UI-TARS-2B
2. Submit trajectories to GPT-4 for preference ranking
3. Apply DPO algorithm to update model weights based on ranked preferences
4. Deploy fine-tuned model for local computer use agent operation

**Design Tradeoffs**
- Using GPT-4 for judgment provides high-quality rankings but introduces ongoing costs and potential biases
- Synthetic data generation enables privacy but may not capture all real-world edge cases
- DPO simplifies preference learning compared to RL but requires careful hyperparameter tuning
- Local deployment ensures privacy but may limit model size and capability compared to cloud alternatives

**Failure Signatures**
- Performance degradation when tasks involve visual elements that GPT-4 struggles to interpret
- Overfitting to synthetic data patterns that don't generalize to real-world desktop environments
- Inconsistent trajectory rankings from GPT-4 leading to conflicting preference signals
- Failure to handle novel UI layouts or application states not present in training data

**First 3 Experiments**
1. Compare DPO performance against baseline RL and supervised learning approaches on OS-World benchmark
2. Evaluate model performance on held-out OS-World tasks not used in training to test generalization
3. Test model robustness by introducing variations in desktop environments and application versions

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on GPT-4 for preference ranking raises scalability and cost concerns for production deployment
- Evaluation restricted to OS-World benchmark limits generalizability to real-world computer use scenarios
- Performance improvements show diminishing returns in 50-step setting, suggesting limitations for complex tasks
- Single judge model creates single point of failure if GPT-4's judgments are suboptimal for certain task types

## Confidence

**High confidence**: The technical methodology of using LLM-as-Judge with DPO is sound and well-implemented. The experimental results showing improvements over the baseline are reproducible and clearly demonstrated.

**Medium confidence**: The claim that this approach achieves privacy-preserving computer use without sacrificing performance. While the results are positive, the evaluation scope is limited, and real-world deployment challenges are not fully addressed.

**Low confidence**: The scalability and cost-effectiveness of the approach for production deployment, given the ongoing dependency on GPT-4 for judgment.

## Next Checks
1. Test the DPO-trained models on additional computer use benchmarks beyond OS-World to assess generalizability across different operating systems and application types
2. Evaluate the approach using multiple judge models (including smaller, more cost-effective LLMs) to determine if GPT-4's performance can be matched or approximated while reducing computational costs
3. Conduct a longitudinal study measuring performance degradation over extended use periods, particularly focusing on whether the preference-ranking mechanism maintains effectiveness as task distributions shift in real-world deployment scenarios