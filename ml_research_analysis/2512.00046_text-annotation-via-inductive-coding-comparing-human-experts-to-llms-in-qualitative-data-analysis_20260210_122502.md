---
ver: rpa2
title: 'Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative
  Data Analysis'
arxiv_id: '2512.00046'
source_url: https://arxiv.org/abs/2512.00046
tags:
- coding
- sentence
- coders
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigated automating inductive coding in qualitative
  data analysis by comparing six open-source LLMs with human experts. The core method
  involved fine-tuning models on a dataset of 1,000 code-quote pairs, augmented with
  SemEval-2014 reviews, and evaluating performance using BERTScore and ROUGE metrics.
---

# Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis

## Quick Facts
- arXiv ID: 2512.00046
- Source URL: https://arxiv.org/abs/2512.00046
- Reference count: 6
- LLMs plateaued in performance after 100 examples and generally produced labels closer to the golden standard than human coders

## Executive Summary
This study compares open-source LLMs with human experts for automating inductive coding in qualitative data analysis. The research evaluates six fine-tuned models on a dataset of 1,000 code-quote pairs, augmented with SemEval-2014 reviews, using BERTScore and ROUGE metrics. Results demonstrate that LLMs achieve reasonable performance with minimal training data and often produce more consistent labels aligned with the golden standard than human coders, particularly for simple sentences. However, human experts excel at interpreting complex, abstract cases. The study reveals that while LLMs can effectively automate straightforward coding tasks, human expertise remains essential for nuanced interpretation in qualitative analysis.

## Method Summary
The research employed a comparative approach between six open-source LLMs and human experts for inductive coding tasks. Models were fine-tuned on 1,000 code-quote pairs combined with SemEval-2014 review data to address data scarcity. Performance evaluation utilized BERTScore and ROUGE metrics to compare LLM outputs against human-generated labels. The study tested model performance across varying training dataset sizes (from 10 to 1,000 examples) to identify optimal training requirements. Human coders participated in the evaluation process, though they were aware of the comparison to machine outputs, which may have influenced their coding behavior.

## Key Results
- LLMs plateaued in performance after 100 examples, indicating sufficient fine-tuning with minimal data
- LLMs generally produced labels closer to the golden standard than human coders, especially for simple sentences
- Human coders excelled at complex, abstract cases but tended to over-interpret and add unnecessary complexity
- Krippendorff's alpha was low (0.2) among human coders, reflecting subjective variability in qualitative coding

## Why This Works (Mechanism)
The success of LLMs in automating inductive coding stems from their ability to identify patterns in labeled data and apply consistent categorization rules. Unlike human coders who may introduce subjective interpretation or over-analyze complex cases, LLMs apply learned patterns uniformly across similar inputs. The models benefit from transfer learning capabilities, allowing them to leverage knowledge from pre-training on large text corpora. For simple sentences with clear patterns, LLMs can apply learned categorizations more consistently than humans, who may vary in their interpretation of abstract concepts or add complexity beyond the actual content.

## Foundational Learning
- Inductive coding methodology - why needed: Provides systematic approach to deriving themes from qualitative data; quick check: Understanding difference between inductive and deductive coding approaches
- Krippendorff's alpha - why needed: Measures inter-rater reliability in qualitative coding; quick check: Ability to interpret reliability coefficients and their implications
- BERTScore and ROUGE metrics - why needed: Evaluate text similarity for model performance assessment; quick check: Understanding metric limitations for qualitative analysis
- Fine-tuning process - why needed: Adapts pre-trained models to specific coding tasks; quick check: Knowledge of few-shot learning capabilities
- Qualitative data analysis principles - why needed: Context for understanding coding challenges; quick check: Familiarity with thematic analysis concepts

## Architecture Onboarding

**Component Map:** Data Preprocessing -> Fine-tuning Pipeline -> Model Evaluation -> Human Expert Comparison

**Critical Path:** The critical path involves preparing the training dataset (combining code-quote pairs with review data), fine-tuning models on varying dataset sizes, evaluating performance using BERTScore/ROUGE, and comparing results with human expert coders.

**Design Tradeoffs:** The study prioritized model accessibility by using open-source LLMs over proprietary alternatives, accepting potential performance limitations. Data augmentation with review datasets addressed scarcity but may introduce domain bias. Using established metrics (BERTScore/ROUGE) provided standardization but may not capture qualitative coding nuances.

**Failure Signatures:** Poor performance on complex, abstract sentences indicates limitations in handling nuanced interpretation. Low Krippendorff's alpha suggests high subjectivity in human coding standards. Inconsistent results across training dataset sizes may reveal sensitivity to training data quality.

**Three First Experiments:**
1. Test model performance with only 10-50 training examples to verify minimal data requirements
2. Evaluate model consistency by coding the same sentence multiple times with different random seeds
3. Compare LLM performance on simple versus complex sentence structures to identify capability boundaries

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- BERTScore and ROUGE metrics were designed for summarization/translation, not qualitative coding, limiting evaluation accuracy
- Low Krippendorff's alpha (0.2) indicates high inter-rater variability and subjective instability in human-coded labels
- Models were evaluated on the same dataset used for fine-tuning without clear validation/test set separation, risking overfitting

## Confidence

**High confidence:** LLMs can achieve reasonable performance with minimal training data (100 examples)

**Medium confidence:** LLMs outperform humans on simple sentences but underperform on complex cases

**Low confidence:** The superiority of LLM-generated labels over human labels when evaluated by metrics

## Next Checks

1. Conduct a blind re-evaluation where expert coders assess LLM outputs without knowing their origin, and compare this to their evaluation of human-coded data

2. Test model performance across multiple qualitative domains (not just social media and reviews) using a held-out test set distinct from the training data

3. Implement qualitative coding-specific evaluation metrics, such as thematic coherence scoring or semantic similarity measures validated by the qualitative research community