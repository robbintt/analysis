---
ver: rpa2
title: Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy
arxiv_id: '2507.02493'
source_url: https://arxiv.org/abs/2507.02493
tags:
- polyp
- learning
- temporal
- clustering
- counting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles automated polyp counting in colonoscopy videos,
  a critical step for improving procedure reporting and quality control. The authors
  propose a temporally-aware supervised contrastive learning approach that leverages
  both visual and temporal information to enhance tracklet representation and clustering.
---

# Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy

## Quick Facts
- arXiv ID: 2507.02493
- Source URL: https://arxiv.org/abs/2507.02493
- Reference count: 31
- Automated polyp counting achieves 2.2x reduction in fragmentation rate compared to prior approaches

## Executive Summary
This paper tackles automated polyp counting in colonoscopy videos by proposing a temporally-aware supervised contrastive learning approach. The method learns to cluster tracklets (video segments) belonging to the same polyp entity using both visual and temporal information. Unlike previous self-supervised methods, it uses supervised contrastive loss with temporally-weighted soft targets to capture intra-polyp variability while preserving inter-polyp discriminability. Evaluated on publicly available datasets, the approach establishes a new state-of-the-art in polyp counting with a 2.2x reduction in fragmentation rate.

## Method Summary
The method processes colonoscopy videos by first detecting polyps and forming tracklets (sequences of consecutive detections of the same polyp). It then encodes each frame using a ResNet-50 backbone pretrained on ImageNet, aggregates frame information with a Transformer, and projects to a 128-dimensional embedding. The key innovation is a temporally-aware supervised contrastive loss that weights positive samples by temporal proximity, encouraging embeddings of the same polyp (even when visually dissimilar due to motion or lighting changes) to be close in feature space. During clustering, the method combines visual similarity with temporal adjacency constraints to reduce false associations between visually similar but temporally distant tracklets.

## Key Results
- Achieves 2.2x reduction in fragmentation rate (FR=2.32 vs 4.18-5.12 for prior methods)
- Improves false positive rate by 5x (FPR=0.15 vs 0.77-0.93 for prior methods)
- Outperforms both self-supervised and supervised contrastive learning baselines in ablation studies
- Maintains strong performance across multiple datasets (REAL-Colon, LDPolyp, SUN, PolypSet)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Temporally-weighted soft targets in supervised contrastive loss improve intra-polyp invariance while maintaining inter-polyp separability.
- **Mechanism:** The ground-truth distribution p_i weights matching candidates by temporal proximity using exponential decay: exp(-λd_i), where d_i is normalized temporal distance. This forces temporally adjacent but visually dissimilar tracklets to be treated as similar in embedding space, while distant matches are suppressed.
- **Core assumption:** Temporal proximity in colonoscopy video correlates with same-polyp identity, even when visual appearance differs due to motion blur, lighting changes, or occlusions.
- **Evidence anchors:** [abstract] "supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability"; [Section 2.1, Eq. 2] Ground-truth distribution formula with temporal weighting; [corpus] Weak/no direct corpus validation of temporal soft targets specifically for polyp counting.
- **Break condition:** If temporal proximity does not correlate with same-polyp identity (e.g., repeated views of same polyp after long temporal gap, or different polyps appearing in quick succession), the weighting scheme could introduce label noise.

### Mechanism 2
- **Claim:** Using multiple positive views per polyp (14 views, 3 polyps per batch) improves representation quality compared to traditional 2-view contrastive learning.
- **Mechanism:** Supervised contrastive loss allows multiple positives per class, explicitly modeling inter-sample relationships. More views provide denser coverage of intra-polyp appearance variability without requiring curriculum learning or hard mining strategies.
- **Core assumption:** Sufficient appearance variability exists within each polyp entity that additional views provide non-redundant signal.
- **Evidence anchors:** [Section 1] "using multiple positive samples reduces the reliance on complex hard positive or negative mining strategies"; [Table 2] Ablation showing FR=2.32 with 14 views/polyp vs FR=2.57-4.54 with fewer views; [corpus] No corpus papers directly compare multi-view supervised contrastive learning for polyps.
- **Break condition:** If views are highly redundant or batch diversity is insufficient, marginal gains diminish; Table 2 shows 28 views/polyp (2 polyps/batch) degrades performance (FR=4.54).

### Mechanism 3
- **Claim:** Temporal adjacency constraint during clustering reduces false positive re-associations between visually similar but temporally distant tracklets.
- **Mechanism:** Final similarity matrix S = α·V + (1-α)·T combines visual similarity V with temporal adjacency T (exponential decay with γ penalty factor). Temporal penalty discourages clustering tracklets with large frame gaps.
- **Core assumption:** Same-polyp tracklets from temporally distant video segments are rare; most re-associations should occur within reasonable temporal windows.
- **Evidence anchors:** [abstract] "temporal adjacency constraint in clustering to reduce false associations between visually similar but temporally distant tracklets"; [Table 3, bottom] Temporal-AP clustering with temporally-aware loss achieves FR=2.32 vs FR=4.18 with standard AP; [Section 3.4] "α, which balances the influence of the two modalities, averages 0.4833 ± 0.1999"; [corpus] No corpus papers explicitly integrate temporal penalties in polyp clustering.
- **Break condition:** If polyps legitimately reappear after long temporal gaps (e.g., scope withdrawal revisiting same region), temporal penalty could prevent valid re-associations.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - **Why needed here:** Standard self-supervised contrastive learning (SimCLR) treats all views of same instance as positive and all others as negative, but doesn't leverage class labels. SupCon allows multiple positives per class, improving intra-class compactness.
  - **Quick check question:** Can you explain why SupCon's loss function handles multiple positives differently than SimCLR's InfoNCE loss?

- **Concept: Temporal Coherence in Video Representation Learning**
  - **Why needed here:** The paper hypothesizes that temporally close frames likely belong to same entity. Understanding spatiotemporal contrastive learning (e.g., cvRL, video SSL) provides context for why temporal weighting might help.
  - **Quick check question:** Why might temporal proximity be a useful inductive bias for video understanding but potentially harmful for static image retrieval?

- **Concept: Affinity Propagation Clustering**
  - **Why needed here:** The clustering module uses Affinity Propagation rather than threshold-based or k-means approaches. Understanding message-passing clustering and preference parameter selection is critical.
  - **Quick check question:** What is the role of the "preference" parameter in Affinity Propagation, and how does it affect the number of clusters?

## Architecture Onboarding

- **Component map:** Input Video → Polyp Detection (external) → Tracklet Formation → Frame Encoder (ResNet-50, ImageNet pretrained) → Transformer Encoder (frame dependencies) → Projection Head (128-dim embedding) → Clustering Module → Clusters → Polyp Count

- **Critical path:**
  1. Tracklet preprocessing (IoU≥0.1, frame subsampling 1/4)
  2. Fragment creation (κ=8 frames/fragment)
  3. Bounding box crop with context (ψ=5x diagonal)
  4. Forward pass through encoder → 128-dim embedding
  5. Compute pairwise similarity matrices
  6. Grid search for optimal α, γ, preference parameters
  7. Affinity Propagation clustering → polyp count = |clusters|

- **Design tradeoffs:**
  - **Fragment length κ=8:** Shorter fragments capture finer temporal dynamics but increase compute; longer fragments may miss short-term variations
  - **Batch composition (14 views × 3 polyps):** More views per polyp improves learning but reduces batch diversity; optimal at 14 views in this setting
  - **α≈0.5 weighting:** Near-equal balance between visual and temporal cues; ablation shows both contribute significantly
  - **Ground-truth tracklets:** Paper uses GT detections to avoid tracker noise; production deployment would need robust tracking

- **Failure signatures:**
  - High fragmentation rate (FR >> 1): Tracklet embeddings insufficiently invariant; check λ tuning, increase training diversity
  - High false positive rate: Temporal penalty too weak or visual similarity alone creating spurious matches; increase γ
  - Temporal constraint too aggressive: Same polyp appearing in non-adjacent segments incorrectly split; reduce γ or increase α
  - Training instability: Check temperature τ, batch composition, learning rate

- **First 3 experiments:**
  1. **Reproduce no-clustering baseline:** Process videos without clustering to establish initial fragmentation rate (paper reports FR=20.74); validates tracklet formation pipeline
  2. **Loss ablation (Table 3 top):** Train with self-supervised → supervised → temporally-aware supervised loss; expect FR reduction: 10.47 → 3.96 → 2.32
  3. **Clustering ablation (Table 3 bottom):** Compare standard AP vs temporal-AP with learned embeddings; validates temporal penalty contribution (FR: 4.18 → 2.32)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following limitations are implicit:

- **Unknown 1:** Temperature parameter τ for softmax in contrastive loss—critical for loss scaling.
- **Unknown 2:** Transformer details (layers, heads, hidden size) and projection head architecture—needed for exact model replication.
- **Unknown 3:** Training hyperparameters (optimizer, learning rate, epochs, LR schedule).
- **Unknown 4:** Data augmentation pipeline for generating views—only mentions "views" without specifics.

## Limitations

- **Ground-truth detection dependency:** Using ground-truth detections for tracklet construction avoids tracking errors but creates a significant deployment gap. Real-world performance would depend on the accuracy of external polyp detection systems.
- **Limited dataset diversity:** The evaluation uses leave-one-out cross-validation within the REAL-Colon dataset (19 videos), but the training combines multiple datasets. Generalization to entirely new clinical environments remains unvalidated.
- **Temporal proximity assumption:** The method's effectiveness relies on the assumption that temporally close frames belong to the same polyp entity, which may not hold for procedures with scope withdrawal and reinsertion.

## Confidence

**High confidence:** The architectural design choices (ResNet-50 encoder, Transformer aggregator, 128-dim embeddings) are standard and well-justified. The ablation studies systematically demonstrate the contribution of each component, particularly the temporally-aware loss and temporal penalty in clustering.

**Medium confidence:** The reported 2.2x reduction in fragmentation rate is significant, but depends on ground-truth detections. The method shows strong performance on the tested datasets, though real-world deployment would require robust polyp detection and tracking systems.

**Low confidence:** The specific hyperparameter values (λ=1, κ=8, α≈0.48, preference range [-5,5]) appear well-tuned for this dataset but may not generalize. The choice of 14 views per polyp seems optimal for this setting but could be dataset-dependent.

## Next Checks

1. **Temporal assumption validation:** Test the method on a dataset with known polyp reappearance patterns (e.g., same polyp detected at beginning and end of procedure) to quantify how temporal penalties affect valid re-associations.

2. **End-to-end evaluation:** Replace ground-truth detections with an off-the-shelf polyp detector (e.g., PolypGuard, PolypGuard+) and evaluate the complete pipeline from raw video to polyp count to measure real-world performance.

3. **Cross-center generalization:** Evaluate the trained model on colonoscopy videos from a different medical center or country to assess performance on unseen camera systems, lighting conditions, and clinical protocols.