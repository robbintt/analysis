---
ver: rpa2
title: Graph Attention-Guided Search for Dense Multi-Agent Pathfinding
arxiv_id: '2510.17382'
source_url: https://arxiv.org/abs/2510.17382
tags:
- search
- mapf
- neural
- magat
- lacam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LaGAT, a hybrid MAPF solver combining neural
  heuristics with search. It enhances the MAGAT architecture (three attention layers,
  edge features) and integrates it into LaCAM via imitation learning from expert trajectories,
  followed by map-specific fine-tuning.
---

# Graph Attention-Guided Search for Dense Multi-Agent Pathfinding

## Quick Facts
- arXiv ID: 2510.17382
- Source URL: https://arxiv.org/abs/2510.17382
- Reference count: 20
- Primary result: LaGAT achieves state-of-the-art performance in dense MAPF scenarios by combining neural heuristics with search-based planning.

## Executive Summary
LaGAT introduces a hybrid approach to Multi-Agent Pathfinding (MAPF) that combines neural heuristics with search-based planning. By integrating a Graph Attention Network (MAGAT+) into the LaCAM solver, LaGAT guides search with learned coordination patterns while maintaining completeness through deadlock detection. The method employs imitation learning from expert trajectories followed by map-specific fine-tuning, achieving superior performance in dense MAPF scenarios compared to both purely search-based and purely learning-based methods.

## Method Summary
LaGAT enhances the MAGAT architecture with three attention layers and edge features, then integrates it into LaCAM via imitation learning from expert trajectories. The model is pre-trained on diverse random and maze maps, then fine-tuned on 1,000 map-specific instances. A deadlock detection mechanism ensures completeness by resetting constraints when agents revisit states. The neural policy outputs action probabilities that guide the PIBT (Priority Inheritance with Backtracking) search in LaCAM, with collision shielding preserving validity.

## Key Results
- Significantly outperforms both search-based (LaCAM) and learning-based (MAPF-GPT, MAGAT+) methods in dense MAPF scenarios
- Achieves higher-quality initial and final solutions within 30-second time limits
- Demonstrates that hybrid approaches can surpass the Pareto frontier of state-of-the-art MAPF solvers
- Map-specific fine-tuning proves critical for achieving optimal performance

## Why This Works (Mechanism)

### Mechanism 1
Neural heuristics improve solution quality by injecting long-horizon coordination signals into myopic greedy search. The MAGAT+ network observes local agent configurations and predicts action probabilities that guide PIBT toward cooperatively efficient states rather than individually optimal ones.

### Mechanism 2
Map-specific fine-tuning bridges the gap between general imitation and specific structural constraints. Pre-training on diverse maps learns general collision avoidance, while fine-tuning on 1,000 instances of the target map adapts attention weights to unique bottlenecks like narrow corridors.

### Mechanism 3
Completeness is preserved through systematic override of neural policy when it induces cyclic behavior. Deadlock detection backtraces search history and resets constraints for agents in repeated states, forcing PIBT to revert to default preferences.

## Foundational Learning

- **LaCAM and PIBT (Lazy Constraints Addition):** Understanding that LaGAT wraps around LaCAM is crucial—neural networks only change the ordering of successors in LaCAM's search tree, not the fundamental search mechanism.
  - *Quick check:* Can you explain the difference between a "configuration" and a "constraint" in LaCAM's search tree?

- **Graph Attention Networks (GAT):** The specific architecture encoding agent interactions—agents exchange feature vectors with neighbors and weight them via attention, capturing congestion better than simple MLPs.
  - *Quick check:* How does adding "edge features" (relative position) help an agent decide whether to yield to a neighbor?

- **Imitation Learning (DAgger/On-demand aggregation):** The system relies on expert trajectories from lacam3, with DAgger querying the expert during student training on failed instances to address distribution shift.
  - *Quick check:* Why is it necessary to query the expert during training on failed instances?

## Architecture Onboarding

- **Component map:** Input (Grid observation + Communication Graph) -> CNN Encoder -> 3-Layer GAT (with Edge Features) -> MLP Decoder -> Action Probabilities -> LaCAM Search (with PIBT) -> Deadlock Detector
- **Critical path:** The inference loop is performance-critical—LaGAT must perform GNN forward passes at every configuration expansion, requiring a lightweight model to ensure overhead doesn't negate search speed gains.
- **Design tradeoffs:** Density vs. utility (works best in dense scenarios ≈25% vertex occupancy), generality vs. performance (map-specific fine-tuning required for optimal results).
- **Failure signatures:** Livestock (agents oscillating without progress—check deadlock detector depth), slow convergence (initial solution fast but improvement slow—ensure LNS post-processing), catastrophic collision (neural suggests invalid move—verify PIBT collision shielding).
- **First 3 experiments:** 1) Density ablation: Run LaGAT vs. LaCAM on 20×20 grid while increasing agents from 10-150, plot crossover point. 2) Deadlock stress test: Construct bottleneck map with high traffic, run with detection ON vs OFF. 3) Inference latency profile: Benchmark C++ vs Python MAGAT+ implementation.

## Open Questions the Paper Calls Out

### Open Question 1
Can LaGAT's performance advantage be maintained on maps that significantly exceed the neural policy's fixed observation and communication radii? The Discussion notes that for maps much larger than the communication range, the local nature of decentralized policies often leads to suboptimal solutions.

### Open Question 2
Can the map-specific fine-tuning requirement be eliminated while retaining high performance in dense scenarios? The method currently relies on a fine-tuning stage using 1,000 instances on the target map.

### Open Question 3
Can the neuro-guided search paradigm be effectively adapted for broader multi-agent coordination problems beyond one-shot MAPF? The Conclusion posits this as a promising foundation for addressing broader classes of multi-agent coordination problems.

## Limitations
- Performance heavily depends on map-specific fine-tuning requiring access to representative training data
- Specialized for dense scenarios (agent density ≈25% of vertices), potentially underperforming in sparse settings
- 30-second time limit may not reflect real-world deployment scenarios with longer planning horizons

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Hybrid architecture combining neural heuristics with LaCAM search is technically sound | High |
| Performance claims are robust within tested dense MAPF scenarios | Medium |
| Trade-off analysis between neural inference overhead and search efficiency is fully explored | Low |

## Next Checks
1. **Density crossover validation:** Systematically vary agent density from 5% to 40% and identify exact crossover points where LaGAT transitions from superior to inferior performance compared to LaCAM.
2. **Dynamic environment robustness:** Test LaGAT on scenarios where agent positions or map topology change mid-execution to assess fine-tuning brittleness.
3. **Inference overhead profiling:** Benchmark MAGAT+ inference latency (C++ vs Python) across different agent counts to quantify "real-time" constraints.