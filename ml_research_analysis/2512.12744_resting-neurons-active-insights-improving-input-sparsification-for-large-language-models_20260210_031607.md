---
ver: rpa2
title: 'Resting Neurons, Active Insights: Improving Input Sparsification for Large
  Language Models'
arxiv_id: '2512.12744'
source_url: https://arxiv.org/abs/2512.12744
tags:
- neurons
- spontaneous
- uni00000014
- arxiv
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance gap in large language models
  (LLMs) when using input sparsification techniques. While input sparsification improves
  efficiency by activating only a subset of input entries, it often leads to significant
  performance degradation compared to full models.
---

# Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models

## Quick Facts
- **arXiv ID:** 2512.12744
- **Source URL:** https://arxiv.org/abs/2512.12744
- **Reference count:** 40
- **Primary result:** Spontaneous neurons substantially reduce the performance gap in large language models caused by input sparsification while maintaining inference efficiency

## Executive Summary
This paper addresses the performance degradation that occurs when large language models use input sparsification techniques to improve efficiency. The authors introduce spontaneous neurons, trainable, input-independent units that compensate for the representational drift caused by zeroing low-magnitude activations. These neurons are learned via knowledge distillation from dense models and can be merged into bias terms at inference time, incurring no computational overhead. Experimental results demonstrate that spontaneous neurons significantly improve perplexity and zero-shot task performance across multiple model architectures and sparsity levels, particularly for complex reasoning and mathematical tasks.

## Method Summary
The method adds learnable spontaneous activation vectors to specific layers of transformer models that use input sparsification (specifically TEAL magnitude-based pruning). These neurons are trained to minimize KL divergence between dense and sparse model outputs on calibration data, then merged into existing bias terms to eliminate inference overhead. The approach works by compensating for the systematic error introduced when low-magnitude activations are zeroed during sparsification, effectively encoding "prior expectations" that stabilize hidden representations across layers.

## Key Results
- Achieves perplexity close to full models even at 50-60% sparsity levels on language modeling tasks
- Consistently improves performance over baseline methods on zero-shot reasoning tasks, particularly for complex reasoning and mathematical understanding
- Compatible with quantization techniques while maintaining high inference efficiency
- Down-projection layer injection alone achieves ~98% of full injection benefits with fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
Input sparsification causes systematic representational drift; spontaneous neurons act as learnable bias corrections that recover this drift without inference overhead. Sparsification zeros low-magnitude activations, producing residual error e(X) = WX - WS(X). The optimal constant bias b* = E[e(X)] minimizes expected approximation error. Spontaneous neurons are trained to approximate b* via KL distillation, then fused into existing bias terms—no extra matrix multiply at inference.

### Mechanism 2
Knowledge distillation from dense to sparse model teaches spontaneous neurons to encode "prior expectations" that stabilize hidden representations across layers. A calibration dataset is passed through both dense f(X) and sparse f(S(X); α) models. The spontaneous activation vector α is optimized to minimize KL divergence between their output distributions, distilling representational priors into α.

### Mechanism 3
MLP down-projection layers benefit most from spontaneous neurons; upper layers (closer to embeddings) show larger gains than lower layers. Empirical ablation shows injecting into MLP down-projection alone achieves perplexity 15.57 vs. 15.74 for attention-only injection. Upper layers capture local representations; lower layers specialize for final output generation.

## Foundational Learning

- **Input Sparsification (Dynamic Activation Pruning)**
  - Why needed here: The method builds on TEAL-style magnitude-based activation masking. You must understand how S(X) selectively zeros low-magnitude entries per input before grasping why compensation is needed.
  - Quick check question: Given activation tensor X ∈ R^{b×k×d} and threshold τ, write the sparsification operation that masks entries below τ.

- **Knowledge Distillation via KL Divergence**
  - Why needed here: Spontaneous neurons are trained by matching dense-to-sparse output distributions. Understanding KL minimization is essential for debugging convergence.
  - Quick check question: Why minimize KL(p_dense || p_sparse) rather than reverse? What happens if distributions have disjoint support?

- **Bias Fusion (Precomputation)**
  - Why needed here: The method's efficiency claim rests on merging W·α into bias at inference time. You must verify this eliminates extra computation.
  - Quick check question: For linear layer Y = WX + b with spontaneous activation α, show how b' = b + Wα eliminates α from the forward pass.

## Architecture Onboarding

- **Component map:**
```
Input X → [Sparsify S(X)] → [Linear W·S(X)] → (+) → Output
                                    ↑
                              [Spontaneous α]
                                    ↓
                           [Fused bias: b + Wα]
```
Spontaneous neurons attach at every linear block (attention Q/K/V/O, MLP up/gate/down). At training, α is a learnable vector per block; at inference, precompute b_new = b_old + Wα and use standard bias addition.

- **Critical path:**
  1. Load pre-trained LLM with existing input sparsification (e.g., TEAL thresholds)
  2. Initialize α vectors (dim = output_dim of each linear layer; paper uses single vector per layer)
  3. Run calibration data through both dense and sparse models
  4. Optimize α via gradient descent on KL(f_dense(X) || f_sparse(X; α))
  5. Fuse: for each layer, b_final = b_original + W @ α
  6. Deploy with sparsification + updated biases

- **Design tradeoffs:**
  - Injection location: Down-projection only vs. all layers (down-projection alone is ~98% as effective with fewer parameters)
  - Number of spontaneous vectors: Paper uses 1 per layer; more vectors increase capacity but also storage
  - Calibration data size: More data improves generalization but increases training time (paper: Wikitext chunks, ~1-2h on 4×A6000)

- **Failure signatures:**
  - Perplexity doesn't improve: Check that α is being optimized (learning rate ~1e-5); verify KL loss is decreasing
  - Inference slower: You forgot to fuse α into bias; still computing W·α at runtime
  - Good perplexity but poor downstream task performance: Calibration data may not cover task domain; try task-specific calibration

- **First 3 experiments:**
  1. Reproduce Table 1 on Llama3-8B at 50% sparsity: Compare TEAL-only vs. TEAL+SPON perplexity on Wikitext. Expected: ~8.34 → ~7.83.
  2. Ablation on injection location: Test MLP-down-only vs. all-blocks injection. Verify down-only achieves within 1% of full injection.
  3. Quantization compatibility: Apply int4 quantization (bitsandbytes) to SPON model at 50% sparsity. Verify perplexity stays below TEAL's fp32 baseline (Section 3.7).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can spontaneous neurons be integrated into the pretraining phase rather than post-hoc, and would this yield better efficiency-accuracy trade-offs than post-training injection?
- Basis in paper: [explicit] The conclusion states: "future work will explore spontaneous neurons in both pretraining and post-training regimes."
- Why unresolved: The current work only applies spontaneous neurons after pretraining via knowledge distillation. Pretraining integration could potentially learn more fundamental sparse representations but may require different optimization strategies.

### Open Question 2
- Question: What are the optimal hardware kernels for jointly exploiting activation sparsity and weight quantization, and what speedup gains are achievable?
- Basis in paper: [explicit] Section 3.7 states: "Realizing this potential, however, requires the development of specialized kernels that jointly support sparsity and quantization, which we leave as future work."
- Why unresolved: While the paper demonstrates that spontaneous neurons work with quantization in PyTorch, no specialized CUDA kernels were developed to exploit both optimizations simultaneously for actual speedup.

### Open Question 3
- Question: Does adding multiple spontaneous neurons per layer yield diminishing returns or additional gains, and what determines the optimal number?
- Basis in paper: [inferred] The paper notes using "a single spontaneous neuron" per layer but does not ablate the effect of using multiple neurons.
- Why unresolved: The representational capacity of a single bias vector may be insufficient for complex tasks or higher sparsity levels, but adding more parameters could reduce efficiency benefits.

## Limitations

- **Implementation Dependence on TEAL**: The method builds explicitly on TEAL's magnitude-based activation pruning, but TEAL's exact threshold selection strategy is not specified in the paper.
- **Knowledge Distillation Data Domain**: The method relies on calibration data to learn spontaneous neurons via KL distillation, but doesn't address how performance degrades with domain shift between calibration and deployment data.
- **Architecture Specificity**: Results are demonstrated primarily on Llama3 and similar transformer architectures with standard MLP down-projection layers, limiting generalization to other architectures.

## Confidence

- **High Confidence**: The basic mechanism of spontaneous neurons is sound; knowledge distillation approach is valid; bias fusion eliminates inference overhead; empirical results are reproducible.
- **Medium Confidence**: Specific claim that MLP down-projection layers are optimal injection points; assertion that spontaneous neurons provide 98% of full injection benefits; compatibility with quantization techniques.
- **Low Confidence**: Generalization across all task domains without task-specific calibration; performance on architectures substantially different from tested transformer models; behavior under extreme sparsification levels (>70%).

## Next Checks

1. **Domain Transfer Experiment**: Train spontaneous neurons on Wikitext, then evaluate on a non-Wikitext domain (e.g., medical text, code, or scientific literature). Measure degradation in perplexity and accuracy compared to in-domain calibration.

2. **Architecture Generalization Test**: Apply the method to a non-transformer architecture (e.g., RWKV or Mamba) or to a model without standard MLP down-projections. Verify whether the layer-wise injection strategy needs modification and measure performance impact.

3. **Extreme Sparsification Boundary**: Systematically test performance at 70-90% sparsification levels to identify the practical limits of spontaneous neurons. Compare against alternative compensation methods and document the point where performance collapses.