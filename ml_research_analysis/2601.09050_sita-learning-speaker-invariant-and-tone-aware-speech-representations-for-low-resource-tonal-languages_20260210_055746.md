---
ver: rpa2
title: 'SITA: Learning Speaker-Invariant and Tone-Aware Speech Representations for
  Low-Resource Tonal Languages'
arxiv_id: '2601.09050'
source_url: https://arxiv.org/abs/2601.09050
tags:
- tone
- retrieval
- speech
- sita
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of learning speaker-invariant
  yet tone-aware speech representations for low-resource tonal languages like Hmong,
  where embeddings of the same word spoken by different speakers must be similar while
  embeddings of the same base word with different tones must remain well-separated.
  To tackle this, the authors propose SITA, a two-stage lightweight adaptation recipe
  for wav2vec-style encoders.
---

# SITA: Learning Speaker-Invariant and Tone-Aware Speech Representations for Low-Resource Tonal Languages

## Quick Facts
- **arXiv ID**: 2601.09050
- **Source URL**: https://arxiv.org/abs/2601.09050
- **Reference count**: 40
- **Primary result**: SITA achieves ~0.61 average Top-1 cross-gender retrieval accuracy while maintaining WER close to fully ASR-adapted XLS-R.

## Executive Summary
This work addresses the challenge of learning speaker-invariant yet tone-aware speech representations for low-resource tonal languages like Hmong, where embeddings of the same word spoken by different speakers must be similar while embeddings of the same base word with different tones must remain well-separated. To tackle this, the authors propose SITA, a two-stage lightweight adaptation recipe for wav2vec-style encoders. Stage 1 uses a cross-gender contrastive loss to encourage speaker invariance and a tone-repulsive loss to prevent tone collapse by explicitly separating same-base different-tone pairs, with optional freezing of lower layers. Stage 2 fine-tunes upper layers with CTC and knowledge distillation to preserve ASR capability. Evaluated on a curated Hmong word corpus and transferred to Mandarin, SITA improves cross-gender lexical retrieval accuracy (achieving ~0.61 average Top-1) and increases hard-negative cosine distance from ~0.01-0.08 to ~0.675, while maintaining ASR performance close to a fully ASR-adapted XLS-R teacher. The method generalizes to unseen speakers and different tonal languages, suggesting it is a broadly applicable approach for adapting multilingual speech encoders to tonal languages.

## Method Summary
SITA is a two-stage adaptation recipe for XLS-R encoders that learns speaker-invariant and tone-aware representations for low-resource tonal languages. Stage 1 trains middle layers (e.g., 13-19) with cross-gender contrastive loss for speaker invariance and tone-repulsive loss to prevent tone collapse, while freezing bottom layers. Stage 2 fine-tunes upper layers with CTC and knowledge distillation from an ASR teacher to preserve recognition capability. The method uses InfoNCE-based contrastive objectives with positive/negative sampling strategies tailored for speaker and tone invariance, layer-wise freezing, and temperature scaling for stable training.

## Key Results
- Achieves ~0.61 average Top-1 cross-gender lexical retrieval accuracy
- Increases hard-negative cosine distance from ~0.01-0.08 to ~0.675, preventing tone collapse
- Maintains WER close to fully ASR-adapted XLS-R teacher (0.511-0.534 vs 0.513-0.534)
- Generalizes to unseen speakers and transfers successfully to Mandarin

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-gender contrastive learning with InfoNCE produces speaker-invariant lexical embeddings by treating gender as within-class nuisance variation.
- **Mechanism**: For each anchor, construct a positive pair from the same lexical item spoken by opposite-gender speakers, and sample negatives from different lexical items. The InfoNCE loss pulls cross-gender positives together while pushing different words apart, forcing the encoder to rely on lexical content rather than speaker acoustics.
- **Core assumption**: Gender is a proxy for nuisance speaker variation; sufficient cross-gender coverage exists in training data.
- **Evidence anchors**: [abstract] "a cross-gender contrastive objective encourages lexical consistency across speakers"; [Section 3.2] "This objective treats gender as within-class nuisance variation: it pulls together embeddings of the same lexical item spoken by different speaker genders while pushing away different lexical items."; [corpus] CLiFT-ASR (arXiv:2511.06860) confirms cross-lingual fine-tuning with contrastive objectives helps low-resource tonal ASR, though doesn't isolate the gender-invariance mechanism.
- **Break condition**: Training data with severe gender imbalance or single-gender speakers; cross-gender positive pairs unavailable.

### Mechanism 2
- **Claim**: Explicit tone repulsion via hard-negative contrastive learning prevents tone collapse by separating same-word different-tone pairs in embedding space.
- **Mechanism**: Construct three sets per anchor: positives (same word + tone), hard negatives (same word, different tone), and soft negatives (different words). The scaled InfoNCE loss explicitly maximizes similarity within-tone while minimizing it cross-tone, complemented by a linear tone classifier trained with cross-entropy.
- **Core assumption**: Tone labels are accurate; the tonal inventory is sufficiently represented in training.
- **Evidence anchors**: [abstract] "a tone-repulsive loss prevents tone collapse by explicitly separating same-word different-tone realizations"; [Section 5.1] "SITA simultaneously maintains strong within-tone cohesion and substantially increases cross-tone separation, achieving a hard-negative cosine distance of 0.675"; [corpus] Tone recognition study (arXiv:2506.03606) finds SSL models encode tone in middle layers, supporting the layer selection strategy, but doesn't validate the repulsion loss directly.
- **Break condition**: Incomplete or incorrect tone labels; tone inventory not fully covered in training.

### Mechanism 3
- **Claim**: Staged training with layer-wise freezing and knowledge distillation preserves tone-aware embeddings while restoring ASR capability.
- **Mechanism**: Stage 1 updates middle layers (e.g., 13-19) for speaker invariance and tone awareness. Stage 2 freezes these embedding layers and fine-tunes only upper layers with CTC + KL distillation from an ASR teacher, preventing the CTC objective from overwriting tone structure.
- **Core assumption**: Recognition-relevant structure is recoverable in upper layers without modifying tone embeddings.
- **Evidence anchors**: [Section 5.2] "Staged Training is Important. Single-stage optimization of all objectives yields strong retrieval... but produces much weaker ASR."; [Table 4] Single-stage achieves 0.689-0.707 Top-1 but WER 0.76-0.78; staged achieves 0.546 Top-1 with WER 0.511-0.534; [corpus] No direct corpus validation of staged training for tone-aware ASR; this mechanism remains less externally verified.
- **Break condition**: Insufficient upper-layer capacity for ASR (feature layer too deep); CTC overfitting on tiny data.

## Foundational Learning

- **Concept**: **InfoNCE / Contrastive Predictive Coding**
  - **Why needed here**: Core objective for both speaker invariance and tone repulsion; understanding positive/negative sampling is essential.
  - **Quick check question**: Given an anchor embedding, how would you construct positive and negative pairs for cross-gender lexical contrastive learning?

- **Concept**: **Wav2vec 2.0 / XLS-R Encoder Architecture**
  - **Why needed here**: SITA is a modular adaptation recipe; you must understand layer indexing, hidden states, and where to extract embeddings.
  - **Quick check question**: In a 24-layer XLS-R transformer, if you extract embeddings at layer 19, how many layers remain for Stage 2 ASR fine-tuning?

- **Concept**: **CTC Loss and Knowledge Distillation**
  - **Why needed here**: Stage 2 relies on CTC for ASR and KL distillation to stabilize training; understanding frame-level alignment and temperature-scaled posteriors is critical.
  - **Quick check question**: Why does CTC require summing over all valid alignments, and how does temperature scaling affect distillation?

## Architecture Onboarding

- **Component map**: XLS-R (24 transformer blocks) -> Stage 1 heads (tone classifier) -> Stage 2 heads (CTC projection) -> Teacher (ASR-adapted XLS-R)
- **Critical path**:
  1. Preprocess audio: VAD segmentation, peak normalization, metadata alignment
  2. Augment: acoustic perturbations (no pitch shift for tone tasks) + FreeVC voice conversion (optional)
  3. Stage 1: Train blocks 13-L with L_speaker + L_tone; freeze bottom 12
  4. Stage 2: Freeze blocks 1-L; train blocks L+1-24 + CTC head with L_CTC + L_KD
  5. Evaluate: cross-gender retrieval, tone geometry (hard/soft negative distance), ASR (WER/CER)

- **Design tradeoffs**:
  - **Feature layer depth**: Deeper (21) → better retrieval/tone separation; shallower (19) → more capacity for ASR
  - **Freezing vs. full fine-tuning**: Freezing stabilizes retrieval; unfreezing may improve ASR but risks overwriting embeddings
  - **Margin vs. InfoNCE tone loss**: Margin yields stronger tone separation but worse unseen-speaker generalization (Table 15)

- **Failure signatures**:
  - **Tone collapse**: Hard-negative cosine distance < 0.1; retrieval fails despite high training accuracy
  - **Speaker leakage**: Cross-gender retrieval Top-1 near chance; embeddings cluster by speaker, not word
  - **ASR collapse**: WER > 0.8 after Stage 2; CTC overfits to few examples or distillation destabilizes training
  - **Layer misconfiguration**: Feature layer = 24 leaves no upper capacity → Stage 2 overwrites Stage 1 structure

- **First 3 experiments**:
  1. **Baseline probe**: Extract XLS-R frozen embeddings (layer 19/21); measure Top-1 cross-gender retrieval and hard-negative cosine distance to quantify initial tone collapse and speaker bias.
  2. **Stage 1 ablation**: Train with L_speaker only, L_tone only, and both jointly; compare retrieval and tone geometry to confirm both losses are necessary.
  3. **Feature layer sensitivity**: Compare layer 19 vs. 21 vs. 24 (with head-only Stage 2) on retrieval-ASR tradeoff; plot Top-1 vs. WER to select operating point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does SITA perform on spontaneous, conversational speech compared to the curated word-level corpus used in this study?
- **Basis in paper**: [explicit] "Our evaluation of SITA is conducted on a curated word-level corpus... our retrieval and tone-geometry metrics may not fully reflect performance on spontaneous, conversational speech or under broader domain shift."
- **Why unresolved**: The current evaluation protocol uses isolated word segments with standardized recording conditions and controlled pauses; real-world deployment would require handling continuous speech, varied speaking rates, and conversational co-articulation effects.
- **What evidence would resolve it**: Evaluation on conversational speech corpora for tonal languages (e.g., code-switched Hmong-English conversations), with metrics adapted for word discovery and tone discrimination in continuous streams.

### Open Question 2
- **Question**: What is the optimal configuration of SITA's loss weights and freezing strategies when prioritizing different downstream tasks (e.g., robust retrieval vs. recognition accuracy)?
- **Basis in paper**: [explicit] "SITA exposes an inherent trade-off between tone separation and ASR accuracy: the appropriate loss weighting and freezing strategy may depend on the downstream priority, and may require task-specific tuning."
- **Why unresolved**: The paper demonstrates that feature layer selection (19 vs. 21 vs. 24) and freezing policies create sharp trade-offs, but does not provide a principled method for selecting these configurations for new languages or task priorities.
- **What evidence would resolve it**: Systematic hyperparameter sweeps across multiple tonal languages with different resource levels, coupled with analysis of how tonal inventory complexity correlates with optimal configurations.

### Open Question 3
- **Question**: Can sensitive demographic attributes (speaker identity, age, accent) still be extracted from SITA embeddings despite the speaker-invariance training objectives?
- **Basis in paper**: [explicit] "While our training objectives encourage invariance to nuisance variation, they do not guarantee the removal of all demographic information or prevent downstream misuse."
- **Why unresolved**: The paper evaluates invariance only along the gender dimension using retrieval tasks; other demographic attributes remain untested, and adversarial recovery of speaker information is not assessed.
- **What evidence would resolve it**: Probing experiments using attribute classifiers on SITA embeddings for speaker verification, age prediction, and accent identification, comparing leakage against baseline multilingual encoders.

### Open Question 4
- **Question**: Does SITA generalize effectively to non-tonal low-resource languages, or are the gains specific to tonal phonology?
- **Basis in paper**: [inferred] The paper demonstrates transfer between two tonal languages (Hmong and Mandarin) with very different resource levels and tonal systems, but does not test on non-tonal low-resource languages where the tone-repulsion loss would be irrelevant.
- **Why unresolved**: The cross-gender contrastive objective could benefit speaker-invariance generally, but the tone-aware loss components are tonal-specific; it is unclear whether the staged training recipe provides benefits beyond existing adaptation methods when tone supervision is absent.
- **What evidence would resolve it**: Application of the SITA framework (with tone objectives removed or replaced) to low-resource non-tonal languages, comparing against standard fine-tuning baselines on retrieval and ASR tasks.

## Limitations
- **Dataset accessibility**: Primary Hmong dataset collected under IRB study with unclear public availability
- **External validation gap**: Staged training benefits not validated on external corpora beyond internal comparisons
- **Voice conversion specifics**: FreeVC augmentation details (speaker pool selection criteria) not specified
- **Mechanism isolation**: Cross-gender and tone-repulsive losses not fully isolated in ablation studies

## Confidence

- **High Confidence**: The overall two-stage framework (Stage 1 for invariance/tone awareness, Stage 2 for ASR) is well-supported by the empirical results (Table 4, Table 5). The retrieval metrics and tone geometry improvements are robust across experiments.

- **Medium Confidence**: The specific contributions of cross-gender contrastive learning and tone-repulsive losses are supported by the paper's analysis, but external validation is limited. The mechanism for why gender is a good proxy for speaker invariance is assumed but not rigorously tested.

- **Low Confidence**: The reproducibility of the exact results is limited by the unclear availability of the Hmong dataset and the unspecified details of the voice conversion augmentation pipeline.

## Next Checks

1. **Dataset Availability and Substitution**: Attempt to obtain the Hmong CLiFT-ASR dataset or identify a publicly available tonal language corpus (e.g., Tone Perfect Mandarin) to replicate the experiments. Measure the impact of dataset size and speaker/tone distribution on the final performance.

2. **Ablation of Staged Training**: Conduct an ablation study comparing single-stage vs. staged training on a held-out subset of the data. Measure not only retrieval and ASR metrics but also the evolution of tone geometry (hard-negative cosine distance) throughout training to confirm that staged training prevents tone collapse.

3. **Generalization to Unseen Speakers and Languages**: Transfer the SITA-adapted XLS-R model to a completely different tonal language (e.g., Mandarin or Cantonese) with unseen speakers. Evaluate cross-gender retrieval and ASR performance to test the robustness of the speaker-invariant and tone-aware representations.