---
ver: rpa2
title: 'Your thoughts tell who you are: Characterize the reasoning patterns of LRMs'
arxiv_id: '2509.24147'
source_url: https://arxiv.org/abs/2509.24147
tags:
- reasoning
- behavior
- output
- taxonomy
- behaviors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOT, an inductive method that constructs
  human-readable taxonomies of reasoning features to characterize how large reasoning
  models (LRMs) think. LOT uses a generative language model to compare reasoning traces
  from two LRMs, identifying distinctive reasoning traits in natural language, then
  builds classifiers to predict the source model based on these features.
---

# Your thoughts tell who you are: Characterize the reasoning patterns of LRMs

## Quick Facts
- arXiv ID: 2509.24147
- Source URL: https://arxiv.org/abs/2509.24147
- Authors: Yida Chen; Yuning Mao; Xianjun Yang; Suyu Ge; Shengjie Bi; Lijuan Liu; Saghar Hosseini; Liang Tan; Yixin Nie; Shaoliang Nie
- Reference count: 32
- Primary result: Achieves 80-100% accuracy classifying 12 LRMs by reasoning style using LOT, outperforming baselines by 14-21%.

## Executive Summary
This paper introduces LOT, an inductive method that constructs human-readable taxonomies of reasoning features to characterize how large reasoning models (LRMs) think. LOT uses a generative language model to compare reasoning traces from two LRMs, identifying distinctive reasoning traits in natural language, then builds classifiers to predict the source model based on these features. The method is iterated to refine the taxonomy until reliable classification is achieved. Applied to 12 open-source LRMs across math, science, and coding tasks, LOT achieves 80-100% accuracy in distinguishing LRMs that differ in scale, base model family, or objective domain.

## Method Summary
LOT is an inductive method that discovers reasoning features by comparing paired reasoning traces from two LRMs solving the same question. An LLM annotator (Llama3.3-70B-Instruct) identifies distinctive features in natural language, which are then used to encode traces as presence/absence vectors (PoR) or frequency vectors (BoR). A logistic classifier is trained to distinguish models based on these features. When classification fails, the failed sample triggers feature expansion through the annotator, and the process iterates until convergence (no changes for 20 consecutive iterations). The method achieves 80-100% accuracy in classifying LRMs across five benchmarks, with BoR encoding providing 3-14% improvements on harder tasks.

## Key Results
- Achieves 80-100% classification accuracy distinguishing LRMs that differ in scale, base model family, or objective domain
- BoR encoding outperforms PoR by 3-14% on harder benchmarks (GPQA-Diamond, CRUXEVAL-O)
- Outperforms baselines (few-shot prompting, VML, human-defined taxonomies) by 14-21%
- Test-time reasoning style alignment improves Qwen3-0.6B, Qwen3-4B, and Qwen3-8B accuracy on GPQA by 3.3-5.7%

## Why This Works (Mechanism)

### Mechanism 1: Inductive Feature Discovery from Paired Comparisons
LOT discovers distinguishing reasoning features by directly comparing reasoning traces from two LRMs solving the same question, rather than relying on predefined taxonomies. The LLM annotator receives paired reasoning traces with source labels, identifies distinctive features in natural language, then annotates new traces using these features to build vector representations for classification. This works because LRMs exhibit systematic, verbalizable differences in reasoning processes that transfer across questions.

### Mechanism 2: Iterative Taxonomy Refinement via Classification Failure
Classification failures signal incomplete feature sets, triggering taxonomy expansion until convergence. When the logistic classifier misclassifies a trace, the failed sample returns to the LLM annotator to propose new distinguishing features. The taxonomy expands, vectors are re-encoded with imputation, and the classifier retrains. This works because classification failures indicate missing features rather than noise or inherent indistinguishability.

### Mechanism 3: Reasoning Style-Performance Causal Link
Certain reasoning behaviors correlate with correctness, and test-time intervention to align styles can improve performance. Odds ratios quantify feature-correctness associations. The intervention pipeline summarizes reasoning, modifies steps based on correlations, then re-expands into full traces. This works because presence of reasoning behaviors causally contributes to correctness, not merely correlates with it.

## Foundational Learning

- **Inductive vs. Deductive Taxonomy Construction**: LOT's core innovation is deriving features from data rather than imposing researcher-defined categories. Quick check: Why might a predefined taxonomy fail to capture that a model "visualizes" molecular structures?
- **Presence of Reasoning (PoR) vs. Bag of Reasoning (BoR) Encodings**: BoR incorporates frequency information, improving accuracy on harder benchmarks by 3-14%. Quick check: On what task type does PoR perform comparably to BoR?
- **Odds Ratio for Feature-Correctness Association**: Identifies which reasoning behaviors to encourage vs. discourage during intervention. Quick check: What does an odds ratio < 1 indicate about a feature's relationship with correct answers?

## Architecture Onboarding

- **Component map**: Annotator LLM (Llama3.3-70B) -> Feature Taxonomy C -> Encoder (PoR/BoR) -> Logistic Classifier φ
- **Critical path**: Initialize taxonomy from one pair → annotate batch → train classifier → detect failures → expand taxonomy → repeat until convergence (N=20 iterations without changes)
- **Design tradeoffs**: PoR (binary presence, faster, works well for larger model gaps) vs BoR (includes frequency, +3-14% accuracy on harder tasks, requires KNN imputation during training)
- **Failure signatures**: Accuracy drops as parameter gap narrows; Qwen3-1.7B intervention fails due to poor instruction-following; context window overflow with long traces
- **First 3 experiments**: 1) Replicate Qwen3-0.6B vs Qwen3-32B classification on MATH-500 with PoR encoding; 2) Compare PoR vs BoR on GPQA-Diamond across three model pairs; 3) Test intervention pipeline on Qwen3-4B: summarize traces, add "verifying solution applicability" steps, measure GPQA accuracy change vs unmodified re-expansion

## Open Questions the Paper Calls Out
None

## Limitations
- The taxonomy construction depends heavily on the annotator LLM's capabilities—Llama3.3-70B-Instruct may miss subtle reasoning patterns that a human taxonomist would catch
- The iterative refinement process could overfit to specific model pairs rather than discovering universal reasoning traits
- The intervention pipeline shows promising results but may not transfer to other model families or tasks due to LRMs' limited instruction-following capabilities during reasoning generation

## Confidence
- **High Confidence**: LOT's ability to achieve 80-100% classification accuracy between LRMs using discovered reasoning features
- **Medium Confidence**: The interpretability of discovered reasoning features and their systematic nature across model families
- **Medium-Low Confidence**: The causal link between reasoning style alignment and performance improvement

## Next Checks
1. **Cross-Architecture Transfer**: Apply LOT to distinguish between LRMs from different base architectures (e.g., Llama vs Qwen vs DeepSeek) on the same tasks to verify whether discovered features generalize beyond scale-based differences
2. **Human-LLM Feature Comparison**: Have human experts independently construct taxonomies for the same model pairs and compare coverage, interpretability, and classification performance against LOT's inductively discovered features
3. **Ablation on Annotation Strategy**: Run LOT with different annotator LLMs (e.g., GPT-4o, Claude-3) and varying batch sizes to quantify sensitivity to these hyperparameters and establish robustness boundaries