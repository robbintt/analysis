---
ver: rpa2
title: Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large
  Language Models
arxiv_id: '2505.02858'
source_url: https://arxiv.org/abs/2505.02858
tags:
- data
- synthetic
- dataset
- posts
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using large language models (LLMs) to generate
  synthetic multi-platform social media datasets that match the quality of real data.
  The authors propose a Multi-Platform Topic Model (MPTM) prompting approach and compare
  it with per-platform prompting across two diverse datasets (US election discussions
  and Dutch influencer content) spanning six platforms.
---

# Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models

## Quick Facts
- arXiv ID: 2505.02858
- Source URL: https://arxiv.org/abs/2505.02858
- Reference count: 40
- Primary result: LLM-generated synthetic social media data matches real data on lexical features, sentiment, topic overlap, and named entities across six platforms

## Executive Summary
This paper investigates using large language models to generate synthetic multi-platform social media datasets that match the quality of real data. The authors propose a Multi-Platform Topic Model (MPTM) prompting approach and compare it with per-platform prompting across two diverse datasets spanning six platforms. The study evaluates three state-of-the-art LLMs (GPT-4o, Claude-3.5, and Gemini-2.0) across multiple fidelity metrics including lexical features, sentiment analysis, topic overlap, embedding similarity, and named entity distributions.

## Method Summary
The authors develop a Multi-Platform Topic Model (MPTM) prompting approach for generating synthetic social media content across multiple platforms simultaneously. They compare this with traditional per-platform prompting strategies using two diverse datasets: US election discussions and Dutch influencer content across six platforms. Three state-of-the-art LLMs are evaluated across multiple fidelity metrics including lexical features (hashtags, mentions, URLs, emojis), sentiment analysis, topic overlap, embedding similarity, and named entity distributions. The evaluation measures how well synthetic datasets preserve key social media characteristics compared to real data.

## Key Results
- Synthetic datasets generally preserve key social media characteristics with high fidelity across lexical features, sentiment, and topic overlap
- No single prompting strategy or LLM consistently outperforms others across all metrics
- GPT-4o underestimates hashtag usage but aligns well with real data for mentions and emojis
- Gemini-2.0 shows platform-specific strengths, particularly for TikTok and YouTube content

## Why This Works (Mechanism)
The Multi-Platform Topic Model (MPTM) approach leverages the inherent cross-platform understanding capabilities of large language models by conditioning generation on topic distributions across multiple platforms simultaneously. This allows LLMs to maintain consistency in thematic content while adapting to platform-specific linguistic patterns. The approach works because modern LLMs have been trained on diverse web data that includes multi-platform content, enabling them to implicitly learn platform-specific writing styles and content conventions. By providing structured prompts that specify both topic constraints and platform contexts, the MPTM approach guides LLMs to generate content that preserves both semantic meaning and platform-appropriate formatting.

## Foundational Learning
The paper builds on the observation that large language models trained on internet-scale data have implicitly learned platform-specific linguistic patterns and content conventions. The MPTM approach extends this by explicitly conditioning generation on cross-platform topic distributions, allowing LLMs to maintain thematic consistency while adapting to different platform characteristics. The evaluation framework demonstrates that synthetic data can preserve surface-level social media features (lexical patterns, sentiment distributions, named entities) when proper prompting strategies are employed. This suggests that LLMs can serve as effective data generators when guided by structured topic models that capture the multi-platform nature of modern social media discourse.

## Architecture Onboarding
Component map: Real data -> LLM input prompts -> LLM generation -> Synthetic data -> Fidelity metrics evaluation
Critical path: Prompt engineering -> LLM generation -> Multi-platform consistency checks -> Fidelity validation
Design tradeoffs: Multi-platform prompting vs per-platform prompting (consistency vs platform specificity)
Failure signatures: Systematic bias in sentiment generation, platform-specific performance variations
First experiments: 1) Evaluate hashtag distribution preservation across platforms, 2) Compare sentiment analysis fidelity between models, 3) Test named entity distribution replication for high-frequency terms

## Open Questions the Paper Calls Out
- How can synthetic datasets be evaluated for deeper semantic and behavioral validity beyond surface-level characteristics?
- What is the impact of temporal dynamics and user interaction patterns on synthetic data quality?
- How do systematic biases in emotional content generation vary across different LLMs and prompting strategies?
- Can synthetic datasets preserve community structures, information cascades, and network effects present in real data?

## Limitations
- Evaluation focuses on surface-level characteristics rather than deeper semantic or behavioral validity
- Limited dataset diversity with only two topic areas tested
- No analysis of temporal dynamics, user interaction patterns, or network effects
- Systematic biases in emotional content generation observed across different LLMs
- Does not address privacy preservation or ethical considerations in synthetic data generation

## Confidence
- Synthetic data matching lexical features: High
- Platform-specific performance variations: Medium
- Topic overlap preservation: Medium
- Embedding similarity measures: Low
- Named entity distribution replication: Low
- Semantic coherence and contextual appropriateness: Low

## Next Checks
1. Conduct longitudinal studies to evaluate temporal consistency and evolution patterns in synthetic datasets compared to real social media data streams
2. Implement network analysis to assess whether synthetic data preserves community structures, information cascades, and interaction patterns present in real datasets
3. Perform blind studies with domain experts to evaluate the semantic coherence and contextual appropriateness of synthetic posts across different platforms and topics
4. Investigate methods to quantify and mitigate systematic biases in emotional content generation across different LLMs and prompting strategies