---
ver: rpa2
title: 'Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas'
arxiv_id: '2602.01418'
source_url: https://arxiv.org/abs/2602.01418
tags:
- pape
- position
- encoding
- attention
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Parabolic Position Encoding (PaPE), a new
  position encoding method for vision transformers based on learnable parabolas applied
  to relative token positions. PaPE is designed from five key principles: translation
  invariance, rotation invariance, distance decay, directionality, and context awareness.'
---

# Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas

## Quick Facts
- arXiv ID: 2602.01418
- Source URL: https://arxiv.org/abs/2602.01418
- Reference count: 27
- Primary result: PaPE achieves state-of-the-art performance across 8 datasets spanning 4 vision modalities, with exceptional classification extrapolation capabilities

## Executive Summary
This paper introduces Parabolic Position Encoding (PaPE), a new position encoding method for vision transformers based on learnable parabolas applied to relative token positions. PaPE is designed from five key principles: translation invariance, rotation invariance, distance decay, directionality, and context awareness. The method uses a quadratic term to encode distance and a linear term to encode direction, making it compatible with efficient attention kernels through separate query-key transformations. Evaluated across 8 datasets spanning 4 vision modalities (images, point clouds, videos, event cameras), PaPE achieves the top performance on 7 out of 8 datasets, with an average accuracy of 66.3%. Notably, PaPE demonstrates exceptional classification extrapolation capabilities, improving accuracy by up to 10.5% over the next-best position encoding when scaling beyond training resolution on ImageNet-1K.

## Method Summary
PaPE encodes relative positions between tokens using learnable parabolas that capture distance decay and directionality while maintaining translation invariance. For each query token, PaPE computes content-dependent coefficients that control how attention decays with distance (via a quadratic term) and how it is modulated by direction (via a linear term). The method factorizes these computations into separate query and key transformations, making it compatible with efficient attention kernels like FlashAttention. The core innovation is a learnable projection of relative positions combined with content-dependent coefficients that allow the model to adapt its positional encoding based on the specific visual features being processed.

## Key Results
- PaPE achieves state-of-the-art performance on 7 out of 8 evaluated datasets spanning images, point clouds, videos, and event cameras
- Average accuracy of 66.3% across all datasets, with top-1 accuracy of 79.2% on ImageNet-1K
- Exceptional classification extrapolation: improves accuracy by up to 10.5% over next-best method when scaling from 224² to 1024² resolution
- Modest computational overhead (13-27% inference time increase) while providing significant accuracy gains

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Distance Decay
PaPE imposes a learnable, token-specific decay on attention as spatial distance increases, biasing vision transformers toward local interactions while retaining the ability to attenuate this bias when beneficial. For each query token xi, PaPE computes ai = -softplus(Wa · xi), yielding a strictly negative coefficient vector. The attention logit includes ⟨ai, ∆r ⊙2 ij⟩, where ∆rij = Wp · (rj - ri) is a learned projection of the relative position. Since ai < 0, the squared term penalizes larger distances, and the penalty is stronger when ai has larger magnitude. The content-dependent ai allows the model to increase decay for tokens that should focus locally (e.g., texture) and reduce it for tokens requiring global context (e.g., scene-level features).

### Mechanism 2: Directional Modulation via Linear Relative-Position Terms
PaPE injects direction-sensitivity into attention by adding a linear term that amplifies or attenuates attention based on the alignment between a learned direction vector (derived from token content) and the relative position vector. For each query xi, PaPE computes bi = Wb · xi. The attention logit includes ⟨bi, ∆rij⟩. The dot product is maximized when ∆rij is aligned with bi (cosine similarity = 1) and minimized when anti-aligned. This means each token can learn to "look" preferentially in specific directions (e.g., upward for sky, downward for ground). Because bi is computed from xi, the preferred direction is context-aware.

### Mechanism 3: Translation-Invariant Relative Position with Kernel-Compatible Factorization
PaPE encodes purely relative positions (ensuring translation invariance) while factorizing the computation into separate query and key transformations, preserving compatibility with efficient attention kernels like FlashAttention. The full PaPE attention logit Sij = ⟨ai, ∆r ⊙2 ij⟩ + ⟨bi, ∆rij⟩ + ⟨qi, kj⟩ depends only on ∆rij = Wp(rj - ri), satisfying translation invariance. To avoid materializing pairwise relative positions, PaPE defines transformed query and key vectors fq(qi, ri) and fk(kj, rj) such that ⟨fq(qi, ri), fk(kj, rj)⟩ exactly equals Sij. This is achieved by concatenating auxiliary dimensions encoding squared positions, linear positions, and coefficients, allowing each position-dependent term to emerge from the dot product of independently transformed vectors.

## Foundational Learning

- **Concept: Self-Attention with Queries, Keys, Values**
  - Why needed: PaPE modifies the attention logit Sij = ⟨qi, kj⟩ by adding position-dependent terms. Understanding the baseline dot-product attention is necessary to interpret what PaPE adds.
  - Quick check: Given tokens xi and xj, can you compute the attention logit before and after adding a bias term based on their distance?

- **Concept: Translation Invariance vs. Absolute Position Encoding**
  - Why needed: PaPE is explicitly designed to be translation invariant, in contrast to sinusoidal encodings that embed absolute positions. This distinction explains PaPE's strong performance on detection tasks where object location should not affect recognition.
  - Quick check: If you shift all token positions by a constant offset, does a sinusoidal position encoding produce the same attention pattern? Does PaPE?

- **Concept: Efficient Attention Kernels (FlashAttention) and Materialization Constraints**
  - Why needed: Naive attention bias methods require materializing the full N×N attention matrix, which FlashAttention avoids. PaPE's query-key transformation is specifically designed to work within this constraint.
  - Quick check: Why can't standard ALiBi (which subtracts a precomputed distance matrix) be used directly with FlashAttention?

## Architecture Onboarding

- **Component map:**
  - Wp ∈ R^{m×p}: Projects raw relative positions (p-dimensional) into a learned m-dimensional space
  - Wa ∈ R^{m×d}, Wb ∈ R^{m×d}: Project token embeddings (d-dimensional) into coefficient vectors ai (distance decay) and bi (direction)
  - softplus(-) activation on ai: Ensures all ai entries are strictly negative, guaranteeing concave parabolas and distance decay behavior
  - Query transform fq (Eq. 13): Concatenates [qi, ⟨ai, s⊙2i⟩, ai, -2ai⊙si, ⟨-bi, si⟩, bi] where si = Wp·ri
  - Key transform fk (Eq. 14): Concatenates [kj, 1, s⊙2j, sj, 1, sj]
  - Effective head dimension expansion: From h to h + 3m + 2

- **Critical path:**
  1. During forward pass, compute ai = -softplus(Wa·xi), bi = Wb·xi for each token
  2. For each position ri, compute si = Wp·ri
  3. Construct transformed queries fq and keys fk using concatenation (Equations 13-14)
  4. Pass fq, fk to standard attention kernel (no modification to kernel required)
  5. The dot product ⟨fq, fk⟩ implicitly computes the PaPE attention logit

- **Design tradeoffs:**
  - m (projection dimension): Higher m increases expressivity but adds parameters and increases head dimension. Table 3 shows saturation at m ≥ 8
  - PaPE vs. PaPE-RI: PaPE-RI enforces rotation invariance by setting bi=0, ai=αi (scalar), Wp=wp·I. This sacrifices directionality
  - Inference overhead: 13-27% relative increase in inference time (Table 4)

- **Failure signatures:**
  - Accuracy collapse at high resolutions without position interpolation: Table 5 shows nD-sincos drops to 9.3% at 1024²
  - Negative impact of position interpolation: Table 6 shows PaPE accuracy drops with interpolation (e.g., -20.1 at 1024²)
  - Rotation invariance underperformance: Table 1 shows PaPE-RI underperforms PaPE on most datasets

- **First 3 experiments:**
  1. Validate implementation correctness on a small dataset by computing attention logits via both the explicit Sij formula and the factorized ⟨fq, fk⟩ dot product
  2. Ablation of m on target dataset by sweeping m ∈ {2, 4, 8, 16, 32} on a validation set
  3. Extrapolation stress test by training at resolution 224², evaluating at 384² and 512² without any model modification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single position encoding mechanism jointly achieve both rotation invariance and directionality?
- Basis in paper: The authors state in Section 6, "we want to investigate whether rotation invariance and directionality can be jointly achieved, since the derivation of PaPE-RI indicates that one can only get one."
- Why unresolved: In the current PaPE formulation, enforcing rotation invariance (PaPE-RI) requires constraining the directional coefficients (bi) to zero, thereby explicitly sacrificing directionality.
- What evidence would resolve it: A theoretical extension or modification of the PaPE equations that satisfies the rotation invariance constraint while retaining non-zero directional components, validated on datasets requiring both properties.

### Open Question 2
- Question: Do higher-order multivariate polynomials offer superior inductive biases for vision transformers compared to the proposed parabolic (quadratic) formulation?
- Basis in paper: The authors note in Section 6 that "PaPE can be viewed as the first instance of a larger family of multivariate polynomials... we are eager to investigate other members of this function family in future work."
- Why unresolved: The paper restricts the attention bias to a parabola (quadratic). It remains untested whether theoretical generalizations to higher-degree polynomials capture more complex spatial relationships or simply introduce overfitting/instability.
- What evidence would resolve it: Empirical comparisons of attention bias using cubic or higher-order polynomial terms against standard PaPE across the 8 evaluated datasets.

### Open Question 3
- Question: Can the dimensionality parameter m be minimized (or the projection Wp removed) without degrading performance to improve efficiency?
- Basis in paper: The authors identify in Section 6 that "PaPE's main limitation is its extra inference cost" and suggest, "Future work will focus on how to lower m in practice, or removing Wp altogether and thereby keeping m=p."
- Why unresolved: The ablation study (Table 4) shows a trade-off where lower m generally reduces accuracy, but it is unknown if architectural changes could decouple this link.
- What evidence would resolve it: The discovery of an efficient approximation or regularization technique that allows PaPE to match reported performance of m=50 using the minimal value of m=p.

### Open Question 4
- Question: Does PaPE's context-aware attention bias improve the efficiency and accuracy of dynamic token selection mechanisms?
- Basis in paper: The authors propose in Section 6, "A complementary direction is to apply PaPE to dynamic token selection. This has the potential to increase efficiency by reducing the number of attention dot products."
- Why unresolved: The current evaluation uses static token sets. It is unclear if the learned distance decay and directionality in PaPE correlate well with the pruning decisions made by dynamic token selection methods.
- What evidence would resolve it: Experiments integrating PaPE into a dynamic ViT architecture (e.g., token merging/pruning) showing improved retention of critical tokens compared to standard position encodings.

## Limitations
- The primary uncertainty lies in the generalizability of PaPE across diverse vision architectures beyond the ViT-B/16 backbone tested in the paper
- Computational overhead (13-27% inference time increase) could be prohibitive for real-time applications
- Evaluation focuses on classification tasks, leaving open questions about PaPE's effectiveness for dense prediction tasks like semantic segmentation or object detection

## Confidence

- **High confidence**: PaPE achieves state-of-the-art performance across multiple vision modalities (7/8 datasets) and demonstrates superior classification extrapolation capabilities at higher resolutions compared to baseline position encodings
- **Medium confidence**: The context-aware distance decay and directional modulation mechanisms provide meaningful inductive biases for vision tasks
- **Low confidence**: The claim that PaPE is "vision-centric" and fundamentally better suited to vision tasks than NLP-derived position encodings

## Next Checks

1. **Architecture Transferability Test**: Implement PaPE in a Swin Transformer backbone and evaluate on COCO object detection and ADE20K semantic segmentation to validate whether PaPE's benefits extend beyond classification to dense prediction tasks.

2. **Efficiency Benchmarking Under Constraints**: Profile PaPE's memory and compute usage when integrated with FlashAttention kernels, measuring the actual overhead on GPU/CPU across different batch sizes and sequence lengths to quantify the practical cost-benefit tradeoff.

3. **Representation Interpretability Analysis**: Visualize the learned ai and bi parameter distributions across different vision modalities and correlate these with specific visual features (e.g., texture vs. structure, foreground vs. background) to provide evidence for whether PaPE truly learns vision-specific positional patterns.