---
ver: rpa2
title: 'Post-Training as Reweighting: A Stochastic View of Reasoning Trajectories
  in Language Models'
arxiv_id: '2511.07368'
source_url: https://arxiv.org/abs/2511.07368
tags:
- cots
- reasoning
- easy
- arxiv
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework to analyze post-training
  methods for large language models, particularly focusing on the tension between
  exploration and overfitting. The authors model reasoning processes as tree-structured
  Markov chains, where pretraining discovers the reasoning structure and post-training
  reweights existing reasoning chains.
---

# Post-Training as Reweighting: A Stochastic View of Reasoning Trajectories in Language Models

## Quick Facts
- arXiv ID: 2511.07368
- Source URL: https://arxiv.org/abs/2511.07368
- Reference count: 40
- This paper provides a theoretical framework analyzing post-training methods for LLMs, showing they exhibit a "simplicity bias" that favors easy reasoning paths over rare but crucial ones.

## Executive Summary
This paper develops a theoretical framework to analyze post-training methods for large language models, particularly focusing on the tension between exploration and overfitting during reasoning task fine-tuning. The authors model reasoning processes as tree-structured Markov chains (TMCs), where pretraining discovers the reasoning structure and post-training reweights existing reasoning chains. They prove that both reinforcement learning with verifiable rewards (RLVR) and inference-time reward aggregation methods systematically favor high-probability (easy) reasoning paths over rare but potentially crucial reasoning paths needed for hard problems. The theoretical analysis shows that exploration-oriented mechanisms like rejecting easy instances and applying KL regularization help preserve these rare reasoning paths, maintaining better multi-task capability while standard RL fine-tuning methods tend to forget reasoning patterns for secondary tasks.

## Method Summary
The paper models reasoning as transitions in a Tree-structured Markov Chain (TMC), where each state represents a reasoning step and transitions represent possible next steps. The base model (pretrained) discovers valid reasoning chains, and post-training reweights these chains based on task-specific rewards. The authors analyze six fine-tuning methods (REINFORCE, RAFT, PPO, RL-rej, GRPO, GRPO-KL) and five inference methods (Soft-BoN, ORM-BoN, PRM-BoN, DPRM-BoN, DPRM-AS) through theoretical proofs and empirical simulations on abstract TMCs. The framework proves that standard RLVR methods exhibit a "simplicity bias" that suppresses rare but valid reasoning paths, while KL-regularized methods can preserve them.

## Key Results
- Standard RLVR methods (REINFORCE, RAFT, PPO, GRPO without KL) systematically suppress rare but valid reasoning paths, causing their probability to approach zero after sufficient iterations.
- Population reward consistency bias causes inference-time methods like Best-of-N to over-select easy paths even when they're wrong for the specific instance.
- KL regularization with appropriate temperature β preserves rare reasoning paths and maintains multi-task capability while still improving performance on the target task.
- Empirical simulations on abstract TMCs support the theoretical findings, demonstrating that diversity-promoting methods maintain better multi-task capability.

## Why This Works (Mechanism)

### Mechanism 1: Advantage-Driven Squeezing Effect
Standard RLVR methods systematically suppress rare but valid reasoning paths by reinforcing already high-probability transitions. At each gradient step, easy-to-reason transitions receive positive logit updates while hard-to-reason transitions receive negative updates, creating a compounding gap where easy paths get stronger and rare paths decay toward zero probability. The advantage function drives this—easy paths have higher expected correctness, thus higher advantage.

### Mechanism 2: Population Reward Consistency Bias
ORM and likelihood-based PRM scores check consistency (frequency) rather than correctness, causing inference-time methods like Best-of-N to over-select easy paths even when they're wrong for the specific instance. Both R_out(o) = E[R(o)] and R_likelihood(ol) = E[R(o)|ol] are expectation-based Bayes-optimal L2 predictors, and since easy-to-reason CoTs have higher population correctness, expectation-based rewards assign them higher scores.

### Mechanism 3: KL Regularization as Probability Redistribution
Adding KL divergence penalty to the RL objective preserves rare reasoning paths by preventing the policy from deviating too far from the base model's distribution. The KL-regularized objective has a Gibbs distribution solution: p_new(o_{l+1}|o_l) ∝ p_base(o_{l+1}|o_l) · exp(A(o_l, o_{l+1})/β). Temperature β controls trade-off: larger β reduces the gap between high and low advantage transitions, ensuring even low-advantage paths retain non-negligible probability.

## Foundational Learning

- **Concept: Markov Chains and Transition Kernels**
  - Why needed here: The entire theoretical framework models reasoning as transitions between states with probabilistic edges. Understanding P(o_{l+1}|o_l), stationary distributions, and mixing times is essential for following the theorems.
  - Quick check question: If a 3-state Markov chain has transition matrix P = [[0.5, 0.3, 0.2], [0.1, 0.6, 0.3], [0.4, 0.1, 0.5]], what's the probability of trajectory 1→2→3?

- **Concept: Policy Gradient and Advantage Functions**
  - Why needed here: The "squeezing effect" is fundamentally driven by how gradient updates depend on the advantage function A(s,a) = Q(s,a) - V(s). Understanding why positive advantage increases action probability and why easy paths have systematically higher advantage is core to the mechanism.
  - Quick check question: In REINFORCE, if trajectory τ has reward R(τ)=1 and log-probability log π(τ)=-2, what's the gradient direction for π(τ)?

- **Concept: KL Divergence and Gibbs Distributions**
  - Why needed here: The solution to KL-regularized optimization is a Gibbs/tilted distribution. Understanding how temperature controls the sharpness of this distribution, and why KL divergence measures distributional distance, explains the antidote mechanism.
  - Quick check question: If base distribution p=[0.7, 0.2, 0.1] and rewards r=[0, 1, 2], what's the Gibbs distribution with β=1? How does it change when β=0.1?

## Architecture Onboarding

- **Component map:**
  Base Model (θ*) -> Task Specification -> Reward Oracle -> Finetuning Loop (RLVR) -> [Optional] KL Penalty -> Inference

- **Critical path:** The post-training pipeline has two critical junctures: (1) During RLVR, the gradient update direction determines whether rare paths are suppressed or preserved—this is where the squeezing occurs. (2) During inference, whether the reward signal checks consistency (population) vs. correctness (instance-specific) determines whether easy paths dominate selection.

- **Design tradeoffs:**
  - KL weight β: Higher β preserves diversity and multi-task capability but reduces task-specific optimization; lower β improves target task accuracy but risks forgetting rare paths.
  - Sample-based vs. population reward: Instance-specific R_{Q,A}(o) is accurate but requires running verifiers for each sample; population R_out(o) is cheaper but biased toward easy paths.
  - Softmax temperature λ in inference: Controls exploration-exploitation in reward-guided sampling; λ→∞ approaches argmax (greedy), λ→0 approaches base model distribution.

- **Failure signatures:**
  1. Pass@K plateaus despite more RL steps: Indicates squeezing has converged—all probability mass is on easy paths, rare paths have been forgotten.
  2. High accuracy on easy instances, near-zero on hard instances: Signature of simplicity bias.
  3. Secondary task performance crashes during fine-tuning: Multi-task forgetting.

- **First 3 experiments:**
  1. Measure squeezing dynamics: Fine-tune on a controlled TMC with both easy and hard valid CoTs. Track probability of hard CoTs every 100 steps under REINFORCE, RAFT, and KL-regularized GRPO.
  2. Validate advantage gap: For a shared state o_l with both easy (high-prob) and hard (low-prob) children, estimate A(o_l, o_{l+1}) via Monte Carlo rollouts from the base model.
  3. Test exploration mechanisms: Compare standard PPO, PPO with easy-instance rejection, and KL-regularized PPO on a multi-task TMC measuring pass@K on both primary and secondary tasks.

## Open Questions the Paper Calls Out

- **Extending to reflective behavior and aha moments**: How to apply TMC analysis to reflective behavior and aha moments.
- **Variable depth reasoning models**: Generalizing the framework to more realistic reasoning models with variable depths.
- **Interaction and correlation of correctness**: Understanding how correctness across different CoTs with varying difficulty levels affects theoretical bounds.
- **Non-linear transformer dynamics**: Extending analysis from linear softmax models to non-linear Transformers given structural coupling in representation space.

## Limitations
- Theoretical framework assumes perfect knowledge of base model transition probabilities and exact advantage computation.
- Linear softmax predictor model with decoupled state representations may not capture all nuances of actual transformer-based language models.
- Simulation experiments use abstract TMCs with simplified reward structures that may not fully generalize to real-world reasoning tasks.

## Confidence
- **High confidence**: The core mechanism of advantage-driven squeezing and its theoretical proof showing hard CoTs' probability → 0 under standard RLVR.
- **Medium confidence**: The population reward consistency bias - theoretical analysis is sound but empirical validation in real LLM post-training scenarios is limited.
- **Medium confidence**: KL regularization as antidote - theoretical guarantees exist but practical effectiveness depends heavily on choosing correct β threshold.

## Next Checks
1. Implement the theoretical mechanisms on an actual LLM reasoning dataset (e.g., GSM8K or MATH) to verify that standard RLVR methods exhibit the predicted squeezing effect on rare but valid reasoning paths.
2. Design a multi-task experiment where tasks share some reasoning patterns but have distinct hard instances to measure whether standard RLVR causes catastrophic forgetting while KL-regularized methods preserve reasoning patterns.
3. Compare performance of ORM/PRM methods using instance-specific ground-truth rewards versus population-based rewards on a reasoning benchmark to validate whether consistency bias leads to systematic errors.