---
ver: rpa2
title: Lingua Custodi's participation at the WMT 2025 Terminology shared task
arxiv_id: '2510.17504'
source_url: https://arxiv.org/abs/2510.17504
tags:
- terminology
- translation
- grpo
- language
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Lingua Custodia's approach to the WMT 2025
  Terminology shared task, focusing on improving machine translation systems' ability
  to correctly translate specified terminology. The authors propose a two-stage fine-tuning
  process for large language models, starting with supervised fine-tuning (SFT) on
  parallel data augmented with explicit terminology constraints, followed by Group
  Relative Policy Optimization (GRPO) with a custom reward function that encourages
  correct terminology usage while preserving general translation quality.
---

# Lingua Custodi's participation at the WMT 2025 Terminology shared task

## Quick Facts
- arXiv ID: 2510.17504
- Source URL: https://arxiv.org/abs/2510.17504
- Reference count: 8
- Primary result: Achieves ≥10 points terminology accuracy gain over baselines on EN→DE/ES/RU language pairs using 4B parameter LLMs

## Executive Summary
This paper presents Lingua Custodia's approach to the WMT 2025 Terminology shared task, focusing on improving machine translation systems' ability to correctly translate specified terminology. The authors propose a two-stage fine-tuning process for large language models, starting with supervised fine-tuning (SFT) on parallel data augmented with explicit terminology constraints, followed by Group Relative Policy Optimization (GRPO) with a custom reward function that encourages correct terminology usage while preserving general translation quality. They employ two 4B parameter instruction-tuned LLMs (Qwen3-4B and Gemma3-4B-it) as base models and create instruction data by annotating parallel sentences with terminology constraints extracted from bilingual dictionaries. The GRPO reward combines BLEU-based translation quality scores with terminology adherence metrics. Experimental results show that the proposed approach achieves ≥10 points of accuracy gain in term match compared to baseline models, with GRPO further improving terminology adherence while maintaining competitive Comet-Kiwi translation quality scores.

## Method Summary
The approach uses two 4B parameter instruction-tuned LLMs (Qwen3-4B and Gemma3-4B-it) trained in a two-stage process. First, supervised fine-tuning is performed on 10,000 parallel sentences per language pair, augmented with explicit terminology constraints extracted from bilingual dictionaries and formatted as instruction templates. Second, GRPO fine-tuning uses 1,000 parallel sentences per pair with 16 model completions per sample, optimizing a reward function that combines normalized BLEU scores with terminology match proportions. Training uses AdamW optimizer with learning rate 5e-6, batch size 16/device for SFT, and 8/device for GRPO, with data filtered by LaBSE similarity threshold of 0.9.

## Key Results
- Achieves ≥10 points terminology accuracy gain over baseline models
- GRPO phase produces substantial boost in term accuracy, achieving over 0.95 term accuracy in some settings
- Maintains competitive Comet-Kiwi translation quality scores despite terminology focus
- Qwen3 shows higher term accuracy gains but larger Comet drops post-GRPO compared to Gemma3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Terminology-aware SFT establishes instruction-following behavior for lexical constraints.
- Mechanism: Parallel sentences are matched against a bilingual terminology dictionary; each training sample includes explicit instruction templates (e.g., "Translate... adhering to the specified terminology: [mapping_list]"). The model learns to condition its output on provided term mappings through standard next-token prediction on annotated data.
- Core assumption: The terminology dictionary coverage is sufficient; instruction templates generalize across phrasings.
- Evidence anchors:
  - [abstract]: "In the first stage, we perform supervised fine-tuning (SFT) on parallel data augmented with explicit terminology constraints."
  - [section 2.2]: "To construct the final training set with terminology control, each parallel sentence is parsed using the dictionary, one instruction template is randomly selected."
  - [corpus]: DuTerm (arXiv:2511.07461) similarly uses terminology-aware fine-tuning with synthetic data, suggesting this pattern is emerging but not yet definitive.
- Break condition: Dictionary has poor coverage for domain-specific terms; instruction templates overfit to specific phrasings.

### Mechanism 2
- Claim: GRPO with composite rewards amplifies terminology adherence beyond SFT alone.
- Mechanism: For each sample, 16 completions are generated; rewards combine normalized BLEU (R_BLEU) and terminology match proportion (R_term). GRPO optimizes policy relative to group performance, pushing the model toward higher-reward outputs without requiring a separate reward model.
- Core assumption: The reward function accurately captures desired behavior; BLEU and term adherence are not fundamentally conflicting.
- Evidence anchors:
  - [section 2.3]: "These rewards are computed per sample and can be combined in a weighted manner during GRPO training."
  - [section 3.3]: "GRPO phase produces a substantial boost in term accuracy for proper and random terminology conditions."
  - [corpus]: Weak direct corpus evidence for GRPO-specific MT gains; this appears to be a novel application requiring further validation.
- Break condition: Reward hacking (model optimizes term matching at expense of fluency); reward weights poorly calibrated.

### Mechanism 3
- Claim: LaBSE-based filtering reduces noise and improves training signal quality.
- Mechanism: Parallel sentence pairs with LaBSE similarity < 0.9 are discarded before sampling. This removes misaligned translations that would provide conflicting supervision.
- Core assumption: LaBSE similarity correlates with translation quality for these language pairs.
- Evidence anchors:
  - [section 3.1]: "Pairs with a similarity score below 0.9 are discarded."
  - [corpus]: LaBSE is a well-established multilingual embedding model (Feng et al., 2022), but corpus lacks direct evidence linking filtering threshold to terminology task performance.
- Break condition: Threshold too aggressive, discarding valid domain-specific phrasing; threshold too lenient, retaining noisy pairs.

## Foundational Learning

- Concept: **Instruction tuning format (chat templates)**
  - Why needed here: Training data must conform to Qwen3/Gemma3 chat formats; mismatch causes training instability or degraded inference behavior.
  - Quick check question: Can you correctly format a multi-turn conversation in both Qwen3 and Gemma3 tokenizer formats?

- Concept: **Reinforcement learning from relative rewards (GRPO/PPO family)**
  - Why needed here: GRPO differs from PPO; understanding group-relative advantage estimation is required to debug training dynamics.
  - Quick check question: Explain how GRPO computes advantages differently from PPO's absolute value estimation.

- Concept: **BLEU normalization and terminology strict match**
  - Why needed here: Reward function design directly impacts optimization trajectory; understanding scale/range of each reward component is critical for weighting.
  - Quick check question: If BLEU scores range 0-100 and term match is 0-1, what happens if you combine them without normalization?

## Architecture Onboarding

- Component map: CommonCrawl/WMT data → LaBSE filtering → terminology dictionary matching → instruction template application → chat format conversion → SFT training → GRPO training → evaluation
- Critical path: Data quality (LaBSE filtering) → dictionary coverage → instruction diversity → reward calibration. Errors in early stages compound.
- Design tradeoffs:
  - Qwen3 shows higher term accuracy gains but larger COMET drops post-GRPO; Gemma3 maintains better quality/term balance.
  - Equal reward weighting (0.5/0.5) used here; domain-specific tuning likely required.
  - Small training set (10K SFT + 1K GRPO per pair) enables fast iteration but may limit generalization.
- Failure signatures:
  - Term accuracy improves but COMET drops >5 points: reward over-emphasizes term matching, reduce term reward weight.
  - Random terminology setting shows >95% accuracy but proper terminology stays ~60%: instruction following works but dictionary/prompt parsing fails on complex cases.
  - EN→RU consistently underperforms: likely resource/coverage gap, not architecture failure.
- First 3 experiments:
  1. Ablate GRPO: Train SFT-only model; compare term accuracy vs. COMET tradeoff to establish GRPO contribution.
  2. Reward weight sweep: Test [0.3/0.7], [0.5/0.5], [0.7/0.3] BLEU/term weightings on a held-out validation set.
  3. Dictionary coverage audit: Measure what fraction of test-set terminology appears in training dictionary; correlate with per-sample term accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated reward functions for terminology adherence achieve a better trade-off between translation quality and term accuracy than simple weighted BLEU + terminology matching?
- Basis in paper: [explicit] Conclusion states: "investigating more sophisticated reward functions tailored for terminology adherence" as future work.
- Why unresolved: Current equal-weighting approach causes COMET drops (e.g., Qwen3_SFT_GRPO drops to 0.7062 for EN→DE proper terminology vs. 0.7979 baseline).
- What evidence would resolve it: Experiments comparing alternative reward formulations (e.g., learned reward models, curriculum-based weighting, conditional rewards) measuring both term accuracy and translation quality metrics.

### Open Question 2
- Question: How can terminology-aware training be effectively extended to low-resource language pairs where parallel data and terminology dictionaries are scarce?
- Basis in paper: [explicit] Conclusion states: "Future work will explore extending terminology-aware training to additional language pairs and low-resource scenarios."
- Why unresolved: Current method relies on large unsupervised bilingual terminology dictionaries and 10,000+ filtered parallel sentences per language pair.
- What evidence would resolve it: Experiments on low-resource pairs with varying data quantities, comparing dictionary-free or few-shot terminology extraction methods.

### Open Question 3
- Question: Can terminology control be unified with other translation constraints like inline tags in a single multi-task model without performance degradation?
- Basis in paper: [explicit] Conclusion states: "we plan to study the interaction between terminology control and other translation tasks such as inline tags to build an 'all-in-one' multilingual and multi-task translation model."
- Why unresolved: The paper only evaluates terminology control in isolation; potential interference between constraint types is unknown.
- What evidence would resolve it: Joint training experiments with both terminology and inline tag constraints, measuring task-specific and overall performance.

### Open Question 4
- Question: What explains the consistent terminology accuracy gap for EN→RU compared to EN→DE and EN→ES, and is this gap addressable?
- Basis in paper: [inferred] Results show EN→RU achieves only 0.5545 term accuracy (proper) with Qwen3_SFT_GRPO vs. 0.7053 for EN→DE, despite identical training procedures.
- Why unresolved: The paper notes the gap but does not investigate causes (linguistic factors, data quality, dictionary coverage, or model capacity).
- What evidence would resolve it: Ablation studies varying dictionary quality, data sources, and controlled linguistic analyses across language pairs.

## Limitations

- Training duration and convergence criteria are not specified, making it difficult to assess whether results represent full convergence or early stopping.
- Terminology dictionary coverage and quality impact are not quantified, with substantial performance gaps suggesting coverage limitations.
- Reward function calibration is sensitive to base model characteristics and likely requires domain-specific tuning, as evidenced by different COMET trade-offs across models.

## Confidence

**High confidence** - The two-stage fine-tuning framework (SFT followed by GRPO) is technically sound and the methodology for creating instruction-augmented parallel data is clearly specified.

**Medium confidence** - The reported improvements in terminology accuracy (≥10 points) and the effectiveness of GRPO in boosting term adherence are supported by experimental results, though the COMET-Kiwi trade-offs suggest the optimization may not be fully balanced.

**Low confidence** - The exact contribution of each component (LaBSE filtering, dictionary quality, instruction template diversity, GRPO specific hyperparameters) to final performance cannot be isolated from the presented results.

## Next Checks

1. **Ablation study of GRPO contribution** - Train SFT-only models with identical hyperparameters and data splits, then compare terminology accuracy versus COMET-Kiwi scores to quantify GRPO's specific impact beyond supervised fine-tuning alone.

2. **Reward function sensitivity analysis** - Systematically vary the BLEU/term accuracy reward weights across [0.3/0.7], [0.5/0.5], and [0.7/0.3] on a held-out validation set to identify optimal balance points and assess model sensitivity to this hyperparameter.

3. **Dictionary coverage audit** - Measure terminology dictionary coverage on the test set by calculating the percentage of test terminology entries present in the training dictionary, then correlate these coverage metrics with per-sample term accuracy to identify coverage gaps affecting performance.