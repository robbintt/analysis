---
ver: rpa2
title: 'TongSearch-QR: Reinforced Query Reasoning for Retrieval'
arxiv_id: '2506.11603'
source_url: https://arxiv.org/abs/2506.11603
tags:
- reasoning
- query
- retrieval
- https
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TongSearch-QR, a family of small-scale language
  models (7B and 1.5B parameters) specifically trained for query reasoning and rewriting
  in reasoning-intensive retrieval tasks. Traditional IR methods struggle with complex
  queries requiring multi-hop inference or deep semantic understanding.
---

# TongSearch-QR: Reinforced Query Reasoning for Retrieval

## Quick Facts
- arXiv ID: 2506.11603
- Source URL: https://arxiv.org/abs/2506.11603
- Reference count: 18
- TongSearch-QR-7B achieves 27.9 nDCG@10 on BRIGHT, outperforming GPT-4o's 26.5

## Executive Summary
Traditional IR methods struggle with complex queries requiring multi-hop inference or deep semantic understanding. TongSearch-QR addresses this by introducing small-scale language models specifically trained for query reasoning and rewriting in reasoning-intensive retrieval tasks. The system rewrites queries to include reasoning-relevant content before retrieval, improving performance on complex information needs. Using reinforcement learning with a novel semi-rule-based reward function and automatically curated training data from Stack Exchange, TongSearch-QR demonstrates strong performance while maintaining computational efficiency.

## Method Summary
TongSearch-QR employs small language models (7B and 1.5B parameters) trained specifically for query reasoning and rewriting. The models use reinforcement learning with a semi-rule-based reward function that evaluates relevance improvement between original and reasoned queries. Training data is automatically curated from Stack Exchange preferences, eliminating the need for large-scale annotated datasets. The approach focuses on rewriting queries to incorporate reasoning-relevant content before passing them to retrieval systems, enabling better handling of complex queries requiring multi-hop inference or deep semantic understanding.

## Key Results
- TongSearch-QR-7B achieves 27.9 nDCG@10 on BRIGHT benchmark
- Outperforms GPT-4o (26.5 nDCG@10) while being significantly smaller
- Demonstrates strong adaptability across both traditional and reasoning-intensive retrievers

## Why This Works (Mechanism)
TongSearch-QR works by transforming complex queries into forms that are more amenable to effective retrieval. The key insight is that many reasoning-intensive queries contain implicit logical relationships or multi-step inference requirements that standard retrieval systems cannot capture. By rewriting these queries to explicitly include reasoning-relevant content, the system bridges the gap between complex information needs and retrieval capabilities. The reinforcement learning approach allows the model to learn optimal rewriting strategies through trial and error, guided by a reward function that directly measures the improvement in retrieval relevance.

## Foundational Learning
- **Reinforcement Learning with Semi-Rule-Based Rewards**: Needed to train models without requiring expensive human annotations. Quick check: Verify reward function components align with actual retrieval improvements.
- **Query Rewriting for Complex Reasoning**: Required because standard retrieval fails on multi-hop inference queries. Quick check: Test if rewritten queries maintain semantic equivalence while adding reasoning cues.
- **Automatic Training Data Curation**: Essential for scalability without manual annotation costs. Quick check: Validate quality of automatically curated Stack Exchange data for reasoning patterns.

## Architecture Onboarding
**Component Map**: User Query -> TongSearch-QR Model -> Reasoned Query -> Retrieval System -> Results
**Critical Path**: Query input flows through the rewriting model, which applies learned reasoning transformations before passing to the retriever.
**Design Tradeoffs**: Small model size (7B, 1.5B) versus performance, automatic data curation versus potential quality issues, semi-rule-based rewards versus fully learned rewards.
**Failure Signatures**: Poor reasoning on domain-specific queries, degradation on simple queries that don't need rewriting, potential bias from Stack Exchange training data.
**Three First Experiments**:
1. Test performance on queries with varying reasoning complexity levels
2. Compare against baseline retrieval without query rewriting
3. Evaluate adaptation speed when switching between different retriever types

## Open Questions the Paper Calls Out
None

## Limitations
- Semi-rule-based reward function may not capture all nuances of complex reasoning tasks
- Evaluation limited to BRIGHT benchmark, restricting understanding of real-world performance
- No analysis of computational efficiency or latency implications for production deployment

## Confidence
- High confidence: Core architectural design and reinforcement learning methodology
- Medium confidence: Performance improvements over GPT-4o on BRIGHT benchmark
- Medium confidence: Claims of adaptability across different retriever types

## Next Checks
1. Evaluate TongSearch-QR performance on multiple reasoning-intensive retrieval benchmarks beyond BRIGHT to assess generalizability.
2. Conduct systematic ablation experiments to quantify the contribution of semi-rule-based reward function components.
3. Measure end-to-end system performance including query rewriting latency and computational overhead in production deployments.