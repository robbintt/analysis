---
ver: rpa2
title: Deep Learning to Automate Parameter Extraction and Model Fitting of Two-Dimensional
  Transistors
arxiv_id: '2507.05134'
source_url: https://arxiv.org/abs/2507.05134
tags:
- network
- training
- neural
- parameters
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work develops a deep learning approach to extract physical\
  \ parameters of two-dimensional (2D) transistors from electrical measurements, enabling\
  \ automated parameter extraction and technology computer-aided design (TCAD) fitting.\
  \ The method uses a secondary neural network to approximate a physics-based device\
  \ simulator, enabling high-quality fits after training on ~500 devices, a factor\
  \ 40\xD7 fewer than other recent efforts."
---

# Deep Learning to Automate Parameter Extraction and Model Fitting of Two-Dimensional Transistors

## Quick Facts
- **arXiv ID**: 2507.05134
- **Source URL**: https://arxiv.org/abs/2507.05134
- **Reference count**: 40
- **Key outcome**: Deep learning approach extracts physical parameters from 2D transistor I-V measurements with median R² = 0.99 using ~500 devices (40× fewer than prior methods)

## Executive Summary
This work presents a deep learning approach to automatically extract physical parameters from electrical measurements of two-dimensional transistors. The method uses a secondary neural network to approximate a physics-based device simulator, enabling high-quality fits after training on ~500 devices—a factor >40× fewer than other recent efforts. The approach achieves a median coefficient of determination (R²) = 0.99 when fitting measured electrical data from experimental monolayer WS₂ transistors. The method generalizes well and scales to fitting 35 parameters simultaneously for high-electron-mobility transistors, significantly reducing the number of computationally expensive TCAD simulations needed.

## Method Summary
The method trains a forward neural network to approximate the physics-based TCAD simulator, then uses this surrogate to generate abundant training data for the inverse network. The inverse network takes I-V curves as input and predicts physical parameters using a tandem architecture with current-space error in the loss function. Physics-guided feature engineering provides derivatives of current with respect to voltage as additional inputs. The approach is validated on experimental monolayer WS₂ transistors and shows excellent performance with median R² = 0.99 for parameter extraction.

## Key Results
- Achieves median R² = 0.99 when fitting experimental monolayer WS₂ transistor data
- Reduces required physics-based training set by >40× through surrogate pre-training
- Scales to simultaneous fitting of 35 parameters for high-electron-mobility transistors
- Extracts parameters including mobility, Schottky barrier height, and defect profiles from I-V measurements

## Why This Works (Mechanism)

### Mechanism 1: Surrogate-Based Data Augmentation
Pre-training the inverse network on surrogate-generated data reduces the required physics-based training set by >40×. A forward neural network is first trained to approximate the expensive TCAD simulator (taking ~1 min/simulation). This surrogate can then generate 100,000 I-V curves in <30 seconds. The inverse network learns the general input-output mapping from this abundant cheap data before fine-tuning on scarce physics-based data. Core assumption: The forward surrogate learns a sufficiently accurate approximation of the TCAD physics that pre-training transfers meaningfully to the fine-tuning task. Break condition: If the forward surrogate has poor accuracy (low R² on held-out TCAD data), the augmented data will embed systematic errors.

### Mechanism 2: Tandem Loss with Current-Space Error
Training the inverse network via a tandem architecture (passing predicted parameters through a forward network) produces better fits than training on parameter-error alone. The inverse problem is non-unique—multiple parameter sets can yield near-identical I-V curves. By including current-reconstruction error E_Id in the loss function, the network is penalized for parameter combinations that produce poor curve fits even if individual parameter errors appear acceptable. Core assumption: The forward network accurately reconstructs current from parameters during training. Break condition: If the forward surrogate is poorly calibrated, the tandem loss will optimize for the wrong objective.

### Mechanism 3: Physics-Guided Feature Engineering
Explicitly providing logarithmic current and voltage derivatives as input features improves extraction accuracy for mobility and Schottky barrier height. Mobility is conventionally extracted from transconductance g_m = ∂I_d/∂V_gs; subthreshold swing (from ∂log₁₀I_d/∂V_gs) correlates with defect profiles. By pre-computing these derivatives, the network receives disentangled physical signatures rather than needing to learn them implicitly from raw curves. Core assumption: The derivatives computed from noisy or interpolated experimental data remain physically meaningful. Break condition: If experimental noise is high, numerical derivatives become unstable and may harm rather than help extraction.

## Foundational Learning

- **Inverse problems and non-uniqueness**
  - Why needed here: The core challenge is that multiple parameter sets can produce identical I-V curves, making naive regression ill-posed.
  - Quick check question: Can you explain why adding current-space error to the loss function mitigates non-uniqueness better than parameter-space error alone?

- **Transfer learning via surrogate pre-training**
  - Why needed here: The method's efficiency gain depends on understanding how knowledge transfers from a surrogate model to the target task.
  - Quick check question: What failure mode would indicate that the surrogate is too inaccurate for effective pre-training?

- **TCAD/compact model parameter interpretation**
  - Why needed here: Extracted parameters are only useful if they correspond to physical quantities (mobility, barrier height, defect profiles).
  - Quick check question: Why might a good I-V fit still yield unphysical parameter values?

## Architecture Onboarding

- **Component map**: Forward NN (parameters → I_d) -> Inverse NN (I_d + derivatives → parameters) -> Forward NN (predicted parameters → reconstructed I_d)

- **Critical path**:
  1. Generate ~500+ TCAD simulations (parameter ranges in Table I)
  2. Train forward surrogate to R² ≥ 0.98 median
  3. Generate 100k augmented samples with forward surrogate
  4. Pre-train inverse network on augmented data
  5. Fine-tune inverse network on original TCAD data
  6. Validate on experimental I-V curves

- **Design tradeoffs**:
  - Training set size vs. accuracy: 500 devices gives median R² = 0.99; scaling to 35 parameters requires ~16,000 devices
  - Noise floor handling: Imposed floor of 5×10⁻⁵ μA/μm prevents training on unmeasurable features
  - Parameter range breadth: Wider ranges increase applicability but may reduce per-range accuracy

- **Failure signatures**:
  - Forward surrogate R² < 0.94 on 5th quantile → augmented data unreliable
  - Large scatter in actual vs. predicted plots for defect parameters (N_D0, N_A0) → parameter mutual redundancy causing identifiability issues
  - Schottky barrier errors concentrated near k_B T → physically expected; low barriers are less distinguishable

- **First 3 experiments**:
  1. **Forward surrogate validation**: Train forward network on 500 devices, report median and 5th-quantile R² on held-out TCAD test set. If median < 0.98, increase training size.
  2. **Ablation of pre-training**: Train inverse network with and without surrogate pre-training on identical TCAD subsets; compare 5th-quantile R² to quantify gain (expected ~2× effective training size equivalence).
  3. **Parameter identifiability check**: For each extracted parameter, plot actual vs. predicted on simulated test set; parameters with high scatter (e.g., N_D0) may require additional measurement modalities or constrained ranges.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do noisy or partially corrupted electrical measurements affect the accuracy and robustness of the deep learning parameter extraction method?
- **Basis in paper**: [explicit] The authors state in the Conclusions, "In this work, we have not investigated how noisy or partially corrupted measurements affect the accuracy of this method."
- **Why unresolved**: The current study tests the method on relatively clean experimental data and synthetic data with a manually imposed noise floor, but real-world data often contains significant noise or artifacts that were not specifically analyzed.
- **What evidence would resolve it**: A systematic evaluation of the neural network's performance when applied to experimental datasets with known, varying signal-to-noise ratios or intentionally corrupted input features.

### Open Question 2
- **Question**: Can the neural network approach be generalized to fit devices with varying geometries without requiring the regeneration of training sets and retraining?
- **Basis in paper**: [explicit] The authors identify as a disadvantage that "fitting devices with different geometries (e.g., varying channel length or oxide thickness... require that training sets be regenerated and neural networks be retrained."
- **Why unresolved**: The current implementation relies on fixed device geometries within the training simulation (e.g., a 500 nm channel length), necessitating a new computationally expensive training phase for every distinct device layout.
- **What evidence would resolve it**: Demonstration of a modified architecture or training procedure (e.g., using geometry parameters as additional input features) that successfully extracts parameters from transistors with varying channel lengths or gate configurations using a single trained model.

### Open Question 3
- **Question**: What is the upper bound of parameter dimensionality or model complexity for which this approach remains computationally tractable when using rigorous physics-based simulators?
- **Basis in paper**: [inferred] While the paper successfully scales to 35 parameters using a compact model, it notes that "acquiring such large training sets is often intractable" for emerging devices requiring computationally expensive simulations (minutes to hours per curve), and the "curse of dimensionality" increases data requirements exponentially.
- **Why unresolved**: The study confirms scalability for fast compact models but highlights that the feasibility for rigorous physics-based models (like quantum transport) depends heavily on "hardware limitations and/or limited licenses," leaving the specific limits undefined.
- **What evidence would resolve it**: A study applying the method to a computationally intensive solver (e.g., non-equilibrium Green's function) to determine the maximum number of extractable parameters before the training data generation time becomes prohibitive.

## Limitations
- Forward surrogate accuracy is critical; poor R² (<0.94) propagates systematic errors to inverse network
- Parameter identifiability issues exist for defect parameters (N_D0, N_A0) showing high mutual redundancy
- Requires Sentaurus Device license or careful validation against open-source TCAD alternatives
- Derivative-based feature engineering may fail on noisy experimental data

## Confidence

- **High confidence**: Surrogate pre-training reduces training set size by >40× (supported by explicit runtime comparison and demonstrated R²=0.99 median on experimental data)
- **Medium confidence**: Tandem loss with current-space error improves fits vs. parameter-only training (mechanism explained, but corpus lacks direct validation studies)
- **Medium confidence**: Physics-guided feature engineering (log I_d, derivatives) improves extraction (ablation study provided, but noise sensitivity not quantified)

## Next Checks

1. **Forward surrogate accuracy validation**: Train forward network on 500 devices, report median and 5th-quantile R² on held-out TCAD test set. If median < 0.98, increase training size.

2. **Ablation of pre-training**: Train inverse network with and without surrogate pre-training on identical TCAD subsets; compare 5th-quantile R² to quantify gain (expected ~2× effective training size equivalence).

3. **Parameter identifiability check**: For each extracted parameter, plot actual vs. predicted on simulated test set; parameters with high scatter (e.g., N_D0) may require additional measurement modalities or constrained ranges.