---
ver: rpa2
title: 'CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial
  Knowledge in CLIP'
arxiv_id: '2509.23098'
source_url: https://arxiv.org/abs/2509.23098
tags:
- spatial
- image
- clip
- copatch
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# CoPatch: Zero-Shot Referring Image Segmentation by Leveraging Untapped Spatial Knowledge in CLIP

## Quick Facts
- arXiv ID: 2509.23098
- Source URL: https://arxiv.org/abs/2509.23098
- Authors: Na Min An; Inha Kang; Minhyun Lee; Hyunjung Shim
- Reference count: 40
- Key outcome: None

## Executive Summary
CoPatch is a zero-shot referring image segmentation method that improves spatial grounding by extracting intermediate-layer patch features from CLIP's visual encoder and integrating context tokens from referring expressions. The method introduces Context Token (CT) extraction, which combines global sentence embeddings with local noun+context phrase embeddings, and generates a clustered spatial map (CoMap) by extracting patch features from intermediate layers (e.g., layer 8-10 for ViT-B/16) where spatial structure is better preserved. These innovations enable more precise mask selection without requiring task-specific training, achieving state-of-the-art zero-shot performance on multiple benchmarks.

## Method Summary
CoPatch is a training-free zero-shot referring image segmentation framework that leverages CLIP's vision-language alignment through three key innovations. First, it extracts context tokens from referring expressions using stanza NLP parsing, combining noun phrases with trailing context tokens into hybrid text features. Second, it extracts patch-level image features from intermediate layers of CLIP's visual encoder (e.g., layer 8-10), applying feature negation before layer normalization to correct visualization artifacts. Third, it generates a clustered spatial map (CoMap) by computing cosine similarity between patch features and text embeddings, then clustering high-similarity patches via BFS. The method uses Mask2Former to generate candidate masks, which are reranked based on cluster overlap and spatial coherence metrics. Hyperparameters include exit layer l, threshold δ, and fusion weight α, with specific values per backbone (ViT-B/32: l=10, δ=0.5, α=0.5; ViT-B/16: l=8, δ=0.3, α=0.7; ViT-H/14: l=22, δ=0.5, α=0.5).

## Key Results
- Achieves state-of-the-art zero-shot RIS performance across RefCOCO, RefCOCO+, RefCOCOg, and PhraseCut benchmarks
- Demonstrates 1-2% mIoU improvement over previous zero-shot methods through context token integration and intermediate-layer feature extraction
- Shows consistent performance gains across different CLIP backbones (ViT-B/32, ViT-B/16, ViT-H/14) with backbone-specific hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Context Token Integration in Text Features
The method augments local text features with contextual tokens to improve spatial disambiguation in referring expressions. By extracting both primary noun phrases (N_O) and trailing context tokens (N_C), then concatenating and jointly encoding them with the global sentence features via γ-weighted fusion, the approach preserves spatial cues (e.g., "behind the woman") that prior methods discard. This fusion: E_ConText = γ·E_Sen + (1-γ)·E_Noun maintains spatial discriminative information that CLIP's text encoder can represent meaningfully.

### Mechanism 2: Intermediate-Layer Patch Features Preserve Spatial Structure
Instead of using CLIP's final global pooled output, the method extracts patch embeddings from layer l (e.g., layer 8-10) where spatial structure is better preserved. The approach applies feature negation before layer normalization to correct an "opposite visualization" artifact. This strategy exploits the observation that spatial information degrades progressively toward the final layer due to global pooling objectives, while intermediate layers retain sufficient semantic content for effective similarity computation.

### Mechanism 3: Clustered Spatial Map (CoMap) for Mask Reranking
The method clusters high-similarity patches and reranks candidate masks by cluster overlap to improve selection over raw similarity scores. After computing patch-level cosine similarity map M^l, adjacent patches exceeding threshold δ are clustered via BFS. The resulting CoMap interpolates these clusters and is used to rerank top-k masks, promoting masks overlapping more with clusters. This filtering approach addresses the issue of semantically redundant, overlapping masks that commonly occur in zero-shot settings.

## Foundational Learning

- **Concept: Vision-Language Alignment (CLIP-style)**
  - Why needed here: The entire method operates on CLIP's joint embedding space; understanding how CLIP aligns images and text via contrastive learning is essential.
  - Quick check question: Given an image and two captions ("a dog on the left" vs. "a cat"), which should have higher cosine similarity with the image embedding?

- **Concept: Patch-based Vision Transformers (ViT)**
  - Why needed here: The method extracts intermediate patch embeddings; knowing how ViT processes images into patches and builds spatial representations is required.
  - Quick check question: If a ViT has 14×14 patches for a 224×224 image, what is the receptive field of one patch?

- **Concept: Referring Image Segmentation (RIS) Pipeline**
  - Why needed here: CoPatch operates within the standard zero-shot RIS pipeline (mask proposal → scoring → selection); context is needed to understand where improvements are targeted.
  - Quick check question: In a zero-shot RIS system, why is mask quality from the proposal model an upper bound on final performance?

## Architecture Onboarding

- **Component map:** Referring expression → stanza parser → extract N_O, N_C → concatenate → CLIP text encoder → hybrid fusion with global sentence embedding → E_ConText; Image → CLIP ViT → extract patch embeddings at layer l → negate → layer norm → projection → E_p^l; Compute cosine similarity between each patch and E_ConText → threshold → BFS clustering → interpolate → CoMap; Mask generator (Mask2Former) → candidate masks → compute image-text similarity → top-k selection → rerank using cluster overlap → optional Spatial Coherence (SC) from HybridGL → final mask

- **Critical path:** The exit layer selection (l) and negation operation in the visual path are non-obvious and critical. The negation resolves an "opposite visualization" artifact; without it, high-similarity regions appear inverted.

- **Design tradeoffs:**
  - Exit layer (early vs. late): Earlier layers preserve spatial structure but may lack semantic specificity; later layers provide semantics but lose spatial precision. Paper uses layer 8-10 for ViT-B/16.
  - Threshold δ: Higher values produce fewer, tighter clusters; lower values increase recall but add noise.
  - Mask generator choice: Mask2Former is faster and provides better upper-bound than SAM; FreeSOLO is weaker.

- **Failure signatures:**
  - Top-1 and top-3 gap is large: CoMap clusters are informative but final selection heuristic is weak.
  - Opposite/incorrect activations in spatial map: Negation step likely missing or misapplied.
  - Identical masks for different expressions: Context token extraction failing or N_C empty.

- **First 3 experiments:**
  1. Reproduce Figure 3: Compute pairwise cosine similarity for position-shifted image pairs across all layers to validate intermediate-layer spatial preservation claim.
  2. Ablate negation: Run CoMap with and without patch embedding negation; visualize raw spatial maps to confirm the artifact fix.
  3. Exit layer sweep: On a held-out subset (e.g., 10% of RefCOCOg val), evaluate top-1 and top-3 mIoU for layers 1-11; confirm optimal l aligns with paper (near layer 8-10).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific context tokens within referring expressions provide the most effective spatial guidance for zero-shot RIS?
- Basis in paper: [explicit] Appendix C.5 states, "We leave as future work to discover which of the context tokens within the referring expression could better guide the model for zero-shot RIS tasks."
- Why unresolved: The current method appends all noun/adjective context tokens uniformly without analyzing which linguistic features (e.g., color vs. preposition) drive the observed performance improvements.
- What evidence would resolve it: An ablation study categorizing context tokens by type (e.g., attributes vs. spatial relations) and measuring the marginal IoU gain for each category.

### Open Question 2
- Question: Can an external spatial reasoning module improve final mask selection accuracy among the top-k candidates generated by CoMap?
- Basis in paper: [explicit] Appendix D.1 notes, "Future work could explore an external spatial guide that can better select the final mask among the top candidate masks provided by our CoMap."
- Why unresolved: The current spatial guider (spatial coherence) does not always select the correct candidate, resulting in a performance gap between Top-1 and Top-3 metrics.
- What evidence would resolve it: Integration of a lightweight external spatial resolver and a demonstration of a reduced performance gap between Top-1 and Top-3 selection on RefCOCO.

### Open Question 3
- Question: Can the intermediate-layer spatial map (CoMap) be effectively adapted for tasks requiring finer boundary precision, such as panoptic or instance segmentation?
- Basis in paper: [explicit] Appendix D.1 mentions, "We leave as a future work to extend the application of the presented spatial map into segmentation tasks that often require more refined segmentation."
- Why unresolved: CoMap relies on patch-level features (e.g., 16 × 16), which may lack the resolution necessary for precise object boundaries without additional processing.
- What evidence would resolve it: Application of the method to panoptic segmentation benchmarks with evaluation of boundary F-scores to verify if patch-level cues suffice for fine-grained contours.

## Limitations

- The method shows only modest performance gains (1-2% mIoU) over baselines, and the paper doesn't adequately address whether these improvements are statistically significant or practically meaningful for real-world applications.
- The exact value of the text fusion weight γ is not specified in the paper, creating ambiguity in how much influence the local noun+context embedding has versus the global sentence embedding in the final text representation.
- The negation operation on patch features is presented as a fix for an "opposite visualization" artifact, but the paper doesn't provide sufficient evidence that this artifact exists in the first place or that negation is the correct remedy.

## Confidence

- **High confidence**: The architectural framework (CLIP-based zero-shot RIS pipeline with mask reranking) is well-established and the paper's implementation details are mostly clear and reproducible.
- **Medium confidence**: The intermediate-layer patch feature extraction rationale is supported by related work and internal experiments, but the specific choice of exit layer (8-10) and negation operation lack independent validation.
- **Low confidence**: The claim that context tokens significantly improve spatial disambiguation is weakly supported - the paper shows improved results but doesn't isolate the contribution of context tokens versus other architectural changes.

## Next Checks

1. **Statistical significance testing**: Run bootstrap confidence intervals on the reported mIoU improvements to determine if the 1-2% gains over baselines are statistically significant across benchmark splits.

2. **Context token ablation**: Create a controlled experiment that isolates the contribution of context tokens by comparing CoPatch with CT extraction disabled versus the baseline method, while keeping all other components constant.

3. **Negation operation validation**: Systematically visualize the spatial maps with and without the negation operation across multiple samples to verify that the "opposite visualization" artifact exists and that negation correctly resolves it, rather than introducing new artifacts.