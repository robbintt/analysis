---
ver: rpa2
title: Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving
arxiv_id: '2601.21351'
source_url: https://arxiv.org/abs/2601.21351
tags:
- attention
- optimal
- decode
- batch
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical framework for sizing Attention-FFN
  disaggregation (AFD) systems in large language model serving. The key challenge
  is that Attention workload is non-stationary (growing with context length and continuously
  replenished with new requests) while FFN workload is stable given the batch size.
---

# Theoretically Optimal Attention/FFN Ratios in Disaggregated LLM Serving

## Quick Facts
- arXiv ID: 2601.21351
- Source URL: https://arxiv.org/abs/2601.21351
- Reference count: 15
- Key outcome: Theoretical framework derives optimal Attention-to-FFN ratio in disaggregated LLM serving, validated by simulation showing <10% relative error

## Executive Summary
This paper addresses the critical challenge of sizing disaggregated Attention-FFN (AFD) systems for LLM inference serving. The key insight is that Attention workload is non-stationary—growing with context length and continuously replenished with new requests—while FFN workload remains stable given the batch size. The authors develop a probabilistic workload model capturing these dynamics and derive closed-form expressions for the optimal Attention-to-FFN ratio that maximizes throughput. Using a trace-calibrated AFD simulator, they validate that theoretical optimal ratios match simulation results within 10% relative error across diverse workloads, while consistently reducing idle time.

The framework reveals that the optimal ratio increases with both batch size and context length, and identifies three operating regimes (Attention-bottleneck, Communication-bottleneck, and FFN-bottleneck) with distinct optimality conditions. The theoretical model provides system designers with actionable guidance for provisioning disaggregated LLM serving infrastructure without requiring extensive empirical tuning.

## Method Summary
The authors develop a theoretical framework for optimal Attention-FFN disaggregation by modeling the non-stationary KV-cache workload dynamics. They derive closed-form expressions for the optimal Attention-to-FFN ratio (r*) that maximizes throughput by balancing the continuously growing Attention workload against the stable FFN workload. The model captures workload replenishment patterns where new requests continuously arrive while completed requests are replaced, creating a dynamic equilibrium. Using a trace-calibrated discrete-event simulator, they validate the theoretical predictions by sweeping across different ratios and measuring throughput and idle time metrics.

## Key Results
- Theoretical optimal ratio r* = max(r_A, r_C, r_peak) matches simulation within 10% relative error across diverse workloads
- Optimal ratio increases with batch size (e.g., from r*=5.9 at B=64 to r*=12.5 at B=1024) and context length
- Identified three operating regimes: Attention-bottleneck (r < r_A), Communication-bottleneck (r_A < r < r_C), and FFN-bottleneck (r > r_C)
- AFD consistently reduces idle time by 15-25% compared to monolithic architectures

## Why This Works (Mechanism)
The framework works because it explicitly models the non-stationary nature of KV-cache workloads, where Attention instances must continuously process growing context lengths while FFN instances maintain stable workloads. By deriving optimal ratios that balance these asymmetric workloads, the system maximizes throughput while minimizing the idle time that plagues monolithic architectures. The three-regime characterization provides clear guidance on which bottleneck limits performance at any given ratio.

## Foundational Learning

**KV-cache growth dynamics**: Why needed - Understanding how KV-cache grows with context length is fundamental to modeling Attention workload. Quick check - Verify that Attention latency increases linearly with sequence length in your hardware.

**Geometric decode length distribution**: Why needed - The probabilistic model assumes decode lengths follow geometric distribution. Quick check - Plot your workload's decode lengths to confirm geometric fit.

**Synchronization barriers in disaggregated systems**: Why needed - Understanding when and how Attention and FFN must synchronize is critical for modeling communication overhead. Quick check - Measure barrier synchronization latency on your target hardware.

**Discrete-event simulation methodology**: Why needed - The validation approach relies on accurate discrete-event simulation of request processing. Quick check - Compare simulation throughput against analytical predictions for simple cases.

## Architecture Onboarding

**Component map**: Attention instances (rA) -> Communication fabric -> FFN instance (1F) -> Output

**Critical path**: Request arrival → KV-cache processing in Attention instances → Cross-device communication → FFN computation → Output synchronization

**Design tradeoffs**: The primary tradeoff is between maximizing Attention parallelism (higher r) and minimizing communication overhead and FFN underutilization. Higher r increases throughput potential but risks creating straggler effects and idle FFN time.

**Failure signatures**: Underutilization occurs when r is too low (Attention bottleneck) or too high (FFN bottleneck with stragglers). Communication becomes the limiting factor when r falls between r_A and r_C.

**Three first experiments**:
1. Implement the analytical solver using parameters from Table 4 to compute theoretical optimal r* for baseline configuration (B=256)
2. Build step-based discrete-event simulator for rA-1F topology and verify throughput peaks near theoretical r* ≈ 9.3
3. Plot idle ratios (η_A vs η_F) to verify crossover at optimal r and identify which bottleneck dominates

## Open Questions the Paper Calls Out

**Hardware generalization**: The theoretical optimal A/F ratio needs validation on real hardware deployments beyond the Huawei Ascend 910C, as current validation is simulation-only using trace-calibrated parameters. Real-system validation is left to future work as AFD implementations mature.

**Load-balancing impact**: Load-balancing strategies can reduce the 15% throughput gap between theory and simulation at large r (e.g., r=32) caused by straggler Attention instances, though some irreducible variance from stochastic dynamics remains.

**Generalized topologies**: The optimal ratio for generalized xA–yF topologies with multiple FFN instances needs characterization, as the current model fundamentally relies on a single shared FFN worker.

## Limitations

- Hardware parameters are presented only as derived latency coefficients rather than raw hardware specs, limiting direct applicability to new hardware
- Simulator uses simplified assumptions about KV-cache growth and geometric decode distributions that may not hold for all workloads
- Synchronization overhead and communication patterns are modeled but not exhaustively validated across diverse hardware topologies

## Confidence

**High confidence**: Theoretical framework derivation, regime identification (Attention/Communication/FFN bottleneck), and optimal ratio trends with batch size/context length

**Medium confidence**: Simulator validation showing <10% error; specific numerical results depend on the trace and hardware model assumptions

**Medium confidence**: Communication-optimal ratio derivation; this requires accurate modeling of bidirectional bandwidth and synchronization overhead

## Next Checks

1. **Cross-hardware validation**: Apply the framework to a different accelerator (e.g., A100/H100) using hardware specs to derive latency parameters, then validate optimal ratio predictions against simulation

2. **Workload trace sensitivity**: Test the framework against non-geometric decode length distributions (e.g., heavy-tailed) to assess robustness of the theoretical predictions

3. **Synchronization overhead measurement**: Instrument the simulator to measure actual barrier synchronization costs and validate the communication model's assumptions about bidirectional bandwidth utilization