---
ver: rpa2
title: Transfer Learning for Temporal Link Prediction
arxiv_id: '2504.10925'
source_url: https://arxiv.org/abs/2504.10925
tags:
- temporal
- graph
- learning
- link
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring temporal link
  prediction models trained on one graph to entirely new graphs with disjoint node
  sets, a scenario not previously considered in the literature. The authors identify
  that state-of-the-art temporal graph networks (TGNs) are inherently limited for
  transfer learning because the majority of their parameters are specific to the training
  graph's node memory.
---

# Transfer Learning for Temporal Link Prediction

## Quick Facts
- arXiv ID: 2504.10925
- Source URL: https://arxiv.org/abs/2504.10925
- Reference count: 40
- Primary result: Proposed structural mapping approach enables zero-shot transfer learning for temporal link prediction across graphs with disjoint node sets

## Executive Summary
This paper addresses a critical gap in temporal link prediction: transferring models between graphs with completely disjoint node sets. The authors identify that state-of-the-art Temporal Graph Networks (TGNs) are fundamentally unsuitable for transfer learning because most parameters are tied to training graph-specific node memory. They propose two solutions: fine-tuning on a fraction of the new graph, and a novel structural mapping approach that learns topological feature embeddings during training. The structural mapping method uses an MLP to map graph topological features to memory embeddings, enabling zero-shot transfer to new graphs. Experiments show this approach achieves comparable or superior performance to fine-tuning while eliminating the need for any target graph data.

## Method Summary
The paper tackles the transfer learning problem where source and target graphs have no overlapping nodes. TGNs traditionally maintain node-specific memory embeddings that cannot transfer to new graphs. The authors propose two solutions: (1) fine-tuning the entire model on a small fraction of the target graph, and (2) a structural mapping module that learns to map topological features (degree, clustering coefficient, PageRank, etc.) to memory embeddings during source graph training. During transfer, this module generates initial memory embeddings for unseen nodes based solely on their topological properties. The approach enables zero-shot transfer learning without requiring any target graph data.

## Key Results
- Structural mapping approach achieves comparable or better transfer performance than fine-tuning on multiple benchmark datasets
- Zero-shot transfer capability demonstrated without requiring any target graph data
- Fine-tuning with only 10% of target graph data provides marginal improvements over structural mapping alone
- Memory initialization through topological features enables effective knowledge transfer between disjoint graphs

## Why This Works (Mechanism)
The structural mapping works because graph topology contains predictive information about node roles and connectivity patterns. By learning to map topological features to memory embeddings during training, the model captures structural regularities that generalize across different graphs. Nodes with similar topological properties tend to exhibit similar temporal behavior patterns, making this mapping effective for transfer learning even when node identities differ completely between source and target graphs.

## Foundational Learning
1. Temporal Graph Networks (TGNs): Why needed - Standard architecture for temporal link prediction; quick check - Memory mechanism stores node states over time
2. Graph Topology Features: Why needed - Provide structural information transferable between graphs; quick check - Degree, clustering, PageRank capture node roles
3. Transfer Learning: Why needed - Enables knowledge reuse across different graph instances; quick check - Fine-tuning vs zero-shot approaches compared

## Architecture Onboarding

Component Map: Source Graph Features -> Structural Mapping Module (MLP) -> Memory Embeddings -> TGN Prediction

Critical Path: During training, the model learns to predict edges while simultaneously learning the mapping from topological features to memory embeddings. During transfer, only the structural mapping module is used to initialize memory for new nodes.

Design Tradeoffs: Zero-shot transfer vs. fine-tuning accuracy (fine-tuning with 10% data provides minimal gains), computational overhead of additional MLP vs. need for transfer capability, choice of topological features vs. representation power.

Failure Signatures: Poor transfer performance when source and target graphs have fundamentally different topologies, degraded accuracy when topological features poorly correlate with temporal behavior, overfitting to source graph structure.

First Experiments: 1) Transfer between graphs with similar topology but different node sets, 2) Transfer between graphs with varying temporal dynamics, 3) Ablation study on importance of different topological features.

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertain generalizability to significantly different graph topologies beyond tested datasets
- Foundation model claims appear premature without testing on truly massive or diverse graph collections
- Computational overhead of structural mapping module during inference not thoroughly analyzed

## Confidence
- High confidence: Core experimental results showing structural mapping outperforms or matches fine-tuning
- Medium confidence: Generalizability claims to other graph types
- Low confidence: Foundation model implications

## Next Checks
1. Test structural mapping approach on graphs with substantially different topological properties (scale-free vs. small-world networks)
2. Conduct ablation studies to quantify computational overhead of structural mapping module
3. Evaluate performance when transferring to graphs with orders of magnitude more nodes than source graph