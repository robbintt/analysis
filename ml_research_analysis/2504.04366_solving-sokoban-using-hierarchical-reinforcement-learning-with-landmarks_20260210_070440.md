---
ver: rpa2
title: Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks
arxiv_id: '2504.04366'
source_url: https://arxiv.org/abs/2504.04366
tags:
- planning
- policy
- sokoban
- training
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces HalfWeg, a novel hierarchical reinforcement
  learning framework that performs top-down recursive planning via learned subgoals,
  successfully applied to the complex combinatorial puzzle game Sokoban. The method
  constructs a six-level policy hierarchy, where each higher-level policy generates
  subgoals for the level below, all learned end-to-end from scratch without domain
  knowledge.
---

# Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks

## Quick Facts
- arXiv ID: 2504.04366
- Source URL: https://arxiv.org/abs/2504.04366
- Authors: Sergey Pastukhov
- Reference count: 9
- Key outcome: 90.2% success rate on Boxoban test puzzles using 6-level hierarchical policy with search augmentation

## Executive Summary
This paper introduces HalfWeg, a novel hierarchical reinforcement learning framework that performs top-down recursive planning via learned subgoals, successfully applied to the complex combinatorial puzzle game Sokoban. The method constructs a six-level policy hierarchy where each higher-level policy generates subgoals for the level below, all learned end-to-end from scratch without domain knowledge. Results show the agent can generate long action sequences from a single high-level call, achieving 90.2% success rate on Boxoban test puzzles when combined with search, with an average solution length of 214.7 steps.

## Method Summary
The framework redefines Sokoban as a generalized planning problem where the agent navigates from state u to target state v with direction flag b ∈ {0,1}. It uses a 6-level hierarchy (PL_0 to PL_5) where PL_0 performs exhaustive search over A^d actions and higher levels output 2^i·d actions via recursive decomposition. Two neural networks are trained: MA (actions) outputs d=4 actions via cross-entropy, and MS (landmarks) outputs target states via MSE. The architecture uses 3×3 conv → ResNet blocks → linear/conv head. Training involves self-play exploration, two-leg search with NDSS=100 subgoals, and SGD updates with refinement rows for better generalization.

## Key Results
- 90.2% success rate on Boxoban test puzzles when combined with search
- Average solution length of 214.7 steps
- Performance depends critically on refinement rows and model capacity (≥64 filters, ≥4 ResNet blocks)

## Why This Works (Mechanism)
The method works by learning to decompose complex planning problems into smaller subproblems through recursive subgoal generation. Each level in the hierarchy learns to generate intermediate landmarks that guide the level below, creating a cascade of increasingly granular planning steps. The two-leg search mechanism allows the agent to explore both toward and away from the current subgoal, enabling it to escape local minima and discover more optimal paths. This top-down recursive planning emerges purely from learning without requiring domain-specific knowledge.

## Foundational Learning
- Hierarchical RL: Why needed - Enables decomposition of long-horizon tasks into manageable subproblems; Quick check - Verify policy hierarchy can generate valid subgoal sequences
- Landmark-based planning: Why needed - Provides intermediate targets for navigation; Quick check - Confirm landmark predictions are reachable states
- Self-play exploration: Why needed - Generates diverse training experiences without human data; Quick check - Ensure exploration covers sufficient state space
- Two-leg search: Why needed - Allows both approach and escape strategies; Quick check - Verify search finds paths in both directions
- Refinement rows: Why needed - Improves generalization by training on intermediate states; Quick check - Confirm training includes intermediate state examples
- Cross-entropy action prediction: Why needed - Handles discrete action selection; Quick check - Verify action probabilities sum to 1

## Architecture Onboarding

Component map: Sokoban env -> State tensor (4 channels) -> MA/MS models -> 6-level policy hierarchy -> Action selection

Critical path: Environment state → Landmark prediction (MS) → Action generation (MA) → Recursive subgoal decomposition → Solution execution

Design tradeoffs: The 6-level hierarchy balances between too shallow (insufficient decomposition) and too deep (vanishing gradients). The choice of 4 actions per MA call and 100 subgoals for search represents a practical compromise between expressiveness and computational cost.

Failure signatures: Early training plateaus indicate undersized models; performance drops without refinement rows suggest overfitting to final states; inability to solve puzzles indicates poor subgoal generation.

First experiments:
1. Train on a single Sokoban puzzle and verify the agent can solve it
2. Test landmark prediction accuracy on held-out states
3. Evaluate subgoal decomposition quality by visualizing intermediate states

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (optimizer type, learning rate, training duration) are underspecified
- 90.2% success rate requires search augmentation, suggesting learned policies alone may be insufficient
- Method only evaluated on Sokoban, limiting claims about broader applicability

## Confidence

**High confidence**: The hierarchical decomposition approach and recursive policy structure are clearly described and validated through ablation studies.

**Medium confidence**: The claimed 90.2% success rate is credible but depends on search augmentation and lacks detailed training hyperparameters.

**Low confidence**: Generalization to other combinatorial domains remains speculative given single-domain evaluation.

## Next Checks

1. Perform hyperparameter sensitivity analysis to determine impact of learning rate, batch size, and model capacity on training stability and final performance.

2. Evaluate trained policies without NDSS=100 subgoal search augmentation to quantify dependence on the search component.

3. Apply the framework to a different combinatorial puzzle (e.g., Rush Hour) with minimal domain-specific modifications to assess true generalizability.