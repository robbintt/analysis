---
ver: rpa2
title: 'General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain
  MRI Tumor Classification'
arxiv_id: '2511.18326'
source_url: https://arxiv.org/abs/2511.18326
tags:
- pretrained
- medical
- datasets
- brain
- tumor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study compared the performance of domain-specific and general-purpose
  pretrained convolutional neural networks for brain tumor classification using small
  MRI datasets. Three models were evaluated: RadImageNet DenseNet121 (domain-specific
  medical pretraining), EfficientNetV2S, and ConvNeXt-Tiny (general-purpose pretraining
  on ImageNet).'
---

# General vs Domain-Specific CNNs: Understanding Pretraining Effects on Brain MRI Tumor Classification

## Quick Facts
- arXiv ID: 2511.18326
- Source URL: https://arxiv.org/abs/2511.18326
- Reference count: 0
- Primary result: ConvNeXt-Tiny (ImageNet pretraining) achieved 93% accuracy vs. 85% (EfficientNetV2S) and 68% (RadImageNet DenseNet121) on brain tumor MRI classification

## Executive Summary
This study systematically compares domain-specific and general-purpose convolutional neural networks for brain tumor classification using small MRI datasets. Three pretrained models were evaluated: RadImageNet DenseNet121 (medical domain-specific), EfficientNetV2S, and ConvNeXt-Tiny (general-purpose ImageNet pretraining). The results demonstrate that ConvNeXt-Tiny achieved the highest accuracy (93%), followed by EfficientNetV2S (85%), while RadImageNet DenseNet121 underperformed significantly at 68%. These findings challenge the conventional wisdom that domain-specific pretraining is superior for medical imaging tasks, suggesting that general-purpose pretraining on large diverse datasets can provide better transfer learning performance when fine-tuning data is limited.

## Method Summary
The study evaluated three convolutional neural networks (DenseNet121 with RadImageNet pretraining, EfficientNetV2S, and ConvNeXt-Tiny with ImageNet pretraining) on a 4-class brain tumor classification task using two combined Kaggle MRI datasets totaling 10,287 images. All models were fine-tuned using a two-phase approach: Phase 1 with frozen base layers (lr=1e-4, max 50 epochs) followed by Phase 2 with partial unfreezing (lr=1e-5, max 10 epochs). Data augmentation included rotation, shift, shear, zoom, and brightness adjustments. Class imbalance was addressed through weighted loss functions. Models were evaluated using accuracy, AUC-ROC, precision-recall curves, and confusion matrices.

## Key Results
- ConvNeXt-Tiny achieved highest accuracy at 93%, significantly outperforming both comparison models
- EfficientNetV2S achieved 85% accuracy, demonstrating solid general-purpose pretraining performance
- RadImageNet DenseNet121 underperformed with 68% accuracy, showing domain-specific pretraining may not generalize well under small-data conditions
- ConvNeXt-Tiny also demonstrated superior AUC (98.5%) and precision-recall metrics

## Why This Works (Mechanism)

### Mechanism 1
General-purpose ImageNet pretraining can outperform domain-specific medical pretraining when fine-tuning data is limited. Large-scale diverse datasets (ImageNet ~1M+ images) may induce more robust low-level feature extractors (edges, textures, shapes) that transfer across domains, whereas domain-specific pretraining on smaller medical datasets may overfit to source-domain specifics that don't fully align with the target task distribution.

### Mechanism 2
Modern architectural innovations (e.g., ConvNeXt's transformer-inspired design) contribute to superior transfer learning independent of pretraining source. ConvNeXt-Tiny incorporates design patterns from vision transformers (layer normalization, larger kernels, GELU activations) which may improve gradient flow and feature reuse during fine-tuning compared to older DenseNet architectures.

### Mechanism 3
Domain-specific pretraining may require larger fine-tuning datasets to realize its advantage. RadImageNet DenseNet121 likely learned features specialized to radiological patterns that require more target-domain samples to adapt effectively; under small-data conditions, the model may underfit or misalign.

## Foundational Learning

- **Transfer Learning (Two-Phase Fine-Tuning)**: Why needed: All three models used frozen-base feature extraction followed by partial unfreezing. Quick check: Can you explain why freezing base layers first prevents catastrophic forgetting during initial training?

- **Data Augmentation for Small Datasets**: Why needed: The study used rotation, shift, shear, zoom, and brightness augmentation to mitigate overfitting on ~10K images. Quick check: Which augmentations are safe for medical MRI (preserve diagnostic features) vs. potentially harmful?

- **Class Imbalance Handling**: Why needed: The dataset had uneven class distribution; class weights were computed to address this. Quick check: How would you verify that class weighting actually improved per-class recall rather than just overall accuracy?

## Architecture Onboarding

- **Component map**: Input pipeline (224×224 RGB resize → preprocess_input → ImageDataGenerator) -> Base models (RadImageNet DenseNet121, EfficientNetV2S, ConvNeXt-Tiny) -> Classification head (custom dense layers) -> Training phases (Phase 1 frozen, Phase 2 partial unfreeze)

- **Critical path**: Load pretrained weights (match exact file) -> Add classification head (4 output classes) -> Train Phase 1 with frozen base, monitor validation accuracy with early stopping -> Unfreeze last N layers -> Fine-tune Phase 2 with reduced learning rate

- **Design tradeoffs**: ConvNeXt-Tiny: Best accuracy (93%) but larger compute; EfficientNetV2S: Good balance (85% accuracy), efficient scaling; RadImageNet DenseNet121: Worst accuracy (68%) but may recover with larger fine-tuning data

- **Failure signatures**: RadImageNet DenseNet121 showed lower accuracy and higher loss + specific confusion between meningioma and glioma; if validation loss diverges while training loss decreases → likely overfitting; if per-class accuracy varies wildly → check class weight computation

- **First 3 experiments**: 1) Reproduce baseline with identical hyperparameters to verify reported accuracies; 2) Pretrain DenseNet121 on ImageNet and compare to isolate architecture vs. pretraining source effects; 3) Gradually increase fine-tuning dataset size for RadImageNet DenseNet121 to identify threshold where domain-specific pretraining becomes competitive

## Open Questions the Paper Calls Out
1. Does the performance gap between domain-specific and general-purpose models persist or reverse when fine-tuning is performed on large-scale datasets?
2. To what extent is the lower performance of RadImageNet DenseNet121 attributable to the DenseNet architecture rather than the domain-specific pretraining?
3. Can hybrid transfer learning strategies that combine domain-specific and general-purpose pretraining outperform single-source approaches?

## Limitations
- Study restricted to small datasets (~10,000 images), leaving scaling behavior of pretraining strategies untested
- Only three specific architectures were evaluated, limiting generalizability to other model families
- Performance differences may be partially attributable to architectural differences rather than pretraining source alone

## Confidence
- **High Confidence**: ConvNeXt-Tiny outperformed both comparison models under tested conditions
- **Medium Confidence**: General-purpose pretraining can outperform domain-specific pretraining for small medical datasets
- **Low Confidence**: Superiority of general-purpose pretraining would hold across all medical imaging tasks or with larger fine-tuning datasets

## Next Checks
1. Test whether DenseNet121 architecture (not just RadImageNet weights) is the limiting factor by comparing with DenseNet121 pretrained on ImageNet
2. Evaluate performance scaling by training RadImageNet DenseNet121 on incrementally larger subsets of the training data
3. Test additional domain-specific pretrained models (e.g., CT-specific, X-ray-specific) on the same brain MRI task to determine if RadImageNet is uniquely suboptimal