---
ver: rpa2
title: 'Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on
  Natural Human Interaction'
arxiv_id: '2512.14865'
source_url: https://arxiv.org/abs/2512.14865
tags:
- audio
- user
- text
- arxiv
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Audio MultiChallenge, a benchmark for evaluating
  end-to-end spoken dialogue systems on natural multi-turn interaction. It extends
  a text-based framework to the audio modality by introducing new axes like Voice
  Editing for mid-utterance speech repairs, and Audio-Cue Inference Memory for recalling
  ambient sounds and paralinguistic signals.
---

# Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction

## Quick Facts
- **arXiv ID:** 2512.14865
- **Source URL:** https://arxiv.org/abs/2512.14865
- **Reference count:** 40
- **Primary result:** 54.65% pass rate on multi-turn audio dialogue tasks; models struggle most with Voice Editing and audio-cue memory.

## Executive Summary
Audio MultiChallenge extends a text-based spoken dialogue evaluation framework to the audio modality, introducing novel axes like Voice Editing for mid-utterance speech repairs and Audio-Cue Inference Memory for recalling ambient sounds and paralinguistic signals. The benchmark features 452 unscripted conversations from 47 speakers (15 hours of audio) with 1,712 atomic rubrics for fine-grained evaluation. Tested models, including Gemini 3 Pro Preview, achieve only 54.65% pass rate overall, with significant drops in Self Coherence over longer contexts and on audio-cue tasks. Models struggle most with Voice Editing and recalling non-semantic audio cues, highlighting gaps in audio-native multi-turn dialogue capabilities.

## Method Summary
Audio MultiChallenge evaluates end-to-end spoken dialogue systems on natural multi-turn audio interactions across four axes: Inference Memory (including Audio-Cue Memory), Instruction Retention, Self Coherence, and Voice Editing. The benchmark uses a hybrid agentic and human-in-the-loop pipeline to generate 452 human-recorded conversations from 47 speakers, totaling 15 hours of audio at 48 kHz, with 1,712 atomic rubrics for fine-grained evaluation. Tested models are evaluated using a fixed-context protocol (pre-seed full conversation history, evaluate only final turn) and graded by an LLM-as-a-judge (o4 Mini) against human-authored atomic rubrics. User turns are provided as base64-encoded audio; assistant turns as raw text.

## Key Results
- Tested models achieve only 54.65% pass rate overall on the Audio MultiChallenge benchmark
- Models show significant drops in Self Coherence over longer contexts
- Voice Editing and Audio-Cue Inference Memory are the most challenging tasks, with 51.47% and 54.65% pass rates respectively

## Why This Works (Mechanism)
The benchmark introduces novel evaluation axes specifically designed for audio dialogue systems, including Voice Editing for mid-utterance speech repairs and Audio-Cue Inference Memory for recalling ambient sounds and paralinguistic signals. By extending a proven text-based framework to the audio modality with atomic rubrics and a fixed-context evaluation protocol, the benchmark provides fine-grained, systematic assessment of model capabilities in natural multi-turn interactions.

## Foundational Learning
- **Atomic rubrics**: Fine-grained evaluation criteria that allow precise measurement of specific capabilities; needed to isolate performance gaps and provide actionable feedback.
- **Fixed-context protocol**: Preloading full conversation history before evaluation; needed to avoid conversation drift and ensure consistent evaluation across turns.
- **LLM-as-a-judge**: Using o4 Mini to automatically grade responses against rubrics; needed for scalable, consistent evaluation across large datasets.
- **Base64 audio encoding**: Converting audio to text-based format for model input; needed to interface with text-based LLM APIs.
- **Voice editing recognition**: Ability to detect and respond to mid-utterance speech corrections; needed for natural human-like dialogue.
- **Audio-cue memory**: Recalling non-semantic audio information (ambient sounds, paralinguistic cues); needed for complete audio dialogue understanding.

## Architecture Onboarding
**Component Map:** Conversation History -> Fixed-Context Input -> Model API -> Text Output -> LLM Judge -> Rubric Scoring -> Pass/Fail Determination

**Critical Path:** User audio input → Base64 encoding → Model context loading → Final turn generation → Rubric evaluation → Pass rate calculation

**Design Tradeoffs:** Fixed-context protocol ensures evaluation consistency but doesn't reflect incremental processing; LLM judging enables scalability but introduces potential bias; atomic rubrics provide precision but require extensive human annotation.

**Failure Signatures:** Conversation drift in sequential evaluation, LLM judge self-enhancement bias, audio perception vs. retention gaps, response length inflation artificially boosting scores.

**3 First Experiments:**
1. Test model on isolated audio cue recognition to distinguish perception from memory issues
2. Compare fixed-context vs. incremental turn-by-turn evaluation on same models
3. Conduct small-scale human evaluation to verify LLM judge agreement on challenging rubrics

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies heavily on automated LLM judging, though human-judge correlations suggest reasonable reliability
- Fixed-context evaluation approach may not reflect realistic deployment scenarios with incremental processing
- Dataset contains only 452 conversations, which may limit statistical power for fine-grained analysis

## Confidence
- **High confidence**: Overall benchmark methodology is sound and well-documented
- **Medium confidence**: Reported model performance gaps appear robust given systematic evaluation approach
- **Medium confidence**: Claim that models struggle most with Voice Editing and audio-cue tasks is supported by data

## Next Checks
1. Conduct small-scale human evaluation (10-15 conversations) to verify LLM judge scores align with human ratings
2. Evaluate one or two models using both fixed-context and incremental turn-by-turn approaches
3. Test evaluated models on isolated audio cue recognition tasks to determine perception vs. memory limitations