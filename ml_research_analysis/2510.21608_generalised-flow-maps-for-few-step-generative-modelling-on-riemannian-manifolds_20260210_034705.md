---
ver: rpa2
title: Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds
arxiv_id: '2510.21608'
source_url: https://arxiv.org/abs/2510.21608
tags:
- flow
- page
- cited
- arxiv
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generalised Flow Maps (GFM), a new class
  of few-step generative models that extend flow-matching methods to arbitrary Riemannian
  manifolds. GFM enables faster inference by learning a global flow map that jumps
  along the trajectory of the probability flow ODE, avoiding costly numerical integration.
---

# Generalised Flow Maps for Few-Step Generative Modelling on Riemannian Manifolds

## Quick Facts
- arXiv ID: 2510.21608
- Source URL: https://arxiv.org/abs/2510.21608
- Reference count: 40
- Primary result: Achieves up to 22× improvement in single-step MMD over flow-matching baselines while maintaining competitive NLL on protein, geospatial, and hyperbolic manifolds

## Executive Summary
This paper introduces Generalised Flow Maps (GFM), a new class of few-step generative models that extend flow-matching methods to arbitrary Riemannian manifolds. GFM enables faster inference by learning a global flow map that jumps along the trajectory of the probability flow ODE, avoiding costly numerical integration. The authors derive three equivalent theoretical conditions (Lagrangian, Eulerian, and semigroup) that characterise GFM and instantiate them with self-distillation-based training methods. On benchmarks including protein torsion angles, geospatial data, and hyperbolic manifolds, GFM achieves state-of-the-art single-step sample quality while maintaining competitive log-likelihoods.

## Method Summary
GFM learns a global flow map X_s,t(x) that maps points between time steps s and t on a Riemannian manifold without numerical integration. The method uses three equivalent characterizations - Lagrangian (tracking trajectories), Eulerian (measuring local changes), and semigroup (propagating initial conditions) - each yielding different self-distillation training objectives. The authors combine these with Riemannian Flow Matching (RFM) to train a neural network that predicts tangent vectors, which are then projected onto the manifold via exponential maps. The approach enables single-step sampling while respecting manifold constraints through geometric primitives like exp_x, log_x, and geodesic distances.

## Key Results
- GFM achieves up to 22× improvement in MMD at 1 NFE compared to flow-matching baselines on Earth and protein datasets
- Maintains competitive log-likelihoods (NLL) across all benchmarks while dramatically improving single-step sample quality
- G-LSD (Lagrangian self-distillation) consistently outperforms Eulerian and Progressive variants across multiple manifolds
- Successfully models data on S², T², SO(3), and hyperbolic Poincaré disk with geodesic-aware distances

## Why This Works (Mechanism)
GFM bypasses the need for iterative ODE integration by learning a global map that directly transports samples between time points. This "one-shot" sampling is possible because the flow map encapsulates the entire trajectory information, allowing the model to jump along the probability flow path. The self-distillation framework ensures the learned map remains consistent with the underlying probability flow ODE while enabling single-step generation.

## Foundational Learning
- **Riemannian manifolds**: Curved spaces where distances and angles are measured intrinsically - needed to handle manifold-constrained data like protein torsions and rotations
- **Exponential/Log maps**: Convert between manifold points and tangent space vectors - required for neural network predictions and projection back to manifold
- **Geodesic distance**: Shortest path between points on curved spaces - used for MMD evaluation instead of Euclidean distance
- **Self-distillation**: Training where model outputs are used as targets - enables learning of flow maps without explicit ODE integration
- **Probability flow ODEs**: Continuous-time generative models using vector fields - provides theoretical foundation for GFM's transport equations

## Architecture Onboarding

**Component map**: Data points → Neural network (f_θ) → Tangent vectors → Exponential map → Manifold samples

**Critical path**: Input sampling on tangent space → Network prediction → Projection to manifold → Geodesic interpolation → Loss computation

**Design tradeoffs**: GFM sacrifices some log-likelihood accuracy for dramatically improved sampling speed; the choice between Lagrangian/Eulerian/Semigroup characterizations involves a speed-accuracy tradeoff

**Failure signatures**: Training collapse without stopgrad terms; high variance on manifolds with non-trivial metrics (hyperbolic); numerical instability in exp/log maps for SO(3)

**First experiments**: 
1. Implement and validate Riemannian primitives (exp/log maps) for S¹, S², T²
2. Train G-LSD on S² volcano dataset and verify single-step MMD improvement
3. Compare GFM vs RFM NLL on Earth dataset to confirm density estimation advantage

## Open Questions the Paper Calls Out
**Open Question 1**: Why does the Generalised Lagrangian self-distillation objective consistently outperform the Eulerian and Progressive variants in sample quality? The paper observes this empirically but provides no theoretical explanation.

**Open Question 2**: Why does the implicit probability flow within GFM yield better log-likelihoods than the explicitly trained RFM vector field? This points to an interesting theoretical direction regarding integral vs differential forms of transport equations.

**Open Question 3**: How can the high training variance of Generalised Eulerian and Mean Flow objectives be mitigated on manifolds with non-trivial metrics like the Poincaré disk? G-ESD and G-MF underfit on hyperbolic data due to variance issues.

## Limitations
- Missing critical hyperparameters: network architecture details, batch size, learning rate, training schedule
- Sampling distribution ambiguity: unclear whether optimal transport coupling or independent sampling was used
- Metric-specific implementation challenges: hyperbolic space requires careful handling of distance functions and tangent projections
- High implementation burden: requires building full Riemannian geometry toolkit for multiple manifolds

## Confidence
- **High confidence**: Theoretical framework (Lagrangian/Eulerian/Semigroup conditions are equivalent), manifold-specific derivations
- **Medium confidence**: Empirical results showing GFM outperforms baselines; methodology for computing MMD and NLL is clear
- **Low confidence**: Reproducibility of exact performance numbers without architecture and training hyperparameters

## Next Checks
1. Implement and validate Riemannian primitives (exp/log maps) for S¹, S², T², SO(3) with roundtrip consistency tests
2. Replicate single-step results on S² volcano dataset using G-LSD variant with default settings
3. Verify the reported 22× improvement in single-step MMD by comparing GFM vs numerical integration baselines at 1 vs 100 NFE