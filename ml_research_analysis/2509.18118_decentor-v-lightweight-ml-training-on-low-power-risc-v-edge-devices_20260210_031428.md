---
ver: rpa2
title: 'Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices'
arxiv_id: '2509.18118'
source_url: https://arxiv.org/abs/2509.18118
tags:
- training
- risc-v
- l-sgd
- learning
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends L-SGD, a lightweight stochastic gradient descent
  optimizer, to RISC-V-based microcontroller units (MCUs), enabling on-device machine
  learning training in resource-constrained environments. It introduces a hybrid quantized
  training strategy that performs most operations in 8-bit fixed-point arithmetic
  while retaining 32-bit floating-point precision only for critical computations,
  addressing numerical stability challenges in quantized training.
---

# Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices

## Quick Facts
- **arXiv ID:** 2509.18118
- **Source URL:** https://arxiv.org/abs/2509.18118
- **Reference count:** 34
- **Primary result:** Achieves 4× memory reduction and 2.2× training speedup on RISC-V MCUs using hybrid int8/float32 quantization with negligible accuracy loss

## Executive Summary
Decentor-V extends L-SGD, a lightweight stochastic gradient descent optimizer, to enable on-device machine learning training on resource-constrained RISC-V microcontroller units (MCUs). The work introduces a hybrid quantization strategy that performs forward pass computations in 8-bit fixed-point arithmetic while retaining 32-bit floating-point precision for backward pass and loss calculations, addressing numerical stability challenges in quantized training. The implementation achieves significant memory and performance improvements on RISC-V platforms compared to 32-bit floating-point baselines while maintaining comparable accuracy across tested datasets.

## Method Summary
The approach combines L-SGD optimization with a hybrid quantization strategy specifically designed for FPU-less RISC-V MCUs like the GAP-8. Forward propagation operations are executed in int8 fixed-point arithmetic, while backward propagation and loss calculations maintain float32 precision to ensure numerical stability. The implementation requires pre-trained float32 weights to initialize the model before quantized fine-tuning, as training from random initialization in int8 leads to gradient saturation. Custom kernels are implemented for efficient int8 operations, including a modified fully connected layer that handles single-neuron output cases and fast approximations for exponential and rounding functions.

## Key Results
- Achieves nearly 4× reduction in memory usage compared to 32-bit floating-point baselines
- Delivers 2.2× speedup in training time on RISC-V platforms
- Maintains negligible accuracy degradation across tested datasets (CogDist and CarEvaluation)
- Demonstrates stable training convergence on both Arm and RISC-V platforms

## Why This Works (Mechanism)
The hybrid quantization approach works by strategically allocating precision where it matters most: using int8 for computationally intensive forward operations to save memory and increase speed, while preserving float32 precision for backward pass calculations where numerical stability is critical. This separation prevents the gradient underflow and overflow issues that typically plague quantized training. The requirement for pre-trained weights ensures that the model starts from a stable point in the parameter space, avoiding the saturation that occurs when attempting to train from scratch in low precision.

## Foundational Learning
- **Hybrid Quantization Strategy:** Why needed - balances memory efficiency with numerical stability. Quick check - verify forward uses int8 while backward uses float32.
- **Pre-trained Weight Initialization:** Why needed - prevents gradient saturation in int8 training. Quick check - confirm weights are loaded before training begins.
- **Custom Kernel Modifications:** Why needed - standard libraries may not handle single-neuron outputs correctly. Quick check - verify accumulator initialization with bias in FC layer.
- **Fast Approximation Functions:** Why needed - efficient implementation of exp and round operations in fixed-point. Quick check - validate type punning/scaling approach.

## Architecture Onboarding
- **Component Map:** Pre-trained float32 model -> Quantization parameters -> int8 forward pass -> float32 backward pass -> Updated int8 weights
- **Critical Path:** Weight initialization → Forward pass (int8) → Loss computation (float32) → Backward pass (float32) → Weight update (int8)
- **Design Tradeoffs:** Memory vs. precision (int8 forward saves memory but requires careful scaling), Speed vs. stability (float32 backward ensures convergence but costs performance)
- **Failure Signatures:** Gradient saturation (from random initialization), Output layer crashes (from standard library limitations), Numerical instability (from improper quantization parameters)
- **First Experiments:** 1) Verify baseline float32 L-SGD convergence, 2) Test int8 forward pass with float32 backward, 3) Measure memory usage and latency improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Training from random initialization in int8 causes numerical saturation, requiring pre-trained weights
- Implementation details for L-SGD's "node delta optimization" are referenced from external papers but not fully specified
- Specific quantization parameters and calibration procedures are not provided in the paper

## Confidence
- **High Confidence:** Feasibility of hybrid quantization strategy on RISC-V MCUs
- **Medium Confidence:** Memory reduction and training speedup claims (requires specific hardware environment)
- **Low Confidence:** Numerical stability of int8 training without pre-trained initialization

## Next Checks
1. Obtain and review referenced papers [8,18] to fully understand L-SGD's "node delta optimization" mechanism
2. Implement and validate quantization parameter calculation and calibration process for accurate weight conversion
3. Conduct experiments to confirm stable convergence when training from pre-trained weights in int8 precision