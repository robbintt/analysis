---
ver: rpa2
title: Does Knowledge About Perceptual Uncertainty Help an Agent in Automated Driving?
arxiv_id: '2502.11864'
source_url: https://arxiv.org/abs/2502.11864
tags:
- agent
- uncertainty
- perception
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how knowledge about perceptual uncertainty
  influences the behavior of a reinforcement learning (RL) agent in an automated driving
  scenario. The authors introduce controlled perturbations in the observation space,
  specifically in semantic segmentation masks, to simulate uncertainty in perception.
---

# Does Knowledge About Perceptual Uncertainty Help an Agent in Automated Driving?

## Quick Facts
- arXiv ID: 2502.11864
- Source URL: https://arxiv.org/abs/2502.11864
- Authors: Natalie Grabowsky; Annika Mütze; Joshua Wendland; Nils Jansen; Matthias Rottmann
- Reference count: 40
- Primary result: Informing RL agents about perceptual uncertainty enables more efficient driving by reducing unnecessary defensive behaviors while maintaining safety

## Executive Summary
This paper investigates whether providing uncertainty information about perception improves reinforcement learning agent performance in automated driving. The authors introduce controlled perturbations in semantic segmentation observations and test four scenarios: correct perception without uncertainty, perturbed perception without uncertainty, perturbed perception with uncertainty information, and correct perception with uncertainty information. The key finding is that agents informed about current uncertainty adapt their behavior more effectively, driving faster when perception is reliable while increasing safety margins when uncertainty is high.

## Method Summary
The experiments use CARLA 0.9.15 simulator with a reinforcement learning agent driving straight in Town 2. The observation space consists of BEV semantic segmentation (4×25 grayscale mask), 6-dimensional non-visual state (velocity, throttle, brake, orientation, lane offset), and optionally a 4-dimensional one-hot uncertainty vector. The agent is trained using PPO with specific hyperparameters (γ=0.999, ε=0.2, learning rate 10^-5) to maximize reward based on velocity with collision penalties. Five perturbation types are tested that remove vehicles from perception: front vehicle invisible (VEXV), following vehicle invisible (XEVV), both front invisible (VEXX), all others invisible (XEXX), and correct perception (VEVV).

## Key Results
- Agents trained with uncertainty information adapt behavior based on current perception reliability
- Without uncertainty information, perturbed perception leads to defensive driving with excessive braking and increased following distances
- Uncertainty-informed agents drive faster when perception is correct while increasing safety margins when perception is uncertain
- Agent trained on correct perception only fails completely when front vehicles become invisible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding uncertainty information enables RL agents to condition behavior on perception reliability
- Core assumption: The uncertainty signal accurately reflects the true perceptual state
- Evidence anchors: [abstract] "agent adapts to the specific situation" when adding uncertainty to observation space; [section III.D] one-hot encoded uncertainty vector allows policy network to learn situation-specific action distributions
- Break condition: Miscalibrated or delayed uncertainty estimation causes worse outcomes than no uncertainty information

### Mechanism 2
- Claim: Agents trained under perturbed perception without uncertainty information learn uniformly defensive policies
- Core assumption: Perturbation distribution during training covers test distribution
- Evidence anchors: [abstract] "unreliable observation space modeled by a perturbed perception leads to defensive driving behavior"; [section IV.E] increased braking frequency and larger following distances
- Break condition: Rare perturbations during training prevent adequate defensive behavior learning; excessive perturbations cause overly conservative policies

### Mechanism 3
- Claim: Informed agents dynamically adjust behavior based on uncertainty state, achieving higher efficiency while maintaining safety
- Core assumption: Sufficient training experience across all uncertainty states enables appropriate conditional behaviors
- Evidence anchors: [section IV.F] "improved capabilities to adapt to current perturbation"; [section IV.F, Fig. 7b] shows agent decreasing distance when informed perception correct, then increasing when perturbations occur
- Break condition: Reaction inertia delays adaptation to sudden perception changes

## Foundational Learning

- **Concept: Partial Observability and Observation Functions**
  - Why needed here: Paper formalizes observation function Ω: S × A → Dist(O) where agent receives perturbed views of true state
  - Quick check question: Can you explain why adding uncertainty information to observation is state augmentation rather than changing underlying POMDP structure?

- **Concept: PPO (Proximal Policy Optimization)**
  - Why needed here: Uses PPO with specific hyperparameters (γ=0.999, ε=0.2, learning rate 10^-5)
  - Quick check question: Why might PPO be preferred over DQN for continuous action spaces in driving scenarios?

- **Concept: Semantic Segmentation as Observation**
  - Why needed here: Agent receives BEV semantic segmentation masks, not raw images; perturbations remove object classes
  - Quick check question: What information is lost using semantic segmentation vs RGB images, and why acceptable for driving task?

## Architecture Onboarding

- **Component map:** CARLA simulator -> BEV semantic segmentation generator -> Perturbation module -> Observation encoder -> PPO policy network -> Inertia model -> Reward calculator
- **Critical path:** Episode spawn → sample perturbation → apply to segmentation → encode observation → policy inference → apply inertia → execute action; termination on collision, timeout (7500 steps), or success (150m traveled)
- **Design tradeoffs:** Ground-truth uncertainty vs estimated (paper uses perfect indicators); discrete perturbation cases vs continuous uncertainty (one-hot encoding is simplistic); inertia model improves realism but delays response; 110-dim observation is compact but may not scale
- **Failure signatures:** Agent trained on correct perception + perturbed test → 100% collision rate with invisible front vehicles; agent without uncertainty info → excessive braking, failure to complete task; rapid perturbation switching → delayed adaptation, potential collisions during transition
- **First 3 experiments:** 1) Baseline validation: Train/test on correct perception only, verify success rate, then test on single perturbation cases to confirm failure mode; 2) Defensive behavior confirmation: Train on perturbed perception without uncertainty info, measure brake frequency and following distance compared to baseline; 3) Uncertainty ablation: Train on perturbed perception with uncertainty info, compare time-to-completion and collision rates against defensive agent across all perturbation types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does imperfect or miscalibrated uncertainty estimation affect agent's behavioral adaptation?
- Basis in paper: [explicit] Authors state they provide "perfectly quantifiable perceptual uncertainties" and acknowledge setup is "simplistic"
- Why unresolved: Study assumes oracle-level uncertainty information unavailable in practice
- What evidence would resolve it: Experiments with uncertainty estimates having known calibration errors, false positive/negative rates, or systematic biases

### Open Question 2
- Question: Does uncertainty information improve performance in more complex driving scenarios involving steering, intersections, and multi-lane roads?
- Basis in paper: [explicit] "proxy learning task deliberately chosen to be simple" and "refrain from steering but include braking action"
- Why unresolved: Current task restricted to straight-line driving with only throttle/brake decisions
- What evidence would resolve it: Experiments in CARLA with lateral control, traffic lights, junctions, and lane changes

### Open Question 3
- Question: How does agent perform when uncertainty information provided as continuous probability values rather than discrete one-hot encoding?
- Basis in paper: [inferred] Uses 4-dimensional one-hot vector for discrete perturbation types, but real uncertainty estimators output continuous confidence scores
- Why unresolved: Mapping from continuous uncertainty estimates to actionable information for RL agents remains unexplored
- What evidence would resolve it: Experiments varying representation (raw probabilities, entropy values, calibration-adjusted scores) and measuring behavioral adaptation

## Limitations

- Uses ground-truth uncertainty indicators rather than estimated perceptual uncertainty, representing idealized scenario
- Perturbations are discrete and deterministic rather than representing continuous nature of real perceptual uncertainty
- Results limited to simple straight-line driving task without steering or complex scenarios

## Confidence

- **High confidence**: Core finding that agents can adapt behavior based on uncertainty information (Mechanism 1 and 3) is well-supported by experimental results
- **Medium confidence**: Claim that agents without uncertainty information learn uniformly defensive policies (Mechanism 2) is supported but could be influenced by specific perturbation distribution used during training
- **Medium confidence**: Claim about improved efficiency while maintaining safety is supported by data but limited to specific driving scenario tested

## Next Checks

1. Replace ground-truth uncertainty indicators with uncertainty estimates from a trained uncertainty estimation module (Monte Carlo dropout or ensemble methods) and verify agent still adapts behavior appropriately

2. Replace discrete one-hot uncertainty encoding with continuous scalar confidence values and test whether agent can learn smooth behavioral adaptations across uncertainty spectrum

3. Test uncertainty-informed agent on scenarios with different perturbation distributions (more frequent or severe perturbations) to evaluate robustness beyond training distribution