---
ver: rpa2
title: 'BoTTA: Benchmarking on-device Test Time Adaptation'
arxiv_id: '2504.10149'
source_url: https://arxiv.org/abs/2504.10149
tags:
- adaptation
- accuracy
- data
- methods
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BoTTA benchmarks Test Time Adaptation (TTA) methods under practical
  constraints faced by mobile and edge devices. It evaluates four key challenges:
  limited target samples, limited category exposure, diverse distribution shifts,
  and overlapping corruptions.'
---

# BoTTA: Benchmarking on-device Test Time Adaptation

## Quick Facts
- arXiv ID: 2504.10149
- Source URL: https://arxiv.org/abs/2504.10149
- Reference count: 40
- Primary result: Evaluates TTA methods under mobile/edge constraints

## Executive Summary
BoTTA benchmarks Test Time Adaptation (TTA) methods under practical constraints faced by mobile and edge devices. It evaluates four key challenges: limited target samples, limited category exposure, diverse distribution shifts, and overlapping corruptions. Experiments on CIFAR-10C and PACS with ResNet-26, ResNet-50, and ViT architectures reveal that most TTA algorithms fail when adaptation data is small, struggle with multi-domain shifts, and suffer accuracy drops with overlapping corruptions. SHOT is most accurate overall but degrades in multi-domain scenarios and uses higher memory than baseline. On-device profiling on Raspberry Pi 4B and Jetson Orin Nano shows T3A uses less memory (1.05×) while SHOT uses more (1.08×) than non-adaptive baseline. Results indicate current TTA methods are insufficient for real-world edge deployment, highlighting the need for more robust, efficient algorithms.

## Method Summary
BoTTA introduces a comprehensive benchmark for evaluating Test Time Adaptation methods under realistic edge deployment constraints. The framework assesses four key challenges: limited target samples (1-100 examples), limited category exposure (fewer than training classes), diverse distribution shifts (synthetic and real), and overlapping corruptions. Experiments are conducted on CIFAR-10C and PACS datasets using ResNet-26, ResNet-50, and ViT architectures. The evaluation includes memory profiling on Raspberry Pi 4B and Jetson Orin Nano to measure on-device feasibility. Four TTA algorithms (TENT, T3A, SHOT, EATA) are compared against a non-adaptive baseline across these scenarios to identify performance bottlenecks and resource constraints.

## Key Results
- Most TTA algorithms fail when adaptation data is small (1-100 examples)
- SHOT achieves highest accuracy but degrades significantly in multi-domain scenarios
- T3A uses less memory (1.05×) than non-adaptive baseline, while SHOT uses more (1.08×)
- All methods suffer accuracy drops when faced with overlapping corruptions

## Why This Works (Mechanism)
The benchmark framework systematically isolates and evaluates specific constraints that affect TTA performance in real-world edge deployments. By controlling the number of target samples, the number of categories, and the types of distribution shifts, BoTTA reveals how different TTA algorithms handle these practical limitations. The inclusion of overlapping corruptions tests algorithm robustness when multiple domain shifts occur simultaneously, while on-device profiling quantifies the feasibility of deploying adaptive models on resource-constrained hardware.

## Foundational Learning
The benchmark builds upon established TTA techniques including entropy minimization, test-time normalization, and instance-specific optimization. It extends these foundations by introducing realistic edge constraints that go beyond standard domain adaptation assumptions. The framework demonstrates that while TTA algorithms perform well in idealized settings, their effectiveness diminishes significantly when faced with practical deployment challenges like limited data availability and computational resources on edge devices.

## Architecture Onboarding
BoTTA leverages standard deep learning architectures (ResNet-26, ResNet-50, ViT) that are commonly used in edge deployment scenarios. The benchmark framework is architecture-agnostic, allowing evaluation of TTA methods across different model families. This approach ensures that findings are not specific to particular architectures but rather reflect the general limitations of TTA algorithms when applied to edge devices with varying computational capabilities.

## Open Questions the Paper Calls Out
- How can TTA methods be designed to maintain performance when adaptation data is severely limited (1-10 examples)?
- What architectural modifications could enable TTA algorithms to handle multiple simultaneous domain shifts more effectively?
- Can memory-efficient TTA methods achieve comparable accuracy to resource-intensive approaches like SHOT?
- How do energy consumption patterns differ between adaptive and non-adaptive methods during extended deployment?

## Limitations
- Evaluation relies on synthetic distribution shifts that may not fully capture real-world deployment scenarios
- Memory overhead measurements limited to Raspberry Pi 4B and Jetson Orin Nano, with potential variation on other edge hardware
- Study focuses exclusively on image classification tasks, limiting generalizability to other modalities like audio or text
- Memory tradeoff analysis between SHOT's accuracy gains and resource constraints is not fully quantified
- Benchmark does not account for energy consumption differences between adaptive and non-adaptive methods during deployment

## Confidence
- **High Confidence**: TTA methods struggle with limited target samples and overlapping corruptions - supported by consistent degradation patterns across multiple experiments and architectures
- **Medium Confidence**: SHOT is most accurate overall but degrades in multi-domain scenarios - while results show this trend, the limited number of domains in PACS may not provide sufficient evidence for generalization
- **Low Confidence**: Current TTA methods are insufficient for real-world edge deployment - this conclusion requires broader hardware diversity and more realistic deployment scenarios beyond synthetic benchmarks

## Next Checks
1. Test the same TTA methods on non-synthetic, real-world domain shift datasets (e.g., WILDS) to validate generalization beyond controlled benchmarks
2. Profile energy consumption and inference latency on additional edge hardware platforms (e.g., ARM Cortex-M, Google Coral) to assess deployment feasibility across diverse devices
3. Conduct ablation studies isolating the impact of each distribution shift type (corruption, blur, noise, etc.) to better understand which shifts are most challenging for current TTA methods