---
ver: rpa2
title: 'Generative AI Takes a Statistics Exam: A Comparison of Performance between
  ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini'
arxiv_id: '2501.09171'
source_url: https://arxiv.org/abs/2501.09171
tags:
- gpt3
- gpt4
- exam
- questions
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the accuracy and text characteristics of three
  ChatGPT versions (3.5, 4.0, and 4o-mini) on a 16-question graduate-level statistics
  exam. Results show GPT3.5 performed poorly, GPT4 performed well, and GPT4o-mini
  scored in between.
---

# Generative AI Takes a Statistics Exam: A Comparison of Performance between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini

## Quick Facts
- arXiv ID: 2501.09171
- Source URL: https://arxiv.org/abs/2501.09171
- Reference count: 40
- Paid ChatGPT versions (4 and 4o-mini) outperform free version (3.5) on graduate statistics exam questions

## Executive Summary
This study evaluates three versions of ChatGPT (3.5, 4.0, and 4o-mini) on a 16-question graduate-level statistics exam to assess their relative performance in statistical reasoning and problem-solving. The research employs both accuracy metrics and text analysis techniques to characterize differences in how each model approaches statistical questions. Results demonstrate significant performance disparities, with GPT-4 achieving the highest accuracy, followed by GPT-4o-mini, while GPT-3.5 performs notably worse. The study also reveals that paid versions of generative AI platforms demonstrate superior performance compared to free versions, raising concerns about potential educational access disparities.

## Method Summary
The researchers administered a 16-question graduate-level statistics exam to three ChatGPT versions: GPT-3.5, GPT-4, and GPT-4o-mini. Each question was presented to all models in randomized order, and responses were evaluated for accuracy against correct answers. Text analysis techniques including word frequency analysis, lexical complexity measures, and topic modeling were applied to characterize differences in response content and structure across the three models. The study compared not only numerical accuracy but also examined the linguistic and structural characteristics of model outputs to understand how different versions approach statistical problem-solving.

## Key Results
- GPT-4 achieved the highest accuracy on the graduate statistics exam, followed by GPT-4o-mini, with GPT-3.5 performing significantly worse
- GPT-4o-mini produced longer, more complex answers than GPT-4, while GPT-3.5 focused more on problem context than statistical methodology
- Topic modeling revealed GPT-4 and GPT-4o-mini generated more statistically relevant content compared to GPT-3.5

## Why This Works (Mechanism)
The performance differences across ChatGPT versions stem from their underlying architectural and training distinctions. GPT-4 benefits from more extensive training data, larger model parameters, and refined fine-tuning on specialized domains including statistics. GPT-4o-mini, despite being a smaller model, incorporates optimized architecture that balances efficiency with capability, allowing it to produce more detailed responses while maintaining reasonable accuracy. GPT-3.5, being an older architecture with fewer parameters and less specialized training, struggles with complex statistical reasoning tasks that require nuanced understanding of probability theory and statistical inference.

## Foundational Learning
1. **Statistical hypothesis testing concepts** - Understanding p-values, significance levels, and test selection is essential for evaluating statistical exam questions and comparing model outputs against correct answers
   *Why needed:* Provides the framework for assessing whether observed performance differences are statistically meaningful
   *Quick check:* Can you explain the difference between Type I and Type II errors?

2. **Text analysis and natural language processing** - Familiarity with word frequency analysis, lexical complexity measures, and topic modeling techniques is crucial for interpreting the linguistic characteristics of model responses
   *Why needed:* Enables understanding of how different models structure their answers and the depth of statistical content they produce
   *Quick check:* Can you describe how TF-IDF differs from simple word frequency counts?

3. **Large language model architecture fundamentals** - Understanding transformer architecture, attention mechanisms, and parameter scaling helps explain performance differences between model versions
   *Why needed:* Provides context for why larger, more recent models typically outperform smaller, older versions
   *Quick check:* Can you explain how self-attention allows transformers to handle long-range dependencies?

## Architecture Onboarding
The comparison involves three distinct generative AI architectures with different capabilities:
GPT-3.5 -> GPT-4 -> GPT-4o-mini represents an evolutionary progression in model sophistication and specialized capabilities. GPT-3.5 operates on a base transformer architecture with approximately 175 billion parameters, while GPT-4 scales to trillions of parameters with enhanced reasoning capabilities. GPT-4o-mini represents a distilled version optimized for efficiency while maintaining core statistical reasoning abilities.

Critical path: Input processing → Context understanding → Statistical reasoning → Answer generation → Output formatting
The critical path emphasizes the importance of accurate problem comprehension and appropriate statistical methodology selection in achieving correct answers.

Design tradeoffs: GPT-4 prioritizes accuracy and comprehensive reasoning over response efficiency, GPT-4o-mini balances capability with computational efficiency, while GPT-3.5 emphasizes speed and general-purpose applicability at the cost of specialized domain performance.

Failure signatures: GPT-3.5 typically fails by providing superficial answers lacking statistical depth, GPT-4 occasionally overcomplicates simple problems with unnecessary methodological complexity, and GPT-4o-mini may produce verbose responses that obscure key statistical insights.

First experiments:
1. Test each model on basic probability questions to establish baseline performance differences
2. Evaluate responses to hypothesis testing problems to assess statistical reasoning depth
3. Compare model performance on questions requiring interpretation of statistical output versus calculation

## Open Questions the Paper Calls Out
None

## Limitations
- The study's 16-question sample size, while representative of graduate-level content, provides a limited scope of statistical knowledge domains
- Evaluation focuses exclusively on exam question performance without examining real-world application scenarios or problem-solving contexts
- Text analysis methods provide interesting insights into answer characteristics but don't directly correlate these features with pedagogical effectiveness or learning outcomes

## Confidence
- GPT-4 outperforms GPT-3.5 and GPT-4o-mini on graduate statistics exam questions: High
- Paid versions demonstrate superior performance compared to free versions: Medium
- Educational access gaps may widen due to performance differences: Low

## Next Checks
1. Expand the question corpus to include 100+ questions spanning multiple statistical subdisciplines and difficulty levels to better characterize model performance boundaries
2. Conduct a controlled study comparing student performance and learning outcomes when using different AI assistant versions for exam preparation versus actual problem-solving
3. Implement longitudinal tracking of model performance across multiple exam cycles to identify whether observed accuracy patterns persist as models receive updates or as statistical education content evolves