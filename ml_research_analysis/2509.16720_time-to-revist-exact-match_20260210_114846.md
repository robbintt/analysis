---
ver: rpa2
title: Time to Revist Exact Match
arxiv_id: '2509.16720'
source_url: https://arxiv.org/abs/2509.16720
tags:
- temporal
- answer
- shot
- smape
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces TempAnswerQA, a benchmark focused on temporal
  QA tasks where answers are strictly numeric (dates or durations). It highlights
  the limitations of exact match (EM) evaluation, which fails to differentiate between
  small and large errors in temporal reasoning.
---

# Time to Revist Exact Match
## Quick Facts
- arXiv ID: 2509.16720
- Source URL: https://arxiv.org/abs/2509.16720
- Reference count: 40
- Key outcome: Exact match evaluation is insufficient for temporal QA; sMAPE and MASE provide more meaningful metrics.

## Executive Summary
This work introduces TempAnswerQA, a benchmark focused on temporal QA tasks where answers are strictly numeric (dates or durations). It highlights the limitations of exact match (EM) evaluation, which fails to differentiate between small and large errors in temporal reasoning. By reframing the task as a numerical estimation problem, the authors apply forecasting metrics—symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE)—to better assess model performance. The findings show that EM alone is inadequate for temporal QA, and specialized metrics are essential for accurate evaluation.

## Method Summary
The authors reframe temporal question answering as a numerical estimation problem and introduce TempAnswerQA as a specialized benchmark for evaluating strictly numeric temporal answers (dates or durations). They compare traditional exact match (EM) evaluation against forecasting metrics, specifically symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE). The evaluation framework includes a plausibility interval based on reference spans to contextualize errors. Models are trained on synthetic data and evaluated on both synthetic and real-world temporal QA tasks to assess performance across different data distributions.

## Key Results
- EM and relative error metrics are largely decoupled, showing EM fails to capture practical error severity in temporal QA
- MASE reshuffles model rankings and exposes gaps in models trained on synthetic data when evaluated on real-world tasks
- Synthetic data training advantages diminish when models are tested on real-world temporal QA tasks with varying date ranges and formats

## Why This Works (Mechanism)
The decoupling between EM and relative error metrics occurs because EM treats all deviations equally, regardless of their practical significance. In temporal QA, a prediction off by one day versus one year represents vastly different levels of reasoning quality, yet both fail EM. Forecasting metrics like sMAPE and MASE capture this error magnitude through percentage-based and scaled measurements, providing a more nuanced evaluation that reflects real-world temporal reasoning requirements.

## Foundational Learning
- Exact Match (EM): A binary evaluation metric that checks if predicted answers exactly match ground truth; insufficient for continuous numeric tasks as it doesn't capture error magnitude
  - Why needed: Standard QA metric that fails to differentiate between small and large numerical errors in temporal reasoning
  - Quick check: Apply EM to date prediction tasks where predictions are off by 1 day vs. 1 year

- Symmetric Mean Absolute Percentage Error (sMAPE): A forecasting metric that measures relative error symmetrically between predictions and actual values
  - Why needed: Captures the magnitude of errors in temporal predictions, unlike EM which only provides binary pass/fail
  - Quick check: Compare sMAPE scores for predictions that are off by 1 day vs. 1 year on a 2024 date

- Mean Absolute Scaled Error (MASE): A relative error metric that scales prediction errors by the mean absolute error of a naive forecast within a plausibility interval
  - Why needed: Provides context-aware evaluation by comparing model performance against simple baseline predictions within temporal ranges
  - Quick check: Calculate MASE for a model predicting dates within a 30-day window versus predictions across multiple years

## Architecture Onboarding
- Component map: TempAnswerQA benchmark -> EM evaluation -> sMAPE/MASE evaluation -> Model ranking comparison
- Critical path: Synthetic data training -> EM evaluation -> sMAPE/MASE evaluation -> Performance gap identification
- Design tradeoffs: Standard EM provides binary pass/fail simplicity but misses error severity; specialized metrics capture error magnitude but require domain-specific plausibility intervals
- Failure signatures: Models may achieve high EM scores while having poor sMAPE/MASE scores, indicating they occasionally get exact answers but consistently make large errors
- First experiments: 1) Compare EM vs sMAPE scores for models trained on synthetic vs real data; 2) Evaluate MASE across different plausibility intervals; 3) Test whether calendar-aware models improve MASE scores

## Open Questions the Paper Calls Out
The paper raises questions about whether the observed limitations of EM extend to other numerical QA domains beyond temporal reasoning, and whether the effectiveness of sMAPE and MASE generalizes across different temporal QA datasets and model architectures.

## Limitations
- The study's focus on strictly numeric temporal answers may not generalize to other QA domains where EM remains a valid evaluation metric
- The benchmark construction and error analysis are confined to a specific temporal reasoning context
- The relative performance of sMAPE and MASE may vary across different temporal QA datasets and model architectures

## Confidence
- EM inadequacy for temporal QA: High (supported by empirical evidence showing decoupling of EM and relative error metrics)
- Superiority of sMAPE and MASE: Medium (effectiveness may vary across different datasets and architectures)

## Next Checks
1. Test whether sMAPE and MASE show similar decoupling patterns on larger, more diverse temporal QA datasets beyond TempAnswerQA
2. Evaluate whether synthetic data training advantages persist when models are tested on real-world temporal QA tasks with varying date ranges and formats
3. Assess the impact of incorporating temporal context (e.g., calendar awareness) into model architectures and whether this improves MASE scores more than traditional approaches