---
ver: rpa2
title: Task-agnostic Prompt Compression with Context-aware Sentence Embedding and
  Reward-guided Task Descriptor
arxiv_id: '2502.13374'
source_url: https://arxiv.org/abs/2502.13374
tags:
- prompt
- compression
- task
- question
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Task-agnostic Prompt Compression (TPC), a
  method for compressing input prompts without requiring explicit questions or handcrafted
  templates. The approach uses a context-relevant task descriptor to generate a task
  description from the input prompt, which is then used with a context-aware sentence
  encoder to select the most relevant sentences for the compressed prompt.
---

# Task-agnostic Prompt Compression with Context-aware Sentence Embedding and Reward-guided Task Descriptor

## Quick Facts
- **arXiv ID**: 2502.13374
- **Source URL**: https://arxiv.org/abs/2502.13374
- **Reference count**: 19
- **Primary result**: Task-agnostic prompt compression method using context-aware sentence embedding and reward-guided task descriptor

## Executive Summary
This paper introduces Task-agnostic Prompt Compression (TPC), a novel method for compressing input prompts without requiring explicit questions or handcrafted templates. The approach uses a context-relevant task descriptor to generate a task description from the input prompt, which is then used with a context-aware sentence encoder to select the most relevant sentences for the compressed prompt. The task descriptor is fine-tuned using reinforcement learning with a reward function that measures the similarity between responses generated from the original and compressed prompts.

## Method Summary
The TPC method consists of two main components: a context-aware sentence encoder and a task descriptor. The context-aware sentence encoder uses a pre-trained language model to generate embeddings for each sentence in the prompt. The task descriptor, trained via reinforcement learning, generates a task description from the input prompt. The method then selects sentences based on their similarity to the task description, creating a compressed prompt that retains the most relevant information. Three model sizes (Base, Large, Huge) are evaluated on LongBench and ZeroSCROLLS benchmarks.

## Key Results
- TPC-Huge achieves up to 3.3 points improvement over prior works in prompt-aware setup on LongBench
- TPC-Huge achieves up to 35.0 points improvement in prompt-agnostic setup on LongBench
- TPC-Base performs comparably to larger models despite being significantly smaller

## Why This Works (Mechanism)
The method works by leveraging the task descriptor to generate context-relevant task descriptions, which guide the sentence selection process. The context-aware sentence encoder ensures that the selected sentences are semantically relevant to the task at hand. The reinforcement learning approach fine-tunes the task descriptor to optimize for response similarity between original and compressed prompts, effectively learning to identify the most critical information for task completion.

## Foundational Learning

**Context-aware Sentence Embedding**
- *Why needed*: To understand semantic relationships between sentences and tasks
- *Quick check*: Verify that embeddings capture task-relevant information through similarity analysis

**Reinforcement Learning for Task Descriptor**
- *Why needed*: To optimize task description generation based on downstream performance
- *Quick check*: Monitor reward function convergence and stability during training

**Prompt Compression Evaluation**
- *Why needed*: To measure effectiveness of compression on actual task performance
- *Quick check*: Compare task performance metrics between original and compressed prompts

## Architecture Onboarding

**Component Map**
Task Input -> Task Descriptor -> Context-aware Sentence Encoder -> Sentence Selector -> Compressed Prompt

**Critical Path**
Task Input → Task Descriptor (RL fine-tuning) → Context-aware Sentence Encoder → Sentence Similarity Scoring → Compressed Prompt Generation

**Design Tradeoffs**
The method trades computational overhead during compression for improved downstream task performance. The use of reinforcement learning for task descriptor optimization requires careful reward function design to avoid reward hacking or collapse to trivial solutions.

**Failure Signatures**
- Poor task descriptor performance leading to irrelevant sentence selection
- Sentence encoder bias toward surface-level similarity rather than task relevance
- Reward function failure to capture true task performance requirements

**First Experiments**
1. Evaluate sentence selection quality using human evaluation on diverse prompt types
2. Test compression performance across different domains not represented in benchmark datasets
3. Analyze computational overhead of compression process versus inference efficiency gains

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on context-aware sentence encoder may introduce bias toward surface-level semantic similarity
- Reinforcement learning approach depends heavily on quality of reward function proxy
- Evaluation focuses on specific benchmarks that may not represent full diversity of real-world scenarios
- Computational overhead during compression process not addressed

## Confidence

**High Confidence**: Experimental results showing TPC-Huge outperforming state-of-the-art methods on evaluated benchmarks

**Medium Confidence**: TPC-Base performance claims compared to larger models may not generalize across all task types

**Medium Confidence**: Strong generalization claims based on benchmark performance lack extensive cross-domain validation

## Next Checks

1. **Cross-domain robustness testing**: Evaluate on prompts from domains not represented in LongBench or ZeroSCROLLS (e.g., medical, legal, or technical documentation)

2. **Ablation study on reward function design**: Systematically test alternative reward functions and analyze sensitivity to similarity metric choice

3. **Computational overhead analysis**: Measure actual inference time and resources required for compression process versus efficiency gains during downstream execution