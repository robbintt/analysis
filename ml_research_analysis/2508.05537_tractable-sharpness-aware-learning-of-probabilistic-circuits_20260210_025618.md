---
ver: rpa2
title: Tractable Sharpness-Aware Learning of Probabilistic Circuits
arxiv_id: '2508.05537'
source_url: https://arxiv.org/abs/2508.05537
tags:
- sharpness
- epoch
- data
- regularized
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first tractable curvature-based regularizer
  for training Probabilistic Circuits (PCs), addressing overfitting in expressive,
  deep architectures. By leveraging the structured DAG of PCs, the authors derive
  exact and efficient expressions for the trace of the Hessian of the log-likelihood,
  a scalar measure of surface curvature.
---

# Tractable Sharpness-Aware Learning of Probabilistic Circuits

## Quick Facts
- arXiv ID: 2508.05537
- Source URL: https://arxiv.org/abs/2508.05537
- Authors: Hrithik Suresh; Sahil Sidheekh; Vishnu Shreeram M. P; Sriraam Natarajan; Narayanan C. Krishnan
- Reference count: 40
- Key outcome: First tractable curvature-based regularizer for training PCs using Hessian trace, reducing overfitting by up to 65% and improving test log-likelihood by up to 49%.

## Executive Summary
This paper introduces a novel approach to regularize Probabilistic Circuits (PCs) by minimizing the trace of the Hessian of the log-likelihood, which serves as a tractable proxy for sharpness in the loss landscape. Unlike Deep Neural Networks where Hessian computation is intractable, the structured DAG architecture of PCs allows efficient computation of this trace. The method guides training toward flatter minima, reducing overfitting especially in low-data regimes, and provides closed-form updates for both gradient-based and EM-based learning algorithms.

## Method Summary
The method computes the trace of the Hessian of the log-likelihood for PCs, which simplifies to the sum of squared gradients. For tree-structured PCs, the full Hessian is computable in closed form, while for general DAGs, the trace remains efficiently computable. The authors propose minimizing this trace as a sharpness-aware regularization objective, which translates into a gradient-norm penalty yielding simple quadratic updates for EM and seamless integration with gradient-based methods. The regularizer is implemented as an additional term in the loss function: $-\log P(x) + \mu \cdot R(\theta, x)$, where $R(\theta, x) = \sum_{n,c} (F_{nc}(x)/\theta_{nc})^2$.

## Key Results
- Reduces overfitting by up to 65% in low-data regimes (1-10% training data)
- Improves test log-likelihood by up to 49% across 20 binary density estimation benchmarks
- Consistently guides PCs toward flatter minima, as visualized in loss landscape plots
- Provides closed-form quadratic updates for EM, avoiding complex cubic equations
- Works across both gradient-based (Einsum Networks) and EM-based (PyJuice HCLT) training frameworks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The trace of the Hessian of the log-likelihood, serving as a proxy for sharpness, is tractable and computable in linear time for Probabilistic Circuits (PCs).
- **Mechanism:** The structured DAG architecture of PCs allows the Hessian trace to be decomposed into the sum of squared gradients. Specifically, the trace simplifies to $\sum (F_{nc}/\theta_{nc})^2$, where $F_{nc}$ are edge flows obtained in a single forward-backward pass.
- **Core assumption:** The PC must satisfy standard structural properties (smoothness and decomposability) to ensure the recursive definition of flow and the validity of the derivative decomposition.
- **Evidence anchors:**
  - [abstract] "the trace of the Hessian... can be computed efficiently for PCs."
  - [section: Hessian for General (DAG-Structured) PCs] "Proposition 4. The diagonal entry... is given by $-(F_{nc}/\theta_{nc})^2$."
  - [corpus] No specific corpus papers validate this mathematical derivation; it is a unique contribution of this work.
- **Break condition:** If the PC structure violates smoothness or decomposability, the closed-form gradient/flow relationship may not hold, breaking the linear-time trace computation.

### Mechanism 2
- **Claim:** Minimizing this tractable Hessian trace acts as a sharpness-aware regularizer that reduces overfitting.
- **Mechanism:** By adding the trace (sum of squared gradients) as a penalty term to the loss function, the optimizer is discouraged from converging into sharp, narrow basins of the loss landscape. This forces the parameters into flatter regions where the likelihood is robust to small perturbations, improving generalization.
- **Core assumption:** Flat minima correlate with better generalization performance in PCs, analogous to the "flat minima" hypothesis in Deep Learning.
- **Evidence anchors:**
  - [abstract] "Minimizing this Hessian trace induces a gradient-norm-based regularizer... guides PCs toward flatter minima."
  - [section: Experiments and Results] Figure 1 visualizes the loss landscape, showing standard training converging to sharp basins while regularized training finds flat basins.
  - [corpus] Related work "LSAM" supports the general principle of landscape smoothing for generalization, though in a different domain.
- **Break condition:** In high-data regimes where overfitting is negligible, this regularization may push parameters away from the optimal maximum likelihood estimate, potentially slightly degrading performance (as seen in 100% data splits).

### Mechanism 3
- **Claim:** The regularizer allows for closed-form parameter updates in Expectation-Maximization (EM), avoiding complex cubic equations.
- **Mechanism:** Directly minimizing the squared gradient (Hessian trace) leads to a cubic update equation. The authors reformulate the objective to minimize the gradient magnitude instead of its square. Since gradients in PCs are non-negative, this is equivalent but results in a quadratic equation with a closed-form solution.
- **Core assumption:** The partial derivatives of the log-likelihood (gradients) are strictly non-negative, allowing the minimization of the gradient to surrogate for the minimization of the squared gradient.
- **Evidence anchors:**
  - [section: Sharpness-Aware EM] "Theorem 2. The EM parameter update... is given by [quadratic formula]."
  - [supplementary: A.3] Proof details the derivation of the quadratic update from the reformulated Lagrangian.
- **Break condition:** If numerical instability arises (e.g., negative flow values due to numerical error), the quadratic formula might yield invalid negative roots.

## Foundational Learning

- **Concept: Circuit Flow (Forward/Backward Pass)**
  - **Why needed here:** The entire method relies on computing "edge flows" ($F_{nc}$) to calculate the Hessian trace. You must understand that flow represents the contribution of a specific edge to the total probability computation.
  - **Quick check question:** Can you explain why computing the flow requires both a bottom-up pass (evaluation) and a top-down pass (derivatives)?

- **Concept: Sharpness-Aware Minimization (SAM)**
  - **Why needed here:** This paper adapts SAM concepts from Neural Networks. You need to distinguish between *approximating* sharpness (standard SAM) vs. *calculating exact* sharpness (this paper).
  - **Quick check question:** Why does high curvature (sharpness) in the loss landscape typically lead to poor generalization?

- **Concept: Expectation-Maximization (EM) in PCs**
  - **Why needed here:** The paper offers specific modifications for EM learning (E-step computes flows, M-step updates weights). Understanding the M-step constraint (weights sum to 1) is vital for the quadratic update logic.
  - **Quick check question:** In the context of PCs, what does the E-step actually compute?

## Architecture Onboarding

- **Component map:** Input Nodes -> Sum Nodes (weighted mixtures) -> Product Nodes (factorizations) -> Flow Engine (computes edge flows)
- **Critical path:**
  1. **Forward Pass:** Compute node outputs $p_n(x)$.
  2. **Backward Pass:** Compute node flows $F_n(x)$ and edge flows $F_{nc}(x)$.
  3. **Trace Calculation:** Accumulate $\sum (F_{nc}/\theta_{nc})^2$.
  4. **Update Rule:** Apply either gradient step (SGD) or solve the quadratic equation (EM) using the accumulated trace/flows.
- **Design tradeoffs:**
  - **Tree vs. DAG:** Tree-structured PCs allow computing the *full* Hessian, while general DAGs restrict the efficient computation to just the *trace*. The paper recommends using the trace for both to maintain generality and linear complexity.
  - **SGD vs. EM:** EM provides closed-form updates (Theorem 2) which are faster/convergent, whereas SGD requires adding the trace term to the loss gradient.
- **Failure signatures:**
  - **Underfitting:** If regularization strength $\mu$ is set too high, the model prioritizes flatness over data likelihood, resulting in poor training accuracy.
  - **Numerical Instability:** In EM, if the discriminant in the quadratic update (Theorem 2) becomes negative (unlikely with valid flows but possible with bad initialization), the update fails.
- **First 3 experiments:**
  1. **Sanity Check:** Replicate the "Trace Validation" (Figure 3, left) on a small PC. Compute the trace using the derived sum-of-squared-gradients formula and compare it against a brute-force PyTorch autograd Hessian trace to verify the derivation.
  2. **Low-Data Ablation:** Train a PC on a synthetic dataset (e.g., Spiral) with only 1-5% of data. Plot "Sharpness" vs. "Validation NLL" over epochs to confirm that spikes in sharpness correlate with the generalization gap.
  3. **EM Update Verification:** Implement the quadratic EM update (Algorithm 2) and confirm that the weights $\theta$ remain strictly positive and sum to 1 after the normalization step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the log-likelihood landscape of Probabilistic Circuits (PCs) exhibit asymmetric valleys analogous to those observed in Deep Neural Networks, and how do they influence generalization?
- Basis in paper: [explicit] The conclusion explicitly identifies the investigation of "asymmetric valleys analogous to those observed in DNNs" as a direction for future work.
- Why unresolved: The current work focuses on the Hessian trace, which measures symmetric curvature (sharpness), but does not analyze the asymmetry of the loss basins.
- What evidence would resolve it: Empirical visualization of the loss landscape showing directional asymmetry in the valleys, or theoretical analysis linking asymmetric geometry to generalization gaps in PCs.

### Open Question 2
- Question: What theoretical framework can explain the convergence behavior of over-parameterized Probabilistic Circuits?
- Basis in paper: [explicit] The authors list "developing a theoretical framework for understanding convergence in over-parameterized PCs" as a specific goal for future research.
- Why unresolved: While the paper empirically demonstrates that the regularizer improves generalization, it does not provide a theoretical guarantee or explanation for the convergence dynamics in high-capacity models.
- What evidence would resolve it: Formal theorems characterizing the optimization trajectory and loss landscape geometry for PCs with millions of parameters.

### Open Question 3
- Question: Is the computation of the full Hessian strictly intractable for general DAG-structured Probabilistic Circuits, or can off-diagonal entries be approximated efficiently?
- Basis in paper: [inferred] The paper notes that while the Hessian trace is tractable, "off-diagonal entries can suffer from a combinatorial explosion," and the authors "conjecture" general intractability without providing a formal proof.
- Why unresolved: The paper proves tractability for tree-structured PCs but relies on a conjecture regarding the exponential growth of dependency paths in general DAGs.
- What evidence would resolve it: A complexity-theoretic proof of intractability (e.g., #P-hardness) or a novel algorithm that computes the full Hessian in polynomial time for specific dense DAG structures.

## Limitations

- **Parameter Instability Risk:** The term $(F_{nc}/\theta_{nc})^2$ can explode if $\theta_{nc}$ approaches 0, requiring careful parameter clipping or epsilon safeguards.
- **Underfitting at High Data:** In 100% data regimes, the regularizer may push parameters away from optimal likelihood, slightly degrading performance when overfitting is negligible.
- **Theoretical Gaps:** The paper relies on empirical validation and the general "flat minima" hypothesis from deep learning, lacking a formal theoretical framework for why sharpness minimization improves generalization in PCs.

## Confidence

- **Mechanism 1 (Tractable Hessian Trace):** High confidence. The mathematical derivation for the trace in terms of edge flows is explicit and well-founded in the DAG structure of PCs.
- **Mechanism 2 (Flat Minima Generalization):** Medium confidence. The experiments support the correlation between low sharpness and improved generalization, but the causal link is primarily empirical and relies on the general "flat minima" hypothesis from deep learning.
- **Mechanism 3 (Quadratic EM Updates):** Medium confidence. The derivation is provided, but the practical stability and performance compared to the original cubic update in all possible edge cases are not exhaustively tested.

## Next Checks

1. **Brute-Force Hessian Trace Validation:** Implement a small PC and compute the Hessian trace using the derived formula. Compare the result against a brute-force PyTorch autograd computation to ensure the mathematical derivation is correct.
2. **Low-Data Sharpness Correlation:** Train a PC on a simple synthetic dataset (e.g., 2D spiral) with only 1-5% of the data. Track the sharpness (trace value) and validation NLL over training epochs to confirm that spikes in sharpness directly correlate with increases in the generalization gap.
3. **EM Update Numerical Stability:** Implement the quadratic EM update rule. Run a series of training sessions with varying initializations and monitor for any instances where the discriminant becomes negative or the parameters become invalid (negative or summing to zero).