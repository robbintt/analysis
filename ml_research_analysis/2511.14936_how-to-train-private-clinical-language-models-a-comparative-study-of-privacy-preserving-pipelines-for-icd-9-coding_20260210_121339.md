---
ver: rpa2
title: 'How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving
  Pipelines for ICD-9 Coding'
arxiv_id: '2511.14936'
source_url: https://arxiv.org/abs/2511.14936
tags:
- privacy
- dp-distil
- training
- dp-small
- dp-synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically compared four privacy-preserving training\
  \ pipelines for ICD-9 coding from clinical discharge summaries, finding that knowledge\
  \ distillation from differentially private teachers consistently outperforms direct\
  \ DP training and synthetic data approaches. At moderate privacy budgets (\u03B5\
  =4,6), DP-Distil recovers up to 63% of non-private performance while maintaining\
  \ strong empirical privacy (MIA AUC\u22480.5)."
---

# How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding

## Quick Facts
- **arXiv ID**: 2511.14936
- **Source URL**: https://arxiv.org/abs/2511.14936
- **Reference count**: 9
- **Primary result**: DP-Distil recovers up to 63% of non-private performance at ε≥4, outperforming direct DP training and synthetic approaches

## Executive Summary
This study systematically compares four privacy-preserving pipelines for ICD-9 coding from clinical discharge summaries, finding that knowledge distillation from differentially private teachers consistently outperforms direct DP training and synthetic data approaches. At moderate privacy budgets (ε=4,6), DP-Distil recovers up to 63% of non-private performance while maintaining strong empirical privacy (MIA AUC≈0.5). DP-Synthetic failed to scale with privacy budget, plateauing at F1≈0.22, while LoRA-No-DP showed measurable vulnerability (MIA AUC=0.565) despite partial implicit privacy protection. The 1B student models distilled from 3B teachers matched or slightly exceeded teacher performance across all privacy budgets, confirming both superior utility and deployment efficiency.

## Method Summary
The study compares four privacy-preserving training pipelines for ICD-9 coding using MIMIC-III discharge summaries: (1) direct DP-SGD training of a 1B model, (2) DP-synthetic data generation followed by standard training, (3) knowledge distillation from DP-trained teachers, and (4) LoRA fine-tuning without formal privacy guarantees. All pipelines use a 1B decoder-only transformer for final inference. DP-SGD is implemented with Moment Accountant accounting, and privacy is evaluated via membership inference attacks (MIA) using loss, confidence, and entropy features. Utility is measured by F1 score on top-10 ICD-9 codes. The study systematically varies privacy budgets (ε∈{2,4,6}) and compares performance across all four pipelines.

## Key Results
- DP-Distil recovers up to 63% of non-private performance at ε≥4 while maintaining strong empirical privacy (MIA AUC≈0.5)
- DP-Synthetic plateaus at F1≈0.22 regardless of privacy budget, failing to scale
- LoRA-No-DP shows measurable vulnerability (MIA AUC=0.565) despite partial implicit privacy protection
- 1B student models from DP-Distil match or exceed 3B teacher performance across all privacy budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from DP-trained teachers recovers more utility than direct DP-SGD at matched privacy budgets (ε≥4).
- Mechanism: High-capacity teacher models (3B parameters) absorb DP noise more robustly due to parameter redundancy, then transfer learned representations via soft labels (continuous logit distributions). These soft labels encode relative class likelihoods rather than hard assignments, providing richer supervision that acts as a regularizer for the student. The student inherits (ε, δ) guarantees through post-processing—no additional privacy cost accrues from distillation.
- Core assumption: Teachers trained at ε/2 retain sufficient signal to produce meaningful soft labels; student capacity is adequate to absorb transferred knowledge.
- Evidence anchors:
  - [abstract]: "knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63% of the non-private performance"
  - [section 4.1]: "High-capacity teacher models to learn more robust representations under DP-SGD despite the injected noise... The use of soft labels plays a central role: rather than exposing the student to one-hot class assignments... the continuous logit distribution encodes relative class likelihoods"
  - [corpus]: Weak direct corpus support for distillation mechanism specifically; neighboring papers focus on synthetic data generation rather than teacher-student architectures.
- Break condition: At very tight budgets (ε=2), budget splitting (ε/2 per teacher) leaves insufficient signal—DP-Small may outperform. Breaks if teacher capacity is insufficient for the task complexity.

### Mechanism 2
- Claim: Synthetic clinical text generation under DP constraints fails to preserve diagnostic signal even at relaxed privacy budgets.
- Mechanism: DP-SGD noise injected during generator training corrupts the model's ability to reproduce statistical and lexical diversity of discharge summaries. This causes label-distribution drift and loss of rare diagnostic patterns. Unlike distillation (which transfers via soft labels), synthetic generation must reconstruct full text—losing task-relevant structure.
- Core assumption: The failure is due to generation fidelity, not downstream classifier capacity; the classifier architecture is matched across pipelines.
- Evidence anchors:
  - [section 3.1]: "DP-Synthetic plateaus at F1≈0.22... remains flat for DP-Synthetic (Figure 2, ≈ 0.22 F1 across ε)"
  - [section 4.2]: "The DP generator struggles to reproduce the statistical and lexical diversity of true discharge summaries, leading to label-distribution drift and loss of rare diagnostic patterns"
  - [corpus]: Paper 96274 ("Synthesizing Privacy-Preserving Text Data") addresses computational barriers to DP synthetic generation but doesn't validate clinical utility.
- Break condition: May succeed with substantially larger generators, different conditioning strategies, or relaxed ε beyond tested range—but current evidence shows consistent failure.

### Mechanism 3
- Claim: LoRA fine-tuning provides measurable but incomplete implicit privacy protection against membership inference.
- Mechanism: Low-rank updates (r=4) constrain the effective parameter space, limiting memorization capacity. This reduces—but does not eliminate—the model's ability to encode training-example-specific patterns exploitable by MIAs.
- Core assumption: The implicit privacy stems from rank constraints, not from regularization effects or data augmentation.
- Evidence anchors:
  - [section 3.2]: "LoRA-No-DP achieves MIA AUC of 0.565... representing a 13% relative increase in attack success over random guessing"
  - [section 4.3]: "Low-rank constraints inherently limit memorisation... However, the gap to formal DP methods (AUC 0.49–0.51) confirms that LoRA alone cannot replace rigorous privacy guarantees"
  - [corpus]: No direct corpus validation; implicit privacy claims in neighboring literature focus on federated learning rather than LoRA.
- Break condition: Higher LoRA ranks (r≥16) likely increase vulnerability; different attack classes (attribute inference, extraction) may show different patterns.

## Foundational Learning

- **Differential Privacy (ε, δ accounting)**:
  - Why needed here: All DP pipelines require understanding how ε controls privacy loss and how sequential composition (splitting ε across teachers) affects guarantees.
  - Quick check question: If you split ε=4 equally across two teachers using sequential composition, what is each teacher's individual budget?

- **Knowledge Distillation (soft labels vs. hard labels)**:
  - Why needed here: DP-Distil's advantage depends on understanding why soft labels (logit distributions) transfer more information than one-hot targets.
  - Quick check question: Why might a student model outperform its teacher when trained on soft labels?

- **Membership Inference Attacks (MIA)**:
  - Why needed here: Empirical privacy evaluation uses MIA AUC≈0.5 as the target; understanding attack vectors (loss, confidence, entropy) informs why DP noise neutralizes them.
  - Quick check question: An MIA AUC of 0.5 indicates what about the model's privacy properties?

## Architecture Onboarding

- **Component map**:
  - DP-Small: 1B model → DP-SGD training → direct deployment
  - DP-Synthetic: 3B generator (DP-SGD) → synthetic notes → 1B classifier (no DP) → deployment
  - DP-Distil: 3B generator (ε/2) + 3B classifier (ε/2) → synthetic notes + soft labels → 1B student (no DP) → deployment
  - LoRA-No-DP: 1B model → standard LoRA → deployment (no formal guarantee)

- **Critical path**: For production deployment at ε≥4, implement DP-Distil: (1) train 3B generative teacher under DP-SGD with budget ε/2, (2) train 3B classification teacher under DP-SGD with budget ε/2, (3) generate synthetic data, (4) extract soft labels, (5) distill to 1B student using MSE loss on logits. Total training: 38–49 hours on single RTX 6000 Ada.

- **Design tradeoffs**:
  - DP-Distil: Highest utility at ε≥4, but 5–16× longer training than alternatives
  - DP-Small: Fastest DP training (5–10h), best at ε=2, but 8.7% lower F1 at ε=6
  - DP-Synthetic: Currently unsuitable—plateaus regardless of budget
  - All DP pipelines produce identical 1B inference models (13ms/example, 4.4GB VRAM)

- **Failure signatures**:
  - DP-Distil underperforms at ε=2 (budget splitting leaves weak teachers)
  - DP-Synthetic plateaus at F1≈0.22 regardless of ε
  - LoRA shows elevated MIA AUC (0.565) indicating residual vulnerability
  - Student-teacher gap narrows at strict budgets (Figure 4)

- **First 3 experiments**:
  1. Replicate DP-Small at ε∈{2,4,6} on a subset of ICD-9 codes (e.g., top-10) to validate DP-SGD implementation and establish baseline utility curve.
  2. Train single 3B DP teacher at ε=4 and distill to 1B student; compare against DP-Small to confirm distillation advantage emerges at moderate budgets.
  3. Run MIA evaluation on LoRA-No-DP and DP-Distil (ε=4) using the 5-feature logistic regression attack; verify AUC≈0.5 for DP and >0.55 for LoRA.

## Open Questions the Paper Calls Out
- Do the relative rankings of privacy-preserving pipelines (DP-Distil > DP-Small > DP-Synthetic) hold across multi-hospital datasets with different documentation styles and coding practices?
- Can DP-Synthetic performance be improved through better generative architectures, or is the fidelity loss fundamental to differentially private text generation for clinical tasks?
- How do these pipelines fare against privacy attacks beyond membership inference, such as attribute inference or training data extraction?
- Does the 512-token truncation systematically disadvantage certain pipelines or ICD codes by omitting relevant diagnostic context?

## Limitations
- Training stability under DP constraints not extensively characterized across hyperparameter variations
- Evaluation limited to single structured prediction task (ICD-9 coding) from one institution
- Privacy evaluation limited to membership inference attacks; other attack surfaces not assessed
- DP-Distil's computational cost (5–16× longer training) may limit practical adoption

## Confidence
- **High Confidence**:
  - DP-Distil outperforms DP-Small and DP-Synthetic at moderate-to-loose privacy budgets (ε≥4)
  - Synthetic data generation under DP constraints fails to preserve diagnostic signal, plateauing at F1≈0.22
  - LoRA fine-tuning provides incomplete implicit privacy protection (MIA AUC=0.565)
  - Student models distilled from 3B teachers match or exceed teacher performance at all tested budgets

- **Medium Confidence**:
  - DP-Distil recovers up to 63% of non-private performance at ε=4,6
  - The mechanism of soft-label transfer is the primary driver of DP-Distil's advantage
  - DP-Small is optimal at ε=2 but underperforms at higher budgets

- **Low Confidence**:
  - Generalization of findings to other clinical NLP tasks or non-ICD coding problems
  - Extrapolation of utility curves beyond tested privacy budgets
  - Long-term stability and security of LoRA-based implicit privacy under evolving attack methods

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary DP-SGD learning rates, batch sizes, and noise multipliers at ε∈{2,3,4} for DP-Small and DP-Distil. Measure impact on both utility (F1) and empirical privacy (MIA AUC).

2. **Cross-Task Generalization**: Apply DP-Distil and DP-Small to a different clinical NLP task (e.g., clinical named entity recognition or relation extraction) using the same privacy budgets (ε=2,4,6). Compare utility and privacy outcomes to ICD-9 coding results.

3. **Expanded Privacy Evaluation**: Conduct MIA with alternative feature sets (e.g., embedding distances, attention weights) and evaluate additional attack classes (attribute inference, property inference, extraction). Compare against DP-Distil (ε=4) and LoRA-No-DP to determine if the observed privacy gaps persist across attack modalities.