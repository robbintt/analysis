---
ver: rpa2
title: 'VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent
  Regularized Preferences'
arxiv_id: '2503.13817'
source_url: https://arxiv.org/abs/2503.13817
tags:
- reward
- preference
- learning
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VARP, a preference-based RL method that improves
  reward learning from vision-language models (VLMs) by combining trajectory sketches
  with agent-aware regularization. The key insight is that single final-state images
  lack sufficient context for accurate preference judgments, so the authors overlay
  2D trajectory sketches on final observations to provide temporal context.
---

# VARP: Reinforcement Learning from Vision-Language Model Feedback with Agent Regularized Preferences

## Quick Facts
- arXiv ID: 2503.13817
- Source URL: https://arxiv.org/abs/2503.13817
- Authors: Anukriti Singh; Amisha Bhaskar; Peihong Yu; Souradip Chakraborty; Ruthwik Dasyam; Amrit Bedi; Pratap Tokekar
- Reference count: 36
- Primary result: Improves preference accuracy from ~68% to 84% and success rates from <50% to ~80% in robotics tasks

## Executive Summary
This paper introduces VARP, a preference-based RL method that improves reward learning from vision-language models (VLMs) by combining trajectory sketches with agent-aware regularization. The key insight is that single final-state images lack sufficient context for accurate preference judgments, so the authors overlay 2D trajectory sketches on final observations to provide temporal context. Additionally, they incorporate agent performance into the reward learning objective to prevent misalignment as policies evolve. Experiments on MetaWorld and DMControl benchmarks show that VARP improves preference accuracy by approximately 15-20% and increases success rates from below 50% to around 80%.

## Method Summary
VARP combines vision-language model feedback with trajectory sketches and agent-aware regularization for preference-based reinforcement learning. The method generates 2D trajectory sketches overlaid on final observations by projecting end-effector positions onto the camera plane with color-coded temporal progression. A two-stage VLM prompting process first analyzes trajectory pairs, then extracts preference labels. The reward model is trained using Bradley-Terry loss plus an agent regularization term that penalizes rewards when the current policy achieves low returns. The method is evaluated on MetaWorld (Drawer Open, Soccer, Sweep Into) and DMControl (Walker, Cheetah) benchmarks using Soft Actor-Critic as the underlying RL algorithm.

## Key Results
- Improves VLM preference accuracy from ~68% to 84% by incorporating trajectory sketches
- Increases success rates from below 50% to around 80% in manipulation tasks
- Boosts episode returns by 20-30% in locomotion tasks compared to standard approaches
- Reduces reward-policy misalignment metric from 0.35 to 0.15 with agent regularization

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Sketches Provide Temporal Context for VLMs
- **Claim:** Overlaying 2D trajectory sketches on final-state images improves VLM preference accuracy by revealing motion history that single images cannot capture.
- **Mechanism:** The robot's end-effector positions are projected onto the camera's 2D plane using known camera parameters, then color-coded (yellow→dark) to show temporal progression. This enriched visual representation allows VLMs to assess path efficiency, not just final outcomes.
- **Core assumption:** VLMs can visually parse trajectory patterns and correlate them with task efficiency when explicitly rendered, even if they cannot infer motion from static images alone.
- **Evidence anchors:** [abstract] "overlay 2D trajectory sketches on final observations to provide temporal context...improving preference accuracy by approximately 15-20%"; [Section IV-B] "color-coding the connecting lines so that their hue shifts from bright yellow at the start to a darker tone at the end"; [Section V-B] "using final images alone yields an average preference accuracy of 68%...incorporating sketched trajectories increases this accuracy to 84%"

### Mechanism 2: Agent-Aware Regularization Prevents Reward-Policy Misalignment
- **Claim:** Incorporating the agent's expected return into the reward loss prevents the reward model from over-valuing trajectories that the current policy cannot achieve high returns on.
- **Mechanism:** The total loss combines VLM preference loss (Bradley-Terry cross-entropy) with an agent penalty term: L_agent(ν) = λ · E_τ∼πθ[-R(τ)]. This grows when the agent's return is low, discouraging r_ν from assigning high rewards to behaviors the current policy executes poorly.
- **Core assumption:** Trajectories with low returns under the current policy indicate either suboptimal behavior or reward misalignment that should be penalized during reward learning.
- **Evidence anchors:** [abstract] "regularize reward learning by incorporating the agent's performance...boosts episode returns by 20-30% in locomotion tasks"; [Section IV-C] "the second term grows when the agent's return is small, preventing r_ν from assigning large rewards to unsuccessful behaviors"; [Section V-D] "agent preferences provide effective regularization...reducing the misalignment metric from 0.35 to 0.15"

### Mechanism 3: Two-Stage VLM Prompting Decomposes Analysis and Labeling
- **Claim:** Separating VLM queries into (1) free-form analysis and (2) constrained label extraction yields more reliable preferences than single-stage prompting.
- **Mechanism:** First, the VLM generates an unstructured description comparing both images against the task goal. Second, the prior analysis is fed back with a prompt requesting a discrete label {-1, 0, 1}. This decomposition reduces ambiguity by forcing explicit reasoning before commitment.
- **Core assumption:** VLMs benefit from "thinking aloud" before producing structured outputs, similar to chain-of-thought prompting for language tasks.
- **Evidence anchors:** [Section IV-A] "In the analysis stage, the VLM generates a free-form response...In the labeling stage, we prompt the VLM again with the prior analysis"; [Figure 1 caption] shows qualitative difference: without sketches, VLM gives vague reasoning; with sketches, it identifies "cleaner and more linear" trajectories

## Foundational Learning

- **Bradley-Terry Preference Model**
  - **Why needed here:** VARP learns rewards by fitting preference probabilities to VLM labels. Understanding how P(τ_a ≻ τ_b) = exp(Σr) / (exp(Σr_a) + exp(Σr_b)) connects cumulative rewards to pairwise comparisons is essential for debugging reward loss.
  - **Quick check question:** If two trajectories have equal true returns, what probability should the Bradley-Terry model assign to preferring either one? (Answer: 0.5)

- **Bilevel Optimization**
  - **Why needed here:** VARP alternates between (outer) updating the reward model and (inner) optimizing the policy. This nested structure means reward updates implicitly affect future policy data distribution, which motivates the regularization term.
  - **Quick check question:** Why is ignoring the inner-loop policy change during outer-loop reward updates a problem? (Answer: The reward may be optimized for data the policy no longer generates.)

- **Off-Policy RL with Learned Rewards (SAC)**
  - **Why needed here:** VARP uses Soft Actor-Critic to optimize π_θ given r_ν. SAC's off-policy nature allows replay buffer relabeling when rewards update, but introduces staleness if rewards drift significantly.
  - **Quick check question:** After updating r_ν, what must be done to the replay buffer before the next policy update? (Answer: Relabel all transitions with the new reward function.)

## Architecture Onboarding

**Component map:**
Environment Rollout → Trajectory Data (s, a, s') → 2D Sketch Projection → Augmented Observation ô = (o, Sketch(τ)) → VLM Two-Stage Query → Preference Dataset D → Reward Model Training (L_VLM + λ · L_agent) → r_ν → Replay Buffer Relabeling → SAC Policy Update → π_θ

**Critical path:**
1. Sketch generation requires camera intrinsics/extrinsics and end-effector tracking (or pixel tracking as fallback).
2. VLM query latency (two GPT-4o calls per pair) is the main throughput bottleneck.
3. Reward model updates must complete before policy updates to avoid stale gradients.

**Design tradeoffs:**
| Decision | Option A | Option B | Tradeoff |
|----------|----------|----------|----------|
| Sketch detail | End-effector only | Full arm joints | A is faster; B captures more motion but cluttered visuals |
| Regularization weight λ | Low (0.01) | High (1.0) | Low trusts VLM more; High prevents reward hacking but may ignore VLM |
| VLM model | GPT-4o | Open-source VLM | 4o has higher accuracy; open-source enables local deployment |

**Failure signatures:**
- Preference accuracy stuck at ~50%: Sketches not visible or task prompt unclear.
- Reward loss decreases but policy returns collapse: λ too high or VLM labels noisy.
- VLM consistently returns y=-1 (no preference): Image pairs too similar; increase trajectory diversity.

**First 3 experiments:**
1. **Validate sketch generation:** Run 10 episodes on MetaWorld Drawer Open, visualize sketches overlaid on final frames, verify temporal color gradient and end-effector path accuracy.
2. **Ablate sketches vs. no-sketches:** Collect 100 trajectory pairs, query VLM with and without sketches, measure preference accuracy against ground-truth rewards (target: >15% gap).
3. **Sweep λ values:** Train VARP on Walker (DMControl) with λ ∈ {0.0, 0.1, 0.5, 1.0}, plot episode returns vs. training steps to identify regularization sweet spot (expect peak at moderate λ).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the trade-off between query cost and feedback granularity be optimized when extending VARP to tasks with long horizons or multiple sub-goals?
- **Basis in paper:** [explicit] The authors state that "tasks with extensive sub-goals or very long horizons could benefit from more granular feedback" but note that "supplying additional intermediate images or subgoal annotations to the VLM... would require significantly more queries and also computationally expensive... optimizing that trade-off remains an open question."
- **Why unresolved:** The paper only evaluates on relatively short-horizon tasks; the computational and annotation costs of multi-step feedback mechanisms were not analyzed.
- **What evidence would resolve it:** Experiments on long-horizon tasks (e.g., multi-stage manipulation) comparing query budgets and policy performance under varying feedback granularities.

### Open Question 2
- **Question:** How robust is VARP under significant occlusions or unusual camera viewpoints where 2D trajectory projections may be ambiguous or incomplete?
- **Basis in paper:** [explicit] The authors acknowledge that "Generating 2D trajectory sketches adds modest overhead, particularly under occlusions or unusual camera viewpoints."
- **Why unresolved:** The experiments use fixed camera setups in simulation; sensitivity to viewpoint variation and partial observability was not systematically tested.
- **What evidence would resolve it:** Ablation studies with varying camera angles and synthetic occlusions, reporting preference accuracy and policy performance degradation curves.

### Open Question 3
- **Question:** Can VARP maintain its performance advantages when deployed on physical robots with unknown or imprecise camera calibration parameters?
- **Basis in paper:** [inferred] The method requires "known camera parameters (focal length, orientation, position)" for accurate sketch projection, but real-world deployments often lack precise calibration.
- **Why unresolved:** All experiments are conducted in simulation with perfect state information; sim-to-real transfer and robustness to calibration noise are not addressed.
- **What evidence would resolve it:** Real-world robotic experiments comparing VARP with ground-truth vs. estimated camera parameters, measuring success rate and preference accuracy.

## Limitations

- Camera calibration dependency: Sketch generation requires accurate camera parameters that may not be available in real-world deployments
- VLM query costs: Two GPT-4o calls per preference pair creates significant computational overhead
- Simulation-only evaluation: Experiments conducted entirely in simulation without physical robot validation
- Limited task diversity: Evaluation restricted to specific MetaWorld and DMControl benchmarks

## Confidence

- **High**: The effectiveness of trajectory sketches in improving preference accuracy (supported by clear before/after comparisons)
- **Medium**: The agent regularization mechanism and overall performance improvements (reasonable but could benefit from more extensive ablation)
- **Medium**: The two-stage VLM prompting approach (plausible but not thoroughly validated against alternatives)

## Next Checks

1. **Camera Parameter Sensitivity**: Test VARP performance across varying levels of camera calibration accuracy (5%, 10%, 20% noise) to quantify the robustness of sketch generation and its downstream impact on preference learning.

2. **VLM Model Agnosticism**: Replace GPT-4o with an open-source VLM (e.g., LLaVA) to verify that the method's effectiveness doesn't depend on proprietary models, and assess performance differences.

3. **Cross-Task Generalization**: Apply VARP to a novel robotics task (e.g., block stacking or drawer closing) not seen during development to evaluate whether the sketch + regularization approach generalizes beyond the trained benchmarks.