---
ver: rpa2
title: 'NOIR: Privacy-Preserving Generation of Code with Open-Source LLMs'
arxiv_id: '2601.16354'
source_url: https://arxiv.org/abs/2601.16354
tags:
- code
- cloud
- client
- noir
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NOIR, the first framework to protect client
  prompts and generated code from cloud observation in LLM-based code generation.
  NOIR uses an encoder and decoder at the client to encode prompts, send embeddings
  to the cloud, and decode enriched embeddings to generate code locally.
---

# NOIR: Privacy-Preserving Generation of Code with Open-Source LLMs

## Quick Facts
- **arXiv ID:** 2601.16354
- **Source URL:** https://arxiv.org/abs/2601.16354
- **Reference count:** 40
- **Primary result:** Achieves 76.7% Pass@1 on MBPP with only 1.77% drop from original LLM while protecting prompts from cloud observation.

## Executive Summary
NOIR is the first framework to protect client prompts and generated code from cloud observation in LLM-based code generation. It uses an encoder and decoder at the client to encode prompts, send embeddings to the cloud, and decode enriched embeddings to generate code locally. To prevent cloud inference of sensitive content, NOIR introduces an indistinguishability-preserving vocabulary (INDVOCAB) that adaptively randomizes token embeddings and a local randomized tokenizer (LTOKENIZER) that assigns tokens to random indices. These components defend against reconstruction and frequency analysis attacks while maintaining high model performance. Extensive experiments using open-source LLMs on benchmarks show that NOIR achieves strong privacy guarantees with minimal performance degradation while significantly outperforming state-of-the-art baselines.

## Method Summary
NOIR employs a split learning architecture where the LLM is divided into client-side encoder/decoder and cloud-side middle blocks. The client uses LTOKENIZER to map tokens to random indices and INDVOCAB with Adaptive Randomized Response (ARR) to inject privacy-preserving noise into embeddings. During split tuning (STUNING), gradients flow back through the cloud's LoRA adapter to update the client-side models. The framework fine-tunes on CodeAlpaca and evaluates on MBPP, HumanEval, and BigCodeBench, measuring Pass@1/r for functionality and Bleu/Rouge/CodeBleu for reconstruction attacks.

## Key Results
- Achieves 76.7% Pass@1 on MBPP with only 1.77% drop from original LLM at ε=13
- Reduces Vec2Text reconstruction success to near-zero levels while maintaining code functionality
- Significantly outperforms T2T and SnD baselines on both utility and privacy metrics
- Maintains performance across different open-source LLM backbones (CodeLlama, CodeQwen, Llama3)

## Why This Works (Mechanism)

### Mechanism 1: Indistinguishability via Adaptive Randomization (ARR)
NOIR employs ARR within INDVOCAB to probabilistically flip embedding features based on similarity, maintaining angular correlation while disrupting geometric proximity needed for reconstruction attacks. The privacy guarantee depends on constraining ε sufficiently; at ε=13, reconstruction attacks are effectively blocked while preserving code functionality.

### Mechanism 2: Gradient Obfuscation via Secret Permutation (LTOKENIZER)
LTOKENIZER assigns tokens to random indices client-side, causing gradients during split tuning to correspond to meaningless indices at the cloud. This breaks BiSR-style gradient reconstruction attacks by preventing the cloud from linking gradients to specific client tokens. The defense assumes the permutation mapping remains secret from the cloud.

### Mechanism 3: Distribution Realignment via Split Tuning (STUNING)
STUNING fine-tunes client encoder/decoder and cloud-side LoRA simultaneously to recover utility lost to privacy noise. By computing gradients per data point, the model learns to align the noisy input distribution with the cloud's latent space. The framework is fragile—training only the decoder or using a shortcut model yields zero functionality, proving all three components are essential.

## Foundational Learning

- **Concept: Split Learning (SL)**
  - Why needed: NOIR is fundamentally a Split Learning architecture where the model is cut into three segments. Understanding SL is required to grasp why the client only holds the first/last layers and how gradients flow back from the cloud to the client without sharing raw data.
  - Quick check: Can the cloud model update its weights if the client sends only embeddings and receives only gradients? (Answer: Yes, via split backpropagation).

- **Concept: Local Differential Privacy (LDP)**
  - Why needed: The paper extends LDP to the "token embedding level" (ε-IND). You must distinguish between standard DP (protecting dataset membership) and NOIR's goal (protecting prompt content from an honest-but-curious curator).
  - Quick check: In NOIR, does the cloud know which dataset the prompt came from? (Answer: Yes, the threat is reconstructing the prompt content itself, not dataset membership).

- **Concept: Reconstruction Attacks (Vec2Text / BiSR)**
  - Why needed: The defense mechanisms are specifically designed to break these attacks. Vec2Text inverts embeddings back to text; BiSR uses gradients to invert training data. Without understanding these threats, the complexity of ARR and LTOKENIZER seems unmotivated.
  - Quick check: Why does NOIR need LTOKENIZER if it already has INDVOCAB? (Answer: INDVOCAB protects embeddings from Vec2Text; LTOKENIZER protects the one-hot vectors/gradients from BiSR during fine-tuning).

## Architecture Onboarding

- **Component map:**
  Client Node: LTOKENIZER -> INDVOCAB -> Encoder (1 attention block) -> [Network] -> Cloud Middle -> [Network] -> Client Decoder (4 attention blocks) -> Code

- **Critical path:**
  The setup of the INDVOCAB (Algorithm 1, lines 4-8) is the most critical initialization step. If the randomization bounds β_i are calculated incorrectly relative to ε, the privacy guarantee breaks or the noise destroys utility.

- **Design tradeoffs:**
  - Pass@r vs. Cost: Increasing r improves utility significantly but linearly increases client-side inference cost and cloud API calls
  - Vocabulary Size: Larger vocabularies inherently offer stronger privacy but require more memory for the embedding table

- **Failure signatures:**
  - Semantic Drift: Generated code is syntactically valid but logically incorrect
  - Reconstruction ASR > 0: If BLEU scores of reconstructed prompts exceed 20 during auditing, ε is likely too high
  - Convergence Failure: Loss plateaus immediately during STUNING, typically caused by LTOKENIZER mapping mismatch

- **First 3 experiments:**
  1. Baseline Privacy Check: Run defense-free split model on MBPP to confirm standard Vec2Text can reconstruct prompts
  2. Ablation of Noise Mechanisms: Run NOIR with INDVOCAB but without LTOKENIZER to measure BiSR ASR
  3. Utility Scaling: Fix ε=13 and vary training dataset size to replicate finding that larger data mitigates noise impact

## Open Questions the Paper Calls Out

### Open Question 1
How can NOIR be enhanced to defend against adaptive adversaries who exploit cross-embedding similarities and clustering across multiple prompts to infer semantic patterns? The authors note that cross-embedding similarity could reveal semantic relationships through clustering, and while dynamic prefix tokens disrupted some patterns, sophisticated adversaries might still discover frequent token embedding sequences to bypass ε-IND.

### Open Question 2
Can a representation-learning module (e.g., D-VAE) effectively bridge the gap between open-source client encoders/decoders and black-box proprietary middle blocks with mismatched embedding dimensions? Preliminary results using a D-VAE adapter achieved only 12.4% Pass@1 on MBPP, indicating the alignment mechanism is not yet practically usable for heterogeneous models.

### Open Question 3
Is it possible for the cloud to detect and block malicious embeddings (e.g., requests for malware generation) without breaking the privacy guarantees or utility of NOIR? The authors note that the cloud must avoid responding to malicious embeddings from harmful prompts, which is harder than detecting harmful prompts since embeddings are intentionally obfuscated via INDVOCAB.

## Limitations

- The LTOKENIZER defense against gradient-based attacks relies on the assumption that a secret permutation mapping can be maintained securely in client environments, which is not independently verified
- The STUNING mechanism is fragile—if the client-side encoder is too small or noise level too high, the entire framework may fail to converge
- Privacy accounting for ARR is based on theoretical upper bounds without empirical validation of actual privacy loss in practice

## Confidence

- **High Confidence:** Core mechanism of INDVOCAB + ARR for protecting embeddings from Vec2Text-style reconstruction attacks
- **Medium Confidence:** Overall framework performance claims (76.7% Pass@1 on MBPP with 1.77% drop)
- **Low Confidence:** LTOKENIZER's effectiveness against gradient-based attacks and security of secret permutation assumption

## Next Checks

1. **Reconstructability Baseline:** Implement Vec2Text attack on defense-free split model using CodeLlama-7B to confirm threat model validity by measuring BLEU scores on MBPP

2. **Gradient Obfuscation Validation:** Implement BiSR-style attack on NOIR with INDVOCAB but without LTOKENIZER to quantify the randomized tokenizer's specific contribution to breaking gradient-based reconstruction

3. **Privacy-Utility Frontier:** Fix ε=13 and systematically vary training dataset size (18k, 90k, 180k, 376k samples) to empirically validate that larger datasets mitigate utility loss from privacy noise and identify minimum viable dataset size