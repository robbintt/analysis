---
ver: rpa2
title: Online Competitive Information Gathering for Partially Observable Trajectory
  Games
arxiv_id: '2506.01927'
source_url: https://arxiv.org/abs/2506.01927
tags:
- information
- planning
- each
- which
- players
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses online planning for non-cooperative robots
  in partially observable settings where agents must gather information about opponents
  to plan optimally. While modern MARL methods can solve such problems offline, online
  planning without heavy pre-computation remains challenging.
---

# Online Competitive Information Gathering for Partially Observable Trajectory Games

## Quick Facts
- arXiv ID: 2506.01927
- Source URL: https://arxiv.org/abs/2506.01927
- Reference count: 29
- Key outcome: Extends model-predictive game play to imperfect-information games using particle-based joint state-observation distributions and stochastic gradient play, enabling active information gathering in continuous pursuit-evasion and warehouse-pickup scenarios

## Executive Summary
This work addresses online planning for non-cooperative robots in partially observable settings where agents must gather information about opponents to plan optimally. While modern MARL methods can solve such problems offline, online planning without heavy pre-computation remains challenging. The key innovation extends model-predictive game play to imperfect-information games by planning policies that map observation histories to actions, rather than single trajectories. This enables active information gathering behavior where agents briefly turn to gather information before executing plans based on what they learned.

## Method Summary
The method uses particle-based representations of joint state-observation distributions and stochastic gradient play to find approximate Nash equilibria in the planning game. Agents maintain K_all particles representing joint distributions over states and observation histories, sample K_batch particles for gradient updates, and use feedforward neural networks to map observation histories to actions. The approach successfully handles the belief hierarchy problem by maintaining joint distributions over states and observation histories rather than higher-order beliefs about opponents' beliefs. Timing results show the method takes 0.06-0.22 seconds per gradient step per player, suggesting real-time execution is within reach with optimization.

## Key Results
- Active information gathering behavior demonstrated in continuous pursuit-evasion and warehouse-pickup scenarios
- Outperforms passive competitors that plan without considering future observations
- Scales to N-player games and environments with obstacles
- Particle-based approach successfully handles belief hierarchy problem
- Timing results suggest real-time execution is feasible with optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planning policies that map observation histories to actions enables agents to anticipate and exploit future information.
- Mechanism: Rather than computing open-loop trajectories, agents compute policies π_θ(z[t]) that map finite observation histories to actions. This creates branching plans where different future observations yield different actions, allowing deliberate information-gathering maneuvers (e.g., turning to check a location before committing).
- Core assumption: A finite history of length T_past is sufficiently informative to capture relevant uncertainty (Assumption 3 restricts to pure strategies).
- Evidence anchors:
  - [abstract] "finite-history/horizon refinement of POSGs which admits competitive information gathering behavior in trajectory space"
  - [section III-B] "Rather than planning single trajectories, players find maps from possible future observation histories to trajectories"
  - [corpus] Sequential Monte Carlo for POMDPs (FMR=0.72) supports particle-based approaches in continuous partially observable settings.
- Break condition: When T_past is insufficient for strategic complexity—HIDE & SEEK evader performance degraded because "optimal evader strategy...is too complex to be captured with only T_past past observations."

### Mechanism 2
- Claim: Maintaining the joint distribution q_t(x, z) over states and all players' observation histories eliminates the need for recursive belief hierarchies.
- Mechanism: Instead of tracking "I believe you believe I believe..." (belief hierarchies that grow exponentially), agents maintain a single particle-based joint distribution. Since all agents solve the identical equilibrium problem (Eq. 1), this joint distribution is common and sufficient for planning.
- Core assumption: All agents solve the same equilibrium problem and find the same equilibrium (common knowledge of rationality and game structure—Assumption 1).
- Evidence anchors:
  - [abstract] "particle-based representations of joint state-observation distributions"
  - [section III-C] "maintaining q_t as the joint distribution over histories of observations and states, players need not track a belief hierarchy"
  - [corpus] Weak direct corpus support for this specific joint-distribution avoidance of belief hierarchies.
- Break condition: Equilibrium selection failure (Figure 2)—when pursuer and evader converge to different local equilibria, their q_t distributions diverge, causing inaccurate opponent modeling.

### Mechanism 3
- Claim: Monte Carlo sampling over particles with gradient descent finds approximate Nash equilibria in continuous trajectory games.
- Mechanism: Sample K_batch particles from q_t, roll out policies for T_future steps using current θ, compute costs via automatic differentiation, apply gradient updates via Adam. Iterate until cost change < ε (Algorithm 2).
- Core assumption: Game components (T, O, r) are differentiable and pure-strategy equilibria exist (Assumptions 2, 3).
- Evidence anchors:
  - [abstract] "stochastic gradient play to solve the resulting game equilibria"
  - [section IV-A] "Using automatic differentiation, the cost gradient with respect to the policy parameters ∇_θ[c(x)p(x,z)] is calculated"
  - [corpus] Sequential Monte Carlo for POMDPs (FMR=0.72) provides theoretical grounding for particle-based policy optimization.
- Break condition: Games admitting only mixed-strategy equilibria; insufficient K_batch causes convergence to "information-unaware minimum" (Table IV).

## Foundational Learning

- **Partially Observable Stochastic Games (POSGs)**
  - Why needed here: The entire formulation extends POSGs—the N-player generalization of POMDPs. You must understand the tuple (N, X, A, Z, T, O, r, X₀) and why planning is NP-hard.
  - Quick check question: Why does partial observability in multi-agent settings create fundamentally different challenges than single-agent POMDPs?

- **Nash Equilibrium and Gradient Play**
  - Why needed here: The method seeks Nash equilibria via iterative gradient updates. Convergence is not guaranteed—understanding when gradient play fails is critical.
  - Quick check question: What game structures cause gradient play to oscillate rather than converge?

- **Particle Filtering and Monte Carlo Integration**
  - Why needed here: q_t is maintained via particles; expected costs are estimated via Monte Carlo sampling over K_batch particles.
  - Quick check question: In Algorithm 3, why are particle weights updated only for the γ-conditioned subset rather than all particles?

## Architecture Onboarding

- **Component map:**
  - Particle Store -> Policy Network -> Equilibrium Solver -> Distribution Updater -> Observation Buffer

- **Critical path:**
  1. Sample K_batch particles → 2. Roll out π_θ for T_future steps → 3. Compute ∇_θ cost per player → 4. Adam update → 5. Repeat to convergence → 6. Forward-update all K_all particles → 7. Execute action using real z̄[t]

- **Design tradeoffs:**
  - K_batch: Low = fast but information-unaware (Table IV); High = accurate but slow (~32s at K_batch=630)
  - T_future: Exponential configuration growth, but early information-gathering reduces future uncertainty (Table III)
  - γ: Conditioning fraction—higher γ improves real-world tracking but risks asymmetric opponent models
  - N_eq: Equilibrium candidates—more handles multi-equilibrium games but multiplies compute

- **Failure signatures:**
  - Equilibrium disagreement: Particles diverge from true opponent behavior; pursuing phantom trajectories
  - Information-unaware convergence: Greedy plans with no detour behavior (low K_batch)
  - Non-convergence: Gradient oscillation in games lacking pure-strategy equilibria

- **First 3 experiments:**
  1. **TAG baseline**: Implement 2-player field-of-view tag (T_past=T_future=6, K_all=1000, K_batch=10). Compare active vs passive modes—expect ~20% cost reduction for active players (Table I).
  2. **K_batch ablation**: Run warehouse with K_batch ∈ {2, 6, 15, 39, 100, 251}. Replicate cost trajectory from Table IV—verify information-gathering emerges only above threshold.
  3. **N_eq robustness**: Run TAG with N_eq ∈ {1, 2, 4, 8}. Track pursuer-evader distance and surprisal of opponent position—expect improved opponent modeling with higher N_eq (Figures 6, 7).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the method achieve real-time performance (sub-100ms planning cycles) through GPU acceleration and code optimization while maintaining solution quality?
- Basis in paper: [explicit] Authors state "our method is not real-time" and that "code optimization, GPU support, better handling of matrix sparsity, and improved parallelization all remain as potential improvements."
- Why unresolved: Current implementation takes several seconds per planning step; no optimized implementation was developed or tested.
- What evidence would resolve it: Benchmark results showing planning times under 100ms per step on GPU-accelerated implementations with equivalent or better task performance metrics.

### Open Question 2
- Question: Would extending the method to support mixed (nondeterministic) strategies improve competitive performance in games where pure-strategy equilibria do not exist or are suboptimal?
- Basis in paper: [explicit] In Limitations, authors note "nondeterministic strategies can be permitted with a minor formulation change" and combining with mixed-strategy approaches "may be a source of further improvement in competitive play."
- Why unresolved: The current formulation only considers deterministic strategies (Assumption 3), so the potential benefits of mixed strategies in partially observable trajectory games remain unquantified.
- What evidence would resolve it: Comparative experiments on games without pure-strategy equilibria, measuring task performance and convergence properties between pure and mixed-strategy variants.

### Open Question 3
- Question: How can the "curse of history" be addressed to handle scenarios requiring longer observation histories without introducing hierarchical beliefs?
- Basis in paper: [explicit] Authors state they "assumed a finite history of observations is suitably informative" and note the hide-and-seek evader performed worse because "optimal evader strategy in this game is too complex to be captured with only Tpast past observations."
- Why unresolved: Increasing Tpast increases computational complexity, and the paper's experiments showed degradation when the chosen history length was insufficient for the task.
- What evidence would resolve it: Development and validation of memory-efficient alternatives (e.g., learned belief compression, recurrent policies) that maintain performance on tasks requiring long-term temporal reasoning.

### Open Question 4
- Question: Under what game classes and parameter configurations does gradient play reliably converge to Nash equilibria in this formulation?
- Basis in paper: [inferred] The paper acknowledges "there are some conditions under which gradient play does not converge to a Nash equilibrium" but relies on empirical observation that "it converges reliably enough for online planning" without formal characterization.
- Why unresolved: No theoretical convergence analysis is provided, and empirical testing was limited to pursuit-evasion and warehouse scenarios.
- What evidence would resolve it: Theoretical analysis identifying sufficient conditions for convergence, or systematic empirical mapping of convergence behavior across game structures, horizon lengths, and particle counts.

## Limitations
- Empirical validation limited to two specific scenarios (pursuit-evasion and warehouse pickup)
- Particle-based approach's scalability to higher-dimensional state spaces remains untested
- Equilibrium selection mechanism lacks rigorous justification for avoiding spurious local minima
- Performance degradation when optimal strategies exceed T_past complexity (HIDE & SEEK)

## Confidence
- **High confidence**: The particle-based representation of joint state-observation distributions effectively avoids belief hierarchies - well-supported by mathematical formulation and demonstrated through active information gathering behavior.
- **Medium confidence**: Gradient play reliably finds approximate Nash equilibria in continuous trajectory games - works in presented examples, but convergence guarantees are limited to specific game structures and may fail in mixed-strategy games.
- **Medium confidence**: Finite-history policies capture sufficient strategic complexity for information gathering - validated in tag and warehouse scenarios, but HIDE & SEEK results suggest limitations when optimal strategies exceed T_past complexity.

## Next Checks
1. **Mixed-strategy game test**: Evaluate the method on a game known to require mixed strategies (e.g., rock-paper-scissors variant). Measure whether gradient play oscillates or converges, and whether the particle-based approach can approximate mixed strategies through policy stochasticity.

2. **High-dimensional scaling experiment**: Scale the warehouse scenario to 10+ players with 20+ dimensional state spaces. Track computation time and tracking accuracy as K_all and K_batch increase. Identify the particle count threshold where real-time performance becomes infeasible.

3. **Robustness to observation noise**: Systematically vary observation noise parameters (σ²_base, C_scale) across multiple orders of magnitude. Measure the impact on equilibrium convergence, information gathering quality, and the method's ability to maintain accurate opponent models under extreme uncertainty.