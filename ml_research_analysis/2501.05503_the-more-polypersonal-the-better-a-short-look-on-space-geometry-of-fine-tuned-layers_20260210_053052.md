---
ver: rpa2
title: The more polypersonal the better -- a short look on space geometry of fine-tuned
  layers
arxiv_id: '2501.05503'
source_url: https://arxiv.org/abs/2501.05503
tags:
- bert
- space
- language
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines how BERT's internal representations change\
  \ when fine-tuned on a new grammatical feature (polypersonal agreement) in Russian.\
  \ The authors compare two model architectures\u2014one with a linear layer and another\
  \ with an LSTM layer\u2014trained on data with polypersonal affixes added as either\
  \ prefixes or suffixes to verbs."
---

# The more polypersonal the better -- a short look on space geometry of fine-tuned layers

## Quick Facts
- arXiv ID: 2501.05503
- Source URL: https://arxiv.org/abs/2501.05503
- Reference count: 20
- Fine-tuning BERT with polypersonal agreement changes latent space topology and improves perplexity

## Executive Summary
This paper investigates how BERT's internal representations change when fine-tuned on a novel grammatical feature—polypersonal agreement—in Russian. The authors compare two model architectures (MLP vs. LSTM grammatical layer) trained on data with polypersonal affixes added as prefixes or suffixes. Using topological data analysis, they measure geometric changes in the latent space and evaluate perplexity and masked language modeling performance. Results show that fine-tuning successfully adapts BERT to the new grammatical system, with polypersonal sentences becoming more distinguishable in the latent space. The LSTM-based model produces more clustered representations but smaller geometric shifts compared to the linear layer model.

## Method Summary
The authors fine-tune RuBERT on a synthetic corpus of Russian sentences modified with polypersonal affixes marking object person/number. They create minimal pairs of standard and polypersonal sentences, with affixes added as either suffixes or prefixes. Two model variants are tested: a frozen RuBERT encoder with either an MLP or LSTM grammatical module, followed by an MLM head. Models are pre-trained for 10 epochs on mixed data (50/50 standard and polypersonal). Evaluation includes pseudo-perplexity measurement, topological analysis of layer embeddings using Vietoris-Rips filtration and bottleneck distance, and layer-wise MLM probing via Ecco.

## Key Results
- Fine-tuning successfully adapts BERT to polypersonal agreement, reducing perplexity on modified sentences
- Polypersonal sentences become more distinguishable from standard sentences in the latent space after fine-tuning
- LSTM-based grammatical module produces more clustered representations (smaller bottleneck distances) but smaller geometric shifts compared to MLP

## Why This Works (Mechanism)

### Mechanism 1
Adding a single trainable grammatical layer on top of a frozen pretrained encoder enables syntactic feature disentanglement without catastrophic forgetting. The frozen base model preserves learned representations of the source language, while the new layer (MLP or LSTM) learns to project representations into a space where novel grammatical features become linearly separable from base forms. Core assumption: The pretrained encoder has sufficient representational capacity to encode the new feature; the added layer primarily reorganizes rather than creates new abstractions. Evidence anchors: abstract states adding a grammatical layer causes the model to separate new and old grammatical systems; section 4.3 shows the grammar module disentangles texts with different grammar in a diverse way. Break condition: If base model is too small or the novel feature requires tokenization changes, frozen representations may lack expressiveness needed for disentanglement.

### Mechanism 2
Suffix-based morphological markers are more readily learned than prefix-based markers due to alignment with pretrained tokenization and linguistic expectations. Suffixes align with Russian's existing inflectional patterns and produce subword tokens that the tokenizer can more naturally segment, reducing distributional shift from pretrained embeddings. Core assumption: Tokenizer behavior and pretrained linguistic biases significantly influence feature acquisition difficulty. Evidence anchors: section 3.2 notes inflectional categories are generally encoded suffixally in Russian; section 4.1 states prefix marking tends to be more difficult for models to process. Break condition: In languages with prefix-heavy morphology or with tokenizers optimized differently, this asymmetry may reverse or disappear.

### Mechanism 3
Topological distance metrics (bottleneck distance on persistence diagrams) can quantify representation separation between grammatical systems. By computing Vietoris-Rips filtrations on token embeddings and comparing persistence diagrams across conditions, the authors measure how much the fine-tuned layer restructures the latent manifold. Core assumption: Topological separation correlates with functional disentanglement usable by downstream heads. Evidence anchors: section 3.3 states bottleneck distance gives an estimate on the max norm difference of layer-setting topology; section 4.3 shows fine-tuning layers creates disentangled and linearly separable representations by changing representation topology. Break condition: If embeddings are high-dimensional and sparse, persistence diagrams may be noisy; correlation with task performance is not guaranteed.

## Foundational Learning

- **Persistent homology and bottleneck distance**
  - Why needed here: The paper's primary geometric analysis relies on topological data analysis; understanding what persistence diagrams represent and how bottleneck distance compares them is essential to interpret results.
  - Quick check question: Can you explain what H0 and H1 homology groups capture in point cloud data?

- **Frozen encoder with trainable adapter heads**
  - Why needed here: The experimental design freezes RuBERT and trains only the added grammatical module; this is a specific transfer learning paradigm with distinct failure modes.
  - Quick check question: What are the tradeoffs between freezing vs. full fine-tuning when adapting to out-of-distribution features?

- **Pseudo-perplexity for masked language models**
  - Why needed here: The evaluation uses PLL-word-l2r scoring, which handles multi-token words differently from standard perplexity.
  - Quick check question: Why does standard perplexity fail for words split into multiple subword tokens?

## Architecture Onboarding

- Component map: RuBERT (frozen) -> Grammatical module (MLP or LSTM) -> MLM head (projection + tied output embeddings)

- Critical path:
  1. Prepare minimal pairs (standard vs. polypersonal Russian sentences with suffix or prefix markers)
  2. Freeze RuBERT, initialize grammatical module
  3. Fine-tune on mixed corpus (50/50 standard and polypersonal)
  4. Evaluate pseudo-perplexity and extract layer-wise embeddings for topological analysis

- Design tradeoffs:
  - MLP vs. LSTM grammatical module: LSTM produces more clustered geometry (smaller bottleneck distances) but may be harder to interpret; MLP shows clearer separation but larger variance
  - Suffix vs. prefix marking: Suffix easier to learn but may not generalize to languages with different morphological typology

- Failure signatures:
  - Perplexity gap between standard and polypersonal text remains large after training → grammatical module not learning feature
  - Topological analysis shows no separation between base and polypersonal embeddings → representations not disentangling
  - Prefix model shows explosive growth on single layer without gradual improvement → possible overfitting to local token patterns

- First 3 experiments:
  1. Replicate suffix polypersonal setup on a smaller Russian corpus; verify perplexity gap decreases relative to unfrozen baseline
  2. Ablate grammatical module (direct MLM head only) to confirm added layer is necessary for disentanglement
  3. Visualize persistence diagrams for a sample of sentences to build intuition for bottleneck distance before/after fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do tokenization effects (positional encoding) versus linguistic improbability drive the observed performance differences between prefix and suffix polypersonal models? Basis: The authors state the extent to which each factor contributes is prior for further work. Why unresolved: The study observes prefix marking is harder to learn and results in different topology, but the experimental design does not isolate whether this is caused by the tokenizer splitting the prefix or the violation of Russian phonotactics. What evidence would resolve it: An ablation study using a language with natural prefixing morphology or a control experiment using a custom tokenizer that treats the prefix as a single token.

### Open Question 2
Why does the LSTM adapter layer exhibit significantly smaller bottleneck distances compared to the MLP adapter while maintaining representational disentanglement? Basis: The authors note regarding the LSTM's clustered geometry that this idea needs further refinement and research in the future. Why unresolved: While the paper documents that the LSTM creates more compact clusters (smaller distances) than the MLP, the underlying mechanism—hypothesized to be variance reduction for recurrent stability—is not proven. What evidence would resolve it: A comparative analysis of activation variance and gradient flow in the LSTM block versus the MLP block during the fine-tuning process.

### Open Question 3
Does the creation of disentangled latent space topologies occur when introducing other novel grammatical features, or is it specific to the morphological mechanics of polypersonal agreement? Basis: The authors acknowledge they could have experimented with any other grammatical feature but only tested polypersonal agreement. Why unresolved: It is unclear if the observed geometric separation (disentanglement) is a universal adaptation mechanism for any new syntactic rule or if it is specific to the agglutinative nature of the affixes used in this study. What evidence would resolve it: Replicating the protocol by introducing a distinct grammatical category (e.g., evidentiality or clusivity) and measuring the resulting topological changes.

## Limitations
- The experiment uses a synthetic linguistic phenomenon, raising questions about generalizability to natural grammatical features
- Topological analysis provides indirect evidence about functional representation changes; correlation with actual grammatical processing remains unestablished
- Several critical implementation details are underspecified, including exact affix paradigms, LSTM architecture parameters, and precise training hyperparameters

## Confidence

- **High confidence**: The empirical observation that fine-tuning with a grammatical module successfully reduces perplexity on polypersonal sentences and increases correct inflection prediction in later layers. The topological methodology is sound and the geometric analysis is internally consistent.
- **Medium confidence**: The claim that LSTM-based grammatical modules produce more clustered representations with smaller geometric shifts. While the bottleneck distance results support this, the interpretation that smaller shifts indicate "more clustered" geometry is indirect and could be confounded by other factors.
- **Low confidence**: The broader claim that adding a single trainable grammatical layer enables robust syntactic feature disentanglement across diverse linguistic phenomena. This is based on one artificial feature in one language, and the paper does not address how well this approach would generalize to more complex or natural grammatical phenomena.

## Next Checks

1. **Ablation on grammatical module necessity**: Train a baseline model with direct MLM head (no intermediate grammatical layer) on the same polypersonal data. Compare perplexity reduction and topological separation to confirm the added layer is essential for the observed effects rather than just additional parameters.

2. **Cross-linguistic morphology validation**: Replicate the experiment with a language having prefix-heavy morphology (e.g., Swahili or Swahili-like synthetic data) to test whether the suffix-advantage claim holds when morphological typology is reversed.

3. **Full fine-tuning comparison**: Run parallel experiments where the entire RuBERT encoder is fine-tuned rather than frozen, to assess whether the frozen approach limits representation integration and whether catastrophic forgetting actually occurs with full fine-tuning.