---
ver: rpa2
title: 'Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection
  in LLM Sentiment Classification'
arxiv_id: '2511.21752'
source_url: https://arxiv.org/abs/2511.21752
tags:
- labels
- alias
- label
- sentiment
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of large language models
  (LLMs) to class-directive prompt injection attacks, where adversarial instructions
  manipulate model outputs by exploiting known label sets (e.g., "positive" vs. "negative").
---

# Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification

## Quick Facts
- arXiv ID: 2511.21752
- Source URL: https://arxiv.org/abs/2511.21752
- Authors: Yanxi Li; Ruocheng Shan
- Reference count: 3
- Key outcome: Semantic label alias pairs restore accuracy degraded by class-directive prompt injection in 9 tested LLMs.

## Executive Summary
This paper introduces Label Disguise Defense (LDD), a lightweight, model-agnostic method to defend large language models against class-directive prompt injection attacks in sentiment classification. The approach replaces original sentiment labels with semantically aligned alias pairs (e.g., "green"/"red") and teaches the model the mapping via few-shot in-context learning. Evaluated across nine models including GPT-5, GPT-4o, LLaMA3.2, and Gemma3, LDD successfully restores accuracy degraded by injection attacks in most cases. A key finding is that semantically aligned aliases (e.g., "good"/"bad") yield stronger robustness than unaligned symbols (e.g., "cat"/"dog"), demonstrating that label semantics can serve as an effective defensive layer against prompt injection.

## Method Summary
Label Disguise Defense (LDD) addresses the vulnerability of LLMs to class-directive prompt injection attacks by replacing original sentiment labels with semantically transformed alias labels and teaching the model these new mappings via few-shot in-context learning. The method was evaluated on binary sentiment classification using an IMDB dataset subset with 8 training examples (4 positive, 4 negative) and 200 test examples (balanced mid-range ratings). Eight alias label pairs were tested (4 aligned: heaven/hell, green/red, good/bad, happy/sad; 4 unaligned: @#$/^ vs. *&%!, i/j, blue/yellow, cat/dog) across four shot configurations (2, 4, 6, 8) and two permutation orders. The defense was applied to nine models including GPT-5, GPT-4o, LLaMA 3.2, Gemma 3, and Mistral variants, with primary metrics being accuracy, recovery count, and regression count.

## Key Results
- LDD successfully restores accuracy degraded by prompt injection attacks in most tested models
- At least one alias pair achieves higher accuracy than the under-attack baseline for the vast majority of models
- Semantically aligned alias labels (e.g., "good"/"bad") yield stronger robustness than unaligned symbols (e.g., "cat"/"dog")
- Accuracy degradation of 10-40% occurs when using unaligned aliases, while aligned aliases maintain or improve performance

## Why This Works (Mechanism)
LDD exploits the semantic understanding of LLMs by replacing predictable sentiment labels with semantically meaningful aliases. When the model encounters an injection attack that attempts to override the label, it must first interpret the semantic meaning of the alias rather than simply recognizing the original label. This semantic transformation acts as a shield because the adversarial instruction no longer matches the expected label format, forcing the model to rely on the few-shot examples that teach the new alias mappings. The approach leverages the model's inherent semantic reasoning capabilities while adding minimal computational overhead through in-context learning.

## Foundational Learning
- **Prompt injection attacks**: Malicious instructions appended to inputs that manipulate model outputs by exploiting known label sets; needed to understand the threat LDD addresses
- **In-context learning**: Few-shot learning through demonstration examples within the prompt; needed to teach models new label mappings without fine-tuning
- **Semantic alignment**: The degree to which alias labels share conceptual relationships with original labels; needed to explain why some alias pairs work better than others
- **Model-agnostic defense**: Techniques that work across different model architectures; needed to establish LDD's broad applicability
- **Few-shot configuration**: The number and ordering of demonstration examples; needed to understand how LDD is implemented

## Architecture Onboarding

### Component Map
Dataset -> Prompt Construction -> Model Inference -> Output Mapping -> Metrics Calculation

### Critical Path
Alias selection → Few-shot prompt construction → Model inference → Output mapping → Accuracy measurement

### Design Tradeoffs
Semantic alignment vs. simplicity (aligned aliases work better but may be more complex to design), computational overhead vs. defense effectiveness (few-shot adds minimal cost but requires careful prompt engineering)

### Failure Signatures
Accuracy drops of 10-40% when using unaligned aliases (cat/dog, @#$/^ vs. *&%!), unstable performance in small models (LLaMA 1B, Gemma 4B) across permutations

### 3 First Experiments
1. Test aligned alias pair "green/red" with 8-shot configuration on GPT-4o under attack
2. Test unaligned alias pair "cat/dog" with 8-shot configuration on the same model and attack
3. Compare baseline zero-shot accuracy vs. LDD-protected accuracy on IMDB test set

## Open Questions the Paper Calls Out
- Can LDD maintain robustness against multi-step reasoning attacks or multi-turn category redefinitions? The current evaluation is limited to direct, single-turn class-directive injections.
- How does semantic alignment in alias labels affect defense effectiveness in multilingual or multi-class classification settings? The authors note the work focused on English binary sentiment classification.
- Does the application of semantic disguise in LDD alter the model's internal reasoning patterns or attribution maps? The authors propose studying its interaction with interpretability methods.

## Limitations
- Attack methodology is narrowly defined to class-directive prompts, excluding more sophisticated or context-aware adversarial strategies
- IMDB dataset subset (8 training + 200 test samples) is extremely small, raising questions about performance on larger, more diverse datasets
- Evaluation focuses exclusively on binary sentiment classification, leaving open questions about LDD's effectiveness on multi-class or complex reasoning tasks

## Confidence
- **High Confidence**: Semantically aligned alias labels restore accuracy degraded by prompt injection in most tested models
- **Medium Confidence**: LDD is "lightweight and model-agnostic" - while method adds minimal overhead, semantic dependency suggests model-specific optimization may be needed
- **Low Confidence**: LDD provides "effective defense" in all scenarios - severe degradation with unaligned aliases (10-40% drops) and narrow attack scope limit generalizability

## Next Checks
1. Evaluate LDD on full IMDB (25,000 reviews) and additional sentiment datasets (SST-2, Amazon reviews) to assess performance scaling
2. Test LDD on multi-class classification tasks (e.g., product reviews with 5-star ratings, topic classification) to validate semantic alignment beyond binary cases
3. Evaluate LDD against more sophisticated prompt injection attacks including context-aware adversarial instructions and instruction interleaving