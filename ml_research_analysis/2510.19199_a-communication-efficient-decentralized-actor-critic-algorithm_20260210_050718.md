---
ver: rpa2
title: A Communication-Efficient Decentralized Actor-Critic Algorithm
arxiv_id: '2510.19199'
source_url: https://arxiv.org/abs/2510.19199
tags:
- learning
- policy
- local
- critic
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a communication-efficient decentralized actor-critic
  algorithm for multi-agent reinforcement learning. The key innovation is integrating
  local training with Markovian mini-batch sampling, where each agent performs several
  local policy and value function updates before communicating with neighbors.
---

# A Communication-Efficient Decentralized Actor-Critic Algorithm

## Quick Facts
- **arXiv ID:** 2510.19199
- **Source URL:** https://arxiv.org/abs/2510.19199
- **Reference count:** 40
- **Primary result:** O(ε⁻³) sample complexity and O(ε⁻¹τ⁻¹) communication complexity for ε-accurate stationary point in multi-agent RL

## Executive Summary
This paper presents a decentralized actor-critic algorithm that significantly reduces communication overhead in multi-agent reinforcement learning by integrating local training with Markovian mini-batch sampling. Each agent performs multiple local policy and value function updates before synchronizing with neighbors, achieving O(ε⁻¹τ⁻¹) communication complexity compared to traditional methods requiring O(ε⁻³) communications. The algorithm uses multi-layer neural networks for value function approximation and provides finite-time convergence guarantees that explicitly account for approximation errors. Empirical results on cooperative navigation tasks demonstrate superior communication efficiency while maintaining performance comparable to state-of-the-art methods.

## Method Summary
The algorithm operates on a connected undirected graph where each agent maintains local policy and critic parameters. Agents perform τ local training steps using Markovian mini-batches drawn from their own trajectories, updating both policy (actor) and value function (critic) parameters. Consensus is enforced through an LT-ADMM framework with bridge variables, requiring only one communication round per τ local steps. The critic uses a multi-layer neural network trained via projected TD learning, while the actor updates using policy gradients computed from the critic's TD-error. The finite-time analysis shows that to achieve ε-accurate stationary point, the sample complexity is O(ε⁻³) and communication complexity is O(ε⁻¹τ⁻¹), where τ is the number of local training steps.

## Key Results
- Achieves O(ε⁻¹τ⁻¹) communication complexity versus O(ε⁻³) for standard methods
- Outperforms state-of-the-art algorithms on cooperative navigation tasks while using significantly fewer communication rounds
- Provides explicit finite-time convergence guarantees that account for neural network approximation error ς_approx
- Sample complexity of O(ε⁻³) is comparable to centralized methods while communication efficiency scales favorably with local training steps τ

## Why This Works (Mechanism)

### Mechanism 1: Local Training with LT-ADMM
- **Claim:** The algorithm reduces communication overhead by allowing agents to perform multiple local updates before synchronizing, rather than communicating at every step.
- **Mechanism:** Agents perform τ local gradient steps on policy (ω) and critic (θ) parameters. Consensus is enforced using an Alternating Direction Method of Multipliers (ADMM) framework with bridge variables (z_ij), requiring only a single communication round per τ steps.
- **Core assumption:** The network topology is a connected, undirected graph (Assumption 1), and the number of local steps τ is balanced against the network's algebraic connectivity (λ_l) to prevent consensus drift.
- **Evidence anchors:**
  - [abstract] "each agent performs several local updates... before exchanging information"
  - [section III-B] "realizes the LT-ADMM 'compute locally, communicate once' principle"
  - [corpus] BiCoLoR (arXiv:2601.12400) similarly combines local training with compression for efficiency.
- **Break condition:** If τ is set too high relative to network connectivity (λ_l), the bound on consensus error grows (Theorem 1), potentially causing the algorithm to diverge or converge to a poor local optimum.

### Mechanism 2: Markovian Mini-Batch Sampling
- **Claim:** Using mini-batches drawn from the Markovian trajectory reduces the variance of gradient estimates, stabilizing the learning process compared to single-sample updates.
- **Mechanism:** Instead of updating parameters after every state transition, the agent collects a batch of B samples (actor) and N_c samples (critic). It averages the gradients over this batch, smoothing out noise inherent in the environment's dynamics.
- **Core assumption:** The Markov chain induced by the policy is geometrically ergodic (Assumption 3), ensuring that the batch distribution approximates the stationary distribution sufficiently fast.
- **Evidence anchors:**
  - [abstract] "combining local training with Markovian mini-batch sampling"
  - [section I-D] "adopt Markovian mini-batches to improve the convergence behavior"
  - [corpus] Neighbors like CoCoL address data heterogeneity but rely on different sampling assumptions; this paper focuses specifically on the variance reduction of on-policy Markovian samples.
- **Break condition:** If the environment mixes slowly (high κζ^t in Assumption 3), the bias in the mini-batch estimate will remain high, slowing convergence or preventing it entirely.

### Mechanism 3: Neural Approximation Error Propagation
- **Claim:** The algorithm provides finite-time convergence guarantees for multi-layer neural network critics, provided the approximation error is bounded.
- **Mechanism:** The critic uses a multi-layer neural network V(s, θ). The convergence analysis (Theorem 1) explicitly accounts for the gap between the true value function and the network's representation capacity (ς_approx), treating it as an error term in the final bound.
- **Core assumption:** The neural network is initialized properly (Assumption 7) and the activation functions are smooth (Assumption 5).
- **Evidence anchors:**
  - [abstract] "final error bound depends on the neural network's approximation quality"
  - [section IV] "final error is bounded by O(ς_approx ...)"
  - [corpus] Distributed neural policy gradient (cited in paper) explores similar neural approximators but often with different consensus mechanisms.
- **Break condition:** If the network width m is too small or architecture is insufficient, ς_approx becomes large, meaning the algorithm converges to a "stationary point" that is arbitrarily far from the optimal policy.

## Foundational Learning

- **Concept: Actor-Critic Architecture**
  - **Why needed here:** This is the foundational structure of the algorithm. The "Critic" estimates the value function (using the Neural Network) to reduce variance, while the "Actor" updates the policy based on the Critic's feedback.
  - **Quick check question:** Can you explain why the TD-error δ_t from the critic is used as the advantage estimate in the actor update?

- **Concept: Distributed Consensus (ADMM)**
  - **Why needed here:** Unlike centralized RL, agents must agree on a policy without a central server. Understanding how ADMM uses auxiliary variables (z_ij) to enforce the constraint ω_i = ω_j is crucial.
  - **Quick check question:** How does the penalty parameter ρ affect the trade-off between convergence speed and consensus error?

- **Concept: Finite-Time Analysis vs. Asymptotic Convergence**
  - **Why needed here:** The paper emphasizes "finite-time convergence," which is a stronger guarantee than asymptotic limits. It tells you how fast you can expect results.
  - **Quick check question:** What does the sample complexity O(ε⁻³) imply about the relationship between accuracy and the number of samples required?

## Architecture Onboarding

- **Component map:** Agent Node -> Local Policy (π_ω) -> Critic Module (V_θ) -> Communication Interface -> Neighbors -> Bridge Variables (z_ij) -> Consensus Enforcement

- **Critical path:**
  1. **Batch Collection:** Agent interacts with environment to fill batch of size B
  2. **Critic Update:** Update θ using N_c samples from the trajectory (Algorithm 2)
  3. **Actor Update:** Compute policy gradient g_i and update ω locally
  4. **Local Loop:** Repeat steps 1-3 for τ iterations
  5. **Sync:** Communicate with neighbors to update z_ij and synchronize ω

- **Design tradeoffs:**
  - **Local Steps (τ):** Increasing τ reduces communication complexity (O(ε⁻¹τ⁻¹)) but increases the risk of "consensus drift" where agents diverge in their policy views
  - **Batch Size (B):** Larger batches reduce gradient variance (improving sample efficiency) but increase compute cost per step
  - **Network Capacity:** A wider network reduces approximation error ς_approx but increases computational overhead and data requirements

- **Failure signatures:**
  - **Diverging Consensus Error:** If the consensus error (Fig 2a) grows instead of shrinking, τ is likely too high or learning rate β is unstable
  - **Plateaued Reward with High Critic Loss:** Indicates the critic network cannot approximate the value function (high ς_approx); consider increasing network width or depth
  - **Slow Convergence:** If sample complexity appears worse than O(ε⁻³), check that the Markov chain is mixing properly (validating Assumption 3)

- **First 3 experiments:**
  1. **Baseline Verification:** Implement the Cooperative Navigation task from Section V. Plot the "smoothed accumulated return" vs. communication rounds to verify you can reproduce Fig 1
  2. **Communication Ablation:** Vary τ ∈ {1, 5, 10} while keeping total samples fixed. Plot the final reward vs. total communication rounds to validate the O(ε⁻¹τ⁻¹) efficiency claim
  3. **Scalability Test:** Increase the number of agents N (e.g., 5, 10, 20) in the ring topology to observe how convergence speed scales with network size (tracking the λ_l dependency)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the algorithm be extended to settings with partial observability where agents only have access to local observations rather than global state information?
- **Basis in paper:** [explicit] The authors state: "extending the analysis to settings with partial observability and heterogeneous information structures would allow agents to operate with local observations or belief states rather than full global states."
- **Why unresolved:** The current analysis relies on all agents observing the global state (s_t is fully observable by all agents), which simplifies policy factorization and gradient estimation. Partial observability introduces hidden state variables and requires maintaining belief states.
- **What evidence would resolve it:** A convergence analysis for a modified algorithm that uses local observations, possibly incorporating recurrent networks or belief state estimators, with bounds on performance degradation relative to the full-observability case.

### Open Question 2
- **Question:** How does the algorithm perform under time-varying or directed communication topologies rather than static undirected graphs?
- **Basis in paper:** [explicit] The authors state: "relaxing the assumption of symmetric and static communication networks to accommodate time-varying or directed graphs could enhance scalability and robustness in large-scale systems."
- **Why unresolved:** Assumption 1 requires a connected, undirected graph. The convergence proof uses properties of the static Laplacian matrix (λ_l, λ_u eigenvalues) that may not hold when topology changes or is directed.
- **What evidence would resolve it:** Modified convergence bounds that incorporate time-varying connectivity measures (e.g., B-strongly connected graphs) or use weighted combinations of gossip matrices for directed graphs.

### Open Question 3
- **Question:** Can asynchronous implementations be developed where agents update and communicate at different rates without sacrificing convergence guarantees?
- **Basis in paper:** [explicit] The authors state: "investigating asynchronous implementations of the proposed algorithm, where agents update and communicate at different rates, would make the framework more suitable for practical distributed computing environments."
- **Why unresolved:** The current algorithm assumes synchronized communication rounds where all agents transmit simultaneously. The ADMM bridge variable updates (14) require coordinated timing across neighbors.
- **What evidence would resolve it:** Convergence analysis showing bounded delay tolerance, or modification using local clocks with bounded asynchrony, with explicit trade-offs between delay bounds and convergence rate.

## Limitations
- Requires connected undirected communication graph (Assumption 1)
- Assumes geometric ergodicity of Markov chain (Assumption 3)
- Neural network approximation error ς_approx appears in final bounds but cannot be explicitly controlled
- Synchronized communication rounds limit practical deployment in asynchronous environments

## Confidence
- **Finite-time convergence analysis:** High
- **Communication efficiency claims:** High (supported by theoretical bounds O(ε⁻¹τ⁻¹))
- **Neural approximation error bounds:** Medium (depends on architecture choices not fully characterized)
- **Empirical validation on cooperative navigation:** High (Fig 1 shows clear communication reduction)

## Next Checks
1. Verify reproduction of Fig 1 by implementing cooperative navigation task and plotting smoothed accumulated return vs communication rounds
2. Test communication efficiency claim by varying τ ∈ {1, 5, 10} and measuring reward vs total communication rounds
3. Validate scalability by increasing agent count N in ring topology and measuring convergence speed relative to λ_l