---
ver: rpa2
title: 'LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less
  Dialect Guided Approach with a Refined Sylheti-English Benchmark'
arxiv_id: '2505.12273'
source_url: https://arxiv.org/abs/2505.12273
tags:
- evaluation
- dialect
- machine
- translation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of evaluating machine translation
  quality for low-resource languages with dialectal variation, where reference translations
  and annotated data are scarce. The authors propose a reference-less evaluation framework
  using large language models (LLMs) enhanced with dialect-specific guidance.
---

# LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark

## Quick Facts
- **arXiv ID:** 2505.12273
- **Source URL:** https://arxiv.org/abs/2505.12273
- **Reference count:** 22
- **Primary result:** +0.1083 Spearman correlation gain over baseline via dialect-guided prompting for Sylheti-English MT evaluation

## Executive Summary
This paper tackles the challenge of evaluating machine translation quality for low-resource languages with dialectal variation, where reference translations and annotated data are scarce. The authors propose a reference-less evaluation framework using large language models (LLMs) enhanced with dialect-specific guidance. They extend an existing dataset with Sylheti-English sentence pairs, machine translations, and human-annotated Direct Assessment (DA) scores, and introduce a dialect-aware tokenizer, a regression head for scalar score prediction, and a novel dialect-guided prompting strategy. Across multiple LLMs, their pipeline consistently improves evaluation accuracy, achieving up to a +0.1083 increase in Spearman correlation compared to existing methods, with particularly strong gains for larger models. The approach demonstrates that integrating dialect-specific context and lightweight model adaptations significantly boosts LLM-based translation evaluation in resource-constrained, dialect-rich settings.

## Method Summary
The authors extend the ONUBAD dataset with 1,500 Sylheti-English sentence pairs (980 from ONUBAD + 520 new), NLLB-200 machine translations, and human DA scores from two native speakers. They introduce a dialect-aware tokenizer by training a Byte-Level BPE tokenizer on Sylheti and merging its unique tokens into the LLM tokenizer. A regression head is trained on top of a frozen LLM to predict DA scores directly from source sentence, MT output, and a glossary of dialect-specific terms. The dialect-guided (DG) prompt template integrates the glossary with the source and MT output. The pipeline is evaluated on Llama-2-7B/13B, OpenChat, and Gemma models, achieving consistent Spearman correlation improvements over baselines.

## Key Results
- Up to +0.1083 Spearman correlation gain over baseline across multiple LLMs
- Consistent improvements for both smaller (7B) and larger (13B) models
- DG prompting outperforms annotation-guided (AG) and dialect + annotation-guided (DAG) variants
- Gains particularly strong for chat-tuned models (Llama-2, OpenChat) vs. base model (Gemma)

## Why This Works (Mechanism)
The approach leverages dialect-specific glossaries to ground LLM evaluation in the linguistic realities of Sylheti, compensating for the lack of reference translations. By extending the tokenizer with Sylheti-specific subwords and training a lightweight regression head on top of a frozen LLM, the method avoids overfitting on the small dataset while preserving the LLM's general reasoning capabilities. The dialect-guided prompt ensures the LLM attends to culturally and linguistically relevant cues, which standard prompts miss in low-resource, dialect-rich settings.

## Foundational Learning
- **Direct Assessment (DA) scoring**: Human rating of MT output quality on a scale (e.g., 0-100), used as ground truth for evaluation
  - Why needed: Provides scalar quality labels for training regression head
  - Quick check: Verify DA scores are z-normalized and averaged across annotators
- **Byte-Level BPE tokenization**: Subword segmentation optimized for multilingual text, extended with dialect-specific tokens
  - Why needed: Ensures Sylheti-specific terms are properly represented in LLM input
  - Quick check: Confirm tokenizer vocab size matches model after `resize_token_embeddings`
- **Spearman rank correlation**: Non-parametric metric measuring rank agreement between predicted and human scores
  - Why needed: Evaluates whether models rank translations correctly, not just predict absolute scores
  - Quick check: Compute both Pearson and Spearman to detect systematic bias
- **Prompt engineering for LLMs**: Structured input templates to elicit desired outputs (e.g., scalar scores vs. verbose text)
  - Why needed: Controls LLM behavior for regression vs. generation tasks
  - Quick check: Inspect LLM outputs for scalar vs. textual responses
- **Lightweight regression head**: Small trainable module (e.g., linear layer) on top of frozen LLM
  - Why needed: Minimizes overfitting on small datasets while leveraging LLM representations
  - Quick check: Monitor training/validation MSE gap for overfitting
- **4-bit quantization + fp16**: Memory-efficient model loading and computation
  - Why needed: Enables training on consumer hardware with limited GPU memory
  - Quick check: Verify model loads without CUDA out-of-memory errors

## Architecture Onboarding

**Component Map**
Tokenizer (Sylheti BPE + LLM merge) -> LLM (frozen) -> Regression Head -> DA score prediction

**Critical Path**
1. Extend LLM tokenizer with Sylheti BPE tokens
2. Build DG prompt (glossary + source + MT)
3. Extract LLM hidden state (last token)
4. Apply regression head (linear layer)
5. Train with MSE loss on DA scores

**Design Tradeoffs**
- Freeze LLM vs. full fine-tuning: avoids overfitting but limits adaptation
- Regression head vs. instruction tuning: simpler, faster, but less flexible
- Dialect glossary vs. full reference: practical for low-resource, but may miss nuance

**Failure Signatures**
- Tokenizer extension causes embedding misalignment → verify vocab size after resize
- Overfitting on small data → large train/validation MSE gap
- LLM outputs verbose text instead of scalar → ensure regression head is used, not generation

**First Experiments**
1. Train regression head with and without tokenizer extension; compare correlation
2. Compare DG vs. AG vs. DAG prompts on held-out data
3. Test regression head on base (non-chat) LLMs with DG prompt

## Open Questions the Paper Calls Out
- Would the dialect-guided prompting strategy generalize effectively to other low-resource dialect-rich languages beyond Sylheti, or is it specific to Bengali dialectal variations?
- Why does the Dialect + Annotation Guided (DAG) prompt consistently underperform the Dialect Guided (DG) prompt, and does adding annotation guidelines introduce confusion?
- Can the proposed approach achieve stronger performance on non-instruction-tuned base models with alternative prompting or adaptation strategies?
- Would scaling the dataset beyond 1,500 samples yield diminishing returns, or does the regression head's effectiveness fundamentally depend on larger annotated corpora?

## Limitations
- Human DA scores from only two annotators per sentence; no inter-annotator agreement metrics reported
- Dataset size (1,500 pairs) and exact train/validation/test split not fully specified
- Prompt templates only conceptually shown; exact formatting unclear
- Tokenizer extension process not rigorously validated for embedding alignment

## Confidence
- **High**: Overall improvement trend (+0.1083 Spearman) and gains across multiple LLMs are well-supported
- **Medium**: Absolute correlation values depend on ground truth quality and data splits, which are not fully disclosed
- **Low**: Generalization claims to other dialects/languages are not empirically validated

## Next Checks
1. Compute and report Krippendorff's alpha or Pearson correlation between the two human annotators' DA scores
2. Systematically vary the order and formatting of the glossary, source, and MT fields in the DG prompt to quantify impact on accuracy
3. Train and evaluate the pipeline with and without the Sylheti-specific tokenizer extensions, and measure changes in correlation and tokenizer-vocabulary consistency