---
ver: rpa2
title: 'Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization'
arxiv_id: '2601.12707'
source_url: https://arxiv.org/abs/2601.12707
tags:
- reward
- proof
- have
- bound
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inverse game theory problem of recovering
  unknown reward functions from observed strategies in competitive games. The authors
  develop a unified framework for entropy-regularized two-player zero-sum matrix games
  and Markov games, establishing identifiability conditions and proposing algorithms
  to recover feasible reward functions.
---

# Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization

## Quick Facts
- arXiv ID: 2601.12707
- Source URL: https://arxiv.org/abs/2601.12707
- Reference count: 40
- This paper develops a unified framework for entropy-regularized inverse game theory, establishing identifiability conditions and algorithms to recover reward functions from observed strategies in competitive games.

## Executive Summary
This paper addresses the inverse game theory problem of recovering unknown reward functions from observed player strategies in competitive environments. The authors develop a unified framework for entropy-regularized two-player zero-sum matrix games and Markov games, establishing identifiability conditions and proposing algorithms to recover feasible reward functions. The key insight is that entropy regularization makes the inverse problem identifiable under linear assumptions, allowing for unique parameter recovery when certain rank conditions are satisfied.

## Method Summary
The method employs entropy regularization to transform the inverse game problem into a linear estimation task. For matrix games, it estimates the equilibrium strategies from observed actions, constructs a linear system based on the QRE conditions, and solves for reward parameters using least squares. For Markov games, it extends this approach by incorporating transition kernel estimation through ridge regression, combining reward recovery with dynamics learning. The algorithms demonstrate strong theoretical guarantees with estimation error decreasing at O(N^{-1/2}) rates.

## Key Results
- Establishes rank conditions for unique parameter recovery in entropy-regularized matrix games
- Provides O(N^{-1/2}) convergence rates for reward estimation
- Extends framework to Markov games with simultaneous transition kernel estimation
- Demonstrates robustness through confidence set construction when parameters are partially identifiable
- Validates theoretical results with numerical experiments showing accurate reward recovery

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The inherent ambiguity of inverse game problems (multiple rewards yielding the same strategy) is resolvable under specific linear parametric assumptions.
- **Mechanism:** By introducing entropy regularization, the Nash equilibrium is replaced by the Quantal Response Equilibrium (QRE). Under the Linear Parametric Assumption, the non-linear QRE constraints collapse into a linear system $X\theta = y$. If the observed strategies satisfy a full-rank condition, the parameter vector $\theta$ becomes uniquely identifiable, transforming an ill-posed problem into a tractable linear estimation.
- **Core assumption:** The reward function $Q$ is linear with respect to a known feature map $\phi$, and the regularization parameter $\eta$ is known.
- **Evidence anchors:** [abstract] "establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions."
- **Break condition:** If the rank condition fails (e.g., limited exploration), the system is only partially identifiable, requiring confidence sets rather than point estimates.

### Mechanism 2
- **Claim:** Finite-sample reward recovery is achieved by first estimating the equilibrium strategies, then solving for reward parameters via least squares.
- **Mechanism:** The algorithm employs a two-step estimator. First, it observes actions to construct empirical estimates of strategies using frequency counts. Second, it plugs these estimates into the linearized QRE constraints to solve for the reward parameter using least squares. The estimation error of the reward is bounded by the TV distance error of the initial strategy estimates.
- **Core assumption:** Observed actions are i.i.d. samples from the true QRE; the feature matrix is well-conditioned.
- **Evidence anchors:** [section 2.2] "We estimate $\theta^*$ using the least-square estimator $\hat{\theta}$... given bounds for TV($\hat{\mu}, \mu^*$)... we bound the Euclidean distance."
- **Break condition:** If the number of samples $N$ is low relative to the size of the action space, the frequency estimators for $(\hat{\mu}, \hat{\nu})$ become too noisy to accurately recover $\theta$.

### Mechanism 3
- **Claim:** In dynamic (Markov) games, reward recovery requires simultaneous estimation of the transition dynamics.
- **Mechanism:** Because the reward $r_h$ depends on future values $V_{h+1}$ via the Bellman equation, the algorithm must estimate the unknown transition kernel $P_h$. It uses ridge regression to approximate the transition model from observed state transitions. The final reward estimate is a "plug-in" reconstruction derived from the estimated Q-functions and the estimated transition model.
- **Core assumption:** The transition kernel and reward function share a linear structure relative to the same feature map.
- **Evidence anchors:** [section 3.2] "To reconstruct the reward functions, it is essential to incorporate an estimator for the transition dynamics... we employ ridge regression."
- **Break condition:** If the dataset does not cover the state space uniformly, the frequency estimators for $P_h$ fail, breaking the Bellman consistency.

## Foundational Learning

- **Concept: Quantal Response Equilibrium (QRE)**
  - **Why needed here:** This is the behavioral model replacing the deterministic Nash Equilibrium. It assumes agents play stochastically to maximize entropy-regularized utility, which is mathematically necessary to create the bijective mapping required for the paper's identifiability proofs.
  - **Quick check question:** How does the temperature parameter $\eta$ control the gap between QRE and Nash Equilibrium?

- **Concept: Inverse Reinforcement Learning (IRL)**
  - **Why needed here:** The paper positions itself within IRL. Understanding that IRL is fundamentally ill-posed (many rewards explain one behavior) highlights why the "Identifiability" results in Section 2.2 are the core theoretical contribution.
  - **Quick check question:** Why does adding entropy regularization help resolve the identifiability issues inherent in standard IRL?

- **Concept: Linear Function Approximation in RL**
  - **Why needed here:** The entire theoretical framework relies on linear parameterizations where rewards, transitions, and Q-values are linear with respect to known features. Without this, the linear system would be non-linear and identifiability guarantees would not hold.
  - **Quick check question:** If the true reward function is non-linear with respect to the chosen features $\phi$, would the "Strong Identifiability" condition still be meaningful?

## Architecture Onboarding

- **Component map:** Input trajectories -> QRE Estimator -> Matrix Constructor -> Solver -> Confidence Set Builder
- **Critical path:** The construction of matrix $X = [A(\nu^*)^\top, B(\mu^*)^\top]^\top$. If the rank of this constructed matrix $< d$, the system is under-determined.
- **Design tradeoffs:**
  - **Frequency vs. MLE:** The frequency estimator requires uniform state coverage. The MLE-based estimator relaxes this to average-case coverage but assumes a linear policy parameterization.
  - **Point Estimate vs. Confidence Set:** If the "Strong Identifiability" rank condition fails, the architecture defaults to outputting a feasible set rather than a single reward vector.
- **Failure signatures:**
  - **Rank Deficiency:** The algorithm outputs a non-degenerate confidence set rather than a point, indicating partial identifiability.
  - **High Variance:** If $\kappa_N$ scales inversely with $N$, but the error remains high, check if rare actions cause instability in the log terms.
- **First 3 experiments:**
  1. **Matrix Game Validation (Setup I):** Replicate the strong identifiability experiment ($d=2$) to verify the $O(N^{-1/2})$ convergence of the parameter estimation error.
  2. **Partial Identifiability Test (Setup II):** Run the algorithm where $d > m+n-2$. Verify that while $\hat{\theta}$ does not converge to $\theta^*$, the resulting QRE still converges to the true QRE.
  3. **Markov Game Scaling:** Vary sample size $T$ in the Markov setting to observe the trade-off between transition kernel estimation error and Q-function estimation error on the final reward recovery.

## Open Questions the Paper Calls Out
- Can the framework be generalized to non-linear payoff functions or reward structures?
- How does the inverse game framework perform in partially observable environments?
- Can the method be extended to online learning settings?
- Can the requirement for uniform state coverage be relaxed while maintaining statistical guarantees?

## Limitations
- The framework relies heavily on linear parametric assumptions, which may not hold in practice
- Requires known entropy regularization parameter η, which may not be available
- For Markov games, requires sufficient exploration for accurate transition kernel estimation
- Confidence set construction may yield overly conservative results in practice

## Confidence
- **High Confidence**: Theoretical identifiability results under linear assumptions, convergence rates (O(N^{-1/2})), and the fundamental mechanism of transforming inverse game problems into linear estimation via entropy regularization
- **Medium Confidence**: Practical performance of the algorithms, particularly in partially identifiable regimes where the rank condition fails, and robustness to violations of the linear parametric assumption
- **Low Confidence**: Generalization to non-zero-sum games or settings with more than two players, as the current framework is specifically designed for two-player zero-sum games

## Next Checks
1. **Robustness to Model Misspecification**: Test the algorithm on synthetic data where the true reward function is non-linear with respect to the chosen features. Quantify the bias in the estimated parameters and assess whether the confidence sets still provide valid coverage.
2. **Unknown Regularization Parameter**: Extend the framework to jointly estimate the entropy regularization parameter η alongside the reward parameters. Compare the performance of this joint estimation approach to the baseline where η is known.
3. **Scaling to Larger Action Spaces**: Evaluate the algorithm's performance in games with large action spaces (e.g., m, n > 100). Investigate whether the frequency-based estimators become too noisy and whether alternative estimation techniques are necessary.