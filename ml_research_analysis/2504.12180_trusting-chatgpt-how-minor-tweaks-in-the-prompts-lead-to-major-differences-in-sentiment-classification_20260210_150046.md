---
ver: rpa2
title: 'Trusting CHATGPT: how minor tweaks in the prompts lead to major differences
  in sentiment classification'
arxiv_id: '2504.12180'
source_url: https://arxiv.org/abs/2504.12180
tags:
- para
- prompts
- como
- prompt
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tested whether subtle changes in prompt structure affect
  sentiment classification results from GPT-4o mini. Using 100,000 Spanish YouTube
  comments about four Latin American presidents, the model classified each comment
  10 times with slightly varied prompts.
---

# Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification

## Quick Facts
- arXiv ID: 2504.12180
- Source URL: https://arxiv.org/abs/2504.12180
- Reference count: 0
- Primary result: Minor prompt variations (lexical, syntactic, modal, unstructured) produce statistically significant differences in GPT-4o mini sentiment classification outputs

## Executive Summary
This study demonstrates that subtle changes in prompt structure can lead to significant differences in sentiment classification results from GPT-4o mini. Using 100,000 Spanish YouTube comments about four Latin American presidents, the researchers classified each comment 10 times with slightly varied prompts. The results showed that even minor changes in wording produced statistically significant differences in classification, with only one prompt pair showing no significant difference. The study also found that unstructured prompts led to inconsistent responses and increased hallucination frequency, challenging the robustness and reliability of LLMs for classification tasks.

## Method Summary
The study used 100,000 Spanish YouTube comments about four Latin American presidents collected from 24 journalistic channels. Researchers created 10 prompt variants covering different linguistic modifications (base OpenAI-style, base Zhang et al.-style, syntactic reordering, lexical-semantic changes, modal variations, unstructured versions). Each comment was classified by GPT-4o mini via OpenAI Batch API with temperature=0, using zero-shot prompting. The analysis included Chi-square tests between all 45 prompt pairs, coincidence matrices, Levenshtein distance calculations, and PCA of prompt embeddings to compare output distributions and prompt similarity.

## Key Results
- Minor prompt variations produced statistically significant differences in classification for nearly all prompt pairs (Chi-square p < 0.05)
- Only one prompt pair showed no significant difference due to high structural similarity
- Unstructured prompts generated the most inconsistent responses (>1000 inconsistent outputs)
- Coincidence between prompts ranged from 0.71 to 0.96, but high agreement didn't guarantee statistical equivalence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM classification outputs vary significantly with non-semantic prompt modifications, challenging the assumption of robustness.
- Mechanism: Prompt tokens are embedded into high-dimensional vectors; minor lexical or syntactic shifts alter attention patterns and output distributions even when semantic intent is unchanged. The model acts as a black box where internal weighting of instruction components cannot be inspected or predicted.
- Core assumption: The observed variation is intrinsic to the model's token processing, not an artifact of sampling (temperature was fixed at 0).
- Evidence anchors:
  - [abstract] "Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar."
  - [section: Results, p.23] "Solo los prompts 1 y 7 tienen la misma PDF... Para el resto de los casos, la hipótesis nula es rechazada."
  - [corpus] Related work (Zhang et al. 2023) confirms LLMs "podrían producir resultados disímiles incluso cuando las indicaciones son semánticamente similares."

### Mechanism 2
- Claim: Grammatically unstructured prompts increase hallucination frequency and inconsistent outputs.
- Mechanism: LLMs trained on well-formed text lack robust priors for telegraphic or fragmented instructions. Without clear syntactic boundaries, attention mechanisms diffuse across tokens, permitting off-target generation (mixed categories, unsolicited explanations, wrong languages).
- Core assumption: The correlation between unstructured prompts and inconsistent responses is causal, not coincidental.
- Evidence anchors:
  - [abstract] "Lack of proper grammar increases hallucinations."
  - [section: Discussion, p.24] "El prompt 9, el desestructurado del grupo A, fue el que más respuestas inconsistentes arrojó con más de 1000."
  - [corpus] Limited direct corpus support; related papers focus on trust dimensions rather than grammar-to-hallucination pathways.

### Mechanism 3
- Claim: Prompt similarity metrics (embeddings, Levenshtein distance) correlate with but do not guarantee classification equivalence.
- Mechanism: Embeddings capture semantic proximity; Levenshtein captures surface token overlap. Both are proxies. The model's internal attention patterns may weight instruction components differently than these metrics predict.
- Core assumption: The single non-significant pair (prompts 1 and 7) represents a boundary case where structural similarity is sufficient for robustness.
- Evidence anchors:
  - [section: Results, p.21] "La correlación entre esta matriz [Levenshtein] y la matriz de coincidencia es de -0.71."
  - [section: Results, p.18-19] Prompts 2 and 6 showed high embedding similarity and 0.96 coincidence, yet Chi-square still found significant differences.

## Foundational Learning

- Concept: Chi-square test for categorical output distributions
  - Why needed here: The paper's core claim rests on Chi-square rejecting the null hypothesis that prompt pairs produce equivalent classification distributions.
  - Quick check question: If two prompts classify 100k items with 95% agreement, could Chi-square still detect significance? (Yes—sample size amplifies small proportional differences.)

- Concept: Temperature parameter in LLM APIs
  - Why needed here: The study fixed temperature at 0 to isolate prompt effects; understanding this is essential to replicate or extend the design.
  - Quick check question: What happens to output variance if temperature is increased from 0 to 0.7? (Sampling stochasticity increases; prompt effects may be masked or amplified unpredictably.)

- Concept: Prompt engineering categories (zero-shot, few-shot, chain-of-thought)
  - Why needed here: The study used zero-shot prompting exclusively; understanding alternatives clarifies what was not tested.
  - Quick check question: Would few-shot prompting likely increase or decrease sensitivity to minor prompt wording changes? (Assumption: Likely decrease, as examples anchor the task, but this is untested here.)

## Architecture Onboarding

- Component map: YouTube API -> 100k Spanish comments (CSV/JSON) -> 10 prompt templates -> OpenAI Batch API (Python, temperature=0, model=gpt-4o-mini) -> Chi-square tests and coincidence matrices
- Critical path: 1. Design prompt variants with documented linguistic modifications. 2. Submit batch requests with fixed temperature; ensure independent processing per prompt. 3. Validate output format (only "positivo," "negativo," "neutral" accepted; flag "inconsistente"). 4. Run Chi-square pairwise; flag any p < 0.05 as significant divergence.
- Design tradeoffs:
  - Zero-shot vs. few-shot: Zero-shot isolates prompt sensitivity but may underperform on complex tasks
  - Temperature=0: Maximizes reproducibility but may underrepresent model uncertainty
  - Batch API: Cost-efficient for 1M classifications, but limited real-time debugging
- Failure signatures:
  - >1% "inconsistente" outputs per prompt → likely prompt ambiguity or grammar issues
  - Chi-square p < 0.05 for semantically identical prompts → model non-robustness confirmed
  - Coincidence >0.95 but Chi-square significant → high agreement at scale masks distributional shifts
- First 3 experiments:
  1. Replicate with temperature=0.2 and 0.5 to quantify variance amplification
  2. Add few-shot examples (3 per class) to a subset of prompts; measure whether sensitivity decreases
  3. Translate prompts to English; run on same data (translated) to test language-specific robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which prompt variations produce sentiment classifications closest to human-coded ground truth?
- Basis in paper: [explicit] "Un próximo paso clave será comparar los resultados obtenidos por los diferentes prompts con un ground truth, es decir, una clasificación realizada por humanos, para identificar cuáles se acercan más a las evaluaciones humanas."
- Why unresolved: The study compared prompts against each other, not against human judgments, so we cannot determine which variant is most accurate.
- What evidence would resolve it: Human annotation of a sample of comments, followed by comparison of each prompt's classifications against this benchmark using accuracy metrics.

### Open Question 2
- Question: Do these prompt sensitivity findings generalize to English and other languages?
- Basis in paper: [explicit] "Además, resulta fundamental probar la hipótesis planteada en este estudio en otros idiomas, con énfasis en el inglés, para evaluar la consistencia del modelo en contextos lingüísticos diversos."
- Why unresolved: The study only tested Spanish-language comments from Latin American contexts; model behavior may differ across languages due to training data composition.
- What evidence would resolve it: Replication of the experimental design using comparable datasets in English and other languages, with identical prompt translation protocols.

### Open Question 3
- Question: Would other LLMs (e.g., Claude, Llama, GPT-4) show similar sensitivity to subtle prompt variations?
- Basis in paper: [inferred] The study tested only GPT-4o mini; results may be model-specific rather than generalizable across LLM architectures.
- Why unresolved: Different models have different training procedures, sizes, and instruction-tuning approaches that could affect robustness.
- What evidence would resolve it: Cross-model comparison using the same prompts and dataset to assess whether prompt sensitivity is universal or model-dependent.

## Limitations

- Findings are based on a single model (GPT-4o mini) at temperature=0, limiting generalizability to other models or stochastic settings
- No few-shot or chain-of-thought prompting variants were tested, so robustness to minor prompt changes under alternative prompt engineering strategies remains unknown
- The correlation between unstructured prompts and hallucination frequency is inferred rather than experimentally validated through controlled grammar restoration tests

## Confidence

- **High confidence**: The statistical significance of differences between prompt pairs (Chi-square results across 45 comparisons) and the correlation between prompt similarity metrics and output coincidence
- **Medium confidence**: The causal relationship between grammatical structure and hallucination frequency, as this relies on indirect evidence and lack of direct grammar restoration testing
- **Medium confidence**: The claim that prompt similarity metrics (embeddings, Levenshtein) are insufficient predictors of output equivalence, though supported by empirical evidence

## Next Checks

1. **Temperature sensitivity test**: Replicate the classification with temperature=0.2 and 0.5 to quantify how stochastic sampling affects prompt sensitivity and whether minor wording changes become masked or amplified
2. **Grammar restoration experiment**: Select the unstructured prompt with highest inconsistency rates, restore grammatical structure systematically, and measure changes in both classification consistency and hallucination frequency
3. **Cross-language validation**: Translate all prompts to English, run on the same dataset (translated), and compare Chi-square significance patterns to test whether findings generalize across languages or reflect language-specific model behavior