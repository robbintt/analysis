---
ver: rpa2
title: 'GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems
  in Visual Contexts'
arxiv_id: '2509.25160'
source_url: https://arxiv.org/abs/2509.25160
tags:
- scene
- total
- image
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose GSM8K-V, a purely visual benchmark for mathematical
  reasoning derived from the GSM8K dataset. They convert text-based grade-school math
  word problems into multi-image visual representations using an automated pipeline
  with human verification.
---

# GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts

## Quick Facts
- **arXiv ID**: 2509.25160
- **Source URL**: https://arxiv.org/abs/2509.25160
- **Reference count**: 40
- **Primary result**: Existing VLMs drop from ~95% accuracy on text-based GSM8K to ~47% on visual GSM8K-V, highlighting significant gaps in visual mathematical reasoning.

## Executive Summary
This paper introduces GSM8K-V, a benchmark converting grade-school math word problems into multi-image visual representations. The benchmark reveals that while VLMs achieve near-saturation on text-based GSM8K (95.22% for Gemini-2.5-Pro), they struggle significantly with visual versions (46.93%), indicating substantial room for improvement in visual mathematical reasoning. The evaluation framework includes implicit (images only) and explicit (images + masked text) modes, along with modality ablations (OCR, captions) to diagnose specific failure patterns.

## Method Summary
GSM8K-V converts text problems into comic-style images using an automated pipeline: (1) GPT-4.1 parses problems into (object, math value, semantic) triples and allocates them across scenes; (2) GPT-4.1 generates scene descriptions using category-specific templates; (3) GPT-Image-1 renders 1024×1024 images. Human dual verification ensures consistency, completeness, and compliance. The benchmark contains 1,319 problems with 5,343 images (avg 4.05 per problem), covering 6 categories and 13 sub-categories.

## Key Results
- VLMs achieve 95.22% on text-based GSM8K but only 46.93% on visual GSM8K-V
- Multi-image input marginally outperforms single concatenated images (29.57% vs 27.45% for GPT-4o)
- Modality ablation shows captions help (32-55%) but don't close the gap to text-only (78-95%)
- Error analysis reveals perception-calculation and instrument-reading failures

## Why This Works (Mechanism)

### Mechanism 1
Multi-image cross-scene information integration creates a bottleneck distinct from single-image understanding. When mathematical information distributes across 2-11 images, VLMs must maintain entity coherence, track temporal order, and bind quantities across scenes. Multi-image input yields marginally higher accuracy than single concatenated images, suggesting scene separation helps preserve sequential dependencies while concatenation obscures critical information.

### Mechanism 2
Perception errors cascade into calculation failures when visual numeracy is required. VLMs misread instruments (clocks, gauges, pie charts) or miscount objects, producing incorrect intermediate values that propagate through arithmetic reasoning. Error analysis shows perception-calculation errors where object quantities are misidentified and instrument-reading errors where visual measurements are misinterpreted.

### Mechanism 3
The modality gap persists even when visual content is structurally described, indicating VLMs lack robust visual-symbolic grounding. OCR-based inputs yield 9-12% accuracy while structured scene descriptions achieve 32-55%, suggesting models cannot solve by text transcription alone and explicit textual grounding helps but doesn't close the gap, indicating implicit visual-to-symbolic mapping is underdeveloped.

## Foundational Learning
- **Concept**: Multi-modal fusion architectures (cross-attention, late fusion, interleaved attention)
  - Why needed: VLMs must integrate visual features across multiple images with text reasoning chains
  - Quick check: Can you explain how your model's attention mechanism handles 4+ images with different scene contexts?
- **Concept**: Visual grounding and binding (object-to-symbol mapping)
  - Why needed: The benchmark requires binding visual quantities to mathematical operations without explicit text labels
  - Quick check: How does your model associate a visual object count with a numerical variable in a reasoning chain?
- **Concept**: Sequential/cross-image reasoning (temporal coherence, entity tracking)
  - Why needed: Problems span multiple scenes where later images depend on earlier context
  - Quick check: When processing image sequences, does your model maintain state across scenes or treat each independently?

## Architecture Onboarding
- **Component map**: Problem decomposition -> Scene description generation -> Image rendering -> Human verification -> Model evaluation
- **Critical path**: Evaluate model on GSM8K-V implicit mode (images only) → compare to explicit mode (images + masked question) → analyze error patterns (perception vs. instrument reading vs. reasoning) → identify whether failures stem from visual grounding or multi-image integration
- **Design tradeoffs**: (1) Implicit vs. explicit query format trades evaluation purity against reference resolution clarity; (2) Single concatenated vs. multi-image input trades simplicity against temporal structure preservation; (3) OCR vs. caption vs. image-only inputs diagnose modality-specific failures
- **Failure signatures**: (1) Perception-calculation: correct arithmetic on wrong counts; (2) Instrument-reading: misinterpreted gauges/clocks producing nonsensical results; (3) Cross-scene binding: failing to connect scene 1 quantities to scene 4 questions
- **First 3 experiments**:
  1. Run implicit evaluation on GSM8K-V subset (100 samples) to establish baseline; compare against text-only GSM8K performance on same questions
  2. Ablate with explicit mode (masked text question provided) to isolate visual grounding vs. problem comprehension failures
  3. Analyze errors by category: prioritize Signboard & Icon (highest VLM performance) vs. Other (lowest VLM performance) to identify modality-specific strengths

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can VLMs develop robust "visual numeracy" to interpret analog instruments (clocks, gauges) without explicit grounding supervision?
**Basis**: Section 5.5 identifies "Instrument-Reading Errors" as a critical bottleneck where models misread visual cues, causing downstream reasoning collapse.
**Why unresolved**: Current VLMs excel at OCR but fail to map visual geometries to quantitative values reliably.
**What evidence would resolve it**: Ablation studies on models fine-tuned on synthetic instrument data tested on photorealistic gauge reading tasks.

### Open Question 2
**Question**: Does training on synthetic, comic-style visual math problems generalize to natural, photorealistic contexts?
**Basis**: The benchmark relies on automated pipeline generating "comic-style" images, which may lack the noise and complexity of real-world visual math.
**Why unresolved**: The paper demonstrates style invariance between Pixar and Giphli styles but does not test against real photography.
**What evidence would resolve it**: Evaluation of models trained on GSM8K-V on a counterpart dataset constructed using real-world photography.

### Open Question 3
**Question**: Why does providing structured text descriptions (captions) of scenes fail to fully close the performance gap to text-only reasoning?
**Basis**: Section 5.4 shows caption-based inputs significantly outperform image-only settings but still fall short of text-only performance (e.g., GPT-4o: 55.1% vs 94.9%).
**Why unresolved**: This suggests the visual modality introduces semantic ambiguity or "missing links" that even detailed textual descriptions fail to capture for the models.
**What evidence would resolve it**: Detailed error analysis comparing reasoning chains in text-only vs. caption-only modes to identify missing semantic primitives.

## Limitations
- The automated pipeline's exact prompts and human annotation criteria are not fully specified, limiting reproducibility
- Comic-style images may not generalize to real-world visual complexity and noise
- The benchmark focuses on structured problems, potentially missing open-ended visual reasoning scenarios

## Confidence
- **High**: Benchmark design choices provide controlled comparisons across modalities with human verification
- **Medium**: Exact reproducibility of automated pipeline is limited by unspecified human annotation criteria
- **Low**: Generalization to real-world visual contexts not tested

## Next Checks
1. Evaluate a subset of GSM8K-V problems with simplified numerical representations (text labels on objects) to quantify the perception bottleneck vs. pure visual reasoning difficulty
2. Test cross-scene binding explicitly by presenting image sequences with progressively removed context to measure dependency strength across scenes
3. Compare VLMs with specialized visual grounding architectures to assess whether the modality gap can be systematically reduced through architectural modifications