---
ver: rpa2
title: A multimodal multiplex of the mental lexicon for multilingual individuals
arxiv_id: '2511.05361'
source_url: https://arxiv.org/abs/2511.05361
tags:
- language
- visual
- multimodal
- lexical
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a multimodal multiplex model of the mental
  lexicon to investigate how visual input influences translation performance in multilingual
  individuals. Building on network models of language, it introduces a visual layer
  connecting perceptual inputs to lexical representations across languages.
---

# A multimodal multiplex of the mental lexicon for multilingual individuals

## Quick Facts
- arXiv ID: 2511.05361
- Source URL: https://arxiv.org/abs/2511.05361
- Reference count: 4
- Primary result: Visual-textual input improves translation accuracy and multilinguals outperform bilinguals in a multiplex mental lexicon model

## Executive Summary
This study introduces a multimodal multiplex model of the mental lexicon to investigate how visual input influences translation performance in multilingual individuals. Building on network models of language, it adds a visual layer connecting perceptual inputs to lexical representations across languages. Participants aged 12-16 were divided into bilingual and multilingual groups, each exposed to either text-only or visual-text stimuli during translation tasks. Results showed higher translation accuracy in the visual-text condition, with multilinguals outperforming bilinguals, supporting the hypothesis that multimodal input enhances lexical retrieval and reduces cross-linguistic interference.

## Method Summary
Participants (n=15, ages 12-16) were divided into bilingual and multilingual groups and exposed to either text-only or visual-text stimuli during translation tasks. Each participant translated 15 Dutch sentences into English, with visual-text condition participants seeing corresponding images. Translation accuracy was measured via cosine similarity between participant embeddings and reference translations using multilingual transformer models. The study employed a 2x2 factorial design (bilingual/multilingual x text-only/visual-text).

## Key Results
- Visual-textual condition showed higher translation accuracy than text-only condition
- Multilingual participants achieved higher similarity scores than bilingual participants
- The multiplex model with visual layer successfully captured enhanced cross-linguistic activation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual input amplifies cross-linguistic activation by providing an additional grounding layer for lexical representations.
- Mechanism: The multimodal multiplex model extends Stella et al.'s (2018) framework by adding a visual layer that connects perceptual representations to linguistic nodes across language layers. When visual input accompanies text, it activates concepts in the visual layer, which then spread activation to corresponding nodes in all language layers simultaneously, strengthening retrieval pathways.
- Core assumption: Activation spreads bidirectionally between visual and lexical layers following small-world network principles.
- Evidence anchors:
  - [abstract] "This supports the hypothesis that multimodal input enhances cross-linguistic activation and lexical retrieval"
  - [Section 2.6] Describes the anatomical proximity of occipitotemporal visual regions to parietal language areas (Wernicke's area, Brodmann area 39), suggesting biological plausibility for cross-modal connectivity
  - [corpus] Weak direct evidence—SpreadPy (arxiv:2507.09628) offers simulation tools for spreading activation in cognitive multiplex networks but does not validate multimodal extensions
- Break condition: If visual input introduces competing semantic interpretations (ambiguous images), interference may exceed facilitation.

### Mechanism 2
- Claim: Multilingual individuals benefit more from visual-textual input than bilinguals due to denser cross-language connectivity.
- Mechanism: Drawing on Kroll et al.'s (2015) parallel activation framework, multilinguals have more interconnected lexical networks across L1, L2, and heritage languages. Visual input provides a modality-neutral conceptual anchor that reduces cross-language competition by strengthening shared semantic representations rather than language-specific orthographic or phonological pathways.
- Core assumption: Heritage language proficiency creates additional lexical connections that can be leveraged during L3/L4 production when appropriately cued.
- Evidence anchors:
  - [abstract] "Multilingual participants in the visual-textual condition achieving the highest similarity scores"
  - [Section 2.5] Abu-Rabia and Sanitsky (2010) showed trilingual speakers outperformed bilinguals on English tasks, indicating skill transfer across languages
  - [corpus] No direct corpus evidence for multilingual advantage in multimodal conditions
- Break condition: If the heritage language has low proficiency or high interference with the target language, visual cues may not overcome lexical competition costs.

### Mechanism 3
- Claim: Cosine similarity between sentence embeddings captures translation proficiency by measuring semantic proximity rather than surface-form accuracy.
- Mechanism: Multilingual transformer models generate sentence embeddings in a shared semantic space. By computing cosine similarity between participant translations and reference embeddings, the method quantifies how closely the participant's output matches the intended meaning, allowing for synonymous expressions while penalizing semantic drift.
- Core assumption: The multilingual transformer produces comparable semantic representations across the specific language pairs (Dutch-English) used in the study.
- Evidence anchors:
  - [Section 4.2.4] "Translation proficiency will then be quantified by computing the cosine similarity between participant and reference embeddings"
  - [Section 5.1] "Higher similarity scores will indicate greater semantic accuracy and fluency in translation performance"
  - [corpus] No validation studies in corpus for this specific embedding-based proficiency measure
- Break condition: If the transformer model has systematic biases or poor cross-lingual alignment for Dutch-English, similarity scores will not reflect true semantic accuracy.

## Foundational Learning

- Concept: **Multilayer network formalism (Kivelä et al., 2014)**
  - Why needed here: The entire multiplex model depends on understanding M = (V_M, E_M, V, L) as a quadruplet where nodes exist across multiple layers with interlayer edges enabling navigation.
  - Quick check question: Can you explain why a multiplex network differs from simply overlaying multiple single-layer networks?

- Concept: **Bilingual Interactive Activation+ (BIA+) model**
  - Why needed here: Provides the psycholinguistic theory that lexical candidates from all known languages activate automatically during word recognition, forming the basis for predicting cross-linguistic effects.
  - Quick check question: What is the functional difference between language nodes in BIA versus BIA+?

- Concept: **Largest Viable Cluster (LVC) and explosive learning**
  - Why needed here: Explains why some words are learned faster and retrieved more easily—they belong to highly interconnected regions across multiple linguistic layers (semantic, phonological, taxonomic).
  - Quick check question: According to Stella et al. (2018), which lexical property—polysemy or concreteness—has a larger effect on LVC emergence?

## Architecture Onboarding

- Component map:
  Stimulus layer (Dutch sentences + optional images) -> Participant layer (4 subgroups) -> Recording pipeline (audio capture → transcription) -> Embedding pipeline (transformer → embeddings) -> Analysis layer (cosine similarity computation)

- Critical path:
  1. Stimulus presentation (fixation 200ms → image 1000ms [VT only] → sentence 2000ms → recording cue 4000ms)
  2. Participant verbal response → audio recording
  3. Transcription → embedding generation → similarity scoring
  4. Group comparison (VT vs. OT; multilingual vs. bilingual)

- Design tradeoffs:
  - Small stimulus set (15 sentences) limits statistical power but controls experiment duration (~15 min) for adolescent participants
  - Age range 12-16 ensures literacy but introduces developmental variability in working memory and processing speed
  - Cosine similarity captures semantic accuracy but may miss grammatical or stylistic differences
  - Visual stimuli selected for cultural neutrality may lack ecological validity for specific sentence contexts

- Failure signatures:
  - No significant difference between VT and OT conditions suggests visual layer does not enhance activation (model may be wrong OR stimuli poorly designed)
  - Multilinguals underperform bilinguals → heritage language may introduce interference rather than facilitation
  - High variance within groups → uncontrolled proficiency differences or stimulus effects dominate
  - Low absolute similarity scores → transformer embeddings may not align well for this task type

- First 3 experiments:
  1. Pilot with 5-10 participants to validate stimulus timing and transcription accuracy before full data collection
  2. Administer standardized English proficiency test to all participants to quantify and potentially covary for baseline differences
  3. Run a validation study comparing cosine similarity scores against human rater judgments of translation quality to confirm the embedding metric captures meaningful proficiency differences

## Open Questions the Paper Calls Out
None

## Limitations
- The visual-text advantage may stem from general attentional effects rather than specific cross-linguistic facilitation, as no control for modality-specific processing differences was implemented
- Small sample size (n=15) limits generalizability and increases vulnerability to individual differences in language learning aptitude
- Cosine similarity with reference translations assumes perfect reference quality but doesn't account for acceptable paraphrase variations that could score lower despite being correct

## Confidence
- **High confidence**: Visual-text condition improves translation accuracy over text-only (supported by multiple results sections)
- **Medium confidence**: Multilingual advantage is due to denser cross-language connectivity rather than general language learning ability (alternative explanations not fully controlled)
- **Low confidence**: The multiplex model's specific visual layer architecture accurately represents neural reality (biological evidence is suggestive but not definitive)

## Next Checks
1. Conduct a within-subjects replication with counterbalancing of VT/OT conditions to control for individual baseline differences
2. Compare embedding-based similarity scores against blinded human ratings of translation quality to validate the automatic metric
3. Test the same paradigm with adult participants to determine if age-related cognitive factors influence the visual-text advantage