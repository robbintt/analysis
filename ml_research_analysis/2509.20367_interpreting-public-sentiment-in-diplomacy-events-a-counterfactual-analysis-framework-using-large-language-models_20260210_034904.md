---
ver: rpa2
title: 'Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis
  Framework Using Large Language Models'
arxiv_id: '2509.20367'
source_url: https://arxiv.org/abs/2509.20367
tags:
- public
- sentiment
- diplomatic
- events
- diplomacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a computational framework to measure and influence
  public sentiment towards diplomatic events. The authors fine-tune a BERT model on
  a dataset of Reddit posts and comments about diplomatic events to predict public
  sentiment, achieving 70% overall accuracy.
---

# Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models

## Quick Facts
- arXiv ID: 2509.20367
- Source URL: https://arxiv.org/abs/2509.20367
- Authors: Leyi Ouyang
- Reference count: 7
- Primary result: Framework achieves 70% accuracy in sentiment classification and successfully shifts negative sentiment in 70% of cases through counterfactual narrative modifications

## Executive Summary
This study presents a computational framework to measure and influence public sentiment towards diplomatic events. The authors fine-tune a BERT model on a dataset of Reddit posts and comments about diplomatic events to predict public sentiment, achieving 70% overall accuracy. They then develop an iterative counterfactual generation algorithm using a large language model to systematically modify event narratives across five categories (participants, process, communication, substance, context) to shift negative sentiment to neutral or positive. The framework successfully changed public sentiment in 70% of cases, with modifications to participants (30.19%) and context (28.62%) proving most effective. The approach demonstrates how strategic narrative framing can influence public opinion on diplomatic matters, offering a data-driven tool for policymakers to anticipate and shape public reactions to international events.

## Method Summary
The framework combines fine-tuned BERT sentiment classification with LLM-based counterfactual generation. First, a BERT model is trained on Reddit data containing diplomatic event discussions to classify sentiment as negative, neutral, or positive. Then, an iterative algorithm systematically modifies event narratives by targeting five categories: participants, process, communication, substance, and context. The LLM generates counterfactuals by altering specific elements within these categories, and each modified narrative is evaluated for sentiment change. The process continues until sentiment shifts to neutral or positive, or no further improvements are possible. The approach uses Reddit as a proxy for public sentiment, leveraging its diverse user base and real-time discussions of international events.

## Key Results
- BERT model achieves 70% accuracy in classifying public sentiment on diplomatic events from Reddit data
- Counterfactual generation successfully shifts negative sentiment to neutral/positive in 70% of test cases
- Modifications to participants (30.19%) and context (28.62%) prove most effective at changing sentiment

## Why This Works (Mechanism)
The framework leverages the contextual understanding capabilities of BERT for accurate sentiment classification, while the LLM's generation abilities enable systematic exploration of narrative modifications. By targeting specific narrative elements across five categories, the approach can identify which aspects of diplomatic events most influence public perception. The iterative nature allows for progressive refinement of counterfactuals, gradually shifting sentiment through cumulative small changes rather than attempting dramatic narrative overhauls that might be less credible.

## Foundational Learning
- BERT fine-tuning for sentiment analysis: Needed to adapt pre-trained language models to domain-specific diplomatic discourse; Quick check: Validate performance on held-out test sets and compare against baseline sentiment classifiers
- Counterfactual generation with LLMs: Required for systematic narrative modification; Quick check: Evaluate generation quality through human assessment and measure coherence with original events
- Iterative optimization algorithms: Essential for progressive sentiment improvement; Quick check: Monitor convergence behavior and track sentiment changes across iterations

## Architecture Onboarding

**Component Map:** BERT model -> Sentiment classification -> LLM counterfactual generator -> Narrative modification engine -> Sentiment evaluation loop

**Critical Path:** Data collection and preprocessing -> BERT fine-tuning -> Counterfactual generation pipeline -> Sentiment evaluation -> Performance analysis

**Design Tradeoffs:** Reddit data provides rich, diverse sentiment signals but introduces platform-specific biases; LLM-based generation offers flexibility but may produce implausible modifications; Iterative approach ensures gradual improvements but requires multiple API calls

**Failure Signatures:** Sentiment classification fails to generalize beyond Reddit discourse; Counterfactuals become implausible or contradict known facts; Algorithm converges prematurely without achieving sentiment shift

**First Experiments:** 1) Test BERT classification on formal diplomatic documents to assess domain transfer; 2) Compare human-generated vs. LLM-generated counterfactuals for plausibility; 3) Evaluate sentiment persistence after 24-48 hours for modified narratives

## Open Questions the Paper Calls Out
None

## Limitations
- 70% accuracy in sentiment classification may not generalize to formal diplomatic discourse or different cultural contexts
- Reddit-based sentiment data may not represent broader population views on diplomatic matters
- Counterfactual generation may reinforce existing narratives rather than introducing genuinely novel perspectives

## Confidence
- Sentiment prediction methodology: Medium
- Counterfactual generation effectiveness: Medium
- Framework applicability to real-world diplomacy: Low

## Next Checks
1. Test framework performance on diverse international diplomatic corpora beyond Reddit, including news articles and official statements from multiple countries
2. Conduct human evaluation studies to compare LLM-generated counterfactuals against expert-crafted diplomatic messaging strategies
3. Implement longitudinal studies to assess whether sentiment shifts from counterfactual modifications persist over time and translate to actual behavioral changes in public opinion