---
ver: rpa2
title: Bridging Training and Merging Through Momentum-Aware Optimization
arxiv_id: '2512.17109'
source_url: https://arxiv.org/abs/2512.17109
tags:
- umtam
- training
- merging
- task
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UMTAM presents a unified framework that integrates memory-efficient
  training and model merging through shared computational structure. By maintaining
  factorized momentum and curvature statistics during training, the method enables
  principled multi-task model composition without redundant post-hoc computation.
---

# Bridging Training and Merging Through Momentum-Aware Optimization

## Quick Facts
- arXiv ID: 2512.17109
- Source URL: https://arxiv.org/abs/2512.17109
- Reference count: 3
- Primary result: Unified training/merging framework achieves 14.9% improvement over linear averaging and 1.6% over TIES

## Executive Summary
UMTAM presents a unified framework that integrates memory-efficient training and model merging through shared computational structure. By maintaining factorized momentum and curvature statistics during training, the method enables principled multi-task model composition without redundant post-hoc computation. Theoretical analysis establishes O(1/√T) convergence for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, UMTAM's curvature-aware parameter selection outperforms magnitude-only baselines by up to 135% at aggressive sparsity levels.

## Method Summary
UMTAM is a unified training and merging framework that maintains factorized momentum and curvature statistics throughout training. During the training phase, gradients are compressed via truncated SVD with error feedback to maintain O(mr+nr) memory instead of O(mn), while factorized second-moment statistics approximate Fisher information. The framework accumulates task-specific saliency scores weighted by curvature. During the merging phase, these accumulated statistics enable curvature-aware parameter selection and importance-weighted aggregation without requiring post-hoc Fisher computation. The approach eliminates the need for separate Fisher computation, enabling a streamlined workflow where training-time curvature information directly informs model composition.

## Key Results
- Curvature-aware parameter selection outperforms magnitude-only baselines by up to 135% at 1% density
- Multi-task merging performance improves by 14.9% over linear averaging and 1.6% over TIES
- Maintains competitive training efficiency with AdamW (0.761 vs 0.762 GPT-2 loss)
- Achieves 82.5% momentum energy capture with rank-32 factorization

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Momentum Compression with Error Feedback
Maintaining momentum in factorized form with error feedback preserves gradient information while reducing memory from O(mn) to O(mr + nr) per layer. At each step, reconstruct momentum from factors (UΣV^T), add new gradient and accumulated error, then re-compress via truncated SVD. The compression error E_t = M̃_t - U_{t+1}Σ_{t+1}V_{t+1}^T is retained and reinjected with decay γ, ensuring no gradient component is permanently lost. Core assumption: Gradients exhibit low-rank structure (stable rank ≈3 empirically), so the (r+1)-th singular value decays rapidly.

### Mechanism 2: Factorized Curvature as Fisher Proxy
Row-wise and column-wise second-moment statistics (R, C) provide curvature estimates comparable to diagonal Fisher approximation, enabling importance-weighted operations without post-hoc computation. During training, accumulate R_t = β_2 R_{t-1} + (1-β_2)diag(GG^T) and C_t = β_2 C_{t-1} + (1-β_2)diag(G^T G). The factorized preconditioner P̂ = (R · C^T / 1^T R)^{-1/2} approximates full second-moment structure in O(m+n) memory. Core assumption: Hessian has approximately separable row/column structure; gradient covariance E[∇f∇f^T] approximates Hessian.

### Mechanism 3: Curvature-Weighted Saliency for Principled Merging
Saliency scores S_τ,(i,j) ∝ (W-W_0)² × √(R_i × C_j) identify task-critical parameters more accurately than magnitude alone, enabling superior sparsification and conflict resolution. During training, accumulate deviation weighted by geometric mean of row/column curvatures. At merge time: (1) threshold to top-k% per task, (2) resolve sign conflicts via importance-weighted election, (3) aggregate via curvature-weighted averaging: Δw_merged = P̂_{combined}^{-1} Σ_τ P̂_τ(M_τ ⊙ Δw_τ). Core assumption: High-curvature parameters are more sensitive to perturbation; preserving them maintains task performance better than preserving large-magnitude parameters in flat regions.

## Foundational Learning

- **Low-rank matrix approximation (SVD/truncated SVD)**
  - Why needed here: UMTAM's memory efficiency depends on understanding why keeping top-r singular values captures most information, and how reconstruction error relates to σ_{r+1}.
  - Quick check question: Given a matrix with singular values [10, 5, 1, 0.1, 0.01], what fraction of Frobenius norm energy is captured by rank-2 approximation?

- **Momentum and adaptive learning rates (Adam family)**
  - Why needed here: UMTAM extends Adam-style optimization; understanding why first/second moments help convergence is prerequisite to appreciating how factorized versions preserve these benefits.
  - Quick check question: Why does Adam use bias correction (β^t terms) in early training, and what happens if omitted?

- **Fisher Information and curvature in loss landscapes**
  - Why needed here: The paper's core claim is that training-time curvature suffices for merging; understanding Fisher-weighted averaging as "uncertainty-weighted parameter combination" is essential.
  - Quick check question: In Fisher merging, should parameters with HIGH Fisher information receive LARGER or SMALLER weight, and why?

## Architecture Onboarding

- **Component map:**
  Training Phase: Gradient computation → Error-augmented momentum (Eq 7) → Truncated SVD (Eq 8-9) → Factorized preconditioner (Eq 10-12) → Parameter update (Eq 13) → Saliency accumulation (Eq 14)
  Merging Phase: Task vectors Δw_τ → Curvature-aware pruning (Eq 23-24) → Sign election (Eq 25-27) → Curvature-weighted aggregation (Eq 28-30)

- **Critical path:** The saliency accumulation loop (Eq 14) is the linchpin—errors here propagate to merging. Verify α decay matches task-switching frequency; verify R, C remain positive (add ε floor).

- **Design tradeoffs:**
  - Rank r: Higher = better approximation but more memory; paper shows r=32 sufficient (82.5% energy capture)
  - Sparsity k: Lower = more compression but more information loss; optimal at k=40% for GLUE
  - SVD frequency T_svd: Less frequent = faster training but stale subspace; amortized cost is O(mnr/T_svd)

- **Failure signatures:**
  - Exploding loss early: Check learning rate × rank interaction; MoFaSGD showed 78.8% degradation at η=1e-4, r=16
  - Near-random merged model: Likely sign election failing—verify conflict resolution logic doesn't zero entire masks
  - Negative CoLA MCC in 4-task merge: Expected behavior for incompatible task combinations; not a bug

- **First 3 experiments:**
  1. **Convergence sanity check**: Train GPT-2 on FineWeb with UMTAM vs AdamW; expect matching loss curves (Table 18 shows 0.761 vs 0.762)
  2. **Spectral validation**: Log momentum stable rank during BERT fine-tuning; expect values ≈3-6, energy ratio >80% at r=32 (Figure 4-5)
  3. **Merging pilot**: Merge 2-3 GLUE task experts with k=20%; compare UMTAM vs TIES; expect 1-2% average improvement (Table 9)

## Open Questions the Paper Calls Out

### Open Question 1
Does the low-rank assumption and rank-invariance of UMTAM hold effectively for Vision Transformers (ViT) and multimodal architectures? The conclusion states, "Our evaluation focuses on language modeling tasks... validation on vision transformers and multimodal architectures remains a promising direction for future research." Why unresolved: The empirical validation is restricted to NLP tasks (GLUE, GPT-2, Mistral), leaving the behavior of gradient spectral decay and curvature-aware merging in visual domains untested. What evidence would resolve it: Experiments replicating the rank-sensitivity and merging performance analysis on standard vision benchmarks (e.g., ImageNet with ViT) or multimodal models.

### Open Question 2
Can UMTAM's accumulated trajectory information be utilized to enable communication-efficient model aggregation in federated learning environments? The conclusion suggests, "Federated settings could leverage trajectory information for communication-efficient model aggregation." Why unresolved: The current framework focuses on a single-node training and merging pipeline; the transmission and aggregation of factorized momentum statistics across distributed clients is not analyzed. What evidence would resolve it: A theoretical and empirical analysis of merging UMTAM-trained models in a federated setting, measuring communication costs and convergence speed relative to FedAvg or similar baselines.

### Open Question 3
Does adaptive or task-specific weighting of UMTAM components (e.g., sign election, curvature aggregation) improve performance on tasks with unique loss landscape geometries? The ablation study notes that removing specific components improved performance on certain tasks (e.g., removing curvature aggregation improved CoLA) and explicitly suggests "opportunities for adaptive component weighting in future work." Why unresolved: The current framework applies fixed algorithmic components across all tasks, but the ablation results indicate that "one-size-fits-all" application may be suboptimal for specific tasks like linguistic acceptability. What evidence would resolve it: A mechanism that dynamically adjusts the influence of sign election or curvature weighting based on task-specific metrics (e.g., stable rank or conflict rate), demonstrating improved average performance.

## Limitations

- Theoretical convergence guarantees depend critically on gradient low-rank structure assumptions that may not hold for all tasks or architectures
- Sign conflict resolution heuristic lacks theoretical justification for arbitrary task combinations, particularly where tasks are fundamentally incompatible
- Empirical validation restricted to language modeling tasks; behavior in vision and multimodal domains remains untested

## Confidence

- **High Confidence**: Training efficiency claims (0.761 vs 0.762 GPT-2 loss), basic memory reduction (O(mn) → O(mr+nr)), and fundamental algorithmic correctness of SVD compression and curvature accumulation
- **Medium Confidence**: Multi-task merging improvements (14.9% over averaging, 1.6% over TIES), curvature-aware pruning advantages (135% at 1% density), and Fisher approximation quality
- **Low Confidence**: Theoretical bounds under non-stationary task distributions, scalability to billion-parameter models, and generalization to non-NLU domains

## Next Checks

1. **Spectral Stability Test**: Monitor momentum stable rank during extended training (50k+ steps) across diverse tasks to verify low-rank structure persists under long-term optimization
2. **Cross-Domain Generalization**: Apply UMTAM to vision tasks (e.g., ViT on ImageNet subsets) to test whether curvature-aware merging benefits transfer beyond language understanding
3. **Conflict Resolution Stress Test**: Systematically merge task pairs with known interference (e.g., CoLA + MNLI) to quantify when sign election fails and whether the framework can detect irreconcilable conflicts