---
ver: rpa2
title: Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue
  Detection
arxiv_id: '2505.23627'
source_url: https://arxiv.org/abs/2505.23627
tags:
- speech
- reading
- miscue
- text
- verbatim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a method to improve reading error annotation
  by extending Whisper ASR with two key modifications: prompting with the target reading
  text and adding special tokens for miscue detection (substitutions, omissions, insertions).
  The approach aims to directly predict both verbatim transcriptions and miscue events,
  addressing limitations of post-hoc ASR-based methods.'
---

# Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection

## Quick Facts
- arXiv ID: 2505.23627
- Source URL: https://arxiv.org/abs/2505.23627
- Reference count: 0
- One-line primary result: Prompting Whisper with reading text improves verbatim transcription, and end-to-end miscue detection is feasible with F1 scores ranging from 0.33 to 0.67.

## Executive Summary
This paper introduces a method to improve reading error annotation by extending Whisper ASR with two key modifications: prompting with the target reading text and adding special tokens for miscue detection (substitutions, omissions, insertions). The approach aims to directly predict both verbatim transcriptions and miscue events, addressing limitations of post-hoc ASR-based methods. Experiments on children's and atypical adult speech show that prompting significantly improves WER over fine-tuning alone, and that end-to-end miscue detection is feasible. The best results achieved WER < 15% and F1 scores for miscue detection ranging from 0.33 to 0.67, outperforming prior work on children's speech.

## Method Summary
The method extends Whisper ASR by (1) prepending the target reading text as prompts before the <SOT> token to provide contextual conditioning, and (2) augmenting the tokenizer vocabulary with three miscue tokens (<OMIT>, <SUBSTITUTE>, <INSERT>) to enable end-to-end miscue detection. The model is fine-tuned with loss computed only on predicted outputs (not the prompt), teaching it to use the prompt as signal rather than copy it. This joint approach predicts both verbatim transcriptions and miscue events in a single forward pass, evaluated on children's speech (ages 5–9) and adult atypical speech (Parkinson's, ALS, cerebral palsy, Down syndrome).

## Key Results
- Prompting significantly improves WER over fine-tuning alone, reducing WER from ~25% to <15% on held-out test sets
- End-to-end miscue detection is feasible with F1 scores ranging from 0.33 to 0.67, though post-hoc calculation from E2E transcripts remains more accurate
- Smaller models (tiny.en) benefit more from joint E2E training with miscue tokens than larger models, showing improved generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prepending target reading text as prompts to Whisper improves verbatim transcription more effectively than fine-tuning alone.
- **Mechanism**: The reading text is tokenized and prepended before the `<SOT>` token, providing contextual conditioning that guides decoding. Loss is computed only on predicted outputs (not the prompt), teaching the model to use the prompt as signal rather than copy it. This context helps resolve ambiguities in under-represented speech (children, atypical).
- **Core assumption**: The model can learn to attend to prompt context without being trained to reproduce it verbatim.
- **Evidence anchors**:
  - [abstract] "demonstrating that incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning"
  - [section 4.1] "prompting is more effective than fine-tuning without prompting, further supporting the use of prompting for providing context"
  - [corpus] "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts" shows related prompt-based reading evaluation approach.
- **Break condition**: If prompts contain words not in the model's vocabulary, or if audio-prompt alignment is systematically mismatched (e.g., wrong reading text), performance degrades.

### Mechanism 2
- **Claim**: Extending the tokenizer with miscue tokens enables end-to-end miscue detection without post-hoc alignment.
- **Mechanism**: Three special tokens (`<OMIT>`, `<SUBSTITUTE>`, `<INSERT>`) are added to the vocabulary. The model learns to emit these tokens at positions where the spoken word deviates from the target, allowing joint prediction of transcription and error type in a single forward pass.
- **Core assumption**: Miscue events can be mapped to discrete token categories and the model has sufficient capacity to learn this secondary task alongside transcription.
- **Evidence anchors**:
  - [section 2.2] "we augment the vocabulary of the tokenizer to include tokens associated with miscue events"
  - [section 4.2] "models can directly predict miscues in an end-to-end manner, although post-hoc calculation approaches are still most accurate"
  - [corpus] Weak/missing: No direct corpus evidence for miscue token augmentation specifically; related work focuses on disfluency tagging rather than reading miscues.
- **Break condition**: If miscue annotations in training data are inconsistent (e.g., different annotator criteria) or if certain miscue types are extremely rare (insertion errors in Xa), the model fails to learn reliable detection for those classes.

### Mechanism 3
- **Claim**: Joint training for transcription and miscue detection improves verbatim transcription, especially for smaller models.
- **Mechanism**: The miscue detection task acts as an auxiliary objective that forces the model to attend to disfluencies and deviations it might otherwise smooth over. This is a form of multi-task regularization. Smaller models (tiny.en) benefit more because they lack capacity to learn implicit error patterns without explicit supervision.
- **Core assumption**: The miscue task provides a complementary learning signal that doesn't conflict with the primary transcription objective.
- **Evidence anchors**:
  - [section 4.1] "the E2E approach has better or similar verbatim transcription performance to tuned models with prompting, especially when train and test-sets are drawn from the same data distribution... and for smaller ASR models"
  - [section 4.2] "smaller models benefit more from the additional task of miscue predictions than larger models"
  - [corpus] Weak/missing: No corpus evidence directly supports this joint-task regularization effect.
- **Break condition**: If miscue frequency differs dramatically between training and test distributions (as seen with XCMU generalization), the auxiliary task may hurt rather than help.

## Foundational Learning

- **Concept: Whisper encoder-decoder architecture and token limits**
  - **Why needed here**: The method modifies Whisper's tokenization and prompts; understanding the 30-second audio / 448-token constraint is essential for data preparation and prompt design.
  - **Quick check question**: If your audio clip is 35 seconds and your prompt is 100 tokens, what must you do before feeding it to Whisper?

- **Concept: Word Error Rate (WER) vs. F1 for sequence evaluation**
  - **Why needed here**: The paper reports both metrics; WER captures transcription quality while F1 captures per-token miscue classification. They can move independently.
  - **Quick check question**: A model achieves 4% WER but 0.33 F1 on `<INSERT>`. What does this tell you about its error profile?

- **Concept: Tokenizer vocabulary extension and embedding initialization**
  - **Why needed here**: Adding miscue tokens requires modifying the tokenizer and initializing new embeddings. Poor initialization can destabilize fine-tuning.
  - **Quick check question**: When adding `<OMIT>`, `<SUBSTITUTE>`, `<INSERT>` to a pretrained tokenizer, how should you initialize their embeddings?

## Architecture Onboarding

- **Component map**:
  - **Input**: LogMel spectrogram (audio) + tokenized reading text prompt prepended to `<SOT>`
  - **Encoder**: Whisper encoder (frozen or fine-tuned depending on experiment)
  - **Decoder**: Whisper decoder with extended vocabulary (3 miscue tokens + `<CORRECT>` implicit label)
  - **Output**: Token sequence containing transcription + miscue annotations
  - **Loss**: Cross-entropy over output tokens only (prompt excluded)

- **Critical path**:
  1. Prepare aligned dataset: audio, target reading text, verbatim transcript, miscue annotations per word
  2. Tokenize reading text, prepend to `<SOT>`, ensuring total tokens ≤ 448
  3. Forward pass through Whisper with extended vocabulary
  4. Compute loss on verbatim + miscue tokens only
  5. Evaluate: strip miscue tokens for WER; align miscue tokens with ground truth for F1

- **Design tradeoffs**:
  - **Prompting vs. fine-tuning only**: Prompting requires target text at inference but provides strong context; fine-tuning alone doesn't leverage this signal.
  - **E2E vs. post-hoc**: E2E is simpler and faster at inference; post-hoc from E2E transcripts achieves higher miscue F1 but requires separate alignment step.
  - **Model size**: Smaller models benefit more from E2E joint training; larger models may not need it and generalize better with tuned+prompted approach.
  - **Token granularity**: Current tokens mark word-level miscues; finer-grained (phoneme-level) or coarser-grained (sentence-level) would require different annotation schemes.

- **Failure signatures**:
  - **High WER (>15%) + low miscue F1**: ASR foundation failing—check audio quality, model size, or whether fine-tuning data is representative.
  - **Low WER but near-zero miscue F1**: Model transcribes well but ignores miscue task—check if miscue tokens are actually being trained (loss masking issue?).
  - **High recall, low precision on miscues**: Model over-predicts errors—check class imbalance; Xa had rare substitution/omission errors.
  - **E2E worse than tuned+prompted on held-out data**: Distribution shift in miscue frequency (XCMU pattern)—consider domain adaptation or more diverse training data.
  - **Model predicts `<CORRECT>` for everything**: Miscue tokens not being learned—verify tokenizer extension, embedding initialization, and loss computation includes miscue tokens.

- **First 3 experiments**:
  1. **Establish baseline**: Run untuned Whisper (tiny.en, small.en, medium.en) on your dataset with and without prompting. Measure WER only. Confirm prompting effect before any fine-tuning.
  2. **Fine-tuning ablation**: Compare (a) fine-tuned unprompted, (b) fine-tuned prompted, (c) E2E with miscue tokens. Measure both WER and per-miscue-type F1.
  3. **Generalization test**: Train on your primary data, evaluate on a held-out dataset with different miscue characteristics (analogous to XCMU). Determine whether E2E or tuned+prompted is more robust to distribution shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the end-to-end miscue detection framework be extended to detect additional speech events (e.g., pauses, filler words, partial-word repetitions) beyond the three miscue types currently supported?
- Basis in paper: [explicit] The authors state: "This framework is theoretically extensible to other speech events such as pauses or filler words. Due to the limited availability of such annotated data... we focus this work on miscue events that are computable with common string comparison operations."
- Why unresolved: Limited annotated data for these additional speech event types prevented their inclusion; the current work prioritized comparison with post-hoc approaches.
- What evidence would resolve it: Evaluation on datasets annotated for pauses, filler words, or other dysfluency types, showing F1 scores comparable to those achieved for omit/substitute/insert miscues.

### Open Question 2
- Question: How can substitution miscue detection be improved, given that the most common error in E2E setups is falsely classifying substitutions as correct?
- Basis in paper: [explicit] The authors report: "The most common errors in the E2E setups falsely classify substitution miscues as correct, presenting opportunities for future improvements."
- Why unresolved: The paper identifies this error pattern but does not investigate causes or mitigation strategies; it may relate to model attention to phonetic similarity or context.
- What evidence would resolve it: Ablation studies or architectural modifications (e.g., phoneme-level features, contrastive training on substitution pairs) demonstrating improved substitution F1 without degrading other miscue types.

### Open Question 3
- Question: What factors contribute to the lower miscue detection performance on atypical adult speech (Xa) compared to children's speech (Xc), and how can this gap be closed?
- Basis in paper: [explicit] The authors observe: "For atypical adult speech, Xa, there is an overall lower performance compared to children's read speech. Observed low precision is likely due to relative rarity and lower diversity of miscues observed."
- Why unresolved: The hypothesized cause (class imbalance and limited miscue diversity) is stated but not systematically validated or addressed through techniques like data augmentation or re-weighting.
- What evidence would resolve it: Experiments controlling for miscue frequency/diversity across datasets, or applying class-balancing techniques to Xa showing improved F1 scores approaching Xc levels.

## Limitations

- **Data access**: The primary datasets (Xc, Xa) are not publicly available, and the exact preprocessing/filtration criteria are unspecified. While XCMU is public, it requires re-annotation for verbatim miscues to match the experimental setup.
- **Hyperparameter transparency**: Critical training details including learning rate, batch size, optimizer schedule, and early stopping criteria are not provided. This limits exact reproduction and makes it difficult to assess whether results are optimal or sensitive to these choices.
- **Generalization boundary**: The promising E2E approach for miscue detection works well when train/test distributions match but degrades on XCMU (different miscue frequency distribution). The paper doesn't establish clear generalization bounds or mitigation strategies for distribution shift.

## Confidence

**High confidence**: Prompting significantly improves WER over fine-tuning alone for both children's and atypical adult speech. This is directly supported by experimental results showing WER reduction from ~25% to <15% on held-out test sets.

**Medium confidence**: Joint E2E training with miscue tokens improves verbatim transcription, especially for smaller models. While supported by WER improvements, the mechanism (auxiliary task regularization) is hypothesized rather than empirically validated through ablation.

**Medium confidence**: End-to-end miscue detection is feasible but post-hoc calculation from E2E transcripts remains more accurate. The F1 scores (0.33-0.67) demonstrate feasibility, but the performance gap to post-hoc methods suggests room for improvement.

**Low confidence**: The claim that smaller models benefit more from E2E joint training than larger models. This is based on observed trends but lacks statistical significance testing or mechanistic explanation for why model capacity would affect auxiliary task benefits.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Run the tuned+prompted approach across a grid of learning rates (1e-5, 5e-5, 1e-4) and batch sizes (16, 32, 64) on a subset of Xc. Measure whether WER improvements from prompting are robust to these choices or require specific optimization.

2. **Distribution shift experiment**: Train on Xc, then evaluate on XCMU with and without domain adaptation (e.g., fine-tuning final layers on XCMU transcripts only). Compare whether E2E or tuned+prompted approach generalizes better when miscue frequency distributions differ.

3. **Class frequency ablation**: Create balanced subsets of Xc/Xa where insertion/omission/miscue frequencies match across splits. Train E2E models on original vs balanced data and measure whether rare class F1 scores improve, indicating class imbalance as the limiting factor.