---
ver: rpa2
title: 'CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement
  Learning'
arxiv_id: '2502.11896'
source_url: https://arxiv.org/abs/2502.11896
tags:
- action
- masking
- policy
- camel
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAMEL introduces a framework that leverages LLM-generated suboptimal
  policies to guide reinforcement learning in continuous action spaces. By dynamically
  constraining the action space through action masking and gradually reducing reliance
  on LLM guidance via epsilon-masking, CAMEL improves exploration efficiency and mitigates
  convergence to suboptimal solutions.
---

# CAMEL: Continuous Action Masking Enabled by Large Language Models for Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2502.11896
- **Source URL**: https://arxiv.org/abs/2502.11896
- **Reference count**: 1
- **Primary result**: CAMEL improves sample efficiency in continuous control RL by using LLM-generated policies to guide action masking, achieving expert-level performance in Hopper-v4 and Ant-v4 while maintaining baseline performance in Walker2d-v4

## Executive Summary
CAMEL introduces a framework that leverages LLM-generated suboptimal policies to guide reinforcement learning in continuous action spaces. By dynamically constraining the action space through action masking and gradually reducing reliance on LLM guidance via epsilon-masking, CAMEL improves exploration efficiency and mitigates convergence to suboptimal solutions. In Hopper-v4 and Ant-v4 environments, LLM-generated policies significantly enhanced sample efficiency, achieving performance comparable to or exceeding expert masking baselines. In Walker2d-v4, where LLMs struggled to model bipedal gait dynamics, CAMEL maintained robust RL performance without degradation, demonstrating adaptability across diverse tasks. The framework shows promise in addressing exploration inefficiencies and convergence challenges in RL, though further research is needed to generalize CAMEL to multimodal LLMs and automate policy evaluation.

## Method Summary
CAMEL combines LLM-generated suboptimal policies with epsilon-masking to improve RL sample efficiency in continuous control tasks. The LLM (Google Gemini 2.0) generates hard-coded Python policies based on environment documentation and MuJoCo XML. These policies are evaluated to create action bounds that constrain the RL agent's exploration space. The framework uses a modified TD3 architecture where the actor network outputs normalized actions that are mapped to constrained bounds via ACTION_MAPPING. Epsilon-masking gradually reduces the masking probability over training, allowing the agent to transition from guided exploration to autonomous learning. The method stores action bounds in the replay buffer and applies them during both training and evaluation, with the masking disabled during evaluation to assess true policy performance.

## Key Results
- In Hopper-v4 and Ant-v4, LLM-generated policies improved sample efficiency, achieving performance comparable to or exceeding expert masking baselines
- In Walker2d-v4, where LLM policies were ineffective, CAMEL maintained baseline RL performance without degradation
- The epsilon-masking mechanism successfully transitioned agents from guided exploration to autonomous policy refinement
- Human evaluation of LLM policies remains necessary due to unreliable episode return metrics for policy selection

## Why This Works (Mechanism)

### Mechanism 1: LLM-Generated Suboptimal Policies as Exploration Priors
- Claim: Hard-coded Python policies generated by LLMs provide directionally useful action bounds that reduce early-exploration variance
- Mechanism: The LLM receives environment documentation and outputs a `policy(obs)` function using PD control logic. This function is evaluated once per environment step to compute action bounds
- Core assumption: The LLM's encoded "common-sense physics" produces actions within a useful neighborhood of optimal
- Evidence anchors: Abstract states LLM policies "offer valuable initial guidance"; section 3.1 shows example PD control policy; related work supports LLM-guided exploration conceptually
- Break condition: When LLM policies are ineffective (e.g., Walker2d-v4 bipedal gait), the mechanism provides no benefit but also causes no degradation

### Mechanism 2: Masking-Aware Action Space Constraint
- Claim: Constraining the actor's output to a bounded region around πLLM reduces the effective search space, improving sample efficiency
- Mechanism: The actor network outputs `x ∈ [0, 1]`, which is mapped via `ACTION_MAPPING(x, alb, aub)` where `scale = (aub − alb)/2`, `bias = (aub + alb)/2`
- Core assumption: The optimal policy lies within the bias-scaled neighborhood of πLLM during early training
- Evidence anchors: Section 3.2 explains the masking-aware TD3 design; Figure 4 shows training curves where CAMEL with random masking still learns; related work supports action masking for incorporating priors
- Break condition: If bias is set too narrow, the agent cannot escape suboptimal regions; if too wide, masking provides no benefit

### Mechanism 3: Epsilon-Masking for Scheduled Guidance Decay
- Claim: Linearly decaying the masking probability allows the agent to transition from guided exploration to autonomous optimization
- Mechanism: At each timestep, with probability `1 − ϵt`, apply LLM-based bounds; otherwise use full action space bounds. Decay: `ϵt = max(1 − t/(fm·T), 0.0)` where `fm = 0.2`
- Core assumption: The RL agent's learned policy improves sufficiently during the masked phase to continue autonomously
- Evidence anchors: Abstract states epsilon-masking "gradually reduces reliance on LLM-generated guidance"; section 3.3 describes the phased reduction; no direct corpus evidence for this specific approach
- Break condition: If `fm` is too small, the agent overfits to LLM guidance; if too large, sample efficiency gains are lost

## Foundational Learning

- **TD3 (Twin Delayed DDPG)**: Why needed: CAMEL is implemented as Masking-Aware TD3; understanding twin critics, delayed policy updates, and target policy smoothing is required to modify the actor/critic architecture. Quick check: Can you explain why TD3 uses two critic networks and takes the minimum during target computation?
- **Continuous Action Masking**: Why needed: Unlike discrete masking, continuous masking requires defining bounded regions and remapping actor outputs. Quick check: Given action bounds [0.2, 0.8], how would you map an actor output of 0.5 (from [0,1]) to the constrained space?
- **Proportional-Derivative (PD) Control**: Why needed: LLM-generated policies use PD control logic; understanding gains (kp, kd) helps interpret and debug generated policies. Quick check: In `torque = kp * error - kd * velocity`, what happens if kd is too large relative to kp?

## Architecture Onboarding

- **Component map**: LLM Policy Generator -> Masking Module -> Masking-Aware Actor -> Action Mapper -> Environment -> Replay Buffer -> Critic Networks -> TD3 Update
- **Critical path**: 
  1. Generate 5 candidate policies via LLM with CoT prompting
  2. Human evaluates rendered episodes → select best policy
  3. Initialize TD3 networks, set `fm = 0.2`, `bias = 0.3`
  4. For each timestep: compute bounds → apply epsilon-masking → actor forward → action mapping → environment step → store transition → TD3 update
- **Design tradeoffs**:
  - `bias`: Larger values allow more exploration but weaken guidance; default 0.3
  - `fm`: Longer masking (smaller fm) increases LLM influence but risks convergence to suboptimal policies; default 0.2
  - Policy selection: Automated metrics (episode return) can mislead; human review is currently required
- **Failure signatures**:
  - Agent performs well under masking but degrades in evaluation → `fm` too small, agent over-relies on LLM
  - No improvement over baseline → LLM policy is ineffective, or `bias` too large
  - Training instability → check action bound computation for NaN/Inf from πLLM
- **First 3 experiments**:
  1. Baseline validation: Run CAMEL-TD3 with expert policy as πLLM on Hopper-v4 to verify implementation matches paper curves
  2. Ablation on `fm`: Test `fm ∈ {0.1, 0.2, 0.4}` on Hopper-v4 and Ant-v4 to measure sensitivity of sample efficiency vs. final performance
  3. Robustness test: Run CAMEL with random action bounds vs. CAMEL with poor LLM policy on Walker2d-v4 to confirm degradation is bounded

## Open Questions the Paper Calls Out
- Can the LLM policy selection process be fully automated to remove the reliance on human expert screening? The paper states that current policy selection requires human experts to review rendered videos because episode returns are unreliable indicators of quality, and lists "automate policy evaluation" as a goal for reducing human intervention. Episode return is a noisy metric, making it difficult to distinguish between valid strategies and suboptimal local optima without visual verification. An automated heuristic or auxiliary model that evaluates rendered behaviors for task-specific attributes would resolve this.
- Can CAMEL be generalized to multimodal observation spaces, such as visual inputs, rather than relying solely on vectorized states? The authors note that the framework's applicability is currently "limited to vectorized observation spaces" and explicitly propose generalizing to "multimodal LLMs for broader observation-action spaces" in future work. The current method relies on LLMs processing explicit numerical state vectors; converting continuous visual data into actionable Python logic requires bridging the gap between pixel-level perception and symbolic control. Demonstration of CAMEL operating on vision-based environments would resolve this.
- How does CAMEL's performance scale with the complexity of the environment dynamics, particularly where LLMs fail to generate effective priors? While the paper notes Walker2d-v4 failed due to the LLM struggling with bipedal gait dynamics, it leaves open whether the framework can actively detect or compensate for such low-quality priors beyond simply degrading to baseline performance. The framework currently assumes the LLM provides *some* value; it lacks a mechanism to autonomously improve or discard a generated policy if it is effectively random or detrimental. A dynamic thresholding mechanism that identifies ineffective πLLM candidates early in training would resolve this.

## Limitations
- The LLM policy generation process relies heavily on human evaluation, making the framework less automatable than presented
- The action mapping bias (0.3) and epsilon-masking fraction (fm=0.2) are treated as fixed hyperparameters without sensitivity analysis
- The framework's reliance on high-quality LLM policies is not addressed - when LLM policies are ineffective, the method provides no benefit

## Confidence
- **High**: The epsilon-masking mechanism provides a principled way to transition from guided to autonomous learning
- **Medium**: Sample efficiency improvements in Hopper-v4 and Ant-v4 are demonstrated, though the human-in-the-loop policy selection limits reproducibility
- **Low**: Claims about avoiding suboptimal convergence and handling ineffective LLM policies are primarily based on the Walker2d-v4 failure case, which is not rigorously tested

## Next Checks
1. Automate the policy selection process using episode returns instead of human evaluation and verify performance remains comparable
2. Conduct systematic ablation studies on bias and fm hyperparameters across all three environments
3. Test the framework's behavior when LLM policies are intentionally made poor or random to verify the claimed robustness