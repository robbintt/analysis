---
ver: rpa2
title: 'Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking'
arxiv_id: '2503.00231'
source_url: https://arxiv.org/abs/2503.00231
tags:
- proverbs
- cultural
- arabic
- alefisolated
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Jawaher, a new benchmark for assessing Large
  Language Models' (LLMs) ability to comprehend and interpret Arabic proverbs. Jawaher
  includes 10,037 proverbs from 20 Arabic dialects, each paired with idiomatic translations
  and explanations in English and Arabic.
---

# Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking

## Quick Facts
- arXiv ID: 2503.00231
- Source URL: https://arxiv.org/abs/2503.00231
- Reference count: 40
- LLMs perform well on idiomatic translation but struggle with culturally nuanced explanations of Arabic proverbs

## Executive Summary
Jawaher introduces a comprehensive benchmark for evaluating Large Language Models' ability to understand and interpret Arabic proverbs across 20 dialects. The dataset contains 10,037 proverbs with idiomatic translations and explanations in English and Arabic, covering diverse themes and cultural contexts. Through extensive experiments with both open-source and closed-source models, the study reveals a significant performance gap: while models excel at literal translation tasks, they struggle considerably with providing culturally nuanced and contextually relevant explanations, highlighting the need for further refinement in figurative language processing.

## Method Summary
The study evaluates LLMs on three tasks using the Jawaher dataset: English translation/equivalent, English explanation, and Arabic explanation of Arabic proverbs. Models are assessed using automatic metrics (BERTScore F1 and BLEURT) and human evaluation (accuracy, idiomaticity, clarity, correctness, depth/detail, cultural relevance). Zero-shot inference is performed using a structured prompt template, with 10 proverbs sampled per dialect (200 total) from the full set of 10,037 proverbs. The evaluation includes open-source models (Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Gemma-2-9B-it) and closed-source models (GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, Cohere Command R+).

## Key Results
- LLMs achieve high performance on English idiomatic translation tasks but struggle with culturally nuanced explanations
- Closed-source models (GPT-4o, Claude 3.5 Sonnet) significantly outperform open-source models across all metrics
- LLM-as-Judge shows very low inter-annotator agreement (Krippendorff's α dropping from 0.70-0.97 to 0.05-0.55) for cultural relevance assessments
- Performance varies considerably across dialects, with no systematic analysis of per-dialect model performance

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of 20 Arabic dialects and multi-faceted evaluation approach. By including proverbs with literal translations, idiomatic equivalents, and detailed explanations, the dataset captures both surface-level translation ability and deeper cultural understanding. The structured evaluation framework combining automatic metrics with human assessment provides a balanced view of model capabilities, while the LLM-as-Judge component attempts to scale evaluation. The dataset's multidialectal nature exposes models to the full spectrum of Arabic linguistic variation, revealing limitations in cross-dialectal comprehension and cultural context transfer.

## Foundational Learning
- **Multidialectal Arabic**: Understanding 20 Arabic dialects is crucial because proverbs vary significantly in meaning and usage across regional varieties, requiring models to handle diverse linguistic features and cultural references.
- **Figurative language processing**: Essential for recognizing that proverbs convey meanings beyond literal translations, demanding cultural knowledge and contextual reasoning beyond standard NLP tasks.
- **Cultural context evaluation**: Needed to assess whether models capture historical background and social significance of proverbs, not just semantic content.
- **Automatic metric limitations**: Understanding that BLEURT and BERTScore measure surface similarity rather than deep cultural comprehension, particularly problematic for figurative language.
- **Zero-shot vs fine-tuning trade-offs**: Recognizing that current evaluation uses zero-shot settings, leaving open whether task-specific training would improve cultural understanding.

## Architecture Onboarding

**Component Map:** Jawaher Dataset -> Sampling Engine -> Prompt Template -> LLM Inference -> Evaluation Pipeline (Automatic Metrics + Human Evaluation + LLM-as-Judge)

**Critical Path:** Proverb Selection → Prompt Generation → Model Inference → Evaluation Scoring → Performance Analysis

**Design Tradeoffs:** Zero-shot evaluation prioritizes general model capability but may underestimate potential with task-specific training; automated metrics enable scalability but fail to capture cultural nuance; multidialectal coverage increases dataset complexity but better reflects real-world linguistic diversity.

**Failure Signatures:** Low BLEURT/BERTScore scores indicate literal translation failures; poor human evaluation on cultural relevance reveals inability to grasp figurative meaning; LLM-as-Judge disagreement suggests automated evaluation inadequacy for cultural tasks.

**First Experiments:**
1. Evaluate model performance separately for each of the 20 dialects to identify systematic performance gaps
2. Compare automatic metric scores with human evaluation on identical samples to quantify metric reliability
3. Test different prompt structures (with/without cultural context instructions) to optimize model performance

## Open Questions the Paper Calls Out
- Would fine-tuning LLMs on proverb-specific datasets significantly improve their ability to generate culturally nuanced explanations compared to zero-shot performance?
- How can automatic evaluation metrics be improved to better capture the quality of figurative language explanations, particularly for cultural nuance and contextual relevance?
- What architectural or training improvements would enable open-source models to close the performance gap with closed-source models on culturally-grounded figurative language tasks?
- To what extent does dialect-specific versus pan-Arabic training data improve model performance on proverb understanding across different Arabic varieties?

## Limitations
- Dataset construction methodology lacks transparency regarding proverb extraction and dialect classification processes
- Heavy reliance on automated metrics (BERTScore, BLEURT) that may not capture cultural understanding nuances
- LLM-as-Judge component shows very low inter-annotator agreement for cultural relevance assessments (Krippendorff's α dropping to 0.05-0.55)
- No systematic analysis of per-dialect model performance, masking potential dialect-specific weaknesses

## Confidence
- **High confidence**: Dataset size and basic structure (10,037 proverbs across 20 dialects with multi-language annotations)
- **Medium confidence**: Overall performance patterns showing LLMs excel at literal translation but struggle with cultural explanations
- **Low confidence**: Specific numerical scores and automated metric results due to unknown implementation details and questionable metric appropriateness for cultural understanding tasks

## Next Checks
1. Conduct dialect-specific analysis by evaluating model performance separately for each of the 20 dialects to identify systematic performance gaps
2. Validate the proverb annotation quality by having native speakers from multiple dialects verify the accuracy of dialect classification and meaning annotations
3. Implement a controlled human evaluation study focusing specifically on cultural relevance and contextual understanding, using the same proverbs that showed poor LLM-as-Judge agreement to establish ground truth scores