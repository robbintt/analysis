---
ver: rpa2
title: 'Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances
  Protein Representations'
arxiv_id: '2505.20052'
source_url: https://arxiv.org/abs/2505.20052
tags:
- protein
- sequence
- tasks
- ankh3
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether multi-task pre-training can enhance
  protein language models beyond traditional single-task approaches. It introduces
  Ankh3, which combines masked language modeling with multiple masking probabilities
  and sequence completion tasks using only protein sequence inputs.
---

# Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations

## Quick Facts
- arXiv ID: 2505.20052
- Source URL: https://arxiv.org/abs/2505.20052
- Reference count: 4
- Primary result: Ankh3 outperforms previous protein language models on secondary structure prediction, fluorescence prediction, GB1 fitness, and contact prediction through multi-task pre-training combining MLM with multiple masking probabilities and sequence completion

## Executive Summary
This paper introduces Ankh3, a protein language model that achieves state-of-the-art performance by combining masked language modeling with sequence completion tasks. The model uses a T5 encoder-decoder architecture with two variants (Large: 1.88B parameters, XL: 5.73B parameters) and is pre-trained on 59 million UniRef50 sequences using three masking probabilities (15%, 20%, 30%) and sequence completion of the remaining 50% of input sequences. Ankh3 demonstrates superior performance across diverse protein modeling tasks including secondary structure prediction (84.4% accuracy), fluorescence prediction (Spearman 64.6), GB1 fitness (90.3), and contact prediction (83.9 precision), particularly excelling at the XL scale where previous models degraded.

## Method Summary
Ankh3 employs a T5 encoder-decoder transformer architecture trained on UniRef50 protein sequences using a multi-task objective combining masked language modeling (MLM) with multiple masking probabilities (15%, 20%, 30%) and sequence completion tasks. The model uses short-span masking with sentinel tokens and truncates sequences to 512 tokens. Pre-training involves randomly selecting one of the three masking probabilities per batch for MLM, while the remaining 50% of sequences are used for sequence completion where the encoder processes the first 50% and the decoder generates the remaining 50%. The XL variant uses 2-way data and 2-way model sharding for efficient training on TPUv4-64 clusters.

## Key Results
- Secondary structure prediction (SSP-3): 84.4% accuracy vs 81.4% for previous best
- Fluorescence prediction: Spearman correlation of 64.6 vs 62.0
- GB1 fitness: 90.3 vs 89.6
- Contact prediction (ProteinNet L/5): 83.9 precision vs 60.95

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task pre-training combining MLM with sequence completion produces richer protein representations than single-task approaches.
- Mechanism: The model learns local token-level patterns through MLM while simultaneously learning global sequence coherence through sequence completion. This dual objective forces the encoder to develop representations useful for both understanding corrupted inputs and generating coherent continuations.
- Core assumption: Different pre-training objectives capture complementary aspects of protein sequence structure that single objectives miss.
- Evidence anchors: [abstract] "This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences."

### Mechanism 2
- Claim: Multiple masking probabilities (15%, 20%, 30%) during pre-training improve generalization across diverse downstream tasks.
- Mechanism: Different corruption levels force the model to learn representations robust to varying information gaps. Lower masking (15%) preserves more context for fine-grained patterns; higher masking (30%) requires stronger global reasoning.
- Core assumption: No single masking rate is optimal for all downstream tasksâ€”different tasks benefit from different corruption levels during pre-training.
- Evidence anchors: [section 2.5] "In the previous Ankh model... some tasks performed better when Ankh was trained on higher denoising probabilities, while other tasks performed better with lower denoising probabilities."

### Mechanism 3
- Claim: Multi-task objectives enable effective scaling to larger models without the degradation seen in single-task scaling.
- Mechanism: Additional pre-training tasks provide richer gradient signal that utilizes increased model capacity. Single-task models may saturate or overfit; multi-task learning maintains optimization pressure across more parameters.
- Core assumption: Previous scaling failures resulted from insufficient task diversity rather than architectural limits.
- Evidence anchors: [section 2.1] "the increase in the capacity of Ankh3-XL model did not lead to performance degradation as in ProtT5 when scaled from ProtT5-XL to ProtT5-XXL."

## Foundational Learning

- **T5 Encoder-Decoder Architecture**
  - Why needed here: Ankh3 uses asymmetric encoder (48 layers) and decoder (24 layers) with cross-attention. Understanding how the decoder conditions on encoder outputs is essential for both pre-training and inference.
  - Quick check question: How does cross-attention in the decoder differ from self-attention, and why does this architecture suit both denoising and sequence completion?

- **Masked Language Modeling with Sentinel Tokens**
  - Why needed here: Ankh3's MLM uses T5-style span masking with sentinel tokens (not single-token masking like BERT). This affects how sequences are corrupted and reconstructed.
  - Quick check question: What is the role of sentinel tokens, and how does span masking differ from random single-token masking?

- **Frozen Representation Transfer**
  - Why needed here: Downstream evaluation freezes the backbone and uses a ConvBERT head. This protocol determines how representations are extracted and pooled.
  - Quick check question: Why freeze the backbone instead of fine-tuning, and what does average pooling assume about sequence representations?

## Architecture Onboarding

- **Component map:**
  - Task prefix token ([NLU] or [S2S]) -> Input sequence with masking or truncation -> Encoder (48 layers, 1536/2560 dim) -> Decoder (24 layers) with cross-attention -> Output predictions

- **Critical path:**
  1. Prepend task-specific token ([NLU] or [S2S]) to input sequence.
  2. For MLM: Apply one of three masking rates uniformly sampled, replace masked spans with sentinel tokens.
  3. For S2S: Truncate to first 50% for encoder input, second 50% becomes decoder target.
  4. Encoder processes input; decoder generates predictions via cross-attention.
  5. For downstream tasks: Extract encoder representations, apply average pooling (per-token tasks skip pooling), feed to ConvBERT head.

- **Design tradeoffs:**
  - 50% completion ratio was fixed due to compute constraints; variable ratios may improve performance but unexplored.
  - X-denoiser (long-span masking) excluded based on prior poor results in Ankh models.
  - Frozen evaluation enables fair comparison but may underestimate fine-tuning potential.

- **Failure signatures:**
  - High variance across seeds (e.g., ESM3 seed 42 showed 8.78% std on contact prediction) suggests instability.
  - Ankh3-Large underperforming on both tasks indicates capacity insufficiency for multi-task learning.
  - Token prefix preference is task-dependent; no universal best choice.

- **First 3 experiments:**
  1. Ablate each pre-training task: Train Ankh3-XL with (a) MLM only, (b) completion only, (c) both. Compare downstream performance to isolate contributions.
  2. Test variable completion ratios (25%, 50%, 75%) to determine if fixed 50% is a bottleneck.
  3. Systematic prefix evaluation: Run all downstream tasks with both [NLU] and [S2S] tokens to confirm task-dependent patterns and identify which prefix suits which task type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would variable sequence completion percentages (beyond the fixed 50% split) enhance model performance on downstream tasks?
- Basis in paper: [explicit] The authors state: "Although variable completion percentages may enhance the performance of the model, it was not experimented in this work due to the limited computation power."
- Why unresolved: Computational constraints prevented exploration of alternative completion ratios during pre-training.
- What evidence would resolve it: Ablation studies comparing models trained with varying completion percentages (e.g., 30%, 40%, 60%, 70%) evaluated on the same downstream benchmarks.

### Open Question 2
- Question: Why does the S2S objective consistently outperform NLU for Ankh3-XL specifically on sequence classification tasks like GB1 and fluorescence prediction?
- Basis in paper: [explicit] The authors note: "One interesting observation that requires deeper investigation is that Ankh3-XL performed better with S2S in sequence classification tasks... However, this pattern should be tested with more tasks to confirm its consistency."
- Why unresolved: The mechanism underlying this task-dependent preference between objectives at larger model scales remains unclear.
- What evidence would resolve it: Systematic evaluation across additional sequence classification tasks and probing studies analyzing representation differences between NLU and S2S embeddings.

### Open Question 3
- Question: Can incorporating the X-denoiser objective (extreme denoising of long spans) improve performance, or does it fundamentally conflict with protein sequence learning?
- Basis in paper: [explicit] The paper states: "X-denoiser was not used due to previous experiences of long span masking in former Ankh models that resulted in poor performance."
- Why unresolved: Long-span masking may disrupt local residue patterns critical to protein structure, but this was not systematically tested with the multi-task framework.
- What evidence would resolve it: Controlled experiments adding X-denoiser to the multi-task objective, comparing against models trained without it on structure-sensitive benchmarks like contact prediction.

## Limitations

- The paper does not specify exact span lengths for short-span masking, only mentioning "short spans" were used, which is a critical hyperparameter for pre-training.
- No direct ablation studies comparing single-task vs multi-task training within the same model scale are provided to isolate the contribution of multi-task learning.
- Evaluation uses frozen backbones with ConvBERT heads, which may underestimate the true potential of fine-tuning compared to end-to-end approaches.

## Confidence

**High Confidence:** The claim that Ankh3-Large (1.88B) and Ankh3-XL (5.73B) models were successfully trained with the specified architectures and achieved state-of-the-art performance on the reported downstream tasks.

**Medium Confidence:** The claim that multi-task pre-training (combining MLM with multiple masking rates and sequence completion) is the primary driver of performance improvements over single-task approaches.

**Low Confidence:** The claim that scaling to Ankh3-XL specifically succeeds due to multi-task objectives while ProtT5 failed due to insufficient task diversity.

## Next Checks

**Check 1: Ablation Study of Pre-training Objectives**
Train three variants of Ankh3-XL: (a) MLM only with 20% masking, (b) sequence completion only, (c) the full multi-task combination. Evaluate all three on the downstream tasks (SSP-3, fluorescence, GB1, contact prediction) to directly quantify the contribution of each pre-training objective.

**Check 2: Variable Completion Ratio Analysis**
Systematically evaluate completion ratios of 25%, 50% (as reported), and 75% during pre-training. Compare downstream performance to determine if the fixed 50% ratio is optimal or if variable ratios could further improve results.

**Check 3: Prefix Token Validation Protocol**
Run comprehensive experiments using both [NLU] and [S2S] prefix tokens across all downstream tasks, not just selected ones. For each task, determine which prefix yields better performance and analyze whether task type (NLU vs S2S) correlates with prefix preference.