---
ver: rpa2
title: "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for \xDC\
  -Tsang, Amdo and Kham Speech Dataset Generation"
arxiv_id: '2509.18060'
source_url: https://arxiv.org/abs/2509.18060
tags:
- dialect
- speech
- tibetan
- tmd-tts
- multi-dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the scarcity of high-quality parallel speech\
  \ corpora for Tibetan dialects (\xDC-Tsang, Amdo, Kham) by proposing TMD-TTS, a\
  \ unified multi-dialect text-to-speech framework. The core method integrates dialect\
  \ embeddings via a fusion module and introduces a Dialect-Specialized Dynamic Routing\
  \ Network (DSDR-Net) that replaces standard FFN layers in the Transformer, enabling\
  \ fine-grained dialectal control through conditional computation."
---

# TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation

## Quick Facts
- arXiv ID: 2509.18060
- Source URL: https://arxiv.org/abs/2509.18060
- Reference count: 0
- Proposes unified TTS model for three Tibetan dialects with generated parallel dataset

## Executive Summary
This paper introduces TMD-TTS, a unified text-to-speech framework for Tibetan dialects (Ü-Tsang, Amdo, Kham) that addresses the scarcity of high-quality parallel speech corpora. The system uses dialect embeddings and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to enable fine-grained dialectal control within a single model. The approach generates a large-scale parallel dataset (TMDD) of 98,142 utterances and 102+ hours of speech, validated through both TTS quality and downstream dialect conversion tasks.

## Method Summary
TMD-TTS integrates dialect embeddings through a fusion module and replaces standard FFN layers in the Transformer architecture with DSDR-Net, which routes computations based on dialect context. The model is trained on synthetic parallel data generated across three Tibetan dialects. The generated TMDD dataset enables evaluation through a speech-to-speech dialect conversion task, where the TTS model synthesizes parallel speech for unseen text. Extensive objective and subjective evaluations compare the unified model against single-dialect baselines.

## Key Results
- Amdo dialect: 94.92% STOI, 3.13 PESQ, 21.32 dB SI-SDR, MOS 3.86
- Dialect similarity: 88.09% DECS and 87.78% DCA for dialect preservation
- TMDD dataset: 98,142 utterances, 102+ hours of parallel speech across three dialects

## Why This Works (Mechanism)
The unified architecture works by incorporating dialect-specific information at the embedding level and routing computations dynamically through DSDR-Net. This allows the model to share phonetic and prosodic knowledge across dialects while maintaining distinct dialect characteristics. The dynamic routing mechanism enables conditional computation that adapts to each dialect's unique patterns without requiring separate models.

## Foundational Learning
- Dialect embedding fusion: why needed - to inject dialect-specific information into shared model; quick check - verify embedding representations cluster by dialect
- Dynamic routing networks: why needed - to route computation based on dialect context; quick check - confirm routing decisions vary across dialects
- Multi-dialect parallel corpora: why needed - to enable joint training and evaluation; quick check - ensure utterance alignment across dialects
- Transformer FFN replacement: why needed - to enable dialect-specific computation paths; quick check - compare layer outputs across dialects
- Objective speech metrics (STOI, PESQ, SI-SDR): why needed - to quantify speech quality objectively; quick check - establish baseline scores for comparison
- Dialect similarity metrics (DECS, DCA): why needed - to measure preservation of dialect characteristics; quick check - validate correlation with human judgments

## Architecture Onboarding
**Component Map**: Text -> Embedding Layer -> Dialect Fusion Module -> DSDR-Net -> Transformer Decoder -> Mel-Spectrogram -> Vocoder -> Speech

**Critical Path**: Text input flows through dialect-aware embedding fusion, passes through DSDR-Net for dialect-specific processing, generates mel-spectrograms via decoder, and produces final speech through vocoder.

**Design Tradeoffs**: Unified model trades specialization for efficiency, using dynamic routing to handle dialect differences rather than separate models. This reduces model count but requires careful routing design to avoid interference between dialects.

**Failure Signatures**: Poor dialect similarity scores indicate routing failures or insufficient dialect-specific adaptation. Low speech quality metrics suggest issues with the fusion mechanism or DSDR-Net implementation.

**3 First Experiments**: 1) Test dialect embedding fusion with simple routing to establish baseline; 2) Evaluate DSDR-Net performance with synthetic dialect labels; 3) Compare unified model against single-dialect baselines on held-out test sets.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to three Tibetan dialects, excluding other varieties and restricting generalizability
- Relies entirely on synthetic evaluation data without human-transcribed speech comparisons
- Dialect similarity metrics (DECS, DCA) are novel and lack external validation or benchmarking

## Confidence
- TTS synthesis claims: High (supported by multiple objective metrics and ablation studies)
- Dialect similarity evaluations: Medium (quantitative metrics reported but external validation lacking)
- Downstream dialect conversion results: Medium (synthetic evaluation corpus, no real speech comparisons)

## Next Checks
1. Compare TMD-TTS outputs against human-transcribed speech in multi-speaker, multi-dialect evaluation
2. Validate DECS and DCA metrics by correlating with human perceptual similarity ratings across Tibetan dialects
3. Extend model to additional Tibetan dialects (e.g., Jone, Dzongkha) and assess cross-dialect transfer performance