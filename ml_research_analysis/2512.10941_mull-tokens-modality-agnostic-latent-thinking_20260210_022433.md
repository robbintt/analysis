---
ver: rpa2
title: 'Mull-Tokens: Modality-Agnostic Latent Thinking'
arxiv_id: '2512.10941'
source_url: https://arxiv.org/abs/2512.10941
tags:
- reasoning
- image
- text
- answer
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mull-Tokens, a simple and effective method
  for multimodal reasoning using modality-agnostic latent tokens. Unlike prior approaches
  that require explicit interleaving of text and image thoughts or generating intermediate
  images, Mull-Tokens allow models to flexibly reason in a unified latent space.
---

# Mull-Tokens: Modality-Agnostic Latent Thinking

## Quick Facts
- arXiv ID: 2512.10941
- Source URL: https://arxiv.org/abs/2512.10941
- Reference count: 40
- Primary result: +3% average accuracy gain on spatial reasoning benchmarks, up to +16% on reasoning-heavy splits

## Executive Summary
This paper introduces Mull-Tokens, a method for multimodal reasoning using modality-agnostic latent tokens. Unlike prior approaches requiring explicit interleaving of text and image thoughts or intermediate image generation, Mull-Tokens allow models to flexibly reason in a unified latent space. The method achieves up to +16% improvement on reasoning-heavy splits and a +3% average gain over strong baselines on spatial reasoning benchmarks, while being faster and more efficient than generating explicit images or verbose text chains.

## Method Summary
Mull-Tokens extend pause tokens from text-only LLMs to multimodal domains by introducing special `<MULL>` tokens that serve as an internal scratchpad for reasoning. The approach uses a three-stage training curriculum: (1) Warm-up with multimodal Chain-of-Thought supervision where tokens are anchored to either text predictions or visual subgoals via cosine similarity; (2) Relaxed fine-tuning using only final answer loss; (3) Optional GRPO refinement rewarding causally useful latent chains. Trained on Qwen2.5-VL (7B) using interleaved reasoning traces from Video-R1 and Zebra-CoT datasets, then fine-tuned on SAT spatial reasoning data.

## Key Results
- +3% average accuracy improvement over direct answer fine-tuning baseline
- +16% improvement on reasoning-heavy splits of spatial reasoning benchmarks
- Outperforms text-only Chain-of-Thought methods on visual tasks while being more efficient than image generation approaches

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Warm-up Anchoring
The model teaches tokens to represent both visual and textual information through supervised anchoring. During Stage 1, each `<MULL>` token is trained to either predict the next text token (via LM head cross-entropy) or encode a subgoal image (via cosine similarity to frozen image encoder output). This multimodal grounding is critical—text-only warm-up yields only +1.07% improvement versus +3.05% with full multimodal anchoring.

### Mechanism 2: Relaxed Supervision for Internal Optimization
After warm-up, the model drops all supervision on `<MULL>` tokens and optimizes only for final answer accuracy. This allows the model to discover internal reasoning trajectories that may differ from the supervised CoT scaffolding, avoiding potential suboptimal patterns in the training data. Without this relaxation, Euclidean distance between subsequent latents tapers off, suggesting representational collapse.

### Mechanism 3: Discrete Tokens with Parallel Self-Attention
Discrete `<MULL>` tokens processed via standard self-attention avoid the sequential update instability and error accumulation of continuous recurrent embeddings. This maintains transformer parallelism while enabling rich internal recurrence. The fixed token count K provides sufficient reasoning depth without the computational overhead of sequential processing.

## Foundational Learning

- **Concept: Latent Reasoning / Test-Time Compute**
  - Why needed: Mull-Tokens extend pause tokens and latent thinking from text-only LLMs to multimodal domains, enabling inference-time compute allocation without explicit outputs.
  - Quick check: Can you explain why adding untrained pause tokens to a model might improve reasoning, even without supervision on what those tokens should represent?

- **Concept: Chain-of-Thought (CoT) and its limitations for visual reasoning**
  - Why needed: The paper positions itself as fixing text-only CoT's failures on visual tasks, where describing puzzles in words fails to capture spatial information needed for solutions.
  - Quick check: Why might describing a visual puzzle in words fail to capture information needed to solve it?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed: Stage 3 refinement uses RL to reward latents that causally contribute to correct answers, shaping internal representations rather than just outputs.
  - Quick check: How does GRPO differ from standard supervised fine-tuning in terms of what receives gradient signals?

## Architecture Onboarding

- **Component map:** Input image (frozen Qwen2.5-VL encoder) + text tokens → K `<MULL>` tokens → Answer head
- **Critical path:** 1) Stage 1: Train on interleaved CoT data with multimodal anchoring losses; 2) Stage 2: Train on direct answers with no MULL supervision; 3) Stage 3: Apply GRPO refinement
- **Design tradeoffs:** K=10-40 tokens balances reasoning depth and efficiency; discrete tokens are faster than continuous but less expressive; frozen vision encoder minimizes complexity
- **Failure signatures:** Latents converge (distance tapers) → missing multimodal warm-up; model ignores tokens → check Stage 1 application; text CoT outperforms → task may be verbally-tractable
- **First 3 experiments:**
  1. Train with Stage 2 only (no warm-up) and compare to full two-stage training
  2. Test K ∈ {10, 20, 40, 80} and plot performance vs token count
  3. Warm up with text-only vs multimodal CoT and quantify multimodal anchoring contribution

## Open Questions the Paper Calls Out

- **Extending to other modalities:** The approach naturally extends to 3D, audio, or trajectory representations by changing the embedding function and similarity loss, but requires multimodal CoT data that is currently scarce.
- **Interpretability of latent tokens:** Without a method to decode latent representations precisely or a systematic evaluation protocol, the internal reasoning process remains partially opaque.
- **Generalization to other backbones:** Experiments focused on Qwen2.5-VL (7B); performance on other multimodal architectures and model scales remains to be verified.
- **Integration with world models:** Learning from world models to discover robust causal reasoning chains could bridge the performance gap between current models and human capabilities.

## Limitations
- Performance heavily depends on availability of high-quality multimodal reasoning traces for warm-up stage
- Method's advantage over text-only CoT is primarily demonstrated on spatial/visual tasks, not general reasoning
- Lack of interpretability for modality-agnostic latent tokens limits understanding of internal reasoning processes

## Confidence
- **High Confidence:** Two-stage curriculum reliably improves spatial reasoning accuracy; discrete tokens are more efficient than continuous embeddings; approach is computationally more efficient than alternatives
- **Medium Confidence:** Improvements are due to better spatial information preservation rather than superior reasoning; K=20 is robust but degrades at higher counts; GRPO provides additional gains depending on reward quality
- **Low Confidence:** Performance on novel tasks without interleaved CoT data is untested; claims about modality-agnostic superiority across all visual tasks are not fully validated; long-term stability and generalization remain uncertain

## Next Checks
1. **Data Efficiency Test:** Train with progressively smaller fractions of multimodal reasoning traces (10%, 25%, 50%) during Stage 1 and plot accuracy vs. training data volume
2. **Cross-Domain Generalization:** Evaluate on a completely different multimodal reasoning domain (e.g., medical imaging) without interleaved CoT training data
3. **Latent Trajectory Analysis:** Visualize and analyze latent trajectories of `<MULL>` tokens, computing trajectory length, angular diversity, and final-state clustering metrics