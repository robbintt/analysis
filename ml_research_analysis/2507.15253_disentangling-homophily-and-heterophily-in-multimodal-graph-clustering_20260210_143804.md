---
ver: rpa2
title: Disentangling Homophily and Heterophily in Multimodal Graph Clustering
arxiv_id: '2507.15253'
source_url: https://arxiv.org/abs/2507.15253
tags:
- graph
- multimodal
- clustering
- learning
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles unsupervised multimodal graph clustering, addressing
  the challenge of hybrid homophilic and heterophilic neighborhood patterns in multimodal
  graphs. The authors propose Disentangled Multimodal Graph Clustering (DMGC), which
  decomposes hybrid graphs into cross-modality homophilic graphs (capturing class
  consistency) and modality-specific heterophilic graphs (preserving inter-class distinctions).
---

# Disentangling Homophily and Heterophily in Multimodal Graph Clustering

## Quick Facts
- **arXiv ID**: 2507.15253
- **Source URL**: https://arxiv.org/abs/2507.15253
- **Reference count**: 40
- **Primary result**: Achieves up to 12.7% accuracy improvement over state-of-the-art methods on six real-world datasets

## Executive Summary
This paper addresses the challenge of unsupervised multimodal graph clustering when nodes exhibit mixed homophilic and heterophilic neighborhood patterns. The proposed Disentangled Multimodal Graph Clustering (DMGC) framework decomposes hybrid graphs into cross-modality homophilic graphs (capturing class consistency) and modality-specific heterophilic graphs (preserving inter-class distinctions). A dual-frequency fusion mechanism filters these graphs to enable effective multimodal integration. Extensive experiments on six real-world datasets demonstrate state-of-the-art clustering performance with significant accuracy improvements.

## Method Summary
DMGC tackles unsupervised multimodal graph clustering by first constructing a cross-modality consensus similarity matrix to identify reliable homophilic neighbors, then building modality-specific heterophilic graphs by selecting neighbors from dissimilarity matrices. The framework applies low-pass filtering on homophilic graphs and high-pass filtering on heterophilic graphs, with a learnable adaptive fusion mechanism balancing the two filtered representations. Multi-level self-supervised alignment objectives (reconstruction, dual-frequency contrastive, cross-modality contrastive, and KL-divergence clustering) guide the learning process without requiring labels.

## Key Results
- Achieves up to 12.7% accuracy improvement over existing methods on IMDB, Amazon, ACM, DBLP, Yelp, and Ele-Fashion datasets
- Shows robustness to noise with less than 1% performance degradation when 20% of node features are randomly corrupted
- Scales to graphs with up to 97K nodes while maintaining linear complexity through anchor-based approximations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing hybrid graphs into separate homophilic and heterophilic views improves clustering when nodes exhibit mixed neighborhood patterns.
- **Mechanism:** The framework constructs cross-modality consensus similarity matrix S, applies Top-k selection to identify reliable homophilic neighbors, and builds modality-specific heterophilic graphs by selecting neighbors from the complement of intra-modality similarity matrices.
- **Core assumption:** Real-world multimodal graphs contain nodes with both class-consistent and class-divergent neighbors that single-filter approaches cannot reconcile.
- **Evidence anchors:** [abstract] describes the decomposition into homophily-enhanced and heterophily-aware graphs; [section] Section 3.1.2 and Equation 3 describe construction using Top-k selection; [corpus] H^3GNNs similarly argues for harmonizing heterophily and homophily through joint encoding.

### Mechanism 2
- **Claim:** Low-pass filtering on homophilic graphs and high-pass filtering on heterophilic graphs captures complementary semantic signals that improve fused representations.
- **Mechanism:** The dual-frequency filter applies message passing via normalized homophilic adjacency for low-pass and heterophilic Laplacian for high-pass filtering, with learnable modality-specific coefficients adaptively balancing the two filtered representations.
- **Core assumption:** Class-consistency information resides in low-frequency graph signals while inter-class distinctions reside in high-frequency signals.
- **Evidence anchors:** [abstract] mentions dual-pass strategy; [section] Equations 6-8 define iterative filtering process; [corpus] GCL-OT notes existing methods rely on homophily assumptions in similarity estimation.

### Mechanism 3
- **Claim:** Multi-level self-supervised alignment enables effective unsupervised clustering without labels.
- **Mechanism:** The framework combines reconstruction loss, InfoNCE-based dual-frequency alignment, cross-modality contrastive loss, and KL-divergence clustering loss to align representations across frequency domains and modalities.
- **Core assumption:** Aligned representations across frequency domains and modalities form tighter, more discriminative clusters.
- **Evidence anchors:** [abstract] mentions self-supervised alignment objectives; [section] Table 3 ablation study shows each loss component degrades performance when removed; [corpus] Related work on contrastive graph learning uses similar alignment principles.

## Foundational Learning

- **Concept: Homophily vs. Heterophily in Graphs**
  - **Why needed here:** The entire framework hinges on recognizing that real-world graphs contain mixed patterns—some neighbors share the same class (homophily), others belong to different classes (heterophily). Standard GNNs assume homophily and fail when this assumption breaks.
  - **Quick check question:** Given a citation network where papers cite both related work (same field) and contrasting viewpoints (different fields), would a standard GCN capture both relationship types effectively?

- **Concept: Graph Signal Processing (Low-pass/High-pass Filtering)**
  - **Why needed here:** The dual-frequency fusion mechanism treats graph signals spectrally—low-pass smoothing preserves shared features within clusters, high-pass filtering amplifies differences across clusters. Understanding this helps interpret why the Laplacian (I - Â) extracts heterophilic signals.
  - **Quick check question:** If you apply repeated low-pass filtering on a graph signal, what happens to local variations between neighboring nodes?

- **Concept: Contrastive Learning with InfoNCE Loss**
  - **Why needed here:** Multiple alignment objectives use InfoNCE to pull positive pairs closer and push negative pairs apart in embedding space. The temperature parameter τ and batch construction critically affect what the model learns.
  - **Quick check question:** In Equation 15, what happens to the gradient signal when τ is set very small vs. very large?

## Architecture Onboarding

- **Component map:** Raw Graph G = (V, A, X) → [Disentangled Graph Construction] → Homophilic graph Â_l and Heterophilic graphs Ĺ_h^i → [Dual-frequency Filtering] → Z_i^l (low-pass) and Z_i^h (high-pass) → [Adaptive Fusion] → Z_i = γ_i Z_i^l + (1-γ_i) Z_i^h → [Alignment & Clustering] → Reconstruction loss L_REC + Dual-frequency contrastive L_Cr + Cross-modality contrastive L_Cm + Clustering loss L_CLU

- **Critical path:** The disentangled graph construction (Section 3.1) is the foundation—if k_l and k_h are poorly chosen, the subsequent filtering receives noisy inputs. Monitor homophily ratio improvement (Figure 4) as a sanity check.

- **Design tradeoffs:**
  - k_l (homophilic neighbors): Higher values (15-20) capture more context but risk mixing heterophilic neighbors; lower values (10) are more robust per Figure 6
  - k_h (heterophilic neighbors): Must remain small (2-6) to avoid contaminating heterophilic graph with homophilic connections per Figure 7
  - Loss balancing (λ, μ): Figure 5 shows λ=0.001, μ=1 is optimal on Amazon; excessive cross-modality alignment (μ=10) suppresses useful intra-modality variations

- **Failure signatures:**
  - Cluster assignments collapse to a single dominant class → check if reconstruction loss is too dominant, suppressing contrastive signals
  - Silhouette score remains low despite training → verify k_l/k_h settings; heterophilic graph may be too sparse or too dense
  - Cross-modality alignment fails on modality-imbalanced data → inspect modality weights w_i and attention coefficients β_i

- **First 3 experiments:**
  1. Reproduce ablation on single dataset (Amazon): Run w/o L_Cr, w/o L_Cm, w/o Homo, w/o Heter variants to confirm each component's contribution matches Table 3
  2. Sensitivity sweep on k_l and k_h: Plot accuracy curves for k_l ∈ {5, 10, 15, 20, 25} and k_h ∈ {2, 4, 6, 8, 10} to find dataset-specific optima
  3. Homophily ratio validation: Compute node-level homophily ratio before and after disentanglement; verify improvement aligns with Figure 4 pattern

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance heavily depends on parameter tuning (k_l, k_h), with degradation when these parameters are set too large
- Adaptive fusion coefficients lack theoretical guarantees for convergence across diverse datasets
- Self-supervised alignment assumptions may break down when modalities capture fundamentally different semantic spaces

## Confidence
**High Confidence**: Core mechanism of decomposing hybrid graphs into homophilic and heterophilic views is well-supported by ablation studies and sensitivity analysis.

**Medium Confidence**: Claims about robustness to noise and scalability to large graphs are demonstrated but lack comparison to specialized scalable methods.

**Low Confidence**: Assertion that the method "scales to graphs with up to 97K nodes" requires clarification—97K is moderate by modern standards, and memory complexity for similarity matrix computation may become prohibitive for larger graphs.

## Next Checks
1. Reproduce the ablation study on Amazon dataset to verify that each loss component contributes as claimed in Table 3.

2. Conduct sensitivity analysis on k_l and k_h parameters across all six datasets to identify dataset-specific optima and validate the patterns shown in Figures 6-7.

3. Validate the scalability claims by testing on graphs larger than 100K nodes, explicitly comparing full vs. approximate similarity computation methods to confirm the stated O(N²) memory usage.