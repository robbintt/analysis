---
ver: rpa2
title: Evaluating Adversarial Vulnerabilities in Modern Large Language Models
arxiv_id: '2511.17666'
source_url: https://arxiv.org/abs/2511.17666
tags:
- safety
- attack
- adversarial
- vulnerabilities
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the security of Google's Gemini 2.5 Flash and
  OpenAI's GPT-4o mini under adversarial jailbreak attempts using self-generated and
  cross-model attack prompts. Four attack types (direct injection, role-playing, context
  manipulation, obfuscation) were applied across five harmful content categories,
  with results measured by a severity score (1-5).
---

# Evaluating Adversarial Vulnerabilities in Modern Large Language Models

## Quick Facts
- arXiv ID: 2511.17666
- Source URL: https://arxiv.org/abs/2511.17666
- Authors: Tom Perel
- Reference count: 8
- GPT-4o mini showed higher vulnerability (average severity 2.16) than Gemini 2.5 Flash (1.96) under adversarial jailbreak attempts.

## Executive Summary
This study compared the security resilience of Google's Gemini 2.5 Flash and OpenAI's GPT-4o mini against adversarial jailbreak attacks. Using self-generated and cross-model attack prompts across four attack types (direct injection, role-playing, context manipulation, obfuscation) and five harmful content categories, the research measured vulnerability through severity scores (1-5). The findings revealed that context manipulation and role-playing were most effective attack vectors, while Gemini demonstrated perfect resistance to obfuscation attacks. GPT-4o mini exhibited overall weaker safety resilience compared to Gemini, highlighting persistent challenges in LLM safety alignment despite continuous improvements.

## Method Summary
The study employed an automated adversarial red-teaming pipeline using 320 total attack trials (160 per model). Attack prompts were generated by either Gemini or GPT-4o mini itself (self-bypass) or by the other model (cross-bypass) at temperature 0.7. Four attack vectors were tested: direct injection, role-playing, context manipulation, and obfuscation, with 20 trials per vector per bypass method. Generated prompts targeted five harm categories (hate speech, illegal activities, malicious code, dangerous content, misinformation). Target models received these adversarial prompts and outputs were scored on a 1-5 severity scale, with refusals scored as 0. The study compared severity scores and success rates across models, attack vectors, and bypass methods.

## Key Results
- GPT-4o mini had higher average severity score (2.16) than Gemini (1.96), indicating weaker safety resilience
- Context manipulation proved most effective vector for both models, exploiting the use-mention distinction
- Gemini 2.5 Flash demonstrated perfect resistance to obfuscation attacks, while GPT-4o mini struggled with token-level manipulations
- Self-bypass and cross-bypass attacks achieved equal overall success rates (35.6%), but self-bypass was more effective for semantic attacks

## Why This Works (Mechanism)

### Mechanism 1: Context Manipulation Dilutes Safety Triggers
Safety filters respond to surface-level signals rather than semantic intent. By embedding harmful requests within benign narrative frames (e.g., "write a novel scene where..."), the surrounding innocuous text dilutes the safety trigger. The model prioritizes the instruction to "be a helpful creative writer" over "be safe," failing to distinguish between mentioning harm versus instructing harm. RLHF-based alignment generalizes poorly across contextual variations, with safety training undervaluing narrative framing effects.

### Mechanism 2: Obfuscation Exploits Tokenization Gaps
Encoded inputs (Base64, Leetspeak, code-based logic) bypass keyword-based filters but remain semantically actionable after decoding. Without aggressive text normalization, raw token adversarial noise passes preprocessing. The vulnerability exists between tokenization and intent recognition—safety filters operate partially at the token/keyword level rather than purely at semantic interpretation. Gemini's perfect resistance suggests it employs aggressive text normalization or secondary screening models.

### Mechanism 3: Self-Bypass Leverages Model Self-Knowledge
When prompted to red-team themselves, models leverage their understanding of their own alignment boundaries to craft prompts exploiting specific linguistic patterns. Self-generated attacks outperformed cross-bypass for semantic vectors (role-playing, context manipulation). This suggests models encode information about their own refusal patterns and can retrieve this knowledge when appropriately prompted. The vulnerability stems from implicit self-knowledge that can be activated through appropriate prompting.

## Foundational Learning

**Concept: Reinforcement Learning from Human Feedback (RLHF)**
*Why needed:* Safety alignment in modern LLMs is achieved through RLHF. Understanding its limitations (context-sensitivity, generalization gaps) is essential for interpreting why jailbreaks work.
*Quick check:* Can you explain why RLHF alignment might fail to generalize to novel prompt formulations?

**Concept: Use-Mention Distinction**
*Why needed:* The paper identifies this linguistic concept as central to context manipulation attacks—models cannot reliably distinguish discussing harm from instructing harm.
*Quick check:* In the sentence "The character explained how to pick a lock," is the model using or mentioning the harmful knowledge?

**Concept: Tokenization and Text Normalization**
*Why needed:* Obfuscation attacks target the gap between raw token input and semantic interpretation. Understanding preprocessing pipelines clarifies why Gemini resisted obfuscation while GPT-4o mini did not.
*Quick check:* What happens when a Leetspeak input ("h0t-w1r3") passes through a tokenizer without normalization?

## Architecture Onboarding

**Component map:** Attack Generator -> Target Model -> Severity Evaluator -> Stratification Layer

**Critical path:** 1) Configure system prompt to override safety alignment for attack generation 2) Generate adversarial prompt (self or cross condition) 3) Submit prompt to target model at temperature 0.7 4) Capture output; if refusal → log as failure; if compliance → assign severity score 5) Aggregate scores by vector, model, and bypass method

**Design tradeoffs:** Temperature 0.7 balances creativity (diverse attacks) with reproducibility. Lower temp increases determinism but may reduce attack diversity. Automated evaluation is scalable but may miss subtle harm; human evaluation is more accurate but expensive. Four attack vectors represent simplified taxonomy excluding advanced methods to maintain experimental tractability.

**Failure signatures:** Direct injection failure indicates fundamental safety filter breakdown (should be near-zero). High context manipulation scores signal weak intent-classification. Obfuscation success indicates missing or weak text normalization layer. Self-bypass > cross-bypass suggests model-specific alignment gaps.

**First 3 experiments:**
1. **Baseline probe:** Run 20 direct injection attempts per model to confirm primary filter integrity. Expected: near-zero success.
2. **Context manipulation stress test:** Run 40 context manipulation trials (20 self-bypass, 20 cross-bypass) targeting each model; compare severity scores to identify most effective narrative frames.
3. **Obfuscation differential analysis:** Run 20 obfuscation attacks (Base64, Leetspeak) per model; log which encodings bypass filters to map preprocessing gaps.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the self-bypass loop be effectively integrated into the training process to create "Constitutionally" self-correcting models? The study validates self-bypass as an attack method but does not implement or test the remediation phase required to close the vulnerability loop. A fine-tuning experiment where models are trained on their own successful self-bypass failures would resolve this.

**Open Question 2:** How can LLM safety alignment be improved to reliably distinguish between "mentioning" harm (in fiction/education) and "using" harm (providing actionable instructions)? Current models prioritize the "helpful creative writer" persona over safety when prompts are embedded in benign narrative frames. A new training objective or architecture that successfully differentiates and refuses actionable instructions even within hypothetical contexts would resolve this.

**Open Question 3:** Does Gemini 2.5 Flash's perfect resistance to obfuscation attacks stem from a distinct text normalization layer or a secondary screening model? The study observes this resistance but lacks architectural access to confirm the mechanism. A white-box analysis or ablation study of Gemini's preprocessing components would isolate which specific layer neutralizes encoded adversarial noise.

## Limitations
- Automated evaluation pipeline may lack nuanced harm assessment needed for reliable severity scoring
- Temporal validity concerns as both models receive continuous safety updates
- Single-turn evaluation misses potential vulnerabilities that emerge in multi-turn conversations

## Confidence

**High Confidence:**
- Gemini 2.5 Flash demonstrating superior safety resilience overall
- Context manipulation effectiveness for both models
- Obfuscation attack failure for Gemini (perfect resistance)

**Medium Confidence:**
- Self-bypass vs cross-bypass performance equivalence
- Self-bypass superiority for semantic attacks
- Direct injection ineffectiveness for both models

**Low Confidence:**
- Severity score distributions for individual harm categories
- Temperature 0.7's optimal balance between attack diversity and reproducibility
- Human vs. automated evaluation impact on results

## Next Checks

1. **Replication with Human Evaluation:** Conduct a subset of 50 trials using human raters for severity scoring to validate automated assessment accuracy and calibrate scoring thresholds.

2. **Multi-Turn Attack Testing:** Extend the evaluation framework to include 3-turn conversation sequences where the attacker model can build context across turns, testing whether vulnerabilities persist or evolve.

3. **Cross-Model Generalization:** Apply the same attack vectors to other contemporary models (Claude, Llama, etc.) to determine if observed patterns represent fundamental alignment challenges or model-specific weaknesses.