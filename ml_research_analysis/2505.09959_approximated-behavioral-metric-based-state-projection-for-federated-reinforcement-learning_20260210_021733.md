---
ver: rpa2
title: Approximated Behavioral Metric-based State Projection for Federated Reinforcement
  Learning
arxiv_id: '2505.09959'
source_url: https://arxiv.org/abs/2505.09959
tags:
- learning
- local
- state
- federated
- fedrag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedRAG, a federated reinforcement learning
  framework that shares approximated behavior metric-based state projection function
  parameters instead of raw state information to enhance performance while preserving
  privacy. The method learns local state projection functions using a reducing approximation
  gap (RAG) distance that captures task-relevant behavioral similarities without exposing
  sensitive data.
---

# Approximated Behavioral Metric-based State Projection for Federated Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.09959
- Source URL: https://arxiv.org/abs/2505.09959
- Reference count: 13
- Key outcome: Proposed FedRAG framework achieves up to 40% performance gains in cross-environment generalization while preserving privacy by sharing approximated behavioral metric-based state projection function parameters instead of raw state information

## Executive Summary
This paper addresses the challenge of federated reinforcement learning (FRL) with heterogeneous environments by proposing FedRAG, which shares state projection function parameters rather than raw state information. The method learns local state projection functions using a reducing approximation gap (RAG) distance that captures task-relevant behavioral similarities without exposing sensitive data. Experiments on DeepMind Control Suite tasks demonstrate that FedRAG outperforms baseline federated learning methods (FedAvg) and non-federated approaches in cross-environment generalization while maintaining privacy protection.

## Method Summary
FedRAG builds on federated soft actor-critic (FeSAC) by introducing a state projection function ϕ_ω that maps raw states to embeddings where behavioral similarity correlates with task-relevant dynamics. The RAG distance approximates behavioral metrics by measuring expected reward differences and next-state distances under learned Gaussian dynamics and reward models. During federated training, clients update their local projection parameters using RAG loss plus L2 regularization to stay close to the global projection, then upload only these parameters for aggregation. The method effectively filters out task-irrelevant background information while maintaining stable performance across varying numbers of clients.

## Key Results
- FedRAG outperforms baseline federated learning methods (FedAvg) and non-federated approaches in cross-environment generalization
- Performance gains of up to 40% in unseen environments compared to state-of-the-art FRL methods
- Demonstrates robustness to environmental heterogeneity with stable performance across varying numbers of clients
- Effectively filters out task-irrelevant background information while maintaining task-relevant behavioral information

## Why This Works (Mechanism)

### Mechanism 1
Sharing state projection function parameters enables cross-environment generalization without transmitting sensitive state information. The state projection function ϕ_ω learns to map states to embeddings where behavioral similarity (measured by RAG distance) correlates with task-relevant dynamics. By aggregating ω across clients with different environments, the global projection captures shared structure while filtering environment-specific noise. The L2 regularization ensures local updates remain aligned with this global representation.

### Mechanism 2
The RAG distance approximation provides a tractable proxy for behavioral similarity that preserves value function differences. RAG distance recursively measures expected reward differences and expected next-state distances under policy π. The approximation avoids intractable Wasserstein distance by sampling next states from learned Gaussian dynamics models. Theorem 2 guarantees that RAG distance upper-bounds value function differences, ensuring embeddings preserve decision-relevant information.

### Mechanism 3
Privacy protection emerges as a side-effect because projection parameters are only indirectly related to raw states. The loss function relates ω to mapped state embeddings and rewards, not to raw states directly. Bayesian inference attacks aiming to reconstruct private data D_k from shared parameters ω face the challenge that the conditional distribution f_{D_k|W^S_k} has weak dependency—the projection abstracts away reconstructable details.

## Foundational Learning

- **Concept: Soft Actor-Critic (SAC)**
  - Why needed here: FedRAG builds on FeSAC (federated SAC) as its base RL algorithm. Understanding maximum entropy RL, the soft Q-function, and automatic temperature tuning is prerequisite to following Equations 1-6.
  - Quick check question: Can you explain why SAC maximizes both reward and entropy, and how the temperature parameter α controls exploration?

- **Concept: Behavioral Metrics / Bisimulation**
  - Why needed here: The RAG distance extends bisimulation-style metrics. Understanding that behavioral similarity measures states by their future reward/dynamics equivalence (not observational similarity) is essential for grasping why the projection generalizes.
  - Quick check question: Given two visually different states that yield identical expected future rewards, would a behavioral metric assign them high or low distance?

- **Concept: Federated Averaging (FedAvg)**
  - Why needed here: FedRAG's aggregation mechanism follows FedAvg-style parameter averaging. The L2 regularization parallels FedProx's proximal term for handling heterogeneity.
  - Quick check question: In FedAvg, what gets aggregated at the server, and why does non-IID data across clients cause client drift?

## Architecture Onboarding

- **Component map**:
  - Local to each client k: State encoder ϕ_{ω_k}, actor π_{ψ_k}, critic Q_{θ_k}, reward model ˆR_{ξ_k}, dynamics model ˆP_{η_k}, replay buffer D_k
  - Shared via federation: Only ω (projection parameters) and Q̄ (global critic for FeSAC baseline comparison)
  - Server: Aggregates ω^G ← (1/N) Σ ω_k, broadcasts back

- **Critical path**:
  1. Client collects transition (s, a, r, s') into D_k
  2. Sample state pairs from D_k; compute RAG loss (Eq. 13) + regularization (