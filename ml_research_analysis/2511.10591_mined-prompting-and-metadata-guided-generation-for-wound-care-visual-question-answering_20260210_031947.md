---
ver: rpa2
title: Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question
  Answering
arxiv_id: '2511.10591'
source_url: https://arxiv.org/abs/2511.10591
tags:
- wound
- clinical
- metadata
- care
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents two complementary approaches for the MEDIQA-WV
  2025 shared task on wound care visual question answering. The first approach uses
  mined few-shot prompting, where training data is embedded and the top-k most similar
  examples are retrieved to serve as demonstrations during generation.
---

# Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering

## Quick Facts
- arXiv ID: 2511.10591
- Source URL: https://arxiv.org/abs/2511.10591
- Reference count: 6
- Primary result: MedGemma-27B achieves deltaBLEU score of 13.04 using mined prompting

## Executive Summary
This paper presents two complementary approaches for the MEDIQA-WV 2025 shared task on wound care visual question answering. The first approach uses mined few-shot prompting, where training data is embedded and the top-k most similar examples are retrieved to serve as demonstrations during generation. The second approach builds on a metadata ablation study that identified four clinically relevant wound attributes (anatomical location, wound type, drainage type, and tissue color) that consistently enhance response quality. Classifiers are trained to predict these attributes for test cases, and the predictions are incorporated into the generation pipeline with confidence-based integration. Experimental results show that mined prompting improves response relevance, with MedGemma-27B achieving a deltaBLEU score of 13.04, while metadata-guided generation further refines clinical precision. The findings highlight the potential of combining retrieval-augmented generation and structured metadata prediction for developing AI-driven tools that provide reliable and efficient wound care support.

## Method Summary
The paper proposes two distinct approaches for wound care VQA. The first approach employs mined few-shot prompting, where training queries are embedded using all-mpnet-base-v2 sentence transformers and the top-k most similar examples are retrieved as demonstrations during generation. MedGemma-27B uses k=5 while InternVL3-38B uses k=25. The second approach involves a metadata ablation study to identify clinically relevant attributes, followed by training classifiers to predict four key wound attributes (anatomical location, wound type, drainage type, tissue color) with confidence scores. A threshold of 0.7 determines whether predictions are presented as facts or with caution in the generation prompt. Both approaches are evaluated on the WoundcareVQA dataset using deltaBLEU, BERTScore, ROUGE-L, and LLM-as-Judge metrics.

## Key Results
- MedGemma-27B with 5-shot mined prompting achieves highest deltaBLEU score of 13.04
- Anatomical location, wound type, and tissue color are the most critical metadata attributes
- Confidence threshold of 0.7 for metadata integration may be overly conservative
- InternVL3-38B requires 25-shot examples to approach MedGemma's performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving semantically similar training examples as few-shot demonstrations improves response relevance for wound care VQA.
- Mechanism: Training queries are embedded using all-mpnet-base-v2 sentence transformers. At inference, each test query is matched against the embedding index to retrieve the top-k most similar examples. These examples are formatted as in-context demonstrations, providing the model with clinically relevant patterns before generating the target response.
- Core assumption: Clinical response patterns transfer across semantically similar wound care queries, even when visual presentations differ.
- Evidence anchors:
  - [abstract] "mined prompting improves response relevance, with MedGemma-27B achieving a deltaBLEU score of 13.04"
  - [section 6.1] "MedGemma-27B (5-shot) approach achieves the highest deltaBLEU score (13.04), representing a 131% improvement over the metadata ablation approach (5.70)"
  - [corpus] Related work on multimodal RAG for medical VQA (arxiv:2510.13856) shows similar retrieval-augmented benefits, though corpus evidence for wound-specific retrieval remains limited.
- Break condition: Performance degrades if test queries fall outside the semantic distribution of training data, or if retrieved examples introduce contradictory clinical guidance.

### Mechanism 2
- Claim: A systematic ablation study can identify which structured metadata attributes most improve generation quality, and some metadata harms performance.
- Mechanism: The authors remove one metadata category at a time from a full-context prompt and measure deltaBLEU change. This quantifies each feature's contribution, revealing that anatomical location, wound type, and tissue color are critical, while wound thickness and drainage amount introduce noise.
- Core assumption: Metadata impact on deltaBLEU reflects clinical utility for response generation.
- Evidence anchors:
  - [section 6.2] "anatomical location (-0.516), wound type (-0.462), and tissue color (-0.455) cause the most significant performance degradation when removed"
  - [section 6.2] "removing wound thickness (+0.500) and drainage amount (+0.486) actually improved performance"
  - [corpus] Weak direct corpus evidence for wound metadata ablation; related work (arxiv:2502.20277) uses generated captions as metadata but doesn't systematically ablate.
- Break condition: Ablation rankings may not transfer to datasets with different annotation protocols or inter-annotator agreement profiles.

### Mechanism 3
- Claim: Confidence-thresholded integration of predicted metadata balances clinical precision against error propagation from misclassification.
- Mechanism: A classifier predicts four metadata attributes with confidence scores. If confidence ≥0.7, the prediction is presented as factual context; if <0.7, the model is instructed to be cautious about that attribute. This prevents low-confidence predictions from corrupting clinical advice.
- Core assumption: Classifier confidence correlates with ground-truth accuracy, and explicit caution instructions reduce hallucination risk.
- Evidence anchors:
  - [section 4.2] "We use a confidence threshold of 0.7 to determine how this information shapes the final response"
  - [section 7] "the conservative confidence threshold (0.7) implemented may have been overly restrictive, limiting the integration of potentially valuable clinical insights"
  - [corpus] No direct corpus comparison for confidence-thresholded metadata integration in medical VQA; this appears to be a relatively unexplored technique.
- Break condition: If classifier confidence is poorly calibrated, high-confidence errors will be integrated as facts, potentially generating unsafe clinical advice.

## Foundational Learning

- Concept: **Sentence Embeddings and Semantic Similarity Search**
  - Why needed here: Mined prompting requires embedding training queries and retrieving nearest neighbors at inference. Without understanding embedding spaces, you cannot debug retrieval quality or tune k.
  - Quick check question: Given two wound queries—"Is my incision infected?" and "Redness around surgical wound"—would you expect high cosine similarity? Why or why not?

- Concept: **Ablation Studies for Feature Attribution**
  - Why needed here: The metadata selection depends on measuring performance drop when removing each attribute. This requires understanding controlled experiments and counterfactual reasoning.
  - Quick check question: If removing feature A improves performance by +0.3 and removing feature B degrades it by -0.4, which feature should be retained? What confound might explain both results?

- Concept: **Confidence Calibration in Classification**
  - Why needed here: The 0.7 threshold assumes classifier confidence reflects correctness. Uncalibrated confidence scores could cause the system to either ignore useful predictions or amplify errors.
  - Quick check question: A classifier predicts "wound type: pressure ulcer" with 0.85 confidence but is wrong 40% of the time at that threshold. Is the model calibrated? What would you do to fix this?

## Architecture Onboarding

- Component map:
  1. Embedding Index: Training queries encoded via all-mpnet-base-v2, stored for nearest-neighbor search
  2. Retrieval Module: At inference, retrieves top-k similar training examples (k=5 for MedGemma, k=25 for InternVL)
  3. Metadata Classifier: MedGemma-27B prompted as few-shot classifier to predict 4 wound attributes with confidence scores
  4. Confidence Gate: Threshold at 0.7 determines integration strategy for each predicted attribute
  5. Response Generator: Either MedGemma-27B or InternVL3-38B generates final response conditioned on query, images, and (optionally) filtered metadata

- Critical path: Image + Query → Metadata Prediction → Confidence Filtering → Few-Shot Retrieval → Response Generation. The paper experiments with these as separate approaches; combining them would require ordering decisions.

- Design tradeoffs:
  - Model selection: MedGemma-27B is medically specialized and performs best with 5-shot context; InternVL3-38B is general-purpose multimodal and needs 25-shot to approach similar performance. Domain specialization vs. context window capacity.
  - Threshold setting: 0.7 was chosen empirically but may be overly conservative. Lower thresholds integrate more metadata but risk error propagation.
  - Feature selection: The ablation study selected 4 features, but the authors note that drainage type (IAA: 0.92) was included despite moderate agreement, while drainage amount was excluded for adding noise. Clinical intuition vs. empirical signal.

- Failure signatures:
  - Retrieval mismatch: If retrieved examples are semantically similar but clinically inappropriate (e.g., similar phrasing but different wound etiology), responses may appear fluent but give wrong advice.
  - Error propagation: In the metadata approach, early-stage classification errors cascade into response generation, especially if confidence is miscalibrated.
  - Context saturation: InternVL with 25-shot may hit context limits or dilute signal with irrelevant examples; MedGemma with 5-shot may underutilize available context.

- First 3 experiments:
  1. Baseline sanity check: Run both models with zero-shot prompting (no retrieval, no metadata) on a held-out validation split to establish floor performance. Compare deltaBLEU, BERTScore, and ROUGE-L against the paper's reported numbers.
  2. Retrieval quality audit: Manually inspect top-5 retrieved examples for 20 test queries. Annotate whether each retrieved example is clinically appropriate (same wound type, similar clinical question). Correlate retrieval appropriateness with response quality.
  3. Confidence calibration analysis: For the metadata classifier, bin predictions by confidence score (0.0-0.3, 0.3-0.6, 0.6-0.8, 0.8-1.0) and compute accuracy per bin using ground-truth metadata. Plot reliability diagram to assess whether 0.7 threshold is justified or needs adjustment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal confidence threshold for metadata integration that balances prediction reliability with clinical utility?
- Basis in paper: [explicit] The authors state "the confidence threshold mechanism (0.7) may have been overly conservative in integrating predicted clinical features, limiting the integration of potentially valuable clinical insights."
- Why unresolved: The 0.7 threshold was chosen heuristically without systematic optimization; the trade-off between avoiding incorrect advice and leveraging ambiguous information remains unexplored.
- What evidence would resolve it: A sweep across threshold values (e.g., 0.5-0.9) evaluated against clinical expert ratings of response appropriateness.

### Open Question 2
- Question: How can error propagation between metadata prediction and response generation stages be minimized in two-stage pipelines?
- Basis in paper: [explicit] "The two-stage pipeline approach... appears to suffer from error propagation between metadata prediction and response generation phases, where inaccuracies in the initial metadata prediction cascade into the final response quality."
- Why unresolved: Current architecture treats prediction and generation as sequential independent modules without feedback mechanisms.
- What evidence would resolve it: Comparison of sequential vs. joint training approaches, or ensemble metadata classifiers as the authors suggest.

### Open Question 3
- Question: Can diversity-based retrieval improve upon semantic similarity for few-shot example selection in medical VQA?
- Basis in paper: [explicit] Future work section proposes "optimizing few-shot sample selection beyond semantic similarity through diversity-based approaches and adaptive context sizing."
- Why unresolved: Current approach uses only all-mpnet-base-v2 embedding similarity; diverse examples may provide broader clinical coverage.
- What evidence would resolve it: Ablation comparing semantic-only vs. diversity-augmented retrieval on deltaBLEU and clinical evaluation scores.

## Limitations
- The 0.7 confidence threshold may be overly conservative, excluding clinically valuable predictions
- Ablation study results may not generalize to datasets with different annotation protocols
- Error propagation between metadata prediction and response generation stages remains a challenge
- Current evaluation metrics don't capture hierarchical clinical importance or uncertainty expression

## Confidence

- **High confidence**: Retrieval-augmented few-shot prompting improves deltaBLEU (13.04 for MedGemma-27B) - directly supported by experimental results with clear quantitative improvement
- **Medium confidence**: Metadata ablation correctly identifies four clinically relevant attributes - methodology is sound but corpus evidence for wound-specific metadata impact is limited
- **Medium confidence**: Confidence-thresholded integration improves clinical precision - conceptually reasonable but limited empirical validation of threshold calibration

## Next Checks

1. **Confidence Calibration Audit**: Generate reliability diagrams for metadata classifier predictions across confidence bins to determine if 0.7 threshold is justified or requires adjustment
2. **Retrieval Quality Analysis**: Manually evaluate semantic relevance of top-5 retrieved examples for 20 test cases, measuring clinical appropriateness vs. semantic similarity
3. **Error Propagation Analysis**: Compare metadata prediction accuracy (anatomical location, wound type, drainage type, tissue color) against end-to-end generation quality to quantify error cascade effects