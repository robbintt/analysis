---
ver: rpa2
title: Suboptimal Shapley Value Explanations
arxiv_id: '2502.12209'
source_url: https://arxiv.org/abs/2502.12209
tags:
- feature
- baselines
- shapley
- sampling
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the suboptimality of random and conditional
  baselines in Shapley value-based feature attribution for deep neural networks. It
  identifies that problematic baselines create asymmetric feature interactions with
  significant directional bias toward model outputs, leading to misleading explanations.
---

# Suboptimal Shapley Value Explanations

## Quick Facts
- arXiv ID: 2502.12209
- Source URL: https://arxiv.org/abs/2502.12209
- Authors: Xiaolei Lu
- Reference count: 35
- Primary result: Uncertainty-based reweighting mechanism improves faithfulness of Shapley explanations by minimizing asymmetric feature interactions

## Executive Summary
This paper identifies fundamental problems with random and conditional baselines in Shapley value-based feature attribution for deep neural networks. The core issue is that problematic baselines create asymmetric feature interactions with significant directional bias toward model outputs, leading to misleading explanations. The paper proposes that p(y|x′_i) = p(y) minimizes these asymmetric interactions and generalizes this idea to maximize label space entropy. A simple uncertainty-based reweighting mechanism using normalized entropy accelerates computation while improving explanation faithfulness across five NLP tasks with BERT and RoBERTa models.

## Method Summary
The method introduces an uncertainty-based reweighting mechanism for Shapley value computation that replaces unweighted averaging with entropy-weighted expectations. For each feature, it samples multiple permutations and baselines, constructs paired instances (with original and replacement features), computes normalized entropy from model predictions on replacement sequences, and applies weighted averaging in the value function computation. The approach uses AdamW optimization with standard hyperparameters and evaluates faithfulness using Log-odds, Sufficiency, and Comprehensiveness metrics across SST-2, SNLI, SNIPS, Yelp-2, and 20Newsgroup datasets.

## Key Results
- Uncertainty reweighting (random-uw and condition-uw) outperforms unweighted baselines on all three faithfulness metrics across all five datasets
- Random-uw achieves best Log-odds (LOR) and Sufficiency (SF) scores while condition-uw excels in Comprehensiveness (CM)
- Human and GPT-4 evaluations reveal substantial gap with baseline explanations (correlations 0.071-0.231) versus human understanding (0.710-0.856)
- The reweighting mechanism shows consistent improvements without requiring expensive greedy search for optimal baselines

## Why This Works (Mechanism)

### Mechanism 1
Problematic baselines create asymmetric feature interactions with directional bias toward model outputs. When replacement features are more influential than originals, they contribute asymmetrically to Shapley calculations. The paper quantifies this via asymmetric interaction φ(T2→T1) using pointwise mutual information differences between conditional and unconditional contexts.

### Mechanism 2
Setting p(y|x′_i) = p(y) minimizes asymmetric interactions by making replacement features statistically independent of labels. This collapses conditional mutual information terms to zero across all subsets. The paper generalizes to maximizing entropy H(Y|x′_i) across label space to avoid explicit probability estimation.

### Mechanism 3
Uncertainty-based reweighting uses normalized entropy Ĥ(x′_i) = H(x′_i)/log₂(L) as proxy for baseline uninformativeness. Higher weights encourage sequences where replacement features are less certain about labels. This weighted expectation substitutes for unweighted averaging in value function computation, improving faithfulness without greedy search.

## Foundational Learning

- **Shapley value computation and value functions**: Understanding marginal contribution averaging over permutations is essential since the paper addresses baseline selection for computing v(S) = E(f_y|x_S ∪ x′_S̄) in Eq. 2
  - Quick check: Given features {A, B, C}, what subsets contribute to φ(A)?

- **Random vs conditional vs interventional baselines**: Section 3.1 defines three baseline types; the paper analyzes suboptimality of random and conditional baselines specifically
  - Quick check: Why does conditional sampling preserve feature dependencies that random sampling ignores?

- **Pointwise mutual information and conditional mutual information**: Eq. 8 defines I(y; x|x′) used throughout asymmetric interaction formalism; understanding information-theoretic measures is required for Eq. 5-7
  - Quick check: What does I(y; x_T1|x_S) - I(y; x_T1|x_S\T2) = 0 imply about the relationship between T2 and T1?

## Architecture Onboarding

- **Component map**: Shapley Sampling module (Algorithms 1-2) -> Baseline generator (random/conditional) -> Uncertainty estimator (entropy computation) -> Reweighted aggregator (weighted expectation) -> Faithfulness evaluator (LOR, SF, CM metrics)

- **Critical path**: 1) Sample permutations and baselines per feature 2) Construct paired instances x_1 (with x_i) and x_2 (with x′_i) 3) Compute normalized entropy from model predictions 4) Accumulate weighted φ(i) += f_y(x_1) - Ĥ(x′_i) × f_y(x_2) 5) Average over samples

- **Design tradeoffs**: Sampling size vs faithfulness (m=1000 used for robustness); random vs conditional baselines (random ignores dependencies but avoids biased interactions; conditional preserves context but may introduce influential replacements); in-distribution vs general sampling (Appendix K shows in-distribution conditional sampling improves over general pretrained models)

- **Failure signatures**: fy(S ∪ i) < fy(S) for highly influential positive features (indicates replacement is more positive than original); low correlation with human/GPT-4 rankings (0.071-0.231 range); negative sufficiency scores (observed in random-uw results, may indicate over-weighting of uncertain baselines)

- **First 3 experiments**: 1) Reproduce motivation example comparing Shapley rankings across random/conditional baselines with and without reweighting 2) Ablation on sampling size with m ∈ {100, 500, 1000} to verify robustness 3) Correlation analysis computing Spearman correlation between method rankings and GPT-4/human rankings on SST-2 subset

## Open Questions the Paper Calls Out

### Open Question 1
Does the uncertainty-based reweighting mechanism generalize effectively to non-NLP domains, such as computer vision or tabular data? The experimental scope is strictly limited to NLP tasks and Transformer architectures. The entropy heuristic relies on label space probabilities which may behave differently with continuous visual features compared to discrete linguistic tokens. Quantitative evaluation on image classification datasets or tabular benchmarks would resolve this.

### Open Question 2
How does the proposed method perform compared to interventional baselines when the underlying causal structure is known? The authors exclude interventional baselines from the main analysis, stating "accessing causal data structure is challenging" and that this specific case was not the focus. Experiments on synthetic datasets with known causal graphs comparing the faithfulness of uncertainty-weighted Shapley values against true interventional Shapley values would resolve this.

### Open Question 3
Can the improvement in faithfulness metrics bridge the substantial gap identified between model explanations and human understanding? While the method improves "faithfulness" (e.g., Log-odds, Comprehensiveness), the paper does not demonstrate that these improved scores correlate with higher human agreement. A user study measuring whether explanations generated via uncertainty-based reweighting achieve higher correlation with human feature rankings than standard baselines would resolve this.

## Limitations

- Independence assumption underlying p(y|x′_i) = p(y) criterion may not hold in all domains, particularly when baseline samples correlate with other task-relevant features
- Entropy-based reweighting assumes model uncertainty calibration, which is known to be problematic in deep networks
- Pairwise asymmetric interaction formalism relies on specific PMI formulations that may not generalize across different feature types or model architectures

## Confidence

**High confidence**: The identification of directional bias in random and conditional baselines, and the experimental demonstration that uncertainty reweighting improves faithfulness metrics across five datasets with BERT and RoBERTa models.

**Medium confidence**: The theoretical derivation connecting p(y|x′_i) = p(y) to minimized asymmetric interactions, as this relies on specific assumptions about feature independence and interaction symmetry that may not universally apply.

**Low confidence**: The claim that maximizing label space entropy is the optimal generalization of the independence criterion, since this could potentially correlate with out-of-distribution artifacts that introduce different forms of bias.

## Next Checks

1. **Ablation study on sampling size and entropy calibration**: Systematically vary m from 100-1000 and compare faithfulness metrics across different entropy computation methods (softmax vs predicted class only) to validate robustness claims and identify optimal calibration approaches.

2. **Cross-domain transferability test**: Apply the uncertainty reweighting mechanism to non-NLP domains (e.g., tabular data or images) to verify whether the baseline suboptimality patterns and entropy-based solutions generalize beyond text data.

3. **Human correlation validation with diverse annotators**: Replicate the GPT-4 and human correlation analysis with a larger pool of human annotators across different expertise levels to better understand the reported gap between model inference and human understanding.