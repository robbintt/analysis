---
ver: rpa2
title: 'CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework for
  Misleading Text Detection on Social Media'
arxiv_id: '2506.05107'
source_url: https://arxiv.org/abs/2506.05107
tags:
- stance
- learning
- text
- contrastive
- implicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CL-ISR, a novel framework that combines contrastive
  learning and implicit stance reasoning to detect misleading texts on social media.
  The framework improves semantic differentiation between truthful and misleading
  content by constructing positive and negative sample pairs, and captures implicit
  stance tendencies and their relationships with topics to identify emotionally manipulative
  or stance-shifting content.
---

# CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework for Misleading Text Detection on Social Media

## Quick Facts
- arXiv ID: 2506.05107
- Source URL: https://arxiv.org/abs/2506.05107
- Reference count: 16
- Key result: CL-ISR achieves F1 scores of 91.7%, 86.3%, and 84.8% on FakeNewsNet, PHEME, and Weibo-Misinfo respectively

## Executive Summary
CL-ISR is a novel framework that combines contrastive learning (CL) and implicit stance reasoning (ISR) to detect misleading texts on social media. The framework improves semantic differentiation between truthful and misleading content by constructing positive and negative sample pairs through data augmentation, and captures implicit stance tendencies and their relationships with topics to identify emotionally manipulative or stance-shifting content. CL-ISR integrates these two components through a multimodal fusion layer with dynamic attention. Experimental results show that CL-ISR outperforms baseline models such as BERT and RoBERTa across three public datasets.

## Method Summary
CL-ISR consists of two parallel modules: a Contrastive Learning module that improves semantic differentiation between truthful and misleading texts by constructing positive and negative sample pairs using data augmentation (random deletion, synonym replacement, insertion), and an Implicit Stance Reasoning module that captures latent attitude signals using a BiLSTM encoder with attention trained on stance labels {-1, 0, +1}. These modules are integrated through a dynamic multimodal fusion layer that adaptively combines semantic and stance features using attention-weighted fusion. The model is trained end-to-end with a multi-task loss combining CL, ISR, and classification objectives, optimized using AdamW with early stopping.

## Key Results
- CL-ISR achieves F1 scores of 91.7% on FakeNewsNet, 86.3% on PHEME, and 84.8% on Weibo-Misinfo
- Ablation studies confirm complementary effectiveness, with full model outperforming individual modules by up to 4.4 percentage points in F1 score
- Cross-domain evaluations demonstrate robust generalization capabilities with ~6% performance drop when transferring from Weibo to FakeNewsNet
- Dynamic multimodal fusion provides ~2.8% improvement over static alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning improves semantic differentiation between truthful and misleading texts by constructing positive/negative sample pairs
- Mechanism: The InfoNCE loss (Equation 2) brings augmented views of the same text closer in embedding space while pushing different-category samples apart. Data augmentation creates training diversity without semantic drift
- Core assumption: Misleading and truthful texts have learnable semantic distinctions that survive augmentation
- Evidence anchors: Abstract states contrastive learning helps capture distinguishing features; Section III.A describes encoder fθ(·), augmentation views, and L_CL minimization
- Break condition: When misleading content mimics truthful style, pair construction may yield false positives/negatives

### Mechanism 2
- Claim: Implicit stance reasoning captures latent attitude signals that surface semantics miss, aiding detection of emotionally manipulative or stance-shifting content
- Mechanism: A BiLSTM encoder with attention (Equations 7–9) produces a stance feature vector s_i. The module is trained on stance labels using cross-entropy loss
- Core assumption: Stance tendencies correlate with misleading intent; annotators can reliably label stance
- Evidence anchors: Abstract states this method is effective for identifying stance-shifting content; Section III.B details stance encoder g_φ(·), attention weighting, and regularization
- Break condition: If stance labels are noisy or inconsistent across annotators

### Mechanism 3
- Claim: Dynamic multimodal fusion adaptively integrates semantic and stance features, yielding better performance than either module alone
- Mechanism: Attention-weighted fusion (Equations 13–14) computes β_i to balance contrastive feature h_i and stance feature s_i. Joint optimization combines L_CL, L_ISR, and L_class with tunable weights
- Core assumption: Semantic and stance signals provide complementary information
- Evidence anchors: Abstract states ablation studies confirm complementary effectiveness; Section III.C & Table 2 show removing dynamic fusion causes -2.8% average decline
- Break condition: If one module consistently produces noisy or uninformative features, dynamic weighting may amplify noise

## Foundational Learning

- Concept: Contrastive learning (InfoNCE loss, positive/negative pair construction, temperature scaling)
  - Why needed here: Core mechanism for semantic differentiation; hyperparameter τ affects distribution smoothness
  - Quick check question: Can you explain how decreasing the temperature τ affects the hardness of negative mining in InfoNCE?

- Concept: Sequence modeling with attention (BiLSTM + additive attention)
  - Why needed here: Backbone for implicit stance reasoning; attention weights identify stance-relevant tokens
  - Quick check question: How does bidirectional LSTM differ from unidirectional in capturing stance cues that appear late in a post?

- Concept: Multi-task learning and loss weighting (α_1, α_2, α_3 tuning)
  - Why needed here: Joint optimization across CL, ISR, and classification; improper weighting can cause one task to dominate
  - Quick check question: What symptom would you observe if α_1 is set too high relative to α_3 during training?

## Architecture Onboarding

- Component map: Input text → BERT tokenizer → parallel CL encoder f_θ(·) and ISR encoder g_φ(·) → dynamic fusion layer f_ω(·) → classification head
- Critical path: Input text → tokenization → parallel CL and ISR encoding → fusion → classification. Training requires paired stance labels for ISR and binary veracity labels for final classification
- Design tradeoffs:
  - Data augmentation strategy: Hybrid (deletion + synonym) yields best F1 (91.7%) vs. random insertion alone (86.7%), but requires careful filtering to avoid semantic drift
  - Fusion complexity: Dynamic attention adds parameters and compute vs. simple concatenation; ablation shows +2.8% gain justifies cost
  - Encoder choice: BiLSTM for ISR is lighter than transformer but may underperform on long-range dependencies
- Failure signatures:
  - High false negatives on short, ambiguous posts: Likely CL module struggling with limited semantic signal
  - Poor cross-lingual transfer (>6% drop): ISR may overfit language-specific stance patterns
  - Overfitting to stance labels: If stance annotation quality is low, L_ISR may not correlate with veracity
- First 3 experiments:
  1. Replicate ablation on FakeNewsNet: Train full CL-ISR, then remove CL, remove ISR, and replace dynamic fusion with static concatenation
  2. Hyperparameter sweep on τ and α weights: Fix other settings, vary τ ∈ {0.05, 0.07, 0.1} and α combinations on validation set
  3. Cross-domain transfer test: Train on FakeNewsNet, evaluate on PHEME without fine-tuning

## Open Questions the Paper Calls Out

- Question: Can the Implicit Stance Reasoning (ISR) module's performance be improved by replacing the BiLSTM architecture with a Transformer-based encoder?
- Question: How does the CL-ISR framework perform when applied to diverse data sources characterized by significantly shorter text lengths?
- Question: Can generative AI-based data augmentation strategies resolve the semantic incoherence introduced by random insertion methods?
- Question: How can the cross-lingual transfer capability be stabilized to minimize the performance gap observed between different language domains?

## Limitations
- Unknown 1: Encoder architecture for CL module—backbone and hidden dimensions not specified
- Unknown 2: Loss weights α₁,α₂,α₃—stated as "determined through tuning" but values not provided
- Unknown 3: Stance label availability—only PHEME mentions stance labels; unclear how ISR is trained on other datasets
- Unknown 4: Data augmentation probabilities and exact hybrid strategy details
- Unknown 5: BiLSTM hidden size, number of layers, and dropout rates for ISR module

## Confidence
- **High confidence**: Core F1 scores on individual datasets and general ablation trends
- **Medium confidence**: Cross-domain transfer performance claims and dynamic fusion superiority
- **Low confidence**: Stance label availability assumptions and specific hyperparameter values for loss weighting

## Next Checks
1. Verify stance label availability across all three datasets; if missing, test pseudo-label generation or PHEME-only ISR training
2. Conduct controlled ablation of fusion strategies: compare dynamic attention vs. static concatenation on FakeNewsNet
3. Perform language-specific analysis: train separate ISR modules for English and Chinese datasets to quantify language-dependent vs. language-agnostic stance signal contributions