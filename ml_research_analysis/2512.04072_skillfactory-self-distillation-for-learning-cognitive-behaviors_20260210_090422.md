---
ver: rpa2
title: 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors'
arxiv_id: '2512.04072'
source_url: https://arxiv.org/abs/2512.04072
tags:
- answer
- correct
- skillfactory
- response
- verdict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkillFactory teaches language models reasoning skills like verification
  and retrying by constructing training data from the model's own outputs, rearranged
  into structured traces showing explicit skill use. This self-distillation approach
  does not require stronger teacher models and instead focuses on learning the structural
  patterns of reasoning behaviors.
---

# SkillFactory: Self-Distillation For Learning Cognitive Behaviors

## Quick Facts
- **arXiv ID**: 2512.04072
- **Source URL**: https://arxiv.org/abs/2512.04072
- **Reference count**: 40
- **Primary result**: SkillFactory improves language models' reasoning skills (verification, retrying) through self-distillation, enabling better generalization to harder tasks without requiring stronger teacher models.

## Executive Summary
SkillFactory introduces a self-distillation approach for teaching language models cognitive behaviors like verification and retrying. The method constructs training data from the model's own outputs by generating multiple solutions per question, reflecting on their correctness, and arranging them into structured traces that explicitly demonstrate skill usage. Across two training regimes (2 epochs SFT followed by GRPO), SkillFactory consistently improves models' ability to generalize to harder task variants and maintain robustness on out-of-domain tasks. The approach shows particular effectiveness when combined with inference-time budget forcing, enabling linear scaling of performance with computational resources.

## Method Summary
SkillFactory generates training data through a three-step pipeline: first, sampling 64 solutions per question using 4 different prompts; second, generating reflections on each solution's correctness using a reflection model; third, constructing traces by sampling n+ correct and n- incorrect solutions, shuffling all but one correct answer, and formatting with structured tags (<sample>, <reflect>, <verdict>). These traces are then used for supervised fine-tuning (2 epochs, LR 1e-6) followed by GRPO training with binary rewards. The method focuses on learning structural patterns of reasoning behaviors rather than relying on stronger teacher models, making it applicable to smaller base models while still enabling effective scaling.

## Key Results
- SkillFactory consistently improves post-RL performance compared to baselines across multiple model sizes and tasks
- Models trained with SkillFactory show higher skill usage, longer and more varied responses
- The method enables effective scaling with inference-time budget forcing, showing linear performance gains with increased computational resources
- SkillFactory models demonstrate better generalization to harder task variants (4-6 argument Countdown) compared to baselines

## Why This Works (Mechanism)
SkillFactory works by explicitly teaching language models the structural patterns of cognitive behaviors through self-generated training data. By constructing traces that show multiple attempts at problem-solving with explicit reflections on correctness, the method provides clear examples of when and how to apply verification and retrying skills. The self-distillation approach avoids the need for stronger teacher models by leveraging the base model's own reasoning capabilities, making it particularly effective for smaller models. The structured trace format with tags and glue phrases creates a clear learning signal about skill application, while the GRPO fine-tuning phase reinforces these behaviors through reward-based learning.

## Foundational Learning
- **Self-distillation concept**: Models learn from their own outputs rather than external teachers; needed to avoid dependency on stronger models
- **Trace construction methodology**: Building structured training examples from multiple solution attempts; needed to explicitly demonstrate skill usage patterns
- **Reflection-based supervision**: Using model-generated verdicts to label training data; needed to create ground truth without manual annotation
- **Budget forcing for inference scaling**: Controlling model behavior through prompt engineering; needed to enable linear scaling with computational resources
- **GRPO fine-tuning**: Reinforcement learning with binary rewards and KL regularization; needed to reinforce learned skills without mode collapse
- **Cognitive skill taxonomy**: Understanding different reasoning behaviors (verification, retrying, planning, decomposition); needed to identify which skills to target

## Architecture Onboarding

**Component Map**
Reflection Model -> Solution Generator -> Trace Constructor -> SFT Trainer -> GRPO Trainer

**Critical Path**
Solution Generation → Reflection Generation → Trace Construction → SFT → GRPO

**Design Tradeoffs**
- Self-distillation vs. teacher models: avoids dependency on stronger models but requires base model to have sufficient reasoning capability
- Structured traces vs. natural language: provides clear learning signals but may be less generalizable to unstructured contexts
- Budget forcing vs. adaptive reasoning: enables scaling but requires explicit prompt engineering

**Failure Signatures**
- Low valid reflection rate indicates poor base model reasoning capability
- SFT overfitting to task structure rather than skills suggests insufficient trace diversity
- Inconsistent skill usage post-RL indicates trace construction issues

**3 First Experiments**
1. Verify reflection accuracy across all tasks by computing F1 scores against ground truth
2. Test trace construction with fixed n+/n- ratios versus uniform sampling to isolate structural effects
3. Implement and validate the exact budget-forcing trigger phrase mechanism across different model sizes

## Open Questions the Paper Calls Out
- Can SkillFactory successfully instill cognitive skills beyond verification and retrying, such as planning, decomposition, or hypothesis generation? The authors state they "focus on the following two" skills, implying other skills could potentially be learned via self-distillation.
- Why does SkillFactory performance degrade when scaling from 1k to 10k SFT examples? The authors hypothesize additional SFT doesn't help because core skills are already learned early, but provide no empirical investigation.
- How does the effectiveness of SkillFactory self-distillation scale with base model size and capability? Experiments only use 1.5B and 7B models, with the paper noting model limitations on certain tasks.
- Can SkillFactory-style skill priming help address the "overthinking" and "underthinking" problems identified in reasoning models? This is proposed as future direction but no experiments test whether structured skill traces calibrate when to retry vs. terminate.

## Limitations
- Trace construction protocol contains critical ambiguities in n+/n- sampling distribution that could significantly affect learning signals
- Budget-forcing mechanism relies on vaguely described model-specific trigger phrases, making implementation difficult
- Method depends heavily on reflection model's ability to correctly identify solution correctness, with substantial variability across tasks
- Claims about robustness on out-of-domain tasks are based on limited data points without variance estimates

## Confidence
**High Confidence**: Claims about SkillFactory improving post-RL performance compared to baselines are well-supported by experimental results across multiple model sizes and tasks.

**Medium Confidence**: Claims about improved generalization to harder variants are supported but could benefit from additional ablations to confirm skill learning versus increased solution diversity.

**Low Confidence**: Claims about robustness on out-of-domain tasks are based on limited data points and single runs without statistical significance measures.

## Next Checks
1. **Ablation study on trace structure**: Test SkillFactory with fixed n+/n- ratios versus uniform sampling to determine whether random trace composition is essential for skill generalization.

2. **Reflection quality analysis**: Implement automated verification of reflection accuracy by comparing verdicts to ground truth across all tasks, measuring F1 scores and investigating correlation with downstream performance.

3. **Inference-time scaling validation**: Implement and test the exact budget-forcing trigger phrase mechanism described as "<sample> tag before closing </think >" across multiple model sizes to verify claimed linear scaling with inference budget.