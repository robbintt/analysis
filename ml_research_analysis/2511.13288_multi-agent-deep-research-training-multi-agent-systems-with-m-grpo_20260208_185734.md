---
ver: rpa2
title: 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO'
arxiv_id: '2511.13288'
source_url: https://arxiv.org/abs/2511.13288
tags:
- training
- multi-agent
- agent
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M-GRPO, a hierarchical reinforcement learning
  framework for training vertical multi-agent systems with distinct LLMs for the main
  planner and specialized sub-agents. The method extends Group Relative Policy Optimization
  to handle variable sub-agent invocation counts through trajectory alignment and
  decoupled training across separate servers.
---

# Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO

## Quick Facts
- arXiv ID: 2511.13288
- Source URL: https://arxiv.org/abs/2511.13288
- Reference count: 34
- Primary result: M-GRPO achieves state-of-the-art performance on GAIA, XBench-DeepSearch, and WebWalkerQA by training specialized sub-agents with decoupled hierarchical reinforcement learning.

## Executive Summary
M-GRPO introduces a hierarchical reinforcement learning framework for training vertical multi-agent systems where a main planner agent delegates tasks to specialized sub-agents. The method extends Group Relative Policy Optimization to handle variable sub-agent invocation counts through trajectory alignment and decoupled training across separate servers. On benchmarks including GAIA, XBench-DeepSearch, and WebWalkerQA, M-GRPO consistently outperforms single-agent GRPO and multi-agent systems with frozen sub-agents, demonstrating improved stability and sample efficiency in tool-augmented reasoning tasks.

## Method Summary
M-GRPO trains a two-agent system (main planner + sub-agent) using decoupled hierarchical reinforcement learning. The main agent handles reasoning and planning while the sub-agent executes search and browsing tools. Training uses trajectory alignment to handle variable sub-agent invocations by duplicating or dropping trajectories to reach a target count (d=8). Each agent computes group-relative advantages separately on their respective servers, with rewards propagating hierarchically (sub-agent receives β₂·r_main_correct term). The framework employs a 2-stage curriculum: stage 1 focuses on format learning with simple graphs, stage 2 trains collaboration on complex graphs and hard queries. Both agents are initialized from Qwen3-30B-A3B and trained on synthetic data.

## Key Results
- M-GRPO outperforms single-agent GRPO and frozen sub-agent baselines on GAIA, XBench-DeepSearch, and WebWalkerQA benchmarks
- Synchronized trajectory alignment implementation shows significant performance gains over unsynchronized version (Fig. 8)
- Stage 2 training with complex graphs and hard queries demonstrates successful collaboration learning
- Decoupled server training enables scalable deployment across 2×32 H800 GPUs without cross-server gradient communication

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical group-relative advantages enable stable credit assignment across agents with different operational frequencies. M-GRPO computes separate advantage estimations for main agent M and sub-agent S using group-relative baselines within each agent's trajectory set, isolating policy updates while preserving hierarchical dependency through reward propagation.

### Mechanism 2
Trajectory alignment via duplication/dropping enables fixed-shape batch training despite variable sub-agent invocations. For target d≈P(dk≤d)=0.99, trajectories with dk<d duplicate random existing trajectories; dk>d drops excess, yielding uniform d×K sub-agent batch size that stabilizes group baseline computation.

### Mechanism 3
Decoupled server deployment with shared-store statistics enables scalable training without cross-server gradients. Server M writes rewards to database; Server S reads required rewards, computes RS locally, updates πθS independently. No backpropagation crosses server boundary—only scalar statistics exchanged.

## Foundational Learning

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed: M-GRPO extends GRPO; requires understanding how group-relative advantages replace value functions for variance reduction.
  - Quick check: Given K rollouts with rewards [0.2, 0.6, 0.8, 0.4], compute the group-relative advantage for the second rollout.

- **Concept**: Hierarchical Reinforcement Learning
  - Why needed: Vertical M→S architecture creates temporal abstraction; main agent operates at coarser timestep than sub-agent.
  - Quick check: In a 2-level hierarchy where one high-level action triggers 5 low-level steps, how does credit assignment differ from flat RL?

- **Concept**: Multi-Agent Credit Assignment
  - Why needed: Final outcome depends on both M's planning and S's execution; reward decomposition determines learning signals.
  - Quick check: If sub-agent executes perfectly but main agent's plan is infeasible, how should reward be split?

## Architecture Onboarding

- **Component map**: Query → Agent Controller → Main Agent Server → Tool Server → Shared Database → Sub-Agent Server → Tool Server → Final Output
- **Critical path**: Query q → Main Agent M generates τM → M invokes sub-agent d times → each generates τSi → Tool Server executes actions → Final output oM → compute RM(oM) → Write RM to shared DB → Sub-agent server reads RM, computes RS(oSi) with β weights → Apply trajectory alignment → Compute group-relative advantages → Update πθM and πθS separately via clipped objectives
- **Design tradeoffs**: β₂ weight (0.4) vs β₃ weight (0.5) balance gradient noise vs. decoupling; d=8 target balances signal preservation vs. compute; decoupled vs joint training trades gradient information for scalability
- **Failure signatures**: Unstable main-agent rewards indicate format weight α₁ too low; sub-agent not learning task quality suggests β₃ too low relative to β₂; high variance in S advantages indicates trajectory alignment issues; cascading coordination failures point to stage 2 curriculum difficulty
- **First 3 experiments**: 1) Reproduce stage 1 format learning on simple QA dataset with K=8 rollouts; 2) Ablate trajectory synchronization comparing synchronized vs unsynchronized implementations; 3) Vary β₂/β₃ ratio testing β₂∈{0.2,0.4,0.6} with β₃=0.5−β₂ on stage 2 complex tasks

## Open Questions the Paper Calls Out

### Open Question 1
Does M-GRPO generalize to vertical multi-agent systems with more than one sub-agent (1+n architecture), and how does training stability scale with the number of specialized sub-agents? All experiments use a single sub-agent; no empirical validation of claimed generalization to multiple sub-agents exists.

### Open Question 2
How sensitive is M-GRPO performance to the reward coefficient choices (α₁, α₂, β₁, β₂, β₃), and can these be learned rather than manually tuned? No ablation study examines performance variation across coefficient configurations or transferability across domains.

### Open Question 3
Does trajectory alignment via random duplication and dropping introduce bias or information loss that affects final policy quality compared to alternative alignment schemes? The paper validates synchronization vs. no synchronization but not whether the specific alignment mechanism is optimal.

## Limitations

- Trajectory alignment mechanism's bias from random duplication/dropping remains poorly quantified
- Decoupled training design eliminates joint gradient computation, potentially losing critical cross-agent credit assignment signals
- LLM-based expert reward (r_expert) implementation details are underspecified

## Confidence

- **High confidence**: Hierarchical GRPO framework design, decoupled server architecture, and 2-stage curriculum structure
- **Medium confidence**: Trajectory alignment effectiveness and specific hyperparameter choices (β weights, d=8 target)
- **Low confidence**: Exact reward computation details for r_expert and optimizer configuration

## Next Checks

1. Replicate the trajectory synchronization ablation with proper significance testing across multiple random seeds to verify Fig. 8 differences are not due to sampling variance

2. Measure sub-agent call frequency distributions across benchmarks and test trajectory alignment performance when truncating at different percentiles (95th, 99th, 99.9th) to quantify bias-variance tradeoff

3. Systematically vary β₂/β₃ ratios in stage 2 training and measure both downstream benchmark performance and intermediate sub-agent tool-use quality metrics to identify optimal reward shaping