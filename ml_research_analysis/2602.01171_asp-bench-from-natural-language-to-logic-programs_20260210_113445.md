---
ver: rpa2
title: 'ASP-Bench: From Natural Language to Logic Programs'
arxiv_id: '2602.01171'
source_url: https://arxiv.org/abs/2602.01171
tags:
- problem
- problems
- agent
- logic
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ASP-Bench is a benchmark for evaluating natural language to Answer
  Set Programming translation, comprising 128 problem instances with semantic verification.
  Using an agentic ReAct framework approach, the system achieved full saturation on
  both easy and hard problems through iterative refinement with solver feedback.
---

# ASP-Bench: From Natural Language to Logic Programs

## Quick Facts
- arXiv ID: 2602.01171
- Source URL: https://arxiv.org/abs/2602.01171
- Authors: Stefan Szeider
- Reference count: 30
- Key outcome: Agentic ReAct framework achieved 100% success on 128 ASP benchmark problems through iterative refinement

## Executive Summary
ASP-Bench is a benchmark designed to evaluate the translation of natural language problems into Answer Set Programming (ASP) formulations. The benchmark comprises 128 problem instances that have been semantically verified for correctness. Using an agentic ReAct framework approach, the system successfully solved all problems through iterative refinement with solver feedback. The benchmark enables systematic evaluation of neurosymbolic systems and provides insights into modeling difficulty across seven reasoning aspects.

## Method Summary
The study employed an agentic ReAct framework that iteratively refines ASP translations based on solver feedback. The system achieved full saturation on both easy and hard problems within the benchmark. Performance was measured through Python execution calls, with hard problems averaging 7.7 calls and the most complex requiring 26 calls. The benchmark construction involved manual translation by authors with semantic verification, and evaluation focused on both success rates and the efficiency of the translation process.

## Key Results
- Agentic ReAct framework achieved 100% success rate across all 128 benchmark problems
- Hard problems required an average of 7.7 Python execution calls, with maximum of 26 calls
- Benchmark enables systematic evaluation of neurosymbolic systems and identifies seven reasoning aspects affecting modeling difficulty

## Why This Works (Mechanism)
The agentic ReAct framework succeeds by iteratively refining ASP translations through continuous interaction with the solver. Each iteration generates feedback that guides the next refinement cycle, allowing the system to progressively approach correct solutions. The semantic verification of benchmark problems ensures that success metrics reflect genuine understanding rather than coincidental correct answers.

## Foundational Learning
- **Answer Set Programming (ASP)**: Logic programming paradigm for knowledge representation and reasoning - needed for understanding the target formalism; quick check: can you explain stable model semantics?
- **ReAct Framework**: Combines reasoning and acting cycles for problem solving - needed for understanding the agentic approach; quick check: can you describe the feedback loop mechanism?
- **Neurosymbolic Integration**: Combining neural and symbolic approaches - needed for understanding the broader context; quick check: can you identify advantages over pure neural approaches?
- **Semantic Verification**: Process of validating logical correctness - needed for understanding benchmark quality; quick check: can you explain how semantic verification differs from syntactic checking?
- **Iterative Refinement**: Progressive improvement through cycles - needed for understanding the solution methodology; quick check: can you trace a single refinement cycle?

## Architecture Onboarding
**Component Map**: Natural Language -> ReAct Agent -> ASP Translator -> ASP Solver -> Feedback -> ReAct Agent (loop)
**Critical Path**: NL input → Translation → Solving → Feedback → Refinement → Solution
**Design Tradeoffs**: Manual benchmark creation ensures quality but limits scale; iterative refinement ensures correctness but may be computationally expensive
**Failure Signatures**: Incorrect translations persist across iterations; solver feedback becomes uninformative; convergence stalls
**First Experiments**: 1) Run single problem through complete ReAct cycle; 2) Test semantic verification on manually created problems; 3) Measure performance on easy vs hard problems

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on manual translation by authors, creating potential subjectivity
- Small sample size of 128 problems may not capture full complexity space
- Success may be attributed to specific problem characteristics rather than universal applicability

## Confidence
**High confidence**: Agentic approach success on benchmark problems (100% success rate)
**Medium confidence**: Claims about modeling difficulty across seven reasoning aspects
**Medium confidence**: Neurosymbolic system evaluation capabilities
**Low confidence**: Scalability claims beyond observed 26 Python execution calls

## Next Checks
1. Conduct blind validation with independent annotators to verify semantic correctness and eliminate author bias
2. Test ReAct framework on expanded benchmark with 500+ problems spanning diverse domains
3. Compare performance against alternative neurosymbolic approaches on identical problem sets