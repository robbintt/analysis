---
ver: rpa2
title: 'FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal
  LLMs'
arxiv_id: '2509.16648'
source_url: https://arxiv.org/abs/2509.16648
tags:
- uncertainty
- samples
- festa
- arxiv
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of trust assessment in multimodal
  large language models (MLLMs) by proposing a novel uncertainty quantification method
  called FESTA. The core idea is to use functionally equivalent (FES) and complementary
  (FCS) sampling techniques to probe the consistency and sensitivity of MLLM predictions.
---

# FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs

## Quick Facts
- arXiv ID: 2509.16648
- Source URL: https://arxiv.org/abs/2509.16648
- Authors: Debarpan Bhattacharya; Apoorva Kulkarni; Sriram Ganapathy
- Reference count: 36
- Key outcome: 33.3% relative improvement for vision-LLMs and 29.6% for audio-LLMs in detecting mispredictions using AUROC metric

## Executive Summary
This paper addresses trust assessment in multimodal large language models (MLLMs) through a novel uncertainty quantification method called FESTA. The approach uses functionally equivalent sampling (FES) to probe model consistency and complementary sampling (FCS) to test sensitivity. By computing uncertainty as KL-divergence from ideally consistent and sensitive models, FESTA achieves significant improvements in selective prediction performance, enabling models to abstain from uncertain predictions and thereby improving overall reliability.

## Method Summary
FESTA combines functionally equivalent sampling (FES) and complementary sampling (FCS) to quantify uncertainty in MLLMs for selective prediction. FES generates transformed inputs that should preserve the original prediction, measuring consistency through deviation from an ideally consistent model. FCS generates counterfactual inputs expected to flip the prediction, measuring sensitivity through deviation from an ideally sensitive model. The final uncertainty score U_FESTA is the sum of U_FES and U_FCS, both computed as KL-divergence from their respective ideal models. The method operates in a black-box, unsupervised setting using K samples per input for robust uncertainty estimation.

## Key Results
- 33.3% relative improvement for vision-LLMs and 29.6% for audio-LLMs in detecting mispredictions (AUROC metric)
- FESTA outperforms standard entropy-based uncertainty quantification by 26.5-63.0% on vision tasks
- Sample efficiency analysis shows K=16-20 samples achieve near-peak performance, reducing computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FES sampling detects inconsistency-based mispredictions by measuring deviation from an ideally consistent model.
- **Mechanism**: Given input X, FES generates K₁ transformed samples (e.g., grayscale, noise, paraphrasing) that preserve task semantics. The model's predictive distribution q_FES(y|X) is computed over these samples. Uncertainty U_FES = -log q_FES(ŷ|X) quantifies how often predictions match the original ŷ. High U_FES indicates model inconsistency—output varies despite functionally identical inputs.
- **Core assumption**: The transformation space (noise, blur, text rephrasing) preserves task-relevant semantics while perturbing model-internal representations.
- **Evidence anchors**:
  - [abstract] "expands the input space to probe the consistency (through equivalent samples)"
  - [Section 3.1] Defines FES transformation Ẋ = E(X) where T(Ẋ) = T(X) and M_ideal(Ẋ) = M_ideal(X)
  - [corpus] Weak/no direct corpus support for this specific mechanism
- **Break condition**: If transformations accidentally alter task semantics (e.g., aggressive cropping removes key objects), U_FES will conflate semantic drift with model inconsistency.

### Mechanism 2
- **Claim**: FCS sampling detects low-uncertainty hallucinations by testing whether the model responds to counterfactual input changes.
- **Mechanism**: FCS generates K₂ samples that should flip the expected answer (e.g., "in front of" → "behind"). U_FCS = -log Σ_{y≠ŷ} q_FCS(y|X) measures sensitivity—a robust model should change predictions under FCS. Models exhibiting "mode collapse" (insensitivity to input) fail FCS checks, yielding low U_FCS values for incorrect predictions.
- **Core assumption**: Complementary transformations correctly reverse the task-expected answer.
- **Evidence anchors**:
  - [abstract] "sensitivity (through complementary samples)"
  - [Section 3.5] "UFCS helps to abstain from such low-uncertainty mis-predictions as such models show no sensitivity to the complementary samples"
  - [corpus] Related work on spatial reasoning failures (arXiv:2601.11644) shows VLMs achieve only 49-54% on directional relationships, supporting need for sensitivity testing
- **Break condition**: If negation-based text transforms trigger model-specific linguistic biases rather than genuine reasoning changes, U_FCS may measure artifact sensitivity rather than task sensitivity.

### Mechanism 3
- **Claim**: KL-divergence from hypothetical ideal models outperforms standard entropy for uncertainty quantification.
- **Mechanism**: U_FESTA = U_FES + U_FCS combines consistency and sensitivity. Rather than computing entropy over raw predictions, FESTA measures divergence from M_consistent (Kronecker delta at ŷ for FES) and M_sensitive (delta at complement for FCS). This pseudo-supervised framing provides more discriminative uncertainty scores.
- **Core assumption**: Ideal model behavior is well-specified for both FES (consistency) and FCS (sensitivity) regimes.
- **Evidence anchors**:
  - [Section 3.4] "We pose the uncertainty of model M as the deviation of its predictive distribution from that of M_cons"
  - [Section 6.2, Tables 10-11] KL-divergence outperforms entropy: 26.5-63.0% relative improvement on vision tasks
  - [corpus] No corpus papers directly validate KL vs. entropy for this application
- **Break condition**: If task structure makes ideal behavior ambiguous (e.g., multi-label outputs), delta-function specification fails.

## Foundational Learning

- **Concept**: Selective Prediction / Abstention
  - **Why needed here**: FESTA's goal is not just uncertainty estimation but enabling models to refuse answering when uncertain. Understanding the abstention-accuracy tradeoff is prerequisite.
  - **Quick check question**: Given a model with 70% accuracy and AUROC=0.85 for detecting errors, what fraction of predictions should be abstained to achieve 90% accuracy on retained predictions?

- **Concept**: KL-Divergence as Distribution Distance
  - **Why needed here**: Core mathematical machinery—FESTA defines uncertainty as D_KL from ideal distributions. Without this, the pseudo-supervised formulation is opaque.
  - **Quick check question**: Why does D_KL(δ_ŷ || q) simplify to -log q(ŷ)? What happens when q(ŷ) → 0?

- **Concept**: Multimodal Perturbation Invariances
  - **Why needed here**: Selecting FES/FCS transforms requires domain knowledge. Grayscale preserves spatial relations; random erasing may not. Audio time-stretching preserves order but alters duration-based queries.
  - **Quick check question**: For an audio "event ordering" task, is adding silence between events an FES or FCS transform? What about reversing the audio?

## Architecture Onboarding

- **Component map**:
  - Input X = [X_O (image/audio), X_T (text prompt)]
  - FES Generator → {Ẋ₁...Ẋ_{K₁}} via transforms (Section 5.5, Appendix A.5)
  - FCS Generator → {X'₁...X'_{K₂}} via counterfactual transforms (Section 5.6)
  - MLLM (black-box) → predictions for all samples
  - Distribution Aggregator → q_FES, q_FCS
  - Uncertainty Computer → U_FES = -log q_FES(ŷ), U_FCS = -log Σ_{y≠ŷ} q_FCS(y)
  - Final Score → U_FESTA = U_FES + U_FCS

- **Critical path**:
  1. Generate K transforms (K₁ FES + K₂ FCS) per input
  2. Query MLLM K times (computational bottleneck)
  3. Aggregate predictions into distributions
  4. Compute KL-divergence from ideal

- **Design tradeoffs**:
  - **Accuracy vs. Cost**: Paper uses K=56-120; experiments show K=16-20 sufficient for robust estimates (Tables 5-9)
  - **FES vs. FCS weighting**: Paper uses equal weighting; ablations show relative contributions vary by task/model (Figures 3-4)
  - **Transform selection**: Generic (noise, blur) vs. task-specific (duration trimming for temporal tasks)—latter requires domain expertise

- **Failure signatures**:
  - **High U_FES, low U_FCS**: Model inconsistent but sensitive (may indicate overfitting)
  - **Low U_FES, high U_FCS**: Model consistent but insensitive (hallucination risk)
  - **Both high**: Model fundamentally unstable
  - **Both low, wrong prediction**: Mode collapse—model confidently wrong (detected by FCS)

- **First 3 experiments**:
  1. **Reproduction on single dataset**: Implement FES-only version on BLINK spatial reasoning (simpler than full FESTA). Use grayscale + text rephrase (K₁=8). Compute U_FES and measure AUROC. Compare to output entropy baseline.
  2. **Ablation FES vs. FCS**: On TREA audio-ordering task, run FES-only, FCS-only, and combined. Compare contributions per model (Qwen2-Audio vs. SALMONN) to validate complementary behavior shown in Figure 3.
  3. **Sample efficiency sweep**: Vary K from 4 to 56 on VSR dataset. Plot AUROC vs. inference cost. Identify practical operating point (target: within 2% of peak AUROC with minimum K).

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on task-specific transformations for FCS sampling limits generalizability to arbitrary tasks
- Computational cost of 56-120× inference overhead, though K=16-20 may suffice
- Evaluation limited to controlled multiple-choice settings with clear complementary answers

## Confidence
- **High confidence**: Core claim that FESTA improves selective prediction (AUROC) over entropy baselines is well-supported by experimental results
- **Medium confidence**: KL-divergence formulation's superiority over entropy is demonstrated but not deeply analyzed
- **Low confidence**: Generalizability of FCS sampling across diverse tasks and performance on real-world applications remain uncertain

## Next Checks
1. **FCS Transform Generality Test**: Implement FESTA on a dataset with more complex answer structures (e.g., numerical reasoning with multiple valid answers). Measure whether FCS sampling can still generate meaningful complementary samples and whether the uncertainty estimates remain discriminative.

2. **Zero-Shot Transfer Evaluation**: Apply the FES transformations (grayscale, noise, blur) and FCS negation patterns from the spatial reasoning dataset directly to a different domain (e.g., medical image analysis) without retraining or adaptation. Measure performance drop to quantify domain dependence.

3. **Cost-Performance Tradeoff Analysis**: Systematically vary K from 4 to 120 on the VSR dataset, measuring AUROC, inference time, and energy consumption. Identify the Pareto-optimal operating point where additional samples yield diminishing returns in uncertainty quality.