---
ver: rpa2
title: 'OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large
  Language Models'
arxiv_id: '2505.16036'
source_url: https://arxiv.org/abs/2505.16036
tags:
- ethical
- language
- safety
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive ethical evaluation of 29 open-source
  large language models across four dimensions: robustness, reliability, safety, and
  fairness. The study employs a multilingual framework using English and Turkish prompts,
  totaling 1,790 prompts, and leverages an LLM-as-a-Judge methodology for large-scale
  assessment.'
---

# OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models

## Quick Facts
- arXiv ID: 2505.16036
- Source URL: https://arxiv.org/abs/2505.16036
- Authors: Burak Erinç Çetin; Yıldırım Özen; Elif Naz Demiryılmaz; Kaan Engür; Cagri Toraman
- Reference count: 40
- Evaluated 29 open-source LLMs across robustness, reliability, safety, and fairness dimensions

## Executive Summary
This study presents a comprehensive ethical evaluation framework for 29 open-source large language models, using both English and Turkish prompts across four key ethical dimensions. The research employs an LLM-as-a-Judge methodology with GPT-4 as the evaluator, analyzing 1,790 prompts to assess model behavior systematically. Results reveal that while models demonstrate strong performance in safety, fairness, and basic robustness, reliability remains a critical weakness, with models frequently generating hallucinated or fabricated content.

The study finds that larger models generally exhibit better ethical performance, with Gemma and Qwen families showing the most ethical behavior across metrics. Cross-linguistic evaluation reveals consistency in ethical assessment, though English prompts show slightly higher performance due to training data dominance. The research establishes a replicable framework for ongoing ethical monitoring of LLMs as they evolve, addressing the gap in systematic evaluation of open-source models.

## Method Summary
The research employs a multilingual framework evaluating 29 open-source LLMs across four ethical dimensions using 1,790 prompts (1,470 English, 320 Turkish). The LLM-as-a-Judge methodology uses GPT-4 to assess model responses across robustness, reliability, safety, and fairness categories. Robustness is tested through jailbreak attempts and adversarial prompts, while reliability focuses on hallucination and factual fabrication detection. Safety evaluation covers hate speech, self-harm, and dangerous content, while fairness assessment examines bias across demographic dimensions. The framework provides systematic, reproducible evaluation with confidence intervals for statistical validity.

## Key Results
- Safety performance was highest across all models, with effective content filtering and harm prevention
- Reliability emerged as the weakest dimension, with models struggling significantly with hallucination and factual fabrication
- Larger models demonstrated better ethical performance overall, with Gemma and Qwen families showing the most consistent ethical behavior
- Cross-linguistic consistency was observed, though English prompts showed slightly higher performance due to training data dominance

## Why This Works (Mechanism)
The LLM-as-a-Judge methodology enables scalable, systematic ethical evaluation that would be impractical with human evaluators alone. By using GPT-4 as an evaluator, the study achieves consistent assessment criteria across thousands of responses while maintaining detailed analysis of model behavior. The multilingual approach reveals how training data distribution affects performance, while the comprehensive prompt set captures diverse ethical scenarios. The four-dimensional framework provides granular insights into specific ethical weaknesses rather than broad generalizations.

## Foundational Learning
**LLM-as-a-Judge methodology** - Using one LLM to evaluate another's outputs; needed for scalable assessment across thousands of responses; quick check: verify judge model consistency across different prompt types
**Ethical dimension decomposition** - Breaking ethics into robustness, reliability, safety, and fairness; needed to identify specific model weaknesses; quick check: ensure dimensions are mutually exclusive and collectively exhaustive
**Cross-linguistic evaluation** - Testing models in multiple languages; needed to reveal training data biases; quick check: verify prompt equivalence across languages
**Statistical significance in LLM evaluation** - Using confidence intervals for metric interpretation; needed for reproducible, scientific assessment; quick check: calculate appropriate sample sizes for reliable metrics

## Architecture Onboarding

**Component Map:** Prompt Generator -> LLM Models -> GPT-4 Judge -> Evaluation Metrics -> Statistical Analysis

**Critical Path:** Prompt generation → Model inference → Judge evaluation → Metric calculation → Cross-linguistic comparison

**Design Tradeoffs:** LLM-as-a-Judge enables scalability but introduces judge model bias; multilingual evaluation increases coverage but requires careful prompt translation; comprehensive metrics provide detail but increase evaluation complexity

**Failure Signatures:** High safety scores with low reliability indicate content filtering without factual accuracy; cross-linguistic performance gaps reveal training data biases; jailbreak success indicates insufficient robustness

**First Experiments:**
1. Run a small subset of prompts through human evaluators to validate GPT-4 judge consistency
2. Test models with adversarial prompts beyond simple jailbreaks to stress robustness assessment
3. Compare model performance on equivalent prompts across English and Turkish to quantify language-specific biases

## Open Questions the Paper Calls Out
None

## Limitations
- LLM-as-a-Judge methodology introduces potential biases from the judging model (GPT-4)
- Turkish prompt set (320) is significantly smaller than English (1,470), potentially affecting cross-linguistic fairness comparison reliability
- Evaluation framework may not capture all forms of unreliable outputs beyond hallucination and factual fabrication
- Correlation between model size and ethical performance does not establish causation

## Confidence
- High confidence in safety evaluation results and cross-linguistic consistency findings
- Medium confidence in fairness assessment and robustness metrics
- Medium-low confidence in reliability evaluation comprehensiveness

## Next Checks
1. Conduct targeted validation using human evaluators to verify LLM-as-a-Judge assessments, particularly for reliability and fairness dimensions
2. Expand Turkish prompt diversity and quantity to strengthen cross-linguistic comparison validity
3. Test models against more sophisticated jailbreak and adversarial attack patterns to validate robustness claims beyond simple attacks