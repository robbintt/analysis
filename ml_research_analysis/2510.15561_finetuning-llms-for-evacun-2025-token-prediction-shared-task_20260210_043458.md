---
ver: rpa2
title: Finetuning LLMs for EvaCun 2025 token prediction shared task
arxiv_id: '2510.15561'
source_url: https://arxiv.org/abs/2510.15561
tags:
- masked
- task
- word
- words
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a token prediction system for EvaCun 2025,
  targeting missing word restoration in Akkadian and Sumerian cuneiform texts. The
  authors fine-tuned three autoregressive LLMs (Command-R, Mistral, and Aya Expanse)
  on transliterated task data using three different prompting strategies: generating
  all missing words at once, predicting one word at a time, and restoring the full
  document.'
---

# Finetuning LLMs for EvaCun 2025 token prediction shared task

## Quick Facts
- **arXiv ID**: 2510.15561
- **Source URL**: https://arxiv.org/abs/2510.15561
- **Reference count**: 2
- **Primary result**: 26.9% accuracy achieved via majority voting across 60 checkpoints

## Executive Summary
This paper presents a token prediction system for the EvaCun 2025 shared task, focusing on restoring missing words in Akkadian and Sumerian cuneiform texts. The authors fine-tune three autoregressive LLMs (Command-R, Mistral, and Aya Expanse) using transliterated cuneiform data with three different prompting strategies. The best-performing approach achieves 22.1% accuracy on held-out validation data, with majority voting across 60 checkpoints improving performance to 26.9%. The work establishes a baseline for future task-specific approaches in this domain.

## Method Summary
The authors fine-tune three autoregressive LLMs on transliterated cuneiform task data using three prompting strategies: generating all missing words simultaneously, predicting one word at a time, and restoring entire documents. They train 60 checkpoints for each model-prompting combination and employ majority voting to select final predictions. The approach targets missing word restoration in Akkadian and Sumerian cuneiform texts, representing an early baseline for this specialized NLP task.

## Key Results
- Best single-model performance: 22.1% accuracy on held-out validation data
- Majority voting across 60 checkpoints: 26.9% accuracy (substantial improvement)
- Three prompting strategies tested: all-at-once, one-at-a-time, and full-document restoration
- Models fine-tuned: Command-R, Mistral, and Aya Expanse

## Why This Works (Mechanism)
The ensemble approach leverages multiple checkpoints to reduce model variance and capture diverse prediction patterns. Fine-tuning on task-specific transliterated cuneiform data allows models to learn domain-specific patterns and vocabulary. The majority voting mechanism helps mitigate individual model errors by selecting the most frequently predicted word across checkpoints.

## Foundational Learning
- **Token prediction**: Predicting missing words in sequences is fundamental for text restoration tasks. Needed for handling incomplete historical texts. Quick check: Can the model predict plausible words given partial context?
- **Fine-tuning**: Adapting pre-trained models to domain-specific tasks improves performance on specialized data. Needed for cuneiform text patterns. Quick check: Does model performance improve after fine-tuning on task data?
- **Ensemble methods**: Combining multiple model outputs reduces variance and improves robustness. Needed for handling model uncertainty. Quick check: Does majority voting improve accuracy over single models?
- **Prompt engineering**: Different prompting strategies can significantly impact model performance. Needed for optimizing generation strategies. Quick check: Which prompting strategy yields highest accuracy?
- **Transliteration handling**: Working with transliterated cuneiform requires understanding of both linguistic and script conversion patterns. Needed for accurate word prediction. Quick check: Can models handle transliterated vs. original script inputs?
- **Akkadian/Sumerian linguistics**: Understanding these ancient languages' structure improves prediction accuracy. Needed for semantic coherence. Quick check: Are predictions linguistically plausible in target languages?

## Architecture Onboarding
- **Component map**: Transliterated data -> Fine-tuning (3 models × 3 prompts) -> 60 checkpoints each -> Majority voting -> Final predictions
- **Critical path**: Data preparation → Fine-tuning → Checkpoint generation → Ensemble voting → Evaluation
- **Design tradeoffs**: Simpler single-model approaches vs. computationally expensive ensemble methods; transliterated data vs. original script handling
- **Failure signatures**: Low accuracy on complex syntactic constructions; inconsistent predictions across checkpoints; poor handling of rare words
- **First experiments**: 1) Compare single-model vs. ensemble performance on validation set; 2) Test different checkpoint selection strategies; 3) Evaluate prompt engineering variations

## Open Questions the Paper Calls Out
None

## Limitations
- Statistical significance testing not performed on accuracy improvements
- Results limited to transliterated cuneiform, not original script
- Modest performance differences between prompting strategies suggest underexplored alternatives
- Lack of error analysis prevents identification of systematic failure modes

## Confidence
- Accuracy improvement from majority voting: Medium confidence (lacks statistical validation)
- Individual model performance claims: High confidence (straightforward evaluation)
- Generalization to non-transliterated cuneiform: Low confidence (domain-specific training)

## Next Checks
1. Conduct paired statistical significance tests between single-model and ensemble performance across multiple random seeds
2. Evaluate model performance on held-out original cuneiform script data rather than transliterations
3. Perform ablation studies on prompt engineering variations and fine-tuning duration to identify optimal training configurations