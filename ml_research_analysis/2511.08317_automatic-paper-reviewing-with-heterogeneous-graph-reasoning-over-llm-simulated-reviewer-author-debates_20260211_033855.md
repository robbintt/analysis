---
ver: rpa2
title: Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated
  Reviewer-Author Debates
arxiv_id: '2511.08317'
source_url: https://arxiv.org/abs/2511.08317
tags:
- reviewer
- review
- author
- graph
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ReViewGraph, a novel framework for automatic
  paper reviewing that leverages heterogeneous graph reasoning over LLM-simulated
  reviewer-author debates. The method simulates multi-round reviewer-author interactions
  through a multi-agent collaboration framework, extracts argumentative relationships
  as typed edges, and constructs a heterogeneous debate graph.
---

# Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates

## Quick Facts
- arXiv ID: 2511.08317
- Source URL: https://arxiv.org/abs/2511.08317
- Authors: Shuaimin Li; Liyang Fan; Yufang Lin; Zeyang Li; Xian Wei; Shiwen Ni; Hamid Alinejad-Rokny; Min Yang
- Reference count: 33
- The paper proposes ReViewGraph, a novel framework for automatic paper reviewing that leverages heterogeneous graph reasoning over LLM-simulated reviewer-author debates.

## Executive Summary
This paper introduces ReViewGraph, a framework that simulates multi-round reviewer-author debates using LLM agents and then reasons over the resulting interactions with a heterogeneous graph neural network to predict paper acceptance decisions. The method explicitly extracts argumentative relationships as typed edges, creating a structured graph that captures the nuanced dynamics of the review process. Experiments on ICLR datasets show consistent improvements over strong baselines, with an average relative improvement of 15.73% in Macro F1 score.

## Method Summary
ReViewGraph operates through a multi-stage pipeline: first, Qwen2.5-VL-72B-Instruct simulates three-stage reviewer-author debates (initial review, author rebuttal, re-evaluation) using role-specific prompts. Next, GPT-4.1-mini extracts opinion triplets and classifies evaluation dimensions from the debate text. These elements are structured into a heterogeneous graph with four node types (Title, Evaluation Dimension, Reviewer Opinion, Author Opinion) and typed edges representing argumentative relations. A two-layer Heterogeneous Graph Transformer processes this graph, applying relation-specific attention mechanisms, before type-wise pooling and a final classification head predicts accept/reject decisions.

## Key Results
- ReViewGraph achieves average Macro F1 scores of 48.34%, 45.33%, and 45.90% on ICLR 2023-2025 test sets
- Outperforms strong baselines including GraphEval (43.57% F1) and fine-tuned models (42.81% F1)
- Ablation studies confirm effectiveness of modeling evaluation dimensions and inter-agent relations
- Case studies demonstrate ability to capture nuanced reviewer dynamics and make context-aware predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring debate content as a heterogeneous graph with typed nodes and semantically-labeled edges improves review prediction accuracy over treating text as undifferentiated sequences.
- Mechanism: The heterogeneous graph explicitly separates four node types and four meta-relation types, allowing the downstream HGT to apply relation-specific projection weights and learn distinct attention patterns for different discourse signals.
- Core assumption: Argumentative relations between opinions carry more predictive signal than raw text similarity.
- Evidence anchors:
  - [abstract]: "Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph."
  - [section]: Ablation shows removing reviewer-author interaction edges (w/o RAR) or inter-reviewer relational edges (w/o IRR) causes moderate F1 drops (~1-3 points across datasets). Replacing heterogeneous structure with homogeneous graph (w/o Hetero) causes additional degradation, confirming heterogeneity value.
  - [corpus]: Related work GraphEval uses similarity-based edges only and underperforms (F1 ~43-46 across datasets), suggesting typed semantic edges provide superior signal.

### Mechanism 2
- Claim: Multi-round reviewer-author debate simulation surfaces latent weaknesses and consensus patterns that single-pass review generation cannot capture.
- Mechanism: The three-stage simulation (Initial Review → Author Rebuttal → Re-evaluation) forces reviewers to either maintain or revise positions based on author clarifications, creating an implicit "certainty" signal preserved through inter-reviewer relations.
- Core assumption: Simulated debates approximate the information dynamics of real human reviewer-author exchanges.
- Evidence anchors:
  - [section]: Case studies show the method correctly predicts rejection when multiple reviewers express aligned concerns that authors fail to substantively address, and correctly predicts acceptance when majority reviewers support despite isolated criticism.
  - [section]: The framework uses Qwen2.5-VL-72B-Instruct for multi-agent simulation, with GPT-4.1-mini for structured extraction.
  - [corpus]: Related work AgentReview (Jin et al. 2024) also uses multi-agent simulation but lacks explicit graph reasoning, and is outperformed by ReViewGraph.

### Mechanism 3
- Claim: Pooling node embeddings by type before concatenation preserves semantic role separation during final classification.
- Mechanism: After L-layer HGT propagation, the model applies mean pooling separately to each node type, then concatenates the pooled vectors to ensure each type contributes a dedicated representation.
- Core assumption: Different node types encode complementary predictive signals that should be explicitly preserved.
- Evidence anchors:
  - [section]: Equations 7-9 define type-specific pooling: h_a = mean(H[L][t] for all nodes v of type a), followed by concatenation h_concat = ||_{a∈A} h_a, then fed to two-layer FFN for prediction.
  - [section]: Ablation shows removing Title nodes causes the largest F1 drop (up to ~5 points on ICLR 2025), confirming type-specific contributions vary in importance.

## Foundational Learning

- Concept: **Heterogeneous Graph Neural Networks (HGNNs)**
  - Why needed here: ReViewGraph's core architecture is a Heterogeneous Graph Transformer (HGT), which extends standard GNNs to handle multiple node and edge types with type-specific parameters.
  - Quick check question: Can you explain why a homogeneous GNN (treating all nodes/edges as the same type) would fail to distinguish between "Reviewer A agrees with Reviewer B" versus "Author accepts Reviewer A's critique"?

- Concept: **In-Context Learning and Prompt Engineering for Extraction**
  - Why needed here: The graph instantiation pipeline relies entirely on prompting GPT-4.1-mini to extract opinion triplets and classify evaluation dimensions—no supervised extraction model is trained.
  - Quick check question: What failure modes would you expect if the extraction prompt fails to distinguish between "neutral" and "accept" author responses?

- Concept: **Multi-Agent LLM Systems**
  - Why needed here: ReViewGraph uses role-specific agents (three reviewers, one author, one senior reviewer) with distinct prompts and staged interaction protocols.
  - Quick check question: How might reviewer agent behavior differ if all agents share the same prompt versus having distinct "conservative" vs. "generous" reviewer personas?

## Architecture Onboarding

- Component map:
  Paper content → Multi-agent debate simulation → Triplet extraction → Dimension classification → Graph instantiation → HGT forward pass → Type-wise pooling → Concatenation → FFN classifier → Decision

- Critical path:
  Paper content → Multi-agent debate simulation → Triplet extraction → Dimension classification → Graph instantiation → HGT forward pass → Type-wise pooling → Concatenation → FFN classifier → Decision

- Design tradeoffs:
  - **Extraction quality vs. cost**: GPT-4.1-mini for extraction is cheaper than GPT-4o but may have higher error rates on complex argument structures
  - **Graph density vs. noise**: More relation types (6 inter-reviewer, 6 reviewer-author) capture nuance but increase extraction errors
  - **HGT depth vs. over-smoothing**: 2 HGT layers preserve locality; deeper risks homogenizing node representations

- Failure signatures:
  - **Sparse graphs**: If triplet extraction yields <5 opinion nodes per paper, graph structure provides weak signal
  - **Dominant edge type**: If >80% of edges are "neutral" or "clarify", relation-specific attention degenerates
  - **Dimension imbalance**: If >70% of opinions classify to one dimension, type-pooling collapses

- First 3 experiments:
  1. **Extraction quality audit**: Sample 20 papers, manually verify triplet extraction accuracy for each relation type; identify systematic confusion patterns (e.g., "accept" vs. "compromise")
  2. **Ablation by edge density**: Stratify test set by graph density (opinion node count); evaluate whether performance gains concentrate in high-density graphs
  3. **Single-dimension stress test**: Train on only one evaluation dimension at a time to quantify each dimension's predictive contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ReViewGraph maintain its predictive performance when applied to real-world human-authored reviewer-author debates instead of simulated ones?
- Basis in paper: [inferred] The method relies entirely on LLM-simulated interactions (Section: Multi-agent Reviewer-Author Debate Simulation) rather than ground-truth historical rebuttal data.
- Why unresolved: It is unclear if simulated debates capture the nuance of real defenses; performance might be inflated if the simulation and reasoning models share underlying biases.
- What evidence would resolve it: Evaluation on datasets containing actual author rebuttals (e.g., PeerRead with response fields) rather than generated ones.

### Open Question 2
- Question: Can the fixed taxonomy of evaluation dimensions (e.g., Methodological Novelty) generalize effectively to other scientific domains?
- Basis in paper: [inferred] The authors define four specific dimensions (Page 3) and evaluate exclusively on ICLR (Machine Learning) datasets (Table 1).
- Why unresolved: Peer review criteria vary significantly across disciplines (e.g., Biology vs. CS); the rigid graph schema may fail to capture domain-specific evaluation metrics.
- What evidence would resolve it: Testing the framework on review data from venues in different fields (e.g., Social Sciences, Physics) without redefining the node types.

### Open Question 3
- Question: How sensitive is the framework to the semantic accuracy of the LLM-based relation extraction step?
- Basis in paper: [inferred] The construction of the heterogeneous graph depends on GPT-4.1-mini extracting triples and classifying opinions correctly (Page 4).
- Why unresolved: Errors in classifying relations (e.g., mistaking "disagree" for "clarify") would propagate through the HGT, potentially leading to incorrect reasoning paths.
- What evidence would resolve it: An analysis measuring the correlation between extraction error rates and the final decision accuracy, or using human-annotated graphs as a control.

## Limitations

- **Extraction quality uncertainty**: The entire graph construction pipeline depends on GPT-4.1-mini correctly identifying 7 distinct relation types and mapping opinions to 4 evaluation dimensions, with no systematic error analysis provided.
- **Simulation realism unproven**: The method treats LLM-simulated reviewer-author debates as equivalent to human dynamics, but no validation shows simulated debates capture the same argumentative patterns as real reviewing.
- **Minimal HGT configuration**: Only 2 layers with 128 hidden dimensions are used, which may limit capacity to capture complex relational dependencies in dense debate graphs.

## Confidence

**High confidence:** The heterogeneous graph structure provides measurable performance gains over homogeneous alternatives, as demonstrated by consistent F1 improvements and ablation studies. The architectural choices (type-pooling, relation-specific attention) are technically sound and empirically validated on the ICLR datasets.

**Medium confidence:** The superiority over baselines (especially fine-tuned methods) is demonstrated, but the analysis lacks ablation on individual edge types to quantify which relations contribute most to performance. The claim that typed semantic edges outperform similarity-based edges (GraphEval) is supported but could be more rigorously tested.

**Low confidence:** Claims about capturing "nuanced reviewer dynamics" and "context-aware predictions" are supported only by qualitative case studies without systematic error analysis or comparison to alternative interpretation methods.

## Next Checks

1. **Extraction quality audit:** Manually annotate 50 papers from the ICLR test sets, verifying the accuracy of all extracted opinion triplets and relation classifications. Compute per-relation F1 scores and identify systematic confusion patterns that could explain any remaining performance gaps.

2. **Edge-type ablation study:** For each of the 10 edge types (4 paper-dimension, 4 inter-reviewer, 2 reviewer-author), train models with only that edge type present. This would quantify which relational signals are most predictive and whether the heterogeneous structure's value comes from diversity or specific critical relations.

3. **Real vs. simulated debate comparison:** Obtain a small set of papers with both human reviewer-author exchanges (if available from OpenReview) and LLM-simulated debates. Analyze structural differences in the resulting graphs (edge distributions, node centrality, relation type frequencies) to assess simulation fidelity and identify potential domain gaps.