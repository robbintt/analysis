---
ver: rpa2
title: On Expressive Power of Quantized Neural Networks under Fixed-Point Arithmetic
arxiv_id: '2409.00297'
source_url: https://arxiv.org/abs/2409.00297
tags:
- networks
- quantized
- theorem
- then
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressive power of quantized neural networks
  under fixed-point arithmetic, addressing the gap between theoretical universal approximation
  results (which assume real parameters and exact operations) and practical quantized
  networks (which use discrete fixed-point parameters with round-off errors). The
  authors provide both necessary and sufficient conditions for universal representation
  of quantized networks under fixed-point arithmetic.
---

# On Expressive Power of Quantized Neural Networks under Fixed-Point Arithmetic

## Quick Facts
- **arXiv ID:** 2409.00297
- **Source URL:** https://arxiv.org/abs/2409.00297
- **Reference count:** 40
- **Key outcome:** Provides necessary and sufficient conditions for universal representation of quantized neural networks under fixed-point arithmetic, showing various popular activations (ReLU, GELU, etc.) satisfy these conditions, and even identity activations with binary weights can achieve universal representation.

## Executive Summary
This paper bridges the gap between classical universal approximation theorems (which assume real parameters and exact operations) and practical quantized neural networks that use discrete fixed-point parameters with round-off errors. The authors establish both necessary and sufficient conditions for quantized networks to represent all fixed-point functions exactly. They show that popular activation functions satisfy their sufficient condition, meaning practical quantized networks can represent arbitrary discrete functions. Interestingly, they demonstrate that even identity activation functions with binary weights (-1,1) can achieve universal representation under fixed-point arithmetic due to rounding-induced nonlinearity. The work quantitatively analyzes parameter requirements and compares with floating-point setups.

## Method Summary
The paper provides a constructive proof using indicator functions to show quantized networks can universally represent fixed-point functions. The method partitions the input space into quantized cubes and constructs indicator functions for each cube using a 3-4 layer network architecture. The key insight is that rounding operations in fixed-point arithmetic introduce nonlinearity even with identity activations, enabling universal representation. The proof establishes that for universal representation, the set of reachable outputs (S_σ,p,s,b) must equal the entire fixed-point number set Q_p,s, which occurs when 1/s belongs to the set of activation output gaps V_σ,p,s. The construction requires exponentially many parameters in bit-width but provides exact representation rather than approximation.

## Key Results
- Established necessary condition: N_σ,p,s,b = Q_p,s must hold for universal representation
- Established sufficient condition: S_σ,p,s,b = Q_p,s enables universal representation through indicator function decomposition
- Showed various popular activations (ReLU, GELU, ELU, SoftPlus, SiLU, Mish) satisfy sufficient condition
- Demonstrated identity activation functions can achieve universal representation due to rounding-induced nonlinearity
- Binary weights (-1,1) with proper conditions can also achieve universal representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sufficient condition S_σ,p,s,b = Q_p,s enables universal representation by constructing arbitrary target functions as sums of quantized indicator functions
- Mechanism: Any target function f*: Q^d_p,s → Q_p,s can be decomposed into indicator functions γ_i × 1_Ci(x) over a partition of input space. If each γ_i ∈ S_σ,p,s,b, then a σ-quantized network can implement every indicator function and thus the target function through summation with rounding
- Core assumption: σ and Q_p,s satisfy Condition 1 (monotonicity-like structure enabling indicator construction); V_σ,p,s contains 1/s enabling representation of all coefficients
- Evidence anchors: [abstract] "We first provide a necessary condition and a sufficient condition on fixed-point arithmetic and activation functions for quantized networks to represent all fixed-point functions." [Section 3.2.3, Theorem 6] "If there exists b ∈ Q_∞,s such that S_σ,p,s,b = Q_p,s, then σ quantized networks under Q_p,s can universally represent."
- Break condition: If V_σ,p,s does not generate all of Q_p,s (e.g., activation outputs are restricted to sublattice), then S_σ,p,s,b ⊂ Q_p,s and universal representation fails

### Mechanism 2
- Claim: Rounding errors in fixed-point arithmetic make even identity activations capable of universal representation, unlike exact real arithmetic where identity networks only express affine maps
- Mechanism: Fixed-point rounding (⌈·⌋_Q_p,s) applied after each affine transformation introduces nonlinearity. The composition ⌈ρ_L⌋ ◦ ⌈σ⌋ ◦ ... ◦ ⌈ρ_1⌋ with σ(x)=x is not equivalent to an affine map because rounding breaks exact linearity at each layer, creating the expressiveness needed for indicator function construction
- Core assumption: Rounding follows "away from zero" tie-breaking and is applied after full affine transformation computation, not per-operation
- Evidence anchors: [abstract] "Interestingly, they find that even identity activation functions and binary weights (-1,1) can achieve universal representation under their conditions." [Section 3.2.3] "If a network uses real parameters and exact mathematical operations, then it can only express affine maps with the identity activation function... Nevertheless, quantized networks with the identity activation function can universally represent."
- Break condition: If intermediate operations were also quantized per-addition/multiplication (as in floating-point arithmetic models), the construction would differ and might not transfer

### Mechanism 3
- Claim: Necessary condition N_σ,p,s,b = Q_p,s must hold for universal representation; failure indicates activation-precision incompatibility
- Mechanism: Multi-layer σ-quantized networks have outputs constrained to N_σ,p,s,b = {⌈b + Σw_i x_i⌋ : w_i ∈ Q_p,s, x_i ∈ ⌈σ⌋(Q_p,s)}. If this set doesn't cover Q_p,s, some target values cannot be reached regardless of network depth or width
- Core assumption: Network has more than one layer; single-layer networks have additional constraints (monotonicity of rounded affine maps)
- Evidence anchors: [Section 3.1, Theorem 1] "If σ quantized networks under Q_p,s can universally represent, then there exists b ∈ Q_∞,s such that N_σ,p,s,b = Q_p,s." [Section 3.1, Lemma 2] Shows Hardtanh-like activations with divisibility constraints fail N_σ,p,s,b = Q_p,s
- Break condition: If there exists r ≥ 3 such that r | s·⌈σ⌋(x) for all x, and sr ∈ Q_p,s, then N_σ,p,s,b ≠ Q_p,s for all b

## Foundational Learning

- Concept: **Fixed-point arithmetic Q_p,s**
  - Why needed here: The entire theory is built on discrete number representation Q_p,s = {k/s : k ∈ Z, -2^p+1 ≤ k ≤ 2^p-1} with rounding. Understanding how precision (p) and scale (s) interact is essential for interpreting the main results
  - Quick check question: Given p=7, s=127, what is the smallest positive number in Q_7,127 and what is q_max?

- Concept: **Universal approximation vs. universal representation**
  - Why needed here: Classical results approximate continuous functions on compact domains with real parameters. This paper proves exact representation of all discrete functions with quantized parameters—a stronger, discrete-domain result
  - Quick check question: Why does the classical universal approximation theorem (real parameters, exact operations) not directly imply the quantized network results?

- Concept: **Indicator function decomposition**
  - Why needed here: The constructive proof represents any target function as f*(x) = Σ γ_i × 1_Ci(x) over a partition {C_i}. Understanding this decomposition is key to grasping how the sufficient condition yields universal representation
  - Quick check question: For a target function f*: Q^2_p,s → Q_p,s with |Q_p,s| = 2^p values, what is the maximum number of indicator functions needed in the decomposition?

## Architecture Onboarding

- Component map:
  - Q_p,s: Fixed-point number set defined by bit-width p and scale s
  - ⌈·⌋_Q_p,s: Rounding operation (ties away from zero)
  - σ: Activation function (e.g., ReLU, GELU, identity)
  - V_σ,p,s: Set of output gaps {⌈σ⌋(x) - ⌈σ⌋(y) : x, y ∈ Q_p,s}
  - S_σ,p,s,b: Reachable outputs with bias b and V_σ,p,s coefficients
  - N_σ,p,s,b: Necessary set constraint on multi-layer outputs

- Critical path:
  1. For given p, s, verify Condition 1 holds for activation σ (monotonicity or equivalent structure)
  2. Compute V_σ,p,s and check if 1/s ∈ V_σ,p,s (implies S_σ,p,s,b = Q_p,s)
  3. If S_σ,p,s,b = Q_p,s for some b, universal representation is guaranteed
  4. For necessary condition, check divisibility: if ∃r ≥ 3 with r | s·⌈σ⌋(x) ∀x, then universal representation fails

- Design tradeoffs:
  - Higher p (more bits) increases q_max and parameter space, but construction cost grows as O(2^{2p})
  - Larger s (finer granularity) improves approximation of real-valued targets but may require deeper networks
  - Binary weights reduce multiplication cost but require deeper/wider networks for equivalent expressiveness
  - Identity activation works but likely requires more layers than ReLU/GELU for practical tasks

- Failure signatures:
  - Activation outputs constrained to lattice (e.g., r | s·⌈σ⌋(x)) → N_σ,p,s,b ≠ Q_p,s
  - ⌈σ⌋ constant on Q_p,s → V_σ,p,s = {0} → only constant functions representable
  - Single-layer network trying to represent non-monotonic function → impossible by rounded affine map structure

- First 3 experiments:
  1. **Verify indicator construction**: Implement the 3-4 layer network construction from Theorem 5 for a simple 1D indicator function γ × 1_{[α,β]}(x) with ReLU activation and p=7, s=1. Confirm exact output match on Q_p,s
  2. **Test necessary condition failure**: Construct a network with σ(x) = 5·Hardtanh(x) and p=3, s=1. Verify that N_σ,p,s,b ≠ Q_p,s by enumerating reachable outputs and confirming some Q_p,s values are missing
  3. **Compare binary vs. full-precision weights**: For a target function f*: Q^2_7,1 → Q_7,1, implement both full-precision and binary-weight constructions. Measure depth/width required to achieve exact representation and compare parameter counts

## Open Questions the Paper Calls Out
None

## Limitations
- The construction requires exponentially many parameters in bit-width (O(2^{2p})), making it theoretically valid but practically infeasible for standard network sizes
- The paper does not bridge the gap between exact representation of discrete functions and practical universal approximation for real-valued functions or common datasets
- Limited empirical validation of quantitative analysis comparing parameter requirements with floating-point arithmetic setups

## Confidence

- **High confidence**: The theoretical framework establishing necessary and sufficient conditions for universal representation of fixed-point functions. The mathematical proofs follow logically from the fixed-point arithmetic model and indicator function decomposition.
- **Medium confidence**: The claim that identity activation functions achieve universal representation through rounding-induced nonlinearity. While the mechanism is logically sound within the paper's framework, the practical implications for learning efficiency remain unclear.
- **Low confidence**: The quantitative analysis comparing parameter requirements with floating-point arithmetic setups. The paper provides theoretical bounds but limited empirical validation of these scaling relationships in practical scenarios.

## Next Checks

1. **Empirical Parameter Scaling Study**: Implement the universal representation construction for small networks (p=3-4, d=1-2) and measure the actual parameter count versus theoretical O(2^{2p}) bound. Compare this scaling with standard universal approximation results for real-valued networks.

2. **Round-off Error Impact Analysis**: For a fixed-width quantized network with ReLU activation, systematically vary the rounding precision and measure the degradation in representing simple target functions. This would validate whether the theoretical conditions predict practical behavior.

3. **Binary Weight Efficiency Test**: Implement the binary weight construction for a simple 2D function and measure the depth/width trade-off compared to full-precision weights. This would provide concrete evidence for the claim that binary weights can achieve universal representation while quantifying the practical cost.