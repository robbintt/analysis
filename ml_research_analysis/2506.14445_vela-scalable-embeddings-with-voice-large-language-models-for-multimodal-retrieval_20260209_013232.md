---
ver: rpa2
title: 'Vela: Scalable Embeddings with Voice Large Language Models for Multimodal
  Retrieval'
arxiv_id: '2506.14445'
source_url: https://arxiv.org/abs/2506.14445
tags:
- retrieval
- vela
- text
- mllms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underexplored ability of multimodal large
  language models (MLLMs) to represent acoustic information for cross-modal retrieval.
  The authors introduce Vela, a framework that adapts MLLMs to generate universal
  multimodal embeddings by using carefully designed prompts and in-context learning
  examples to bridge the modality gap between text and audio.
---

# Vela: Scalable Embeddings with Voice Large Language Models for Multimodal Retrieval

## Quick Facts
- arXiv ID: 2506.14445
- Source URL: https://arxiv.org/abs/2506.14445
- Reference count: 0
- Key outcome: Vela achieves competitive retrieval performance on standard benchmarks (AudioCaps R@1=18.1, Clotho R@1=17.9) while introducing new benchmarks for long-text and conditional instruction-based retrieval

## Executive Summary
This paper introduces Vela, a framework that adapts multimodal large language models (MLLMs) to generate universal multimodal embeddings for cross-modal retrieval without requiring paired audio-text training data. Unlike traditional CLAP models that rely on contrastive learning with audio-text pairs, Vela employs a single-modality training approach using only text pairs while freezing the MLLM's modality encoder and projector. The framework uses carefully designed "in one word" prompts and in-context learning examples to bridge the modality gap between text and audio representations. Experiments demonstrate competitive performance on standard benchmarks and superior results on newly introduced long-text and instruction-based retrieval tasks.

## Method Summary
Vela uses Qwen2-Audio-7B-Instruct as the backbone with frozen Whisper-v3 audio encoder and projector components. The framework fine-tunes only the LLM portion via QLoRA (r=32, α=8, dropout=0.05) using a 4-bit quantization approach. Training employs contrastive loss on text triplets from NLI datasets (~273k pairs) without any audio-text pairs. The key innovation lies in using "Summarize in one word" prompts for both modalities, combined with 100 curated in-context learning examples (65 text + 35 audio) to align embeddings. The last token's hidden state serves as the universal embedding representation for retrieval tasks.

## Key Results
- Vela achieves Recall@1 of 18.1 on AudioCaps and 17.9 on Clotho benchmarks
- Outperforms CLAP models on newly introduced Vela-long benchmark for long-text retrieval
- Demonstrates superior performance on Vela-conditional benchmark for instruction-based retrieval
- Ablation studies confirm effectiveness of prompt design, in-context learning, and single-modality training paradigm

## Why This Works (Mechanism)

### Mechanism 1
Prompting MLLMs with "Summarize in one word" compresses multimodal inputs into a shared embedding space via the last token's hidden state. Autoregressive models accumulate contextual information across tokens; the final token's hidden state reflects a compressed probability distribution over the input semantics. By using parallel prompt templates for text and audio, the model produces modality-agnostic representations.

### Mechanism 2
In-context learning examples inject modality alignment priors that reduce the modality gap more effectively than prompt design alone. Providing curated text-audio exemplars before the target input conditions the MLLM's attention toward semantically relevant features rather than syntactically salient words. This addresses lexical salience bias where models emphasize high-frequency syntactic elements over semantic content critical for alignment.

### Mechanism 3
Single-modality contrastive training on text pairs transfers discriminative capabilities to multimodal retrieval without paired audio-text data. After unifying embeddings via prompts and in-context learning, the embedding space becomes modality-agnostic. Training the LLM component with contrastive loss on text triplets from NLI data improves semantic discrimination, which transfers to audio queries because text and audio embeddings already share structure.

## Foundational Learning

- **Modality Gap in Contrastive Learning**
  - Why needed here: Vela's core contribution is reducing this gap without paired audio-text training. Understanding why separate encoders produce isolated embedding clusters clarifies why unified prompting matters.
  - Quick check question: Can you explain why two randomly initialized encoders would produce non-overlapping embedding distributions even before training?

- **In-Context Learning in Autoregressive Models**
  - Why needed here: The paper relies on demonstration examples to condition model behavior without weight updates. Understanding how attention layers use context is essential for debugging alignment quality.
  - Quick check question: How does providing input-output demonstration pairs change the model's prediction without modifying parameters?

- **Contrastive Learning with InfoNCE Loss**
  - Why needed here: The training objective uses temperature-scaled cosine similarity over positive-negative pairs. Understanding this loss clarifies why text-only training transfers.
  - Quick check question: What happens to the gradient signal when the negative samples are too similar to the anchor?

## Architecture Onboarding

- **Component map**: Audio/Text → Frozen Whisper-v3 encoder → Frozen projector → Fine-tuned Qwen2-Audio-7B-LLM → Last token embedding → Cosine similarity for retrieval

- **Critical path**: 
  1. Inference: Audio/Text → Frozen encoder → Frozen projector → Fine-tuned LLM → Last token embedding → Cosine similarity for retrieval
  2. Training: Text triplets → Same pipeline → Contrastive loss → QLoRA gradient updates (r=32, α=8, dropout=0.05)

- **Design tradeoffs**: 
  - 4-bit quantization enables training on 8×3090 GPUs but may degrade precision on subtle acoustic features
  - 200 demonstration examples curated to 100 balances context length vs. alignment quality
  - Text-only training eliminates audio-text pair collection cost but assumes prompt-based unification is sufficient

- **Failure signatures**: 
  - High gap score (>40) after training suggests in-context examples not loading correctly
  - R@1 <10 on standard benchmarks indicates prompt template mismatch with base model
  - Long-text retrieval degradation suggests context window overflow or attention dilution

- **First 3 experiments**: 
  1. Ablation: Prompt-only embeddings — Remove in-context examples and training, measure gap score and R@1 to baseline the unification mechanism
  2. Gap score vs. retrieval correlation — Plot modality gap against R@1 across configurations to validate gap reduction as causal mechanism
  3. Demonstration set sensitivity — Reduce in-context examples from 100 to 20 and measure performance drop to determine example budget requirements

## Open Questions the Paper Calls Out

### Open Question 1
Can the Vela framework generalize to other MLLM architectures beyond Qwen2-Audio? The authors identify their work as an adaptation for MLLMs generally to generate universal embeddings, yet the experimental setup confines the validation to the Qwen2-Audio-7B-Instruct model. Different MLLMs utilize varying architectures for modality projectors and LLM backbones which may respond differently to the "in one word" compression and specific in-context learning examples.

### Open Question 2
To what extent does the "in one word" summarization prompt cause information loss for complex, multi-event audio scenes? The method relies on extracting the hidden state of the last token to represent the entire audio or text input. The fundamental capacity of a single-token embedding to represent dense audio information (e.g., a clip with simultaneous speech, music, and environmental noise) remains unverified compared to vector-averaging methods.

### Open Question 3
Would incorporating a small amount of paired audio-text data during training improve performance on standard benchmarks without sacrificing the model's zero-shot capabilities? The authors acknowledge that Vela shows slightly lower performance than LAION-CLAP on AudioCaps Recall@10, attributing it to the "zero-shot nature" and the fact that AudioCaps is present in the baselines' training data.

### Open Question 4
Does the "in one word" prompting strategy effectively transfer to non-acoustic modalities such as vision? The abstract claims the framework generates "universal multimodal embeddings" and bridges gaps "across various modalities," but the methodology and experiments are restricted to the acoustic domain. Visual tokens in MLLMs often possess different semantic densities and spatial relationships than sequential audio tokens.

## Limitations

- The "in one word" summarization approach may lose information for complex, multi-event audio scenes that cannot be adequately captured by a single semantic point
- Performance on standard benchmarks (AudioCaps R@1=18.1) remains below state-of-the-art CLAP models that use paired audio-text training data
- The method's effectiveness for non-acoustic modalities like vision remains untested despite claims of "universal" embeddings

## Confidence

- **Unified embedding generation via prompts**: High confidence. Well-supported by analogous work (E5-v) and ablation studies
- **In-context learning for modality alignment**: Medium confidence. Ablation shows benefit but lacks validation on arbitrary demonstration sets
- **Single-modality training transfer effectiveness**: Low-Medium confidence. Novel approach lacks ablation of training without pre-alignment or comparison to multimodal fine-tuning baselines

## Next Checks

1. **Gap-score sensitivity analysis**: Systematically vary the number and quality of in-context examples (10, 20, 50, 100) and measure both gap score and retrieval performance to establish minimum viable demonstration set size

2. **Cross-domain generalization test**: Evaluate Vela on non-natural sound domains (industrial machinery, speech in noise) where SoundBible-based in-context examples may not provide adequate alignment priors

3. **Training regime ablation**: Compare single-modality training against paired audio-text contrastive learning while keeping all other components (prompts, in-context examples) constant to isolate training paradigm contribution