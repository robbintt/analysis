---
ver: rpa2
title: Dropping Just a Handful of Preferences Can Change Top Large Language Model
  Rankings
arxiv_id: '2508.11847'
source_url: https://arxiv.org/abs/2508.11847
tags:
- arena
- data
- rankings
- chatbot
- dropping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to test the robustness of Bradley-Terry
  (BT) rankings to small worst-case data drops, using an AMIP-based influence approximation.
  Applied to multiple LLM evaluation platforms, the method shows that top model rankings
  can flip after removing as few as 2 preferences (0.003% of total).
---

# Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings

## Quick Facts
- arXiv ID: 2508.11847
- Source URL: https://arxiv.org/abs/2508.11847
- Authors: Jenny Y. Huang; Yunyi Shen; Dennis Wei; Tamara Broderick
- Reference count: 40
- Primary result: Top LLM rankings can flip after removing as few as 2 preferences (0.003% of total)

## Executive Summary
This paper introduces a method to test the robustness of Bradley-Terry (BT) rankings to small worst-case data drops, using an AMIP-based influence approximation. Applied to multiple LLM evaluation platforms, the method shows that top model rankings can flip after removing as few as 2 preferences (0.003% of total). This fragility persists even with bootstrap-based rankings and is similar for human- and LLM-judged systems, except for MT-bench, which is more robust due to expert annotation and discriminative prompts. The method identifies the specific dropped preferences driving flips, enabling inspection of these influential cases. The findings suggest leaderboard rankings are often unstable and warrant caution in interpretation.

## Method Summary
The authors develop an influence approximation method based on AMIP (Approximate Maximum Influence Perturbation) to identify the smallest set of preferences whose removal would cause a ranking flip. This approach avoids computationally expensive re-fitting of Bradley-Terry models by using influence functions to approximate the effect of dropping individual preferences. The method is applied to seven major LLM evaluation platforms, analyzing both human- and LLM-annotated preference data. For each platform, the algorithm searches for the minimal number of preference drops needed to cause a flip in the top-3 model rankings, with particular attention to flips affecting the top-ranked model.

## Key Results
- Top model rankings can flip after removing as few as 2 preferences (0.003% of total)
- Ranking fragility persists even with bootstrap-based rankings
- MT-bench shows relative robustness due to expert annotation and discriminative prompts
- Human- and LLM-judged systems show similar fragility levels
- The method identifies specific influential preferences that drive ranking changes

## Why This Works (Mechanism)
The Bradley-Terry model estimates pairwise comparison probabilities between models based on preference data. Small perturbations in this data can significantly shift these estimated probabilities, particularly when preferences are close to decision boundaries. The AMIP-based influence approximation efficiently identifies these critical preferences without requiring full model retraining. This works because influence functions provide first-order approximations of how model parameters (and thus rankings) change when individual data points are removed, capturing the sensitivity of the ranking to specific preferences.

## Foundational Learning
**Bradley-Terry model**: A statistical framework for modeling pairwise comparisons and deriving rankings from preference data. Why needed: Forms the mathematical foundation for aggregating pairwise preferences into model rankings. Quick check: Verify that BT parameters sum to appropriate normalization.

**Influence functions**: Mathematical tools that approximate how model parameters change when individual data points are removed. Why needed: Enable efficient computation of data point importance without full retraining. Quick check: Confirm that influence approximations correlate with actual parameter changes.

**AMIP (Approximate Maximum Influence Perturbation)**: A framework for finding minimal data perturbations that cause model failures. Why needed: Provides the algorithmic approach for identifying critical preference drops. Quick check: Validate that identified drops actually cause ranking flips.

**Preference aggregation**: The process of combining pairwise preferences into global rankings. Why needed: Central to how LLM evaluation platforms produce leaderboards. Quick check: Ensure aggregation method preserves transitivity where appropriate.

**Bootstrap rankings**: Rankings derived from multiple resampled datasets to assess stability. Why needed: Provides a robustness check against sampling variability. Quick check: Compare bootstrap ranking stability to original rankings.

## Architecture Onboarding
**Component map**: Preference data -> Bradley-Terry model -> Influence approximation -> Critical preference identification -> Ranking stability assessment

**Critical path**: The influence approximation step is most critical, as it determines which preferences are identified as influential and ultimately drives the ranking stability assessment.

**Design tradeoffs**: The method trades computational efficiency (using influence approximations) for some accuracy compared to full retraining, but maintains sufficient precision for identifying critical preferences.

**Failure signatures**: Ranking flips occur when influential preferences are near decision boundaries in the BT parameter space, or when multiple models have similar underlying strengths.

**First experiments**: 1) Apply method to synthetic preference data with known ground truth rankings, 2) Compare influence approximation accuracy against full retraining on small datasets, 3) Test method on datasets with varying levels of preference noise.

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on limited set of publicly available leaderboards may not generalize to all evaluation platforms
- Bradley-Terry model may not capture all nuances of preference dynamics, especially with correlated preferences
- Study does not account for potential variations in evaluation protocols across different platforms
- Method's efficiency comes at the cost of some approximation error compared to full model retraining

## Confidence
**High confidence**: Rankings can flip with as few as 2 preference drops (empirically validated across multiple platforms)
**Medium confidence**: Ranking fragility is a widespread issue across all LLM evaluation systems (limited to specific platforms studied)
**Medium confidence**: Human- and LLM-judged systems exhibit similar fragility (underlying reasons not deeply explored)

## Next Checks
1. Test the AMIP-based influence approximation on additional LLM evaluation platforms with varying annotation protocols to assess generalizability
2. Conduct ablation studies to isolate the impact of preference correlation and non-independence on ranking stability
3. Compare the fragility of rankings derived from alternative preference aggregation methods (e.g., Plackett-Luce) to validate the robustness of findings