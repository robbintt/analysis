---
ver: rpa2
title: Compressed BC-LISTA via Low-Rank Convolutional Decomposition
arxiv_id: '2601.23148'
source_url: https://arxiv.org/abs/2601.23148
tags:
- c-bc-lista
- forward
- convolutional
- training
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of Sparse Signal Recovery (SSR)
  in multichannel imaging, specifically in applications like ultrasound imaging, where
  physics-based forward models are large and computationally expensive. The authors
  propose a Compressed Block-Convolutional (C-BC) measurement model that leverages
  low-rank convolutional decomposition to compress both forward and backward operators
  while preserving reconstruction accuracy.
---

# Compressed BC-LISTA via Low-Rank Convolutional Decomposition

## Quick Facts
- arXiv ID: 2601.23148
- Source URL: https://arxiv.org/abs/2601.23148
- Reference count: 0
- This work proposes compressed Block-Convolutional (C-BC) measurement model using low-rank CNN decomposition for sparse signal recovery in multichannel ultrasound imaging, achieving 6.9M parameters vs 1738M for MLP-LISTA while improving reconstruction accuracy.

## Executive Summary
This paper addresses sparse signal recovery (SSR) in multichannel imaging, particularly ultrasound, where physics-based forward models are large and computationally expensive. The authors propose a compressed Block-Convolutional (C-BC) measurement model that leverages low-rank convolutional decomposition to compress both forward and backward operators while preserving reconstruction accuracy. The method uses Orthogonal Matching Pursuit (OMP) to select a compact set of basis filters from an analytically initialized convolutional forward model, then constructs a mixing coefficients matrix to approximate the full model. This approach is applied to the Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) network, creating C-BC-LISTA.

## Method Summary
The method compresses physics-based forward models using low-rank convolutional decomposition. OMP selects M basis filters from the analytic weight matrix W, then computes mixing coefficients C via least-squares to approximate W ≈ CB. This is implemented as a two-layer CNN: Conv1D with basis filters B followed by 1×1 Conv with mixing coefficients C. The backward operation reverses these steps using transposed convolutions with shared weights. This compressed module replaces the forward/backward operators in LISTA blocks, with 10 stacked blocks and optional weight sharing. Training uses on-the-fly synthetic data generation with MSE loss, and both forward and backward models remain trainable.

## Key Results
- C-BC-LISTA achieves 6.9 million parameters vs 1738 million for MLP-LISTA, reducing model size from 6.96 GB to 27 MB
- Improves reconstruction accuracy with PAE and SE metrics across multiple SNR levels
- OMP-initialized structured compression outperforms SVD-based and random initializations for convergence speed and final accuracy
- M=16 basis filters often achieves lowest PAE/SE while providing strongest regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The weight matrix of physics-derived convolutional kernels in time delay measurements is approximately low-rank, enabling compact basis filter approximation.
- Mechanism: OMP selects M rows from W to form basis filters B, computes mixing coefficients C via least-squares (C = WBT(BBT)−1), reducing parameters from Cout × K to M × (K + Cout).
- Core assumption: Signal subspace can be captured by M ≪ Cout basis filters without significant reconstruction error.
- Evidence anchors: OMP selection and low-rank approximation described in abstract and section 2.1; neighbor papers discuss decomposition methods but not specifically for ultrasound.

### Mechanism 2
- Claim: Low-rank CNN decomposition enables drop-in two-layer replacement with exact transposed adjoint for gradient fidelity.
- Mechanism: Factorization W ≈ CB implemented as Conv1D (basis B) + 1×1 Conv (mixing C), with backward operation reversing steps using transposed convolutions with same weights.
- Core assumption: Gradients through compressed model remain faithful to original physics-based model.
- Evidence anchors: Section 2.4 describes exact adjoint relationship up to numerical precision; neighbor papers discuss neural operators but not gradient fidelity in compressed models.

### Mechanism 3
- Claim: OMP-initialized analytic compression outperforms SVD-based and random initialization for convergence speed and final reconstruction accuracy.
- Mechanism: OMP selects actual rows from W preserving physical interpretability, while SVD produces dense basis vectors. Random initialization causes stalling or catastrophic divergence.
- Core assumption: Physics-derived structure provides better optimization landscape than learned abstract features.
- Evidence anchors: Section 3.3 shows random initialization severely degrades performance while OMP-initialized converges fastest; neighbor papers do not directly compare initialization strategies.

## Foundational Learning

- Concept: **Orthogonal Matching Pursuit (OMP)**
  - Why needed here: OMP greedily selects basis filters from analytic weight matrix by iteratively choosing rows with highest correlation energy to residual, serving as core compression mechanism.
  - Quick check question: Given residual matrix R and candidate rows Wj, how would you compute the selection criterion for next basis filter?

- Concept: **Algorithm Unrolling / Deep Unrolling**
  - Why needed here: LISTA unrolls ISTA iterations into neural network blocks where each block corresponds to one iteration; C-BC-LISTA builds on this by compressing forward/backward operators within each block.
  - Quick check question: In equation (1), what do matrices (I − 1/L ATA) and 1/L AT correspond to in unrolled network architecture?

- Concept: **Block-Toeplitz Structure in Time Delay Measurements**
  - Why needed here: Physics of time delay-based acquisitions produces forward matrices with block-Toeplitz structure that can be reformulated as convolutions, enabling compression approach.
  - Quick check question: Why does reciprocity principle between transmitters and receivers imply diagonal slices symmetrically positioned about center have identical characteristics?

## Architecture Onboarding

- Component map: Analytic forward model → OMP basis selection (M filters) → Construct B, C matrices → Initialize Conv1D and 1×1 Conv layers → Stack LISTA blocks → Train with MSE loss

- Critical path: Analytic forward model derivation → OMP basis selection → construct B, C matrices → initialize Conv1D and 1×1 Conv layers → stack LISTA blocks → train with MSE loss

- Design tradeoffs:
  - **M (number of basis filters)**: More filters (M=64) accelerate convergence via over-parameterization; fewer filters (M=16) provide stronger regularization and often better final accuracy
  - **Weight sharing vs independent per block**: Sharing kernels across blocks saves memory; unique shrinkage parameters per block maintain flexibility
  - **Trainable vs fixed forward model**: Freezing forward model slows training; keeping both forward and backward trainable yields best performance

- Failure signatures:
  - Random initialization (Xavier stalls, Kaiming/orthogonal diverge after initial epochs)
  - SVD initialization converges but at higher loss than OMP
  - Aggressive compression with M too small for true signal subspace rank increases reconstruction error

- First 3 experiments:
  1. Implement OMP basis selection on synthetic weight matrix (Cout=400, K from model), verify reconstruction error ∥W − CB∥F decreases as M increases, confirm parameter count reduction matches theoretical M × (K + Cout)
  2. Train C-BC-LISTA (M=32, 10 blocks) with OMP, SVD-leading, and Xavier initialization on identical synthetic data, plot validation loss curves to confirm OMP converges fastest and to lowest loss
  3. Compare C-BC-LISTA (M=16, 32, 64) against BC-LISTA and MLP-LISTA on held-out test data at multiple SNR levels (5dB, 20dB, noiseless), measure PAE, SE, parameter count, and storage size to verify 27MB vs 6.96GB claim

## Open Questions the Paper Calls Out

- How does C-BC-LISTA perform on real-world ultrasound measurements compared to simulated data? Authors state "In future work, we will evaluate these models on real-world measurements."

- Can C-BC-LISTA be effectively integrated into task-based learning frameworks for compressed sensing, and how does it compare to task-agnostic approaches? Authors propose to "investigate their integration into task-based learning frameworks for CS, with comparisons to task-agnostic approaches."

- How does the method scale to larger channel configurations (e.g., 128-channel FMC imaging) where competing SOTA methods become inapplicable? Authors restrict experiments to 32 channels "because of the computational resource limit" but note the method "can be used in large-scale scenarios."

- Can the compressed forward model enable efficient generative model-based adaptive sensing? Authors state they "intend to apply the compressed model to facilitate efficient generative model-based adaptive sensing."

## Limitations
- Convolution hyperparameters (kernel size K, stride S, padding P) for slice-wise forward model not explicitly stated
- Training hyperparameters including optimizer type, learning rate, batch size, and learning rate schedule unspecified
- Physics-based initialization of original weight matrix W before OMP assumed but not detailed
- Performance characteristics and memory requirements uncharacterized at clinically/economically relevant array sizes

## Confidence

**High Confidence:** Low-rank convolutional decomposition mechanism and implementation through OMP basis selection with mixing coefficients. Theoretical framework for compressed forward/backward operators with exact adjoint relationships is well-specified and experimentally validated.

**Medium Confidence:** Superiority of OMP initialization over SVD and random methods. Ablation results are clear and dramatic, though exact OMP stopping criterion not fully specified.

**Low Confidence:** Computational efficiency claims (27MB vs 6.96GB) require careful verification of implementation details including data types, quantization, and actual model architecture.

## Next Checks
1. Implement OMP basis selection on synthetic weight matrix (Cout=400, K from model), verify reconstruction error ∥W − CB∥F decreases as M increases, confirm parameter count reduction matches theoretical M × (K + Cout).

2. Train C-BC-LISTA (M=32, 10 blocks) with OMP, SVD-leading, and Xavier initialization on identical synthetic data, plotting validation loss curves to confirm OMP converges fastest and to lowest loss as reported.

3. Compare C-BC-LISTA (M=16, 32, 64) against BC-LISTA and MLP-LISTA on held-out test data at multiple SNR levels (5dB, 20dB, noiseless), measuring PAE, SE, parameter count, and storage size to verify 27MB vs 6.96GB claim and performance improvements.