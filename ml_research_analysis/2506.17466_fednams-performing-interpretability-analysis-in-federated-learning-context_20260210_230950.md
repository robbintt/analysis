---
ver: rpa2
title: 'FedNAMs: Performing Interpretability Analysis in Federated Learning Context'
arxiv_id: '2506.17466'
source_url: https://arxiv.org/abs/2506.17466
tags:
- client
- output
- layer
- learning
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedNAMs, a federated learning framework that
  combines Neural Additive Models (NAMs) with federated learning to achieve both interpretability
  and accuracy. The approach trains client-specific models on local data, aggregates
  feature-specific networks, and preserves privacy.
---

# FedNAMs: Performing Interpretability Analysis in Federated Learning Context

## Quick Facts
- arXiv ID: 2506.17466
- Source URL: https://arxiv.org/abs/2506.17466
- Reference count: 40
- Primary result: Federated learning framework combining Neural Additive Models with FL to achieve interpretable predictions while preserving privacy

## Executive Summary
FedNAMs introduces a federated learning framework that combines Neural Additive Models (NAMs) with federated learning to achieve both interpretability and accuracy. The approach trains client-specific models on local data, aggregates feature-specific networks, and preserves privacy. Experiments on UCI Heart Disease, OpenML Wine, and Iris datasets show strong interpretability with minimal accuracy loss compared to traditional federated DNNs. Key predictive features are identified per dataset (e.g., volatile acidity, heart rate, petal length). The method outperforms Captum in providing feature-specific interpretability, offering detailed insights into model decision-making across decentralized nodes.

## Method Summary
FedNAMs implements a federated learning system where each client trains a Neural Additive Model (NAM) locally. Each NAM consists of K separate neural networks (FeatureNNs), one per input feature, with 3 hidden layers of 20 neurons each. Clients train their models using standard optimization with early stopping and dropout. The server aggregates by averaging FeatureNN weights independently per feature across clients (weighted by sample count). This preserves interpretability since each global FeatureNN represents the averaged contribution of its corresponding feature. The final prediction is the sum of all feature contributions plus a bias term. The framework handles both binary and multi-class classification tasks.

## Key Results
- FedNAMs achieves interpretable feature contributions with minimal accuracy loss compared to traditional federated DNNs
- Feature-specific interpretability shows volatile acidity, heart rate, and petal length as top predictive features for wine, heart disease, and iris datasets respectively
- FedNAMs outperforms Captum in providing detailed, feature-specific interpretability rather than aggregate importance scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing predictions into feature-specific neural networks enables interpretable contributions without post-hoc explanation methods.
- Mechanism: Each input feature xi passes through its own dedicated sub-network (FeatureNN), producing a scalar contribution. The final prediction sums these contributions additively: g(E[y]) = β + Σ fi(xi). This architectural constraint makes feature contributions directly inspectable rather than inferred.
- Core assumption: Features contribute independently to the output; meaningful feature interactions are either absent or can be approximated additively.
- Evidence anchors: [abstract] "individual networks concentrate on specific input features"; [section 1] "NAMs decompose the prediction task into individual functions, each contributing to the final prediction in a transparent manner"; [corpus] Paper "Challenges in interpretability of additive models" (arxiv:2504.10169) warns of nonidentifiability issues in GAMs—suggests interpretability claims require caution.
- Break condition: If strong feature interactions exist (e.g., x1 × x2 significantly affects y), the additive assumption fails and interpretability becomes misleading.

### Mechanism 2
- Claim: Aggregating feature-specific functions across clients (rather than whole models) preserves interpretability in federated settings.
- Mechanism: Each client trains local FeatureNNs. During aggregation, the server averages feature functions independently: f_final(xk) = (1/n) Σ_i fik(xk). This maintains per-feature interpretability at the global level while respecting FL's data decentralization.
- Core assumption: Feature contributions learned on local data distributions generalize when averaged; client data distributions are not extremely heterogeneous.
- Evidence anchors: [section 4, equations 3-7] Explicit aggregation formulas for feature functions; [section 6] "consistent feature contributions" observed across clients despite local variations; [corpus] FedNAM+ follow-up paper (arxiv:2506.17872) extends this framework, suggesting the mechanism has traction.
- Break condition: Under extreme non-IID data, averaged feature functions may not represent any client's actual data distribution, producing globally unfaithful interpretations.

### Mechanism 3
- Claim: Feature-specific networks produce visualizable contribution curves that outperform post-hoc attribution methods in granularity.
- Mechanism: Since each fi is a learned neural function, plotting fi(xi) vs. xi directly shows the learned relationship. Unlike SHAP/Captum which provide scalar importance values, this yields continuous shape functions showing directional effects across feature ranges.
- Core assumption: The learned fi approximates the true underlying relationship between xi and y (modulo other features).
- Evidence anchors: [section 6.1] "our framework offers more detailed and feature-specific interpretability than Captum"; [figures 4, 5] Visualization of output variation per feature across clients; [table 1 vs. table 2] FedNAMs show client-specific contribution magnitudes; Captum shows single aggregate values; [corpus] Limited direct corpus validation; related FL papers focus on privacy/accuracy, not interpretability mechanisms.
- Break condition: If feature networks overfit to local noise, visualized shapes may reflect artifacts rather than true relationships.

## Foundational Learning

- Concept: **Generalized Additive Models (GAMs)**
  - Why needed here: FedNAMs are neural instantiations of GAMs; understanding g(E[y]) = β + Σ fi(xi) is prerequisite to grasping why interpretability is architecturally guaranteed.
  - Quick check question: Can you explain why a GAM is more interpretable than a standard neural network with hidden layers mixing all features?

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: FedNAMs adapt FedAvg's aggregation rule to feature-specific networks; understanding weight averaging across clients is essential.
  - Quick check question: In standard FedAvg, what gets aggregated? In FedNAMs, what gets aggregated instead of (or in addition to) weights?

- Concept: **Feature Attribution vs. Intrinsic Interpretability**
  - Why needed here: The paper positions FedNAMs against post-hoc methods like LIME/SHAP/Captum; understanding this distinction clarifies the tradeoffs.
  - Quick check question: Why might a post-hoc attribution method give different results than inspecting an intrinsically interpretable model's structure?

## Architecture Onboarding

- Component map: Input features (x1...xK) -> K separate FeatureNNs -> Sum of outputs + bias β -> Output layer with link function g(·)
- Critical path: 1. Define number of features K → instantiate K FeatureNN modules; 2. Train locally per client with early stopping and dropout; 3. Server aggregates FeatureNN weights using equations 3-7; 4. Global model's FeatureNNs are averaged versions; predictions remain interpretable
- Design tradeoffs:
  - **Interpretability vs. interaction modeling**: Pure additivity cannot capture x1×x2 effects. Extension would require explicit interaction terms (e.g., fij(xi, xj)), exploding complexity.
  - **Client heterogeneity**: Averaging assumes compatible feature functions; divergent local distributions may require clustered FL or personalized layers.
  - **FeatureNN capacity**: 3×20 architecture chosen empirically; larger capacity risks overfitting, smaller may underfit complex relationships.
- Failure signatures:
  - Feature contribution plots show erratic/non-smooth curves → overfitting or insufficient data
  - High discrepancy between client feature rankings → non-IID data requiring mitigation
  - Accuracy significantly below baseline DNN → additive assumption too restrictive for task
- First 3 experiments:
  1. **Baseline sanity check**: Train FedNAM on a synthetic dataset where true relationship is additive (y = f1(x1) + f2(x2) + noise). Verify recovered fi match ground truth.
  2. **Ablation on heterogeneity**: Split data into 3 clients with controlled distribution shifts. Measure how feature contribution consistency degrades as shift increases.
  3. **Interaction stress test**: Train on dataset with known feature interactions (e.g., XOR-style). Compare FedNAM accuracy vs. standard FL-DNN to quantify additive constraint cost.

## Open Questions the Paper Calls Out
None

## Limitations
- No ablation study on non-IID client data distributions to verify feature function generalization assumptions
- Missing statistical significance testing for accuracy differences versus baseline methods
- Hyperparameter details (learning rates, rounds, client splits) are not specified, limiting reproducibility

## Confidence
- **High confidence**: The additive architecture with feature-specific networks is clearly specified and mathematically sound.
- **Medium confidence**: Interpretability claims hold for synthetic additive data but transferability to complex real-world relationships is uncertain without interaction modeling.
- **Low confidence**: Comparative performance claims versus Captum and standard FL-DNNs lack statistical validation and clear baselines.

## Next Checks
1. Test FedNAMs on a synthetic dataset with known additive ground truth, then gradually introduce feature interactions to measure accuracy degradation and interpretability fidelity.
2. Systematically vary client data distributions from IID to extreme non-IID and measure how feature contribution consistency changes across the aggregation process.
3. Compare FedNAM accuracy and feature attribution quality against a post-hoc method (SHAP/Captum) on the same federated setup, reporting statistical significance and confidence intervals.