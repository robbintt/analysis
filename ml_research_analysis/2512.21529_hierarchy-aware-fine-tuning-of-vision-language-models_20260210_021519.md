---
ver: rpa2
title: Hierarchy-Aware Fine-Tuning of Vision-Language Models
arxiv_id: '2512.21529'
source_url: https://arxiv.org/abs/2512.21529
tags:
- hierarchical
- tp-kl
- hisce
- levels
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a hierarchical fine-tuning method for vision-language
  models (VLMs) that adapts them to structured taxonomies. The approach introduces
  two novel loss functions: Tree-Path KL Divergence (TP-KL) for enforcing vertical
  consistency along taxonomy paths, and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE)
  for horizontal consistency among sibling classes.'
---

# Hierarchy-Aware Fine-Tuning of Vision-Language Models

## Quick Facts
- arXiv ID: 2512.21529
- Source URL: https://arxiv.org/abs/2512.21529
- Authors: Jiayu Li; Rajesh Gangireddy; Samet Akcay; Wei Cheng; Juhua Hu
- Reference count: 40
- Primary result: Hierarchical fine-tuning improves consistency across taxonomy levels in vision-language models

## Executive Summary
This paper introduces a hierarchical fine-tuning approach for vision-language models (VLMs) that adapts them to structured taxonomies. The method addresses the limitation of standard cross-entropy fine-tuning, which treats taxonomy levels independently and can produce inconsistent predictions across hierarchical levels. By introducing two novel loss functions - Tree-Path KL Divergence (TP-KL) for vertical consistency and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency - the approach reshapes the text embedding space into a hierarchy-aligned manifold while maintaining the vision-language alignment.

The proposed method is evaluated on four diverse datasets including fine-grained visual classification tasks (birds, aircraft, butterflies) and medical imaging (chest X-rays). Results demonstrate consistent improvements in Full-Path Accuracy (FPA) and Tree-based Inconsistency Error (TICE) compared to standard fine-tuning approaches, with minimal parameter overhead through integration with LoRA. The approach shows particular promise for applications requiring coherent predictions across multiple levels of taxonomic hierarchy.

## Method Summary
The hierarchical fine-tuning method operates by fine-tuning both vision and language encoders of a pre-trained VLM to align with a given taxonomy structure. The approach introduces two complementary loss functions: TP-KL divergence enforces consistency along taxonomy paths by regularizing the predicted distributions at different levels to follow the hierarchical relationships, while HiSCE loss ensures consistency among sibling classes at the same taxonomic level. Both losses operate in the shared embedding space of VLMs and are integrated with LoRA for efficient parameter updates. The method requires complete taxonomy information for the target dataset and reshapes the embedding space to reflect hierarchical relationships while preserving the vision-language alignment learned during pre-training.

## Key Results
- Consistent improvements in Full-Path Accuracy (FPA) across all four tested datasets (CUB-200-2011, FGVC-Aircraft, Butterfly-200, ChestX-ray14)
- Reduction in Tree-based Inconsistency Error (TICE) demonstrating better hierarchical consistency
- Minimal parameter overhead through LoRA integration while achieving superior performance
- Improved interpretability through hierarchy-aligned embedding space

## Why This Works (Mechanism)
The method works by enforcing both vertical and horizontal consistency constraints in the shared embedding space of VLMs. The TP-KL loss ensures that predictions at different taxonomy levels follow the hierarchical relationships by regularizing the KL divergence between parent and child distributions along taxonomy paths. The HiSCE loss promotes smoothness among sibling classes by encouraging similar representations for related classes at the same taxonomic level. Together, these losses reshape the embedding manifold to reflect the underlying taxonomy structure while maintaining the vision-language alignment capabilities of the pre-trained model.

## Foundational Learning
- Taxonomy hierarchies: Structured classification systems with parent-child relationships; needed to define hierarchical consistency constraints
- KL divergence: Measures distributional difference between predictions at different hierarchy levels; quick check: verify non-negativity and asymmetry properties
- Cross-entropy loss: Standard classification objective; needed as baseline for comparison and integration with hierarchical losses
- LoRA (Low-Rank Adaptation): Parameter-efficient fine-tuning technique; quick check: confirm rank decomposition preserves model capacity
- Embedding spaces: Shared representations for vision and language modalities; needed for hierarchical consistency enforcement
- Fine-grained classification: Distinguishing between visually similar classes; needed to test method on challenging taxonomy structures

## Architecture Onboarding
Component map: Input images -> Vision encoder -> Shared embedding space -> Text encoder -> Hierarchical losses (TP-KL, HiSCE) -> Prediction heads -> Output
Critical path: Vision encoder output → Shared embedding → Text encoder → Hierarchical consistency losses → Parameter updates
Design tradeoffs: Parameter efficiency vs. full fine-tuning, computational overhead vs. consistency gains, taxonomy completeness requirements vs. practical applicability
Failure signatures: Inconsistent predictions across hierarchy levels, degraded vision-language alignment, computational bottlenecks with deep/wide taxonomies
Three first experiments:
1. Ablation study removing TP-KL loss to isolate vertical consistency contribution
2. Test on incomplete taxonomy to evaluate robustness to missing hierarchy information
3. Scalability test with deeper/wider taxonomies to measure computational overhead

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on complete taxonomy structures; performance may degrade with incomplete or noisy hierarchies
- Limited testing on non-biological hierarchies (organizational, product taxonomies)
- Computational overhead scales with taxonomy depth and breadth, potentially problematic for very large hierarchies
- Assumes perfect hierarchical ground truth, which may not reflect real-world taxonomy inconsistencies

## Confidence
- Hierarchy alignment improvements: High
- Parameter efficiency claims: Medium
- Interpretability improvements: Low

## Next Checks
1. Test the method on datasets with incomplete or noisy taxonomies to evaluate robustness when hierarchical ground truth is imperfect
2. Evaluate scalability by applying the method to taxonomies with significantly greater depth (10+ levels) and breadth (100+ siblings per node)
3. Conduct ablation studies removing either the vertical (TP-KL) or horizontal (HiSCE) consistency losses to quantify their individual contributions