---
ver: rpa2
title: Neural Morphological Tagging for Nguni Languages
arxiv_id: '2505.12949'
source_url: https://arxiv.org/abs/2505.12949
tags:
- morphological
- neural
- languages
- tagging
- nguni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores neural approaches for morphological tagging
  of Nguni languages (isiNdebele, isiXhosa, isiZulu, Siswati). Morphological parsing
  is framed as a pipeline of segmentation followed by tagging, and the study focuses
  on the tagging step using pre-segmented morphemes from existing segmenters.
---

# Neural Morphological Tagging for Nguni Languages

## Quick Facts
- **arXiv ID**: 2505.12949
- **Source URL**: https://arxiv.org/abs/2505.12949
- **Reference count**: 12
- **Primary result**: Neural morphological taggers achieve macro F1 scores above 60% and micro F1 above 90% on gold segmentations for Nguni languages

## Executive Summary
This paper investigates neural approaches for morphological tagging of Nguni languages (isiNdebele, isiXhosa, isiZulu, Siswati), focusing specifically on the tagging step using pre-segmented morphemes. The study evaluates both models trained from scratch (bi-LSTMs and CRF with bi-LSTM features) and finetuned pretrained language models (XLM-R, Afro-XLMR, Nguni-XLMR) across multiple languages and segmentation conditions. Neural taggers significantly outperform rule-based baselines, with models trained from scratch generally outperforming finetuned PLMs. The research demonstrates that canonical-based models perform better than surface-based ones when evaluated on predicted segmentations, establishing the viability of neural morphological tagging for these morphologically complex languages.

## Method Summary
The paper frames morphological parsing as a pipeline of segmentation followed by tagging, focusing on the tagging step using pre-segmented morphemes from existing segmenters. Two classes of neural taggers are evaluated: models trained from scratch (bi-LSTMs and CRF with bi-LSTM features) and finetuned pretrained language models (XLM-R, Afro-XLMR, Nguni-XLMR). Models are tested on both gold-annotated and model-predicted segmentations, using both canonical (standardised morphemes) and surface (as-composed morphs) segmentation forms. The evaluation uses macro and micro F1 scores to measure performance across multiple Nguni languages.

## Key Results
- Neural taggers significantly outperform rule-based baselines
- Models trained from scratch generally outperform finetuned PLMs
- Best models achieve macro F1 scores above 60% and micro F1 above 90% on gold segmentations
- Canonical-based models perform better than surface-based ones on predicted segmentations

## Why This Works (Mechanism)
Neural models excel at capturing morphological patterns in Nguni languages due to their ability to learn distributed representations of morphemes and their contextual relationships. The superior performance of models trained from scratch suggests that the morphological complexity of Nguni languages may not be fully captured by general-purpose multilingual models, which may lack sufficient exposure to these specific language patterns during pretraining. The advantage of canonical-based models on predicted segmentations indicates that standardization helps neural models better handle the variability introduced by morphological composition and segmentation errors.

## Foundational Learning
- **Bi-directional LSTM networks**: Why needed - To capture contextual information from both left and right contexts of morphemes; Quick check - Verify that bidirectional processing improves performance over unidirectional alternatives
- **Conditional Random Fields (CRFs)**: Why needed - To model sequential dependencies between morphological tags and ensure globally consistent tag sequences; Quick check - Compare CRF performance against independent tagging to quantify the benefit of sequence modeling
- **Pretrained multilingual models (XLM-R, Afro-XLMR, Nguni-XLMR)**: Why needed - To leverage cross-linguistic transfer and knowledge from large-scale pretraining; Quick check - Measure performance gains from finetuning versus training from scratch to assess pretraining utility
- **Morphological segmentation**: Why needed - To decompose words into constituent morphemes as input for the tagging task; Quick check - Analyze tagging performance degradation when using predicted vs gold segmentations
- **Canonical vs surface forms**: Why needed - To evaluate whether standardized morpheme representations improve model robustness; Quick check - Compare model performance on canonical and surface forms to quantify standardization benefits

## Architecture Onboarding

### Component Map
Input Segmentation -> Feature Extraction (Bi-LSTM/PLM) -> Sequence Tagging (CRF/Softmax) -> Output Morphological Tags

### Critical Path
The critical path is: Input Segmentation → Feature Extraction → Sequence Tagging. The feature extraction step (bi-LSTM or PLM) is crucial as it determines the quality of contextual representations used for tagging decisions.

### Design Tradeoffs
The choice between training from scratch versus finetuning pretrained models represents a fundamental tradeoff between model specialization and leveraging transfer learning. Training from scratch allows models to learn language-specific morphological patterns without interference from other language influences but requires more data and computational resources. Finetuning pretrained models offers faster training and potential cross-linguistic benefits but may not capture the specific morphological nuances of Nguni languages effectively.

### Failure Signatures
Performance degradation when using predicted segmentations indicates sensitivity to segmentation quality. The relative underperformance of finetuned PLMs suggests these models may not effectively transfer knowledge to the specific morphological patterns of Nguni languages. Surface-based models performing worse than canonical-based ones on predicted segmentations reveals vulnerability to morphological composition variations.

### Exactly 3 First Experiments
1. Compare bi-LSTM vs PLM performance on a held-out validation set to quantify the benefit of contextual representations
2. Test canonical vs surface form performance on gold segmentations to isolate the effect of standardization
3. Evaluate model performance across the four Nguni languages to identify language-specific challenges

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses only on the tagging step of morphological parsing, not evaluating an end-to-end system
- Performance drops significantly when using predicted rather than gold segmentations, indicating pipeline sensitivity
- Limited dataset sizes may constrain generalizability and lead to overfitting to specific corpus characteristics

## Confidence
- **High Confidence**: Relative performance ranking between model classes and the superior performance of canonical-based models on predicted segmentations
- **Medium Confidence**: Absolute performance metrics on gold segmentations, as these depend on dataset-specific characteristics
- **Low Confidence**: General claims about viability of neural morphological taggers without qualification about the pipeline approach limitations

## Next Checks
1. Conduct ablation studies to quantify how segmentation errors specifically impact tagging performance
2. Evaluate best-performing models on additional annotated datasets or out-of-domain text to assess robustness
3. Implement and evaluate an integrated segmentation-tagging pipeline to measure combined performance