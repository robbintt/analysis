---
ver: rpa2
title: 'ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations'
arxiv_id: '2511.05359'
source_url: https://arxiv.org/abs/2511.05359
tags:
- attacks
- privacy
- data
- attack
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces CONVERSE, a dynamic benchmark for evaluating\
  \ privacy and security risks in multi-turn agent-to-agent conversations across three\
  \ domains (travel, real estate, insurance) with 864 contextualized attacks. It tests\
  \ contextual integrity and data abstraction in 12 user profiles, revealing that\
  \ privacy attacks succeed in 37\u201388% of cases and security breaches in 2\u2013\
  60%, with higher-capability models leaking more."
---

# ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations

## Quick Facts
- **arXiv ID**: 2511.05359
- **Source URL**: https://arxiv.org/abs/2511.05359
- **Reference count**: 19
- **Primary result**: Introduces CONVERSE benchmark for contextual privacy/security in multi-turn agent-to-agent conversations with 864 contextualized attacks

## Executive Summary
ConVerse introduces a dynamic benchmark for evaluating privacy and security risks in multi-turn agent-to-agent conversations across travel, real estate, and insurance domains. Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. The benchmark evaluates contextual integrity and data abstraction in 12 user profiles, revealing that privacy attacks succeed in 37-88% of cases and security breaches in 2-60%. Results show higher-capability models leak more information, with contextual attacks exploiting the utility-vulnerability coupling inherent in cooperative dialogue.

## Method Summary
ConVerse uses a multi-agent framework with an AI assistant, LLM-simulated user environment, and external service agent. The environment contains personal data and tools (email, calendar, banking), while the external agent introduces contextualized attacks across turns using plausible justifications. Seven state-of-the-art models are evaluated under identical protocols using GPT-5 as LLM-matcher. Privacy attacks measure extraction snippet leakage, while security attacks assess unauthorized tool use and preference manipulation. Data generation uses Claude Sonnet 4.0 with manual validation, and attacks are stratified by data-proximity (Unrelated, Related and Useful, Private).

## Key Results
- Privacy attacks succeed in 37-88% of cases across models, with higher-capability models leaking more
- Security breaches occur in 2-60% of cases, with context-aware attacks succeeding more than explicit requests
- "Related and Useful" data leaks most (90-94% ASR) due to semantic relevance bypassing privacy filters
- GPT-5 achieves 84.68% ASR with 96.55% coverage, while Gemini 2.5 Flash achieves 37.91% ASR with 82.81% coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextually embedded attacks succeed because they exploit the utility-vulnerability coupling inherent in cooperative dialogue
- Mechanism: Malicious requests are introduced within semantically relevant task discourse using plausible justifications (e.g., "standard protocol," domain-specific rationales), causing assistants to treat manipulation as legitimate coordination rather than adversarial input
- Core assumption: Models lack explicit boundary-reasoning to distinguish helpful information sharing from contextual coercion
- Evidence anchors: Abstract states multi-turn agent-to-agent conversations embed malicious requests within plausible discourse; Section 6.4 notes attacks use institutional language and appear after substantial task progress; related work confirms multi-agent systems introduce novel risks through collaboration protocols
- Break condition: If assistants implement explicit verification steps against trusted sources before disclosing sensitive data or executing actions

### Mechanism 2
- Claim: Higher-capability models leak more because improved task completion correlates with reduced scrutiny of information requests
- Mechanism: Stronger models achieve higher utility through deeper engagement with external agents, inadvertently treating more requests as legitimate task requirements
- Core assumption: Current alignment training optimizes for helpfulness without domain-boundary reasoning
- Evidence anchors: Abstract notes stronger models leaking more; Table 6 shows GPT-5: 84.68% ASR with 96.55% coverage vs. Gemini 2.5 Flash: 37.91% ASR with 82.81% coverage
- Break condition: If capability improvements include explicit privacy-abstraction training that separates task-relevance from disclosure-appropriateness

### Mechanism 3
- Claim: Data-proximity to task domain predicts leakage rates because models cannot distinguish "relevant" from "shareable"
- Mechanism: The three-tier taxonomy shows "Related and Useful" data leaks most (90-94% ASR) because semantic relevance bypasses privacy filters that might catch "Unrelated" data (57-71% ASR)
- Core assumption: RAG systems retrieve semantically similar content without contextual integrity checks
- Evidence anchors: Section 4.2.2 states RAG systems retrieve semantically relevant information regardless of contextual integrity principles; Table 2 shows "Related and Useful" data: 90.91-94.12% ASR across domains; related work on contextual integrity supports this theoretical framing
- Break condition: If retrieval includes a contextual-integrity filter that evaluates disclosure appropriateness, not just semantic relevance

## Foundational Learning

- Concept: Contextual Integrity Theory (Nissenbaum, 2009)
  - Why needed here: Provides the normative framework for the three-tier privacy taxonomy; privacy is defined by appropriate information flow within context-specific norms
  - Quick check question: Can you explain why a patient sharing medical data with their doctor preserves privacy, but the same data shared with a travel agent violates it?

- Concept: Agent-to-Agent Communication Protocols
  - Why needed here: ConVerse evaluates autonomous negotiation where neither party is human; understanding how trust and information boundaries are negotiated through language is essential
  - Quick check question: How does an AI assistant decide whether an external agent's request for "emergency contact information" is legitimate coordination or social engineering?

- Concept: Data Abstraction vs. Binary Filtering
  - Why needed here: The benchmark tests whether models can share "asset ranges rather than detailed itemized lists"â€”a capability beyond simple access control
  - Quick check question: Given a user's calendar showing "Sept 3: Rock climbing with Alice Smith at Peak District," what information should be shared with a travel agent scheduling a Sept 16 trip?

## Architecture Onboarding

- Component map: User Environment -> AI Assistant -> External Agent -> Evaluation Module
- Critical path: 1. Initialize with user task + profile data 2. Assistant queries environment, communicates with external agent, invokes tools 3. External agent introduces contextual attacks after establishing cooperative context (typically turn 2-3) 4. Judge evaluates privacy leakage and security violations
- Design tradeoffs: LLM-simulated environment vs. programmatic (paper uses LLM for flexibility, trading realism for ease of implementation); Pre-generated ground truth vs. real-time evaluation (enables modular judge replacement but requires upfront annotation cost); Attack timing (attacks placed mid-conversation mirror real social engineering but complicate causal attribution)
- Failure signatures: Privacy (assistant forwards extraction snippets verbatim without abstraction); Security (assistant executes tool actions based solely on external agent claims without user verification); Utility-security tradeoff (high coverage with high ASR indicates over-cooperation)
- First 3 experiments: 1. Replicate Table 2 analysis: Run Claude Sonnet 4 on Insurance domain, stratify ASR by data-proximity category 2. Test a simple defense: Add verification step requiring assistants to confirm sensitive disclosures with user 3. Ablate attack timing: Move malicious requests to turn 1 vs. turn 3

## Open Questions the Paper Calls Out

- Question: Can grounding agent reasoning in trusted databases or authentication protocols effectively mitigate the "false institutional authority" attacks observed in the benchmark?
  - Basis in paper: Section 8 proposes "verification and grounding" as a defense; Appendix B.2.2 notes agents never challenge institutional claims
  - Why unresolved: Current models treat unverifiable "standard protocols" as fact without cross-referencing capabilities
  - What evidence would resolve it: An ablation study measuring ASR reductions when agents are equipped with verification tools

- Question: How do privacy and security vulnerabilities propagate or compound in systems involving multiple users and their assistants interacting with multiple service providers?
  - Basis in paper: Section 8 identifies "Scaling multi-agent systems" as a key limitation, noting current work is restricted to single-pair interactions
  - Why unresolved: Complexity of trust boundaries in many-to-many agent ecosystems remains unmeasured
  - What evidence would resolve it: Extending ConVerse to simulate multi-user, multi-provider environments

- Question: What specific forms of human oversight optimize the trade-off between task utility and the high privacy leakage (37-88%) observed in autonomous settings?
  - Basis in paper: Section 8 lists "User-in-the-loop dynamics" as a limitation, noting the benchmark assumes fully autonomous operation
  - Why unresolved: Unclear if standard human-in-the-loop patterns effectively block contextually embedded attacks without negating utility
  - What evidence would resolve it: User studies quantifying utility loss versus security gains with various intervention protocols

## Limitations

- LLM-simulated environments and LLM-as-judge evaluation introduce fidelity questions about ecological validity
- 864 attack instances cover only three domains and 12 user profiles, limiting generalizability
- Security metric relies on observed tool logs rather than actual system integration
- Exact system prompts for all agents are not specified, making attack realism difficult to assess

## Confidence

- **High Confidence**: Core finding that contextually embedded attacks succeed more frequently than explicit requests (privacy ASR: 37-88%, security ASR: 2-60%) is well-supported by stratified analysis
- **Medium Confidence**: Claim that higher-capability models leak more due to reduced scrutiny is supported by correlation but lacks mechanistic explanation
- **Low Confidence**: Assertion that attacks succeed because models lack "explicit boundary-reasoning" is speculative without testing boundary-reasoning mechanisms

## Next Checks

1. **Ecological Validity Test**: Deploy the benchmark with actual API-integrated tools (not LLM-simulated) across the three domains to measure real security breaches
2. **Defense Mechanism Validation**: Implement a simple contextual integrity filter that blocks sharing of "Related and Useful" data unless explicitly approved, then measure ASR reduction
3. **Model Architecture Ablation**: Compare ASR across models while controlling for capability to isolate whether leakage correlates with capability or specific architectural choices in privacy handling