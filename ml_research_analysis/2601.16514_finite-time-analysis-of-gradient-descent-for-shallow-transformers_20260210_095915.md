---
ver: rpa2
title: Finite-Time Analysis of Gradient Descent for Shallow Transformers
arxiv_id: '2601.16514'
source_url: https://arxiv.org/abs/2601.16514
tags:
- gradient
- transformer
- lemma
- descent
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a finite-time analysis of projected gradient
  descent for shallow Transformers, focusing on their ability to handle long-term
  dependencies. The key contributions are: (1) The width required for nonasymptotic
  convergence scales only logarithmically with the sample size, a significant improvement
  over previous work.'
---

# Finite-Time Analysis of Gradient Descent for Shallow Transformers

## Quick Facts
- **arXiv ID:** 2601.16514
- **Source URL:** https://arxiv.org/abs/2601.16514
- **Reference count:** 40
- **Primary result:** Finite-time analysis showing shallow Transformers need only logarithmic width for convergence, with optimization error independent of sequence length

## Executive Summary
This paper provides the first finite-time analysis of projected gradient descent for shallow multi-head Transformers, demonstrating that they can handle long-term dependencies more effectively than recurrent architectures. The key insight is that attention gradients remain bounded regardless of sequence length, unlike RNNs where gradients can explode exponentially. Through a combination of NTK analysis and projection techniques, the authors show that logarithmic overparameterization suffices for convergence, a significant improvement over previous work requiring cubic scaling.

## Method Summary
The analysis focuses on shallow multi-head Transformers with scalar outputs, using projected gradient descent with symmetric random initialization. The method combines NTK linearization around initialization with a projection mechanism that keeps parameters within a bounded neighborhood. Training uses a step size of η = 1/√τ for τ iterations, with projection onto sets Ω_ρ ensuring parameters stay near initialization. The theoretical framework characterizes three error sources: linearization error (O(1/√m)), approximation error (O(√(log(n/δ)/m))), and optimization error (O(1/√τ)).

## Key Results
- Width required for convergence scales only logarithmically with sample size n, improving upon previous cubic scaling results
- Optimization error is independent of sequence length T, unlike recurrent architectures with exponential T-dependence
- The analysis preserves attention nonlinearity and allows for independent heads, with experiments validating theoretical predictions
- Transformers show superior long-term dependency handling compared to IndRNNs, though with higher memory usage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention gradient norm is uniformly bounded independent of sequence length T
- **Mechanism:** Softmax-weighted covariance matrix M(X; W^i) satisfies ||M||_F ≤ 1 uniformly because softmax outputs lie on probability simplex and inputs are bounded. This prevents gradient explosion regardless of T.
- **Core assumption:** Input tokens are bounded: max_t ||X_t||_2 ≤ 1
- **Break condition:** Unbounded input norms (||X_t||_2 → ∞) cause gradient bound failure

### Mechanism 2
- **Claim:** Logarithmic overparameterization (width ~ log(n)) suffices for convergence
- **Mechanism:** Symmetric initialization ensures zero output, projection keeps iterates near initialization, linearization error is O(1/√m), approximation error is O(√(log(n/δ)/m)) via random features concentration
- **Core assumption:** Target function belongs to sup-norm constrained RKHS F_ν̄ induced by Transformer NTK
- **Break condition:** Target outside F_ν̄ (requires O(m) transportation mapping norm)

### Mechanism 3
- **Claim:** Projected GD achieves 1/√τ optimization rate without requiring positive definiteness of NTK Gram matrix
- **Mechanism:** Lyapunov argument V(φ) = ||φ - φ̃||_2^2 with change-of-feature error ε_CoF that quantifies gradient deviation from initialization values
- **Core assumption:** Projection radius ρ ≥ ν̄, step size η = 1/√τ
- **Break condition:** Projection radius too small (ρ < ν̄) prevents reaching approximating parameters

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK) regime**
  - **Why needed here:** The analysis linearizes the network around initialization and treats training as kernel regression. Understanding wide networks behave like linear models in parameter space is essential.
  - **Quick check question:** If I double the width m, should training dynamics change significantly in NTK regime? (Answer: No, NTK converges to deterministic limit)

- **Concept: RKHS and transportation mappings**
  - **Why needed here:** Target function class F_ν̄ is defined via transportation mappings v that transport initial weights to parameter configurations representing functions in NTK's RKHS.
  - **Quick check question:** What constraint on v ensures f̃ ∈ F_ν̄? (Answer: sup_φ |v_c(φ)| ≤ ν̄_c, sup ||v_u||_2 ≤ ν̄_u, sup ||v_w||_F ≤ ν̄_w)

- **Concept: Projected gradient descent**
  - **Why needed here:** Projection onto Ω_ρ enforces parameters stay near initialization, key to controlling linearization and change-of-feature errors.
  - **Quick check question:** Does projection onto convex set increase distance to any point inside set? (Answer: No, projection is non-expansive toward points in set)

## Architecture Onboarding

- **Component map:**
  Input X ∈ R^{d×T} (bounded tokens) → [m independent attention heads] → For each head i: a(X; W^i) = X · softmax(X^T W^i q_X) → h(X; θ^i) = σ(U_i^T a(X; W^i)) → [linear aggregation] f(X; φ) = (1/√m) Σ_i c_i h(X; θ^i)

- **Critical path:** Gradient w.r.t. W^i flows through softmax → attention output a → feed-forward σ → linear readout. Softmax Jacobian produces M(X; W^i), whose bounded norm enables T-independence.

- **Design tradeoffs:**
  - *Memory vs. sequence length:* Attention stores O(T) context; memory grows with T (RNNs have O(1) memory but exponential T-dependence)
  - *Overparameterization vs. sample size:* Width m ≳ log(n) suffices here; prior work required m ≳ n^3
  - *Projection radius vs. approximation:* Larger ρ allows larger ν̄ (more complex target functions) but increases linearization error

- **Failure signatures:**
  - Gradient explosion on attention weights → likely input normalization violated (||X_t||_2 > 1)
  - Training loss plateaus above zero → target function may be outside F_ν̄
  - Performance degrades with T → check if positional encodings missing or unbounded activations used

- **First 3 experiments:**
  1. **Width scaling validation:** Train teacher-student models with m ∈ {16, 64, 256, 1024} on fixed n=5000 samples. Plot min training loss vs. m on log-log scale. Expected slope ≈ -0.5.
  2. **Sequence length independence:** Fix m=64, vary T ∈ {16, 32, 64, 128} on autoregressive task with lag L inside context window. Compare Transformer min validation loss vs. IndRNN baseline. Transformer should be flat; IndRNN degrades with L.
  3. **Projection ablation:** Compare ProjGD with unconstrained GD on same task. Unconstrained GD should show larger gradient norms and potential instability; ProjGD should maintain bounded gradients and stable convergence.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can logarithmic overparameterization and sequence-length-independent optimization error bounds be extended to deep Transformer architectures?
  - **Basis in paper:** Explicitly listed as future direction in conclusion
  - **Why unresolved:** Current proofs rely on specific gradient structure and NTK decomposition of shallow attention layer
  - **What evidence would resolve it:** Finite-time analysis for multi-layer Transformers establishing similar convergence rates

- **Open Question 2:** Do guarantees regarding optimization error and memory trade-offs hold for sequence-to-sequence learning tasks?
  - **Basis in paper:** Extension to sequence-to-sequence learning identified as future direction
  - **Why unresolved:** Current analysis focuses on scalar targets, whereas sequence-to-sequence requires managing token-wise outputs
  - **What evidence would resolve it:** Theoretical framework analyzing convergence of encoder-decoder or decoder-only models

- **Open Question 3:** Does convergence rate persist when training moves outside near-initialization (kernel) regime into feature learning regime?
  - **Basis in paper:** Analysis depends on projected gradient descent to enforce kernel regime
  - **Why unresolved:** Transportation mapping and linearization bounds assume parameters stay close to initialization
  - **What evidence would resolve it:** Finite-time convergence proof that doesn't require projection or near-initialization assumptions

## Limitations

- Analysis relies on bounded input tokens (||X_t||_2 ≤ 1), easily violated in real-world data
- Logarithmic width scaling is highly sensitive to unknown transportation mapping norm constants
- Projection mechanism may introduce optimization difficulties if feasible set is too restrictive
- Comparison to IndRNNs limited to specific architectures may not generalize to all RNN variants

## Confidence

- **High confidence:** T-independence of gradient norms well-supported by softmax probability simplex property and bounded input assumption
- **Medium confidence:** Logarithmic width requirement depends on RKHS approximation bounds which are valid but may have loose constants
- **Low confidence:** Practical implications of projection radius trade-off unclear without empirical guidance on typical ν̄ values

## Next Checks

1. **Input norm sensitivity:** Systematically vary input token norm bound from 0.1 to 10 and measure resulting gradient explosion scale and training stability

2. **RKHS membership verification:** Generate synthetic teacher functions with known transportation norms ν̄ and train student Transformer with varying projection radii ρ. Plot minimum training loss vs. ρ/ν̄ to identify critical threshold

3. **Projection efficiency:** Compare training dynamics and final performance of ProjGD versus unconstrained GD with weight decay tuned to mimic projection constraint