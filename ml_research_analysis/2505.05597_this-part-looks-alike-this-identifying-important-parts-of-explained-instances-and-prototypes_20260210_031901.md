---
ver: rpa2
title: 'This part looks alike this: identifying important parts of explained instances
  and prototypes'
arxiv_id: '2505.05597'
source_url: https://arxiv.org/abs/2505.05597
tags:
- prototype
- feature
- importance
- prototypes
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to identify important overlapping features
  between an instance and its nearest prototype, termed alike parts, using feature
  importance scores derived from SHAP. It incorporates these scores into prototype
  selection algorithms to enhance both interpretability and diversity.
---

# This part looks alike this: identifying important parts of explained instances and prototypes

## Quick Facts
- arXiv ID: 2505.05597
- Source URL: https://arxiv.org/abs/2505.05597
- Reference count: 13
- This part looks alike this: identifying important parts of explained instances and prototypes

## Executive Summary
This paper proposes a method to identify "alike parts" - important overlapping features between an instance and its nearest prototype - using feature importance scores from SHAP. The approach extends prototype selection algorithms (A-PETE, G-KM, SM-A) by incorporating a β-weighted feature importance term into the optimization objective. Results show improved interpretability through diverse prototype sets that differ in important feature profiles, with some algorithms achieving higher predictive accuracy on benchmark datasets.

## Method Summary
The method computes SHAP values for both instances and prototypes, normalizes them by squaring and scaling, then multiplies element-wise to create feature importance weights. Features with weights above the mean form a binary "alike parts" mask. Prototype selection algorithms are modified to minimize both distance and feature importance dissimilarity using a β-weighted objective. The approach was evaluated on six Kaggle benchmark datasets using Random Forest models, with surrogate 1-NN classifiers measuring interpretability via accuracy.

## Key Results
- FI-informed A-PETE and G-KM algorithms achieved higher predictive accuracy than raw versions on Apple Quality dataset (G-KM: 0.785 → 0.861)
- Mean feature importance similarity of alike parts increased with β up to 2.0, then stabilized or decreased
- The method successfully handled datasets with mixed numerical/categorical features and supported missing values
- Prototypes selected with FI-diversity showed more varied important feature profiles compared to raw algorithms

## Why This Works (Mechanism)

### Mechanism 1
Normalized SHAP score products identify shared influential features between instances and prototypes more reliably than raw importance values. For each feature l, compute SHAP values for both instance xi and prototype pj, square them to treat positive/negative contributions equally, normalize across all features, then multiply: wl = ϕ̂(h, xil) · ϕ̂(h, pjl). Features with weights above the mean are retained as "alike parts" via binary mask. Core assumption: Squaring before normalization preserves relative importance while avoiding sign cancellation; mean-threshold selection meaningfully separates important from unimportant features. Evidence anchors: [Section 3.1] demonstrates weight computation producing intuitive feature selection; [Table 1] shows Size, Crunchiness, Ripeness selected for Apple Quality example.

### Mechanism 2
Incorporating feature importance similarity into prototype selection objective improves prototype diversity with respect to which features drive predictions. The optimization function f(P) is extended from pure distance minimization to include β-weighted feature importance product: f(P) = Σᵢ min_{pj∈P}(d(xi, pj) + β · fⁱ(xi, pj)). This encourages prototypes to differ in their important-feature profiles, not just spatial distribution. Core assumption: Diversity in feature importance profiles translates to more meaningful prototype sets for human interpretation; β can be tuned without destabilizing the optimization. Evidence anchors: [Section 3.2] shows first term promotes good representation while second encourages diversification; [Figure 1] demonstrates FI-informed A-PETE selects different features as important across prototypes.

### Mechanism 3
The β parameter provides tunable control over the trade-off between spatial proximity and feature importance alignment, with dataset-dependent optimal values. β scales the feature importance term relative to distance in Equation 6. Higher β prioritizes finding prototypes with similar important features; lower β prioritizes spatial closeness. Grid search or Bayesian optimization can identify dataset-specific optima. Core assumption: A single β value works reasonably across instances within a dataset; the relationship between β and outcomes is sufficiently smooth for optimization. Evidence anchors: [Section 4.3] shows non-monotonic relationship between β, mean feature importance, and alike-part length; [Figure 4] demonstrates β ≤ 2.0 often provides good balance.

## Foundational Learning

- Concept: **SHAP (SHapley Additive exPlanations)**
  - Why needed here: The entire method depends on computing feature importance scores for instances and prototypes; understanding what SHAP outputs (attribution values per feature) and their properties (additivity, model-agnosticism) is prerequisite.
  - Quick check question: Given a trained classifier and an instance, can you explain what a SHAP value of +0.3 for feature X means versus -0.2 for feature Y?

- Concept: **k-medoids clustering and prototype selection**
  - Why needed here: The modified algorithms (A-PETE, G-KM, SM-A) build on greedy k-medoids approximation; understanding how medoids minimize within-cluster distance clarifies what the original objective function optimizes.
  - Quick check question: How does k-medoids differ from k-means, and why might medoids be preferred for interpretable prototype selection?

- Concept: **Prototype-based explanation paradigms (local vs. global)**
  - Why needed here: The paper addresses both local explanation (instance-to-prototype alike parts) and global explanation (diverse prototype set); distinguishing these use cases clarifies the dual goals.
  - Quick check question: If a user asks "why did the model predict class A for this specific instance?" versus "what patterns does the model use overall?", which explanation type addresses each?

## Architecture Onboarding

- Component map:
  1. **SHAP Compute Module**: Takes black-box model h and instance x, returns per-feature importance vector ϕ(h, x)
  2. **Normalization & Weight Module**: Implements Equations 1-2; caches fⁱ scores for all instance-prototype pairs
  3. **Binary Mask Generator**: Implements Equation 3; outputs alike-parts indicator for any instance-prototype pair
  4. **Prototype Selector (Modified)**: Wraps A-PETE/G-KM/SM-A with extended objective (Equation 6); takes β as hyperparameter
  5. **Surrogate Evaluator**: 1-NN classifier using selected prototypes; computes accuracy on test set for validation

- Critical path:
  1. Train or load black-box classifier h (Random Forest in paper)
  2. Compute and cache SHAP values for all training instances
  3. Run modified prototype selection with chosen β
  4. For each test instance, find nearest prototype and compute alike-parts mask
  5. Present prototype + highlighted features to user; optionally evaluate surrogate accuracy

- Design tradeoffs:
  - **SHAP vs. other importance methods**: Paper claims any feature importance method works, but SHAP chosen for widespread adoption; computationally expensive for large datasets
  - **Mean threshold vs. top-k selection**: Mean adapts to dataset but may select too many features in uniform-importance scenarios (e.g., Wine Quality dataset); top-k would be more predictable but less adaptive
  - **Pre-computation vs. on-demand**: fⁱ scores cached before optimization (Section 3.2) improves efficiency but increases memory; on-demand computation slower but more scalable

- Failure signatures:
  - Accuracy drops significantly below raw algorithm: β likely too large, over-prioritizing feature alignment at expense of spatial coverage
  - Alike-parts mask includes nearly all features: Dataset may have uniformly high feature importance (cf. Apple/Wine Quality); mean threshold inappropriate
  - Prototypes cluster in narrow region despite high β: Feature importance term may be numerically dominated by distance term; check scaling

- First 3 experiments:
  1. **Baseline reproduction**: Run raw A-PETE, G-KM, SM-A on one benchmark dataset (e.g., Diabetes); verify reported accuracy ranges and prototype counts match paper
  2. **β sensitivity sweep**: For same dataset, run modified algorithm with β ∈ {0.0, 0.5, 1.0, 1.5, 2.0}; plot accuracy vs. β and mean feature importance of alike parts to reproduce Figure 4 pattern
  3. **Alike-parts qualitative check**: Select 5 test instances, display their nearest prototype and alike-parts mask; manually verify highlighted features are plausibly important (compare to domain knowledge, e.g., Glucose for Diabetes)

## Open Questions the Paper Calls Out

### Open Question 1
Does the identification of "alike parts" significantly improve human comprehension of model decisions compared to raw prototype explanations? The authors state in Section 5 that "Future research should explore its effectiveness from the user perspective, assessing whether these explanations enhance human understanding of model decisions." This remains unresolved as the current evaluation relies on proxy metrics rather than formal user studies.

### Open Question 2
Can the proposed method be effectively adapted for non-tabular data modalities like images or text? The conclusion notes that "evaluating the approach on non-tabular modalities, such as images and text, is necessary to assess its broader applicability." The current methodology is defined for feature vectors, and it's unclear how "alike parts" would be extracted and visualized for unstructured data.

### Open Question 3
How sensitive is the method to the choice of the underlying explanation technique (e.g., SHAP vs. LIME)? The authors state in Section 3.1 that "any feature importance method can be applied," but experiments exclusively use SHAP. Different feature importance methods often yield different rankings, leaving the robustness of the "alike parts" identification unverified.

## Limitations

- SHAP value computation is computationally expensive for large datasets, creating scalability challenges
- Mean-threshold selection for alike parts may fail when feature importance is uniformly distributed across all features
- The β parameter requires dataset-specific tuning, with no clear guidance for generalization across domains

## Confidence

- Mechanism 1 (SHAP-based alike parts): **High** - well-grounded in SHAP literature and validated through weight computation examples
- Mechanism 2 (FI-informed objective): **Medium** - novel modification with limited corpus validation
- Mechanism 3 (β balancing): **Medium** - empirical evidence from figures but lacks theoretical bounds

## Next Checks

1. Test alike-part selection on a dataset with uniform feature importance (e.g., Wine Quality) to verify mean-threshold doesn't degenerate to selecting all/none of features
2. Evaluate whether β values transfer between datasets with similar characteristics (e.g., try Diabetes β on Breast Cancer)
3. Implement and test missing value handling by removing random features from test instances and checking alike-part computation