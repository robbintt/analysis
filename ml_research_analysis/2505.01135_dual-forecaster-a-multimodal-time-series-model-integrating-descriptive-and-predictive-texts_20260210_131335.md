---
ver: rpa2
title: 'Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and
  Predictive Texts'
arxiv_id: '2505.01135'
source_url: https://arxiv.org/abs/2505.01135
tags:
- time
- series
- textual
- forecasting
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Dual-Forecaster, a multimodal time series
  forecasting model that integrates both descriptively historical and predictive textual
  information to improve forecasting accuracy. The model leverages three cross-modality
  alignment techniques: a historical text-time series contrastive loss, a history-oriented
  modality interaction module, and a future-oriented modality interaction module,
  all based on a PaLM backbone for advanced multimodal comprehension.'
---

# Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts

## Quick Facts
- **arXiv ID:** 2505.01135
- **Source URL:** https://arxiv.org/abs/2505.01135
- **Reference count:** 40
- **Primary result:** Outperforms state-of-the-art models on 15 datasets with significant MSE/MAE improvements using contrastive alignment and cross-attention fusion of historical/predictive text with time series

## Executive Summary
Dual-Forecaster is a multimodal time series forecasting model that integrates descriptive historical text and predictive future text with numerical time series data. The model employs three cross-modality alignment techniques: a contrastive loss to align text and time series embeddings, a history-oriented modality interaction module using sequential cross-attention, and a future-oriented modality interaction module using final cross-attention. Built on a PaLM backbone for advanced multimodal comprehension, Dual-Forecaster achieves state-of-the-art or competitive performance across fifteen diverse datasets, demonstrating significant improvements in forecasting accuracy through its ability to capture complex relationships between textual and time series data.

## Method Summary
Dual-Forecaster processes multimodal time series data through a three-stage alignment framework. The text branch uses a frozen 6-layer RoBERTa encoder with an attentional pooler to generate embeddings from historical and future text separately. The temporal branch segments time series into patches, applies linear embedding, and processes them through PaLM layers with multi-head self-attention. Three alignment techniques are then applied: (1) a contrastive loss aligns historical text and time series CLS token embeddings, (2) a history-oriented modality interaction module uses sequential cross-attention to refine time series embeddings with historical text, and (3) a future-oriented modality interaction module applies cross-attention to inject predictive text signals. The model outputs forecasts using a Student's T-distribution prediction head and is trained with a combined loss of forecast loss and contrastive loss.

## Key Results
- Dual-Forecaster outperforms or matches state-of-the-art models on 15 datasets including synthetic, captioned-public, Weather-captioned, and Time-MMD domains
- Ablation studies show contrastive loss contributes 4.3% average performance improvement, history-oriented interaction adds 0.9%, and future-oriented interaction provides 0.7%
- Case study on captioned-public datasets demonstrates model adapts to trend transitions when future text is provided, while unimodal baselines extrapolate historical patterns

## Why This Works (Mechanism)

### Mechanism 1: Pre-Alignment via Contrastive Learning
- Claim: Aligning text and time series representations into a unified embedding space before fusion improves multimodal comprehension.
- Mechanism: A contrastive loss forces the CLS token embeddings of paired historical text and time series to be similar in high-dimensional space, while pushing unpaired samples apart.
- Core assumption: Text descriptions and their corresponding time series share semantically meaningful relationships that can be captured through similarity matching in embedding space.
- Evidence anchors:
  - [section 3.2.1] "we attempt to align text features and time series embeddings into the unified high-dimensional space before fusing"
  - [section 4.5] Ablation shows 4.3% average performance reduction without contrastive loss
  - [corpus] BALM-TSF (arXiv 2509.00622) similarly emphasizes alignment challenges between textual and numerical modalities

### Mechanism 2: History-Oriented Modality Interaction
- Claim: Sequential cross-attention between historical text and time series embeddings enables distribution alignment between modalities.
- Mechanism: Uses stacked PaLM layers with MHSA (self-attention on time series) followed by MHCA (cross-attention where time series attends to historical text queries).
- Core assumption: Historical text contains descriptive signals (trends, seasonality descriptions) that directly inform the interpretation of past time series patterns.
- Evidence anchors:
  - [section 3.2.2] "sequentially process and aggregate the textual and temporal information based on the MHSA and MHCA mechanism"
  - [section 4.5] Removing this module causes ~0.9% average performance degradation

### Mechanism 3: Future-Oriented Textual Guidance
- Claim: Predictive text (future descriptions) guides the model to adapt forecasts to anticipated distribution shifts.
- Mechanism: A final MHCA layer uses aligned time series embeddings as queries and future text embeddings as keys/values, producing "textual insights-following forecasting."
- Core assumption: Future text contains actionable predictive signals (e.g., planned events, trend changes) that correlate with horizon-period dynamics.
- Evidence anchors:
  - [section 3.2.3] "ensuring textual insights-following forecasting for obtaining more reasonable forecasts"
  - [section 4.1, Figure 2] Case study shows Dual-Forecaster adapts to trend transitions when future text is provided

## Foundational Learning

- **Contrastive Learning for Cross-Modal Alignment**
  - Why needed here: The model uses contrastive loss to learn joint embeddings; without understanding why maximizing similarity for paired samples works, the alignment mechanism is opaque.
  - Quick check question: Given two modalities A and B, how does contrastive loss encourage their paired representations to cluster together while pushing unpaired samples apart?

- **Cross-Attention Mechanisms**
  - Why needed here: Both modality interaction modules rely on MHCA; understanding query/key/value roles is essential for debugging attention patterns.
  - Quick check question: In cross-attention where time series embeddings attend to text embeddings, which modality provides queries and which provides keys/values?

- **Patching and Tokenization for Time Series**
  - Why needed here: The temporal branch segments time series into patches (P consecutive non-overlapping segments) before embedding.
  - Quick check question: Why might patching improve transformer processing of time series compared to point-wise tokenization?

## Architecture Onboarding

- **Component map:**
  - Text branch: Frozen pre-trained RoBERTa (6 layers) → Attentional pooler → Text embeddings (historical and future processed separately)
  - Temporal branch: Time series patching → Linear embedding + CLS token → Unimodal encoder (n_uni PaLM layers with MHSA) → Time series embeddings
  - Modality Interaction I: Historical text embeddings + time series embeddings → n_mul PaLM layers (MHSA + MHCA) → Aligned time series embeddings
  - Modality Interaction II: Aligned embeddings + future text embeddings → MHCA layer → Final embeddings → Student's T-distribution prediction head

- **Critical path:**
  1. Contrastive loss computed from CLS tokens of historical text and time series (pre-alignment)
  2. History-oriented interaction produces aligned embeddings (sequential MHSA + MHCA)
  3. Future-oriented interaction injects predictive text signals (single MHCA)
  4. Forecast loss from Student's T-distribution head

- **Design tradeoffs:**
  - Frozen vs. fine-tuned text encoder: RoBERTa is frozen to leverage pre-trained semantics; tradeoff is inability to adapt to domain-specific time series language
  - Separate historical/future text processing: Allows distinct roles (description vs. prediction) but doubles text encoding cost
  - PaLM backbone: High-capacity multimodal comprehension; tradeoff is computational overhead (Table 6 shows speed increases from 0.022s to 0.068s/iter with full alignment)

- **Failure signatures:**
  - Contrastive loss not decreasing: Check text-time series pairing quality; mislabeled or generic annotations break alignment
  - Attention maps show uniform weights: May indicate text lacks discriminative information; inspect annotation quality
  - Forecasts ignore future text signals: Verify future text actually correlates with horizon dynamics; attention visualization should show focused attention on relevant tokens

- **First 3 experiments:**
  1. Ablation on synthetic dataset with controlled text quality: Vary annotation accuracy (perfect vs. noisy) to isolate contrastive alignment contribution; confirm MSE degradation matches paper's 4.3% finding
  2. Attention visualization on captioned-public dataset: Reproduce Figure 16 attention maps to verify future text tokens are being attended to; check if specific phrases receive high attention
  3. Zero-shot transfer test (ETTm1 → ETTm2): Validate that multimodal alignment enables cross-domain generalization; compare against paper's reported 7.9% MSE improvement over iTransformer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a more elegant and standardized time series annotator be developed to improve annotation quality on real-world datasets?
- Basis in paper: [explicit] The authors state in Appendix B that current annotation methodologies often lead to inadequate quality, explicitly calling for future work to focus on "developing a more elegant time series annotator."
- Why unresolved: Manual annotation is resource-intensive, and existing automated methods (like the IEPF algorithm used in the paper) may fail to capture complex semantic nuances or accurate future predictive insights.
- What evidence would resolve it: The development of an automated annotation framework that generates textual descriptions with higher semantic alignment to the time series than current rule-based methods, leading to lower forecast error when used with Dual-Forecaster.

### Open Question 2
- Question: To what extent can the performance of Dual-Forecaster be improved through comprehensive hyper-parameter tuning?
- Basis in paper: [explicit] Appendix B notes that "Due to resource constraints, a comprehensive hyper-parameter tuning was not performed," and the authors suggest that "the reported results of Dual-Forecaster may be sub-optimal."
- Why unresolved: The model contains numerous architectural dimensions (e.g., $n_{uni}$, $n_{mul}$, patch length $L_p$) that were not fully optimized due to computational limits.
- What evidence would resolve it: A systematic ablation study or optimization search (e.g., Bayesian optimization) over the configuration space that yields significantly lower MSE/MAE than the results reported in Table 7.

### Open Question 3
- Question: Can the Dual-Forecaster framework be effectively expanded to handle other multimodal time series analysis tasks beyond forecasting?
- Basis in paper: [explicit] Appendix B proposes that "further research should explore the potential of expanding Dual-Forecaster to encompass a broad spectrum of multimodal time series analysis capabilities."
- Why unresolved: The current architecture, loss functions (Student's T-distribution), and output projection layers are specifically engineered for regression tasks (forecasting horizons).
- What evidence would resolve it: Successful adaptation of the model's cross-modality alignment techniques to tasks like time series classification or anomaly detection, demonstrating competitive performance against specialized unimodal or multimodal baselines.

## Limitations
- The annotation quality of real-world datasets is acknowledged as inadequate, potentially limiting the model's performance on practical applications
- Computational overhead increases significantly with full alignment (0.022s to 0.068s/iter), making deployment on resource-constrained devices challenging
- Comprehensive hyperparameter tuning was not performed due to resource constraints, suggesting reported results may be sub-optimal

## Confidence
- **High:** The ablation studies (4.3% improvement from contrastive loss, 0.9% from history-oriented interaction) provide strong empirical evidence for the proposed mechanisms
- **Medium:** The architectural details for the PaLM backbone and attentional pooler are not fully specified, creating uncertainty in exact implementation
- **Low:** The model's performance on truly noisy or generic text annotations is not thoroughly evaluated, leaving questions about robustness to annotation quality

## Next Checks
1. Verify the contrastive loss effectively aligns modalities by monitoring similarity matrices between paired text and time series embeddings during training
2. Reproduce the attention visualization from Figure 16 on captioned-public datasets to confirm future text tokens receive appropriate attention weights
3. Conduct a zero-shot transfer experiment from ETTm1 to ETTm2 to validate cross-domain generalization capabilities reported in the paper