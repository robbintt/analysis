---
ver: rpa2
title: Weight Initialization and Variance Dynamics in Deep Neural Networks and Large
  Language Models
arxiv_id: '2510.09423'
source_url: https://arxiv.org/abs/2510.09423
tags:
- variance
- layers
- initialization
- relu
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theory-grounded and empirically validated
  analysis of weight initialization and variance dynamics across compact ReLU MLPs
  and GPT-2-style transformers. The author conducts a logarithmic sweep of initial
  standard deviation values, mapping vanishing and exploding gradient regimes and
  identifying a broad stable training band between 10^-2 and 10^-1.
---

# Weight Initialization and Variance Dynamics in Deep Neural Networks and Large Language Models

## Quick Facts
- arXiv ID: 2510.09423
- Source URL: https://arxiv.org/abs/2510.09423
- Authors: Yankun Han
- Reference count: 14
- One-line primary result: Kaiming (fan-in) initialization converges faster and more stably than Xavier for ReLU/GELU architectures; shallow layers expand rapidly while deeper layers adapt gradually in GPT-2-style transformers.

## Executive Summary
This paper presents a theory-grounded and empirically validated analysis of weight initialization and variance dynamics across compact ReLU MLPs and GPT-2-style transformers. The author conducts a logarithmic sweep of initial standard deviation values, mapping vanishing and exploding gradient regimes and identifying a broad stable training band between 10^-2 and 10^-1. A controlled comparison shows that Kaiming (fan-in) initialization converges faster and more stably than Xavier under ReLU, consistent with variance-preserving theory. In a from-scratch 12-layer GPT-2-style model, the paper tracks layerwise Q/K/V weight variance during pretraining, revealing depth-dependent equilibration: shallow layers expand rapidly while deeper layers change more gradually, eventually settling into narrow variance bands. Together, these results connect classic initialization principles with modern transformer behavior, demonstrating that rectifier-aware fan-in scaling preserves forward signal magnitude more faithfully, easing gradient flow and yielding quicker, more stable optimization.

## Method Summary
The paper conducts three experiments: (1) a sensitivity sweep of initial weight standard deviation on MNIST classification with a 4-layer ReLU MLP, (2) a controlled comparison of Xavier vs Kaiming initialization on a binary classification task using the UCI Wine dataset, and (3) layerwise variance tracking of Q/K/V weight matrices during pretraining of a 12-layer GPT-2-style transformer. Training uses Adam or AdamW optimizers with fixed learning rates, and initialization schemes are applied uniformly across weight matrices. Variance dynamics are monitored through checkpoints, and performance metrics include loss trajectories, accuracy, and gradient stability.

## Key Results
- A robust training band exists for initial weight standard deviation ($\sigma \in [10^{-2}, 10^{-1}]$) that prevents gradient pathologies in deep ReLU networks.
- Kaiming (fan-in) initialization provides faster and more stable convergence than Xavier initialization for ReLU-based architectures.
- Transformers exhibit depth-dependent variance equilibration, where shallow layers adapt rapidly while deep layers adjust gradually, converging to narrow variance bands.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A robust training band exists for initial weight standard deviation ($\sigma \in [10^{-2}, 10^{-1}]$) that prevents gradient pathologies in deep ReLU networks.
- **Mechanism:** If $\sigma$ is too small ($<10^{-3}$), forward activations and backward gradients vanish due to repeated multiplication by small numbers. If $\sigma$ is too large ($>1$), signal variance explodes, causing saturation or divergence. The identified band balances these forces, preserving signal magnitude across layers.
- **Core assumption:** The depth and width of the network are sufficient for signal compounding effects to manifest (tested on 4-layer MLPs and 12-layer Transformers).
- **Evidence anchors:**
  - [abstract] "identifies a robust training band... that avoids vanishing or exploding gradients."
  - [section 4.1] "A broad stable band emerges for $\sigma \in [10^{-2}, 10^{-1}]$... training is smooth."
  - [corpus] Neighbor paper "Intrinsic training dynamics of deep neural networks" supports the view that initialization determines the trajectory of high-dimensional training dynamics.
- **Break condition:** Networks with extreme depth-to-width ratios or aggressive normalization schemes (e.g., specific batch norm momentum) may shift or compress this effective band.

### Mechanism 2
- **Claim:** Kaiming (fan-in) initialization provides faster and more stable convergence than Xavier initialization for ReLU-based architectures.
- **Mechanism:** ReLU activations zero out approximately half of the inputs ($c_\phi \approx 1/2$), reducing the effective variance of the signal. Kaiming initialization compensates for this by scaling weights by $\sqrt{2/n_{in}}$, whereas Xavier assumes linear linearity ($c_\phi \approx 1$) and under-scales the weights, dampening the gradient flow.
- **Core assumption:** The activation function kills roughly half the units (valid for ReLU/GELU); assumption may weaken for other activations.
- **Evidence anchors:**
  - [abstract] "Kaiming (fan-in) initialization converges faster and more stably than Xavier under ReLU."
  - [section 3] "Typical cases: ReLU... $\sigma^2_W \approx 2/n_{in}$ (He/Kaiming)."
  - [corpus] Neighbor "Optimized Weight Initialization on the Stiefel Manifold..." explicitly identifies improper initialization as a cause of "dying ReLU" and gradient instability.
- **Break condition:** If using activations that are not rectified linear units (e.g., Tanh, Sigmoid) or if the network utilizes specific skip-connection scaling that artificially boosts signal, Xavier or LeCun may be preferable or equivalent.

### Mechanism 3
- **Claim:** Transformers exhibit depth-dependent variance equilibration, where shallow layers adapt rapidly while deeper layers adjust gradually, converging to narrow variance bands.
- **Mechanism:** Shallow layers receive high-SNR gradients from the input data and expand quickly to model low-level statistics. Deeper layers are constrained by longer residual paths and lower gradient SNR, causing slower adjustments. LayerNorm and AdamW further dampen variance drift in deeper sections.
- **Core assumption:** The optimizer (AdamW) and architecture (Pre-LN GPT-2) allow for gradient flow that differentiates by depth.
- **Evidence anchors:**
  - [abstract] "shallow layers expand rapidly while deeper layers adjust more gradually."
  - [section 4.3] "lower layers show rapid and pronounced expansion... higher layers expand much more slowly."
  - [corpus] Neighbor "Variance Control via Weight Rescaling in LLM Pre-training" corroborates that variance management is critical during LLM training, though specific depth-dependent rates are detailed primarily in the source text.
- **Break condition:** Architectures without residual connections or with fundamentally different norm placements (e.g., Post-LN without correction) may fail to exhibit this gradual deep-layer adaptation, risking instability instead.

## Foundational Learning

- **Concept: Variance Preservation**
  - **Why needed here:** The entire theoretical framework of the paper rests on maintaining activation and gradient variance $\text{Var}[x^l] \approx \text{Var}[x^{l-1}]$ to ensure signal flows through depth without vanishing or exploding.
  - **Quick check question:** If a network has 100 layers and weights are initialized with $\sigma=10^{-4}$, what happens to the gradient magnitude by layer 50?

- **Concept: Rectifier Non-linearity (ReLU/GELU)**
  - **Why needed here:** Unlike sigmoid/tanh, rectifiers output zero for negative inputs. This kills signal variance, necessitating the "factor of two" gain in Kaiming initialization ($\sqrt{2}$) which distinguishes it from Xavier.
  - **Quick check question:** Why does Xavier initialization, designed for tanh, underperform in a ReLU network?

- **Concept: Fan-in vs. Fan-out**
  - **Why needed here:** The paper specifies "fan-in" mode for Kaiming initialization. Fan-in looks at the number of input connections to a neuron to determine variance, prioritizing forward signal stability.
  - **Quick check question:** When initializing a weight matrix of shape $(4096, 1024)$, which dimension is used for calculating "fan-in"?

## Architecture Onboarding

- **Component map:** Input/Embedding -> Attention (Q/K/V) -> MLP (GELU) -> LayerNorm
- **Critical path:**
  1. **Initialization:** Select $\sigma \in [10^{-2}, 10^{-1}]$. Use Kaiming (fan-in) for ReLU/GELU blocks.
  2. **Early Training (Warmup):** Monitor "shallow layer" weight std expansion.
  3. **Stabilization:** Observe convergence of deep layers into equilibrium bands.

- **Design tradeoffs:**
  - **Xavier vs. Kaiming:** Trade stability for convergence speed. Kaiming is empirically faster for rectifiers (Evidence: E2).
  - **Fixed vs. Learned Init:** The paper suggests future work on "adaptive, depth-aware initialization" (Section 5), trading complexity for potentially reduced early transients.
  - **Assumption:** The paper assumes standard AdamW settings; extreme learning rates might violate the "stable band" boundaries.

- **Failure signatures:**
  - **Vanishing Gradients:** Loss remains flat, shallow layer weights do not expand (Init $\sigma$ too small).
  - **Exploding Gradients:** Loss spikes or NaNs, weight std grows uncontrollably (Init $\sigma$ too large).
  - **Stagnation:** Deep layers fail to adapt (check residual connections or gradient clipping).

- **First 3 experiments:**
  1. **Sensitivity Sweep:** Replicate E1 on your specific architecture. Log sweep $\sigma$ from $10^{-4}$ to $10^{0}$ to verify the stability band for your model depth.
  2. **Initializer Bake-off:** Compare Kaiming vs. Xavier on a shallow classifier (like E2) to verify the "factor of two" advantage in your software stack.
  3. **Layerwise Monitor:** Instrument a standard training run to plot $\sigma$ of Q/K/V weights per layer over time. Look for the "rapid expansion" in shallow layers vs "gradual" deep layer adaptation described in E3.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the observed depth-dependent variance equilibration persist in significantly larger language models and datasets?
  - **Basis in paper:** [explicit] The Outlook section explicitly asks to "evaluate whether the depth-dependent equilibration persists under larger models and datasets."
  - **Why unresolved:** The empirical analysis in Experiment E3 is restricted to a 12-layer GPT-2-style model, leaving the scaling behavior of larger architectures unverified.
  - **What evidence would resolve it:** Replicating the layerwise Q/K/V variance tracking on models exceeding 1B parameters or 100+ layers.

- **Open Question 2:** How do optimizer hyperparameters like warmup and weight decay interact with initialization to maintain variance stability?
  - **Basis in paper:** [explicit] The authors identify "optimizer and schedule coupling" as a necessary extension to maintain stable variance across different batch sizes.
  - **Why unresolved:** The experiments fix the optimizer (AdamW) and learning rate schedule, isolating initialization effects but ignoring their coupling with optimization dynamics.
  - **What evidence would resolve it:** A factorial study varying warmup length and weight decay strength alongside initialization scales to observe changes in variance trajectories.

- **Open Question 3:** Can adaptive, depth-aware initialization schemes reduce early training transients compared to fixed schemes?
  - **Basis in paper:** [explicit] The Outlook section proposes "adaptive, depth-aware initialization" where shallow layers start closer to their eventual variance levels.
  - **Why unresolved:** The paper analyzes fixed initialization schemes (Xavier, Kaiming) and observes early transients; it does not test if learned initial scales improve this.
  - **What evidence would resolve it:** Implementing per-layer learnable initialization parameters and measuring the reduction in early variance fluctuation and convergence speed.

## Limitations
- The stable initialization band (10^-2 to 10^-1) was identified primarily through MNIST classification with ReLU MLPs; extending this to other datasets, architectures (CNNs, ResNets), or tasks (regression, reinforcement learning) remains unvalidated.
- The Kaiming vs. Xavier comparison, while theoretically sound, was demonstrated on a small UCI dataset rather than large-scale vision or language tasks where optimization dynamics may differ.
- The GPT-2 variance tracking, though showing clear depth-dependent equilibration, lacks quantitative benchmarks against baseline transformers and does not explore alternative initialization schemes or different transformer variants.

## Confidence
- **High:** The variance preservation framework is mathematically rigorous and consistent with prior literature on rectifier networks.
- **Medium:** Results are reproducible in controlled settings but may shift with architectural modifications, optimization hyperparameters, or data distribution changes.
- **Medium-Lower:** Limited ablation studies and absence of cross-model validation for GPT-2-specific variance dynamics.

## Next Checks
1. **Architecture Transfer Test:** Replicate the std sweep (E1) on CIFAR-10 with a ResNet-18 to verify the stable band holds for residual connections and convolutional layers.
2. **Task Generalization Test:** Apply Kaiming vs. Xavier initialization comparison (E2) on ImageNet classification to confirm faster convergence claims scale to large vision datasets.
3. **Transformer Variant Test:** Track variance dynamics in a BERT-style encoder (pre-LN with bidirectional attention) to determine if depth-dependent equilibration generalizes beyond GPT-2 decoder-only models.