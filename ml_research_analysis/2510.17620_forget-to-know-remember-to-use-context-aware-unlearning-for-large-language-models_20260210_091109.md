---
ver: rpa2
title: 'Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language
  Models'
arxiv_id: '2510.17620'
source_url: https://arxiv.org/abs/2510.17620
tags:
- unlearning
- contextual
- utility
- context
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language model unlearning methods effectively remove targeted\
  \ knowledge but degrade the model\u2019s ability to use that knowledge when provided\
  \ in context, harming contextual utility. This work introduces context-aware unlearning,\
  \ which augments existing objectives with a KL-divergence term that aligns contextual\
  \ responses with the original model, preserving contextual utility."
---

# Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2510.17620
- Source URL: https://arxiv.org/abs/2510.17620
- Reference count: 15
- Key outcome: Context-aware unlearning restores contextual QA performance to near-perfect levels (LLM-Judge ≥ 0.95) without sacrificing forgetting effectiveness or overall model utility

## Executive Summary
Standard unlearning methods effectively remove targeted knowledge from LLMs but degrade the model's ability to use that knowledge when provided in context, harming contextual utility. This work introduces context-aware unlearning, which augments existing objectives with a KL-divergence term that aligns contextual responses with the original model, preserving contextual utility. Experiments on Gemma-2B-IT and Qwen3-8B show that this approach restores contextual QA performance to near-perfect levels without sacrificing forgetting effectiveness or overall model utility, addressing a critical gap in practical LLM unlearning.

## Method Summary
The method introduces a three-term objective J(w) = -λ_f L_f + λ_r L_r + λ_c C(S_ctx_f, w) where C computes KL-divergence between the unlearned model's conditional distribution and the frozen original model when context is provided. This KL term anchors the unlearned model's contextual predictions to the original model's behavior, preserving the ability to use forgotten knowledge when explicitly provided in prompts. The approach is method-agnostic and integrates with existing unlearning techniques like NPO, RMU, and UNDIAL by simply adding the context preservation term to their objectives.

## Key Results
- Context-aware unlearning restores Contextual QA LLM-Judge scores to ≥0.95 across all tested methods (NPO, RMU, UNDIAL) on both Gemma-2B-IT and Qwen3-8B
- The approach preserves forgetting effectiveness, maintaining Direct QA scores at ≤0.05 while restoring contextual utility
- Model utility on retain sets remains stable or improves slightly compared to vanilla unlearning methods
- λ_c values are relatively insensitive across methods (NPO: 0.5-2.0, RMU: 0.01-0.5, UNDIAL: 0.5-1.0)

## Why This Works (Mechanism)

### Mechanism 1: KL-Divergence Context Alignment
Adding a KL-divergence term between the unlearned model and original model on contextual queries preserves contextual utility without degrading forgetting. The context term C(S_ctx_f, w) = (1/|S_ctx_f|) Σ KL(p_w(·|q,c) || p_orig(·|q,c)) explicitly anchors the unlearned model's conditional distribution to the original model when context c is provided, counteracting the suppression cascade from the forget term.

### Mechanism 2: Gradient Suppression Propagation in Standard Unlearning
Standard unlearning objectives corrupt token representations beyond output logits, causing downstream processing failures when forgotten tokens reappear as context. Forget terms reshape internal representations, not just final predictions. When these tokens appear in context, their corrupted embeddings reduce grounding ability.

### Mechanism 3: Modular Objective Augmentation
The context-aware term is method-agnostic and can be integrated into any two-term unlearning objective with minimal modification. The three-term structure J(w) = -λ_f L_f + λ_r L_r + λ_c C(S_ctx_f, w) treats contextual preservation as orthogonal to forgetting/retention, allowing plug-in integration without altering core unlearning logic.

## Foundational Learning

**Concept: KL-Divergence as Distributional Regularizer**
Why needed here: The context term uses KL-divergence to keep the unlearned model's contextual predictions aligned with the original; understanding why KL (vs. other divergences) matters for implementation and debugging
Quick check question: If KL-divergence between contextual distributions is near-zero but contextual utility is still poor, what does this suggest about the original model's grounding behavior?

**Concept: In-Context Learning vs. Parametric Knowledge**
Why needed here: The paper distinguishes between "recall from memory" (parametric) and "use when provided" (in-context); conflating these leads to misdiagnosing failure modes
Quick check question: A model correctly answers from context but also hallucinates the same answer without context—which capability is preserved and which is failed?

**Concept: Gradient Ascent/NPO/RMU Trade-offs**
Why needed here: Different unlearning methods have different failure signatures for contextual utility (e.g., RMU collapses to near-zero, NPO degrades but doesn't collapse); method selection depends on deployment constraints
Quick check question: If deployment requires maximal forgetting with minimal training time, which method's contextual degradation pattern is most acceptable?

## Architecture Onboarding

**Component map:**
Forget set S_f → forget term L_f (method-specific: NPO/RMU/GradAscent)
Retain set S_r → retain term L_r (standard NLL or representation alignment)
Context set S_ctx_f → context term C (KL-divergence to frozen original model p_orig)
Hyperparameters: λ_f, λ_r, λ_c balance the three objectives
Frozen reference: p_orig for KL computation (do not update during unlearning)

**Critical path:**
1. Identify forget set S_f and construct context pairs S_ctx_f = {(q, c, a)} where c contains ground-truth answer
2. Initialize unlearned model from original model (p_w = p_orig at start)
3. For each batch: compute L_f on S_f, L_r on S_r (if used), and KL term on S_ctx_f
4. Backpropagate combined loss J(w); monitor Direct QA (forgetting), Contextual QA (contextual utility), and retain-set performance
5. Stop when Direct QA converges (within tolerance) and Contextual QA/utility stabilize near original levels

**Design tradeoffs:**
Higher λ_c → better contextual utility but risk of weakening forgetting if λ_c >> λ_f
RMU provides strongest forgetting but worst contextual utility without augmentation; NPO/UNDIAL offer gentler trade-offs
UNDIAL's re-labeling strategy preserves context better inherently but under-unlearning risk (LLM-Judge remains above retrain baseline)
Paper finds λ_c relatively insensitive (Figure 9), but values differ across methods (NPO: 0.5-2.0, RMU: 0.01-0.5, UNDIAL: 0.5-1.0)

**Failure signatures:**
Contextual QA near zero but Direct QA also near zero → over-forgetting (λ_f too high or λ_c too low)
Contextual QA restored but Direct QA rises → under-forgetting (λ_c too high relative to λ_f)
Model utility drops → retain term insufficient or forget term affecting non-target representations
RMU outputs incoherent text in Direct QA → representation corruption expected; check that context-aware variant recovers contextual behavior

**First 3 experiments:**
1. Reproduce vanilla RMU on TOFU 5% forget set; confirm Direct QA LLM-Judge near 0 and Contextual QA near 0 (establishing baseline failure mode)
2. Add context term with λ_c = 0.5; verify Contextual QA LLM-Judge rises to ≥0.95 while Direct QA remains ≤0.05
3. Ablate λ_c across {0.1, 0.5, 1.0, 2.0} on a held-out forget set variant; confirm insensitivity pattern and identify optimal range for deployment method

## Open Questions the Paper Calls Out

**Open Question 1**
Question: Does context-aware unlearning generalize to high-stakes real-world unlearning tasks, such as removing hazardous knowledge or PII, as effectively as it does for the fictitious author profiles in the TOFU benchmark?
Basis in paper: [inferred] The paper relies exclusively on the TOFU benchmark (fictitious authors) to isolate unlearning effects from pre-training knowledge, leaving real-world efficacy unverified
Why unresolved: The dynamics of forgetting fictitious versus factual pre-trained knowledge may differ significantly
What evidence would resolve it: Evaluation of the method on benchmarks like WMDP (hazardous knowledge) or Enron (PII) to see if contextual utility remains high without compromising safety or privacy

**Open Question 2**
Question: Does preserving the model's ability to process forgotten information in context inadvertently lower the barrier for adversarial attacks to recover the forgotten knowledge?
Basis in paper: [inferred] While the paper restores the ability to use context, it does not evaluate whether this "contextual utility" creates a security vulnerability that allows adversaries to "reverse" the unlearning via specific prompts
Why unresolved: Optimizing for contextual alignment might maintain latent representations that could be exploited, a risk not assessed in the utility/forgetting metrics
What evidence would resolve it: Testing the context-aware unlearned models against extraction attacks (e.g., prior-work "ununlearning" techniques) to measure recovery rates

**Open Question 3**
Question: How robust is the KL-alignment method when the provided context is noisy, incomplete, or contains distractors, compared to the clean ground-truth context used in training?
Basis in paper: [inferred] The context term (Eq. 2) aligns predictions using $S^{ctx}_f$, which contains ground-truth context. The paper briefly tests "paraphrased" context but does not explore noisy or misleading contexts typical in real-world RAG
Why unresolved: The method may overfit to the specific ground-truth context provided during unlearning, failing to generalize to messy retrieval inputs
What evidence would resolve it: Experiments using retrieval-augmented settings where the retrieved context includes irrelevant or contradictory information

## Limitations
- Results are constrained to the TOFU benchmark with fictitious author profiles, limiting generalizability to real-world unlearning scenarios involving sensitive information or copyright-protected content
- The LLM-Judge evaluation introduces subjectivity through the chosen judge model (Claude 3.5 Sonnet v2) and binary scoring threshold
- The computational overhead of computing KL-divergence with a frozen reference model is not quantified

## Confidence

**High Confidence**: The core finding that standard unlearning methods degrade contextual utility while context-aware unlearning restores it is well-supported by quantitative metrics (ROUGE-L, LLM-Judge) and qualitative outputs.

**Medium Confidence**: The claim that KL-divergence specifically is the optimal alignment mechanism (vs. other distributional divergences or simpler regularizers) is plausible but not rigorously compared against alternatives.

**Low Confidence**: The paper's assertions about representation-level corruption causing contextual utility failure are largely theoretical. The generalization to larger models (e.g., 70B+ parameters) and different domains remains unproven.

## Next Checks

1. **Ablation of KL-Divergence Alternatives**: Replace the KL-divergence term with alternative distributional alignment methods (e.g., L2 distance between contextual logits, cross-entropy to original model's context outputs, or feature-level alignment in frozen layers) and compare contextual utility preservation against the proposed method.

2. **Real-World Domain Transfer**: Apply context-aware unlearning to a real-world unlearning scenario (e.g., removing copyrighted training examples or sensitive personal information) rather than the synthetic TOFU benchmark. Evaluate both forgetting effectiveness and contextual utility on domain-appropriate QA pairs.

3. **Computation Overhead Analysis**: Quantify the additional computational cost of maintaining and querying the frozen reference model p_orig for KL-divergence computation. Measure wall-clock time per training step, memory overhead for storing reference activations, and scalability limits as context length or model size increases.