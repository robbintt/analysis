---
ver: rpa2
title: 'ART: Adaptive Resampling-based Training for Imbalanced Classification'
arxiv_id: '2509.00955'
source_url: https://arxiv.org/abs/2509.00955
tags:
- class
- training
- performance
- learning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ART addresses class imbalance in supervised classification by introducing
  a dynamic, class-wise adaptive resampling method. Unlike static resampling techniques,
  ART periodically updates the training data distribution based on class-wise macro
  F1 scores computed on a validation set, thereby shifting focus toward underperforming
  classes.
---

# ART: Adaptive Resampling-based Training for Imbalanced Classification

## Quick Facts
- arXiv ID: 2509.00955
- Source URL: https://arxiv.org/abs/2509.00955
- Authors: Arjun Basandrai; Shourya Jain; K. Ilanthenral
- Reference count: 40
- Key outcome: ART improves macro F1 by an average of 2.64 percentage points on tabular datasets compared to imbalanced training, with statistically significant gains (p < 0.05) in most settings.

## Executive Summary
ART addresses class imbalance in supervised classification through a dynamic, class-wise adaptive resampling method. Unlike static resampling techniques, ART periodically updates the training data distribution based on class-wise macro F1 scores computed on a validation set, shifting focus toward underperforming classes. This adaptive mechanism allows the model to respond to changing class difficulty during training without relying on complex heuristics or instance-level adjustments. Evaluated across five diverse datasets spanning tabular, image, and text modalities, ART consistently outperforms existing resampling and loss-based methods, demonstrating its robustness and effectiveness as a reliable choice for imbalanced classification tasks.

## Method Summary
ART introduces a periodic feedback loop that computes class-wise macro F1 scores on a validation set every b_f epochs. It uses these scores to calculate difficulty weights (1 - F1), normalizes them, and blends them with the original class priors using a convex combination controlled by parameter c. This blended distribution guides a weighted sampler that draws mini-batches during training, effectively increasing the frequency of underrepresented or underperforming classes. The method operates independently of the specific model architecture or loss function, requiring only an epoch-based training loop and a validation set for periodic updates.

## Key Results
- ART improves macro F1 by an average of 2.64 percentage points on tabular datasets compared to imbalanced training
- Statistically significant improvements (p < 0.05) achieved across most evaluated settings
- Consistent performance gains observed across five diverse datasets spanning tabular, image, and text modalities
- Outperforms existing resampling and loss-based methods in all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1: Performance-Feedback Resampling
ART dynamically shifts the sampling distribution toward classes with lower validation F1 scores, improving macro-metric performance compared to static sampling. The algorithm computes class-wise difficulty scores (1 - F1) every b_f epochs, normalizes them to create sampling weights, and increases the probability of selecting underperforming classes. This assumes that low F1 scores correlate with learning difficulty that can be mitigated by increased observation frequency rather than irreducible noise.

### Mechanism 2: Class-Level Aggregation for Stability
By basing resampling decisions on aggregate class metrics (Macro F1) rather than instance-level losses, ART reduces noise sensitivity inherent in hard-mining methods. This class-level focus smooths the optimization landscape by treating all instances within a struggling class as equally deserving of attention, avoiding the instability associated with instance-level difficulty estimation.

### Mechanism 3: Prior Blending for Robustness
ART interpolates between the adaptive sampling distribution and original data priors using a convex combination controlled by parameter c. This prevents training instability during early epochs or sudden performance drops by ensuring the model never completely abandons the majority class distribution, maintaining useful signals from the original data distribution.

## Foundational Learning

**Concept: Macro F1 Score**
- Why needed here: This is the feedback signal for the control loop. Understanding that F1 balances precision and recall is crucial, as optimizing for it naturally handles the trade-off between false positives and false negatives better than accuracy.
- Quick check question: If a model predicts the majority class 100% of the time, what is the Macro F1 score for the minority class? (Answer: 0)

**Concept: Convex Interpolation**
- Why needed here: The paper uses a blending constant c ∈ [0,1] to merge distributions. Understanding convex combinations is necessary to tune how "aggressive" the resampling is vs. how "conservative" the original distribution is.
- Quick check question: If c=0.8 and the adaptive weight for a class is 0.1 while its prior is 0.5, what is the final sampling probability? (Answer: 0.8(0.5) + 0.2(0.1) = 0.42)

**Concept: Resampling vs. Reweighting**
- Why needed here: ART modifies the data distribution (resampling) rather than the loss function (reweighting). Distinguishing these is key to implementation; ART changes what the model "sees" in the batch, not how the loss is calculated.
- Quick check question: Does ART require a custom loss function implementation? (Answer: No, it requires a custom data sampler)

## Architecture Onboarding

**Component map:** Validation Evaluator -> Distribution Manager -> Weighted Sampler -> Trainer

**Critical path:** Train Epoch → Validate & Calc F1 → Update Sampler Weights → Next Epoch
*Note: The "Update" step is the critical integration point.*

**Design tradeoffs:**
- Latency vs. Responsiveness: Low b_f (update every epoch) reacts fast but may oscillate. High b_f is stable but slow to correct imbalance.
- Metric Choice: The paper uses Macro F1. Switching to Recall would aggressively target false negatives but might harm precision.

**Failure signatures:**
- Oscillation: Macro F1 jumps up and down epoch-to-epoch. (Fix: Increase blending constant c or frequency b_f)
- Majority Collapse: Accuracy on majority class drops to near zero. (Fix: Increase c to respect priors more)

**First 3 experiments:**
1. **Sanity Check:** Run ART vs. Baseline on a trivially imbalanced dataset (e.g., MNIST-LT with low imbalance ratio) to verify the feedback loop updates weights correctly.
2. **Hyperparameter Scan (c):** Sweep the blending constant c from 0.0 to 1.0 on the validation set to find the "safe zone" where training is stable but adaptive.
3. **Frequency Test (b_f):** Test b_f=1 (every epoch) vs b_f=10 to determine how quickly the specific dataset's class difficulties evolve.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating advanced synthetic generation techniques (e.g., SMOTE, ADASYN) into the dynamic resampling loop improve performance over the current simple random oversampling approach? The authors suggest trying various methods for dynamic class-level rebalancing could provide better performance. This remains unresolved as the current implementation relies on basic random duplication and removal of samples without testing complex sample generation.

### Open Question 2
Is ART applicable to non-iterative learners (e.g., decision trees) and large-scale architectures like Large Language Models (LLMs)? The authors list "Broader model classes" as future work, planning to evaluate ART on large language models and standard logistic regression pipelines. This is unresolved because ART assumes an epoch-based training loop unsuitable for non-iterative learners and has only validated on lightweight MLPs and CNNs.

### Open Question 3
Can ART effectively adapt to high-rate time-series data where class difficulty fluctuates rapidly? Future work suggests applying ART to imbalanced audio classification to test its adaptability to high-rate time-series data where class difficulty changes rapidly during training. This is unresolved as ART currently updates weights at fixed intervals, and it's unclear if this discrete update frequency is fast enough to track rapidly shifting difficulties in audio or high-frequency time-series tasks.

### Open Question 4
Does the computational overhead of calculating class-wise F1 scores on a validation set become prohibitive for massive datasets? The authors note that computing class-wise F1 every b_f epochs incurs overhead proportional to the validation-set size, which may be non-trivial for very large datasets. This is unresolved as the paper evaluates on relatively small datasets, and the trade-off between evaluation cost and adaptive resampling benefit hasn't been quantified for big data regimes.

## Limitations

- The effectiveness of class-wise difficulty scores assumes low F1 reliably indicates classes that can benefit from increased sampling, which may not hold if minority classes lack feature representation
- The blending constant c is described as "tuned" without disclosing the procedure, raising reproducibility concerns
- Performance could degrade if the validation set is not representative of the test distribution, potentially causing overfitting to validation-specific artifacts

## Confidence

**High Confidence:** ART consistently improves macro F1 across multiple datasets compared to baseline imbalanced training (p < 0.05 in most cases).

**Medium Confidence:** The claim that class-level aggregation reduces noise sensitivity compared to instance-level methods is supported by theoretical reasoning but lacks direct empirical comparison within the paper.

**Medium Confidence:** The assertion that prior blending prevents training instability is based on ablation studies, but the specific impact of different c values across diverse datasets is not fully characterized.

## Next Checks

1. **Dataset Shift Test:** Evaluate ART's performance when the validation set distribution differs from the test set distribution to assess robustness to distribution mismatch.

2. **Baseline Comparison:** Implement a direct comparison between ART and instance-level difficulty-based resampling methods on the same datasets to empirically verify the noise reduction claim.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary the blending constant c and update frequency b_f across all datasets to map the stability-performance tradeoff space and identify universal best practices.