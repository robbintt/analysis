---
ver: rpa2
title: 'Typhoon OCR: Open Vision-Language Model For Thai Document Extraction'
arxiv_id: '2601.14722'
source_url: https://arxiv.org/abs/2601.14722
tags:
- thai
- document
- typhoon
- documents
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Typhoon OCR, an open vision-language model
  for Thai and English document extraction. It addresses the challenge of limited
  high-quality training data for Thai, which has a complex script and lacks explicit
  word boundaries.
---

# Typhoon OCR: Open Vision-Language Model For Thai Document Extraction

## Quick Facts
- arXiv ID: 2601.14722
- Source URL: https://arxiv.org/abs/2601.14722
- Reference count: 3
- Primary result: Open vision-language model for Thai/English document extraction outperforms larger proprietary models like GPT-4o on Thai documents while being compact (2B parameters)

## Executive Summary
Typhoon OCR is an open vision-language model for Thai and English document extraction that addresses the challenge of limited high-quality training data for Thai, which has a complex script and lacks explicit word boundaries. The model is fine-tuned from Qwen2.5-VL using a multi-stage data pipeline combining traditional OCR, VLM-based restructuring, and synthetic data. Typhoon OCR operates in two modes: Default Mode for unstructured documents and Structure Mode for complex layouts. Comprehensive evaluations show that Typhoon OCR achieves performance comparable to or exceeding larger proprietary models like GPT-4o and Gemini 2.5 Flash on Thai financial reports, government forms, and books. The latest iteration, Typhoon OCR V1.5, is a compact 2B parameter model that reduces reliance on metadata and simplifies deployment while maintaining high accuracy.

## Method Summary
Typhoon OCR uses a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data to address the scarcity of high-quality Thai document annotations. The model is fine-tuned from vision-language backbones (Qwen2.5-VL for V1, Qwen3-VL for V1.5) using full-parameter supervised fine-tuning. V1.5 applies quantization-aware training for efficient low-precision inference. The training corpus includes a mix of real-world Thai documents, synthetic data, and open datasets, with images resized to 1,800px width and maximum sequence lengths of 16,384-17,000 tokens. The model operates in two modes: Default Mode for unstructured documents and Structure Mode for complex layouts with tables and forms.

## Key Results
- Typhoon OCR V1.5 (2B parameters) outperforms larger proprietary models like GPT-4o and Gemini 2.5 Flash on Thai document extraction tasks
- V1.5 reduces model size from 7B to 2B parameters while improving average performance, suggesting data quality is more important than model scale
- The model achieves high accuracy on Thai financial reports, government forms, books, and infographics, though performance degrades on low-resolution, blurry, or occluded inputs

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual VLMs with language-specific supervision yields substantial gains for low-resource OCR without training from scratch. Transfer learning preserves general visual-linguistic representations from pretraining while adapting to Thai script properties (stacked diacritics, implicit word boundaries) through domain-aligned supervision. The core assumption is that the pretrained backbone has sufficient multilingual capacity to specialize further, with adaptation quality depending on supervision quality rather than model scale alone. Evidence includes the paper's claim that the model is fine-tuned using a Thai-focused training dataset, and prior research suggesting fine-tuning large pretrained models with language-specific supervision can yield substantial gains. The break condition is if the pretrained backbone lacks sufficient Thai script exposure, fine-tuning may fail to overcome fundamental recognition gaps.

### Mechanism 2
Multi-stage data pipeline mitigates noise from automated annotation while enabling scalable dataset construction. The pipeline progressively refines supervision: Stage 1 extracts raw text via traditional OCR; Stage 2 reorganizes using VLMs with structured prompts; Stage 3 applies automated quality control via agent-based consistency checks; Stage 4 performs human verification on samples. The core assumption is that agentic QC can reliably detect structural inconsistencies and misalignment between visual input and generated annotations. Evidence includes the paper's description of the 4-stage pipeline combining traditional OCR, VLM-based restructuring, and curated synthetic data. The break condition is if agentic QC false-negative rate is high, systematic annotation errors may propagate to training.

### Mechanism 3
Synthetic data augmentation compensates for scarce annotated Thai documents with complex layouts, equations, and charts. The pipeline generates controlled-layout documents by sampling Thai vocabulary, combining with visual elements and mathematical expressions, then applying Augraphy distortions to simulate real-world acquisition artifacts. The core assumption is that synthetic documents sufficiently approximate real-world visual and linguistic variation to improve generalization. Evidence includes the paper's statement that synthetic documents are generated to compensate for the scarcity of annotated Thai documents containing complex layouts, and the intentional maintenance of a substantial synthetic portion (37.6%) in V1.5. The break condition is if synthetic-to-real distribution shift is large, model may overfit to synthetic artifacts without real-world transfer.

## Foundational Learning

- **Vision-Language Model (VLM) architectures**: Why needed - Typhoon OCR builds on Qwen2.5-VL/Qwen3-VL backbones; understanding vision encoder + language decoder integration is prerequisite. Quick check - Can you explain how visual tokens are aligned with language model embeddings in multimodal transformers?

- **OCR evaluation metrics (BLEU, ROUGE-L, Levenshtein)**: Why needed - Paper uses these metrics to compare against GPT-4o/Gemini; understanding what each captures is essential for interpreting results. Quick check - Why might BLEU be insufficient alone for evaluating structured document extraction?

- **Quantization-aware training**: Why needed - V1.5 applies quantization-aware training for efficient low-precision inference. Quick check - What is the tradeoff between quantization-aware training overhead and inference efficiency gains?

## Architecture Onboarding

- **Component map**: Image preprocessing (resize to 1800px) -> Qwen2.5-VL/Qwen3-VL backbone -> SFT training (olmOCR/Axolotl) -> 4-stage data pipeline (OCR -> VLM restructuring -> agentic QC -> human review)

- **Critical path**: 1) Data curation quality (Stage 3-4 QC) → supervision signal quality, 2) Resolution standardization (1800px) → training stability, 3) Mode selection (V1) or unified mode (V1.5) → output format appropriateness

- **Design tradeoffs**: PDF metadata vs. image-only input (PDF provides layout cues but increases latency; V1.5 removes this dependency), Dual-mode vs. unified (V1's Default/Structure modes increase flexibility but add user complexity; V1.5 simplifies to single mode), Model size vs. performance (V1.5 2B outperforms V1 7B on average, suggesting data quality > scale)

- **Failure signatures**: Degraded inputs (low-resolution, blur, occlusion) → performance drop noted in conclusion, Visually heterogeneous documents (infographics, handwriting) → higher Levenshtein distance vs. proprietary models, Long documents with complex figures → figure understanding remains limitation

- **First 3 experiments**: 1) Baseline reproduction: Fine-tune Qwen2.5-VL-3B on Thai financial reports subset; measure BLEU/ROUGE-L vs. paper-reported scores, 2) Ablation: Train without synthetic data (remove 37.6%); assess impact on equation/chart recognition, 3) Robustness test: Evaluate on degraded inputs (apply Augraphy distortions to test set); quantify performance degradation

## Open Questions the Paper Calls Out

- How can Typhoon OCR's performance be stabilized on severely degraded inputs, such as low-resolution images, motion blur, and occlusions? The authors state in the Conclusion that performance degrades under these conditions and suggest a need for "improved data recipes or explicit modeling of noise and capture artifacts."

- How does Typhoon OCR perform on standardized academic benchmarks relative to proprietary models? The authors note that while they used an in-house corpus, "broader assessment on academic benchmarks, such as ThaiOCRBench... is a natural next step."

- Can the current architecture be extended to support higher-level reasoning tasks beyond text extraction? The Conclusion states that the current series "focuses on document extraction and does not explicitly support higher-level reasoning tasks," identifying this as a direction for future research.

## Limitations

- Critical implementation details remain undisclosed: exact SFT hyperparameters, Stage 2 VLM restructuring and Stage 3 agentic QC prompts, and proprietary Thai financial reports/government forms are inaccessible, blocking exact reproduction

- Synthetic data contribution (37.6% of V1.5 corpus) is asserted to be beneficial, but no ablation study validates this claim; it's unclear whether synthetic augmentation meaningfully improves Thai OCR over smaller curated real datasets

- While Typhoon OCR outperforms larger models on some Thai document types, it still lags on visually heterogeneous documents (infographics, handwriting) and figure understanding, suggesting the approach may not generalize to all real-world document extraction scenarios

## Confidence

- **High Confidence**: Fine-tuning multilingual VLMs with language-specific supervision yields substantial gains for low-resource OCR. This mechanism is well-established in the literature and directly supported by experimental results showing Typhoon OCR's strong performance.

- **Medium Confidence**: Multi-stage data pipeline with agentic QC mitigates annotation noise and enables scalable dataset construction. The pipeline design is plausible and supported by general data quality literature, but specific prompts and QC agent design are not disclosed.

- **Low Confidence**: Synthetic data augmentation compensates for scarce annotated Thai documents. While the paper maintains a substantial synthetic portion, no comparative study proves synthetic data is more effective than curated real data for this task.

## Next Checks

1. **SFT Hyperparameter Sensitivity**: Systematically vary learning rate, batch size, and scheduler in fine-tuning Typhoon OCR; measure impact on BLEU/ROUGE-L scores on Thai financial reports to identify optimal settings.

2. **Synthetic Data Ablation**: Train Typhoon OCR V1.5 without synthetic data (use only V1 corpus and DocLayNet); compare performance on equation/chart recognition tasks to quantify synthetic data's contribution.

3. **Degraded Input Robustness**: Apply Augraphy distortions (noise, blur, occlusion) to the test set; evaluate performance degradation and inspect failure cases visually to validate robustness claims.