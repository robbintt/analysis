---
ver: rpa2
title: Telling Human and Machine Handwriting Apart
arxiv_id: '2601.11700'
source_url: https://arxiv.org/abs/2601.11700
tags:
- handwriting
- data
- human
- classifier
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of detecting whether handwritten
  input is produced by a human or synthetically generated, a critical task for preventing
  fraud in biometric authentication systems. They develop a classifier using a shallow
  recurrent neural network (RNN) with gated recurrent units (GRU) that takes raw trajectory
  data as input rather than engineered features.
---

# Telling Human and Machine Handwriting Apart

## Quick Facts
- arXiv ID: 2601.11700
- Source URL: https://arxiv.org/abs/2601.11700
- Reference count: 40
- Key result: GRU-based RNN achieves 98.3% AUC and 1.4% EER in detecting human vs. synthetic handwriting

## Executive Summary
This paper addresses the critical security challenge of distinguishing human-generated handwriting from synthetic data produced by various generative models. The authors develop a shallow recurrent neural network using gated recurrent units (GRU) that takes raw trajectory offsets as input rather than engineered features. Their approach enables the model to learn internal representations directly from non-featurized movement sequences, achieving excellent performance across ten public datasets and seven different synthetic generation methods. The classifier maintains strong performance even with limited training data and demonstrates good generalization to out-of-domain scenarios.

## Method Summary
The authors employ a shallow RNN with a single GRU layer (100 hidden units) that processes raw spatiotemporal offset sequences (Δx, Δy, Δt) from online handwriting trajectories. The model uses trajectory offsets rather than velocity features, allowing it to capture richer joint spatiotemporal structure. Training involves binary cross-entropy loss with Adam optimization (learning rate 0.0005), batch size 128, and early stopping on validation accuracy. The classifier is evaluated using AUC and EER metrics across diverse datasets including gestures, characters, digits, signatures, and mouse movements, with synthetic samples generated by methods including GANs, Transformers, and Diffusion models.

## Key Results
- Achieves 98.3% average AUC and 1.4% average EER across all synthesizers and datasets
- Maintains performance with only 10% of training data
- Generalizes well to out-of-domain scenarios
- Trajectory offsets outperform velocity features for classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A shallow GRU-based RNN can distinguish human from synthetic handwriting by detecting subtle motor-control imperfections that current synthesizers fail to replicate reliably.
- Mechanism: The GRU layer learns temporal representations from raw spatiotemporal offset sequences. Human handwriting contains non-stochastic, high-frequency neuromotor variability. Synthetic generators either smooth these imperfections or produce detectably regular patterns in component timing and position.
- Core assumption: Human handwriting encodes consistent but hard-to-simulate micro-motor variations; synthesizers produce distributions that deviate detectably in temporal or spatial structure.
- Evidence anchors: [abstract] "subtle motor variations in human handwriting movements, which are difficult for computational models to replicate reliably, can be effectively detected using this approach"; [section] "Human handwriting movements are imperfect, in the sense that they contain non-stochastic spatiotemporal variations that computational models are unable to reproduce reliably."
- Break condition: If future synthesizers inject realistic high-frequency motor noise, or if humans produce unnaturally smooth trajectories, discriminability may degrade.

### Mechanism 2
- Claim: Raw trajectory offsets (Δx, Δy, Δt) provide richer discriminative signal than engineered velocity features, enabling better detection across diverse synthesizers.
- Mechanism: Offsets preserve joint spatiotemporal structure, allowing the GRU to attend to directional and fine-grained timing cues. Velocity collapses information to a scalar per timestep, losing directional and micro-timing patterns that differentiate certain generators.
- Core assumption: The joint spatiotemporal representation encodes more task-relevant information than velocity alone, and the model has sufficient capacity to learn it.
- Evidence anchors: [abstract] "nonfeaturized trajectory data as input… enables the model to learn internal representations directly"; [section] "our classifier can effectively distinguish… achieving perfect (or very close to 100%) AUC scores… when using trajectory data as input. Using velocity data as input proved to be more challenging for classification"
- Break condition: If training data is extremely limited or sequences are very short, the richer representation may overfit; velocity may generalize better in some narrow domains.

### Mechanism 3
- Claim: The classifier generalizes across synthesizers and symbol types because it learns a representation of authentic human motor variability rather than generator-specific artifacts.
- Mechanism: Training on diverse human samples from multiple devices exposes the model to a broad spectrum of valid motor variation. Synthetic samples from varied generators share a common lack of authentic micro-variation, which the model captures as a unified synthetic signature.
- Core assumption: Despite architectural differences, all current synthesizers produce detectable deviations from natural human motor variability.
- Evidence anchors: [abstract] "achieves excellent performance… across all synthesizers and datasets"; [section] "it is possible to tell human and machine handwriting apart because… human handwriting movements are imperfect, in the sense that they contain non-stochastic spatiotemporal variations that computational models are unable to reproduce reliably."
- Break condition: If a synthesizer explicitly models human micro-variability, the unified synthetic signature may disappear.

## Foundational Learning

- Concept: Binary classification vs. symbol recognition vs. writer identification
  - Why needed here: To frame the task correctly: binary classification (human vs. synthetic), not symbol recognition or writer identification.
  - Quick check question: Can you distinguish this task from identifying what symbol was written or who wrote it?

- Concept: GRU vs. LSTM for sequence classification
  - Why needed here: To understand why GRU is preferred: it balances temporal dependency modeling with computational efficiency relative to LSTM.
  - Quick check question: Why might a GRU be preferred over an LSTM for this sequence classification task?

- Concept: AUC vs. EER metrics in security contexts
  - Why needed here: To interpret evaluation metrics: AUC measures discriminative power across thresholds; EER captures the operating point where false acceptance and false rejection rates are equal.
  - Quick check question: In a security context, why care about both AUC and EER?

## Architecture Onboarding

- Component map: Raw (x, y, t) tuples → trajectory offsets (Δx, Δy, Δt) → pad/truncate to 400 timesteps → GRU layer (100 units) → dropout (0.25) → sigmoid output

- Critical path: 1. Convert raw tuples to offsets (normalizes for device scale) 2. Pad/truncate to 400 timesteps 3. Pass through GRU → dropout → sigmoid output 4. Train with binary cross-entropy, Adam (lr=0.0005), early stopping (patience=40 on accuracy)

- Design tradeoffs: Trajectory vs. velocity input: richer signal vs. potential overfitting; trajectory more robust across generators. Shallow GRU vs. deep: fewer parameters, faster training; deeper may be unnecessary given strong baseline results. User-independent splits: more realistic deployment but require diverse data.

- Failure signatures: High EER on certain synthesizers (e.g., ΣΛ reconstructions) when using velocity input: velocity misses critical spatiotemporal cues. Large OOD performance drop when training on narrow datasets: suggests overfitting to domain-specific patterns. Classifier confidence near 0.5 broadly: model hasn't learned discriminative features; verify data pipeline and representation.

- First 3 experiments: 1. Reproduce baseline: Train on a single dataset (e.g., MobileTouch) with trajectory input; report AUC and EER on held-out test. 2. Ablate input representation: Compare trajectory vs. velocity across multiple datasets; quantify gap. 3. Stress-test OOD: Train on one dataset, test on all others; identify which source yields the best universal detector and hypothesize why.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the classifier maintain high performance on non-Latin writing scripts (e.g., Chinese or Arabic) which possess different kinematic profiles?
- Basis in paper: [explicit] The authors state in Section VI that "Future work should consider handwritten characters in non-Latin writing scripts, such as Chinese or Arabic," as most current datasets are English-centric.
- Why unresolved: The study focuses on Latin characters and gestures; it is unclear if the learned "internal representation" of human movement generalizes to the complex stroke orders and kinematics of scripts like Chinese.
- What evidence would resolve it: Evaluation of the current model on datasets of Chinese or Arabic handwriting synthesized by comparable SoTA generators.

### Open Question 2
- Question: Can the task be effectively framed as one-class anomaly detection trained exclusively on human movements?
- Basis in paper: [explicit] Section VI notes that "Future work could also consider a one-class classification approach, where for example the model is trained on human movements only."
- Why unresolved: The current architecture requires paired human/synthetic data for binary classification, which creates a dependency on synthesizer-specific artifacts.
- What evidence would resolve it: A comparative study showing the performance of a one-class RNN against the binary classifier, particularly against unseen synthetic generation methods.

### Open Question 3
- Question: What specific trajectory features determine the classification decision, and can they be visualized?
- Basis in paper: [explicit] The authors mention in Section VI that "Future work could also consider explainability tools in order to better understand the discriminating capability of the trained models."
- Why unresolved: While the authors hypothesize the model detects "non-stochastic spatiotemporal variations," the exact internal representations learned by the GRU remain opaque.
- What evidence would resolve it: Application of explainability tools (e.g., saliency maps or attention weights) to highlight which segments of a trajectory influence the "human" vs. "synthetic" decision.

### Open Question 4
- Question: Does incorporating high-dimensional inertial data (e.g., from IMUs) improve detection capabilities compared to standard trajectory offsets?
- Basis in paper: [explicit] The conclusion suggests it "would be interesting to consider other input representations, following emerging research directions on the analysis of high-dimensional inertial data."
- Why unresolved: The current model relies on (∆x, ∆y, ∆t) tuples; it is unknown if adding acceleration or orientation data captures motor nuances missed by position data alone.
- What evidence would resolve it: Experiments comparing trajectory-based input against 6-axis IMU data on the same classification task.

## Limitations
- Performance may degrade against adaptive synthesizers designed to mimic human micro-variability
- Implementation details for all seven synthesizers are not fully specified, creating reproducibility challenges
- Limited evaluation of cross-script generalization (focuses on Latin characters)

## Confidence
- **High Confidence**: The classifier architecture effectively distinguishes human from synthetic handwriting using the evaluated datasets and synthesizers
- **Medium Confidence**: The claim that the model generalizes across different symbol types and generation methods is supported but relies on assumptions about synthesizer behavior
- **Medium Confidence**: The assertion that trajectory offsets provide richer discriminative signal than velocity features is demonstrated empirically but lacks theoretical justification

## Next Checks
1. **Adversarial Robustness Test**: Implement a modified version of one synthesizer that explicitly adds realistic high-frequency motor noise sampled from empirical human data distributions. Evaluate whether the classifier maintains its performance on these adversarially enhanced samples.
2. **Cross-Domain Generalization**: Train the classifier on datasets from one device modality (e.g., touchscreen) and test exclusively on datasets from a completely different modality (e.g., pen tablet or mouse movements). Measure performance degradation and identify which motor features transfer versus modality-specific artifacts.
3. **Few-Shot Adaptability**: Starting from the trained universal detector, fine-tune on only 100 human samples from a new, unseen dataset. Measure how quickly the model adapts to new handwriting styles while maintaining synthetic detection capability, simulating deployment in a new user population.