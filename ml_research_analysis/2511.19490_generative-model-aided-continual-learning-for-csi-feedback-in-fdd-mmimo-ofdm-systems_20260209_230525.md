---
ver: rpa2
title: Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM
  Systems
arxiv_id: '2511.19490'
source_url: https://arxiv.org/abs/2511.19490
tags:
- uni00000003
- uni00000051
- uni00000048
- feedback
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in deep autoencoder
  (DAE) models for CSI feedback in FDD massive MIMO-OFDM systems due to user mobility.
  A GAN-aided continual learning approach is proposed, using GAN generators as memory
  units to capture CSI distributions from past scenarios and generate synthetic data
  to assist training in new environments.
---

# Generative Model-Aided Continual Learning for CSI Feedback in FDD mMIMO-OFDM Systems

## Quick Facts
- **arXiv ID:** 2511.19490
- **Source URL:** https://arxiv.org/abs/2511.19490
- **Reference count:** 15
- **Key outcome:** GAN-aided continual learning reduces catastrophic forgetting in CSI feedback DAEs, achieving NMSE close to multi-task learning upper bound with 22× lower storage overhead

## Executive Summary
This paper addresses catastrophic forgetting in deep autoencoder models for CSI feedback in FDD massive MIMO-OFDM systems caused by user mobility. The proposed solution uses GAN generators as memory units to capture CSI distributions from past scenarios, generating synthetic data to assist training on new environments. This approach significantly reduces storage overhead compared to storing raw CSI matrices while maintaining performance close to multi-task learning benchmarks. Evaluation on DeepMIMO datasets demonstrates effective forgetting mitigation with a compression ratio of 1/16.

## Method Summary
The method trains a GAN per scenario to capture CSI distribution, storing only generator parameters as memory units. When facing new scenarios, synthetic samples are generated from all stored generators and combined with current real data for joint training of the CSI feedback autoencoder. The approach uses EM distance (Wasserstein) loss with consistency regularization for stable GAN training. Storage overhead is reduced by orders of magnitude compared to joint learning approaches while maintaining NMSE performance close to the multi-task learning upper bound.

## Key Results
- NMSE performance approaches multi-task learning upper bound with compression ratio 1/16
- Storage cost reduced from 78.125 MB (joint learning) to 3.552 MB (proposed method) - 22× reduction
- Outperforms direct transfer methods in maintaining performance across multiple scenarios
- Maintains consistently high performance across diverse scenarios without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Storing GAN generator parameters as memory units reduces storage overhead while preserving knowledge of past CSI distributions
- For each encountered scenario, the GAN generator learns to approximate the CSI distribution; only generator parameters (~3.5MB for three scenarios) are stored permanently
- When facing new scenarios, synthetic data is generated on-demand from stored generators and combined with current real data for joint training
- Core assumption: The GAN generator can sufficiently approximate the true CSI distribution such that synthetic data provides comparable training signal to real historical data

### Mechanism 2
- Training the CSI feedback autoencoder on a mixture of synthetic historical data and real current data mitigates catastrophic forgetting
- When scenario t arrives, generate K synthetic samples from each stored generator; train the DAE on {synthetic₀,...,ₜ₋₁ ∪ realₜ}
- This exposes the model to all previously encountered distributions during each training phase
- Core assumption: Generated samples are sufficiently diverse and representative of original distributions

### Mechanism 3
- EM distance (Wasserstein) loss with consistency regularization stabilizes GAN training for high-dimensional CSI distributions
- The discriminator is constrained to be 1-Lipschitz; consistency regularization adds penalties for inconsistent discriminator outputs under different dropout masks
- This improves convergence when Pr(H) and Pf(H̃) distributions differ significantly
- Core assumption: CSI distributions are sufficiently smooth and connected in latent space for Wasserstein distance to provide meaningful gradients

## Foundational Learning

- **Catastrophic Forgetting in Neural Networks**
  - Why needed here: The core problem being solved; understanding why DAEs lose performance on old scenarios when fine-tuned on new data is essential
  - Quick check question: Can you explain why sequential gradient updates overwrite previously learned representations in standard DAE training?

- **Generative Adversarial Networks (GANs)**
  - Why needed here: The memory mechanism relies on understanding adversarial training, generator-discriminator dynamics, and convergence criteria
  - Quick check question: What happens to training if the discriminator becomes too accurate too quickly?

- **CSI Feedback in FDD Massive MIMO**
  - Why needed here: Context for why compression is necessary (Nt × Nc channel matrices) and what performance metric (NMSE) matters
  - Quick check question: Why does FDD mode require explicit CSI feedback whereas TDD does not?

## Architecture Onboarding

- **Component map:**
  - Encoder (fenc): Compresses H ∈ ℂ^(Nt×Nc) → s ∈ ℂ^(V×1) with compression ratio γ = V/(2NtNc)
  - Decoder (fdec): Reconstructs Ĥ from codeword s
  - GAN Generator G(z): Maps noise z ~ N(0,I) to synthetic CSI H̃; stored as memory unit
  - GAN Discriminator D(·): Distinguishes real vs synthetic CSI; discarded after training
  - Memory Unit M: Collection of stored generators {G₀, G₁, ..., Gₜ} from all scenarios

- **Critical path:**
  1. Collect real CSI data from scenario t
  2. Train GAN on dataₜ until convergence (store Gₜ)
  3. Generate K synthetic samples from each G₀,...,Gₜ₋₁ in M
  4. Train DAE on {synthetic₀,...,ₜ₋₁ ∪ realₜ}
  5. Add Gₜ to M, proceed to scenario t+1

- **Design tradeoffs:**
  - K (samples per generator): Higher K → better forgetting mitigation but longer training time
  - Generator capacity: Larger generators → better distribution capture but higher storage
  - Training epochs for GAN: Under-trained generators produce poor synthetic data; over-training wastes compute

- **Failure signatures:**
  - NMSE on previously seen scenarios significantly worse than MTL upper bound → insufficient synthetic data (increase K) or poor GAN convergence
  - Generator loss diverges → check Lipschitz constraint enforcement, reduce learning rate
  - Current scenario performance degrades → synthetic/real data ratio too high

- **First 3 experiments:**
  1. Baseline reproduction: Train CsiNet on Scenario A only, then sequentially on B and C without memory. Measure NMSE degradation on A and B after training on C to quantify catastrophic forgetting.
  2. Memory scaling: With fixed K=5000, vary number of stored generators (1, 2, 3) and measure NMSE across all scenarios. Verify storage overhead matches Table II projections.
  3. Synthetic data quality ablation: Train DAE using only synthetic data from a well-converged GAN (no real data) on a held-out test set. Compare NMSE to training with real data to assess distribution capture fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inference latency and memory overhead scale in real-time systems when the number of stored generators (N_g) grows significantly large?
- Basis in paper: Section III.C explicitly states that the computational complexity of the inference process scales as O(N_g N_t N_c), but the evaluation is limited to only three distinct scenarios
- Why unresolved: It is unclear if the linear growth in storage and computation remains practical for edge devices after encountering hundreds of diverse environments over a long operational period
- What evidence would resolve it: Performance metrics (latency, memory usage) evaluated on hardware when N_g exceeds 50 or 100 distinct scenarios

### Open Question 2
- Question: Can the framework autonomously detect environment shifts to trigger the training of new generators, rather than relying on pre-defined scenario labels?
- Basis in paper: The methodology in Section III.B assumes a clear transition to "Scenario t" to update the memory unit M, utilizing specific dataset rows (A, B, C) without defining a threshold for what constitutes a "new" scenario
- Why unresolved: Real-world user mobility involves continuous, fluid channel variations rather than discrete jumps, which may lead to unnecessary generator proliferation or failure to capture gradual drift
- What evidence would resolve it: Integration of an unsupervised change-detection mechanism to dynamically manage the memory unit M

### Open Question 3
- Question: To what extent does GAN mode collapse or imperfect distribution modeling contribute to the performance gap compared to the Multi-Task Learning (MTL) upper bound?
- Basis in paper: Table I shows that even with K=10,000 generated samples, the proposed method slightly underperforms compared to the MTL upper bound
- Why unresolved: The paper asserts the generator captures the distribution but does not statistically analyze if the synthetic data fully covers the tails or diversity of the real channel distribution
- What evidence would resolve it: A statistical comparison between the generated CSI distributions and the ground truth

## Limitations
- CsiNet architecture not fully specified, making exact reproduction difficult
- No ablation studies on synthetic data quality when used alone
- Performance evaluation limited to three synthetic DeepMIMO scenarios; real-world testing needed
- Storage comparison excludes other continual learning methods like EWC or knowledge distillation

## Confidence

- **High:** Catastrophic forgetting exists in DAEs for CSI feedback (well-established phenomenon)
- **Medium:** Proposed GAN memory mechanism effectively mitigates forgetting (supported by results but architecture underspecified)
- **Low:** GAN generators adequately capture CSI distributions (no standalone synthetic data evaluation)

## Next Checks

1. **Synthetic Data Fidelity Test:** Train CsiNet using only synthetic samples from a converged GAN (no real data) on held-out test sets. Compare NMSE to training with real data to quantify distribution capture quality.

2. **Storage Efficiency Benchmark:** Implement and compare against elastic weight consolidation (EWC) and reservoir sampling baselines on the same three-scenario sequence. Measure both NMSE performance and actual storage requirements.

3. **Real-World Generalization:** Apply the method to channel measurements from an actual FDD mMIMO testbed or over-the-air collected data with realistic mobility patterns. Validate whether synthetic data generation scales beyond DeepMIMO's synthetic channels.