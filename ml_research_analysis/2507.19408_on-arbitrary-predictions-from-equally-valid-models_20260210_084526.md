---
ver: rpa2
title: On Arbitrary Predictions from Equally Valid Models
arxiv_id: '2507.19408'
source_url: https://arxiv.org/abs/2507.19408
tags:
- predictions
- multiplicity
- predictive
- performance
- rashomon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates model multiplicity in medical imaging,
  where multiple models can perform equally well but produce conflicting predictions
  for the same patient. The authors train 50 model instances per configuration across
  five medical datasets and four architectures, varying only the random seed.
---

# On Arbitrary Predictions from Equally Valid Models

## Quick Facts
- arXiv ID: 2507.19408
- Source URL: https://arxiv.org/abs/2507.19408
- Reference count: 40
- One-line primary result: Validation performance is an unreliable predictor of test performance in medical imaging, and ensemble consensus with abstention strategies can effectively eliminate measurable predictive multiplicity.

## Executive Summary
This study investigates model multiplicity in medical imaging, where multiple models can perform equally well but produce conflicting predictions for the same patient. The authors train 50 model instances per configuration across five medical datasets and four architectures, varying only the random seed. They find that validation performance is an unreliable predictor of test performance, and that a substantial proportion of predictions are arbitrary when using a single model. Ensembling with abstention strategies can effectively eliminate measurable predictive multiplicity, achieving perfect agreement among ensemble members while deferring uncertain cases to expert review. Higher model capacity that improves accuracy also reduces predictive multiplicity. The results demonstrate that relying on a single model is problematic in medical settings and advocate for ensemble-based approaches to improve diagnostic reliability and reduce arbitrary predictions.

## Method Summary
The authors train 50 model instances per configuration across five medical datasets (Breast Ultrasound, Blood Cell, OCT Scan, Abdominal CT, X-ray/CheXpert) and four architectures (ResNet50, GC ViT, EfficientNetB2, ConvNeXtBase). Models vary only in random seed for final layer initialization while keeping all other hyperparameters identical. They use ImageNet-pretrained models with AdamW optimizer, cosine decay learning rate, and sweep learning rates across [0.01, 0.001, 0.0001]. The study characterizes the empirical Rashomon set through ambiguity (percentage of samples with conflicting predictions) and expected pairwise agreement metrics, then evaluates ensemble consensus with abstention strategies to eliminate multiplicity.

## Key Results
- Validation performance is an unreliable predictor of test performance, with test accuracy fluctuating 3-6% versus 0.2-0.3% validation variation
- Ensembles of size five achieve near-perfect expected pairwise agreement (implying no observed pairwise disagreement) while deferring uncertain cases
- Higher model capacity that improves accuracy also reduces predictive multiplicity as a byproduct
- Single-model approaches produce arbitrary predictions on a substantial proportion of samples in medical imaging

## Why This Works (Mechanism)

### Mechanism 1: Ensemble Consensus with Abstention Eliminates Measurable Multiplicity
By requiring consensus across multiple equally-valid models before making a prediction, the system identifies samples where predictions are arbitrary (models disagree) versus stable (models agree). Disagreement triggers abstention/deferral rather than arbitrary output. Small ensembles (n=5) with unanimous agreement requirements can eliminate measurable predictive multiplicity in practice.

### Mechanism 2: Validation Performance Is Unreliable for Model Selection
Small validation sets exhibit sampling variability that distorts apparent model quality. Models may appear similar on validation but diverge on test, or vice versa. Low validation variance can misleadingly suggest stability while obscuring genuine behavioral differences. This finding is conditional on small/shifted medical datasets.

### Mechanism 3: Higher Accuracy (via Capacity) Reduces Multiplicity
Error provides the "space" for disagreement. Higher-accuracy models leave less room for conflicting predictions across equally-valid alternatives. The relationship is mediated by accuracy, not capacity directly—capacity only helps when it improves accuracy.

## Foundational Learning

- **Concept: Rashomon Set / Model Multiplicity**
  - Why needed here: The entire paper operationalizes this concept—the set of equally-performing models that produce different predictions. Without understanding this, the core problem (arbitrary predictions) is unintelligible.
  - Quick check question: If two models achieve 95% accuracy on validation but disagree on 10% of test samples, are they in the same Rashomon set?

- **Concept: Expected Pairwise Agreement**
  - Why needed here: This is the paper's primary metric for quantifying multiplicity reduction. It measures how often two independently-sampled models/ensembles make the same prediction.
  - Quick check question: If expected pairwise agreement is 100% but coverage is 50%, what does this mean for the system?

- **Concept: Abstention/Deferral Strategies**
  - Why needed here: The proposed solution relies on refusing to predict when consensus fails. This is a design pattern, not just a metric.
  - Quick check question: What is the trade-off between coverage and reliability when tightening consensus thresholds?

## Architecture Onboarding

- **Component map**: Empirical Rashomon Set Generator -> Agreement Calculator -> Abstention Gate -> Coverage Tracker
- **Critical path**: Rashomon set construction -> ambiguity quantification -> ensemble configuration -> abstention threshold tuning -> deployment with deferral pipeline
- **Design tradeoffs**:
  - Ensemble size vs. computational cost (paper shows n=5 sufficient for near-perfect agreement)
  - Consensus strictness vs. coverage (unanimous agreement = high reliability, lower coverage; relaxed thresholds = more predictions, more risk)
  - Model capacity vs. multiplicity (indirect—only helps if accuracy improves)
- **Failure signatures**:
  - High pairwise agreement but low accuracy → correlated systematic errors across models
  - Large validation-test gap → dataset shift or sampling issues; Rashomon set characterization may be distorted
  - Sensitivity to model order in ambiguity analysis → training instability, configuration not ready for deployment
- **First 3 experiments**:
  1. Replicate Rashomon set characterization on your own dataset: train 20-50 models with seed variation, plot validation vs. test performance scatter to assess reliability gap
  2. Measure baseline ambiguity: compute percentage of samples with conflicting predictions across 5-model ensembles to quantify current multiplicity exposure
  3. Tune abstention threshold: test unanimous vs. 90% vs. 80% consensus requirements, plotting coverage-accuracy curves to select domain-appropriate operating point

## Open Questions the Paper Calls Out

- Are there consistent regularities or shared characteristics among samples that exhibit high versus low inter-model agreement? The authors intend to investigate in future works whether there are regularities in the samples that have high or low agreement.

- How does predictive multiplicity directly influence downstream clinical decision-making and patient outcomes? The authors explicitly list this as a limitation, noting they do not directly assess how predictive multiplicity influences downstream clinical decision-making or patient outcomes.

- Does predictive multiplicity manifest in non-classification medical tasks, such as image segmentation or regression? The authors acknowledge they focus exclusively on classification problems, thereby omitting the full diversity of clinical scenarios.

## Limitations

- Findings are highly contingent on small/shifted medical datasets where validation-test performance correlations are weak
- The ensemble-based abstention solution assumes uncorrelated model errors within the Rashomon set, which may not hold in practice
- The accuracy-multiplicity relationship is observed empirically but lacks theoretical grounding

## Confidence

- High confidence: Ensemble consensus with abstention effectively eliminates measurable predictive multiplicity in the studied settings
- Medium confidence: Validation performance is unreliable for model selection within the Rashomon set; higher accuracy reduces multiplicity
- Low confidence: The specific numerical thresholds (n=5 ensemble, unanimous agreement) generalize across medical domains

## Next Checks

1. Test whether validation metrics become more reliable when using larger, more representative validation sets from the same medical domains
2. Evaluate ensemble performance when model errors are intentionally correlated (e.g., through data augmentation or architecture choices)
3. Verify that the accuracy-multiplicity relationship holds for non-ConvNet architectures (e.g., Transformers, MLP-Mixers) on comparable medical imaging tasks