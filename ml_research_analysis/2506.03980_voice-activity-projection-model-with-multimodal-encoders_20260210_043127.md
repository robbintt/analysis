---
ver: rpa2
title: Voice Activity Projection Model with Multimodal Encoders
arxiv_id: '2506.03980'
source_url: https://arxiv.org/abs/2506.03980
tags:
- turn-taking
- prediction
- face
- multimodal
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses turn-taking prediction in human-agent interactions
  by incorporating pre-trained facial image encoders into voice activity projection
  (VAP) models. The authors propose three variants of multimodal VAP models that combine
  audio signals with facial images, optionally including head/gaze/body features and
  action units.
---

# Voice Activity Projection Model with Multimodal Encoders

## Quick Facts
- arXiv ID: 2506.03980
- Source URL: https://arxiv.org/abs/2506.03980
- Reference count: 0
- Primary result: Pre-trained facial encoders improve turn-taking prediction accuracy to 0.794 for shift prediction on NoXi dataset

## Executive Summary
This paper addresses turn-taking prediction in human-agent interactions by incorporating pre-trained facial image encoders into voice activity projection (VAP) models. The authors propose three variants of multimodal VAP models that combine audio signals with facial images, optionally including head/gaze/body features and action units. The key innovation is replacing manually extracted facial action units with richer facial embeddings from a pre-trained Dynamic Facial Expression Recognition Transformer (Former-DFER). Experiments on the NoXi dataset show that the proposed models achieve competitive or superior performance on shift prediction and backchannel prediction metrics compared to state-of-the-art multimodal models, with the best shift prediction accuracy reaching 0.794.

## Method Summary
The proposed models integrate pre-trained facial encoders (Former-DFER for faces, CPC for audio) with optional body features into a unified VAP architecture. The model predicts a 2×4 binary tensor encoding each user's voice activity across four future time bins (200ms, 400ms, 600ms, 800ms). Three variants are proposed: Proposed1 (audio + face encoder only), Proposed2 (adds head/gaze/body features), and Proposed3 (adds action units). The architecture uses per-user multimodal fusion before inter-user fusion, contrasting with previous approaches. Multi-task learning with VAD and VAP heads forces the model to learn shared temporal representations.

## Key Results
- Proposed3 achieves best shift prediction accuracy of 0.794 on NoXi dataset
- All proposed models outperform baseline models on shift prediction metrics
- Pre-trained facial encoders capture subtle turn-taking cues missed by manually-coded action units
- Per-user multimodal fusion before inter-user fusion improves coordination modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained facial expression encoders capture richer turn-taking cues than manually-coded action units.
- **Mechanism:** The Former-DFER encoder, pre-trained on DFEW for dynamic emotion recognition in video, embeds sequential dynamics and subtle facial expressions that action units miss. These embeddings flow through a Face Transformer before inter-modal fusion, allowing the model to learn implicit social signals (e.g., micro-expressions, gaze shifts) without explicit feature engineering.
- **Core assumption:** Turn-yielding and turn-holding intentions are reflected in facial dynamics that emotion-recognition pre-training captures transferably.
- **Evidence anchors:**
  - [abstract] "incorporates richer representations from pre-trained facial image encoders... demonstrating the effectiveness of pre-trained facial encoders for capturing subtle expressions"
  - [section 3] "the facial action unit only captures superficial facial expressions for each image frame and does not include sequential dynamics and subtle expressions"
  - [corpus] Related work "Visual Cues Enhance Predictive Turn-Taking" confirms visual cues including facial expression improve prediction, but does not isolate pre-trained vs. hand-coded features.
- **Break condition:** If facial dynamics in your target domain differ substantially from DFEW (e.g., non-frontal faces, extreme lighting, occluded faces), the pre-trained encoder may not transfer effectively.

### Mechanism 2
- **Claim:** Unified VAP state representation enables joint prediction of turn-shift and backchannel events.
- **Mechanism:** Instead of separate classifiers for shift and backchannel, the model predicts a 2×4 binary tensor encoding each user's voice activity across four future time bins (200ms, 400ms, 600ms, 800ms). Multi-task learning with VAD and VAP heads forces the model to learn shared temporal representations useful for both coarse (speaking/not) and fine-grained (backchannel) predictions.
- **Core assumption:** Future voice activity can be predicted from 20 seconds of multimodal context, and backchannels are distinguishable purely by duration/silence patterns.
- **Evidence anchors:**
  - [section 2] "VAP state is a discrete representation shaped as 2 × 4 binary bins... the duration of each time bin is defined as 0.2, 0.4, 0.6, and 0.8 sec."
  - [section 4.2] "backchannel in VAP models... defined solely in terms of the duration of speaking segments and the surrounding silence chunks"
  - [corpus] "Triadic Multi-party VAP" extends this representation to three-party settings, confirming the unified representation's generalizability.
- **Break condition:** If your application requires distinguishing backchannel types (continuers vs. assessments) or multimodal backchannels (nodding without speech), the current duration-only definition will underperform.

### Mechanism 3
- **Claim:** Per-user multimodal fusion before inter-user fusion improves coordination modeling.
- **Mechanism:** Unlike Onishi's approach (merge users per modality, then fuse across modalities), this architecture first fuses each user's audio + face + body signals via an Inter-modal Transformer, then merges across users via an Inter-user Transformer. This preserves individual-level multimodal coherence before modeling dyadic interaction.
- **Core assumption:** Turn-taking cues emerge from intra-personal multimodal coordination (e.g., facial expression + prosody alignment) that should be modeled before inter-personal dynamics.
- **Evidence anchors:**
  - [section 3] "We first merged multimodal signals for each person separately, followed by the fusion across user embeddings."
  - [section 5] "the improvement stems from the interaction between the face encoder and body signals"
  - [corpus] No direct corpus comparison of fusion strategies; this architectural choice remains under-tested.
- **Break condition:** If modalities have different temporal granularities or missing data rates per user, early per-user fusion may amplify noise.

## Foundational Learning

- **Concept: Voice Activity Detection (VAD) vs. Voice Activity Projection (VAP)**
  - Why needed here: VAD detects current speech; VAP *predicts future* speech states. Understanding this distinction is essential for grasping why the model uses 20-second context windows to predict 2-second futures.
  - Quick check question: If you only need to know whether someone is currently speaking, which component suffices? If you need to know when to start responding before they finish, which do you need?

- **Concept: Pre-trained encoders for modality-specific representations**
  - Why needed here: The model replaces hand-crafted features (OpenFace action units) with learned embeddings from Former-DFER and CPC. You must understand transfer learning assumptions to diagnose domain shift failures.
  - Quick check question: If your deployment uses profile-view cameras instead of frontal faces, will the pre-trained face encoder still work? How would you test this?

- **Concept: Multi-task learning with shared representations**
  - Why needed here: The model jointly trains VAD and VAP heads, sharing the backbone. This regularization prevents overfitting but creates coupling between tasks.
  - Quick check question: If VAD accuracy is high but shift prediction is low, what does this suggest about the shared representation?

## Architecture Onboarding

- **Component map:**
  Raw Audio (16kHz, stereo) → CPC Audio Encoder → Audio Transformer → Inter-modal Transformer (per user) → Inter-user Transformer → VAP head + VAD head
  Face Images (112×112) → Former-DFER Encoder → Face Transformer → Inter-modal Transformer (per user) → Inter-user Transformer → VAP head + VAD head
  Head/Gaze/Body → (optional, concatenated) → Inter-modal Transformer (per user) → Inter-user Transformer → VAP head + VAD head

- **Critical path:** Face image preprocessing (dlib face detection → crop → resize to 112×112) must be accurate; misaligned crops degrade Former-DFER embeddings significantly.

- **Design tradeoffs:**
  - Proposed1 (audio + face encoder only) is simpler but underperforms on shift-hold vs. Baseline2.
  - Proposed3 (all modalities including AU) achieves best shift prediction (0.794) but requires OpenFace/OpenPose, increasing deployment complexity.
  - 200GB RAM requirement for preloading data limits training accessibility.

- **Failure signatures:**
  - Low backchannel accuracy (all models ~0.43–0.51): likely due to duration-only backchannel definition missing functional types.
  - SP drops when head/gaze/body added without face encoder (Baseline2_2): body features may introduce noise without richer face context.
  - Training instability if gradient accumulation misconfigured with batch size 256 across two GPUs.

- **First 3 experiments:**
  1. **Reproduce Proposed1 on NoXi French subset** to validate pipeline; compare SP accuracy against reported 0.709. If significantly lower, check face crop alignment and CPC encoder loading.
  2. **Ablate face encoder vs. action units** on shift prediction: train Proposed1 with AU only, then face encoder only. Quantify contribution gap to confirm encoder value.
  3. **Test domain robustness** by evaluating on NoXi German/Japanese subsets without retraining. Report SP/BC degradation to assess cross-cultural generalization (paper notes multilingual validation as future work).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating distinct backchannel types (e.g., continuers vs. assessments) as prediction targets improve the accuracy of backchannel prediction beyond the current duration-based definition?
- Basis in paper: [explicit] Section 5 states, "This implies the necessity of considering backchannel types for behavior modeling... we should predict not only backchannel timings, but also types of backchannels for improvements."
- Why unresolved: The current VAP model defines backchannels solely by duration (1-second speech chunks surrounded by silence), ignoring functional and multimodal varieties like nodding or verbal assessments, which results in lower accuracy (0.503).
- What evidence would resolve it: A modified VAP model trained to classify or predict specific backchannel types, demonstrating higher class-balanced accuracy on the NoXi dataset compared to the current binary backchannel metric.

### Open Question 2
- Question: Can the inclusion of a pre-trained body encoder further enhance turn-taking prediction performance, specifically for shift prediction?
- Basis in paper: [explicit] Section 5 notes that the improvement in the "Proposed3" model likely stems from the interaction between the face encoder and body signals, explicitly stating, "further improvement could arise with the inclusion of a body encoder."
- Why unresolved: While the study successfully replaced superficial facial features with a pre-trained encoder, body signals are currently limited to "normalized 2D body positions," which may lack the rich sequential or temporal context necessary for optimal performance.
- What evidence would resolve it: Comparative experiments showing that a model variant utilizing a pre-trained body encoder outperforms the current "Proposed3" model on shift prediction metrics.

### Open Question 3
- Question: To what extent does training the model on a mixture of multilingual data improve its resilience and generalizability compared to training on a single-language subset?
- Basis in paper: [explicit] Section 6 concludes, "Please note that there is room to improve resilience by mixing multilingual data to train models, as Inoue previously validated [30]."
- Why unresolved: The current study utilized only the French subset of the NoXi dataset to simplify the experimental setting, leaving the model's ability to generalize across different languages and cultural interaction styles unverified.
- What evidence would resolve it: An evaluation of the model's performance consistency (shift-hold, short-long, shift, and backchannel accuracy) when trained and tested on the full multilingual NoXi dataset (German, UK, French, Japanese).

## Limitations
- Current evaluation limited to NoXi dataset with frontal-facing participants
- 200GB RAM requirement for preloading data limits accessibility
- Backchannel prediction uses simplified duration-only definition missing functional distinctions
- Performance on multiparty dialogues, non-frontal angles, and noisy real-world conditions untested

## Confidence

- **High confidence:** Pre-trained facial encoders improve shift prediction (0.709 → 0.794) and shift-hold accuracy (0.527 vs. 0.385 Baseline2_1)
- **Medium confidence:** Unified VAP representation for joint prediction is theoretically sound but backchannel accuracy remains low (~0.43-0.51)
- **Medium confidence:** Per-user multimodal fusion strategy shows performance gains but lacks direct comparison with alternative fusion orders

## Next Checks

1. **Cross-cultural generalization test:** Evaluate all model variants on NoXi German and Japanese subsets without retraining to quantify cross-cultural robustness, addressing the paper's noted limitation of only testing French data.

2. **Backchannel functional analysis:** Extend the backchannel definition beyond duration to include turn-taking function labels (continuer, assessment, backchannel invitation) if available in the dataset, to determine if performance improves with richer supervision.

3. **Real-world deployment simulation:** Test model performance on degraded face images (occluded, non-frontal, low-resolution) to assess robustness of the pre-trained facial encoder under realistic conditions.