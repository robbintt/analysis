---
ver: rpa2
title: "Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology:\
  \ A case study of Yolox\xF3chtil Mixtec ASR"
arxiv_id: '2512.06169'
source_url: https://arxiv.org/abs/2512.06169
tags:
- tokenizers
- which
- tokenizer
- morphology
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of automatic speech recognition\
  \ (ASR) for languages with non-concatenative morphology, focusing on Yolox\xF3chitl\
  \ Mixtec (YM). The authors propose two novel tokenization schemes that separate\
  \ tonal and segmental information to better capture the language's complex morphology."
---

# Morphologically-Informed Tokenizers for Languages with Non-Concatenative Morphology: A case study of Yoloxóchtil Mixtec ASR

## Quick Facts
- **arXiv ID**: 2512.06169
- **Source URL**: https://arxiv.org/abs/2512.06169
- **Authors**: Chris Crawford
- **Reference count**: 0
- **Primary result**: Novel tokenizers that separate tonal and segmental information achieve competitive performance with traditional methods on Yoloxóchitl Mixtec ASR

## Executive Summary
This paper addresses the challenge of automatic speech recognition (ASR) for languages with non-concatenative morphology, focusing on Yoloxóchitl Mixtec (YM). The authors propose two novel tokenization schemes that separate tonal and segmental information to better capture the language's complex morphology. The Segment-and-Melody tokenizer extracts tones without predicting segmentation, while the Sequence-of-Processes tokenizer predicts segmentation for words. Experiments show these novel tokenizers are competitive with traditional methods like BPE and Unigram, with the Segment-and-Melody model outperforming in word error rate (WER) but not character error rate (CER). Morphological consistency scores correlate with ASR performance, suggesting morphology-aware tokenizers are beneficial. The paper highlights the potential of nonlinear tokenizers for languages with non-concatenative morphology, though further research is needed to evaluate their effectiveness in downstream tasks.

## Method Summary
The paper introduces two novel tokenization approaches for non-concatenative morphology in ASR. The Segment-and-Melody tokenizer uses regex to split words into segmental and tonal components, treating tones as independent tokens. The Sequence-of-Processes tokenizer employs a WFST to generate all valid morphological segmentations, then uses a ByT5 language model to rescore and select the most likely segmentation. Both methods are compared against standard tokenizers (BPE, Unigram) using a Conformer/Transformer hybrid ASR architecture trained on 52.4 hours of Yoloxóchitl Mixtec speech data. The study evaluates performance using WER, CER, and introduces a morphological consistency metric (F1 score) to measure alignment with morpheme boundaries.

## Key Results
- Segment-and-Melody tokenizer achieves the lowest WER (26.4%) among all tested tokenizers
- Morphological F1 score correlates negatively with WER (Spearman's ρ = -0.469, p=0.049)
- Traditional tokenizers (BPE, Unigram) perform competitively on CER but less effectively on WER
- Sequence-of-Processes tokenizer is computationally expensive (9 hours preprocessing) but shows promise for explicit morphological segmentation

## Why This Works (Mechanism)

### Mechanism 1: Isolation of Non-Concatenative Features via Nonlinear Splitting
- **Claim**: Separating tonal melodies from segmental roots improves Word Error Rate (WER) by preventing subword tokenizers from creating arbitrary, meaningless units.
- **Mechanism**: Standard BPE greedily merges frequent character strings (e.g., merging a tone `4` with a following clitic `=`). The Segment-and-Melody (SegMel) tokenizer forces a nonlinear split, extracting tones into a separate token stream (e.g., `ta'bi` and `14|4`). This ensures the model learns the segmental root and the tonal pattern as independent variables rather than conflated noise.
- **Core assumption**: The acoustic model can attend to segmental features and tonal features more effectively when they are decoupled in the target vocabulary.
- **Evidence anchors**:
  - [abstract] Mentions preserving information about tonal morphology by separating words in a nonlinear manner.
  - [section 4.2.1] Demonstrates the regex split `ta'14bi4` → `ta'|bi` and `14|4`.
  - [corpus] Neighbors like "Comparative analysis of subword tokenization" validate that standard approaches often overlook linguistic principles, though this paper specifically targets non-concatenative issues.
- **Break condition**: If a language lacks distinct tonal morphology (e.g., purely concatenative morphology), this added token complexity offers no benefit and may degrade performance due to sequence lengthening.

### Mechanism 2: Constrained Decoding via WFST-LM Lattice Rescoring
- **Claim**: Finite-State Transducers (FSTs) restrict the search space to valid morphological forms, enabling the prediction of segmentations from unsegmented text.
- **Mechanism**: A WFST generates all linguistically licensed segmentations for an ambiguous input (e.g., `i4in4` has 4 valid parses). A neural language model (ByT5) scores the probability of these paths in context. The system combines the FST weight (structural validity) and LM score (contextual likelihood) via modified Viterbi/Beam Search to select the optimal output.
- **Core assumption**: The "gold standard" morphological rules encoded in the FST are complete and the language model is sufficiently trained to disambiguate homophones.
- **Evidence anchors**:
  - [section 4.2.2] Describes the cascaded system where an FST generates forms and an LM selects the best one.
  - [algorithm 1] Defines the scoring function ŵ(q') combining LM probability and FST weight.
  - [corpus] Weak/missing; standard ASR literature often uses WFSTs for decoding, but this specific "generation + rescoring" for morphological segmentation is a novel configuration in this paper.
- **Break condition**: High latency or OOM errors occur if the FST generates too many hypotheses (large beam width) or if the LM inference is slow.

### Mechanism 3: Morphological Consistency as a Performance Proxy
- **Claim**: A tokenizer's Morphological F1 score (consistency with morpheme boundaries) negatively correlates with ASR WER, making it a better predictor than information density (Sparsity).
- **Mechanism**: Tokenizers that align with linguistic morphemes (high F1) force the model to learn compositional representations. Conversely, tokenizers that split morphemes (low F1) require the model to "memorize" statistical artifacts. The paper shows that as F1 increases, error rates tend to decrease, whereas Sparsity (entropy ratio) shows no significant correlation.
- **Core assumption**: Morpheme boundaries are the atomic units of meaning for the ASR task, and models struggle to re-synthesize meaning from broken morphemes.
- **Evidence anchors**:
  - [section 5.3.1] Reports Spearman's ρ = -0.469 for F1 vs. WER (p=0.049) and no correlation for Sparsity.
  - [section 5.2] Shows SegMel (Base) has the highest F1 (0.716).
  - [corpus] Neighbors like "Beyond Fertility" critique standard metrics; this paper proposes Morph.-F1 as a superior evaluation metric.
- **Break condition**: If the language has extremely poor data quality or the dictionary used for F1 calculation is inaccurate, the metric loses predictive power.

## Foundational Learning
- **Concept: Non-Concatenative Morphology**
  - **Why needed here**: Standard tokenizers (BPE) assume words are built by adding prefixes/suffixes. Yoloxóchitl Mixtec changes meaning via internal changes (tone shifts, e.g., `3→4` for habitual aspect).
  - **Quick check question**: Can you distinguish why BPE fails on a word where the "morpheme" is a tone change inside the word, rather than a suffix attached to the end?
- **Concept: Weighted Finite-State Transducers (WFSTs)**
  - **Why needed here**: This paper relies on WFSTs to mathematically define and constrain valid morphological transformations before passing data to the neural model.
  - **Quick check question**: If you have an input string and a set of rewrite rules, how does a WFST differ from a simple regex replacement in handling ambiguity and probabilities?
- **Concept: Hybrid CTC/Attention ASR Architecture**
  - **Why needed here**: The experiments use a Conformer/Transformer hybrid. Understanding how CTC (alignment-focused) and Attention (sequence-focused) interact helps interpret why certain tokenizers fit better.
  - **Quick check question**: In a Hybrid system, which component (CTC or Attention) is likely more sensitive to the length of the token sequence generated by the tokenizer?

## Architecture Onboarding
- **Component map**: Log-Mel filterbanks (83-dim) + Pitch → 12-block Transformer/Conformer → Tokenizer (SegMel/ProcSeq/BPE/Unigram) → Hybrid CTC/Attention → Output
- **Critical path**:
  1. Preprocess text using `SegMel` (regex) or `ProcSeq` (FST+LM)
  2. Train ASR model using ESPnet2 with the custom token list
  3. Decode using standard Beam Search (SegMel) or Constrained Beam Search (ProcSeq)
- **Design tradeoffs**:
  - **SegMel vs. ProcSeq**: `SegMel` is faster (regex) and yields lower WER, but `ProcSeq` explicitly predicts segmentation (morphemes). `ProcSeq` is computationally expensive (9 hours preprocessing vs 2.5 mins).
  - **Vocab Size**: BPE/500 performs best; increasing to 1k degrades performance significantly (greedy merging of unrelated units).
  - **Model Architecture**: Conformer models struggle with novel tokenizers (high CER ~90%) compared to Transformers.
- **Failure signatures**:
  - **High CER / Low WER (SegMel)**: Model makes "clustered" errors (one word very wrong) vs. BPE's "scattered" errors (many words slightly wrong).
  - **High Latency (ProcSeq)**: FST not fully optimized/determinized; redundant arcs between states.
  - **Conformer Collapse**: Conformer models fail without fine-tuning (CER ~90%) compared to Transformer.
- **First 3 experiments**:
  1. **Baseline Reproduction**: Train a Transformer with `Unigram/500` on the 52.4h corpus to verify the ~8.6% CER baseline.
  2. **Tokenizer Ablation**: Implement `SegMel` tokenizer and compare WER/CER against `BPE/500`. Hypothesis: WER decreases, CER increases slightly.
  3. **Metric Correlation**: Calculate Morph-F1 and Sparsity for `SegMel` and `BPE` to confirm the negative correlation with WER found in the paper.

## Open Questions the Paper Calls Out
None

## Limitations
- Data scarcity and generalization: Single language (Yoloxóchitl Mixtec) with 52.4 hours of speech limits generalizability
- Computational overhead: Sequence-of-Processes tokenizer requires 9 hours preprocessing vs 2.5 minutes for Segment-and-Melody
- Evaluation scope: Focuses on ASR metrics without testing downstream tasks like machine translation

## Confidence
- **High confidence**: The mechanism by which nonlinear tokenization prevents arbitrary subword merges (Mechanism 1) is well-supported by experimental results
- **Medium confidence**: The WFST-LM rescoring approach (Mechanism 2) shows promise but has higher computational costs and hasn't been fully optimized
- **Low confidence**: The scalability of these approaches to larger datasets and different non-concatenative languages remains untested

## Next Checks
1. **Cross-linguistic validation**: Apply the Segment-and-Melody tokenizer to another non-concatenative language (e.g., Somali, Arabic, or Navajo) with similar morphological complexity but different phonological properties to test generalizability.
2. **Computational optimization**: Implement FST determinization and pruning strategies to reduce the preprocessing time of the Sequence-of-Processes tokenizer from 9 hours to under 30 minutes while maintaining performance.
3. **Downstream task evaluation**: Test the novel tokenizers in a machine translation pipeline where Yoloxóchitl Mixtec speech is translated to Spanish text, measuring whether the morphological awareness improves translation quality beyond ASR performance.