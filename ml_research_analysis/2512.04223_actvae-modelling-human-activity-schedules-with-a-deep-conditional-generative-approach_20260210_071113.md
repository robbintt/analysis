---
ver: rpa2
title: 'ActVAE: Modelling human activity schedules with a deep conditional generative
  approach'
arxiv_id: '2512.04223'
source_url: https://arxiv.org/abs/2512.04223
tags:
- schedules
- activity
- labels
- generative
- actvae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce ActVAE, a deep conditional generative model for activity
  schedules, combining structured latent generative modeling (VAE) with conditional
  capability. This allows schedules to be generated based on input labels such as
  age or employment status.
---

# ActVAE: Modelling human activity schedules with a deep conditional generative approach

## Quick Facts
- **arXiv ID**: 2512.04223
- **Source URL**: https://arxiv.org/abs/2512.04223
- **Reference count**: 14
- **Key outcome**: ActVAE combines VAE with conditional capability to generate realistic, diverse activity schedules based on socio-demographic labels, outperforming purely generative and purely conditional baselines

## Executive Summary
ActVAE introduces a conditional variational autoencoder architecture for generating human activity schedules based on individual characteristics like age, employment status, and income. The model learns to separate label-driven variation from random variation through structured latent representation, enabling controllable generation while maintaining diversity. Evaluated against two baselines on the 2023 UK National Travel Survey, ActVAE demonstrates superior performance in both density estimation and conditional response, particularly where label effects are strong. The authors find that random variation accounts for most schedule diversity, suggesting label information alone is insufficient for realistic schedule generation.

## Method Summary
ActVAE is a conditional VAE that generates 24-hour activity schedules as sequences of activity-duration pairs conditioned on individual socio-demographic labels. The architecture uses LSTM-based encoders and decoders with label embeddings added to initial hidden states, a 6-dimensional latent space, and a combined loss function balancing reconstruction accuracy, duration prediction, and KL divergence regularization. The model is trained on 59,265 UK NTS schedules using inverse label frequency weighting, with teacher forcing during training and random latent sampling during generation.

## Key Results
- ActVAE generates more realistic and diverse schedules than both purely generative (GenerativeRNN) and purely conditional (ConditionalRNN) baselines
- Label-driven variation explains only ~16% of schedule variation, with random variation being relatively more significant
- The model shows strong conditional response where label effects are strong, particularly for work status and age
- ActVAE is practical for transport demand modeling, trains rapidly, and scales to large populations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAE's structured latent representation enables data-efficient density estimation for high-dimensional schedule distributions
- Mechanism: The VAE learns p(x) via p(x|z) where z follows a standard normal prior. This compressed latent space allows the model to interpolate between observed schedules rather than memorizing them, enabling tractable likelihood estimation during training
- Core assumption: The true schedule distribution can be approximated by decoding from a low-dimensional Gaussian latent space
- Evidence anchors: [abstract] "combining structured latent generative modeling (VAE) with conditional capability"; [section 3.3] "VAEs are the most data-efficient generative architecture" with "good density estimation"

### Mechanism 2
- Claim: Conditional generation requires disentangling label information from the latent representation
- Mechanism: CVAE extends VAE by conditioning encoder as qϕ(z|x,y) and decoder as pθ(x|z,y). Controllable generation requires z ⊥ y, achieved through DKL regularization that discourages label leakage into z. The β hyperparameter controls the strength of this disentanglement
- Core assumption: Label-driven schedule variation can be separated from random variation through the loss function's KL divergence term
- Evidence anchors: [abstract] "generating more realistic and diverse schedules than both baselines, particularly where label effects are strong"; [section 4.3] "For controllable generation, i.e. where new samples can be generated for specific labels, the known latents and random latents need to be independent"

### Mechanism 3
- Claim: Combining explicit generation with conditionality outperforms purely conditional or purely generative approaches for activity schedule modeling
- Mechanism: Explicit generation models the full distribution p(x|y), capturing unexplained variance. Conditional-only models output only the most likely schedule, losing diversity. Generative-only models cannot respond to label changes. ActVAE combines both: labels provide conditional signal, latent z captures remaining randomness
- Core assumption: Schedule variation has both label-explainable and random components; both are necessary for realistic generation
- Evidence anchors: [abstract] "ActVAE generates more realistic and diverse schedules than both baselines"; [section 7.1] "This suggests that the random variation of schedules, i.e. variation that is not explainable by labels, is relatively significant"

## Foundational Learning

- **Concept**: Variational Auto-Encoders and the Evidence Lower Bound (ELBO)
  - Why needed here: ActVAE is fundamentally a CVAE; understanding reconstruction vs. regularization tradeoff (controlled by β) is essential for tuning
  - Quick check question: Can you explain why minimizing DKL(qϕ(z|x) || p(z)) is necessary for generation, but too much regularization hurts reconstruction?

- **Concept**: Conditional generative models and the independence assumption p(z,y) = p(z)p(y)
  - Why needed here: The paper's core challenge is maintaining this independence for controllable generation; failure leads to inability to generate for specific labels
  - Quick check question: Why can't you just remove conditionality from the encoder to ensure z ⊥ y?

- **Concept**: Activity schedule representation (sequence of activity-duration pairs)
  - Why needed here: The paper uses a specific continuous encoding (discrete activity type + continuous duration); alternative encodings (purely discrete, aggregated) have different data efficiency properties
  - Quick check question: How does the maximum sequence length L=16 affect the model's ability to represent complex schedules?

## Architecture Onboarding

- **Component map**: Raw schedule + labels -> Encoder LSTM (4 layers, 256 hidden) -> Latent block (μ, σ) -> Sample z -> Decoder LSTM (4 layers, 256 hidden) + label embedding -> Generated schedule
- **Critical path**: Training → encoder processes [schedule, labels] → latent z is sampled → decoder generates schedule from [z, labels] → loss backpropagates. At inference: sample z ~ N(0,I), decode with target labels
- **Design tradeoffs**:
  - Higher β → better disentanglement but worse reconstruction
  - Larger label embedding H → better conditionality but may degrade generation quality
  - Addition vs. concatenation for label injection: addition preserves generative capability better
  - Latent size: Paper uses 6; larger may improve expressiveness but reduce data efficiency
- **Failure signatures**:
  - Poor conditional response: Labels leak into latent (check I(z;y) via MINE estimation)
  - Homogeneous outputs: β too high or latent size too small
  - Infeasible schedules: Consecutive home/work/education activities generated
  - Underestimates label effects by ~10%: Incomplete disentanglement (expected per Section 7.6)
- **First 3 experiments**:
  1. Reproduce baseline comparison (ActVAE vs. ConditionalRNN vs. GenerativeRNN) on a held-out subset to validate implementation
  2. Ablate β: Train with β ∈ {0.001, 0.01, 0.1} and measure both joint density estimation (Djoint) and conditional response magnitude
  3. Test label injection strategy: Compare addition vs. concatenation at decoder hidden layer, measuring I(z;y) and conditional capability on work status label

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supplementary classifier models provide a more robust quantitative evaluation of the conditional capability of schedule generators?
- Basis in paper: [explicit] Section 8.1 states that quantifying conditional capability is challenging because standard metrics are influenced by generative performance. The authors explicitly suggest adopting supplementary classifier models (e.g., "does this schedule look like it belongs to an employed person?") as a useful avenue of progress
- Why unresolved: Current evaluation methods, such as mutual information estimation and joint density estimation, struggle to isolate conditional capability from random schedule variation, making it difficult to validate that the model responds correctly to input labels
- What evidence would resolve it: Development of a classifier-based evaluation framework that successfully distinguishes between conditional adherence and random variance in generated schedules

### Open Question 2
- Question: Does explicitly modelling time as a budget or incorporating time-of-day context improve the accuracy of short-duration, structured activity timings?
- Basis in paper: [explicit] Section 8.1 notes that "activity times, particularly short-duration structured activities, such as escort... are still imperfect." The authors propose that architectures incorporating better time-of-day context or explicit time budgets may resolve this
- Why unresolved: The current ActVAE architecture treats durations as continuous values without hard time constraints, leading to inaccuracies in activities with strict temporal structures (e.g., school runs)
- What evidence would resolve it: An ablation study comparing the current continuous duration encoding against a model with an explicit time-budget constraint or specialized temporal embeddings, showing improved Earth Mover Distance (EMD) for timing metrics

### Open Question 3
- Question: Can a conditional VAE architecture be effectively scaled to jointly model household interactions, multi-day durations, and trip attributes (mode, location)?
- Basis in paper: [explicit] Section 8.1 states, "Our existing work is limited to 24-hour scheduling of individuals... future work should explore the approach for jointly modelling households, longer durations and additional features such as trips, modes, distances and locations"
- Why unresolved: The authors note that combining these features into a single deep generative model is challenging without "dramatic increases in data availability," which limits the feasibility of training such models on current datasets
- What evidence would resolve it: A demonstration of a multi-modal ActVAE that generates joint schedules for households or multi-day periods while maintaining density estimation performance comparable to individual, single-day models

## Limitations

- The paper's key innovation (disentangling label effects from random variation via CVAE) lacks direct corpus support and appears to be a novel theoretical contribution without established precedents
- Data access barrier (UK NTS 2023 requires credentials) prevents independent validation of the exact preprocessing pipeline and sample statistics
- Quantitative claim that "random variation is relatively more significant than label-driven variation" (~16% explained by labels) cannot be fully verified due to data access limitations

## Confidence

- **High confidence**: VAE as a data-efficient architecture for density estimation (supported by established literature and the paper's architecture choices)
- **Medium confidence**: The combined generative-conditional approach outperforms purely generative or purely conditional baselines (supported by internal comparisons but not external validation)
- **Low confidence**: The quantitative claim that "random variation is relatively more significant than label-driven variation" (~16% explained by labels) due to the data access barrier preventing verification of the exact preprocessing and evaluation methodology

## Next Checks

1. **Data pipeline verification**: Obtain UK NTS 2023 access credentials and implement the PAM-based preprocessing pipeline to verify that the reported sample statistics (59,265 schedules) and encoding scheme (continuous activity-duration pairs, max length 16) match the paper's description

2. **Disentanglement sensitivity**: Systematically vary β in {0.001, 0.01, 0.1} while measuring both joint density estimation performance and conditional response magnitude for the work status label, confirming that β=0.01 provides optimal tradeoff between reconstruction quality and label disentanglement

3. **Alternative label injection comparison**: Implement and compare label addition vs. concatenation at the decoder hidden layer, measuring mutual information I(z;y) and conditional capability on the age label to verify the paper's claim that addition better preserves generative capability