---
ver: rpa2
title: KIT's Offline Speech Translation and Instruction Following Submission for IWSLT
  2025
arxiv_id: '2505.13036'
source_url: https://arxiv.org/abs/2505.13036
tags:
- translation
- speech
- language
- track
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the KIT submissions for the IWSLT 2025 Offline
  Speech Translation and Instruction Following tracks, leveraging large language models
  (LLMs) to enhance performance across both tasks. For the Offline ST track, a cascaded
  pipeline fuses outputs from multiple ASR systems using an LLM with document-level
  context, followed by translation and automatic post-editing refinement.
---

# KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025

## Quick Facts
- arXiv ID: 2505.13036
- Source URL: https://arxiv.org/abs/2505.13036
- Reference count: 16
- Primary result: KIT achieved strong performance in both IWSLT 2025 Offline Speech Translation and Instruction Following tracks using cascaded and end-to-end Speech-LLM architectures respectively.

## Executive Summary
This paper describes KIT's submissions for the IWSLT 2025 Offline Speech Translation (ST) and Instruction Following (IF) tracks. For Offline ST, the system uses a cascaded approach with document-level context fusion of multiple ASR outputs via LLM, followed by translation and automatic post-editing refinement. For the IF track, an end-to-end Speech-LLM integrates a speech encoder with an LLM, trained with contrastive pretraining and augmented with task-specific data. The systems demonstrate improved translation quality, ASR robustness, and generalization across languages and tasks, with post-editing enhancing fluency and context recovery.

## Method Summary
The KIT submission employs two distinct architectures for the two tracks. The Offline ST track uses a cascaded pipeline where audio is segmented via VAD, processed by multiple ASR systems (Whisper v2, v3, Phi-4, and fine-tuned variants), fused using an LLM with document-level context, translated using a fine-tuned MT model, and refined through automatic post-editing. For the IF track, an end-to-end Speech-LLM approach integrates a frozen SeamlessM4T encoder with a frozen LLaMA-3.1 LLM through a trained Q-Former projector, using contrastive pretraining and task-specific fine-tuning. Both systems incorporate quality-filtered fine-tuning and are evaluated on multiple datasets including Bazinga, EPTV, and ITV.

## Key Results
- LLM-based fusion of multiple ASR outputs improved downstream translation quality (MetricX from 2.27→2.01 for ACL) despite no WER improvement
- Contrastive pretraining significantly outperformed no pretraining and ASR-only pretraining in the IF track (18.82 WER vs 25.1 WER)
- Automatic post-editing improved translation quality across most language pairs (MetricX from 2.01→1.84 for ACL)
- Document-level processing and quality-filtered fine-tuning consistently enhanced performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fusing multiple ASR system outputs via an LLM with document-level context improves downstream speech translation quality, even when individual ASR WER is not improved.
- Mechanism: Different ASR systems make different error types (Table 2 analysis). An LLM processes combined hypotheses from Whisper v2, v3, Phi-4, and fine-tuned variants at document level, leveraging broader context to produce more coherent transcripts that translate better.
- Core assumption: ASR errors that impact translation quality (e.g., punctuation, casing inconsistencies) may not be captured by WER but are correctable via LLM fusion.
- Evidence anchors:
  - [section] Table 2: LLM-Fuse WER (17.03 ITV, 10.77 ACL) does not outperform individual systems; Table 4: ST MetricX improves from 2.27→2.01 (ACL) with LLM-Fuse before APE.
  - [section] Section 2.2: "different models tend to make different types of errors, suggesting that combining these systems could be a promising strategy."
  - [corpus] Related IWSLT 2025 submissions (CUNI, IT-IST) similarly leverage multiple ASR components, but corpus does not directly validate fusion-to-ST-quality transfer.
- Break condition: If ASR systems produce highly correlated errors rather than complementary ones, fusion gains diminish. If target LLM lacks sufficient document-level training data, fusion may introduce artifacts.

### Mechanism 2
- Claim: Contrastive pretraining of the speech-to-LLM projector improves downstream instruction-following task performance compared to direct fine-tuning or ASR-only pretraining.
- Mechanism: A Q-Former projector (4 transformer layers, 4 query tokens) bridges SeamlessM4T encoder features to LLaMA-3.1-8B. Contrastive pretraining using ASR data with cosine similarity or Wasserstein loss aligns speech and text representations before task-specific fine-tuning.
- Core assumption: The alignment learned during contrastive pretraining transfers across tasks (ASR, ST, SQA, SSUM) even when fine-tuning data is limited or domain-shifted.
- Evidence anchors:
  - [section] Table 5: Contrastive pretraining (18.82 WER, 77.31 COMET en-de) outperforms no pretrain (25.1 WER, 72.49 COMET) and ASR pretrain (21.42 WER, 76.72 COMET).
  - [section] Section 3.2: "contrastive pretraining yields notable improvements over the other training strategies."
  - [corpus] IT-IST submission uses continuous speech encoder integration with LLM; NAVER submission aligns speech/text for multi-task learning—consistent pattern but no direct validation of contrastive loss specifically.
- Break condition: If contrastive pretraining data distribution differs substantially from downstream task domains, alignment may not transfer. Wasserstein loss showed mixed results across tasks (Table 5), suggesting sensitivity to loss choice.

### Mechanism 3
- Claim: Automatic Post-Editing (APE) using a larger LLM refines translations by correcting terminology errors and recovering context lost during audio segmentation.
- Mechanism: A Tower 13B model is fine-tuned on synthetic (source, hypothesis, reference) triplets generated from the MT system. It receives both source transcript and machine-translated output, producing polished translations at document level.
- Core assumption: The larger 13B model can learn error correction patterns from limited synthetic data (100k triplets) via LoRA adapters.
- Evidence anchors:
  - [section] Table 4: APE improves MetricX from 2.01→1.84 (ACL) and 4.12→4.03 (ITV).
  - [section] Section 2.3.3: "We choose the larger 13B model for this task, as we expect it to be adaptable to correct the output with limited fine-tuning."
  - [corpus] Weak corpus validation—related papers do not explicitly evaluate APE for ST refinement.
- Break condition: If synthetic APE data does not reflect real error distributions, model may over-correct or introduce hallucinations. For en→zh (Table 8), post-editing did not improve performance, suggesting language-specific limitations.

## Foundational Learning

- Concept: **Q-Former as Modality Bridge**
  - Why needed here: Connects frozen speech encoder (SeamlessM4T) to frozen LLM (LLaMA-3.1) in constrained IF track where only these components are permitted.
  - Quick check question: Can you explain why only the Q-Former is trained while encoder and LLM remain frozen?

- Concept: **Quality-Filtered Fine-tuning**
  - Why needed here: Parallel data contains noisy pairs; using XCOMET to select top 500k high-quality pairs improves MT adaptation over raw data.
  - Quick check question: Why might lower WER (Phi-4: 9.71 ACL) not correlate with better translation quality (Table 3)?

- Concept: **Voice Activity Detection with Length Constraints**
  - Why needed here: Long-form audio must be segmented; VAD provides cut points, but length constraints (merging short segments, splitting long ones) optimize ASR performance.
  - Quick check question: Why does a chunk size of 25 seconds outperform shorter chunks (Table 1) in noisy conditions?

## Architecture Onboarding

- Component map:
  ```
  Offline Track:
    Audio → VAD Segmentation → [Whisper v2, Whisper v3, Phi-4, Whisper v2+FT]
         → LLM Fuse (Llama-3-8B LoRA) → Sentence Segmentation
         → MT (Tower 7B LoRA, quality-filtered) → APE (Tower 13B LoRA) → Output

  IF Track:
    Audio → VAD Segmentation (ASR/ST) OR 60s Chunks (SQA/SSUM)
         → SeamlessM4T Encoder (frozen) → Q-Former (trained)
         → LLaMA-3.1-8B (frozen) → Post-Edit LLM (LLaMA-3.1-8B LoRA) → Output
  ```

- Critical path:
  1. Segmentation quality directly impacts all downstream tasks—optimize chunk size per task (20s for ASR, 25s for ST, 60s for SQA/SSUM).
  2. Contrastive pretraining must precede fine-tuning for IF track—skip direct fine-tuning.
  3. Post-editing context window size differs by task (5 sentences for ASR, 15 for ST)—tune separately.

- Design tradeoffs:
  - Cascaded (Offline) vs. End-to-End (IF): Cascaded allows specialized data per component; End-to-End enables multi-task generalization.
  - Wasserstein vs. Cosine contrastive loss: Wasserstein slightly better for some tasks (Table 5), but results are mixed—cosine is safer default.
  - Gold segmentation vs. VAD segmentation: VAD causes performance drop (Table 6); post-editing partially recovers but adds inference cost.

- Failure signatures:
  - ST quality drops with Phi-4 ASR despite lower WER (Table 3): Check punctuation/casing consistency.
  - Post-editing degrades en→zh performance (Table 8): Post-editing model lacks Chinese capability—skip for unsupported languages.
  - IF model struggles with unanswerable SQA questions: Apply chain-of-thought tagging (Section 3.2).

- First 3 experiments:
  1. Replicate segmentation ablation (Table 1) on your target domain to determine optimal chunk size before training ASR systems.
  2. Train Q-Former with contrastive pretraining using both cosine and Wasserstein losses; compare on held-out ASR/ST sets to select best approach.
  3. Generate synthetic APE data from your MT output and evaluate whether Tower 13B LoRA fine-tuning improves MetricX by >0.1 points before committing to APE pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SHAS semantic segmentation perform when trained specifically on noisy audio data compared to current VAD-based approaches?
- Basis in paper: [explicit] "while we did not explore it in this work, it is unclear how well SHAS segmentation performs when trained on noisy data. Semantic segmentation of noisy inputs could yield performance gains."
- Why unresolved: The authors only evaluated VAD and end-to-end speaker segmentation methods; SHAS was excluded because it "underperform[s] in the presence of background noise" with current training, but training on noisy data was not explored.
- What evidence would resolve it: Ablation experiments comparing SHAS trained on noisy datasets (e.g., Bazinga, ITV, EPTV) against the current VAD-based segmentation on WER and downstream ST metrics.

### Open Question 2
- Question: Can target-language-specific LLMs (e.g., German-specialized models) improve document-level APE performance compared to multilingual models?
- Basis in paper: [explicit] "incorporating LLM specific to the target language (e.g. German LLM) for APE at the document level could offer promising improvements."
- Why unresolved: The current APE model (Tower 13B) is multilingual; the authors hypothesize language-specific models may better capture document-level context and terminology.
- What evidence would resolve it: Comparative evaluation of German-specific LLMs versus Tower 13B on document-level APE for English→German ST, measuring COMET, MetricX, and ChrF2.

### Open Question 3
- Question: Why does lower ASR WER (e.g., Phi-4 on ACL) not consistently correlate with better downstream translation quality?
- Basis in paper: [inferred] "translation quality is lower when using transcripts from the Phi-4 ASR model, despite it having the lowest WER in Table 2. We hypothesize that this is due to inconsistencies in punctuation and casing, which are not captured by WER but can impact translation quality."
- Why unresolved: The correlation between ASR metrics and ST quality remains poorly understood; WER may not capture error types most harmful to MT.
- What evidence would resolve it: Fine-grained error analysis comparing Phi-4 vs. Whisper outputs on punctuation, casing, and semantic errors, with controlled experiments measuring translation impact.

### Open Question 4
- Question: Can a unified architecture achieve both high translation quality (as in cascaded Offline systems) and flexible instruction-following (as in end-to-end IF systems)?
- Basis in paper: [explicit] "For future work, we aim to explore a unified architecture capable of producing high-quality translations while also supporting instruction-following capabilities."
- Why unresolved: Current work uses separate architectures optimized for different constraints (cascaded for quality, end-to-end for flexibility); no single model has been shown to excel at both.
- What evidence would resolve it: Design and evaluation of a hybrid model combining document-level refinement modules with end-to-end Speech-LLM training, tested on both Offline ST and IF benchmarks.

## Limitations

- ASR-to-ST quality transfer remains poorly understood as WER improvements do not correlate with translation quality gains
- Post-editing effectiveness varies significantly across language pairs, with en→zh showing degradation
- The specific error types that ASR fusion corrects but WER misses are not analyzed in detail

## Confidence

**High Confidence:** The cascaded pipeline architecture (VAD→ASR fusion→MT→APE) and its general effectiveness across constrained settings. The segmentation size optimization (25s chunks) and quality-filtered fine-tuning approach are well-supported by ablation results.

**Medium Confidence:** The contrastive pretraining improvements and their generalizability across tasks. While Table 5 shows consistent gains, the mechanism explaining why contrastive learning transfers better than direct fine-tuning is not fully articulated.

**Low Confidence:** The specific error types that ASR fusion corrects but WER misses, and why post-editing fails for certain language pairs. These represent gaps in understanding rather than implementation issues.

## Next Checks

1. **Error Type Analysis:** Conduct a detailed error analysis comparing individual ASR outputs to LLM-fused outputs, categorizing errors by type (punctuation, casing, named entities, etc.) and measuring their impact on translation quality. This will validate whether WER misses error types that matter for translation.

2. **Cross-Lingual Pretraining Study:** Evaluate the contrastive pretraining approach across multiple language pairs (including en→zh) to determine whether gains transfer or are language-specific. Test whether additional language-specific pretraining data improves performance on underperforming pairs.

3. **Post-Editing Generalization Test:** Generate synthetic APE data for multiple language pairs including en→zh and evaluate whether fine-tuning separate post-editing models per language pair recovers the performance drop, or whether the degradation indicates fundamental limitations in the approach.