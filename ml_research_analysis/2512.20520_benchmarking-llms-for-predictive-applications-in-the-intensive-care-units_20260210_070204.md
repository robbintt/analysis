---
ver: rpa2
title: Benchmarking LLMs for Predictive Applications in the Intensive Care Units
arxiv_id: '2512.20520'
source_url: https://arxiv.org/abs/2512.20520
tags:
- llms
- data
- loss
- clinical
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks Large Language Models (LLMs) like GatorTron,
  Llama, and Mistral against Small Language Models (SLMs) such as BioBERT and Word2Vec
  for predicting shock in ICU patients. Using MIMIC-III data, embeddings from physician
  notes and vitals were extracted and fed into classifiers (Random Forest, Gradient
  Boosting, etc.).
---

# Benchmarking LLMs for Predictive Applications in the Intensive Care Units

## Quick Facts
- arXiv ID: 2512.20520
- Source URL: https://arxiv.org/abs/2512.20520
- Authors: Chehak Malhotra; Mehak Gopal; Akshaya Devadiga; Pradeep Singh; Ridam Pal; Ritwik Kashyap; Tavpritesh Sethi
- Reference count: 24
- One-line primary result: GatorTron achieved highest weighted recall (80.5%) for predicting shock in ICU patients, but overall LLM performance was comparable to SLMs.

## Executive Summary
This study benchmarks Large Language Models (LLMs) including GatorTron, Llama, and Mistral against Small Language Models (SLMs) like BioBERT and Word2Vec for predicting shock in ICU patients using MIMIC-III data. The research extracts embeddings from physician notes and vitals, then feeds them into traditional classifiers (Random Forest, Gradient Boosting) to predict shock index abnormalities. GatorTron achieved the highest weighted recall of 80.5%, but overall performance was comparable between LLMs and SLMs. Fine-tuning with focal loss addressed class imbalance but didn't significantly improve results, suggesting that LLMs are not inherently superior to SLMs for clinical prediction tasks.

## Method Summary
The study uses MIMIC-III v1.4 to identify ICU stays longer than 24 hours, calculating Shock Index (SI = HR/SBP) from vital signs. Patients are labeled "abnormal" if SI ≥ 0.7 for more than 30 minutes after 24 hours of normal SI. Clinical notes are preprocessed by lowercasing, removing stop words/punctuation, masking encrypted text, and masking specific diagnoses and drugs. GatorTron-Base is used to extract embeddings from "Therapeutics" (512-token window) and "History of Present Illness" sections (chunked and mean-pooled). The pipeline includes ExtraTreesClassifier for feature selection (top 100), SMOTE for class imbalance, and Random Forest classification with stratified 80:20 patient-level splitting and 100 bootstrap iterations.

## Key Results
- GatorTron Base achieved the highest weighted recall of 80.5% (0.805 ± 0.0152) with Random Forest classifier
- Overall LLM performance was comparable to SLMs, suggesting LLMs are not inherently superior for clinical prediction tasks
- Fine-tuning with focal loss did not yield significant improvements, likely due to limited fine-tuning cohort size
- Model scale alone (LLM parameter count) did not guarantee superior predictive performance

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained clinical language model embeddings encode predictive signals from unstructured clinical text that can be extracted for downstream classification tasks. LLMs (GatorTron, Llama 8B, Mistral 7B) process clinical notes (HOPI, therapeutics) to generate contextual embeddings. These embeddings serve as numerical features for traditional ML classifiers (Random Forest, XGBoost, Gradient Boosting) to predict shock index abnormalities, decoupling representation learning from classification. Core assumption: Clinical text contains latent predictive information about patient trajectories that LLMs can capture and transfer to tabular prediction tasks without task-specific architecture modifications. Evidence anchors: GatorTron achieved highest weighted recall (80.5%); Paging Dr. GPT paper demonstrates extracting information from clinical notes enhances patient predictions. Break condition: If clinical notes lack temporal predictive signals relevant to shock onset, or if embeddings fail to capture task-specific features, performance degrades to baseline levels.

### Mechanism 2
Model scale alone does not guarantee superior predictive performance for clinical trajectory forecasting tasks. LLMs (billions of parameters) and SLMs (BioBERT, Word2Vec) were benchmarked identically on shock prediction. Despite parameter count differences, both achieved comparable performance, suggesting current pre-training objectives don't translate to temporal prediction advantages. Core assumption: LLM pre-training objectives (language modeling, NER, phenotyping) may not develop representations suited for longitudinal patient trajectory prediction. Evidence anchors: Overall performance was comparable between LLMs and SLMs; even large language models trained on millions of parameters could not perform better than small language models; MIMIC-Sepsis benchmark shows similar trends in ICU trajectory modeling tasks. Break condition: If LLMs receive task-specific pre-training or fine-tuning on trajectory prediction objectives, their performance advantage may emerge over SLMs.

### Mechanism 3
Fine-tuning on small clinical datasets can degrade performance compared to using frozen pre-trained embeddings. Fine-tuning LLMs with focal loss or cross-entropy on limited ICU cohorts (<500 samples) led to overfitting (validation loss increased across epochs), while frozen pre-trained embeddings maintained better generalization across metrics. Core assumption: Small fine-tuning cohorts provide insufficient signal to adapt large models without catastrophic forgetting or overfitting to noise. Evidence anchors: Non-fine-tuned models consistently delivered superior or comparable performance; fine-tuning did not yield significant improvements due to limited fine-tuning cohort size. Break condition: With larger fine-tuning datasets (thousands of patients) or regularization strategies tailored to clinical data, fine-tuning may improve over frozen embeddings.

## Foundational Learning

- **Shock Index (SI) as clinical endpoint**: Why needed here: SI (heart rate / systolic blood pressure) defines the binary classification target (normal <0.7 vs abnormal ≥0.7), grounding the prediction task in clinical physiology. Quick check question: What physiological state does an elevated shock index indicate, and why is early prediction clinically valuable?

- **Class imbalance handling in clinical ML**: Why needed here: Dataset has ~4:1 imbalance (355 normal vs 87 abnormal SI patients); study uses focal loss and SMOTE to prevent majority-class bias. Quick check question: How does focal loss mathematically differ from cross-entropy, and what parameter controls its focusing strength?

- **Embedding extraction from transformer models**: Why needed here: Study extracts fixed-dimensional embeddings from variable-length clinical notes using [CLS] token pooling or mean aggregation, enabling downstream classification. Quick check question: How do you handle documents exceeding the model's maximum token length (e.g., 512 for GatorTron) during embedding extraction?

## Architecture Onboarding

- **Component map:** Data Layer (MIMIC-III notes + vitals) → Label Engine (SI calculation → labeling) → Feature Extractor (LLMs/SLMs → embeddings) → Classifier Layer (ExtraTrees feature selection → SMOTE → ensemble classifiers) → Evaluation (stratified split → bootstrap iterations)

- **Critical path:** 1) Cohort curation: Filter ICU stays >24h, compute SI labels, map physician notes to next-day labels; 2) Embedding generation: Chunk long texts for GatorTron (512 limit), use full context for Llama/Mistral (4096); 3) Classification pipeline: Feature selection → SMOTE → train classifier → evaluate on held-out test set

- **Design tradeoffs:** Frozen vs fine-tuned embeddings: Frozen preferred for small data; fine-tuning risks overfitting with limited samples. LLM vs SLM selection: LLMs offer longer context windows but SLMs achieve comparable accuracy with lower compute cost. Focal loss vs cross-entropy: Focal loss improves specificity on minority class but may reduce overall AUC-ROC

- **Failure signatures:** Validation loss increasing across epochs → overfitting during fine-tuning (mitigate with dropout, early stopping). High recall but low specificity → class imbalance not fully resolved (adjust focal loss gamma, SMOTE ratio). LLM underperforming SLM → pre-training objective mismatch with prediction task (consider trajectory-specific pre-training)

- **First 3 experiments:** 1) Baseline replication: Train Random Forest on BioClinicalBERT+DocBERT embeddings with SMOTE, evaluate weighted recall and AUC-ROC; 2) Fine-tuning ablation: Compare frozen GatorTron embeddings vs focal-loss fine-tuned on same train/test splits, track validation loss curves; 3) Context window analysis: Test whether Llama's 4096-token context improves over GatorTron's 512-token limit by evaluating on notes with varying lengths

## Open Questions the Paper Calls Out

- Does training LLMs specifically on predictive clinical trajectory tasks yield significantly better performance than models trained on cross-sectional tasks like named entity recognition? The authors conclude that future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping. Current LLMs are primarily trained on static text comprehension rather than temporal patient trajectory modeling, which may explain why they failed to outperform SLMs in this study. A comparative study where an LLM is pre-trained specifically on temporal clinical outcomes versus a standard clinically pre-trained LLM, evaluated on the same predictive benchmark would resolve this.

- Can fine-tuning strategies effectively improve LLM performance for predictive tasks when applied to larger, more diverse patient cohorts? The authors identify the "small dataset size" as a key limitation that constrained our ability to fine-tune LLMs effectively, noting that fine-tuning did not yield consistent improvements over pre-trained embeddings. The study utilized a small cohort (442 total patients) relative to the parameter size of LLMs like Llama 8B, potentially leading to overfitting or failure to converge during fine-tuning. Replication of the benchmark using the full MIMIC dataset or a larger multi-center ICU dataset would determine if fine-tuning provides statistically significant gains with more data.

- Do the comparable performance metrics between LLMs and SLMs generalize to other critical care prediction tasks beyond shock index? The study focused exclusively on shock index prediction. The authors state the need to determine where these models stand in predictive contexts, implying uncertainty about whether these results hold for other complex clinical events (e.g., sepsis or mortality). Shock prediction is a specific physiological decompensation task; it remains unknown if the LLMs' advanced contextual understanding provides an advantage in predicting events that rely more heavily on complex narrative reasoning or longitudinal history. Applying the same LLM vs. SLM benchmarking methodology to a diverse set of ICU prediction targets would resolve this.

## Limitations

- Single clinical prediction task (shock index) on one dataset (MIMIC-III) limits generalizability of findings
- Absence of temporal modeling architectures (LSTM, transformer decoders) represents missed opportunity to leverage LLMs' sequential processing capabilities
- Study doesn't explore ensemble methods combining LLM and SLM embeddings that might capture complementary information

## Confidence

- **High confidence**: GatorTron's superior performance (80.5% weighted recall) over other models when using frozen embeddings; the failure of fine-tuning on small datasets; comparable overall performance between LLMs and SLMs
- **Medium confidence**: The claim that LLMs are "not inherently superior" to SLMs for clinical prediction tasks, given the limited task complexity and single dataset evaluation
- **Low confidence**: The suggestion that future LLM training should focus on clinical trajectories rather than current objectives, as this extrapolation requires additional empirical validation across diverse clinical prediction tasks

## Next Checks

1. **Task Complexity Gradient**: Test the same LLM/SLM pipeline on increasingly complex clinical prediction tasks (e.g., multi-label diagnosis prediction, temporal disease progression) to determine whether LLM advantages emerge with task complexity.

2. **Temporal Modeling Integration**: Incorporate temporal neural architectures (transformer decoders, LSTMs) to leverage LLMs' sequential processing capabilities for longitudinal patient trajectory prediction, rather than treating embeddings as static features.

3. **Ensemble Method Exploration**: Combine LLM and SLM embeddings through weighted averaging or attention mechanisms to capture complementary information, potentially improving performance beyond either model class alone.