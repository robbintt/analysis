---
ver: rpa2
title: 'ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering'
arxiv_id: '2512.00725'
source_url: https://arxiv.org/abs/2512.00725
tags:
- clustering
- color
- embeddings
- dataset
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESMC, a method for user-driven clustering
  that leverages hidden states of text tokens in multimodal large language models
  (MLLMs) as feature-specific embeddings. The approach addresses the limitation of
  typical deep clustering methods by enabling clustering based on arbitrary user-defined
  criteria.
---

# ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering

## Quick Facts
- **arXiv ID:** 2512.00725
- **Source URL:** https://arxiv.org/abs/2512.00725
- **Reference count:** 40
- **Primary result:** ESMC achieves NMI scores of 0.8138 and 0.8817 on Stanford Cars color and type criteria, respectively, surpassing baseline methods.

## Executive Summary
This paper introduces ESMC, a method for user-driven clustering that leverages hidden states of text tokens in multimodal large language models (MLLMs) as feature-specific embeddings. The approach addresses the limitation of typical deep clustering methods by enabling clustering based on arbitrary user-defined criteria. ESMC extracts target embeddings from MLLM hidden states corresponding to user prompts and employs a lightweight clustering head with pseudo-label learning to enhance clustering accuracy. Extensive experiments on seven datasets demonstrate competitive performance across various clustering tasks, with notable improvements in Normalized Mutual Information (NMI) and Rand Index (RI) metrics.

## Method Summary
ESMC extracts target embeddings from specific hidden states of text tokens in MLLMs by using the "Logit Lens" technique to project intermediate activations to vocabulary space. The method identifies the token position and layer where generated keywords consistently yield high logits for the target attribute. These high-dimensional embeddings are then processed by a 2-layer MLP clustering head trained on pseudo-labels from the top α% of samples nearest to K-means centroids. The approach enables flexible, user-defined clustering criteria without requiring retraining of the MLLM backbone.

## Key Results
- ESMC achieves NMI scores of 0.8138 and 0.8817 on Stanford Cars color and type criteria, respectively
- The method demonstrates robustness to prompt variations and shows efficient training with a 2-layer MLP clustering head
- Performance improvements are consistent across seven benchmark datasets with various clustering tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hidden states of specific prompt tokens in MLLMs encode condensed visual feature representations of the input image.
- **Mechanism**: When an MLLM processes an image and a text prompt (e.g., "The color of the car is"), the internal transformer representation of the task-defining token ("color") is activated by corresponding visual features in the image.
- **Core assumption**: MLLM's cross-attention layers bind semantic concepts to visual features in a way that is extractable from specific token positions.
- **Evidence anchors**: Logit Lens visualizations show target token positions predict correct attribute values with high probability; MLLM hidden states are strongly related to corresponding features.

### Mechanism 2
- **Claim**: A lightweight 2-layer MLP trained on high-confidence pseudo-labels effectively projects high-dimensional MLLM embeddings into a separable clustering space.
- **Mechanism**: Raw MLLM embeddings are high-dimensional and noisy. By initializing clusters with K-means, selecting top α% of samples nearest to centroids as pseudo-labels, and training a shallow MLP, the system reduces dimensionality and refines cluster boundaries.
- **Core assumption**: Initial K-means centroids on MLLM embeddings are sufficiently accurate to provide clean training signals for the top α% of samples.
- **Evidence anchors**: Inclusion of clustering head leads to substantial improvements due to high dimensionality causing curse of dimensionality; pseudo-label learning enhances clustering accuracy.

### Mechanism 3
- **Claim**: Intermediate layer embeddings resist language bias better than final output text, providing more faithful visual feature representations.
- **Mechanism**: Final output layer prioritizes linguistic fluency and prior text probability, often ignoring image details. Intermediate layers retain raw fusion of visual and linguistic information before autoregressive generation dominates.
- **Core assumption**: Hallucination or generic description behavior emerges primarily in later/deeper layers or decoding stage.
- **Evidence anchors**: ESMC leverages intermediate hidden states to mitigate language bias; directly using MLLM output has risks for producing unstructured and generic image descriptions.

## Foundational Learning

- **Concept**: **Logit Lens**
  - **Why needed here**: Used to project intermediate hidden states back to vocabulary space to verify which token positions "light up" for specific visual features.
  - **Quick check question**: Can you explain how projecting a mid-layer transformer activation through the unembedding matrix ($W_u$) helps identify the semantic content of that layer?

- **Concept**: **Pseudo-Labeling**
  - **Why needed here**: Essential for training the lightweight clustering head without ground truth labels, using only most confident K-means assignments as training targets.
  - **Quick check question**: Why does limiting pseudo-labels to top α% closest to centroids improve training signal compared to using all samples?

- **Concept**: **Curse of Dimensionality in Clustering**
  - **Why needed here**: Explains why raw MLLM embeddings (dimension = vocabulary size, e.g., ~32k-128k) perform poorly with distance-based metrics like K-means.
  - **Quick check question**: How does distance concentration in high-dimensional spaces render Euclidean distance less discriminative for clustering?

## Architecture Onboarding

- **Component map**: Image + User Prompt -> LLaVA-1.5 Backbone -> Target Embedding Extractor -> 2-Layer MLP Head -> Final Clusters

- **Critical path**: The precise localization of the Target Embedding. The system relies on finding exact token index $k$ (in prompt sequence) and layer $l$ that correlates with visual feature. If this index is wrong, clustering operates on noise.

- **Design tradeoffs**:
  - Prompt Engineering vs. Robustness: Paper claims robustness to prompt variants, but embedding selection logic implicitly relies on stable tokenization.
  - MLP vs. Raw K-means: MLP head adds training time and complexity but required to counteract high dimensionality of vocabulary-space embeddings.

- **Failure signatures**:
  - Generic Outputs: If extracted embeddings result in one giant cluster, "criteria" token may not be attending to visual features (language dominance).
  - Feature Confusion: Clustering by "color" but getting "type" clusters implies embedding extraction index is picking up different token's representation.

- **First 3 experiments**:
  1. Token Sensitivity Heatmap: Run Logit Lens visualization to confirm target token (e.g., "color") actually predicts correct attribute values (e.g., "red") with high probability.
  2. Ablation on Layer Selection: Extract embeddings from layers 10, 20, 30, etc., and run simple K-means to plot performance vs. depth.
  3. Head Ablation: Compare raw K-means on extracted embeddings vs. MLP head with α=0.2 to quantify "curse of dimensionality" penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does correlation between text prompt embeddings and visual features generalize to MLLM architectures with early fusion mechanisms or non-LLaMA backbones?
- Basis in paper: Authors explicitly note in Section 5 that they studied LLaVA (LLaMA-based), which fuses text and image features at relatively late stage, leaving validity of method on other architectures unconfirmed.
- Why unresolved: Mechanism relies on how specific architectures inject visual tokens; different fusion timings or backbone types may not exhibit same distinct logit distributions for target embeddings.
- What evidence would resolve it: Experiments applying ESMC to early-fusion MLLMs (e.g., Git) or different foundation models (e.g., Gemma, GPT-based vision models) showing similar logit correlation and clustering performance.

### Open Question 2
- Question: Can target embedding localization step be automated without relying on external LLMs like GPT-4 for keyword generation?
- Basis in paper: Method currently requires GPT-4 to generate relevant keywords and sampling process to locate embeddings, which authors note may require manual verification if keywords do not overlap with ground truth.
- Why unresolved: Dependency on external, proprietary model for prompt analysis introduces complexity and potential failure points if external model's internal knowledge diverges from target MLLM's.
- What evidence would resolve it: Demonstration of fully self-contained pipeline where target MLLM autonomously identifies semantically relevant embeddings via internal activation patterns without external textual guidance.

### Open Question 3
- Question: Can internal embedding selection mechanism be effectively adapted for multimodal retrieval tasks?
- Basis in paper: Authors state in Section 5 that they believe MLLMs offer effective ways for tasks beyond clustering, specifically listing "retrieval" as domain for future work.
- Why unresolved: While method successfully isolates feature-specific embeddings for clustering, unknown if these embeddings maintain sufficient semantic fidelity and discriminative power for precise image-text retrieval across large databases.
- What evidence would resolve it: Benchmarking selected embeddings on standard multimodal retrieval tasks (e.g., COCO, Flickr30k) to evaluate if they outperform standard pooled representations.

## Limitations
- The core claim relies heavily on assumption that MLLM architecture reliably fuses visual information into designated token positions, which is not explicitly verified beyond Logit Lens visualizations
- Pseudo-labeling approach inherits limitations of K-means initialization and may struggle with complex, non-globular cluster structures
- Robustness claims to arbitrary prompt variations and performance on truly novel or highly complex clustering tasks remain largely theoretical

## Confidence
- **High Confidence**: Mechanism of using Logit Lens to identify semantically relevant token positions and empirical clustering performance on standard benchmarks
- **Medium Confidence**: Claim that intermediate layer embeddings resist language bias better than final outputs is plausible but direct comparative evidence between layers is limited
- **Low Confidence**: Robustness to arbitrary prompt variations and method's performance on truly novel or highly complex clustering tasks are asserted but not rigorously tested

## Next Checks
1. Cross-Architecture Token Localization: Apply Logit Lens methodology to different MLLM architecture (e.g., Flamingo or BLIP-2) to verify if feature-specific token activation is consistent or architecture-dependent
2. Adversarial Prompt Testing: Systematically vary prompt templates and measure variance in clustering performance to quantify true robustness to prompt engineering
3. Complex Cluster Structure Benchmark: Test ESMC on dataset with known non-globular or imbalanced cluster structures to assess limitations of K-means + pseudo-labeling approach