---
ver: rpa2
title: Modeling Professionalism in Expert Questioning through Linguistic Differentiation
arxiv_id: '2507.20249'
source_url: https://arxiv.org/abs/2507.20249
tags:
- professionalism
- question
- features
- questions
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a linguistic framework to model professionalism\
  \ in expert questioning, using financial analyst questions as a testbed. By annotating\
  \ structural and pragmatic features\u2014such as discourse regulators, prefaces,\
  \ and request types\u2014the authors show these elements strongly correlate with\
  \ both human judgments of professionalism and question origin (human vs."
---

# Modeling Professionalism in Expert Questioning through Linguistic Differentiation
## Quick Facts
- arXiv ID: 2507.20249
- Source URL: https://arxiv.org/abs/2507.20249
- Reference count: 5
- Primary result: 96% accuracy achieved using linguistic features to classify professional vs non-professional expert questions

## Executive Summary
This study introduces a linguistic framework to model professionalism in expert questioning, using financial analyst questions as a testbed. By annotating structural and pragmatic features—such as discourse regulators, prefaces, and request types—the authors show these elements strongly correlate with both human judgments of professionalism and question origin (human vs. LLM-generated). A Random Forest classifier trained solely on these interpretable features achieves 96% accuracy, outperforming both SVM and gemini-2.0-flash baselines. The results demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling, challenging the assumption that professional discourse requires complexity. Instead, conciseness and readability emerge as key indicators.

## Method Summary
The authors developed a linguistic framework by first annotating 400 questions from financial analyst conference calls with structural and pragmatic features including discourse regulators, prefaces, request types, and readability metrics. Human annotators assessed professionalism on a 5-point scale while an LLM model classified questions as human or AI-generated. The framework identified 22 linguistic features across four categories: structural elements (question count, sentence length), prefaces (discourse markers, acknowledgments), request types (clarity, specificity), and readability scores (Flesch-Kincaid, Coleman-Liau). A Random Forest classifier was trained on these features, achieving 96% accuracy in distinguishing professional from non-professional questions. The study validated that conciseness and clarity, rather than complexity, serve as primary indicators of professional questioning.

## Key Results
- Random Forest classifier achieved 96% accuracy using only interpretable linguistic features
- Human-generated questions significantly outperformed LLM-generated questions (gemini-2.0-flash) in professionalism classification
- Conciseness and readability emerged as stronger indicators of professionalism than linguistic complexity
- Strong correlation between annotated features and both human professionalism judgments and question origin classification

## Why This Works (Mechanism)
The framework succeeds because it captures the implicit structural and pragmatic conventions that characterize professional discourse in expert contexts. Financial analysts employ specific linguistic patterns—such as concise phrasing, appropriate discourse markers, and clear request structures—that signal domain expertise and communicative competence. These features create a recognizable "professional signature" that distinguishes expert questioning from casual or AI-generated alternatives. The approach works because it identifies learnable patterns rather than attempting to define professionalism through abstract criteria.

## Foundational Learning
- **Discourse Regulators**: Why needed - Signal turn-taking and engagement in professional dialogue; Quick check - Verify presence/absence in sample questions
- **Prefaces and Acknowledgments**: Why needed - Establish context and show conversational awareness; Quick check - Count frequency in professional vs non-professional questions
- **Request Type Classification**: Why needed - Differentiates direct, specific inquiries from vague or convoluted questions; Quick check - Categorize question types manually
- **Readability Metrics**: Why needed - Quantify complexity vs clarity trade-off in professional communication; Quick check - Calculate Flesch-Kincaid scores across samples
- **Feature Annotation Protocols**: Why needed - Ensure consistent labeling of linguistic elements; Quick check - Conduct inter-annotator reliability testing

## Architecture Onboarding
Component map: Annotation -> Feature Extraction -> Classification -> Validation
Critical path: Question annotation (human/LLM) → Linguistic feature extraction → Random Forest training → Accuracy evaluation → Correlation analysis
Design tradeoffs: Interpretable features vs. complex neural embeddings; manual annotation vs. automated feature extraction; domain-specific vs. general linguistic patterns
Failure signatures: Poor inter-annotator agreement, feature redundancy, overfitting to domain-specific patterns, weak correlation between features and professionalism judgments
First experiments:
1. Manually annotate 50 questions to establish feature extraction reliability
2. Train baseline classifier using only readability metrics
3. Compare human vs. LLM question classification using different prompting strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to financial analyst questions, raising questions about domain generalizability
- Reliance on small annotation team may introduce subjectivity in feature labeling
- LLM baseline comparison uses single model version without exploring prompting variations
- Readability metrics may not capture all nuances of professional communication

## Confidence
High: 96% classification accuracy, strong correlation with human judgments, interpretable feature framework
Medium: Domain generalizability claims, inter-annotator subjectivity, temporal stability of LLM comparison
Low: None identified

## Next Checks
1. Test framework across at least three additional professional domains (medical, legal, academic)
2. Conduct inter-annotator reliability analysis with larger, more diverse annotation teams
3. Evaluate model performance against newer LLM generations and alternative prompting strategies