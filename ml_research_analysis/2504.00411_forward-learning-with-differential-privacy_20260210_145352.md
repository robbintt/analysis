---
ver: rpa2
title: Forward Learning with Differential Privacy
arxiv_id: '2504.00411'
source_url: https://arxiv.org/abs/2504.00411
tags:
- privacy
- differential
- noise
- dp-ulr
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses differential privacy in deep learning by proposing
  DP-ULR, a forward-learning algorithm that leverages inherent randomness to provide
  privacy guarantees without backpropagation. The core method introduces a novel sampling-with-rejection
  technique and a privacy controller that dynamically adjusts noise levels during
  training.
---

# Forward Learning with Differential Privacy

## Quick Facts
- arXiv ID: 2504.00411
- Source URL: https://arxiv.org/abs/2504.00411
- Authors: Mingqian Feng, Zeliang Zhang, Jinyang Jiang, Yijie Peng, Chenliang Xu
- Reference count: 40
- Key outcome: DP-ULR achieves competitive performance compared to DP-SGD on MNIST and CIFAR-10, outperforming DP-SGD with large batches while maintaining similar privacy guarantees.

## Executive Summary
This paper introduces DP-ULR, a forward-learning algorithm that provides differential privacy guarantees without backpropagation. The key innovation is injecting Gaussian noise during the forward pass and using likelihood ratio gradients, which inherently provides randomized gradients that can be bounded for differential privacy. The method includes a sampling-with-rejection technique for privacy amplification and a dynamic privacy controller that adjusts noise levels during training. Experiments show DP-ULR outperforms DP-SGD with large batch sizes (64-500) across various noise levels on MNIST and CIFAR-10, while maintaining similar privacy loss limits.

## Method Summary
DP-ULR is a forward-learning algorithm that injects Gaussian noise into intermediate layer outputs during forward propagation. Gradients are estimated via likelihood ratio method, creating inherently randomized gradients bounded for DP analysis. The method includes a sampling-with-rejection technique that ensures minimum batch size for privacy amplification, and a dynamic privacy controller that adjusts noise levels based on batch-specific Jacobian structure. Unlike traditional DP-SGD which adds noise to computed gradients, DP-ULR provides privacy through the inherent randomness of forward gradient estimation.

## Key Results
- DP-ULR achieves 91.55% accuracy vs 90.47% for DP-SGD on MNIST with batch size 200 and σ₀=0.5
- With large batches (200-500), DP-ULR consistently outperforms DP-SGD across all noise levels (σ₀=0.5-8)
- Privacy cost impairment from rejection sampling is negligible (<10⁻¹⁰) in practical settings
- DP-ULR maintains similar privacy loss limits while offering advantages in parallelizability and suitability for non-differentiable modules

## Why This Works (Mechanism)

### Mechanism 1: Likelihood Ratio Gradient Estimation with Inherent Randomness
- Claim: Forward-learning algorithms that inject noise during the forward pass can provide differential privacy without explicit gradient noise injection.
- Mechanism: Gaussian noise is added to intermediate layer outputs during forward propagation. Gradients are estimated via likelihood ratio method: $\hat{g}_l(d) = \frac{1}{\sigma^2}(D_{\theta_l}v_l)^\top \cdot zL$, where $z \sim \mathcal{N}(0, \sigma^2I)$. This creates inherently randomized gradients bounded for DP analysis.
- Core assumption: The covariance matrix of gradient proxies remains full-rank across batches (Assumption 3.3).
- Evidence anchors:
  - [abstract] "the introduction of noise during the forward pass indirectly provides internal randomness protection to the model parameters and their gradients"
  - [Section 3.2] Proposition 3.2 shows variance scales as $\text{Var}(\hat{g}_l(d)) \approx \frac{L_0^2}{\sigma^2}(D_{\theta_l}v_l)^\top \cdot D_{\theta_l}v_l$
  - [corpus] "Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients" confirms forward gradients avoid backpropagation limitations
- Break condition: When layer parameter dimension $d_{\theta_l} > d_{v_l}$ (output dimension), Jacobian becomes singular, causing rank-deficient covariance and lost randomness in certain directions.

### Mechanism 2: Sampling-with-Rejection for Privacy Amplification
- Claim: Rejecting undersized batches provides quadratic privacy amplification with minimal additional privacy cost.
- Mechanism: Each example is sampled independently with probability $q$, but if batch size $|B_t| < N_B$, the batch is rejected and resampled. This ensures minimum randomness across all parameter directions.
- Core assumption: Dataset size $\bar{N}$ is large enough that rejection probability is small.
- Evidence anchors:
  - [Section 3.3, Theorem 3.5] Privacy cost: $\gamma = \frac{q \cdot p(N_B-1; \bar{N}, q)}{1 - P(N_B-1; \bar{N}, q)} + \frac{2q^2}{\sigma^2}\alpha$
  - [Section 4.1] With $\bar{N}=10000, q=0.01, N_B=50$, impairment term is $< 10^{-10}$ vs. main term $> 10^{-6}$
  - [corpus] No direct corpus evidence on rejection sampling for DP; related work focuses on standard Poisson sampling
- Break condition: When $N_B > q\bar{N}$ (rejection threshold exceeds expected batch size), privacy impairment becomes non-negligible.

### Mechanism 3: Dynamic Noise Calibration via Privacy Controller
- Claim: Adjusting noise $\sigma$ based on batch-specific Jacobian structure maintains target gradient variance $\sigma_0$ for consistent DP guarantees.
- Mechanism: Before gradient estimation, compute noise-free forward pass to obtain Jacobian $D_{\theta_l}v_l$ and loss $L_0$. Select $\sigma$ satisfying: $\sigma^2 \leq \frac{\min(\lambda(\sum_{d \in B_t} \tilde{\Sigma}_{\hat{g}(d)}))}{KC^2\sigma_0^2}$
- Core assumption: Assumption 3.3 holds—sufficient batch diversity ensures full-rank covariance.
- Evidence anchors:
  - [Section 3.2] "The adjustment must be dynamic because of the example-specific Jacobian matrix and non-noise loss"
  - [Algorithm 1, lines 5-6] Compute required noise std $\sigma$ and accumulate privacy cost using privacy controller
  - [corpus] "Stabilization of Perturbed Loss Function" uses similar dynamic perturbation but for LDP, not forward-learning
- Break condition: If eigenvalues approach zero (low data diversity), required $\sigma$ becomes impractically small, degrading utility.

## Foundational Learning

- Concept: **Rényi Differential Privacy (RDP)**
  - Why needed here: DP-ULR tracks privacy via RDP for its tight composition: $T$ steps compose additively as $(\alpha, \sum_i \gamma_i)$. Understanding RDP-to-$(\epsilon, \delta)$-DP conversion is essential for comparing to DP-SGD.
  - Quick check question: Given $(\alpha, \gamma)$-RDP, can you compute the equivalent $(\epsilon, \delta)$-DP guarantee?

- Concept: **Likelihood Ratio Gradient Estimation**
  - Why needed here: Unlike backpropagation, DP-ULR estimates gradients via perturbation analysis. Understanding why $\mathbb{E}_z[\hat{g}_l(d)] = \nabla_{\theta_l}\ell$ (Proposition 3.1) is crucial for trusting the optimizer.
  - Quick check question: Why does variance scale inversely with $\sigma^2$ (Proposition 3.2), and what does this imply for the privacy-utility tradeoff?

- Concept: **Jacobian Full-Rank Conditions**
  - Why needed here: DP guarantees break when covariance is rank-deficient. Understanding when $D_{\theta_l}v_l$ produces singular transformations helps diagnose failures and apply remediation (Appendix A.4).
  - Quick check question: For a linear layer with input $(C, H_{in})$ and output $(C, H_{out})$, what batch size ensures full-rank covariance?

## Architecture Onboarding

- Component map: Dataset D → [Batch Sampler with Rejection] → B_t → [K parallel forward passes with noise injection] → {L_k, ĝ_{t,k}} → [Gradient Aggregation + Clipping] → g_t → [Privacy Controller: compute σ, track RDP] → θ_{t+1}

- Critical path: The noise-free forward pass (for Jacobian computation) → noise std calibration → K noisy forward passes → gradient clipping. Any failure in Jacobian computation propagates to incorrect $\sigma$ and invalid DP bounds.

- Design tradeoffs:
  - **Large batch ($N_B \geq 200$)**: Better DP-ULR performance (Table 1 shows 91.55% vs 90.47% on MNIST at $\sigma_0=0.5$), but higher memory.
  - **Small batch ($N_B \leq 64$)**: DP-SGD outperforms; DP-ULR suffers from noise redundancy.
  - **High $\sigma_0$ (8)**: DP-ULR maintains convergence while DP-SGD degrades (57.88% vs 33.63% at batch 64).
  - **Remediation strategy**: If Assumption 3.3 fails, either add noise to virtual linear layer (full-rank Jacobian) or inject extra noise along eigenvector directions (Appendix A.4).

- Failure signatures:
  - **Rank-deficient covariance**: Gradient variance collapses in certain directions; privacy bound becomes invalid. Fix: increase $N_B$ or apply remediation.
  - **Overfitting with extreme losses**: Near-zero loss for some samples, very high for others (Appendix D). Fix: regularization or early stopping.
  - **Convex fluctuations in CNN training**: Figure 4 shows abrupt accuracy drops. Fix: tune $q$ and $\sigma_0$ together.

- First 3 experiments:
  1. **MLP on MNIST with batch sweep**: Test $B \in \{64, 200, 500\}$ at $\sigma_0 \in \{0.5, 1, 2, 4, 8\}$. Verify DP-ULR outperforms DP-SGD at large batches, underperforms at small batches. Track $\epsilon$ curves to confirm impairment term is negligible (Figure 3).
  2. **Rank deficiency diagnostic**: For each batch, compute $\min(\lambda(\sum \tilde{\Sigma}_{\hat{g}(d)}))$. Plot against batch size to empirically validate Assumption 3.3 threshold $N_0$.
  3. **CNN on CIFAR-10 with stability analysis**: Test ResNet-5 with $q \in \{1/50, 1/25\}$, $\sigma_0 \in \{4, 5, 8\}$. Identify parameter combinations that avoid Figure 4's fluctuations while matching DP-SGD's final accuracy (~45-50%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the utility improvements from advanced DP-SGD strategies (e.g., mega-batching, augmentation) be successfully generalized to the DP-ULR framework?
- Basis in paper: [explicit] Appendix D explicitly states that generalizing incremental DP-SGD improvements is beyond the scope of the current investigation and represents a promising research avenue.
- Why unresolved: The authors focused on establishing the theoretical privacy bounds of the core DP-ULR algorithm rather than integrating orthogonal optimization heuristics used in state-of-the-art DP-SGD.
- What evidence would resolve it: A modified DP-ULR implementation incorporating these strategies that demonstrates a superior privacy-utility trade-off compared to the baseline DP-ULR.

### Open Question 2
- Question: How can the specific overfitting behavior observed in DP-ULR—characterized by extreme loss variance despite high accuracy—be effectively mitigated?
- Basis in paper: [explicit] Appendix D identifies "Addressing this overfitting issue" as a specific area for future exploration after observing near-zero loss for some samples and very high losses for others.
- Why unresolved: While the algorithm achieves competitive accuracy, the current noise injection mechanism fails to regularize the loss landscape uniformly, leading to unstable training dynamics.
- What evidence would resolve it: A regularization technique or loss function modification that reduces the variance of per-sample losses without degrading the classification accuracy.

### Open Question 3
- Question: Can the inherent "noise redundancy" in DP-ULR's non-isotropic gradient estimation be reduced to improve utility, particularly for small batch sizes?
- Basis in paper: [inferred] Section 3.4 states that "redundant noise... impairs the utility of training," and Section 4.2 shows DP-ULR underperforms DP-SGD with small batches.
- Why unresolved: The method currently bounds the minimum eigenvalue of the gradient covariance, resulting in excess noise in other directions which harms training efficiency.
- What evidence would resolve it: A modified noise injection strategy that produces isotropic gradients, thereby closing the performance gap with DP-SGD in small-batch scenarios.

## Limitations

- **Unspecified K values**: The number of repeat gradient proxy calculations (K) is not specified in experimental results, affecting reproducibility and computational cost analysis.
- **Rank deficiency vulnerability**: DP guarantees depend critically on full-rank Jacobian covariance (Assumption 3.3), which may fail for small batches or specific architectures.
- **Limited integration with DP-SGD advances**: The paper focuses on establishing DP-ULR's theoretical foundation rather than integrating advanced techniques like mega-batching that have improved DP-SGD performance.

## Confidence

- **High confidence**: Theoretical DP analysis using RDP composition, privacy controller design, and basic forward-learning mechanism
- **Medium confidence**: Experimental comparisons with DP-SGD, particularly the batch size dependence claims
- **Low confidence**: Rank deficiency remediation strategies and their practical effectiveness

## Next Checks

1. **Rank Deficiency Diagnostic**: Implement systematic monitoring of minimum eigenvalue of batch covariance matrices across different batch sizes and architectures to empirically validate Assumption 3.3 and identify threshold conditions for rank deficiency.

2. **K-Value Sensitivity Analysis**: Test DP-ULR performance across multiple K values (repeat counts) to quantify the tradeoff between computational cost and gradient variance reduction, determining optimal K for different batch sizes.

3. **Rejection Sampling Impact Study**: Measure actual rejection rates and privacy impairment across diverse dataset sizes and sampling probabilities to validate the theoretical claim that impairment remains below 10⁻¹⁰ in practical settings.