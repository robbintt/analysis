---
ver: rpa2
title: Language models align with brain regions that represent concepts across modalities
arxiv_id: '2508.11536'
source_url: https://arxiv.org/abs/2508.11536
tags:
- brain
- language
- quartile
- consistency
- predictivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether language models (LMs) can represent
  cross-modal conceptual meaning in a brain-like manner. The authors introduce a novel
  semantic consistency metric that quantifies how consistently brain regions respond
  to the same concept across different modalities (sentences, word clouds, and images)
  using fMRI data.
---

# Language models align with brain regions that represent concepts across modalities

## Quick Facts
- arXiv ID: 2508.11536
- Source URL: https://arxiv.org/abs/2508.11536
- Reference count: 40
- Language models show better brain alignment in regions with higher semantic consistency, even in areas with weak language selectivity.

## Executive Summary
This study investigates whether language models can represent cross-modal conceptual meaning in a brain-like manner. The authors introduce a novel semantic consistency metric that quantifies how consistently brain regions respond to the same concept across different modalities (sentences, word clouds, and images) using fMRI data. They identify three brain regions showing high semantic consistency and evaluate 15 transformer LMs for their ability to predict brain responses in these regions. The key finding is that LMs show better brain alignment in regions with higher semantic consistency, even in areas with weak language selectivity, suggesting that these models may capture modality-independent conceptual representations.

## Method Summary
The study uses fMRI data from 17 participants who viewed 180 concepts across three paradigms: sentences, pictures, and word clouds. Semantic consistency is computed per voxel by correlating brain responses to the same concept across paradigms, using permutation tests to identify significant voxels. These voxels are aggregated into three left-hemisphere ROIs using the Glasser atlas. Brain encoding is performed using ridge regression to map LM hidden states to brain activations, with hyperparameters tuned per fold. Representational Similarity Analysis (RSA) compares the geometry of brain and LM concept representations.

## Key Results
- Language models show better brain alignment in regions with higher semantic consistency (r[S]=0.79, r[WC]=0.74)
- Language-only models demonstrate strong alignment in ROI 3 (weak language selectivity, high semantic consistency), suggesting cross-modal conceptual understanding
- Semantic consistency is a better predictor of alignment than language selectivity
- Alignment increases monotonically across semantic consistency quartiles when language selectivity is held constant

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Brain voxels that respond consistently to concepts across modalities can be identified through a correlation-based consistency metric.
- **Mechanism:** For each voxel, compute average responses to 180 concepts under each paradigm (sentences, pictures, word clouds), producing three response vectors (β^S, β^P, β^WC). The semantic consistency C = (1/3)[r(β^S, β^P) + r(β^S, β^WC) + r(β^WC, β^P)] averages pairwise Pearson correlations. Voxels passing significance in two independent permutation-test halves are retained.
- **Core assumption:** Modality-independent conceptual representations exist and manifest as stable response patterns to the same concept regardless of input format.
- **Evidence anchors:**
  - [abstract]: "a novel measure of meaning consistency across input modalities, which quantifies how consistently a brain region responds to the same concept across paradigms"
  - [section 4.1]: Equation (1) formalizes the metric; permutation tests validate significance
  - [corpus]: Related work (Bavaresco et al., 2024, arXiv:2407.17914) shows VLMs capture multimodal knowledge with higher alignment
- **Break condition:** If consistency were driven by modality-specific processing only, high-C voxels would cluster exclusively in visual cortex (for pictures) or language network (for text), not across diverse regions.

### Mechanism 2
- **Claim:** Language-only LMs develop representations that align with brain regions encoding cross-modal concepts, even without visual training.
- **Mechanism:** Through exposure to diverse linguistic contexts describing concepts, LMs internalize structured representations that approximate modality-independent meaning. This is evidenced by significant predictivity in semantically consistent regions with weak language selectivity (ROI 3).
- **Core assumption:** Text corpora encode sufficient information about conceptual structure to support cross-modal generalization.
- **Evidence anchors:**
  - [abstract]: "LMs might internally represent cross-modal conceptual meaning"
  - [section 6.1]: "ROI 3... demonstrates that semantic consistency drives predictivity even decoupled from language: rC = 0.33 ± 0.03, rL = 0.01 ± 0.02"
  - [corpus]: Abdou et al. (2021, cited) found LMs can represent perceptual concepts like color from text-only training
- **Break condition:** If this fails, language-only models would show alignment only with language-selective regions; predictivity in ROI 3 (low language selectivity, high semantic consistency) would be at baseline.

### Mechanism 3
- **Claim:** Semantic consistency predicts LM–brain alignment because shared representational geometry enables better prediction.
- **Mechanism:** Ridge regression maps LM hidden states to brain activations. Regions encoding stable, modality-independent conceptual structure have more systematic activation patterns that LM representations—also structured around conceptual meaning—can predict more accurately.
- **Core assumption:** Both systems converge on similar representational geometry for concepts, making their patterns more predictable from each other.
- **Evidence anchors:**
  - [section 6.1]: "a brain region is better predicted if it responds more consistently to concepts... (r[S] = 0.79, r[WC] = 0.74)"
  - [section 6.1, Figure 4]: Predictivity rises monotonically across semantic consistency quartiles when language selectivity is held fixed
  - [corpus]: Limited direct corpus evidence for this specific correlation mechanism; related work (Huh et al., 2024, cited) discusses convergent representations
- **Break condition:** If correlation were confounded by signal-to-noise differences, controlling for inter-participant noise ceiling should eliminate it—but correlations remain positive (r[S]=0.46, r[WC]=0.63 after adjustment; Appendix D.2).

## Foundational Learning

- **Representational Similarity Analysis (RSA):**
  - Why needed here: Section 5.4 uses RSA to compare pairwise concept-distance matrices between brain and LM, complementing the encoding approach.
  - Quick check question: Can you explain why RSA uses correlation distances (1 − r) rather than raw distances?

- **fMRI BOLD Signal & GLMsingle Processing:**
  - Why needed here: The brain data pipeline (Appendix A) uses GLMsingle to estimate per-stimulus β values from noisy, temporally-sampled BOLD responses.
  - Quick check question: Why does the 3-second stimulus + 2-second TR require temporal upsampling before GLM fitting?

- **Ridge Regression with Cross-Validated Regularization:**
  - Why needed here: Section 5.3 formalizes brain encoding as ridge regression with fold-wise α tuning; understanding overfitting risk in high-dimensional (d) vs. low-sample (n) settings is critical.
  - Quick check question: If d >> n, why does L2 regularization help, and what does α control?

## Architecture Onboarding

**Component map:**
- Input: Pereira et al. (2018) fMRI dataset (180 concepts × 3 paradigms × 4–6 stimuli each × 17 participants)
- Brain ROI pipeline: Semantic consistency metric → permutation tests (two halves) → probabilistic map → Glasser atlas overlay → threshold/cluster → 3 left-hemisphere ROIs
- LM feature extraction: Hidden states from all layers × pooling methods (mean, last-token, [CLS])
- Alignment evaluation: Brain encoding (ridge regression, 5-fold CV, fold-wise α tuning) + RSA (180×180 distance-matrix Spearman correlation)

**Critical path:**
1. Compute semantic consistency per voxel (requires matched concepts across all 3 paradigms)
2. Identify significant voxels via two independent permutation tests
3. Aggregate to anatomical ROIs via Glasser atlas thresholding
4. Extract LM representations; select optimal layer/pooling per model-ROI-paradigm
5. Fit ridge regression with fold-wise leave-one-out α search; evaluate via Pearson r on held-out folds

**Design tradeoffs:**
- Flexible layer selection + fold-wise α tuning maximizes per-model performance but obscures inter-model differences (Discussion acknowledges this)
- Pereira dataset constrains concept space to 180 GloVe-clustered words; generalization to broader concepts is untested
- ROI identification is left-hemisphere only; right-hemisphere consistency not analyzed

**Failure signatures:**
- Semantic consistency values near chance → metric not capturing stable cross-modal representations
- Predictivity correlates with language selectivity but not semantic consistency → LMs align with linguistic processing, not conceptual abstraction
- RSA correlations not exceeding shuffled baseline → representational geometries do not match

**First 3 experiments:**
1. ROI stability test: Re-compute semantic consistency on a held-out subset of participants to verify ROI reproducibility
2. Cross-dataset generalization: Apply the 3 ROIs to another multimodal fMRI dataset with different concept sets
3. VLM ablation: Compare text-only vs. text+image inputs to VLMs to quantify the contribution of visual grounding to alignment gains in ROI 3 (ventral visual region)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do language models of different sizes or architectures show distinct brain alignment levels in semantically consistent regions when using a standardized encoding pipeline?
- Basis in paper: [Explicit] The authors note in the Discussion (Section 7) that their flexible brain encoding pipeline, which tunes hyperparameters per fold, "obscures the differences between them," preventing the observation of trends seen in prior work.
- Why unresolved: The methodology was optimized for maximum predictivity rather than comparative analysis, masking potential performance gaps between models.
- What evidence would resolve it: A comparative study using a fixed encoding pipeline without per-fold hyperparameter tuning on these specific ROIs.

### Open Question 2
- Question: Does the superior temporal ROI 1 serve as a functional "gateway" that actively integrates language processing with abstract conceptual representations?
- Basis in paper: [Explicit] The authors state in the Discussion (Section 7): "We hypothesize that ROI 1 may serve as a gateway between the language system and the more abstract semantic areas."
- Why unresolved: The study identified the region via spatial overlap and alignment correlations but did not perform functional connectivity analyses to prove the directionality or integrative nature of the information flow.
- What evidence would resolve it: Functional connectivity or causal modeling analyses (e.g., TMS) to determine if ROI 1 mediates information transfer between language and semantic networks.

### Open Question 3
- Question: Why does representational similarity alignment (RSA) fail to increase with model scale or instruction tuning in these semantic regions, contrary to findings in language-selective networks?
- Basis in paper: [Explicit] The authors report in Section 7 that they "do not observe the trends noted in prior work," specifically that alignment does not increase with scale or instruction tuning.
- Why unresolved: This suggests that the scaling laws observed in language networks may not apply to cross-modal conceptual areas, but the paper does not identify the architectural or training factors responsible.
- What evidence would resolve it: Layer-wise analysis of larger models and ablation studies comparing base vs. instruction-tuned weights specifically on cross-modal tasks.

### Open Question 4
- Question: Do language models align with these brain regions because they capture amodal conceptual content or simply linguistic associations derived from the concept labels?
- Basis in paper: [Inferred] While the paper concludes LMs capture "cross-modal conceptual meaning" (Abstract), the "picture" paradigm included text labels, and the models (even VLMs) are heavily trained on text. The specific mechanism of alignment remains ambiguous.
- Why unresolved: The correlation between model embeddings and brain activity could be driven by the shared linguistic label rather than the non-linguistic sensory features of the concept.
- What evidence would resolve it: An experiment using purely visual stimuli without text labels, or analyzing alignment in brain regions that respond to non-nameable visual features.

## Limitations
- Semantic consistency metric depends on finding matched stimuli across all three paradigms, limiting the concept space to 180 items
- ROI identification uses only left-hemisphere voxels, leaving right-hemisphere contributions unexplored
- Language-only models show strong alignment in ROI 3, but the mechanism remains unclear

## Confidence
- High confidence in semantic consistency metric and ROI identification methodology
- Medium confidence in brain encoding results due to complex preprocessing and hyperparameter tuning
- Medium confidence in claim that language-only models capture cross-modal concepts
- Low confidence in specificity of findings to the 180-concept set from Pereira et al. (2018)

## Next Checks
1. ROI stability test: Re-compute semantic consistency on a held-out subset of participants to verify ROI reproducibility
2. Cross-dataset generalization: Apply the 3 ROIs to another multimodal fMRI dataset with different concept sets
3. VLM ablation: Compare text-only vs. text+image inputs to VLMs to quantify the contribution of visual grounding to alignment gains in ROI 3 (ventral visual region)