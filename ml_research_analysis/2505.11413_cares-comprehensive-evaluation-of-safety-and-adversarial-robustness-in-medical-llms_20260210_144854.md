---
ver: rpa2
title: 'CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical
  LLMs'
arxiv_id: '2505.11413'
source_url: https://arxiv.org/abs/2505.11413
tags:
- level
- safety
- medical
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARES is a benchmark for evaluating medical LLM safety under adversarial
  conditions. It includes over 18,000 prompts covering eight clinical safety principles,
  four harm levels, and four prompting styles (direct, indirect, obfuscated, role-play).
---

# CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs

## Quick Facts
- arXiv ID: 2505.11413
- Source URL: https://arxiv.org/abs/2505.11413
- Reference count: 40
- CARES evaluates medical LLM safety under adversarial conditions with over 18,000 prompts

## Executive Summary
CARES is a comprehensive benchmark designed to evaluate the safety and adversarial robustness of medical large language models. The benchmark introduces a novel three-way response protocol (Accept, Caution, Refuse) and a Safety Score metric that rewards appropriate refusals while penalizing unsafe acceptances and over-cautious responses. CARES systematically tests model robustness against jailbreak-style prompt manipulations across multiple clinical safety principles and harm levels.

The evaluation reveals that even state-of-the-art medical LLMs remain vulnerable to adversarial attacks, with significant variations in safety performance depending on prompt style and harm level. A lightweight jailbreak classifier was developed to improve safety by guiding models to respond more appropriately when adversarial intent is detected, demonstrating the potential for hybrid approaches in enhancing medical LLM safety.

## Method Summary
CARES employs a comprehensive evaluation framework with over 18,000 prompts spanning eight clinical safety principles, four harm levels, and four prompting styles (direct, indirect, obfuscated, role-play). The benchmark introduces a three-way response protocol and a Safety Score metric that balances the trade-off between over-refusal and unsafe acceptance. Models are evaluated using both zero-shot and few-shot settings, with a lightweight jailbreak classifier trained to detect adversarial prompts and guide appropriate responses.

## Key Results
- State-of-the-art medical LLMs show significant vulnerability to jailbreak-style adversarial prompts
- Models exhibit over-refusal behavior on safe queries, particularly with obfuscated prompting styles
- The lightweight jailbreak classifier improves safety performance by 15-25% across tested models
- Safety Score varies significantly across harm levels, with higher harm scenarios showing more appropriate refusal behavior

## Why This Works (Mechanism)
CARES works by systematically exposing medical LLMs to a diverse range of adversarial prompt scenarios that mimic real-world attack vectors. The benchmark's strength lies in its comprehensive coverage of clinical safety principles and its use of multiple prompting styles to test model robustness. The three-way response protocol provides granular evaluation of model behavior, while the Safety Score metric offers a balanced assessment that accounts for both false positives and false negatives in safety decisions.

## Foundational Learning
- **Clinical safety principles**: Understanding the eight core safety principles is essential for evaluating whether models appropriately handle sensitive medical scenarios
- **Jailbreak detection**: Learning how adversarial prompts are constructed and detected is crucial for developing robust safety mechanisms
- **Response classification**: Mastering the three-way response protocol (Accept, Caution, Refuse) is necessary for interpreting model behavior
- **Safety scoring**: Understanding the metric's weighting system is key to evaluating model performance across different scenarios
- **Prompt engineering**: Knowledge of direct, indirect, obfuscated, and role-play prompting styles is required to understand attack vectors
- **Human evaluation protocols**: Familiarity with inter-rater reliability and labeling procedures is important for benchmark validation

## Architecture Onboarding
Component map: CARES benchmark -> Prompt generation -> Model evaluation -> Human rating -> Safety Score calculation -> Jailbreak classifier training
Critical path: Prompt generation → Model evaluation → Human rating → Safety Score calculation
Design tradeoffs: Comprehensive prompt coverage vs. evaluation complexity; human rating accuracy vs. scalability
Failure signatures: Inconsistent safety responses across prompting styles; over-refusal on safe queries; jailbreak susceptibility
Three first experiments: 1) Replicate Safety Score calculation on a subset of prompts, 2) Test model performance across different harm levels, 3) Evaluate jailbreak classifier effectiveness on novel adversarial prompts

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the results suggest several areas for future investigation, including the generalizability of the benchmark across languages and healthcare contexts, the long-term effectiveness of jailbreak classifiers against evolving attack techniques, and the potential for automated evaluation methods to reduce reliance on human raters.

## Limitations
- Potential biases in prompt corpus construction around clinical safety principles and harm categories
- Reliance on human raters introduces inter-rater variability and subjectivity in response labeling
- Benchmark focuses primarily on English-language prompts, limiting multilingual generalizability
- Jailbreak classifier effectiveness may not generalize beyond tested model architectures

## Confidence
- High confidence in benchmark's systematic approach across multiple dimensions
- Medium confidence in Safety Score metric's nuanced behavior capture
- Medium confidence in transferability to real-world clinical deployments

## Next Checks
1. Conduct reproducibility study with independent raters to validate human evaluation component
2. Test benchmark effectiveness across multiple languages and healthcare contexts
3. Evaluate jailbreak classifier performance against evolving attack techniques over time