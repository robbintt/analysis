---
ver: rpa2
title: Memory-Driven Self-Improvement for Decision Making with Large Language Models
arxiv_id: '2509.26340'
source_url: https://arxiv.org/abs/2509.26340
tags:
- action
- prior
- memory
- memory-driven
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) to specific sequential decision-making (SDM) tasks with limited task-related
  data. The authors propose a memory-driven self-improvement framework that combines
  LLM general prior knowledge with a compact memory of domain-specific experiences.
---

# Memory-Driven Self-Improvement for Decision Making with Large Language Models
arXiv ID: 2509.26340
Source URL: https://arxiv.org/abs/2509.26340
Reference count: 28
One-line result: Memory-driven framework improves LLM decision making by over 40% on in-distribution tasks and 75% on unseen tasks.

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) to specific sequential decision-making (SDM) tasks with limited task-related data. The authors propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. The core idea involves two complementary roles: memory-driven value estimation, which converts informative interactions into compact memory representations and enables informed exploration choices, and LLM prior refinement, which periodically updates the LLM's decision prior using historical state-action pairs and their Q-values stored in memory.

## Method Summary
The proposed method implements a memory-driven self-improvement loop where memory and LLM prior mutually reinforce each other. Memory-Driven Q-learning (Mem-Q) uses BERT-base embeddings to encode state-action pairs stored in memory with Monte Carlo returns, estimating Q-values via k-NN search using inverse distance kernels. The full method (Mem-EM) uses an LLM (Qwen2.5-3B/7B) to generate K action candidates, ranks them using Mem-Q, and selects actions via Q-weighted multinomial sampling. The LLM prior is periodically updated using LoRA fine-tuning on memory samples reweighted by exp(Q/τ). Experiments on ALFWorld and Overcooked demonstrate significant performance improvements with over 40% improvement on in-distribution tasks and over 75% improvement when generalized to unseen tasks.

## Key Results
- Mem-EM achieves over 40% improvement on in-distribution tasks compared to traditional RL and LLM-based baselines
- Mem-EM shows over 75% improvement when generalized to unseen tasks in ALFWorld
- The approach requires only six fine-tuning steps with LoRA during training, demonstrating superior sample efficiency
- Mem-EM demonstrates better generalization ability compared to pure RL and LLM-based baselines

## Why This Works (Mechanism)
The method works by creating a feedback loop between experience memory and LLM reasoning. Memory provides grounded value estimates from actual interactions, while the LLM provides generalization and reasoning capabilities. The k-NN retrieval with inverse distance kernel enables efficient value estimation without requiring large neural networks. LoRA fine-tuning allows the LLM to incorporate task-specific knowledge while maintaining general capabilities. The Q-weighted multinomial sampling balances exploration and exploitation by considering both LLM prior and memory-based value estimates.

## Foundational Learning
- **BERT embeddings for state-action pairs**: Needed to convert textual observations into comparable representations for k-NN retrieval. Quick check: Verify embeddings capture semantic similarity between similar state-action descriptions.
- **k-NN value estimation with inverse distance kernel**: Required to estimate Q-values for unseen state-action pairs using memory. Quick check: Test retrieval accuracy on held-out state-action pairs with known returns.
- **Monte Carlo return calculation**: Essential for determining the value of trajectories without requiring temporal difference learning. Quick check: Verify return calculations match expected values for simple test trajectories.
- **LoRA fine-tuning**: Needed to efficiently update LLM parameters without full fine-tuning. Quick check: Confirm LoRA updates improve action selection quality on validation tasks.
- **Q-weighted multinomial sampling**: Required to balance LLM prior knowledge with memory-based value estimates. Quick check: Test that sampling explores diverse actions while favoring high-value options.
- **Memory management with LRU replacement**: Critical for maintaining relevant experiences while preventing memory overflow. Quick check: Monitor memory utilization and ensure high-value experiences are preserved.

## Architecture Onboarding
Component map: State Observations -> BERT Encoder -> Memory Storage -> k-NN Retrieval -> Q-value Estimation -> LLM Prior -> Action Selection -> Environment -> Memory Update

Critical path: Environment provides state -> LLM generates K candidates -> Mem-Q estimates Q-values via k-NN -> Q-weighted sampling selects action -> Environment executes -> Return calculated -> Memory updated

Design tradeoffs: Memory capacity vs. computational overhead during k-NN retrieval; LoRA rank vs. fine-tuning effectiveness; K candidates vs. exploration quality; temperature τ vs. exploitation vs. exploration balance.

Failure signatures: Invalid action generation from LLM (high "Randomly select" fallback rate); Memory instability (degrading performance due to low-value early trajectories); Poor k-NN retrieval (inaccurate Q-value estimates); Ineffective LoRA updates (LLM prior not improving).

First experiments: 1) Test BERT embedding quality on state-action similarity tasks. 2) Verify k-NN retrieval accuracy with known returns. 3) Validate LoRA fine-tuning effectiveness on small synthetic tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- The k-NN based value estimation assumes semantically similar state-action pairs have similar returns, which may not hold in tasks with sparse rewards or complex dynamics.
- The memory-driven approach requires careful tuning of memory capacity and retrieval parameters, with performance potentially degrading significantly if misconfigured.
- The method's effectiveness depends heavily on the quality of the initial LLM prior and its ability to generate reasonable action candidates.
- Experimental evaluation focuses on two text-based environments, which may not represent the diversity of real-world sequential decision-making tasks.
- The paper does not address computational overhead during inference or memory scaling challenges for larger state spaces.

## Confidence
High confidence in experimental results and implementation details within studied environments, as methodology is clearly described and ablation studies provide convincing evidence for components' contributions.
Medium confidence in claimed generalization benefits, as unseen task performance improvements are substantial but based on limited task variations within the same domain.
Low confidence in method's applicability to non-textual or continuous action spaces without significant modifications, as current formulation relies heavily on textual state representations and discrete action spaces.

## Next Checks
1. Reproduce core results on ALFWorld Pick and Examine tasks using provided BERT embeddings and k-NN retrieval parameters (M=20, memory capacity=15,000) to verify baseline Mem-Q performance and memory stability.
2. Implement LoRA fine-tuning with different ranks (r=8, r=16) to assess sensitivity to fine-tuning configuration and verify whether reported improvements are robust to these hyperparameters.
3. Test memory capacity scaling by varying memory size from 100 to 15,000 entries and measuring impact on sample efficiency and performance stability, particularly examining trade-off between memory size and computational overhead during k-NN retrieval.