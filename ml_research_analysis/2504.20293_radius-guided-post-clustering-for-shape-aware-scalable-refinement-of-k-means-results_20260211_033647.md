---
ver: rpa2
title: Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means
  Results
arxiv_id: '2504.20293'
source_url: https://arxiv.org/abs/2504.20293
tags:
- k-means
- clustering
- clusters
- fcps
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a simple post-processing method to improve
  k-means clustering on non-convex shapes. After running k-means with an intentionally
  overestimated number of clusters, each cluster is assigned a radius (distance to
  its farthest point), and clusters whose radii overlap are merged.
---

# Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results

## Quick Facts
- arXiv ID: 2504.20293
- Source URL: https://arxiv.org/abs/2504.20293
- Reference count: 16
- One-line primary result: A lightweight post-processing method that recovers non-convex cluster shapes by merging overlapping-radius k-means fragments, achieving 98-100% success on benchmark data.

## Executive Summary
This paper introduces a simple post-processing method to improve k-means clustering on non-convex shapes. After running k-means with an intentionally overestimated number of clusters, each cluster is assigned a radius (distance to its farthest point), and clusters whose radii overlap are merged. This geometric rule enables recovery of ground-truth structure even when k is not precisely known. The method was tested on the FCPS benchmark suite, achieving median clustering success rates of 98–100% in noise-free conditions, and demonstrated strong performance after recursive spatial partitioning, supporting scalability to large or distributed datasets. Implemented as a lightweight extension to scikit-learn, it requires minimal tuning and no additional parameters. Limitations include sensitivity to extremely high k values and reduced effectiveness on elongated or highly irregular shapes. The approach offers a practical enhancement to k-means for shape recovery and distributed clustering.

## Method Summary
The method runs k-means with an intentionally overestimated number of clusters k, then computes each cluster's radius as the maximum distance from its centroid to any assigned point. Clusters whose centroids are within the sum of their radii are considered connected and merged into single clusters via connected components analysis. This recovers non-convex shapes by merging over-fragmented convex pieces. The approach can be applied after distributed clustering by reassembling spatially split fragments based on radius overlap.

## Key Results
- Median clustering success rates of 98–100% on FCPS benchmark suite in noise-free conditions
- Stable cluster counts across a range of overestimated k values for datasets with coherent structure
- Successful reassembly of distributed clusters in 6/8 FCPS datasets with median success ≥0.84

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-segmentation followed by radius-guided merging can recover non-convex cluster shapes that standard k-means cannot detect.
- Mechanism: When k is intentionally overestimated, k-means fragments true clusters into smaller convex pieces. Each fragment's radius (distance from centroid to farthest assigned point) typically extends toward neighboring fragments of the same ground-truth cluster. Fragments belonging to the same structure have overlapping radii, while fragments from different structures often do not. A graph connectivity check identifies merge candidates in a single pass.
- Core assumption: The true clusters are sufficiently dense and well-separated such that intra-cluster fragment radii overlap more than inter-cluster fragment radii.
- Evidence anchors:
  - [abstract] "as long as k is overestimated (but not excessively), the method can often reconstruct non-convex shapes through meaningful merges"
  - [Section 3.2] Defines radius computation and merge condition: `||ci - cj|| ≤ ri + rj`
  - [corpus] No direct validation; corpus contains alternative approaches (K*-Means, CoHiRF) but no independent confirmation of radius-guided merging.
- Break condition: When k is set too high (approaching n), micro-clusters emerge with no recoverable structure; when clusters are elongated or closely spaced, radii may overlap incorrectly (Tetra failure case).

### Mechanism 2
- Claim: The merge rule relaxes the requirement to specify exact k in advance.
- Mechanism: Rather than requiring k to match ground truth, the method tolerates a range of overestimated values. Post-processing collapses oversegmented partitions back toward natural structure. Stability of resulting cluster count across slightly different k values can serve as a heuristic for natural cluster number estimation.
- Core assumption: Natural clusters produce stable merge outcomes across a band of overestimated k values.
- Evidence anchors:
  - [Section 3.3] "If the data contain coherent clusters, their number should remain stable across a range of slightly overestimated k values"
  - [Table 2] Shows successful k ranges (e.g., Atom: 15 ≤ k ≤ 20) producing consistent results
  - [corpus] Weak; K*-Means paper addresses parameter-free clustering via different mechanism.
- Break condition: If k varies into "excessively high" territory, the number of resulting clusters becomes unstable and typically increases.

### Mechanism 3
- Claim: Radius-based merging enables distributed clustering by reconnecting fragments split across spatial partitions.
- Mechanism: When the feature space is tiled, individual clusters may be split across tile boundaries. Because each fragment's radius extends beyond its tile, the global merge step can reconnect fragments whose radii overlap across boundaries—restoring the original cluster without global coordination during initial clustering.
- Core assumption: Partition boundaries do not systematically bisect clusters in ways that leave fragments too small or too sparse to reconnect.
- Evidence anchors:
  - [Section 4] "the radius of each partial cluster will usually extend beyond its tile"
  - [Table 3] Shows 6/8 datasets achieving >0.84 median success after partitioned clustering and reassembly
  - [corpus] No corpus papers validate this distributed merge mechanism.
- Break condition: Thin or sparse structures (e.g., Chainlink's interlocking rings) may produce fragments too small to reconnect after partitioning.

## Foundational Learning

- Concept: **Voronoi Tessellation**
  - Why needed here: Standard k-means implicitly partitions space into Voronoi cells, which are always convex. Understanding why this constrains k-means to convex clusters clarifies why post-processing is required for non-convex shapes.
  - Quick check question: Can a Voronoi cell ever be non-convex? (Answer: No—the intersection of half-spaces is always convex.)

- Concept: **Connected Components in Graphs**
  - Why needed here: The merge rule constructs an adjacency graph where nodes are clusters and edges indicate radius overlap. Merging is performed by finding connected components, not iterative pairwise merging.
  - Quick check question: Given an adjacency matrix where entry (i,j)=1 if clusters i and j overlap, how do you identify all clusters that should merge into one? (Answer: Find connected components via BFS/DFS or union-find.)

- Concept: **Cluster Validity Metrics (e.g., Adjusted Rand Index)**
  - Why needed here: The paper reports "clustering success" rates against ground truth. Understanding how such metrics work is essential for evaluating whether post-processing actually improves results.
  - Quick check question: Why is raw accuracy insufficient for comparing clusterings? (Answer: Label permutations and cluster count mismatches require adjusted metrics.)

## Architecture Onboarding

- Component map:
  1. Standard k-means module
  2. Radius computation
  3. Adjacency graph builder
  4. Connected component finder
  5. Label propagator
  6. (Optional) Tiling layer

- Critical path: k-means output → radius computation → adjacency construction → connected components → label propagation. The radius computation is O(nk) and adjacency is O(k²); both are lightweight relative to k-means itself.

- Design tradeoffs:
  - Higher initial k → better shape recovery but increased risk of over-fragmentation and noise sensitivity
  - Simpler radius metric (max distance) vs. alternatives (quantile-based, average) — paper uses max for simplicity but notes alternatives as future work
  - Tile size in distributed mode — larger tiles reduce cross-boundary merge complexity but limit parallelism

- Failure signatures:
  - Single merged cluster: k too high relative to data density (all radii overlap)
  - No merging occurs: k underestimated or clusters genuinely well-separated
  - Inconsistent results across runs: k-means initialization instability (use k-means++ seeding)
  - Partitioned mode returns too many clusters: Fragments too small (Chainlink pattern) — consider larger tiles or higher local k

- First 3 experiments:
  1. Replicate on synthetic two-moons or concentric circles data: Run standard k-means with k=2 (expect failure), then k=10-20 with post-processing (expect recovery). Verify visually and with ARI against ground truth.
  2. Stress-test k sensitivity: For a single FCPS dataset (e.g., Atom), sweep k from ground-truth value to 5× ground truth. Plot final cluster count and ARI vs. initial k to identify the stable plateau.
  3. Validate partitioned mode: Split an FCPS dataset into 4 tiles, cluster each independently with local k, then merge globally. Compare to non-partitioned result and quantify information loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative radius definitions, such as quantile-based distances or scaling factors, improve the recovery of elongated or irregularly dense clusters compared to the maximum-distance definition?
- Basis in paper: [explicit] Section 6.1 states that future work may explore "alternative radius definitions (e.g., average or quantile-based distance, or scaling)".
- Why unresolved: The current definition uses the distance to the farthest point, which approximates clusters as spherical extents. This causes the merge rule to fail for elongated shapes or clusters with non-uniform density, as seen in the poor reassembly of the Chainlink dataset.
- What evidence would resolve it: Ablation studies on complex datasets (e.g., FCPS Chainlink) comparing the max-distance radius against mean, median, or percentile-based radii to measure improvements in clustering success rates for non-convex shapes.

### Open Question 2
- Question: How does the geometric merge heuristic perform in high-dimensional feature spaces where the concept of radius becomes less distinct due to the curse of dimensionality?
- Basis in paper: [explicit] Section 6.1 lists "extending the method to high-dimensional data" as an open direction.
- Why unresolved: The paper validates the method primarily on 2D and 3D datasets from the FCPS suite. In high-dimensional spaces, distance metrics tend to concentrate, potentially making the radius overlap condition (distance < sum of radii) trivial or ineffective for distinguishing distinct clusters.
- What evidence would resolve it: Experiments applying the radius-guided merge to datasets with high dimensionality (e.g., >20 features) to determine if the geometric overlap rule maintains statistical power compared to density-based alternatives.

### Open Question 3
- Question: Can a principled noise-handling mechanism be integrated into the post-processing step to robustly filter outliers without discarding sparse but valid clusters?
- Basis in paper: [explicit] Section 6.1 notes that "a more principled noise-handling extension remains an avenue for future development," as the method is currently designed for noise-free conditions.
- Why unresolved: The current approach relies on an ad-hoc heuristic of excluding clusters with a radius of zero (isolated outliers). There is no established mechanism to distinguish between noise points and valid data points in sparse regions during the merge process.
- What evidence would resolve it: Derivation and testing of a noise-threshold parameter or probabilistic merge criteria that explicitly models noise, evaluated against ground-truth data containing varying levels of label noise.

## Limitations
- Sensitive to extremely high k values, which can cause all clusters to merge into a single trivial cluster
- Reduced effectiveness on elongated or highly irregular shapes due to spherical radius approximation
- Performance on high-dimensional data and noisy datasets remains untested

## Confidence
- Standard post-processing mechanism (radius-guided merging): High
- Scalability via spatial partitioning: Medium
- Generalizability to noisy/high-dimensional data: Low

## Next Checks
1. Validate the method on high-dimensional datasets (e.g., MNIST, UCI datasets) to assess scalability beyond 2D benchmarks.
2. Test robustness to varying noise levels and cluster overlap by injecting synthetic noise or blurring cluster boundaries.
3. Compare runtime and merge quality against alternative parameter-free clustering methods (e.g., DBSCAN, HDBSCAN) on identical datasets.