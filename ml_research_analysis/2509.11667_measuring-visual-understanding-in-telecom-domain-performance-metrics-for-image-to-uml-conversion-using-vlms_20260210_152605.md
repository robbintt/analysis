---
ver: rpa2
title: 'Measuring Visual Understanding in Telecom domain: Performance Metrics for
  Image-to-UML conversion using VLMs'
arxiv_id: '2509.11667'
source_url: https://arxiv.org/abs/2509.11667
tags:
- ground
- truth
- metrics
- vlms
- diagrams
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of evaluating Vision-Language
  Model (VLM) performance in converting telecom sequence diagrams to PlantUML format,
  a task critical for automating the extraction of structured information from technical
  documents. Existing methods lack systematic metrics for assessing such conversions,
  especially for complex diagram components.
---

# Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs

## Quick Facts
- arXiv ID: 2509.11667
- Source URL: https://arxiv.org/abs/2509.11667
- Authors: HG Ranjani; Rutuja Prabhudesai
- Reference count: 22
- Primary result: Introduces systematic metrics for evaluating VLM performance on telecom sequence diagram to PlantUML conversion, revealing specific weaknesses in complex structural elements.

## Executive Summary
This paper addresses the challenge of evaluating Vision-Language Models (VLMs) for converting telecom sequence diagrams to PlantUML format. The authors propose a systematic evaluation methodology using version control tools to compare VLM-generated outputs with manually curated ground truth scripts. Their approach focuses on 50 telecom domain diagrams from 3GPP specifications, introducing performance metrics across multiple components including node identification, message flow, sequence ordering, and structural elements like notes, boxes, and groups.

## Method Summary
The methodology involves using Git-based diff comparison between ground truth and VLM outputs, applying Levenshtein distance with linear sum assignment for optimal line pairing, and categorizing errors via regex pattern matching. The study compares outputs from Claude Sonnet and GPT-4V on 50 telecom sequence diagrams, with manual normalization of syntax errors before evaluation. Performance metrics are calculated for nodes, edges, messages, and structural elements (notes, boxes, groups).

## Key Results
- Both VLMs accurately capture basic elements (nodes, edges, messages) but struggle with complex constructs like notes, boxes, and groups
- Claude outperforms GPT-4 overall in diagram conversion accuracy
- GPT-4 shows less degradation with increasing diagram complexity, an unexpected finding requiring further investigation
- Error rates for structural elements are particularly high: 52-100% deletion rates for boxes, 69-77% for groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Version control diff tools enable systematic comparison of diagram-as-code outputs.
- Mechanism: Git diff generates patch files between ground truth and VLM outputs. These are preprocessed to retain only structural lines (arrows, participant declarations, group markers). The text-based representation allows line-by-line comparison that visual similarity metrics cannot provide.
- Core assumption: PlantUML scripts are semantically equivalent if their structural elements match, even if formatting differs.
- Evidence anchors:
  - [abstract] "We use version control tools to capture differences and introduce standard performance metrics to measure accuracies along various components"
  - [Section 2.2.1] "Git diff or patch files are generated for each VLM model output with respect to main branch"
  - [corpus] Weak direct corpus support; related work [12] uses SSIM/BLEU which the authors contrast against.
- Break condition: If VLM outputs use semantically different but syntactically valid puml constructs (e.g., `actor` vs `participant`), diff-based comparison may flag false errors unless normalized.

### Mechanism 2
- Claim: Levenshtein distance + linear sum assignment enables optimal line pairing between ground truth and VLM output.
- Mechanism: For each pair of removed (ground truth) and added (VLM output) lines, Levenshtein distance quantifies textual difference. The Jonker-Volgenant algorithm (via `linear_sum_assignment`) finds optimal matching that minimizes total distance. Unpaired removed lines = deletions; unpaired added lines = insertions; paired lines with distance > 0 = substitutions.
- Core assumption: Minimal edit distance correlates with semantic correspondence for sequence diagram lines.
- Evidence anchors:
  - [Section 2.2.2] "Levenshtein distance is calculated between every element of removed and added lines... Linear sum assignment implementation of modified Jonker-Volgenant algorithm is applied to find optimal matching"
  - [corpus] No direct corpus precedent for this pairing mechanism in diagram evaluation.
- Break condition: If VLM reorders semantically equivalent lines (e.g., swapping note declarations), the algorithm may misclassify as deletions/insertions rather than ordering changes.

### Mechanism 3
- Claim: Component-stratified error metrics isolate failure modes in complex structural elements.
- Mechanism: Errors are categorized via regex pattern matching into node, edge, message, and structural (notes/boxes/groups) types. For each category, insertion/deletion/substitution rates are computed as percentages of ground truth element counts.
- Core assumption: Errors in different components have independent causes and should be measured separately.
- Evidence anchors:
  - [Table 2] Shows 69.23% group deletion rate for Claude, 76.92% for GPT-4; 52.63% box deletion for Claude, 100% for GPT-4
  - [Section 4] "higher errors in both VLMs outputs with respect to structural elements such as box, group and notes"
  - [corpus] Related work [9] uses node-F1 and edge-F1 only; this work extends to structural elements.
- Break condition: If structural elements are nested (e.g., note inside group), simple regex categorization may misattribute errors to wrong component.

## Foundational Learning

- Concept: **PlantUML sequence diagram syntax**
  - Why needed here: Understanding `participant`, `->`, `-->`, `note over`, `group`, `box` syntax is required to interpret error metrics and debug VLM outputs.
  - Quick check question: Given `Alice -> Bob : Hello`, which is the node, edge, and message?

- Concept: **Git diff / patch file format**
  - Why needed here: The evaluation pipeline relies on parsing unified diff output (lines starting with `-` for deletions, `+` for additions).
  - Quick check question: In a diff output, what do `---`, `+++`, `@@` headers signify vs. content lines?

- Concept: **Edit distance (Levenshtein) for text alignment**
  - Why needed here: Understanding how string similarity enables line pairing helps diagnose why certain lines are matched vs. flagged as unmatched.
  - Quick check question: What is the Levenshtein distance between `"UE-A -> SGW"` and `"UEA -> SGW"`?

## Architecture Onboarding

- Component map:
  Input Layer: 3GPP document images → manual filtering to sequence diagrams
  Conversion Layer: VLM (Claude 3.7 Sonnet / GPT-4V) with standardized prompt → raw puml scripts
  Normalizer: Manual syntax corrections (spurious characters, invalid arrow syntax) — not counted as errors
  Diff Engine: Git repository (3 branches: main/claude/gpt) → patch files per diagram
  Alignment Module: Preprocessing → Levenshtein distance matrix → linear sum assignment → paired/unpaired classification
  Error Categorizer: Regex pattern matching → component-wise error counts
  Metrics Calculator: Aggregation at file and dataset level → insertion/deletion/substitution rates per component

- Critical path: Image selection → Ground truth manual creation → VLM prompt standardization → Diff extraction → Line pairing → Error categorization → Metric aggregation. The ground truth quality is the bottleneck; any errors there propagate to all evaluations.

- Design tradeoffs:
  - Dataset size vs. metric validation: 50 diagrams is modest but sufficient for metric methodology validation; not intended for VLM benchmarking at scale.
  - Manual normalization vs. automated evaluation: Authors manually fix syntax errors (e.g., `..>` → `-->`) to avoid penalizing VLMs for trivial issues, but this introduces human judgment.
  - Text-based diff vs. visual comparison: Text diff captures semantic structure better than SSIM but requires syntactically valid puml.

- Failure signatures:
  - High group/box/note error rates (>50%) → VLM struggles with nested or complex structural elements; may need fine-tuning data augmentation.
  - Error rate increases with script length (Claude pattern) → Visual context degradation; consider chunking or hierarchical processing.
  - GPT-4 error rate decreases with complexity → Assumption: may indicate over-confident hallucination on simpler diagrams; needs investigation (authors note this is "unexpected").

- First 3 experiments:
  1. Reproduce metrics on 5 sample diagrams: Manually create ground truth, run both VLMs, execute diff pipeline, verify your error categorization matches paper's definitions. This validates your understanding of the metric calculations.
  2. Ablate the normalization step: Compare error rates with and without manual syntax corrections to quantify how much "syntactic noise" affects apparent VLM performance.
  3. Test prompt sensitivity: Add chain-of-thought instructions (as suggested in conclusion) for 5 diagrams and measure whether structural element error rates improve. Authors explicitly note this as future work.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does GPT-4V exhibit a trend of decreasing error rates with increasing diagram complexity (lines of script), and does this robustness persist across a larger dataset?
- Basis in paper: [explicit] The authors note that GPT-4’s error rate shows a decreasing trend with complexity, stating this "is not intuitive" and "hints that GPT-4 retains higher visual context... This needs further investigation."
- Why unresolved: The study used a modest dataset (50 images), and the observed behavior for GPT-4 contrasted with Claude (which showed expected increasing error rates) and intuition.
- What evidence would resolve it: Ablation studies on the full 4010-image dataset, or a similarly large benchmark, specifically analyzing error rates against script length to confirm if the trend is a statistical anomaly or a robust feature of the model's architecture.

### Open Question 2
- Question: Can fine-tuning Vision-Language Models (VLMs) with targeted training data significantly reduce error rates for complex structural elements like notes, boxes, and groups?
- Basis in paper: [explicit] The authors observe high error rates in structural elements and conclude that their "experiments and performance metrics indicates a need for better representation of these components in training data for fine-tuned VLMs."
- Why unresolved: The current study evaluated off-the-shelf models (Claude Sonnet and GPT-4V) and identified specific weaknesses in "group, box, notes" components, but did not attempt to fine-tune a model to see if this specific data augmentation resolves the issue.
- What evidence would resolve it: An experiment where a VLM is fine-tuned on a dataset rich in these structural elements, followed by an evaluation using the proposed metrics to measure the reduction in insertion, deletion, and substitution rates for notes, boxes, and groups.

### Open Question 3
- Question: To what extent does a chain-of-thought (CoT) prompting strategy improve the accuracy of image-to-UML conversion compared to the simple prompts used in this study?
- Basis in paper: [explicit] The authors mention that they "focused on simple prompts for the VLMs," and explicitly list "advanced prompts to introduce chain-of-thought approach" as a direction for "Future experiments."
- Why unresolved: The baseline established in the paper relies on standard instruction prompts; the potential for improved reasoning or error reduction by guiding the model step-by-step (CoT) remains unquantified in this specific domain.
- What evidence would resolve it: A comparison of outputs generated using simple prompts versus CoT prompts on the same dataset, measured by the proposed node, edge, and structural element metrics.

## Limitations
- Dataset is relatively small (50 diagrams), limiting generalizability
- Manual syntax normalization introduces potential human bias
- Ground truth creation was entirely manual, making it time-consuming and potentially inconsistent
- Only two VLM models were evaluated, restricting comparative analysis

## Confidence
- **High Confidence**: The diff-based evaluation methodology is sound and well-specified
- **Medium Confidence**: Component-wise error metrics provide useful diagnostic information
- **Medium Confidence**: Performance differences between Claude and GPT-4 are well-documented
- **Low Confidence**: Generalizability to other diagram types and larger datasets has not been established

## Next Checks
1. Apply the evaluation pipeline to a small subset of diagrams where you manually verify each error classification to ensure the regex patterns and categorization rules match the paper's intent
2. Compare error rates with and without the manual syntax normalization step to quantify its impact on measured performance
3. Implement the suggested chain-of-thought reasoning prompt (mentioned in conclusion) for 5 diagrams and measure whether structural element error rates improve as hypothesized