---
ver: rpa2
title: 'PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora'
arxiv_id: '2510.14377'
source_url: https://arxiv.org/abs/2510.14377
tags:
- questions
- answer
- question
- document
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses a class of questions in question answering
  (QA) requiring aggregation of data across all documents in a knowledge base without
  a clear stopping point for retrieval. These "pluri-hop" questions are formalized
  by three criteria: recall sensitivity (omitting even a single relevant passage leads
  to an incorrect answer), exhaustiveness (it is impossible to infer from the retrieved
  context whether the evidence set is complete), and exactness (there is only one
  best answer to the question).'
---

# PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora

## Quick Facts
- arXiv ID: 2510.14377
- Source URL: https://arxiv.org/abs/2510.14377
- Authors: Mykolas Sveistrys; Richard Kunert
- Reference count: 11
- Primary result: PluriHopRAG achieves 18-52% relative F1 score improvements over baselines for answering pluri-hop questions

## Executive Summary
This work introduces and formalizes "pluri-hop" questions in question answering - a class of questions requiring aggregation of data across all documents in a knowledge base without a clear stopping point for retrieval. These questions are characterized by three criteria: recall sensitivity (omitting even one relevant passage leads to incorrect answers), exhaustiveness (impossible to infer from retrieved context whether evidence set is complete), and exactness (only one best answer exists). The authors introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English, which is shown to be 8-40% more repetitive than other common datasets, better reflecting practical challenges of recurring report corpora. Current approaches struggle with these questions, reaching at most 40% in statement-wise F1 score, prompting the authors to propose PluriHopRAG - a RAG architecture that decomposes queries into document-level subquestions and uses cross-encoder filtering to discard irrelevant documents before costly LLM reasoning, achieving significant performance improvements.

## Method Summary
The authors formalize pluri-hop questions and create a diagnostic dataset (PluriHopWIND) from wind industry reports. They evaluate multiple RAG approaches including traditional pipelines, graph-based variants, and multimodal approaches, finding all struggle with pluri-hop questions. To address this, they propose PluriHopRAG which follows a "check all documents individually, filter cheaply" approach: decomposing queries into document-level subquestions and using cross-encoder filters to discard irrelevant documents before LLM reasoning. This approach achieves 18-52% relative F1 score improvements depending on the base LLM used.

## Key Results
- PluriHopWIND dataset is 8-40% more repetitive than other common datasets, providing higher density of distractor documents
- Current RAG approaches tested on PluriHopWIND reach at most 40% statement-wise F1 score on pluri-hop questions
- PluriHopRAG achieves relative F1 score improvements of 18-52% compared to baseline approaches
- The proposed approach demonstrates the value of exhaustive retrieval and early filtering as alternatives to top-k methods

## Why This Works (Mechanism)
PluriHopRAG works by fundamentally changing the retrieval strategy for pluri-hop questions. Instead of retrieving a small set of top-k documents and hoping to capture all relevant information, it exhaustively processes all documents individually through document-level subquestions. The cross-encoder filter then cheaply eliminates irrelevant documents before the expensive LLM reasoning step. This approach addresses the core challenge of pluri-hop questions - the impossibility of knowing from retrieved context whether the evidence set is complete - by ensuring no document is prematurely excluded from consideration.

## Foundational Learning
- Pluri-hop questions: QA tasks requiring aggregation across all documents without clear stopping point for retrieval
  - Why needed: Standard RAG assumes retrievable context is sufficient, but pluri-hop questions require exhaustive search
  - Quick check: Can you identify whether a question requires checking all documents versus stopping after finding sufficient evidence?

- Recall sensitivity: Missing even one relevant passage leads to incorrect answers
  - Why needed: Highlights the binary nature of correctness in pluri-hop questions versus partial credit scenarios
  - Quick check: Does omitting a single relevant document change the final answer?

- Exhaustiveness: Impossible to infer from retrieved context whether evidence set is complete
  - Why needed: Distinguishes pluri-hop from standard QA where additional retrieval can be stopped when answer is found
  - Quick check: Can you determine from current context whether you've found all necessary evidence?

- Cross-encoder filtering: Jointly encoding query and document to score relevance
  - Why needed: Provides cheap filtering before expensive LLM reasoning on all documents
  - Quick check: Does the cross-encoder correctly identify irrelevant documents with high precision?

## Architecture Onboarding

Component map: Query -> Document-level subquestion decomposition -> Cross-encoder filter -> LLM reasoning -> Answer aggregation

Critical path: The cross-encoder filter is critical - it must efficiently eliminate irrelevant documents to make exhaustive retrieval computationally feasible. Poor filtering performance directly impacts runtime and cost.

Design tradeoffs: Exhaustive retrieval vs. computational cost (exhaustive approach ensures recall but increases processing time); early filtering vs. risk of missing relevant documents (aggressive filtering risks incorrect answers); document-level vs. passage-level processing (document-level aligns with pluri-hop nature but may miss cross-document evidence within single passages).

Failure signatures: High false positive rate in cross-encoder filter (many irrelevant documents passed to LLM, increasing cost); high false negative rate (relevant documents filtered out, causing incorrect answers); poor subquestion decomposition (LLM cannot effectively answer document-level questions); aggregation failure (LLM cannot synthesize answers across multiple documents).

First experiments:
1. Evaluate cross-encoder filter performance on held-out data to establish false positive/negative rates
2. Test subquestion decomposition quality by having humans rate whether document-level questions are answerable
3. Measure computational overhead of exhaustive retrieval vs. standard top-k approaches on sample corpus

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation focused primarily on a single domain (wind industry reports) and language pair (German-English), limiting generalizability
- Dataset size is relatively small (48 questions across 191 documents), limiting statistical power for some analyses
- Computational costs of the proposed approach are not detailed, despite emphasis on exhaustive retrieval
- Absolute performance levels remain low (maximum 40% F1), indicating the problem remains fundamentally challenging

## Confidence

High confidence: The core formalization of pluri-hop questions and demonstration that current RAG approaches struggle with this setting.

Medium confidence: The characterization of PluriHopWIND as more repetitive and distractor-rich compared to other datasets; the effectiveness of PluriHopRAG architecture and its 18-52% relative improvements.

Low confidence: The broader applicability of the "check all documents individually, filter cheaply" approach across different domains and languages.

## Next Checks

1. Evaluate PluriHopRAG on additional domains and languages beyond wind industry reports to test generalizability of both the dataset and approach.

2. Conduct ablation studies to quantify the individual contributions of the two key components (document-level subquestion decomposition and cross-encoder filtering) to overall performance improvements.

3. Measure and report the computational overhead of exhaustive retrieval compared to standard top-k approaches, including both retrieval and filtering costs across different document corpus sizes.