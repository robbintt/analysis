---
ver: rpa2
title: 'Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting'
arxiv_id: '2512.05243'
source_url: https://arxiv.org/abs/2512.05243
tags:
- poetry
- prompt
- language
- creative
- poems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes using poetic prompts as a diagnostic tool for
  analyzing algorithmic biases in large language models. By employing creative text
  prompting techniques, the researchers explored how models describe and evaluate
  a renowned poet's work, particularly examining their willingness to adapt or rewrite
  original creative content for different audiences.
---

# Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting

## Quick Facts
- arXiv ID: 2512.05243
- Source URL: https://arxiv.org/abs/2512.05243
- Reference count: 0
- Authors: P. D. Edgar; Alia Hall
- Key outcome: Poetic prompting reveals algorithmic biases through praise-heavy rhetoric that obscures political specificity and varying thresholds for creative appropriation

## Executive Summary
This study proposes using poetic prompts as a diagnostic tool for analyzing algorithmic biases in large language models. By employing creative text prompting techniques, the researchers explored how models describe and evaluate a renowned poet's work, particularly examining their willingness to adapt or rewrite original creative content for different audiences. The analysis revealed that models tend to use highly praising language when describing poets and their work, often defaulting to terms like "universal" and "profound" that may obscure the specific cultural and political contexts of the poetry. Additionally, the models showed varying degrees of willingness to rewrite original works, with some refusing on ethical grounds while others readily complied, highlighting the complex ethical considerations around creative appropriation.

## Method Summary
The researchers used a scaffolded prompt sequence with three LLMs (ChatGPT, Claude, DeepSeek) analyzing Maya Angelou's poetry across five prompt types: Investigation, Analysis, Composition, Adaptation, and Explanation. They ran approximately 30 prompts per model across three poems, capturing all outputs including refusals. Analysis involved word frequency counts, word clouds, and Critical Discourse Analysis focusing on rhetorical tropes, adaptation willingness, and explanation quality. The study examined how models described the poet, analyzed individual works, composed original poetry, adapted content for different audiences, and explained their modifications.

## Key Results
- Models default to panegyric language ("profound," "powerful," "universal") that can mask political complexity in poetry
- Different models show measurable differences in ethical hesitation about creative appropriation, with ChatGPT readily rewriting while Claude refused to adapt "Still I Rise" on ethical grounds
- Poetic prompting may lower model guardrails compared to informational queries, revealing associations about cultural figures that remain suppressed in standard contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poetic prompting may lower model guardrails and reveal associations suppressed in standard informational queries.
- Mechanism: The paper proposes that aesthetic/creative contexts reduce perceived stakes for objectivity enforcement, creating a "back door" into word-association patterns. Models may relax certain content filters when operating in a "subjective expression" mode rather than factual inquiry mode.
- Core assumption: Models treat poetic output as lower-accountability than informational output, enabling different rhetorical patterns to emerge.
- Evidence anchors:
  - [section I.A]: "The mode of poetic writing...lowers the stakes of objectivity in deference to the value of subjective expression...appealing to aesthetic motives may lower the stakes for a model or provide a back door into the kinds of word associations that a model makes."
  - [section I.B]: Notes that ChatGPT's model spec "Assume best intentions from the user" allows circumvention through creative context.
  - [corpus]: Weak direct corpus support for this specific mechanism; neighbor papers focus on black-box optimization rather than creative probing.
- Break condition: If models begin applying equal scrutiny to creative and informational outputs, the diagnostic utility diminishes.

### Mechanism 2
- Claim: Praise-heavy rhetoric may obscure political complexity by substituting generic positive adjectives for substantive engagement with cultural specificity.
- Mechanism: Models default to "panegyric" (praise) language—"profound," "powerful," "moving"—which creates an appearance of respect while potentially flattening politically challenging content. This functions as a form of benevolent stereotyping.
- Core assumption: Positive-sounding language can mask the same erasure dynamics that negative stereotypes produce.
- Evidence anchors:
  - [section III.B]: "While the information...is encyclopedic in breadth, it's also panegyric (public-facing praising) in tone."
  - [section III.C]: "These adjectives reflect positively on the writer in question, but could tend to overshadow the complexity of the work."
  - [corpus]: No direct corpus validation for this rhetorical flattening mechanism.
- Break condition: If models are explicitly fine-tuned to balance praise with political specificity, this pattern should reduce.

### Mechanism 3
- Claim: Different models exhibit measurably different thresholds for creative appropriation, reflecting divergent safety alignment strategies.
- Mechanism: Safety guardrails around copyright/appropriation vary by model vendor. ChatGPT readily rewrote all three poems; Claude resisted "Still I Rise" on ethical grounds; DeepSeek showed technical resistance followed by partial compliance when using "DeepThink" reasoning mode.
- Core assumption: Model refusal behavior reveals underlying value hierarchies in alignment training.
- Evidence anchors:
  - [section III.D]: "ChatGPT presented no barriers to performing entire rewrites...Claude rewrote 'Alone,' but resisted rewriting 'Still I Rise' on ethical grounds."
  - [section III.D]: Claude stated the poem "draws its immense power specifically from Black experience" and refused to "broaden" it.
  - [corpus]: Weak support; corpus focuses on black-box attacks and prompt optimization, not cultural appropriation guardrails.
- Break condition: If vendors converge on standardized appropriation policies, cross-model differences should diminish.

## Foundational Learning

- Concept: **Critical Discourse Analysis (CDA)**
  - Why needed here: The paper uses CDA to examine "the interplay between language, power, and ideology" in model outputs. Without this framework, you cannot systematically identify how rhetoric constructs or obscures power relations.
  - Quick check question: Can you explain how word choice in model outputs might reinforce or obscure political claims even without explicit factual errors?

- Concept: **Prompt Patterns (per White et al.)**
  - Why needed here: The Poetry Prompt Pattern builds on established pattern categories ("Interaction" patterns). Understanding the intent/motivation/structure framework is necessary to extend or modify the approach.
  - Quick check question: How would you structure a prompt pattern that tests model behavior around a different sensitive cultural domain?

- Concept: **Algorithmic Bias Beyond Toxicity**
  - Why needed here: The paper explicitly contrasts its approach with "negative algorithmic biases" research, focusing instead on "positive-to-neutral cultural stereotypes." This reframing is essential for interpreting findings.
  - Quick check question: Why might "positive" stereotypes (e.g., universal praise) still constitute harmful bias?

## Architecture Onboarding

- Component map: Investigation -> Analysis -> Composition -> Adaptation -> Explanation
- Critical path:
  1. Define subject (poet/topic) and prepare scaffolded prompts
  2. Run prompts across target models with session isolation
  3. Capture all output genres including refusals
  4. Apply CDA coding to identify rhetorical tropes
  5. Compare adaptation willingness and explanation quality
- Design tradeoffs:
  - Open-ended poetic prompts yield richer data but increase hallucination risk
  - Multiple models increase comparison utility but require managing different interface behaviors
  - Term adjustments (e.g., "rewrite" → "adapt") may be needed mid-experiment
- Failure signatures:
  - ChatGPT: May refuse intersectional prompts even after discussing components separately
  - Claude: May refuse adaptation entirely; requires "companion poem" framing
  - DeepSeek: May return server errors or require reasoning-mode activation
- First 3 experiments:
  1. Replicate with a different Black poet (e.g., Langston Hughes) to test generalization of praise-rhetoric patterns
  2. Apply the Persona Prompt Pattern combination suggested in conclusions to test tonal variation
  3. Test non-Black culturally specific poets to compare whether "universality" language appears differently across racial/cultural categories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining the Poetry Prompt Pattern with the Persona Prompt Pattern reveal different or additional algorithmic biases compared to Poetry Prompt Patterns alone?
- Basis in paper: [explicit] The authors explicitly suggest that future researchers "could combine the Poetry Prompt Pattern with the Persona Prompt Pattern... to test tonal and expressive trends."
- Why unresolved: The current study only tested poetry-specific scaffolds without layering persona constraints, so interaction effects remain unknown.
- What evidence would resolve it: A comparative study using both patterns simultaneously on the same subject matter, analyzing shifts in rhetoric, appropriation willingness, and stereotyping.

### Open Question 2
- Question: Do the identified rhetorical patterns—praise-laden language, universality conflation, and variable ethical hesitation—generalize across diverse poets from different cultural and political contexts?
- Basis in paper: [inferred] The study examined only Maya Angelou; the authors note "researchers could conduct similar research by asking LLMs to write poetry about different nuanced topics" but did not test cross-cultural generalizability.
- Why unresolved: Angelou represents a specific profile (Black feminist writer, Civil Rights era, canonical status); whether models respond similarly to contemporary, non-Western, or politically contentious poets remains untested.
- What evidence would resolve it: Replication of the prompt scaffold with poets spanning different eras, ethnicities, political orientations, and canonical status levels.

### Open Question 3
- Question: What specific model training or design factors account for the observed differences in ethical hesitation (Claude and DeepSeek resisting, ChatGPT readily rewriting)?
- Basis in paper: [inferred] The paper documents divergent behaviors but does not isolate whether differences stem from training data, safety fine-tuning, architectural choices, or corporate policy implementation.
- Why unresolved: The study treats models as black boxes, observing outputs without access to training provenance or safety mechanism documentation.
- What evidence would resolve it: Collaborative research with model providers offering transparency into safety training procedures, or systematic testing across multiple model versions to identify when behavioral shifts emerge.

## Limitations
- The study relies on qualitative analysis without quantitative validation across broader corpora
- Analysis focuses on a single poet (Maya Angelou), limiting generalizability to other cultural figures
- The study does not account for potential temporal variation in model behavior as alignment training evolves

## Confidence
- **High Confidence**: The observation that models use generic praise language ("universal," "profound") when describing poets, and that this rhetoric can obscure political specificity
- **Medium Confidence**: The claim that poetic prompting reveals different rhetorical patterns than informational queries, though the exact mechanism remains unclear
- **Medium Confidence**: The finding that models show different thresholds for creative appropriation, reflecting alignment differences, though the interpretation of these differences as "value hierarchies" is interpretive
- **Low Confidence**: The proposed mechanism that creative contexts inherently lower model guardrails without systematic comparative testing

## Next Checks
1. **Generalization Test**: Replicate the analysis with 3-5 poets from different cultural backgrounds and genres to determine whether the praise-rhetoric pattern and adaptation thresholds vary systematically across identity categories.

2. **Temporal Stability Test**: Run the same prompt set across the same models 3-6 months apart to assess whether rhetorical patterns and refusal behaviors change with alignment updates, particularly for adaptation prompts.

3. **Control Condition Test**: Compare model outputs for the same poets using both poetic prompts and equivalent informational queries to quantify whether "creative context" actually produces measurably different rhetorical patterns or if observed differences stem from other factors.