---
ver: rpa2
title: 'KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language
  Models in the Korean Language'
arxiv_id: '2503.23730'
source_url: https://arxiv.org/abs/2503.23730
tags:
- language
- judge
- criteria
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KOFFVQA introduces a Korean-language visual question answering
  benchmark with objective grading criteria to evaluate large vision-language models.
  Unlike existing benchmarks that rely on subjective judge models or fixed answer
  choices, KOFFVQA uses pre-defined, question-specific criteria to allow reliable,
  fine-grained scoring by even small open-source models.
---

# KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language

## Quick Facts
- arXiv ID: 2503.23730
- Source URL: https://arxiv.org/abs/2503.23730
- Reference count: 40
- Primary result: Introduces objective grading criteria for Korean VQA evaluation, demonstrating that larger VLMs don't guarantee better performance and that free-form responses with criteria-based judging provide more consistent evaluation than baseline comparison methods.

## Executive Summary
KOFFVQA addresses the critical need for reliable Korean-language visual question answering evaluation by introducing a benchmark with objective, pre-defined grading criteria for free-form responses. Unlike existing benchmarks that rely on subjective judge models or fixed answer choices, KOFFVQA uses question-specific criteria to enable consistent scoring by even small open-source models. The benchmark covers 10 subcategories across perception, reasoning, and safety/bias with 275 human-crafted questions. Evaluation on 47 models reveals that larger size doesn't guarantee better performance, and models excel in different subcategories, highlighting the benchmark's effectiveness at exposing language-specific and cultural knowledge gaps in VLMs.

## Method Summary
KOFFVQA evaluates VLMs on Korean free-form VQA using pre-defined grading criteria with partial scores summing to 10 per question. The pipeline involves: (1) VLM generates free-form Korean response to image-question pairs, (2) langid library filters non-Korean responses (set to 0), (3) LLM judge (Gemma 2 9B, temp=0) grades using criteria only (no image input), and (4) scores are averaged per subcategory and scaled to 0-100 range. The approach eliminates judge model unreliability by constraining evaluation to specific, verifiable features in responses, and avoids VLM hallucination issues by using text-only LLM judging.

## Key Results
- Larger VLMs don't guarantee better performance on Korean tasks; many English-excelling models fail simple Korean questions
- Criteria-based evaluation yields significantly more consistent results than baseline comparison methods
- Providing images to judge models degrades accuracy due to hallucination, validating the LLM-as-judge approach
- Different models excel in different subcategories, demonstrating the benchmark's ability to reveal specialized capabilities and weaknesses

## Why This Works (Mechanism)

### Mechanism 1: Criteria-Based Judgment as an Attentional Constraint
Providing predefined, objective grading criteria with partial scores to an LLM judge reduces evaluation variance and improves consistency compared to baseline comparison approaches. The grading criteria act as explicit instructions that constrain the judge model's attention to specific, verifiable features in the response. By structuring evaluation as a rule-matching task with discrete partial points, the system minimizes subjective interpretation and free-form reasoning, which are primary sources of inconsistency in judge models.

Core assumption: The judge model can reliably perform objective rule-matching (e.g., checking for presence of specific phrase or concept) without introducing its own subjective quality standards.

Break condition: If grading criteria are ambiguous, overlapping, or require nuanced interpretation (e.g., "is the explanation clear?"), the judge model's subjectivity will re-emerge, increasing variance.

### Mechanism 2: Visual Hallucination Avoidance via LLM-as-Judge
Using a text-only LLM judge yields more consistent and accurate evaluation than a VLM judge provided with image input, even when both receive identical grading criteria. VLMs are prone to hallucinating non-existent visual details. When a VLM judge is given an image, it may base its evaluation on these hallucinations rather than the provided criteria. An LLM judge, lacking visual input, cannot hallucinate image details and is forced to rely solely on text-based criteria and response.

Core assumption: The grading criteria are comprehensive and self-contained, meaning a judge does not need to consult the original image to verify correctness of a response.

Break condition: If a question requires verifying a visual detail not explicitly captured in the criteria (e.g., novel object's shape), an LLM judge would fail, whereas a non-hallucinating VLM judge could succeed.

### Mechanism 3: Language-Specific Stress Testing for Latent Deficits
Evaluating VLMs in a non-English language like Korean uncovers performance deficits hidden by English-only benchmarks, particularly in cultural knowledge and language-specific OCR. Multilingual models often have imbalanced training data. A dedicated benchmark forces the model to generate text in the target language and reason about culturally specific visual content (e.g., Korean documents, landmarks).

Core assumption: Model performance is not uniform across languages and cultures; direct measurement in target language is required for accurate assessment.

Break condition: If a model's training data is perfectly balanced across all languages and cultural contexts, then a language-specific benchmark would yield results highly correlated with English benchmarks.

## Foundational Learning

- **Concept: LLM-as-a-Judge**
  - Why needed here: This is the core evaluation paradigm. Understanding how to design prompts and criteria to control judge's behavior is essential for grasping the benchmark's design.
  - Quick check question: How would you design a grading rubric to objectively evaluate whether a response "correctly identifies the main emotion in the image"?

- **Concept: VLM Hallucination**
  - Why needed here: The paper's central argument for using an LLM judge over a VLM judge is predicated on the VLM's tendency to hallucinate.
  - Quick check question: If a VLM judge is asked to evaluate a response about an image it can "see," what are two ways its own visual processing could lead to an incorrect evaluation?

- **Concept: Multilingual & Cultural Bias in VLMs**
  - Why needed here: The paper demonstrates that English benchmarks are insufficient proxies for performance in other languages and cultural contexts.
  - Quick check question: A model scores 95% on an English VQA benchmark. Without testing it, what specific capabilities would you be uncertain about regarding its performance for Korean users?

## Architecture Onboarding

- **Component map:**
  1. VLM Under Test: Input: (Image, Question in Korean) → Output: (Free-form Korean Response)
  2. Response Filter: Uses `langid` library. If response is not Korean (excluding numbers/symbols), score is forced to 0
  3. LLM Judge: Input: (VLM Response, Predefined Grading Criteria) → Output: (Score 0-10, Reasoning). Note: No image is provided
  4. Score Aggregator: Averages raw scores per subcategory and overall, then scales to 0-100 range

- **Critical path:** VLM Response Generation → Language ID Check → **LLM Judgment via Criteria** → Score Scaling. The defining architectural choices are the **Grading Criteria** and the deliberate **omission of image input** to the judge.

- **Design tradeoffs:**
  - **Free-form vs. Multiple Choice:** Free-form tests generation but requires a judge. KOFFVQA prioritizes generation
  - **LLM vs. VLM Judge:** LLM ensures consistency but cannot verify visual details. KOFFVQA prioritizes consistency, assuming criteria are sufficient
  - **Criteria-Based vs. Baseline Comparison:** Criteria are more reliable but require manual curation. KOFFVQA invests in curation for reproducibility

- **Failure signatures:**
  - **Judge overrides criteria:** The LLM applies its own subjective standards despite explicit rules
  - **Ambiguous criteria:** Rules are too vague, forcing the judge to interpret, leading to inconsistent results
  - **VLM language failure:** The model reverts to English or generates incoherent Korean text, triggering a zero score from the filter
  - **Judge visual hallucination (if VLM used):** If architecture is changed to use VLM judge, it may penalize correct responses based on hallucinated image details

- **First 3 experiments:**
  1. **Reproduce Judge Consistency:** Evaluate a fixed set of VLM responses using a small open-source LLM (e.g., Gemma 2 9B) with temperature > 0 over multiple runs. Calculate the standard deviation of scores. Confirm it is low, as reported in the paper's Table 2.
  2. **Ablate Evaluation Method:** Compare KOFFVQA's criteria-based evaluation against a baseline-comparison method (providing a "ground truth" response). Run both on a response subset and measure standard deviation and agreement with human graders to validate the paper's consistency claims.
  3. **Portability Test:** Create 5 new questions for a new subcategory (e.g., "Korean Traditional Food"). Write objective grading criteria and evaluate 2-3 VLMs. This tests your understanding of the curation pipeline and the design's applicability to new domains.

## Open Questions the Paper Calls Out

- **Can the benchmark creation process be automated effectively using synthetic data generation?**
  - The paper states that while current data is human-curated, the approach can be scaled using synthetic data for both questions and grading criteria. This specific criteria-based approach's automated scaling remains untested.
  - What evidence would resolve it: A comparative study evaluating the reliability and consistency of synthetically generated grading criteria versus human-authored criteria on the same set of VLM responses.

- **Can specific training of judge models eliminate the errors of "arbitrarily overruling criteria" and "unexplainable misjudgment"?**
  - The study relies on off-the-shelf models (Gemma 2 9B) which occasionally ignore instructions or hallucinate reasons; fine-tuning for strict criteria adherence was not performed.
  - What evidence would resolve it: Experiments training a judge model specifically on objective adherence to grading rubrics, showing a reduction in the failure categories identified in the paper.

- **Is the degradation in accuracy when providing images to judge models a solvable alignment issue or a fundamental limitation of current VLM architectures?**
  - The paper demonstrates the problem (visual hallucination in judges) but leaves open the possibility that more robust future models could leverage visual context correctly.
  - What evidence would resolve it: Re-evaluating the "KOFFVQA-V" method (judge with image) using future VLMs specifically trained against hallucination to see if they can surpass the accuracy of the text-only LLM judge.

## Limitations

- Judge model adherence to grading criteria remains imperfect, with occasional overrides of explicit rules suggesting evaluation isn't perfectly objective
- The study doesn't report inter-annotator agreement between human graders and the LLM judge, leaving open the question of whether "objective" criteria truly align with human judgment
- More systematic error analysis is needed to fully characterize when and why VLMs hallucinate during judgment

## Confidence

- **High confidence:** The core finding that VLMs struggle with Korean language tasks despite strong English performance is well-supported by direct experimental evidence
- **Medium confidence:** The claim about LLM judges being more consistent than VLM judges due to hallucination avoidance is demonstrated but could benefit from more systematic error analysis
- **Medium confidence:** The assertion that free-form evaluation with criteria is significantly more consistent than baseline comparison needs more direct comparison data

## Next Checks

1. **Inter-annotator reliability test:** Have 3-5 human experts independently grade 50 random responses using the same criteria. Calculate inter-rater reliability (Cohen's kappa) and compare variance to the LLM judge's performance.

2. **Hallucination audit:** For 30 VLM judge evaluations, manually verify whether hallucinations actually occurred and whether they affected grading outcomes. Compare error rates between VLM and LLM judges on identical tasks.

3. **Criteria ambiguity analysis:** For 20 responses where judge and criteria potentially conflicted, analyze whether conflicts stemmed from genuinely ambiguous criteria versus judge overreach. Refine criteria based on findings and re-evaluate.