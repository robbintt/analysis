---
ver: rpa2
title: 'StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation
  via Visual Question Answering Feedback'
arxiv_id: '2510.20093'
source_url: https://arxiv.org/abs/2510.20093
tags:
- sketch
- diffusion
- sketches
- images
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StableSketcher, a framework that enhances
  Stable Diffusion to generate human-drawn pixel-based sketches. The approach fine-tunes
  the VAE using a reconstruction loss combined with LPIPS for better perceptual quality
  and applies reinforcement learning with a VQA-based reward function to improve prompt
  fidelity.
---

# StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback

## Quick Facts
- **arXiv ID:** 2510.20093
- **Source URL:** https://arxiv.org/abs/2510.20093
- **Reference count:** 40
- **Primary result:** StableSketcher achieves FID of 143.68 and TIFAScore of 0.68, outperforming Stable Diffusion baselines in sketch generation quality and prompt fidelity.

## Executive Summary
This paper introduces StableSketcher, a framework that enhances Stable Diffusion to generate human-drawn pixel-based sketches. The approach fine-tunes the VAE using a reconstruction loss combined with LPIPS for better perceptual quality and applies reinforcement learning with a VQA-based reward function to improve prompt fidelity. A new dataset, SketchDUO, is introduced, providing sketch-caption-QA triplets with positive and negative examples to capture desired and undesired sketch styles. Experiments show StableSketcher achieves the best performance, with FID of 143.68 and TIFAScore of 0.68, outperforming Stable Diffusion baselines in both image quality and text-image alignment. Ablation studies confirm the effectiveness of the VQA-based reward and VAE loss design.

## Method Summary
StableSketcher fine-tunes Stable Diffusion v1.5 for pixel-based sketch generation through a three-stage process: (1) VAE fine-tuning with MSE + LPIPS loss to improve perceptual reconstruction quality, (2) UNet fine-tuning on sketch-caption pairs using the fine-tuned VAE, and (3) DDPO reinforcement learning with a VQA-based reward function. The method uses a novel dataset, SketchDUO, containing 35,851 sketches with positive/negative examples and QA pairs. The VQA model (fine-tuned mPLUG-large) provides a reward signal (TIFAScore) that evaluates element-level prompt fidelity through question-answering.

## Key Results
- StableSketcher achieves FID of 143.68 and TIFAScore of 0.68 on the SketchDUO test set
- Outperforms Stable Diffusion baselines in both image quality and text-image alignment
- Ablation studies confirm the effectiveness of VQA-based reward and VAE loss design

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning the VAE with MSE + LPIPS loss improves sketch reconstruction quality over the standard KL-regularized VAE. The standard Autoencoder KL loss over-regularizes the latent space, causing posterior collapse where fine-grained sketch details are discarded. By reducing KL weight and adding LPIPS (which captures perceptual similarity via multi-layer CNN features), the VAE preserves contours and line structures that MSE alone misses.

### Mechanism 2
VQA-based reward (TIFAScore) provides more reliable prompt fidelity feedback than caption-based metrics like BERTScore. TIFAScore evaluates element-level alignment by checking whether each prompt element (e.g., "3 layers," "white background") is satisfied via Q&A pairs, rather than computing global semantic similarity. The reward R_VQA = α·R_instance + (1-α)·R_sketch balances semantic correctness and style fidelity.

### Mechanism 3
Contrastive positive/negative examples in SketchDUO help the model distinguish desired sketch abstraction from common failure modes. Negative examples explicitly capture Stable Diffusion's failure modes (excessive detail, shading, color, non-white backgrounds), providing contrastive learning signal. The model learns to avoid these while positive examples reinforce target style.

## Foundational Learning

- **Latent Diffusion Models (LDMs)**
  - Why needed here: StableSketcher builds on Stable Diffusion's latent space; understanding how VAE encoding + UNet denoising + CLIP conditioning interact is essential.
  - Quick check question: Can you explain why LDMs are more efficient than pixel-space diffusion, and what role the VAE plays in reconstruction quality?

- **Policy Gradient Methods (DDPO)**
  - Why needed here: The VQA reward is integrated via DDPO, which treats denoising steps as actions in a Markov decision process.
  - Quick check question: How does DDPO differ from standard RL policy gradient, and why is it suitable for diffusion models?

- **Perceptual Loss (LPIPS)**
  - Why needed here: LPIPS captures perceptual similarity that MSE misses, critical for sketch contours and line structures.
  - Quick check question: Why would two images with low MSE still look perceptually different, and how does LPIPS address this?

## Architecture Onboarding

- **Component map:** Text Encoder (CLIP ViT-L/14) → VAE (fine-tuned) → UNet (fine-tuned) → Generated sketch → VQA Model (fine-tuned) → TIFAScore → DDPO gradient → UNet update
- **Critical path:** Prompt → CLIP encoder → UNet (with fine-tuned VAE) → generated sketch → VQA model → TIFAScore → DDPO gradient → UNet update
- **Design tradeoffs:**
  - VAE loss: MSE alone stable but poor perceptual quality; LPIPS improves perceptual but requires careful weighting (0.1×)
  - Reward α balance: 0.5 balances instance vs. sketch-style, but may need adjustment for different domains
  - Dataset size: 35.8K sketches sufficient for 30 classes, but generalization to long-tail objects limited
- **Failure signatures:**
  - VAE posterior collapse: Generated images become near-white if KL weight too high
  - Reward hacking: Model optimizes for high TIFAScore without improving visual quality if QA pairs don't cover relevant attributes
  - Style drift: Without negative examples, model reverts to photorealistic bias
- **First 3 experiments:**
  1. VAE reconstruction sanity check: Encode/decode SketchDUO images; measure MSE, LPIPS, and visual contour preservation vs. baseline VAE
  2. VQA reward calibration: Generate 50 sketches with baseline SD; compute TIFAScore and compare human ratings to validate reward-human alignment
  3. Ablation on negative samples: Train with positive-only vs. positive+negative; measure FID and TIFAScore delta to quantify contrastive benefit

## Open Questions the Paper Calls Out
- Can StableSketcher effectively generalize to complex, multi-object scenes or long-tail object categories absent from the SketchDUO dataset?
- Does incorporating per-stroke metadata (order, length, curvature) into the training objective improve the structural coherence or temporal logic of generated sketches?
- Does the reduction or removal of the KL divergence term in the VAE loss function, implemented to prevent posterior collapse, negatively impact the continuity of the latent space for tasks like sketch interpolation?

## Limitations
- Dataset generalization: Performance on long-tail sketch categories remains untested due to SketchDUO's 30-class limitation
- Hyperparameter sensitivity: Critical hyperparameters (VAE LPIPS weight 0.1, RL reward balance α=0.5) lack systematic sensitivity analysis
- Computational cost: DDPO training requirements (GPU hours, memory) are undisclosed, limiting scalability assessment

## Confidence
- **High confidence**: VAE fine-tuning mechanism and reconstruction improvements
- **Medium confidence**: VQA-based reward effectiveness (TIFAScore shows improvement but relies on VQA model accuracy)
- **Medium confidence**: SketchDUO dataset construction and contrastive learning benefits

## Next Checks
1. VAE robustness test: Systematically vary LPIPS weight (0.01, 0.1, 1.0) and KL regularization to identify stability boundaries
2. VQA reward validation: Generate 100 sketches with baseline and StableSketcher; collect human ratings of prompt fidelity and compare correlation with TIFAScore
3. Cross-domain generalization: Apply StableSketcher to a held-out sketch category (e.g., animals or vehicles) not in SketchDUO to assess zero-shot performance and identify failure patterns