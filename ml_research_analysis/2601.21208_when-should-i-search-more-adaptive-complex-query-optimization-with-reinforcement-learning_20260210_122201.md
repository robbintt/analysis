---
ver: rpa2
title: 'When should I search more: Adaptive Complex Query Optimization with Reinforcement
  Learning'
arxiv_id: '2601.21208'
source_url: https://arxiv.org/abs/2601.21208
tags:
- query
- retrieval
- acqo
- performance
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing complex queries
  in retrieval-augmented generation (RAG) systems. While existing methods focus on
  optimizing single queries, real-world scenarios often involve complex queries requiring
  multiple parallel or sequential search strategies for disambiguation and decomposition.
---

# When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21208
- Source URL: https://arxiv.org/abs/2601.21208
- Reference count: 16
- ACQO achieves state-of-the-art performance on complex query benchmarks, outperforming established baselines in both accuracy and efficiency.

## Executive Summary
This paper addresses the challenge of optimizing complex queries in retrieval-augmented generation (RAG) systems, where traditional methods struggle with query decomposition and multi-strategy search. The authors propose Adaptive Complex Query Optimization (ACQO), an RL framework that adaptively determines when to expand the search process. ACQO uses curriculum reinforcement learning with two stages to stabilize training and includes modules for adaptive query reformulation and robust result aggregation.

## Method Summary
ACQO employs a two-stage Curriculum Reinforcement Learning approach to optimize complex queries in RAG systems. Stage I trains on full data using max-pool rewards over sub-query subsets, while Stage II filters to challenging queries and adds a logarithmic precision bonus. The framework features Adaptive Query Reformulation to dynamically decompose queries and Rank-Score Fusion (RSF) to aggregate multi-query results. Training uses the DAPO optimizer with specific hyperparameters and is performed separately for different retrievers like BM25 and ANCE.

## Key Results
- ACQO achieves state-of-the-art performance on TopiOCQA, HotpotQA, and MultiHop-RAG benchmarks
- Significant improvements in MRR@K, NDCG@K, and Recall@K metrics compared to established baselines
- Improved computational efficiency while maintaining broad compatibility with different retrieval architectures

## Why This Works (Mechanism)
ACQO addresses the challenge of complex query optimization by dynamically deciding when to decompose queries and how to aggregate results from multiple search strategies. The two-stage curriculum RL approach first learns basic query reformulation patterns, then focuses on challenging cases with precision bonuses. The RSF module combines rank and score information to ensure robust result aggregation, while adaptive decomposition prevents unnecessary query expansion.

## Foundational Learning
- **Reinforcement Learning**: Why needed - to learn optimal query reformulation strategies; Quick check - verify RL convergence and reward signals
- **Curriculum Learning**: Why needed - to stabilize training and focus on challenging queries; Quick check - confirm filtering by τ threshold effectively identifies learning frontier
- **Rank-Score Fusion**: Why needed - to aggregate results from multiple search strategies; Quick check - verify RSF correctly combines rank and score signals

## Architecture Onboarding
- **Component Map**: Retriever -> ACQO (Reformulation + RSF) -> Reward Computation -> DAPO Optimizer
- **Critical Path**: Query input → Reformulation decision → Sub-query generation → Multi-retrieval → RSF aggregation → Reward calculation → Policy update
- **Design Tradeoffs**: Stage I focuses on coverage vs. Stage II on precision; max-pool vs. ensemble rewards; adaptive vs. static decomposition
- **Failure Signatures**: Vanilla RL stuck at ~41% MAP (insufficient exploration); performance degradation on hard queries (inadequate Stage II filtering)
- **First Experiments**: 1) Verify retriever indices build correctly; 2) Test RSF aggregation on synthetic multi-query results; 3) Validate reward computation matches piecewise specification

## Open Questions the Paper Calls Out
None

## Limitations
- Key hyperparameters (λ, δ, η decay) are unspecified, requiring extensive tuning for replication
- Prompt template for query decomposition is referenced but not fully extractable from PDF
- "Broad compatibility" claim is supported only by BM25 and ANCE experiments

## Confidence
- **Performance superiority claims**: Medium confidence - substantial gains reported but hyperparameter sensitivity introduces uncertainty
- **RL training stability**: Medium confidence - two-stage curriculum well-justified but exact schedules unknown
- **Computational efficiency claims**: Low confidence - lacks detailed runtime/resource usage comparisons

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary λ and δ around plausible values to assess impact on performance
2. **Prompt template reconstruction**: Use context clues to reconstruct the query reformulation prompt and validate its effect
3. **Cross-retriever generalization**: Evaluate ACQO on at least one additional retriever (e.g., DPR) to test broad compatibility claims