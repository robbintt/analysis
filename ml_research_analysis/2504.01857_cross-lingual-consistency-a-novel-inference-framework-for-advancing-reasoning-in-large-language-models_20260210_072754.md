---
ver: rpa2
title: 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning
  in Large Language Models'
arxiv_id: '2504.01857'
source_url: https://arxiv.org/abs/2504.01857
tags:
- consistency
- accuracy
- reasoning
- english
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning performance
  in small-scale large language models (LLMs), particularly those under 10B parameters,
  which often struggle with semantic drift and logical inconsistencies due to multilingual
  training corpus biases. The authors propose Cross-Lingual Consistency (CLC), an
  inference framework that enhances reasoning by integrating multilingual Chain-of-Thought
  (CoT) paths through majority voting.
---

# Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2504.01857
- Source URL: https://arxiv.org/abs/2504.01857
- Reference count: 1
- Primary result: CLC achieves 9.5-18.5% absolute accuracy gains on mathematical reasoning tasks vs. monolingual self-consistency

## Executive Summary
This paper addresses reasoning performance limitations in small-scale LLMs (<10B parameters) caused by linguistic biases in multilingual training corpora. The authors propose Cross-Lingual Consistency (CLC), an inference framework that enhances reasoning by integrating multilingual Chain-of-Thought paths through majority voting. By leveraging multiple language versions of reasoning prompts, CLC neutralizes language-specific biases and escapes monolingual reasoning traps. Experiments on CMATH and MGSM datasets demonstrate significant improvements over traditional self-consistency, with gains particularly pronounced in low-resource language scenarios.

## Method Summary
CLC is an inference-only framework that extends self-consistency to multilingual settings. The method translates input problems into N target languages, generates K candidate answers per language using zero-shot CoT with sampling, aggregates all N×K answers, and applies majority voting to select the final answer. The framework assumes models can generate meaningful reasoning paths in each target language and that language-specific errors are unlikely to align across languages, enabling error filtering through probabilistic consensus.

## Key Results
- DeepSeek-Math-7B-Instruct: +9.5% absolute accuracy gain on CMATH vs. English self-consistency
- Qwen2.5-Math-7B-Instruct: +6.5% absolute accuracy gain on CMATH
- Gemma2-9B-Instruct: +6.0% absolute accuracy gain on CMATH, 4.1%-18.5% gains on MGSM
- Optimal language cardinality peaks at n=6 before degradation from probabilistic consensus dilution

## Why This Works (Mechanism)

### Mechanism 1
Multilingual ensemble voting neutralizes language-specific biases inherited from imbalanced training corpora. Each language induces different error patterns due to training data distribution, and majority voting filters out non-overlapping errors.

### Mechanism 2
Multilingual reasoning expands the searchable solution space beyond monolingual "reasoning traps." Different languages encode problems with different surface forms, activating different reasoning pathways that may find correct paths inaccessible in the original language.

### Mechanism 3
Probabilistic consensus through majority voting creates self-correcting ensemble, but effectiveness peaks at optimal language cardinality before degrading. Adding languages increases pattern coverage initially, but beyond threshold, low-quality languages introduce conflicting predictions that dilute consensus.

## Foundational Learning

- **Self-Consistency (monolingual)**: Why needed: CLC extends self-consistency from single-language to multilingual sampling. Quick check: Can you explain why greedy decoding underperforms self-consistency on mathematical reasoning tasks?

- **Chain-of-Thought (CoT) Prompting**: Why needed: CLC operates on CoT reasoning paths. The framework presumes models can generate step-by-step reasoning. Quick check: What is the difference between zero-shot CoT and few-shot CoT, and why might zero-shot be preferred for multilingual settings?

- **Majority Voting / Plurality Consensus**: Why needed: The decision rule for deterministic tasks in CLC. Understanding when majority voting works vs. when semantic clustering is needed is essential. Quick check: For a problem with answer "42," if you have votes [42, 42, 41, 42, 40], what is the majority consensus?

## Architecture Onboarding

- **Component map**: Translation Layer -> Multilingual Inference Engine -> Answer Aggregator -> Decision Module
- **Critical path**: Input question → translate to selected languages → for each language: run K sampling epochs with zero-shot CoT prompts → pool all candidate answers across all languages → apply majority vote
- **Design tradeoffs**: More languages vs. inference cost (each adds K passes), translation quality vs. speed (machine vs. professional), epochs per language (10 used, diminishing returns)
- **Failure signatures**: Cardinality overload (accuracy peaks then declines at n>6), language capability mismatch (poor-performing languages add noise), translation artifacts (poorly translated prompts cause reasoning failures)
- **First 3 experiments**:
  1. Bilingual baseline: Replicate Chinese-English CLC on CMATH with 7B model; compare monolingual self-consistency vs. bilingual-consistency. Expect +6–10% absolute gain.
  2. Language ablation: On MGSM, test CLC with incrementally more languages (n=2 to n=11). Plot accuracy curve to identify optimal cardinality.
  3. Low-resource language impact: Remove lowest-performing language (e.g., Telugu) from 11-language ensemble and measure accuracy change.

## Open Questions the Paper Calls Out

- **Optimal language selection**: How to predict and decide the optimal set of languages without exhaustive enumeration? Currently requires combinatorial search, which is computationally expensive and dataset-specific.

- **Large-parameter model efficacy**: Does CLC yield improvements in large-parameter models (>100B)? The study focuses on sub-10B models; benefits for larger models like GPT-4 and PaLM-2 are unknown.

- **Semantic clustering for non-deterministic tasks**: Is the proposed semantic clustering approach effective for non-deterministic reasoning tasks? Experiments are restricted to deterministic mathematical reasoning using majority voting.

## Limitations
- Sampling hyperparameters (temperature, top_p, top_k) not specified, critical for generating diverse reasoning paths
- Translation quality impact on accuracy gains not quantified; machine translation may introduce semantic drift
- Low-resource languages can degrade consensus through probabilistic voting dilution beyond optimal cardinality

## Confidence
- **High Confidence**: Cross-lingual ensemble voting improves mathematical reasoning accuracy over monolingual self-consistency (9.5-18.5% absolute gains)
- **Medium Confidence**: Optimal language cardinality exists (peaking at n=6), and low-resource languages can degrade consensus through probabilistic voting
- **Low Confidence**: Translation quality impact on accuracy gains and sensitivity to sampling hyperparameters; these implementation details are unspecified

## Next Checks
1. **Sampling Sensitivity Analysis**: Systematically vary temperature and top_p to identify optimal settings for generating diverse reasoning paths. Measure self-consistency gains vs. greedy decoding.

2. **Translation Quality Experiment**: Compare CLC performance using Google Translate vs. professional translation for Chinese→English conversion on CMATH. Quantify translation-induced semantic drift.

3. **Language Cardinality Thresholding**: On MGSM, test CLC with incremental languages (n=2, 4, 6, 8, 10, 11) and measure accuracy degradation points. Validate whether removing low-resource languages improves consensus accuracy.