---
ver: rpa2
title: Rethinking Genomic Modeling Through Optical Character Recognition
arxiv_id: '2602.02014'
source_url: https://arxiv.org/abs/2602.02014
tags:
- page
- sequence
- task
- genomic
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OpticalDNA reframes genomic modeling as an OCR-style document\
  \ understanding task, converting DNA sequences into structured visual layouts and\
  \ training a vision\u2013language model to learn compact, reconstructible visual\
  \ tokens. By defining prompt-conditioned objectives over reading, region grounding,\
  \ retrieval, and masked span completion, it enables region-aware representations\
  \ that preserve fine-grained genomic information under reduced effective token budgets."
---

# Rethinking Genomic Modeling Through Optical Character Recognition

## Quick Facts
- arXiv ID: 2602.02014
- Source URL: https://arxiv.org/abs/2602.02014
- Reference count: 40
- One-line primary result: OpticalDNA reframes genomic modeling as an OCR-style document understanding task, achieving the best average AUROC (0.852) on eQTL tasks with nearly 20× fewer effective tokens.

## Executive Summary
OpticalDNA reframes genomic modeling as an OCR-style document understanding task by converting DNA sequences into structured visual layouts. The approach trains a vision–language model to learn compact, reconstructible visual tokens through prompt-conditioned objectives over reading, region grounding, retrieval, and masked span completion. This enables region-aware representations that preserve fine-grained genomic information under reduced effective token budgets. Experiments show OpticalDNA consistently outperforms state-of-the-art baselines on long-range genomic benchmarks, achieving superior efficiency and accuracy.

## Method Summary
OpticalDNA converts 1D DNA sequences into multi-page RGB images with monospace text rendering, then applies a frozen SAM-Conv-CLIP-L visual encoder followed by learned projector and multi-page fusion to produce L=100 visual tokens. The system is pretrained on six OCR-style tasks (transcription, grounding, ROI reading, mask completion, retrieval, classification) using a DeepSeek-3B MoE decoder with LoRA fine-tuning. The approach uses a two-stage HG38 pretraining process (227,600 + 190,000 steps) with 8×H100 GPUs, achieving compression ratios of 19.0-21.2× while maintaining high fidelity.

## Key Results
- Achieves best average AUROC of 0.852 on eQTL tasks with nearly 20× fewer effective tokens
- Surpasses models with up to 985× more activated parameters while tuning only 256k trainable parameters
- Consistently outperforms state-of-the-art baselines on long-range genomic benchmarks

## Why This Works (Mechanism)

### Mechanism 1: 2D Spatial Inductive Bias for Sparse Genomic Signals
Rendering 1D DNA into 2D layouts improves accuracy-efficiency trade-offs by converting discontinuous long-range dependencies into spatial structures amenable to region-aware processing. The 2D grid structure allows convolutional backbones to process local neighborhoods efficiently, exploiting the spatial organization of rendered nucleotides rather than requiring exhaustive sequential attention. Region-aware visual tokenization enables the model to "jump" between informative regions rather than scanning background. This mechanism assumes sparse functional signals in genomes benefit more from selective, structure-aware processing than from uniform sequential modeling.

### Mechanism 2: Understanding-Driven Visual Compression
The visual encoder produces compact, reconstructible tokens that reduce the effective token budget while preserving fine-grained genomic information. The SAM-Conv-CLIP-L frontend with 16× downsampling, followed by a learned projector and multi-page fusion, compresses each 640×640 page into ~100 tokens. The fusion module uses a single attention layer with mean pooling to aggregate across pages, creating a fixed-length representation invariant to page count. This mechanism assumes OCR-trained visual encoders can capture nucleotide-level information in a form that is compressible yet reconstructible for downstream tasks.

### Mechanism 3: Prompt-Conditioned Genomic Primitive Learning
Training on six OCR-style tasks (reading, grounding, retrieval, completion, classification) aligns representations with practical genomic workflows. Each task provides different supervision: T1/T2 teach nucleotide transcription and spatial grounding; T3/T4 isolate region-conditioned reading and contextual inference; T5/T6 test retrieval and global aggregation. The prompt-conditioned decoder learns to route visual representations to task-appropriate outputs. This mechanism assumes genomic understanding decomposes into learnable primitives analogous to document OCR operations.

## Foundational Learning

- **Vision-Language Models (VLMs) for Document Understanding**
  - Why needed here: OpticalDNA builds on DeepSeek-OCR, a VLM trained on document images with spatial grounding capabilities. Understanding how VLMs bind visual regions to text is essential.
  - Quick check question: Can you explain how a VLM like DeepSeek-OCR represents both image patches and text tokens in a shared embedding space?

- **Convolutional Inductive Biases vs. Sequential Attention**
  - Why needed here: The paper explicitly compares 2D CNNs to 1D sequence models. Understanding locality, receptive fields, and translation equivariance clarifies why 2D helps sparse genomic signals.
  - Quick check question: Why would a 2D convolution be more efficient than a 1D Transformer for processing a rendered 1800-nucleotide DNA page?

- **Genomic Sequence Analysis Fundamentals**
  - Why needed here: Tasks like splice site prediction, eQTL analysis, and variant localization are the downstream evaluation targets. Understanding what these tasks require (motif recognition, long-range dependencies, coordinate indexing) contextualizes the task design.
  - Quick check question: What makes eQTL prediction challenging for sequence models, and how does long-context modeling help?

## Architecture Onboarding

- **Component map**: DNA -> multi-page RGB images (640×640) with per-nucleotide bounding-box annotations -> SAM-Conv-CLIP-L (frozen) -> 16× downsampling -> 16×16 patches -> projector Πθ -> Fθ (single attention layer, mean pooling across pages) -> L=100 fused tokens -> DeepSeek-3B MoE (570M activated params) with LoRA -> downstream heads

- **Critical path**: DNA → rendered pages → visual frontend → projector → fusion → decoder (pretraining) / pooled representation (downstream)

- **Design tradeoffs**: Fixed L=100 tokens vs. page-count-dependent tokens trades adaptability for fixed-budget inference; frozen visual frontend vs. end-to-end training prioritizes stability and OCR knowledge transfer vs. DNA-specific optimization; OCR-style task diversity vs. single-objective pretraining provides richer representations but more complex training.

- **Failure signatures**: Poor transcription accuracy indicates visual frontend not capturing nucleotide glyphs; degraded grounding precision suggests fusion not preserving spatial correspondence; weak OOD generalization indicates overfitting to pretraining genome with insufficient cross-subspecies coverage.

- **First 3 experiments**: 1) Render a 2kb DNA sequence to verify nucleotide visibility, bounding-box alignment, and multi-page handling; 2) Run pretrained OpticalDNA on T1/T2 with held-out sequences to measure transcription accuracy and grounding IoU; 3) Fine-tune a linear probe on a single eQTL tissue to validate representation transferability with minimal trainable parameters.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can subsequence localization (T5) performance be improved by constraining query specificity (e.g., limiting subsequence lengths or bounding occurrence counts), or does scaling model capacity alone address the grounding bottleneck? The paper notes T5 achieves only ~57-60% text correctness with localization accuracy dropping sharply at higher IoU thresholds.

- **Open Question 2**: Does mask-span curriculum or context-aware sampling during pretraining improve masked completion (T4) performance, particularly for longer masked spans? The paper shows Text-EM drops from 54.8% (span=1) to <1% (span=8), suggesting strong sensitivity to context availability and target span length.

- **Open Question 3**: Can longer-context extensions of OpticalDNA achieve reliable chromosome-level classification, or is the task fundamentally limited by repeat-derived motif ambiguity across chromosomes? Current T6 accuracy is only 6.2% (HG38) and 10.4% (rice), motivating future work on longer-context modeling toward chromosome-scale prediction.

## Limitations

- Critical dependency on unavailable DeepSeek-OCR pretrained weights and SAM-Conv-CLIP-L checkpoint for visual front-end
- Six OCR-style tasks untested in isolation; uncertainty whether primitives fully capture genomic reasoning complexity
- Parameter efficiency claims (985× fewer activated parameters) depend on unspecified LoRA configurations and exact target modules

## Confidence

- **High confidence**: 2D spatial inductive bias mechanism and multi-page fusion design are well-supported by architecture and comparisons
- **Medium confidence**: OCR-style task set and alignment with genomic primitives is conceptually sound but lacks external validation
- **Low confidence**: Compression ratios and parameter efficiency claims depend heavily on unavailable pretrained weights and unspecified LoRA configurations

## Next Checks

1. **Bounding-box alignment verification**: Render sample DNA pages and overlay stored nucleotide bounding boxes to verify spatial correspondence accuracy, testing the core assumption that OCR-trained visual encoders can capture nucleotide-level information.

2. **Pretraining task isolation**: Run OpticalDNA on T1 (transcription) and T3 (grounding) tasks separately with held-out sequences to measure baseline performance before full pretraining, isolating whether OCR task formulations work as intended.

3. **Downstream transfer with minimal fine-tuning**: Fine-tune a linear probe on a single eQTL tissue from DNALONGBENCH using the pretrained OpticalDNA features, validating the representation transferability claim with minimal trainable parameters as specified.