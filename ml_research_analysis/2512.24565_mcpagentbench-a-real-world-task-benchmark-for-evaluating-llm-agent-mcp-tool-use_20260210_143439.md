---
ver: rpa2
title: 'MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool
  Use'
arxiv_id: '2512.24565'
source_url: https://arxiv.org/abs/2512.24565
tags:
- task
- tool
- tasks
- tools
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use

## Quick Facts
- arXiv ID: 2512.24565
- Source URL: https://arxiv.org/abs/2512.24565
- Reference count: 8
- Primary result: Evaluates LLM agents' MCP tool-use capabilities using 178 tasks with distractor tools and mock execution

## Executive Summary
MCPAgentBench is a benchmark designed to evaluate how well LLM agents can use MCP (Model Context Protocol) tools in realistic scenarios. It introduces distractor tools to test discrimination abilities, uses mock servers for stable evaluation, and measures both task completion (TFS) and efficiency (TEFS) to reveal planning deficits. The benchmark exposes significant gaps between models' ability to finish tasks versus doing so efficiently, with most models showing over 10-point drops in efficiency scores.

## Method Summary
The benchmark uses 178 test cases spanning single-tool, dual-serial, dual-parallel, and multi-tool tasks across four domains. It combines a large pool of 20,000+ MCP tools with simulated "mock" servers generated by LLMs, ensuring stable and reproducible evaluation. Each task presents a candidate tool list containing both correct tools and distractors, forcing agents to discriminate relevant tools from noise. The evaluation measures Task Finish Score (TFS) for exact tool+parameter matches and Task Efficiency Finish Score (TEFS) for correct execution order, using an AutoGen-based sandbox for execution.

## Key Results
- Nearly all evaluated models exhibit over 10-point decline in TEFS compared to TFS
- OpenAI models show extreme serial bias, achieving 0 TEFS on parallel tasks
- Claude Sonnet 4.5 succeeds with aggressive parallel strategies where others fail
- Performance drops significantly as candidate tool count increases from 10 to 30

## Why This Works (Mechanism)

### Mechanism 1: Distractor-Based Robustness Testing
The benchmark constructs candidate tool lists combining correct tools with unrelated distractors, forcing agents to discriminate relevant tools from noise. This tests semantic matching and rejection abilities rather than simple execution in sterile environments. The discrimination challenge becomes trivial if distractors are too semantically distant, or ambiguous if too similar.

### Mechanism 2: Localized Mock Execution for Stability
Instead of calling remote MCP servers, the framework generates Python stub functions based on tool definitions and executes them in an Autogen sandbox. This ensures consistent responses and stable latency while maintaining identical function signatures. The approach sacrifices some realism for reproducibility and stability.

### Mechanism 3: Efficiency Delta via TEFS
TFS checks if correct tools and parameters were used regardless of order, while TEFS requires the optimal execution order (parallel vs. serial). The delta reveals models that default to safe but slow serial execution. This metric assumes a single optimal ordering exists, potentially penalizing valid alternative strategies.

## Foundational Learning

- **Concept: Model Context Protocol (MCP)**
  - Why needed: MCP is the standardized interface layer connecting LLM to tools, prerequisite to understanding structured tool definitions
  - Quick check: How does MCP differ from a standard REST API definition in terms of agent interoperability?

- **Concept: Serial vs. Parallel Tool Invocation**
  - Why needed: The paper explicitly differentiates Dual-Tool Serial (dependency exists) from Dual-Tool Parallel (independent), forming the basis for TEFS
  - Quick check: If Tool A fetches a user ID and Tool B fetches a profile using that ID, is this a serial or parallel dependency?

- **Concept: Dynamic Sandbox Loading**
  - Why needed: The system must dynamically load specific tool subsets for specific tasks to create the "distractor" challenge
  - Quick check: Why is dynamic loading preferred over providing the entire 20,000-tool library to the agent at once?

## Architecture Onboarding

- **Component map:** MCP Tool Collection -> Tag Set & Matcher -> Mock Generator -> Autogen Sandbox -> Evaluator
- **Critical path:** Data Collection → Tagging → Task-Tool Matching → Mock Code Generation → Sandbox Execution → Scoring (TFS/TEFS)
- **Design tradeoffs:**
  - Realism vs. Stability: Uses simulated mock tools instead of live MCP servers to ensure reproducibility, at the cost of potentially oversimplifying complex tool behaviors
  - Candidate List Size: Increasing K (number of tools) increases difficulty but also increases token consumption and latency
- **Failure signatures:**
  - Serial Bias: High TFS but near 0 on "Dual Parallel" TEFS indicates failure to recognize independence
  - Over-Parallelization: Attempting parallel calls on serial-dependent tasks forces TEFS failure
  - Hallucinated Parameters: Correct tool names but invented parameters not found in schema
- **First 3 experiments:**
  1. Baseline Run: Execute standard 178-task set to establish TFS vs. TEFS baselines
  2. Distractor Scaling: Vary candidate tool count from 10 to 30 to measure selection robustness
  3. Ablation on Mock Fidelity: Replace simple mock stubs with more complex logic to test execution feedback handling

## Open Questions the Paper Calls Out

### Open Question 1
Does high performance on simulated "mock" MCP tools generalize to live, remote MCP servers with network latency and stateful execution errors? The paper solves stability issues with simulated tools but doesn't quantify performance degradation when facing real-world API interactions. Evidence needed: comparative study using mock stubs versus real API equivalents.

### Open Question 2
What causes advanced models like GPT-5 and o3 to exhibit strict "serial" bias, resulting in zero efficiency scores on parallel tasks? The paper documents the behavioral dichotomy but doesn't investigate underlying model mechanics. Evidence needed: ablation study analyzing token-level decision probabilities or testing if prompt engineering can induce parallel behavior.

### Open Question 3
Does the dataset's strict requirement for a "unique solution" unfairly penalize agents that devise valid but alternative tool invocation sequences? While strict matching ensures automated grading consistency, it assumes one optimal path exists. Evidence needed: manual review of failed cases where agent's tool sequence differs but logically satisfies the task.

## Limitations
- Mock-server approach may overestimate agent capabilities in production settings where real MCP server behaviors differ
- Distractor selection strategy remains underspecified, potentially making discrimination trivial or ambiguous
- TEFS metric's reliance on single "golden solution" may penalize valid alternative strategies that achieve the same outcome more efficiently

## Confidence

- **High Confidence:** Dataset construction methodology and TFS metric definition are clearly specified and reproducible
- **Medium Confidence:** TEFS metric and efficiency gap interpretation are logically sound but assume one optimal ordering exists
- **Low Confidence:** Generalizability to live MCP server environments given mock-based evaluation approach

## Next Checks
1. **Mock Fidelity Test:** Replace subset of mock tools with actual MCP server implementations and measure performance deltas
2. **Distractor Diversity Analysis:** Systematically vary distractor semantic similarity and measure impact on selection accuracy across model families
3. **Alternative Solution Validation:** Manually verify whether alternative execution orders could be equally or more efficient than "golden solution" for TFS-failed cases