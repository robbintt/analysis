---
ver: rpa2
title: 'LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport'
arxiv_id: '2509.23436'
source_url: https://arxiv.org/abs/2509.23436
tags:
- attention
- lotformer
- linear
- transport
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LOTFormer introduces a linear-time doubly stochastic attention\
  \ mechanism by factorizing the attention matrix through a low-rank optimal transport\
  \ plan conditioned on a learnable pivot measure. Instead of computing a full n\xD7\
  n transport plan, it solves two entropic OT problems between queries\u2192pivot\
  \ and pivot\u2192keys, then composes them into a glued coupling that is provably\
  \ doubly stochastic and has rank at most r\u226An."
---

# LOTFormer: Doubly-Stochastic Linear Attention via Low-Rank Optimal Transport

## Quick Facts
- arXiv ID: 2509.23436
- Source URL: https://arxiv.org/abs/2509.23436
- Authors: Ashkan Shahbazi, Chayne Thrash, Yikun Bai, Keaton Hamm, Navid NaderiAlizadeh, Soheil Kolouri
- Reference count: 32
- Primary result: Linear-time doubly stochastic attention via low-rank optimal transport, enabling plug-and-play replacement of softmax attention in pretrained checkpoints

## Executive Summary
LOTFormer introduces a linear-time doubly stochastic attention mechanism by factorizing the attention matrix through a low-rank optimal transport plan conditioned on a learnable pivot measure. Instead of computing a full n×n transport plan, it solves two entropic OT problems between queries→pivot and pivot→keys, then composes them into a glued coupling that is provably doubly stochastic and has rank at most r≪n. This allows applying attention to values in O(nr) time without forming the full matrix. The pivot locations and masses are learned end-to-end. Across ImageNet-1K and Long Range Arena benchmarks, LOTFormer matches or exceeds strong linear attention and doubly stochastic baselines in both accuracy and efficiency, and enables direct plug-and-play replacement of softmax attention in pretrained checkpoints.

## Method Summary
LOTFormer factorizes attention through a learnable pivot measure with small support r≪n. It solves two entropic OT problems: queries to pivot (Γ^(1)) and pivot to keys (Γ^(2)), then composes them via Γ^(1)^T Diag(σ)^(-1) Γ^(2) to form a doubly stochastic attention matrix of rank at most r. The pivot locations Z and masses σ are learned end-to-end. This enables O(nd_k r + Tnr) complexity versus O(n²d_k) for standard attention, with the attention applied to values in three steps without materializing the full matrix.

## Key Results
- Matches or exceeds strong linear attention and doubly stochastic baselines on ImageNet-1K and Long Range Arena
- Achieves Top-1 accuracy of ~74.8% on ImageNet-1K with DeiT-Tiny using r=32
- Enables plug-and-play replacement of softmax attention in pretrained BERT checkpoints with minimal fine-tuning
- Provides theoretical guarantees: attention matrix is provably doubly stochastic with rank at most r

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Transport Factorization via Pivot Conditioning
Introducing a learnable pivot measure with small support r ≪ n yields a transport plan with rank at most r, enabling O(nr) attention without forming the full n×n matrix. The attention matrix A = (Γ^(1))^T Diag(σ)^(-1) Γ^(2) factorizes through the pivot: Γ^(1) ∈ R^(n×r) (queries→pivot), Γ^(2) ∈ R^(n×r) (pivot→keys). By construction, rank(A) ≤ r, and applying attention to values becomes O = Γ^(1) (Diag(σ)^(-1) (Γ^(2) V)), each step O(nr). The pivot measure with r ≪ n support is sufficient to approximate the query-key coupling; transport quality degrades gracefully as r decreases.

### Mechanism 2: Doubly Stochastic Normalization via Entropic OT
Solving entropic OT problems with uniform marginals produces couplings that are doubly stochastic by construction, balancing token participation across rows and columns. Each Γ^(1), Γ^(2) is an entropic OT plan with prescribed marginals (p^0, p^1) and (p^0, p^2). The glued coupling A preserves both row sums (∑_j A_ij = 1) and column sums (∑_i A_ij = 1), yielding a doubly stochastic matrix. Uniform marginals (p^1 = p^2 = 1/n) are appropriate for self-attention; non-uniform marginals would require task-specific tuning.

### Mechanism 3: End-to-End Learnable Pivot and Masses
Learning pivot locations Z = {z_t} and masses σ ∈ Δ^(r-1) jointly with attention projections allows the pivot to adapt to the geometry of queries and keys without explicit supervision. The pivot is parameterized as learnable vectors z_t ∈ R^(d_k) and probabilities p_t^0 > 0. Gradients flow through Sinkhorn iterations (T steps) and the glued composition, updating Z and σ alongside W^Q, W^K, W^V. Sinkhorn iterations (typically T=5) provide sufficiently accurate OT solutions for gradient-based learning; numerical stability is maintained with ε > 0.

## Foundational Learning

- Concept: **Optimal Transport and Couplings**
  - Why needed here: LOTFormer frames attention as a transport plan; understanding couplings Γ ∈ U(p, q) with marginal constraints is essential.
  - Quick check question: Given two discrete probability distributions p, q, what does Γ ∈ U(p, q) mean, and why does Γ1 = p and Γ^T 1 = q?

- Concept: **Entropic Regularization and Sinkhorn Algorithm**
  - Why needed here: LOTFormer uses entropic OT (OT_ε) solved via Sinkhorn iterations; the entropy term H(Γ) and Sinkhorn scaling forms K^(1), K^(2) are central to the algorithm.
  - Quick check question: What effect does ε have on the transport plan as ε → 0 vs. ε → ∞, and how many Sinkhorn iterations are typically needed?

- Concept: **Doubly Stochastic vs. Row-Stochastic Matrices**
  - Why needed here: Standard softmax attention is row-stochastic (each row sums to 1); DS attention additionally normalizes columns, which LOTFormer exploits for balanced token participation.
  - Quick check question: Why does row-stochastic attention permit column sums to vary widely, and what failure mode does this cause?

## Architecture Onboarding

- Component map:
  Inputs X → Q, K, V via W^Q, W^K, W^V → OT(Q,Z) and OT(Z,K) via Sinkhorn → Γ^(1), Γ^(2) → Y=Γ^(2)V → Z'=Diag(σ)^(-1)Y → Output O=Γ^(1)Z'

- Critical path:
  1. Project tokens to Q, K, V
  2. Compute QZ^T and KZ^T (O(nd_k r))
  3. Run T Sinkhorn iterations for each OT problem (O(T nr) per head)
  4. Apply factorized attention to V via the three-step multiplication
  5. (Optional) Apply [CLS]-softmax and polarization for vision tasks

- Design tradeoffs:
  - r (pivot size): Larger r → higher capacity and accuracy, but O(nr) cost increases. Paper finds r=32 optimal for DeiT-Tiny.
  - ε (entropic regularization): Smaller ε → sharper, sparser couplings; larger ε → smoother, more diffuse. Paper finds ε≈1 optimal; ε<0.1 hard to optimize with few iterations.
  - T (Sinkhorn iterations): More iterations → more accurate DS normalization. Paper uses T=5 as default; T=10 yields marginal gains at higher cost.
  - [CLS] handling: Full DS on [CLS] degrades global pooling; [CLS]-softmax preserves aggregation ability.

- Failure signatures:
  - Causal masking + DS → identity matrix: DS constraints with lower-triangular causal mask force A = I; LOTFormer is bidirectional-only.
  - r too small: Attention maps show coarse clustering, missing fine-grained token correspondences; accuracy drops sharply.
  - ε too small with insufficient T: Sinkhorn fails to converge; gradients become unstable, loss diverges.
  - ε too large: Couplings over-smooth; attention becomes near-uniform, losing discriminative power.

- First 3 experiments:
  1. Sanity check on synthetic data: Create small n=8, r=2 sequences with known cluster structure; verify LOTFormer learns pivot locations aligned with clusters and produces interpretable Γ^(1), Γ^(2).
  2. Ablation on r: Train DeiT-Tiny on ImageNet-100 with r ∈ {4, 8, 16, 32, 64}; plot accuracy vs. forward/backward time to identify compute-optimal r.
  3. Plug-and-play conversion: Load a pretrained BERT-base checkpoint, swap encoder self-attention with LOTFormer (r=32, T=5, ε=1), and run distillation + fine-tuning on MNLI; measure percent recovery vs. baseline softmax.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can doubly stochastic attention be adapted for causal/autoregressive settings without collapsing to degenerate solutions?
- Basis in paper: [explicit] Appendix F states: "Extending DS attention to autoregressive settings would require modifying the constraints or the mask structure, which we leave to future work."
- Why unresolved: Under standard causal masking, DS constraints force the unique solution A=I (identity), making nontrivial autoregressive self-attention impossible with current constraints.
- What evidence would resolve it: A modified constraint formulation or mask structure that preserves both causal structure and meaningful token interactions while maintaining some form of balanced attention.

### Open Question 2
- Question: What is the theoretical expressivity gap between LOTFormer's learned pivot measure and the optimal low-rank OT plan?
- Basis in paper: [inferred] Section 3 notes LOTFormer's pivot is "learned end-to-end as part of the attention mechanism, but not optimized explicitly for transport cost minimization," while optimal low-rank OT minimizes transport cost in that class.
- Why unresolved: The paper avoids optimal low-rank OT for computational reasons (requires n×n cost matrix), but does not characterize what approximation quality or representational capacity is lost.
- What evidence would resolve it: Theoretical bounds on suboptimality, or empirical comparisons in regimes where forming the full cost matrix is feasible.

### Open Question 3
- Question: What is the principled way to select pivot size r for a given architecture and compute budget?
- Basis in paper: [inferred] Table 7 shows accuracy-efficiency trade-offs across r∈{4,8,16,32,64} with peak at r=32 for DeiT-Tiny, but the paper states r requires "per-backbone tuning" without providing systematic guidance.
- Why unresolved: The selection appears empirical in this work, with no theory predicting optimal r from sequence length, model dimension, or task properties.
- What evidence would resolve it: A predictive rule or adaptive mechanism for r that matches or improves upon grid-search performance across diverse backbones.

## Limitations
- **Causal Masking Incompatibility**: Doubly stochastic constraints with causal masking degenerate to identity attention, limiting LOTFormer to bidirectional architectures only.
- **Pivot Size vs. Data Complexity**: Low-rank approximation quality depends critically on r; it's unclear how LOTFormer scales to very long sequences or highly diverse data.
- **Entropic Regularization Trade-off**: Small ε requires more Sinkhorn iterations for stability; optimal ε settings may be task-dependent and not fully explored.

## Confidence
- **High Confidence**: The core mechanism (rank-r factorization via pivot, doubly stochastic normalization via entropic OT, end-to-end learnability) is mathematically sound and clearly demonstrated in experiments. The efficiency claims (O(nr) vs. O(n²)) are verifiable.
- **Medium Confidence**: The empirical benefits on ImageNet-1K and Long Range Arena are convincing, but comparisons are primarily against other linear/bi-stochastic methods rather than full softmax baselines. The ablation on r and ε is present but limited in scope.
- **Low Confidence**: There is no ablation on the number of Sinkhorn iterations T, and the paper does not explore non-uniform marginals for the pivot. The plug-and-play conversion claim is supported by one BERT distillation experiment but lacks broader validation across model families.

## Next Checks
1. **Synthetic Sequence Clustering**: Create a controlled n=16 sequence with 4 known clusters and test whether LOTFormer (r=4) learns pivot locations that align with clusters and produces interpretable low-rank attention maps.
2. **Causal Masking Failure Mode**: Implement a masked LOTFormer variant and verify that DS constraints with causal masks force the attention matrix to identity, confirming the bidirectional-only limitation.
3. **r Scaling Law**: Train DeiT-Small on ImageNet-100 with r ∈ {8, 16, 32, 64, 128} and plot accuracy vs. forward/backward time to empirically determine the compute-optimal r for larger models.