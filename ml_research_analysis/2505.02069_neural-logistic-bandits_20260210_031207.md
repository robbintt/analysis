---
ver: rpa2
title: Neural Logistic Bandits
arxiv_id: '2505.02069'
source_url: https://arxiv.org/abs/2505.02069
tags:
- inequality
- lemma
- term
- follows
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of learning nonlinear reward\
  \ functions in contextual bandits using neural networks, particularly in logistic\
  \ bandit settings. The main obstacles are handling large ambient feature dimensions\
  \ and minimizing dependencies on the problem-dependent constant \u03BA, which can\
  \ scale exponentially with the decision set size."
---

# Neural Logistic Bandits

## Quick Facts
- arXiv ID: 2505.02069
- Source URL: https://arxiv.org/abs/2505.02069
- Reference count: 40
- Key outcome: Achieves variance- and data-adaptive regret bounds in neural contextual bandits by replacing worst-case variance κ with neural network-estimated variance in the design matrix

## Executive Summary
This paper addresses the challenge of learning nonlinear reward functions in contextual bandits using neural networks, particularly in logistic bandit settings. The main innovation is introducing a novel Bernstein-type tail inequality for self-normalized vector-valued martingales that bypasses direct dependence on ambient feature dimension while minimizing dependence on the problem-dependent constant κ. This enables two algorithms - NeuralLog-UCB-1 with regret bound O(εd√(κT)) and NeuralLog-UCB-2 with regret bound O(εd√(T/κ*)) - where εd is the effective dimension. The key insight is replacing worst-case variance bounds with neural network-estimated variance in the design matrix, achieving both variance- and data-adaptive regret bounds that improve upon existing methods.

## Method Summary
The method employs a two-layer ReLU neural network initialized via NTK-inspired schemes, where the reward function is linearized around initialization using Neural Tangent Kernel theory. Two algorithms are proposed: NeuralLog-UCB-1 uses worst-case variance κ in the design matrix V_t = Σ_i (1/m)g(x_i;θ_0)g(x_i;θ_0)^T + κλ_tI, while NeuralLog-UCB-2 uses neural network-estimated variance W_t = Σ_i (ẋμ(f(x_i;θ_i))/m)g(x_i;θ_0)g(x_i;θ_0)^T + λ_tI. Both algorithms employ adaptive regularization λ_t that grows with data complexity, enabling fully data-adaptive concentration bounds without projection steps. The UCB action selection uses the linearized function g(x;θ_0)^T(θ* - θ_0) with confidence bounds derived from the novel Bernstein-type inequality. Training occurs every 50 rounds via gradient descent on regularized log-likelihood.

## Key Results
- Introduces novel Bernstein-type inequality for self-normalized vector-valued martingales that scales with effective dimension rather than ambient dimension
- Achieves NeuralLog-UCB-1 regret bound O(εd√(κT)) and NeuralLog-UCB-2 regret bound O(εd√(T/κ*))
- Empirically validates theoretical findings on both synthetic datasets (h_1(x) = 0.2(x^Tθ)^4, h_2(x) = 20cos(x^Tθ), h_3(x) = 5x^Tθ x) and real-world datasets (mnist, mushroom, shuttle)
- Shows NeuralLog-UCB-2 outperforms baseline methods by approximately 50% reduction in cumulative regret compared to NCBF-UCB

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A new Bernstein-type inequality for self-normalized vector-valued martingales enables regret bounds that scale with effective dimension rather than ambient dimension, while minimizing κ dependence.
- **Mechanism:** The inequality incorporates non-uniform variances (ẋμ(x^T_iθ*)) into the design matrix H_t = Σ_i ẋμ(x^T_iθ*)x_ix_i^T + λI, yielding a tail bound of order Õ(√ẽd) where ẽd = log det(R/λ_0H̄ + I). This avoids both the direct √d dependence of prior Bernstein bounds and the √κ factor from variance-oblivious methods.
- **Core assumption:** The true reward function h(x) can be linearized as h(x) = g(x; θ_0)^T(θ* - θ_0) near initialization (NTK approximation), with bounded gradient norms ∥g(x; θ_0)∥_2 ≤ C_1√(mL).
- **Evidence anchors:** [Abstract] "We introduce a novel Bernstein-type inequality... designed to bypass a direct dependence on the ambient dimension." [Section 3, Theorem 3.1] Formal statement with ∥s_t∥_{H_t^(-1)} ≤ 8√(log det(H_t/det(λI)) · log(4t²/δ))
- **Break condition:** If the neural network width m is insufficient (Condition C.2 violated), the linearization error f(x;θ) - g(x;θ_0)^T(θ-θ_0) becomes large, breaking the concentration inequality.

### Mechanism 2
- **Claim:** Adaptive regularization λ_t (updated per round) enables fully data-adaptive concentration between estimated parameters θ_t and true parameters θ* without requiring projection steps.
- **Mechanism:** The regularization parameter follows λ_t = 64/S² · log det(Σ_i (1/4mλ_0)g(x_i;θ_0)g(x_i;θ_0)^T + I) · log(4t²/δ) + 16C_1²L/(S²λ_0)log²(4t²/δ), growing with observed data complexity. This yields Lemma 6.4: √m∥θ* - θ_t∥_{H_t(θ*)} ≤ ν_t^(1), avoiding the projection step required in prior logistic bandit methods.
- **Core assumption:** The sequence {λ_t}_t≥1 is monotonically non-decreasing (which the update rule guarantees by construction).
- **Evidence anchors:** [Section 4, Equation (4)] Exact update formula for λ_t [Section 6, Lemma 6.4] Concentration result using H_t(θ*) [Corpus] Weak explicit coverage of adaptive regularization in neural bandits — this appears novel to the paper
- **Break condition:** If λ_t fails to grow appropriately with data complexity, the confidence ellipsoid becomes too tight, causing optimistic action selection to over-exploit.

### Mechanism 3
- **Claim:** NeuralLog-UCB-2 achieves tighter regret Õ(ẽd√(T/κ*)) by replacing true variance ẋμ(h(x)) with neural network-estimated variance ẋμ(f(x; θ_t)) in the design matrix.
- **Mechanism:** The algorithm maintains W_t = Σ_i (ẋμ(f(x_i;θ_i))/m)g(x_i;θ_0)g(x_i;θ_0)^T + λ_tI and a confidence set W_t = {θ : √m∥θ - θ_t∥_{W_t} ≤ ν_t^(2)}. The optimistic action selection becomes closed-form: x_t ← arg max_{x,θ∈W_t₋₁} ⟨g(x;θ_0), θ - θ_0⟩. The estimated variance eliminates worst-case κ dependence by tracking actual variance evolution.
- **Core assumption:** The estimated variance satisfies 3H̄_t(θ*) ⪰ W̄_t ⪰ (1/3)H̄_t(θ*) (Proposition F.2), requiring the neural network predictions to stay sufficiently close to the true reward function.
- **Evidence anchors:** [Section 7, Algorithm 2] Full algorithm with W_t construction [Section 7, Theorem 7.2] Regret bound Õ(S²ẽd√(T/κ*) + ...) [Corpus] "Enjoying Non-linearity in Multinomial Logistic Bandits" — related variance-aware analysis in multinomial setting
- **Break condition:** If ∥θ_t - θ_0∥_2 exceeds S (norm parameter), the estimated variance ẋμ(f(x;θ_t)) may deviate significantly from true variance, breaking the design matrix equivalence.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK) theory**
  - Why needed here: All theoretical guarantees rely on Lemma 6.1/C.3, which linearizes the neural network near initialization. Without understanding NTK, you cannot interpret why effective dimension replaces ambient dimension.
  - Quick check question: Can you explain why h(x) ≈ g(x;θ_0)^T(θ* - θ_0) holds for wide neural networks, and what conditions on width m are required?

- **Concept: Self-normalized martingale concentration**
  - Why needed here: The core theoretical innovation is a Bernstein-type inequality for martingales with non-uniform variance. Understanding how traditional Hoeffding/Bernstein bounds work is prerequisite.
  - Quick check question: What is the difference between Hoeffding's inequality (uniform variance bound) and Bernstein's inequality (variance-aware), and why does incorporating ẋμ(x^T_iθ*) into the design matrix improve the bound?

- **Concept: Logistic bandit fundamentals (κ parameter, variance-aware analysis)**
  - Why needed here: The paper builds directly on Faury et al. (2020) and Abeille et al. (2021). The κ parameter (where 1/κ is minimum variance) scales exponentially with decision set size, and variance-adaptive analysis is the key to reducing κ dependence.
  - Quick check question: Why does the sigmoid function's derivative ẋμ(z) = μ(z)(1-μ(z)) create variance heterogeneity, and how does the parameter κ bound 1/κ ≤ ẋμ(·)?

## Architecture Onboarding

- **Component map:**
  Main Loop (T rounds):
  Feature extraction: g(x; θ_0) / √m (fixed at initialization)
  UCB computation:
    Algorithm 1: UCB = μ(f(x; θ_t₋₁)) + R√κν_t⁻¹∥g(x;θ_0)/√m∥_{V_t⁻¹₋₁}
    Algorithm 2: UCB = g(x;θ_0)^T(θ_t₋₁-θ_0) + ν_t^(2)∥g(x;θ_0)/√m∥_{W_t⁻¹₋₁}
  Design matrix updates:
    Algorithm 1: V_t = Σᵢ (1/m)g(xᵢ;θ_0)g(xᵢ;θ_0)ᵀ + κλ_tI
    Algorithm 2: W_t = Σᵢ (ẋμ(f(xᵢ;θᵢ))/m)g(xᵢ;θ_0)g(xᵢ;θ_0)ᵀ + λ_tI
  Regularization update: λ_t (Equation 4)
  Neural network training: TrainNN subroutine (gradient descent on regularized log-likelihood)

- **Critical path:**
  1. Initialization: Set θ_0 via standard initialization (W_l = [W_0; W_0] for l<L, W_l = [w, -w] for l=L). Compute λ_0 = 8√(2C_1L^(1/2)S^(-1))log(4/δ).
  2. Per-round UCB: For Algorithm 2 (recommended), compute ι_t₋₁ (involves log det computation), then ν_t₋₁^(2), then UCB for all K actions.
  3. Action selection: Optimistic selection — choose action maximizing UCB.
  4. Parameter update: Train neural network with J=2log(λ_tS/(√Tλ_t+C_4T^(3/2)L))·TL/λ_t gradient steps.

- **Design tradeoffs:**
  - Algorithm 1 vs 2: Algorithm 1 uses worst-case variance κ (simpler, worse regret Õ(ẽd√(κT)). Algorithm 2 uses estimated variance (tighter regret Õ(ẽd√(T/κ*)), but requires maintaining W_t with variance estimates.
  - Diagonal approximation (practical): Paper uses diagonal matrices for V_t, W_t in experiments (O(p) storage vs O(p²)), but theory requires full matrices.
  - Gradient choice: Theory uses g(x; θ_0), experiments use g(x; θ_t) (current gradient) for practical performance.
  - Width m: Must satisfy Condition C.2 (m ≥ C_0 max{T⁴K⁴L⁶log(...)/λ_H⁴, L^(-3/2)λ_0^(1/2)[...]}) — larger m improves approximation but increases computation.

- **Failure signatures:**
  - Regret not improving: Check if m is too small (linearization error grows), or if λ_0 is incorrectly set (confidence bounds too tight/loose).
  - Algorithm 2 worse than Algorithm 1: Estimated variance ẋμ(f(x;θ_t)) may be inaccurate if θ_t strays far from θ_0. Check if ∥θ_t - θ_0∥_2 stays bounded.
  - Computational bottleneck: Log det computation for full matrices is O(p³). If using p > 1000, switch to diagonal approximation or incremental updates.

- **First 3 experiments:**
  1. Synthetic validation with known h(x): Implement with h_1(x) = 0.2(x^Tθ)^4, d=20, K=5, T=2000. Compare Algorithm 2 against baseline (NCBF-UCB, Logistic-UCB-1). Expected: Algorithm 2 should achieve ~50% lower cumulative regret than NCBF-UCB after 2000 rounds (Figure 1).
  2. Effective dimension sweep: Vary the number of distinct context-arm vectors (10, 50, 10000) to control ẽd. Verify that regret scales with ẽd (not d) by plotting regret vs ẽd at T=2000.
  3. Real-world classification (MNIST): Convert to K-armed bandit (K=10 classes). Preprocess: resize to 14×14, flatten to d=196, normalize to [-1,1]. Expected: Neural methods (Algorithms 1,2) should outperform linear baselines by ~20-30% reduction in regret (Figure 2a).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on the norm parameter S be eliminated from the leading regret term without incurring extra computational costs such as projection steps?
- Basis in paper: [explicit] Section 9 identifies eliminating the dependence on S without additional computations (like the projection step mentioned in Remark 7.3) as a "promising future research direction."
- Why unresolved: Recent frameworks (Sawarni et al., 2024; Lee et al., 2024a) remove S but require projection steps with O(d²) complexity or additional constraints.
- What evidence would resolve it: An algorithm that achieves Õ(ẽd√(T/κ*)) regret efficiently without requiring a projection step.

### Open Question 2
- Question: Do the theoretical regret guarantees hold when using low-rank or diagonal approximations for the design matrices V_t and W_t?
- Basis in paper: [inferred] Appendix A notes that diagonal approximations were used in experiments to reduce the computational burden, whereas the theoretical analysis assumes full matrices.
- Why unresolved: There is a discrepancy between the empirical implementation (diagonal matrices) and the theoretical derivation (full matrices).
- What evidence would resolve it: A theoretical proof extending the concentration inequalities and regret bounds to diagonal or sketched design matrix variants.

### Open Question 3
- Question: Can the lower-order regret terms, specifically the S⁴κẽd² term in Algorithm 2, be reduced or removed?
- Basis in paper: [inferred] Theorem 7.2 achieves a leading term of Õ(S²ẽd√(T/κ*)) but retains lower-order terms that scale with κ and S⁴.
- Why unresolved: These terms arise from bounding the rounds where the neural network approximation is poor (the set |T_1|), and it is unclear if this bound is tight.
- What evidence would resolve it: A refined analysis demonstrating that the set of rounds with poor approximation is smaller, thereby tightening the lower-order terms.

## Limitations
- Theoretical framework critically depends on Neural Tangent Kernel approximation, requiring sufficiently wide networks
- Uses diagonal approximations in experiments that deviate from theoretical assumptions requiring full matrices
- Empirical performance on real-world datasets may not fully validate theoretical guarantees

## Confidence
- **High confidence:** Core mechanism of variance-adaptive concentration (Algorithm 2's design matrix using estimated variance) and the novel Bernstein-type inequality structure
- **Medium confidence:** Effective dimension scaling claims and the specific regret bounds, as these depend on the NTK approximation quality in finite-width networks
- **Low confidence:** Empirical performance on real-world datasets, given the use of diagonal approximations and practical modifications not fully justified by theory

## Next Checks
1. **Mechanism verification:** Test Algorithm 2 with varying effective dimensions (10 vs 10000) to empirically validate that regret scales with effective dimension rather than ambient dimension
2. **Approximation limits:** Measure the linearization error ∥f(x;θ) - g(x;θ_0)^T(θ-θ_0)∥₂ across training to confirm NTK approximation quality throughout execution
3. **Robustness check:** Compare Algorithm 2 performance when using true variance ẋμ(h(x)) vs estimated variance ẋμ(f(x;θ_t)) in the design matrix to quantify the impact of variance estimation