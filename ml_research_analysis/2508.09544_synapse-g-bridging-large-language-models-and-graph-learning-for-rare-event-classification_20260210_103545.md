---
ver: rpa2
title: 'SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event
  Classification'
arxiv_id: '2508.09544'
source_url: https://arxiv.org/abs/2508.09544
tags:
- data
- positive
- seeds
- synthetic
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SYNAPSE-G, a pipeline for rare event classification
  using LLM-generated synthetic data combined with graph-based semi-supervised learning.
  The method addresses the cold-start problem by generating synthetic seeds, propagating
  labels via similarity graphs, and iteratively expanding labeled data.
---

# SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification

## Quick Facts
- arXiv ID: 2508.09544
- Source URL: https://arxiv.org/abs/2508.09544
- Reference count: 32
- One-line primary result: SYNAPSE-G effectively classifies rare events using LLM-generated synthetic data and graph-based semi-supervised learning, achieving high recall with minimal labeled data

## Executive Summary
SYNAPSE-G addresses the cold-start problem in rare event classification by generating synthetic data via LLM, constructing a similarity graph, and using semi-supervised label propagation to expand the labeled set iteratively. The method combines LLM-generated seeds with graph-based learning to discover rare events without initial labeled data. Theoretical analysis shows precision and recall depend on the validity and diversity of synthetic seeds. Experiments on SST2 and MHS datasets demonstrate SYNAPSE-G's effectiveness, achieving high recall with minimal labeled data (e.g., 28.6% of true positives identified with only 2.4% of MHS data labeled).

## Method Summary
SYNAPSE-G uses LLM-generated synthetic data as initial seeds for rare event classification, addressing the cold-start problem. The method embeds synthetic seeds and unlabeled data, constructs a similarity graph, and applies semi-supervised label propagation (IBG or LP) to identify candidate positives. An oracle (human or LLM) labels top candidates, expanding the seed set iteratively. The expanded dataset trains/fine-tunes a classifier. The approach is validated on SST2 and MHS datasets, demonstrating effectiveness in discovering rare events with minimal labeled data.

## Key Results
- SYNAPSE-G identifies 28.6% of true positives in MHS dataset with only 2.4% of data labeled
- Label Propagation (LP) outperforms Iterative Bipartite Graph (IBG), achieving higher recall
- Synthetic seed quality (validity and diversity) directly impacts precision and recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic data can serve as a viable initial seed set for rare event classification when no labeled data exists.
- Mechanism: An LLM generates examples representative of the rare event class (e.g., hate speech) using carefully crafted prompts. These synthetic examples are embedded and act as positive seeds, enabling label propagation without initial ground truth labels.
- Core assumption: The LLM-generated synthetic data exhibits sufficient validity (proportion of true positives) and diversity (coverage of the positive class distribution) to bootstrap learning.
- Evidence anchors:
  - [abstract]: "This synthetic data serve as seeds for semi-supervised label propagation on a similarity graph constructed between the seeds and a large unlabeled dataset."
  - [section 4.1]: "To address the cold start problem (absence of initial labeled data), we use an LLM to generate an initial seed set of labeled data, DS, to bootstrap the learning process."
  - [corpus]: Related work (e.g., "Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events") supports synthetic data's role in rare event scenarios, but specific validity/diversity guarantees for this pipeline are not proven externally.
- Break condition: If synthetic seeds have low validity (p → 0) or poor diversity, precision and recall degrade significantly (see Proposition 1).

### Mechanism 2
- Claim: Semi-supervised label propagation on a similarity graph can effectively expand the labeled set by connecting synthetic seeds to similar unlabeled instances.
- Mechanism: A similarity graph is constructed over synthetic seeds and unlabeled data using cosine similarity of embeddings (e.g., BERT/Gecko). Labels are propagated via graph connectivity (IBG) or global label propagation (LP), identifying candidate positive examples.
- Core assumption: The embedding space and graph structure accurately capture semantic similarity, such that similar points should have similar labels.
- Evidence anchors:
  - [abstract]: "This seed set is used in semi-supervised label propagation, expanding the labeled set by connecting seeds to similar unlabeled instances on a similarity graph."
  - [section 4.2]: "We propose two semi-supervised approaches which we denote as Iterative Bipartite Graph (IBG) and Label Propagation (LP)."
  - [corpus]: Standard label propagation assumptions (e.g., smoothness in label distribution) are well-established, but effectiveness for rare events depends on graph construction quality.
- Break condition: If the graph fails to connect positive instances to seeds (e.g., due to poor embeddings or low similarity threshold), recall is capped. If it connects too many negatives, precision drops.

### Mechanism 3
- Claim: Iterative querying of an oracle (human or LLM) to label propagated candidates can refine the dataset and improve classifier performance over rounds.
- Mechanism: After propagation, top candidates are labeled by an oracle. Newly labeled positives are added to the seed set, and the process repeats. This expands the labeled dataset iteratively, training/fine-tuning a classifier.
- Core assumption: The oracle provides accurate labels, and the propagation strategy efficiently surfaces true positives.
- Evidence anchors:
  - [abstract]: "This identifies candidate positive examples, subsequently labeled by an oracle (human or LLM). The expanded dataset then trains/fine-tunes a classifier."
  - [section 4.2]: "Connected VU nodes are queried for labels. New positives are added to VP, and the process repeats."
  - [corpus]: Active learning principles are well-studied, but efficiency for rare events in this specific pipeline is empirically validated only on SST2/MHS.
- Break condition: If oracle labels are noisy or propagation efficiency decays (e.g., precision drops in later iterations), the feedback loop may amplify errors.

## Foundational Learning
- Concept: **Label Propagation on Graphs**
  - Why needed here: This is the core semi-supervised technique used to spread labels from synthetic seeds to unlabeled data. It assumes label smoothness over the graph.
  - Quick check question: Can you explain why label propagation might fail if the graph connects dissimilar points?
- Concept: **Synthetic Data Quality (Validity vs. Diversity)**
  - Why needed here: Theoretical analysis shows precision and recall depend on the validity (p) and diversity (h(S+)) of synthetic seeds. Balancing these is critical.
  - Quick check question: If synthetic seeds have high validity but low diversity, what might happen to recall?
- Concept: **Cold-Start Problem in Rare Event Classification**
  - Why needed here: SYNAPSE-G specifically targets the absence of initial labeled data for rare events, using synthetic data as a bootstrap.
  - Quick check question: Why can't traditional supervised learning handle this scenario without adaptation?

## Architecture Onboarding
- Component map:
  Synthetic Data Generator (LLM with prompts) -> Embedding Model (BERT/Gecko) -> Similarity Graph Constructor -> Label Propagation Engine (IBG/LP) -> Oracle (Human/LLM Rater) -> Classifier Training/Fine-tuning
- Critical path:
  1. Generate synthetic seeds (LLM)
  2. Embed seeds and unlabeled data (BERT/Gecko)
  3. Build similarity graph (cosine similarity > threshold, prune to dmax)
  4. Propagate labels (IBG or LP), select top candidates
  5. Query oracle for labels, expand seed set
  6. Iterate until budget/exhaustion, then train classifier
- Design tradeoffs:
  - **IBG vs. LP**: IBG is simpler but plateaus quickly; LP leverages global graph structure and achieves higher recall but requires full graph construction
  - **Similarity threshold (τ)**: Higher thresholds improve precision but limit reachability (lower recall)
  - **dmax**: Increasing dmax improves recall with diminishing returns, but very high values may hurt precision
  - **Seed selection**: ACS improves diversity over random sampling, but adds computational cost
- Failure signatures:
  - **Low precision**: Graph connects too many negatives (low similarity threshold, high dmax, or invalid synthetic seeds)
  - **Low recall**: Graph fails to connect positives (high similarity threshold, low dmax, or low-diversity seeds)
  - **Stagnation in IBG**: Quick plateau due to local connectivity
- First 3 experiments:
  1. **Single-shot validation on SST2**: Compare precision-recall curves for random vs. ACS seeds with varying τ and dmax
  2. **Iterative comparison on SST2**: Compare IBG vs. LP in an "ideal" setting (precision=1) to measure recall vs. query ratio
  3. **Real-world rare event on MHS**: Evaluate SYNAPSE-G vs. LR-Baseline on the transgender hate speech subset, measuring recall at low query ratios (e.g., 2.4%)

## Open Questions the Paper Calls Out
- Question: How do specific prompt engineering strategies for synthetic data generation influence the validity and diversity of seeds, and consequently, the precision-recall balance in SYNAPSE-G?
- Question: Can the validity threshold that dictates whether diversity improves or degrades precision be dynamically estimated to optimize adaptive sampling without ground truth?
- Question: How does the theoretical guarantee of precision and recall degrade when real-world graph structures violate the d-regular and independence assumptions?

## Limitations
- Synthetic seed quality dependence: Precision and recall are directly bounded by the validity and diversity of LLM-generated seeds
- Oracle assumption: The iterative process assumes perfect oracle labels, but noisy labels could degrade performance
- Graph construction scalability: Practical performance and memory efficiency for large datasets are not detailed

## Confidence
- High confidence: Theoretical framework linking precision/recall to seed validity and diversity is mathematically sound
- Medium confidence: Empirical results demonstrate effectiveness, but datasets may not represent all rare event scenarios
- Low confidence: Performance with noisy oracles or extremely imbalanced datasets is not tested

## Next Checks
1. **Seed Quality Validation**: Conduct controlled experiment varying LLM prompt strategies to quantify how seed validity and diversity affect final precision and recall
2. **Scalability Benchmark**: Test graph construction and propagation on datasets 10x larger than SST2/MHS to measure runtime and memory usage
3. **Robustness to Label Noise**: Simulate noisy oracle labels (e.g., 90% accuracy) and measure degradation in precision/recall over iterations