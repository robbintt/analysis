---
ver: rpa2
title: Trading off Consistency and Dimensionality of Convex Surrogates for the Mode
arxiv_id: '2402.10818'
source_url: https://arxiv.org/abs/2402.10818
tags:
- loss
- convex
- surrogate
- embedding
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how to trade off surrogate loss dimensionality,
  consistency, and number of problem instances for multiclass classification. The
  authors formalize "partial consistency" when embedding outcomes into a low-dimensional
  surrogate space, which is a departure from previous work requiring dimension $n-1$
  for consistency.
---

# Trading off Consistency and Dimensionality of Convex Surrogates for the Mode

## Quick Facts
- arXiv ID: 2402.10818
- Source URL: https://arxiv.org/abs/2402.10818
- Reference count: 40
- Primary result: Convex surrogate dimensionality can be reduced from n-1 to d with partial consistency under low-noise assumptions, at the cost of allowing hallucinations when d < n-1

## Executive Summary
This paper investigates the tradeoff between surrogate loss dimensionality, consistency, and the number of problem instances in multiclass classification. The authors show that while traditional surrogates require n-1 dimensions for full consistency, embedding outcomes into lower-dimensional convex polytopes (e.g., unit cube, permutahedron) enables dimensionality reduction at the cost of "hallucinations" - cases where the optimal surrogate report maps to an outcome with zero probability. They introduce a method to check calibration under low-noise assumptions and demonstrate that n=2^d outcomes can be embedded into d-dimensional unit cubes and n=d! outcomes into d-dimensional permutahedra while maintaining consistency under appropriate conditions.

## Method Summary
The method involves embedding n outcomes as vertices of a d-dimensional convex polytope (where d < n-1) and using the induced square loss with a MAP link function. Under the low-noise assumption (one class has probability at least 1-α), calibration holds if scaled polytopes anchored at each vertex are pairwise disjoint. For the full mode elicitation problem, the approach uses m parallel models with different cross-polytope embeddings, aggregating pairwise comparisons to determine the mode. The key insight is that this allows learning the mode over n outcomes using only n/2 dimensions across all models.

## Key Results
- Hallucinations always occur when embedding n outcomes into fewer than n-1 dimensions
- Unit cube embedding achieves consistency for n=2^d outcomes in d dimensions under α < 0.5
- Permutahedron embedding achieves consistency for n=d! outcomes in d dimensions under α < 1/d
- Multi-instance approach learns mode over n outcomes using n/2 dimensions total

## Why This Works (Mechanism)

### Mechanism 1: Polytope Embedding Reduces Dimensionality via Convex Hull Geometry
- Claim: Embedding n outcomes onto vertices of a d-dimensional polytope (d < n-1) allows surrogate optimization in lower dimensions while preserving correct classification under restricted conditions.
- Mechanism: The induced square loss L²_φ(u,y) = (1/c)||u - v_y||²_2 + f(y) has unique expected minimizer u* = φ(p) = Σ_y p_y v_y, which lies inside the polytope. The MAP link ψ_φ(u) projects u onto the polytope then selects the nearest vertex.
- Core assumption: The loss is strictly convex; the embedding φ is affine; vertices uniquely map to outcomes.
- Evidence anchors:
  - [abstract] "We examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space."
  - [section 3.1] Proposition 2: "the unique report which minimizes the expected loss is u* = φ(p) such that u* ∈ P."
  - [corpus] Related work on Fenchel-Young losses (Blondel et al.) supports polytope-based surrogates, though corpus does not directly address this paper's dimensionality tradeoff.
- Break condition: If the polytope has fewer than n vertices, or if the embedding is not bijective onto vert(P), the minimizer may not correspond to a valid distribution.

### Mechanism 2: Hallucination as Inevitable Failure Mode Under Dimensional Constraint
- Claim: When d < n-1, there always exist distributions where the optimal surrogate report maps to an outcome with zero probability ("hallucination").
- Mechanism: By Helly's Theorem, the intersection of convex hulls of all vertex subsets (excluding one vertex each) is non-empty. Points in this intersection can be expressed as convex combinations that exclude each vertex, yet the link must map to some vertex—potentially one with zero weight.
- Core assumption: Finite outcomes; d < n-1; link function maps every point to some outcome.
- Evidence anchors:
  - [abstract] "We show... with less than n-1 dimensions, there exist distributions for which a phenomenon called hallucination occurs."
  - [section 3.2, Theorem 3] "H = ∪_y∈Y conv(vert(P)\{v_y}) ∩ ψ_φ^y and furthermore H ≠ ∅."
  - [corpus] No direct corpus evidence on hallucination; this appears novel to this work.
- Break condition: If d ≥ n-1, the standard simplex embedding avoids hallucination (by prior lower bounds).

### Mechanism 3: Low-Noise Assumption Restores Calibration via Disjoint Scaled Polytopes
- Claim: Under a low-noise assumption (one class has probability ≥ 1-α), calibration holds if scaled polytopes P_y^α anchored at each vertex are pairwise disjoint.
- Mechanism: Define Θ_α = {p : max_y p_y ≥ 1-α}. For sufficiently small α, the scaled polytope P_y^α = φ(conv(Ψ_y^α)) lies entirely within a strict calibration region near v_y. The modified link ψ_{P,α}(u) = argmin_y ||u - P_y^α|| recovers the mode correctly.
- Core assumption: α ∈ [0, 0.5) for unit cube; α ∈ [0, 1/d) for permutahedron; polytope structure permits disjoint scaled regions.
- Evidence anchors:
  - [abstract] "We derive a result to check if consistency holds under a given polytope embedding and low-noise assumption."
  - [section 4.1, Theorem 5] "There exists an α ∈ [0, .5) such that... (L²_φ, ψ_{P,α}) is ℓ_{0-1}-calibrated over Θ_α."
  - [corpus] Agarwal & Agarwal (2015) use low-noise for log(n)-dimensional surrogates but lack this polytope characterization.
- Break condition: If α exceeds the threshold, scaled polytopes overlap and calibration fails.

## Foundational Learning

- Concept: **Convex Surrogate Losses**
  - Why needed here: The entire framework optimizes a convex L: ℝ^d × Y → ℝ as a tractable proxy for discrete 0-1 loss.
  - Quick check question: Can you explain why convexity guarantees a unique global minimizer for expected loss?

- Concept: **Calibration vs. Consistency**
  - Why needed here: The paper distinguishes full calibration (over all distributions) from partial calibration (over subsets Θ_α).
  - Quick check question: Given a surrogate (L, ψ), what does it mean for it to be ℓ-calibrated over P ⊆ Δ_Y?

- Concept: **Polytope Geometry (Vertices, Convex Hull, Faces)**
  - Why needed here: Embeddings map outcomes to vertices; hallucination regions arise from convex hull intersections.
  - Quick check question: For a d-dimensional cube, how many vertices exist, and what is conv(vert(P)\{v_y})?

## Architecture Onboarding

- Component map:
  - Embedding module → Loss module → Link module → (Multi-instance) Aggregation module

- Critical path:
  1. Choose polytope (cube for n=2^d, permutahedron for n=d!, cross-polytope for multi-instance)
  2. Set α based on prior over label noise (α < 0.5 for cube, α < 1/d for permutahedron)
  3. Train model to minimize L²_φ; apply ψ_{P,α} at inference
  4. (Multi-instance) Train m models with different cross-polytope embeddings; aggregate via Algorithm 1 or 2

- Design tradeoffs:
  - Lower d → faster optimization but larger hallucination region and stricter α requirements
  - Permutahedron allows factorial compression (d! outcomes in d dims) but calibration region shrinks exponentially
  - Multi-instance approach uses O(n²) parallel models but each is d-dimensional; total compute may exceed single n-1-dim model

- Failure signatures:
  - **Hallucination**: Model confidently predicts class y when true p_y = 0; check if φ(p) ∈ H
  - **Overlapping scaled polytopes**: Calibration breaks; verify P_y^α ∩ P_ŷ^α = ∅ for all y ≠ ŷ
  - **Conflicting comparisons** (multi-instance): Binary relation M not transitive; fallback to Algorithm 2

- First 3 experiments:
  1. **Unit cube embedding on synthetic n=16, d=4**: Vary α ∈ [0, 0.5]; measure calibration accuracy on Θ_α distributions. Verify hallucination occurs when p uniform.
  2. **Permutahedron embedding on ranking task (d=5, n=120)**: Compare against one-hot baseline; measure degradation as α approaches 1/d.
  3. **Multi-instance cross-polytope on n=8, d=3**: Run m = 2^{d-1} = 4 parallel models; aggregate via Algorithm 1; compare mode recovery rate vs. single 7-dim surrogate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical size of the hallucination region $H$ and how frequently do models report points within it in practice?
- Basis in paper: [explicit] The authors state, "Future work could investigate the size of the hallucination region in theory, and the frequency of reports in the hallucination region in practice."
- Why unresolved: While Theorem 3 proves the hallucination region $H$ is non-empty for dimensions $d < n-1$, the paper provides no bounds on its volume or empirical probability.
- Evidence would resolve it: Deriving theoretical bounds on the Lebesgue measure of $H$ relative to the polytope $P$, or empirical studies showing the frequency of hallucinated reports on standard datasets.

### Open Question 2
- Question: Can a computationally efficient method be constructed to identify strict calibration regions for a given polytope embedding?
- Basis in paper: [explicit] The authors list as a future direction, "construct[ing] a method that efficiently identifies the strict calibration regions... providing better guidance on whether or not a particular polytope embedding aligns with one’s prior."
- Why unresolved: The paper notes that characterizing these regions is currently a "collision detection problem, which is often computationally hard."
- Evidence would resolve it: An algorithm that computes strict calibration regions in polynomial time relative to the number of outcomes $n$ and dimension $d$.

### Open Question 3
- Question: Can the concepts of partial consistency and polytope embeddings be applied to cost-sensitive multiclass classification?
- Basis in paper: [explicit] The conclusion explicitly asks to "explore whether concepts from this paper could be applied to the underlying problem of cost-sensitive multiclass classification."
- Why unresolved: The current theoretical analysis and calibration proofs are restricted to the 0-1 loss (mode elicitation).
- Evidence would resolve it: A derivation of calibration conditions or strict calibration regions for a loss function with asymmetric misclassification costs.

### Open Question 4
- Question: How should the link function handle surrogate reports $u$ when the intersection of properties $\cap_{p \in \phi^{-1}(u)} \gamma(p)$ is non-empty?
- Basis in paper: [explicit] A footnote in Section 3.3 states, "We leave the more general case of linking $u$ when $\cap_{p \in \phi^{-1}(u)} \gamma(p) \neq \emptyset$ to future work."
- Why unresolved: The paper restricts its analysis to "strict calibrated regions" where the link maps to a unique mode, avoiding the complexity of ambiguous intersections.
- Evidence would resolve it: A generalized link function definition and proof of calibration for points where multiple distributions with different modes embed to the same surrogate report.

## Limitations

- The low-noise assumption significantly restricts the domain where calibration holds, particularly for permutahedron embeddings where the calibration region shrinks as d! grows
- The multi-instance approach requires m = 2^{d-1} parallel models, which may be computationally prohibitive for larger problems
- The paper does not provide empirical validation or specify implementation details for the embedding functions, loss constants, or training procedures

## Confidence

- **High Confidence**: Theorem 3 on hallucination inevitability (d < n-1), Proposition 2 on expected loss minimization (u* = φ(p)), and the dimensional reduction claims for specific polytopes (2^d → d, d! → d)
- **Medium Confidence**: Calibration results under low-noise assumptions (Theorems 5-6, Corollaries 7-8) given the dependence on polytope geometry and α thresholds
- **Low Confidence**: Multi-instance mode elicitation method due to lack of empirical results and the computational complexity of solving the minimum set cover problem for optimal m selection

## Next Checks

1. **Hallucination Region Characterization**: Implement the polytope embedding for n=8, d=3 and explicitly compute H = ∪_y conv(vert(P)\{v_y}) ∩ ψ_y⁻¹(y). Verify that points in H produce incorrect mode predictions when p_y = 0 for the corresponding y.

2. **Low-Noise Calibration Threshold**: For the unit cube embedding (n=16, d=4), systematically vary α ∈ [0, 0.5] and measure calibration accuracy on distributions from Θ_α. Confirm that performance degrades as α approaches 0.5, indicating the breakdown of disjoint scaled polytopes.

3. **Multi-Instance Consistency**: Implement the cross-polytope embedding for n=8, d=3 with m=4 parallel models. Test Algorithm 1 on synthetic distributions with varying noise levels, and verify that conflicting comparisons trigger the fallback to Algorithm 2. Measure mode recovery accuracy against a single 7-dimensional surrogate baseline.