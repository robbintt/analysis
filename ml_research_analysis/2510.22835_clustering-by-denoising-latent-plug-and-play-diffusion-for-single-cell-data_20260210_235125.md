---
ver: rpa2
title: 'Clustering by Denoising: Latent plug-and-play diffusion for single-cell data'
arxiv_id: '2510.22835'
source_url: https://arxiv.org/abs/2510.22835
tags:
- data
- prior
- diffusion
- training
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a plug-and-play diffusion framework for denoising
  single-cell RNA-seq data to improve clustering accuracy. The key innovation is a
  latent diffusion model trained on reference data, which is then applied to noisy
  target data through a Gibbs sampling procedure that alternates between likelihood
  alignment in the original high-dimensional space and prior alignment in the learned
  low-dimensional latent space.
---

# Clustering by Denoising: Latent plug-and-play diffusion for single-cell data

## Quick Facts
- arXiv ID: 2510.22835
- Source URL: https://arxiv.org/abs/2510.22835
- Reference count: 40
- Key outcome: Plug-and-play diffusion framework improves single-cell clustering by denoising via Gibbs sampling between high-dimensional likelihood and low-dimensional latent prior

## Executive Summary
This paper introduces a novel denoising framework for single-cell RNA-seq data that improves clustering accuracy by separating noise from biological signal through a plug-and-play diffusion approach. The method trains a diffusion model on clean reference data in a low-dimensional latent space, then applies this prior to noisy target data through a Gibbs sampling procedure that alternates between likelihood alignment in the original high-dimensional space and prior alignment in the learned latent space. This "input-space steering" addresses the latent-space collapsing issue common in PCA-based methods, where distinct cell types can be projected too close together.

## Method Summary
The method learns a low-rank factor model from reference data (X = VU + ε), then trains a diffusion model on the latent embeddings. For denoising target data, it uses split Gibbs sampling: a likelihood step projects from latent back to high-dimensional space enforcing consistency with observations, while a prior step applies the learned diffusion model in latent space. The alignment parameter ρ controls the balance between data fidelity and prior guidance, enabling adaptive noise handling and uncertainty quantification. The framework is trained once on reference data and then applied to multiple target datasets without retraining.

## Key Results
- On synthetic data across varied noise levels and dataset shifts, DICE improved clustering accuracy over PCA baselines with higher silhouette scores (0.37 vs 0.25) and lower cLISI (1.17 vs 1.27)
- On real single-cell datasets (CITE-seq and human fetal brain), the method produced embeddings with better biological coherence, with cluster boundaries aligning more closely with known cell type markers and developmental trajectories
- The approach provides principled uncertainty quantification through multiple Gibbs sampling runs and offers tunable interpolation between prior and observed data

## Why This Works (Mechanism)

### Mechanism 1: Input-Space Steering via Split Gibbs Sampling
Separates observation space from denoising space through auxiliary variables Z, enabling tractable posterior sampling. Alternates between likelihood step (projecting from latent back to original space) and prior step (denoising in latent space), preventing latent-space collapse where distinct cell types are projected too close together.

### Mechanism 2: Diffusion Prior Captures Complex Cell State Manifolds
Diffusion models trained on latent embeddings learn richer, multi-modal distributions than Gaussian or VAE-based priors, enabling recovery of distinct cell type clusters without restrictive parametric assumptions.

### Mechanism 3: ρ-Tunable Noise Adaptation
The alignment parameter ρ provides principled control over the bias-variance tradeoff between data fidelity and prior guidance, operating as soft relaxation of MAP estimation.

## Foundational Learning

- **Diffusion models (DDPM) and score matching**: Why needed - the core denoising engine is a diffusion model; Quick check - Given a noised sample x_t = √ᾱ_t x_0 + √(1-ᾱ_t) ε, what does the trained network ε_θ predict, and how does this relate to the score function?
- **Plug-and-play (PnP) methods and Bayesian inverse problems**: Why needed - DICE frames denoising as posterior sampling with an implicit prior; Quick check - Why does introducing auxiliary variable Z make posterior sampling tractable when the prior is only defined implicitly via a diffusion model?
- **Low-rank factor models for gene expression**: Why needed - the generative model X = VU + ε assumes expression lies near a low-dimensional subspace; Quick check - If V is learned from reference data, what assumptions must hold for it to provide useful structure when applied to target data from a different tissue or technology?

## Architecture Onboarding

- **Component map**: PCA projection module -> Diffusion training module -> Gibbs inference module -> Evaluation module
- **Critical path**: Reference preprocessing → PCA → Diffusion training → (separately) Target preprocessing → Initialize U^(0) → DICE iterations → Cluster denoised embeddings
- **Design tradeoffs**: Latent dimension k (higher captures more variation but increases cost), Gibbs iterations T (more improves convergence but increases inference time), Annealing schedule {ρ_t} (linear decrease works well)
- **Failure signatures**: Over-smoothing (all cells cluster into few large groups → ρ too large), poor cluster separation (UMAP shows overlapping populations → ρ too small), novel cell types absorbed (known reference clusters present but new cell types missing → prior too strong)
- **First 3 experiments**: 1) Synthetic validation with 2-cluster Gaussian mixture to verify implementation correctness, 2) ρ sensitivity sweep to observe spread increase with ρ, 3) Real data sanity check on CITE-seq subset comparing to PCA, MAGIC, and kNN-smoothing

## Open Questions the Paper Calls Out

- **Can the DICE framework be extended to model non-linear latent structures and non-i.i.d. noise?**: The paper explicitly states future work includes extending beyond linear low-rank structures and i.i.d. noise assumptions, which may not capture complex biological relationships in all datasets.
- **How can the computational efficiency of the Gibbs sampling procedure be improved for large-scale deployment?**: The Discussion section lists improving sampling efficiency as a key direction, as the current iterative procedure is computationally intensive compared to simpler baselines.
- **How can the interpolation parameter ρ be optimally selected without ground truth labels?**: While the paper analyzes ρ sensitivity, it relies on validation metrics for selection and does not provide an automated, unsupervised mechanism for optimal ρ selection.

## Limitations
- Robustness uncertainty when reference and target datasets differ substantially in tissue origin, experimental protocols, or biological states
- ρ-tuning procedure requires empirical calibration that may not generalize across datasets without substantial trial-and-error
- Computational cost of running multiple Gibbs sampling chains may be prohibitive for very large single-cell datasets

## Confidence
- High confidence: Core mechanism of input-space steering via split Gibbs sampling is theoretically sound and well-supported by synthetic experiments
- Medium confidence: Biological interpretation of denoised embeddings in real datasets is convincing but could benefit from additional validation on independent datasets
- Medium confidence: Scalability claims are reasonable given diffusion model architecture but not extensively tested on very large datasets

## Next Checks
1. Cross-platform validation: Apply the method to transfer learning scenarios where reference and target come from different single-cell technologies to test robustness of the shared V matrix assumption
2. Novel cell type detection: Systematically evaluate the method's ability to preserve completely novel cell populations when ρ is varied, comparing against ground truth for synthetic data with disjoint cell type sets
3. Computational scaling benchmark: Profile memory usage and runtime for datasets with 100K-1M cells to quantify practical limits of the Gibbs sampling inference procedure and identify potential optimizations