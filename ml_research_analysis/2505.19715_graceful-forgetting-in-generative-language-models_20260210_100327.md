---
ver: rpa2
title: Graceful Forgetting in Generative Language Models
arxiv_id: '2505.19715'
source_url: https://arxiv.org/abs/2505.19715
tags:
- forgetting
- learning
- unlearning
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses negative transfer in generative language models
  by proposing a method to selectively unlearn harmful knowledge during fine-tuning.
  The approach, called Learning With Forgetting (LWF), uses self-generated texts to
  represent the knowledge to be forgotten, evaluates forgetting confidence using Fisher
  Information Matrix-weighted parameter updates, and periodically unlearns high-confidence
  data via gradient ascent.
---

# Graceful Forgetting in Generative Language Models

## Quick Facts
- arXiv ID: 2505.19715
- Source URL: https://arxiv.org/abs/2505.19715
- Reference count: 12
- One-line primary result: LWF improves fine-tuning performance in domain-specific QA tasks by selectively unlearning harmful knowledge

## Executive Summary
This paper addresses negative transfer in generative language models during fine-tuning by introducing a method called Learning With Forgetting (LWF). The approach enables models to selectively unlearn knowledge from source tasks that may harm performance on target domain-specific QA tasks. LWF uses self-generated texts to represent the knowledge to be forgotten, evaluates forgetting confidence using Fisher Information Matrix-weighted parameter updates, and periodically unlearns high-confidence data via gradient ascent. Experiments demonstrate consistent improvements across five domain-specific QA tasks, particularly in mixed forgetting scenarios.

## Method Summary
LWF operates by first generating synthetic texts that represent knowledge to be forgotten, using top-1 predictions from the model. During fine-tuning, it computes forgetting confidence scores based on Fisher Information Matrix-weighted parameter gradients, identifying which source knowledge is most harmful. The method then applies gradient ascent updates to selectively unlearn high-confidence forgetting data while continuing to learn target task information. This periodic forgetting process is interleaved with standard fine-tuning, allowing the model to maintain useful source knowledge while removing detrimental information.

## Key Results
- LWF improves fine-tuning performance on five domain-specific QA tasks compared to standard fine-tuning
- Consistent gains observed in mixed forgetting setup where multiple source tasks compete
- Outperforms structural regulation methods (BSS and SRS) when adapted to generative models

## Why This Works (Mechanism)
The mechanism leverages the Fisher Information Matrix to identify parameters most responsible for harmful knowledge transfer. By weighting parameter updates with this information, LWF can precisely target and unlearn specific knowledge representations without affecting beneficial information. The periodic forgetting schedule prevents catastrophic forgetting while maintaining the ability to adapt to new domains.

## Foundational Learning
- Fisher Information Matrix: measures parameter sensitivity to data distribution changes; needed to identify which parameters store harmful knowledge; quick check: verify parameter importance scores correlate with forgetting confidence
- Gradient ascent for unlearning: reverses parameter updates to remove information; needed to selectively erase harmful knowledge; quick check: monitor loss increase on forgetting data
- Self-generated forgetting texts: synthetic data representing source knowledge; needed to create concrete targets for unlearning; quick check: ensure generated texts cover source task distribution
- Periodic forgetting schedule: intermittent unlearning during training; needed to balance forgetting with continued learning; quick check: verify forgetting frequency doesn't cause catastrophic forgetting

## Architecture Onboarding

**Component map:** Input data -> Self-generation module -> Forgetting confidence evaluator -> Periodic unlearner -> Fine-tuning loop -> Output model

**Critical path:** Self-generated forgetting texts → Fisher-weighted confidence scores → Gradient ascent updates → Parameter modification

**Design tradeoffs:** Balance between forgetting harmful knowledge and preserving useful information; computational overhead of Fisher Information Matrix calculations versus forgetting accuracy

**Failure signatures:** Excessive forgetting causing performance degradation on source tasks; insufficient forgetting leaving harmful knowledge intact; computational instability during gradient ascent updates

**3 first experiments:**
1. Verify self-generated forgetting texts accurately represent source task knowledge distribution
2. Test Fisher Information Matrix approximation accuracy for confidence scoring
3. Validate gradient ascent updates effectively reduce harmful knowledge without catastrophic forgetting

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation limited to five domain-specific QA tasks with similar characteristics, limiting generalizability to other generative modeling scenarios
- Self-generated forgetting texts based on top-1 predictions may not fully capture knowledge distribution to be forgotten
- Computational overhead and stability concerns from Fisher Information Matrix approximation and gradient ascent-based forgetting not extensively discussed

## Confidence
- High: LWF demonstrates improved performance over standard fine-tuning in tested domain-specific QA tasks
- High: LWF shows consistent gains in mixed forgetting setup
- Medium: LWF outperforms structural regulation methods (BSS and SRS) when adapted to generative models
- Low: Method's effectiveness and computational efficiency in large-scale generative tasks beyond QA

## Next Checks
1. Evaluate LWF on diverse generative tasks such as story generation, dialogue, and code generation to assess cross-domain robustness
2. Perform ablation studies isolating forgetting confidence weighting, periodic forgetting schedule, and self-generated text quality to quantify individual component contributions
3. Conduct runtime and memory overhead analysis during fine-tuning to determine scalability for larger models and datasets