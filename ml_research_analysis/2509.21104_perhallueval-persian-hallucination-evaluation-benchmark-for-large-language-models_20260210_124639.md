---
ver: rpa2
title: 'PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language
  Models'
arxiv_id: '2509.21104'
source_url: https://arxiv.org/abs/2509.21104
tags:
- persian
- recall
- hallucinated
- hallucination
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerHalluEval, the first dynamic hallucination
  evaluation benchmark tailored for the Persian language, addressing the lack of Persian-specific
  hallucination detection resources. The authors develop a three-stage LLM-driven
  pipeline with human validation to generate and filter plausible hallucinated answers
  and summaries for QA and summarization tasks, distinguishing between intrinsic (contradiction
  to source) and extrinsic (unsupported content) hallucinations.
---

# PerHalluEval: Persian Hallucination Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2509.21104
- Source URL: https://arxiv.org/abs/2509.21104
- Reference count: 0
- First dynamic Persian hallucination evaluation benchmark with 8,000 samples

## Executive Summary
PerHalluEval addresses the critical gap in Persian language model evaluation by introducing the first dynamic hallucination benchmark tailored for Persian. The authors develop a three-stage LLM-driven pipeline with human validation to generate and filter plausible hallucinated answers and summaries for QA and summarization tasks. Using log probabilities and human annotation, they curate challenging hallucination examples and evaluate 12 diverse LLMs, revealing that models generally struggle to detect hallucinated Persian text. The benchmark establishes a foundation for improving Persian language model reliability and cultural alignment.

## Method Summary
The benchmark employs a three-stage pipeline: (1) Generation using three LLMs (GPT-4o, Gemini 2.0 Flash, Llama-3.3-70B) with structured prompts defining five hallucination patterns; (2) Filtering using GPT-4o verifier with log-probability scoring (Score = P('Y') - P('N')) to select the most plausible hallucinations; (3) Human validation by three annotators with majority voting. The process creates 8,000 samples (4,000 QA, 4,000 summarization) from PQuAD and PN-Summary corpora, with external document context provided for summarization tasks.

## Key Results
- Models show significantly better performance on summarization (where source documents are provided) than QA tasks
- Persian-tuned models (Dorna, PersianMind) exhibit "Yes-Man" bias, affirming hallucinated content as factual
- Providing external document context partially mitigates hallucination in summarization
- Log-probability-based selection successfully identifies challenging hallucinations that models find believable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log-probability-based selection identifies hallucinations that are semantically plausible enough to challenge LLMs
- Mechanism: GPT-4o verifier outputs binary judgments with log probabilities; Score = P('Y') - P('N') quantifies plausibility; highest-scoring candidate selected
- Core assumption: Log probabilities correlate with semantic plausibility in Persian
- Evidence anchors: [abstract] "we used the log probabilities of generated tokens to select the most believable hallucinated instances"; [section 3.3] log probabilities provide more reliable semantic plausibility assessment
- Break condition: If log probabilities don't reflect plausibility in Persian due to tokenization artifacts or low-resource language effects

### Mechanism 2
- Claim: Providing external source documents improves hallucination detection in summarization tasks
- Mechanism: Models compare summary claims against provided original articles for grounding
- Core assumption: Context is sufficient and models can effectively compare summary against source
- Evidence anchors: [abstract] "providing external knowledge... could mitigate hallucination partially"; [section 5.2] summarization scores higher due to external article provision
- Break condition: If context exceeds model window or models fail to attend to relevant passages

### Mechanism 3
- Claim: Structured instruction design with explicit hallucination patterns generates diverse, challenging examples
- Mechanism: Prompts include four components—task overview, five hallucination pattern definitions, few-shot examples, and output constraints
- Core assumption: Pattern taxonomy covers hallucination space adequately for Persian
- Evidence anchors: [section 3.2] instruction structure description; [section 3.4] well-distributed source-model shares (29-40%)
- Break condition: If Persian cultural context introduces patterns not captured in taxonomy

## Foundational Learning

- **Intrinsic vs. Extrinsic Hallucinations**
  - Why needed here: Benchmark explicitly distinguishes these; evaluation metrics differ
  - Quick check question: If summary states "meeting was in Tehran" when source says "Tehran," is this intrinsic or extrinsic? (Intrinsic—directly contradicts source.)

- **Log Probabilities as Confidence Signals**
  - Why needed here: Filtering stage relies on interpreting log probabilities as plausibility measures
  - Quick check question: If P('Y') = 0.7 and P('N') = 0.3, what is the Score, and does this indicate high or low plausibility? (Score = 0.4; relatively high plausibility.)

- **Low-Resource Language Challenges**
  - Why needed here: Persian morphology, pro-drop syntax, Ezafe construction, RTL script affect performance
  - Quick check question: Why might model hallucinate more on "Persian-specific" content than generic content? (Sparse internal knowledge representation; less training data.)

## Architecture Onboarding

- Component map: Data Selection -> Multi-Generator Pipeline -> Filtering/Scoring -> Human Validation -> Evaluation Framework
- Critical path: Sample selection → Hallucination generation (parallel across 3 models) → Log-prob scoring → Candidate selection → Human validation → Model evaluation
- Design tradeoffs: Dynamic benchmark vs. reproducibility; GPT-4o as verifier only to avoid bias; Persian-specific annotations enable cultural analysis but reduce generic coverage
- Failure signatures: Low inter-annotator agreement (Gwet's AC1 < 0.7); generator dominance in selected candidates; near-random Hamming scores (~0.5)
- First 3 experiments: 1) Baseline establishment across all 12 models with documented metrics; 2) Ablation on context for summarization; 3) Persian-specific vs. generic split analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "Yes-Man" bias (high Factual Recall, low Hallucination Recall) in Persian-tuned LLMs be effectively mitigated?
- Basis in paper: [explicit] Section 5.3 identifies Persian-tuned models exhibit strong over-affirmation bias
- Why unresolved: Authors observe phenomenon but don't propose specific fine-tuning objectives to correct behavior
- What evidence would resolve it: Comparative study of Persian LLMs fine-tuned with rejection-based datasets evaluated on PerHalluEval

### Open Question 2
- Question: To what extent does RAG improve hallucination detection in open-domain Persian QA?
- Basis in paper: [inferred] Section 5.2 shows better summarization performance with external context
- Why unresolved: Study compares tasks rather than isolating external knowledge variable within single task
- What evidence would resolve it: Ablation study applying external retrieval to PerHalluEval QA task

### Open Question 3
- Question: Why is hallucination detection performance in summarization more sensitive to Persian fine-tuning than QA tasks?
- Basis in paper: [explicit] Section 5.4 documents performance drop after Persian fine-tuning specifically in summarization
- Why unresolved: Paper documents disparity but doesn't analyze fine-tuning datasets or mechanisms causing task-specific degradation
- What evidence would resolve it: Analysis of fine-tuning data composition regarding summarization verification and layer-wise representation analysis

## Limitations
- Benchmark relies on GPT-4o for both hallucination scoring and human validation, introducing potential centralization risk
- 38% Persian-specific subset creates test distribution that may not represent general Persian LLM usage patterns
- Log-probability approach assumes consistent tokenization behavior across models, but Persian's RTL script may introduce variability

## Confidence
- **High Confidence**: Three-stage pipeline methodology is well-specified and reproducible; clear demonstration that Persian LLMs struggle with hallucination detection
- **Medium Confidence**: Log probability correlation with plausibility lacks direct Persian-specific validation; cultural alignment findings based on single subset
- **Low Confidence**: Assertion about Persian-tuned models not outperforming others based on limited model comparison

## Next Checks
1. Conduct controlled study correlating human annotator plausibility ratings with log-probability scores specifically for Persian text
2. Test hallucination examples through multiple verifier models to assess consistency and identify centralization bias
3. Replicate cultural analysis by creating additional Persian-specific subsets across different cultural domains to validate generalizability