---
ver: rpa2
title: Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative
  Matrix Factorization
arxiv_id: '2506.10920'
source_url: https://arxiv.org/abs/2506.10920
tags:
- features
- concept
- snmf
- concepts
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identifying interpretable
  features in large language models by decomposing MLP activations. The core method,
  semi-nonnegative matrix factorization (SNMF), factorizes MLP activations into sparse
  linear combinations of neurons and maps these to their activating inputs, providing
  intrinsic interpretability.
---

# Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization

## Quick Facts
- **arXiv ID:** 2506.10920
- **Source URL:** https://arxiv.org/abs/2506.10920
- **Reference count:** 40
- **Primary result:** SNMF outperforms SAEs in causal steering while maintaining comparable concept detection

## Executive Summary
This paper introduces semi-nonnegative matrix factorization (SNMF) as a method to decompose MLP activations into interpretable features. Unlike sparse autoencoders that require separate autointerp pipelines, SNMF provides intrinsic interpretability by factorizing activations into sparse neuron combinations and mapping them directly to activating inputs. Experiments on Llama 3.1, Gemma 2, and GPT-2 demonstrate that SNMF-derived features achieve higher causal steering effectiveness than SAEs while maintaining comparable performance in concept detection tasks.

## Method Summary
The method factorizes MLP activation matrix A into Z (features as sparse neuron combinations) and Y (nonnegative coefficients mapping inputs to features) such that A ≈ ZY. Z is initialized with uniform random values and Y with normal random values, then updated iteratively using multiplicative updates from Ding et al. (2010). A winner-take-all sparsity constraint keeps only the top p% entries per feature column. The method tests feature counts k∈{100,200,300,400} with p=1% for Llama/Gemma and p=5% for GPT-2. Features are evaluated through concept detection scores and causal steering tasks where features are amplified during inference.

## Key Results
- SNMF features outperform SAEs in causal steering tasks across multiple datasets and models
- Concept detection scores are comparable to SAEs, with SNMF achieving slightly higher scores in some cases
- Hierarchical structure analysis reveals shared core neurons encoding general concepts (e.g., "weekday") with exclusive neurons differentiating specific instances (e.g., "Monday" vs "Tuesday")
- Early MLP layers show higher concept detection scores while late layers perform better for output steering

## Why This Works (Mechanism)

### Mechanism 1: Parts-Based Decomposition via Nonnegativity Constraints
SNMF constrains coefficient matrix Y to nonnegative values, encouraging additive parts-based representations that align with MLP composition. This forces the decomposition to express activations as additive combinations rather than allowing cancellations, mirroring how MLP outputs are linear combinations of neuron vectors.

### Mechanism 2: Sparsity via Winner-Take-All Creates Composable Features
Enforcing ℓ0 sparsity through hard winner-take-all (keeping only top p% entries per feature column) produces features that isolate specific neuron groups. This enables compositional reuse, where features for semantically-related concepts share core neuron sets while being differentiated by exclusive neurons.

### Mechanism 3: Intrinsic Interpretability via Token Attribution
The coefficient matrix Y provides direct token-to-feature attribution without requiring post-hoc interpretation pipelines. Top-activating tokens for each feature are directly available via Y, contrasting with SAEs that need separate autointerp training.

## Foundational Learning

- **Concept: Nonnegative Matrix Factorization (NMF)**
  - Why needed: SNMF extends NMF by allowing Z to have mixed signs while constraining Y to nonnegative values. Understanding NMF's parts-based representation property is prerequisite.
  - Quick check: Given matrix A ≈ ZY where Y ≥ 0, what does it mean that the representation is "parts-based"?

- **Concept: MLP Layer as Key-Value Memory**
  - Why needed: The paper builds on viewing MLP output as Σ aᵢvᵢ where vᵢ are value vectors. SNMF features are linear combinations of these neurons.
  - Quick check: In Eq. (2), what do aᵢ and vᵢ represent, and how does SNMF's Z modify this view?

- **Concept: Sparsity-Interpretability Tradeoff**
  - Why needed: The WTA sparsity (p=1%) is critical for feature quality. Understanding why sparse representations tend toward interpretability contextualizes this design.
  - Quick check: Why does enforcing sparsity in Z encourage features to encode specific rather than polysemantic concepts?

## Architecture Onboarding

- **Component map:** Input tokens → Model forward pass → Extract MLP activations A → Initialize Z, Y → Iteratively update → Output Z (features), Y (token attributions)

- **Critical path:**
  1. Activation collection — Sample diverse text corpus (200 sentences across 20 concepts for k=100)
  2. Hyperparameter selection — k (feature count) and p (sparsity percentage); paper uses k∈[100,400], p=1% for Llama/Gemma
  3. Convergence monitoring — Reconstruction loss ||A - ZY||_F should stabilize
  4. Feature labeling — Extract top tokens from Y, feed to LLM for concept description

- **Design tradeoffs:**
  | Choice | Option A | Option B | Guidance |
  |--------|----------|-----------|----------|
  | k (features) | Small (100-200) | Large (1000+) | Small k yields coarse features; scalability unknown |
  | p (sparsity) | Aggressive (1%) | Moderate (5%) | Aggressive sparsity improves interpretability but risks reconstruction loss |
  | Layer selection | Early layers | Late layers | Early layers show higher concept detection; late layers better for steering |

- **Failure signatures:**
  - Dead features: All-zero columns in Z after convergence → increase initialization variance
  - Feature collapse: Multiple features have near-identical Z vectors → k too large for concept diversity
  - Poor steering: Amplifying feature doesn't change output → feature not causally relevant
  - Negative concept detection scores: Feature activates more on neutral than concept inputs → capturing noise

- **First 3 experiments:**
  1. **Baseline reproduction:** Run SNMF on GPT-2 Small layer 0 activations with k=100, p=5%. Verify concept detection scores match paper trends (>75% positive).
  2. **Ablation on sparsity:** Compare p∈{1%, 5%, 10%} on held-out concepts. Measure tradeoff between reconstruction error and steering effectiveness.
  3. **Hierarchical validation:** Apply recursive SNMF (k=[100, 50, 25]) on time-unit dataset. Verify "weekday" emerges as parent of individual days via shared neurons.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does SNMF scale effectively to thousands of MLP features (large k), and what new levels of feature granularity emerge at this scale compared to the tested k < 500?
- **Open Question 2:** Can advanced initialization strategies (e.g., K-means, SVD) or projected gradient descent improve the convergence and performance of SNMF over the currently used multiplicative updates?
- **Open Question 3:** Is the SNMF decomposition method effective on non-MLP activations, such as attention heads or the residual stream, or is it dependent on the specific additive structure of the MLP layer?

## Limitations
- The sparsity parameter p=1% was not systematically optimized across architectures
- Sample size (200 sentences across 20 concepts) may not capture full feature diversity in larger models
- Layer-specific performance differences suggest architecture-dependent behavior not fully characterized

## Confidence
- **High Confidence:** SNMF successfully decomposes MLP activations into sparse neuron combinations
- **Medium Confidence:** SNMF-derived features outperform SAEs in causal steering tasks
- **Medium Confidence:** The hierarchical structure with shared core neurons and exclusive refinements
- **Low Confidence:** SNMF provides "intrinsic interpretability" compared to SAEs

## Next Checks
1. **Sparsity Sweep:** Systematically evaluate p∈{0.5%, 1%, 2%, 5%} on held-out concepts to characterize the sparsity-performance tradeoff
2. **Cross-Architecture Generalization:** Test SNMF on vision transformer MLPs and smaller language models to assess architectural dependence
3. **Concept Coverage Analysis:** Measure the proportion of MLPs' activation space explained by SNMF features versus SAE features to quantify "intrinsic interpretability" claims