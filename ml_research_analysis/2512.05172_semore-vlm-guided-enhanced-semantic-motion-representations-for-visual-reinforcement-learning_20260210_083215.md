---
ver: rpa2
title: 'Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement
  Learning'
arxiv_id: '2512.05172'
source_url: https://arxiv.org/abs/2512.05172
tags:
- learning
- motion
- visual
- representations
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Semore introduces a novel VLM-guided framework for visual reinforcement
  learning that addresses the challenge of limited representation learning capability
  in visual RL. The method employs a dual-stream network to separately extract semantic
  and motion representations from RGB flows, while utilizing VLM with common-sense
  knowledge to retrieve key information from observations.
---

# Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2512.05172
- **Source URL**: https://arxiv.org/abs/2512.05172
- **Reference count**: 10
- **Primary result**: Achieves state-of-the-art performance in CARLA benchmarks, outperforming other methods with an episode reward of 201±54 in JayWalk scenarios, a driving distance of 233±66 meters, and a crash intensity of 2043±98.

## Executive Summary
Semore introduces a novel VLM-guided framework for visual reinforcement learning that addresses the challenge of limited representation learning capability in visual RL. The method employs a dual-stream network to separately extract semantic and motion representations from RGB flows, while utilizing VLM with common-sense knowledge to retrieve key information from observations. A separately supervised approach is adopted to simultaneously guide the extraction of semantics and motion, allowing them to interact spontaneously. The method achieves state-of-the-art performance in CARLA benchmarks, outperforming other methods with an episode reward of 201±54 in JayWalk scenarios, a driving distance of 233±66 meters, and a crash intensity of 2043±98.

## Method Summary
Semore employs a dual-stream backbone to extract semantic and motion representations from RGB image sequences. The semantic encoder (4-layer CNN) processes static features, while the motion encoder (similar CNN) processes residuals of adjacent frames. A VLM (Qwen2-VL-7B-Instruct) generates text semantics from observations, and CRIS (CLIP-driven segmentation) produces knowledge-aware feature masks H_ka. Semantic supervision is achieved via L1 similarity loss between F_s and H_ka, while motion enhancement uses bidirectional cross-attention between motion features F_m and H_ka. Fused features are fed to an SAC agent with DeepMDP-style reward prediction head. A selective replay buffer uses LLM filtering with decay factor δ. Total loss combines transition, semantic, reward, policy, and Q-function losses.

## Key Results
- Achieved state-of-the-art performance in CARLA benchmarks with episode reward of 201±54 in JayWalk scenarios
- Demonstrated driving distance of 233±66 meters and crash intensity of 2043±98
- Outperformed competing methods across three CARLA scenarios (JayWalk, HighBeam, HighWay)

## Why This Works (Mechanism)
The method's effectiveness stems from its dual-stream architecture that separately extracts semantic and motion representations, guided by VLM-generated text semantics and CRIS segmentation masks. The semantic encoder captures static scene information while the motion encoder processes temporal differences between frames. The bidirectional cross-attention mechanism allows semantic features to enhance motion representation by providing context about important objects. The separately supervised approach ensures both representations are learned effectively while their interaction remains flexible. The selective replay buffer further improves training efficiency by focusing on high-value experiences.

## Foundational Learning
- **Dual-stream architecture**: Separates semantic and motion representation learning to handle different types of visual information effectively
  - Why needed: Different visual features require distinct processing paths for optimal representation
  - Quick check: Verify separate feature extraction paths for static vs. dynamic elements
- **VLM-guided semantic extraction**: Uses visual-language models to generate text descriptions that guide semantic feature learning
  - Why needed: Provides common-sense knowledge and contextual understanding for semantic representation
  - Quick check: Ensure VLM generates consistent and relevant object descriptions
- **Cross-attention mechanism**: Enables semantic features to enhance motion representation through bidirectional information flow
  - Why needed: Motion features benefit from semantic context about important objects and their relationships
  - Quick check: Verify attention weights highlight relevant object-motion relationships
- **Selective replay buffer**: Filters experiences using LLM to focus training on valuable transitions
  - Why needed: Improves sample efficiency by prioritizing informative experiences
  - Quick check: Monitor buffer composition and decay factor progression

## Architecture Onboarding
- **Component map**: RGB frames -> Dual-stream encoders (semantic + motion) -> VLM/CRIS feature extraction -> Cross-attention fusion -> SAC agent with reward prediction
- **Critical path**: Observation → Dual-stream processing → VLM/CRIS integration → Cross-attention fusion → SAC decision
- **Design tradeoffs**: Separate semantic/motion streams provide specialized processing but increase model complexity; VLM guidance improves semantic quality but adds dependency on external models
- **Failure signatures**: Poor semantic masks indicate VLM/CRIS misalignment; slow convergence suggests replay buffer filtering is too aggressive
- **First experiments**: 1) Visualize semantic masks to verify VLM/CRIS alignment, 2) Test dual-stream encoding with simplified attention, 3) Evaluate selective buffer impact with varying δ decay rates

## Open Questions the Paper Calls Out
None

## Limitations
- Method's effectiveness strongly tied to VLM and CRIS quality, with limited specification of prompt templates and segmentation details
- Selective replay buffer reliability depends on LLM judgment, introducing potential variance in training efficiency
- Dual-stream architecture hyperparameters (channel sizes, learning rates) not fully detailed, limiting exact replication
- Reliance on external VLM services raises scalability and real-time applicability concerns
- Evaluation limited to CARLA scenarios, with unknown robustness in complex unstructured environments

## Confidence
- **High Confidence**: Core architecture (dual-stream encoder, VLM-guided semantic extraction, motion feature enhancement) is clearly defined and logically consistent
- **Medium Confidence**: Overall training pipeline and selective replay buffer design are well described, but lack precise hyperparameter details needed for exact replication
- **Low Confidence**: Exact performance impact of VLM prompt templates and CRIS segmentation configurations, as these are only partially specified

## Next Checks
1. **Reproduce VLM/CRIS Integration**: Implement the VLM (Qwen2-VL-7B-Instruct) and CRIS pipeline to generate knowledge-aware segmentation masks, ensuring alignment with the paper's feature mask examples
2. **Hyperparameter Sensitivity**: Systematically vary CNN channel sizes, learning rates, and α/λ SAC parameters to determine their influence on episode reward and crash intensity
3. **Replay Buffer Dynamics**: Monitor buffer composition over training to verify selective sampling's effect on convergence speed and whether it introduces early-stage data scarcity