---
ver: rpa2
title: 'LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering
  for BioASQ Challenge'
arxiv_id: '2509.08596'
source_url: https://arxiv.org/abs/2509.08596
tags:
- bioasq
- question
- phase
- query
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot ensemble approach for biomedical
  question answering using large language models (LLMs) and retrieval-augmented generation
  (RAG). The authors developed a multi-stage information retrieval pipeline combining
  LLM-generated queries, BM25 search, and semantic reranking to retrieve relevant
  PubMed documents for answering BioASQ challenge questions.
---

# LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge

## Quick Facts
- arXiv ID: 2509.08596
- Source URL: https://arxiv.org/abs/2509.08596
- Reference count: 19
- Zero-shot ensemble approach achieved 0.92-1.00 accuracy on BioASQ Yes/No questions, ranking 1st place

## Executive Summary
This paper presents a zero-shot ensemble approach for biomedical question answering using large language models (LLMs) and retrieval-augmented generation (RAG). The authors developed a multi-stage information retrieval pipeline combining LLM-generated queries, BM25 search, and semantic reranking to retrieve relevant PubMed documents for answering BioASQ challenge questions. For QA, they used multiple LLMs with zero-shot prompting, followed by answer synthesis using Gemini 2.0 Flash to resolve contradictions and generate unified responses. Their system achieved state-of-the-art results on BioASQ 13 Phase A+ Yes/No questions (accuracy of 0.92, ranking 1st on batch 4) and strong performance on Phase B Yes/No questions (accuracy of 1.00 on batches 1-2, ranking 1st). The study found that longer contexts generally hurt answer quality, particularly for List and Factoid questions, highlighting the importance of focused retrieval in RAG systems.

## Method Summary
The method employs a multi-stage retrieval pipeline: first generating structured Elasticsearch queries using Gemini 2.0 Flash, then performing BM25 retrieval with up to 10,000 documents, followed by semantic reranking to select top 300 documents. For question answering, multiple LLMs (Gemini 2.0 Flash, Gemini 2.5 Flash, and Claude 3.7 Sonnet) are used with zero-shot prompting, and answers are synthesized using Gemini 2.0 Flash with confidence scoring. The system employs temperature adjustment (0.1 to 0.0) when confidence scores fall below 0.5. The approach is evaluated on BioASQ 13 challenge datasets across three question types: Yes/No, Factoid, and List questions.

## Key Results
- Achieved 0.92 accuracy on BioASQ 13 Phase A+ Yes/No questions (1st place on batch 4)
- Achieved 1.00 accuracy on BioASQ 13 Phase B Yes/No questions (1st place on batches 1-2)
- Found that longer contexts generally hurt answer quality for List and Factoid questions due to "information dilution"

## Why This Works (Mechanism)
The ensemble approach works by combining multiple LLM perspectives to improve answer accuracy and handle contradictions through synthesis. The multi-stage retrieval pipeline ensures focused context selection by first using keyword-based BM25 search followed by semantic reranking, which helps address the challenge of retrieving relevant biomedical documents. The zero-shot prompting with confidence-based answer synthesis allows the system to dynamically adjust its response generation based on the perceived reliability of the answers.

## Foundational Learning
- **Retrieval-augmented generation (RAG)**: Combines information retrieval with language model generation to provide context-aware answers; needed to access relevant biomedical literature without fine-tuning
- **Semantic reranking**: Uses semantic similarity to reorder retrieved documents; needed to improve relevance of top results beyond keyword matching
- **Zero-shot prompting**: Generates responses without task-specific fine-tuning; needed to adapt LLMs to biomedical QA without training data
- **Confidence scoring**: Evaluates answer reliability to trigger re-generation; needed to handle uncertainty in ensemble outputs
- **Temperature adjustment**: Controls randomness in generation; needed to produce more deterministic outputs when confidence is low

## Architecture Onboarding

**Component map:** PubMed corpus -> Elasticsearch index -> Gemini 2.0 query generation -> BM25 retrieval -> Gemini 2.5 refinement (fallback) -> Semantic reranking -> Top 300 docs -> LLMs (Flash, Flash Preview, Claude) -> Answer synthesis (Flash) -> Confidence scoring -> Final answer

**Critical path:** Query generation -> Retrieval (BM25 + reranking) -> QA (ensemble) -> Synthesis -> Output

**Design tradeoffs:** Zero-shot vs. fine-tuning (speed vs. accuracy), ensemble size vs. latency, context length vs. answer quality, retrieval depth vs. computational cost

**Failure signatures:** Low Yes/No accuracy indicates retrieval noise; poor Factoid/List performance with longer contexts suggests information dilution; malformed structured outputs indicate LLM generation issues

**First experiments:** 1) Test query generation with PubMed abstracts; 2) Validate semantic reranking improves top-10 relevance; 3) Compare single LLM vs. ensemble performance on BioASQ 11 dev set

## Open Questions the Paper Calls Out

**Open Question 1:** Can decoupling answer generation from formatting into separate pipeline stages significantly improve performance on List and Factoid question types in biomedical RAG systems?

**Open Question 2:** What is the optimal context length for different biomedical QA question types, and can it be dynamically determined based on query characteristics?

**Open Question 3:** Can uncertainty detection methods effectively trigger dynamic retrieval in biomedical RAG systems to reduce unnecessary retrievals while maintaining accuracy?

**Open Question 4:** Why does Claude 3.7 Sonnet outperform models with larger context windows on long-context extraction tasks?

## Limitations
- Zero-shot approach may not match fine-tuned models on complex biomedical questions
- Limited ablation studies prevent isolating individual component contributions
- Focus on batch-level performance without detailed error analysis
- No comparison with fine-tuned baselines beyond leaderboard rankings

## Confidence
- High confidence: Yes/No question performance claims (0.92-1.00 accuracy, 1st place rankings)
- Medium confidence: Context length impact findings (observed trend but limited to three LLMs)
- Low confidence: Ensemble methodology superiority claims (no direct comparison with single-model approaches)

## Next Checks
1. Conduct controlled ablation studies varying one component at a time to quantify individual contributions to performance gains

2. Test the information dilution hypothesis by implementing and comparing alternative context selection strategies for Factoid and List questions

3. Validate generalizability by replicating the evaluation pipeline on a different biomedical QA dataset (e.g., PubMedQA or MedQA-USMLE)