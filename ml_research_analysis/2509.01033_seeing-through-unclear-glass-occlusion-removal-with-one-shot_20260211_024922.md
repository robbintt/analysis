---
ver: rpa2
title: 'Seeing through Unclear Glass: Occlusion Removal with One Shot'
arxiv_id: '2509.01033'
source_url: https://arxiv.org/abs/2509.01033
tags:
- occlusion
- image
- auxiliary
- removal
- occlusions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of removing occlusions from
  images taken through unclear glass, such as dirt, raindrops, muddy water, and particles.
  Unlike existing methods that rely on synthetic data or single-occlusion types, the
  authors propose an all-in-one framework using a one-shot test-time adaptation mechanism.
---

# Seeing through Unclear Glass: Occlusion Removal with One Shot

## Quick Facts
- arXiv ID: 2509.01033
- Source URL: https://arxiv.org/abs/2509.01033
- Authors: Qiang Li; Yuanming Cao
- Reference count: 40
- One-line primary result: Outperforms state-of-the-art on OROS dataset with PSNR up to 29.91 dB and SSIM of 0.879

## Executive Summary
This paper tackles the challenging problem of removing various types of occlusions from images taken through unclear glass, such as dirt, raindrops, muddy water, and particles. Unlike existing methods that handle single occlusion types or rely on synthetic data, the authors propose an all-in-one framework using a one-shot test-time adaptation mechanism. The method effectively decomposes occlusions into partial (blurred) and complete (underexposed) components and employs a dual-branch neural network with shared parameters. By leveraging a self-supervised auxiliary task during inference, the model adapts to the specific optical properties of each test image, achieving state-of-the-art performance on their newly introduced OROS dataset.

## Method Summary
The method decomposes occlusions into partial (blurred) and complete (underexposed) components, employing a dual-branch neural network with shared parameters. The primary branch reconstructs the clean image, while the auxiliary branch reconstructs the degraded input for self-supervised learning. During inference, test-time adaptation updates the shared model weights using the auxiliary reconstruction loss, allowing the model to adapt to specific occlusion properties of each test image. The approach is trained on a newly introduced OROS dataset containing 2,970 real-world paired images (clean and occluded) across four occlusion types: dirt, raindrops, muddy water, and particles.

## Key Results
- Achieves PSNR of 29.91 dB and SSIM of 0.879 on OROS dataset, outperforming state-of-the-art methods
- Demonstrates strong generalization to unseen occlusion types during inference
- Successfully balances luminance and detail restoration, particularly for challenging muddy water occlusions

## Why This Works (Mechanism)

### Mechanism 1: Test-Time Adaptation via Self-Supervision
The method updates shared model weights using a self-supervised auxiliary task during inference, allowing adaptation to specific optical properties of occlusions in a single test image. The auxiliary branch reconstructs the degraded input, and minimizing this reconstruction loss updates the shared encoder weights. The core assumption is that degradation patterns in a single image are sufficient to guide optimization without overfitting. Evidence shows this mechanism improves performance, but overfitting occurs if too many updates (>4-6) are performed.

### Mechanism 2: Dual-Task Decomposition (Deblurring & Inpainting)
Occlusion removal is framed as solving defocus deblurring (partial occlusion) and inpainting (complete occlusion) tasks concurrently. The unified degraded occlusion model decomposes partial occlusions into blurring and complete occlusions into underexposed regions. The network implicitly learns to route these tasks, deblurring margins and inpainting centers of occlusions. This physical model-based decomposition enables effective handling of varying degradation types.

### Mechanism 3: Attention Transfer via Auxiliary Reconstruction
The auxiliary reconstruction task generates an attention mask that guides the primary removal task to occluded regions. By reconstructing the degraded image, the auxiliary branch learns the spatial location and features of the occlusion, which are passed to the primary branch via an Occlusion Feature Mask. This mechanism helps the primary network distinguish between background texture and occlusion artifacts, improving removal accuracy.

## Foundational Learning

- **Concept: Test-Time Adaptation (TTA)**
  - Why needed: This architecture requires running gradient descent on input data during inference to adapt to specific occlusion types
  - Quick check: Do you understand how to implement a backward pass and optimizer step in PyTorch without updating the global training state, strictly for inference-time fine-tuning?

- **Concept: Multi-Scale SSIM (MS-SSIM) Loss**
  - Why needed: The paper uses MS-SSIM + L1 loss to weigh structural similarity at different resolutions, vital for restoring luminance and detail simultaneously
  - Quick check: Can you explain why MS-SSIM might be preferred over pure pixel-wise L2 loss for tasks involving structural restoration like deblurring?

- **Concept: Physics-Based Image Formation Models**
  - Why needed: The network design is motivated by image formation and defocus imaging equations; understanding attenuation (α) and intensification terms is key to debugging failures
  - Quick check: How does the attenuation ratio (α) differ between "partial" (blurred) and "complete" (underexposed) occlusions in this model?

## Architecture Onboarding

- **Component map:** Input -> Shared Encoder -> Auxiliary Branch (Occlusion Reconstruction) + Primary Branch (Clean Image) -> Output
- **Critical path:** 1) Load base model (trained offline), 2) For each test image: run forward pass through shared encoder + auxiliary branch, 3) Compute auxiliary loss, 4) Backpropagate auxiliary loss to update shared parameters (Test-Time Adaptation), 5) Run forward pass through updated shared encoder + primary branch to get final result
- **Design tradeoffs:** Inference speed vs. quality (0.17s without TTA vs. 1.30s with 6 gradient updates); generalization vs. overfitting (more updates improve performance but risk overfitting if n > 4-6)
- **Failure signatures:** Black textures/stripes indicate failure to handle complete occlusion inpainting; overexposed artifacts suggest unbalanced luminance restoration; performance drops with >4-6 updates due to overfitting
- **First 3 experiments:** 1) Baseline Validation: run base model (no TTA) on OROS to confirm 29.16 PSNR baseline, 2) Update Sweep: ablate number of gradient updates (n=1 to 10) on "Dirt" image to find optimal step count, 3) Mask Ablation: disable Occlusion Attention Mask to measure its contribution

## Open Questions the Paper Calls Out
- Can the framework handle dynamic scenes or moving backgrounds without relying on the static alignment used in OROS dataset collection?
- Is occlusion removal performance robust when the distance between glass and camera lens falls outside the 3-12 cm range used for training?
- Does the "inpainting" approach for complete occlusions recover semantically correct background content or merely hallucinate plausible textures?

## Limitations
- Test-time adaptation mechanism may not generalize to extreme degradation scenarios or high-frequency patterns
- Exact implementation details of the Occlusion Feature Mask are not fully specified, affecting reproducibility
- The method's performance on dynamic scenes with moving backgrounds remains untested

## Confidence
- **High Confidence:** Dual-task decomposition framework and overall architecture design
- **Medium Confidence:** Test-time adaptation mechanism effectiveness and generalization
- **Low Confidence:** Specific implementation details of Occlusion Feature Mask and multi-scale U-Net architecture

## Next Checks
1. Systematically vary the number of test-time gradient updates (n=1 to 10) on multiple occlusion types to identify the optimal trade-off between performance and overfitting risk
2. Implement and evaluate a version without the Occlusion Feature Mask to quantify its contribution to overall performance improvement
3. Test the trained model on a different occlusion dataset (e.g., synthetic reflections or real-world underwater images) to assess generalization beyond OROS dataset