---
ver: rpa2
title: Collaborative Compression for Large-Scale MoE Deployment on Edge
arxiv_id: '2509.25689'
source_url: https://arxiv.org/abs/2509.25689
tags:
- quantization
- arxiv
- pruning
- memory
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying extremely large
  MoE models like DeepSeek-V3 on edge devices with strict memory constraints (128GB).
  The core method combines expert pruning, mixed-precision quantization, and activation
  optimization in a collaborative compression framework.
---

# Collaborative Compression for Large-Scale MoE Deployment on Edge

## Quick Facts
- arXiv ID: 2509.25689
- Source URL: https://arxiv.org/abs/2509.25689
- Reference count: 0
- Reduced 671B parameter MoE to 103GB storage while maintaining accuracy

## Executive Summary
This paper presents a collaborative compression framework for deploying extremely large MoE models like DeepSeek-V3 on edge devices with strict memory constraints. The approach combines expert pruning, activation optimization, and mixed-precision quantization to reduce storage from 1.3TB to 103GB while maintaining model quality. The compressed model achieves over 5 tokens/s generation speed on an AMD Ryzen AI Max+ laptop with 128GB RAM, demonstrating practical deployment feasibility on consumer-grade hardware.

## Method Summary
The framework employs three synergistic compression techniques: expert pruning removes low-contribution experts based on activation frequency and routing scores, reducing parameters from 671B to 508B; activation optimization adjusts per-layer expert activation counts to match pruned experts and hardware constraints; and mixed-precision quantization uses tensor-level sensitivity analysis to retain higher precision for key weights while quantizing most to extremely low bits. These methods work collaboratively to maximize compression while preserving model accuracy across multiple benchmarks.

## Key Results
- Reduced DeepSeek-V3 from 671B parameters to 508B through expert pruning
- Compressed storage from 1.3TB to 103GB while maintaining higher accuracy than uniform low-bit methods
- Achieved over 5 tokens/s generation speed on AMD Ryzen AI Max+ laptop with 128GB RAM
- Successfully deployed compressed MoE model on edge hardware with strict memory constraints

## Why This Works (Mechanism)
The collaborative approach works by addressing the unique characteristics of MoE architectures - sparse expert utilization and heterogeneous parameter importance. Expert pruning exploits the fact that many experts contribute minimally to overall performance, while activation optimization ensures the remaining experts are efficiently utilized across layers. Mixed-precision quantization leverages the varying sensitivity of different weight tensors to precision loss, preserving critical weights while aggressively compressing less important ones. This multi-pronged strategy achieves higher compression ratios than any single technique alone.

## Foundational Learning

- **Expert Pruning**: Removing under-utilized experts based on activation frequency and routing scores - needed to reduce parameter count without major accuracy loss, quick check: measure activation frequency distribution across experts
- **Activation Optimization**: Adjusting per-layer expert activation counts - needed to balance computational load and memory constraints, quick check: verify activation counts match hardware capabilities
- **Mixed-Precision Quantization**: Variable bit-width quantization based on weight sensitivity - needed to preserve accuracy while maximizing compression, quick check: analyze quantization error distribution across layers
- **Routing Mechanism**: Dynamic expert selection during inference - needed to maintain model capacity with fewer active experts, quick check: validate routing scores remain stable after pruning
- **Sensitivity Analysis**: Identifying important weights for higher precision retention - needed to guide quantization strategy, quick check: compare weight sensitivity metrics across layers
- **Collaborative Compression**: Combining multiple techniques synergistically - needed to achieve higher compression ratios than individual methods, quick check: measure contribution of each component to overall performance

## Architecture Onboarding

**Component Map**: Expert Pruning -> Activation Optimization -> Mixed-Precision Quantization -> Model Deployment

**Critical Path**: The compression pipeline flows sequentially through pruning, activation adjustment, and quantization, with each stage dependent on the previous one's output.

**Design Tradeoffs**: Higher compression ratios achieved through aggressive pruning and quantization versus maintaining model accuracy and inference speed. The framework prioritizes storage reduction while accepting some computational overhead from complex routing mechanisms.

**Failure Signatures**: Catastrophic accuracy drop indicates excessive pruning or inappropriate quantization levels. Slow inference suggests activation optimization didn't properly account for hardware constraints. Memory overflow during deployment indicates insufficient compression.

**First Experiments**: 1) Test compression on smaller MoE models to validate methodology, 2) Profile activation patterns on target hardware to optimize activation counts, 3) Benchmark different quantization strategies to determine optimal precision distribution

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on heuristic-based pruning without theoretical guarantees on quality impact
- Sensitivity analysis assumes linear relationships between weight importance and performance
- Evaluation limited to single hardware platform (AMD Ryzen AI Max+ with 128GB RAM)
- No analysis of potential catastrophic forgetting or interference from expert pruning

## Confidence
- Compression methodology: Medium
- Reported compression ratios and accuracy retention: Medium
- 5 tokens/s generation speed claim: Low
- Evaluation across multiple benchmarks: Medium

## Next Checks
1) Perform ablation studies to isolate contribution of each compression component to overall performance
2) Test compressed model on additional edge hardware platforms with different memory constraints and processor architectures
3) Conduct long-form inference testing with varying sequence lengths to evaluate performance degradation patterns and potential catastrophic forgetting effects