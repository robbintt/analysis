---
ver: rpa2
title: 'These Are Not All the Features You Are Looking For: A Fundamental Bottleneck
  in Supervised Pretraining'
arxiv_id: '2506.18221'
source_url: https://arxiv.org/abs/2506.18221
tags:
- features
- learning
- training
- transfer
- mixture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental limitation in supervised pretraining
  called the "information saturation bottleneck." The authors show that deep learning
  models fail to learn new features once they encode similar competing features during
  training, permanently losing critical features needed for transfer learning. Even
  when pretraining on a mixture that includes the target task, models perform inconsistently
  on subtasks and cannot match direct training performance.
---

# These Are Not All the Features You Are Looking For: A Fundamental Bottleneck in Supervised Pretraining

## Quick Facts
- arXiv ID: 2506.18221
- Source URL: https://arxiv.org/abs/2506.18221
- Authors: Xingyu Alice Yang; Jianyu Zhang; Léon Bottou
- Reference count: 11
- These are not all the features you are looking for: a fundamental bottleneck in supervised pretraining.

## Executive Summary
This paper identifies a fundamental limitation in supervised pretraining called the "information saturation bottleneck." The authors show that deep learning models fail to learn new features once they encode similar competing features during training, permanently losing critical features needed for transfer learning. Even when pretraining on a mixture that includes the target task, models perform inconsistently on subtasks and cannot match direct training performance. The study provides empirical evidence from multiple published works and proposes richer feature representations as a solution. Their proposed "TIME-CAT" method, which concatenates multiple models trained for fewer steps, significantly improves transfer accuracy (+12.5% on INATURALIST 18, +6% on CIFAR-100) while maintaining the same pretraining compute. The findings suggest that simply scaling up models may not be as effective as focusing on task-specific training when possible.

## Method Summary
The authors propose TIME-CAT, a method that concatenates feature representations from multiple models trained for fewer steps each, as opposed to a single model trained for longer. They train multiple ResNet50 models independently on ImageNet for reduced steps (e.g., 4 models × 100K steps), then concatenate their feature maps before the classification layer. A new linear classifier is trained on top of these frozen concatenated features for transfer tasks. The baseline approach trains a single ResNet50 for 450K steps. The method aims to preserve more diverse features useful for transfer by leveraging the observation that early training discovers different feature subsets across random seeds before any single model saturates.

## Key Results
- Models pretrained on mixture distributions can permanently lose features needed for subtasks, even when those subtasks are components of the pretraining mixture
- TIME-CAT achieves +12.5% improvement on INATURALIST 18 and +6% on CIFAR-100 compared to baseline single-model pretraining
- Pretraining on mixtures does not guarantee learning features for all subtasks due to information saturation bottleneck
- Concatenating features from multiple models provides richer representations than single models trained longer at equivalent compute cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models pretrained on mixture distributions can permanently lose features needed for subtasks, even when those subtasks are components of the pretraining mixture.
- Mechanism: The information saturation bottleneck occurs when networks encode similar competing features early in training. Once these features saturate the gradient signal, the model cannot learn new features that provide redundant information for the training objective—even if those features are critical for transfer to specific subtasks.
- Core assumption: SGD with large step sizes exhibits implicit sparsity bias, causing networks to construct sparse solutions that retain only the first-discovered features sufficient for minimizing training error.
- Evidence anchors:
  - [abstract] "networks fail to learn new features once they encode similar competing features during training"
  - [section 4.1] "networks struggle to learn new features when they have encoded similar competing features during training"
  - [corpus] Weak/missing - no direct corpus support for this specific mechanism
- Break condition: If regularization or architecture changes eliminate sparsity bias, the bottleneck may not manifest.

### Mechanism 2
- Claim: Data ordering and mixture balance determine which subset of features a model learns, not the presence of features in the data.
- Mechanism: The counterexample (Section 3) shows that optimal classifiers for mixture distributions can be sparsely represented using single features, determined by mixture weights. Different random seeds or data shuffles produce different sparse solutions, each missing features needed for some subtasks.
- Core assumption: Neural networks collapse representations into problem-space features and prefer sparse solutions when multiple feature sets achieve similar training error.
- Evidence anchors:
  - [section 3] "Depending on which point is ignored, the optimal classifier is one of the four solutions... can be sparsely represented using just one feature"
  - [section 4.4] "these subsets of features work similarly on ImageNet... each set of features provides the same information to reduce training error"
  - [corpus] Weak/missing - corpus papers address different aspects of data composition effects
- Break condition: If training proceeds to zero test error (unlimited data regime), the bottleneck may not apply.

### Mechanism 3
- Claim: Concatenating multiple models trained for fewer steps yields richer representations than single models trained longer, at equivalent compute cost.
- Mechanism: TIME-CAT leverages the observation that early training discovers different feature subsets across random seeds. By combining these before saturation occurs in any single model, the concatenated representation preserves more diverse features useful for transfer.
- Core assumption: Models trained for fewer steps capture different feature subsets that would otherwise be pruned during extended training on the same objective.
- Evidence anchors:
  - [section 5.1.2] "combining more networks that have been pretrained for fewer steps... (+12.5%) on INATURALIST 18 and (+6%) on CIFAR-100"
  - [table 3] CAT5 (5 models × 80K steps) achieves 46.2% on INAT18 vs baseline 33.7%
  - [corpus] Weak/missing - no corpus papers address model concatenation strategies
- Break condition: If compute budget doesn't allow parallel training, or if single-model inference cost is prohibitive.

## Foundational Learning

- Concept: **Transfer Learning via Linear Probing**
  - Why needed here: The paper's framework evaluates transfer by freezing pretrained features and training only the final classifier. Understanding this distinction from full fine-tuning is essential for interpreting results.
  - Quick check question: Can you explain why linear probing is sufficient to assess whether pretrained features contain target-task features?

- Concept: **Mixture Distributions and Subpopulations**
  - Why needed here: The paper models real-world pretraining data as weighted mixtures of subdistributions. The theoretical counterexample relies on understanding how classifiers behave differently on mixtures vs. components.
  - Quick check question: If a feature is predictive for a subtask P[i], is it automatically useful for the mixture P[mix]? (Hint: see Proposition 2.1)

- Concept: **Neural Tangent Kernel (NTK) Features**
  - Why needed here: Section 2.1.1 shows that full fine-tuning can be analyzed as linear probing in NTK feature space. This justifies studying linear probing as representative of broader transfer behavior.
  - Quick check question: Why does the Taylor expansion argument mean full fine-tuning is still constrained by pretrained features?

## Architecture Onboarding

- Component map:
  - Feature extractor φ(X; θ): learns representations during pretraining, frozen during linear probing
  - Linear classifier γ: trained on target task using frozen features
  - Concatenated ensemble: combines feature extractors from multiple independently trained models

- Critical path:
  1. Pretrain multiple ResNet50 models on ImageNet for reduced steps (e.g., 4 models × 100K steps vs 1 model × 450K steps)
  2. Extract and concatenate feature representations before the classification layer
  3. Train new linear classifier on target dataset using concatenated features

- Design tradeoffs:
  - Ensemble width vs. depth: More models = more features but higher inference cost
  - Training duration per model: Fewer steps = less saturation but weaker individual features
  - The paper shows 4-5 models hitting diminishing returns for their setup

- Failure signatures:
  - Transfer performance significantly below direct training baseline (>5% gap suggests missing features)
  - Inconsistent performance across subtasks of the pretraining mixture
  - Large variance in transfer performance across random seeds

- First 3 experiments:
  1. Replicate the CAT4 vs baseline comparison on ImageNet→INAT18 transfer to validate the phenomenon in your architecture
  2. Ablate the number of concatenated models (2, 4, 8) to find the saturation point for your compute budget
  3. Test whether the bottleneck appears in your domain by comparing mixture-pretrained vs. directly-trained models on held-out subtasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single model training process be modified to learn "rich representations" equivalent to concatenation, or is aggregation strictly necessary?
- Basis in paper: [explicit] Section 6 states the proposed concatenation solution is "incomplete" and represents only "initial steps towards understanding the problem."
- Why unresolved: The paper demonstrates that simply scaling models (ResNet50W2) fails to capture lost features, leaving the "fresh problem space" of retrieving them without explicit aggregation unresolved.
- What evidence would resolve it: A novel training objective or regularization technique that allows a single model to match the transfer accuracy (+12.5%) of the TIME-CAT ensemble.

### Open Question 2
- Question: What is the theoretical cause of the "information saturation bottleneck" beyond sparsity bias?
- Basis in paper: [explicit] Section 6 explicitly states the paper "does not claim to fully discover, solve, or explain this" phenomenon.
- Why unresolved: The analysis relies heavily on a toy counterexample and empirical correlations (e.g., gradient starvation) rather than a formal theoretical framework explaining why features become permanently unrecoverable.
- What evidence would resolve it: A formal theoretical characterization of the gradient dynamics that lead to saturation, identifying specific conditions under which features are lost.

### Open Question 3
- Question: To what extent can optimized data mixing or ordering strategies prevent information saturation during pretraining?
- Basis in paper: [inferred] Section 1 notes that factors like "data distribution or ordering affect the features" learned, but the paper does not propose a method to control this.
- Why unresolved: While the paper identifies data ordering as a root cause of which feature subset is learned, it focuses on remediation via concatenation rather than preventative training schedules.
- What evidence would resolve it: A curriculum learning strategy that ensures consistent retention of features across all subgroups, eliminating the gap between direct and transfer learning.

## Limitations
- The paper's theoretical framework relies heavily on a specific interpretation of SGD's implicit sparsity bias that is not universally accepted in the deep learning community
- The proposed TIME-CAT method requires parallel training of multiple models, which may not be practical in all settings
- The analysis focuses on linear probing as a proxy for transfer, which may not fully capture the benefits of full fine-tuning

## Confidence
- **High Confidence**: The empirical observation that pretrained models struggle to learn new features once similar competing features saturate the gradient signal during training. The specific performance improvements from TIME-CAT on INATURALIST 18 and CIFAR-100 are directly measured and reported.
- **Medium Confidence**: The theoretical explanation for why the information saturation bottleneck occurs, based on implicit sparsity bias and feature redundancy assumptions. While logically consistent with the empirical observations, alternative explanations cannot be ruled out.
- **Low Confidence**: The generality of the proposed solution (TIME-CAT) across different architectures and domains beyond the tested image classification tasks. The method's effectiveness may depend heavily on specific characteristics of computer vision datasets.

## Next Checks
1. Replicate the information saturation phenomenon on a synthetic dataset where you can control feature distributions and directly observe whether pretrained models fail to learn features present in the training data but not initially discovered.

2. Test TIME-CAT's effectiveness when training models for different durations (e.g., 50K vs 200K steps) to identify the optimal balance between feature diversity and individual model quality for your specific domain.

3. Evaluate whether the bottleneck manifests when using architectures with different implicit regularization properties (e.g., Vision Transformers vs ResNet) to test the mechanism's dependence on SGD's sparsity bias.