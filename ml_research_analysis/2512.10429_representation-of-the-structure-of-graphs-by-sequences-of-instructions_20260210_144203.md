---
ver: rpa2
title: Representation of the structure of graphs by sequences of instructions
arxiv_id: '2512.10429'
source_url: https://arxiv.org/abs/2512.10429
tags:
- binary
- graph
- train
- representation
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to represent graphs using sequences
  of instructions, aimed at enabling the use of deep learning language models for
  graph processing. Traditional graph representations, such as adjacency matrices,
  are not naturally suited for processing by language models.
---

# Representation of the structure of graphs by sequences of instructions

## Quick Facts
- arXiv ID: 2512.10429
- Source URL: https://arxiv.org/abs/2512.10429
- Reference count: 4
- Introduces instruction sequence representation for graphs enabling deep learning language models to process graph structure

## Executive Summary
This paper introduces a novel method to represent graphs using sequences of simple instructions (U, D, L, R, E), enabling the use of transformer models for graph processing tasks. Traditional graph representations like adjacency matrices are not naturally suited for language models, but this method converts them into sequential strings that can be processed by standard NLP architectures. The approach shows faster learning, better generalization, and improved accuracy compared to binary adjacency matrix representations on synthetic 3D point-based graph classification tasks.

## Method Summary
The method encodes an adjacency matrix using a greedy algorithm that traverses active cells in order of proximity, generating a string of movement instructions (U/D/L/R) and edge insertion commands (E). The transformation is reversible and produces compact representations especially for sparse graphs. A synthetic dataset of 3D point-based graphs was created with three classes using different random walk and torus-based generation methods. Transformer models were trained on both the new instruction string representation and traditional binary string representations to classify these graphs.

## Key Results
- Models using instruction string representation learned faster and achieved better validation cross-entropy than binary string models
- Instruction string models attained higher classification accuracy and AUC scores on the synthetic dataset
- The instruction string representation was faster to process across all tested transformer sizes
- For sparse graphs, the instruction string length scales as ~N²√ρ compared to N² for binary representation

## Why This Works (Mechanism)

### Mechanism 1
The instruction sequence representation compresses sparse graphs more efficiently than binary adjacency matrix flattening. A greedy algorithm traverses the adjacency matrix using pointer movements to visit active cells in order of proximity. Each active cell requires only the movement instructions to reach it plus one E-instruction, rather than encoding every cell explicitly. For sparse graphs with density ρ, the expected string length scales as ~N²√ρ rather than N². This relies on the assumption that graphs are sparse and that greedy nearest-neighbor traversal produces sufficiently short paths between active cells.

### Mechanism 2
Local structural changes in the graph produce proportionally local changes in the instruction sequence. The canonical algorithm produces a deterministic traversal path. When flipping a single matrix cell, this requires either inserting a small substring (2δ+1 instructions, where δ is the Manhattan distance to the nearest point on the existing path) or removing/modifying an existing E-instruction's context. The Levenshtein distance between strings scales with the magnitude of graph change. This assumes the canonical algorithm's greedy path is stable under small perturbations.

### Mechanism 3
Transformer models learn faster and achieve better generalization on instruction sequences than on flattened binary adjacency matrices. Instruction sequences expose structural regularities (repeated movement patterns to nearby edges) that transformers can learn via attention over local subsequences. Binary flattening destroys 2D spatial locality—adjacent matrix cells may be far apart in the flattened string. The instruction sequence implicitly encodes the 2D structure through movement semantics, providing a more learnable signal. This assumes transformers benefit from the sequential "narrative" of building the matrix step-by-step more than from direct pixel-like encoding.

## Foundational Learning

- **Concept: Adjacency Matrix Representation**
  - **Why needed here:** The entire method operates on the adjacency matrix as the intermediate representation between graph and instruction sequence. Understanding matrix symmetry properties is essential.
  - **Quick check question:** Given a 4-node undirected graph with edges (1,2), (2,3), (3,4), what is the adjacency matrix M, and which cells are active?

- **Concept: Manhattan Distance (L1 Norm)**
  - **Why needed here:** The canonical algorithm uses Manhattan distance to find the nearest unvisited active cell. The asymptotic analysis derives the expected nearest-neighbor distance under this metric.
  - **Quick check question:** In a 10×10 matrix, what is the Manhattan distance between cells (2,3) and (7,9)?

- **Concept: Levenshtein (Edit) Distance**
  - **Why needed here:** Section 2.3 uses Levenshtein distance to formalize how "local" changes in the graph correspond to "local" changes in the string representation.
  - **Quick check question:** What is the Levenshtein distance between "UDRE" and "UDDRE"?

## Architecture Onboarding

- **Component map:** Adjacency Matrix -> Canonical Encoder -> Instruction String -> Tokenizer -> Transformer Encoder -> Classification Head
- **Critical path:**
  1. Ensure vertex ordering is defined (prerequisite for canonical string uniqueness)
  2. Run canonical encoder to produce I_G
  3. Tokenize with 5-token vocabulary + padding/special tokens as needed
  4. Pass through transformer; pool or use [CLS]-equivalent for graph-level output
  5. Apply classification head for downstream task
- **Design tradeoffs:**
  - Greedy vs. optimal path: The canonical algorithm is not guaranteed to produce the shortest possible string—only a unique one. Optimal compression would require solving a variant of the traveling salesman problem over active cells.
  - Assumption: Sparse graphs benefit most; dense graphs may not see compression gains.
  - Vertex ordering sensitivity: Different vertex orderings produce different canonical strings; the method assumes a consistent ordering is defined a priori.
- **Failure signatures:**
  - No convergence: Check if graphs are too dense (ρ > 0.5), causing excessive sequence lengths
  - Poor generalization: Verify that training/validation sets use consistent vertex ordering schemes
  - Training instability: Confirm vocabulary is restricted to exactly 5 tokens; unexpected characters indicate encoding bugs
- **First 3 experiments:**
  1. **Reproduction check:** Implement the canonical encoder and verify reversibility (M → I_G → M') on random sparse matrices with ρ=0.2, N=20
  2. **Compression ratio benchmark:** Measure |I_G| / N² across varying sparsity levels (ρ ∈ {0.05, 0.1, 0.2, 0.5}) and confirm the ~√ρ scaling trend from Equation (9)
  3. **Ablation on synthetic data:** Replicate the 3-class synthetic experiment with both binary and instruction string representations, comparing convergence speed and final accuracy with a fixed transformer configuration

## Open Questions the Paper Calls Out

- **How does the instruction string representation perform on standard real-world graph classification benchmarks compared to the synthetic 3D point-based graphs tested?**
  - Basis in paper: The paper explicitly relies on a "synthetic dataset" and describes the experiments as "tentative" in the Abstract and Conclusion.
  - Why unresolved: Synthetic data often lacks the noise, irregularity, and feature complexity of real-world networks, leaving the practical applicability uncertain.
  - What evidence would resolve it: Evaluation results on established graph classification datasets (e.g., MUTAG, PROTEINS) comparing the proposed method against baselines.

- **How can node attributes or edge weights be integrated into the instruction sequence representation to handle non-structural graph data?**
  - Basis in paper: The methodology defines the graph strictly by its adjacency matrix and topology, ignoring the node features that are central to the GNN approaches discussed in the Introduction.
  - Why unresolved: Many deep learning tasks rely on node features (e.g., atom types in chemistry); a representation limited to topology may be insufficient for these tasks.
  - What evidence would resolve it: A modified instruction set or embedding strategy that incorporates continuous feature values without losing the compactness or locality benefits.

- **Can a non-greedy canonicalization algorithm be developed to produce provably optimal (shortest) instruction strings for dense or complex graphs?**
  - Basis in paper: Section 2.1 states, "There is no guarantee that this is the shortest possible string to represent G," noting that the provided algorithm is merely a heuristic to keep length small.
  - Why unresolved: A greedy approach might result in unnecessarily long sequences for specific graph structures, reducing the efficiency and processing speed gains claimed in the results.
  - What evidence would resolve it: An algorithm capable of generating strings consistently shorter than the greedy baseline or a formal proof of the approximation ratio.

## Limitations
- The method's performance on real-world graph datasets remains untested, as experiments were limited to synthetic data
- The compression benefits may diminish for dense graphs where the instruction string length approaches 2N²-1
- The method only handles structural information and cannot incorporate node attributes or edge weights without modification

## Confidence
- **High Confidence:** The synthetic dataset generation methodology is clearly specified and reproducible. The transformer training procedure with hyperparameters is detailed enough for replication. The observation that instruction strings achieve faster convergence and better accuracy is empirically supported.
- **Medium Confidence:** The theoretical analysis of compression ratios and Levenshtein distance bounds appears mathematically sound but depends on assumptions that weren't stress-tested. The claim that instruction sequences expose learnable structural patterns to transformers is plausible but lacks ablation studies.
- **Low Confidence:** The generalization of results to real-world graphs is questionable given the synthetic nature of the dataset. The method's sensitivity to vertex ordering and its behavior on non-sparse or highly regular graphs isn't thoroughly explored.

## Next Checks
1. **Perturbation Study:** Systematically add/remove edges at varying Manhattan distances from existing paths in synthetic graphs and measure actual Levenshtein distances between resulting instruction strings. Compare against the theoretical 2δ+1 bound to validate locality preservation.
2. **Compression Ratio Validation:** Generate graphs across a range of densities (ρ ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 0.8}) and measure empirical string lengths. Plot |I_G|/N² against √ρ to verify the asymptotic scaling predicted in Equation (9).
3. **Real-World Benchmark:** Apply the method to established graph classification datasets (e.g., MUTAG, PROTEINS) and compare performance against standard GNN approaches. Focus on datasets with varying graph sizes and densities to test robustness beyond the synthetic setting.