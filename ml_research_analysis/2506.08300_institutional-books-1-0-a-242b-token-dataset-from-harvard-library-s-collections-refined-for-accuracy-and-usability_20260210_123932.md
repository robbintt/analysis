---
ver: rpa2
title: 'Institutional Books 1.0: A 242B token dataset from Harvard Library''s collections,
  refined for accuracy and usability'
arxiv_id: '2506.08300'
source_url: https://arxiv.org/abs/2506.08300
tags:
- text
- language
- collection
- data
- volumes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Institutional Books 1.0, a 242B token dataset
  of public domain books from Harvard Library's Google Books collection, refined for
  accuracy and usability. The authors processed 983,004 volumes through deduplication,
  OCR artifact analysis, topic classification, and text post-processing.
---

# Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability

## Quick Facts
- arXiv ID: 2506.08300
- Source URL: https://arxiv.org/abs/2506.08300
- Reference count: 0
- 983,004 public domain volumes, 242B tokens from Harvard Library's Google Books collection

## Executive Summary
Institutional Books 1.0 is a 242B token dataset of public domain books from Harvard Library's Google Books collection, refined for accuracy and usability in LLM training. The dataset includes 983,004 volumes spanning 250+ languages, with 60% of content from 1820-1920. The authors processed the collection through deduplication, OCR artifact analysis, topic classification, and text post-processing to improve data quality while maintaining clear provenance and documentation.

## Method Summary
The authors retrieved 1,075,899 volumes from Harvard Library via the GRIN API, then filtered to 983,004 public domain volumes using HathiTrust's rights database. They processed the collection through multiple parallel pipelines: language detection (pyfranc), topic classification (fine-tuned mBERT, 97.8% accuracy), deduplication (Simhash), OCR quality assessment (Google scores + OCRoscope), and text analysis (tokenizability). An OCR post-processing pipeline using a 71% accurate line classifier and Model2Vec embeddings was applied to 860K volumes in five major languages. The dataset was released on HuggingFace with both original and post-processed text, metadata, and experimental classifications.

## Key Results
- 60% of content from 1820-1920, with strong representation across centuries
- 250+ languages detected; English (73.77%), German (8.15%), French (6.17%) dominate
- 91.41% of volumes have public domain status (pd, pdus, or cc-zero)
- Experimental topic classification achieves 97.8% accuracy on 1,000 benchmark samples

## Why This Works (Mechanism)

### Mechanism 1: Frugal Computing with Static Embeddings
OCR line classification achieves useful signal for post-processing using compressed static embeddings trained on minimal context. The pipeline distills sentence-transformers/LaBSE to Model2Vec, then fine-tunes as a classifier using only positional information (page/line numbers). This coarse-grained prediction guides text reconstruction without requiring attention mechanisms. The approach enables inference on a single Apple M4 MAX SoC in approximately 5 days for 860K volumes.

### Mechanism 2: Multi-Signal Quality Filtering
Combining Google-provided OCR scores, computed OCRoscope scores, and tokenizability metrics identifies volumes with systematic OCR failures. Three independent quality signals cross-validate to flag volumes where signals diverge - for example, low tokenizability plus high OCR score suggests layout issues rather than character recognition problems. This multi-signal approach provides more diagnostic power than any single metric alone.

### Mechanism 3: Rights Determination via Institutional Provenance
Bibliographic identifiers combined with HathiTrust's rights database determine copyright status at scale with high confidence. Harvard barcodes prefixed with "hvd" match to HathiTrust API records, retrieving pre-computed rights determinations. This approach filters to pd/pdus/cc-zero status for 91.41% of the collection, leveraging institutional provenance for authoritative copyright assessment.

## Foundational Learning

- **Locality-sensitive hashing (Simhash)**: Needed for deduplication of ~1M volumes requiring sub-quadratic comparison; enables O(n) approximate near-duplicate detection. Quick check: Given two 7-character shingle sets {abc, bcd, cde} and {abc, bcd, xyz}, how would you compute their approximate similarity using Simhash?

- **Type-token ratio (TTR)**: Used as text complexity/quality proxy; volumes with anomalously low TTR may have OCR repetition artifacts. Quick check: A text with 1000 words of which 400 are unique has what TTR? What might a TTR of 0.1 indicate for an English prose volume?

- **Static vs. contextual embeddings**: The OCR post-processing uses Model2Vec (static) rather than BERT (contextual) for efficiency; understanding this tradeoff is critical for reproducing the pipeline. Quick check: If a line contains "Page 45 of 320", can a static embedding model distinguish whether "45" is a page number or part of the text? Why or why not?

## Architecture Onboarding

- **Component map**:
GRIN API → Raw archives (.tar.gz)
    ↓
Decryption → OCR text + metadata (JSONL/CSV)
    ↓
Parallel analysis pipelines:
  ├─ Language detection (pyfranc, 768-char chunks)
  ├─ Topic classification (fine-tuned mBERT, 168M params)
  ├─ Deduplication (Simhash, 7-char shingles)
  ├─ OCR quality (Google score + OCRoscope)
  └─ Text analysis (Polyglot tokenization, TTR, tokenizability)
    ↓
OCR post-processing (Model2Vec classifier + heuristics)
    ↓
Rights filtering (HathiTrust API)
    ↓
Output: Dataset (HuggingFace) + Pipeline code (GitHub)

- **Critical path**:
  1. GRIN retrieval (rate-limited, ~15 days for full collection)
  2. OCR post-processing inference (single M4 MAX, ~5 days for 860K volumes)
  3. Rights determination (API-dependent; may require retries for missing records)

- **Design tradeoffs**:
  - Accuracy vs. cost: Topic classifier uses 168M-param mBERT instead of larger models; achieves 97.8% on benchmark but "has not been reviewed by librarians at volume level"
  - Completeness vs. correctness: Deduplication lists near-duplicates rather than removing them; users must decide whether to filter based on their use case
  - Language coverage vs. quality: OCR post-processing limited to 5 languages (eng, deu, fra, ita, spa) covering ~81% of collection; other languages retain original OCR only

- **Failure signatures**:
  - Low tokenizability score (< 30.0) for English volumes → likely tables/graphs/music sheets
  - High standard deviation in OCR scores (> 15) → inconsistent scan quality within volume
  - Multiple languages detected in single volume with >40% split → likely parallel translation or anthology
  - "UNKNOWN" line type classifications spike → model outside training distribution (e.g., unusual layouts)

- **First 3 experiments**:
  1. Validate topic classifier on held-out subset: Sample 100 volumes from each LCC category, manually verify classification accuracy matches reported 97.8% benchmark or identify systematic confusions
  2. Measure deduplication precision: Randomly sample 20 near-duplicate groups, manually confirm they represent same underlying work vs. false positives from similar-length unrelated texts
  3. A/B test OCR post-processing impact: Fine-tune a small language model on original OCR vs. post-processed text, measure perplexity difference on held-out subset to quantify "tokenizability" improvement empirically

## Open Questions the Paper Calls Out

### Open Question 1
What explains the consistent ~10-point difference between Google Books and OCRoscope OCR quality scores for 18th and 20th century texts? The paper notes this difference but cannot fully interpret it, as the two scoring systems use different methodologies (Google's proprietary system vs. OCRoscope's CLD2-based text validity assessment).

### Open Question 2
Why were 70,922 volumes listed in Google's API but not retrievable, and are these volumes recoverable? The authors encountered missing archives without clear diagnostic information and hypothesize unfulfilled scanning intentions but lack confirmation.

### Open Question 3
Does the OCR post-processing pipeline measurably improve performance on downstream ML/NLP tasks? While the paper measures proxy metrics (tokenizer efficiency, sentence segmentation), it explicitly states "Further analysis is needed to confirm this trend" regarding practical impact.

### Open Question 4
What is the volume-level accuracy of the ML-assisted topic classification when reviewed by domain experts? The 97.8% benchmark accuracy measures agreement with existing metadata rather than ground-truth topical accuracy, and the authors explicitly state the results "have not been reviewed by librarians at volume level."

## Limitations

- OCR post-processing pipeline achieves only 71% accuracy on its classification task, meaning approximately 29% of lines may be incorrectly processed
- Heavy temporal bias (60% from 1820-1920) may limit representativeness for modern language patterns
- Five-language post-processing coverage (~81% of collection) leaves substantial portions with potentially lower-quality OCR

## Confidence

- **Topic classification accuracy (97.8%)**: Medium confidence - based on 1,000-sample benchmark with clear methodology, but limited manual review and potential overfitting to training distribution
- **Rights determination completeness (91.41% public domain)**: Medium confidence - depends on HathiTrust's authoritative database but acknowledges jurisdictional limitations and missing record scenarios
- **OCR post-processing utility**: Low-Medium confidence - 71% accuracy suggests substantial error rate; impact on LLM training quality remains unmeasured empirically
- **Multi-signal quality filtering effectiveness**: Medium confidence - uses three independent metrics with reasonable theoretical foundation, but cross-validation results not demonstrated

## Next Checks

1. **Rights determination validation**: Sample 100 volumes flagged as public domain, manually verify copyright status through alternative sources (Library of Congress, Internet Archive) to measure false positive rate and jurisdictional accuracy

2. **OCR post-processing impact measurement**: Conduct controlled experiment training identical LLM architectures on original OCR vs. post-processed text subsets (10,000 volumes each), measuring perplexity and downstream task performance to quantify practical improvement

3. **Topic classifier generalization test**: Evaluate classifier on 200 randomly selected volumes from underrepresented classes (e.g., MILITARY SCIENCE with only 110 training samples), comparing predicted vs. actual LCC classification to identify systematic weaknesses