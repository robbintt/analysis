---
ver: rpa2
title: Synthesizing and Adapting Error Correction Data for Mobile Large Language Model
  Applications
arxiv_id: '2505.18488'
source_url: https://arxiv.org/abs/2505.18488
tags:
- data
- synthetic
- training
- original
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for improving error correction in mobile
  large language models (LLMs) by synthesizing high-quality training data and adapting
  it to the mobile application domain. The approach involves prompting LLMs with domain
  knowledge to generate error correction pairs, followed by reweighting the synthetic
  data distribution to better match the mobile application domain using a privacy-preserving
  reweighting model.
---

# Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications

## Quick Facts
- arXiv ID: 2505.18488
- Source URL: https://arxiv.org/abs/2505.18488
- Reference count: 40
- One-line primary result: A method for improving mobile LLM error correction through domain-knowledge-guided synthetic data generation and likelihood-difference reweighting achieves 2.47% to 7.18% relative improvements on production metrics.

## Executive Summary
This paper presents a comprehensive framework for improving error correction in mobile large language models by synthesizing high-quality training data and adapting it to the mobile application domain. The approach combines domain-knowledge-guided LLM synthesis with self-verification, likelihood-difference reweighting for domain alignment, and a two-phase continue training strategy. The method is validated through both offline evaluation and live A/B tests, demonstrating significant improvements in error correction performance for mobile typing applications.

## Method Summary
The method involves three key components: first, synthesizing error correction pairs by prompting LLMs with domain knowledge and filtering through self-correction verification; second, training a small privacy-preserving language model via federated learning with differential privacy to score synthetic samples, then learning reweighting parameters that predict live A/B metrics from likelihood differences; and third, fine-tuning the base model through continue training - initially on full synthetic data, then on a mixture of original and reweighted synthetic data using LoRA adapters. The approach specifically targets mobile typing applications where domain-specific error patterns and privacy constraints make traditional approaches challenging.

## Key Results
- The reweighting model achieves validation residual of 2.07×10⁻⁶, significantly better than baseline (5.44×10⁻⁵)
- ContMixFil approach achieves 73.31% (original, weighted) and 89.51% (synthetic, weighted) top-1 good ratio
- Relative improvements of 2.47% to 7.18% on key production metrics including click-through rate and accept rate

## Why This Works (Mechanism)

### Mechanism 1: Domain-Knowledge-Guided LLM Synthesis with Self-Verification
Prompting LLMs with explicit grammatical error knowledge and enforcing self-correction verification produces higher-quality synthetic EC pairs than detector-based pipelines. The LLM is prompted with a role (teacher), error type taxonomy, and a generate-then-correct workflow; only pairs where LLM-corrected output matches the original clean text are retained (~60% pass rate). This filtration process removes around 40% of the data after self-correction verification; error analysis shows 52% verb errors, 15% missing words, 10% plural.

### Mechanism 2: Likelihood-Difference Reweighting for Domain Alignment
The calibrated difference between a DP-FL fine-tuned LM score and a public pre-trained LM score predicts which synthetic samples align with the mobile domain. A small LM (~8M params) fine-tuned via federated learning with differential privacy (ε < 10) produces Sf scores; reweighting model learns w(θ, y) = σ(θf·Sf - θp·Sp + θb) to predict live A/B metrics from offline evaluation. Learned parameters (θf=40.64, θp=-30.44, θb=-1.59) show positive correlation with Sf calibrated by Sp.

### Mechanism 3: Two-Phase Continue Training with Domain-Filtered Mixture
Training first on full synthetic data, then continuing with original + reweighted-filtered synthetic data improves both offline and live metrics. Phase 1 (1000 steps, ~1 epoch on full synthetic) establishes broad EC capability; Phase 2 refines with domain-aligned data (w(θ,y) ≥ 1 threshold keeps ~50%) plus small original dataset. ContMixFil achieves 2.47% to 7.18% relative improvement on key production metrics.

## Foundational Learning

- Concept: Federated Learning with Differential Privacy
  - Why needed here: Enables training Sf on real mobile typing data without privacy violations; provides domain signal for reweighting
  - Quick check question: Explain why ε < 10 provides formal privacy guarantees and what utility cost it imposes

- Concept: Importance Weighting for Covariate Shift
  - Why needed here: Synthetic data from C4 has distribution shift from mobile typing; reweighting corrects this mismatch
  - Quick check question: Given Sf(y)=-3, Sp(y)=-6, and learned θ, compute the reweighting score w(θ,y)

- Concept: LoRA Fine-tuning
  - Why needed here: Enables efficient adaptation of billion-parameter models for mobile deployment
  - Quick check question: Why does LoRA allow multiple task-specific adapters without increasing base model footprint?

## Architecture Onboarding

- Component map: C4 → Gecko embedding → 20K clusters → sample 10/cluster → Gemini Ultra prompt → typing simulator → 1.2M EC pairs → DP-FL small LM (Sf) + public LM (Sp) → regression learns θ from K=10+ live A/B deployments → sample weights → Phase 1 (full synthetic, 1000 steps) → Phase 2 (original + filtered synthetic mixture) → LoRA adapter for Gemini Nano

- Critical path: Reweighting model requires live A/B metrics from previous deployments; if starting fresh, collect 10+ model variants before learning θ

- Design tradeoffs:
  - Clustering vs. vanilla sampling: improves diversity (Fig. 3b) at preprocessing cost
  - Privacy ε vs. signal quality: lower ε strengthens privacy but may degrade Sf
  - Batch size: 4× helps synthetic validation (Fig. 5b) but needs careful LR tuning

- Failure signatures:
  - Offline improves, live doesn't → check reweighting residual (Table 2); likely domain mismatch
  - Phase 2 degrades → verify mixture ratio (1:4 recommended); check filtering threshold
  - High filtration rate (>60%) → prompt may over-modify; verify error-only constraint in Table 1

- First 3 experiments:
  1. Validate clustering subsampling: Compare 20K-cluster vs. random sampling on held-out error type coverage
  2. Verify likelihood-difference hypothesis: Train Sf on proxy private data, confirm θf > 0, θp < 0 after optimization
  3. Ablate continue training: Compare full synthetic, mixture-from-start, and continue training on proxy live metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced domain adaptation techniques outperform the proposed linear reweighting model in bridging the distribution gap between synthetic and mobile application data?
- Basis: [explicit] The authors state "There are many other potential domain adaptation methods for future experiments."
- Why unresolved: The current work focuses specifically on a reweighting model parameterized by a linear combination of small LM scores.
- What evidence would resolve it: Experiments comparing non-linear or attention-based adaptation methods against the linear baseline on live A/B test metrics.

### Open Question 2
- Question: Can alternative privacy-preserving information sources, such as histograms generated in trusted execution environments, improve the accuracy of the reweighting model?
- Basis: [explicit] The conclusion notes that "other forms of information such as histogram... are worth considering" alongside next-generation FL systems.
- Why unresolved: The current system relies exclusively on scores from a small, differentially private language model.
- What evidence would resolve it: Ablation studies replacing LM scores with histogram features or TEE-derived data to predict live metrics.

### Open Question 3
- Question: Does utilizing soft sample weights during training yield better model performance than the binary filtering strategy used in the experiments?
- Basis: [inferred] The authors implement hard filtering for ease of maintenance, acknowledging the trade-off implies soft weighting is a valid alternative.
- Why unresolved: The paper only evaluates reweighting by applying a threshold (w ≥ 1) to discard low-scoring samples rather than using the continuous weight values.
- What evidence would resolve it: Comparing model convergence and accuracy when using continuous weights in the loss function versus the binary inclusion method.

## Limitations
- The approach requires K=10+ historical A/B deployments to learn reweighting parameters, creating a cold-start problem for new applications
- The 60% filtration rate after self-correction verification may be discarding valuable error correction pairs without detailed analysis of the lost data
- The method assumes Gemini Ultra's grammar knowledge perfectly covers mobile typing error distributions, which may not hold for all error types

## Confidence

**High Confidence**:
- The general framework of using LLM synthesis with self-verification to create high-quality error correction pairs
- The mechanism of using likelihood difference between DP-FL and public LMs for domain alignment
- The observation that reweighting improves both offline and live metrics compared to baseline approaches

**Medium Confidence**:
- The specific learned parameters (θf, θp, θb) will generalize to other mobile applications
- The optimal filtration rate of 60% is universally applicable
- The 1:4 mixture ratio is optimal for all scenarios

**Low Confidence**:
- The assumption that Gemini Ultra's grammar knowledge perfectly covers mobile typing error distributions
- The claim that the two-phase training approach is universally superior to other strategies
- The robustness of the approach to different types of error correction tasks beyond mobile typing

## Next Checks

1. **Cold-Start Reweighting Validation**: Design an experiment to validate the reweighting model without requiring 10+ historical A/B deployments. This could involve using proxy metrics or transfer learning from similar domains to initialize the reweighting parameters.

2. **Error Type Coverage Analysis**: Conduct a detailed analysis of the discarded 40% of synthetic data to identify which error types are being lost and whether this represents a systematic bias. This will help validate whether the current filtration rate is optimal or needs adjustment.

3. **Ablation Study on Training Strategy**: Perform a comprehensive ablation study varying the mixture ratio (original:synthetic) and filtering threshold (wt) to identify the optimal configuration. Additionally, compare the two-phase continue training approach against alternative strategies such as simultaneous training on mixed data or curriculum learning.