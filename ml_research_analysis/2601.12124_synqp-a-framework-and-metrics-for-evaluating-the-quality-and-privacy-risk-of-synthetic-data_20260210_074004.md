---
ver: rpa2
title: 'SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk
  of Synthetic Data'
arxiv_id: '2601.12124'
source_url: https://arxiv.org/abs/2601.12124
tags:
- data
- synthetic
- privacy
- risk
- synqp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynQP is an open framework for benchmarking privacy risks in synthetic
  data generation (SDG) using simulated sensitive data. The core innovation is generating
  pseudo-identifiable datasets from non-identifiable real data, enabling fair privacy
  evaluations that account for SDG's probabilistic nature.
---

# SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data

## Quick Facts
- arXiv ID: 2601.12124
- Source URL: https://arxiv.org/abs/2601.12124
- Authors: Bing Hu; Yixin Li; Asma Bahamyirou; Helen Chen
- Reference count: 33
- Key outcome: SynQP framework shows DP reduces privacy risks (SD-IDR and SD-MIA) but degrades data quality, with non-private models achieving near-perfect utility (≥0.97) while privacy risk remains below 0.09 regulatory threshold

## Executive Summary
SynQP is an open framework for benchmarking privacy risks in synthetic data generation (SDG) using simulated sensitive data. The core innovation is generating pseudo-identifiable datasets from non-identifiable real data, enabling fair privacy evaluations that account for SDG's probabilistic nature. The framework includes a new identity disclosure risk metric (SD-IDR) that considers small numerical variations in synthetic records, addressing limitations of traditional exact-match approaches. When applied to CTGAN, TVAE, and GaussianCopula models, SD-IDR showed that non-private models achieved near-perfect utility (≥0.97) while privacy risk remained below the 0.09 regulatory threshold when DP was applied. The framework revealed DP consistently reduced both SD-IDR and membership-inference attack risk across all models, providing a standardized tool for evaluating and improving privacy protections in synthetic health data applications.

## Method Summary
The SynQP framework generates pseudo-identifiable data from non-identifiable real datasets by simulating quasi-identifiers (age, gender, marital status, occupation, ethnicity, address) from census distributions using inverse transform sampling, then linking these to real diabetes and BMI data through nearest-neighbor matching. The framework trains SDG models (CTGAN, TVAE, GaussianCopula) with optional local differential privacy (DP) via Laplacian noise injection at the input level. Privacy is evaluated using SD-IDR, which extends traditional identity disclosure risk by allowing small numerical variations through variational budgets, and SD-MIA, which measures membership inference risk as the difference between training data disclosure risk and holdout disclosure risk. Quality is assessed through Hellinger distance and maximum likelihood estimation (MLE) via logistic regression.

## Key Results
- Non-private SDG models achieved near-perfect utility (≥0.97) while maintaining privacy risk below 0.09 regulatory threshold
- DP consistently reduced both SD-IDR and membership-inference attack risk across all models
- SD-IDR revealed that exact-match metrics systematically underestimate privacy risk for probabilistic SDG models
- CTGAN showed largest quality degradation with DP (MLE scores 0.47, 0.94, 0.87 for CTGAN, GaussianCopula, and TVAE respectively)

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-identifiable Data Simulation for Privacy Benchmarking
Simulating quasi-identifiers from census distributions and linking them to non-identifiable real data creates benchmark datasets for privacy evaluation without exposing sensitive information. The framework seeds a population with quasi-identifiers (age, gender, marital status, occupation, ethnicity, address) using inverse transform sampling from census data, then links non-identifying datasets for real use cases through nearest-neighbor matching to create pseudo-identifiable training data. This enables fair privacy benchmarking while avoiding the need to use truly identifiable information.

### Mechanism 2: Variational Budget Accounting for Probabilistic SDG
Privacy metrics that allow small numerical variations better estimate re-identification risk for probabilistic generative models than exact-match approaches. SD-IDR extends traditional identity disclosure risk by introducing a variational budget (ε), where records match if categorical columns are identical AND cumulative numerical differences fall below a threshold. This captures the stochastic noise inherent in GAN/VAE generation and addresses the systematic underestimation of privacy risk by exact-match metrics.

### Mechanism 3: Differential Privacy Quality-Privacy Tradeoff
Local differential privacy through Laplacian noise injection reduces both identity disclosure and membership inference risks but degrades data fidelity and utility. The framework applies DP(x, ϵ) = (1-ϵ)x + ϵ×Laplace(μX, σX) at the input level before model training, trading off between privacy protection and statistical quality of generated data. This demonstrates the fundamental tension between achieving strong privacy guarantees and maintaining data utility in synthetic data applications.

## Foundational Learning

- **Differential Privacy (Local vs. Global)**
  - Why needed here: The paper uses local DP with input-level noise injection. Understanding the distinction from global DP (which adds noise to query outputs) is essential for interpreting why quality degrades and whether alternative DP strategies might be preferable.
  - Quick check question: Why does adding noise before training (local DP) affect synthetic data quality more than adding noise to aggregate statistics (global DP)?

- **Quasi-identifiers and k-Anonymity**
  - Why needed here: The framework's core innovation is simulating quasi-identifiers to create benchmark data. Without understanding how quasi-identifiers enable re-identification (e.g., 87% of US population uniquely identifiable by zip+birth date+gender), the motivation for SD-IDR remains unclear.
  - Quick check question: Given a dataset with [age, gender, occupation, city], explain why these are quasi-identifiers even though none uniquely identifies an individual alone.

- **Membership Inference Attacks (MIA) as Overfitting Detection**
  - Why needed here: SD-MIA is framed as IDR(X, Xsyn) - IDR(X̂), measuring whether synthetic data reveals training membership. Understanding MIA as a proxy for model overfitting is critical for interpreting negative SD-MIA values as underfitting indicators.
  - Quick check question: If SD-MIA is negative, what does this suggest about the generative model's training, and why might this be problematic?

## Architecture Onboarding

- **Component map:**
  Quasi-identifier simulation module (census distributions → age, gender, occupation, ethnicity, address) → Real data linking module (non-identifiable datasets → nearest-neighbor matching) → SDG training module (CTGAN/TVAE/GaussianCopula with optional DP) → Synthetic data generation (output: 10,000 rows) → Quality evaluation (Hellinger distance, MLE via logistic regression) → Privacy evaluation (SD-IDR across variational budgets, SD-MIA)

- **Critical path:**
  Quasi-identifier simulation quality → Real data linkage accuracy → SDG model training → Privacy metric validity. If simulated quasi-identifiers don't reflect realistic correlation structures, all downstream privacy evaluations are compromised.

- **Design tradeoffs:**
  - Exact match (IDR) vs. variational budget (SD-IDR): IDR is simpler but underestimates risk; SD-IDR is more conservative but requires threshold tuning
  - DP noise level (ϵ): Lower ϵ = stronger privacy but worse utility. Paper uses ϵ = 0.8 (high noise) for demonstration
  - Model selection: GaussianCopula achieves best fidelity (HD = 0.06) without DP; CTGAN shows largest quality degradation with DP

- **Failure signatures:**
  - Negative SD-MIA values: Indicates model underfitting (synthetic data less similar to training data than random sample)
  - High SD-IDR with low IDR: Confirms exact-match metrics underestimate risk
  - SD-MIA increasing with variational budget (TVAE pattern): Suggests overfitting; model memorized training records
  - SD-MIA decreasing with variational budget (CTGAN/GaussianCopula pattern): Suggests underfitting potential

- **First 3 experiments:**
  1. Validate simulation realism: Compare simulated quasi-identifier distributions against an independent census sample to verify that age/gender/occupation correlations are preserved (not just marginals)
  2. Calibrate variational budget: Run SD-IDR across ε = {0, 1, 2, 3, 5, 10} on a held-out dataset to identify the threshold where risk plateaus, informing regulatory threshold selection
  3. Stress-test SD-MIA interpretation: Train three model variants (underfit, well-fit, overfit) by varying epochs, then confirm SD-MIA correctly identifies underfitting (negative values) vs. overfitting (positive values increasing with budget)

## Open Questions the Paper Calls Out

- Does using k-nearest neighbors (k-NN) to infill real data into the simulated population improve the preservation of multivariate dependencies compared to the single nearest-neighbor approach used in the demonstration?
- To what extent does ignoring correlations between demographics (age, gender) and other quasi-identifiers (occupation, marital status) affect the accuracy of the simulated population and resulting privacy metrics?
- How does the SynQP framework perform when applied to a broader range of synthetic data generation models and diverse datasets beyond the diabetes and BMI case studies?

## Limitations
- The validity of simulated quasi-identifier distributions as proxies for real identifiable populations remains unverified, potentially limiting generalizability of privacy risk estimates to real-world deployments
- Variational budget thresholds (ε = 1, 2, 3) appear arbitrarily chosen without empirical grounding, raising concerns about false positive/negative rates in SD-IDR
- Local DP formulation (input-level noise) may be overly conservative compared to alternative DP strategies, potentially misrepresenting achievable utility-privacy tradeoffs

## Confidence
- High confidence in utility degradation under DP (supported by direct MLE comparisons showing consistent performance drops)
- Medium confidence in SD-IDR capturing true privacy risk (mechanism plausible but simulation realism unverified)
- Low confidence in SD-MIA interpretation as overfitting detector (underfitting interpretation supported but empirical validation needed)

## Next Checks
1. Validate simulation realism: Compare simulated quasi-identifier distributions against independent census samples to verify preserved correlation structures beyond marginal distributions
2. Calibrate variational thresholds: Empirically determine SD-IDR threshold where risk plateaus across ε = {0, 1, 2, 3, 5, 10} to inform regulatory threshold selection
3. Stress-test MIA interpretation: Train model variants with controlled overfitting/underfitting (varying epochs) and confirm SD-MIA correctly identifies both conditions before drawing conclusions about privacy-utility tradeoffs