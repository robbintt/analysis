---
ver: rpa2
title: How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate
  Change of Mind in Large Language Models
arxiv_id: '2507.03120'
source_url: https://arxiv.org/abs/2507.03120
tags:
- advice
- answer
- confidence
- change
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study reveals that LLMs exhibit conflicting behaviors: they
  are both overconfident in their initial answers and overly sensitive to contradictory
  feedback. Using a novel experimental paradigm, the researchers showed that LLMs
  (Gemma 3, GPT4o, and o1-preview) display a choice-supportive bias, reinforcing confidence
  in their initial answers and resisting change, even when faced with opposing advice.'
---

# How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models

## Quick Facts
- arXiv ID: 2507.03120
- Source URL: https://arxiv.org/abs/2507.03120
- Reference count: 36
- LLMs show both choice-supportive bias and excessive sensitivity to contradictory feedback

## Executive Summary
This study reveals that LLMs exhibit conflicting behaviors: they are both overconfident in their initial answers and overly sensitive to contradictory feedback. Using a novel experimental paradigm, the researchers showed that LLMs (Gemma 3, GPT4o, and o1-preview) display a choice-supportive bias, reinforcing confidence in their initial answers and resisting change, even when faced with opposing advice. Additionally, LLMs overweight inconsistent advice compared to consistent advice, deviating from normative Bayesian updating. These findings explain both the stubbornness and excessive sensitivity to criticism observed in LLMs, providing insights into their confidence mechanisms.

## Method Summary
The researchers developed a novel experimental paradigm to study how LLMs update their confidence when receiving advice. They tested three state-of-the-art LLMs (Gemma 3, GPT4o, and o1-preview) using 75 multiple-choice questions across various domains. The experimental design involved presenting LLMs with initial answers, then providing either consistent or inconsistent advice, and measuring how their confidence and answer choices changed. The study employed both quantitative metrics and qualitative analysis to examine the direction and magnitude of confidence updates, comparing LLM behavior against normative Bayesian updating predictions.

## Key Results
- LLMs display choice-supportive bias, maintaining confidence in initial answers even when presented with contradictory evidence
- LLMs overweight inconsistent advice compared to consistent advice, deviating from Bayesian updating norms
- The study found both stubbornness in maintaining initial positions and excessive sensitivity to criticism across tested models

## Why This Works (Mechanism)
None

## Foundational Learning
- **Choice-supportive bias**: The tendency to retroactively ascribe positive attributes to chosen options while amplifying negative features of rejected alternatives. Needed to understand why LLMs resist changing initial answers; quick check: observe if LLMs maintain confidence when presented with contradictory evidence.

- **Bayesian updating**: The mathematical framework for rationally incorporating new evidence into existing beliefs. Needed as the normative baseline for comparing LLM confidence updates; quick check: calculate expected confidence changes based on evidence strength.

- **Confidence calibration**: The alignment between stated confidence levels and actual probability of correctness. Needed to measure whether LLM confidence statements accurately reflect their reliability; quick check: compare confidence predictions against actual accuracy rates.

- **Adversarial prompting**: The practice of crafting inputs designed to elicit specific behaviors or responses from LLMs. Needed to understand how criticism framing affects LLM responses; quick check: test different criticism styles on confidence updates.

- **Decision inertia**: The tendency to maintain initial decisions despite new information. Needed to explain resistance to answer changes; quick check: measure response changes when advice conflicts with initial answers.

## Architecture Onboarding

**Component map:** Input Processing -> Initial Answer Generation -> Confidence Assessment -> Advice Reception -> Confidence Update -> Final Answer

**Critical path:** The sequence from receiving advice to confidence update represents the most crucial pathway, as it determines whether LLMs will change their minds or maintain their positions.

**Design tradeoffs:** The study highlights a fundamental tension between model stability (maintaining consistent answers) and adaptability (responding appropriately to new information). LLMs must balance these competing needs while avoiding both excessive stubbornness and unwarranted volatility.

**Failure signatures:** When LLMs exhibit either extreme resistance to change despite strong contradictory evidence, or excessive volatility in response to minimal contradictory feedback, indicating poor confidence calibration.

**First experiments:**
1. Test confidence updates across different reasoning domains (mathematical, logical, ethical)
2. Vary the strength and framing of contradictory advice to identify sensitivity thresholds
3. Compare human versus LLM confidence calibration on identical tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design relies on controlled multiple-choice questions that may not generalize to open-ended reasoning tasks
- Sample size of 75 questions may not capture full variability across different domains and reasoning types
- Difficulty distinguishing genuine underconfidence from artifacts of criticism framing or delivery

## Confidence
- Choice-supportive bias and deviation from Bayesian updating: High confidence
- Sensitivity to contradictory feedback: Medium confidence (methodological concerns about framing effects)
- Generalizability to real-world reasoning: Low confidence

## Next Checks
1. Test the same experimental paradigm across different reasoning domains (mathematical, logical, ethical) to assess domain-specific variations in confidence updating
2. Implement blinded human evaluation of criticism framing to control for potential experimenter demand characteristics
3. Conduct real-time confidence tracking during reasoning chains to distinguish between genuine underconfidence and surface-level response adjustments