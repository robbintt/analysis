---
ver: rpa2
title: 'MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental
  Health Counseling Sessions'
arxiv_id: '2509.04183'
source_url: https://arxiv.org/abs/2509.04183
tags:
- counseling
- client
- counselor
- agent
- header
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAGneT, a multi-agent framework for generating
  synthetic multi-turn mental health counseling sessions. The framework decomposes
  counselor response generation into coordinated sub-tasks handled by specialized
  LLM agents, each modeling a key psychological technique such as reflection, questioning,
  solution provision, normalization, and psycho-education.
---

# MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions

## Quick Facts
- arXiv ID: 2509.04183
- Source URL: https://arxiv.org/abs/2509.04183
- Authors: Aishik Mandal; Tanmoy Chakraborty; Iryna Gurevych
- Reference count: 40
- One-line primary result: MAGneT-generated sessions are preferred by experts in 77.2% of cases and improve fine-tuned model performance by 6.9% over baselines.

## Executive Summary
MAGneT introduces a multi-agent framework for generating synthetic multi-turn mental health counseling sessions that decompose counselor response generation into specialized sub-tasks handled by distinct LLM agents. Each agent models a key psychological technique—reflection, questioning, solution provision, normalization, and psycho-education—guided by a CBT-based planning agent and a technique selector to produce coherent, psychologically grounded responses. This approach addresses limitations in prior single-agent methods by better capturing the structure and nuance of real counseling sessions.

The framework significantly outperforms existing methods: experts preferred MAGneT-generated sessions in 77.2% of cases across nine counseling aspects, and fine-tuning an open-source model on MAGneT-generated data improved counseling skills by 6.9% on average over baseline synthetic datasets. The method demonstrates how cognitive decomposition via specialized agents, combined with session-level CBT planning and coordinated response fusion, can generate more realistic and therapeutically effective counseling interactions.

## Method Summary
MAGneT generates synthetic 40-turn mental health counseling sessions via a multi-agent role-play system. The pipeline uses 150 client intake forms from the CACTUS dataset, expanded to 450 generation seeds using three client attitude variations (Positive, Neutral, Negative). The framework employs Llama3-8B-Instruct for most agents and GPT-4o-mini for the technique selector. A CBT agent generates session-level treatment plans, five specialized response agents generate technique-specific candidate responses, a technique selector chooses appropriate techniques per turn, and a response generator fuses these into final counselor utterances. The system fine-tunes Llama3-8B-Instruct on 5,400 training pairs using QLoRA (rank=64, alpha=16, lr=2e-4, 3 epochs) with DeepSpeed on 4x V100 GPUs.

## Key Results
- Experts preferred MAGneT-generated sessions in 77.2% of cases across nine counseling aspects
- Fine-tuning on MAGneT-generated data improved counseling skills by 6.9% on average over baseline synthetic datasets
- CTRS (counseling skills), WAI (therapeutic alliance), and PANAS (emotional shifts) scores all showed significant improvements over single-agent baselines

## Why This Works (Mechanism)

### Mechanism 1: Cognitive Decomposition via Specialized Agents
Decomposing complex counseling responses into specialized sub-tasks (reflection, questioning, solution provision, normalization, psycho-education) improves response quality and psychological grounding compared to monolithic single-agent generation. Each specialized agent focuses on one therapeutic technique defined in clinical literature, generating candidate responses conditioned on dialogue history and client profile. This modular decomposition allows finer-grained control and better alignment with established therapeutic practices.

### Mechanism 2: Session-Level CBT Planning Combined with Turn-Level Technique Selection
Combining session-level CBT planning with dynamic turn-level technique selection produces more coherent, therapeutically aligned conversations than either strategy alone. A CBT agent generates a structured treatment plan at session start based on client intake form and first utterance, providing high-level behavioral goals and cognitive reframing strategies. A technique selector agent then chooses appropriate techniques for each turn based on the plan and dialogue context, providing both strategic direction and tactical flexibility.

### Mechanism 3: Coordinated Response Fusion
Synthesizing candidate responses from multiple specialized agents produces more comprehensive counselor utterances than any single-technique approach. Five specialized response agents generate candidate responses simultaneously, and the response generation agent fuses these candidates based on technique selector recommendations into a coherent final utterance. This preserves multiple therapeutic functions within single responses while maintaining coherence.

## Foundational Learning

- **Concept**: Cognitive Behavioral Therapy (CBT) framework
  - Why needed here: MAGneT's planning agent generates CBT-based treatment plans. CBT posits that maladaptive interpretations of events contribute to mental health issues, and therapy involves identifying and reframing thought patterns.
  - Quick check question: Can you explain the cognitive model underlying CBT and how cognitive restructuring works?

- **Concept**: Multi-agent LLM coordination
  - Why needed here: MAGneT relies on coordinating multiple LLM agents with different roles (CBT planner, technique selector, specialized response agents, response generator). Understanding agent orchestration and how to structure prompts for specialization is critical.
  - Quick check question: How would you design prompts to ensure agents specialize without duplicating functionality?

- **Concept**: Counseling evaluation metrics (CTRS, WAI, PANAS)
  - Why needed here: The paper uses multiple psychological scales to evaluate generated sessions. CTRS measures counseling skills, WAI measures therapeutic alliance, and PANAS measures emotional shifts.
  - Quick check question: What aspects of counseling quality does CTRS capture that PANAS does not?

## Architecture Onboarding

- **Component map**: Client initialization -> CBT Agent -> Technique Selector -> (5 Specialized Agents) -> Response Generation Agent -> Client Agent -> (loop for 40 turns)

- **Critical path**: 1. Initialize client from intake form + attitude, 2. Generate CBT plan, 3. For each turn: a) Generate 5 candidate responses, b) Select techniques, c) Fuse final response, d) Generate client response, 4. Continue for 40 turns or until client termination

- **Design tradeoffs**: Model selection varies (GPT-4o-mini for technique agent, Llama3-8B-Instruct for others); session length fixed at 40 turns; Qwen2.5-8B produces weaker CBT plans than Llama; ablation shows removing technique agent has broader impact than removing CBT agent

- **Failure signatures**: Repetitive responses in later turns; negative-attitude clients show worse PANAS negative-affect shifts; Qwen backbone produces weaker CBT plans; CBT planning can introduce rigidity

- **First 3 experiments**: 1. Reproduce ablation study comparing MAGneT vs. MAGneT-C, MAGneT-T, MAGneT-C-T on CTRS, WAI, PANAS, 2. Swap backbone models testing Qwen vs. Llama for CBT agent specifically, 3. Negative-attitude analysis manual inspection of poor PANAS sessions

## Open Questions the Paper Calls Out

1. Can the MAGneT framework be extended to generate longitudinal, multi-session counseling trajectories rather than single-session interactions?
2. How can the framework be adapted to incorporate non-verbal multimodal cues, such as tone of voice and facial expressions?
3. How can the framework mitigate the worsening of negative affect in clients with initially negative attitudes during CBT-based exploration?
4. Does incorporating cultural norms and linguistic diversity into the agent profiles improve the cultural sensitivity of the generated sessions?

## Limitations

- Evaluation relies entirely on LLM-as-a-judge methodology, which introduces potential biases and validity concerns
- Technical approach assumes therapeutic expertise can be meaningfully decomposed into distinct sub-skills without empirical validation
- Claims about capturing "nuance of real counseling" are not directly tested against human-generated sessions

## Confidence

- **High confidence**: Technical implementation of multi-agent framework and hierarchical coordination between CBT planning and turn-level technique selection
- **Medium confidence**: Evaluation results showing superior performance over baselines, though reliant on LLM-as-a-judge metrics
- **Low confidence**: Claims about capturing "nuance of real counseling" without direct comparison to human sessions

## Next Checks

1. Implement a more rigorous ablation study with statistical significance testing rather than qualitative expert rankings
2. Validate LLM-as-a-judge scores by having human counselors evaluate a subset of MAGneT-generated sessions using the same CTRS, WAI, and PANAS metrics
3. Examine coherence and therapeutic effectiveness of MAGneT sessions beyond 40 turns by analyzing whether CBT planning goals are consistently pursued throughout conversations