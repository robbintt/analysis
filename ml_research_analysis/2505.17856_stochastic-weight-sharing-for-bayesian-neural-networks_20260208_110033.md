---
ver: rpa2
title: Stochastic Weight Sharing for Bayesian Neural Networks
arxiv_id: '2505.17856'
source_url: https://arxiv.org/abs/2505.17856
tags:
- neural
- bayesian
- training
- learning
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a stochastic weight-sharing quantization method,
  2DGBNN, for Bayesian Neural Networks (BNNs) that leverages 2D adaptive Gaussian
  distributions, Wasserstein distance estimations, and alpha blending to encode the
  stochastic behavior of a BNN in a lower-dimensional, soft Gaussian representation.
  The method significantly reduces computational overhead by compressing model parameters
  by approximately 50x and reducing model size by 75%, while achieving accuracy and
  uncertainty estimations comparable to state-of-the-art on various computer vision
  benchmarks including CIFAR-10, CIFAR-100, and ImageNet1k.
---

# Stochastic Weight Sharing for Bayesian Neural Networks

## Quick Facts
- **arXiv ID:** 2505.17856
- **Source URL:** https://arxiv.org/abs/2505.17856
- **Reference count:** 30
- **Primary result:** Achieves 50× parameter compression and 75% model size reduction on BNNs while maintaining accuracy comparable to state-of-the-art on CIFAR-10/100 and ImageNet1k

## Executive Summary
This paper introduces 2DGBNN, a stochastic weight-sharing quantization method for Bayesian Neural Networks (BNNs) that achieves dramatic compression by clustering variational parameters in a 2D mean-variance space. The approach fits Gaussian Mixture Models to the $(\mu, \sigma)$ pairs of BNN weights, storing a small set of shared Gaussian distributions rather than individual parameters for each weight. By carefully handling "outlier" weights and using alpha-blending at cluster boundaries, the method maintains both accuracy and uncertainty quality while reducing parameters by 50× and model size by 75% across multiple computer vision benchmarks.

## Method Summary
The 2DGBNN method operates in three stages: First, a standard BNN is pre-trained to obtain initial mean and variance estimates for each weight. Second, weights are partitioned into "outliers" (those with high absolute mean or high gradient magnitude) and "inliers," with a GMM fitted to the 2D $(\mu, \sigma)$ space of inliers. The GMM is then refined by merging similar Gaussians using Wasserstein distance. Finally, the BNN is fine-tuned where inliers sample from shared GMM components and outliers retain individual distributions, with alpha-blending used for weights near cluster boundaries to prevent discontinuities.

## Key Results
- Achieves 50× parameter compression (0.02M parameters for ResNet-18 on CIFAR-10 vs 1.03M baseline)
- Reduces model size by 75% while maintaining accuracy within 1-2% of baseline
- Achieves competitive accuracy (90.92% on CIFAR-10 with ResNet-20) and uncertainty quality (ECE ~0.019) compared to state-of-the-art
- Successfully scales to large models including ResNet-101 and Vision Transformer on ImageNet1k

## Why This Works (Mechanism)

### Mechanism 1: 2D Parameter Space Clustering
The method reduces parameter count by clustering weights in the 2D $(\mu, \sigma)$ space rather than compressing weights directly. It fits a GMM to these 2D points and stores a small set of shared Gaussians ($K \ll n_w$) instead of individual distributions. The core assumption is that variational parameters form a distribution approximable by a finite Gaussian mixture. Break condition: If distributions are highly multi-modal, compression benefits diminish.

### Mechanism 2: Outlier-Based Fidelity Preservation
Critical weights are excluded from sharing through outlier detection (high $|\mu|$ or top 1% gradient magnitude). These retain unshared Gaussian parameters, ensuring weights important for output or undergoing significant updates aren't blurred by clustering. Core assumption: Important weights deviate significantly from bulk distribution. Break condition: If critical weights aren't captured by mean or gradient thresholds, the model may fail to converge.

### Mechanism 3: Stochastic Fidelity via Alpha-Blending
Alpha-blending and Wasserstein merging preserve stochastic fidelity at cluster boundaries. Weights between clusters are sampled via weighted averages of neighboring Gaussians, and redundant Gaussians are merged using Wasserstein distance to maintain distribution geometry. Core assumption: Hard assignments introduce quantization errors; soft assignments mitigate this. Break condition: Incorrect Mahalanobis distance thresholds make assignments costly without performance gain.

## Foundational Learning

- **Mean-Field Variational Inference (VI)**: Standard VI requires storing mean and variance for each weight, doubling parameters compared to deterministic networks. This paper's approach solves this bottleneck by clustering parameters.
  - *Quick check:* Can you explain why standard VI requires storing twice the number of parameters compared to a deterministic network?

- **Wasserstein Distance ($W_2$)**: Used to merge Gaussian clusters because it considers the geometry of distributions' locations, making it suitable for merging clusters in the $\mu-\sigma$ plane. Unlike KL divergence, it works well for non-overlapping or slightly overlapping Gaussians.
  - *Quick check:* Why is Wasserstein distance preferred over KL divergence for measuring the distance between two non-overlapping or slightly overlapping Gaussians in a clustering context?

- **Gaussian Mixture Models (GMM) & EM Algorithm**: The core compression fits a GMM to weight parameters. Understanding GMM's probability density assignments helps explain how "inlier" weights are reassigned via alpha-blending.
  - *Quick check:* In a GMM, how does the responsibility (posterior probability) of a cluster for a given data point relate to the "alpha-blending" weights mentioned in the paper?

## Architecture Onboarding

- **Component map:** Pre-training -> Outlier Detection -> GMM Learning -> Merging -> Fine-tuning
- **Critical path:** Outlier Detection threshold ($\tau_w$) and GMM initialization. Poor pre-trained weights or aggressive outlier thresholds cause GMM to fit noise or incorrectly compress critical weights.
- **Design tradeoffs:**
  - Compression vs. Accuracy: Increasing $\tau_w$ increases compression but likely reduces accuracy
  - Cluster Count ($K$): Higher $K$ preserves accuracy but increases memory and computation
- **Failure signatures:**
  - Accuracy Collapse: Too aggressive Wasserstein merging destroys model capacity
  - Memory Blowup: Too few outliers stores nearly as many parameters as original BNN
- **First 3 experiments:**
  1. Baseline Reproduction: Run 3-stage pipeline on CIFAR-10/ResNet-20, verify $K$ is reasonable (1.5k-2k) and accuracy within 1-2% of baseline
  2. Threshold Sweep: Vary $\tau_w$ (0.1, 0.2, 0.4) and plot trade-off curve between parameters and accuracy
  3. Ablation on Alpha-Blending: Disable alpha-blending (hard assignment only) to measure drop in NLL and ECE

## Open Questions the Paper Calls Out

- **Integration into fully Bayesian framework:** The paper notes future work will explore integrating the method into a fully Bayesian framework and applying further quantization to outlier weights. The current approach uses GMM approximation with independence assumptions that upper-bound true ELBO rather than providing exact Bayesian inference.

- **Index vector compression:** While Huffman coding or multi-level index tables could reduce index vector size by several factors, the paper leaves this investigation for future work. For small networks, the uint8 index vector creates significant storage overhead, making 2DGBNN larger than INT8 quantization despite fewer trainable parameters.

- **Optimal outlier proportion:** The paper reports approximately 1.8% of weights are generally allocated as outliers based on empirical observation, but the sensitivity to this choice across architectures and datasets remains unexplored.

## Limitations

- Compression efficacy critically depends on assumed Gaussian structure of variational parameters; non-Gaussian or highly multi-modal distributions may lose significant information
- Outlier detection thresholds (especially $\tau_w=0.2$) appear empirically chosen without theoretical justification for generalizability across architectures
- Wasserstein distance thresholds for merging Gaussians are sensitive hyperparameters not fully explored across datasets

## Confidence

- **High confidence:** Parameter compression ratio (50×) and model size reduction (75%) - directly computable from stored parameters
- **Medium confidence:** Accuracy preservation claims - reported on multiple benchmarks but dependent on specific outlier/threshold choices
- **Low confidence:** Uncertainty quality (ECE/NLL) claims - while reported, relationship between compression and calibration quality needs more systematic ablation

## Next Checks

1. **Threshold sensitivity analysis:** Systematically vary $\tau_w$ from 0.05 to 0.5 and measure the accuracy-compression trade-off curve to identify optimal operating points

2. **Distribution validation:** Test GMM fitting on non-vision datasets (e.g., time series) where weight distributions may deviate from 2D Gaussian assumptions

3. **Uncertainty degradation analysis:** Disable alpha-blending and measure resulting drop in NLL/ECE to quantify importance of soft assignments at cluster boundaries