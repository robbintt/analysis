---
ver: rpa2
title: Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas
arxiv_id: '2507.21103'
source_url: https://arxiv.org/abs/2507.21103
tags:
- para
- ncia
- como
- respostas
- bulas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an automated semantic analysis system for pharmaceutical
  package inserts using RAG (Retrieval-Augmented Generation) combined with Large Language
  Models (LLMs). The method integrates vector search via embeddings, semantic data
  extraction, and contextualized natural language generation.
---

# Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas

## Quick Facts
- **arXiv ID**: 2507.21103
- **Source URL**: https://arxiv.org/abs/2507.21103
- **Reference count**: 0
- **Key outcome**: Automated semantic analysis of pharmaceutical package inserts using RAG + LLM, achieving ~70% precision and ~3.5 completeness on 30 documents

## Executive Summary
This work proposes an automated semantic analysis system for pharmaceutical package inserts using RAG (Retrieval-Augmented Generation) combined with Large Language Models (LLMs). The method integrates vector search via embeddings, semantic data extraction, and contextualized natural language generation. Experiments used 30 public drug package inserts from Anvisa, applying semantic queries evaluated by accuracy, completeness, response time, and consistency metrics. Results showed both Gemini and OpenRouter-ModelGPT achieved ~70% precision and ~3.5 completeness on a 1-5 scale. Response times were within acceptable limits, and Kappa scores indicated substantial human evaluator agreement (0.78 for OpenRouter, 0.52 for Gemini). The approach effectively improves intelligent information retrieval and interpretation of complex unstructured technical texts, offering a promising solution for automated pharmaceutical document analysis.

## Method Summary
The system extracts text from 30 pharmaceutical package inserts using pdfplumber, segments content into 300-token chunks, and generates embeddings with all-MiniLM-L6-v2. These embeddings are indexed using FAISS for vector search. A hybrid retrieval approach combines vector similarity with keyword and regex patterns to retrieve relevant document chunks. Retrieved context is passed to LLM backends (Gemini and OpenRouter-ModelGPT) to generate natural language responses to 10 standardized queries covering indications, contraindications, posology, and adverse effects. The pipeline evaluates performance using precision, completeness (1-5 scale), response time, and inter-annotator consistency (Cohen's Kappa).

## Key Results
- Gemini and OpenRouter-ModelGPT both achieved approximately 70% precision in answering standardized queries
- Completeness scores averaged 3.5 out of 5 for both LLM backends
- Human evaluator agreement (Cohen's Kappa) reached 0.78 for OpenRouter and 0.52 for Gemini, indicating substantial consistency
- Response times remained within acceptable limits for practical deployment

## Why This Works (Mechanism)
The RAG architecture bridges the gap between document retrieval and semantic understanding by combining dense vector search with LLM-based response generation. The hybrid search strategy (vector + keyword + regex) ensures robust retrieval even when semantic embeddings may miss domain-specific terminology in Portuguese pharmaceutical texts.

## Foundational Learning
- **Vector embeddings for semantic search**: Transform text into numerical representations capturing meaning; needed to find relevant document chunks beyond exact keyword matching; quick check: measure retrieval precision with and without embeddings
- **Chunking strategy**: Segment documents into fixed-size blocks to balance semantic coherence and retrieval granularity; needed to prevent information loss in long documents; quick check: vary chunk sizes and measure retrieval accuracy
- **Hybrid search methodology**: Combine semantic vector search with traditional keyword/regex matching; needed to handle domain-specific terminology and abbreviations; quick check: compare hybrid vs. vector-only retrieval performance

## Architecture Onboarding

**Component Map**: PDF Extraction -> Text Chunking -> Embedding Generation -> FAISS Indexing -> Hybrid Retrieval -> LLM Response Generation

**Critical Path**: The bottleneck occurs at PDF extraction and text segmentation, as complex layouts can fragment information. The hybrid retrieval step is critical for ensuring relevant context reaches the LLM.

**Design Tradeoffs**: Fixed 300-token chunks balance semantic coherence with retrieval coverage but may split related information. The hybrid search adds regex complexity but improves recall for domain-specific terms at the cost of additional processing time.

**Failure Signatures**: Poor PDF extraction manifests as missing or garbled text blocks; low retrieval relevance indicates embedding misalignment with domain terminology; inconsistent human evaluations suggest unclear scoring rubrics or evaluator bias.

**First Experiments**: 1) Test PDF extraction on 3 complex documents to verify text integrity, 2) Run embedding generation on sample chunks to confirm semantic clustering, 3) Execute single query through complete pipeline to validate end-to-end functionality

## Open Questions the Paper Calls Out

**Open Question 1**: Can the RAG+LLM architecture maintain high precision while dynamically adapting semantic summarization levels for non-specialist patients versus medical professionals?
- **Basis in paper**: [Explicit] The conclusion explicitly identifies the "need to improve semantic summarization mechanisms and their adaptation for non-specialist publics."
- **Why unresolved**: The current study evaluated responses based on accuracy and completeness but did not test the system's ability to modulate technical language complexity for different audiences.
- **What evidence would resolve it**: A comparative study measuring readability scores (e.g., Flesch-Kincaid) and user satisfaction ratings from both laypeople and health professionals using the same underlying data.

**Open Question 2**: How does the system's retrieval accuracy and latency scale when applied to heterogeneous documents like clinical protocols and electronic health records compared to the standardized package inserts used?
- **Basis in paper**: [Explicit] The authors recommend "expanding the application to other types of technical and scientific documents in the health area, such as clinical protocols, regulations and electronic records."
- **Why unresolved**: The current experiment was limited to 30 standardized package inserts (bulas); electronic records typically contain noisier, unstructured data which may challenge the pdfplumber extraction and chunking strategy.
- **What evidence would resolve it**: Benchmarking the existing pipeline against a dataset of electronic health records (EHRs) to measure extraction error rates and RAG retrieval precision.

**Open Question 3**: To what extent does the "hybrid search" method (vector + keyword + regex) contribute to performance compared to vector search alone, and does it mitigate specific failure modes in pharmaceutical texts?
- **Basis in paper**: [Inferred] The methodology mentions a "hybrid search" combining FAISS with keyword/regex patterns, but the results section attributes performance generally to "RAG+LLM" without isolating the contribution of the hybrid component.
- **Why unresolved**: It is unclear if the ~70% precision is driven by semantic understanding or the regex safety net, which is critical for determining the system's generalizability to less structured documents.
- **What evidence would resolve it**: An ablation study comparing the "hybrid search" configuration against a vector-only search on the same queries to measure the delta in precision and completeness.

## Limitations
- Limited to 30 standardized Portuguese pharmaceutical package inserts, restricting generalizability to other document types and languages
- Human evaluation methodology lacks detailed scoring rubrics and evaluator training protocols
- 300-token chunking may fragment semantically coherent information across multiple chunks

## Confidence

**High Confidence**: The RAG architecture implementation using FAISS vector indexing with MiniLM embeddings is technically sound and reproducible based on the provided code repository and methodology description.

**Medium Confidence**: The reported evaluation metrics (precision, completeness, Kappa scores) appear internally consistent but lack sufficient methodological detail for full independent verification.

**Low Confidence**: The generalizability claims about effectiveness for "complex unstructured technical texts" extend beyond the single document type (pharmaceutical package inserts) and language (Portuguese) evaluated in this study.

## Next Checks

1. **Baseline Comparison Validation**: Implement and evaluate traditional keyword search and LLM-only approaches on the same 30-document corpus using identical queries to establish performance baselines for meaningful comparison.

2. **Chunk Size Optimization**: Conduct ablation studies varying chunk sizes (200, 300, 400 tokens) to identify optimal balance between semantic coherence and retrieval coverage, measuring impact on precision and completeness metrics.

3. **Cross-Document Generalization**: Test the system on 10-15 documents from different technical domains (e.g., medical device manuals, clinical trial reports) while maintaining Portuguese language, to assess architecture transferability beyond pharmaceutical package inserts.