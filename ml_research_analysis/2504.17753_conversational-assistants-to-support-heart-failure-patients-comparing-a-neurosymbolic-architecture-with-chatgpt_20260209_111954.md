---
ver: rpa2
title: 'Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic
  Architecture with ChatGPT'
arxiv_id: '2504.17753'
source_url: https://arxiv.org/abs/2504.17753
tags:
- system
- systems
- food
- hffood-gpt
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares a neurosymbolic dialogue system (HFFood-NS)
  with a ChatGPT-based system (HFFood-GPT) for helping heart failure patients monitor
  salt intake. A within-group user study with 20 African American heart failure patients
  was conducted to evaluate both systems' performance and usability.
---

# Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT

## Quick Facts
- arXiv ID: 2504.17753
- Source URL: https://arxiv.org/abs/2504.17753
- Reference count: 26
- This study compares neurosymbolic dialogue system (HFFood-NS) with ChatGPT-based system (HFFood-GPT) for heart failure patients monitoring salt intake.

## Executive Summary
This study evaluates two conversational assistants for helping heart failure patients track salt intake: a neurosymbolic system (HFFood-NS) and a ChatGPT-based system (HFFood-GPT). A within-group study with 20 African American heart failure patients found that HFFood-NS achieved higher accuracy (37% vs 24%) and task completion rates (84% vs 62%), while HFFood-GPT handled speech errors better and processed complex queries more flexibly. Despite these performance differences, patients showed no clear preference between systems, with 11 preferring HFFood-NS and 9 preferring HFFood-GPT. Both systems were considered helpful and most patients expressed willingness to use them again.

## Method Summary
The study compared two dialogue systems for heart failure patients to query salt content in food using the USFDC dataset. HFFood-NS used a PPTOD-based model for dialogue state tracking combined with neuro-symbolic rules for database lookup and calculations, with template-based responses. HFFood-GPT employed GPT-4 Assistant with retrieval enabled over the USFDC dataset. Both systems used the same speech interface (Whisper-1 for STT, TTS-1 for TTS). The within-subject study with 20 patients evaluated task completion, accuracy, slot accuracy, WER, processing time, response length, and user preference through pre/post surveys.

## Key Results
- HFFood-NS achieved 37% accuracy vs 24% for HFFood-GPT
- HFFood-NS completed 84% of tasks vs 62% for HFFood-GPT
- HFFood-NS averaged 14.5 words per response vs 54.5 for HFFood-GPT
- HFFood-GPT made fewer speech errors (WER 0.41 vs 0.48) and handled complex queries better
- No clear user preference: 11 preferred HFFood-NS, 9 preferred HFFood-GPT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neuro-symbolic integration improves accuracy for structured domain tasks by delegating calculation and retrieval to deterministic rules.
- Mechanism: A T5-based model (PPTOD) predicts dialogue states (slots), while symbolic rules fetch values from a structured database (USFDC) and perform unit conversions.
- Core assumption: Slot predictions from the neural model are sufficiently accurate for symbolic rules to act on.
- Evidence anchors:
  - [abstract] "The evaluation shows that the in-house system is more accurate... and is less verbose than the one based on ChatGPT"
  - [section 3.1.1] "This integration significantly enhances system performance, achieving a 20% improvement in joint goal accuracy"
  - [corpus] Weak external validation; no corpus papers directly replicate neuro-symbolic combinations in healthcare dialogue.
- Break condition: If the neural slot predictor fails on domain-specific vocabulary, the symbolic layer cannot recover.

### Mechanism 2
- Claim: Slot-based dialogue state tracking enables transparent error analysis and fail-safe handling.
- Mechanism: The DST module maintains explicit slot representations (Food, Cook, Type, FoodWeight, Metric). When slots are unfilled, the system queries up to two times; if FoodWeight remains unknown, it defaults to 100g.
- Core assumption: Users can provide slot values in recognizable forms.
- Evidence anchors:
  - [abstract] "HFFood-NS... completed more tasks (84% vs 62%)"
  - [section 5.1, Table 2] Slot accuracy analysis shows Foodweight and Metric slots had highest error counts (56 and 60 incorrect respectively)
  - [corpus] Not validated externally; slot-based DST is a standard approach but not specifically tested in this domain.
- Break condition: When user descriptions diverge from expected slot vocabularies, the slot predictor fails.

### Mechanism 3
- Claim: LLM-based systems handle linguistic variation and complex queries better but sacrifice consistency and controllability.
- Mechanism: GPT-4 uses retrieval-augmented generation with the USFDC dataset. Its conversational flexibility allows it to interpret non-standard inputs, but prompt constraints are frequently violated.
- Core assumption: The retrieval augmentation grounds responses in the provided dataset.
- Evidence anchors:
  - [abstract] "ChatGPT-based system made fewer speech errors... and handled complex queries better"
  - [section 6.2] "HFFood-GPT demonstrated flexibility in handling these inputs, whereas HFFood-NS struggled with slot identification"
  - [section 6.2] "We limited responses to 40 words, but... the system averaged 54.5 words per response"
  - [corpus] Related work explores ChatGPT for heart failure dialogues but doesn't evaluate this specific trade-off.
- Break condition: When the LLM ignores prompt constraints or accesses external knowledge, system behavior becomes unpredictable.

## Foundational Learning

- Concept: Task-oriented Dialogue Systems (ToDS) pipeline
  - Why needed here: HFFood-NS follows the classic NLU → DST → Dialog Manager → NLG pipeline. Understanding this decomposition is prerequisite to modifying or debugging the system.
  - Quick check question: Name the four standard ToDS pipeline components and the role of DST.

- Concept: Slot Filling and Slot Accuracy
  - Why needed here: Both systems are evaluated on slot accuracy; HFFood-NS uses slots explicitly while HFFood-GPT is retroactively evaluated against them.
  - Quick check question: Given user input "I ate 2 pounds of fried chicken," identify which slots are filled and which remain empty.

- Concept: Neuro-symbolic AI
  - Why needed here: HFFood-NS combines neural predictions with symbolic rules. Understanding when to use each is critical for system design.
  - Quick check question: Why would a symbolic rule outperform a neural model for unit conversion (e.g., pounds to grams)?

## Architecture Onboarding

- Component map:
  - HFFood-NS: Speech (Whisper-1) → DST (PPTOD-based) → Symbolic Rules (DB lookup + math) → Template NLG → Speech (TTS-1)
  - HFFood-GPT: Speech (Whisper-1) → GPT-4 Assistant (retrieval + code interpreter) → Speech (TTS-1)
  - Shared: USFDC dataset, Gradio UI, same STT/TTS stack

- Critical path:
  - HFFood-NS: Slot prediction accuracy → DB query success → Math correctness → Template response
  - HFFood-GPT: Prompt adherence → Retrieval grounding → Response conciseness

- Design tradeoffs:
  - Accuracy vs. flexibility: HFFood-NS is more accurate (37% vs 24%) but less flexible; HFFood-GPT handles varied inputs but is less reliable.
  - Verbosity vs. completeness: HFFood-NS averages 14.5 words; HFFood-GPT averages 54.5 words despite 40-word prompt constraint.
  - Error transparency: HFFood-NS enables detailed error analysis; HFFood-GPT is a black box.

- Failure signatures:
  - HFFood-NS: "Missed Slot" (27 instances), "Wrong Food Identified" (9), "Wrong Math" (5) — check DST confidence and rule implementation.
  - HFFood-GPT: Prompt violations (word count, multi-question turns), DB references exposed to users (38.5% of utterances), inconsistent units (g, mg, tsp, categorical).

- First 3 experiments:
  1. Ablation test: Run HFFood-NS without symbolic rules (pure PPTOD) to quantify the 20% joint accuracy contribution claimed.
  2. Prompt robustness test: Vary HFFood-GPT prompt constraints (word limit, question limits) and measure compliance rates.
  3. Error classification on held-out data: Collect novel food descriptions and compare slot filling success rates between systems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the strengths of neurosymbolic architectures and LLM-based systems be optimally combined into a hybrid conversational system for healthcare applications?
- Basis in paper: [explicit] Authors state: "Moving forward, we aim to develop hybrid conversational systems that combine the strengths of both systems."
- Why unresolved: The paper identifies complementary strengths but does not propose or test any integration architecture.
- What evidence would resolve it: A prototype hybrid system evaluated against both original systems on the same metrics with the same population, demonstrating superior or balanced performance across accuracy, usability, and flexibility.

### Open Question 2
- Question: What factors beyond task accuracy and completion drive user preference in healthcare dialogue systems?
- Basis in paper: [inferred] Despite significant performance differences (37% vs 24% accuracy, 84% vs 62% task completion), patients showed no clear preference (11 vs 9 split), suggesting unmeasured factors influence preference.
- Why unresolved: The study did not systematically identify which qualitative aspects predict user preference when quantitative performance differs substantially.
- What evidence would resolve it: Regression or mediation analysis linking specific qualitative interaction features to user preference across a larger sample.

### Open Question 3
- Question: How do these findings generalize to non-hospitalized heart failure patients and broader demographic groups?
- Basis in paper: [inferred] Study limitations section notes only 20 African American hospitalized patients were recruited; the specific population may not represent outpatients or other ethnic groups.
- Why unresolved: No data exists on whether the accuracy-usability trade-offs hold in home settings, with different accents/dialects, or across populations with varying health literacy levels.
- What evidence would resolve it: Replication studies with outpatients, diverse ethnic groups, and varied clinical settings using the same within-subject comparison protocol.

### Open Question 4
- Question: Does stated willingness to use such systems translate to sustained real-world adoption and improved self-care behaviors?
- Basis in paper: [inferred] Post-survey showed most patients would use the system again and recommend it, but the study captured only immediate impressions during a single hospital interaction, not longitudinal behavioral outcomes.
- Why unresolved: Hawthorne effects and social desirability bias may inflate reported willingness; actual dietary salt reduction behavior change remains unmeasured.
- What evidence would resolve it: Longitudinal deployment study tracking actual usage patterns, dietary changes, and clinical outcomes over weeks or months.

## Limitations

- Small sample size (n=20) limits generalizability of user preference findings
- Study population restricted to African American heart failure patients at one medical center
- Template-based conversational dataset for HFFood-NS training is not fully described, making exact replication difficult
- ChatGPT-based system frequently violated prompt constraints (word limits, question limits)

## Confidence

**High Confidence:** The core finding that HFFood-NS achieved higher task completion (84% vs 62%) and accuracy (37% vs 24%) compared to HFFood-GPT is well-supported by the experimental data and statistical analysis.

**Medium Confidence:** User preference results showing 11 preferring HFFood-NS and 9 preferring HFFood-GPT, with no clear winner, are supported but should be interpreted cautiously due to sample size.

**Low Confidence:** The assertion that HFFood-GPT's linguistic flexibility represents a fundamental advantage over HFFood-NS is based on limited observations and doesn't account for the trade-off with accuracy and consistency.

## Next Checks

1. **Ablation Study on DST Performance:** Conduct controlled experiments removing the neuro-symbolic rules from HFFood-NS to quantify the exact contribution of the symbolic layer to the claimed 20% improvement in joint goal accuracy.

2. **Prompt Constraint Compliance Testing:** Systematically vary the prompt constraints for HFFood-GPT (word limits, question limits, DB-only instructions) across multiple runs to measure compliance rates and identify which constraints are most frequently violated.

3. **Error Analysis on Novel Inputs:** Collect and test a diverse set of novel food descriptions (regional dishes, brand names, non-standard quantities) to compare how each system handles inputs outside the training distribution, focusing on slot filling accuracy and response quality.