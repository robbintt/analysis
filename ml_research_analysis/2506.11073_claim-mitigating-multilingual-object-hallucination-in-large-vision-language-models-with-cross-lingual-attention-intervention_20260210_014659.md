---
ver: rpa2
title: 'CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language
  Models with Cross-Lingual Attention Intervention'
arxiv_id: '2506.11073'
source_url: https://arxiv.org/abs/2506.11073
tags:
- attention
- lvlms
- language
- english
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multilingual object hallucination in large
  vision-language models (LVLMs), where models generate responses inconsistent with
  visual input when processing non-English queries. The authors propose CLAIM, a near
  training-free method that aligns cross-lingual attention patterns by identifying
  language-specific attention heads, estimating language shift vectors from English
  to target languages, and intervening in attention outputs during inference.
---

# CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention

## Quick Facts
- arXiv ID: 2506.11073
- Source URL: https://arxiv.org/abs/2506.11073
- Reference count: 15
- Primary result: CLAIM achieves 13.56% average improvement on POPE benchmark and 21.75% on MME hallucination subsets across multiple languages

## Executive Summary
CLAIM addresses multilingual object hallucination in LVLMs where models generate responses inconsistent with visual input when processing non-English queries. The method identifies language-specific cross-modal attention heads, estimates shift vectors from English to target languages, and intervenes in attention outputs during inference. CLAIM achieves significant improvements across multiple languages (up to 30% in Spanish) by aligning cross-lingual attention patterns, effectively mitigating both object-level and attribute-level hallucinations while being near training-free.

## Method Summary
CLAIM works by first training linear SVM probes on attention outputs to identify language-specific cross-modal heads (those that integrate visual and linguistic information differently across languages). The method then estimates shift vectors by computing the difference between English and target-language attention outputs on identical images with semantically equivalent caption queries. During inference on non-English queries, these shift vectors are added to the attention outputs of identified heads. The intervention is applied selectively to intermediate layers (10-17) where language-specific cross-modal attention is most prominent, with the number of heads (K) and intervention intensity (α) tuned per language and model.

## Key Results
- 13.56% average improvement on POPE benchmark across 6 languages
- 21.75% improvement on MME hallucination subsets (existence, count, color, position)
- Up to 30% improvement in Spanish, with consistent gains across low and high-resource languages
- Analysis reveals multilingual attention divergence is most prominent in intermediate layers (10-17)

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Attention Divergence Correction
Non-English queries cause LVLMs to attend less to relevant image regions compared to English queries. CLAIM corrects this by computing shift vectors as the difference between English and target-language attention outputs on identical images with semantically equivalent caption queries. During inference, this vector is added to the attention outputs of identified language-specific cross-modal heads, effectively aligning non-English attention patterns to English patterns that better ground visual information.

### Mechanism 2: Intermediate Layer Specialization
Language-specific cross-modal attention heads are concentrated in intermediate layers (10-17), making them optimal intervention targets. Probes trained to classify language from attention outputs achieve highest accuracy at these layers, with peak discriminability around layer 14-21. This concentration suggests intermediate layers integrate visual and linguistic information, and disrupting their language-specific behavior affects downstream generation.

### Mechanism 3: Shift Vector Generalization Across Languages
A single language-pair shift vector (e.g., English→Chinese) can generalize to improve performance on unseen languages. Mono-Shift experiments show that applying a shift derived from English-Chinese pairs improves performance on Russian, Spanish, and other languages, suggesting shared cross-lingual attention structures among non-English languages.

## Foundational Learning

- **Cross-modal attention in LVLMs**: Understanding how visual tokens and text tokens interact via self-attention is prerequisite to identifying which heads are "cross-modal" and language-sensitive. Quick check: Given concatenated visual embeddings P and text embeddings T as input to a decoder layer, which attention patterns indicate the model is grounding text in image regions?

- **Probing classifiers for mechanistic interpretability**: The method trains linear probes to identify attention heads that behave differently across languages. Understanding probing methodology is essential to replicate head identification. Quick check: If a probe trained on attention outputs achieves high classification accuracy for language labels, what does that imply about the information encoded in those heads?

- **Training-free intervention methods**: CLAIM is explicitly near training-free. Understanding the trade-offs of inference-time intervention vs. fine-tuning clarifies when to apply this approach. Quick check: Compared to fine-tuning on multilingual data, what are the computational cost and potential failure modes of modifying attention outputs at inference time?

## Architecture Onboarding

- **Component map**: Visual encoder → Feature projector → Language decoder (LLM) → Attention heads per layer
- **Critical path**: 1) Sample images with paired English/target-language caption queries; 2) Forward pass to extract attention outputs O for last input token at each head; 3) Train linear SVM probes to classify language from masked attention outputs (visual tokens only); 4) Select top-K heads with highest probe accuracy; 5) Compute shift vector S = mean(O_english - O_target) for each selected head; 6) During inference on non-English queries: add α·S to attention outputs before projection
- **Design tradeoffs**: K (number of heads) affects intervention strength vs. noise; α (intervention intensity) balances effect vs. disruption; Language-specific vs. unified shift trades accuracy vs. simplicity; Calibration data as few as N=50 samples may add noise
- **Failure signatures**: Model outputs become incoherent or repetitive (α too large); No improvement over baseline (K too small or heads not cross-modal); Performance degrades on high-resource languages while improving low-resource (over-correction); Method inapplicable (model doesn't expose attention outputs or treats tokens asymmetrically)
- **First 3 experiments**: 1) Probe accuracy heatmap: Train probes on all heads across layers for a language pair; visualize which layers/heads have highest accuracy; 2) Ablation on K and α: Grid search on POPE-COCO popular subset; identify stable operating region; 3) Cross-dataset generalization: Apply shift vectors estimated on COCO to MME hallucination subsets; verify improvement persists

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of shift vectors across vastly different language families remains untested, particularly for languages with non-Latin scripts or fundamentally different syntactic structures
- Method assumes English visual perception capabilities are transferable, but may fail if English training data contains its own biases or hallucinations
- Layer-specific effectiveness (intermediate layers 10-17) may vary significantly across different LVLM architectures

## Confidence

| Claim | Confidence |
|-------|------------|
| Cross-lingual attention divergence exists and correcting it improves multilingual object hallucination | High |
| Intermediate layers are the optimal intervention targets | Medium |
| Single shift vectors can generalize across languages | Medium |
| No negative impact on non-hallucination capabilities | Low |

## Next Checks
1. Test shift vector transferability on typologically distant languages (Arabic, Japanese, Hindi) to validate cross-lingual generalization assumptions
2. Evaluate performance degradation on non-hallucination tasks (VQA reasoning, counting, complex reasoning) to ensure capability preservation
3. Apply CLAIM to alternative LVLM architectures (Llama-3-V, IDEFICS) to verify layer-specific intervention effectiveness is not architecture-dependent