---
ver: rpa2
title: Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge
arxiv_id: '2601.13649'
source_url: https://arxiv.org/abs/2601.13649
tags:
- language
- answer
- bias
- languages
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates language bias in pairwise LLM-as-a-judge
  systems, focusing on performance disparities between languages when judging same-language
  pairs and preferences when judging cross-language pairs. The authors use the MMMLU
  dataset with 14 languages and evaluate multiple judge models including GPT-5.1,
  various Qwen and Llama models, Aya-Expanse, and expert judge models.
---

# Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2601.13649
- Source URL: https://arxiv.org/abs/2601.13649
- Authors: Xiaolin Zhou; Zheng Luo; Yicheng Gao; Qixuan Chen; Xiyang Hu; Yue Zhao; Ruishan Liu
- Reference count: 40
- Key outcome: Pairwise LLM-as-a-judge systems exhibit significant language bias, with European languages outperforming African languages in same-language judging, and a strong preference for English answers in cross-language judging that cannot be fully explained by perplexity.

## Executive Summary
This paper investigates language bias in pairwise LLM-as-a-judge systems using the MMMLU dataset across 14 languages. The authors find significant performance disparities when evaluating same-language pairs, with European languages consistently outperforming African languages, particularly in culturally-related subjects. In cross-language judging, most models exhibit a strong preference for English answers regardless of correctness, with answer language having more influence than question language. Critically, the authors demonstrate that language bias is not merely an artifact of low-perplexity bias, as language identity explains significant additional variance beyond perplexity alone, especially for low-resource languages.

## Method Summary
The study evaluates language bias in pairwise LLM-as-a-judge across two settings: same-language judging (measuring performance disparities) and inter-language judging (measuring preference for high-resource languages). Using MMMLU with 14 languages, the authors preprocess questions to create pairwise comparisons (ground truth vs. cyclic adjacent incorrect answer) and run four configurations per target language with position-swapping. They collect judge preferences and implement a 3-step masked perplexity pipeline to isolate answer-specific perplexity. The analysis uses linear regression to decompose variance, comparing reduced models with perplexity alone versus full models adding language indicators, and computes F-statistics to test significance.

## Key Results
- Same-language judging shows European languages consistently outperform African languages, with this bias more pronounced in culturally-related subjects.
- Inter-language judging reveals most models favor English answers regardless of correctness, with answer language having more influence than question language.
- Language identity explains significant additional variance beyond perplexity alone (R² improvement), demonstrating bias reflects deeper representational asymmetries beyond fluency.
- M-Prometheus, a multilingual judge model, shows different behavior than other models, suggesting fine-tuning can mitigate some bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In pairwise LLM-as-a-judge settings, judge models exhibit a **performance disparity** across language families when evaluating same-language pairs.
- Mechanism: Judge model's ability to distinguish correct from incorrect answers varies systematically by language region. Representational quality is higher for European languages than African languages due to training data volume and quality differences. For low-resource languages like Yoruba, weaker internal representations lead to less accurate evaluation, amplified for culturally-requiring subjects.
- Core assumption: Judge accuracy correlates with quality of model's internal language representations shaped by pre-training data.
- Evidence anchors:
  - [abstract] "significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects."
  - [Page 3, Section 4.1] "Model performance strongly correlates with region of language. We observe that the languages with highest average model performances are all European languages... followed by Asian languages... and finally African languages (SW, YO)."
  - [corpus] Related work (MyCulture paper) confirms LLMs exhibit cultural biases due to training data dominated by high-resource languages.
- Break condition: Would break if performance disparities disappear when controlling for answer quality, or if solely explained by answer perplexity.

### Mechanism 2
- Claim: In inter-language judging, judge models exhibit an **answer-language preference bias**, systematically favoring answers in high-resource languages (especially English) regardless of correctness.
- Mechanism: When presented with answers in different languages, judge model's preference is influenced more by answer language than content. High-resource languages like English have lower perplexity and are associated with higher-quality training data, leading model to preferentially select them even when incorrect. Question language has secondary, sometimes opposite effect.
- Core assumption: Model learns spurious correlation between language identity (or features like perplexity) and answer quality during pre-training.
- Evidence anchors:
  - [abstract] "most models favor English answers, with answer language having more influence than question language."
  - [Page 5, Section 5.2] "the answer effect is mostly positive, which means that the English answers is undesirably preferred, regardless of whether they are truly correct."
  - [corpus] No direct corpus paper validates this specific inter-language preference, but related work discusses preference biases in LLM-as-a-Judge broadly.
- Break condition: Would break if preference for English disappears when using judge model specifically fine-tuned on balanced multilingual data.

### Mechanism 3
- Claim: **Language bias is not fully reducible to low-perplexity bias**; language identity provides additional explanatory power.
- Mechanism: While prior hypothesis suggests models prefer low-perplexity options and high-resource languages have lower perplexity, the paper shows that adding explicit language identity features to regression model significantly improves ability to predict preference (R² improvement). This indicates language bias stems from deeper representational and evaluative asymmetries, not just fluency.
- Core assumption: "Style-fused" perplexity collection pipeline successfully isolates perplexity related to answer content from framing effects.
- Evidence anchors:
  - [abstract] "perplexity is slightly correlated with language bias, regression and F-test analyses show that language identity explains significant additional variance beyond perplexity alone, especially for low-resource languages."
  - [Page 7, Section 6.2] "This further demonstrates that language bias exists even when log perplexity difference between options are the same, which indicates that there exists language bias beyond perplexity."
  - [Page 8, Figure 8] Shows R² decomposition, with large "language influence" component for low-resource languages like Yoruba.
  - [corpus] Related work on LLM-as-a-Judge discusses position bias but doesn't specifically refute this mechanism.
- Break condition: Would break if flaws in perplexity collection pipeline introduced systematic noise that language identity variable spuriously captured.

## Foundational Learning

- Concept: **LLM-as-a-Judge (pairwise paradigm)**
  - Why needed here: Entire paper evaluates this specific evaluation framework. Understanding difference between pointwise, pairwise, and listwise judging is foundational.
  - Quick check question: Can you explain why pairwise comparison ("Which answer is better, 1 or 2?") is more susceptible to position bias than pointwise score assignment?

- Concept: **Perplexity as proxy for model "familiarity" or "surprise"**
  - Why needed here: Key concept behind "low-perplexity bias" hypothesis that paper tests. Must understand what perplexity measures to follow causal argument in Section 6.
  - Quick check question: If model assigns lower perplexity to response in English than one in Yoruba, what does that fundamentally indicate about its training data?

- Concept: **Linear Regression with Categorical Variables (One-Hot Encoding) for Feature Attribution**
  - Why needed here: Core analysis in Section 6.3 uses this method to quantify variance in preference explained by "language identity" (categorical feature) beyond "perplexity difference" (continuous feature).
  - Quick check question: In F-test described in Appendix F, what does null hypothesis represent? What does significant F-statistic allow you to conclude?

## Architecture Onboarding

- Component map:
  1. Dataset Processor: Filters and preprocesses MMMLU, creating same-language and cross-language question-answer pairs
  2. Judge Model Interface: Wrapper that prompts LLM with pairwise comparison template, handles position-swapping for bias control
  3. Perplexity Pipeline: Multi-step system (Collect Natural Response -> Fuse Style -> Compute Masked Perplexity) for isolating answer-specific perplexity
  4. Analysis Engine: Runs linear regressions and F-tests to decompose variance and test perplexity hypothesis

- Critical path:
  1. Define experiment (same-language vs. inter-language) and languages
  2. For each (question, answer) pair, run Judge Model Interface to get preference decision
  3. For same pairs, run Perplexity Pipeline to get perplexity score
  4. Aggregate all decisions and scores
  5. Feed aggregated data into Analysis Engine to compute performance disparities, preference metrics, and variance decomposition

- Design tradeoffs:
  - Pipeline Complexity vs. Noise: Perplexity pipeline is complex and relies on GPT-4.1 for style fusion, introducing potential noise but necessary to avoid confounding framing style with answer content
  - Cost vs. Thoroughness: Position-swapping doubles inference cost; paper notes couldn't do this for GPT-5.1 due to budget
  - Generality vs. Specificity: Using 14 languages provides breadth, but cultural nuance is coarse-grained (e.g., "Africa" represented by only Swahili and Yoruba)

- Failure signatures:
  - Position bias overwhelms signal: If model has strong position bias and you don't swap positions, results are invalid
  - Perplexity conflation: If perplexity pipeline fails to separate answer content from model response style, analysis will incorrectly attribute stylistic preference to language identity
  - Prompt template leakage: If prompt fails to explicitly instruct model to ignore language, you may measure combination of language bias and instruction-following failure

- First 3 experiments:
  1. Reproduce Key Performance Gap: Replicate same-language judging experiment for single high-resource (French) and low-resource (Yoruba) language using small open-source model. Verify performance disparity exists as baseline.
  2. Ablate Perplexity Hypothesis: Using same model and single language pair (English-Yoruba), run simple perplexity check (naive concatenation) and style-fused perplexity pipeline. Compare correlation with preference to see if pipeline matters.
  3. Test Intervention - Explicit Debiasing Prompt: Modify prompt template to more strongly emphasize ignoring language (e.g., provide few-shot examples of cross-language judging) and measure if this reduces answer-language effect for inter-language setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does explicit chain-of-thought reasoning affect language bias in LLM-as-a-judge systems, particularly in relationship between perplexity and preference?
- Basis in paper: [explicit] Authors state: "Most recent LLMs include reasoning, which could potentially influence the output of LLM-as-a-judge... we did not consider the potential impact of reasoning in our perplexity experiments."
- Why unresolved: Experiments included GPT-5.1 with reasoning mode but did not isolate or analyze reasoning's effect on language bias; perplexity experiments excluded reasoning considerations entirely.
- What evidence would resolve it: Controlled comparison of language bias magnitude with and without reasoning, and whether reasoning amplifies or mitigates perplexity-independent language effects.

### Open Question 2
- Question: What mitigation strategies can effectively reduce language bias in pairwise LLM-as-a-judge without compromising judgment quality?
- Basis in paper: [inferred] Paper systematically documents language bias but does not test any debiasing interventions; conclusion states "language bias remains a fundamental limitation of current pairwise LLM-as-a-judge systems."
- Why unresolved: Work focuses on characterizing and quantifying bias rather than remediation.
- What evidence would resolve it: Experiments testing interventions (e.g., multilingual fine-tuning, calibration, prompt engineering) that measurably reduce accuracy gaps across language families.

### Open Question 3
- Question: What are specific representational asymmetries in judge models that cause language bias beyond perplexity, and how do they arise during training?
- Basis in paper: [explicit] Authors conclude: "language bias is not merely an artifact of fluency or likelihood, but reflects deeper representational and evaluative asymmetries within judge models." Paper demonstrates this exists but does not identify underlying mechanisms.
- Why unresolved: Regression and F-test analyses establish that language identity explains variance beyond perplexity, but internal model representations were not probed.
- What evidence would resolve it: Probing experiments on model embeddings or attention patterns that correlate with language-specific bias, or ablation studies identifying training data characteristics that predict bias magnitude.

## Limitations
- Reliance on GPT-4.1 for style-fusion step in perplexity pipeline introduces potential noise and opacity
- Dataset filtering criteria not fully specified, which could affect reproducibility
- Evaluation covers 14 languages but may not capture all relevant linguistic and cultural nuances, particularly for African languages represented by only two languages

## Confidence
- High confidence: Existence of performance disparities across language families in same-language judging (Mechanism 1)
- Medium confidence: Answer-language preference mechanism in inter-language judging (Mechanism 2)
- Medium confidence: Conclusion that language bias extends beyond perplexity (Mechanism 3)

## Next Checks
1. Reproduce the performance gap for a single high-resource (French) and low-resource (Yoruba) language pair using an open-source model
2. Compare simple perplexity correlation with the style-fused pipeline results for a single language pair
3. Test whether explicitly debiasing prompts (with few-shot examples) reduce the answer-language effect in inter-language judging