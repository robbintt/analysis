---
ver: rpa2
title: 'BAGEL: Projection-Free Algorithm for Adversarially Constrained Online Convex
  Optimization'
arxiv_id: '2502.16744'
source_url: https://arxiv.org/abs/2502.16744
tags:
- convex
- algorithm
- regret
- have
- oracle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational challenge of projection-based
  methods for Constrained Online Convex Optimization (COCO), which achieve O(T^{1/2})
  regret but require expensive projection operations. To overcome this, the authors
  propose BAGEL, a projection-free algorithm that leverages a Separation Oracle (SO)
  instead of traditional projection oracles.
---

# BAGEL: Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization

## Quick Facts
- **arXiv ID**: 2502.16744
- **Source URL**: https://arxiv.org/abs/2502.16744
- **Reference count**: 40
- **Primary result**: Projection-free algorithm achieving O(T^{1/2}) regret and O(T^{1/2}logT) cumulative constraint violation for convex cost functions in constrained online convex optimization

## Executive Summary
This paper addresses the computational bottleneck in Constrained Online Convex Optimization (COCO) where projection-based methods achieve O(T^{1/2}) regret but require expensive projection operations. The authors propose BAGEL, a projection-free algorithm that leverages a Separation Oracle instead of traditional projection oracles. By combining an adaptive step-size scheme with a surrogate cost function that incorporates cumulative constraint violations, BAGEL achieves the same O(T^{1/2}) regret and O(T^{1/2}logT) cumulative constraint violation bounds as projection-based methods while maintaining computational efficiency. The algorithm is particularly effective for strongly convex cost functions, achieving O(logT) regret and O(T^{1/2}logT) cumulative constraint violation with appropriate parameter choices.

## Method Summary
BAGEL operates by maintaining a "virtual" point that is iteratively updated using Online Gradient Descent with adaptive step-sizes. Instead of projecting this point onto the constraint set K (which would be computationally expensive), the algorithm uses an Infeasible Projection via Separation Oracle (IP-SO). When the virtual point lies outside K, the Separation Oracle returns a separating hyperplane, and the algorithm takes small steps opposite to the normal vector until the point lies within a shrunk version of the set. The key innovation is a surrogate loss function that dynamically weights constraint violations using a convex potential function, ensuring that constraint satisfaction is prioritized exactly when cumulative violations become critical. This mechanism, combined with the adaptive step-size, prevents the number of SO calls from exploding while maintaining the optimal convergence rates.

## Key Results
- Achieves O(T^{1/2}) regret and O(T^{1/2}logT) cumulative constraint violation for convex cost functions
- Matches the O(T^{1/2}) regret and cumulative constraint violation bounds of projection-based methods when β=1/2
- Requires O(T^{2β}) calls to the separation oracle where β ∈ [0,1/2] is a trade-off parameter
- For strongly convex cost functions, achieves O(logT) regret and O(T^{1/2}logT) cumulative constraint violation
- Establishes a regime where projection-free methods can attain the same convergence rates as projection-based counterparts while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Separation Oracle Over Projection
- **Claim**: Replacing the Projection Oracle (PO) with a Separation Oracle (SO) allows the algorithm to remain projection-free while achieving faster convergence rates than Linear Optimization Oracle (LOO) methods, provided the geometry permits an efficient SO.
- **Mechanism**: Instead of computing the Euclidean projection onto the convex set K, the algorithm uses an Infeasible Projection via Separation Oracle (IP-SO). If a proposed point y is outside K, the SO returns a separating hyperplane. The algorithm iteratively takes small steps opposite to the normal vector of this hyperplane until the point lies within a shrunk version of the set (K_δ).
- **Core assumption**: The action set K admits an efficient Separation Oracle, and the ratio of the enclosing to enclosed ball (geometry condition number) is not prohibitively large.
- **Evidence anchors**: [abstract] "...leveraging an infeasible projection via SO, we can match the time-horizon dependence of projection-based methods..."; [section 3] "In contrast, the SO is expensive for B_*... but efficient for B_2 [spectral norm ball] since... [it] requires only the top singular vector pair."

### Mechanism 2: Surrogate Loss with Adaptive Penalty
- **Claim**: Using a surrogate loss function that dynamically weights constraint violations ensures constraint satisfaction is prioritized exactly when cumulative violations become critical.
- **Mechanism**: The algorithm constructs a surrogate function ̂f_t = f̃_t + Φ'(Q_t)g̃_t, where Q_t is the cumulative constraint violation and Φ is a convex potential function. As violations accumulate (Q_t grows), the gradient Φ'(Q_t) increases, effectively scaling the penalty for further violations. This forces the base Online Gradient Descent (OGD) algorithm to shift focus from minimizing cost to reducing violation.
- **Core assumption**: The potential function Φ is convex, non-decreasing, and differentiable (e.g., exponential or quadratic).
- **Evidence anchors**: [section 5] "BAGEL... combines costs and constraints into a single surrogate cost function... [with] cumulative constraint violation as weights."; [proof sketch] "Since Φ(·) is convex... ̂f_t(x*) ≥ f̃_t(x*) + Φ'(Q_t)g̃_t(x*) + Φ(Q_t) - Φ(Q_{t-1})."

### Mechanism 3: Adaptive Step-Size for Oracle Efficiency
- **Claim**: An adaptive step-size scheme is strictly required to prevent the number of SO calls from exploding to O(T^2), making the algorithm computationally tractable.
- **Mechanism**: Standard constant step-sizes cause the "virtual" gradient steps to travel too far, requiring many SO iterations to project back. BAGEL uses a step-size η_m ∝ 1/√(ε + ∑∥∇τ∥²). This dampens the step length when gradients are large (e.g., when the surrogate penalty term explodes), ensuring the distance to the shrunk set decreases rapidly and limiting the total number of SO queries.
- **Core assumption**: The constraint constant ε > 0 is maintained to stabilize the denominator; if ε = 0, bounds degrade or require different analysis.
- **Evidence anchors**: [section 4] "...this seemingly minor modification is critical for computational feasibility... reduces the total number of calls... from a prohibitive O(T^2) to a near-linear Õ(T)."; [remark 2] "...if we only assume that ε ≥ 0... the total number of calls... would be bounded by O((D/r)^2 T K^(-1) δ^(-2))."

## Foundational Learning

- **Concept: Oracle Complexity in Projection-Free OCO**
  - **Why needed here**: The paper's central thesis is trading off oracle strength (LOO vs. SO) for better convergence rates (T^{3/4} vs. T^{1/2}). You must understand the cost difference between minimizing a linear function (LOO), finding a separating hyperplane (SO), and Euclidean projection (PO).
  - **Quick check question**: Given a spectral norm ball, which oracle is more efficient: Separation or Linear Optimization?

- **Concept: Regret vs. Cumulative Constraint Violation (CCV)**
  - **Why needed here**: BAGEL optimizes a bi-criteria objective. You need to distinguish between competing against the best fixed action in hindsight (Regret) and the total magnitude of constraint breaches over time (CCV).
  - **Quick check question**: If an algorithm has O(√T) Regret but O(T) CCV, is it suitable for safety-critical applications?

- **Concept: Infeasible Projection**
  - **Why needed here**: The IP-SO mechanism does not return a point in the set K immediately but rather a point "closer" to the set. Understanding that the algorithm tracks a "virtual" point helps explain why the theoretical bounds hold.
  - **Quick check question**: Does the IP-SO guarantee that the returned point y lies within the action set K?

## Architecture Onboarding

- **Component map**: Base OGD Module -> IP-SO Wrapper -> Surrogate Cost Generator
- **Critical path**:
  1. Receive f_t, g_t -> Calculate Q_t (violation accumulator)
  2. Compute surrogate gradient ∇̂f_t
  3. Pass gradient to Base OGD -> Get virtual point y_m
  4. Feed y_m to IP-SO -> Iteratively call SO until y_{m+1} is valid

- **Design tradeoffs**:
  - **β Parameter**: Lower β reduces oracle calls (Õ(T^{2β})) but worsens regret/CCV (O(T^{1-β})). β=1/2 matches projection-based performance; β < 1/2 saves computation
  - **Geometry (D/r)**: The ratio of diameter to enclosed ball radius appears in the oracle complexity. "Flat" or "needle-like" sets in high dimensions may degrade performance significantly compared to LOO methods

- **Failure signatures**:
  - **Exploding Gradients**: If ε is set too low or Φ grows too fast, the surrogate gradient may become massive, causing numerical instability or excessive SO calls
  - **Non-termination of IP-SO**: If the step size is too large relative to δ, the projection loop might oscillate near the boundary of K

- **First 3 experiments**:
  1. **Synthetic validation on a Ball**: Implement BAGEL on a simple Euclidean ball where SO is trivial. Verify that Regret and CCV scale as √T
  2. **Matrix Completion (Spectral Norm Ball)**: Compare BAGEL (using rank-1 SVD as SO) against a Projection-based baseline (using full SVD). Measure wall-clock time vs. Regret
  3. **Ablation on Adaptive Step-Size**: Run the algorithm with a constant step size to empirically validate the theoretical claim that SO calls explode to O(T^2) without the adaptive term

## Open Questions the Paper Calls Out
None

## Limitations
- Performance critically depends on having an efficient Separation Oracle for the specific geometry; for general convex sets, SO could be as expensive as projection
- The adaptive step-size mechanism, while theoretically justified, may be sensitive to parameter tuning in practice
- The O(T^{2β}) oracle complexity may still be prohibitive for very large T when β < 1/2

## Confidence
- **High confidence** in the regret bounds and oracle complexity guarantees for the base algorithm
- **Medium confidence** that the computational advantages hold in practice without specific problem structure
- **Medium confidence** in practical stability of the adaptive step-size mechanism
- **Medium confidence** in the practical scalability across the β parameter range

## Next Checks
1. **Empirical validation of oracle efficiency**: Implement BAGEL on spectral norm ball constraints and measure actual SO call counts versus theoretical bounds across different β values
2. **Step-size sensitivity analysis**: Compare oracle complexity and constraint violation under adaptive versus constant step-sizes to empirically verify the O(T^2) blowup claim
3. **Geometry impact study**: Test BAGEL on sets with varying (D/r) ratios to quantify the theoretical concern about "flat" high-dimensional sets degrading performance relative to LOO methods