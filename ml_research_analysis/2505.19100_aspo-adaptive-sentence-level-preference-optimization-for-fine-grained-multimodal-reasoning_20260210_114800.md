---
ver: rpa2
title: 'ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal
  Reasoning'
arxiv_id: '2505.19100'
source_url: https://arxiv.org/abs/2505.19100
tags:
- arxiv
- aspo
- preference
- wang
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASPO (Adaptive Sentence-level Preference Optimization)
  to address the limitations of traditional Direct Preference Optimization (DPO) in
  multimodal reasoning tasks. The core problem is that DPO optimizes entire responses
  using binary rewards, failing to account for variations among individual response
  segments and leading to suboptimal solutions.
---

# ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning

## Quick Facts
- arXiv ID: 2505.19100
- Source URL: https://arxiv.org/abs/2505.19100
- Reference count: 14
- Primary result: ASPO achieves 2.87 average improvement points across multiple multimodal reasoning benchmarks when applied to LLaVA-1.5-13B

## Executive Summary
This paper introduces ASPO (Adaptive Sentence-level Preference Optimization) to address limitations in traditional Direct Preference Optimization (DPO) for multimodal reasoning tasks. DPO optimizes entire responses using binary rewards, which fails to account for variations among individual response segments and leads to suboptimal solutions. ASPO treats sentences as the fundamental units of optimization, calculating adaptive rewards at the sentence level based on image-text similarity and textual perplexity metrics. This enables more precise alignment by prioritizing accurate reasoning and rejecting errors.

Experimental results demonstrate that ASPO substantially improves performance across multiple multimodal reasoning benchmarks. When applied to LLaVA-1.5-13B, ASPO achieves average improvements of 2.87 points across MMVet, MMBench-Dev, MMBench-Test, MMBench-Chinese, SEED-Image, LLaVA-bench-in-the-wild, SQAI, POPE, and SHR benchmarks, outperforming other preference optimization methods. The approach effectively enhances both faithfulness to image inputs and reasoning capabilities of multimodal models.

## Method Summary
ASPO introduces a fine-grained approach to preference optimization that treats sentences as the fundamental units of optimization rather than entire responses. The method calculates adaptive rewards at the sentence level by combining image-text similarity metrics with textual perplexity measurements. This sentence-level granularity allows the model to prioritize accurate reasoning within specific segments while rejecting errors in others. The optimization process involves computing rewards for each sentence independently, then aggregating these sentence-level rewards to guide the overall model update. This approach enables more precise alignment between model outputs and desired reasoning patterns compared to traditional response-level binary reward systems.

## Key Results
- ASPO achieves 2.87 average improvement points across nine multimodal reasoning benchmarks when applied to LLaVA-1.5-13B
- Outperforms other preference optimization methods on MMVet, MMBench-Dev, MMBench-Test, MMBench-Chinese, SEED-Image, LLaVA-bench-in-the-wild, SQAI, POPE, and SHR benchmarks
- Demonstrates enhanced faithfulness to image inputs and improved reasoning capabilities in multimodal models
- Shows effectiveness of sentence-level optimization compared to traditional response-level approaches

## Why This Works (Mechanism)
ASPO works by addressing the fundamental limitation of traditional preference optimization methods that treat entire responses as monolithic units. By breaking down responses into sentence-level components and calculating adaptive rewards based on both image-text similarity and textual perplexity, ASPO can identify and reinforce accurate reasoning segments while downweighting or correcting erroneous ones. This granular approach allows the model to learn more nuanced patterns of multimodal reasoning, as it can distinguish between correct and incorrect reasoning within the same response. The adaptive reward mechanism ensures that sentences requiring more attention based on their quality metrics receive appropriate optimization weight, leading to more efficient learning and better overall performance.

## Foundational Learning

**Direct Preference Optimization (DPO)** - A method for aligning language models with human preferences using pairwise comparisons. Needed to understand the baseline approach that ASPO improves upon. Quick check: Verify understanding of how binary rewards are applied at the response level in traditional DPO.

**Multimodal Reasoning** - The ability of models to process and reason about both visual and textual information simultaneously. Essential for grasping the problem domain ASPO addresses. Quick check: Confirm understanding of how image-text alignment impacts reasoning quality.

**Sentence-level Processing** - Breaking down responses into individual sentences for separate analysis and optimization. Critical for understanding ASPO's granular approach. Quick check: Understand the difference between token-level, sentence-level, and response-level processing.

**Image-Text Similarity Metrics** - Measures of how well generated text aligns with visual content. Needed to comprehend how ASPO evaluates the quality of multimodal reasoning. Quick check: Know common metrics like CLIP similarity used for image-text alignment.

**Textual Perplexity** - A measure of how well a probability model predicts a sample. Used in ASPO to assess the quality of generated text independently of the image. Quick check: Understand how perplexity relates to text quality and reasoning coherence.

## Architecture Onboarding

Component Map: Input Image + Text -> Sentence Tokenizer -> Sentence-level Reward Calculator (Image-Text Similarity + Perplexity) -> Adaptive Reward Aggregator -> Model Optimizer -> Fine-tuned Model

Critical Path: The core optimization loop where sentences are processed individually, rewards are calculated based on multimodal quality metrics, and these rewards drive model updates. This path is critical because it directly determines how the model learns from preference data.

Design Tradeoffs: Sentence-level optimization provides finer granularity and more precise error correction but increases computational overhead compared to response-level methods. The adaptive reward mechanism adds complexity but enables more nuanced learning compared to fixed reward approaches.

Failure Signatures: Poor image-text alignment scores indicate the model is generating text disconnected from visual content. High perplexity in certain sentence types may reveal specific reasoning weaknesses. Inconsistent rewards across sentences within the same response suggest the model has difficulty maintaining coherent reasoning throughout.

Three First Experiments:
1. Test sentence-level reward calculation on a small validation set to verify the adaptive reward mechanism produces expected values
2. Compare optimization convergence speed between sentence-level and response-level approaches using identical data
3. Perform ablation study removing either image-text similarity or perplexity components to assess their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on benchmark performance improvements without extensive qualitative analysis of how sentence-level optimization affects reasoning quality or error patterns
- The adaptive reward calculation mechanism based on image-text similarity and textual perplexity may not fully capture all aspects of multimodal reasoning quality
- Limited discussion of computational overhead introduced by sentence-level processing compared to traditional response-level optimization
- Does not address how ASPO performs across different model scales or architectures beyond the LLaVA-1.5-13B demonstration

## Confidence
- Core claim that ASPO improves multimodal reasoning performance across benchmarks: Medium-High
- Assertion that sentence-level optimization is superior to response-level optimization: Medium

## Next Checks
1. Conduct ablation studies comparing ASPO with sentence-level baselines that use fixed rather than adaptive rewards to isolate the benefit of the adaptive component
2. Perform qualitative error analysis on model outputs to understand how sentence-level optimization affects specific types of reasoning errors versus response-level methods
3. Test ASPO across multiple model architectures and sizes to evaluate generalizability beyond the LLaVA-1.5-13B implementation