---
ver: rpa2
title: 'HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control'
arxiv_id: '2602.02268'
source_url: https://arxiv.org/abs/2602.02268
tags:
- graph
- attention
- transformer
- sparse
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HopFormer introduces a sparse graph Transformer that injects structure
  exclusively through head-specific n-hop masked attention, without positional encodings
  or architectural modifications. This design provides explicit, interpretable control
  over receptive fields and enables genuinely sparse attention with linear computational
  cost in mask sparsity.
---

# HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control

## Quick Facts
- arXiv ID: 2602.02268
- Source URL: https://arxiv.org/abs/2602.02268
- Reference count: 40
- Primary result: Sparse graph Transformer with explicit receptive field control achieves competitive or superior performance on node- and graph-level benchmarks

## Executive Summary
HopFormer introduces a sparse graph Transformer architecture that injects structural information exclusively through head-specific n-hop masked attention, avoiding positional encodings or architectural modifications. This design provides explicit, interpretable control over receptive fields while enabling genuinely sparse attention with linear computational cost proportional to mask sparsity. Experiments demonstrate that localized attention patterns yield stable, high performance on small-world graphs, challenging the assumption that dense global attention is necessary for strong graph learning.

## Method Summary
The HopFormer architecture implements sparse graph Transformers by masking attention matrices according to n-hop neighborhoods specific to each attention head. Instead of relying on positional encodings or structural modifications, the model directly constrains attention scope through precomputed adjacency masks. Each head attends only to nodes within its designated hop radius, creating a controllable trade-off between computational efficiency and receptive field coverage. The approach maintains linear computational complexity when masks are sparse, making it scalable to large graphs while preserving interpretability through explicit hop-based constraints.

## Key Results
- Sparse attention with explicit hop control matches or exceeds performance of complex graph Transformers on small-world benchmarks
- Localized attention patterns provide stable performance, while global attention shows diminishing returns on graphs with weak small-world effects
- Linear computational cost achieved through mask sparsity enables efficient scaling to larger graphs

## Why This Works (Mechanism)
HopFormer succeeds by aligning attention mechanisms with the natural topology of small-world graphs. Small-world networks exhibit high clustering and short average path lengths, meaning most relevant information propagates within few hops. By restricting attention to these natural neighborhoods, the model reduces computational overhead while preserving essential relational information. The explicit hop control allows systematic exploration of receptive field sizes, revealing that dense global attention is often redundant when local structure captures most dependencies.

## Foundational Learning
- **Small-world graphs**: Characterized by high clustering coefficients and short average path lengths, making local neighborhoods sufficient for most information propagation. Needed to understand when sparse attention suffices.
- **Graph adjacency matrices**: Binary or weighted representations of node connectivity that define hop distances. Required to precompute n-hop masks for attention restriction.
- **Masked attention mechanisms**: Techniques for constraining which nodes can attend to each other during transformer operations. Core mechanism enabling sparsity.
- **Computational complexity in attention**: Quadratic scaling with sequence length in standard transformers versus linear scaling when masks are sparse. Critical for efficiency analysis.

## Architecture Onboarding

**Component map:** Graph input -> Adjacency matrix computation -> n-hop mask generation -> Head-specific masked attention -> Transformer layers -> Output

**Critical path:** Graph structure → Mask computation → Attention application → Feature transformation → Prediction

**Design tradeoffs:** Sparsity vs. expressivity (fewer hops reduce computation but may miss distant dependencies), interpretability vs. performance (explicit hop control aids understanding but may limit flexibility), precomputation cost vs. runtime efficiency (masks must be computed but enable fast inference).

**Failure signatures:** Poor performance on graphs with long-range dependencies (scale-free networks), degraded accuracy when adjacency information is noisy or incomplete, inefficiency when masks become dense (approaching quadratic complexity).

**First experiments:** 1) Compare performance across different hop radii (1-hop vs 2-hop vs global) on small-world benchmarks. 2) Measure computational speedup from sparsity across varying graph sizes. 3) Test robustness to edge noise by randomly removing or adding edges to adjacency matrices.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation on small-world graphs; performance on scale-free or heterogeneous graphs with long-range dependencies untested
- Effectiveness depends on known, accurate graph structure; noisy or incomplete adjacency matrices may degrade performance
- Linear complexity claim relies on mask sparsity; dense masks would revert to quadratic cost
- No theoretical guarantees provided for convergence or generalization bounds

## Confidence
High confidence in empirical superiority of sparse attention on tested small-world benchmarks.
Medium confidence in broader claim that global attention is often unnecessary across diverse graph types, given limited dataset scope.
Low confidence in extrapolation to noisy, dynamic, or scale-free graph domains without additional validation.

## Next Checks
1. Test HopFormer on heterogeneous and scale-free graph benchmarks to assess performance outside small-world domains
2. Evaluate robustness to edge noise and missing adjacency entries to validate structural dependency assumptions
3. Conduct systematic ablation comparing sparse HopFormer to dense variants across varying graph sizes and sparsity levels