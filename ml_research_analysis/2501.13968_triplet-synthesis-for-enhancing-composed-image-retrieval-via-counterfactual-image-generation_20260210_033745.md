---
ver: rpa2
title: Triplet Synthesis For Enhancing Composed Image Retrieval via Counterfactual
  Image Generation
arxiv_id: '2501.13968'
source_url: https://arxiv.org/abs/2501.13968
tags:
- image
- triplets
- images
- retrieval
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel triplet synthesis method for Composed
  Image Retrieval (CIR) by leveraging counterfactual image generation. The method
  addresses the challenge of manually collecting high-quality training triplets, which
  is time-consuming and labor-intensive.
---

# Triplet Synthesis For Enhancing Composed Image Retrieval via Counterfactual Image Generation

## Quick Facts
- **arXiv ID:** 2501.13968
- **Source URL:** https://arxiv.org/abs/2501.13968
- **Reference count:** 0
- **Primary result:** Proposed method improves CIR model R@1 from 39.66% to 40.75% on CIRR dataset

## Executive Summary
This paper addresses the data scarcity challenge in Composed Image Retrieval (CIR) by proposing a novel triplet synthesis method using counterfactual image generation. The core innovation is leveraging Language-guided Counterfactual Image (LANCE) models to automatically generate diverse training triplets by modifying reference captions and generating corresponding counterfactual images. The approach significantly improves CIR model performance, especially in data-scarce scenarios, by providing cleaner training signals that focus on specific attribute changes rather than incidental differences present in manually collected image pairs.

## Method Summary
The method generates synthetic training triplets through a pipeline: (1) caption the reference image using BLIP-2, (2) perturb the caption with a fine-tuned LLM (LLaMA-7B + LoRA) to generate modification text and counterfactual caption, (3) generate the target image using Stable Diffusion with null-text inversion and prompt-to-prompt editing, and (4) train the CIR model on these synthetic triplets combined with original data. The approach uses controlled counterfactual editing to ensure precise alignment between modification text and visual changes while preserving background elements.

## Key Results
- BLIP baseline improved from 39.66% to 40.75% R@1 on CIRR dataset with synthetic triplets
- Combiner baseline improved from 40.13% to 41.65% R@1 on CIRR dataset with synthetic triplets
- Performance improvements were particularly significant in data-scarce scenarios (30% of original training data)

## Why This Works (Mechanism)

### Mechanism 1: Controlled counterfactual editing reduces signal-to-noise ratio
The method uses LANCE and prompt-to-prompt editing to modify only specific attributes while preserving global structure, forcing the CIR model to learn precise visual-semantic alignment without being distracted by incidental differences inherent in real image pairs.

### Mechanism 2: Synthesis decouples triplet quantity from image pool diversity
Rather than relying on combinatorial availability of similar images in datasets, the system generates target images on demand, allowing creation of training signal for modifications that rarely occur naturally.

### Mechanism 3: Fine-tuned LLM perturbations create semantically dense modification texts
A fine-tuned LLM perturbs captions to generate complex modification text and counterfactual caption, bridging the modality gap by ensuring modification text is linguistically precise and directly actionable by the Stable Diffusion generator.

## Foundational Learning

**Concept: Composed Image Retrieval (CIR)**
- **Why needed here:** Understanding the target task where CIR takes (Image, Text) -> (Target Image) with relative text modifications
- **Quick check question:** If a user provides a picture of a sunny beach and the text "make it night time," what constitutes the "Target Image"?

**Concept: Latent Diffusion Inversion (Null-text Inversion)**
- **Why needed here:** Understanding how to edit real input images by finding the latent noise code that reconstructs the original image
- **Quick check question:** Why can't we simply use standard Stable Diffusion "img2img" with high strength parameter for this task?

**Concept: Cross-Attention Control (Prompt-to-Prompt)**
- **Why needed here:** Understanding how diffusion models map text tokens to image regions for localized changes
- **Quick check question:** In the attention map, which corresponds to the visual layout of the "dog" token: Keys, Values, or Query?

## Architecture Onboarding

**Component map:** Captioner (BLIP-2) -> Perturbator (LLAMA-7B + LoRA) -> Synthesizer (Stable Diffusion + Null-text Inversion + Prompt-to-Prompt) -> Retriever (BLIP/Combiner)

**Critical path:** The alignment between the Perturbator and Synthesizer - if the LLM generates modification text the diffusion model cannot execute cleanly, the triplet is poisoned.

**Design tradeoffs:**
- **Precision vs. Reality:** Prioritizes precise alignment over photo-realism, accepting synthetic artifacts for cleaner training signal
- **Computational Cost:** Heavy diffusion inference upfront cost to reduce data collection time

**Failure signatures:**
- **Semantic Bleeding:** Generated target image changes unintended attributes, suggesting misconfigured prompt-to-prompt weight
- **Hallucinated Edits:** LLM produces captions describing scenes diffusion models render poorly, resulting in inconsistent triplets

**First 3 experiments:**
1. Generate 50 triplets and visually inspect if target image reflects modification text while preserving background
2. Train standard CIR model on 10% of CIRR dataset with and without 1,000 synthetic triplets, compare R@1
3. Replace Prompt-to-Prompt editing with InstructPix2Pix to test if localized edit quality drives performance gains

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Implementation details for LoRA fine-tuning and Stable Diffusion configuration are unspecified
- Performance comparisons only against BLIP and Combiner baselines, not newer state-of-the-art methods
- No quantitative metrics for image quality assessment or validation that synthetic images have fewer "unnecessary" elements

## Confidence

- **High Confidence:** Core mechanism of using counterfactual generation for controlled training triplets is sound and technically feasible
- **Medium Confidence:** Claims about effectiveness in data-scarce scenarios are supported by 30% data reduction experiments
- **Low Confidence:** Assertions about "high-quality" synthetic images and fewer unnecessary elements are not empirically validated

## Next Checks

1. **Triplet Quality Audit:** Generate 100 synthetic triplets and conduct human evaluation of modification text accuracy, background preservation, and overall image quality compared to retrieved triplets

2. **Noise Sensitivity Analysis:** Systematically vary prompt-to-prompt editing strength and measure impact on background preservation, modification accuracy, and downstream CIR performance

3. **Cross-Domain Generalization Test:** Train CIR model on synthetic triplets from one domain and evaluate on held-out domain to test generalizability of synthetic data