---
ver: rpa2
title: 'RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code'
arxiv_id: '2503.07832'
source_url: https://arxiv.org/abs/2503.07832
tags:
- file
- test
- agents
- https
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RefactorBench evaluates language agents on multi-file code refactoring
  tasks, revealing poor performance (22% success rate) compared to humans (87%). A
  key finding is that agents struggle to track and reason about past actions.
---

# RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code

## Quick Facts
- arXiv ID: 2503.07832
- Source URL: https://arxiv.org/abs/2503.07832
- Reference count: 40
- Primary result: Language agents achieve 22% success on multi-file refactoring vs 87% for humans; state-aware interfaces improve performance by 43.9%

## Executive Summary
RefactorBench introduces a benchmark for evaluating language agents on multi-file code refactoring tasks, revealing significant performance gaps between agents and humans. The benchmark comprises 100 tasks across 9 Python repositories, with agents required to generate patches that satisfy AST-based unit tests. While humans achieve 87% task resolution, baseline agents only reach 22%. A key finding is that agents struggle to track and reason about their past actions during multi-step editing processes. The study demonstrates that conditioning agents on representations of their previous edits (state updates) improves performance by 43.9%, highlighting the importance of stateful reasoning in complex agent tasks.

## Method Summary
RefactorBench evaluates language agents on multi-file code refactoring using 9 open-source Python repositories with 3 instruction variants per task (lazy/base/descriptive). Tasks require generating patches that satisfy AST-based unit tests, with an average of 6.51 tests per task. The baseline SWE-agent is modified with refactoring-specific prompts, while the state-aware variant tracks recent edits via a cached state variable (σN) appended to observations. Evaluation uses task resolution rate (percentage passing all AST subtests) with a $10/instance cost limit. Human performance is established through a controlled user study with concrete metrics.

## Key Results
- Baseline agents achieve 22% task resolution rate compared to 87% for human developers
- State-aware interfaces improve performance by 43.9% on the benchmark
- Agents frequently edit wrong files (44% missed target files) and lose sight of objectives due to context flooding from linting errors (avg 1,466 tokens per error block)

## Why This Works (Mechanism)
The study demonstrates that language agents struggle with stateful reasoning in multi-file refactoring tasks because they cannot effectively track and reason about their past actions. By conditioning agents on representations of their previous edits (state updates), the system provides crucial context that helps agents maintain coherence across multiple editing steps. This state-aware approach addresses the fundamental challenge of multi-step reasoning where agents need to remember what changes they've already made and how those changes affect subsequent decisions.

## Foundational Learning
- **Stateful reasoning**: Understanding how to maintain and utilize state information across multiple reasoning steps - needed because agents lose track of previous actions during complex tasks; quick check: trace agent decision-making across multiple edits
- **AST-based testing**: Using abstract syntax tree validation instead of functional tests - needed because refactoring correctness can be verified without execution; quick check: confirm test failures correspond to actual code issues
- **Context management**: Handling information overload in long reasoning sequences - needed because error handling generates excessive tokens that obscure objectives; quick check: measure token usage during error recovery
- **Multi-file coordination**: Managing dependencies and order across multiple code files - needed because refactoring often requires coordinated changes across related files; quick check: verify all required files are modified appropriately

## Architecture Onboarding
**Component map**: Repositories -> Tasks -> Instructions -> Agent -> SWE-tools -> State tracker -> AST tests
**Critical path**: Task instruction → Agent generates edit → SWE-agent applies changes → State updates recorded → Next step receives state context → AST tests validate
**Design tradeoffs**: Simple state tracking vs complex memory systems; immediate validation vs allowing temporary errors; single-agent vs multi-agent coordination
**Failure signatures**: Wrong file selection (44%), context flooding from linting errors (1,466 tokens avg), inability to handle temporary error states
**First experiments**: 1) Test state-aware agent on single-file tasks to isolate multi-file effects; 2) Compare performance with and without state tracking on identical tasks; 3) Measure context window usage during error handling sequences

## Open Questions the Paper Calls Out
- **Conflict resolution in multi-agent environments**: The benchmark lacks instances where multiple agents operate on the same state with opposing goals, leaving the efficacy of state-update policies in adversarial or concurrent settings unknown. Evaluation on multi-agent benchmarks requiring negotiation would resolve this.
- **Scaling inference-time compute for open-ended tasks**: Current repeated sampling strategies are insufficient for real-world refactoring without robust critic models. Benchmark results using reasoning models with high compute budgets paired with advanced critic models would provide evidence.
- **Mitigating context flooding**: Current interfaces flood context windows with linting errors and formatting rejections. Trajectory analysis showing sustained goal adherence in agents using interfaces with speculative decoding or aggressive truncation would resolve this.

## Limitations
- The exact SWE-agent configuration beyond prompt modifications is not fully specified
- Model-specific hyperparameters for different LLM backends are not reported
- Reference solution comparison shows only minor quality differences (10.8% syntax issues, 2.8% style deviations)

## Confidence
- **High confidence**: Baseline human vs agent performance comparison (controlled user study with concrete metrics)
- **Medium confidence**: State-aware interface improvements (ablation studies show consistent gains but methodology has some ambiguity)
- **Medium confidence**: Analysis of failure modes (qualitative observations from trajectories, but not systematically quantified)

## Next Checks
1. Replicate the state-aware interface improvement on a held-out subset of repositories not used in the original evaluation
2. Conduct ablation studies isolating the effect of state tracking from other context modifications in the SWE-agent prompts
3. Test agent performance on single-file refactoring variants to determine whether observed failures are specific to multi-file reasoning or general code editing limitations