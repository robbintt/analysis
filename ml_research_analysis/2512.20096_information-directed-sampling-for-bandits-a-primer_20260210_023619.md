---
ver: rpa2
title: 'Information-directed sampling for bandits: a primer'
arxiv_id: '2512.20096'
source_url: https://arxiv.org/abs/2512.20096
tags:
- regret
- optimal
- information
- policy
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the Multi-Armed Bandit (MAB) problem through\
  \ the lens of Information Directed Sampling (IDS) policies, focusing on two-state\
  \ Bernoulli bandits as a tractable model. The authors extend the IDS framework to\
  \ the discounted infinite-horizon setting by introducing a modified information\
  \ measure and a tuning parameter \u03B1 that modulates the balance between exploration\
  \ and exploitation."
---

# Information-directed sampling for bandits: a primer

## Quick Facts
- arXiv ID: 2512.20096
- Source URL: https://arxiv.org/abs/2512.20096
- Reference count: 0
- This paper examines the Multi-Armed Bandit problem through the lens of Information Directed Sampling policies, focusing on two-state Bernoulli bandits as a tractable model.

## Executive Summary
This paper examines the Multi-Armed Bandit (MAB) problem through the lens of Information Directed Sampling (IDS) policies, focusing on two-state Bernoulli bandits as a tractable model. The authors extend the IDS framework to the discounted infinite-horizon setting by introducing a modified information measure and a tuning parameter α that modulates the balance between exploration and exploitation. For symmetric two-state bandits, where both coins have identical winning probabilities, the authors derive an analytical solution showing that IDS achieves bounded cumulative regret as the discount factor approaches one. In the case of one fair coin, where one action provides no information, IDS(α=0) produces regret that scales logarithmically with the horizon, matching classical asymptotic lower bounds.

## Method Summary
The paper introduces Information Directed Sampling for discounted infinite-horizon two-state Bernoulli bandits. The method operates on belief distributions over latent states and selects actions by minimizing the information ratio Ψα(π,b) = Δ(π,b)^(1/α) / Iπ(b)^(1/α−1) at each step. For implementation, value iteration is used to compute optimal policies, while IDS policies are derived analytically for special cases (symmetric and one-fair-coin bandits) and numerically for general cases. The information measure is modified to include both immediate entropy and discounted expected entropy reduction, making it suitable for infinite-horizon settings. The tuning parameter α controls the exploration-exploitation trade-off, with α=0 favoring exploration and α=1 being purely greedy.

## Key Results
- IDS achieves bounded cumulative regret for symmetric two-state bandits as discount factor approaches one
- IDS(α=0) achieves logarithmic regret in the one-fair-coin case, matching Lai-Robbins asymptotic lower bounds
- For asymmetric cases, IDS(0) generally outperforms IDS(1/2), though an optimal α exists that minimizes maximum relative regret depending on problem parameters

## Why This Works (Mechanism)

### Mechanism 1: Information Ratio Minimization
Minimizing the ratio of immediate regret to information gain at each step produces a tractable upper bound on cumulative regret. At each belief state b, IDS selects policy π minimizing Ψα(π,b) = Δ(π,b)^(1/α) / Iπ(b)^(1/α−1). This one-step optimization replaces intractable Bellman equation solving while implicitly capturing exploration-exploitation trade-offs through the ratio structure.

### Mechanism 2: Discounted Information Measure for Infinite Horizon
A modified information function combining immediate entropy with discounted expected entropy reduction maintains regret bounds in the infinite-horizon discounted setting. Iπ(b) = (1−γ)H(b) + γ[H(b) − Σ π(a|b)pb(y|a)H(b′ay)] ensures the information measure scales appropriately with γ. As γ→1, this reduces to mutual information; as γ→0, it converges to a constant, making IDS equivalent to greedy exploitation.

### Mechanism 3: Asymmetric Information Gain Exploitation via α-Tuning
When actions have asymmetric information profiles (different biases δ_a), the tuning parameter α shifts IDS toward exploration (small α) or exploitation (large α) to match problem structure. α=1 minimizes one-step regret (pure exploitation); α→0 weights information more heavily, causing IDS to select informative but currently suboptimal actions.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs) and Belief States**
  - Why needed here: IDS operates on belief distributions b(s) over latent states rather than observations directly. Understanding how beliefs update via Bayes' rule and how value functions depend on beliefs is foundational.
  - Quick check question: Given prior belief β=0.3 and observation y=1 after action a=1 with θ(s=1,a=1)=0.7, θ(s=−1,a=1)=0.4, what is the updated belief?

- **Concept: Bellman Optimality Equation and Value Iteration**
  - Why needed here: The paper compares IDS against optimal policies computed via Bellman equation. Understanding value iteration's convergence clarifies what "optimal" means and why it's typically intractable.
  - Quick check question: Why does the Bellman operator B have contraction coefficient γ in the max-norm?

- **Concept: Shannon Entropy and Mutual Information**
  - Why needed here: The information measure Iπ(b) combines entropy terms and expected entropy reduction. Understanding H(b) = −Σ_s b(s)log b(s) and how observations reduce entropy is essential for interpreting IDS's objective.
  - Quick check question: For a two-state belief with b(s=1)=0.8, what is the Shannon entropy? What happens to entropy after observing definitive evidence?

## Architecture Onboarding

- **Component map:** Belief representation (β∈[−1,1]) -> Belief update module (Bayes rule) -> Information ratio calculator -> Policy optimizer -> Regret tracker
- **Critical path:** 1) Initialize belief b_0 (typically uniform: β=0) 2) For each timestep: compute optimal π by minimizing Ψα, sample action a∼π, observe y, update belief 3) Regret accumulates when actions deviate from true optimal a*(s)
- **Design tradeoffs:** α selection (α→0 favors exploration; α=1 is purely greedy), discount factor γ (higher γ increases effective horizon), numerical vs analytical solutions
- **Failure signatures:** IDS(α=1/2) produces "sawtooth" regret patterns, too large α causes under-exploration, too small α becomes "too explorative" for long horizons
- **First 3 experiments:** 1) Symmetric bandit validation (θ+ = θ- = 0.7, γ=0.99): Verify IDS produces identical regret to optimal greedy policy 2) One fair coin regret scaling (θ- = 0.5, θ+ = 0.55, varying γ): Confirm IDS(0) regret scales as c_1 + c_2 log(1−γ) 3) α-sensitivity mapping (θ- = 0.55, θ+ = 0.75, γ=0.99): Compute maximum relative regret ΔR(α) across α∈[0,1]

## Open Questions the Paper Calls Out

### Open Question 1
What is the functional dependence of the optimal tuning parameter α on the Bernoulli parameters (θ±) and the discount factor (γ)? While the paper demonstrates that an optimal α exists to minimize relative regret, the relationship is complex and lacks a simple monotonic trend.

### Open Question 2
How does the performance and stability of the IDS policy change if the Shannon entropy in the information measure is replaced by Rényi entropy? The current derivation relies specifically on properties of Shannon entropy and Bregman divergences.

### Open Question 3
How must the information gain be redefined for environments where agent actions influence state transitions (full feedback), and how does this affect the regret bounds? The current modified information measure is derived under the assumption that the agent's actions do not affect the environment's state transition probabilities.

## Limitations
- Theoretical framework relies on local information-to-regret ratio remaining meaningful across belief trajectories, not validated for non-stationary environments
- Choice of α remains largely heuristic despite empirical characterization, with no formal analysis of the gap between IDS(α) and optimal policy regret
- Discounted information measure modification lacks empirical validation beyond the two-state Bernoulli case studied

## Confidence
- **High confidence:** Symmetric bandit results (bounded regret as γ→1, IDS = optimal policy) - these follow directly from symmetry assumptions
- **Medium confidence:** One-fair-coin logarithmic regret bounds - derivation follows Lai-Robbins framework but depends on accurate characterization of optimal decision boundaries
- **Low confidence:** General α-tuning recommendations - empirical observations show α=0 often outperforms α=1/2, but the "optimal α" characterization is problem-specific

## Next Checks
1. **Numerical stability verification:** Reproduce the analytical regret bounds (Eqs. 19, 23) for varying γ values and confirm logarithmic scaling in the one-fair-coin case matches empirical results
2. **Decision boundary sensitivity analysis:** Map the optimal α landscape across different (θ-, θ+) pairs and γ values to verify the reported 1-5% maximum relative regret gap between IDS(0) and optimal policy
3. **Generalization stress test:** Apply IDS to asymmetric bandits where θ+ - θ- is small (<0.1) to evaluate whether the information ratio remains a reliable exploration signal when action differences are subtle