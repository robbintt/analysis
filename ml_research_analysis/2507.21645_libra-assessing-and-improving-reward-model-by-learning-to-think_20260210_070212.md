---
ver: rpa2
title: 'Libra: Assessing and Improving Reward Model by Learning to Think'
arxiv_id: '2507.21645'
source_url: https://arxiv.org/abs/2507.21645
tags:
- reward
- arxiv
- reasoning
- bench
- libra
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Libra Bench, a reasoning-oriented reward model
  benchmark constructed from challenging mathematical problems and responses from
  advanced reasoning models. To address limitations of existing reward models in complex
  reasoning tasks, the authors propose a learning-to-think approach combining rejection
  sampling and reinforcement learning, resulting in the Libra-RM series.
---

# Libra: Assessing and Improving Reward Model by Learning to Think

## Quick Facts
- arXiv ID: 2507.21645
- Source URL: https://arxiv.org/abs/2507.21645
- Authors: Meng Zhou; Bei Li; Jiahao Liu; Xiaowen Shi; Yang Bai; Rongxiang Weng; Jingang Wang; Xunliang Cai
- Reference count: 40
- Primary result: Libra-RM-32B-MATH achieves 81.7% accuracy on Libra Bench, outperforming existing reward models like AceMath-72B-RM (66.6%) and GPT-4.1 (69.1%)

## Executive Summary
This paper addresses the challenge of training effective reward models for complex reasoning tasks, where existing approaches struggle with accurate correctness judgments. The authors introduce Libra Bench, a benchmark constructed from challenging mathematical problems and responses from advanced reasoning models, specifically designed to evaluate reward models' correctness judgment capabilities. To tackle this problem, they propose a learning-to-think approach that combines rejection sampling and reinforcement learning, resulting in the Libra-RM series of reward models that demonstrate state-of-the-art performance on both specialized reasoning benchmarks and general reward model tasks.

## Method Summary
The method involves two key components: a specialized benchmark (Libra Bench) and a novel training approach (learning-to-think). Libra Bench is constructed from MATH-500 level 5 problems and AIME 2024/2025 questions, with responses collected from five advanced reasoning models and annotated for correctness using a verifiable-to-verifiable (V2V) pipeline that transforms reasoning problems into judgment tasks. The training approach uses a two-stage pipeline: first, supervised fine-tuning with rejection sampling from DeepSeek-R1 on judging prompts, followed by reinforcement learning with GRPO (Group Relative Policy Optimization) using correctness judgments as verifiable rewards. The resulting Libra-RM models are generative reward models that produce Chain-of-Thought reasoning before outputting judgments, enabling better accuracy on reasoning tasks.

## Key Results
- Libra-RM-32B-MATH achieves 81.7% accuracy on Libra Bench, significantly outperforming AceMath-72B-RM (66.6%) and GPT-4.1 (69.1%)
- Inference-time scaling with Long-CoT reasoning improves reward model accuracy by 6-13% compared to non-thinking models
- Mixed training data (judging + non-judging reasoning) improves RM performance by 0.9% on Libra Bench
- Downstream DPO experiments show correlation between Libra Bench accuracy and improved reasoning performance on AIME problems

## Why This Works (Mechanism)

### Mechanism 1
Treating correctness judgment as a verifiable task enables RL optimization without reference-dependent rewards. The V2V pipeline transforms problems with known answers into judgment tasks where binary correctness serves as a verifiable reward signal. This works because answer correctness can be reliably extracted from model outputs and matches ground truth with sufficient accuracy. The paper notes a 0.148% disagreement rate in model-based annotation for MATH-500 level 5. Break condition: If answer extraction fails on complex formats, the verification pipeline degrades.

### Mechanism 2
Inference-time scaling (Long-CoT) improves reward model accuracy on reasoning tasks. Generative RMs with deep thinking capabilities produce Chain-of-Thought reasoning before outputting judgments, allowing the model to work through the problem independently before comparing against the candidate response. This reduces anchoring bias. Evidence shows thinking models achieve 73.7%-78.7% accuracy vs. 55.1%-69.1% for non-thinking models on Libra Bench. Break condition: If the model's reasoning is unreliable or the task doesn't benefit from decomposition, CoT adds cost without gain.

### Mechanism 3
Mixed training data (judging + non-judging reasoning) improves RM performance by strengthening underlying answering ability. Non-judging data enhances the model's domain competence, and better problem-solving capability transfers to better evaluation capability. The paper observes a 0.9% accuracy gain when adding non-judging reasoning data. Break condition: If judging and answering diverge in required skills, the transfer weakens.

## Foundational Learning

- **Generative vs. Discriminative Reward Models**: Why needed: Libra-RM is generative (outputs textual judgments with scores) rather than discriminative (scalar head), enabling CoT reasoning before judgment. Quick check: Can you explain why a generative RM can produce intermediate reasoning tokens while a discriminative RM cannot?

- **GRPO (Group Relative Policy Optimization)**: Why needed: The RL stage uses GRPO with Clip-Higher strategy. Understanding group-based advantage estimation is necessary to modify training. Quick check: How does GRPO differ from PPO in computing advantages, and why does this matter for sample efficiency?

- **Rejection Sampling**: Why needed: SFT data is collected by sampling from DeepSeek-R1 and selecting responses matching target outputs, distilling strong reasoning into the RM. Quick check: What failure mode occurs if the teacher model (DeepSeek-R1) systematically produces outputs the student cannot replicate?

## Architecture Onboarding

- **Component map**: Libra Bench (3 subsets × 5 models) → Qwen2.5-32B base → SFT on judging + non-judging data → RL with GRPO → Libra-RM outputs

- **Critical path**: 1) Data curation: Collect verifiable problems → sample responses → annotate correctness → balance. 2) SFT: Rejection sampling from DeepSeek-R1 on judging prompts + standard reasoning prompts. 3) RL: GRPO training on verifiable judging tasks with correctness reward and length penalty.

- **Design tradeoffs**: Specialization vs. generality (Libra-RM-32B-MATH excels on reasoning but Libra-RM-32B handles ranking), annotation method (rule-based vs. model-based), prompt template (Answer-Then-Compare vs. direct rating).

- **Failure signatures**: High false positive rate on incorrect samples, verbose outputs inflating token count, RL-zero without SFT converging slower and to lower accuracy.

- **First 3 experiments**: 1) Baseline reproduction: Evaluate existing RMs on Libra Bench to confirm thinking vs. non-thinking gap. 2) Ablation on data composition: Train variants with/without D_rs_reason and measure accuracy delta. 3) Downstream validation: Run DPO with Libra-RM-32B-MATH vs. baseline RM on R1-Distill-Qwen-7B and measure AIME pass@1.

## Open Questions the Paper Calls Out

The paper identifies several open questions: Can the V2V framework and Libra-RM generalize to domains lacking formally verifiable ground truths like creative writing or qualitative reasoning? Does performance gain from using Libra-RM scale monotonically with unlabeled data volume or does it saturate? To what extent does RL-zero approach close the performance gap with SFT+RL pipeline given extended compute and larger model sizes?

## Limitations

- Benchmark construction relies on responses from existing reasoning models, inheriting their biases and errors
- Rule-based annotation works well for integers but requires model-based evaluation for complex expressions, introducing annotation noise
- Training pipeline depends on access to strong reasoning models (DeepSeek-R1) for rejection sampling
- Specialized training on mathematical problems may not transfer effectively to other domains

## Confidence

**High Confidence**: The empirical performance claims on Libra Bench (81.7% accuracy for Libra-RM-32B-MATH vs 66.6% for AceMath-72B-RM) are well-supported by the experimental results.

**Medium Confidence**: The claim that inference-time scaling consistently improves accuracy is supported by the 6-13% accuracy gap, but could benefit from more systematic ablation studies.

**Medium Confidence**: The assertion that mixed training data improves RM performance through transfer is plausible given the 0.9% accuracy gain, but lacks external validation.

**Low Confidence**: The claim about generalizability to non-mathematical reasoning tasks is asserted but not empirically validated beyond general RM benchmarks.

## Next Checks

1. **Cross-domain generalization test**: Evaluate Libra-RM on non-mathematical reasoning tasks (code generation, logical reasoning, commonsense QA) to validate broader applicability.

2. **Annotation robustness study**: Conduct human evaluation of model-based correctness annotations on complex answer formats to quantify annotation noise and its impact on training signal quality.

3. **Teacher model sensitivity analysis**: Train Libra-RM variants using different teacher models (GPT-4, Claude, open-source alternatives) for rejection sampling to assess robustness to teacher quality.