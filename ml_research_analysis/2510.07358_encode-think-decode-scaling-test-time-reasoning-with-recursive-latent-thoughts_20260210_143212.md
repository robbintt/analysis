---
ver: rpa2
title: 'Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts'
arxiv_id: '2510.07358'
source_url: https://arxiv.org/abs/2510.07358
tags:
- reasoning
- layers
- https
- semanticscholar
- corpusid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Encode-Think-Decode (ETD), a method that
  enhances reasoning capabilities of large language models by iterating over reasoning-relevant
  layers during training and inference. Instead of scaling parameters or using chain-of-thought
  prompting, ETD trains models to treat a subset of layers as a recurrent "thinking"
  block, allowing them to perform deeper latent reasoning without additional parameters,
  data, or hyperparameters.
---

# Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts

## Quick Facts
- **arXiv ID**: 2510.07358
- **Source URL**: https://arxiv.org/abs/2510.07358
- **Reference count**: 40
- **Primary result**: ETD improves reasoning accuracy by up to 28.4% on GSM8K and 36% on MATH using 1B parameter models

## Executive Summary
Encode-Think-Decode (ETD) introduces a novel approach to enhance reasoning capabilities in large language models without scaling parameters, data, or hyperparameters. Instead of traditional chain-of-thought prompting, ETD treats a subset of layers as a recurrent "thinking" block that iterates during both training and inference. This allows models to perform deeper latent reasoning through recursive processing. The method achieves state-of-the-art results on mathematical reasoning benchmarks using small 1B parameter models, outperforming alternatives while maintaining parameter efficiency.

## Method Summary
ETD identifies a contiguous subset of reasoning-relevant layers within a model and treats them as a recurrent block during training. The model learns to encode input, recursively "think" through these layers multiple times, then decode the final output. During inference, the same layers are applied iteratively without additional parameters. An adaptive depth strategy dynamically allocates more iterations to difficult inputs based on heuristics. The approach is implemented as a fine-tuning procedure that modifies the training objective to encourage iterative reasoning behavior within the designated layer subset.

## Key Results
- ETD achieves up to 28.4% accuracy improvement on GSM8K benchmark
- MATH performance improves by 36% using OLMo-2 1B Base model
- Outperforms alternative recursive approaches across 17 reasoning benchmarks

## Why This Works (Mechanism)
ETD works by enabling models to perform deeper reasoning through iterative processing of a dedicated layer subset. By training models to treat these layers as recurrent during both training and inference, the approach allows the model to refine its understanding through multiple passes. The adaptive depth mechanism allocates computational resources efficiently by spending more iterations on difficult problems while maintaining efficiency on easier ones. This creates a parameter-efficient path to stronger reasoning without requiring model scaling or additional data.

## Foundational Learning

**Recurrent neural networks** - Neural networks with loops allowing information persistence across time steps. Needed to understand how ETD treats layers as recurrent blocks. Quick check: Can identify RNN variants and their applications.

**Chain-of-thought prompting** - Method of decomposing complex reasoning into intermediate steps. Needed to contrast ETD's approach with traditional prompting methods. Quick check: Can explain how CoT differs from direct answer generation.

**Fine-tuning vs pre-training** - Fine-tuning adapts pre-trained models to specific tasks with relatively small datasets. Needed to understand ETD's training procedure. Quick check: Can distinguish between objectives and data requirements for each approach.

**Latent representations** - Intermediate hidden states in neural networks that capture abstract features. Needed to understand how ETD manipulates reasoning-relevant layers. Quick check: Can describe how latent states evolve through network layers.

**Transformer architecture** - Self-attention based neural network architecture. Needed to understand how ETD modifies standard transformer layers. Quick check: Can explain self-attention mechanism and layer composition.

## Architecture Onboarding

**Component map**: Input -> Encoder Layers -> Recurrent Thinking Block -> Decoder Layers -> Output

**Critical path**: The recurrent thinking block (subset of layers) forms the core of ETD's reasoning capability. This block is applied multiple times during both training and inference to enable iterative refinement of latent representations.

**Design tradeoffs**: ETD trades computational efficiency at inference time for improved reasoning accuracy. The adaptive depth mechanism helps mitigate this by allocating more iterations only to difficult inputs. The approach requires careful selection of which layers to designate as the thinking block.

**Failure signatures**: If the wrong layers are designated as the thinking block, the model may fail to improve reasoning capability. Without proper training, the model may not learn to utilize the recurrent structure effectively. The adaptive depth heuristics may misallocate iterations on certain input distributions.

**3 first experiments**:
1. Test ETD on a simple arithmetic reasoning task to verify basic functionality
2. Compare performance with and without adaptive depth on a mixed-difficulty benchmark
3. Evaluate the impact of different layer subset selections on reasoning performance

## Open Questions the Paper Calls Out
The paper acknowledges that the adaptive depth mechanism relies on heuristics rather than learned policies, which may not generalize optimally across different domains. The approach's effectiveness at larger model scales (70B+ parameters) remains untested. Additionally, the training efficiency and computational overhead compared to standard fine-tuning are not addressed.

## Limitations
- Focus on smaller model scales (1B and 7B parameters) leaves uncertainty about effectiveness at larger scales
- Adaptive depth mechanism relies on heuristics rather than learned policies
- Training efficiency is not addressed with no comparison to standard fine-tuning

## Confidence

**High confidence**: ETD achieves state-of-the-art performance on GSM8K and MATH with 1B parameter models compared to baselines

**Medium confidence**: ETD's performance advantage holds across diverse reasoning benchmarks

**Medium confidence**: The adaptive depth strategy improves results without increasing average iterations

## Next Checks

1. Evaluate ETD on non-mathematical reasoning tasks (e.g., code generation, commonsense reasoning) to assess domain generalization
2. Compare training efficiency and wall-clock time against standard fine-tuning to quantify computational overhead
3. Test ETD's effectiveness on larger model scales (e.g., 70B+ parameters) to determine scalability limits