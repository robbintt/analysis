---
ver: rpa2
title: 'EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving'
arxiv_id: '2512.14946'
source_url: https://arxiv.org/abs/2512.14946
tags:
- cache
- compression
- quality
- eviction
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EVICPRESS jointly optimizes KV cache eviction and compression across
  multi-tier storage to reduce LLM inference latency while maintaining quality. It
  introduces a utility function to quantify the trade-off between generation quality
  and loading delay for each cache placement configuration, enabling per-context adaptation
  based on compression sensitivity.
---

# EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving

## Quick Facts
- **arXiv ID:** 2512.14946
- **Source URL:** https://arxiv.org/abs/2512.14946
- **Reference count:** 40
- **Key outcome:** EVICPRESS reduces LLM inference latency by 2.19x while maintaining quality through joint KV cache compression and eviction optimization.

## Executive Summary
EVICPRESS addresses the challenge of efficiently serving large language models by jointly optimizing KV cache compression and eviction across multi-tier storage (GPU, CPU, SSD). The system introduces a utility function that balances generation quality against loading delays, enabling per-context adaptation based on compression sensitivity. By profiling each context to determine optimal eviction-compression decisions and periodically updating them, EVICPRESS achieves significant latency reductions while maintaining quality targets. The system uses a greedy heuristic to manage cache placement when storage tiers fill up.

## Method Summary
EVICPRESS implements a joint optimization framework that profiles each context's sensitivity to compression and eviction across storage tiers. The method involves an offline profiling phase where quality and latency are measured for all compression configurations (keydiff, knorm, snapkv) on training queries. During online inference, a greedy policy optimizer (Algorithm 1) places and compresses KV caches to maximize total utility, defined as the product of quality, frequency, and inverse latency. The system periodically re-profiles if quality drops exceed a threshold, adapting to changing workloads.

## Key Results
- Reduces time-to-first-token by up to 2.19x compared to baselines
- Achieves 2.0-3.6x higher throughput at 80% quality target
- Evaluated across 12 datasets and 5 models using LongBench benchmark

## Why This Works (Mechanism)
The joint optimization approach works because it recognizes that different contexts have varying sensitivity to compression and eviction strategies. By profiling each context's quality-latency trade-offs, EVICPRESS can make context-aware decisions that maximize utility. The multi-tier storage approach leverages cost-effective storage tiers (CPU, SSD) while maintaining performance through intelligent compression and placement strategies.

## Foundational Learning
- **KV Cache Management:** Why needed - stores intermediate attention states to avoid recomputation; Quick check - verify cache hit rates improve with EVICPRESS
- **Lossy Compression:** Why needed - reduces storage requirements while maintaining acceptable quality; Quick check - measure quality degradation across compression ratios
- **Utility-Based Optimization:** Why needed - balances quality vs. latency trade-offs; Quick check - verify utility function correctly ranks configurations
- **Multi-Tier Storage:** Why needed - balances cost and performance across different storage technologies; Quick check - measure access patterns across tiers
- **Context-Level Profiling:** Why needed - different contexts have different compression sensitivities; Quick check - verify profiling overhead is acceptable
- **Greedy Heuristic Optimization:** Why needed - provides computationally tractable solution to NP-hard placement problem; Quick check - compare against optimal solutions on small instances

## Architecture Onboarding

**Component Map:** GPT-5 Query Generator -> Profiler -> Utility Calculator -> Greedy Policy Optimizer -> Cache Manager -> PagedAttention Blocks

**Critical Path:** Request Arrival -> Cache Lookup -> (Cache Miss) -> Profiler Selection -> Cache Placement -> Generation

**Design Tradeoffs:** The system trades profiling overhead for better cache utilization, and compression artifacts for storage efficiency. The greedy heuristic sacrifices optimality for computational tractability.

**Failure Signatures:** Excessive profiling overhead, oscillating quality due to query drift, cache thrashing when storage tiers fill up, and quality degradation beyond acceptable thresholds.

**First Experiments:**
1. Implement GPT-5 query generation and MiniLM quality scoring pipeline
2. Integrate Algorithm 1 with multi-tier storage and verify cache placement decisions
3. Test end-to-end latency improvements with varying compression ratios

## Open Questions the Paper Calls Out
None specified.

## Limitations
- Heavy computational overhead from profiling phase may impact practical deployment
- Quality improvements depend heavily on specific multi-tier hardware configuration
- Evaluation relies on GPT-5-generated queries which may not reflect real-world workloads
- Greedy heuristic lacks rigorous approximation guarantees
- Implementation details for integration with PagedAttention blocks are underspecified

## Confidence

- **High Confidence:** Core technical contribution of joint optimization is well-supported
- **Medium Confidence:** Reported improvements are convincing within controlled experiments
- **Low Confidence:** Exact implementation details and computational overhead characterization

## Next Checks

1. **Replication of Quality Measurement Pipeline:** Implement GPT-5 query generation and MiniLM6-v2-based quality scoring to verify profiling overhead and quality trends.

2. **End-to-End System Integration:** Implement Algorithm 1 and integrate with multi-tier storage to test cache placement and periodic re-profiling mechanisms.

3. **Cross-Platform Generalization:** Evaluate EVICPRESS on different hardware configurations to assess robustness of reported improvements.