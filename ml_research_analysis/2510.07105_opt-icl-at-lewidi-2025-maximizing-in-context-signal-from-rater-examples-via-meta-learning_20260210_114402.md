---
ver: rpa2
title: 'Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via
  Meta-Learning'
arxiv_id: '2510.07105'
source_url: https://arxiv.org/abs/2510.07105
tags:
- turn
- in-context
- system
- datasets
- rater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes a system submission to the Learning With Disagreements
  (LeWiDi) competition, which involves predicting how individual annotators rated
  instances across four datasets involving subjective judgments. The system, Opt-ICL,
  leverages large language models' in-context learning abilities by including rater
  examples in-context at inference time.
---

# Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning

## Quick Facts
- arXiv ID: 2510.07105
- Source URL: https://arxiv.org/abs/2510.07105
- Reference count: 8
- System was overall winner on both competition tasks, with average rank of 1.5

## Executive Summary
This paper presents Opt-ICL, a system submission to the Learning With Disagreements (LeWiDi) competition that predicts how individual annotators rated instances across four datasets involving subjective judgments. The system leverages large language models' in-context learning capabilities by including rater examples at inference time, combined with a two-step meta-learning training procedure. The approach achieved first place on both competition tasks, demonstrating the effectiveness of meta-learning for handling rater-specific preferences in subjective judgment tasks.

## Method Summary
Opt-ICL employs a meta-learning approach to train language models for predicting individual annotator ratings on subjective judgment tasks. The system uses a two-stage training procedure: first, post-training on diverse datasets requiring in-context learning (Spectrum Tuning), then dataset-specific fine-tuning. At inference, the model includes relevant rater examples in-context to capture individual rating patterns and preferences. This approach capitalizes on the in-context learning capabilities of large language models to adapt to the preferences of specific raters without requiring explicit model updates.

## Key Results
- Achieved overall first place on both competition tasks with average rank of 1.5
- Including rater examples in-context was crucial for performance
- Dataset-specific fine-tuning provided additional benefits on larger datasets
- Post-training on other in-context learning datasets significantly improved performance on one dataset
- Performance improved with model scale

## Why This Works (Mechanism)
The system works by leveraging the inherent in-context learning capabilities of large language models to adapt to individual rater preferences without requiring explicit model updates. By including examples of how specific raters have evaluated similar instances in the past, the model can infer the rater's unique judgment criteria and apply them to new instances. The meta-learning approach, which involves training on diverse datasets requiring in-context learning, helps the model develop robust strategies for extracting relevant information from in-context examples and applying it effectively to new prediction tasks.

## Foundational Learning
- **In-context learning**: LLMs can perform new tasks by conditioning on input-output examples without parameter updates - needed to adapt to individual raters using their past examples, check by measuring performance with varying numbers of in-context examples
- **Meta-learning**: Learning across multiple tasks to improve generalization - needed to develop robust strategies for extracting rater preferences, check by comparing with non-meta-learned baselines
- **Rater-specific modeling**: Capturing individual annotator preferences rather than assuming universal judgments - needed because subjective tasks have high inter-rater disagreement, check by measuring performance on individual raters versus group averages

## Architecture Onboarding
- **Component map**: Pre-training data -> Spectrum Tuning (meta-learning) -> Dataset-specific fine-tuning -> Inference with in-context rater examples -> Rater-specific predictions
- **Critical path**: Input instance + rater examples -> In-context processing -> Preference inference -> Rating prediction
- **Design tradeoffs**: Tradeoff between model scale and computational cost, balance between number of in-context examples and context window limitations, choice between general meta-learning versus dataset-specific fine-tuning
- **Failure signatures**: Poor performance when rater examples are insufficient or unrepresentative, degradation when model scale is too small, overfitting when fine-tuning on small datasets
- **3 first experiments**: 1) Vary number of in-context rater examples to find optimal amount, 2) Compare meta-learning versus direct fine-tuning on new datasets, 3) Test performance across different model scales to verify scaling benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to a single competition with four specific datasets, limiting generalizability
- Lacks detailed statistical analysis of performance differences between conditions
- Limited information about system performance on edge cases with high rater disagreement

## Confidence
- **High confidence**: Competition results clearly show system effectiveness; methodology is well-described
- **Medium confidence**: Ablation study findings supported but lack detailed statistical analysis
- **Medium confidence**: Claims about model scale benefits are reasonable but lack specific supporting evidence

## Next Checks
1. Conduct controlled experiments varying the number and diversity of in-context rater examples to quantify the relationship between example quality and prediction accuracy
2. Test the system on additional datasets outside the competition to evaluate generalization to different types of subjective judgments and disagreement patterns
3. Perform statistical significance testing on ablation study results to determine which performance differences are robust versus those that could be due to random variation