---
ver: rpa2
title: Towards the Resistance of Neural Network Watermarking to Fine-tuning
arxiv_id: '2505.01007'
source_url: https://arxiv.org/abs/2505.01007
tags:
- frequency
- watermark
- components
- network
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network watermarking method that is
  robust to fine-tuning, weight scaling, and weight permutations. The key insight
  is that specific frequency components of convolutional filters remain stable during
  training when input features contain only low-frequency components.
---

# Towards the Resistance of Neural Network Watermarking to Fine-tuning

## Quick Facts
- **arXiv ID:** 2505.01007
- **Source URL:** https://arxiv.org/abs/2505.01007
- **Reference count:** 22
- **Key outcome:** Neural network watermarking method robust to fine-tuning, weight scaling, and weight permutations with 100% detection rates under various attacks.

## Executive Summary
This paper proposes a novel neural network watermarking method that embeds ownership information into specific frequency components of convolutional filters. The key insight is that certain frequency components remain stable during training when input features contain only low-frequency components. The method provides theoretical guarantees for robustness against fine-tuning, weight scaling, and permutation attacks, and includes an additional loss term that degrades network performance when watermarks are removed.

## Method Summary
The method embeds ownership information into specific frequency components of convolutional filters that remain stable during training when input features are low-frequency. A parallel watermark module containing a low-pass filter followed by a convolutional layer is added to the backbone network. The watermark is extracted from frequency components using a revised DFT, with invariant frequencies identified through theoretical analysis. An adversarial binding loss is added during training that forces the network to map inputs to a pseudo-category when watermark weights are corrupted, providing defense against overwriting attacks.

## Key Results
- Watermark detection rates of 100% under fine-tuning attacks on multiple datasets
- 100% detection rates under weight scaling and permutation attacks
- Model accuracy drops from ~90% to ~40% when overwriting attacks are applied to models trained with binding loss
- Theoretical guarantees for robustness based on frequency domain analysis

## Why This Works (Mechanism)

### Mechanism 1
Specific frequency components of convolutional filters remain approximately invariant during gradient descent if input features are low-frequency. The paper derives the gradient update rule in the frequency domain, showing that if input $F_X$ is constrained to low frequencies, the coefficient evaluates to zero at specific orthogonal frequencies, mathematically forcing the gradient component to zero. The core assumption is that the input feature to the watermark module effectively contains only low-frequency components (enforced by a low-pass filter). Break condition: If the low-pass filter leaks high-frequency information or fine-tuning learning rate is excessively high.

### Mechanism 2
The watermark survives weight scaling and permutation attacks because frequency components transform equivariantly. The watermark is embedded in the direction (cosine similarity) of frequency vectors rather than raw values. Scaling weights $W \to aW$ scales frequencies $F \to aF$, preserving direction. Permuting filters permutes frequency vectors, which can be re-aligned via a matching algorithm during detection. Break condition: If detection algorithm fails to solve the permutation matching problem in layers with many filters.

### Mechanism 3
Overwriting attacks degrade model performance via an adversarial "binding" loss. The network is trained with an additional loss term that simulates an attack and forces the network to map inputs to a "pseudo-category" in that corrupted state. This creates a dependency where the network's useful knowledge is contingent on the specific configuration of watermark weights. Break condition: If the attacker can surgically overwrite weights while preserving the specific mathematical properties the loss function protects.

## Foundational Learning

- **Concept:** Discrete Fourier Transform (DFT) on Convolutional Kernels
  - **Why needed here:** The method relies on extracting frequency components from spatial kernel weights using a revised DFT. You must understand how spatial weights map to spectral amplitudes to interpret the "invariant frequencies."
  - **Quick check question:** If a $3 \times 3$ kernel has all equal weights (1.0), is its energy concentrated in low or high frequencies?

- **Concept:** Circular Convolution & Padding
  - **Why needed here:** Theorem 3.1 assumes circular padding. Standard convolutions (zero-padding) introduce high-frequency artifacts at borders. Understanding this difference is critical for why the "revised DFT" and low-pass filtering are necessary for the math to hold.
  - **Quick check question:** Why does zero-padding introduce high-frequency components that might violate the low-frequency input assumption?

- **Concept:** Equivariance vs. Invariance
  - **Why needed here:** The paper claims "equivariance" to scaling/permutation (the watermark changes predictably) but "invariance" to fine-tuning (the watermark stays the same). Distinguishing these determines how you design the detector (matching vs. direct comparison).
  - **Quick check question:** If a filter is scaled by 2, does the *value* of the frequency component change? Does the *direction* (cosine sim) change?

## Architecture Onboarding

- **Component map:** Backbone Network -> Watermark Module (Low-Pass Filter -> Convolutional Layer) -> Frequency Extractor -> Loss Aggregator
- **Critical path:** The connection between the Low-Pass Filter cutoff ($r$) and the Invariant Frequency Set ($S'$). If $r$ is too large, the gradient at $S'$ becomes non-zero and the watermark drifts during fine-tuning.
- **Design tradeoffs:**
  - **Robustness vs. Capacity:** Setting the low-pass filter cutoff $r$ too low ensures invariance but may render the Watermark Module useless for the main task.
  - **Sensitivity vs. Accuracy:** High $\lambda$ for adversarial loss ensures the model "breaks" if stolen/rewritten, but might harm clean accuracy of the original model.
- **Failure signatures:**
  - **Spectral Drift:** Heatmaps of $\Delta F_W$ show non-zero values at indices $(3i, 3j)$. Fix: Tighten the low-pass filter $\Lambda$.
  - **Dead Module:** $L_{attack}$ drives all outputs to the pseudo-class even without attack. Fix: Reduce noise magnitude $\epsilon$ or loss weight $\lambda$.
- **First 3 experiments:**
  1. **Validation of Invariance:** Train the model, extract $F_W$, fine-tune on a distinct dataset, and re-extract $F_W$. Plot the difference heatmap to verify zeros at specific $(u,v)$ coordinates.
  2. **Overwriting "Booby-Trap" Test:** Train with $L_{attack}$. Attempt to overwrite the weights with random noise. Verify that classification accuracy drops below a usable threshold (e.g., < 50%).
  3. **Detection Rate vs. Permutation:** Apply random channel permutations to the watermark module weights and run the detection algorithm (Eq. 14). Confirm the matching process recovers 100% detection.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proof assumptions depend on strict conditions about input spectra and filter dimensions that may not hold exactly in practice
- Experimental scope limited to CIFAR-10, CIFAR-100, Caltech-101, and Caltech-256 datasets using AlexNet and ResNet18 architectures
- Parallel module integration ambiguity regarding how exactly the watermark module output connects to the backbone

## Confidence

- **High Confidence:** The equivariance properties of frequency components to weight scaling and permutation are mathematically sound and supported by experimental results showing 100% detection rates.
- **Medium Confidence:** The theoretical invariance of specific frequency components during fine-tuning is mathematically proven under idealized conditions, but practical implementation factors may introduce deviations not fully explored.
- **Low Confidence:** The adversarial binding mechanism's effectiveness across diverse attack strategies and the practical significance of performance degradation under attack require further validation.

## Next Checks

1. **Practical Invariance Validation:** Implement the watermark module and train a model on CIFAR-10. Extract frequency components, then fine-tune on a completely different dataset (e.g., Tiny ImageNet). Measure the heatmap difference and verify that invariant frequencies in $S'$ show minimal change as claimed.

2. **Binding Loss Effectiveness Test:** Train a model with $L_{attack}$ on CIFAR-10. Systematically overwrite different percentages of watermark module weights (0%, 25%, 50%, 100%) and measure the corresponding accuracy degradation to quantify the practical impact of the binding mechanism.

3. **Permutation Matching Robustness:** Apply random permutations to watermark module filters in a trained model and implement the detection algorithm (Eq. 14). Test with varying numbers of filters (64, 128, 256) to assess how permutation complexity affects detection accuracy and computational feasibility.