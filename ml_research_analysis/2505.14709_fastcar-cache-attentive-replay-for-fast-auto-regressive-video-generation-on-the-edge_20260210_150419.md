---
ver: rpa2
title: 'FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation
  on the Edge'
arxiv_id: '2505.14709'
source_url: https://arxiv.org/abs/2505.14709
tags:
- generation
- arxiv
- video
- replay
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck in auto-regressive
  video generation, where MLP modules dominate latency during decoding. The authors
  propose FastCar, a framework that exploits temporal redundancy between adjacent
  frames by reusing cached MLP outputs from previous frames when a temporal attention
  score exceeds a threshold.
---

# FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge

## Quick Facts
- arXiv ID: 2505.14709
- Source URL: https://arxiv.org/abs/2505.14709
- Reference count: 40
- This paper addresses the computational bottleneck in auto-regressive video generation, where MLP modules dominate latency during decoding. The authors propose FastCar, a framework that exploits temporal redundancy between adjacent frames by reusing cached MLP outputs from previous frames when a temporal attention score exceeds a threshold. Theoretical analysis establishes that high temporal attention scores correlate with small MLP output differences, justifying the replay strategy. A hardware accelerator with dynamic resource scheduling is developed for FPGA deployment. Experiments show FastCar achieves over 2.1× decoding speedup and higher energy efficiency compared to sparse attention methods, while maintaining better generation quality and alleviating drifting issues when combined with sparse attention.

## Executive Summary
This paper introduces FastCar, a framework for accelerating auto-regressive video generation by exploiting temporal redundancy between adjacent frames. The key insight is that MLP modules in the decode phase dominate inference latency, and when temporal attention scores between aligned tokens in consecutive frames are high, the MLP outputs can be cached and replayed from the previous frame. This avoids redundant computation while maintaining generation quality. The framework includes a hardware accelerator with dynamic resource scheduling for efficient FPGA deployment.

## Method Summary
FastCar accelerates auto-regressive video generation by caching and replaying MLP outputs from previous frames when temporal attention scores exceed a threshold. The method computes a Temporal Attention Score (TAS) between each token and its spatially-aligned counterpart in the previous frame during attention computation. If the mean TAS across heads exceeds threshold τ, the cached MLP output from the previous frame is reused; otherwise, the MLP is computed normally and the output is cached. A hardware accelerator with dynamic resource scheduling (DRS) manages the irregular workload patterns caused by variable replay decisions across batches and cores. The framework is evaluated on VILA-U with LLaMA-2-7B backbone and RQ-VAE, showing over 2.1× speedup while maintaining VBench scores above 71%.

## Key Results
- Achieves over 2.1× decoding speedup compared to dense attention baseline
- Maintains VBench scores above 71% even at 80% replay ratio
- Combines with sparse attention to recover from quality degradation (VBench from 60%→71%)
- Demonstrates higher energy efficiency on FPGA with dynamic resource scheduling

## Why This Works (Mechanism)

### Mechanism 1: MLP Output Caching via Temporal Attention Score (TAS)
- Claim: When the Temporal Attention Score exceeds a threshold, MLP outputs from the previous frame can be reused with bounded error.
- Mechanism: The TAS (scaled dot-product between query vector of current token and key vector of its spatially-aligned token from the previous frame) correlates with MLP output similarity across frames. High TAS indicates the token is attending strongly to the same spatial location in the prior frame, suggesting temporal consistency in the hidden representations. The paper proves theoretically (Theorem 4.7) that MLP output difference is bounded by input difference plus √(1−TAS).
- Core assumption: Adjacent frames in video generation exhibit high temporal redundancy in MLP activations; this redundancy is predictable from attention patterns.
- Evidence anchors:
  - [abstract] "TAS is proposed to determine whether to apply the replay strategy... with detailed theoretical analysis and justification"
  - [Section 4.4] Theorem 4.7: "∥Yj,: − Yj−,:∥2 ≤ C(∥Xj,: − Xj−,:∥2 + √(1 − st,i + γM))"
  - [Section 3] Figure 2 shows cosine similarity between MLP outputs of neighboring frames ranges 0.4–0.8+
  - [corpus] TokenTrim (arXiv:2602.00268) similarly exploits temporal redundancy for AR video generation

### Mechanism 2: MLP-Dominated Decode Phase Bottleneck
- Claim: In AR video generation (unlike diffusion), MLP modules dominate inference latency during decoding, not attention.
- Mechanism: AR models generate tokens sequentially; attention only operates on the growing KV cache marginally per step. However, MLP processes all hidden dimensions fully at each token generation. The paper's profiling (Figure 1) shows MLP latency exceeds attention by ~5× across sequence lengths 2k–8k.
- Core assumption: This bottleneck is structural to the AR decode paradigm and persists across model scales.
- Evidence anchors:
  - [Section 3] "MLP modules consistently dominate the overall latency" with detailed breakdown in Figure 1
  - [abstract] "MLP modules in the decode phase dominate the inference latency"
  - [Section 3] "efficiency-oriented techniques designed for attention modules are less effective in AR"
  - [corpus] Sparse-to-Dense (arXiv:2505.19155) observes similar attention sparsity patterns in Video-LLMs during decoding

### Mechanism 3: Dynamic Resource Scheduling (DRS) for Irregular Workloads
- Claim: Runtime workload imbalance from cache replay can be efficiently managed with lightweight scheduling logic.
- Mechanism: When replay triggers variably across batches/cores, DRS uses an Index Register (32-bit, tracking replay vs. compute status) and Mapping Registers (round-robin assignment) to dynamically dispatch work to available cores. This avoids static instruction bloat from enumerating all replay patterns.
- Core assumption: Replay decisions are sufficiently predictable within a decode step that lightweight scheduling overhead (hundreds of cycles) is negligible vs. MLP computation (thousands of cycles).
- Evidence anchors:
  - [Section 5] "DRS incurs minimal overhead by completing its dispatch operations in just hundreds to thousands of cycles"
  - [Section 5] Figure 3 shows the hardware architecture with DRS module
  - [corpus] No direct corpus evidence for DRS-specific techniques; this appears novel to this work

## Foundational Learning

- Concept: **Auto-Regressive (AR) Video Generation Paradigm**
  - Why needed here: FastCar is specific to AR decode, not diffusion; understanding why MLP dominates requires knowing tokens are generated sequentially with growing KV cache.
  - Quick check question: Why does sparse attention (e.g., StreamingLLM) fail to accelerate AR video generation significantly?

- Concept: **Temporal Redundancy in Video Tokens**
  - Why needed here: The entire replay strategy rests on adjacent frames having similar MLP outputs; you need intuition for when this holds vs. breaks.
  - Quick check question: Would you expect higher replay ratios for slow camera pans or rapid action sequences? Why?

- Concept: **Lipschitz Continuity in Neural Networks**
  - Why needed here: The theoretical justification (Theorems 4.4–4.7) relies on bounded operator norms and Lipschitz assumptions to connect TAS to MLP output bounds.
  - Quick check question: What does the γM term in Theorem 4.4 represent, and when would it be large?

## Architecture Onboarding

- Component map:
  - Attention Module -> Compute Q/K/V projections and TAS (free, already computed)
  - MLP Module -> SwiGLU-style computation (act(XWG) ◦ XWU)WD; target for caching/replay
  - Replay Cache -> Stores MLP outputs from frame t−1 keyed by spatial position
  - Threshold Comparator -> Evaluates mean TAS against τ; gates MLP vs. replay
  - DRS Module -> Index Register + Mapping Registers; dispatches batches to cores dynamically

- Critical path:
  1. Compute attention → extract TAS (free, already computed)
  2. Compare mean TAS to threshold τ
  3. If τ exceeded: fetch from Replay Cache; else: compute MLP and update cache
  4. DRS dispatches instruction streams to available cores based on which batches skipped MLP

- Design tradeoffs:
  - Lower τ → higher replay ratio → faster but potential quality degradation (Table 1: 80% replay keeps VBench >71%)
  - Layer-wise vs. consistent thresholds: Ablation (Figure 4) shows consistent thresholds perform better
  - Intermediate layers show lower replay ratios (Figure 5), suggesting they encode temporal dynamics

- Failure signatures:
  - **Drifting in sparse attention**: SA methods degrade VBench from 74%→60% (Table 1); FastCar+SA combination (Table 2) recovers to 71%+
  - **Quality collapse at very high replay**: If τ set too aggressively without monitoring PSNR/SSIM, output may diverge
  - **Workload imbalance on multi-core**: Without DRS, some cores idle while others process non-replay batches

- First 3 experiments:
  1. **Reproduce latency profiling**: Measure MLP vs. attention latency breakdown at 2k/4k/8k sequence lengths on your hardware; verify MLP dominance.
  2. **Sweep threshold τ**: Plot replay ratio vs. VBench score using τ ∈ {0, −0.5, −1, −2, −4, −8}; identify the "knee" where quality degrades sharply.
  3. **Visualize replay distribution across layers**: For a fixed τ, record which layers/positions trigger replay; confirm intermediate layers have lower replay ratios (per Figure 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FastCar generalize effectively to other auto-regressive video generation architectures beyond VILA-U (e.g., NOVA, ART·V, VideoPoet)?
- Basis in paper: [explicit] The conclusion states: "In future work, we plan to extend our framework to a broader range of model families."
- Why unresolved: Experiments only validate on VILA-U with LLaMA-2-7B backbone. Other architectures may have different MLP/attention dynamics or temporal redundancy patterns.
- What evidence would resolve it: Benchmark results applying FastCar to diverse AR video models, measuring speedup and quality preservation across architectures.

### Open Question 2
- Question: Can the threshold τ be learned adaptively rather than manually tuned for optimal trade-offs between speed and quality?
- Basis in paper: [inferred] The threshold τ is manually set, and ablation studies (Figure 4, Table 6) show τ significantly impacts replay ratio and quality. No adaptive mechanism is proposed.
- Why unresolved: Manual threshold selection requires separate tuning per model or task; the optimal τ varies (e.g., τ ≈ -8 achieves 87% replay ratio).
- What evidence would resolve it: Demonstration of a learned or content-adaptive threshold mechanism achieving comparable or better quality-speed trade-offs without manual tuning.

### Open Question 3
- Question: How does FastCar scale to longer video durations (beyond 8 frames) and higher resolutions (beyond 256×256)?
- Basis in paper: [inferred] Experiments are limited to 8 frames at 256×256 resolution. The paper claims advantages for "high-resolution and long-duration video generation" but does not empirically validate this.
- Why unresolved: Temporal redundancy patterns may differ across longer frame sequences; cache management overhead may increase with longer videos.
- What evidence would resolve it: Scaling experiments on 16, 32, 64+ frames and 512×512 or higher resolutions, measuring latency, memory, and quality degradation.

## Limitations
- Theoretical generalization gap: The theoretical bounds assume Lipschitz continuity and may not fully capture real-world model behavior across different architectures
- Quality degradation threshold: Manual threshold selection requires separate tuning, and the relationship between replay ratio and perceptual quality is not fully characterized
- Hardware implementation overhead: Complete FPGA resource utilization and power consumption measurements are not provided

## Confidence

**High Confidence**: MLP modules dominate decode-phase latency in AR video generation (Section 3 profiling, Figure 1); TAS can be computed without extra attention computation (leveraging existing dot-products); FastCar achieves >2.1× speedup over dense attention baseline (Table 1, Figure 4).

**Medium Confidence**: The theoretical bound connecting TAS to MLP output similarity (Theorem 4.7) is mathematically sound but may not fully capture real-world model behavior; replay ratios of 17-87% maintain acceptable quality (VBench >71%) across different settings; DRS overhead is negligible relative to MLP computation.

**Low Confidence**: Generalization of TAS-based replay across different AR video generation architectures (beyond VILA-U); exact FPGA resource utilization and power efficiency measurements; behavior under extreme conditions (rapid motion, scene changes, out-of-distribution content).

## Next Checks
1. **Cross-Architecture TAS Validation**: Apply the FastCar replay mechanism to a different AR video generation model (e.g., Sora-style or LongVILA) and measure whether the TAS-MLP output similarity relationship and quality-speed tradeoffs generalize. This tests the mechanism's architecture independence.

2. **Perceptual Quality Boundary Testing**: Systematically sweep τ from very high (minimal replay) to very low (maximal replay) while measuring VBench, PSNR, SSIM, LPIPS, and conducting human perceptual studies to identify the precise quality degradation threshold and its relationship to TAS distributions.

3. **FPGA Resource and Power Profiling**: Implement the complete FastCar system on FPGA and measure actual LUT, BRAM, DSP utilization, and power consumption. Compare the end-to-end energy efficiency (GFLOPs/W) against both the dense baseline and sparse attention methods to validate the claimed efficiency gains.