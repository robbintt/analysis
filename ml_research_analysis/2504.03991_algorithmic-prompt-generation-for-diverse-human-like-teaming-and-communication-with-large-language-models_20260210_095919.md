---
ver: rpa2
title: Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication
  with Large Language Models
arxiv_id: '2504.03991'
source_url: https://arxiv.org/abs/2504.03991
tags:
- agents
- prompt
- communication
- human
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse, human-like
  teaming behaviors in collaborative multi-agent environments without requiring large-scale
  human data collection. The proposed PLAN-QD framework combines Quality Diversity
  (QD) optimization with LLM-powered agents to algorithmically generate prompts that
  elicit diverse team behaviors in a cooking game domain (Steakhouse).
---

# Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models

## Quick Facts
- arXiv ID: 2504.03991
- Source URL: https://arxiv.org/abs/2504.03991
- Reference count: 40
- Primary result: PLAN-QD framework generates diverse LLM agents that replicate human teaming behaviors without requiring large-scale human data collection

## Executive Summary
This paper presents PLAN-QD, a framework that combines Quality Diversity optimization with large language models to generate diverse, human-like teaming behaviors in collaborative multi-agent environments. The approach addresses the challenge of creating diverse LLM agents that can model human teaming dynamics without extensive human data collection. PLAN-QD uses a mutator LLM to iteratively improve prompts based on measure functions, creating an archive of diverse high-performing agents in a cooking game domain (Steakhouse). The framework was validated through a human-subjects study showing that PLAN-QD successfully replicated human behavioral trends and achieved significantly higher coverage compared to random prompt generation.

## Method Summary
PLAN-QD combines Quality Diversity (QD) optimization with LLM-powered agents to algorithmically generate prompts that elicit diverse team behaviors. The framework uses a mutator LLM to iteratively improve prompts based on measure functions that capture human behavior diversity. These measure functions track metrics such as recipe usage rates, time spent in different kitchen areas, and communication patterns. The mutator proposes prompt variations that either increase a specific measure or decrease the highest-performing measure to encourage diversity. PLAN-QD was validated in a cooking game domain (Steakhouse) using a human-subjects study with 54 participants to establish behavioral baselines.

## Key Results
- PLAN-QD achieved 15-38% improvement in coverage metrics compared to random prompt generation
- Agents matched human communication effects in 12/16 layout-metric combinations
- Framework successfully captured both explicitly targeted behaviors and additional diverse behaviors beyond human study observations

## Why This Works (Mechanism)
PLAN-QD works by leveraging the iterative nature of Quality Diversity optimization combined with LLM capabilities for prompt generation and mutation. The framework treats prompt generation as an optimization problem where the search space is the space of possible prompts. The mutator LLM explores this space by generating variations that target specific behavioral measures, while the archive maintains diversity by ensuring multiple solutions exist for different behavioral patterns. The measure functions act as objectives that guide the search toward human-like behaviors without requiring direct access to human data during training.

## Foundational Learning
- Quality Diversity optimization: A class of algorithms that maintain collections of diverse, high-performing solutions rather than single optima. Needed to generate varied team behaviors; check by verifying archive maintains solutions with distinct measure values.
- Measure functions: Quantitative metrics that capture specific aspects of behavior (e.g., communication frequency, task completion rates). Needed to guide prompt generation toward human-like behaviors; check by validating measures correlate with observed human behavior patterns.
- LLM-powered mutators: Using LLMs to generate and modify prompts based on behavioral objectives. Needed to explore prompt space efficiently; check by comparing mutator-generated prompts against random variations.
- Behavioral archiving: Maintaining collections of diverse agent behaviors rather than optimizing for single best performance. Needed to capture full spectrum of human teaming dynamics; check by analyzing archive diversity metrics.

## Architecture Onboarding

Component map:
Human behavior study -> Measure function definition -> PLAN-QD framework (Mutator LLM -> Archive -> Agent evaluation) -> Behavioral comparison

Critical path:
Measure function definition → Mutator LLM prompt generation → Agent behavior evaluation → Archive update → Performance measurement

Design tradeoffs:
- Explicit measure targeting vs. emergent behavior discovery
- Prompt diversity vs. task performance optimization
- Computational cost of iterative LLM-based mutation vs. scalability

Failure signatures:
- Archive converges to narrow behavioral patterns
- Mutator gets stuck in local optima of prompt space
- Generated agents fail to capture measured human behaviors
- Measure functions inadequately represent true human behavior diversity

First experiments:
1. Run PLAN-QD with single measure function to verify basic optimization capability
2. Test archive diversity maintenance by measuring behavioral similarity between stored solutions
3. Compare mutator LLM performance against random prompt generation baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Validation limited to single domain (cooking game/Steakhouse) with small human study (n=54)
- Measure functions may not fully capture richness of human teaming dynamics
- Reliance on LLM capabilities introduces potential biases from training data

## Confidence
- High: PLAN-QD outperforms random prompt generation in coverage metrics
- Medium: PLAN-QD successfully replicates human behavioral trends
- Low: Generalizability to other domains and collaborative tasks

## Next Checks
1. Replicate study across multiple collaborative domains (emergency response, team sports, collaborative design) to test generalizability
2. Conduct larger-scale human-subjects study with diverse participant pools to validate behavioral measures
3. Compare PLAN-QD against alternative diversity-promoting methods (evolutionary strategies, RL with diversity rewards)