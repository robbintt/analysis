---
ver: rpa2
title: Scaling Test-Time Compute Without Verification or RL is Suboptimal
arxiv_id: '2502.12118'
source_url: https://arxiv.org/abs/2502.12118
tags:
- policy
- compute
- reward
- test-time
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the effectiveness of verifier-based (VB) vs.
  verifier-free (VF) approaches for scaling test-time compute in large language models
  (LLMs).
---

# Scaling Test-Time Compute Without Verification or RL is Suboptimal

## Quick Facts
- arXiv ID: 2502.12118
- Source URL: https://arxiv.org/abs/2502.12118
- Authors: Amrith Setlur; Nived Rajaraman; Sergey Levine; Aviral Kumar
- Reference count: 40
- Primary result: Verifier-based methods outperform verifier-free methods for scaling test-time compute in LLMs, with performance gap widening as compute and training data increase.

## Executive Summary
This paper analyzes the effectiveness of verifier-based (VB) versus verifier-free (VF) approaches for scaling test-time compute in large language models. The authors prove that VB methods, which use verification signals during training, achieve better performance and scale more efficiently as test-time compute increases compared to VF methods that rely on supervised finetuning of "expert" traces without verification. Empirically, on both didactic and math reasoning problems using 3B/8B/32B LLMs, the authors find that VB methods consistently outperform VF methods, with the performance gap widening as test-time compute and training data increase. The study reveals that common pre-trained LLMs exhibit the required properties (heterogeneity and anti-concentration) that enable VB methods to scale test-time compute effectively.

## Method Summary
The paper compares VB and VF methods for test-time compute scaling. VB methods train a verifier to predict bi-level rewards (0 until correct answer, then 1) and use it to select high-reward trajectories. VF methods use supervised finetuning on expert traces without verification signals. Both approaches are evaluated on MATH problems with varying token budgets (H), comparing accuracy as H scales. The theoretical analysis shows VB methods achieve O(H/n) suboptimality while VF methods have Œ©(‚àöH/‚àön) lower bound under conditions of base policy heterogeneity and anti-concentration.

## Key Results
- VB methods achieve O(H/n) suboptimality while VF methods have Œ©(‚àöH/‚àön) lower bound
- The VB-VF performance gap scales as Œ©(‚àöH) when jointly scaling test compute and training data
- Common pre-trained LLMs exhibit heterogeneity and anti-concentration properties enabling VB effectiveness
- Empirical results show VB consistently outperforms VF across 3B/8B/32B models with widening gap as H and n increase

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Verifier-free methods suffer suboptimality that scales with base policy heterogeneity
- **Mechanism:** When expert traces have high variance in reward-to-go across different correct solutions (heterogeneity), any supervised finetuning algorithm must resolve uncertainty about which expert generated the data. The information-theoretic lower bound shows suboptimality of Œ©(œÉ‚Çë/‚àön), where œÉ‚Çë (expert heterogeneity) scales as O(H) in practice. Intuitively, VF methods must "memorize" diverse traces without knowing which paths generalize.
- **Core assumption:** The expert policy lies within a bounded œá¬≤-divergence ball around the base policy (œÄ‚Çë ‚àà Œ†Œ∫), reflecting practical constraints that expert traces must be somewhat close to base model outputs.
- **Evidence anchors:**
  - [Theorem 5.4]: "Given any œÅ, r, œÄb, expert policy œÄe and k ‚â§ |ùí≥|/4... for any verifier-free algorithm... max J(œÄ‚Ä≤) ‚àí J(œÄÃÇvf_n) = Œ©(ÃÉœÉ‚Çë‚àö(log|Œ†‚Ä≤|/n))"
  - [Section 5.1]: Establishes that expert heterogeneity œÉ‚Çë relates to base heterogeneity œÉb via Lemma 5.3
  - [Corpus: "Sample, Scrutinize and Scale"]: Related work on sampling-based search scaling, supporting the broader paradigm but not directly addressing this mechanism
- **Break condition:** When base policy has low heterogeneity (œÉb << H), VF methods can achieve comparable performance (see Figure 6 in paper)

### Mechanism 2
- **Claim:** Verifier-based methods achieve suboptimality O(H/n) independent of heterogeneity under anti-concentration
- **Mechanism:** VB methods use reward annotations to identify high-reward regions of the policy space directly. The anti-concentration property (Property 5.6) ensures the base policy has constant probability mass on traces with rewards exceeding the mean by œÉ‚àöŒµ. This coverage allows VB methods to find improved policies without needing to "guess" which heterogeneous traces are optimal‚Äîthe reward signal resolves this uncertainty.
- **Core assumption:** Base policy is c‚ÇÄ-anticoncentrated: for each problem x, Pr[r(œÑ) ‚â• E[r(œÑ)] + œÉb,x‚àöŒ∫] ‚â• c for constant c > 0. Verified empirically in Figure 10 (‚âà0.25-anticoncentrated).
- **Evidence anchors:**
  - [Theorem 5.7]: "J(œÄÃÑŒ∫) ‚àí J(œÄÃÇvb_n) <~ (1/c‚ÇÄ)¬∑H¬∑log(|‚Ñõ|/Œ¥)/n"
  - [Property 5.6]: Formal definition of anti-concentration
  - [Figure 10]: Empirical verification on MATH easy/hard problems showing ‚âà0.25-0.27 anti-concentration coefficient
- **Break condition:** If anti-concentration fails (reward distribution too sharp), VB methods lose their advantage; also if verifier accuracy degrades significantly (Appendix C, Figure 12)

### Mechanism 3
- **Claim:** The VB-VF performance gap scales as Œ©(‚àöH) when jointly scaling test compute and training data
- **Mechanism:** Combining the lower bound (VF: Œ©(H/‚àön)) and upper bound (VB: O(H/n)), when n scales linearly with H (n = Œ©(H)), VB achieves O(1) suboptimality while VF retains Œ©(‚àöH) dependency. The gap emerges from VF's inability to resolve expert uncertainty without reward signals, whereas VB's pessimistic verification (Algorithm 1, step 3) directly optimizes for high-reward regions covered by the base policy.
- **Core assumption:** Heterogeneity œÉb = Œ©(H) and anti-concentration both hold; the reward function is bi-level (Property 4.1)
- **Evidence anchors:**
  - [Theorem 5.8]: "J(œÄÃÇvb_n) ‚àí J(œÄÃÇvf_n) = Œ©(H/‚àön)"
  - [Figure 5c, 7c]: Super-linear gap growth when scaling both H and n
  - [Corpus: "To Backtrack or Not to Backtrack"]: Discusses when sequential search limits reasoning, relevant to understanding compute efficiency boundaries
- **Break condition:** If training data n doesn't scale with H, or if base model lacks heterogeneity (homogeneous correct solutions)

## Foundational Learning

- **Concept:** Bi-level rewards
  - **Why needed here:** The paper formalizes test-time compute efficiency using bi-level rewards‚Äîrewards stay 0 until correct solution, then 1 thereafter. This captures both accuracy and token-efficiency, enabling theoretical analysis of compute scaling.
  - **Quick check question:** Given a trajectory of length H that finds the correct answer at token h, what is the total bi-level reward? (Answer: H - h + 1)

- **Concept:** œá¬≤-divergence and coverage
  - **Why needed here:** The theoretical framework constrains expert policies to lie within œá¬≤-divergence balls around the base policy. This formalizes the practical constraint that SFT data must come from "reasonable" model outputs, not arbitrary distributions.
  - **Quick check question:** Why use œá¬≤ instead of KL divergence for the expert constraint? (Answer: Simplicity for analysis per Section 5; also Lemma A.2 shows change-of-measure bounds)

- **Concept:** Anti-concentration (Erd≈ës 1945)
  - **Why needed here:** This is the critical condition enabling VB superiority. It requires non-trivial probability mass on rewards exceeding the mean‚Äîensuring the base policy "covers" improvable regions.
  - **Quick check question:** If a base policy has mean reward 10 with std 5 on a problem, and Œ∫=0.5, what anti-concentration threshold must be exceeded for c-anticoncentration? (Answer: E[r] + œÉ‚àöŒ∫ ‚âà 10 + 3.54 ‚âà 13.54)

## Architecture Onboarding

- **Component map:** Base LLM (œÄb) -> Verifier (rÃÇ) -> VB Policy (œÄÃÇvb_n) OR Expert Traces -> VF Policy (œÄÃÇvf_n)
- **Critical path:**
  1. Characterize base LLM heterogeneity (compute œÉb,x across problem distribution)
  2. Verify anti-concentration coefficient (estimate c‚ÇÄ from reward distribution)
  3. Collect n trajectories from base LLM, annotate with bi-level rewards
  4. Train verifier using least-squares (or 0/1 loss variant)
  5. Apply pessimistic optimization (Algorithm 1, step 3) or run BoN search at test time

- **Design tradeoffs:**
  - **More heterogeneous base models:** Wider VB-VF gap, but requires better verifier accuracy
  - **Larger training budget n:** Helps both methods, but VB gains more (1/n vs 1/‚àön scaling)
  - **Longer test horizon H:** Amplifies VB advantage, but increases verifier classification difficulty

- **Failure signatures:**
  - Verifier accuracy drops at high H (see Appendix C, Figure 12): reward hacking in RL
  - Low heterogeneity (œÉb << H): VF may outperform VB (Figure 6)
  - Sharp reward distribution: anti-concentration fails, VB loses advantage

- **First 3 experiments:**
  1. **Heterogeneity measurement:** Sample multiple correct traces per problem from base LLM, compute œÉb,x variance in bi-level rewards; bucket problems by heterogeneity and compare SFT vs BoN performance (replicate Figure 9)
  2. **Anti-concentration verification:** For each problem, plot distribution of bi-level rewards from base LLM; compute fraction exceeding E[r] + œÉ‚àöŒ∫ threshold; verify c‚ÇÄ ‚âà 0.2-0.3 range (replicate Figure 10)
  3. **Scaling gap validation:** Fix base model, train separate VF and VB systems at varying (n, H) combinations with n ‚àù H; plot accuracy gap vs H to verify super-linear growth (replicate Figure 5c, 7c patterns)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the separation between verifier-based (VB) and verifier-free (VF) methods change when using dense process-based rewards instead of sparse outcome rewards?
- Basis in paper: [explicit] Section 8 states: "Future work should consider building on our analysis to compare verifier-based algorithms that query sparse vs. dense forms of verification, e.g., process-based rewards."
- Why unresolved: The paper's theory and experiments rely on bi-level (sparse) rewards; the dynamics of dense feedback remain uncharacterized.
- What evidence would resolve it: Theoretical bounds or empirical scaling curves comparing VB methods using process rewards versus those using outcome rewards across varying compute budgets.

### Open Question 2
- Question: Can the theoretical separation between VB and VF methods be extended to generative reward functions?
- Basis in paper: [explicit] Section 8 proposes extending the analysis of bi-level rewards "to other classes of reward functions, including generative rewards."
- Why unresolved: The current theoretical proofs rely specifically on the structure of bi-level rewards, which generative rewards may not satisfy.
- What evidence would resolve it: A formal proof demonstrating that the ‚àöH gap holds, shrinks, or disappears when using generative reward models for verification.

### Open Question 3
- Question: How do VB and VF methods scale comparatively when test-time compute budgets exceed 32k tokens?
- Basis in paper: [explicit] Section 8 notes that training models for long contexts is expensive and that "an analysis of scaling behaviors... would be crucial."
- Why unresolved: Empirical experiments were constrained by compute costs, leaving the asymptotic scaling behavior at extremely long horizons unverified.
- What evidence would resolve it: Empirical benchmarks measuring the performance gap between VB and VF methods on reasoning tasks with token budgets significantly larger than 32k.

## Limitations
- Empirical validation limited to a single math reasoning dataset (MATH)
- Anti-concentration coefficient may vary across domains beyond math
- Theoretical analysis assumes expert traces lie within œá¬≤-balls around base policy
- Results may not generalize to tasks where correct solutions have low heterogeneity

## Confidence
- Theoretical claims (Theorem 5.4, 5.7, 5.8): High - rigorous proofs with clear assumptions
- Anti-concentration measurement: Medium - empirical verification but limited to single dataset
- VB-VF gap scaling with H: Medium - clear pattern in Figures 5c/7c but based on single domain
- Base model heterogeneity as key driver: Medium - demonstrated empirically but could vary by task

## Next Checks
1. Measure anti-concentration coefficient on diverse reasoning tasks (coding, commonsense, etc.) to test domain generalization
2. Compare VB methods against stronger VF baselines including RL-based approaches and beam search
3. Analyze performance when base model heterogeneity is artificially reduced to test the œÉb >> H condition