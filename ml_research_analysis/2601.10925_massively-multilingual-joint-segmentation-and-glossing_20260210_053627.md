---
ver: rpa2
title: Massively Multilingual Joint Segmentation and Glossing
arxiv_id: '2601.10925'
source_url: https://arxiv.org/abs/2601.10925
tags:
- glossing
- segmentation
- language
- glosses
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents PolyGloss, a multilingual model for joint
  segmentation and glossing of interlinear glossed text (IGT). The core method involves
  training a seq2seq model on extended IGT data using three task formats: multitask,
  concatenated, and interleaved, with interleaved format achieving best results.'
---

# Massively Multilingual Joint Segmentation and Glossing

## Quick Facts
- arXiv ID: 2601.10925
- Source URL: https://arxiv.org/abs/2601.10925
- Reference count: 26
- Primary result: PolyGloss achieves MER 0.234, F1 0.862, and alignment 1.0 across 9 languages

## Executive Summary
This paper introduces PolyGloss, a multilingual model for joint morphological segmentation and glossing of interlinear glossed text (IGT). The model uses a seq2seq architecture with an interleaved output format where each gloss label is immediately followed by its corresponding morpheme in parentheses, enforcing perfect alignment between segmentation and glosses. PolyGloss outperforms both GLOSSLM on glossing and various open-source LLMs on segmentation, glossing, and alignment. The model demonstrates strong cross-linguistic transfer benefits and can be rapidly adapted to new languages using LoRA with minimal training data.

## Method Summary
PolyGloss uses a ByT5-base encoder with byte-level tokenization to handle diverse scripts and morphological boundaries. The model is trained on an extended IGT corpus (353k examples across 2,077 languages) using three task formats: multitask, concatenated, and interleaved. The interleaved format, where glosses and morphemes are paired as "GLOSS(morpheme)", achieves the best results. Training involves continued pretraining with specific hyperparameters (lr=5E-5, batch=64, 15 epochs) on GH200 GPUs. The model generates interleaved outputs that are parsed to extract separate segmentation and gloss lines. Validation perplexity strongly correlates with glossing accuracy (r² = 0.951), enabling automatic quality estimation for unseen languages.

## Key Results
- PolyGloss achieves morpheme error rate (MER) of 0.234 and morpheme F1 score of 0.862 on average across nine evaluation languages
- Interleaved format achieves perfect alignment score (1.000) compared to ~0.97 for multitask format
- Strong multilingual transfer benefits: model outperforms GLOSSLM on glossing for all languages except Arapaho (the largest training language)
- Validation perplexity correlates strongly (r² = 0.951) with glossing performance, enabling automatic quality prediction
- LoRA adaptation enables rapid learning on new languages with 50-200 examples

## Why This Works (Mechanism)

### Mechanism 1
The interleaved output format enforces structural alignment between morpheme segments and glosses, improving interpretability without sacrificing accuracy. Each gloss label is immediately followed by its corresponding morpheme in parentheses (e.g., `you.know(wōlē)-ZERO(0)=ART(n)`). This creates a hard constraint: any well-formed output must have matching counts and positions. The causal language modeling objective then learns joint probability distributions over segment-gloss pairs rather than independent predictions. Core assumption: The model learns cross-task dependencies during training that generalize to unseen examples. Evidence: Interleaved model achieves MER 0.234, F1 0.862, and alignment 1.000.

### Mechanism 2
Multilingual joint training enables cross-linguistic transfer that benefits low-resource languages more than monolingual models. The shared ByT5 encoder learns morphological patterns that generalize across typologically similar languages. Languages with limited training data (e.g., Gitksan with 89 examples) leverage representations learned from well-resourced languages (e.g., Arapaho with 36k examples). Core assumption: Morphological analysis shares computational structure across languages despite surface differences. Evidence: For all languages except Arapaho, the multilingual model is far superior on glossing, demonstrating clear transfer learning benefits.

### Mechanism 3
Validation perplexity predicts downstream glossing accuracy, enabling automatic quality estimation for unseen languages. Perplexity measures how well the model's predicted distribution matches held-out text. Lower perplexity indicates the model has captured language-specific morphological patterns. The strong correlation (r² = 0.951) suggests perplexity captures variance in morphological knowledge, not just surface fluency. Core assumption: The relationship between perplexity and accuracy generalizes beyond the 9 evaluation languages. Evidence: Figure 2 shows linear relationship across all 9 languages.

## Foundational Learning

- Concept: Interlinear Glossed Text (IGT) format
  - Why needed here: Understanding the three-line structure (transcription, segmentation, glosses) is prerequisite to understanding what the model predicts and why alignment matters.
  - Quick check question: Given the Arapaho example `tááto(his) niisóó-(two)`, can you identify which part is the morpheme vs. the gloss?

- Concept: Seq2seq with byte-level tokenization
  - Why needed here: PolyGloss uses ByT5, which processes raw bytes rather than subwords—critical for handling rare scripts and morpheme boundary insertion without vocabulary issues.
  - Quick check question: Why would a subword tokenizer struggle to output the same input string with hyphens inserted for morpheme boundaries?

- Concept: Morpheme Error Rate (MER) vs. Accuracy
  - Why needed here: MER accounts for insertions/deletions in predicted sequences, whereas accuracy assumes fixed-length outputs. This matters when models over- or under-segment.
  - Quick check question: If a model predicts 5 glosses for a word that should have 3, why would accuracy be misleading but MER capture the error?

## Architecture Onboarding

- Component map: Raw text + translation -> ByT5 encoder (byte-level) -> Interleaved prompt template -> Autoregressive decoder -> Parse interleaved output to extract segmentation and gloss lines

- Critical path:
  1. Data preprocessing (align segmentation/gloss, format interleaved examples)
  2. Continued pretraining on PolyGloss corpus (341k examples)
  3. Evaluation via beam search (2 beams)
  4. Optional LoRA adaptation for new languages

- Design tradeoffs:
  - Interleaved format: Perfect alignment but requires parsing; Concatenated: Natural dependency but risk of error propagation; Multitask: Simple but misalignment risk
  - ByT5 vs. LLM: ByT5 handles byte-level segmentation well; LLMs with subword tokenizers struggled in ablation
  - Single multilingual model vs. per-language adaptation: Convenience vs. potential accuracy ceiling for high-resource languages

- Failure signatures:
  - Format collapse: Model outputs raw text without parentheses → check prompt formatting
  - Hallucinated morphemes: Gloss count ≠ segment count → switch from multitask to interleaved format
  - Poor low-resource performance: High perplexity on validation → add similar-language data or use LoRA
  - LoRA not adapting: No improvement with 50+ examples → increase rank or epochs

- First 3 experiments:
  1. Baseline sanity check: Run PolyGloss (interleaved) on one evaluation language (e.g., Arapaho), compute MER/F1/alignment. Compare against numbers in Table 3-5 to verify setup.
  2. Format ablation: Train three models (multitask, concatenated, interleaved) on a subset (10k examples). Measure alignment score to confirm interleaved advantage.
  3. LoRA adaptation test: Hold out one language entirely. Fine-tune with LoRA (rank 8, 50-200 examples). Plot MER vs. training size to reproduce Figure 6 pattern.

## Open Questions the Paper Calls Out

### Open Question 1
Can decoder-only instruction-tuned LLMs with appropriate hyperparameter tuning match or exceed ByT5's performance on joint segmentation and glossing? Basis: Appendix A states "we expect with the right hyperparameters, these models should be able to at least match the accuracy of the ByT5 model, which could be explored by future work." Unresolved because Qwen 0.6B experiments failed to converge to competitive losses despite manual tuning; authors hypothesize subword tokenization issues and pretraining mismatch as causes. Resolution: Successful training of decoder-only models matching ByT5 metrics across the nine evaluation languages with systematically tuned hyperparameters.

### Open Question 2
Can reinforcement learning with the alignment score as a verifiable reward, scaled to the full training dataset, improve glossing and segmentation performance? Basis: Section 9 notes "This approach could be scaled to the full training dataset as an additional post-training step." Unresolved because GRPO experiments were conducted only at small scale on Gitksan with 50 epochs, showing slight improvement. Resolution: Large-scale RL training across all languages showing significant MER and F1 improvements over the base interleaved model.

### Open Question 3
How effectively can RL-based alignment optimization adapt models to new languages that lack gold morphological segmentations? Basis: Section 9 proposes "RL could be used when adapting the model to a new language that lacks gold-labeled morphological segmentations, since the alignment score will force the segmentation to (minimally) be aligned with the gold glosses." Unresolved because the approach was proposed but not empirically validated; alignment score requires no gold segmentation. Resolution: Experiments adapting to unseen languages using only gloss annotations, comparing to LoRA adaptation with full IGT data.

### Open Question 4
Does the strong perplexity–performance correlation (r² = 0.951) generalize to languages beyond the nine evaluated? Basis: The correlation was computed on only nine languages; Section 6.1 frames perplexity as a "rough predictor" but validation breadth is limited. Unresolved because the evaluation set covers diverse languages but is small; languages with very different morphological typologies may not follow this pattern. Resolution: Perplexity–MER correlation computed on held-out languages from the 2,077-language corpus not used in evaluation.

## Limitations
- Dataset quality control: Extended corpus relies on automated deduplication and cleaning procedures with unspecified heuristics for filtering "very low-quality examples"
- Evaluation scope: Results based on only 9 evaluation languages with cross-validation rather than truly held-out languages
- Model comparison fairness: PolyGloss has access to additional training data sources not available to GLOSSLM

## Confidence
- High confidence: Interleaved format demonstrably improves alignment scores (achieving perfect 1.0 vs. ~0.97 for multitask format), and multilingual transfer benefits are well-supported
- Medium confidence: Perplexity-accuracy correlation (r² = 0.951) appears robust within 9 evaluation languages, but generalization to unseen languages remains to be tested
- Low confidence: Claims about handling "unseen languages" are based on cross-validation rather than truly held-out languages, and dataset quality control procedures lack full specification

## Next Checks
1. Extended evaluation on held-out languages: Select 5-10 additional languages from training corpus that are typologically diverse and completely held out during training. Measure whether perplexity-accuracy correlation (r² = 0.951) holds and whether LoRA adaptation with 50-200 examples achieves comparable MER scores.

2. Dataset quality audit: Randomly sample 100 examples from training corpus and manually verify segmentation-gloss alignment quality. Document specific formatting issues and their prevalence to quantify potential noise in training data.

3. Format ablation with strict controls: Train multitask, concatenated, and interleaved models on identical training subsets (10k examples each) while holding all other hyperparameters constant. Measure not just alignment scores but also correlation between training loss and downstream accuracy to determine whether interleaved format's advantage stems from structural constraints or other factors.