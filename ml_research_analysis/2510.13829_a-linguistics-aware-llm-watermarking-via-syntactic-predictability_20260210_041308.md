---
ver: rpa2
title: A Linguistics-Aware LLM Watermarking via Syntactic Predictability
arxiv_id: '2510.13829'
source_url: https://arxiv.org/abs/2510.13829
tags:
- stela
- watermark
- language
- detection
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STELA, a novel watermarking method that aligns
  watermark strength with linguistic degrees of freedom using POS n-gram-based syntactic
  predictability. Unlike prior approaches that require model logits for detection,
  STELA dynamically modulates watermark insertion and detection based on grammatical
  flexibility, weakening signals in constrained contexts to preserve quality and strengthening
  them where linguistic choices are abundant to enhance detectability.
---

# A Linguistics-Aware LLM Watermarking via Syntactic Predictability

## Quick Facts
- arXiv ID: 2510.13829
- Source URL: https://arxiv.org/abs/2510.13829
- Reference count: 40
- Primary result: STELA achieves up to 0.996 TPR@5%FPR while maintaining quality comparable to baselines

## Executive Summary
This paper introduces STELA, a novel watermarking method that aligns watermark strength with linguistic degrees of freedom using POS n-gram-based syntactic predictability. Unlike prior approaches that require model logits for detection, STELA dynamically modulates watermark insertion and detection based on grammatical flexibility, weakening signals in constrained contexts to preserve quality and strengthening them where linguistic choices are abundant to enhance detectability. Evaluated across three typologically distinct languages—English, Chinese, and Korean—using three different LLMs, STELA achieves superior watermark detection performance while maintaining text quality comparable to leading baselines.

## Method Summary
STELA computes POS n-gram conditional entropy from reference corpora to create lookup tables mapping each POS context to an indeterminacy score λ(c_t). During generation, it applies adaptive logit bias δ'_t = δ · λ(c_t) only to green-listed tokens, concentrating watermark signal in linguistically flexible positions. Detection uses a weighted z-score where each token's contribution is weighted by its context's λ value, enabling model-free verification without access to model logits. The method calibrates base strength δ = 2.0/E[λ] per language to normalize average watermark strength across languages with different syntactic properties.

## Key Results
- Achieves up to 0.996 TPR@5%FPR across three languages and models
- Outperforms baseline methods in watermark detection while maintaining comparable perplexity scores
- Shows strongest gains in Chinese and Korean where linguistic flexibility varies more dramatically
- k=2 optimal for English, k=4 for Chinese/Korean based on POS n-gram context length
- Universal POS tagsets work well; fine-grained tagsets can improve detection

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Indeterminacy as a Model-Free Entropy Proxy
- Claim: POS n-gram conditional entropy provides a linguistically-grounded, model-independent signal of where watermark insertion is safe vs. risky.
- Mechanism: For each POS context c_t (k-1 preceding tags), compute normalized conditional Shannon entropy H(P(π_t|c_t))/log(K_{c_t}) from a human-written corpus. High λ(c_t) → many grammatically valid continuations → semantic freedom. Low λ(c_t) → syntactic constraint (e.g., mandatory particle).
- Core assumption: Syntactic predictability correlates with token-level generation flexibility sufficiently to guide watermark strength allocation.
- Evidence anchors: [abstract] "STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy"; [Section 4.1] Equations 4–5 define λ(c_t) as normalized conditional entropy.

### Mechanism 2: Adaptive Bias Scaling During Generation
- Claim: Scaling logit bias by λ(c_t) concentrates watermark signal in linguistically flexible positions while protecting quality in constrained positions.
- Mechanism: At each step, compute adaptive bias δ'_t = δ · λ(c_t). Apply δ'_t only to green-listed tokens. This weakens bias where grammar dictates few options (avoiding forced unnatural tokens) and strengthens where choice is abundant (maximizing detectability).
- Core assumption: The watermark can be sufficiently embedded in high-λ positions alone to achieve detection; low-λ positions contribute minimally to the statistic.
- Evidence anchors: [Section 4.2] Equation 6: δ'_t = δ · λ(c_t); Figure 1 illustrates strong bias in high-indeterminacy contexts, weak bias in low-indeterminacy contexts.

### Mechanism 3: Weighted Z-Score Detection Focused on High-λ Tokens
- Claim: Weighting each token's contribution to the detection statistic by λ(c_t) amplifies signal from intentionally watermarked positions and suppresses noise from constrained positions.
- Mechanism: Compute weighted z-score z' where each token contributes w_t = λ(c_t) if green-listed. Expected value and variance adjust for non-uniform weights. Detection remains model-free—only requires POS tagging and hash function.
- Core assumption: Watermark insertion and detection weight distributions are aligned; tokens with high insertion bias also receive high detection weight.
- Evidence anchors: [Section 4.3] Equation 9 defines weighted z-score; explicitly states "detection statistic exhibits higher sensitivity and robustness compared to the uniform counting baseline".

## Foundational Learning

- **Part-of-Speech (POS) Tagging and N-gram Language Modeling**
  - Why needed here: The entire λ(c_t) signal depends on extracting POS tag sequences and estimating conditional probabilities P(π_t|c_t) from corpora.
  - Quick check question: Given the tagged sequence [DT, JJ, NN, ??], what are plausible next POS tags, and how would you estimate their probabilities from a corpus?

- **Shannon Entropy and Conditional Entropy**
  - Why needed here: λ(c_t) is normalized conditional entropy; understanding what high vs. low entropy means for uncertainty is essential.
  - Quick check question: If P(π_t|c_t) = [0.9, 0.05, 0.05] for three possible tags, is entropy high or low? What if P = [0.33, 0.33, 0.34]?

- **KGW Watermarking Framework (Green/Red Lists, Logit Biasing, Z-Test Detection)**
  - Why needed here: STELA modifies KGW's insertion and detection; understanding the baseline is prerequisite.
  - Quick check question: In KGW, how is the green list determined, and what statistic does the detector compute to test for watermark presence?

## Architecture Onboarding

- **Component map:** POS tagging reference corpora → n-gram frequency counting → λ(c_t) lookup table computation → generation-time adaptive bias application → weighted z-score detection

- **Critical path:** 1. Ensure λ table is built from corpora matching target domain/language; mismatch degrades signal quality. 2. Integrate POS tagger into generation loop with minimal latency overhead. 3. Calibrate base δ per language using E[λ] to ensure comparable effective strength across languages.

- **Design tradeoffs:** k=2 optimal for English, k=4 for Chinese/Korean; universal POS tagsets work well but fine-grained tagsets can improve detection; δ = 2.0/E[λ] normalizes average strength per language.

- **Failure signatures:** 1. Detection collapse in low-λ domains (systematic quality drop). 2. POS tagger mismatch between insertion and detection phases. 3. Cross-domain generalization failure when domain syntax diverges from reference corpora.

- **First 3 experiments:** 1. Reproduce λ table for one language using specified corpora and tagger; verify E[λ] matches reported values. 2. Ablate context length k: Run detection with k ∈ {2, 3, 4} for each language; confirm performance patterns. 3. Compare universal vs. language-specific tagsets: Run STELA with UD tags vs. native tagsets; measure TPR@5%FPR delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does STELA preserve semantic coherence and task-specific utility better than entropy-based baselines when measured beyond perplexity?
- Basis in paper: [explicit] The authors acknowledge that their quality assessment "relies solely on perplexity," which "may not fully capture other critical dimensions... such as grammatical correctness, stylistic naturalness, or semantic coherence."
- Why unresolved: Perplexity measures fluency probability but is a poor proxy for semantic logical consistency or the success of the text in performing a specific function (e.g., code generation, reasoning tasks).
- What evidence would resolve it: Evaluation using semantic similarity metrics (e.g., BERTScore) or downstream task performance benchmarks (e.g., reasoning accuracy) on watermarked versus unwatermarked text.

### Open Question 2
- Question: Can domain-adaptive estimation of linguistic indeterminacy (λ) improve detection robustness when generating text in specialized domains?
- Basis in paper: [explicit] The authors note that the λ signal is estimated from reference corpora (Wikipedia/C4) and warn that "when the domain... diverges substantially," the "weighting scheme may become suboptimal."
- Why unresolved: General-purpose corpora may not accurately reflect the syntactic predictability of specialized fields like medicine or law, potentially leading to suboptimal watermark strength calibration in those contexts.
- What evidence would resolve it: A comparative analysis of STELA's TPR and F1 scores on domain-specific texts (e.g., legal briefs) using both general and domain-specific corpora for λ estimation.

### Open Question 3
- Question: How robust is STELA in low-resource languages where high-quality POS taggers are unavailable?
- Basis in paper: [explicit] The paper highlights that application in low-resource settings is a "pressing challenge" due to the dependency on high-quality POS taggers, noting that effectiveness "may be diminished where the performance of the tagger itself is inherently low."
- Why unresolved: STELA's detection relies on reconstructing the exact POS context; if the tagger errors increase in low-resource languages, the lookup for λ will be incorrect, potentially collapsing the detection statistic.
- What evidence would resolve it: Experiments simulating low-resource environments or using lower-quality taggers to measure the correlation between tagger accuracy and watermark detectability (TPR).

## Limitations

- **POS tagging dependency**: Detection accuracy heavily depends on consistent POS tagger performance across insertion and detection phases
- **Domain-specific syntax**: λ values estimated from web corpora may not transfer well to specialized domains with rigid syntactic constraints
- **Low-resource language constraints**: Effectiveness diminishes in languages lacking high-quality POS taggers, limiting practical applicability

## Confidence

- **High Confidence**: The core mechanism of using POS n-gram conditional entropy as an entropy proxy for watermark placement is theoretically sound and empirically validated
- **Medium Confidence**: Cross-lingual generalization claims are supported but limited by sample size and specific language pairs chosen
- **Medium Confidence**: Model-free detection capability is convincing but practical constraints around POS tagger consistency weren't fully stress-tested

## Next Checks

1. **Tagger Consistency Stress Test**: Run the complete STELA pipeline using different versions of the same POS tagger (e.g., spaCy v3.x vs v4.x for English) between insertion and detection phases. Measure degradation in TPR@5%FPR to quantify sensitivity to tagger version drift.

2. **Domain Transfer Evaluation**: Apply STELA to specialized corpora (legal documents, scientific papers, technical manuals) where syntactic patterns differ substantially from web text. Compare detection performance against the reported Wikipedia-based baseline to assess domain robustness.

3. **Ablation of Green List Size**: Systematically vary γ (green list ratio) from 0.1 to 0.9 and measure the trade-off between watermark detectability (TPR@5%FPR) and text quality (PPL). This would clarify whether the chosen γ=0.5 represents an optimal balance or a conservative choice.