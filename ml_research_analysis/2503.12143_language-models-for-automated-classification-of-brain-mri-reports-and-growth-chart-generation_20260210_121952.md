---
ver: rpa2
title: Language Models for Automated Classification of Brain MRI Reports and Growth
  Chart Generation
arxiv_id: '2503.12143'
source_url: https://arxiv.org/abs/2503.12143
tags:
- report
- brain
- findings
- axial
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed fine-tuned language models (LMs) to automate
  classification of brain MRI reports as normal or abnormal. Fine-tuned BERT-based
  models achieved high performance (F1-Score 97%) on a large dataset of 44,661 reports.
---

# Language Models for Automated Classification of Brain MRI Reports and Growth Chart Generation

## Quick Facts
- arXiv ID: 2503.12143
- Source URL: https://arxiv.org/abs/2503.12143
- Reference count: 40
- Fine-tuned BERT-based models achieved F1-Score >97% for classifying brain MRI reports as normal or abnormal

## Executive Summary
This study developed fine-tuned language models (LMs) to automate classification of brain MRI reports as normal or abnormal. Fine-tuned BERT-based models achieved high performance (F1-Score >97%) on a large dataset of 44,661 reports. Unbalanced training with minority class weighting mitigated class imbalance, while Gemini 1.5-Pro showed promising reasoning capabilities for report categorization. The LMs enabled automated generation of brain growth charts from normal MRI scans, producing results nearly identical to human-annotated charts (r = 0.99). This approach offers scalable analysis of radiology reports, addressing limitations in manual annotation for large-scale brain imaging research.

## Method Summary
The researchers fine-tuned BERT-family models (BERT, BioBERT, ClinicalBERT, RadBERT) on 44,661 brain MRI reports from Children's Hospital of Philadelphia (CHOP) and Geisinger. They employed weighted loss functions to address class imbalance and evaluated model performance on held-out test sets. The best-performing models were then used to automatically classify reports for growth chart generation using GAMLSS statistical models. They also tested a large language model (Gemini 1.5-Pro) for reasoning-based categorization and evaluated temporal out-of-distribution performance.

## Key Results
- Fine-tuned LMs achieved F1-Score >97% for classifying normal vs abnormal brain MRI reports
- Unbalanced training with tenfold higher penalty for misclassifying normal reports mitigated class imbalance
- Automated LM-derived brain growth charts were nearly identical to human-annotated charts (r = 0.99)
- Full-text reports provided significantly more information than impression-only sections
- Performance degraded 20% on out-of-distribution 2023 data compared to pre-2023 training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned domain-specific language models can achieve high performance (>97% F1) for binary classification of radiology reports, with class weighting being critical for real-world data.
- Mechanism: Transfer learning from models pre-trained on biomedical and clinical corpora (BioBERT, ClinicalBERT) provides strong foundational representations of medical terminology. A weighted loss function counteracts the severe class imbalance inherent in clinical datasets, which are skewed heavily toward abnormal findings.
- Core assumption: The textual patterns in radiology reports contain sufficient signal to distinguish normal from abnormal scans, and the minority ("normal") class is sufficiently represented for the model to learn its features when up-weighted.
- Evidence anchors:
  - [abstract] "Fine-tuned LMs achieved high classification performance (F1-Score >97%), with unbalanced training mitigating class imbalance."
  - [section] Results, Exp 2: "To address the class imbalance... we employed a weighted loss function that assigned a tenfold higher penalty to misclassifications of normal reports."
  - [corpus] Related work "brat" (arXiv:2512.18679) supports the efficacy of aligned multi-view embeddings for brain MRI analysis.
- Break condition: The mechanism would likely fail if deployed on a dataset with a drastically different reporting style, language, or definition of "normal" without re-fine-tuning.

### Mechanism 2
- Claim: Full-text radiology reports contain significantly more diagnostic signal than the summary "Impression" section alone.
- Mechanism: The detailed narrative often contains crucial context about image quality, artifact presence, and specific benign findings that may be summarized or omitted from the final impression. Models using full text can access this broader context.
- Core assumption: The incremental information in the narrative text is relevant to the classification task and not just noise.
- Evidence anchors:
  - [section] Results, Exp 4: "Full text provided significantly more information than only the summary 'Impression' section of reports... all models exhibited notably lower sensitivity... and F1-score... when trained on the impression section alone."
  - [corpus] No direct corpus support for this specific finding was identified among the top neighbors.
- Break condition: Performance gains from full text may diminish if the narrative sections are consistently low-quality, boilerplate, or if the impression section is mandated to be exhaustive.

### Mechanism 3
- Claim: Automated report classification enables the scalable creation of clinical cohorts that produce growth charts statistically indistinguishable from human-annotated ones.
- Mechanism: The high precision and recall of the LM classifier generate a large set of "normal" scans with minimal contamination from abnormal cases. When fed into a robust statistical model (GAMLSS), this large sample size dilutes the impact of the few remaining errors, producing normative trajectories that highly correlate with gold-standard human charts.
- Core assumption: The errors made by the LM classifier are random and not systematically biased toward a specific subgroup that would distort the growth curves.
- Evidence anchors:
  - [abstract] "LM-derived brain growth charts were nearly identical to human-annotated charts (r = 0.99, p < 2.2e-16)."
  - [section] Methods: "Subsets were created for scan sessions marked as normal by human annotators or LMs... Growth charts were generated... using generalized additive models for location, scale, and shape (GAMLSS)."
  - [corpus] No direct corpus support for this specific finding was identified among the top neighbors.
- Break condition: The correlation would break if the LM misclassifications introduce a systematic bias, for instance, by consistently mislabeling a specific anatomical anomaly as normal.

## Foundational Learning

- **Concept: Transfer Learning in NLP**
  - Why needed here: To understand why models pre-trained on general or biomedical text (like BERT or BioBERT) can be effectively repurposed for a specialized radiology task with limited labeled data.
  - Quick check question: Why is fine-tuning a pre-trained model often more effective than training a model from scratch on a dataset of 44,661 reports?

- **Concept: Class Imbalance & Weighted Loss**
  - Why needed here: Clinical datasets are almost always imbalanced (more abnormal than normal). Standard training leads to a model biased toward the majority class. Understanding this is crucial for interpreting model metrics.
  - Quick check question: If a model achieves 99% accuracy on a dataset where 99% of reports are abnormal, is it a good model? Why or why not?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: Models often degrade in performance when applied to data from a different time period, institution, or patient demographic than they were trained on. This is a key consideration for deployment.
  - Quick check question: The study found a performance drop on data from 2023 when trained on pre-2023 data. What factors might cause this "temporal drift"?

## Architecture Onboarding

- **Component map**: Raw Report Text -> Tokenization -> Transformer Encoder -> [CLS] Embedding -> Linear Classifier -> Weighted Loss -> Optimization
- **Critical path**: Raw Report Text -> Tokenization -> Transformer Encoder -> [CLS] Embedding -> Linear Classifier -> Weighted Loss -> Optimization
- **Design tradeoffs**:
  - **Model Selection**: RadBERT shows superior performance on out-of-distribution adult data but requires pre-training on a large radiology corpus. BioBERT/ClinicalBERT offer a good balance for pediatric/general clinical text.
  - **Input Truncation**: BERT has a 512-token limit. The choice of how to truncate long reports (head-only, tail-only, head+tail) can impact performance, as the narrative may be cut off.
  - **Inference Cost vs. Capability**: Fine-tuned smaller models (e.g., BERT-base) are extremely fast and cheap to run compared to prompting large LLMs like Gemini 1.5-Pro, making them more suitable for batch processing hundreds of thousands of reports.

- **Failure signatures**:
  - **Precision Drop on OOD Data**: A significant decrease in precision (and F1) when testing on data from a new year or institution. *Fix*: Periodically retrain with recent data or incorporate data augmentation techniques to improve robustness.
  - **Failure on Nuanced "Normal" Cases**: Models may struggle with reports that mention "stable" but benign pathology, "no change" from a prior abnormal scan, or image artifacts. *Fix*: Include these specific edge cases in the training set or use a stepwise reasoning LLM as a secondary check.
  - **Overconfidence on Unseen Patterns**: The model may output high-confidence wrong predictions on report styles it hasn't seen. *Fix*: Implement a confidence threshold and route low-confidence predictions for human review.

- **First 3 experiments**:
  1. **Validate Class Weighting**: Train the model with and without the weighted loss function on the imbalanced dataset. Compare the F1-score, precision, and recall for the minority ("Normal") class to confirm the hypothesis.
  2. **Compare Domain-Specific Models**: Fine-tune and evaluate BERT, BioBERT, ClinicalBERT, and RadBERT on both in-distribution and out-of-distribution test sets. Identify the best-performing model for the target data characteristics (e.g., pediatric vs. adult).
  3. **Test Temporal & Institutional Robustness**: Evaluate the chosen model on held-out data from a future time period (e.g., 2023 vs. pre-2023) and/or a different institution (e.g., Geisinger vs. CHOP) to quantify performance degradation and set expectations for real-world deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting strategy for handling class imbalance in radiology report classification?
- Basis in paper: [explicit] The authors state that "investigations into optimizing the weighting strategy for imbalanced datasets beyond the tenfold higher penalty empirically identified in this study" are needed.
- Why unresolved: The study utilized a heuristic tenfold penalty determined empirically, but the ideal mathematical or algorithmic approach to weighting for this specific clinical NLP task remains undetermined.
- What evidence would resolve it: A systematic ablation study comparing various class weighting schemes (e.g., inverse frequency, focal loss) on model precision and recall across different imbalance ratios.

### Open Question 2
- Question: Does fine-tuning state-of-the-art large language models (LLMs) significantly outperform the current BERT-based ensemble?
- Basis in paper: [explicit] The Discussion notes, "Further research is needed to evaluate the efficacy of a fine-tuned large scale language model (LLM) for the task of normal radiology report categorization."
- Why unresolved: The authors prioritized resource efficiency using BERT-based models; while they tested Gemini 1.5-Pro for reasoning, they did not fine-tune such large models due to computational costs.
- What evidence would resolve it: Comparative performance metrics (F1-score, accuracy) between the current ensemble and fully fine-tuned LLMs (e.g., Llama 3, GPT-4) on the same held-out test sets.

### Open Question 3
- Question: What specific factors drive the performance degradation observed in temporal out-of-distribution (OOD) testing?
- Basis in paper: [explicit] The authors state, "The impact of the observed performance degradation on out-of-distribution radiology reports... require further investigation."
- Why unresolved: While the study noted a 20% drop in precision on 2023 data, it did not isolate whether this was caused by shifts in clinical vocabulary, reporting style, or patient demographics over time.
- What evidence would resolve it: A granular error analysis on the 2023 dataset identifying specific linguistic or feature shifts that correlate with misclassification rates.

## Limitations

- The study's performance metrics are derived from a single healthcare system's dataset, raising concerns about external validity across different institutions.
- The model's handling of nuanced normal cases (stable benign findings, image artifacts, "no change" from prior abnormal scans) remains unclear and could represent systematic failure modes.
- While the correlation between LM-derived and human-annotated growth charts is high (r = 0.99), the study does not report on potential systematic biases that might exist despite the strong correlation.

## Confidence

- **High Confidence**: The effectiveness of weighted loss functions for handling class imbalance and the superiority of full-text reports over impression-only sections for classification.
- **Medium Confidence**: The generalizability of the model to out-of-distribution data and different healthcare systems.
- **Medium Confidence**: The practical utility of the approach for large-scale growth chart generation.

## Next Checks

1. **Cross-Institutional Validation**: Deploy the fine-tuned models on radiology reports from multiple healthcare systems with different reporting templates and terminology. Quantify performance degradation and identify specific failure patterns related to institutional differences in report structure and language.

2. **Edge Case Analysis**: Create a curated test set of challenging normal cases including reports with "stable" findings, image artifacts, and "no change" from prior abnormalities. Evaluate model precision on these cases specifically and compare against random samples to determine if misclassification is systematic.

3. **Temporal Robustness Testing**: Train models on data from specific time periods (e.g., 2018-2020) and evaluate on progressively newer data (2021, 2022, 2023). Measure the rate of performance degradation over time and determine the optimal retraining frequency to maintain >95% F1-score.