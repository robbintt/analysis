---
ver: rpa2
title: Small Language Models for Curriculum-based Guidance
arxiv_id: '2510.02347'
source_url: https://arxiv.org/abs/2510.02347
tags:
- course
- slms
- teaching
- questions
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of small language models (SLMs)
  for curriculum-based guidance in education. The researchers developed AI teaching
  assistants using eight open-source SLMs with retrieval-augmented generation (RAG)
  to provide personalized learning support aligned with course materials.
---

# Small Language Models for Curriculum-based Guidance

## Quick Facts
- arXiv ID: 2510.02347
- Source URL: https://arxiv.org/abs/2510.02347
- Authors: Konstantinos Katharakis; Sippo Rossi; Raghava Rao Mukkamala
- Reference count: 11
- Primary result: SLMs with RAG achieved 0% hallucination rate and matched GPT-4o performance on curriculum-aligned educational tasks

## Executive Summary
This study investigates small language models (SLMs) as AI teaching assistants for curriculum-based guidance in education. The researchers developed a RAG-augmented system using eight open-source SLMs (7-17B parameters) to provide personalized learning support aligned with course materials. SLMs were benchmarked against GPT-4o on a graduate-level mathematics course, demonstrating comparable performance in delivering accurate, pedagogically aligned responses. The RAG pipeline significantly reduced hallucinations from 37.19% to 0%, while SLMs offered sustainability benefits due to lower computational and energy requirements.

## Method Summary
The researchers created AI teaching assistants using eight open-source SLMs with retrieval-augmented generation (RAG) to provide personalized learning support aligned with course materials. Course documents were converted from PDF to text, with images described via GPT-4o mini, then embedded using OpenAI embeddings in a Chroma vector store. A LangChain-based system orchestrated prompt construction with system messages and guidance prefixes, combined retrieved context with user queries, and routed them to SLMs. Eight models were evaluated (LLaMA 3.1/4, Qwen 3, DeepSeek-R1, Mistral, Gemma 3, Phi-4, Granite 3.3) using a syllabus-grounded evaluation framework of 50 questions covering assignments and theory. Responses were manually scored for accuracy, thoroughness, and pedagogical alignment.

## Key Results
- Gemma 3 and Granite 3.3 outperformed GPT-4o on assignment questions, while LLaMA 4, Phi-4, and DeepSeek-R1 matched GPT-4o on theory questions
- RAG pipeline reduced hallucination rate from 37.19% to 0%
- All models successfully tracked conversation context in 6/6 cases
- SLMs with 7-17B parameters can be deployed on consumer-grade GPUs for real-time educational applications

## Why This Works (Mechanism)

### Mechanism 1: RAG Grounding Reduces Hallucinations
- Claim: Retrieval-augmented generation can reduce hallucinations by grounding responses in verified course materials.
- Mechanism: Instead of relying solely on parametric knowledge, the system retrieves relevant document chunks from a vector store containing preprocessed lecture slides and readings. These chunks are injected as context, constraining the model to generate responses anchored in course content rather than potentially fabricated information.
- Core assumption: The retrieval system surfaces relevant context; the model attends to and uses that context appropriately.
- Evidence anchors:
  - [abstract]: "The RAG pipeline significantly reduced hallucinations (from 37.19% to 0%)"
  - [Page 7]: "the average hallucination rate being 37.19% across all models prior to introducing the RAG-pipeline, while when using a RAG-pipeline the hallucination rate dropped to 0%"
  - [corpus]: TeachBench paper proposes "syllabus-grounded evaluation framework" for LLM teaching ability, supporting curriculum-aligned approaches—but no direct replication of hallucination reduction claims.
- Break condition: If retrieval fails to surface relevant chunks (poor embeddings, badly chunked documents, or out-of-domain queries), the model may revert to parametric knowledge and hallucinate.

### Mechanism 2: Prompt Engineering Enforces Pedagogical Alignment
- Claim: Explicit system messages and prompt prefixes can steer models toward guidance rather than direct solutions.
- Mechanism: A two-stage prompt construction: (1) a system message establishing the teaching assistant persona and prohibiting direct answers; (2) a fixed prefix prepended to user queries—"Don't solve this for me. Instead, guide me through the process..."—that reinforces instructional behavior at inference time.
- Core assumption: Models reliably follow multi-part instructions; smaller models have sufficient instruction-following capability.
- Evidence anchors:
  - [Page 8]: "Adding explicit system messages and structured guidance within prompts effectively directed model behavior away from providing direct answers and instead toward curriculum-based guidance"
  - [Page 4-5]: Full system message and prompt prefix specification shown
  - [corpus]: No direct corpus evidence for this specific prompt design pattern in educational settings.
- Break condition: Some models may ignore instructions under certain query phrasings; evaluation showed variation (e.g., DeepSeek-R1 followed guidance in only 1/6 cases, LLaMA 4 Scout in 0/6).

### Mechanism 3: Task-Specific SLM Competitiveness via RAG Augmentation
- Claim: SLMs (7–17B parameters) can approximate GPT-4o performance on curriculum-aligned educational tasks when augmented with targeted retrieval.
- Mechanism: The RAG pipeline externalizes knowledge requirements, reducing the parametric knowledge burden on the model. This allows smaller models to perform competently on domain-specific tasks where they would otherwise struggle, as the reasoning load (rather than knowledge retrieval) becomes the primary challenge.
- Core assumption: The educational task requires reasoning over provided context more than broad world knowledge; the domain is sufficiently bounded.
- Evidence anchors:
  - [Page 7-8]: "Gemma 3 was the top-performing model, surpassing even GPT-4o" on assignment questions; "LLaMA 4, Phi-4, and DeepSeek-R1 matched GPT-4o's performance" on theory questions
  - [Page 2]: Models operated "with less than 17 billion parameters, allowing for deployment on consumer-grade GPUs"
  - [corpus]: "Smaller, Smarter, Closer" paper positions SLMs as viable for resource-constrained edge environments; "Fixed-Persona SLMs" demonstrates scalable dialogue on consumer hardware—supporting feasibility but not educational performance claims.
- Break condition: For tasks requiring extensive multi-step reasoning, broad world knowledge, or long-context synthesis, SLMs may still underperform larger models regardless of RAG augmentation.

## Foundational Learning

- **Concept: Vector embeddings and semantic search**
  - Why needed here: The RAG pipeline relies on embedding queries and documents into vector space for similarity-based retrieval. Without understanding this, you cannot debug retrieval quality.
  - Quick check question: Given a query about "eigenvalues in machine learning," what determines whether the system retrieves the relevant lecture slide?

- **Concept: Language model sampling parameters (temperature, top-p, top-k)**
  - Why needed here: The study sets temperature to 0.4 across all models. Understanding why lower temperature is chosen—and how it affects determinism—is essential for reproducible educational responses.
  - Quick check question: If a student receives different answers to the same question on repeated asks, which parameter should you adjust first?

- **Concept: Context window constraints**
  - Why needed here: SLMs have smaller context windows than large models. Retrieved context + conversation history + system prompt must fit within this limit, or truncation will degrade performance.
  - Quick check question: Your RAG system retrieves 4 documents of ~800 tokens each. Your SLM has a 4K context window. How much conversation history can you retain before overflow?

## Architecture Onboarding

- **Component map:** Document preprocessing pipeline -> Vector store (Chroma) -> LangChain orchestration -> SLM inference engines -> Gradio interface

- **Critical path:** User query → Query embedding → Vector similarity search → Retrieve top-k chunks → Construct prompt (system message + retrieved context + query with guidance prefix) → SLM inference → Return response

- **Design tradeoffs:**
  - OpenAI embeddings vs. local embeddings: Higher quality retrieval vs. full privacy/cost independence
  - Temperature 0.4: More consistent outputs vs. potentially less adaptive responses
  - Model selection: Gemma 3/Granite 3.3 best for assignments; LLaMA 4/Phi-4/DeepSeek-R1 best for theory—no single model dominates both
  - Parameter count (7B–17B): Lower resource requirements vs. reduced reasoning depth

- **Failure signatures:**
  - Model gives direct solution instead of guidance → prompt adherence failure; check system message placement and guidance prefix
  - Model answers with irrelevant or generic content → retrieval failure; verify embedding quality and chunk relevance
  - Model loses conversation context → memory management issue; check LangChain memory configuration
  - Hallucinated facts not in course materials → RAG not being used; verify retrieval is actually triggered

- **First 3 experiments:**
  1. **Retrieval validation:** Ask 5 questions with known answers in your corpus. Inspect retrieved chunks—do they contain the relevant information? Measure retrieval precision.
  2. **Prompt adherence test:** Submit 5 assignment-style questions. Score responses: guidance (pass) vs. direct answer (fail). Compare across models.
  3. **SLM benchmark:** Run the full 50-question evaluation set on 2–3 SLMs. Compare accuracy to the GPT-4o baseline using the paper's scoring methodology (manual evaluation of accuracy and pedagogical alignment).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these findings generalize to non-mathematical academic disciplines?
- Basis in paper: [inferred] The authors note the results "could differ for courses in other disciplines" as linear algebra presents unique symbolic reasoning challenges.
- Why unresolved: The study was restricted to a single graduate mathematics course, limiting claims about generalizability to other domains.
- What evidence would resolve it: Cross-disciplinary benchmarking using the same SLM-RAG architecture in humanities or qualitative social science courses.

### Open Question 2
- Question: Does domain-specific fine-tuning improve pedagogical alignment compared to RAG-only approaches?
- Basis in paper: [explicit] The authors state they "plan to fine-tune the top-performing SLMs using domain-specific data to improve their pedagogical alignment."
- Why unresolved: The current architecture relied on unmodified model weights, relying solely on retrieval and prompting to guide the SLMs.
- What evidence would resolve it: Ablation studies comparing base models against fine-tuned variants on curriculum-specific accuracy and guidance quality.

### Open Question 3
- Question: What is the measurable impact of SLM assistants on actual student learning performance?
- Basis in paper: [explicit] Future work involves "monitoring learning performance metrics" to move beyond model accuracy to educational efficacy.
- Why unresolved: Current evaluation focused on the model's output quality (accuracy/hallucination), not the student's resulting knowledge gain or engagement.
- What evidence would resolve it: Longitudinal data from the planned Autumn 2025 pilot comparing assessment grades and retention rates of users versus non-users.

## Limitations

- Limited to single graduate mathematics course; generalizability to other disciplines unknown
- Manual evaluation methodology lacks detailed scoring rubric and inter-rater reliability reporting
- Hardware specifications (GPU model, VRAM, inference speed) not detailed beyond "consumer-grade"

## Confidence

- Mechanism 1 (RAG grounding): High
- Mechanism 2 (Prompt engineering): Medium
- Mechanism 3 (SLM competitiveness): Medium

## Next Checks

1. Validate retrieval quality by testing whether relevant chunks are returned for 5 known-answer questions
2. Test prompt adherence by submitting 5 assignment-style questions and scoring for guidance vs. direct answers
3. Benchmark 2-3 SLMs on the full 50-question evaluation set and compare accuracy to GPT-4o baseline