---
ver: rpa2
title: 'From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics'
arxiv_id: '2509.05617'
source_url: https://arxiv.org/abs/2509.05617
tags:
- emotion
- emotional
- lyrics
- emotions
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the task of multi-label emotional attribution
  of song lyrics, aiming to predict six emotional intensity scores (joy, sadness,
  anger, fear, surprise, and disgust) using large language models (LLMs). A manually
  labeled dataset is constructed using the Mean Opinion Score (MOS) methodology, aggregating
  annotations from multiple human raters to ensure reliable ground-truth labels.
---

# From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics

## Quick Facts
- arXiv ID: 2509.05617
- Source URL: https://arxiv.org/abs/2509.05617
- Reference count: 0
- Primary result: Fine-tuned BERT models significantly outperform zero-shot LLMs for multi-label emotion prediction in pop song lyrics

## Executive Summary
This paper presents a benchmark for multi-label emotion estimation in pop song lyrics, focusing on predicting six emotional intensity scores: joy, sadness, anger, fear, surprise, and disgust. The study constructs a manually labeled dataset using Mean Opinion Score methodology and evaluates both publicly available LLMs in zero-shot scenarios and a BERT-based model fine-tuned for this specific task. The experimental results demonstrate that fine-tuned BERT models substantially outperform zero-shot approaches like Grok 3 across all emotional categories, highlighting the effectiveness of domain-specific fine-tuning for emotion recognition in creative texts. The work provides valuable insights for music information retrieval applications that rely on emotional attribution of song lyrics.

## Method Summary
The research employs a comprehensive approach combining dataset construction with model evaluation. A manually labeled dataset was created using Mean Opinion Score methodology, where multiple human raters provided annotations for song lyrics to establish reliable ground-truth labels. The study evaluates two primary approaches: zero-shot performance of publicly available LLMs (including Grok 3) and a BERT-based model specifically fine-tuned for multi-label emotion prediction. The evaluation metrics include Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) across all six target emotions. The fine-tuning process involves training the BERT model on the constructed dataset to predict continuous emotional intensity scores rather than discrete classifications, allowing for more nuanced emotion representation in song lyrics.

## Key Results
- Fine-tuned BERT models achieved MAE of 0.33 and RMSE of 0.1507, significantly outperforming zero-shot Grok LLM with MAE of 0.50 and RMSE of 0.367
- BERT-based models showed consistent superiority across all six emotions (joy, sadness, anger, fear, surprise, and disgust)
- The study demonstrates that domain-specific fine-tuning substantially improves emotion recognition performance in creative texts compared to general-purpose zero-shot approaches

## Why This Works (Mechanism)
The superior performance of fine-tuned BERT models stems from their ability to capture domain-specific linguistic patterns and emotional nuances present in song lyrics. Unlike zero-shot LLMs that rely on general language understanding, the fine-tuned BERT model learns the unique vocabulary, metaphorical expressions, and emotional intensity patterns characteristic of pop music. The continuous score prediction approach allows for more granular emotion representation compared to discrete classification, better capturing the intensity variations in emotional expression. The Mean Opinion Score methodology ensures high-quality ground truth by aggregating multiple human annotations, reducing individual rater bias and providing reliable training targets for the fine-tuned models.

## Foundational Learning
- **Mean Opinion Score (MOS) methodology**: Aggregated human annotations to create reliable ground truth labels for emotion intensity scoring, addressing subjectivity in emotion annotation
- **Multi-label emotion prediction**: Framework for predicting continuous intensity scores across six emotions simultaneously rather than binary classification
- **Fine-tuning vs zero-shot learning**: Comparison of domain-specific adaptation against general-purpose language model capabilities
- **Continuous emotion scoring**: Approach for representing emotion intensity on a spectrum rather than discrete categories
- **Music information retrieval**: Application domain for emotion recognition in song lyrics for music recommendation and analysis
- **Creative text processing**: Specialized NLP challenges in handling metaphorical, poetic, and culturally-specific language in song lyrics

## Architecture Onboarding

Component Map:
Dataset Construction -> BERT Fine-tuning -> Emotion Prediction -> Performance Evaluation

Critical Path:
Human annotation collection → MOS aggregation → BERT model fine-tuning → Zero-shot LLM comparison → MAE/RMSE calculation

Design Tradeoffs:
- Fine-tuning requires labeled data but achieves superior performance vs zero-shot approaches that need no training
- Continuous scoring captures nuance but requires more complex annotation vs binary classification
- BERT architecture chosen for efficiency vs larger transformer models with higher computational cost

Failure Signatures:
- High MAE/RMSE indicating poor emotion intensity prediction
- Inconsistent performance across different emotions suggesting model bias
- Poor generalization to unseen lyrics indicating overfitting to training data

First Experiments:
1. Compare fine-tuned BERT against multiple zero-shot LLMs beyond Grok 3
2. Test model performance across different musical genres and languages
3. Evaluate inter-annotator agreement to quantify human rater subjectivity impact

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies on subjective human annotations, potentially introducing bias
- Results limited to English pop song lyrics, reducing generalizability to other languages and genres
- Comparison only covers BERT fine-tuning vs zero-shot LLMs, missing other architectural approaches
- Study doesn't explore parameter tuning space for either fine-tuned or zero-shot models

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Fine-tuned BERT models outperform zero-shot LLMs (MAE 0.33 vs 0.50) | High |
| Fine-tuned transformer models have potential for emotion recognition in creative texts | Medium |
| Study provides insights into model selection strategies for emotion-based MIR | Medium |

## Next Checks
1. Conduct cross-cultural validation by replicating the experiment with multilingual song lyrics and diverse musical genres to assess model robustness across different emotional expression patterns
2. Implement inter-annotator agreement analysis to quantify the subjectivity in human ratings and assess its impact on model performance metrics
3. Compare the fine-tuned BERT model against a broader range of architectures, including domain-specific transformers and newer LLM variants with emotion-focused fine-tuning, to establish a more comprehensive benchmark