---
ver: rpa2
title: 'Judge Q: Trainable Queries for Optimized Information Retention in KV Cache
  Eviction'
arxiv_id: '2509.10798'
source_url: https://arxiv.org/abs/2509.10798
tags:
- cache
- tokens
- arxiv
- eviction
- soft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction

## Quick Facts
- **arXiv ID:** 2509.10798
- **Source URL:** https://arxiv.org/abs/2509.10798
- **Reference count:** 11
- **Primary result:** Introduces trainable soft tokens to optimize KV cache eviction, improving long-context generation quality by up to 3 points on RULER and 1 point on LongBench.

## Executive Summary
Judge Q addresses the memory bottleneck in LLM inference by introducing trainable soft tokens that act as proxy queries for KV cache eviction. Unlike heuristic-based methods, Judge Q learns to estimate global information importance by aligning the attention patterns of soft tokens with those of actual decoded tokens. This approach enables more accurate retention of critical context during pre-filling, reducing memory usage without significant performance loss. The method is efficient, requiring only embedding-layer tuning, and demonstrates strong results across multiple benchmarks.

## Method Summary
Judge Q trains a set of `n=32` soft tokens by minimizing the MSE between their attention distribution over the input and the attention distribution of actual decoded tokens. The soft tokens are appended to the input during pre-filling, and their attention scores are used to rank and retain the most important KV pairs. The model weights are frozen except for the embedding layer of the soft tokens, making the approach lightweight and efficient. During inference, the soft tokens are removed after eviction, and generation proceeds with the pruned cache.

## Key Results
- Achieves up to 3-point improvement on RULER and 1-point gain on LongBench compared to baseline eviction methods.
- Outperforms heuristic-based methods (H2O, SnapKV, PyramidKV) in retaining critical KV pairs for long-context tasks.
- Demonstrates that training on model-generated responses yields better alignment than using ground truth data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Trainable soft tokens act as proxy queries to estimate global KV importance more accurately than local window heuristics.
- **Mechanism:** The paper trains a set of soft tokens (embeddings) to minimize the Mean Squared Error (MSE) between their attention distribution over the input and the attention distribution of the *actual decoded tokens*. By aligning these attention maps, the soft tokens learn to "probe" the KV cache for information that will likely be required during generation, rather than relying on the static presence of recent tokens.
- **Core assumption:** The attention patterns of future decoded tokens are predictable and can be approximated by a fixed set of learned queries during the pre-filling stage.
- **Evidence anchors:**
  - [Abstract]: "we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens."
  - [Section 3]: "...directly using the original decoded tokens to select critical key-value pairs... representing the theoretical upper bound..."
  - [Corpus]: *Lookahead Q-Cache* utilizes pseudo queries for eviction, supporting the hypothesis that predictive queries outperform static local windows.
- **Break condition:** If the downstream task requires reasoning patterns significantly out-of-distribution from the training data (ShareGPT), the soft tokens may fail to approximate the necessary attention heads, reducing hit rates.

### Mechanism 2
- **Claim:** Optimizing only the embedding layer allows for efficient adaptation of eviction policies without destabilizing the pre-trained model's knowledge.
- **Mechanism:** The framework freezes the transformer weights (attention and FFN) and updates only the embedding vectors corresponding to the `n` soft tokens. This restricts the search space to finding an "optimal prompt" that triggers existing model capabilities to focus on critical context, rather than re-learning how to process context.
- **Core assumption:** The frozen model possesses sufficient representational capacity to attend to distant context if explicitly queried by the right input embeddings.
- **Evidence anchors:**
  - [Abstract]: "This method only tunes the model's embedding layer at a low training cost."
  - [Section 4.2]: "...exclusively fine-tune the subset of parameters... while keeping the remaining parameters frozen."
  - [Corpus]: Evidence specifically for embedding-only eviction tuning is sparse in the provided corpus; related work like *GraphKV* focuses on graph-based selection logic rather than embedding tuning.
- **Break condition:** If the model's pre-trained attention heads are fundamentally unable to retrieve specific distant information (e.g., "lost in the middle" phenomenon), no embedding tuning will successfully force retention of those KV pairs.

### Mechanism 3
- **Claim:** Training on model-generated responses creates a tighter alignment for eviction than training on human-authored ground truth.
- **Mechanism:** The paper observes that aligning soft tokens with the specific outputs the model *would* generate is more effective than aligning with external gold labels. This creates a self-consistent loop where the eviction policy optimizes for the model's own reasoning traces.
- **Core assumption:** The model's internal reasoning and attention patterns are more consistent with its own generated outputs than with human reference texts.
- **Evidence anchors:**
  - [Section 6.2]: "...using model-generated responses outperforms directly using the original responses from the datasets."
  - [Section 5]: Validation on Llama and Mistral using self-generated data.
  - [Corpus]: No direct comparison found in neighbors; this is a specific design choice in Judge Q.
- **Break condition:** If the model hallucinates or produces low-quality responses during training data generation, the soft tokens may learn to optimize for spurious or irrelevant attention patterns.

## Foundational Learning

- **Concept:** KV Cache Eviction (Pruning)
  - **Why needed here:** The paper attempts to solve the memory bottleneck caused by linear KV cache growth. You must understand that standard eviction (e.g., H2O, SnapKV) relies on heuristics (like recent tokens) which Judge Q aims to replace.
  - **Quick check question:** Why does using the "last window" as a query fail when the question is at the start of the prompt?

- **Concept:** Soft Tokens / Prompt Tuning
  - **Why needed here:** This is the core implementation. The method does not change the model code but appends learnable vectors. Understanding that these vectors are optimized via gradients (unlike discrete text prompts) is essential.
  - **Quick check question:** During inference, are the soft tokens decoded into text, or are they strictly internal attention probes?

- **Concept:** Attention Score as Importance Metric
  - **Why needed here:** The mechanism relies on the premise that high attention weights correlate with high information importance.
  - **Quick check question:** In Judge Q, how is the "importance score" of a KV pair calculated using the soft tokens?

## Architecture Onboarding

- **Component map:** Input Processor -> Training Loop -> Inference (Prefill) -> Eviction Policy -> Decoder
- **Critical path:** The alignment between the *Soft Token Attention Map* and the *Response Attention Map* (Section 4.2, Eq. 5). If this alignment fails (MSE remains high), the eviction will be random or detrimental.
- **Design tradeoffs:**
  - **Soft Token Count (`n`):** Paper finds `n=32` optimal. Lower `n` loses coverage; higher `n` increases training latency without quality gains (Section 6.3).
  - **Training Data:** Using generic pre-training data (Wiki/Git) underperforms vs. instruction-tuning data (ShareGPT) because eviction behavior is task-specific (Section 6.2).
- **Failure signatures:**
  - **Local Collapse:** If the model still focuses on the end of the prompt, the soft token learning rate may be too low or the MSE loss may have collapsed to a local minimum favoring positional bias.
  - **Over-pruning:** On tasks requiring dense retrieval (e.g., passkey retrieval in RULER), aggressive budget reduction (e.g., 128 tokens) may still fail even with optimized queries.
- **First 3 experiments:**
  1. **Baseline Verification:** Reproduce the "Theoretical Upper Bound" experiment (Table 1) using actual decoded tokens to verify the potential gain on your specific model.
  2. **Hit Rate Ablation:** Measure the "Critical Key-Value Hit Rate" (Eq. 6) for Judge Q vs. SnapKV. This isolates the quality of the selection mechanism from the generation quality.
  3. **Data Sensitivity:** Train two sets of soft tokens—one on ground-truth responses and one on model-generated responses—and compare LongBench scores to validate the self-generated data hypothesis.

## Open Questions the Paper Calls Out
- Can the Judge Q framework be effectively extended to support streaming KV cache eviction during the decoding phase, rather than limiting eviction to the pre-filling stage?
- How does training Judge Q with longer sequence data affect its ability to capture global information and retain context in extremely long inputs?
- Is there a universal optimal number of soft tokens ($n$) that balances training efficiency and retention accuracy, or must this hyperparameter be tuned per model?
- Does fine-tuning parameters beyond the embedding layer (e.g., attention weights) significantly improve the alignment between soft tokens and actual decoded tokens?

## Limitations
- The method is currently limited to pre-filling eviction and does not support dynamic eviction during decoding.
- Performance may degrade on tasks requiring reasoning patterns significantly out-of-distribution from the training data.
- The optimal number of soft tokens (`n=32`) is empirical and may require tuning for different model sizes or architectures.

## Confidence
- **Method validity:** High
- **Reproducibility:** Medium (key hyperparameters and implementation details are missing)
- **Empirical results:** High (strong performance on multiple benchmarks)

## Next Checks
1. Reproduce the "Theoretical Upper Bound" experiment to verify the potential gain of Judge Q on your specific model.
2. Compare the "Critical Key-Value Hit Rate" of Judge Q vs. SnapKV to isolate the quality of the selection mechanism.
3. Train soft tokens on both ground-truth and model-generated responses to validate the self-generated data hypothesis.