---
ver: rpa2
title: 'From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling
  Framework for In-context Learning'
arxiv_id: '2510.24528'
source_url: https://arxiv.org/abs/2510.24528
tags:
- target
- task
- tasks
- examples
- cross-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cost-efficient pipeline for in-context
  learning (ICL) on novel tasks by combining cross-task examples with graph-based
  label propagation. The method first uses a graph-based similarity metric (GraphSim)
  to select relevant examples from high-resource source tasks, then applies a graph
  neural network to propagate labels from a small pseudo-labeled set to the entire
  target dataset (GLIP), avoiding extensive LLM inference.
---

# From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning

## Quick Facts
- **arXiv ID**: 2510.24528
- **Source URL**: https://arxiv.org/abs/2510.24528
- **Authors**: Zihan Chen; Song Wang; Xingbo Fu; Chengshuai Shi; Zhenyu Lei; Cong Shen; Jundong Li
- **Reference count**: 24
- **Primary result**: Graph-based pseudo-labeling framework for in-context learning that reduces labeling costs while maintaining performance

## Executive Summary
This paper introduces a cost-efficient pipeline for in-context learning (ICL) on novel tasks by combining cross-task examples with graph-based label propagation. The method first uses a graph-based similarity metric (GraphSim) to select relevant examples from high-resource source tasks, then applies a graph neural network to propagate labels from a small pseudo-labeled set to the entire target dataset (GLIP), avoiding extensive LLM inference. Experiments across five tasks with multiple LLMs show that this approach outperforms existing cross-task baselines, reduces labeling costs, and achieves performance close to in-task upper bounds, demonstrating its effectiveness and scalability for low-resource settings.

## Method Summary
The framework addresses the high computational cost of in-context learning by creating a two-stage pipeline that leverages knowledge from high-resource source tasks. First, it employs a GraphSim metric to identify the most relevant examples from source tasks based on task description similarity. Then, it uses a Graph Neural Network (GNN) called GLIP to propagate labels from a small set of pseudo-labeled examples across the entire target dataset. This approach significantly reduces the number of expensive LLM inference calls needed while maintaining competitive performance. The method is evaluated across five different tasks using multiple LLMs including GPT-3.5 and Llama-2, demonstrating consistent improvements over traditional cross-task ICL approaches.

## Key Results
- Achieves performance close to in-task upper bounds while using significantly fewer LLM inference calls
- Outperforms existing cross-task baselines across five different tasks and multiple LLMs
- Reduces labeling costs by leveraging graph-based pseudo-labeling instead of full dataset annotation
- Demonstrates scalability and effectiveness for low-resource settings

## Why This Works (Mechanism)
The framework works by intelligently transferring knowledge from source tasks with abundant labeled data to target tasks with limited resources. The GraphSim metric captures semantic similarity between tasks through a graph-based representation, enabling the selection of most relevant examples for the target task. The GLIP component then uses graph neural networks to propagate labels through the target dataset based on learned relationships, mimicking the way humans infer patterns from limited examples. This approach bridges the gap between traditional few-shot learning and fully supervised learning by creating an intermediate step that balances cost and performance.

## Foundational Learning

**Graph Neural Networks (GNNs)**
- *Why needed*: To propagate labels through complex relationships in the target dataset
- *Quick check*: Can capture non-linear relationships and dependencies between data points

**Task Similarity Metrics**
- *Why needed*: To identify relevant source examples for cross-task knowledge transfer
- *Quick check*: Must accurately measure semantic overlap between different task descriptions

**In-Context Learning (ICL)**
- *Why needed*: The target learning paradigm being optimized
- *Quick check*: Requires understanding of how LLMs process examples without parameter updates

**Label Propagation**
- *Why needed*: To extend limited pseudo-labeled data across entire datasets
- *Quick check*: Should maintain label consistency while adapting to new data

**Graph-based Representations**
- *Why needed*: To model relationships between examples and tasks
- *Quick check*: Must preserve important structural information while being computationally tractable

## Architecture Onboarding

**Component Map**: Source Tasks -> GraphSim -> Example Selection -> Pseudo-labeling -> GLIP -> Target Dataset

**Critical Path**: The pipeline's critical path is the GraphSim selection followed by GLIP propagation. Performance bottlenecks would likely occur at the pseudo-labeling stage if the selected examples are not representative, or during GLIP if the graph structure doesn't capture relevant relationships.

**Design Tradeoffs**: The framework trades some precision (compared to full in-task labeling) for significant cost reduction. It assumes that cross-task knowledge transfer is effective, which may not hold for highly specialized or domain-specific tasks. The GraphSim metric must balance between being too strict (missing useful examples) and too permissive (including irrelevant examples).

**Failure Signatures**: 
- Poor GraphSim selection leading to irrelevant examples
- GLIP propagation failures due to inadequate graph structure
- Performance degradation when source and target tasks have minimal semantic overlap
- Overfitting to source task patterns that don't generalize

**First 3 Experiments**:
1. Evaluate GraphSim's ability to select relevant examples by comparing with random selection and human-curated examples
2. Test GLIP's label propagation accuracy on a small labeled subset of the target dataset
3. Measure the impact of varying the number of source examples on final target task performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the methodology suggests several areas for future investigation: how to handle tasks with significant domain shifts where source and target label distributions differ substantially, the optimal balance between cross-task and in-task examples, and how to adapt the framework for continuous learning scenarios where new tasks are added over time.

## Limitations

- Performance depends heavily on the quality and relevance of available source tasks
- Assumes label distributions are relatively consistent between source and target tasks
- Graph-based representations may not capture all contextual nuances that LLMs inherently understand
- Effectiveness may degrade for highly specialized or domain-specific tasks with limited semantic overlap

## Confidence

- **High Confidence**: Cost reduction claims and relative performance improvements over cross-task baselines
- **Medium Confidence**: Scalability assertions and performance proximity to in-task upper bounds
- **Medium Confidence**: The effectiveness of GraphSim for cross-task example selection

## Next Checks

1. Test framework performance on tasks with significant domain shifts where source and target label distributions differ substantially
2. Evaluate the impact of varying the size and quality of the pseudo-labeled set on GLIP performance
3. Compare GraphSim-selected examples against human-curated examples to quantify the quality difference in cross-task selection