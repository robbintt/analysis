---
ver: rpa2
title: 'Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models'
arxiv_id: '2503.22886'
source_url: https://arxiv.org/abs/2503.22886
tags:
- task
- tokens
- tasks
- maskedmimic
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Task Tokens, a method for adapting behavior
  foundation models (BFMs) to specific tasks while preserving their generalization
  capabilities. The core idea is to train a task-specific encoder that maps observations
  to additional input tokens for the BFM, allowing integration of task-specific optimization
  with the BFM's pre-trained behavioral priors.
---

# Task Tokens: A Flexible Approach to Adapting Behavior Foundation Models

## Quick Facts
- arXiv ID: 2503.22886
- Source URL: https://arxiv.org/abs/2503.22886
- Authors: Ron Vainshtein; Zohar Rimon; Shie Mannor; Chen Tessler
- Reference count: 11
- One-line result: Task Tokens achieves high success rates (94.88-99.75%) across multiple tasks while maintaining human-like motion quality

## Executive Summary
This paper introduces Task Tokens, a method for adapting behavior foundation models (BFMs) to specific tasks while preserving their generalization capabilities. The approach uses a task-specific encoder that maps observations to additional input tokens for the BFM, allowing integration of task-specific optimization with the BFM's pre-trained behavioral priors. The method employs reinforcement learning to train the task encoder while keeping the BFM frozen, enabling parameter-efficient adaptation.

Key results demonstrate that Task Tokens achieves superior performance compared to fine-tuning and PULSE methods across multiple control tasks, with success rates ranging from 76.61% to 99.75%. The approach shows improved sample efficiency, converging in approximately 50 million steps versus 300 million for PULSE, and maintains natural, human-like motion quality as confirmed by human studies.

## Method Summary
Task Tokens introduces a parameter-efficient method for adapting behavior foundation models to specific tasks by incorporating task-specific tokens into the BFM's input. The approach trains a task encoder that maps observations to additional tokens, which are then processed by the frozen BFM to generate actions. This is achieved through reinforcement learning while keeping the BFM parameters fixed, allowing the model to leverage pre-trained behavioral priors while adapting to task-specific requirements. The method uses a token generator network that produces 64 task tokens, which are combined with proprioceptive state information and passed through a transformer architecture. The approach demonstrates improved sample efficiency and maintains generalization capabilities compared to traditional fine-tuning methods.

## Key Results
- Achieves high success rates across multiple tasks: Reach (94.88%), Direction (99.26%), Steering (88.69%), Long Jump (99.75%), Strike (76.61%)
- Demonstrates superior sample efficiency, converging in ~50 million steps versus 300 million for PULSE
- Maintains natural, human-like motion quality as confirmed by human studies
- Shows improved robustness to out-of-distribution perturbations in friction and gravity

## Why This Works (Mechanism)
Task Tokens works by creating a bridge between task-specific requirements and the general behavioral priors learned by foundation models. The method preserves the BFM's pre-trained capabilities while allowing adaptation through task tokens that encode task-relevant information. By keeping the BFM frozen and only training the task encoder, the approach maintains the generalization properties of the foundation model while efficiently adapting to new tasks. The use of reinforcement learning enables the task encoder to learn optimal representations for each specific task without disrupting the underlying behavioral patterns.

## Foundational Learning

**Behavior Foundation Models**: Pre-trained models that capture general motion patterns from large datasets of human or animal behavior. Needed because they provide rich priors that can be adapted to specific tasks without learning from scratch. Quick check: Verify the BFM was trained on diverse motion data covering various skills.

**Task-Specific Token Encoding**: The process of mapping observations to additional input tokens that capture task-relevant information. Needed to bridge the gap between general behavior and specific task requirements. Quick check: Ensure the token generator produces meaningful representations for each task.

**Reinforcement Learning for Adaptation**: Using RL to train the task encoder while keeping the BFM frozen. Needed to maintain generalization while achieving task-specific performance. Quick check: Monitor both task success and motion quality during training.

## Architecture Onboarding

**Component Map**: Environment -> State Encoder -> Task Token Generator -> BFM Transformer -> Action Output -> Reward Signal -> RL Optimizer

**Critical Path**: State Encoder -> Task Token Generator -> BFM Transformer -> Action Output

**Design Tradeoffs**: The approach trades some task-specific optimization potential for maintaining generalization capabilities. Using frozen BFMs limits adaptation flexibility but preserves pre-trained behavioral priors. The 64-token configuration balances expressiveness with computational efficiency.

**Failure Signatures**: Poor task performance indicates inadequate token generation or insufficient RL training. Unnatural motions suggest the BFM is being overridden too aggressively. Low sample efficiency may indicate poor task token initialization or suboptimal RL hyperparameters.

**First 3 Experiments**: 1) Validate task token generation produces meaningful representations for simple tasks. 2) Test adaptation performance on a single task before scaling to multiple tasks. 3) Compare motion quality and task success between Task Tokens and fine-tuning baselines.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Restricted task diversity with evaluation focused on only five relatively simple control tasks
- Reliance on privileged state information (joint angles, velocities, end-effector positions) rather than vision-based observations
- Uncertainty about performance on complex, long-horizon tasks involving intricate contact dynamics

## Confidence
- High: Task success rates across multiple benchmark tasks with clear success criteria
- Medium: Human-likeness assessments based on subjective evaluations that may vary across populations
- Medium: Robustness claims to out-of-distribution perturbations evaluated only across limited perturbation ranges

## Next Checks
1. Evaluate Task Tokens on more complex tasks involving multi-contact manipulation or whole-body control scenarios to verify scalability
2. Implement and test the state encoder with vision-based observations rather than privileged state information to assess real-world applicability
3. Conduct ablation studies to determine the minimum number of task tokens required for effective performance