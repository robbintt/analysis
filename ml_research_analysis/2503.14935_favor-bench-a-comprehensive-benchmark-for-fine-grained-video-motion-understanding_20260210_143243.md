---
ver: rpa2
title: 'FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding'
arxiv_id: '2503.14935'
source_url: https://arxiv.org/abs/2503.14935
tags:
- video
- motion
- evaluation
- understanding
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding

## Quick Facts
- **arXiv ID**: 2503.14935
- **Source URL**: https://arxiv.org/abs/2503.14935
- **Reference count**: 40
- **Primary result**: New benchmark for fine-grained video motion understanding with 1,776 videos and 8,184 QA pairs across 6 motion tasks

## Executive Summary
FAVOR-Bench introduces a comprehensive benchmark for fine-grained video motion understanding, addressing the gap between holistic event-level and detailed motion-level comprehension in multimodal large language models (MLLMs). The benchmark includes both close-ended multiple-choice questions and open-ended video captioning tasks, with carefully designed annotations and filtering processes to ensure questions genuinely require temporal motion understanding. Through extensive evaluation of 15+ models and SFT training on a larger motion-focused dataset (FAVOR-Train), the authors demonstrate that current MLLMs struggle with detailed motion analysis despite strong overall performance, highlighting the need for specialized training data and evaluation methods for fine-grained motion understanding.

## Method Summary
FAVOR-Bench employs a multi-stage pipeline: video collection from Charades, TV-series, animations, and egocentric datasets; detailed manual annotation of subjects, motion lists with timestamps, camera motion, and captions; semi-automatic QA generation using DeepSeek-R1; blind filtering to remove questions solvable without video (via Qwen2-72B); single-frame filtering to eliminate questions answerable from 5 sampled frames (via GPT-4o); and manual verification. The benchmark includes 6 close-ended tasks (Action Sequence, Holistic Action Classification, Single Action Detail, Multi Action Detail, Camera Motion, and Number of Subjects Matching) and open-ended video captioning with both GPT-assisted and novel LLM-free evaluation frameworks using NLTK extraction and Sentence-BERT similarity. FAVOR-Train (17,152 videos) supports supervised fine-tuning to improve motion understanding.

## Key Results
- Strong MLLMs show significant performance gaps in camera motion tasks (Gemini-1.5: 41.58% vs. overall 49.87%)
- Action sequence scores high (>85%) but action match scores low (<40%) in LLM-free evaluation
- SFT on FAVOR-Train yields consistent improvements: +0.9% on TVBench, +2.7% on MotionBench-Dev
- Single-frame filtering reduces QA pairs from 20,402 to 12,096, with final 8,184 after manual verification
- FAVOR-Bench reveals current models' inability to prioritize temporal dynamics over static content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-automatic QA generation with multi-stage filtering produces challenging, motion-requiring questions.
- Mechanism: DeepSeek-R1 generates initial QA pairs from structured annotations; blind filtering removes questions solvable without video (via Qwen2-72B); single-frame filtering removes questions answerable from 5 sampled frames (via GPT-4o); manual verification ensures quality.
- Core assumption: Questions that survive both filters genuinely require temporal motion understanding.
- Evidence anchors: [abstract] "8,184 multiple-choice question-answer pairs spanning six distinct sub-tasks"; [section 3.3.2] "12,096 QA pairs remain" (from 20,402) after filtering.

### Mechanism 2
- Claim: LLM-free evaluation framework provides interpretable, reproducible motion caption assessment.
- Mechanism: NLTK-based extraction parses camera motion, subjects, action lists, and temporal sequences from model responses; Sentence-BERT computes semantic similarity matrices; weighted combination of precision/recall/order scores yields final evaluation.
- Core assumption: Structured extraction reliably captures motion elements; semantic similarity correlates with human judgment of correctness.
- Evidence anchors: [abstract] "novel cost-efficient LLM-free and a GPT-assisted caption assessment method"; [section 3.4.2] Equations 1-3 define similarity-weighted precision and recall with length factor penalty.

### Mechanism 3
- Claim: Fine-grained motion annotations enable supervised fine-tuning that transfers to external benchmarks.
- Mechanism: FAVOR-Train (17,152 videos with fine-grained manual captions) used for SFT on Qwen2.5-VL; model learns to output detailed motion descriptions.
- Core assumption: Fine-grained annotation quality is sufficient; SFT on motion-focused data generalizes beyond in-domain test sets.
- Evidence anchors: [abstract] "finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on motion-related tasks of TVBench, MotionBench and our FAVOR-Bench"; [section 4.3/Table 3] +0.9% on TVBench, +2.7% on MotionBench-Dev.

## Foundational Learning

- **Temporal dynamics vs. static content**:
  - Why needed here: Core distinction in FAVOR-Bench is motion-level (not event-level) understanding; models often default to static descriptions.
  - Quick check question: Given a video of someone walking then stopping, can you describe the *transition* rather than just "a person standing"?

- **Single-frame bias**:
  - Why needed here: Many video QA questions are answerable from one frame; FAVOR-Bench explicitly filters these out.
  - Quick check question: If a question asks "What is the person wearing?" in a video, does this require temporal understanding?

- **Semantic similarity with Sentence-BERT**:
  - Why needed here: LLM-free evaluation relies on embedding-based matching between predicted and ground-truth action sequences.
  - Quick check question: Would "walks forward" and "moves ahead" receive high semantic similarity? Should they?

## Architecture Onboarding

- **Component map**: Video sources → Manual annotation (subjects, motion lists with timestamps, camera motion, captions) → QA generation (DeepSeek-R1) → Filtering (blind + single-frame) → Manual verification → Close-ended/Open-ended evaluation (LLM-free/GPT-assisted) → SFT training pipeline

- **Critical path**: QA generation quality depends on annotation quality; filtering reduces 20,402 → 8,184 QA pairs (60% removal); LLM-free evaluation requires robust extraction rules.

- **Design tradeoffs**: LLM-free vs. GPT-assisted evaluation (cost/interpretability vs. nuanced judgment); 5-frame sampling for single-frame filtering (more frames = stricter filter); number of annotators (8 hired, 2-week process).

- **Failure signatures**: Models scoring high on Holistic Action Classification but low on Single Action Detail; camera motion task particularly challenging for strong models; action sequence scores high (>85%) but action match scores low (<40%) in LLM-free evaluation.

- **First 3 experiments**:
  1. Replicate blind filtering on a sample: run Qwen2-72B on held-out QA pairs without video; verify correlation with question difficulty.
  2. Test LLM-free extraction robustness: manually verify NLTK parsing accuracy on 50 model responses; identify systematic extraction failures.
  3. Ablate FAVOR-Train components: train separate models on ego-centric vs. third-person subsets; measure transfer to each benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications would enable MLLMs to better understand camera motion dynamics, given that strong models significantly underperform on camera motion tasks?
- Basis in paper: [explicit] "Camera Motion task proves challenging, especially for models with strong comprehensive capabilities... Their CM scores are significantly lower than the overall scores, suggesting that MLLMs face difficulties in understanding viewpoint dynamics and focus shifts."
- Why unresolved: The paper identifies this weakness but only shows that FAVOR-Train improves camera motion understanding (+16.54%), without exploring architectural solutions.
- What evidence would resolve it: Ablation studies comparing different temporal encoding mechanisms or attention patterns specifically designed for viewpoint dynamics.

### Open Question 2
- Question: Why do MLLMs achieve high action sequence ordering accuracy (>85%) while failing at comprehensive action matching (<40%), and how can this gap be closed?
- Basis in paper: [explicit] Supplementary results show "most models achieve high scores (over 85%) in both subject-specific and temporal action sequences, they have significant difficulty with action matching (typically scoring below 40%)."
- Why unresolved: This suggests models recognize temporal patterns when actions are identified but fail to detect many fine-grained motions entirely—the paper does not investigate the root cause.
- What evidence would resolve it: Analysis of attention maps during action detection tasks, or probing experiments to determine whether the failure is in low-level motion feature extraction vs. high-level semantic grounding.

### Open Question 3
- Question: Can models be trained to prioritize temporal dynamics over static content in open-ended description tasks without explicit fine-grained motion supervision?
- Basis in paper: [explicit] "even when explicitly instructed to focus on the temporal dynamics in the video, models predominantly emphasize static content and often lack fine-grained analysis of the motions and activities."
- Why unresolved: FAVOR-Train shows supervised fine-tuning helps, but the mechanism by which static content dominates responses—and whether this can be addressed through training objectives or prompting strategies alone—remains unexplored.
- What evidence would resolve it: Comparative experiments using different loss functions or instruction-tuning approaches that penalize static-only descriptions.

## Limitations

- Semi-automatic QA generation process may discard valid motion questions or retain trivial ones despite multi-stage filtering
- LLM-free evaluation framework lacks direct human judgment correlation validation
- Transfer learning improvements to external benchmarks are relatively modest (+0.9% to TVBench, +2.7% to MotionBench)
- Fine-grained motion annotation process is resource-intensive (8 annotators, 2-week process)

## Confidence

- **Medium Confidence**: Claims about the effectiveness of the multi-stage filtering process in producing motion-requiring questions
- **Medium Confidence**: LLM-free evaluation framework claims and semantic similarity assessment
- **High Confidence**: Dataset creation and annotation quality claims with documented quality control measures

## Next Checks

1. **Filter Effectiveness Validation**: Conduct human evaluation of a stratified sample of filtered-out and retained QA pairs to assess whether filtering accurately identifies motion-requiring questions.

2. **LLM-Free Evaluation Validation**: Compare LLM-free evaluation scores against human judgments for 100 random model outputs to calculate correlation coefficients and validate Sentence-BERT similarity assessment.

3. **Transfer Learning Robustness**: Perform ablation studies on FAVOR-Train to determine which components contribute most to improvements and test generalization of motion-focused subsets.