---
ver: rpa2
title: 'Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop
  multilingual parallel corpus with multi-word expression annotation'
arxiv_id: '2011.03783'
source_url: https://arxiv.org/abs/2011.03783
tags:
- translation
- mwes
- corpus
- sentence
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AlphaMWE, a multilingual parallel corpus with
  verbal multi-word expression (vMWE) annotations across Arabic, Chinese, German,
  Italian, Polish, and English. The corpus is constructed by translating an English
  vMWE-annotated corpus using machine translation (MT) followed by human post-editing
  and annotation.
---

# Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop multilingual parallel corpus with multi-word expression annotation

## Quick Facts
- arXiv ID: 2011.03783
- Source URL: https://arxiv.org/abs/2011.03783
- Reference count: 0
- Primary result: Introduces AlphaMWE, a multilingual parallel corpus with vMWE annotations across six languages, revealing significant MT challenges in translating idiomatic and metaphorical expressions

## Executive Summary
This paper presents AlphaMWE, a novel multilingual parallel corpus with verbal multi-word expression (vMWE) annotations across Arabic, Chinese, German, Italian, Polish, and English. The corpus was constructed by translating an English vMWE-annotated corpus using machine translation followed by human post-editing and annotation. The authors evaluated four state-of-the-art MT systems (DeepL, GoogleMT, Bing, Baidu) on their ability to translate MWEs, finding significant challenges particularly with idiomatic expressions, metaphors, and named entities. The resulting resource includes sentence-aligned bilingual and multilingual vMWE pairs, with Arabic covering both standard and dialectal variations, and serves as a valuable dataset for monolingual and cross-lingual NLP tasks.

## Method Summary
The corpus construction pipeline began with an English vMWE-annotated corpus from the PARSEME shared task 2018 (750 sentences). The authors first compared four MT systems on a small sample to select DeepL as the primary engine (GoogleMT for Arabic, human translation for dialectal Arabic). MT output was then post-edited by humans who corrected translations while preserving vMWE meaning and alignment. Annotators identified and tagged target language vMWEs corresponding to source vMWEs, with a second manual quality check ensuring consensus. The HOPE metric was used to categorize and quantify MT errors, particularly focusing on MWE-related failures across four categories: common sense, idioms, named entities, and metaphors.

## Key Results
- AlphaMWE corpus contains 750 sentence-aligned bilingual and multilingual vMWE pairs across six languages
- HOPE metric evaluation showed 21% major translation errors and 44% minor errors in English-Arabic translations
- MWE-related errors comprised nearly 20% of all translation errors across the corpus
- MT systems consistently failed to properly translate idioms, metaphors, and named entities, often producing literal translations or dropping content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Utilizing Machine Translation (MT) as a bootstrap step significantly accelerates parallel corpus construction compared to translation from scratch, provided human post-editing follows.
- **Mechanism:** The pipeline generates a "good enough" draft using SOTA MT (specifically DeepL in this study), which human annotators then refine. This leverages the fluency of neural MT while relying on humans for semantic accuracy and MWE preservation.
- **Core assumption:** The selected MT engine produces output that is fluent enough to be corrected faster than translating manually, even for complex idiomatic content.
- **Evidence anchors:**
  - [Abstract]: Mentions "MT assisted and human-in-the-loop... corpus construction."
  - [Section 3.1]: Explicitly states the rationale: "saves a lot of time in comparison to human translation from scratch."
  - [Corpus]: The resulting AlphaMWE dataset maintains sentence alignment across 6 languages, validating the structural integrity of the MT-first approach.
- **Break condition:** If the MT system produces consistently nonsensical or "hallucinated" outputs for a specific language pair (e.g., dialectal Arabic), the post-editing burden may exceed the cost of translating from scratch.

### Mechanism 2
- **Claim:** Multi-stage cross-validation by native speakers reduces annotation noise and resolves ambiguity in non-compositional expressions.
- **Mechanism:** The process enforces a "second manual quality rechecking" phase until consensus is reached. This specifically targets the subjectivity inherent in identifying MWE boundaries and translating idioms.
- **Core assumption:** Linguistic intuition of native speakers is superior to algorithmic tagging for identifying verbal MWEs (vMWEs) in context.
- **Evidence anchors:**
  - [Abstract]: Highlights "Strict quality control was applied for error limitation."
  - [Section 3.1]: Describes the "cross-validation strategy, where each sentence receives at least a second person's quality checking."
  - [Section 6.1]: Notes that single-annotator approaches in source corpora led to errors that this pipeline aimed to correct.
- **Break condition:** If annotator guidelines are ambiguous regarding MWE categories (e.g., distinguishing Light Verb Constructions from literal phrases), consensus may fail or introduce systematic bias.

### Mechanism 3
- **Claim:** Defining a fine-grained error taxonomy (e.g., idioms, metaphors, named entities) exposes specific semantic blind spots in SOTA MT systems.
- **Mechanism:** By categorizing "where" MT fails (e.g., literal translation of idioms), the resource moves beyond scalar quality scores (like BLEU) to actionable linguistic feedback.
- **Core assumption:** MT models fail on MWEs largely due to a lack of context-awareness and non-compositional semantic representation.
- **Evidence anchors:**
  - [Section 4]: Introduces a categorization of errors including "Common sense," "Idioms," and "Metaphor."
  - [Section 5]: Uses the HOPE metric to quantify errors, finding 21% major errors in English-Arabic translations.
  - [Section 6.2]: Discusses "Mis-use of MWEs" where MT selects a target idiom that statistically matches but is semantically wrong.
- **Break condition:** If the error taxonomy is too broad, it fails to guide model improvement; if too narrow, it becomes subjective to the annotator's linguistic theory.

## Foundational Learning

- **Concept: Multi-Word Expressions (MWEs) & Non-compositionality**
  - **Why needed here:** MWEs (e.g., "kick the bucket") are phrases where meaning cannot be derived from constituent words. This paper focuses on verbal MWEs (vMWEs) which are notoriously difficult for MT.
  - **Quick check question:** Can you distinguish a "Light Verb Construction" (e.g., "take a walk") from a standard verb phrase?

- **Concept: Human-in-the-loop (HITL) Post-editing**
  - **Why needed here:** The core methodology relies on humans correcting MT output. Understanding the difference between "light" post-editing (fluency only) and "deep" post-editing (semantic accuracy/MWE preservation) is critical.
  - **Quick check question:** Does the pipeline treat the MT output as a suggestion (translation aid) or a constraint (minimal editing)?

- **Concept: Cross-lingual Alignment**
  - **Why needed here:** The value of AlphaMWE lies in aligning source MWEs (English) with target MWEs (e.g., Chinese). Alignment is not always 1:1; an English idiom may map to a single word in German or a different idiom in Arabic.
  - **Quick check question:** How should the corpus handle a source MWE that translates to a non-MWE phrase in the target language?

## Architecture Onboarding

- **Component map:** PARSEME English Corpus (vMWE annotated) -> MT Engine (DeepL) -> Human Post-Editing & Annotation Tool -> Cross-checking module (2nd annotator review) -> AlphaMWE (Sentence-aligned, MWE-tagged parallel corpus)

- **Critical path:** The selection of the MT engine is the primary bottleneck. As shown in [Section 3.2], choosing an engine that produces structurally divergent errors (like Baidu's reordering errors vs. Google's entity errors) dictates the difficulty of the subsequent human post-editing phase.

- **Design tradeoffs:**
  - **MT Bias vs. Efficiency:** Using MT outputs biases the dataset toward the errors/styles of that specific engine. The authors acknowledge this limitation in [Section 7].
  - **Dialectal Inclusion:** Standard MT failed for dialectal Arabic (Egyptian/Tunisian), forcing a manual translation switch, which increases cost but ensures coverage [Section 3.1].

- **Failure signatures:**
  - **Literalism:** MT translating "cutting capers" as physical cutting actions [Section 4.1.4].
  - **Entity Dropping:** MT dropping subject nouns or named entities (e.g., "SQL Server") [Section 3.2].
  - **Register Mismatch:** Translating formal text into informal target text or vice versa.

- **First 3 experiments:**
  1. **Metric Validation:** Calculate HOPE scores for a new MT model using the English-Arabic subset to benchmark against the reported 21% major error rate.
  2. **Error Injection:** Fine-tune a small NMT model on AlphaMWE to see if "MWE Missed Chance" (MMC) errors decrease compared to a baseline.
  3. **Cross-lingual Transfer:** Test if MWE tags in English can automatically predict MWE boundaries in Polish or Chinese using the alignment data provided.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel neural network architectures be designed to explicitly incorporate MWE compositionality during the training stage to improve translation quality?
- Basis in paper: [explicit] The conclusion (Section 7) explicitly asks, "is it possible to design novel neural network structures to incorporate the MWE compositionality as part of the MT learning stage?"
- Why unresolved: Current state-of-the-art systems frequently translate idiomatic and metaphorical MWEs literally (e.g., "cutting capers" translated as physically cutting objects), failing to capture non-compositionality.
- What evidence would resolve it: A new model architecture that yields significantly fewer literal translation errors on the AlphaMWE idiomatic subset compared to standard Transformer baselines.

### Open Question 2
- Question: Does providing document-level context improve the translation of context-dependent MWEs for current models, or do they fail to utilize this information?
- Basis in paper: [inferred] Sections 4.1.6 and 6.3 demonstrate that providing paragraph context to MT systems often results in the same incorrect output as sentence-level translation.
- Why unresolved: The paper shows that systems fail to use context to resolve ambiguity (e.g., "time of day" or "complaint"), suggesting a limitation in how context is integrated rather than just a lack of data.
- What evidence would resolve it: A comparative evaluation showing that document-level models successfully disambiguate terms like "complaint" (illness vs. statement) using discourse cues where sentence-level models fail.

### Open Question 3
- Question: How can bilingual terminologies and dictionaries be effectively integrated into NMT to mitigate errors in metaphorical and idiomatic expressions?
- Basis in paper: [explicit] Section 7 asks, "how to integrate bilingual terminologies or dictionaries including paraphrases and synonyms to improve metaphorical and idiomatic phrase translation."
- Why unresolved: The analysis reveals specific failure modes in "Super Sense" and "Metaphor" categories (e.g., translating "salutatory emptiness" with negative connotations) which external lexical resources might resolve.
- What evidence would resolve it: Experiments integrating external MWE lexicons into the decoding process, resulting in improved HOPE metric scores specifically for the "MWE Missed Chance" (MMC) error category.

## Limitations

- **MT Engine Bias:** The corpus relies on a single MT engine (DeepL), introducing systematic bias in the error patterns observed
- **Sample Size for Engine Selection:** The comparison of four MT systems used only ~10 sentences, potentially missing edge cases
- **Dialectal Arabic Inconsistency:** Dialectal Arabic segments were manually translated from scratch rather than MT-assisted, creating pipeline inconsistency

## Confidence

- **Methodology:** High - The cross-validation process and multiple quality checks provide strong validation
- **MT Error Analysis:** Medium - Limited sample size (10 sentences) for engine selection introduces uncertainty
- **Arabic Dialectal Coverage:** Medium - Manual translation approach for dialects differs from main pipeline

## Next Checks

1. Benchmark the AlphaMWE corpus against newer MT systems (e.g., GPT-4, Claude) using the HOPE metric to assess if error patterns have evolved
2. Measure inter-annotator agreement rates on vMWE boundary identification and alignment decisions to quantify annotation consistency
3. Conduct extrinsic evaluation by training an MWE identification model on AlphaMWE and testing on held-out data from the original PARSEME corpus to validate cross-lingual transfer quality