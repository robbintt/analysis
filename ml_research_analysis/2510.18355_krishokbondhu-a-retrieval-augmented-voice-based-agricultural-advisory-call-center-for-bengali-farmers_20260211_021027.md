---
ver: rpa2
title: 'KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call
  Center for Bengali Farmers'
arxiv_id: '2510.18355'
source_url: https://arxiv.org/abs/2510.18355
tags:
- agricultural
- krishokbondhu
- responses
- bengali
- farmers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KrishokBondhu is a voice-enabled agricultural advisory system for
  Bengali-speaking farmers that combines OCR-based document digitization, semantic
  retrieval (LanceDB), and the Gemma 3-4B language model to deliver context-grounded
  responses via a phone-based interface. The system processes authoritative Bengali
  agricultural texts, embeds them for semantic search, and generates voice responses
  to spoken queries.
---

# KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers
## Quick Facts
- arXiv ID: 2510.18355
- Source URL: https://arxiv.org/abs/2510.18355
- Reference count: 24
- Achieved 72.7% high-quality responses and 4.53/5.00 overall score, a 44.7% improvement over KisanQRS benchmark

## Executive Summary
KrishokBondhu is a voice-enabled agricultural advisory system designed for Bengali-speaking farmers that combines OCR-based document digitization, semantic retrieval using LanceDB, and the Gemma 3-4B language model to deliver context-grounded responses via phone-based interface. The system processes authoritative Bengali agricultural texts, embeds them for semantic search, and generates voice responses to spoken queries. In pilot testing, KrishokBondhu demonstrated significant improvements over existing baselines, achieving 72.7% high-quality responses and scoring 4.53/5.00 overall, representing a 44.7% improvement over the KisanQRS benchmark, with especially large gains in contextual richness (+367%) and completeness (+100.4%).

## Method Summary
The system architecture integrates Optical Character Recognition (OCR) for document digitization, semantic search using LanceDB for retrieval, and the Gemma 3-4B language model for response generation. Bengali agricultural documents are first digitized using OCR, then embedded and indexed for semantic search. When farmers call with queries, their spoken questions are processed through ASR, the system retrieves relevant passages using semantic similarity, and the language model generates context-grounded voice responses. The approach combines document processing, semantic search, and language generation to provide expert-level agricultural guidance in Bengali through accessible voice interaction.

## Key Results
- Achieved 72.7% high-quality responses in pilot testing
- Scored 4.53/5.00 overall, representing 44.7% improvement over KisanQRS benchmark
- Showed dramatic improvements in contextual richness (+367%) and completeness (+100.4%)

## Why This Works (Mechanism)
The system leverages retrieval-augmented generation (RAG) to ground responses in authoritative agricultural documents rather than relying solely on model knowledge. By using semantic search on embedded Bengali texts, it can find relevant passages even when queries use different terminology than the source documents. The combination of OCR digitization, semantic retrieval, and language model generation creates a system that can handle diverse farmer queries while maintaining accuracy through grounding in expert-validated content.

## Foundational Learning
- **OCR Digitization**: Converts printed Bengali agricultural documents into machine-readable text - needed to make paper-based agricultural knowledge accessible to digital systems; quick check: test OCR accuracy on varied document quality
- **Semantic Search with Embeddings**: Maps text to vector representations for similarity search - needed to find relevant information even when query and document use different wording; quick check: measure retrieval precision at different embedding dimensions
- **RAG Architecture**: Combines retrieval with language model generation - needed to ground responses in factual documents rather than model hallucination; quick check: compare RAG vs non-RAG response accuracy
- **Voice Interface Design**: Enables phone-based access for farmers - needed to overcome literacy and smartphone barriers in rural areas; quick check: test ASR accuracy across different Bengali dialects
- **LanceDB Semantic Search**: Provides efficient vector similarity search - needed to quickly find relevant passages in large document collections; quick check: measure retrieval speed with varying document corpus sizes
- **Bengali Language Processing**: Handles a low-resource language with specific linguistic challenges - needed to serve the target user population effectively; quick check: evaluate response quality across different Bengali script variants

## Architecture Onboarding
- **Component Map**: OCR -> Document Embedding -> LanceDB Index -> ASR Query -> Semantic Search -> Gemma 3-4B -> Voice Response
- **Critical Path**: Spoken query → ASR transcription → semantic search → document retrieval → language model generation → voice synthesis → farmer response
- **Design Tradeoffs**: Larger language models could improve response quality but increase computational cost and latency; simpler retrieval methods would be faster but less accurate for semantic matching
- **Failure Signatures**: Poor OCR quality leads to missing information in retrieval; ASR errors in query transcription cause irrelevant document matches; insufficient training data limits model's ability to generate natural-sounding responses
- **First 3 Experiments**: 1) Test OCR accuracy on diverse document quality levels, 2) Measure semantic search precision with varying embedding dimensions, 3) Evaluate response quality with different RAG configuration parameters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation based on only 11 sample questions rated by 3 human raters, limiting statistical power and generalizability
- Performance claims rely on comparison to KisanQRS rather than direct expert validation of "expert-level" guidance
- OCR preprocessing pipeline accuracy on real-world document conditions (varying fonts, handwritten annotations, degraded scans) not characterized
- Voice interaction quality depends on ASR accuracy, which is not evaluated separately from overall system performance

## Confidence
- **High confidence**: The system architecture combining OCR, semantic retrieval, and language model generation is technically sound and well-documented
- **Medium confidence**: Performance metrics showing 72.7% high-quality responses and 4.53/5.00 overall score are plausible given the technology stack, but limited sample size and rater count reduce confidence in generalizability
- **Low confidence**: Claims about real-world deployment readiness and expert-level guidance quality require additional validation beyond the controlled pilot study

## Next Checks
1. Conduct evaluation with at least 30 diverse agricultural questions rated by 5-10 domain experts rather than general raters, with inter-rater reliability metrics reported
2. Test system performance on a corpus of 100+ real-world agricultural documents with varying quality (handwritten notes, degraded scans, different Bengali scripts) to characterize OCR robustness and retrieval accuracy under field conditions
3. Deploy a small-scale field trial with 50+ farmers over 3+ months, tracking query success rates, farmer satisfaction, and actual adoption behavior to validate real-world utility beyond controlled testing