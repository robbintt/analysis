---
ver: rpa2
title: Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation
  with Large Language Models
arxiv_id: '2506.05675'
source_url: https://arxiv.org/abs/2506.05675
tags:
- event
- causal
- causality
- mefa
- fuzzy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses zero-shot event causality identification (ECI)
  in text, focusing on mitigating causal hallucination in large language models (LLMs).
  The proposed MEFA framework decomposes causality into three main tasks (temporality,
  necessity, sufficiency) and three auxiliary tasks (dependency, causal clue extraction,
  coreference), using LLM-based uncertain reasoning and deterministic reasoning for
  each task.
---

# Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models

## Quick Facts
- **arXiv ID:** 2506.05675
- **Source URL:** https://arxiv.org/abs/2506.05675
- **Reference count:** 40
- **Primary result:** MEFA achieves 6.2% higher F1-score and 9.3% higher precision than the second-best unsupervised baseline on three benchmark datasets

## Executive Summary
This paper introduces MEFA, a zero-shot framework for Event Causality Identification (ECI) that addresses the challenge of causal hallucination in large language models (LLMs). The framework decomposes causality reasoning into three main tasks (temporality, necessity, sufficiency) and three auxiliary tasks (dependency, causal clue extraction, coreference), using LLM-based uncertain reasoning for each task. A fuzzy Choquet integral aggregates multi-source evidence to produce causality scores, significantly reducing hallucination-induced errors. Experiments on three benchmark datasets show MEFA outperforms state-of-the-art unsupervised methods, particularly in precision.

## Method Summary
MEFA processes event pairs by decomposing causality identification into six sub-tasks executed by an LLM. The three main tasks (temporality, necessity, sufficiency) use uncertain reasoning to produce probability distributions, while three auxiliary tasks use deterministic reasoning for supporting evidence. Each sub-task is processed through carefully designed prompts that elicit specific responses. The framework then converts these responses into numerical scores and aggregates them using a fuzzy Choquet integral, which captures non-linear dependencies between evidence sources. The aggregated scores are compared against a threshold to determine causality direction and existence.

## Key Results
- MEFA achieves 6.2% higher F1-score and 9.3% higher precision than the second-best unsupervised baseline
- The framework significantly reduces hallucination-induced errors compared to vanilla LLM approaches
- MEFA outperforms traditional methods on both sentence-level (CTB, ESL) and document-level (MAVEN-ERE) causality identification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing causality into distinct sub-tasks constrains the LLM's reasoning space, reducing causal hallucinations.
- **Mechanism:** Instead of direct "Does A cause B?" queries, MEFA forces evaluation of logical prerequisites independently. Failure in one prerequisite (e.g., lack of necessity) can veto positive causality, acting as a logical consistency filter.
- **Core assumption:** Causality requires the conjunction of temporality, necessity, and sufficiency, evaluable independently by an LLM.
- **Evidence anchors:** [abstract] decomposes causality into three main tasks... [section IV-A] task decomposition ensures causality is evaluated from multiple perspectives... [corpus] HCR-Reasoner supports this strategy.
- **Break condition:** If LLM fails to understand sub-task nuances, decomposition propagates rather than corrects errors.

### Mechanism 2
- **Claim:** Fuzzy Choquet integral aggregation outperforms simple averaging by capturing non-linear dependencies between evidence sources.
- **Mechanism:** Unlike linear weights assuming independence, the fuzzy measure $\mu$ assigns weight to coalitions of evidence, allowing prioritization of synergy or penalty of redundancy.
- **Core assumption:** Evidence sources are not independent; their interactions are mathematically significant for final decisions.
- **Evidence anchors:** [abstract] employs fuzzy aggregation to integrate evidence... [section IV-C] The term $\sum_{j,k \in S_i} x_j x_k$ captures synergy gain... [section V-B-4] fuzzy Choquet integral outperforms other methods.
- **Break condition:** Incorrect hyperparameter tuning ($a, b$) leads to over-penalization or over-reward of evidence coalitions.

### Mechanism 3
- **Claim:** Eliciting uncertainty from LLM prevents single-point failures from minor reasoning errors.
- **Mechanism:** Generating response vectors like `{BEFORE: 0.85, AFTER: 0.10}` retains information about model doubt, allowing graded decisions rather than failing entirely on one incorrect output.
- **Core assumption:** LLMs can reliably quantify their own uncertainty in format correlating with actual likelihood of correctness.
- **Evidence anchors:** [section IV-A] LLM performs uncertainty-quantified reasoning for primary tasks... [section V-B-3] introduction of uncertain reasoning leads to significant improvements... [corpus] ACCESS benchmark highlights struggle with out-of-distribution settings.
- **Break condition:** If LLM is systematically overconfident, uncertainty values become noise rather than signal.

## Foundational Learning

- **Concept:** **Fuzzy Measures & Choquet Integral**
  - **Why needed here:** The Choquet integral (Eq. 3) generalizes weighted sum by accounting for interactions between criteria.
  - **Quick check question:** If Temporality and Necessity are highly correlated, how does a Fuzzy Choquet integral treat them differently than a weighted average?

- **Concept:** **Counterfactual Reasoning (Necessity vs. Sufficiency)**
  - **Why needed here:** Paper decomposes causality into Necessity and Sufficiency; must distinguish them to design prompts.
  - **Quick check question:** Does Figure 3 prompt design correctly elicit distinction between "PRECONDITION" (Necessity) and "SUFFICIENCY"?

- **Concept:** **Causal Hallucination**
  - **Why needed here:** Problem MEFA solves; refers to LLMs generating plausible-sounding but non-existent causal links.
  - **Quick check question:** In Figure 1, why does basic prompt mistakenly link "ran away" and "wearing" as causal?

## Architecture Onboarding

- **Component map:** Input (Context $D$ + Event Pair $(e_i, e_j)$) -> Prompt Engine (6 parallel templates) -> LLM Backbone (processes prompts) -> Quantification Module (converts text to numerical scores) -> Aggregation Layer (computes Fuzzy Choquet Integral) -> Decision Layer (applies threshold $\theta$)

- **Critical path:** The hyperparameters ($\theta$, $a$, $b$, $\delta$, $\beta$) are highly sensitive. The validation set generation using CauseNet and sentence rewriting (Appendix IV) is most critical for reproducibility.

- **Design tradeoffs:**
  - Precision vs. Recall: MEFA optimizes for Precision (reducing false positives) at cost of Recall
  - Complexity vs. Interpretability: Fuzzy Integral is mathematically complex but offers transparent score
  - Zero-shot vs. Supervised: Gets domain flexibility but performance lags behind supervised models

- **Failure signatures:**
  - "Strong" Dependency Bias: If LLM rarely outputs "Strong" dependency, $w_d$ may dampen all scores
  - Hallucination Propagation: If LLM hallucinates "Strong" dependency for unrelated events, aggregation likely passes error through

- **First 3 experiments:**
  1. Run SP (Simple Prompt) and ZSCoT baselines on small CTB slice to confirm high False Positive rate
  2. Swap Choquet integral for simple Weighted Average on same data to check performance drop
  3. Grid search on threshold $\theta$ (start with 0.6) to observe Precision-Recall tradeoff curve

## Open Questions the Paper Calls Out

- **Open Question 1:** How can zero-shot frameworks like MEFA bridge performance gap with supervised models, particularly for DECI?
  - **Basis:** [explicit] Conclusion states MEFA exhibits limitations in performance gap compared to supervised methods (particularly for DECI tasks)
  - **Why unresolved:** Complexity of inter-sentence dependencies in DECI still poses challenge that current zero-shot reasoning and fuzzy aggregation don't fully overcome
  - **What evidence would resolve it:** Modified MEFA achieving statistically comparable F1-scores to supervised baselines on DECI benchmarks

- **Open Question 2:** Can few-shot learning strategies with adaptive parameter tuning replace manual hyperparameter optimization?
  - **Basis:** [explicit] Conclusion notes framework relies on multiple hyperparameters requiring optimization
  - **Why unresolved:** Current model requires manually generated validation set and grid search, limiting "instant adaptability"
  - **What evidence would resolve it:** Automated MEFA dynamically adjusting weights using few-shot examples without external validation data

- **Open Question 3:** Does enhancing LLMs with causal reasoning pretraining improve consistency of decomposed sub-tasks better than prompt engineering?
  - **Basis:** [explicit] Conclusion outlines future development of ECI-specialized LLMs enhanced through causal reasoning pretraining
  - **Why unresolved:** Current method relies on meticulously designed prompts to guide generic LLMs that still suffer from reasoning inconsistencies
  - **What evidence would resolve it:** Experiments comparing uncertainty quantification and sub-task accuracy of generic vs. specialized causal-reasoning LLMs

## Limitations

- Performance gap remains compared to supervised methods, particularly for complex document-level tasks
- Framework requires multiple hyperparameters that need optimization through external validation sets
- Generalizability across diverse domains and different LLM architectures needs further validation

## Confidence

- **High Confidence:** Decomposition framework's effectiveness in reducing false positives (hallucinations) is well-supported by 9.3% precision improvement over baselines
- **Medium Confidence:** Zero-shot capability claims supported by experimental results but limited to three datasets; superiority of fuzzy aggregation over simple averaging demonstrated but could be dataset-dependent
- **Low Confidence:** Optimal hyperparameter values likely dataset-specific; paper doesn't provide sensitivity analysis across different domains

## Next Checks

1. **Cross-domain validation:** Test MEFA on a dataset from different domain (e.g., scientific literature) using same hyperparameters to assess generalizability

2. **Ablation on uncertainty elicitation:** Compare MEFA's performance when using deterministic outputs instead of probabilistic responses to quantify uncertainty quantification contribution

3. **Aggregation method comparison:** Systematically compare fuzzy Choquet integral against weighted averaging, geometric mean, and other non-linear aggregation methods across multiple datasets to validate claimed superiority