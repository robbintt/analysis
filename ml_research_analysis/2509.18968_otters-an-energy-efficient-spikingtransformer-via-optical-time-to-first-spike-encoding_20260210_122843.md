---
ver: rpa2
title: 'Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike
  Encoding'
arxiv_id: '2509.18968'
source_url: https://arxiv.org/abs/2509.18968
tags:
- energy
- spike
- otters
- neuron
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Otters, a spiking transformer that achieves\
  \ state-of-the-art energy efficiency by replacing digital TTFS decay computations\
  \ with natural signal decay in custom In2O3 optoelectronic synapses. The device\u2019\
  s analog output serves as the fused product of weight and temporal decay, eliminating\
  \ costly multiplications."
---

# Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding

## Quick Facts
- arXiv ID: 2509.18968
- Source URL: https://arxiv.org/abs/2509.18968
- Reference count: 34
- Primary result: Achieves 4.06 mJ per attention block with 83.22% GLUE accuracy

## Executive Summary
This paper introduces Otters, a spiking transformer architecture that achieves state-of-the-art energy efficiency by leveraging optical time-to-first-spike encoding with custom In2O3 optoelectronic synapses. The key innovation replaces digital TTFS decay computations with natural signal decay in analog optoelectronic devices, eliminating costly multiplications while maintaining competitive accuracy. A QNN-to-SNN conversion algorithm enables lossless mapping from quantized models to the spiking domain, complemented by a novel 1-bit KV cache for self-attention.

## Method Summary
The approach combines optical time-to-first-spike encoding with In2O3 optoelectronic synapses to perform spike encoding, transformation, and decoding in an energy-efficient manner. The device's analog output naturally fuses weight and temporal decay, eliminating digital multiplication operations. A hardware-aware training method improves robustness to device variability, while a QNN-to-SNN conversion algorithm enables lossless mapping from quantized models. The system achieves 83.22% average accuracy on GLUE while consuming only 4.06 mJ per attention block.

## Key Results
- Achieves 83.22% average accuracy on GLUE benchmark, 3% higher than prior SNNs
- Consumes only 4.06 mJ per attention block
- 41.36× less energy than full-precision BERT
- 1.77× more efficient than leading SNNs

## Why This Works (Mechanism)
The approach exploits the natural temporal decay properties of optoelectronic devices to perform computation, replacing expensive digital operations with analog signal processing. The In2O3 optoelectronic synapses provide the fused product of weight and temporal decay directly, eliminating the need for explicit multiplication operations. The QNN-to-SNN conversion maintains precision while enabling the spiking computation model, and the 1-bit KV cache optimizes memory usage for self-attention operations.

## Foundational Learning

1. **Time-to-First-Spike (TTFS) encoding** - A neuromorphic computing paradigm where information is encoded in spike timing rather than amplitude; needed for energy-efficient temporal processing in spiking neural networks.

2. **Optoelectronic synapses** - Hybrid optical-electronic devices that can perform both optical sensing and electronic processing; required for integrating photonic input with electronic computation.

3. **QNN-to-SNN conversion** - Algorithm for mapping quantized neural networks to spiking neural networks; essential for preserving accuracy when transitioning from conventional to spiking architectures.

4. **In2O3 material properties** - Indium oxide semiconductor characteristics including photoconductivity and temporal decay; critical for understanding device behavior and performance characteristics.

5. **Self-attention mechanism** - Transformer architecture component for modeling relationships between elements in sequence; fundamental building block that must be adapted for spiking implementation.

6. **Hardware-aware training** - Training methodology that incorporates device non-idealities and constraints; necessary for achieving robust performance in real-world hardware implementations.

## Architecture Onboarding

**Component Map**: Optical input → In2O3 optoelectronic synapses → Spiking neurons → 1-bit KV cache → Self-attention module → Output decoding

**Critical Path**: Input signal → Photonic encoding → Device temporal decay → Spike generation → Attention computation → Classification output

**Design Tradeoffs**: Analog computation vs. precision (natural decay provides efficiency but may introduce variability), spiking vs. continuous signals (energy efficiency vs. implementation complexity), custom hardware vs. flexibility (optimized performance vs. limited adaptability).

**Failure Signatures**: Device variability leading to inconsistent spike timing, accumulated quantization errors during QNN-to-SNN conversion, insufficient temporal resolution causing information loss, hardware non-idealities affecting training convergence.

**First Experiments**:
1. Characterize temporal decay characteristics of In2O3 optoelectronic synapses under varying light intensities
2. Validate QNN-to-SNN conversion accuracy across different quantization levels and model architectures
3. Benchmark energy consumption of attention computation comparing spiking vs. conventional implementations

## Open Questions the Paper Calls Out

None

## Limitations

- Energy efficiency claims assume idealized hardware conditions without full system-level overhead accounting
- QNN-to-SNN conversion effectiveness limited to GLUE benchmark without broader architectural validation
- Hardware-aware training demonstrated only with specific In2O3 optoelectronic technology platform

## Confidence

- Hardware energy efficiency claims: Medium confidence
- QNN-to-SNN conversion effectiveness: Medium confidence
- Hardware-aware training robustness: Medium confidence

## Next Checks

1. Implement complete system-level power analysis including ADCs, interconnects, and control logic to verify claimed energy efficiency gains
2. Test QNN-to-SNN conversion pipeline across diverse transformer architectures beyond GLUE benchmark
3. Evaluate approach with alternative optoelectronic materials and devices to establish technology independence