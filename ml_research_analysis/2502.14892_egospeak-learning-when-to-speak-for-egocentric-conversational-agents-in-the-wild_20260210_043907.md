---
ver: rpa2
title: 'EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the
  Wild'
arxiv_id: '2502.14892'
source_url: https://arxiv.org/abs/2502.14892
tags:
- speech
- video
- egospeak
- transformer
- easycom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EgoSpeak predicts when to initiate speech for conversational agents
  using egocentric streaming video. It processes real-time first-person video and
  audio to determine when the camera wearer should speak, enabling natural turn-taking.
---

# EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild

## Quick Facts
- **arXiv ID:** 2502.14892
- **Source URL:** https://arxiv.org/abs/2502.14892
- **Authors:** Junhyeok Kim; Min Soo Kim; Jiwan Chung; Jungbin Cho; Jisoo Kim; Sungwoong Kim; Gyeongbo Sim; Youngjae Yu
- **Reference count:** 26
- **Primary Result:** Multimodal egocentric turn-taking prediction achieving 58.7-69.0% mean average precision

## Executive Summary
EgoSpeak addresses the challenge of determining when conversational agents should speak during real-time egocentric interactions. The system processes streaming first-person video and audio to predict turn-taking opportunities, enabling more natural conversational flow. By leveraging multimodal inputs including RGB video, audio, and optical flow, the model achieves real-time performance exceeding 99 FPS while outperforming baseline approaches on established egocentric conversation datasets.

## Method Summary
EgoSpeak employs a multimodal architecture that fuses visual, auditory, and motion information from egocentric streams to predict speaking opportunities. The system processes continuous untrimmed video in real-time, using optical flow to capture motion dynamics and pretraining on YouTube conversations to improve generalization. The model outputs predictions at each time step indicating whether the camera wearer should initiate speech, supporting natural turn-taking in conversational agents.

## Key Results
- Achieves 58.7-69.0% mean average precision on EasyCom and Ego4D datasets
- Optical flow integration provides performance improvements over visual-only approaches
- YouTube pretraining yields modest gains in prediction accuracy
- Real-time processing capability exceeds 99 FPS for continuous video streams
- Outperforms random and silence-based baseline methods

## Why This Works (Mechanism)
The effectiveness stems from multimodal fusion that captures both the visual context of interactions and the temporal dynamics of conversational flow. By processing egocentric video streams in real-time, the system can detect subtle cues that indicate turn-taking opportunities, such as changes in speaker attention, body language, and conversational rhythm. The incorporation of optical flow helps capture motion patterns that signal conversational transitions, while pretraining on general conversational data provides useful priors for egocentric scenarios.

## Foundational Learning
1. **Egocentric Computer Vision** - Understanding first-person visual perspectives and their unique challenges; needed for modeling real-world conversational dynamics from the camera wearer's viewpoint
2. **Multimodal Fusion Techniques** - Combining visual, auditory, and motion information; required to capture the full context of conversational interactions
3. **Temporal Modeling in Continuous Streams** - Processing untrimmed video sequences; essential for real-time turn-taking prediction
4. **Optical Flow Computation** - Estimating motion between video frames; provides crucial information about conversational dynamics and body movements
5. **Conversational Turn-Taking Theory** - Understanding when and how people naturally take turns in conversation; provides the theoretical foundation for prediction targets

## Architecture Onboarding

**Component Map:**
RGB Video Stream -> Visual Encoder -> Feature Fusion -> Prediction Head
Audio Stream -> Audio Encoder -> Feature Fusion -> Prediction Head
Optical Flow Stream -> Motion Encoder -> Feature Fusion -> Prediction Head

**Critical Path:**
Real-time video input → Optical flow computation → Multimodal feature extraction → Temporal modeling → Turn-taking prediction

**Design Tradeoffs:**
- Modality selection (RGB+Audio+Optical Flow vs. subsets)
- Real-time processing speed vs. prediction accuracy
- Pretraining strategy (YouTube conversations vs. other sources)
- Temporal resolution of predictions

**Failure Signatures:**
- Poor performance in low-light or occluded conditions
- Degraded accuracy when audio quality is compromised
- Difficulty with cross-cultural conversational norms
- Challenges with rapid conversational transitions

**First Experiments to Run:**
1. Ablation study removing optical flow to quantify its contribution
2. Cross-dataset evaluation to test generalization
3. Modality dropout experiments to assess robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to US and Saudi Arabian conversational contexts, limiting cultural generalizability
- Performance metrics (58.7-69.0% mAP) indicate substantial room for improvement
- Does not thoroughly examine failure modes under challenging environmental conditions
- Lacks qualitative human evaluation of conversational naturalness

## Confidence
- **High Confidence**: Technical implementation details and real-time processing capability (>99 FPS) are well-documented
- **Medium Confidence**: Performance improvements from optical flow and pretraining are supported by experiments
- **Medium Confidence**: Claims of enabling natural turn-taking are supported quantitatively but lack qualitative human evaluation

## Next Checks
1. Conduct cross-cultural validation on diverse conversational datasets to assess generalizability beyond current geographic scope
2. Perform robustness testing under degraded conditions (low light, audio noise, occlusions) to evaluate real-world deployment readiness
3. Implement user studies with human conversational partners to qualitatively assess conversational naturalness