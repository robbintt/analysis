---
ver: rpa2
title: Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models
arxiv_id: '2510.26732'
source_url: https://arxiv.org/abs/2510.26732
tags:
- reasoning
- evaluation
- across
- performance
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study establishes an infrastructure-agnostic benchmark for
  evaluating reasoning capabilities across three computational paradigms: HPC supercomputing
  (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (8
  H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic
  domains using a dual-metric framework measuring final-answer accuracy and step-by-step
  reasoning quality.'
---

# Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models

## Quick Facts
- **arXiv ID:** 2510.26732
- **Source URL:** https://arxiv.org/abs/2510.26732
- **Reference count:** 10
- **Primary result:** Hermes-4-70B (70B) achieves highest accuracy (0.598) among extended models, outperforming its 405B counterpart (0.573), establishing parameter efficiency paradox.

## Executive Summary
This study establishes an infrastructure-agnostic benchmark for evaluating reasoning capabilities across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (8 H200 GPUs). We evaluate 15 foundation models across 79 problems spanning eight academic domains using a dual-metric framework measuring final-answer accuracy and step-by-step reasoning quality.

Key findings reveal that Hermes-4-70B (70B parameters) achieves the highest overall accuracy (0.598) among extended models, outperforming its 405B counterpart (0.573) and Meta's LLaMA 3.1-405B (0.560), establishing a parameter efficiency paradox where training data quality matters more than model size. The study identifies a fundamental transparency-correctness trade-off: DeepSeek-R1 achieves record step-accuracy (0.716) but only moderate final scores (0.457, r=0.249 correlation), while Qwen3 models exhibit near-zero correlation (r=0.095), suggesting "shortcut learning" that bypasses explicit reasoning chains.

## Method Summary
The study evaluates 15 foundation models across 79 problems in 8 academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, Optimization) with difficulty stratification (25 easy, 36 medium, 18 hard). Using vLLM 0.5.0+ with temperature=0.2, max_tokens=300, FP16/BF16 precision, and 3 runs per problem, the evaluation computes final-score and step-accuracy using all-MiniLM-L6-v2 SentenceTransformer cosine similarity. The dual-metric framework measures both final answer correctness and intermediate reasoning quality, validated across MareNostrum 5 H100, university cluster 8×H200, and Nebius AI Studio cloud platforms.

## Key Results
- Hermes-4-70B (70B) achieves highest accuracy (0.598) among extended models, outperforming its 405B counterpart (0.573) and LLaMA 3.1-405B (0.560)
- DeepSeek-R1 achieves record step-accuracy (0.716) but only moderate final scores (0.457) with weak correlation (r=0.249)
- Infrastructure validation confirms reasoning quality is model-intrinsic with <3% variance across HPC, Cloud, and University clusters
- Dense Phi-4-mini (14B, 0.674) dramatically outperforms sparse Phi-3.5-MoE (42B, 0.569)

## Why This Works (Mechanism)

### Mechanism 1: Parameter Efficiency Paradox (Data Density > Scale)
- **Claim:** Smaller, densely-trained models (70B parameters) can outperform significantly larger sparse models (405B parameters) in reasoning tasks, provided training data is highly curated.
- **Mechanism:** Hermes-4-70B (70B) achieves higher accuracy (0.598) than Hermes-4-405B (0.573) and LLaMA 3.1-405B (0.560). This suggests that reasoning capability saturates if additional parameters aren't matched with proportional increases in high-quality reasoning data.
- **Evidence:** Abstract states "Hermes-4-70B... outperforming its 405B counterpart... establishing a parameter efficiency paradox where training data quality matters more than model size."

### Mechanism 2: Transparency-Correctness Trade-off (System 1 vs. System 2)
- **Claim:** High fidelity in step-by-step reasoning (transparency) does not guarantee final answer correctness, and vice versa; models exhibit a dichotomy between "deliberate" reasoning and "heuristic" shortcuts.
- **Mechanism:** DeepSeek-R1 exhibits "System 2" behavior: high step-accuracy (0.716) but moderate final scores (0.457) with weak correlation (r=0.249). Conversely, Qwen3 shows "System 1" characteristics: high consistency (0.013 std dev) but near-zero correlation (r=0.095).
- **Evidence:** Abstract states "DeepSeek-R1 achieves record step-accuracy... but only moderate final scores... Qwen3 models exhibit near-zero correlation... suggesting 'shortcut learning'."

### Mechanism 3: Infrastructure-Agnostic Invariance
- **Claim:** Reasoning performance is an intrinsic property of the model weights and serving configuration, not the underlying hardware, showing <3% variance across HPC, Cloud, and University clusters.
- **Mechanism:** By running identical benchmarks on MareNostrum 5 (supercomputer), Nebius (cloud), and local University Cluster (8x H200s), the study confirms minimal variance (e.g., LLaMA-3.1-8B: -2.9%).
- **Evidence:** Abstract states "Infrastructure validation confirms reasoning quality is model-intrinsic with <3% variance across platforms."

## Foundational Learning

- **Concept: Semantic Similarity Scoring (Cosine Similarity)**
  - **Why needed here:** The study evaluates "reasoning" not by binary correctness, but by the distance between the model's output and a reference solution in vector space.
  - **Quick check question:** How would the score differ if the model produces a logically correct answer that uses different terminology than the reference solution?

- **Concept: Mixture-of-Experts (MoE) Active Parameters**
  - **Why needed here:** The study compares "dense" models (all parameters active) against "sparse" MoE models (only a subset active).
  - **Quick check question:** Why might a 42B MoE model underperform a 14B dense model on reasoning tasks despite having a higher total parameter count?

- **Concept: Correlation Coefficient (r-value)**
  - **Why needed here:** The paper uses correlation (r=0.249 vs r=0.095) to prove that good "steps" don't always lead to good "answers."
  - **Quick check question:** If the correlation between step-accuracy and final score is 0.0, what does that imply about the model's internal process?

## Architecture Onboarding

- **Component map:** 79-problem benchmark (8 domains) → 15 Foundation Models → 3 Infrastructures (MareNostrum, Nebius, Cluster) → all-MiniLM-L6-v2 SentenceTransformer → Final Score + Step Accuracy + Consistency

- **Critical path:**
  1. Infrastructure Baseline: Verify infrastructure invariance by running the 19-problem set on two different compute nodes
  2. Model Selection: Select one "Transparent" model (DeepSeek-R1) and one "Efficient" model (Hermes-4-70B) to test the trade-off hypothesis
  3. Domain Stress Test: Run the Optimization domain (lowest scores in study) specifically to gauge failure modes

- **Design tradeoffs:**
  - Metrics: The study trades *interpretability of logic* (Step Accuracy) for *outcome reliability* (Final Score)
  - Architectures: Dense models (Phi-4) offer better stability (0.032 std dev) vs. MoE models which may have "expert routing" instability

- **Failure signatures:**
  - DeepSeek Paradox: High verbosity/step-accuracy (0.716) but wrong final answer (0.457)
  - Shortcut Learning (Qwen3): Very consistent answers (0.013 variance) with low step-correlation
  - Optimization Domain Failure: Universal failure across all models (avg ~0.41)

- **First 3 experiments:**
  1. Reproducibility Check: Deploy LLaMA-3.1-8B on local GPU vs. cloud API endpoint using 19-problem set
  2. Efficiency Audit: Compare Hermes-4-70B vs. Hermes-4-405B on 10 "Hard" Calculus problems
  3. Transparency Audit: Inspect DeepSeek-R1 outputs on problems where Step Accuracy was high but Final Score was low

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or training modifications could enable models to dynamically switch between transparent "System 2" deliberative reasoning and efficient "System 1" heuristic shortcuts based on problem complexity?
- **Basis:** The authors state: "Designing AI systems that appropriately invoke each mode—using fast heuristics when sufficient, escalating to careful reasoning when necessary—remains an open challenge."
- **Why unresolved:** Current models exhibit either high transparency with moderate accuracy (DeepSeek-R1) or high accuracy with opaque reasoning (Qwen3), but none adaptively balance both modes.

### Open Question 2
- **Question:** What specific training data characteristics or supervision signals enable smaller dense models (e.g., Hermes-4-70B, Phi-4-mini) to outperform larger sparse architectures on reasoning tasks?
- **Basis:** The conclusion states: "Reasoning capability has entered a data-limited rather than parameter-limited regime. Future progress may therefore depend more on reasoning-centric data and supervision signals than on raw scale expansion."
- **Why unresolved:** The paper identifies the parameter efficiency paradox but doesn't isolate which data quality factors drive the observed gains.

### Open Question 3
- **Question:** How can hybrid architectures combining foundation models with symbolic solvers achieve both the flexibility of neural reasoning and the correctness guarantees of formal methods?
- **Basis:** The conclusion explicitly identifies this as future work: "investigate hybrid architectures combining LLMs with symbolic solvers."
- **Why unresolved:** The paper demonstrates that neural models struggle with domains requiring formal precision (Optimization: mean 0.408), but pure symbolic systems lack the generalization capabilities shown by LLMs across domains.

## Limitations

- The 79-problem benchmark, while diverse, may not be representative of all reasoning scenarios, particularly in Optimization where the study suggests LLMs struggle with high-dimensional constraint satisfaction but doesn't quantify this gap against specialized solvers.
- The semantic similarity metric using cosine similarity of text embeddings may not fully capture reasoning quality - logically valid but differently phrased solutions could be penalized.
- The study doesn't control for training data composition when comparing 70B vs 405B models, making it unclear whether the efficiency gain is due to data quality or architectural differences.

## Confidence

- **High Confidence**: Infrastructure-invariance finding (<3% variance across platforms) - Directly supported by controlled experiment across three distinct computational environments with quantitative variance measurements.
- **Medium Confidence**: Parameter efficiency paradox (70B outperforming 405B) - Raw accuracy comparison is clear, but mechanism (data quality vs. model size) is inferred rather than directly measured.
- **Low Confidence**: Transparency-correctness trade-off mechanism - Correlation analysis provides statistical evidence, but interpretation of "shortcut learning" vs. "explicit reasoning" relies on qualitative analysis not systematically validated.

## Next Checks

1. **Correlation validation**: Manually inspect 20 model outputs where step-accuracy and final-score show high discrepancy (top 10 and bottom 10 r-values) to verify whether "shortcut learning" interpretation matches qualitative reasoning patterns.

2. **Domain generalization test**: Run the same 79-problem benchmark on the top 3 models (Hermes-4-70B, DeepSeek-R1, Phi-4-mini) against an independently curated problem set from a different source to test benchmark-specific overfitting.

3. **Step extraction reproducibility**: Implement and test the exact step-segmentation methodology on sample outputs to ensure the step-accuracy metric is reproducible and not dependent on ambiguous parsing decisions.