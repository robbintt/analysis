---
ver: rpa2
title: 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation'
arxiv_id: '2502.03860'
source_url: https://arxiv.org/abs/2502.03860
tags:
- longcot
- bolt
- arxiv
- reasoning
- thoughts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BOLT, a method to develop long chain-of-thought
  (LongCoT) reasoning in language models without relying on distillation from existing
  LongCoT models or extensive human annotations. BOLT consists of three stages: LongCoT
  data bootstrapping via in-context learning with minimal examples (10 in experiments),
  LongCoT supervised finetuning, and online training via direct preference optimization.'
---

# BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation

## Quick Facts
- arXiv ID: 2502.03860
- Source URL: https://arxiv.org/abs/2502.03860
- Reference count: 40
- Primary result: Bootstraps LongCoT reasoning without distillation, achieving state-of-the-art performance across multiple benchmarks

## Executive Summary
BOLT introduces a three-stage method to develop Long Chain-of-Thought (LongCoT) reasoning in language models without relying on distillation from existing LongCoT models or extensive human annotations. The approach uses in-context learning with minimal examples (10 in experiments) to bootstrap LongCoT data, followed by supervised finetuning and online direct preference optimization. BOLT is applied to models ranging from 7B to 70B parameters, demonstrating significant performance improvements across diverse reasoning benchmarks including Arena-Hard, WildBench, ZebraLogic, and MATH500.

## Method Summary
BOLT consists of three stages: (1) LongCoT data bootstrapping via in-context learning with minimal examples (10 ICL examples, 8 samples per query) using a strong model (Llama-3.1-70B-Instruct), (2) supervised finetuning for 4 epochs on the bootstrapped data, and (3) online direct preference optimization for 3 iterations. The method generates ~220k LongCoT examples by filtering responses using a reward model (ORM), then trains target models via SFT followed by DPO with preference pairs constructed from the highest and lowest-scored responses. The approach achieves format compliance through explicit constraints and rewards during training.

## Key Results
- Achieves 44.1% performance on Arena-Hard benchmark
- Scores 42.96% on WildBench
- Demonstrates strong cell-level accuracy on ZebraLogic
- Shows significant improvements on MATH500 benchmark

## Why This Works (Mechanism)
The method works by leveraging the reasoning capabilities of a strong model (Llama-3.1-70B-Instruct) to generate high-quality LongCoT examples through in-context learning, then progressively refining these examples through supervised finetuning and preference optimization. The approach avoids the need for expensive human annotations or distillation from specialized LongCoT models by using automated filtering and reward-based selection to maintain quality while scaling up the dataset.

## Foundational Learning
- **In-Context Learning (ICL)**: Few-shot prompting technique where examples are provided in the prompt to guide model behavior without parameter updates. Critical for bootstrapping initial LongCoT data without training.
- **Direct Preference Optimization (DPO)**: Reward-based fine-tuning method that optimizes for human preferences by comparing response pairs. Enables online refinement of LongCoT quality.
- **Reward Modeling**: Automated scoring system (ORM) used to filter and rank generated responses. Essential for scaling up data generation without human annotation.
- **Format Compliance**: Explicit constraints on output structure (internal thoughts vs external solutions). Ensures generated responses follow the LongCoT format consistently.
- **Online Training**: Iterative refinement where new data is continuously generated and filtered. Allows progressive improvement of reasoning quality.

## Architecture Onboarding
**Component Map**: ICL (10 examples) → Bootstrapping Model (70B) → ORM Filtering → SFT (4 epochs) → Online DPO (3 iterations × 2 epochs)

**Critical Path**: The bootstrapping stage is critical - poor quality ICL examples or weak bootstrapping model will propagate errors through SFT and DPO stages. The ORM filtering step is also crucial for maintaining data quality.

**Design Tradeoffs**: Uses minimal ICL examples (10) vs larger sets for efficiency, automated filtering vs human annotation for scalability, and online DPO vs offline training for progressive refinement.

**Failure Signatures**: Short/incoherent thoughts in SFT model indicates bootstrapping issues; noisy reward signals in DPO suggest ORM problems; format violations indicate insufficient constraint enforcement.

**First Experiments**:
1. Test bootstrapping with 8B vs 70B model to validate scale independence claim
2. Compare ArmoRM vs alternative reward models for filtering quality
3. Validate format compliance impact by testing with/without explicit format rewards

## Open Questions the Paper Calls Out
None

## Limitations
- Critical dependence on quality of 10 ICL examples for bootstrapping
- Unspecified ORM for initial SFT-stage filtering (only DPO ORM confirmed)
- Lack of implementation details for rule-based format reward function
- Unclear implementation of difficulty scoring criteria

## Confidence
**High Confidence**: Three-stage pipeline structure and use of Llama-3.1-70B-Instruct for generation; documented performance improvements across benchmarks

**Medium Confidence**: Hyperparameter choices (learning rates, batch sizes, epochs) and their relative importance

**Low Confidence**: Exact implementation of 10 ICL examples, SFT-stage ORM, and rule-based format reward function

## Next Checks
1. Validate bootstrapping quality by testing weaker bootstrapping model (8B vs 70B) and comparing format compliance and reasoning action counts

2. Validate reward model choice by running ablation comparing ArmoRM vs ArceeAI-Reward-v1.0 vs GPT-4V for both SFT filtering and DPO scoring

3. Validate format compliance impact by testing performance with and without explicit format constraints during online training