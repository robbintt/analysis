---
ver: rpa2
title: Non-Linear Counterfactual Aggregate Optimization
arxiv_id: '2509.03438'
source_url: https://arxiv.org/abs/2509.03438
tags:
- policy
- outcome
- learning
- policies
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method for optimizing non-linear functions
  of aggregated outcomes in offline contextual bandits, addressing the limitations
  of traditional expected reward maximization. The core idea is to leverage concentration
  properties of sums of individual outcomes and use a Gaussian approximation via the
  Central Limit Theorem to derive a scalable descent algorithm that directly optimizes
  the stated non-linear objective.
---

# Non-Linear Counterfactual Aggregate Optimization

## Quick Facts
- **arXiv ID:** 2509.03438
- **Source URL:** https://arxiv.org/abs/2509.03438
- **Reference count:** 14
- **Primary result:** Introduces a method to optimize non-linear functions of aggregated outcomes in offline contextual bandits using Gaussian approximation via CLT, showing improved robustness over standard IPS.

## Executive Summary
This paper addresses the challenge of optimizing non-linear criteria in offline contextual bandits, where traditional methods like IPS focus on expected reward and can produce overconfident, unreliable policies. The authors propose a novel approach that leverages the concentration properties of sums of individual outcomes, using a Gaussian approximation via the Central Limit Theorem to derive a scalable descent algorithm. This allows direct optimization of non-differentiable or complex objectives, such as maximizing the probability of exceeding a performance threshold, which is more appropriate for applications like A/B testing. Experiments on synthetic data demonstrate that the method yields robust policies with high average rewards and better median outcomes compared to standard IPS and Logarithmic Smoothing baselines.

## Method Summary
The method optimizes non-linear functions of aggregated outcomes in offline contextual bandits by approximating the distribution of the total aggregated outcome as a Gaussian. It estimates the empirical mean and variance of IPS-weighted outcomes and optimizes the expectation of the criterion with respect to the policy parameters. This approach enables gradient-based optimization of non-differentiable objectives (like threshold indicators) by smoothing them into differentiable landscapes. The algorithm uses a reparameterization trick to enable scalable optimization via stochastic gradient descent, directly targeting criteria like the probability of improvement rather than expected reward.

## Key Results
- Directly optimizing probability of improvement yields robust policies with high average rewards and better median outcomes compared to IPS and Logarithmic Smoothing baselines.
- IPS often produces unreliable, overconfident policies that collapse to single actions with high but unreliable reward estimates.
- The method naturally constrains variance through risk-averse criteria, preventing the extreme importance weighting issues common in standard IPS.

## Why This Works (Mechanism)

### Mechanism 1: Gaussian Surrogate for Aggregate Sums
The method approximates the distribution of the total aggregated outcome as a Gaussian using the Central Limit Theorem, enabling optimization of non-differentiable criteria via gradient descent. This smooths discontinuous objectives (like step functions for A/B test success) into differentiable landscapes. The approach requires large sample sizes and finite variance of importance weights.

### Mechanism 2: Implicit Variance Penalization via Risk Aversion
Optimizing non-linear criteria (like threshold functions) inherently constrains variance by forcing the policy to maximize the mean while minimizing variance to squeeze the distribution past the threshold. This induces hedging behavior naturally, without explicit regularization terms.

### Mechanism 3: Stochastic Gradient Descent on Reparameterized Expectations
The method enables scalable optimization by deriving analytical gradient estimators that rely on sampling from the Gaussian surrogate rather than the environment. The gradient update decomposes into components driven by gradients of the mean and variance, allowing standard autograd systems to optimize policy parameters efficiently.

## Foundational Learning

- **Concept: Central Limit Theorem (CLT) & Aggregation**
  - Why needed: The entire method rests on the assumption that the sum of rewards behaves like a Gaussian distribution.
  - Quick check: Does the sum of binary rewards from 1000 users necessarily form a Gaussian, or are there heavy-tail distributions where this fails?

- **Concept: Inverse Propensity Scoring (IPS)**
  - Why needed: IPS is used to estimate the mean and variance from off-policy data; understanding its high variance properties is key to understanding what the paper fixes.
  - Quick check: If the logging policy has zero probability for an action that the new policy wants to take, what happens to the IPS weight and the resulting variance estimate?

- **Concept: Policy Gradient / Score Function Estimator**
  - Why needed: Algorithm 1 requires computing gradients of mean and variance with respect to policy parameters.
  - Quick check: In the gradient update rule, how does the term $(h_ℓ - μ_k)$ modulate the update strength for the mean component?

## Architecture Onboarding

- **Component map:** Data Loader -> Statistics Estimator -> Monte Carlo Sampler -> Objective Evaluator -> Optimizer
- **Critical path:** The accuracy of the Statistics Estimator is the bottleneck; if IPS variance is unbounded due to support mismatch, the Gaussian approximation degrades.
- **Design tradeoffs:** Trades mathematical exactness for tractability; optimizes a non-linear proxy that may introduce bias but controls variance and aligns better with business goals.
- **Failure signatures:** Over-confident collapse (entropy → 0), divergence (high learning rate or impossible thresholds), violated CLT assumptions (small sample sizes).
- **First 3 experiments:**
  1. Implement the "Multi-armed bandit with K=1000" setup and verify the learned policy shifts mean and reduces variance compared to π₀.
  2. Log entropy of π_θ over time to verify it doesn't collapse to near-zero unlike IPS.
  3. Stress-test with different improvement thresholds (10%, 30%, 50%) and measure probability of improvement on validation set.

## Open Questions the Paper Calls Out

- Under what specific theoretical conditions does the proposed algorithm guarantee convergence and optimization quality?
- Can the method be effectively extended to handle multidimensional outcomes, such as enforcing budget constraints?
- How robust is the Gaussian approximation when the objective function encourages high variance or extreme importance weights?

## Limitations

- Relies heavily on CLT approximation which requires large sample sizes and finite variance of importance weights.
- Performance on real-world, high-dimensional, multi-context bandit problems is uncertain.
- Sensitivity to hyperparameters (learning rate, number of samples) is not thoroughly explored.

## Confidence

- **High Confidence:** The core mechanism of using Gaussian surrogate for non-differentiable objectives is sound and improves upon IPS in reducing overconfidence.
- **Medium Confidence:** Theoretical justification for CLT approximation and its robustness to finite sample sizes needs more rigorous analysis.
- **Low Confidence:** Scalability and performance on complex, real-world datasets with high-dimensional contexts are uncertain.

## Next Checks

1. Systematically vary sample size N and measure accuracy of Gaussian approximation and resulting policy performance to quantify when CLT assumption is valid.
2. Track maximum and average importance weights during optimization while varying divergence between π_θ and π₀ to identify safe operating region.
3. Apply the method to a publicly available offline contextual bandit dataset and compare performance against IPS and Logarithmic Smoothing baselines using relevant non-linear criteria.