---
ver: rpa2
title: 'Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation
  for OffensiveLanguage Identification'
arxiv_id: '2508.11166'
source_url: https://arxiv.org/abs/2508.11166
tags:
- tulu
- offensive
- language
- dataset
- code-mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces the first benchmark dataset for Offensive
  Language Identification (OLI) in code-mixed Tulu, a low-resource Dravidian language.
  The dataset comprises 3,845 YouTube comments annotated with high inter-annotator
  agreement (Krippendorff''s alpha = 0.984) across four categories: Not Offensive,
  Not Tulu, Offensive Untargeted, and Offensive Targeted.'
---

# Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification

## Quick Facts
- arXiv ID: 2508.11166
- Source URL: https://arxiv.org/abs/2508.11166
- Reference count: 8
- Introduces first benchmark dataset for Offensive Language Identification in code-mixed Tulu

## Executive Summary
This study presents the first benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu, a low-resource Dravidian language. The dataset comprises 3,845 YouTube comments annotated across four categories: Not Offensive, Not Tulu, Offensive Untargeted, and Offensive Targeted. A suite of deep learning models, including RNN variants and transformers, were evaluated. The BiGRU model with self-attention achieved the best performance with 82% accuracy and a 0.81 macro F1-score, while transformer models underperformed due to limited Tulu representation in pretraining corpora.

## Method Summary
The study created a corpus of 3,845 YouTube comments in code-mixed Tulu, annotated with high inter-annotator agreement (Krippendorff's alpha = 0.984). Models were trained on 2,692 comments with 577 for validation and 576 for testing. Text preprocessing included removal of mentions, hashtags, punctuation, digits, and English stop words. The best-performing BiGRU+SA model used randomly initialized 100-dimensional embeddings, a bidirectional GRU encoder (128 units), and a self-attention mechanism. Models were trained using Adam optimizer (lr=0.001) with early stopping on validation loss.

## Key Results
- BiGRU model with self-attention achieved 82% accuracy and 0.81 macro F1-score
- Transformer models (mBERT, XLM-RoBERTa) underperformed with 68% and 58% accuracy respectively
- High inter-annotator agreement (Krippendorff's alpha = 0.984) validates annotation quality
- Model effectively classified all four categories, including minority offensive classes

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional recurrent architectures with self-attention improve offensive language detection in code-mixed, low-resource text
- BiGRU captures dependencies in both directions while self-attention emphasizes informative tokens and suppresses noise
- Core assumption: Offensive patterns are position-independent and attention can isolate them better than unidirectional models
- Evidence: BiGRU+SA achieved 82% accuracy; attention-based models showed better minority class performance than simple RNNs

### Mechanism 2
- Multilingual transformers underperform on low-resource code-mixed languages due to representation gaps
- mBERT and XLM-RoBERTa lack Tulu-specific subword units and syntactic patterns in pretraining
- Core assumption: Domain gap between high-resource pretraining data and code-mixed Tulu prevents effective transfer
- Evidence: Transformers achieved 68%/58% accuracy vs 82% for BiGRU+SA; XLM-Roberta returned zero F1-scores for offensive categories

### Mechanism 3
- Attention mechanisms mitigate class imbalance effects by focusing learning on minority-class tokens
- Self-attention dynamically weights tokens associated with underrepresented offensive classes
- Core assumption: Minority-class instances contain distinguishable tokens that attention can prioritize
- Evidence: Attention-based models achieved F1-scores above 0.75 for minority categories vs 0.00 for simple RNNs

## Foundational Learning

- **Bidirectional RNNs (BiGRU/BiLSTM)**: Capture non-local dependencies in code-mixed text where offensive cues may appear before or after targets. *Quick check: Can you explain why a unidirectional GRU might miss a contextual cue that appears after an offensive term?*

- **Self-Attention Mechanism**: Learns which tokens matter for classification in noisy code-mixed text with spelling variations. *Quick check: How does self-attention differ from pooling layers in terms of what information it preserves?*

- **Macro F1-Score for Imbalanced Data**: Averages per-class performance equally, preventing majority class dominance from masking poor minority class performance. *Quick check: Why would a model with 80% accuracy still be considered poor if all correct predictions are on the majority class?*

## Architecture Onboarding

- **Component map**: Tokenized sequences → Embedding (100d) → BiGRU (128 units) → Self-Attention (256) → Dense (Softmax, 4 classes)

- **Critical path**: 1) Preprocess text (remove mentions, hashtags, punctuation, digits, stopwords) 2) Tokenize and pad sequences 3) Initialize embeddings randomly 4) Train BiGRU+SA with early stopping 5) Evaluate using macro F1

- **Design tradeoffs**: BiGRU+SA outperforms transformers here but may reverse with Tulu pretraining data; random embeddings necessary but limit semantic generalization; attention size 256 balances nuance capture vs overfitting risk

- **Failure signatures**: Model predicts only majority class → check class balance; validation loss plateaus → reduce complexity or increase dropout; transformer returns zero F1 on minority classes → pretraining gap

- **First 3 experiments**: 1) Baseline: Train SimpleRNN and GRU to establish performance floor 2) Architecture comparison: Train BiGRU, BiLSTM, CNN, and attention variants; compare macro F1 3) Transformer benchmark: Fine-tune mBERT and XLM-RoBERTa; confirm underperformance and analyze per-class failure modes

## Open Questions the Paper Calls Out

1. Can newer architectures like T5 and GPT-3, or cross-lingual transfer learning techniques, overcome the performance deficit observed with mBERT and XLM-RoBERTa in Tulu?

2. Does incorporating explicit linguistic features like morphological analysis or sentiment cues improve classification accuracy beyond current neural models?

3. To what extent would creating or adapting pre-trained Tulu-specific word embeddings improve performance over randomly initialized vectors?

## Limitations

- Model generalizability to other code-mixed Tulu text sources or other low-resource Dravidian languages remains untested
- Random initialization of embeddings due to unavailability of Tulu-specific pretrained embeddings may limit semantic understanding
- Class imbalance handling effectiveness of attention mechanisms was not compared against explicit techniques like oversampling or class weights

## Confidence

- **High Confidence**: BiGRU+SA outperforms other tested architectures on TuluOffense dataset with clear performance gap
- **Medium Confidence**: Explanation for transformer underperformance due to lack of Tulu in pretraining data
- **Medium Confidence**: General mechanism by which attention helps with class imbalance without direct comparison to explicit techniques

## Next Checks

1. Evaluate BiGRU+SA model on other code-mixed datasets (Nepali-English, Telugu-English) to test generalizability to other low-resource, code-mixed languages

2. Train BiGRU+SA model using related language's pretrained embeddings (Kannada FastText) to quantify impact of embedding quality on performance

3. Implement QLoRA or adapter-based method to fine-tune mBERT/XLM-RoBERTa and compare performance to BiGRU+SA, testing if parameter-efficient fine-tuning can mitigate pretraining gap