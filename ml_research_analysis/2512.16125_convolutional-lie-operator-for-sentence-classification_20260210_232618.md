---
ver: rpa2
title: Convolutional Lie Operator for Sentence Classification
arxiv_id: '2512.16125'
source_url: https://arxiv.org/abs/2512.16125
tags:
- sentence
- convolutional
- classification
- representations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Convolutional Lie Operator (CLie) models
  for sentence classification by incorporating Lie group convolutions into standard
  convolutional architectures. The authors propose two models, SCLie and DPCLie, which
  replace conventional convolutional layers with Lie convolutions to capture non-Euclidean
  symmetries and transformations in language data.
---

# Convolutional Lie Operator for Sentence Classification

## Quick Facts
- arXiv ID: 2512.16125
- Source URL: https://arxiv.org/abs/2512.16125
- Reference count: 5
- Key outcome: CLie models achieve 0.851 vs 0.821 accuracy on SSTb and 0.867 vs 0.856 average accuracy across tasks

## Executive Summary
This paper introduces Convolutional Lie Operator (CLie) models for sentence classification by incorporating Lie group convolutions into standard convolutional architectures. The authors propose two models, SCLie and DPCLie, which replace conventional convolutional layers with Lie convolutions to capture non-Euclidean symmetries and transformations in language data. Experiments on seven benchmark datasets show that CLie models achieve higher accuracy than traditional convolutional models, with improvements of 0.851 vs 0.821 on SSTb and 0.867 vs 0.856 average accuracy across all tasks. The models also demonstrate smoother, more continuous sentence representations in t-SNE visualizations.

## Method Summary
The paper introduces Convolutional Lie Operator (CLie) models that replace standard convolutional layers with Lie group convolutions to capture non-Euclidean symmetries in sentence data. Two models are proposed: SCLie (shallow) and DPCLie (deep pyramid variant). The models use pre-trained Word2Vec embeddings from Google News (CBOW, ~100B words) with random initialization for unknown words. Experiments are conducted on six benchmark datasets (CR, MPQA, TREC, SSTb, MR, Subj) with classification accuracy as the primary metric. The authors also evaluate on a symmetry inference task using the SIS dataset, measuring Pearson and Spearman correlations.

## Key Results
- SCLie and DPCLie achieve 0.851 vs 0.821 accuracy on SSTb dataset
- Average accuracy across all tasks: 0.867 vs 0.856 for CLie vs traditional CNN
- DPCLie shows 0.26 Pearson correlation vs 0.23 for DPCNN on symmetry inference task

## Why This Works (Mechanism)
Lie group convolutions can capture non-Euclidean symmetries and transformations in language data that traditional convolutions miss. By incorporating these mathematical structures, the models can better represent complex relationships between words and phrases that exhibit rotational, translational, or other geometric invariances. This allows for more expressive sentence representations that align with the underlying geometric structure of language.

## Foundational Learning
- **Lie Groups**: Continuous symmetry groups used in mathematics; needed for capturing geometric transformations in data; quick check: verify group properties (closure, associativity, identity, inverse)
- **Group Convolutions**: Convolution operations defined over group structures; needed for applying convolutional operations to non-Euclidean spaces; quick check: confirm convolution respects group operations
- **Word Embeddings**: Dense vector representations of words; needed as input features for the models; quick check: verify embedding dimensions match model requirements
- **Sentence Classification**: Task of categorizing sentences into predefined classes; needed as the target application; quick check: confirm label distribution across datasets
- **t-SNE Visualization**: Dimensionality reduction technique for visualizing high-dimensional data; needed for qualitative assessment of learned representations; quick check: verify perplexity and iteration parameters

## Architecture Onboarding
- **Component Map**: Input Embeddings -> Word Embedding Layer -> Lie Convolutional Layers -> Pooling -> Fully Connected Layers -> Output
- **Critical Path**: The Lie convolutional layers are the critical innovation, replacing standard convolutions to capture non-Euclidean symmetries
- **Design Tradeoffs**: Lie convolutions offer more expressive power but increase computational complexity compared to standard convolutions
- **Failure Signatures**: If Lie convolutions don't improve performance, it may indicate the data lacks exploitable non-Euclidean symmetries or the Lie group structure is not well-suited to the task
- **First Experiments**: 1) Test Lie convolutions on synthetic data with known symmetries; 2) Compare t-SNE visualizations between standard and Lie convolutions; 3) Evaluate ablation studies removing Lie convolutions

## Open Questions the Paper Calls Out
None

## Limitations
- No statistical significance testing or confidence intervals reported for performance improvements
- SIS dataset appears to be introduced in this paper without external validation
- No ablation studies to isolate the impact of Lie convolutions specifically
- Word2Vec embeddings may introduce domain mismatch for some tasks

## Confidence
- High: Mathematical framework of Lie convolutions being valid
- Medium: Experimental results showing improvements
- Low: Practical significance of improvements in real-world applications

## Next Checks
1. Replicate experiments with statistical significance testing and confidence intervals on all reported metrics
2. Conduct ablation studies isolating the impact of Lie convolutions versus other architectural changes
3. Test models on additional sentence classification tasks beyond the six benchmark datasets to assess generalizability