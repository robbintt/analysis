---
ver: rpa2
title: A comparison of data filtering techniques for English-Polish LLM-based machine
  translation in the biomedical domain
arxiv_id: '2501.16533'
source_url: https://arxiv.org/abs/2501.16533
tags:
- data
- filtering
- laser
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates data filtering methods for English-Polish\
  \ machine translation in the biomedical domain. Three filtering techniques\u2014\
  LASER, MUSE, and LaBSE\u2014were applied to the UFAL Medical Corpus to create varying\
  \ dataset sizes, which were then used to fine-tune the mBART50 model."
---

# A comparison of data filtering techniques for English-Polish LLM-based machine translation in the biomedical domain

## Quick Facts
- arXiv ID: 2501.16533
- Source URL: https://arxiv.org/abs/2501.16533
- Reference count: 3
- LASER filtering consistently outperforms other methods, achieving a BLEU score of 17.411 using only 60% of the filtered data, surpassing both the baseline model trained on all data (17.402 BLEU) and random 60% subsets

## Executive Summary
This study evaluates three cross-lingual embedding methods (LASER, MUSE, and LaBSE) for filtering English-Polish parallel corpora in the biomedical domain. The researchers applied these methods to the UFAL Medical Corpus, creating subsets at 20% and 60% retention levels, then fine-tuned mBART50 on each subset. Results show that LASER filtering consistently outperforms other methods and random downsampling, achieving higher BLEU scores while using less data. The findings suggest that semantic similarity-based filtering can improve translation quality by removing low-quality or misaligned sentence pairs from training data.

## Method Summary
The researchers preprocessed the UFAL Medical Corpus to obtain 700k clean English-Polish sentence pairs, then applied three embedding methods (LASER, MUSE, LaBSE) to score each pair using cosine similarity. They retained top-scoring 20% and 60% subsets per method, then fine-tuned mBART50 for 3 epochs on each filtered dataset. The models were evaluated using SacreBLEU on the Khresmoi test set (1,500 pairs), with additional human evaluation for fluency and medical terminology accuracy.

## Key Results
- LASER-60% achieved 17.411 BLEU, surpassing both full corpus baseline (17.402 BLEU) and random 60% subsets (17.234 BLEU)
- MUSE-60% also improved over random baselines but showed significant degradation at 20% retention, particularly with medical terminology
- LaBSE underperformed despite high similarity correlation with LASER (r=0.81), suggesting correlation doesn't guarantee filtering quality

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual Semantic Similarity Filtering
Filtering parallel corpora by semantic similarity between sentence pairs can maintain or improve translation quality while reducing training data volume. Embedding models encode source and target sentences into vectors; cosine similarity scores identify misaligned or low-quality pairs; removing low-scoring pairs concentrates training on higher-quality examples, reducing gradient noise during optimization. Core assumption: Low similarity scores indicate poor translation quality rather than legitimate cross-lingual semantic variation.

### Mechanism 2: Noise Reduction Concentrates Learning Signal
Web-scraped parallel corpora contain low-quality entries that contribute minimally to model performance while consuming training capacity. Filtering removes misaligned pairs, untranslated segments, and corrupted examples; the model trains on cleaner signal, potentially improving parameter updates per epoch and reducing overfitting to noise patterns. Core assumption: The filtering method's quality criteria align with the downstream translation task requirements.

### Mechanism 3: Architecture-Specific Embedding Quality
Different embedding architectures may yield different filtering efficacy even when their raw scores correlate strongly. LASER uses BiLSTM with BPE tokenization producing fixed-size vectors; LaBSE uses BERT-based dual-encoder. Despite high correlation (r=0.81), differences in how each model handles biomedical terminology and sentence structure appear to affect which pairs are retained near threshold boundaries. Core assumption: The embedding model's training objective and architecture affect which sentence pairs score near filtering thresholds, influencing final training data composition.

## Foundational Learning

- Concept: **Parallel Corpus Filtering vs. Random Downsampling**
  - Why needed here: The paper's central finding depends on understanding that informed filtering (LASER) outperforms random selection (Base-60%), not just using less data.
  - Quick check question: Why does LASER-60% (17.411 BLEU) outperform Base-60% (17.234 BLEU) when both use the same amount of training data?

- Concept: **Multilingual Sentence Embeddings**
  - Why needed here: The three methods compared (LASER, MUSE, LaBSE) represent fundamentally different approaches to creating cross-lingual representations—understanding this is essential for method selection.
  - Quick check question: LASER uses BiLSTM architecture while LaBSE uses BERT-based dual-encoder; how might this architectural difference affect their filtering behavior on biomedical text?

- Concept: **BLEU Score Limitations**
  - Why needed here: The paper supplements BLEU with human evaluation because BLEU alone may not capture fluency or medical terminology accuracy.
  - Quick check question: In Table 2, MUSE-20% mistranslates "Meningococcal Disease" as "Choroba gruczołu krokowego" (prostate disease)—would BLEU alone detect this medical terminology error?

## Architecture Onboarding

- Component map:
  UFAL Medical Corpus → preprocessing (deduplication, length filters 15-200 chars, character validation) → 700k clean pairs → LASER/MUSE/LaBSE → cosine similarity scoring → threshold-based retention (20%/60%) → mBART50 (600M parameters) with MBart50Tokenizer, 16-bit precision, AdamW optimizer → Khresmoi test set (1,500 pairs) → SacreBLEU + human fluency assessment

- Critical path:
  1. Score all 700k sentence pairs with chosen embedding method
  2. Rank by cosine similarity; retain top percentage
  3. Fine-tune mBART50 for 3 epochs with 80/20 train/validation split
  4. Evaluate on held-out Khresmoi dataset
  5. Conduct human evaluation for fluency and terminology accuracy

- Design tradeoffs:
  - Retention threshold: 60% = near-baseline quality with 40% compute reduction; 20% = faster but -0.288 BLEU vs. full data
  - Method choice: LASER = best quality (recommended); MUSE = moderate, struggles with medical terminology at low retention; LaBSE = not recommended despite theoretical advantages
  - Training epochs: 3 epochs fixed across all experiments; may be suboptimal for smaller filtered datasets

- Failure signatures:
  - LaBSE-60% underperforms random baseline → high correlation (r=0.81) with LASER scores is insufficient predictor of filtering quality
  - MUSE-20% mistranslates medical terminology → filter appears to score short terminology-heavy sentences lower
  - Marginal BLEU gains (0.009) at 60% retention → filtering benefits may plateau; validate computational savings justify implementation complexity

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train mBART50 on Base-none, Base-all, Base-60% (random), Base-20% (random) to confirm filtering value over downsampling for your compute environment.
  2. **Validate filtering method on your domain**: Apply LASER, MUSE, LaBSE filtering at 60% retention to your target domain; compare BLEU and spot-check medical terminology accuracy.
  3. **Threshold optimization**: Test LASER at 40%, 50%, 60%, 70%, 80% retention to identify minimum viable dataset size before quality degrades below acceptable threshold for your application.

## Open Questions the Paper Calls Out

### Open Question 1
Why does LaBSE underperform compared to LASER in filtering tasks despite a high score correlation (r=0.81)?
- Basis in paper: [explicit] The authors note LaBSE "underperformed despite high correlation" and suggest "differences in scoring specific sentences might explain LASER’s superior performance."
- Why unresolved: The study observed the performance gap but did not conduct a granular analysis of the specific sentence-level scoring divergences that cause the quality difference.
- Evidence: A fine-grained error analysis comparing cases where LaBSE and LASER scores diverge significantly to identify which linguistic features LaBSE penalizes or rewards incorrectly.

### Open Question 2
Can early stopping criteria or fewer training epochs suffice for achieving optimal results with filtered data compared to the fixed 3-epoch training used in this study?
- Basis in paper: [explicit] The limitations section states future work could "vary the number of epochs and perhaps use a different stopping criterion... assessing whether fewer epochs could suffice in achieving optimal results."
- Why unresolved: All models were trained for a fixed duration (3 epochs), which obscures whether the efficiency gains from filtering could be further amplified by optimized training durations.
- Evidence: Training curves comparing validation loss dynamics between filtered and unfiltered datasets to identify if filtered data converges faster or requires different stopping points.

### Open Question 3
Would integrating a domain-specific filtering metric alongside semantic similarity improve the retention of pertinent medical terminology?
- Basis in paper: [explicit] The authors note that "no filtering method that assessed the domain specificity of the sentences was used... which led to the inclusion of sentences that might not be entirely pertinent."
- Why unresolved: Current methods rely on generic semantic similarity, which may retain sentences with high alignment but low medical relevance or filter out dense medical jargon.
- Evidence: An ablation study combining LASER filtering with a domain classifier to ensure medical relevance, followed by evaluation on terminology-heavy test sets.

## Limitations

- Three-epoch training protocol may not represent optimal convergence for smaller filtered datasets
- Human evaluation limited to fluency assessment without systematic error analysis or domain-specific terminology accuracy measurement
- Absence of cross-validation or multiple random seeds for baseline comparisons introduces uncertainty about result stability

## Confidence

- **High Confidence**: LASER filtering consistently outperforms random downsampling and other embedding methods at 60% retention level, achieving statistically significant BLEU improvements (+0.177 over baseline)
- **Medium Confidence**: LASER-60% filtering maintains or improves translation quality while reducing training data by 40%, though marginal gains (0.009 BLEU) suggest benefits may plateau
- **Low Confidence**: Claims about LaBSE underperformance despite high correlation require deeper investigation; the relationship between embedding similarity and translation quality isn't fully explained

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically test LASER filtering at multiple retention levels (40%, 50%, 60%, 70%, 80%) to identify optimal data reduction points before quality degradation occurs.

2. **Cross-Lingual Generalizability**: Apply LASER filtering methodology to other low-resource language pairs in biomedical domain to validate whether observed benefits extend beyond English-Polish.

3. **Extended Training Protocol**: Compare convergence curves for full vs. filtered datasets to determine whether 3 epochs represents optimal training duration for each data volume level.