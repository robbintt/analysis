---
ver: rpa2
title: 'CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography'
arxiv_id: '2504.10090'
source_url: https://arxiv.org/abs/2504.10090
tags:
- aperture
- visual
- camera
- reasoning
- focal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CameraBench, a benchmark designed to evaluate
  visual reasoning capabilities in multimodal large language models (MLLMs) through
  photography-related tasks. The benchmark challenges models to identify numerical
  camera settings from images, requiring both visual comprehension and understanding
  of underlying physics.
---

# CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography

## Quick Facts
- **arXiv ID**: 2504.10090
- **Source URL**: https://arxiv.org/abs/2504.10090
- **Reference count**: 22
- **Primary result**: MLLMs show inconsistent visual reasoning for camera settings, with reasoning-focused models outperforming standard versions

## Executive Summary
CameraBench introduces a novel benchmark to evaluate visual reasoning capabilities in multimodal large language models (MLLMs) through photography-related tasks. The benchmark challenges models to identify numerical camera settings from images, requiring both visual comprehension and understanding of underlying physics. Using 100 samples from CameraSettings25K, the authors evaluate several MLLMs on binary-choice and five-choice tasks involving different camera parameters. Results show that while some MLLMs perform well on easier tasks, they struggle with more challenging visual reasoning tasks requiring identification of single parameter differences.

## Method Summary
CameraBench evaluates MLLMs on visual reasoning tasks using photography images and their associated camera settings. The benchmark uses 100 random samples from CameraSettings25K (a combination of CameraSettings20K and MIT-AdobeFiveK datasets). For each image, models must identify camera parameters including focal length, aperture, ISO, and exposure time through multiple-choice questions. Tasks include binary-choice questions where all settings differ, binary-choice questions differing in only one parameter, and five-choice questions. Models are evaluated with temperature and Top-P set to 0.1, with answer options randomly shuffled to prevent selection bias. Performance is measured against random baselines of 0.5 for binary-choice and 0.2 for five-choice tasks.

## Key Results
- MLLMs achieve high accuracy (0.78-0.85) on binary-choice tasks when all camera settings differ
- Performance drops to near-random (0.50) on binary-choice tasks differing in only one parameter
- Reasoning-focused MLLMs outperform standard MLLMs, highlighting the importance of reasoning capabilities
- No single MLLM consistently dominates across all six benchmark tasks

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Multimodal learning**: Integration of visual and textual information processing in MLLMs, essential for understanding how models combine image features with camera setting knowledge
- **Visual reasoning**: Ability to draw logical conclusions from visual inputs, crucial for tasks requiring inference about camera settings from image appearance
- **Photography physics**: Understanding of exposure triangle (aperture, shutter speed, ISO) and focal length effects, needed to map numerical settings to visual characteristics
- **Camera settings normalization**: Conversion of EXIF data to standardized 35mm full-frame format, important for consistent evaluation across different camera systems
- **Visual difference detection**: Capacity to distinguish subtle visual changes between images, fundamental for identifying single-parameter differences
- **Multiple-choice evaluation**: Standard assessment format for comparing model predictions against ground truth options

## Architecture Onboarding
- **Component map**: Image encoder -> Visual feature extractor -> Language model -> Multiple-choice predictor
- **Critical path**: Visual input → Feature extraction → Contextual reasoning → Answer selection
- **Design tradeoffs**: Binary vs. five-choice formats balance difficulty and evaluation precision; reasoning-focused vs. standard models trade computational cost for performance
- **Failure signatures**: Near-random performance on single-parameter tasks suggests limitations in either visual perception or physics understanding
- **3 first experiments**:
  1. Evaluate model performance on all-different settings task to establish baseline visual comprehension
  2. Test single-parameter binary choices to identify specific reasoning weaknesses
  3. Compare reasoning-focused vs. standard MLLMs on identical tasks to quantify reasoning impact

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can MLLMs distinguish visual differences related to numerical camera settings?
- **Basis in paper**: [explicit] "Motivated by these findings, we investigate the following questions: Do MLLMs exhibit visual reasoning capabilities in photography? Specifically, can MLLMs distinguish visual differences across numerical camera settings?"
- **Why unresolved**: Models perform well on tasks with all-different settings (0.78–0.85) but near-random on single-parameter distinctions (~0.50), showing inconsistent visual reasoning
- **What evidence would resolve it**: Consistent above-random performance across all CameraBench tasks, particularly single-parameter binary choices

### Open Question 2
- **Question**: Why do models fail when only one camera parameter differs between options?
- **Basis in paper**: [inferred] Table 1 shows random performance on single-parameter binary choices (Focal Length, Aperture, ISO, Exposure Time) despite strong performance when all settings differ
- **Why unresolved**: Paper demonstrates the gap but doesn't investigate whether it stems from visual perception limitations, insufficient physics understanding, or architectural constraints
- **What evidence would resolve it**: Ablation studies analyzing model attention patterns and intermediate representations during single-parameter reasoning tasks

### Open Question 3
- **Question**: Will findings generalize beyond the 100-sample evaluation?
- **Basis in paper**: [inferred] "Due to resource limitations, we randomly sample 100 images from CameraSettings25K" with promised "Future work will involve more extensive benchmarking and a comprehensive evaluation using the entire CameraSettings25K dataset"
- **Why unresolved**: Small sample may not capture the full diversity of photography scenarios, lighting conditions, or camera settings combinations
- **What evidence would resolve it**: Consistent performance patterns on the full 25,127-image dataset across all six tasks

## Limitations
- Benchmark relies on only 100 random samples from CameraSettings25K, potentially limiting generalizability
- Evaluation includes only 9 MLLMs, which may not comprehensively represent current model capabilities
- Binary-choice and five-choice formats may not fully capture real-world photography assistant complexity

## Confidence
- **High confidence**: Benchmark design methodology and extension of existing vision-language model evaluation approaches
- **Medium confidence**: Reported performance differences between reasoning-focused and standard MLLMs are internally consistent
- **Low confidence**: Claim that no single MLLM consistently dominates across all tasks requires validation with larger sample sizes

## Next Checks
1. Reproduce the benchmark with multiple random seeds to assess performance stability across different sample selections
2. Expand evaluation to include additional contemporary MLLMs and alternative prompt formats to verify robustness of findings
3. Conduct human evaluation on a subset of samples to establish performance upper bound and validate benchmark difficulty calibration