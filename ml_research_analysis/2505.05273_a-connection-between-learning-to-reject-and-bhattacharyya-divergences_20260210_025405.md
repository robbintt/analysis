---
ver: rpa2
title: A Connection Between Learning to Reject and Bhattacharyya Divergences
arxiv_id: '2505.05273'
source_url: https://arxiv.org/abs/2505.05273
tags:
- learning
- divergence
- density
- joint
- rejection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects learning to reject with Bhattacharyya divergences.
  The problem addressed is optimal rejection in classification, where a model can
  abstain from making predictions on uncertain inputs.
---

# A Connection Between Learning to Reject and Bhattacharyya Divergences

## Quick Facts
- **arXiv ID**: 2505.05273
- **Source URL**: https://arxiv.org/abs/2505.05273
- **Reference count**: 30
- **Primary result**: Learning to reject using joint ideal distributions corresponds to thresholding Bhattacharyya divergences, less aggressive than Chow's Rule (KL-based)

## Executive Summary
This paper establishes a theoretical connection between learning to reject in classification and Bhattacharyya divergences. Unlike prior work that uses marginal density ratios and Chow's Rule (thresholding KL divergence), the paper proposes learning a joint ideal distribution over inputs and labels. When using a modified log-loss, the resulting optimal rejector thresholds the skewed Bhattacharyya divergence between estimated and ground-truth class probabilities. This approach is theoretically shown to be less aggressive in rejecting examples compared to Chow's Rule, as it considers the joint distribution rather than just marginal input density.

## Method Summary
The paper addresses post-hoc learning to reject, where a fixed classifier is given and the goal is to learn when to abstain. The core method involves computing an ideal joint distribution Q that minimizes expected loss plus a divergence penalty D(Q||P) from the true joint P. For the KL divergence case (marginal), this yields Chow's Rule with ρ(x) ∝ exp(−E_{Y∼η⋆}[ℓ(Y,h(x))]/λ). For the joint case with modified log-loss, the density ratio becomes ρ_j(x) ∝ C_{1/λ}(η(x)||η⋆(x)), the skewed Bhattacharyya coefficient. Rejection is then performed by thresholding the Bhattacharyya divergence B_{1−1/λ}(η⋆||η) ≥ κ_j.

## Key Results
- Bhattacharyya-based rejection is less aggressive than Chow's Rule (Lemma 2: r_j(x;κ) ⊆ r(x;λ·κ))
- The skew parameter λ changes the geometry of the rejection boundary rather than just the threshold value
- Modified log-loss minimization with joint ideal distribution yields density ratio proportional to Bhattacharyya coefficient

## Why This Works (Mechanism)
The mechanism works by learning an ideal joint distribution that balances classification accuracy with proximity to the true distribution. The modified log-loss encourages the classifier to align with the ground-truth posterior η⋆ while regularizing via divergence from the true joint P. The density ratio from this optimization naturally emerges as a Bhattacharyya coefficient, which measures overlap between distributions more smoothly than KL divergence, leading to less aggressive rejection.

## Foundational Learning
- **Density ratio estimation**: Understanding how to compute dQ/dP for distributions Q and P is crucial for deriving the rejection function. Quick check: Verify ρ(x) sums to 1 when integrated over X.
- **Bhattacharyya divergence**: This symmetric measure of distribution overlap (B_β = -log C_β) is the key quantity for rejection decisions. Quick check: Compute C_β for two Gaussian distributions to verify it's maximized at zero divergence.
- **Modified log-loss**: The loss ℓ̃_log(y,h(x)) = -log(η_y(x)/η⋆_y(x)) is essential for the theoretical connection. Quick check: Confirm this loss is minimized when η(x) = η⋆(x).
- **Ideal distribution optimization**: The argmin_Q formulation balances expected loss with divergence regularization. Quick check: Verify the first-order optimality conditions for Q.
- **Chow's Rule**: The baseline approach using KL divergence thresholding for rejection. Quick check: Implement ρ(x) ∝ exp(-E[ℓ(Y,h(x))]/λ) for KL case.
- **Joint vs marginal distributions**: Understanding why considering the joint P(X,Y) rather than just P(X) changes the rejection behavior. Quick check: Compare rejection regions for a simple 2D synthetic example.

## Architecture Onboarding

**Component Map**: Input X → Classifier h(x) → Class probabilities η(x) → Ground-truth η⋆(x) → Density ratio ρ_j(x) → Rejection decision r(x;κ)

**Critical Path**: The critical computational path is computing the Bhattacharyya coefficient C_β(η||η⋆) = Σ_y η_y^β · η⋆_y^(1−β), then deriving the rejection threshold from the skewed divergence.

**Design Tradeoffs**: Bhattacharyya-based rejection trades off aggressive rejection (high precision) for better coverage by using a smoother divergence measure that considers distribution overlap rather than information loss.

**Failure Signatures**: Numerical underflow in computing C_β when class probabilities are near zero; degenerate rejection if η⋆ is used incorrectly as hard labels rather than soft probabilities.

**First Experiments**:
1. Verify Lemma 2 empirically: On synthetic data with known η⋆, measure rejection rates of Chow's Rule vs. Bhattacharyya-based rejection across varying thresholds.
2. Implement log-domain Bhattacharyya computation to prevent numerical underflow.
3. Test on soft-labeled data: Use CIFAR-100 with soft labels from an ensemble to compare rejection quality between methods.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the skew parameter λ influence the geometry of the rejection boundary and the error-reject trade-off compared to the linear thresholding of Chow's Rule?
- **Basis in paper**: The paper observes that "In the joint case, λ changes the skew of the Bhattacharyya divergence — which changes the boundary an example is rejected, rather than just the threshold value — that we consider to determine rejection."
- **Why unresolved**: The paper derives the theoretical connection but does not characterize the practical implications of varying λ on the shape of the rejection region or model calibration.
- **What evidence would resolve it**: Empirical simulations or theoretical derivations showing how different λ values alter the decision boundaries in synthetic and real-world datasets.

### Open Question 2
- **Question**: Can closed-form density ratio rejectors be derived when the dissimilarity function D in the joint optimization objective is a general f-divergence rather than the KL divergence?
- **Basis in paper**: The author states, "In this paper, we will focus on the case where we consider the canonical KL divergence," implying that the behavior of other divergences in the joint ideal distribution setting remains unexplored.
- **Why unresolved**: The current proofs rely heavily on properties specific to the KL divergence to link the loss to the density ratio; it is unclear if Bhattacharyya or other divergences emerge from different regularizers.
- **What evidence would resolve it**: A derivation of the optimal Q and corresponding density ratio ρ_j when D is substituted with alternative divergences (e.g., χ² or Hellinger distance).

### Open Question 3
- **Question**: What is the finite-sample complexity of estimating the joint ideal density ratio ρ_j given that it depends on the unknown ground-truth posterior η⋆?
- **Basis in paper**: The theoretical results assume access to the true η⋆, but in practice, η⋆ must be estimated, introducing error into the rejection mechanism.
- **Why unresolved**: The paper establishes the theoretical optimality of the method but does not analyze the estimator's variance or bias when η⋆ is replaced by a model's imperfect probability estimates.
- **What evidence would resolve it**: Generalization bounds or convergence analysis for the rejection rate when using estimated class probabilities η̂ instead of η⋆.

## Limitations
- Ground-truth ideal posterior availability: Theoretical framework assumes access to η⋆(x), limiting practical applicability without strong density estimation
- Hyperparameter tuning ambiguity: No guidance on selecting λ or rejection thresholds κ_j/τ for fair comparison with Chow's Rule
- Evaluation protocol unspecified: Lack of dataset, architecture, and baseline specifications makes empirical validation unclear

## Confidence

- **High**: Theoretical derivation of rejection functions via density ratio estimation and the equivalence between Bhattacharyya-based rejection and modified log-loss minimization
- **Medium**: Proof that Bhattacharyya-based rejection is less aggressive than Chow's Rule (Lemma 2), assuming access to η⋆
- **Low**: Practical utility claims, due to missing experimental setup and hyperparameter guidance

## Next Checks
1. Verify Lemma 2 empirically: On synthetic data with known η⋆, measure rejection rates of Chow's Rule vs. Bhattacharyya-based rejection across varying thresholds to confirm the subset relationship r_j(x;κ) ⊆ r(x;λ·κ).
2. Implement log-domain Bhattacharyya computation: Prevent numerical underflow by verifying C_β(η||η⋆) = Σ_y exp(β·log η + (1−β)·log η⋆_y) is stable for small η_y or η⋆_y.
3. Test on soft-labeled data: Use CIFAR-100 with soft labels from an ensemble to compare rejection quality (false rejection rate vs. coverage) between Chow's Rule and Bhattacharyya-based rejection, validating reduced aggressiveness translates to better coverage.