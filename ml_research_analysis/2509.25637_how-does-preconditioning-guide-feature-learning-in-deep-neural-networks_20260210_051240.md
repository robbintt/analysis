---
ver: rpa2
title: How Does Preconditioning Guide Feature Learning in Deep Neural Networks?
arxiv_id: '2509.25637'
source_url: https://arxiv.org/abs/2509.25637
tags:
- train
- test
- learning
- generalization
- preconditioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how preconditioning influences feature
  learning and generalization in deep neural networks. The authors establish that
  preconditioning determines the similarity metric via the Gram matrix, thereby controlling
  the spectral bias in feature learning.
---

# How Does Preconditioning Guide Feature Learning in Deep Neural Networks?

## Quick Facts
- arXiv ID: 2509.25637
- Source URL: https://arxiv.org/abs/2509.25637
- Authors: Kotaro Yoshida; Atsushi Nitanda
- Reference count: 40
- This paper investigates how preconditioning influences feature learning and generalization in deep neural networks.

## Executive Summary
This paper investigates how preconditioning influences feature learning and generalization in deep neural networks. The authors establish that preconditioning determines the similarity metric via the Gram matrix, thereby controlling the spectral bias in feature learning. They show that the exponent $p$ in the preconditioner $\Sigma_X^p$ governs the emphasis on high- or low-variance components, and that generalization depends critically on the alignment between this spectral bias and the teacher's spectral structure.

Through synthetic experiments with varying noise levels and alignment scenarios, they demonstrate that models become sensitive to the variance components emphasized by the preconditioner, with optimal performance occurring when $p$ matches the teacher's spectral emphasis. In out-of-distribution settings, preconditioners that suppress spurious features while highlighting invariant ones improve robustness. For knowledge transfer, they find that $p=-1$ (uniform treatment across all spectral components) yields the best transfer performance.

## Method Summary
The authors develop a theoretical framework linking preconditioning to spectral bias in feature learning. They analyze how the exponent $p$ in the preconditioner $\Sigma_X^p$ controls emphasis on different spectral components of the data covariance matrix. Through synthetic experiments, they systematically vary $p$ values and measure their impact on learning sensitivity, generalization performance, and transfer learning capabilities across different noise levels and teacher-student alignment scenarios.

## Key Results
- Preconditioning determines similarity metric via Gram matrix, controlling spectral bias in feature learning
- Optimal generalization occurs when preconditioner spectral emphasis aligns with teacher's spectral structure
- For knowledge transfer, $p=-1$ (uniform treatment of spectral components) yields best performance
- Out-of-distribution robustness improves when preconditioners suppress spurious features while highlighting invariant ones

## Why This Works (Mechanism)
Preconditioning shapes the geometry of the loss landscape by controlling which spectral components of the data receive more emphasis during optimization. The Gram matrix, which defines the inner product in feature space, is directly influenced by the preconditioner. When the preconditioner emphasizes certain spectral components (via the exponent $p$), the model learns to prioritize those features during training. This spectral bias determines which patterns the model becomes sensitive to, ultimately affecting both generalization and robustness. The mechanism works because optimization dynamics are guided by the preconditioned geometry, making the model more or less sensitive to different types of variance in the data.

## Foundational Learning
- **Spectral bias**: The tendency of neural networks to learn functions with specific frequency characteristics. Why needed: Understanding how models prioritize different frequency components is crucial for explaining generalization behavior. Quick check: Verify that models trained with different preconditioners show distinct learning curves for high vs low frequency components.
- **Gram matrix similarity**: The inner product matrix that defines similarity between features in the learned representation space. Why needed: This metric determines how the model perceives and groups similar patterns. Quick check: Compute Gram matrices under different preconditioners and verify they capture different similarity structures.
- **Covariance matrix spectral structure**: The eigenvalue decomposition of data covariance revealing which directions have high vs low variance. Why needed: Preconditioning operates on this spectral structure to emphasize certain components. Quick check: Compare eigenvalue spectra of different datasets to understand their inherent biases.
- **Feature variance decomposition**: Breaking down learned features into components with different variance magnitudes. Why needed: Preconditioning explicitly controls emphasis on high vs low variance components. Quick check: Measure feature variance contributions under different preconditioning schemes.

## Architecture Onboarding

Component map: Data → Preconditioner (Σ_X^p) → Gram matrix → Optimization dynamics → Learned features → Generalization performance

Critical path: Preconditioner selection → Spectral bias induction → Feature learning sensitivity → Generalization outcome

Design tradeoffs: High p values emphasize low-variance components (potentially overfitting to noise) vs low/negative p values emphasize high-variance components (potentially missing subtle patterns). The choice involves balancing sensitivity to different spectral components against robustness to noise and domain shifts.

Failure signatures: Poor generalization when preconditioner spectral emphasis misaligns with teacher's structure; overfitting when p is too high (emphasizing noise); underfitting when p is too low (missing important patterns); transfer failure when source and target domains have mismatched spectral properties.

First experiments:
1. Synthetic data with controlled spectral structure: Create data where ground truth features have known variance distributions, then test how different preconditioners recover these features
2. Noise sensitivity analysis: Add varying levels of label noise to data and measure how preconditioner choice affects robustness
3. Transfer learning validation: Train on source domain with specific spectral structure, then test transfer to target domains with different spectral properties using different preconditioners

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on linear settings and simple synthetic data, potentially missing complexity of real-world deep learning scenarios
- Claim that p=-1 universally optimizes transfer learning requires validation across diverse architectures and datasets
- Assumption of independent and identically distributed features may not reflect structured dependencies in natural data

## Confidence

High confidence: The mathematical derivation linking preconditioning to Gram matrix similarity and spectral bias is rigorous and well-established.

Medium confidence: The synthetic experiments demonstrating the impact of p on feature learning sensitivity are convincing but limited in scope.

Medium confidence: The generalization results showing alignment between preconditioner and teacher spectral structure are compelling but require validation on real-world datasets.

Low confidence: The claim about p=-1 being optimal for knowledge transfer across diverse scenarios needs more extensive empirical validation.

## Next Checks

1. Test the preconditioning framework on real-world datasets (e.g., CIFAR, ImageNet) with varying levels of label noise and domain shifts to validate the synthetic findings.

2. Extend the analysis to deep nonlinear networks to assess whether the linear preconditioning insights hold in more complex architectures.

3. Conduct ablation studies across multiple transfer learning scenarios with diverse source-target domain pairs to rigorously test the p=-1 transfer learning claim.