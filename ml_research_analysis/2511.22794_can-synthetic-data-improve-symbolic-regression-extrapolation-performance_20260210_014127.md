---
ver: rpa2
title: Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?
arxiv_id: '2511.22794'
source_url: https://arxiv.org/abs/2511.22794
tags:
- data
- extrapolation
- synthetic
- teacher
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether synthetic data can improve symbolic
  regression (SR) extrapolation performance. Using Kernel Density Estimation (KDE)
  to identify sparse input regions, synthetic data is generated via knowledge distillation
  from teacher models (NN, RF, GP) to train student GP models.
---

# Can Synthetic Data Improve Symbolic Regression Extrapolation Performance?

## Quick Facts
- arXiv ID: 2511.22794
- Source URL: https://arxiv.org/abs/2511.22794
- Reference count: 35
- GP models improve extrapolation when trained on synthetic data from GPe, especially using GPe → GPp transfer

## Executive Summary
This study investigates whether synthetic data can improve symbolic regression (SR) extrapolation performance. Using Kernel Density Estimation (KDE) to identify sparse input regions, synthetic data is generated via knowledge distillation from teacher models (NN, RF, GP, GPe) to train student GP models. Across six benchmark datasets, GP models often improve when trained on synthetic data, especially in extrapolation areas. The most consistent improvements occur when synthetic data from GPe is used to train GPp. Changes in interpolation areas show only slight changes. Statistical significance analysis reveals heterogeneous errors, where model performance varies across input space regions. The results demonstrate that synthetic data augmentation offers a practical solution for better extrapolation in symbolic regression.

## Method Summary
The approach uses KDE to identify sparse regions in input space, then generates synthetic data in those regions using knowledge distillation. A teacher model trained on original data predicts targets for synthetic inputs, which are then used to train a student GP model. The framework tests different teacher-student combinations (NN, RF, GP, GPe) across six benchmark datasets, comparing interpolation and extrapolation performance.

## Key Results
- GP models show more improvements than disimprovements when trained on synthetic data, especially for extrapolation
- GPe → GPp transfer consistently improves extrapolation across all six datasets
- Only about half of performance improvements reach statistical significance
- Interpolation performance shows only slight changes when using synthetic data

## Why This Works (Mechanism)

### Mechanism 1: KDE-Based Sparse Region Targeting
KDE estimates probability density function using Gaussian kernel. Samples below 10th percentile of log-density scores are flagged as extrapolation zones. Gaussian noise (ε=0.3) is added to sparse-region inputs to create synthetic feature points, labeled by teacher model. Low-density regions correspond to extrapolation zones where prediction error increases systematically.

### Mechanism 2: Knowledge Distillation Through Synthetic Target Generation
Teacher model trained on original data generates predictions on synthetic inputs in sparse regions. These synthetic (X̂, ŷ) pairs augment training set for student GP. Teacher models provide reasonable predictions in sparse regions that give useful inductive bias for GP symbolic expressions, even if imperfect.

### Mechanism 3: Complementary Model Selection Strategies
GPe (lowest error GP) generates synthetic data to train GPp (heuristic-selected GP). GPe and GPp represent problems differently due to their selection criteria. GPe's predictive patterns provide beneficial inductive bias for GPp when trained on GPe's synthetic outputs.

## Foundational Learning

- **Symbolic Regression via Genetic Programming**: GP searches expression space and tends to overfit; understanding complexity/accuracy trade-offs is essential for interpreting results. Quick check: Why does GP produce unstable extrapolation compared to linear regression?

- **Kernel Density Estimation (KDE)**: KDE separates interpolation from extrapolation regions; understanding bandwidth selection and density thresholds is critical. Quick check: If you increase KDE bandwidth, what happens to proportion of samples flagged as "sparse"?

- **Knowledge Distillation**: Teacher-student paradigm structures augmentation approach; understanding what knowledge transfers is critical. Quick check: Why might teacher model with low training error still be poor choice for generating synthetic extrapolation labels?

## Architecture Onboarding

- **Component map**: Training data → KDE Module → Synthetic Input Generator → Teacher Model → Augmented Dataset → Student Model → Evaluation

- **Critical path**: KDE bandwidth/threshold selection → determines synthetic data placement; Teacher model choice → determines synthetic label quality; Noise level ε → controls synthetic input diversity; Student GP hyperparameters → determines expression search capacity

- **Design tradeoffs**: 
  - Teacher selection: NN/RF provide bounded predictions but may underfit; GPe provides accurate but potentially unstable extrapolations
  - Synthetic sample count: ~1/9 of interpolation samples; too few provides insufficient coverage, too many overwhelms signal
  - Noise level (ε=0.3): Too low clusters near training data, too high falls in implausible regions
  - Validation: Use interpolation validation performance to select between original vs. augmented models

- **Failure signatures**: 
  1. GP-as-teacher disimprovement: Avoid using GP to generate synthetic data for non-GP students
  2. Dataset heterogeneity: Test on multiple datasets before concluding generalization
  3. Non-significant improvements: Only ~50% of positive differences are statistically significant

- **First 3 experiments**:
  1. Reproduce GPe→GPp on single dataset with provided hyperparameters; verify GPe-synthetic data improves GPp extrapolation
  2. Ablate KDE threshold: Test 5th, 10th, 20th percentile thresholds on same dataset; measure impact on extrapolation RMSE change
  3. Compare teacher models on held-out dataset: Train NN, RF, GPe teachers on one dataset, generate synthetic data, train GPp student; evaluate on all six benchmark datasets

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- KDE-based extrapolation detection relies on fixed 10th percentile threshold that may not generalize across datasets
- Synthetic data generation noise level (ε=0.3) was chosen empirically without systematic exploration
- Study focuses on six benchmark datasets; generalization to real-world scientific problems remains untested
- Only about half of improvements reach statistical significance, suggesting heterogeneous effectiveness

## Confidence
- **High Confidence**: GPe→GPp teacher-student pattern consistently improves extrapolation across all six datasets
- **Medium Confidence**: KDE successfully identifies extrapolation-prone regions based on residual analysis
- **Low Confidence**: Claim that synthetic augmentation "offers a practical solution" given limited statistical significance and dataset variation

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary KDE bandwidth (0.1-1.0) and percentile threshold (5th-20th) on Gurson dataset to establish robustness ranges
2. **Noise Level Impact**: Test ε values from 0.1 to 1.0 on same dataset to quantify trade-off between synthetic input diversity and label reliability
3. **Cross-Dataset Teacher Transfer**: Train teachers on Gurson, generate synthetic data, and evaluate GPp students on all six benchmark datasets to assess whether teacher quality transfers across problems