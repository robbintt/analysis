---
ver: rpa2
title: Real-world Reinforcement Learning from Suboptimal Interventions
arxiv_id: '2512.24288'
source_url: https://arxiv.org/abs/2512.24288
tags:
- policy
- human
- training
- learning
- intervention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SiLRI introduces a state-wise Lagrangian reinforcement learning
  approach to leverage suboptimal human interventions in real-world robot manipulation.
  It formulates the problem as a constrained RL optimization where constraint bounds
  are determined by state-wise human uncertainty, using learnable Lagrange multipliers
  to adaptively balance imitation and RL objectives.
---

# Real-world Reinforcement Learning from Suboptimal Interventions

## Quick Facts
- arXiv ID: 2512.24288
- Source URL: https://arxiv.org/abs/2512.24288
- Reference count: 40
- Primary result: SiLRI achieves 90% success rate on real-world manipulation tasks, learning at least 50% faster than state-of-the-art HIL-SERL method.

## Executive Summary
SiLRI introduces a state-wise Lagrangian reinforcement learning approach to leverage suboptimal human interventions in real-world robot manipulation. It formulates the problem as a constrained RL optimization where constraint bounds are determined by state-wise human uncertainty, using learnable Lagrange multipliers to adaptively balance imitation and RL objectives. Evaluated on eight challenging manipulation tasks across two robot embodiments, SiLRI reaches 90% success rate at least 50% faster than the state-of-the-art HIL-SERL method, achieving a 100% success rate on long-horizon tasks where other RL methods struggle. It also demonstrates superior robustness in dynamic environments compared to online imitation learning baselines.

## Method Summary
SiLRI builds on off-policy actor-critic RL with four networks: actor π, critic Q, behavior policy β, and Lagrange multiplier λ. The method introduces state-wise constraints ||π(s) - β(s)||² ≤ κ·σ_β(s)² that adapt based on human intervention uncertainty, formalized as a min-max optimization problem. Interventions flow to dual buffers (D_I for behavior policy, D_R for RL), with β updated periodically to maintain stable entropy estimates. The Lagrange network outputs per-state multipliers that automatically balance BC vs RL objectives, trained with 100x slower learning rate to prevent instability. The system operates asynchronously with periodic actor parameter synchronization.

## Key Results
- Achieves 90% success rate on real-world manipulation tasks
- Learns at least 50% faster than HIL-SERL baseline
- Achieves 100% success rate on long-horizon tasks where other RL methods struggle
- Demonstrates superior robustness in dynamic environments compared to online imitation learning baselines

## Why This Works (Mechanism)

### Mechanism 1: State-wise Constraint Relaxation via Human Uncertainty Estimation
Constraining policy deviation proportionally to human intervention consistency accelerates early learning while preserving ability to surpass human performance. Human intervention data is modeled as a multivariate Gaussian β(s). States with low variance indicate confident human control; high variance indicates uncertainty. The constraint tightens bounds in low-entropy states (forcing imitation) and relaxes them in high-entropy states (allowing RL-driven exploration). Core assumption: human action variance correlates with intervention quality—low variance implies near-optimal guidance, high variance implies suboptimality worth ignoring.

### Mechanism 2: Learnable State-wise Lagrange Multiplier via Min-Max Optimization
Jointly optimizing policy and Lagrange multipliers as a saddle-point problem enables automatic, per-state balancing of BC vs RL objectives without manual tuning. The Lagrangian L(π,λ) = -J(π) + λ^T[D(π,β) - κΣ_β] is minimized over π and maximized over λ. The λ-network outputs per-state multipliers: when constraint violated, λ increases to strengthen imitation; when satisfied, λ decays toward zero, letting RL dominate. Slow learning rate (0.01x) prevents λ explosion. Core assumption: a single scalar multiplier per state suffices; the dual problem converges to a useful saddle point despite non-convexity.

### Mechanism 3: Asynchronous Dual-Buffer Training with Periodic Behavior Policy Updates
Separating intervention data (D_I) from online RL data (D_R) while updating the behavior policy at low frequency maintains stable entropy estimates and prevents premature collapse of action diversity. Transitions flow to both buffers; intervention samples go to D_I. Behavior policy β is updated every 50 new intervention samples (not every gradient step), preserving its stochasticity for accurate uncertainty estimation. Equal batch sampling (128 each) ensures BC signal remains present throughout training. Core assumption: intervention data remains sufficiently diverse; periodic β updates preserve meaningful variance estimates.

## Foundational Learning

- **Constrained Optimization with Lagrange Multipliers**
  - Why needed here: The entire SiLRI formulation recasts RL + imitation as a constrained optimization; understanding how Lagrangians convert constraints to penalties is essential to read Eqn. 4-8.
  - Quick check question: Given a constraint g(x) ≤ 0, write the Lagrangian and explain when the multiplier increases vs. decreases during dual optimization.

- **Off-Policy Actor-Critic Methods (TD3/SAC family)**
  - Why needed here: SiLRI builds on standard off-policy RL with double Q-networks, target networks, and separate actor/critic updates; Section IV-C assumes this background.
  - Quick check question: Why does off-policy RL use target networks and what problem does double Q-learning address?

- **Behavior Cloning and Multivariate Gaussian Policies**
  - Why needed here: The behavior policy β is a multivariate Gaussian fitted to human demonstrations; understanding log-likelihood loss and entropy is necessary to interpret Eqn. 12 and uncertainty estimation.
  - Quick check question: For a Gaussian policy N(μ, σ²), how does changing σ affect the KL divergence from another Gaussian with the same mean?

## Architecture Onboarding

- **Component map:**
  - Actor π: Visual encoder + MLP → 7-DoF action (6 pose + gripper), trained with combined RL + BC loss weighted by λ(s)
  - Critic Q: Double Q-networks with target networks, standard TD loss
  - Lagrange λ: State-conditional MLP → scalar (Softplus output), trained to maximize constraint violation penalty
  - Behavior β: Visual encoder + MLP → Gaussian parameters (μ, σ), trained via BC on D_I

- **Critical path:**
  1. Collect 20 demonstration trajectories → pre-train β for 500 steps
  2. Online rollouts: human intervention via teleop or policy action → log transitions to D_R and D_I
  3. Every 50 intervention samples → update β
  4. Every gradient step → sample from D_R and D_I → update Q, π, λ asynchronously
  5. Periodically sync actor parameters from learner to inference process

- **Design tradeoffs:**
  - **Slow λ learning rate (3e-6)**: Stability vs. adaptivity—too fast and λ explodes; too slow and constraint adaptation lags
  - **Constant κ=6 + margin c=0.1**: Looser constraints enable more exploration but reduce early acceleration from imitation
  - **Separate gripper policy**: BC-only gripper control (mentioned in robustness experiments) simplifies training but hurts recovery in some tasks

- **Failure signatures:**
  - Lagrange multiplier grows unbounded → policy collapses to pure imitation, cannot surpass human
  - Success rate plateaus below human level → λ may be decaying too fast; check β variance estimates
  - High intervention ratio persists → RL term not learning; inspect Q-value estimates and reward classifier

- **First 3 experiments:**
  1. **Ablate λ-network**: Replace with fixed constant (λ=0.5) on one task; expect faster early learning but degraded final performance (replicating Fig. 6 yellow line)
  2. **Vary β update frequency**: Test updating β every 10 vs. 50 vs. 100 samples; monitor entropy collapse and success rate curves
  3. **Stress-test under operator variability**: Have skilled vs. unskilled operators run Push-T; compare SiLRI vs. HG-Dagger sensitivity to intervention quality (replicating Fig. 7)

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating vision-language-action (VLA) models with pre-existing manipulation skills substantially reduce the required online fine-tuning time from 60+ minutes to human-comparable learning speeds (minutes of practice)? Basis in paper: "In future work, we plan to incorporate vision-language-action models with pre-existing manipulation skills to reduce the required online fine-tuning time." Why unresolved: Current SiLRI requires 60+ minutes for tasks humans learn in minutes; the authors identify this gap but have not tested VLA integration. What evidence would resolve it: Experiments comparing SiLRI with VLA-pretrained baselines on the same manipulation tasks, measuring time-to-proficiency.

### Open Question 2
What richer intervention signal modalities beyond direct action commands could improve robustness in dynamic environments and large exploration spaces? Basis in paper: "Beyond using intervention data directly, richer intervention signals could be leveraged to implicitly constrain the exploration space and improve robustness." Why unresolved: SiLRI's robustness degrades under dynamic disturbances (47% drop on Hang Chinese Knot), but alternative intervention signal designs remain unexplored. What evidence would resolve it: Ablation studies comparing action-only interventions against preference signals, verbal corrections, or trajectory sketches on dynamic-robustness benchmarks.

### Open Question 3
How does SiLRI scale to multi-operator settings where different humans exhibit divergent uncertainty patterns across the same states? Basis in paper: Experiments used controlled single-operator settings (skilled vs. unskilled), but the state-wise uncertainty estimation assumes a unified behavior policy β, which may not capture inter-operator variability. Why unresolved: The multivariate Gaussian model for β aggregates all intervention data; divergent operator styles could corrupt uncertainty estimates. What evidence would resolve it: Experiments with multiple operators providing interventions simultaneously, analyzing whether separate behavior policies per operator or a unified mixture model performs better.

### Open Question 4
Can explicit upper bounds on Lagrange multiplier values replace the heuristic slow-learning-rate approach to improve training stability and convergence speed? Basis in paper: The paper uses a 100x slower learning rate for λ to prevent unbounded growth, but notes "there is no explicit upper bound on the output of the Lagrange network." Why unresolved: Slow learning introduces lag in constraint adaptation; architectural constraints (e.g., capped output layers) might enable faster, more stable multiplier dynamics. What evidence would resolve it: Ablations comparing softplus+capped outputs against the current slow-learning approach, measuring convergence time and stability across tasks.

## Limitations

- **Unknown network architectures**: Exact layer specifications for all four networks (actor, critic, behavior policy, Lagrange multiplier) are not provided, only cited references.
- **Reward classifier details missing**: The architecture, training data, and update frequency for the "ever-correcting" reward classifier module are not described.
- **Polyak averaging coefficient unspecified**: The target network update coefficient τ is not stated (commonly 0.005-0.01 in similar work).

## Confidence

- **High confidence**: Quantitative performance improvements (90% success rate, 50% faster learning, 100% success on long-horizon tasks) - these are directly measurable and reported with statistical comparisons to baselines.
- **Medium confidence**: Mechanism validity (state-wise constraint relaxation, learnable Lagrange multipliers, dual-buffer training) - the theoretical formulation is sound, but empirical validation relies on ablation studies that don't isolate each mechanism independently.
- **Low confidence**: Real-world robustness claims - while qualitative improvements are reported, the evaluation lacks systematic testing across varying environmental conditions, operator skill levels, and hardware perturbations.

## Next Checks

1. **Ablate λ-network**: Replace with fixed constant (λ=0.5) on one task; expect faster early learning but degraded final performance, validating the adaptive Lagrange multiplier mechanism.

2. **Vary β update frequency**: Test updating behavior policy every 10 vs. 50 vs. 100 intervention samples; monitor entropy collapse and success rate curves to validate the periodic update schedule.

3. **Operator skill variability**: Have skilled vs. unskilled operators run Push-T task; compare SiLRI vs. HG-Dagger sensitivity to intervention quality, testing the assumption that human variance correlates with action quality.