---
ver: rpa2
title: 'Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image,
  Audio, and Video'
arxiv_id: '2510.03458'
source_url: https://arxiv.org/abs/2510.03458
tags:
- retrieval
- text
- video
- arxiv
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Omni-Embed-Nemotron is a unified multimodal retrieval model capable
  of processing text, image, audio, and video inputs within a shared embedding space.
  Built on the Qwen-Omni architecture, it uses a bi-encoder design with modality-specific
  encoding for audio and video, avoiding token interleaving to preserve full contextual
  information.
---

# Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video

## Quick Facts
- arXiv ID: 2510.03458
- Source URL: https://arxiv.org/abs/2510.03458
- Reference count: 34
- Unified multimodal retrieval across text, image, audio, and video without modality-specific training data

## Executive Summary
Omni-Embed-Nemotron is a unified multimodal retrieval model that processes text, image, audio, and video inputs within a shared embedding space. Built on the Qwen-Omni architecture, it uses a bi-encoder design with modality-specific encoding for audio and video, avoiding token interleaving to preserve full contextual information. The model employs contrastive learning with hard-negative mining for training, leveraging text-text and text-image pairs, and achieving strong zero-shot performance on video and audio retrieval without requiring modality-specific training data.

## Method Summary
The model uses a bi-encoder architecture where audio and video are encoded independently in separate streams, preserving each modality's native temporal structure. Training employs InfoNCE contrastive loss with hard-negative mining (K=2, threshold=0.95) on text-text and text-image pairs. The Qwen2.5-Omni-3B backbone is modified to use bidirectional attention instead of causal attention, with LoRA (r=16, α=32) applied to the LLM only while freezing audio and visual encoders from Qwen-Omni.

## Key Results
- Achieves average NDCG@10 of 0.7064 on FineVideo video retrieval benchmark
- Demonstrates strong zero-shot transfer to audio/video retrieval without modality-specific training data
- Shows late fusion of audio and video significantly outperforms early fusion (+9.6 NDCG@10: 0.5662 vs. 0.4700 on FineVideo)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-interleaved modality encoding improves retrieval performance over token-interleaved fusion for cross-modal matching.
- Mechanism: Audio and video are encoded independently in separate streams, preserving each modality's native temporal structure without splitting tokens for synchronization. This allows the model to capture complete modality-specific context, which is then combined via late fusion rather than early fusion.
- Core assumption: Retrieval tasks benefit more from complete modality representations than from temporal alignment across modalities.
- Evidence anchors:
  - [abstract] "uses a bi-encoder design with modality-specific encoding for audio and video, avoiding token interleaving to preserve full contextual information"
  - [section 2] "According to our experiments, this non-interleaved design improves retrieval performance, as reported in Table 2 and 3"
- Break condition: If early fusion outperforms late fusion on a new dataset, the non-interleaved assumption may not hold for that domain.

### Mechanism 2
- Claim: Zero-shot cross-modal transfer to audio/video is achievable without modality-specific training data through frozen pretrained encoders and LoRA-tuned alignment.
- Mechanism: The model leverages pretrained audio and visual encoders (frozen) from Qwen-Omni, which already contain modality-specific representations. LoRA tuning on the LLM backbone, combined with bidirectional attention and contrastive training on text-text and text-image pairs, aligns these pretrained representations into a shared embedding space without requiring audio or video training pairs.
- Core assumption: Pretrained modality encoders from Qwen-Omni already encode sufficient audio/video semantics; only cross-modal alignment needs learning.
- Evidence anchors:
  - [abstract] "achieving strong zero-shot performance on video and audio retrieval without requiring modality-specific training data"
  - [section 3.3] "Even without using audio or video data during training, our model demonstrates strong performance on these modalities"
- Break condition: If audio/video retrieval performance drops sharply on out-of-domain audio/video content not resembling Qwen-Omni's pretraining distribution, transfer is limited.

### Mechanism 3
- Claim: Hard-negative mining with a similarity threshold improves contrastive learning by filtering false negatives.
- Mechanism: During training, the top-k negatives (K=2) are selected only if their similarity to the query is less than 95% of the query-positive similarity. This excludes high-similarity negatives that are likely false negatives (semantically relevant but not labeled as positive), preventing the model from being penalized for correctly recognizing semantic matches.
- Core assumption: Negatives with similarity >95% of the positive pair are likely unlabeled positives; excluding them improves training signal quality.
- Evidence anchors:
  - [section 3.1] "This encourages the model to learn from challenging negatives, while removing potential false negatives that have high similarity scores"
- Break condition: If threshold (0.95) is too aggressive for a domain with many fine-grained distinctions, it may over-filter legitimate hard negatives.

## Foundational Learning

- Concept: **Bi-encoder architecture for retrieval**
  - Why needed here: The model uses independent encoding of queries and corpus items into a shared embedding space, enabling efficient similarity search at scale.
  - Quick check question: Can you explain why bi-encoders are more scalable than cross-encoders for large corpus retrieval?

- Concept: **Contrastive learning (InfoNCE loss)**
  - Why needed here: The model is trained to maximize similarity for positive pairs and minimize it for negative pairs. This is the core training objective that creates the unified embedding space across modalities.
  - Quick check question: In InfoNCE loss, what happens to the gradient signal when all negatives have very low similarity to the query?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The model freezes modality encoders and only applies LoRA to the LLM backbone (r=16, α=32). This reduces training cost while adapting cross-modal alignment.
  - Quick check question: What is the relationship between LoRA rank (r) and the number of trainable parameters?

## Architecture Onboarding

- Component map:
  - Input processors (modality-specific preprocessing) -> Frozen encoders (audio/visual from Qwen-Omni) -> LoRA-tuned LLM backbone (Thinker with bidirectional attention) -> Pooling layer -> Similarity computation

- Critical path:
  1. Input preprocessing per modality (controls sequence length and compute)
  2. Independent encoding via frozen modality encoders
  3. LLM backbone processing with bidirectional attention
  4. Pooling to single embedding vector
  5. Similarity computation for retrieval

- Design tradeoffs:
  - Non-interleaved vs. interleaved encoding: Non-interleaved preserves full modality context but requires separate encoding passes
  - Late vs. early fusion: Late fusion (encoding separately, then combining scores) outperforms early fusion (combining tokens before encoding) on FineVideo
  - Frozen encoders vs. full fine-tuning: Freezing reduces compute but limits adaptation to new domains

- Failure signatures:
  - Early fusion underperforms audio-only on FineVideo (0.4700 vs. 0.5407) → suggests modality signal is being lost
  - Large sequence lengths (e.g., 23,960 for audio+video) → high memory/compute cost for long videos
  - Performance drop on text-centric LPM vs. text-only baselines → modality fusion may introduce noise

- First 3 experiments:
  1. Modality ablation: Run retrieval on FineVideo and LPM using (a) text-only, (b) audio-only, (c) video-only, (d) audio+video early fusion, (e) audio+video late fusion. Compare to Table 2 and 3 to validate late fusion advantage.
  2. Hard-negative threshold sweep: Test threshold values [0.90, 0.95, 0.99] on a validation set to check if 0.95 is optimal or domain-dependent.
  3. Sequence length impact: Vary `max_pixels` and `audio_kwargs.max_length` to measure compute (latency/memory) vs. retrieval quality tradeoff on long videos (>5 minutes).

## Open Questions the Paper Calls Out

- Question: What explains the lack of cross-modal transfer when training on video data, and how can multimodal training be designed to improve generalization across modalities?
  - Basis in paper: [explicit] The authors state that adding FineVideo training data "improved performance on the FineVideo evaluation benchmark... However, it did not enhance retrieval across other modalities or generalize better to unseen video datasets."
  - Why unresolved: The paper does not investigate why in-domain video training fails to transfer, nor does it explore alternative training strategies that might yield cross-modal benefits.

- Question: Why does late fusion of audio and video significantly outperform early fusion, and is there an optimal fusion strategy that could further improve retrieval performance?
  - Basis in paper: [explicit] Table 3 shows early fusion achieves NDCG@10 of 0.4700 while late fusion achieves 0.5662 on FineVideo.
  - Why unresolved: The paper documents the performance gap but does not provide theoretical analysis or systematic exploration of alternative fusion approaches.

- Question: What mechanisms enable the model's strong zero-shot transfer to audio and video retrieval without modality-specific training data?
  - Basis in paper: [explicit] The authors note: "Even without using audio or video data during training, our model demonstrates strong performance on these modalities."
  - Why unresolved: The paper demonstrates the phenomenon but does not investigate whether transfer comes from the Qwen-Omni backbone, the multimodal alignment from image training, or architectural properties.

## Limitations

- Evaluation relies on a limited set of benchmarks (FineVideo, LPM, ViDoRe, MTEB) with relatively small dataset sizes
- Zero-shot transfer claims for audio and video are not validated with modality-specific training data
- Key training hyperparameters (epochs, batch size, learning rate) are not disclosed, making precise reproduction challenging

## Confidence

- **High Confidence**: The bi-encoder architecture design, frozen modality encoders, and InfoNCE contrastive learning framework are clearly specified and technically sound
- **Medium Confidence**: The reported benchmark performance and zero-shot transfer claims are plausible given the architecture, but limited validation data prevents strong validation
- **Low Confidence**: The optimal hard-negative threshold (0.95) and late fusion implementation details are not empirically validated across diverse domains

## Next Checks

1. **Hard-negative threshold sensitivity**: Test threshold values [0.90, 0.95, 0.99] on a held-out validation set to determine if the 0.95 threshold is optimal or domain-dependent, and measure impact on false negative rates

2. **Modality ablation on FineVideo**: Run controlled experiments comparing text-only, audio-only, video-only, audio+video early fusion, and audio+video late fusion to confirm the reported +9.6 NDCG@10 improvement for late fusion over early fusion

3. **Sequence length and compute tradeoff**: Systematically vary `max_pixels` and `audio_kwargs.max_length` on long videos (>5 minutes) to quantify the relationship between input sequence length, memory/compute cost, and retrieval quality degradation