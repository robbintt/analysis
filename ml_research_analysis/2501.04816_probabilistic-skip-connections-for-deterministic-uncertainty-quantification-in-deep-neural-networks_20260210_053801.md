---
ver: rpa2
title: Probabilistic Skip Connections for Deterministic Uncertainty Quantification
  in Deep Neural Networks
arxiv_id: '2501.04816'
source_url: https://arxiv.org/abs/2501.04816
tags:
- feature
- layer
- network
- collapse
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach for deterministic uncertainty
  quantification (UQ) in deep neural networks by identifying intermediate layers that
  maintain both sensitivity and smoothness without requiring spectral normalization
  during training. The key idea is to use neural collapse metrics to find appropriate
  intermediate layers and create probabilistic skip connections (PSCs) that retrofit
  existing models for uncertainty estimation.
---

# Probabilistic Skip Connections for Deterministic Uncertainty Quantification in Deep Neural Networks

## Quick Facts
- arXiv ID: 2501.04816
- Source URL: https://arxiv.org/abs/2501.04816
- Authors: Felix Jimenez; Matthias Katzfuss
- Reference count: 40
- Key outcome: Novel approach retrofits existing deep networks for uncertainty quantification by identifying intermediate layers that maintain sensitivity and smoothness, achieving performance comparable to spectral normalization without requiring network retraining

## Executive Summary
This paper introduces Probabilistic Skip Connections (PSCs) as a method to retrofit pretrained deep neural networks for uncertainty quantification (UQ) without requiring spectral normalization (SN) training or architectural modifications. The key insight is that intermediate layers in standard networks can exhibit both sensitivity to input perturbations and smoothness in feature space, properties needed for effective UQ. By using neural collapse metrics (NC1 and NC4) to identify such layers, the method extracts low-dimensional projections of intermediate features and fits probabilistic models to enable uncertainty estimation. Extensive experiments demonstrate that PSCs achieve UQ and out-of-distribution detection performance comparable to or better than SN-trained networks while avoiding the need for retraining.

## Method Summary
PSCs retrofit existing pretrained networks by identifying intermediate layers that maintain both sensitivity (distinct inputs map to distinct features) and smoothness (similar inputs map to similar features) using neural collapse metrics NC1 and NC4. The method extracts features from these identified layers, applies Tucker decomposition for dimensionality reduction, and fits two probabilistic models: a Gaussian Discriminant Analysis model for out-of-distribution detection and a linear classifier with Kronecker-factored Laplace approximation for calibrated in-distribution predictions. The approach works with any pretrained network without requiring spectral normalization during training, and effectively truncates the network at an "effective depth" where features retain desirable geometric properties for uncertainty quantification.

## Key Results
- PSCs achieve UQ and OOD detection performance comparable to or better than spectral normalization approaches without requiring network retraining
- The method works effectively on architectures without residual connections, expanding applicability beyond previous deterministic UQ methods
- PSCs maintain competitive accuracy while significantly improving expected calibration error (ECE) compared to base models
- Tucker decomposition preserves sensitivity and smoothness properties of intermediate features while enabling efficient low-dimensional representation

## Why This Works (Mechanism)

### Mechanism 1: Neural Collapse Metrics as Proxies for Sensitivity and Smoothness
NC1 (within-class variance ratio) and NC4 (nearest-centroid accuracy) identify intermediate layers where features retain variability and preserve semantic similarity, satisfying the informal bi-Lipschitz constraint needed for UQ without requiring spectral normalization training.

### Mechanism 2: Effective Depth Truncation via Intermediate Layer Bypass
Without spectral normalization, collapse occurs at intermediate layers, but pre-collapse layers retain desirable geometric properties. PSCs effectively truncate the network at this "effective depth," yielding uncertainty quantification properties comparable to SN-trained networks.

### Mechanism 3: Low-Rank Projection Preserves Feature Geometry
Tucker decomposition projects high-dimensional intermediate representations to low dimensions while preserving the sensitivity and smoothness properties needed for uncertainty quantification, as demonstrated by stable NC1 and NC4 metrics post-projection.

## Foundational Learning

### Concept: Bi-Lipschitz Constraint for Feature Extractors
- **Why needed here**: UQ requires feature extractors where L1·dX(x1,x2) ≤ ||h(x1)-h(x2)|| ≤ L2·dX(x1,x2); both bounds matter for distinct inputs mapping to distinct features and similar inputs mapping to similar features
- **Quick check question**: If a feature extractor has L1 ≈ 0 (near-zero sensitivity), what happens when two semantically different inputs pass through the network?

### Concept: Neural Collapse Phenomena (NC1–NC4)
- **Why needed here**: NC1 and NC4 serve as practical diagnostics for layer selection in PSCs; understanding their measurement and relationship to bi-Lipschitz properties is essential for implementation
- **Quick check question**: What does NC1 < 0.2 indicate about a layer's feature representations, and why is this the paper's "collapse cutoff"?

### Concept: Tucker Decomposition and Tensor Mode Products
- **Why needed here**: The projection step uses Tucker decomposition with mode products to reduce dimensionality while preserving structure; understanding factor matrices and core tensor is necessary for implementation
- **Quick check question**: Why does Tucker decomposition (vs. standard PCA) allow independent control over channel dimension reduction and spatial dimension reduction?

## Architecture Onboarding

### Component Map:
Pretrained Network → Layer Scan (NC1/NC4 on validation set) → Candidate Layer(s)
                                                                       ↓
                              Combine & Reshape (concat if multiple layers)
                                                                       ↓
                              Tucker Projection (fit A, B factor matrices)
                                                                       ↓
                              Feature Vector Z ∈ R^(c_proj × d_proj)
                                                                       ↓
                                 ┌─────────────┴─────────────┐
                                 ↓                           ↓
                           GDA Model                  Linear + K-FAC Laplace
                        (feature density p(z)         (predictive distribution
                         for OOD detection)            for in-distribution UQ)

### Critical Path:
1. **Layer selection**: Compute NC1 and NC4 at each layer using validation set; select layer(s) with highest NC4 among those with NC1 > 0.2
2. **Projection fitting**: Two passes through training data—first for channel-wise means, second for channel-wise covariances; then compute Tucker decomposition to obtain factor matrices A and B
3. **PSC fitting**: Fit GDA (class-conditional Gaussians) for OOD scoring; fit linear classifier with KFAC Laplace approximation for calibrated predictions

### Design Tradeoffs:
- **c_proj and d_proj selection**: Lower values accelerate inference but risk inducing collapse; paper recommends smallest values where NC1/NC4 don't change dramatically
- **Layer depth vs. accuracy**: Earlier layers have higher NC1 (more sensitivity) but potentially lower NC4; method trades off by maximizing NC4 subject to NC1 > 0.2
- **Combining adjacent layers**: If NC1 ≈ 0.2 at boundary, combining layers before and after can provide robustness but increases projection dimensionality

### Failure Signatures:
1. **All layers collapsed (NC1 < 0.2)**: No suitable candidate layer exists; consider architectures with residual connections or SN training
2. **OOD and in-distribution feature densities overlap significantly**: Projection too aggressive or layer selection incorrect—re-examine NC1/NC4 at chosen layer
3. **ECE worse than base model**: Laplace approximation may be miscalibrated; verify KFAC covariance estimation

### First 3 Experiments:
1. **NC1/NC4 layer profiling**: For your target pretrained network, compute and plot NC1 and NC4 across all layers on validation set. Identify candidate layer region(s) where NC1 > 0.2 and NC4 is maximized.
2. **Projection dimension sweep**: At selected candidate layer, sweep c_proj ∈ {10, 25, 50, 100} and d_proj ∈ {5, 10, 15}, recomputing NC1/NC4 after projection. Identify stable plateau region before metrics degrade.
3. **OOD detection benchmark**: Compare AUROC for OOD detection (e.g., CIFAR-10 vs. SVHN or Fashion-MNIST) across three configurations: base model, PSC-augmented model, and SN-trained baseline. Expect PSC ≈ SN > base.

## Open Questions the Paper Calls Out

### Open Question 1
Can Probabilistic Skip Connections be effectively extended to tasks such as reinforcement learning or time-series forecasting?
The current work validates the method solely on classification tasks, though the conclusion explicitly identifies extending to other UQ tasks as future work.

### Open Question 2
How can the identification of intermediate layers maintaining feature sensitivity and smoothness be refined to improve robustness across diverse architectures?
The current method relies on specific neural collapse metrics (NC1 and NC4) as proxies, which may not be optimal for all network types, and the conclusion calls for refining these techniques.

### Open Question 3
Is it possible to standardize projection dimensions for common architectures to create a true drop-in replacement without parameter sweeping?
Currently, projection dimensions (c_proj and d_proj) must be chosen empirically, and the conclusion identifies standardizing these dimensions for common architectures as a key limitation.

## Limitations

- **Projection dimension selection**: The method requires finding "smallest values such that neither NC1 nor NC4 change dramatically" but provides no systematic guidance or default values, creating significant reproducibility challenges
- **Universal collapse risk**: The approach fundamentally cannot work if all intermediate layers exhibit NC1 < 0.2, which may occur in very deep networks without residual connections or in overtrained models
- **Theoretical foundation gaps**: While empirical validation is strong, the connection between NC1/NC4 metrics and the formal bi-Lipschitz constraint remains heuristic rather than rigorously proven

## Confidence

- **High confidence**: Core empirical findings (PSC performance comparable to SN without retraining, effectiveness on non-residual architectures) are well-supported by extensive experiments across multiple datasets and network architectures
- **Medium confidence**: The mechanism explaining why NC1/NC4 identify suitable layers is plausible and supported by empirical observations, but the theoretical connection to bi-Lipschitz properties requires further formalization
- **Medium confidence**: Tucker decomposition preserves feature geometry as claimed, with empirical validation showing stable NC metrics post-projection, though specific advantages over simpler dimensionality reduction methods could be better justified

## Next Checks

1. **Universal collapse diagnosis**: Systematically test PSC layer selection across architectures of varying depth (ResNet-18 through ResNet-50) to establish failure boundaries and identify when SN training becomes necessary
2. **Projection sensitivity analysis**: Quantify the impact of c_proj and d_proj selection on UQ performance by systematically varying these parameters and measuring degradation points for NC metrics and downstream UQ metrics
3. **Mechanism isolation experiment**: Compare PSC performance when using alternative layer selection criteria (e.g., mutual information maximization, gradient magnitude) versus the NC1/NC4 approach to isolate the specific contribution of the neural collapse diagnostics