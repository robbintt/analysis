---
ver: rpa2
title: 'LLMs for energy and macronutrients estimation using only text data from 24-hour
  dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt'
arxiv_id: '2509.13268'
source_url: https://arxiv.org/abs/2509.13268
tags:
- dietary
- data
- energy
- page
- hour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) without image inputs poorly estimate\
  \ energy and macronutrients from text-based dietary recalls. Using a 10-shot chain-of-thought\
  \ prompt and parameter-efficient fine-tuning (PEFT) on 1,129 adolescent dietary\
  \ records, an open-source LLM improved substantially: mean absolute error for energy\
  \ dropped from 652 kcal to 171-191 kcal across test sets, and Lin\u2019s concordance\
  \ correlation coefficient rose from 0.46 to over 0.89 for all nutrients."
---

# LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt

## Quick Facts
- arXiv ID: 2509.13268
- Source URL: https://arxiv.org/abs/2509.13268
- Reference count: 40
- Major finding: Fine-tuned open-source LLMs accurately estimate energy and macronutrients from plain-text dietary recalls, reducing MAE from 652 kcal to 171-191 kcal.

## Executive Summary
This study demonstrates that large language models, when fine-tuned using parameter-efficient methods and a 10-shot chain-of-thought prompt, can accurately estimate energy and macronutrient intake from text-based 24-hour dietary recalls. The approach eliminates the need for image inputs or detailed nutrient databases, making dietary assessment more scalable and less burdensome. Using a dataset of 1,129 adolescent dietary records, the model achieved substantial improvements in prediction accuracy, with Lin’s concordance correlation coefficients exceeding 0.89 for all nutrients.

## Method Summary
The study employed a parameter-efficient fine-tuning (PEFT) approach on an open-source LLM using 1,129 adolescent 24-hour dietary recall records. A 10-shot chain-of-thought prompting strategy was used to guide the model in estimating energy and macronutrient values from plain-text food descriptions. The model was evaluated on multiple test sets, comparing performance before and after fine-tuning using metrics such as mean absolute error (MAE) and Lin’s concordance correlation coefficient. Bland-Altman plots were used to assess agreement and bias.

## Key Results
- MAE for energy estimation dropped from 652 kcal to 171-191 kcal after fine-tuning.
- Lin’s concordance correlation coefficient improved from 0.46 to over 0.89 for all nutrients.
- Bland-Altman analysis showed good agreement without systematic bias across test sets.

## Why This Works (Mechanism)
The success of this approach stems from the combination of parameter-efficient fine-tuning, which adapts the model to the specific task without full retraining, and chain-of-thought prompting, which provides structured reasoning steps for nutrient estimation. By leveraging only text data, the method bypasses the need for image inputs or detailed nutrient databases, reducing complexity and improving scalability. The fine-tuning process allows the model to learn patterns in dietary descriptions and map them to accurate nutrient values.

## Foundational Learning
1. **Parameter-efficient fine-tuning (PEFT)**: Adapts pre-trained models to new tasks with minimal parameter updates, reducing computational cost. *Why needed*: Enables efficient adaptation without full model retraining. *Quick check*: Verify parameter count changes before and after fine-tuning.
2. **Chain-of-thought prompting**: Guides models through step-by-step reasoning for complex tasks. *Why needed*: Improves accuracy in nutrient estimation by structuring the reasoning process. *Quick check*: Test model outputs with and without chain-of-thought prompts.
3. **Bland-Altman plots**: Assess agreement between predicted and actual values while identifying systematic bias. *Why needed*: Validates model performance beyond simple error metrics. *Quick check*: Plot residuals to check for patterns or bias.

## Architecture Onboarding
**Component Map**: Text input -> LLM with PEFT -> Nutrient estimates -> Evaluation metrics (MAE, CCC, Bland-Altman)
**Critical Path**: Text preprocessing -> 10-shot chain-of-thought prompt -> Parameter-efficient fine-tuning -> Model evaluation
**Design Tradeoffs**: Using only text data reduces complexity but may miss visual cues like portion sizes. PEFT balances adaptation accuracy with computational efficiency.
**Failure Signatures**: High MAE or low CCC indicates poor adaptation; systematic bias in Bland-Altman plots suggests model miscalibration.
**3 First Experiments**:
1. Test model performance on adult dietary records from a different region.
2. Compare LLM predictions against dietitian-calculated values using the same text inputs.
3. Evaluate model sensitivity to variations in prompt structure or length.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are constrained by the adolescent sample and short recall period.
- The 10-shot prompting strategy may not generalize across diverse dietary contexts or languages without adaptation.
- Absence of image inputs limits the model’s ability to capture portion sizes or visual cues.

## Confidence
- **High Confidence**: Improvements in MAE and concordance correlation coefficients post-fine-tuning are robust and internally validated.
- **Medium Confidence**: Generalizability to adult populations or other dietary assessment methods remains untested.
- **Low Confidence**: Long-term stability of model performance and sensitivity to prompt variations are not yet established.

## Next Checks
1. External validation on adult dietary records from different geographic regions and cultural contexts.
2. Comparison of LLM predictions against dietitian-calculated values using the same text inputs.
3. Assessment of model performance when integrated with user-generated text from mobile dietary tracking apps.