---
ver: rpa2
title: Improving LLM Leaderboards with Psychometrical Methodology
arxiv_id: '2501.17200'
source_url: https://arxiv.org/abs/2501.17200
tags:
- https
- factor
- benchmarks
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces psychometric modeling\u2014particularly\
  \ Confirmatory Factor Analysis (CFA)\u2014to improve the ranking of large language\
  \ models (LLMs) on leaderboards. The key innovation is treating benchmark scores\
  \ as parcels of items and using CFA to extract common variance and reduce measurement\
  \ noise, producing more reliable and interpretable LLM ability estimates than simple\
  \ averages."
---

# Improving LLM Leaderboards with Psychometrical Methodology

## Quick Facts
- arXiv ID: 2501.17200
- Source URL: https://arxiv.org/abs/2501.17200
- Authors: Denis Federiakin
- Reference count: 0
- Primary result: CFA on benchmark parcels produces more reliable LLM ability estimates than simple averaging, revealing plateauing scaling trends

## Executive Summary
This paper introduces psychometric modeling—particularly Confirmatory Factor Analysis (CFA)—to improve the ranking of large language models (LLMs) on leaderboards. The key innovation is treating benchmark scores as parcels of items and using CFA to extract common variance and reduce measurement noise, producing more reliable and interpretable LLM ability estimates than simple averages. The method was applied to two versions of the Hugging Face Open LLM Leaderboard, involving thousands of models across six benchmarks each.

The analysis revealed that while raw averages tend to overestimate weak models and underestimate strong ones (showing an inverted U-shaped trend), factor scores provide a more stable and accurate ranking. Additionally, the study found that LLM performance scaling is plateauing—further increases in model size yield diminishing returns—more clearly visible in factor scores than in benchmark averages. CFA also revealed interpretable residual dependencies between benchmarks, reflecting nuanced relationships in LLM abilities.

## Method Summary
The approach transforms benchmark scores via logit transformation to unbounded scale, then fits unidimensional CFA using MLR estimator in lavaan. The model iteratively adds residual covariances via modification indices to improve fit. MAP factor scores are extracted as latent ability estimates, with McDonald's ω measuring reliability. The method was applied to Hugging Face Open LLM Leaderboard v.1 (3,792 models, 6 benchmarks) and v.2 (1,543 models, 6 benchmarks), with optional normalization to correct for guessing.

## Key Results
- Raw benchmark averages show inverted U-shaped trend (overestimate weak, underestimate strong models), while factor scores provide stable rankings
- Reliability of factor scores improved from 0.58 (raw data) to 0.79 (normalized data), indicating better signal-to-noise separation
- LLM performance scaling plateau is more evident in factor scores than benchmark averages, suggesting diminishing returns from model size increases
- Residual covariances reveal interpretable dependencies: negative correlation between HellaSwag and TruthfulQA suggests conflict between common sense and factual accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CFA on benchmark parcels produces more reliable LLM ability estimates than simple averaging by extracting common signal and filtering noise.
- Mechanism: CFA models observed benchmark scores as manifestations of a latent "general ability" factor plus benchmark-specific residuals. Factor scores weight each benchmark by its factor loading and discriminability, reducing influence of noisy benchmarks.
- Core assumption: A single latent factor underlies performance across diverse benchmarks, with residuals capturing benchmark-specific noise.
- Evidence anchors: Abstract confirms key innovation; p. 27-28 shows factor scores reveal scaling slowdown better than averages.
- Break condition: If benchmarks measure substantively independent abilities, unidimensional CFA will show poor fit (CFI < 0.9, RMSEA > 0.1).

### Mechanism 2
- Claim: Parceling enables psychometric analysis when items vastly outnumber LLMs.
- Mechanism: Benchmark-level averages serve as continuous observed variables in CFA. Continuous Response Model with logit transformation maps bounded scores to unbounded latent responses suitable for FA.
- Core assumption: Within-benchmark items are sufficiently homogeneous that their average retains interpretable variance structure.
- Evidence anchors: p. 10-11 describes parceling approach and CRM derivations with logit transformation.
- Break condition: If within-benchmark items are highly heterogeneous, parceling obscures structure and produces misleading factor loadings.

### Mechanism 3
- Claim: Residual covariances reveal interpretable dependencies between benchmarks not captured by general ability factor.
- Mechanism: After fitting unidimensional model, modification indices identify benchmark pairs with systematically correlated residuals. Adding these parameters improves fit and exposes substantive relationships.
- Core assumption: Residual correlations reflect meaningful shared variance (e.g., overlapping skill requirements) rather than pure noise.
- Evidence anchors: p. 20-26 interprets negative HellaSwag–TruthfulQA correlation and positive MMLU–GSM8K correlation as substantive relationships.
- Break condition: If residual correlations are driven by data artifacts (shared prompting strategies, annotation biases), substantive interpretation is invalid.

## Foundational Learning

- **Latent Variable Models (Factor Analysis / IRT)**: Understanding that observed scores are modeled as functions of unobserved latent variables plus error. Quick check: Can you explain why factor loadings differ across benchmarks and what a low loading implies about that benchmark's relationship to the latent factor?

- **Psychometric Reliability (McDonald's ω)**: Understanding that reliability measures signal-to-noise ratio in factor scores, not consistency of individual items. Quick check: If reliability is 0.58, approximately what proportion of variance in observed scores is attributable to the latent factor versus noise?

- **Model Fit Indices (CFI, TLI, RMSEA, SRMR)**: Understanding how to interpret whether a model (CFI=0.965, RMSEA=0.134) is acceptable or problematic. Quick check: What does SRMR < 0.05 indicate about the discrepancy between observed and model-implied covariance matrices?

## Architecture Onboarding

- **Component map**: Data layer (benchmark scores → optional normalization) → Transformation layer (logit transform) → Model layer (unidimensional CFA with MLR) → Estimation layer (MAP factor scores) → Output layer (factor scores for ranking)

- **Critical path**: 1) Retrieve benchmark scores for all LLMs 2) Apply logit transformation 3) Fit initial unidimensional CFA with MLR 4) Iteratively add residual covariances 5) Extract MAP factor scores 6) Compare factor-score rankings to naive averages

- **Design tradeoffs**: Unidimensional vs. multi-dimensional models (single-factor pragmatic but may mask ability structure); Raw vs. normalized scores (normalized improves reliability but complicates interpretation); Parceling vs. item-level modeling (parceling enables tractable analysis but loses item-level diagnostics)

- **Failure signatures**: Negative residual variance (Heywood case, model overfitted); Poor fit despite high loadings (RMSEA > 0.2 suggests unidimensional assumption strained); Reliability < 0.6 (factor scores too noisy for meaningful differentiation)

- **First 3 experiments**: 1) Reproduce Study 2b on current leaderboard data to validate pipeline 2) Test measurement invariance across model architectures to check if single "general ability" factor applies across families 3) Cross-leaderboard linking via anchor benchmarks to establish common ability scale

## Open Questions the Paper Calls Out
None

## Limitations
- The single-factor model assumption may mask important ability distinctions between LLMs, as high inter-factor correlations would cause convergence issues in multi-dimensional models
- The approach is highly sensitive to the quality and consistency of benchmark scoring, as systematic biases in original scoring would propagate through the psychometric analysis
- The generalizability to leaderboards with different benchmark compositions or non-English language models is unclear due to the paper's focus on specific Hugging Face leaderboards

## Confidence
**High Confidence**: The methodological framework of applying CFA to benchmark parcels is sound and well-established in psychometrics. The transformations and MAP estimation follow standard practice. The observed improvement in reliability is statistically meaningful and reproducible.

**Medium Confidence**: The substantive interpretations of residual covariances are plausible but not definitively established. Alternative explanations like shared annotation biases are not ruled out.

**Low Confidence**: The generalizability of the approach to leaderboards with different benchmark compositions or to non-English language models is uncertain due to the paper's specific focus.

## Next Checks
1. **Cross-Architecture Measurement Invariance**: Test whether factor loadings differ significantly across major LLM architecture families. If they do, the "general ability" factor may not be invariant, undermining ranking validity.

2. **Out-of-Sample Prediction**: Apply the fitted CFA model from one leaderboard version to predict rankings on a held-out subset or different leaderboard. Measure correlation between predicted and actual factor scores to assess generalization.

3. **Benchmark Contribution Sensitivity Analysis**: Systematically remove individual benchmarks from the CFA and measure impact on model fit and factor score rankings. Identify whether certain benchmarks disproportionately drive results or whether factor scores remain stable when any single benchmark is excluded.