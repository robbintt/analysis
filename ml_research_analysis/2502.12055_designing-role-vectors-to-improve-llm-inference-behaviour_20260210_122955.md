---
ver: rpa2
title: Designing Role Vectors to Improve LLM Inference Behaviour
arxiv_id: '2502.12055'
source_url: https://arxiv.org/abs/2502.12055
tags:
- role
- arxiv
- performance
- activation
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces role vectors as a method to steer LLM behavior
  by directly manipulating internal activations. Role vectors are derived from difference-in-means
  vectors between role-specific and generic prompts.
---

# Designing Role Vectors to Improve LLM Inference Behaviour

## Quick Facts
- arXiv ID: 2502.12055
- Source URL: https://arxiv.org/abs/2502.12055
- Authors: Daniele Potertì; Andrea Seveso; Fabio Mercorio
- Reference count: 11
- Primary result: Role vectors derived from activation-space difference-in-means can improve LLM performance on domain-specific tasks, with only 16% of beneficial directions aligning with intended roles.

## Executive Summary
This work introduces role vectors as a method to steer LLM behavior by directly manipulating internal activations. Role vectors are derived from difference-in-means vectors between role-specific and generic prompts. Two intervention methods are used: activation addition, which reinforces role-specific directions, and directional ablation, which removes them. Results show that role vectors can improve model performance on domain-specific tasks, with larger models exhibiting more interpretable directional signals. On average, only 16% of directions that improve performance align with the intended role. Ablating optimal directions leads to performance degradation in the target domain but sometimes improves unrelated domains.

## Method Summary
The methodology extracts role vectors by computing the difference-in-means between activations from role-specific prompts and generic prompts across transformer layers. For each role, 128 prompts are generated using PersonaHub personas via Claude 3.5 Haiku, contrasted against 128 generic prompts from Alpaca. Role vectors are then applied through activation addition (reinforcing role-specific directions with scaling coefficient α) or directional ablation (removing role-specific directions via orthogonal projection). A grid search over layers (restricted to first 80%), positions, and α values identifies optimal intervention parameters, which are evaluated on MMLU benchmark splits across 8 domain categories.

## Key Results
- Role vectors improve performance on domain-specific tasks, with Qwen-7B-Chat showing +25% improvement for mathematician role on math split
- Only 16% of directions that improve performance are interpretable as the intended role via patch scoping
- Ablating optimal directions causes performance degradation in target domains but sometimes improves unrelated domains
- Larger models exhibit more interpretable directional signals in activation space

## Why This Works (Mechanism)

### Mechanism 1: Difference-in-Means Direction Extraction
- Claim: Contrasting role-specific activations against generic activations isolates direction vectors that encode domain-relevant behavioral tendencies.
- Mechanism: For each role r, compute mean activations μ for role-specific prompts and ν for generic prompts; the vector d = μ − ν represents the directional shift in activation space associated with that role.
- Core assumption: Role-relevant features are encoded as linear directions in activation space that can be isolated via contrastive averaging.
- Evidence anchors:
  - [abstract] "Role vectors are derived from difference-in-means vectors between role-specific and generic prompts."
  - [Section 3.2] Equations 1-2 formalize the difference-in-means computation for extracting role-specific directions.
- Break condition: If role-specific and generic prompts activate overlapping features without clear separation, difference-in-means yields noisy or non-directional vectors.

### Mechanism 2: Activation Addition as Steering
- Claim: Adding scaled direction vectors to the residual stream shifts model behavior toward role-specific outputs.
- Mechanism: Given direction d and scaling coefficient α, modify activations via x′ ← x + αd.
- Core assumption: The extracted direction is causally relevant to role performance, not merely correlated.
- Evidence anchors:
  - [Section 4, Table 3] Performance improvements shown for Qwen-7B-Chat with α = 3.0 exceeding baseline in target domains.
- Break condition: If α is too large, text generation quality degrades; if direction is misaligned with true causal features, addition introduces noise.

### Mechanism 3: Directional Ablation as Causal Validation
- Claim: Removing a direction via orthogonal projection tests whether that direction is causally necessary for domain performance.
- Mechanism: Project out the target direction from all activations via x′ ← x − d(d⊤x).
- Core assumption: The direction encodes information that cannot be easily recovered from other activations.
- Evidence anchors:
  - [Section 4, Table 5] Ablating optimal directions causes performance degradation in target domains.
  - [Section 4] "Ablating optimal directions leads to performance degradation in the target domain but sometimes improves unrelated domains."
- Break condition: If the model has redundant representations of the same feature, ablation effects will be muted or unpredictable.

## Foundational Learning

- Concept: Residual stream architecture in decoder-only transformers
  - Why needed here: Role vectors are injected into and ablated from the residual stream; understanding how information flows through layers is essential for selecting intervention points.
  - Quick check question: At which layer would you expect high-level semantic features like "role" to be most prominently encoded—early, middle, or late layers?

- Concept: Difference-in-means as a contrastive representation method
  - Why needed here: The core extraction method relies on subtracting baseline activations from role-conditioned activations to isolate task-relevant directions.
  - Quick check question: If your role-specific and generic datasets have different token distributions, how might this bias the extracted direction?

- Concept: Orthogonal projection for directional ablation
  - Why needed here: Ablation requires removing a direction while preserving all orthogonal components; understanding projection is critical for correct implementation.
  - Quick check question: Why is the formula x − d(d⊤x) used instead of simply subtracting d?

## Architecture Onboarding

- Component map: Dataset generator using PersonaHub personas → Activation extraction module computing mean activations → Difference-in-means calculator producing 29 role vectors → Intervention engine applying activation addition or directional ablation → Evaluation loop testing on MMLU benchmark splits.

- Critical path: Dataset generation → Activation extraction (run inference on role-specific and generic prompts) → Compute difference-in-means vectors → Grid search over (layer, position, α) to find optimal intervention → Evaluate on domain-matched test splits.

- Design tradeoffs: Higher α values (3.0 vs 1.0) tend to improve target-domain performance but risk degrading generation quality; restricting to first 80% of layers avoids unembedding interference but may miss late-layer semantic features; patch scoping evaluation shows only 16% of performant directions are interpretable as the intended role.

- Failure signatures: (1) Performance improvements in target domain accompanied by severe degradation in unrelated domains indicates overfitting to direction; (2) Ablation causes no performance change suggests direction is redundant or non-causal; (3) Patch scoping reveals direction encodes unintended concept indicates extracted vector captured spurious correlations.

- First 3 experiments:
  1. Replicate difference-in-means extraction for a single role (e.g., "mathematician") on a small model, visualizing activation distributions at multiple layers to confirm directional separation exists.
  2. Run grid search over layers [10, 20, 30] and α ∈ [1.0, 3.0] for activation addition, measuring both target-domain accuracy and perplexity on a held-out corpus.
  3. Apply directional ablation for the best-performing direction and compare performance drops across all 8 domain splits to verify specificity to the intended domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the mechanistic components or superposed features within role vectors that cause performance improvements even when the vector does not semantically align with the target role?
- Basis in paper: [explicit] The authors state in the Conclusion and Future Work: "Future work will study this mechanistically using Activation Patching (Causal Mediation Analysis) techniques using SAE features... to explain this phenomenon better."
- Why unresolved: The results show that only 16% of directions that improve performance are interpreted as the intended role via patch-scoping, leaving the causal mechanism of the remaining 84% unexplained.
- What evidence would resolve it: Identification of specific Sparse Autoencoder (SAE) features or causal circuits within the "non-interpretable" vectors that correlate with correct reasoning in the target domain.

### Open Question 2
- Question: To what extent does the application of role vectors inadvertently reinforce harmful biases or induce overconfidence in the model?
- Basis in paper: [explicit] The authors acknowledge in the Limitations section: "Steering models using role vectors may inadvertently reinforce biases or lead to over-confidence in certain domains. Careful evaluation will be conducted in future works..."
- Why unresolved: The current study focuses exclusively on performance metrics (MMLU accuracy) and does not evaluate safety alignment or bias levels post-intervention.
- What evidence would resolve it: Evaluation of steered models on bias benchmarks (e.g., BBQ or CrowS-Pairs) to measure changes in stereotypical or biased outputs compared to the baseline.

### Open Question 3
- Question: Do role vectors derived from instruction-tuned models transfer effectively to base models or models of significantly larger scale?
- Basis in paper: [inferred] The methodology explicitly excludes base models ("We do not consider base versions of the models"), and the Limitations note results might differ in "untested models, especially larger ones."
- Why unresolved: It is unclear if the "role directions" identified are intrinsic to the pre-training data or artifacts of the instruction-tuning alignment process.
- What evidence would resolve it: Applying the computed role vectors to the corresponding base versions of the evaluated models (e.g., Llama 3.1 base) and measuring performance changes.

## Limitations

- Only 16% of optimal directions align with intended roles, suggesting many successful interventions capture unintended concepts
- Grid search methodology may overfit to evaluation set by selecting optimal (layer, position) pairs based on test performance
- Evaluation framework using MMLU splits conflates task difficulty with role-specific expertise
- Optimal directions often harm unrelated domains, indicating capture of brittle, non-robust features

## Confidence

*High Confidence:* The methodological framework for extracting and applying role vectors is clearly specified and reproducible. The directional ablation technique provides causal evidence that extracted vectors encode task-relevant information.

*Medium Confidence:* The claim that role vectors can improve domain-specific performance is supported but potentially overfit to the MMLU evaluation setup. The interpretation that 16% alignment represents "limited interpretability" is reasonable but could also indicate the method captures valuable non-obvious features.

*Low Confidence:* The paper's broader claims about scalable, reliable behavior steering using role vectors are not well-supported. The finding that optimal directions often harm unrelated domains raises serious questions about practical applicability.

## Next Checks

1. **Cross-validation robustness test**: Implement k-fold cross-validation on the role-specific and generic prompt datasets to assess whether the optimal (layer, position, α) combinations generalize across different data splits, addressing potential overfitting concerns.

2. **Zero-shot transfer evaluation**: Test whether role vectors optimized for one domain (e.g., mathematician) provide zero-shot benefits on semantically related but distinct domains (e.g., physicist), or whether the observed improvements are domain-specific artifacts.

3. **Alternative representation analysis**: Compare difference-in-means vectors against other representation learning techniques (e.g., contrastive learning, PCA) to determine whether the linear separability assumption is optimal or whether nonlinear methods might capture more robust role-relevant features.