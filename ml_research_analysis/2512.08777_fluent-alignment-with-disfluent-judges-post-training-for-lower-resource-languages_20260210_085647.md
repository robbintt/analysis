---
ver: rpa2
title: 'Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages'
arxiv_id: '2512.08777'
source_url: https://arxiv.org/abs/2512.08777
tags:
- language
- training
- fluency
- norwegian
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel method for aligning language models
  in lower-resource languages using on-policy reinforcement learning, which avoids
  exposure to disfluent text during training. The approach leverages a pre-trained
  base model fine-tuned with short English SFT, then aligned using online on-policy
  RL where the model learns from its own generated responses without relying on any
  instruction-tuning dataset in the target language.
---

# Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages

## Quick Facts
- arXiv ID: 2512.08777
- Source URL: https://arxiv.org/abs/2512.08777
- Reference count: 40
- Primary result: On-policy RL alignment without translated text produces more fluent models than supervised fine-tuning on translated data

## Executive Summary
This paper addresses the challenge of aligning large language models in lower-resource languages while maintaining native-level fluency. The authors propose a novel method that avoids exposure to disfluent translated text during training by using on-policy reinforcement learning with a native-language judge model. The approach fine-tunes a base model on short English SFT data, then aligns it using online RL where the model learns from its own generated responses. Human evaluation with native Norwegian speakers demonstrates that this method produces significantly more fluent models compared to supervised fine-tuning on translated data, while crucially avoiding any exposure to translated text during training.

## Method Summary
The method employs on-policy reinforcement learning to align language models in lower-resource languages without exposing them to disfluent translated text. The process begins with a pre-trained base model (Llama-3.1-8B) fine-tuned on short English SFT data. During alignment, the model generates responses that are evaluated by a native-language judge model (also fine-tuned on English SFT), which provides rewards based on fluency and relevance. The key innovation is that the policy model learns exclusively from its own generated responses, never seeing any translated instruction-tuning data. This approach leverages the fact that alignment can be performed using disfluent judge models to guide fluent policy models, as long as the judge understands the target language.

## Key Results
- On-policy RL method achieved 79.7% win-rate against baseline models in human evaluation
- Models trained on translated SFT data scored only 60.0% win-rate against baselines
- Minimal exposure to translated text (10 examples) measurably degraded fluency compared to complete avoidance
- Multilingual baseline performed poorly with only 10.3% win-rate in human evaluation

## Why This Works (Mechanism)
The method works by separating the alignment process from exposure to disfluent training data. By using on-policy RL, the model learns to generate fluent responses based on rewards from a native-language judge, without ever seeing the potentially harmful translated instruction-tuning data. This creates a curriculum where the model gradually improves its fluency while avoiding the negative transfer that occurs when training on translated text.

## Foundational Learning
- On-policy reinforcement learning: Why needed - Enables learning from model's own outputs without requiring high-quality instruction data in target language; Quick check - Model should improve reward scores over training iterations
- Judge model alignment: Why needed - Provides evaluation criteria in target language without requiring large native instruction datasets; Quick check - Judge should consistently identify fluent vs disfluent responses
- Language transfer learning: Why needed - Allows base model to acquire alignment capabilities from English SFT before applying to target language; Quick check - Model should generate coherent responses in target language after English fine-tuning

## Architecture Onboarding

Component map:
Base model (Llama-3.1-8B) -> English SFT fine-tuning -> On-policy RL alignment -> Target language judge evaluation

Critical path:
English SFT → On-policy RL with judge → Fluency evaluation

Design tradeoffs:
- Judge quality vs. training stability: Higher-quality judges provide better feedback but may be harder to align
- Training duration vs. fluency: Longer training improves fluency but increases computational cost
- Base model size vs. resource requirements: Larger models generalize better but require more compute

Failure signatures:
- Judge collapse: If judge model becomes overly strict or inconsistent, policy learning degrades
- Exposure contamination: Any exposure to translated text during training measurably reduces fluency
- Reward hacking: Policy may exploit judge weaknesses rather than improving genuine fluency

First experiments:
1. Verify base model can generate coherent responses in target language after English SFT
2. Test judge model's ability to distinguish fluent vs disfluent responses
3. Run small-scale on-policy RL training to confirm learning signal is effective

## Open Questions the Paper Calls Out
The study identifies several key uncertainties that require further investigation. The generalizability of the disfluency-avoidance principle across typologically diverse languages remains unproven, as the experiments only cover Norwegian Bokmål. The human evaluation methodology, while rigorous, involved only 10 native speakers, raising questions about statistical power and potential cultural bias in fluency assessments. Additionally, the choice of Llama-3.1-8B as the base model limits understanding of how the approach scales to different model sizes and architectures.

## Limitations
- Results only validated for Norwegian Bokmål, limiting cross-linguistic generalizability
- Human evaluation sample size of 10 native speakers may not provide sufficient statistical power
- Single base model architecture (Llama-3.1-8B) prevents conclusions about scaling behavior
- No exploration of judge model size requirements for effective alignment guidance

## Confidence

High confidence:
- On-policy RL with native-language judges produces fluent models without translated data exposure
- Complete avoidance of translated text during training is essential for native-level fluency

Medium confidence:
- Even minimal exposure to translated data measurably degrades fluency
- Disfluent judge models can effectively guide fluent policy training

## Next Checks
1. Replicate experiments with 3-5 additional low-resource languages spanning different language families to test cross-linguistic generalizability
2. Conduct larger-scale human evaluation with 50+ native speakers and include blind testing against commercial language models
3. Perform systematic ablation study varying judge model size and training data quantity to determine minimum viable judge quality