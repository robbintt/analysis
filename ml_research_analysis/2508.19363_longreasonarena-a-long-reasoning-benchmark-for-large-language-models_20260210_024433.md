---
ver: rpa2
title: 'LongReasonArena: A Long Reasoning Benchmark for Large Language Models'
arxiv_id: '2508.19363'
source_url: https://arxiv.org/abs/2508.19363
tags:
- reasoning
- long
- length
- input
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LongReasonArena, a benchmark designed to evaluate
  long reasoning capabilities of Large Language Models (LLMs) by requiring algorithmic
  execution with controllable reasoning steps. Unlike existing long-context benchmarks
  that focus on comprehension of long inputs, LongReasonArena emphasizes active generation,
  structuring, and self-correction in long reasoning processes.
---

# LongReasonArena: A Long Reasoning Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2508.19363
- Source URL: https://arxiv.org/abs/2508.19363
- Reference count: 18
- Primary result: State-of-the-art models achieve only 7.5% accuracy on 1M-token reasoning tasks

## Executive Summary
LongReasonArena introduces a novel benchmark for evaluating long reasoning capabilities of Large Language Models by requiring algorithmic execution with controllable reasoning steps. Unlike existing long-context benchmarks that focus on comprehension of long inputs, LongReasonArena emphasizes active generation, structuring, and self-correction in long reasoning processes. The benchmark categorizes tasks into three difficulty levels (1K, 100K, and 1M tokens of reasoning) based on the number of execution steps in solution code, revealing significant performance drops as reasoning complexity increases.

## Method Summary
The benchmark uses algorithmic problems from LeetCode, evaluating models' ability to execute code and compute final answers through multi-step reasoning. Three difficulty levels are defined by execution line counts: Level 1 (10²-10⁴ lines, ~1K tokens), Level 2 (10⁴-10⁵ lines, ~100K tokens), and Level 3 (10⁵-10⁶ lines, ~1M tokens). Evaluation uses 12 models including reasoning and non-reasoning variants, with accuracy measured as the percentage of correct final answers within \boxed{}. The benchmark employs a Qwen2.5-Coder-32B-Instruct generator to create problem-input pairs and filters out guessable samples.

## Key Results
- Accuracy exhibits a linear decline with respect to the logarithm of expected reasoning steps (R² > 0.9, p < 1e-4)
- DeepSeek-R1 achieves only 7.5% accuracy on the most challenging 1M-token reasoning tasks
- Models discover only 4.9 distinct paths on average in backtracking tasks like Word Search
- Retrieval failures occur even when arithmetic computation is correct, showing models struggle with value tracking during extended reasoning

## Why This Works (Mechanism)

### Mechanism 1: Log-Linear Accuracy Degradation
Model accuracy on algorithmic tasks declines linearly with the logarithm of required reasoning steps. Execution line counts approximate reasoning steps; as steps increase, error probability compounds or retrieval operations fail more frequently.

### Mechanism 2: Retrieval Failure Under Long Reasoning
Retrieval during multi-step reasoning fails more than retrieval in passive long-input tasks. Models must repeatedly access intermediate values across thousands of steps; index misalignment or value skipping occurs as context grows.

### Mechanism 3: Backtracking Limitations in Exploratory Search
Models explore few distinct paths during depth-first search-style reasoning, leading to incomplete backtracking. In tasks like Word Search, models revisit failed paths instead of branching; average distinct paths ~4.9, far below exhaustive search.

## Foundational Learning

**Concept: Execution line count as reasoning proxy**
- Why needed here: Difficulty levels (1K/100K/1M tokens) are defined by execution lines; interpreting results requires understanding this proxy.
- Quick check question: If a sample has 55,000 execution lines, which difficulty level is it?

**Concept: Log-linear scaling**
- Why needed here: Performance degrades predictably; engineers should expect constant accuracy drop per order-of-magnitude step increase.
- Quick check question: If accuracy is 50% at 1K steps, what approximate accuracy do you expect at 100K steps?

**Concept: Retrieval vs. comprehension in long contexts**
- Why needed here: LongReasonArena separates passive comprehension from active retrieval during reasoning.
- Quick check question: Why does strong Needle-in-a-Haystack performance not guarantee success here?

## Architecture Onboarding

**Component map**: Input generator (Qwen2.5-Coder-32B) → problem-input pairs → difficulty classification (Level 1/2/3) → model evaluation → output verification (order-dependent/independent comparators)

**Critical path**: Generating high-coverage inputs; filtering guessable samples; counting execution lines accurately; verifying outputs robustly

**Design tradeoffs**: Execution line proxy may misestimate model-specific strategies; 32K input length cap prioritizes reasoning over comprehension

**Failure signatures**: (1) Index errors—correct values with wrong indices; (2) Full retrieval errors—failure to recall seen values; (3) Repetitive path exploration in backtracking tasks

**First 3 experiments**:
1. Reproduce Two Sum retrieval analysis (array length 10–1000); log accuracy and error types.
2. Run Word Search (word lengths 5–20); measure distinct paths explored per sample.
3. Compare reasoning vs. non-reasoning models on Level 1 vs. Level 3 to quantify the gap.

## Open Questions the Paper Calls Out

**Open Question 1**: How can LLMs be improved to effectively perform retrieval operations within long reasoning chains, given that simple retrieval tasks (like Needle-in-a-Haystack) are easy in long-input contexts but fail when embedded in extended reasoning processes?

**Open Question 2**: Why do models generate longer reasoning traces for incorrect answers than correct ones, and what modifications would enable models to leverage extended reasoning productively?

**Open Question 3**: What architectural or training modifications are necessary to enable effective backtracking in long reasoning chains beyond the ~5 distinct paths currently explored by reasoning models?

## Limitations

- Execution line count proxy may misestimate reasoning complexity and cannot capture model-specific optimization strategies
- Single-codebase generator (Qwen2.5-Coder-32B-Instruct) creates potential bias toward code-generation strengths
- 32K input length cap prioritizes reasoning over comprehension, potentially underestimating models that excel at long-context understanding

## Confidence

**High confidence**: Log-linear accuracy degradation with reasoning steps (R² > 0.9, p < 1e-4); retrieval failure patterns in Two Sum analysis; distinct path limitation in Word Search tasks

**Medium confidence**: Retrieval during reasoning fails more than in passive comprehension tasks; models struggle with backtracking due to lack of explicit state management; reasoning models consistently outperform non-reasoning models

**Low confidence**: Generalization to non-algorithmic reasoning domains; applicability to smaller models not tested; assumption that execution line counts capture all relevant reasoning complexity factors

## Next Checks

**Check 1**: Run the same benchmark problems with a different input generator (e.g., GPT-4o or Claude) and compare execution line distributions to validate the log-linear relationship holds across generators.

**Check 2**: Design a modified Two Sum variant where intermediate values appear multiple times with different indices to test limits of retrieval mechanism beyond simple sequential access.

**Check 3**: Implement a scratchpad mechanism that explicitly tracks visited states and unexplored branches to determine whether backtracking limitations are inherent to LLMs or could be mitigated with better state management.