---
ver: rpa2
title: 'AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented
  Generation'
arxiv_id: '2509.17486'
source_url: https://arxiv.org/abs/2509.17486
tags:
- attention
- documents
- compression
- context
- attncomp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AttnComp introduces an adaptive, efficient, and context-aware compression
  framework for Retrieval-Augmented Generation (RAG). By leveraging the attention
  mechanism of LLMs, it dynamically selects relevant documents based on their cumulative
  attention scores, achieving higher accuracy than both uncompressed baselines and
  existing compression methods.
---

# AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.17486
- Source URL: https://arxiv.org/abs/2509.17486
- Authors: Lvzhou Luo; Yixuan Cao; Ping Luo
- Reference count: 40
- Key outcome: Achieves 17x compression with 1.9 point accuracy gain over uncompressed baseline, reduces latency to 49% of baseline

## Executive Summary
AttnComp introduces an adaptive, efficient, and context-aware compression framework for Retrieval-Augmented Generation (RAG). By leveraging the attention mechanism of LLMs, it dynamically selects relevant documents based on their cumulative attention scores, achieving higher accuracy than both uncompressed baselines and existing compression methods. The framework achieves 17x compression rate while improving accuracy by 1.9 points over the uncompressed baseline, and reduces end-to-end latency to 49% of the uncompressed baseline. It also provides reliable confidence estimates for RAG responses, enabling better assessment of answer trustworthiness.

## Method Summary
AttnComp employs a lightweight compression model that leverages the attention mechanism of LLMs to identify relevant information from retrieved documents. The compressor adds a cross-attention layer to the first L=13 frozen transformer layers of a backbone LLM, which computes query-to-context attention weights. Documents are ranked by their aggregated attention scores and retained until cumulative attention exceeds threshold p=0.95 (Top-P compression). A confidence score is estimated by assessing instruction attention, inversely tracking retrieval quality. The framework dynamically adapts compression rates based on query complexity, retaining fewer documents for single-hop questions and more for multi-hop reasoning tasks.

## Key Results
- Improves accuracy by 1.9 points over uncompressed baseline while achieving 17x compression rate
- Reduces end-to-end latency to 49% of uncompressed baseline on HotpotQA
- Achieves Pearson correlation of 0.35 between confidence scores and answer quality (F1)

## Why This Works (Mechanism)

### Mechanism 1: Middle-Layer Attention Heads Encode Relevance Signals
- Claim: Certain attention heads in middle transformer layers naturally capture query-document relevance without explicit training.
- Core assumption: Attention patterns learned during pretraining generalize to retrieval relevance detection.
- Evidence: HotpotQA results show attention heads in middle layers consistently focus on supporting evidence; HotpotQA averaged 7.5 documents retained vs 3.7 for PopQA.

### Mechanism 2: Top-P Cumulative Threshold Enables Adaptive Compression Rates
- Claim: Retaining documents until cumulative attention exceeds threshold p produces compression rates that automatically scale with information density.
- Core assumption: Attention score distribution approximates true relevance distribution.
- Evidence: Dynamic document retention (0 to 23 documents) across query types; HotpotQA averaged 7.5 documents, PopQA averaged 3.7.

### Mechanism 3: Instruction Attention Correlates with Retrieval Quality
- Claim: After fine-tuning, attention allocated to instruction token serves as proxy for overall retrieval relevance.
- Core assumption: Fine-tuning successfully trains model to use instruction attention as "escape hatch" when retrieval fails.
- Evidence: Pearson correlation of 0.35 between confidence and F1; instances with confidence below 0.1 yield F1 of 0.13, above 0.9 yield F1 of 0.91.

## Foundational Learning

- **Cross-attention mechanisms in transformers**
  - Why needed: AttnComp's compressor adds cross-attention layer that computes query-to-context relevance.
  - Quick check: Given query hidden states X_q and context hidden states X_c, which matrix multiplication produces attention scores before softmax?

- **Cumulative distribution-based selection (Top-P/nucleus sampling)**
  - Why needed: Top-P compression algorithm adapts technique from text generation to document selection.
  - Quick check: If documents have attention scores [0.4, 0.3, 0.2, 0.1] and p=0.7, which documents are retained?

- **Attention sinks and initial token bias**
  - Why needed: Section 3 documents LLMs assign disproportionate attention to initial tokens when context is irrelevant.
  - Quick check: Why might an LLM attend strongly to first token of irrelevant context, and how does AttnComp exploit this behavior?

## Architecture Onboarding

- **Component map**: Retriever -> Compressor Model -> Top-P Compression Module -> Confidence Estimator -> Reader LLM

- **Critical path**: 
  1. Retrieve top-k documents using E5-base-v2
  2. Concatenate instruction + documents + query
  3. Forward pass through L frozen layers → hidden states X_c, X_q
  4. Cross-attention computes query-to-context weights A
  5. Aggregate A to document-level scores
  6. Apply Algorithm 1 to select D'
  7. Generate answer with compressed context

- **Design tradeoffs**:
  - Layer depth L: Middle layers (13-15) balance relevance detection and computational cost
  - Threshold p: Higher p (0.95-0.98) maximizes accuracy; lower p increases compression
  - Granularity: Document-level achieves 17x rate; sentence-level achieves 21x with minor accuracy drop

- **Failure signatures**:
  - All documents filtered when retrieval is entirely irrelevant
  - High attention on distractors without fine-tuning
  - Over-compression on multi-hop QA with fixed-rate methods

- **First 3 experiments**:
  1. Baseline comparison on HotpotQA against uncompressed RAG, top-5/10, RECOMP-ext, LongLLMLingua, Provence
  2. Ablation on layer depth L ∈ {7, 13, 15, 23, 31} with and without fine-tuning
  3. Confidence calibration check: compute Pearson correlation between confidence and F1 on held-out set

## Open Questions the Paper Calls Out

- **Iterative retrieval trigger**: How can AttnComp's confidence estimation be utilized to trigger autonomous iterative retrieval? The authors state confidence scores "suggest a possible avenue for future research on autonomous iterative RAG" but leave full exploration to future work.

- **Larger model generalization**: Does AttnComp retain performance advantages when applied to LLMs significantly larger than 8 billion parameters? All experiments conducted on models up to 8B parameters.

- **MoE architecture adaptation**: Can attention-guided compression strategy be effectively adapted for Mixture-of-Experts architectures? Applicability to MoE models is unexplored.

- **Refined confidence estimation**: Can confidence estimation be refined to distinguish between retrieval quality and model's internal parametric conflicts? Current method focuses solely on retrieved document quality.

## Limitations

- Performance heavily depends on quality of initial retrieval; attention mechanism may not reliably distinguish signal from noise in low-quality retrieval scenarios
- All experiments focus on question-answering datasets; effectiveness may not transfer to other RAG applications like long-form generation
- Framework lacks automatic threshold adaptation, requiring manual tuning for new domains

## Confidence

- **High confidence**: Core mechanism of using middle-layer attention heads for relevance detection is well-supported by quantitative results and ablation studies
- **Medium confidence**: Instruction attention confidence estimation shows positive correlation but lacks strong validation across diverse failure modes
- **Low confidence**: Claims about cross-document reasoning capabilities are primarily inferred from compression rates rather than direct measurement

## Next Checks

1. **Retrieval quality robustness test**: Evaluate AttnComp on systematically degraded retrieval quality to measure how confidence signal degrades and whether compression rates remain optimal when relevant content is scarce

2. **Cross-domain transfer validation**: Apply AttnComp to non-QA RAG tasks (summarization, dialogue, code generation) to test whether middle-layer attention heads maintain relevance detection capability across domains

3. **Confidence calibration refinement**: Implement isotonic regression or temperature scaling on confidence scores and measure calibration error on held-out set to test whether confidence intervals achieve desired coverage