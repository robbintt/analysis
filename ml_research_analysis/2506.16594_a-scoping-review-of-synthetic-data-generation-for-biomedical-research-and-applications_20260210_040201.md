---
ver: rpa2
title: A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications
arxiv_id: '2506.16594'
source_url: https://arxiv.org/abs/2506.16594
tags:
- data
- synthetic
- clinical
- generation
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review systematically analyzed 59 studies on synthetic
  data generation for biomedical research, published between 2020 and 2025. The review
  found that large language models (LLMs) are predominantly used for generating synthetic
  biomedical data, with 72.9% of studies employing prompting-based approaches.
---

# A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications

## Quick Facts
- arXiv ID: 2506.16594
- Source URL: https://arxiv.org/abs/2506.16594
- Reference count: 40
- 59 studies analyzed on synthetic data generation for biomedical research (2020-2025)

## Executive Summary
This scoping review systematically examined 59 studies on synthetic data generation for biomedical research published between 2020 and 2025. The review found that large language models (LLMs) dominate the field, with 72.9% of studies using prompting-based approaches to generate synthetic biomedical data. Unstructured text emerged as the most common data type (78.0%), followed by tabular data (13.6%) and multimodal sources (8.4%). Quality assessment practices varied considerably, with 55.9% of studies employing human-in-the-loop evaluations while 27.1% relied on intrinsic metrics. The review identified key trends including the shift toward multimodal data generation, increased use of open-source models, and the need for standardized evaluation frameworks. However, accessibility remains problematic, as 49.2% of studies did not clearly specify data availability.

## Method Summary
The review conducted systematic searches of PubMed and bioRxiv databases, identifying studies that generated synthetic data for biomedical applications. Studies were screened for relevance and analyzed based on their generation approaches, data types, quality assessment methods, and availability practices. The review period spanned 2020-2025, capturing the rapid evolution of synthetic data generation techniques during this timeframe. Analysis focused on categorizing generation methods, data types, evaluation approaches, and identifying emerging trends and challenges in the field.

## Key Results
- Large language models (LLMs) dominate synthetic data generation, with 72.9% using prompting-based approaches
- Unstructured text is the most common data type (78.0%), followed by tabular data (13.6%) and multimodal sources (8.4%)
- Quality assessment varies widely, with 55.9% using human-in-the-loop evaluations and 27.1% relying on intrinsic metrics
- 49.2% of studies do not clearly specify data availability, raising accessibility concerns

## Why This Works (Mechanism)
Synthetic data generation addresses critical challenges in biomedical research by providing accessible, privacy-preserving alternatives to real patient data. The approach enables researchers to overcome data scarcity, protect patient privacy, and create diverse training datasets without ethical constraints. By leveraging large language models and advanced generation techniques, synthetic data can approximate real-world biomedical scenarios while maintaining statistical properties and distributions essential for training robust AI models.

## Foundational Learning

**Biomedical Data Privacy** - Why needed: Patient data protection is paramount in healthcare research. Quick check: Verify compliance with HIPAA and GDPR regulations when handling real patient data.

**Large Language Model Prompting** - Why needed: Effective prompting determines the quality and relevance of generated synthetic data. Quick check: Test different prompt structures to optimize output quality and domain specificity.

**Quality Assessment Metrics** - Why needed: Standardized evaluation ensures synthetic data maintains utility for downstream applications. Quick check: Compare human evaluation scores with automated metrics across multiple studies.

## Architecture Onboarding

**Component Map**: Data Source -> Preprocessing Pipeline -> LLM Generator -> Quality Assessment -> Output Validation

**Critical Path**: The most critical path involves the interaction between the LLM generator and quality assessment components, as this determines the final utility of synthetic data for biomedical applications.

**Design Tradeoffs**: 
- Privacy vs. utility: Higher privacy preservation may reduce data utility for specific applications
- Open-source vs. proprietary models: Tradeoff between cost and performance
- Human-in-the-loop vs. automated evaluation: Balance between accuracy and scalability

**Failure Signatures**:
- Generated data contains factual errors or medical inconsistencies
- Synthetic data fails to capture domain-specific terminology and context
- Quality metrics show significant deviation from real data distributions

**Three First Experiments**:
1. Generate synthetic clinical notes using different LLM prompting strategies and compare quality metrics
2. Evaluate privacy preservation by testing differential privacy guarantees on synthetic datasets
3. Assess utility by training machine learning models on synthetic vs. real data and comparing performance

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the standardization of evaluation frameworks for synthetic biomedical data, the long-term utility of synthetic data in clinical applications, and the development of domain-specific quality metrics. Additionally, questions remain about the optimal balance between data realism and privacy preservation, and how to ensure reproducibility across different generation approaches and data types.

## Limitations

The search strategy may have missed relevant studies not indexed in PubMed or bioRxiv, or those using different terminology for synthetic data generation. The review period (2020-2025) may not capture the full evolution of methods, particularly given the rapid advancement in LLM capabilities during this timeframe. Quality assessment methodology relies on reported metrics, which may vary significantly in rigor and standardization across studies.

## Confidence

**High confidence**: The dominance of LLM-based approaches (72.9% prompting) and the prevalence of unstructured text as the primary data type (78.0%) are well-supported by the included studies and align with broader trends in biomedical AI research.

**Medium confidence**: The characterization of quality assessment practices (55.9% human-in-the-loop, 27.1% intrinsic metrics) is based on reported methodologies, but variations in how these assessments were conducted across studies introduce uncertainty.

**Medium confidence**: The identified trends (multimodal generation, open-source adoption, evaluation framework needs) are inferred from the distribution of methods across studies, but may reflect publication bias toward novel approaches rather than comprehensive field representation.

## Next Checks

1. Conduct a systematic search of additional databases (ArXiv, IEEE Xplore, Google Scholar) to verify the comprehensiveness of the identified studies and identify any methodological gaps.

2. Perform a meta-analysis of the reported quality metrics to establish baseline performance standards for synthetic biomedical data across different data types and generation approaches.

3. Survey the authors of the included studies to clarify data availability practices and validate the reported use of human-in-the-loop evaluations versus automated metrics.