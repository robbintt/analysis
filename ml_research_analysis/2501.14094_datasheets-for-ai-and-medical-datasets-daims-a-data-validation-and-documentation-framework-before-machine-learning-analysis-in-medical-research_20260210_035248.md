---
ver: rpa2
title: 'Datasheets for AI and medical datasets (DAIMS): a data validation and documentation
  framework before machine learning analysis in medical research'
arxiv_id: '2501.14094'
source_url: https://arxiv.org/abs/2501.14094
tags:
- data
- medical
- research
- datasets
- daims
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAIMS, a framework extending Datasheets for
  Datasets to medical AI research, providing a checklist for data standardization,
  a software tool for validation, and a flowchart for selecting ML methods. The checklist
  covers 24 data quality requirements, with the tool validating 15 items automatically.
---

# Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research

## Quick Facts
- arXiv ID: 2501.14094
- Source URL: https://arxiv.org/abs/2501.14094
- Authors: Ramtin Zargari Marandi; Anne Svane Frahm; Maja Milojevic
- Reference count: 30
- Primary result: Introduces DAIMS, a framework extending Datasheets for Datasets to medical AI research, providing a checklist for data standardization, a software tool for validation, and a flowchart for selecting ML methods.

## Executive Summary
DAIMS is a framework designed to address data validation and documentation inconsistencies in medical machine learning research. It extends the Datasheets for Datasets framework by adapting it to medical field conventions and adding specific sections for GDPR compliance, ethical review processes, and outcome variable specification. The framework includes a 24-item checklist for data quality, a software tool that automatically validates 15 structural requirements, and a flowchart mapping research questions to appropriate ML analysis methods. DAIMS aims to promote transparency and reproducibility in medical AI research by providing standardized documentation and validation procedures.

## Method Summary
The DAIMS framework consists of three main components: a documentation questionnaire (DAIMS datasheet) with sections on motivation, composition, collection, preprocessing, uses, distribution, and maintenance; a data dictionary template specifying variable-level metadata including name, type, units, range, role, and observation error; and a 24-item checklist for data standardization. The framework includes a software tool built with Streamlit and Python that automatically validates 15 of the 24 checklist items for tabular datasets in wide format. A flowchart guides users from problem identification through data modality assessment to recommended ML method selection. The framework is publicly available as both a GitHub repository and online application.

## Key Results
- Automated validation tool checks 15/24 data quality requirements for tabular medical datasets
- Checklist covers structural properties (wide format, unique IDs, duplicate rows, missing value encoding) and compliance requirements (GDPR, standards adherence)
- Flowchart maps research questions to ML methods through problem type identification (regression, classification, survival) and data modality assessment
- Framework extends Datasheets for Datasets with medical field terminology and additional sections for ethical and regulatory compliance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated validation of structural data properties reduces manual effort while catching formatting issues that commonly break ML pipelines.
- Mechanism: A software tool (Streamlit/Python) scans tabular datasets against 15 predefined quality criteria (wide format, unique IDs, duplicate rows, missing value encoding, etc.), generating a binary pass/fail score (#done/24). This creates a standardized baseline that must be cleared before ML analysis.
- Core assumption: The 15 automatable items represent the most common and detectable structural errors; the 9 manual items (standards compliance, translation, informative missingness) require human judgment the tool cannot replicate.
- Evidence anchors:
  - [abstract]: "The checklist consists of 24 common data standardization requirements, where the tool checks and validate a subset of them."
  - [section]: Table 1 shows items 1-15 can be validated automatically, items 16-24 require manual verification; "Some items are left to be checked manually because of the technical complexities around them."
  - [corpus]: Corpus papers emphasize documentation standards but lack empirical validation of automated vs. manual checking effectiveness.
- Break condition: When datasets contain context-dependent errors (e.g., implausible clinical values that fall within defined ranges, or ICD code version transitions) that require domain expertise to detect.

### Mechanism 2
- Claim: Domain-adapted documentation templates increase completeness and usability for medical researchers compared to generic frameworks.
- Mechanism: DAIMS extends Gebru et al.'s "Datasheets for Datasets" by replacing technical ML terminology with medical field conventions, adding sections for GDPR compliance, ethical review processes, and outcome variable specification. This reduces cognitive friction for physician-scientists filling documentation.
- Core assumption: Lowering terminology barriers and adding domain-relevant sections will increase adoption and documentation completeness, though this is not empirically validated in the paper.
- Evidence anchors:
  - [abstract]: "DAIMS addresses inconsistencies in data validation and documentation for ML in medical research, promoting transparency and reproducibility."
  - [section]: "The form... includes our modifications and extensions that include replacing less familiar terminology with more commonly used terms in the medical field, enhancing definitions and instructions."
  - [corpus]: Eval Factsheets and AIRS Framework papers similarly propose structured documentation but do not provide comparative adoption metrics.
- Break condition: When teams treat documentation as a one-time compliance exercise rather than a living document; versioning discipline fails.

### Mechanism 3
- Claim: Decision-flow mapping from data/problem characteristics to model classes reduces method selection errors for non-expert practitioners.
- Mechanism: A flowchart routes users through problem type identification (regression, classification, survival), data modality (tabular, image, text, time-series), and interpretability requirements to recommend specific model families (logistic regression, ResNet, RNNs, ensemble methods).
- Core assumption: The flowchart captures sufficient decision nuance for common medical ML tasks; users can self-assess their problem type correctly.
- Evidence anchors:
  - [abstract]: "a flowchart mapping research questions to suggested ML methods"
  - [section]: "The flowchart starts with identifying the problem type, such as predicting numerical outcomes, categorical outcomes, or survival analysis... recommending interpretable models (e.g., logistic regression or decision trees) for tabular data."
  - [corpus]: Corpus lacks comparative studies on flowchart-based vs. other method selection approaches.
- Break condition: When multimodal or novel problem structures don't fit predefined pathways; users may force-fit inappropriate methods.

## Foundational Learning

- **Concept: Tabular Data Structure (Wide vs. Long Format)**
  - Why needed here: The checklist's first item requires wide format (one row per patient, one column per variable). Understanding this is prerequisite to passing automated validation.
  - Quick check question: Given a dataset with multiple visits per patient, can you reshape it so each patient has exactly one row?

- **Concept: Medical Coding Standards (ICD, SNOMED CT)**
  - Why needed here: Manual checklist items 19-20 require verifying terminology standards and temporal consistency. Without understanding these vocabularies, you cannot complete manual validation.
  - Quick check question: If a dataset contains "DMII" and "E11.9" as diagnosis entries, can you identify which follows ICD-10 and whether they represent the same condition?

- **Concept: ML Task Taxonomy (Classification, Regression, Survival)**
  - Why needed here: The flowchart's entry point requires correctly identifying your problem type. Misidentification leads to wrong model recommendations.
  - Quick check question: Your outcome is "days until hospital readmission or censoring"—is this a regression or survival analysis problem?

## Architecture Onboarding

- **Component map:**
  - DAIMS_DatasetName_DDMMYYYY.docx -> Documentation questionnaire (motivation, composition, collection, preprocessing, uses, distribution, maintenance)
  - Data Dictionary -> Variable-level metadata (name, type, units, range, role, observation error)
  - Checklist -> 24 binary validation items; 15 auto-checkable via tool
  - Validation Tool -> Streamlit/Python app for automated checks (GitHub + online app)
  - ML Flowchart -> Decision tree for method selection

- **Critical path:**
  1. Define variables in Data Dictionary before dataset inspection
  2. Run Validation Tool to auto-check items 1-15; resolve failures
  3. Manually verify items 16-24 (standards, translation, sensitive data removal)
  4. Complete DAIMS datasheet questionnaire
  5. Navigate flowchart to identify appropriate ML methods
  6. Proceed to ML pipeline (e.g., MAIT as suggested in Discussion)

- **Design tradeoffs:**
  - Automation scope: Only 15/24 items automatable; trade-off between technical feasibility and coverage
  - Generality vs. specificity: Broad medical focus means less guidance for niche subdomains (rare diseases, genomics)
  - Flowchart simplicity vs. completeness: Does not cover LLMs, causal ML, or explainability methods explicitly (noted as future work)

- **Failure signatures:**
  - Tool crashes on non-tabular data (images, raw text): DAIMS designed for tabular datasets only
  - Checklist passes but model fails: Structural validity does not guarantee clinical plausibility
  - Flowchart leads to dead end: Multimodal or unlabeled data may not map cleanly; requires expert consultation
  - Stale datasheet: Documentation not updated when dataset version changes

- **First 3 experiments:**
  1. Run the online validation tool on a sample medical dataset (e.g., MIMIC-III derived cohort) and document which items fail and why.
  2. Complete a Data Dictionary for 10 variables from your dataset, including "Typical Observation Error" for continuous measures—verify you can obtain this information from device specs or lab protocols.
  3. Trace through the flowchart for two different research questions on the same dataset (e.g., binary classification of diagnosis vs. survival prediction of time-to-event), documenting the model recommendations and where the paths diverge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DAIMS flowchart be effectively extended to incorporate emerging Large Language Models (LLMs) and causal machine learning methods?
- Basis in paper: [explicit] The authors state that "Future versions of the flowchart can be extended to incorporate emerging models and methods, like Large Language Models (LLMs) and causal ML."
- Why unresolved: These advanced methods were omitted from the current version, creating a gap in guidance for modern medical AI applications involving unstructured text or causal inference.
- What evidence would resolve it: A validated extension of the DAIMS flowchart that includes decision nodes for text-based and causal inference tasks.

### Open Question 2
- Question: Can the framework be expanded to include model interpretation (XAI), fairness, and deployment strategies without becoming overly complex for users?
- Basis in paper: [explicit] The paper identifies "methods for explainable AI... model safety and fairness approaches, as well as model deployment strategies" as important future directions, while noting that covering all aspects is limited by "complexity."
- Why unresolved: Balancing the inclusion of post-modeling steps (like fairness and deployment) with the framework's goal of simplicity remains a design challenge.
- What evidence would resolve it: User acceptance testing of an expanded flowchart that measures navigability and perceived complexity when including these additional stages.

### Open Question 3
- Question: Is it technically feasible to automate the validation of the 9 currently manual checklist items, such as verifying international standards or encoding informative missingness?
- Basis in paper: [inferred] The paper notes that while the tool validates 15 items, "the rest of the items must be checked manually" due to "technical complexities."
- Why unresolved: The "technical complexities" suggest that detecting issues like non-compliance with international terminology or semantic errors currently exceeds the software's capabilities.
- What evidence would resolve it: The development of algorithms capable of parsing and verifying semantic data quality rules (e.g., identifying "informative missingness") without human intervention.

## Limitations
- Coverage limited to tabular datasets; does not address images, text, or multimodal data
- 9 of 24 checklist items require manual verification due to technical complexities
- ML flowchart does not include emerging methods like LLMs, causal inference, or advanced XAI techniques
- No empirical validation of framework adoption or effectiveness in real research settings

## Confidence
- Confidence in automated validation component: **High** - clearly specified and reproducible with provided GitHub repository and online app
- Confidence in checklist completeness: **Medium** - 24-item structure is well-defined but not empirically validated for all medical ML scenarios
- Confidence in ML method selection flowchart: **Low** - lacks validation studies and coverage of emerging ML paradigms

## Next Checks
1. Conduct a comparative study measuring documentation completeness and quality when using DAIMS versus ad-hoc documentation approaches in actual medical ML projects.

2. Test the validation tool on diverse medical datasets (different specialties, data collection methods) to identify edge cases where automated checks fail or produce false positives/negatives.

3. Extend the ML flowchart to include LLM-based approaches and causal inference methods, then validate its recommendations against expert consensus for complex multimodal medical problems.