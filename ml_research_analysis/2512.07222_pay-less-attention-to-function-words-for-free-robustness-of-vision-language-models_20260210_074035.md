---
ver: rpa2
title: Pay Less Attention to Function Words for Free Robustness of Vision-Language
  Models
arxiv_id: '2512.07222'
source_url: https://arxiv.org/abs/2512.07222
tags:
- fare
- tecoa
- defense
- words
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the trade-off between robustness and performance
  in vision-language models (VLMs) under adversarial attacks. It proposes Function-word
  De-Attention (FDA), which reduces the impact of function words by calculating and
  subtracting cross-attention between function words and images from the original
  attention.
---

# Pay Less Attention to Function Words for Free Robustness of Vision-Language Models

## Quick Facts
- arXiv ID: 2512.07222
- Source URL: https://arxiv.org/abs/2512.07222
- Authors: Qiwei Tian; Chenhao Lin; Zhengyu Zhao; Chao Shen
- Reference count: 40
- Primary result: Reduces average attack success rate by 18-53% with minimal performance loss

## Executive Summary
This paper addresses the trade-off between robustness and performance in vision-language models (VLMs) under adversarial attacks. It proposes Function-word De-Attention (FDA), which reduces the impact of function words by calculating and subtracting cross-attention between function words and images from the original attention. This improves alignment and robustness. Extensive experiments on 3 models, 2 tasks, and 3 datasets under 6 attacks show FDA reduces average attack success rate by 18-53% with minimal (<1%) performance loss, outperforms SOTA baselines, and generalizes well across settings.

## Method Summary
FDA works by identifying function words in the input text, computing their cross-attention with visual features in the fusion encoder, and differentially subtracting this attention from the original attention map. The method applies this subtraction in shallow layers (L₀-L₁) and heads (H₀-H₅) of the fusion encoder. A learnable control gate adjusts the subtraction strength, and the approach requires no adversarial training during fine-tuning.

## Key Results
- Reduces attack success rate (ASR) by 18-53% across models and tasks
- Maintains clean performance with <1% drop on retrieval tasks
- Outperforms SOTA baselines TeCoA and FARE in both robustness and clean performance
- Generalizes to visual grounding task without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Function Words as Cross-Modal Attack Vectors
- Claim: Function words create exploitable vulnerability because their semantic emptiness allows adversarial perturbations to hijack attention without meaningful grounding.
- Evidence: Post-attack, 80.3% of images show higher similarity to function words than content words (0% pre-attack).
- Break condition: If function words carried task-critical semantic information (e.g., negation words like "not"), de-attending them would harm performance.

### Mechanism 2: Differential Attention Subtraction
- Claim: Subtracting function-word attention removes cross-modal distractions while preserving content-word grounding.
- Evidence: Equations 4-6 detail the subtraction mechanism with control gate G.
- Break condition: If G learns near-zero values or function-word attention is uncorrelated with adversarial perturbations, the mechanism becomes ineffective.

### Mechanism 3: Improved Vision-Language Embedding Alignment
- Claim: FDA produces tighter cross-modal embedding clusters, reducing the "attack surface" for misalignment.
- Evidence: T-SNE visualizations show FDA produces tighter clusters than TeCoA/FARE.
- Break condition: If adversarial attacks shift to exploit content-word attention patterns, alignment alone may not suffice.

## Foundational Learning

- **Cross-Attention in Fusion Encoders**: FDA operates on cross-attention heads where text tokens (queries) attend to visual tokens (keys/values).
  - Quick check: Given text embeddings F^T and visual embeddings F^V, how would you compute cross-attention scores and what does each dimension represent?

- **Function Words vs. Content Words**: The method relies on distinguishing function words (determiners, prepositions, auxiliary verbs) from content words (nouns, verbs, adjectives).
  - Quick check: Why would masking nouns cause catastrophic performance drops (Table 6: R@1 drops from 95.90% to 58.90%) while masking function words causes minimal change?

- **Adversarial Training Trade-offs**: Baselines TeCoA and FARE use adversarial training, which improves robustness at the cost of clean performance (4-9% drops).
  - Quick check: Why does adding adversarial examples to training typically harm clean performance, and how does FDA avoid this trade-off?

## Architecture Onboarding

- **Component map**: Input (I, T) → Text Encoder T → F^T ─┬──→ Fusion Encoder (cross-attention) → Output
                    ↓                   │         ↑
                Function Mask M_tf     │         │
                    ↓                   │    ┌────┴────┐
              F^{T_f} (function only) ─┘    │   FDA   │
                                               └────────┘

- **Critical path**: 1) Identify function words using dictionary lookup 2) Generate mask M_tf 3) Extract F^{T_f} 4) Compute function-word cross-attention in L₀-L₁, H₀-H₅ 5) Apply softmax along visual and textual dimensions 6) Subtract gated distractions from original attention 7) Backpropagate normally

- **Design tradeoffs**: 
  - Shallow layers (L₀-L₁, H₀-H₅) work best; deeper layers may have "absorbed" distractions
  - Full dictionary (208 words) vs. shortlisted (93 words) show marginal difference; recommend shortlisted
  - Fusion encoder only (H) outperforms text encoder only (T) or both (T&H)

- **Failure signatures**:
  - No robustness gain: Check if function-word dictionary matches tokenizer vocabulary
  - Performance collapse: If FDA applied to all layers/heads (L_all, H_all), reduce to L₀-L₁, H₀-H₅
  - Adaptive attack vulnerability: MAPGD bypasses FDA by masking function words during attack

- **First 3 experiments**:
  1. Run FDA on ALBEF with Flickr30k retrieval, ε=2/255; expect ~22% ASR drop with <0.5% clean performance change
  2. Compare L₀ vs L₀-L₁ vs L_all (all with H₀-H₅) on T2IR/I2TR; confirm shallow layers outperform
  3. Apply FDA-trained model to RefCOCO+ visual grounding without fine-tuning; check if L_all improves zero-shot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FDA be effectively adapted for dual-encoder architectures like CLIP?
- Basis: Section 4.5 states FDA is not directly implementable for CLIP but notes "implementation on CLIP-like models would be a valuable exploration for future work."
- Why unresolved: Dual-encoder models lack the fusion encoders required for the current cross-attention subtraction mechanism.
- Evidence needed: A reformulation of the FDA mechanism applicable to separate encoders that demonstrates comparable ASR drops on standard CLIP benchmarks.

### Open Question 2
- Question: Does the "free robustness" scale efficiently to larger VLMs or remain stable with parameter-efficient fine-tuning (PEFT) methods like LoRA?
- Basis: Section 4.5 notes authors "did not implement FDA to fine-tune a larger VLM or verify effectiveness using LoRa due to hardware limitation."
- Why unresolved: Unverified whether FDA introduces computational bottlenecks or gradient conflicts with massive models or low-rank adaptation.
- Evidence needed: Results showing FDA performance/robustness trade-off on models like BLIP-2 or LLaVA, or when fine-tuning with LoRA.

### Open Question 3
- Question: Can robustness be further improved by replacing static differential subtraction with a learned, modular attention mechanism?
- Basis: Section 4.5 suggests FDA "could be potentially improved through a modular or algorithmic approach for more refined removal."
- Why unresolved: Current implementation relies on heuristic subtraction; a more dynamic, learned module might capture complex noise patterns better.
- Evidence needed: Comparative ablation study where static subtraction is replaced by a small learnable neural network module showing statistically significant ASR reduction.

## Limitations
- **Control Gate Implementation**: The learnable control gate G is described but not specified, making implementation ambiguous.
- **Generalization to Out-of-Domain Attacks**: All evaluated attacks use pixel-space perturbations; effectiveness against semantic attacks remains untested.
- **Linguistic Universality**: The 93-word function dictionary is English-specific; cross-linguistic effectiveness is unknown.

## Confidence

**High Confidence**: Empirical results showing FDA reduces ASR by 18-53% with minimal clean performance loss are well-supported by extensive experiments across 3 models, 2 tasks, and 3 datasets.

**Medium Confidence**: The claim that FDA achieves "free" robustness by avoiding adversarial training is well-founded for evaluated settings, but broader implications for other attack types and domains require validation.

**Low Confidence**: The adaptive attack robustness claim is weak—FDA completely fails under MAPGD, and the paper doesn't explore more sophisticated adaptive strategies.

## Next Checks

1. **Control Gate Sensitivity Analysis**: Implement FDA with three different control gate architectures (scalar, vector, learned projection) and evaluate performance variation to determine if the gate's design is a critical hyperparameter.

2. **Cross-Lingual Function Word Transfer**: Apply FDA trained on English Flickr30k to a multilingual VLM (e.g., m-AlBEF) on non-English retrieval tasks to measure robustness gains and identify linguistic patterns where the method succeeds or fails.

3. **Adaptive Attack Benchmark**: Design a content-word targeting attack that perturbs noun/verb attention while preserving function-word attention; evaluate FDA's effectiveness against this attack compared to standard adversarial training baselines.