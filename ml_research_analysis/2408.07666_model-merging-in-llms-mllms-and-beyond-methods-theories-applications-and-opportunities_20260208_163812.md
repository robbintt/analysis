---
ver: rpa2
title: 'Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications
  and Opportunities'
arxiv_id: '2408.07666'
source_url: https://arxiv.org/abs/2408.07666
tags:
- merging
- arxiv
- methods
- learning
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model merging combines the parameters of multiple pre-trained expert
  models into a single model without requiring access to original training data or
  expensive computation. This survey comprehensively reviews model merging methods,
  their theoretical foundations, and applications across foundation models and over
  ten machine learning subfields.
---

# Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities

## Quick Facts
- arXiv ID: 2408.07666
- Source URL: https://arxiv.org/abs/2408.07666
- Authors: Enneng Yang; Li Shen; Guibing Guo; Xingwei Wang; Xiaochun Cao; Jie Zhang; Dacheng Tao
- Reference count: 40
- Primary result: Comprehensive survey proposing taxonomy of model merging methods and applications across foundation models and machine learning subfields

## Executive Summary
This paper presents a comprehensive survey of model merging techniques that combine parameters from multiple pre-trained expert models into a single model without requiring original training data or expensive computation. The authors propose a new taxonomy dividing methods into pre-merging (e.g., merge-friendly fine-tuning, architecture transformation) and during-merging (e.g., weighted averaging, subspace projection, optimization-based) techniques. The survey covers applications across foundation models including large language models (detoxification, human preference alignment), multimodal models (cross-modal fusion), visual generation (style mixing), continual learning (mitigating catastrophic forgetting), and adversarial defense. Despite progress, challenges remain in closing performance gaps with independent models, ensuring trustworthiness, and developing efficient strategies for heterogeneous models.

## Method Summary
The paper surveys model merging methods organized into a taxonomy of pre-merging and during-merging techniques. Pre-merging methods include merge-friendly fine-tuning (task vectors, MoE routing initialization) and architecture transformation (dimensionality matching, neuron permutation for alignment). During-merging methods encompass weighted averaging, task arithmetic, subspace projection (TIES, DARE), and optimization-based approaches (WUDI, Fisher merging). The survey provides detailed formulas and mechanisms for each approach, with empirical validation on vision (ViT-B/32, ViT-L/14 on 8 visual classification tasks) and language models (RoBERTa-Large on 8 GLUE tasks; Llama-3.1-8B on 5 text generation tasks). Implementation references include MergeKit and FusionBench frameworks that contain implementations of surveyed methods.

## Key Results
- Model merging enables knowledge integration without original training data or expensive computation
- Task Arithmetic framework allows linear combination of task vectors to achieve multitask learning
- TIES-Merging (trim to top 20% parameters) and DARE (random drop with rescaling) effectively reduce interference in merged models
- Applications span LLMs, MLLMs, visual generation, continual learning, and adversarial defense
- Performance gaps remain between merged models and independently trained expert models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model capabilities can be treated as directional vectors in weight space that are added to a base model.
- Mechanism: Task Arithmetic defines task vector τ_t = Θ^(t) - Θ^(0) (fine-tuned weights minus pre-trained weights). Merging is achieved by linear combination: Θ^merge = Θ^(0) + λ Σ τ_t.
- Core assumption: Task vectors are approximately linear and independent; functional change induced by vector is preserved when added to different base or combined with others.
- Evidence anchors: [abstract] "Model merging combines parameters... without requiring access to original training data"; [section 2.3.1] "Task Arithmetic introduced concept of 'task vector'... multitask learning can be accomplished by adding task vectors."
- Break condition: When task vectors are highly correlated or contradictory (opposing sign gradients), simple addition causes destructive interference degrading performance below base model.

### Mechanism 2
- Claim: Effective merging requires models to be aligned within same loss basin (Linear Mode Connectivity).
- Mechanism: Two models with identical performance may have permuted neurons. If in different basins, linear interpolation crosses high-loss barriers. Git Re-basin or optimal transport aligns weights so Θ^(1) and Θ^(2) are in same basin before averaging.
- Core assumption: Models share common initialization or can be transformed to satisfy LMC (linear path exists between them with low loss).
- Evidence anchors: [section 2.2.3] "LMC implies multiple local minima may be equivalent in weight space... many works proposed to permute weights of one model to align with other"; [section 3.1] "Two independent models starting from same pre-trained model usually satisfy LMC."
- Break condition: Merging fails if models trained from scratch on different tasks without alignment, or if architectural differences prevent neuron correspondence.

### Mechanism 3
- Claim: Conflict in parameter updates can be resolved by restricting merging to critical subspaces or sparse masks.
- Mechanism: Interference from redundant/conflicting parameters. Methods like TIES-Merging prune task vectors to top-k% largest magnitudes (sparse subspace) and resolve sign disagreements by majority vote.
- Core assumption: Important information for task concentrated in small subset of high-magnitude parameters; distinct tasks predominantly use disjoint/non-conflicting parameters.
- Evidence anchors: [section 2.3.3] "TIES-Merging proposes to trim each individual model... retaining only top 20% of parameters... suggests eliminating parameter sign conflicts"; [abstract] "Applications include mitigating catastrophic forgetting and adversarial defense."
- Break condition: If critical information distributed across low-magnitude weights or tasks fundamentally irreconcilable (requiring exact same weights for opposite functions), masking drops essential performance.

## Foundational Learning

- Concept: **Fine-tuning (PEFT/LoRA)**
  - Why needed here: Most model merging assumes "pre-training → fine-tuning" paradigm. Cannot define "task vector" or check mode connectivity without understanding experts are derivatives of common ancestor.
  - Quick check question: If I have Model A (pre-trained) and Model B (fine-tuned on math), how do I isolate "math capability" using Task Arithmetic?

- Concept: **Loss Landscape & Basins**
  - Why needed here: Theoretical justification for why weight averaging doesn't result in garbage relies on geometry of loss landscape. Must understand "flat minima" and "barriers" to diagnose why merged model failed.
  - Quick check question: Why does averaging two checkpoints from same training run (SWA) usually work, but averaging two random models usually fail?

- Concept: **Multi-Task Learning (MTL) vs. Ensemble**
  - Why needed here: Merging is middle ground between MTL (hard to train, data access required) and Ensembles (expensive inference). Distinguishing helps select right approach.
  - Quick check question: How does model merging differ from standard ensemble learning in terms of inference cost and storage?

## Architecture Onboarding

- Component map:
  1. Inputs: T expert models {Θ^(t)} and usually base model Θ^(0)
  2. Pre-Merging (Optional): Alignment (neuron permutation) or Transformation (heterogeneous architectures)
  3. During-Merging Core:
     * Basic: Weight Averaging, Task Arithmetic (Δ extraction)
     * Advanced: Weighting (AdaMerging/Fisher), Subspace (TIES/DARE), Optimization (WUDI)
  4. Post-Calibration (Optional): Representation surgery or routing initialization

- Critical path:
  1. Verification: Ensure all source models share same architecture (or apply Section 2.2.2 transformations)
  2. Baseline: Calculate Θ^(0) + λ Σ (Θ^(t) - Θ^(0)). Test on validation sets for all tasks
  3. Refinement: If Task A fails, apply TIES-Merging to prune noise. If general performance drops, check alignment

- Design tradeoffs:
  - Static (Arithmetic/TIES) vs. Dynamic (Routing): Static methods merge once (cheap inference, fixed capability). Dynamic methods (WEMoE) route per sample (better performance, higher complexity)
  - Data-Free vs. Data-Dependent: Methods like Task Arithmetic are data-free (scalable, privacy-safe). Methods like RegMean or AdaMerging use validation data to calculate weights (higher performance, requires data access)

- Failure signatures:
  - Catastrophic Drop: Merged model performs worse than base model on all tasks. Diagnosis: Severe weight interference; try Sparsity (TIES) or Alignment (Re-basin)
  - Specific Task Collapse: Task A works, Task B drops to random chance. Diagnosis: Task vector for B dominated by A; try adjusting coefficients λ or using uncertainty-based weighting (Fisher-Merging)

- First 3 experiments:
  1. Task Arithmetic Baseline: Merge 2 models (one fine-tuned for code, one for math) using Θ^merge = Θ^base + λ(Δ_code + Δ_math). Sweep λ
  2. Interference Check: Compare accuracy of merged model on Task A vs. independent expert for Task A. If drop >10%, test TIES-Merging to prune conflicting weights
  3. Heterogeneity Test (Advanced): Attempt to merge model from same family but different size (Llama-7B and Llama-13B) using Architecture Transformation (Section 2.2.2) or distillation approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can model merging techniques be developed that work effectively for models trained independently from scratch on different tasks, without relying on shared pre-training initialization?
- Basis in paper: [explicit] Section 6, Point (1) explicitly challenges reliance on "pretraining-finetuning" paradigm and asks how to ensure effectiveness under "more relaxed conditions," such as training from scratch
- Why unresolved: Current methods depend heavily on models being in same loss basin (Linear Mode Connectivity), generally guaranteed only when fine-tuning from same initialization
- What evidence would resolve it: Merging methodology that achieves performance comparable to independent expert models when merging diverse architectures or weights trained from random initialization

### Open Question 2
- Question: What theoretical frameworks can explain mechanics of merging models fine-tuned on different datasets (cross-task merging), distinct from existing theories for single-trajectory or same-task merging?
- Basis in paper: [explicit] Section 6, Point (2) notes existing theory focuses on same-trajectory or same-dataset settings, leaving gap in understanding merging across different datasets
- Why unresolved: Theoretical work (Linear Mode Connectivity, Neural Tangent Kernels) usually assumes shared initialization or data; explaining cross-task interference reduction lacks unified theoretical framework
- What evidence would resolve it: Generalization bounds or causality frameworks that mathematically define necessary and sufficient conditions for successful cross-task knowledge fusion

### Open Question 3
- Question: How can stakeholders ensure "Trustworthy Model Merging" by developing active defenses against unauthorized merging and robust mechanisms to detect IP theft or backdoor injections?
- Basis in paper: [explicit] Section 6, Point (3) highlights urgency of reliability, specifically distinguishing between "active defense" (preventing merging) and "passive defense" (detecting IP), as well as defending against poisoning
- Why unresolved: Model merging creates unique security vulnerabilities where malicious modules (poisoned LoRAs) can be injected, or IP can be obscured, with few existing detection mechanisms
- What evidence would resolve it: Algorithms that effectively fingerprint merged models to trace original components, or "break" model functionality upon unauthorized merging attempts

### Open Question 4
- Question: Can heterogeneous models with different architectures be merged efficiently without relying on expensive, data-dependent knowledge distillation?
- Basis in paper: [explicit] Section 6, Point (5) identifies difficulty of merging heterogeneous models and high cost associated with current architectural transformation methods
- Why unresolved: Current solutions typically use distillation to force common architecture, which is computationally expensive and requires access to original task data
- What evidence would resolve it: Training-free alignment method (optimal transport or functional matching) that maps parameters across different architectures directly

## Limitations

- Performance gaps remain significant between merged models and independently trained expert models, suggesting current mechanisms are incomplete
- Many claims about effectiveness across diverse domains (adversarial defense, continual learning) lack empirical validation in the paper itself
- Survey focuses primarily on neural network parameter merging without addressing alternative approaches like knowledge distillation or attention-based routing

## Confidence

- **High Confidence:** Taxonomy organization (pre-merging vs. during-merging methods) and general description of Task Arithmetic and weight averaging mechanisms
- **Medium Confidence:** Claims about specific performance improvements from TIES-Merging and DARE methods
- **Low Confidence:** Applications to adversarial defense and continual learning due to brief treatment and limited empirical depth

## Next Checks

1. **Interference Testing:** Systematically measure magnitude of interference when merging models with correlated vs. orthogonal task vectors. Use ablation studies to quantify how much performance degradation stems from weight conflicts versus other factors.

2. **Cross-Domain Generalization:** Validate whether merging methods effective for vision models (ViT) transfer to language models (LLaMA) without architectural modifications. Test with concrete examples of merging vision and language experts.

3. **Efficiency vs. Performance Tradeoffs:** Conduct controlled experiments comparing inference-time efficiency of merged models against ensemble methods, measuring both computational cost and accuracy across different task combinations.