---
ver: rpa2
title: Learning Skills from Action-Free Videos
arxiv_id: '2512.20052'
source_url: https://arxiv.org/abs/2512.20052
tags:
- learning
- skill
- flow
- skills
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOF (Skill Abstraction from Optical Flow),
  a framework that learns latent skills from action-free videos for robot control.
  The key insight is to use optical flow as an intermediate representation to capture
  motion patterns aligned with robot actions, enabling the learning of temporally
  coherent skill abstractions.
---

# Learning Skills from Action-Free Videos

## Quick Facts
- arXiv ID: 2512.20052
- Source URL: https://arxiv.org/abs/2512.20052
- Reference count: 0
- SOF achieves 0.69 average success rate on MetaWorld multi-task learning

## Executive Summary
This paper introduces SOF (Skill Abstraction from Optical Flow), a framework that learns latent skills from action-free videos for robot control. The key insight is to use optical flow as an intermediate representation to capture motion patterns aligned with robot actions, enabling the learning of temporally coherent skill abstractions. The approach consists of three stages: learning skill tokens from optical flow sequences using a quantized autoencoder, training a skill policy to predict these tokens from observations and instructions, and mapping predicted flows to executable actions via either a learned or learning-free module.

Experiments on MetaWorld and LIBERO benchmarks demonstrate consistent improvements over prior video-based methods including Diffusion Policy, UniPi, A VDC, and LAPA. SOF achieves an average success rate of 0.69 on multi-task learning in MetaWorld, outperforms baselines on long-horizon tasks, and shows effective cross-embodiment transfer between Sawyer and Panda robot arms. The optical flow-based skill learning proves more effective than pixel-space alternatives, with ablation studies showing a 13% improvement over next-frame prediction approaches.

## Method Summary
SOF learns latent skills from action-free videos through a three-stage pipeline. First, a quantized autoencoder learns to compress sequences of optical flow into discrete skill tokens using Finite Scalar Quantization (FSQ). Second, a decoder-only Transformer policy predicts these skill tokens from visual observations and instructions. Third, a Flow2Action module converts the predicted optical flow plans into executable robot actions. The framework uses FlowFormer++ for optical flow extraction and operates on both simulation (MetaWorld, LIBERO) and real-world benchmarks, with experiments showing cross-embodiment transfer between Sawyer and Panda robot arms.

## Key Results
- SOF achieves 0.69 average success rate on MetaWorld multi-task learning
- Optical flow-based skill learning outperforms pixel-space alternatives by 13% in ablation studies
- Flow2Action module significantly outperforms direct skill-to-action mapping (49% vs 15-21% success)
- Effective cross-embodiment transfer between Sawyer and Panda robot arms

## Why This Works (Mechanism)

### Mechanism 1
Optical flow serves as a superior intermediate representation for skill learning compared to raw pixels or single-step latent actions. By extracting optical flow from action-free videos, the model filters out static background noise and appearance variations (lighting, texture) that are irrelevant to action. It focuses learning on the "what moves where," which is tightly coupled with physical robot degrees of freedom. The paper posits that this disentangles motion priors from visual priors.

Core assumption: The optical flow estimator accurately captures the motion dynamics relevant to the robot's end-effector, and this motion is transferable across embodiments.

Evidence anchors:
- "...intermediate representation based on optical flow that captures motion information aligned with both video dynamics and robot actions."
- "Optical flow offers several advantages: it is action-directed... and noise-resistant..."
- [Translating Flow to Policy] validates flow as a grounded interface for high-level planners trained on action-free data.
- [CLAP] notes that standard latent action models suffer from "visual entanglement," supporting the need for structured intermediates like flow.

Break condition: The flow estimator fails to capture subtle manipulation contacts or occlusions cause flow artifacts that the skill encoder interprets as valid motion.

### Mechanism 2
Temporal abstraction via quantized skill tokens enables long-horizon planning and compositional generalization. The framework aggregates sequences of optical flow into discrete tokens using Finite Scalar Quantization (FSQ). This forces the model to compress redundant motion patterns into reusable "skills" (e.g., "reach," "grasp"). Planning occurs in this compact token space rather than continuous action space, reducing the burden on the policy network to model low-level dynamics.

Core assumption: Recurring motion patterns exist across the dataset that can be effectively discretized without losing essential precision for control.

Evidence anchors:
- "...learning temporally coherent skill abstractions."
- Shows SOF outperforms BC/DP on long-horizon tasks (LIBERO-10) specifically because it "efficiently composes reusable skills."
- [AMPLIFY] supports the hypothesis that "actionless motion priors" effectively accelerate robot learning from video.

Break condition: The discretization resolution of FSQ is too coarse for fine motor adjustments, or the codebook collapses, limiting the diversity of learnable skills.

### Mechanism 3
Explicit decoding to optical flow (Flow2Action) outperforms direct skill-to-action mapping. Rather than mapping a discrete skill token directly to robot joint angles (which is ambiguous), the model first decodes the token into a predicted optical flow plan conditioned on the current image. This flow is then translated to actions. This acts as a "semantic buffer," ensuring the action generator adheres to the visual motion plan.

Core assumption: The Flow2Action module (whether learned or heuristic) can reliably solve the inverse problem of converting pixel motion to robot state changes.

Evidence anchors:
- Describes the two-stage execution: decode skill → flow → action.
- Ablation shows Flow2Action achieves ~49% success vs ~15-21% for direct Skill2Action mappings.
- [ViPRA] explores video prediction for actions.

Break condition: The generated flow plan is physically impossible for the specific robot embodiment (kinematic constraints), causing the Flow2Action module to output erratic actions.

## Foundational Learning

- Concept: **Optical Flow Estimation**
  - Why needed here: This is the input modality for the skill encoder. You cannot debug the skill abstraction if the flow maps are noisy or missing.
  - Quick check question: Can you visualize the output of `FlowFormer++` on a video frame? Does it highlight the moving object or the background?

- Concept: **Finite Scalar Quantization (FSQ)**
  - Why needed here: This replaces VQ-VAE for creating the discrete skill codebook. It projects continuous vectors to a set of fixed scalar values.
  - Quick check question: Do you understand how FSQ binds continuous representations to discrete grids without an explicit codebook lookup loss?

- Concept: **Inverse Dynamics / Flow-to-Action**
  - Why needed here: This is the final execution step. You need to understand how pixel displacement maps to robot end-effector displacement.
  - Quick check question: Given two images and a flow field, can you calculate the delta in robot pose required to achieve that visual shift?

## Architecture Onboarding

- Component map:
  1. **Flow Extractor:** Offline pre-processor (FlowFormer++ / NeuFlow-v2)
  2. **Skill AE (Stage 1):** Encoder + FSQ + Decoder. Input: Flow segments. Output: Reconstructed Flow
  3. **Skill Policy (Stage 2):** Decoder-only Transformer. Input: Observation + Instruction. Output: Sequence of Skill Token IDs
  4. **Flow2Action (Stage 3):** Small ResNet or Heuristic. Input: Predicted Flow + Current Frame. Output: Action

- Critical path:
  1. Extract flow from `D_video`
  2. Train Skill Autoencoder to reconstruct flow from discrete tokens (Stage 1)
  3. Freeze encoder/decoder; train Transformer policy to predict tokens from images/text (Stage 2)
  4. Train/Fit Flow2Action module using `D_act` (Stage 3)

- Design tradeoffs:
  - **Learning-free vs. Learning-based Flow2Action:** The paper notes the learning-free method (AVDC style) performs slightly better but requires depth/segmentation. The learning-based method is more flexible but requires action-labeled data `D_act`
  - **Skill Horizon (H):** Too short = no temporal abstraction; too long = difficult optimization/reconstruction

- Failure signatures:
  - **Visual Entanglement:** If skills are learned on pixels instead of flow, you see "contradictory motions" where the robot moves toward an object then away randomly
  - **Small Object Blindness:** Section 5.3 notes failure on small objects (cream/bottles) as flow estimators may miss subtle gradients
  - **Embodiment Mismatch:** Section 5.5 shows success, but failure occurs if the Flow2Action module hasn't seen the specific arm kinematics

- First 3 experiments:
  1. **Flow Reconstruction Sanity Check:** Train Stage 1 only. Visualize: Input Flow vs. Decoded Flow. If the decoded flow is blurry or missing the object, the tokens are insufficient
  2. **Token Clustering Visualization:** Run Stage 2 on a held-out video. Plot the sequence of token IDs. Do "reaching" motions across different tasks cluster to the same token ID?
  3. **Flow2Action Overfit Test:** Take a single trajectory from `D_act`. Train Stage 3 on only this trajectory. If it cannot overfit, the network capacity or the flow-to-action mapping logic is broken

## Open Questions the Paper Calls Out

### Open Question 1
Can extending SOF from 2D optical flow to 3D scene flow overcome limitations from occlusions and fixed camera positions? The authors state in Limitations: "our reliance on flow introduces certain limitations, such as occlusions between the robot arm and objects, sensitivity to visual instability, and dependence on fixed camera positions. To address these challenges, future work may explore alternative representations as action surrogates, such as extending to 3D using scene flow."

### Open Question 2
Can SOF be effectively extended to human and egocentric videos for scalable skill learning beyond robot demonstration data? The authors state: "Future work may extend SOF to broader data sources, such as human and egocentric videos, enabling more scalable and diverse skill learning beyond robot videos."

### Open Question 3
How can SOF better handle small objects and complex rotational motions where it currently underperforms? The authors report that SOF "struggles to detect small objects such as cream containers or bottles, and it fails to execute motions requiring large rotations, such as opening drawers" on LIBERO-GOAL tasks.

### Open Question 4
What mechanisms could close the performance gap between learning-free and learning-based Flow2Action modules while maintaining flexibility? The ablation study (Figure 10) shows the learning-free approach consistently outperforms the learning-based method, yet the authors use learning-based in main experiments because learning-free "requires additional inputs (e.g., depth, segmentation) and prior knowledge about the environment."

## Limitations

- Flow estimator accuracy is critical and brittle, particularly for small object manipulation tasks where subtle contact dynamics are missed
- Skill quantization with 1024 tokens and 32-frame blocks may not capture full diversity of manipulation skills, especially for fine-grained motor control
- Flow2Action mapping lacks theoretical grounding and depends heavily on specific robot embodiment kinematics
- Only tested on simulation benchmarks with specific camera viewpoints; real-world lighting variations and dynamic environments not addressed

## Confidence

**High Confidence:** The empirical results showing SOF outperforming baseline methods (Diffusion Policy, UniPi, A VDC, LAPA) on standard benchmarks. The MetaWorld multi-task success rate of 0.69 is directly measurable and reproducible.

**Medium Confidence:** The claim that optical flow is a superior intermediate representation compared to pixel-space alternatives. While supported by qualitative visualizations and the small object failure mode, this relies heavily on the assumption that current flow estimators capture all relevant motion dynamics.

**Low Confidence:** The generalizability of the skill quantization approach across different manipulation tasks and environments. The paper only tests on simulation benchmarks with specific camera viewpoints and doesn't address real-world variations.

## Next Checks

1. **Flow Estimation Quality Audit:** Quantify optical flow accuracy on small objects and subtle manipulation contacts by comparing FlowFormer++ outputs against ground truth optical flow in controlled scenarios. Measure precision-recall for detecting object motion vs. background motion across different object sizes.

2. **Skill Token Robustness Test:** Evaluate skill policy performance when flow estimation is degraded (adding noise, simulating occlusion) to determine the brittleness of the optical flow intermediate representation. Test if the skill codebook can recover from corrupted flow inputs.

3. **Cross-Embodiment Generalization Stress Test:** Extend the cross-embodiment evaluation to arms with significantly different kinematic structures (e.g., SCARA vs. 7-DOF arms) and quantify the performance drop. This would validate whether the flow-based abstraction truly decouples motion from embodiment.