---
ver: rpa2
title: 'Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies'
arxiv_id: '2505.14972'
source_url: https://arxiv.org/abs/2505.14972
tags:
- cultural
- safety
- culturally
- evaluation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROSS, a benchmark for evaluating the cultural
  safety of large vision-language models (LVLMs) across 16 countries, 3 domains, and
  14 languages. It features 1,284 visually grounded queries where cultural norm violations
  only emerge in context.
---

# Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies

## Quick Facts
- arXiv ID: 2505.14972
- Source URL: https://arxiv.org/abs/2505.14972
- Reference count: 40
- Key outcome: CROSS benchmark reveals significant cultural safety gaps in LVLMs, with top models scoring as low as 61.79% in awareness and 37.73% in compliance, while SFT and DPO enhancements substantially improve safety metrics.

## Executive Summary
This paper introduces CROSS, a benchmark for evaluating the cultural safety of large vision-language models (LVLMs) across 16 countries, 3 domains, and 14 languages. It features 1,284 visually grounded queries where cultural norm violations only emerge in context. A theory-driven framework, CROSS-Eval, measures four dimensions: awareness, education, compliance, and helpfulness. Evaluations of 21 leading LVLMs show significant safety gaps, with top models scoring as low as 61.79% in awareness and 37.73% in compliance. To address these gaps, the authors develop two enhancement strategies—supervised fine-tuning and preference tuning—which substantially improve GPT-4o's cultural safety metrics while preserving general multimodal performance.

## Method Summary
The authors introduce CROSS, a benchmark for evaluating cultural safety in LVLMs, featuring 1,284 visually grounded queries across 16 countries, 3 domains, and 14 languages. The CROSS-Eval framework measures four dimensions: awareness, education, compliance, and helpfulness. To improve safety, they develop Safety-SFT (converting CVQA MCQs to open-ended safety queries via GPT-4o pipeline) and Safety-DPO (contrastive pairs with 4 negative types). Training involves 1 epoch for SFT or mixed negative types for DPO. Results show significant safety gaps across 21 LVLMs, with enhancement strategies substantially improving GPT-4o's performance while maintaining general capability on MMMU and MME benchmarks.

## Key Results
- Top LVLMs scored as low as 61.79% in cultural awareness and 37.73% in compliance on CROSS-Eval
- Safety-DPO achieved the highest improvements across all four dimensions while maintaining general capability
- InternVL models showed minimal gains due to weak cultural grounding (58-59% CVQA accuracy vs. GPT-4o's 87.54%)

## Why This Works (Mechanism)
The framework works by creating contextually rich scenarios where cultural violations only become apparent when images are interpreted alongside queries, forcing models to demonstrate genuine cultural understanding rather than pattern matching. The four-dimensional evaluation captures different aspects of cultural safety: recognizing violations (awareness), explaining them (education), respecting boundaries (compliance), and providing practical guidance (helpfulness). The enhancement strategies work by explicitly training models on culturally diverse scenarios, with DPO showing superior performance due to its contrastive learning approach that helps models distinguish between culturally appropriate and inappropriate responses.

## Foundational Learning
- **Cultural Safety Dimensions**: Understanding the four dimensions (awareness, education, compliance, helpfulness) is needed to properly evaluate whether models can recognize, explain, respect, and guide around cultural norms. Quick check: Can you map each evaluation prompt type to its corresponding dimension?
- **Multimodal Context**: Cultural violations emerge only when visual and textual information are combined, requiring models to integrate cross-modal understanding. Quick check: Can you identify which queries would lose their cultural significance if presented as text-only?
- **Contrastive Learning**: Safety-DPO uses contrastive pairs to teach models the difference between appropriate and inappropriate cultural responses. Quick check: Can you identify the four types of negative responses used in the contrastive pairs?
- **General Capability Preservation**: Models must maintain performance on general benchmarks (MMMU, MME) while improving cultural safety. Quick check: What was the typical performance drop on MMMU/MME after SFT-only approaches?
- **Cultural Grounding**: Models need pretraining on culturally diverse data to effectively learn cultural safety. Quick check: Why did InternVL models show minimal improvement despite alignment training?
- **Evaluation Consistency**: Using multiple inference runs and different models helps ensure reliable cultural safety assessment. Quick check: How many inference runs were averaged for each evaluation?

## Architecture Onboarding
- **Component Map**: CVQA dataset → Safety-SFT/Safety-DPO data generation → Model training → CROSS-Eval assessment → General capability testing (MMMU/MME)
- **Critical Path**: Data generation (GPT-4o pipeline) → Model alignment (SFT/DPO) → CROSS-Eval measurement → General capability validation
- **Design Tradeoffs**: Country-level vs. finer cultural resolution; synthetic data generation vs. human annotation; single evaluator (GPT-4o) vs. multiple cultural experts
- **Failure Signatures**: Minimal improvement on smaller models (<60% baseline CVQA); general capability degradation (>2% drop on MMMU/MME); inconsistent evaluation scores across inference runs
- **First Experiments**: 1) Generate Safety-SFT data using GPT-4o pipeline with extraction, scenario, question, and response prompts; 2) Run baseline CROSS-Eval on target LVLM with system and dimension prompts; 3) Apply Safety-DPO with all 4 negative types and evaluate on CROSS and general benchmarks

## Open Questions the Paper Calls Out
- How can multimodal cultural safety benchmarks capture finer-grained cultural variations beyond country-level resolution, such as differences across cities, age groups, or subcultures?
- How can models be trained to navigate conflicting cultural norms when users from different cultural backgrounds interact in the same context?
- What mechanisms can enable users to personalize or calibrate what "cultural safety" means to them without requiring extensive technical knowledge?
- To what extent do culturally diverse pretraining corpora enable more effective cultural safety alignment compared to fine-tuning approaches alone?

## Limitations
- The evaluation framework relies heavily on GPT-4o as both generator and evaluator, potentially introducing cultural biases
- The 14 languages and 16 countries covered represent a limited global scope with significant gaps in African, Latin American, and Central Asian cultures
- The synthetic data generation process may introduce subtle cultural assumptions that skew benchmark metrics

## Confidence
- **High confidence** in the methodology's technical soundness and the general pattern of safety gaps across LVLMs
- **Medium confidence** in the absolute performance numbers due to potential evaluator bias from using GPT-4o as the sole judge
- **Medium confidence** in the enhancement strategies' effectiveness across all LVLMs, as results show significant variation between models
- **Low confidence** in the generalizability of the 14 languages and 16 countries to represent global cultural safety requirements

## Next Checks
1. **Cultural Expert Review**: Have cultural experts from the 16 represented countries independently evaluate a random 10% sample of CROSS queries to verify that cultural norm violations are accurately identified and contextualized beyond GPT-4o's judgments.

2. **Cross-Evaluator Validation**: Run the same CROSS-Eval assessment using at least two different large language models as evaluators (e.g., Claude, Gemini) to establish consistency in dimension scoring and identify potential GPT-4o-specific biases.

3. **Long-term Cultural Drift Test**: After applying Safety-DPO alignment, monitor model outputs over 3-6 months across different cultural contexts to detect any degradation or reinforcement of cultural stereotypes, particularly in underrepresented regions.