---
ver: rpa2
title: 'PreSumm: Predicting Summarization Performance Without Summarizing'
arxiv_id: '2504.05420'
source_url: https://arxiv.org/abs/2504.05420
tags:
- document
- documents
- summarization
- scores
- presumm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces PreSumm, a novel task to predict a document\u2019\
  s summarization performance without generating a summary. The authors show that\
  \ documents with low PreSumm scores often have coherence issues, complex content,\
  \ or lack a clear main theme."
---

# PreSumm: Predicting Summarization Performance Without Summarizing

## Quick Facts
- **arXiv ID:** 2504.05420
- **Source URL:** https://arxiv.org/abs/2504.05420
- **Reference count:** 25
- **Primary result:** Introduces PreSumm, a task to predict summarization difficulty from source documents alone, enabling pre-generation filtering and quality estimation without summary generation.

## Executive Summary
This paper introduces PreSumm, a novel task to predict a document's summarization performance without generating a summary. The authors show that documents with low PreSumm scores often have coherence issues, complex content, or lack a clear main theme. PreSumm models outperform baselines in two downstream tasks: identifying documents for manual summarization and filtering noisy documents in multi-document summarization. The supervised PreSummReg model achieves strong correlation with human ACU scores, even outperforming some reference-based metrics. PreSumm generalizes well to unseen datasets and can predict human summarizer agreement. Manual analysis confirms that low-scoring documents are more difficult for humans to read, aligning with PreSumm predictions.

## Method Summary
PreSummReg uses a Longformer-base-4096 encoder with a linear regression head on the [CLS] token to predict per-document average ACU scores across multiple summarization systems. The model is trained via MSE loss on the RoSE dataset (2,000 train / 500 test documents from CNNDM, XSum, SamSum). An alternative PreSummClas approach uses pairwise classification with RankNet. The frozen-backbone variant (PreSummRegFroz) keeps the Longformer encoder frozen during training.

## Key Results
- PreSummReg achieves Kendall τ = 0.321 correlation with human ACU scores, outperforming reference-free metrics
- Cross-dataset generalization: PreSummReg maintains strong correlation on DUC/TAC Pyramid annotations
- Downstream task success: 10-20% manual summarization budget selection yields higher average ACU scores than baselines
- Multi-document summarization improvement: Reordering by PreSumm score before truncation improves ROUGE across token limits

## Why This Works (Mechanism)

### Mechanism 1
Document summarization difficulty is partially a function of intrinsic source properties, not just model architecture. If multiple summarization systems fail on the same documents, shared document-level features (e.g., coherence, complexity) likely drive performance. PreSumm learns to detect these features and predict average system performance directly from the source. Core assumption: Difficulty factors are stable across systems and datasets. Evidence: Documents with low PreSumm scores often have coherence issues, complex content, or lack a clear main theme; Kendall τ = 0.446 for cross-system document ranking correlation. Break condition: If different summarization systems have fundamentally different failure modes, the assumption of shared difficulty drivers may not hold.

### Mechanism 2
A regression model over long-context document encodings can approximate average summarization quality without generation. The Longformer encoder processes up to 4096 tokens, producing a global [CLS] representation. A linear regression head maps this to a predicted ACU-style score, trained via MSE against averaged system scores. Core assumption: The [CLS] embedding captures global document properties relevant to summarization difficulty. Evidence: PreSummReg achieves Kendall τ = 0.321, outperforming reference-free metrics that require the generated summary. Break condition: If documents require reasoning beyond token-level attention, Longformer may underrepresent difficulty signals.

### Mechanism 3
Predicting difficulty enables pre-generation filtering, improving downstream summarization efficiency and quality. Low PreSumm scores identify documents likely to yield poor summaries. These can be routed to manual summarization (hybrid workflow) or removed from multi-document sets to reduce noise before model inference. Core assumption: PreSumm-predicted difficulty aligns with actual downstream quality degradation. Evidence: PreSummReg-based selection for manual replacement yields higher average ACU scores than baselines; reordering documents by PreSumm score before truncation improves ROUGE. Break condition: If removal biases content distribution, overall information coverage may suffer.

## Foundational Learning

- **Concept: Transformer-based long-document encoding (Longformer)**
  - Why needed here: PreSumm requires processing full documents (up to 4096 tokens) to capture global coherence and theme signals; standard BERT (512 tokens) would truncate key information.
  - Quick check question: Given a 2000-token document, would a standard BERT model suffice for PreSumm? Why or why not?

- **Concept: Correlation metrics for evaluation (Kendall τ, Spearman)**
  - Why needed here: The task is ranking documents by predicted summarization quality; correlation measures how well predictions align with gold-standard ACU scores across systems.
  - Quick check question: If PreSumm achieves Kendall τ = 0.321 against human ACU scores, what does this indicate about ranking quality?

- **Concept: Atomic Content Units (ACUs) for summarization evaluation**
  - Why needed here: PreSumm is trained and evaluated against ACU scores, which decompose summaries into verifiable factual statements, providing more reliable human judgments than Likert scales.
  - Quick check question: Why might ACU scores be preferred over ROUGE for training a difficulty predictor?

## Architecture Onboarding

- **Component map:**
  Source Document → Longformer Encoder (4096 tokens, frozen or fine-tuned) → [CLS] Token (768-dim) → Regression Head (Linear: 768 → 1) → Predicted Score d*j

- **Critical path:**
  1. Ensure input tokenization preserves up to 4096 tokens (avoid premature truncation)
  2. Verify [CLS] token extraction from the final Longformer layer
  3. Train regression head with MSE loss against averaged ACU scores; validate on held-out documents

- **Design tradeoffs:**
  - Fine-tuned Longformer (PreSummReg) vs. frozen encoder (PreSummRegFroz): Fine-tuning improves correlation (τ = 0.321 vs. 0.279) but requires more data and compute
  - Regression vs. pairwise classification: Regression is simpler; classification may better optimize ranking but requires O(n²) pairs during training

- **Failure signatures:**
  - Predictions near the mean for all documents → model underfitting or insufficient signal in training data
  - Strong performance on in-domain but near-zero on out-of-distribution → overfitting to dataset-specific features
  - Low correlation despite high training accuracy → label noise or mismatch between training objective and evaluation metric

- **First 3 experiments:**
  1. Replicate PreSummReg training on RoSE training split (2000 documents); evaluate Kendall τ on test split (500 documents) against ACU scores
  2. Ablate input length: truncate documents to 512, 1024, and 4096 tokens; measure correlation degradation to quantify long-context contribution
  3. Cross-dataset generalization test: apply PreSummReg (trained on RoSE/CNN-DM/XSum/SamSum) to DUC/TAC Pyramid annotations without retraining; compare against baselines per Table 4

## Open Questions the Paper Calls Out

### Open Question 1
How does the PreSumm model's performance generalize to domains outside of news summarization? The authors state in the Limitations section that the study focuses on the news domain only and that "we did not examine dataset out of the news domain. Therefore our conclusions are limited to this domain." It is unclear if the document features identified (news-style structure, inverted pyramid) apply to domains with different rhetorical structures, such as scientific papers, legal documents, or creative writing. Evaluation of PreSumm models on diverse datasets like PubMed (scientific), BillSum (legal), or literary texts to see if correlation with human scores holds would resolve this.

### Open Question 2
Can a PreSumm model be trained effectively on large-scale automatic metrics without inheriting their specific biases? The authors note in the Limitations that future work involves training on larger datasets, but "using ACU scores would be difficult... avoided by using an annotated metric... However, [it] will make us biased towards the chosen metric." There is a trade-off between the scarcity of human annotation (ACU) and the availability of automatic metrics (ROUGE), but it is unknown if learning to predict a flawed metric renders the PreSumm model useless for predicting actual human-perceived quality. A comparative study training separate PreSumm models on different automatic metrics (e.g., ROUGE vs. BERTScore) and evaluating their correlation with human judgments on a held-out test set would resolve this.

### Open Question 3
Can PreSumm predictions be utilized to automatically pre-process or edit a document to improve its eventual summarization quality? The introduction states that insights from PreSumm could "enable strategic pre-processing document edits to facilitate the summarization process," but the paper only demonstrates filtering or re-ordering documents, not modifying them. While the model identifies low-scoring documents (due to complexity or coherence), it is not established whether these issues can be algorithmically reversed (e.g., via simplification or coherence repair) to increase the PreSumm score and the final summary quality. An experiment where documents identified as "difficult" are rewritten or simplified by a text-editing model, followed by a successful increase in summarization performance (ACU scores), would resolve this.

## Limitations
- Limited generalization evidence: While PreSumm shows strong performance on out-of-domain datasets, the evaluation only covers two external test sets and hasn't been tested on entirely different document types or languages.
- Assumption of shared difficulty across systems: The core premise that document-level difficulty is consistent across summarization systems is plausible but not empirically validated across different architectures.
- ACU score reliability: PreSumm is trained and evaluated against ACU scores, which require human annotation and may not capture all aspects of summary quality like coherence or fluency.

## Confidence
- **High confidence:** The PreSummReg model architecture and training procedure are well-specified and reproducible. The correlation results on held-out RoSE data (Kendall τ = 0.321) are robust and statistically significant.
- **Medium confidence:** The cross-dataset generalization results suggest PreSumm captures domain-agnostic difficulty signals, but the evaluation scope is limited. The extrinsic task improvements demonstrate practical utility but rely on assumptions about downstream quality degradation.
- **Low confidence:** The claim that low PreSumm scores align with human readability judgments is supported by manual analysis but not quantitatively validated against human perception studies beyond the ACU annotations.

## Next Checks
1. **Cross-system failure mode analysis:** Systematically categorize which documents different summarization systems fail on to empirically validate whether shared difficulty factors drive PreSumm's predictions across architectures.

2. **Zero-shot cross-lingual transfer:** Apply PreSummReg (trained on English datasets) to non-English summarization benchmarks without fine-tuning to test whether difficulty detection generalizes beyond language.

3. **Long-context contribution quantification:** Ablate input length (512 vs. 4096 tokens) and measure correlation degradation to precisely quantify how much the Longformer's extended context contributes to PreSumm's performance.