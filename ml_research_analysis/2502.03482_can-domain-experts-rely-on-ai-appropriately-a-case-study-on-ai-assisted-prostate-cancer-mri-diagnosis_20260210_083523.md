---
ver: rpa2
title: Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate
  Cancer MRI Diagnosis
arxiv_id: '2502.03482'
source_url: https://arxiv.org/abs/2502.03482
tags:
- human
- performance
- cases
- radiologists
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how AI assistance affects decision-making
  accuracy among board-certified radiologists diagnosing prostate cancer from MRI
  scans. The research compares three conditions: human-only diagnosis, human-AI collaboration
  after independent diagnosis, and human-AI collaboration with upfront AI predictions
  and performance feedback.'
---

# Can Domain Experts Rely on AI Appropriately? A Case Study on AI-Assisted Prostate Cancer MRI Diagnosis

## Quick Facts
- arXiv ID: 2502.03482
- Source URL: https://arxiv.org/abs/2502.03482
- Reference count: 40
- Primary result: Human-AI teams outperform humans alone but underperform AI alone due to under-reliance

## Executive Summary
This study investigates how AI assistance affects diagnostic accuracy among board-certified radiologists diagnosing prostate cancer from MRI scans. The research compares three conditions: human-only diagnosis, human-AI collaboration after independent diagnosis, and human-AI collaboration with upfront AI predictions and performance feedback. Across 175 patient cases, the findings reveal that while human-AI teams consistently outperform humans alone, they still underperform AI alone due to under-reliance. Providing performance feedback and upfront AI assistance slightly increases AI adoption but does not significantly improve overall diagnostic accuracy. However, the ensemble of human-AI team decisions can outperform AI alone, suggesting promising directions for complementary human-AI collaboration.

## Method Summary
The study uses the PI-CAI challenge dataset with 1411 cases filtered from 1500, split into 1211 training and 200 test cases (75 for Study 1, 100 for Study 2 with 50 overlap). Three MRI sequences per case (T2W, ADC, DWI) are processed using nnU-Net for lesion segmentation. The AI model is trained from scratch with batch size 8, learning rate 0.001, AdamW optimizer, and 1000 epochs on NVIDIA A40 GPU. Evaluation metrics include AUROC, accuracy, sensitivity, specificity, NPV, PPV at patient level, and accuracy, sensitivity, PPV at lesion level with 10% overlap threshold. Statistical significance is tested using bootstrapped z-tests with 10,000 iterations and 95% confidence intervals.

## Key Results
- Human-AI teams consistently outperform human-only diagnosis across all metrics
- Human-AI teams underperform AI alone, demonstrating under-reliance on AI recommendations
- Upfront AI assistance with performance feedback slightly increases AI adoption but doesn't significantly improve accuracy
- Ensemble of human-AI team decisions can outperform AI alone

## Why This Works (Mechanism)
The study demonstrates that while radiologists can benefit from AI assistance, they tend to under-rely on AI recommendations when making diagnostic decisions. This under-reliance occurs despite AI predictions being generally more accurate than human diagnoses alone. The mechanism appears to involve cognitive factors where radiologists may overvalue their own expertise or be skeptical of AI predictions, leading them to override correct AI suggestions with incorrect human judgments. The study suggests that this behavior represents a fundamental challenge in human-AI collaboration for medical diagnosis.

## Foundational Learning
- Bootstrapped z-tests: why needed - to establish statistical significance of performance differences between groups; quick check - verify p-values match reported significance levels
- 10% overlap threshold: why needed - defines clinically meaningful lesion detection; quick check - ensure connected component analysis, not standard IoU
- Patient-level vs lesion-level metrics: why needed - captures both diagnostic and detection performance; quick check - verify metrics computed separately for each level

## Architecture Onboarding
**Component Map**: MRI Data -> nnU-Net Segmentation -> AI Predictions -> Human Diagnosis (3 conditions) -> Performance Metrics
**Critical Path**: Training data preparation → nnU-Net training → AI inference → Human evaluation → Statistical analysis
**Design Tradeoffs**: Single AI model (nnU-Net) vs ensemble approaches; binary classification vs multi-class; fixed 0.5 threshold vs adaptive
**Failure Signatures**: Data leakage across train/test; mismatched metric definitions; statistical test implementation errors
**First Experiments**:
1. Verify unique patient IDs across train/test splits with biopsy-confirmed test labels
2. Train nnU-Net with specified hyperparameters and validate AUROC reaches ~0.79
3. Implement bootstrapped z-test with 10,000 iterations and verify significance results

## Open Questions the Paper Calls Out
None

## Limitations
- Unknown reproducibility of exact train/test splits and data preprocessing details
- Mixture of annotation sources (biopsy, expert, AI-derived) introduces label consistency uncertainty
- Focus on single AI model architecture and one medical imaging task limits generalizability

## Confidence
- **High confidence**: Human-AI teams consistently outperform human-only diagnosis with clear statistical evidence
- **Medium confidence**: Under-reliance conclusion supported but influenced by study design requiring pre-commitment to diagnoses
- **Low confidence**: Upfront assistance claims based on small effect sizes and limited sample sizes in Study 2

## Next Checks
1. Reconstruct exact train/test split using patient IDs and verify biopsy-confirmed labels only with no patient overlap
2. Implement nnU-Net training pipeline with specified hyperparameters and validate AUROC reaches ~0.79 on test set
3. Conduct bootstrapped z-test significance analysis with 10,000 iterations to verify p-values align with reported statistical significance