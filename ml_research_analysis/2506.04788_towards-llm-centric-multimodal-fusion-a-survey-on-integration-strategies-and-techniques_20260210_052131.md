---
ver: rpa2
title: 'Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies
  and Techniques'
arxiv_id: '2506.04788'
source_url: https://arxiv.org/abs/2506.04788
tags:
- language
- wang
- zhang
- fusion
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically analyzes 125 MLLMs by proposing a novel
  taxonomy that classifies modality integration mechanisms into four types: Projection,
  Abstraction, Semantic Embedding, and Cross-attention. It further distinguishes three
  fusion levels (Early, Intermediate, Hybrid) and categorizes representation learning
  as Joint, Coordinated, or Hybrid.'
---

# Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques

## Quick Facts
- arXiv ID: 2506.04788
- Source URL: https://arxiv.org/abs/2506.04788
- Reference count: 40
- 125 MLLMs systematically classified into a 3D taxonomy framework

## Executive Summary
This survey addresses the fragmentation in multimodal large language model (MLLM) research by proposing a unified taxonomy that classifies 125 MLLMs based on their modality integration strategies. The authors introduce a three-dimensional framework analyzing models through Fusion Mechanisms (Projection, Abstraction, Semantic Embedding, Cross-attention), Fusion Levels (Early, Intermediate, Hybrid), and Representation Learning approaches (Joint, Coordinated, Hybrid). This systematic classification reveals how architectural components serve different functional purposes across models and provides researchers with a structured approach to understanding and designing multimodal systems.

## Method Summary
The authors conducted a qualitative analysis of 125 MLLM architectures published between 2021-2025, including prominent models like LLaVA, Flamingo, and BLIP-2. They developed a novel taxonomy by examining the connector modules between modality encoders and LLM backbones, tracing data flow patterns, and analyzing embedding space configurations. The classification framework distinguishes between different integration strategies based on component architecture, data flow timing, and training objectives. The analysis focuses on understanding how non-text modalities are mapped into LLM-compatible representations and fused with text tokens.

## Key Results
- Proposed a comprehensive 3D taxonomy classifying MLLMs by Fusion Mechanism, Fusion Level, and Representation Learning
- Revealed that identical architectural components (e.g., MLPs, Q-Formers) serve different functional purposes based on researcher intent
- Identified Projection layers as the dominant integration strategy, mapping visual features into LLM embedding space
- Demonstrated that Cross-attention enables dynamic modality fusion while Abstraction layers control token counts

## Why This Works (Mechanism)
The taxonomy works by systematically mapping architectural components to their functional roles in modality integration. Projection mechanisms align visual features with text embeddings through linear transformations, enabling seamless fusion. Cross-attention allows the LLM to dynamically attend to visual information during processing, creating context-dependent multimodal understanding. Abstraction layers reduce token complexity, making multimodal input manageable for LLMs. The framework captures how these mechanisms operate at different fusion levels (input vs. intermediate layers) and representation spaces (shared vs. aligned).

## Foundational Learning
- **Fusion Mechanism Classification**: Models use different connector components (MLPs, Q-Formers, Attention) to integrate modalities. Why needed: Determines how visual information is processed and made compatible with LLM architecture. Quick check: Identify the connector between encoder and LLM to classify mechanism.
- **Fusion Level Identification**: Data can be injected at input (Early), internal layers (Intermediate), or both (Hybrid). Why needed: Affects when and how cross-modal interaction occurs. Quick check: Trace data flow to see if interaction happens before first block or during processing.
- **Representation Learning Types**: Models use Joint (shared space), Coordinated (aligned spaces), or Hybrid approaches. Why needed: Determines embedding compatibility and fusion efficiency. Quick check: Examine loss functions and embedding space design.

## Architecture Onboarding
- **Component Map**: Modality Encoder -> Connector (Mechanism) -> LLM Backbone -> Output
- **Critical Path**: Visual feature extraction → Feature projection/attention → LLM token generation
- **Design Tradeoffs**: Early fusion offers simplicity but limited flexibility; Intermediate fusion provides better integration but increases complexity; Cross-attention enables dynamic fusion but requires architectural modifications.
- **Failure Signatures**: Misclassification occurs when component intent is unclear; Performance issues arise from token count mismatches or embedding space misalignment.
- **First Experiments**: 1) Classify a new MLLM using the 3D framework, 2) Compare models with identical mechanisms but different performance, 3) Test framework generalizability on recently published architectures.

## Open Questions the Paper Calls Out
1. How can advanced reasoning capabilities from text-only LLMs be effectively transferred to open-source multimodal counterparts? The paper notes that reasoning-focused LLMs have made significant progress, but their multimodal counterparts remain underdeveloped in open-source research.

2. What architectural mechanisms can provide MLLMs with persistent memory capabilities beyond standard context windows? Current MLLMs lack persistent memory mechanisms essential for more general intelligence.

3. How can the field establish standardized metrics for cross-modal evaluation and comparison of diverse MLLM architectures? Cross-modal evaluation remains difficult due to lack of standardized metrics across different modalities.

4. How can Retrieval-Augmented Generation be optimally incorporated into multimodal systems to mitigate hallucinations? RAG incorporation is identified as a primary research direction for addressing hallucination problems and improving accuracy.

## Limitations
- Classification relies heavily on implicit architectural intentions not explicitly documented in publications
- Qualitative analysis lacks quantitative validation of taxonomy's predictive performance across tasks
- Analysis limited to architectures published between 2021-2025, potentially missing emerging trends

## Confidence
- High confidence: Four mechanism categories are well-defined and distinguishable
- Medium confidence: Fusion level classification is generally reliable but can be ambiguous
- Low confidence: Representation learning distinctions require access to training details often not specified

## Next Checks
1. Apply the taxonomy to a recently published MLLM not included in the original 125-model analysis to test framework generalizability
2. Conduct a case study comparing models with identical mechanism labels but different performance outcomes
3. Interview MLLM researchers to validate whether proposed mechanism classifications align with intended design goals