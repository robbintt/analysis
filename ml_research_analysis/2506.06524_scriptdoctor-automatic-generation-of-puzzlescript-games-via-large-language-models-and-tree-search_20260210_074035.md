---
ver: rpa2
title: 'ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language
  Models and Tree Search'
arxiv_id: '2506.06524'
source_url: https://arxiv.org/abs/2506.06524
tags:
- games
- game
- puzzlescript
- generated
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ScriptDoctor, a system that uses large language
  models (LLMs) to automatically generate complete PuzzleScript games through iterative
  generation and testing. The system integrates LLM-generated code with compilation
  feedback from the PuzzleScript engine and tree search-based playtesting to create
  functional games.
---

# ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search

## Quick Facts
- arXiv ID: 2506.06524
- Source URL: https://arxiv.org/abs/2506.06524
- Authors: Sam Earle; Ahmed Khalifa; Muhammad Umair Nasir; Zehua Jiang; Graham Todd; Andrzej Banburski-Fahey; Julian Togelius
- Reference count: 24
- Primary result: ScriptDoctor automatically generates compilable and solvable PuzzleScript games using LLM-generated code iteratively repaired with compilation and playtesting feedback

## Executive Summary
ScriptDoctor presents an automated pipeline for generating complete PuzzleScript games using large language models. The system employs iterative generation where an LLM produces code, a modified PuzzleScript engine attempts compilation, and a BFS solver evaluates solvability. Feedback from both compiler errors and solver results guides subsequent LLM attempts, with the process continuing for up to 10 iterations. The approach demonstrates that few-shot prompting with human-authored games significantly improves code functionality, with reasoning models like o1 and o3-mini outperforming GPT-4o in generating compilable and playable games.

## Method Summary
The system uses a central Python server orchestrating interactions between an LLM and the PuzzleScript engine. The server sends prompts containing documentation, human-authored examples, and design ideas to the LLM, which returns PuzzleScript code. This code undergoes compilation testing and, if successful, BFS solver evaluation. The feedback loop continues for up to 10 iterations, with the LLM attempting to fix errors or improve solvability based on structured feedback. The pipeline tests three LLMs (GPT-4o, o1, o3-mini) with context lengths ranging from 10k to 70k tokens, measuring compilation success rates, solvability percentages, and solution complexity through BFS node counts.

## Key Results
- Few-shot prompting with ~30,000 tokens of human-authored games significantly improves code functionality
- Reasoning models (o1, o3-mini) outperform GPT-4o in generating compilable and solvable games
- Generated games show solution complexities ranging from 1,498 to 22,771 nodes explored by BFS solver
- Diminishing returns observed beyond 30,000 tokens of context window

## Why This Works (Mechanism)
The system works through iterative refinement where the LLM generates code based on examples and documentation, then receives targeted feedback about what went wrong. Compiler errors identify syntax issues, while solver feedback reveals gameplay problems like unsolvable levels or unintended exploits. This structured feedback loop allows the model to learn from its mistakes without requiring fine-tuning. The constrained nature of PuzzleScript—being a domain-specific language for 2D grid-based puzzle games—makes automated playtesting feasible through BFS, providing reliable quality metrics that guide generation.

## Foundational Learning
- Concept: PuzzleScript Domain-Specific Language (DSL)
  - Why needed here: The entire system is built around generating valid code in this specific, highly constrained language for 2D grid-based puzzle games
  - Quick check question: Can you describe the key sections of a PuzzleScript file (e.g., objects, rules, levels) and what a collision rule looks like?
- Concept: Breadth-First Search (BFS) Solver
  - Why needed here: This is the automated playtester. It determines if a generated game level is solvable and is the primary source of feedback for the LLM generator beyond basic compilation
  - Quick check question: How does a BFS solver guarantee finding the shortest solution path, and what are its limitations in very large or complex state spaces (e.g., exponential growth)?
- Concept: Large Language Model (LLM) In-Context Learning
  - Why needed here: The system does not fine-tune the LLM. It relies entirely on the model's ability to learn from the prompt (few-shot examples) and feedback (error messages) to produce valid output
  - Quick check question: Explain the difference between fine-tuning a model and providing it with few-shot examples in its context window at inference time

## Architecture Onboarding
- Component map: Python server -> LLM API -> PuzzleScript engine -> BFS solver -> Python server (iterative loop)
- Critical path: The most important data flow is the iterative repair loop: LLM Output -> Compiler/Solver -> Structured Feedback (errors/solvability data) -> LLM Prompt
- Design tradeoffs: The paper explicitly notes a tradeoff between the tractability of PuzzleScript for automated playtesting and the expressiveness of the generated games. While the system automates the entire pipeline, it limits itself to a specific genre of 2D puzzle games
- Failure signatures: Key failure modes include generating levels with overly short or non-existent solutions, implementing "broken" mechanics that make levels trivially solvable in unintended ways, and producing syntax errors that the LLM fails to correct within 10 iterations
- First 3 experiments:
  1. **Baseline Functionality:** Run the ScriptDoctor pipeline with GPT-4o using few-shot prompting (e.g., 30k tokens of context) and quantify the percentage of games that compile and are solvable
  2. **Model Comparison:** Run the same generation pipeline using reasoning models (o1, o3-mini) and compare the compilability rate, solvability percentage, and solution complexity (BFS node count) against the GPT-4o baseline
  3. **Feedback Ablation:** Modify the system to provide only compiler errors or only solver feedback to the LLM and measure the impact on the final generated game quality to understand the contribution of each feedback signal

## Open Questions the Paper Calls Out
- Can vision-language models (VLMs) effectively diagnose and repair "broken" game mechanics when provided with visual gameplay context?
- What specific inputs, beyond few-shot examples, can overcome the diminishing returns of context window scaling in this domain?
- Can novelty-seeking evolutionary algorithms leverage ScriptDoctor's metrics to generate more diverse games than the current iterative repair process?

## Limitations
- Limited to a specific genre of 2D puzzle games due to PuzzleScript's constraints
- Computational complexity of BFS solver becomes problematic for more complex games
- Lack of human evaluation means generated games may be technically functional but not enjoyable or well-designed

## Confidence
- High confidence in technical implementation of iterative pipeline and compilation/solvability metrics
- Medium confidence in comparative performance of different LLMs (limited experimental runs)
- Low confidence in claims about quality and novelty of generated game design (no empirical validation)

## Next Checks
1. Conduct statistical significance testing with multiple runs per configuration to validate comparative LLM performance claims
2. Implement alternative playtesting approaches (Monte Carlo Tree Search or heuristic-based solvers) to assess scalability beyond simple puzzle games
3. Perform human subject studies to evaluate actual playability and design quality of generated games, not just technical solvability