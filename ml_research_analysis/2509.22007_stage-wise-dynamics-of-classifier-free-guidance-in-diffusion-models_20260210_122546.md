---
ver: rpa2
title: Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models
arxiv_id: '2509.22007'
source_url: https://arxiv.org/abs/2509.22007
tags:
- guidance
- conditional
- diversity
- diffusion
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes Classifier-Free Guidance (CFG) in diffusion\
  \ models under multimodal conditionals, revealing a three-stage sampling dynamic:\
  \ early Direction Shift (toward the global mean), mid Mode Separation (basin-based\
  \ dynamics), and late Concentration (within-mode contraction). The analysis shows\
  \ CFG\u2019s early-stage acceleration induces initialization bias toward dominant\
  \ modes, suppressing weaker modes and reducing global diversity."
---

# Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models

## Quick Facts
- arXiv ID: 2509.22007
- Source URL: https://arxiv.org/abs/2509.22007
- Authors: Cheng Jin; Qitan Shi; Yuantao Gu
- Reference count: 40
- Key outcome: This paper analyzes Classifier-Free Guidance (CFG) in diffusion models under multimodal conditionals, revealing a three-stage sampling dynamic: early Direction Shift (toward the global mean), mid Mode Separation (basin-based dynamics), and late Concentration (within-mode contraction). The analysis shows CFG's early-stage acceleration induces initialization bias toward dominant modes, suppressing weaker modes and reducing global diversity. In the late stage, CFG amplifies within-mode contraction, suppressing fine-grained variability. Experiments on Stable Diffusion v3.5 confirm these predictions: strong early guidance reduces global diversity, while strong late guidance diminishes local variation. A time-varying guidance schedule derived from the theory—weaker in early/late stages, stronger in the middle—consistently improves both quality (ImageReward) and diversity (FID, saturation) across guidance scales and NFE budgets, particularly where vanilla CFG degrades. The findings explain diversity loss under CFG and suggest principled scheduling to mitigate it.

## Executive Summary
This paper provides a theoretical analysis of Classifier-Free Guidance (CFG) in diffusion models, revealing a three-stage sampling dynamic that explains how CFG affects diversity. The authors show that CFG induces an early-stage bias toward dominant modes, suppresses weaker modes in the middle stage, and amplifies within-mode contraction in the late stage. These mechanisms lead to reduced diversity in generated samples. The paper proposes a time-varying guidance schedule that adjusts the guidance scale throughout sampling to balance quality and diversity, which is validated experimentally on Stable Diffusion v3.5.

## Method Summary
The paper analyzes CFG dynamics using a continuous-time probability flow ODE formulation under Gaussian mixture assumptions. The key innovation is a time-varying guidance schedule (TV-CFG) that adjusts the guidance scale ω(t) throughout sampling: weaker in early/late stages, stronger in the middle. For early-stage validation, the authors compare high (ω=9) vs low (ω=3) guidance at 20% of NFE=50 iterations. For late-stage validation, they inject small noise perturbations at 20% iterations and compare constant-low (ω=3) vs late-high schedules. The TV-CFG schedule follows a triangular shape peaking mid-sampling with normalization ensuring ∫ω_t dt = ω. Experiments use Stable Diffusion v3.5 on COCO 2017 validation set with metrics including ImageReward, CLIP score, FID, and saturation.

## Key Results
- Theoretical analysis reveals three-stage dynamics: early Direction Shift (toward global mean), mid Mode Separation (basin-based), and late Concentration (within-mode contraction)
- Strong early guidance causes initialization bias toward dominant modes, suppressing weaker modes and reducing global diversity
- Strong late guidance amplifies within-mode contraction, diminishing fine-grained variability like texture and pose
- TV-CFG schedule (weaker early/late, stronger mid) consistently improves quality (ImageReward) and diversity (FID, saturation) across guidance scales
- TV-CFG particularly prevents quality collapse at low NFE budgets where vanilla CFG degrades

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In the early sampling stage (high noise), strong guidance causes trajectories to accelerate toward the conditional mean, inducing "norm inflation" and an initialization bias that suppresses weaker modes.
- **Mechanism:** Theoretically, under high noise, the score function reflects global statistics rather than mode details. The paper proves (Theorem 3.2) that CFG drives trajectories closer to $\omega \bar{\mu}$ (scaled mean) than the standard conditional flow. This displacement biases the initial state toward dominant modes before the sampling dynamics can distinguish individual mode basins.
- **Core assumption:** The conditional distribution can be modeled as a Gaussian mixture, and the noise level is sufficiently high that fine-grained mode structure is suppressed.
- **Evidence anchors:**
  - [abstract] "In the Direction Shift stage, guidance accelerates movement toward the weighted mean, introducing initialization bias and norm growth."
  - [section 3.1] Theorem 3.2 proves CFG trajectories remain closer to $\omega \bar{\mu}$ in expectation during early stages.
  - [corpus] Neighbor paper "Emergence of Distortions" supports the general phenomenon of diversity loss under CFG, though this paper provides the specific stage-wise causal chain.
- **Break condition:** If the guidance scale $\omega=1$ (vanilla conditional), the mean-shift bias disappears.

### Mechanism 2
- **Claim:** Mid-stage CFG dynamics are "neutral" regarding mode geometry, but the *inherited* initialization bias from the early stage effectively prevents trajectories from entering weaker mode basins.
- **Mechanism:** The paper establishes (Theorem 3.3) that the attraction basin for a weaker mode $U_{s2}$ exists and is independent of $\omega$. However, because the early stage displaced most trajectories toward the dominant mean, few trajectories physically reside in the weak mode's basin when the separation dynamics begin (Proposition 3.4).
- **Core assumption:** The Gaussian mixture components are sufficiently separated for distinct basins to exist.
- **Evidence anchors:**
  - [abstract] "...inherited bias suppresses weaker modes, reducing global diversity."
  - [section 3.2] Theorem 3.3 formalizes the persistence of weaker mode basins despite guidance.
  - [corpus] Corpus signals generally acknowledge the "diversity trade-off," which this mechanism explains via spatial displacement.
- **Break condition:** If the initial noise happens to land in the weak mode basin $U_{s2}$, the trajectory will converge to the weak mode regardless of $\omega$.

### Mechanism 3
- **Claim:** In the late stage (low noise), strong guidance amplifies "within-mode contraction," reducing fine-grained variability (e.g., texture, pose) among samples of the same class.
- **Mechanism:** As noise decays, the score function acts as a linear restoring force toward the mode center. Theorem 3.5 shows that scaling this score by $\omega > 1$ increases the contraction force, causing pairs of trajectories to converge more tightly than in standard conditional sampling.
- **Core assumption:** Noise is negligible, and dynamics are dominated by local geometry around the mode.
- **Evidence anchors:**
  - [abstract] "...guidance amplifies within-mode contraction, diminishing fine-grained variability."
  - [section 3.3] Theorem 3.5 proves pairwise distance decreases faster under CFG.
  - [corpus] Related work "Entropy Rectifying Guidance" addresses similar entropy reduction, consistent with this contraction finding.
- **Break condition:** If $\omega$ is reduced in the late stage (as proposed in the TV schedule), the excessive contraction is mitigated.

## Foundational Learning

- **Concept: Probability Flow ODE**
  - **Why needed here:** The paper analyzes diffusion not as a stochastic process, but as a deterministic ODE (Eq. 3) to rigorously prove trajectory divergence/convergence properties.
  - **Quick check question:** How does the score function $\nabla \log p_t(x_t)$ dictate the drift direction in the ODE formulation?

- **Concept: Classifier-Free Guidance (CFG)**
  - **Why needed here:** Understanding the linear interpolation between conditional and unconditional scores (Eq. 5) is required to see why scaling $\omega$ distorts the target distribution.
  - **Quick check question:** Why does setting $\omega > 1$ improve "semantic alignment" but violate the strict probability flow of the true conditional distribution?

- **Concept: Gaussian Mixture Models (GMMs)**
  - **Why needed here:** The paper uses GMMs (Assumption A2) as a tractable proxy for real-world multimodal distributions to analyze basin dynamics.
  - **Quick check question:** In a GMM, how does the distance between modes relate to the "attraction basins" discussed in Section 3.2?

## Architecture Onboarding

- **Component map:** Input: Noise $x_T$, Condition $y$, Time $t$ -> Core: Scheduler (replaced by Time-Varying Function $\omega(t)$) -> Flow: Standard Diffusion ODE Solver -> TV-CFG Scale Logic -> Denoiser

- **Critical path:**
  1. Identify the timestep $t$ or NFE step index $n$.
  2. Apply the TV schedule logic: Ramp up $\omega$ early, sustain high in mid-stage, ramp down late (Appendix D).
  3. Compute the guided score $\hat{s}_t$ using the dynamic $\omega(t)$.

- **Design tradeoffs:**
  - **Vanilla CFG:** High semantic fidelity, low diversity (risk of mode collapse).
  - **TV-CFG (Ours):** Balanced fidelity/diversity. Requires tuning the "peak" guidance and ramp rates.
  - **Interval-CFG:** Extreme diversity but risks semantic instability (Fig 4).

- **Failure signatures:**
  - **Oversaturation/Norm Inflation:** Images look washed out or artifact-heavy (Section 3.1). Caused by high early $\omega$.
  - **Identical Outputs:** Different seeds produce near-identical images (low MSE). Caused by high late $\omega$ (Section 4.1, Fig 3).

- **First 3 experiments:**
  1. **Diversity Validation:** Run "Early-High" vs "Early-Low" on a prompt with multiple valid interpretations (e.g., "A view of a bathroom"). Verify global layout diversity (Fig 2).
  2. **Contraction Validation:** Run "Late-High" vs Constant-Low with identical seeds + small perturbations. Verify MSE is lower in Late-High (Fig 3).
  3. **Robustness Check:** Compare TV-CFG vs Vanilla at low NFE (10-15 steps). Verify TV-CFG prevents the quality collapse seen in Vanilla (Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the theoretical stage boundaries ($t_{s1}, t_{s2}, t_{s3}$) and three-stage dynamics generalize to variance-preserving (VP) or other non-flow-matching noise schedules?
- Basis in paper: [explicit] Assumption (A3) restricts the analysis to a specific flow-matching noise schedule ($\alpha(t) = 1/(1-t)$) for technical convenience.
- Why unresolved: The timing of the transition from "Direction Shift" to "Mode Separation" depends on the signal-to-noise ratio trajectory, which differs substantially between flow-matching and VP-SDE formulations.
- What evidence would resolve it: Theoretical derivation of the stage-wise dynamics under a VP-SDE noise schedule or empirical verification of the time-varying guidance schedule's effectiveness when the transition points are adjusted for different solvers.

### Open Question 2
- Question: How does discretization error in low-NFE regimes specifically amplify the initialization bias and norm inflation observed in the Direction Shift stage?
- Basis in paper: [inferred] The theory is derived for continuous-time ODEs, but experiments show vanilla CFG "nearly collapses" at low NFEs (Table 2), which the authors attribute to norm inflation destabilizing dynamics.
- Why unresolved: The current theory describes the vector field's ideal behavior but does not quantify how large discretization steps interact with the guidance-induced acceleration in the early stage.
- What evidence would resolve it: A discrete-time error analysis specifically modeling the interaction between step size and the guidance-induced acceleration in the early stage.

### Open Question 3
- Question: Can the theoretical framework be extended to conditional distributions that are non-Gaussian or supported on low-dimensional manifolds?
- Basis in paper: [explicit] Assumption (A2) models the conditional distribution as a Gaussian mixture, which the authors acknowledge is a simplification of real-world data.
- Why unresolved: The proofs rely on closed-form Tweedie formulas and the geometric properties of Gaussian mixtures (Theorem 3.3, 3.5), which may not hold for complex, non-Gaussian data manifolds.
- What evidence would resolve it: Empirical analysis of trajectory dynamics on synthetic data with known non-Gaussian structure to see if the "Concentration" and "Mode Separation" behaviors persist.

### Open Question 4
- Question: Does the mechanism of within-mode contraction apply to modern guidance variants like CFG++ or APG, or do their corrective components mitigate this effect?
- Basis in paper: [inferred] While experiments show the TV schedule improves APG (Table 2), the theoretical justification for the "Concentration" stage (Theorem 3.5) is derived strictly for standard CFG.
- Why unresolved: Variants like APG modify the guidance vector to reduce artifacts, potentially altering the restoring force dynamics that cause within-mode contraction in the late stage.
- What evidence would resolve it: Analysis of pairwise trajectory distances in the late stage for CFG++ or APG to determine if they exhibit the same contraction properties as standard CFG.

## Limitations
- Theoretical analysis relies on idealized Gaussian mixture assumptions that may not capture real image manifold complexity
- Empirical validation limited to controlled perturbations rather than systematic ablation across diverse domains
- Time-varying schedule requires careful tuning of peak guidance and ramp rates for optimal performance
- Analysis focuses on unconditional diffusion; conditional generation dynamics may differ

## Confidence
- Stage-wise dynamics characterization: **High** - The theoretical proofs (Theorems 3.2-3.5) are mathematically rigorous and supported by controlled experiments
- TV-CFG performance improvements: **Medium** - Consistently validated on Stable Diffusion v3.5 with COCO, but generalization to other models/distributions requires further testing
- Diversity loss mechanism explanation: **Medium** - The three-stage framework provides compelling mechanistic insight, though real-world mode geometry may deviate from GMM assumptions

## Next Checks
1. Test TV-CFG on non-photographic domains (e.g., artistic generation, medical imaging) to assess generalization beyond COCO-style data
2. Implement systematic ablation of the TV schedule parameters (peak location, ramp steepness) across multiple guidance scales to identify robust design principles
3. Compare TV-CFG diversity metrics with alternative approaches like temperature scaling or entropy regularization under identical computational budgets