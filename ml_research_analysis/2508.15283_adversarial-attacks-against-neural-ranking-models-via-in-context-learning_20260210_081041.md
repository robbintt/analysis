---
ver: rpa2
title: Adversarial Attacks against Neural Ranking Models via In-Context Learning
arxiv_id: '2508.15283'
source_url: https://arxiv.org/abs/2508.15283
tags:
- adversarial
- documents
- ranking
- harmful
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Few-Shot Adversarial Prompting (FSAP), a black-box
  framework that leverages in-context learning to generate adversarial documents capable
  of deceiving neural ranking models (NRMs). Unlike prior approaches that edit existing
  documents, FSAP conditions LLMs on small sets of previously observed harmful examples
  to autonomously synthesize new misleading content.
---

# Adversarial Attacks against Neural Ranking Models via In-Context Learning

## Quick Facts
- arXiv ID: 2508.15283
- Source URL: https://arxiv.org/abs/2508.15283
- Authors: Amin Bigdeli; Negar Arabzadeh; Ebrahim Bagheri; Charles L. A. Clarke
- Reference count: 40
- Primary result: Few-Shot Adversarial Prompting (FSAP) framework generates adversarial documents that outrank factual content in neural ranking models with up to 97.2% success rate.

## Executive Summary
This paper introduces Few-Shot Adversarial Prompting (FSAP), a black-box framework that leverages in-context learning to generate adversarial documents capable of deceiving neural ranking models. Unlike prior approaches that edit existing documents, FSAP conditions LLMs on small sets of previously observed harmful examples to autonomously synthesize new misleading content. The framework is evaluated on the TREC 2020 and 2021 Health Misinformation Tracks, demonstrating that FSAP-generated documents consistently outrank factual content while maintaining high undetectability.

## Method Summary
FSAP is a black-box framework that uses in-context learning to generate adversarial documents. The framework has two instantiations: FSAPIntraQ uses harmful examples from the same query for topic fidelity, while FSAPInterQ generalizes across unrelated queries. The method constructs a prompt by concatenating query-document pairs representing harmful examples, then conditions an LLM on this sequence to produce a new document for a target query. The approach requires no gradient-based optimization and works with black-box LLM APIs.

## Key Results
- FSAPInterQ achieved a Mean Helpful Defeat Rate (MHDR) of up to 97.2% on TREC 2021
- Generated documents consistently outranked factual content across four neural ranking models
- FSAP demonstrated high undetectability, maintaining competitive rankings while evading adversarial detection
- The framework's effectiveness was confirmed across multiple LLMs including GPT-4o and DeepSeek-R1

## Why This Works (Mechanism)

### Mechanism 1: In-Context Pattern Transfer via Few-Shot Prompting
The framework leverages LLMs' in-context learning ability to infer manipulation patterns from a small support set, bypassing gradient-based optimization. FSAP constructs a prompt by concatenating query-document pairs that represent harmful examples, conditioning the LLM to produce new adversarial documents. The core assumption is that LLMs can generalize "adversarial style" from the support set to new target queries. If support set examples are semantically inconsistent, the LLM may fail to converge on a coherent adversarial pattern.

### Mechanism 2: Semantic Decoupling in Neural Ranking Models
FSAP exploits the potential decoupling between topical relevance and factual verification in NRMs. These models score documents based on semantic similarity and topical alignment, which FSAP manipulates by generating documents that maximize relevance signals while embedding misinformation. The core assumption is that target NRMs prioritize semantic fluency and query-document similarity over credibility signals. If the ranking model incorporates robust credibility verification, the attack's effectiveness would likely drop.

### Mechanism 3: Generalization via Inter-Query Prompting
FSAPInterQ demonstrates that adversarial patterns are transferable across distinct semantic domains, allowing attacks on new topics without specific harmful examples. The framework constructs support sets using harmful documents from unrelated queries, abstracting "adversarial priors" that transcend specific topic content. If the target query requires highly specialized domain knowledge not present in the few-shot examples, generated content may hallucinate into incoherence.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed: FSAP relies entirely on ICL to turn an LLM into an adversarial generator without weight updates
  - Quick check: How does the LLM's behavior change when you provide 3 examples of harmful documents versus 0 examples in the prompt?

- **Concept: Neural Ranking Models (NRMs)**
  - Why needed: These are the target systems that use deep semantic matching rather than keyword matching
  - Quick check: Why might a neural ranker score a grammatically fluent but factually false document higher than a clunky but true document?

- **Concept: Mean Helpful Defeat Rate (MHDR)**
  - Why needed: This is the primary metric measuring the fraction of helpful documents outranked by adversarial content
  - Quick check: If an attack achieves 50% MHDR on a set of 10 helpful documents, how many of them did the adversarial document outperform?

## Architecture Onboarding

- **Component map:** Support Set -> Prompt Constructor -> Generator LLM -> Victim NRM -> Evaluator
- **Critical path:** The Prompt Constructor, as selection and formatting of few-shot examples determine the trade-off between undetectability and attack success
- **Design tradeoffs:**
  - FSAPIntraQ vs. FSAPInterQ: IntraQ offers better topic fidelity but requires labeled harmful data for specific queries, while InterQ generalizes to new topics with zero-resource requirements but risks topic drift
  - Undetectability vs. Deception: Stronger adversarial stances may rank high but are easily flagged, while FSAPInterQ optimizes for balance
- **Failure signatures:**
  - Low Stance Alignment: LLM generates generic or factual responses instead of harmful ones
  - High Detection Rate: Generated text exhibits obvious LLM artifacts or excessive sensationalism
  - Low MHDR: Adversarial document is semantically off-topic, causing low relevance scores
- **First 3 experiments:**
  1. Baseline Comparison: Run FSAPInterQ against "Liar Attack" baseline on TREC 2021 using MonoT5, comparing MHDR scores
  2. Detection Pass Rate: Test generated documents from experiment 1 through a zero-shot GPT-4o detector to confirm higher detection pass rate
  3. Support Set Ablation: Run FSAPInterQ with k=1, 3, 5 shots and plot performance curve to verify plateau around k=5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical conditions determine the transferability and generalization success of few-shot adversarial prompting across diverse neural ranking architectures?
- Basis: The authors explicitly call for developing an "adversarial generalization theory" to characterize conditions under which attacks remain effective across different models and queries
- Why unresolved: The work relies on empirical observation without a formal mathematical framework explaining why specific few-shot patterns transfer effectively
- What evidence would resolve it: A formal theoretical framework providing generalization guarantees grounded in adversarial risk

### Open Question 2
- Question: How can the dynamics between adversarial document generation and detection mechanisms be formalized as a game-theoretic equilibrium problem?
- Basis: The conclusion identifies "Game-Theoretic modeling of detection and evasion dynamics" as a specific future direction
- Why unresolved: The paper evaluates attack success against static ranking models without modeling the feedback loop where defenders update detection tools
- What evidence would resolve it: A study modeling the attacker-defender interaction and identifying equilibrium strategies

### Open Question 3
- Question: Is Few-Shot Adversarial Prompting effective in domains where "harmful" content is defined by subjective bias rather than objective factual inversion?
- Basis: The evaluation is strictly limited to the TREC Health Misinformation Track with objective medical falsehoods
- Why unresolved: The framework demonstrates success in binary true/false contexts, but it's unclear if InterQ transfer works for subjective domains
- What evidence would resolve it: Experiments applying FSAP to political or social media datasets where harm is defined by bias or toxicity

## Limitations
- Dependence on LLM's in-context learning capability introduces variability across different models and support set compositions
- Potential topic drift when adversarial patterns are transferred across semantically distant domains
- Attack effectiveness assumes victim NRM lacks explicit credibility verification mechanisms

## Confidence
- **High Confidence:** Experimental results showing FSAPInterQ's superior MHDR performance (up to 97.2%) and undetectability are well-supported
- **Medium Confidence:** Claim about exploiting semantic decoupling in NRMs is plausible but relies on implicit assumptions about ranking model behavior
- **Low Confidence:** Generalizability of FSAPInterQ across all query types is uncertain due to potential topic drift

## Next Checks
1. Test FSAP-generated documents against multiple adversarial detection models (fine-tuned classifiers, rule-based filters) to assess undetectability beyond the GPT-4o detector
2. Evaluate FSAPInterQ on queries requiring highly specialized knowledge (legal, medical domains) to measure impact of topic drift on attack success
3. Incorporate credibility verification layer (fact-checking API or RAG grounding) into victim NRM and measure resulting MHDR reduction to validate semantic decoupling exploitation claims