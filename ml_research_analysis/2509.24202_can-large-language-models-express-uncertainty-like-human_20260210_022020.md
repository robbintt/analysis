---
ver: rpa2
title: Can Large Language Models Express Uncertainty Like Human?
arxiv_id: '2509.24202'
source_url: https://arxiv.org/abs/2509.24202
tags:
- answer
- confidence
- question
- predicted
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight, human-aligned approach to
  LLM uncertainty estimation through linguistic confidence (LC), where models express
  uncertainty using hedging language rather than numerical scores. To enable systematic
  evaluation, the authors construct a large-scale human-annotated dataset of hedging
  expressions mapped to confidence scores, and develop a near-zero-cost DistilRoBERTa-based
  mapper for efficient conversion.
---

# Can Large Language Models Express Uncertainty Like Human?

## Quick Facts
- arXiv ID: 2509.24206
- Source URL: https://arxiv.org/abs/2509.24206
- Authors: Linwei Tao; Yi-Fan Yeh; Bo Kai; Minjing Dong; Tao Huang; Tom A. Lamb; Jialin Yu; Philip H. S. Torr; Chang Xu
- Reference count: 40
- Primary result: LC+ prompting and fine-tuning framework achieve competitive uncertainty estimation through linguistic confidence expressions

## Executive Summary
This paper introduces a lightweight, human-aligned approach to LLM uncertainty estimation through linguistic confidence (LC), where models express uncertainty using hedging language rather than numerical scores. The authors construct a large-scale human-annotated dataset of hedging expressions mapped to confidence scores, and develop a near-zero-cost DistilRoBERTa-based mapper for efficient conversion. They conduct comprehensive studies across modern LLMs on QA benchmarks, showing that carefully designed prompting (LC+) achieves competitive calibration and discriminability comparable to strong baselines like semantic uncertainty.

## Method Summary
The authors propose a novel uncertainty estimation framework based on linguistic confidence (LC), where LLMs express uncertainty through hedging expressions rather than numerical scores. They construct a human-annotated dataset mapping hedging phrases to confidence levels and develop a DistilRoBERTa-based mapper for efficient conversion. The approach involves two main strategies: LC+ prompting (carefully designed prompts to elicit hedging expressions) and a fine-tuning framework that uses semantic uncertainty as supervision to train models to express uncertainty more faithfully. The method is evaluated across three QA benchmarks (SimpleQA, NQ-Open, PopQA) using Qwen3-8B models.

## Key Results
- Vanilla LC performs poorly, but LC+ prompting achieves competitive calibration and discriminability comparable to semantic uncertainty baselines
- Fine-tuning framework leveraging semantic uncertainty as supervision consistently improves uncertainty expression reliability on Qwen3-8B
- LC framework outperforms baselines like verbalized numerical confidence on NQ-Open benchmark
- LC is positioned as scalable, efficient, and user-friendly method for LLM uncertainty estimation

## Why This Works (Mechanism)
The paper's approach works by leveraging the natural language capabilities of LLMs to express uncertainty through hedging expressions, which humans intuitively understand. By mapping hedging phrases to confidence scores and training models to use these expressions appropriately, the framework aligns model uncertainty communication with human expectations. The LC+ prompting strategy carefully guides models to produce hedging expressions, while the fine-tuning framework uses semantic uncertainty as supervision to improve the reliability of uncertainty expression.

## Foundational Learning
- Hedging expressions in natural language - why needed: Core mechanism for linguistic confidence; quick check: Can map common hedging phrases to confidence intervals
- Confidence calibration in LLMs - why needed: Essential for reliable uncertainty estimation; quick check: Compare predicted vs actual accuracy across confidence levels
- Semantic uncertainty estimation - why needed: Serves as strong baseline and supervision signal; quick check: Verify semantic uncertainty methods produce reliable uncertainty scores
- Prompt engineering for uncertainty elicitation - why needed: LC+ requires careful prompt design; quick check: Test multiple prompt variations for hedging expression quality
- Cross-modal uncertainty expression - why needed: Paper identifies potential for extension beyond text; quick check: Explore how hedging might translate to visual or multimodal contexts

## Architecture Onboarding

**Component map:** Human annotation dataset -> DistilRoBERTa mapper -> LC+ prompting framework -> Fine-tuning with semantic supervision -> Uncertainty evaluation on QA benchmarks

**Critical path:** Human-annotated hedging dataset construction → Mapper training → LC+ prompt design → Model fine-tuning → Evaluation on QA benchmarks

**Design tradeoffs:** Linguistic confidence vs numerical scores (user-friendliness vs precision), zero-cost mapper vs potential accuracy loss, prompt-based vs fine-tuning approaches (flexibility vs performance)

**Failure signatures:** Poor hedging expression quality (vague or inappropriate hedging), miscalibration (over/under-confidence), context-dependency issues (polysemous hedging phrases), cultural/linguistic biases in hedging patterns

**First experiments:**
1. Test LC+ prompting effectiveness on a small set of questions before full benchmark evaluation
2. Evaluate mapper accuracy on held-out hedging expressions
3. Compare hedging expression quality between vanilla and LC+ prompting approaches

## Open Questions the Paper Calls Out
- Generalizability to other model architectures and scales beyond Qwen3-8B
- Cross-linguistic validity of hedging patterns and mappings
- Practical deployment considerations including latency and token costs
- Extension to reasoning and multimodal tasks beyond QA

## Limitations
- Empirical scope limited to specific model family (Qwen3-8B) and three QA datasets
- Human-annotated hedging dataset may reflect cultural and linguistic biases
- Evaluation framework assumes straightforward mapping between hedging phrases and confidence levels despite acknowledged context-dependency
- Improvement margins need statistical significance testing across multiple random seeds

## Confidence

- LC+ prompting effectiveness: Medium - results promising but limited to specific models and tasks
- Fine-tuning framework improvements: Medium - demonstrated improvements but sample efficiency unclear
- LC as "scalable, efficient, and user-friendly": Medium - theoretically sound but practical deployment needs more exploration

## Next Checks

1. Test the LC framework across diverse model families (GPT, Claude, open-source alternatives) and scales (7B, 70B+) to assess architectural generalizability

2. Conduct cross-linguistic validation to evaluate whether hedging patterns and mappings transfer across languages beyond English

3. Implement real-world deployment trials measuring actual user comprehension and trust calibration, comparing LC expressions against numerical confidence scores in practical applications