---
ver: rpa2
title: Text embedding models can be great data engineers
arxiv_id: '2505.14802'
source_url: https://arxiv.org/abs/2505.14802
tags:
- data
- adept
- embeddings
- text
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADEPT, a framework that uses text embeddings
  to replace conventional data engineering steps in time series classification. Instead
  of preprocessing, feature engineering, and cleaning, ADEPT serializes raw time series
  into text, embeds them with a pretrained LLM-based model, and applies a Variational
  Information Bottleneck (VIB) to filter noise and enhance information gain.
---

# Text embedding models can be great data engineers

## Quick Facts
- arXiv ID: 2505.14802
- Source URL: https://arxiv.org/abs/2505.14802
- Reference count: 40
- ADEPT achieves state-of-the-art accuracy across diverse domains without preprocessing or feature engineering

## Executive Summary
ADEPT is a framework that replaces conventional data engineering pipelines with text embeddings for time series classification. Raw time series are serialized into text, embedded using frozen pretrained text models, compressed with Variational Information Bottleneck (VIB), and classified using a Transformer autoencoder. The framework achieves state-of-the-art accuracy on diverse domains including astrophysics (97.83%), healthcare EEG data (73.68%), Bitcoin price forecasting (88.49%), and industrial IoT failure detection (74.35%), demonstrating that text embeddings can effectively replace complex preprocessing pipelines.

## Method Summary
ADEPT serializes raw time series into text format, embeds them using frozen pretrained text models (OpenAI or Nomic), applies VIB compression to filter noise and enhance task-relevant information, then uses a Transformer autoencoder for classification. The pipeline processes time series by chunking into M segments, serializing each chunk to text, generating embeddings, compressing with VIB to create compact latent codes, and applying multi-head attention over these codes for final classification. This approach eliminates traditional preprocessing steps like imputation, normalization, and feature engineering.

## Key Results
- PLAsTiCC astrophysics: 97.83% accuracy on 15-class classification
- SelfRegulationSCP2 healthcare EEG: 73.68% accuracy on 8-class classification
- Bitcoin price forecasting: 88.49% accuracy on 3-class classification
- HRI IoT failure detection: 74.35% accuracy on 6-class classification
- VIB compression is critical: v1.0 (no VIB) achieves only 45.40% on Bitcoin vs. 88.49% for v2.0

## Why This Works (Mechanism)

### Mechanism 1
Pretrained text embeddings capture task-relevant spatiotemporal structure from raw time series without domain-specific preprocessing. Raw time series segments are serialized to text and processed by a frozen text embedding model whose distributional priors—learned from web-scale corpora—encode sequential patterns that transfer to numerical time series structure. The core assumption is that entropy in textually dense representations approximates that of numerically dense engineered features.

### Mechanism 2
Variational Information Bottleneck (VIB) compresses high-dimensional text embeddings into task-relevant codes, filtering noise and improving generalization. A stochastic encoder maps embeddings to a latent Gaussian distribution with KL divergence penalty forcing the latent distribution toward a standard normal prior, discarding information not predictive of class labels while retaining task-relevant signal. The core assumption is that high-dimensional text embeddings contain substantial noise relative to the downstream classification task.

### Mechanism 3
Multi-head attention over VIB-filtered embeddings captures both intra-view temporal dependencies and inter-view cross-correlations for final classification. After VIB compression, embeddings from each temporal view are assembled into sequences and processed by a Transformer autoencoder. The core assumption is that temporal dependencies within views and cross-view correlations are both predictive of class labels.

## Foundational Learning

- **Variational Autoencoders / Information Bottleneck**: Understanding how VIB compresses representations via KL-regularized latent spaces; the reparameterization trick enables gradient flow through stochastic encoders. Quick check: Can you explain why minimizing KL divergence toward N(0,I) encourages disentangled, compact latent codes?

- **Text Embeddings and Distributional Semantics**: ADEPT relies on frozen text embedders to represent serialized time series; understanding their properties (dimensionality, sequence handling, tokenization) informs chunking strategy. Quick check: What happens if a serialized time series segment exceeds the embedding model's maximum context length?

- **Transformer Attention and Positional Encoding**: The final classifier is a Transformer autoencoder; understanding multi-head self-attention, positional encodings, and autoencoder pretraining is required to debug and tune this stage. Quick check: Why does the autoencoder pretrain on reconstruction before fine-tuning for classification?

## Architecture Onboarding

- **Component map**: RFR Processing -> Text Embedding -> VIB Encoder -> Transformer Classifier
- **Critical path**: Chunk size (M) selection affects both embedding quality and sequence length for Transformer; VIB dimension (d) and β control compression strength—critical for noise filtering; per-view VIB training must converge before Transformer pretraining begins
- **Design tradeoffs**: Fewer chunks (larger L) may overload embedder or mix heterogeneous patterns; more chunks (smaller L) risks fragmenting temporal dependencies and increases sequence length; higher β provides more compression but risks underfitting; on-prem vs. API embedders trade privacy/control against potential accuracy
- **Failure signatures**: ADEPT v1.0 underperforming v2.0 indicates unfiltered embeddings carry noise; Bitcoin v1.0 at 45.4% vs. v2.0 at 88.5% shows raw embeddings insufficient for noisy financial data; rare classes with zero recall (HRI dataset class 7009) indicate imbalanced data + compression may over-represent majority classes
- **First 3 experiments**: 1) Replicate PLAsTiCC experiment with M=10 chunks, d=256 VIB, β=1e-4; verify ~97% accuracy and inspect t-SNE cluster separation before/after VIB. 2) Ablate VIB: Compare v1.0 vs. v2.0 on SelfRegulationSCP2 to quantify VIB contribution (58.97% → 73.68%). 3) Vary β (1e-5, 1e-4, 1e-3) on Bitcoin dataset to find compression sweet spot and observe tradeoff between noise suppression and task-relevant information loss.

## Open Questions the Paper Calls Out

1. How can the optimal number of chunks (M) for time series decomposition be determined systematically, rather than through manual tuning or domain intuition? The paper acknowledges this as a trade-off but only provides heuristic guidance.

2. What properties of text embedding models determine their effectiveness for time series representation, and can we predict which embedding models will work best for a given time series domain? The paper treats the embedding model as a black box without investigating what makes certain pretrained text embeddings suitable for numerical sequences.

3. Why does ADEPT v1.0 fail catastrophically on the Bitcoin dataset (45.4% accuracy) while succeeding on others, and what data characteristics predict when the VIB component is essential versus optional? The paper demonstrates that VIB helps but does not explain why the raw embeddings fail so dramatically on financial data specifically.

4. Can the information-theoretic equivalence between "textually dense raw format representation" and "numerically dense vector representations" be formally proven or quantified? The claim is stated as an intuition rather than formally derived or empirically validated through information-theoretic measurements.

## Limitations
- Exact serialization format for text embeddings is not fully specified, which could impact reproducibility and embedding quality
- HRI dataset used in experiments is proprietary, limiting independent validation across all four test domains
- Paper does not provide extensive ablation studies on chunk size optimization or systematic exploration of the VIB compression parameter space
- While VIB mechanism is claimed critical, paper lacks detailed analysis of what specific types of noise or redundancy are being filtered out

## Confidence
- High confidence in core claim that VIB-filtered text embeddings outperform raw embeddings (v2.0 consistently beats v1.0)
- Medium confidence in generalizability claim across domains (strong results on three public datasets but limited access to fourth)
- Medium confidence in mechanism explanation for why text embeddings transfer to time series (relies on intuitive arguments rather than empirical validation)

## Next Checks
1. Ablation study on serialization format: Test multiple text serialization approaches (CSV, space-delimited, timestamp-included vs. excluded) to determine sensitivity to formatting choices and identify optimal representation for different time series characteristics.

2. VIB parameter sensitivity analysis: Systematically vary β (1e-5, 1e-4, 1e-3, 1e-2) and latent dimension d (128, 256, 512) across all datasets to map the compression-accuracy tradeoff curve and identify whether optimal parameters are domain-specific or transferable.

3. Transferability stress test: Create synthetic time series with characteristics fundamentally different from typical text (e.g., extremely high-frequency oscillations, non-sequential patterns) to empirically test the break condition where cross-modal transfer fails, validating the core assumption about embedding generalization.