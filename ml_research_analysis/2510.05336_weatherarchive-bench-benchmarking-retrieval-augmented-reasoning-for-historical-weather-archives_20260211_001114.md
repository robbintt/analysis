---
ver: rpa2
title: 'WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical
  Weather Archives'
arxiv_id: '2510.05336'
source_url: https://arxiv.org/abs/2510.05336
tags:
- uni00000013
- uni00000011
- climate
- weather
- historical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'WeatherArchive-Bench addresses the challenge of evaluating retrieval-augmented
  reasoning systems on historical weather archives, which are rich in societal vulnerability
  and resilience narratives but difficult to process due to archaic language and OCR
  noise. The benchmark introduces two tasks: WeatherArchive-Retrieval, which tests
  retrieval models'' ability to locate relevant passages from over one million news
  segments, and WeatherArchive-Assessment, which evaluates LLMs'' capacity to classify
  vulnerability and resilience indicators from extreme weather narratives.'
---

# WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives

## Quick Facts
- **arXiv ID**: 2510.05336
- **Source URL**: https://arxiv.org/abs/2510.05336
- **Reference count**: 40
- **Primary result**: Hybrid retrieval (BM25 + cross-encoder) outperforms pure dense models on historical archives; LLMs struggle with implicit socio-environmental reasoning despite strong explicit factual extraction

## Executive Summary
WeatherArchive-Bench introduces a benchmark for evaluating retrieval-augmented reasoning systems on historical weather archives, addressing the challenge of processing documents rich in societal vulnerability and resilience narratives but degraded by archaic language and OCR noise. The benchmark consists of two tasks: WeatherArchive-Retrieval tests retrieval models' ability to locate relevant passages from over one million news segments, while WeatherArchive-Assessment evaluates LLMs' capacity to classify vulnerability and resilience indicators from extreme weather narratives. Experiments reveal that hybrid retrieval approaches combining BM25 with cross-encoders significantly outperform pure dense models, while even advanced LLMs struggle with implicit socio-environmental reasoning despite excelling at explicit factual extraction. These results highlight the need for improved methods to handle historical archival data and complex climate-domain reasoning.

## Method Summary
The benchmark utilizes 1,035,862 OCR-corrected passages (~256 tokens each) from historical newspapers spanning 1880-1899 and 1995-2014, covering Southern Quebec. Retrieval is evaluated using sparse methods (BM25plus, BM25okapi, SPLADE), dense embeddings (ANCE, SBERT, OpenAI/Gemini/Granite/Arctic), and hybrid approaches combining BM25 candidate generation with cross-encoder reranking. The Assessment task employs LLMs (GPT, Claude, Gemini, Qwen, LLaMA, Mistral, DeepSeek) to classify vulnerability/resilience indicators and generate free-form answers, with evaluation using classification metrics (F1, precision, recall), QA metrics (BLEU, ROUGE, BERTScore, token F1), and LLM-as-judge scoring. Oracle labels were generated using GPT-4.1 with human validation showing moderate inter-annotator agreement (κ=0.67).

## Key Results
- Hybrid retrieval (BM25 + cross-encoder) achieves significantly higher Recall@3 (0.585) than pure dense methods (0.075-0.420)
- LLMs achieve strong performance on explicit exposure indicators (F1=0.547) but struggle with implicit sensitivity indicators (F1=0.406)
- Functional and spatial scale classification proves more difficult than temporal scale, with models over-predicting transportation and national scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval (BM25 + cross-encoder) outperforms pure dense retrieval on historical archives with domain-specific terminology
- Mechanism: Sparse BM25 captures exact lexical matches on specialized/archaic terms (e.g., "inundation" vs. modern "flood"), while cross-encoders refine top-k candidates via fine-grained query-document interaction modeling
- Core assumption: Climate-related queries contain technical terminology where exact lexical overlap is more discriminative than dense semantic similarity; historical vocabulary shifts are not well-represented in dense embedding spaces trained on contemporary web text
- Evidence anchors: [abstract] "Experiments show that hybrid retrieval approaches combining BM25 with cross-encoders outperform pure dense models"; [section 5.1] "BM25 variants continue to perform strongly, often matching or surpassing dense alternatives in ranking quality at top k... In such cases, exact lexical matching is critical as sparse methods are able to capture these specialized terms directly"

### Mechanism 2
- Claim: LLMs perform better on explicit factual extraction (e.g., infrastructure damage) than on implicit socio-environmental reasoning (e.g., governance quality, social capital)
- Mechanism: Exposure and adaptability indicators are often explicitly stated in historical narratives (e.g., "dam collapsed," "relief supplies sent"), while sensitivity requires inferring contextual factors like institutional strength that are rarely directly documented
- Core assumption: The model's factual extraction capability relies on pattern recognition from pretraining; implicit reasoning requires integration of contextual cues and domain knowledge that may not be present or learnable from surface patterns alone
- Evidence anchors: [abstract] "even advanced LLMs struggle with implicit socio-environmental reasoning"; [section 5.2] "Models perform well on explicit indicators of exposure and adaptability... In contrast, sensitivity indicator classification requires reasoning about the degree to which a system is affected... factors that are seldom directly expressed in historical archives"

### Mechanism 3
- Claim: OCR noise and archaic language in historical archives degrade both retrieval effectiveness and LLM comprehension compared to clean contemporary text
- Mechanism: OCR errors create token mismatches for sparse retrieval; dense embeddings may smooth over corrupted tokens but lose semantic precision; LLMs have limited exposure to historical archives in pretraining corpora
- Core assumption: Post-OCR correction (e.g., GPT-4o-based cleaning) sufficiently normalizes text for downstream tasks; the remaining gap is due to vocabulary/distribution shift, not unrecoverable text degradation
- Evidence anchors: [abstract] "archaic language and OCR noise" as key challenges; [section 3.1] "digitized via OCR and subsequently cleaned to correct recognition errors via GPT-4o"

## Foundational Learning

- Concept: Sparse vs. Dense Retrieval Tradeoffs
  - Why needed here: Understanding why BM25 outperforms dense embeddings on historical terminology is critical for designing retrieval pipelines for specialized domains
  - Quick check question: Given a query with archaic term "tempest" (historical storm), would a dense embedder trained on modern news corpora likely match passages using "hurricane"? What does BM25 do instead?

- Concept: Vulnerability-Resilience Framework (Exposure, Sensitivity, Adaptability)
  - Why needed here: The Assessment task operationalizes climate social science concepts; without understanding the framework, LLM outputs cannot be meaningfully evaluated
  - Quick check question: If a passage describes a flood destroying a bridge and emergency crews repairing it within 48 hours, which vulnerability dimensions are explicitly vs. implicitly determinable?

- Concept: Cross-Encoder Reranking Architecture
  - Why needed here: The best-performing retrieval configuration uses BM25 for candidate generation + cross-encoder for reranking; understanding the two-stage pipeline is essential for replication and optimization
  - Quick check question: Why does applying a cross-encoder to all 1M+ passages scale poorly? What is the computational tradeoff compared to dense retrieval alone?

## Architecture Onboarding

- Component map:
  1. Corpus: 1,035,862 OCR-corrected, chunked passages (~256 tokens each) from historical newspapers (1880-1899, 1995-2014)
  2. Retrieval stage: Sparse (BM25plus/okapi, SPLADE), Dense (ANCE, SBERT, OpenAI/Gemini/IBM/Snowflake embeddings), Hybrid (BM25 + cross-encoder reranking)
  3. Assessment stage: LLMs (GPT, Claude, Gemini, Qwen, LLaMA, Mistral, DeepSeek) classify vulnerability/resilience indicators and generate free-form answers
  4. Evaluation: Classification metrics (F1, precision, recall), QA metrics (BLEU, ROUGE, BERTScore, token F1), LLM-as-judge

- Critical path:
  1. Ingest corpus → chunk → embed/index → retrieve top-k for each query → feed retrieved context to LLM → classify indicators OR generate answer → compare to oracle labels
  2. Bottleneck: Retrieval quality directly gates downstream answer quality; errors in retrieval cannot be recovered by stronger LLMs

- Design tradeoffs:
  1. Sparse vs. dense: Sparse preserves exact terminology but lacks semantic generalization; dense captures semantic similarity but fails on domain/archaic vocabulary shift
  2. Two-stage retrieval: Higher precision at top-k but added latency and compute from cross-encoder; justified when downstream tasks are sensitive to top-rank quality
  3. Oracle generation with LLMs (GPT-4.1): Scales annotation but assumes LLM alignment with expert judgment (validated at κ=0.67 inter-annotator agreement, ω=0.75 LLM win rate vs. humans)

- Failure signatures:
  1. Dense retriever returns semantically related but terminologically mismatched passages (e.g., "storm warnings" instead of "storm surge fatalities")
  2. LLM under-predicts sensitivity or mislabels functional/spatial scales (e.g., over-predicts "transportation," misses "local")
  3. OCR artifacts surviving post-correction cause token-level retrieval misses

- First 3 experiments:
  1. Reproduce Table 2 retrieval results: Compare BM25plus, BM25okapi, SPLADE, and top-3 dense embeddings on Recall@10 and nDCG@10 to validate hybrid superiority
  2. Ablate OCR correction: Evaluate retrieval with raw vs. GPT-4o-corrected text to quantify noise impact on Recall@k
  3. Probe implicit reasoning: Select passages where sensitivity is determinable only via inference; compare zero-shot vs. chain-of-thought-prompted LLM performance to isolate reasoning gaps from extraction gaps

## Open Questions the Paper Calls Out
- What retrieval augmentation strategies can effectively adapt models to archaic vocabulary and historical narrative structures in archival documents?
- How can LLMs be improved to reason about implicit socio-environmental relationships such as governance quality and social capital that are rarely stated explicitly in historical archives?
- What architectural or training modifications enable models to better assess cross-system dependencies and multi-scale coordination in resilience analysis?

## Limitations
- The unavailability of raw OCR documents due to copyright restrictions prevents independent verification of OCR correction impact on retrieval performance
- The LLM-as-judge evaluation framework introduces uncertainty about alignment between automated scoring and expert human judgment, particularly for implicit reasoning tasks
- Moderate inter-annotator agreement (κ=0.67) suggests ambiguity in human labeling that may affect assessment task results

## Confidence
- **High Confidence**: Experimental results demonstrating hybrid retrieval superiority are well-supported by multiple retrieval metrics across different query types
- **Medium Confidence**: Claims about LLM struggles with implicit reasoning are supported by structured classification results, but underlying mechanisms could involve multiple factors
- **Low Confidence**: Assertions about OCR noise and archaic language as primary performance drivers cannot be independently verified due to unavailable raw text

## Next Checks
1. Implement controlled experiment comparing BM25, dense, and hybrid retrieval on corpus subset with raw OCR text available to quantify OCR correction impact
2. Systematically vary reranking depth (top-10, top-50, top-100) to measure marginal benefit of cross-encoder versus increasing sparse candidates
3. Select inference-only sensitivity passages and compare zero-shot, few-shot, and chain-of-thought prompting across multiple LLM families to isolate reasoning gaps from extraction gaps