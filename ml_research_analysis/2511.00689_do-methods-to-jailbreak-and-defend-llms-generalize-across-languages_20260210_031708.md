---
ver: rpa2
title: Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?
arxiv_id: '2511.00689'
source_url: https://arxiv.org/abs/2511.00689
tags:
- languages
- standard
- language
- unsafe
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic multilingual evaluation
  of jailbreak attacks and defenses across ten languages, including high-, medium-,
  and low-resource languages. The study evaluates two state-of-the-art jailbreak methods
  (logic-based and adversarial prompt-based) on six LLMs using two adversarial datasets,
  HarmBench and AdvBench.
---

# Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?

## Quick Facts
- **arXiv ID:** 2511.00689
- **Source URL:** https://arxiv.org/abs/2511.00689
- **Reference count:** 12
- **Primary result:** High-resource languages are safer under standard queries but more vulnerable to logic-based jailbreaks, revealing a trade-off between linguistic proficiency and safety robustness.

## Executive Summary
This paper presents the first systematic multilingual evaluation of jailbreak attacks and defenses across ten languages, including high-, medium-, and low-resource languages. The study evaluates two state-of-the-art jailbreak methods (logic-based and adversarial prompt-based) on six LLMs using two adversarial datasets, HarmBench and AdvBench. Results show that high-resource languages are generally safer under standard queries but more vulnerable to adversarial attacks, revealing a trade-off between linguistic proficiency and safety robustness. A simple multilingual classifier effectively detects unsafe responses, though performance varies by language and model. The findings highlight that safety alignment does not uniformly transfer across languages, underscoring the need for language-aware safety benchmarks and more robust multilingual defenses.

## Method Summary
The study evaluates jailbreak attacks and defenses across 10 languages using HarmBench and AdvBench datasets. Logic-based jailbreaks convert harmful queries to logical expressions, while Andrius25 applies an optimized adversarial suffix. Defenses include self-verification prompts and classifier-based detection using response embeddings. The evaluation covers six LLMs (GPT-4o, Claude Sonnet 4, Qwen2.5-7b/14b, Llama3.1-8b/70b) with inference at temperature 0. Unsafe rates are measured across languages, with GPT-4o-mini used for few-shot prompting and GPT-4o as the safety judge.

## Key Results
- High-resource languages (e.g., English, Spanish) are safer under standard queries but more vulnerable to logic-based jailbreaks
- A classifier trained on unsafe responses effectively detects unsafe outputs from novel jailbreaks across languages
- Safety alignment does not uniformly transfer across languages, with low-resource languages showing higher baseline unsafe rates

## Why This Works (Mechanism)

### Mechanism 1: Proficiency-Robustness Trade-off
Higher linguistic proficiency may paradoxically increase vulnerability to semantic jailbreaks while decreasing vulnerability to standard harmful queries. Models handle high-resource languages with high semantic precision, allowing them to robustly refuse standard harmful requests but also accurately interpret and execute complex adversarial instructions that lie outside standard safety training distributions.

### Mechanism 2: Output-Based Detection Generalization
A classifier trained on unsafe responses to standard queries can generalize to detect unsafe responses elicited by novel jailbreaks, even without seeing the jailbreak technique. The semantic footprint of harmful content remains relatively consistent across different prompting strategies and languages within the embedding space.

### Mechanism 3: Linguistic Resource Disparity in Safety Alignment
Safety alignment fails to transfer uniformly to low-resource languages, resulting in higher leakage of harmful content even without complex attacks. Reinforcement Learning from Human Feedback (RLHF) and safety fine-tuning are predominantly performed on English or high-resource language data, leaving low-resource language representations insufficiently mapped to cross-lingual safety alignment.

## Foundational Learning

- **Concept: Safety Alignment (RLHF)**
  - **Why needed here:** The paper analyzes attacks that bypass this specific training phase. Understanding that alignment creates a "refusal" behavior helps explain why attacks seek "out-of-distribution" inputs.
  - **Quick check question:** Why might a model refuse a direct request but comply with the same request encoded in logic? (Answer: The logic format was likely not included in the refusal training data).

- **Concept: High- vs. Low-Resource Languages (HRL/LRL)**
  - **Why needed here:** The core independent variable of the study. Resource level dictates the strength of the model's "competence" and "safety" in that language.
  - **Quick check question:** Why is English considered "safe" for standard queries but "unsafe" for logic jailbreaks? (Answer: High competence in English allows the model to understand the harmful intent of complex logic, which it wasn't specifically trained to refuse).

- **Concept: Logic-based vs. Adversarial Suffix Jailbreaks**
  - **Why needed here:** The paper distinguishes these two attack vectors. Logic attacks exploit reasoning; Suffix attacks exploit token probabilities.
  - **Quick check question:** Which jailbreak method relies on the model's ability to perform formal reasoning? (Answer: Logic-based jailbreaks).

## Architecture Onboarding

- **Component map:** HarmBench/AdvBench (Translated into 10 languages) -> Logic Jailbreak (Predicate translation) & Adversarial Suffix (Andrius25) -> 6 LLMs (Open/Closed source) -> Responses -> Safety Judge (GPT-4o)

- **Critical path:** The translation of jailbreaks is critical. For Logic Jailbreak, predicates must be translated while preserving logical form. For Andrius25, the optimized suffix is often kept in English/Code tokens even for non-English queries.

- **Design tradeoffs:**
  - Defense: Prompt-based "Self-Verification" is computationally cheaper but requires high model intelligence (fails on weaker models like Qwen7b). Classifiers are robust but require training data and struggle with cross-model generalization on standard queries.
  - Language Selection: Testing on LRLs reveals safety holes but suffers from unreliable evaluation (e.g., lack of native speakers for verification).

- **Failure signatures:**
  - High Resource + Logic Attack: Expect high unsafe rates (English is weakest here)
  - Low Resource + Standard Query: Expect high unsafe rates (Safety alignment is thin)
  - Self-Verification on Weak Models: Expects low improvement or negative results

- **First 3 experiments:**
  1. Baseline Profiling: Run HarmBench standard queries on target LLM across all 10 languages to establish the "Resource-Safety" correlation
  2. Jailbreak Injection: Apply Logic Jailbreak to the HRL queries (English/Spanish) to validate the "Proficiency-Vulnerability" inversion
  3. Defense Validation: Train a lightweight classifier on Qwen7b (standard) responses and test its transferability to GPT-4o (logic-jailbroken) responses to verify if output-based detection holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific logical operators (e.g., negation, conditionals) differentially impact jailbreak success rates across distinct adversarial datasets like HarmBench and AdvBench?
- Basis in paper: Section 4.3 states: "The contradictory results of the impact of negation in the two datasets suggests that more research should be done to analyze the effects of logical operators in jailbreaking."
- Why unresolved: The study observed that negation increased attack success in HarmBench but reduced it in AdvBench, making it unclear if operators help or hinder logic-based attacks universally.
- What evidence would resolve it: A controlled ablation study isolating specific logical operators within a unified set of harmful queries to measure their individual contribution to bypassing safety alignment.

### Open Question 2
- Question: Why do multilingual safety classifiers trained on one model's responses fail to generalize to standard (non-adversarial) queries from other model families, despite generalizing well to jailbreaks?
- Basis in paper: Section 4.5 notes that while the classifier generalizes to jailbreaks, "results are worse for the responses from non-Qwen7b models to the standard queries."
- Why unresolved: The embedding space analysis suggests safe and unsafe responses are distinguishable, yet the classifier learned from one model (Qwen7b) appears sensitive to the specific distribution of standard refusals in other models.
- What evidence would resolve it: Experiments testing classifier transportability across models using a combined training set of standard refusals from multiple model families to see if the generalization gap closes.

### Open Question 3
- Question: To what extent does the semantic content of predicate names versus the structural form of logical expressions drive the vulnerability of LLMs to logic-based jailbreaking?
- Basis in paper: Section 4.3 shows that replacing predicate names with synonyms "leads to large differences in unsafe rates," contradicting the hypothesis that success relies solely on the logical form being out-of-distribution.
- Why unresolved: The authors expected the structure to be the main vector, but the variance caused by synonym replacement suggests the semantic meaning of the predicates plays a significant, unstated role.
- What evidence would resolve it: An experiment using logical expressions with abstract/non-sensical predicate names versus semantic synonyms to isolate the effect of semantic leakage from logical structure.

## Limitations

- Translation quality and consistency across languages may introduce bias or noise into results, particularly for low-resource languages
- Classifier-based defense shows poor generalization across different model families for standard queries
- Logic-based attack effectiveness varies significantly depending on the presence of specific logical operators

## Confidence

- **High Confidence:** The finding that high-resource languages are safer under standard queries but more vulnerable to logic-based jailbreaks is well-supported by the results
- **Medium Confidence:** The claim that safety alignment does not uniformly transfer to low-resource languages is supported by data but limited by translation quality and lack of native verification
- **Low Confidence:** The exact mechanism by which linguistic proficiency increases vulnerability to logic-based attacks is inferred but not fully explained

## Next Checks

1. **Native Speaker Validation for LRLs:** Re-run the standard query and Logic Jailbreak evaluations for low-resource languages (Bengali, Telugu, Swahili) with responses verified by native speakers to ensure the accuracy of the "unsafe" labels and validate the claim about safety alignment transfer.

2. **Defense Architecture Ablation:** Perform an ablation study on the classifier-based defense by testing its performance when trained on a combination of responses from multiple models versus a single model, to quantify the trade-off between training data diversity and model-specific adaptation.

3. **Attack Transferability Analysis:** Systematically test the transferability of the Andrius25 suffix attack by applying the optimized suffix from English to other high-resource languages (e.g., Spanish, German) and measuring if the attack's effectiveness degrades, to determine if the suffix is truly language-agnostic or relies on English-specific tokenization.