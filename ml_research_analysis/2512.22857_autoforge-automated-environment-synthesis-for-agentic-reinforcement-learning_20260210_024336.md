---
ver: rpa2
title: 'AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning'
arxiv_id: '2512.22857'
source_url: https://arxiv.org/abs/2512.22857
tags:
- arxiv
- user
- environment
- agent
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoForge introduces a unified pipeline for automated synthesis
  of simulated environments and high-difficulty tasks, enabling efficient and scalable
  reinforcement learning for language-based agents. The key innovation lies in (1)
  a method for automatically generating complex tool-use tasks and environments from
  tool descriptions, and (2) an Environment-level Relative Policy Optimization (ERPO)
  algorithm that mitigates simulated user instability and improves training stability
  via environment-level advantage estimation.
---

# AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.22857
- Source URL: https://arxiv.org/abs/2512.22857
- Authors: Shihao Cai; Runnan Fang; Jialong Wu; Baixuan Li; Xinyu Wang; Yong Jiang; Liangcai Su; Liwen Zhang; Wenbiao Yin; Zhen Zhang; Fuli Feng; Pengjun Xie; Xiaobin Wang
- Reference count: 17
- Primary result: AutoForge achieves strong performance on τ-bench, τ2-Bench, and VitaBench, narrowing the gap with closed-source models and showing strong out-of-domain generalization on ACEBench-zh.

## Executive Summary
AutoForge introduces a unified pipeline for automated synthesis of simulated environments and high-difficulty tasks, enabling efficient and scalable reinforcement learning for language-based agents. The key innovation lies in (1) a method for automatically generating complex tool-use tasks and environments from tool descriptions, and (2) an Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and improves training stability via environment-level advantage estimation. Comprehensive experiments on three benchmarks demonstrate that AutoForge significantly outperforms open-source baselines and narrows the gap with closed-source models.

## Method Summary
AutoForge synthesizes environments by parsing tool descriptions into dependency graphs, generating random walks to form complex tool sequences, and instantiating executable tasks with verifiable final states. It trains agents using ERPO, which computes advantages at the environment level and employs a Masking Erroneous User (MEU) mechanism to filter out user-error trajectories. The method combines DAG-based task synthesis with environment-level reward normalization to stabilize RL training and improve generalization.

## Key Results
- AutoForge significantly outperforms open-source baselines on τ-bench, τ2-Bench, and VitaBench.
- Strong out-of-domain generalization demonstrated on ACEBench-zh.
- ERPO with environment-level advantage estimation and MEU improves training stability and task success rates.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated DAG-based task synthesis from tool descriptions produces complex, verifiable environments at scale.
- Mechanism: Tool docs → dependency graph → random walks → merged sequences + reasoning nodes/edges → DAG → instantiated DB state + Python tools → executable task with gold final state for verification.
- Core assumption: Tool descriptions are accurate and the LLM can reliably infer valid input/output dependencies between tools.
- Evidence anchors:
  - [abstract] Unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks.
  - [section 3.2–3.3] DAG construction with reasoning nodes/edges; task verified by final state equality rather than exact tool sequence.
  - [corpus] EnvScaler, CuES, and AgentScaler also use programmatic/dependency-based synthesis, but AutoForge emphasizes DAG complexity and verifiability.
- Break condition: Tool descriptions are incomplete or ambiguous → LLM hallucinates invalid edges → generated tasks are unsolvable or unverifiable.

### Mechanism 2
- Claim: Masking Erroneous User (MEU) behaviors stabilizes RL by preventing unfair penalization of correct agent actions.
- Mechanism: During rollout, an LLM judge flags trajectories where the simulated user hallucinated or omitted information; these are masked (excluded from advantage/loss computation).
- Core assumption: The LLM judge reliably detects user errors, and masking does not bias the remaining sample distribution.
- Evidence anchors:
  - [abstract] ERPO mitigates simulated user instability via environment-level advantage estimation.
  - [section 3.4, Figure 3b] Training curve without MEU trends downward; MEU yields stable ascent.
  - [corpus] Not explicitly addressed in neighbor papers; most assume fixed user simulators.
- Break condition: Judge is unreliable or systematically biased → masking removes valid trajectories or retains erroneous ones → advantage estimation remains corrupted.

### Mechanism 3
- Claim: Environment-level advantage estimation reduces outlier sensitivity and improves training stability.
- Mechanism: Normalize rewards across all trajectories within an environment rather than per-question group; compute A_env = (R - mean_env) / std_env.
- Core assumption: Environments define stable baselines; groups within environments can be too small or high-variance for reliable std estimation.
- Evidence anchors:
  - [abstract] Environment-level relative policy optimization (ERPO) performs advantage estimation at the environment level, improving training efficiency and stability.
  - [section 3.4, Figure 3a] Environment-level advantage yields higher and more stable reward curves than group-level.
  - [corpus] Not explicitly discussed in neighbors.
- Break condition: Environments are too few or too homogeneous → environment-level std is still unstable or uninformative.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: Represents tool sequences with explicit dependencies and reasoning nodes; enables topological execution and verification.
  - Quick check question: Why would a cycle in a tool dependency graph make task execution and verification ambiguous?

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: AutoForge extends GRPO’s group-level advantage to environment-level; understanding GRPO baseline is essential.
  - Quick check question: How does GRPO’s group-based advantage estimation differ from PPO’s use of a value function?

- Concept: Advantage Normalization Strategies
  - Why needed here: ERPO’s core change is normalization scope (environment vs. group); affects stability and credit assignment.
  - Quick check question: In what scenarios would group-level std be unreliable compared to environment-level std?

## Architecture Onboarding

- Component map:
  Environment Synthesis (State + Python functions) → Tool-Sequence DAG Generator → Task Instantiator (init state, gold state) → ERPO Trainer (MEU filter + env-level advantage) → Agent Rollout.

- Critical path:
  1. Parse tool docs → build dependency graph.
  2. Sample/merge sequences → add reasoning nodes/edges → DAGs.
  3. Instantiate environment state + generate task intent → execute DAG to get gold state.
  4. Run RL rollouts with simulated user; mask user-error trajectories; compute env-level advantages; update policy.

- Design tradeoffs:
  - DAG complexity vs. verifiability: More reasoning nodes increase task difficulty but must remain deterministically verifiable via state equality.
  - Simulated user strength: Stronger user models (e.g., GPT-5) yield higher agent scores but may obscure agent limitations; weaker users necessitate MEU.
  - Environment count vs. generalization: Few environments risk overfitting; scaling environments may improve OOD but increases synthesis cost.

- Failure signatures:
  - Downward training reward without MEU (user errors penalize correct actions).
  - Noisy/flat reward curves with group-level advantage in small groups.
  - Task verification failures due to LLM-synthesized tools diverging from real tool behavior.

- First 3 experiments:
  1. Ablate environment-level vs. group-level advantage; plot reward stability and final task success.
  2. Ablate MEU; compare training curves and analyze types of user errors masked.
  3. Evaluate OOD generalization on ACEBench-zh; compare backbone vs. AutoForge-SFT vs. AutoForge-RL.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does scaling the number of synthetic environments affect RL training efficiency and out-of-domain generalization capabilities?
- Basis in paper: [explicit] The authors state, "Investigating how scaling up the number of synthetic environments affects RL training and out-of-domain generalization would be beneficial."
- Why unresolved: The current experiments utilize only a few environments (10 virtual environments), leaving the impact of massive environment scaling unexplored.
- What evidence would resolve it: A study comparing agent performance and training dynamics across varying numbers of synthesized environments (e.g., 10 vs. 100 vs. 1000) on benchmarks like ACEBench.

### Open Question 2
- Question: Can turn-level value supervision be integrated into ERPO to enhance step-by-step decision-making beyond current outcome-based rewards?
- Basis in paper: [explicit] The limitations section notes, "We will further explore turn-level value supervision to improve the agent’s step-by-step decision-making."
- Why unresolved: The current ERPO algorithm relies solely on outcome-based rewards (comparing final state to golden state), potentially lacking granular feedback for intermediate steps.
- What evidence would resolve it: Experiments implementing a value function within the ERPO framework showing improved performance in complex, multi-turn tasks compared to the outcome-only baseline.

### Open Question 3
- Question: Can the synthesis pipeline automatically construct high-quality environments from general text or task topics instead of structured tool descriptions?
- Basis in paper: [explicit] The authors aim to "relax these input requirements, enabling the automatic construction of high-quality environments from task topics or general text."
- Why unresolved: The current method depends specifically on tool description documents to generate state structures and function sets.
- What evidence would resolve it: A modified pipeline successfully generating executable environments and tasks from unstructured text inputs, verified by functional correctness and agent training utility.

## Limitations

- DAG Verifiability Under Real-World Tool Drift: The paper relies on tool descriptions and LLM-inferred dependencies to construct verifiable DAGs. However, real-world tools often change APIs or semantics over time, which may break the assumption that gold final states remain valid.
- Simulated User Generalization: The MEU mechanism assumes the LLM judge can reliably detect user errors. However, there is limited discussion on the judge's calibration, false-positive/false-negative rates, or how well it generalizes to unseen user behaviors.
- Environment-Level Advantage Stability: While environment-level normalization is claimed to reduce variance, the paper does not provide extensive ablation studies on how the number of environments or their diversity affects the stability of this normalization.

## Confidence

- **High Confidence**: The overall framework design (DAG-based synthesis + ERPO + MEU) is logically coherent and well-motivated by existing RL challenges.
- **Medium Confidence**: The empirical results show clear improvements over baselines, but the analysis of failure modes (e.g., when MEU or environment-level advantage breaks) is limited.
- **Low Confidence**: The paper does not provide quantitative validation of the LLM judge's reliability or the long-term verifiability of synthesized tasks under tool drift.

## Next Checks

1. **Judge Reliability Audit**: Systematically evaluate the LLM judge's precision and recall in detecting user errors across diverse simulated user behaviors, including edge cases and adversarial inputs.

2. **Environment Diversity Stress Test**: Vary the number and diversity of environments in ERPO training and measure the impact on reward stability, final task success, and OOD generalization.

3. **Longitudinal Tool Drift Simulation**: Introduce controlled API changes or semantic shifts in tool descriptions and measure how quickly synthesized tasks become unverifiable or unsolvable, testing the robustness of the DAG construction pipeline.