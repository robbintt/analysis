---
ver: rpa2
title: 'Routine: A Structural Planning Framework for LLM Agent System in Enterprise'
arxiv_id: '2507.14447'
source_url: https://arxiv.org/abs/2507.14447
tags:
- tool
- routine
- execution
- step
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Routine is a structured planning framework designed to improve
  the stability and accuracy of multi-step tool calling in enterprise agent systems.
  It addresses the problem of common models lacking domain-specific process knowledge,
  which leads to disorganized plans and missing key tools.
---

# Routine: A Structural Planning Framework for LLM Agent System in Enterprise

## Quick Facts
- arXiv ID: 2507.14447
- Source URL: https://arxiv.org/abs/2507.14447
- Reference count: 40
- Primary result: Routine framework increases GPT-4o multi-step tool calling accuracy from 41.1% to 96.3% in enterprise agent systems

## Executive Summary
Routine is a structured planning framework designed to improve the stability and accuracy of multi-step tool calling in enterprise agent systems. It addresses the problem of common models lacking domain-specific process knowledge, which leads to disorganized plans and missing key tools. Routine provides a clear, well-formatted plan that guides execution models through complex workflows. In real-world enterprise evaluations, Routine significantly improved model performance: GPT-4o's accuracy increased from 41.1% to 96.3%, and Qwen3-14B's from 32.6% to 83.3%. Further fine-tuning on Routine-generated and distilled datasets raised Qwen3-14B's accuracy to 95.5%, approaching GPT-4o's level. These results demonstrate Routine's effectiveness in enhancing model instruction-following, distilling domain-specific patterns, and accelerating stable agent deployment in enterprise environments.

## Method Summary
The authors built Routine using an open-source dataset (BUTTON) containing 8,000 single-turn, multi-step tool-call instances. They synthesized 4,209 filtered common Routine-following samples and 537 scenario-specific HR agent samples via distillation. The framework uses LoRA fine-tuning on Qwen2.5/Qwen3 Instruct models with LLaMA-Factory, DeepSpeed ZeRO-3, and Flash Attention-2. Key training settings include 4Ã— NVIDIA A10 GPUs, LoRA rank 8, batch size 1/GPU, gradient accumulation 4, and learning rate 1.0e-4. The execution model follows structured Routine prompts containing step numbers, names, descriptions, tools, and input/output parameters, with special tokens for routines and variables.

## Key Results
- GPT-4o accuracy increased from 41.1% to 96.3% using Routine framework
- Qwen3-14B accuracy improved from 32.6% to 83.3% with Routine prompting
- Fine-tuning on distilled Routine data raised Qwen3-14B accuracy to 95.5%, approaching GPT-4o's performance
- Structure accuracy improved from 63.5% to 100%, tool selection from 75.0% to 97.0%, and parameter accuracy from 80.0% to 97.5%

## Why This Works (Mechanism)

### Mechanism 1: Explicit Process Constraint
Providing a structured, natural language script (Routine) shifts the execution model's cognitive load from "planning and reasoning" to "instruction following and pattern matching." The framework constrains the model's output search space by explicitly defining step names, descriptions, and tool names, bypassing the model's weakness in domain-specific planning by externalizing that logic into a prompt. If the Routine omits the specific Tool Name, performance degrades significantly (5-15% drop), as the model must revert to reasoning rather than executing.

### Mechanism 2: Variable State Decoupling
Abstracting large parameter values into temporary memory keys prevents context window saturation and syntax errors common in smaller models. Instead of pasting massive JSON objects from step 1 into the prompt for step 2, the system stores objects and passes reference keys (e.g., memory_xxx). The execution model outputs the key, and the system reconstructs the value for the tool. This significantly reduces context pressure, as supported by the authors' claim that variable memory reduces context window issues.

### Mechanism 3: Process-to-Weights Distillation
Structured Routines serve as high-quality "reasoning chains" for distillation, allowing smaller models to internalize domain logic. A teacher model uses the Routine to generate perfect execution traces, which fine-tune a student model (e.g., Qwen3-14B), effectively "baking" the procedural knowledge into the weights. This enables the agent to complete multi-step tool calls without relying on explicit Routine at inference time.

## Foundational Learning

- **Instruction Following vs. Reasoning**
  - Why needed here: The paper explicitly separates the Planning Module (reasoning) from the Execution Module (instruction following). You must understand that "smart" planning is not required at execution time if the plan is robust.
  - Quick check question: Can a 7B parameter model replace a GPT-4 class model if the plan is pre-written? (Answer: Yes, if the task is strictly instruction following).

- **Model Context Protocol (MCP)**
  - Why needed here: The architecture relies on MCP for tool standardization. Understanding that tools are defined externally via a protocol, rather than hardcoded in the prompt, is key to the system's extensibility.
  - Quick check question: How does the execution model know the available tools? (Answer: It retrieves standardized definitions from the MCP server).

- **Branching Logic in Prompts**
  - Why needed here: Real enterprise workflows are rarely linear. The Routine format handles conditional branches (e.g., "If condition A, do Step X-1"). You need to see how natural language prompts can encode control flow.
  - Quick check question: How does the model handle a fork in the road? (Answer: It evaluates the "Branch Condition" described in the Routine against the current observation).

## Architecture Onboarding

- **Component map:** Planning Module -> Procedure Memory -> Execution Module -> Variable Memory -> Tool Module (MCP Server)
- **Critical path:** The generation of the Routine itself. If the Routine misses a step or fails to specify the tool name, the execution model will fail regardless of its capability.
- **Design tradeoffs:** Using Routines makes the agent highly accurate (96%) but rigid. It cannot easily handle tasks outside the defined Routines without creating a new plan. Prompting with Routines is cheap and adaptable; distilling Routines into weights is expensive to train but faster at inference and reduces prompt token costs.
- **Failure signatures:**
  - Tool Hallucination: Model calls a tool not in the list. Fix: Ensure Routine explicitly names the tool.
  - Parameter Syntax Error: Model outputs broken JSON when passing large strings. Fix: Use Variable Memory keys.
  - Context Drift: Model loses track of which step it is on. Fix: Enforce strict "Step X" headers in the Routine format.
- **First 3 experiments:**
  1. Baseline vs. Routine: Run the 200-sample test set with no plan vs. a manual Routine to measure the stability gap.
  2. Ablation on Tool Names: Remove tool names from the Routine (leaving only descriptions) to verify if the model is reasoning or just pattern matching.
  3. Variable Stress Test: Force a tool to return a 10k-token string. Test the model's ability to pass this variable in the next step with and without Variable Memory enabled.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Reinforcement Learning (RL) be effectively integrated with Routine-based fine-tuning to improve agent autonomy when adapting to dynamic workflows and new tools?
- Basis in paper: The authors note in the "Limitation and Future Work" section that current reliance on expert drafts limits generalization, suggesting RL-based frameworks and reward modeling as a future paradigm to improve adaptability.
- Why unresolved: The current system relies on static instruction fine-tuning and expert-annotated drafts, which creates brittleness when enterprise tools or processes change unexpectedly.
- What evidence would resolve it: Evaluations comparing the success rate of RL-enhanced agents against SFT-only agents on unseen tools and modified workflows.

### Open Question 2
- Question: Does a hierarchical multi-agent framework centered on Routines reduce plan complexity and improve stability compared to single-agent execution?
- Basis in paper: The paper explicitly states the intent to explore a multi-agent framework where a high-level agent coordinates specialized agents to reduce the length and complexity of individual Routine plans.
- Why unresolved: The current study focuses on a single execution model following one Routine; complex workflows may still result in overly long context windows or error propagation in a single agent.
- What evidence would resolve it: Benchmarks comparing task accuracy and context window usage between single-agent and hierarchical multi-agent systems on complex, multi-domain tasks.

### Open Question 3
- Question: To what extent does training on Routine-distilled data impair a model's ability to solve tasks that require deviating from the distilled patterns?
- Basis in paper: While the paper shows distilled models achieve 95.5% accuracy, it also notes that fine-tuning on specific scenarios creates a "trade-off where their common problem-solving ability was reduced" (Section 5.2.2).
- Why unresolved: It is unclear if the high performance in specific scenarios comes at the cost of "zero-shot" flexibility required for novel edge cases not covered by the distillation dataset.
- What evidence would resolve it: Testing fine-tuned models on "out-of-distribution" queries that require logical deviations from the standard Routine flow.

## Limitations
- Narrow evaluation scope: Performance gains demonstrated only on BUTTON-derived tasks and one HR domain, leaving generalizability to other enterprise workflows uncertain
- Limited ablation studies: No ablation on variable memory, no comparison to alternative planning frameworks
- Small test set: 200-sample test set is insufficient for drawing robust conclusions about production readiness

## Confidence
- GPT-4o results: High confidence (strong, consistent improvement)
- Qwen3-14B gains: Medium confidence (sensitive to fine-tuning quality and dataset curation)
- Variable memory benefits: Low confidence (lack of explicit ablation or context-window stress tests)

## Next Checks
1. Test Routine generalizability on at least two additional enterprise domains (e.g., finance, customer support) to confirm cross-domain robustness
2. Perform an ablation study isolating variable memory's contribution to accuracy and context efficiency
3. Compare Routine against alternative structured planning methods (e.g., POLARIS-style typed plans) to validate relative effectiveness