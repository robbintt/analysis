---
ver: rpa2
title: Dynamic Coalition Structure Detection in Natural Language-based Interactions
arxiv_id: '2502.16339'
source_url: https://arxiv.org/abs/2502.16339
tags:
- coalition
- game
- language
- players
- agreements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting dynamic coalition
  structures in multi-agent interactions where agents coordinate using natural language,
  as exemplified by the board game Diplomacy. The authors propose a two-stage approach
  that first extracts potential agreements from pairwise dialogues using a hybrid
  method combining a large language model parser with specialized intent prediction
  models, then assigns rationalizability scores to agreements based on subjective
  rationalizability from hypergame theory, using deep reinforcement learning to compute
  strategic values.
---

# Dynamic Coalition Structure Detection in Natural Language-based Interactions

## Quick Facts
- arXiv ID: 2502.16339
- Source URL: https://arxiv.org/abs/2502.16339
- Authors: Abhishek N. Kulkarni; Andy Liu; Jean-Raphael Gaglione; Daniel Fried; Ufuk Topcu
- Reference count: 40
- One-line primary result: A two-stage approach combining hybrid agreement detection (F1 0.55) with hypergame-based rationalizability scoring successfully identifies honored coalitions in language-based multi-agent interactions

## Executive Summary
This paper addresses the challenge of detecting dynamic coalition structures in multi-agent interactions where agents coordinate using natural language, as exemplified by the board game Diplomacy. The authors propose a two-stage approach that first extracts potential agreements from pairwise dialogues using a hybrid method combining a large language model parser with specialized intent prediction models, then assigns rationalizability scores to agreements based on subjective rationalizability from hypergame theory, using deep reinforcement learning to compute strategic values. Their method effectively addresses challenges of incomplete information, mental modeling of opponents, and multilateral negotiations in language-based coordination.

## Method Summary
The approach consists of two main stages: agreement detection and coalition structure prediction. For agreement detection, a hybrid method combines GPT-4o for explicit territory mention extraction with a 2.7B-parameter CICERO intent model that analyzes shifts in action probability distributions before and after dialogue. A logistic regression classifier then determines whether agreements exist over specific units. For coalition structure prediction, the method uses hypergame theory to compute subjective rationalizability scores by modeling each player's beliefs about others' games, combined with deep reinforcement learning (DORA) to estimate strategic values in the exponentially large action space of Diplomacy. The final coalition structure is determined by ranking agreements by their mutual rationalizability scores.

## Key Results
- Hybrid agreement detection achieves F1 score of 0.55, outperforming pure language model (0.34) and intent distribution-based (0.44) approaches
- Rationalizability score successfully distinguishes honored from violated agreements, achieving MRR scores of 0.94 (honored) versus 0.00 (violated) in top-1 ranking
- Method outperforms approximate Nash equilibrium-based approaches in coalition structure prediction accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid agreement detection outperforms single-modality approaches by combining explicit parsing with distributional shift analysis.
- Mechanism: A large language model (GPT-4o) filters dialogue to extract explicitly mentioned territories, while a specialized 2.7B-parameter intent model (from CICERO) captures implicit coordination by comparing move probability distributions before and after dialogue. Logistic regression then classifies whether an agreement exists over each unit. The filter reduces noise from unrelated dialogue shifts; the intent model catches implicit agreements that parsers miss.
- Core assumption: Agreements correlate with measurable shifts in action probability distributions post-dialogue, and explicitly mentioned territories are necessary (but not sufficient) signals of coordination.
- Evidence anchors:
  - [abstract] "hybrid agreement detection method achieves an F1 score of 0.55, outperforming both pure language model (0.34) and intent distribution-based (0.44) approaches"
  - [Section 4.1] "extract all locations that were explicitly mentioned... we then leverage the intent models... computing a distribution of move likelihoods... before and after dialogue"
  - [corpus] Weak direct corpus support; neighbor papers mention LLM-based agreement detection (Finding Common Ground) but not this specific hybrid architecture.
- Break condition: If dialogues are short with no distributional shift, or if agreements are entirely implicit with no territory mentions, recall degrades. Imbalanced data (2.6% positive rate) requires careful threshold tuning.

### Mechanism 2
- Claim: Subjective rationalizability from hypergame theory predicts agreement adherence better than Nash equilibrium approximations because it models asymmetric beliefs.
- Mechanism: Each player constructs a subjective game based on their private dialogue history. The rationalizability score wt_i(α) = V_i(α) × V_j^i(α) multiplies (1) the strategic value of honoring the agreement for player i, and (2) player i's belief that player j will honor it. The final observer score wt(α) = wt_i(α) × wt_j(α) requires mutual perceived rationality.
- Core assumption: Players honor agreements when both (a) the agreement is strategically valuable for them, and (b) they believe their counterparty will also honor it—captured by L2-hypergame subjective rationalizability (Def. 3).
- Evidence anchors:
  - [abstract] "rationalizability score successfully distinguishes honored from violated agreements, achieving MRR scores of 0.94 (honored) versus 0.00 (violated) in the top-1 ranking"
  - [Section 4] "wt_i(α) = V_i(α) * V_j^i(α)... this weight reflects how subjectively rationalizable an agreement is for P_i"
  - [corpus] Neighbor paper "Advanced Game-Theoretic Frameworks for Multi-Agent AI" mentions dynamic coalition formation but does not validate hypergame approaches specifically.
- Break condition: If players act irrationally (e.g., emotional betrayal) or if belief inference from dialogue is systematically wrong, the score decouples from actual behavior.

### Mechanism 3
- Claim: Deep reinforcement learning (DORA) enables tractable strategic value estimation in exponentially large action spaces.
- Mechanism: DORA approximates Nash equilibria without enumerating all joint actions. A neural network π(s; θ_π) samples high-likelihood candidate actions; regret minimization solves the restricted matrix game; a value network V(s; θ_V) bootstraps from successor states. For agreements, the system samples joint actions conditioned on agreement adherence and computes expected next-state values (Eq. 2).
- Core assumption: The sampled action subspace contains equilibria relevant to agreement evaluation, and the value network generalizes across unseen states.
- Evidence anchors:
  - [Section 4.2] "DORA is a Nash Q-Learning based approach... accommodates the large action spaces of Diplomacy by training a neural network π(s; θ_π) to predict joint action probability distribution"
  - [Section 5.3] "our R-Score yields a significantly higher MRR and a lower Brier score than the value model score in all cases"
  - [corpus] No direct corpus validation of DORA specifically; "Deviation Ratings" addresses multi-agent evaluation but uses different methodology.
- Break condition: If critical equilibria fall outside the sampled action set, or if the game state distribution shifts from training data (e.g., novel Diplomacy variants), value estimates become unreliable.

## Foundational Learning

- Concept: **Hypergame Theory (L0/L1/L2 levels)**
  - Why needed here: Diplomacy has incomplete information and asymmetric awareness—players don't know others' private negotiations. Standard game theory assumes common knowledge; hypergames model subjective games and beliefs about others' games.
  - Quick check question: In an L2-hypergame, what does player 2 know that player 1 doesn't realize they know?

- Concept: **Nash Q-Learning and Regret Minimization**
  - Why needed here: DORA combines RL value learning with game-theoretic equilibrium computation. Understanding Q-learning, the Bellman update, and regret-based equilibrium solvers is necessary to modify or debug the strategic value module.
  - Quick check question: Why does DORA sample actions rather than enumerate them, and what does regret minimization guarantee in the restricted game?

- Concept: **Behavioral Cloning and Intent Models**
  - Why needed here: The intent model is trained via behavioral cloning on "truthful" player dialogues from WebDiplomacy. Understanding distributional outputs, entropy, and calibration is essential for interpreting intent shifts.
  - Quick check question: What does a drop in entropy over action distributions after dialogue indicate about coordination?

## Architecture Onboarding

- Component map: LLM Parser (GPT-4o) -> Territory extraction -> Intent Model (CICERO 2.7B) -> Action distribution analysis -> Agreement Classifier (Logistic Regression) -> Agreement set -> DORA sampling (π-network) -> Value estimation (V-network) -> Per-player rationalizability -> Final coalition structure graph

- Critical path: Dialogue → Parser filter → Intent distribution extraction → Classifier → Agreement set → DORA sampling → Value estimation → Per-player rationalizability → Final coalition structure graph

- Design tradeoffs:
  - **Parser precision vs. recall**: Aggressive filtering reduces false positives but misses implicit agreements (F1 0.55 reflects this balance)
  - **CICERO dependency**: Uses pre-trained CICERO models; training new models requires substantial human gameplay data
  - **Pairwise-only belief modeling**: Current method filters dialogue to bilateral exchanges; multilateral belief inference is future work
  - **Binary honored/violated evaluation**: Real-world coalitions may be partially honored

- Failure signatures:
  - Low F1 on agreement detection (<0.4): Parser too strict or intent model mis calibrated
  - MRR near 0.5 for honored agreements: Rationalizability score not differentiating—check value network convergence
  - High variance across game phases: Early-game value estimates unreliable due to longer horizons
  - Asymmetric performance between player roles: Intent model biased toward certain player positions

- First 3 experiments:
  1. **Ablate parser**: Run agreement detection with intent-only and parser-only variants on held-out games; expect ~0.1 F1 drop each vs. hybrid
  2. **Threshold sweep**: Vary agreement classifier threshold; plot precision-recall to find optimal operating point for downstream coalition prediction
  3. **Value network sanity check**: On known equilibria (e.g., simple 2-player Diplomacy endgames), verify V-network rankings match game-theoretic ground truth

## Open Questions the Paper Calls Out
None

## Limitations
- Binary classification of agreements (honored vs. violated) oversimplifies real-world coalition dynamics where partial compliance and conditional fulfillment are common
- Heavy reliance on pre-trained CICERO models creates dependency on WebDiplomacy training data quality and distribution
- Pairwise filtering approach cannot capture fully multilateral coalition dynamics where group-level commitments emerge from collective dialogue

## Confidence
- **High confidence** in agreement detection mechanism (F1 0.55 vs. 0.34-0.44 baselines) due to direct quantitative comparison and clear performance advantage
- **Medium confidence** in hypergame-based rationalizability scoring because while MRR scores (0.94 vs. 0.00) show strong discrimination, the binary evaluation framework and lack of direct corpus validation introduces uncertainty
- **Medium confidence** in DORA strategic value estimation due to dependence on pre-trained models without direct validation on known equilibria, though methodology aligns with established Nash Q-learning principles

## Next Checks
1. **Partial Agreement Analysis**: Develop a multi-level compliance metric (fully honored, partially honored, violated) and retrain/validate the rationalizability scoring to predict this spectrum rather than binary outcomes.

2. **Cross-Game Generalization**: Apply the complete pipeline to a different multi-agent coordination game (e.g., Settlers of Catan or Werewolf) to test whether the hybrid agreement detection and hypergame-based scoring transfer across domains.

3. **Value Network Ground Truth Verification**: Construct simple 2-3 player Diplomacy endgames with known Nash equilibria, run the DORA value network, and verify that V-network rankings match the game-theoretic ground truth before applying to complex full-game scenarios.