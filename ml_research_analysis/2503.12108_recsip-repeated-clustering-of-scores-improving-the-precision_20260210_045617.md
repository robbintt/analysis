---
ver: rpa2
title: 'RECSIP: REpeated Clustering of Scores Improving the Precision'
arxiv_id: '2503.12108'
source_url: https://arxiv.org/abs/2503.12108
tags:
- recsip
- responses
- llms
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the reliability challenge in Large Language
  Models (LLMs) by proposing a framework that reduces incorrect responses through
  repeated clustering of LLM outputs. The core method, RECSIP, queries multiple LLMs
  in parallel, evaluates their responses using language scores (ROUGE, METEOR, and
  cross-encoder), clusters similar responses, and iteratively generates callback questions
  when models disagree until consensus is reached or models cannot agree further.
---

# RECSIP: REpeated Clustering of Scores Improving the Precision

## Quick Facts
- arXiv ID: 2503.12108
- Source URL: https://arxiv.org/abs/2503.12108
- Reference count: 35
- Primary result: RECSIP achieved 83.5% accuracy on MMLU-Pro, outperforming best single model by 5.8 percentage points

## Executive Summary
This paper addresses the reliability challenge in Large Language Models (LLMs) by proposing RECSIP, a framework that reduces incorrect responses through repeated clustering of LLM outputs. The approach queries multiple LLMs in parallel, evaluates responses using language scores (ROUGE, METEOR, and cross-encoder), clusters similar responses, and iteratively generates callback questions when models disagree until consensus is reached or models cannot agree further. When evaluated on MMLU-Pro using GPT-4o, Claude, and Gemini, RECSIP achieved 83.5% accuracy, demonstrating effective filtering of wrong responses through model disagreement. The framework is modular and adaptable for various applications, though it requires more computational resources and depends on individual model performance.

## Method Summary
RECSIP queries multiple LLMs in parallel, scoring and clustering their responses to improve reliability. The method uses ROUGE, METEOR, and cross-encoder (stsb-distilroberta-base) to calculate pairwise similarity scores between all responses. Responses are clustered via binary similarity comparison, and if multiple clusters exist, a callback prompt is generated presenting one response per cluster as multiple-choice options. Models are then asked to select the best fitting response. This process repeats until consensus is reached or cluster count stabilizes. The framework avoids using LLMs for scoring or decision-making, relying instead on deterministic similarity metrics to prevent additional stochastic errors.

## Key Results
- RECSIP achieved 83.5% overall accuracy on MMLU-Pro, outperforming the best single model by 5.8 percentage points
- The system effectively filters wrong responses by leveraging model disagreement, though returns "Disunity" in 28% of cases
- Iterative callbacks helped recover correct answers in approximately 9% of initially incorrect responses

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Model Consensus via Clustering
The framework aggregates responses from structurally diverse LLMs and clusters them to filter out single-model hallucinations. By querying GPT-4o, Claude, and Gemini in parallel and calculating pairwise similarity scores, the system leverages the assumption that correct reasoning paths are more likely to converge semantically across diverse models than incorrect ones. The cross-encoder similarity metric helps identify when models with different training data and architectures converge on similar responses, increasing the probability of correctness.

### Mechanism 2: Iterative Callback for Disambiguation
When clusters disagree, RECSIP generates a callback prompt presenting the original question alongside one representative response from each conflicting cluster as multiple-choice options. This reformulation forces models to re-evaluate options, potentially recovering the correct answer through better recognition performance (selecting from a list) rather than generative performance (recalling from scratch). The callback mechanism effectively narrows the solution space when initial responses diverge.

### Mechanism 3: Non-Generative Evaluation Guardrails
RECSIP uses deterministic scoring metrics (ROUGE, METEOR, cross-encoders) instead of LLM-as-judge to evaluate response similarity. This prevents the introduction of additional stochastic errors during the evaluation phase and avoids the "LLM evaluating LLM" loop where the evaluator model might hallucinate rankings or fail to detect subtle logical differences. The approach assumes semantic similarity detected by cross-encoders correlates strongly with logical equivalence in the context of the query.

## Foundational Learning

- **Self-Consistency (Chain-of-Thought)**: RECSIP extends self-consistency from single models to multiple models. Understanding that "majority vote on reasoning paths improves accuracy" is the baseline logic for the clustering mechanism. Quick check: How does sampling multiple reasoning paths differ from sampling multiple models?

- **Cross-Encoders vs. Bi-Encoders**: The paper relies on a cross-encoder for clustering accuracy. You must understand that cross-encoders allow joint attention on sentence pairs (higher accuracy, slower) compared to embeddings. Quick check: Why would a cross-encoder be preferred over cosine similarity of embeddings when detecting subtle differences in "reasoning" responses?

- **Model Heterogeneity**: The system's success depends on the models being diverse. If models are too similar, they replicate each other's errors. Quick check: Does using GPT-4o and GPT-4-turbo count as sufficient heterogeneity for this framework?

## Architecture Onboarding

- **Component map**: Input -> Generators (n distinct LLM clients) -> Scorer (pairwise similarity matrix) -> Clusterer (threshold-based grouping) -> Arbiter (cluster count check) -> Callback Prompt (if clusters > 1) -> Output (Consensus or Disunity)

- **Critical path**: The definition of the clustering threshold in the Scorer/Clusterer. If set too low, distinct answers merge (false consensus). If set too high, identical answers split (false disagreement). The cross-encoder penalty for length difference must be counter-balanced by ROUGE/METEOR limits.

- **Design tradeoffs**: Latency vs. Precision (callback loop doubles/triples latency and cost); Coverage vs. Accuracy (filters wrong answers effectively but returns "Disunity" in 28% of cases, refusing to answer).

- **Failure signatures**: "Disunity" Loops (system exits with "The models could not agree," seen in 47% of Law and 38% of Engineering tasks); Formatting Drift (if LLM output format shifts, Regex extractor fails); Late Switching (model initially correct switches to wrong answer during callback, ~9% of wrong answers).

- **First 3 experiments**: 1) Threshold Calibration: Run MMLU subset varying only Cross-Encoder threshold to maximize Correct/Wrong consensus ratio; 2) Ablation on "Callback": Measure performance drop when disabling iterative callback (single-shot clustering only) to quantify disambiguation value; 3) Heterogeneity Test: Compare performance using 3× same model vs. 3 distinct models to validate "diverse reasoning paths" hypothesis.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance fundamentally constrained by shared systematic knowledge gaps or training biases across models, leading to false agreement
- Computational overhead requires 2-3× inference time and cost compared to single-model queries
- 28% "Disunity" rate limits practical applicability for production systems requiring high coverage

## Confidence

- **High Confidence**: Overall accuracy improvement (83.5% vs 77.7%) and use of non-LLM metrics for evaluation are well-supported by experimental results
- **Medium Confidence**: Claim that iterative callbacks recover ~9% of initially incorrect answers requires more granular analysis of model switching behavior
- **Low Confidence**: Generalizability of 5.8 percentage point improvement across different benchmarks remains uncertain without validation beyond MMLU-Pro

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the cross-encoder clustering threshold across full validation range to identify optimal precision-recall tradeoffs and determine threshold-dependence of reported performance.

2. **Heterogeneity Validation**: Compare RECSIP performance using three instances of same model family versus three distinct model architectures to empirically validate "diverse reasoning paths" hypothesis and quantify contribution of model heterogeneity.

3. **Knowledge Gap Robustness Test**: Design adversarial test cases where models share known common misconceptions or training biases, then measure whether RECSIP correctly identifies and rejects consensus on incorrect answers or falsely validates them through clustering.