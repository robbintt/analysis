---
ver: rpa2
title: 'ACE: Adapting sampling for Counterfactual Explanations'
arxiv_id: '2509.26322'
source_url: https://arxiv.org/abs/2509.26322
tags:
- counterfactual
- explanations
- function
- datasets
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACE introduces a Bayesian optimization approach for generating
  counterfactual explanations with minimal black-box evaluations. It combines Gaussian
  Process classification with Monte Carlo Expected Improvement to approximate the
  decision boundary efficiently.
---

# ACE: Adapting sampling for Counterfactual Explanations

## Quick Facts
- arXiv ID: 2509.26322
- Source URL: https://arxiv.org/abs/2509.26322
- Reference count: 40
- ACE introduces a Bayesian optimization approach for generating counterfactual explanations with minimal black-box evaluations.

## Executive Summary
ACE presents a novel approach to generating counterfactual explanations that significantly reduces the number of black-box queries required. By combining Gaussian Process classification with Monte Carlo Expected Improvement, ACE approximates decision boundaries efficiently while handling mixed continuous and categorical inputs through a hybrid optimization strategy. The method enforces interpretability constraints through a penalized cost function and demonstrates superior sample efficiency across eight real-world datasets.

## Method Summary
ACE addresses the counterfactual explanation problem by framing it as Bayesian optimization with a surrogate model. It models the black-box classifier as a thresholded smooth function and fits a Gaussian Process to approximate this latent function. The algorithm uses Monte Carlo Expected Improvement to select informative query points, while handling mixed input spaces through a combination of L-BFGS-B and Branch-and-Bound optimization. Constraints like validity, plausibility, and actionability are incorporated via a Lagrangian penalty method that gradually tightens feasibility requirements during optimization.

## Key Results
- ACE consistently required fewer queries than baselines (e.g., 70 vs. 28,000+ for Growing Spheres)
- Maintained high validity rates (100%) and plausibility scores (α(x) ≈ 1) across datasets
- In MNIST experiments, found valid counterfactuals in 9 queries versus 50+ for competing methods

## Why This Works (Mechanism)

### Mechanism 1: Sample Efficiency via Latent Surrogate Modeling
ACE reduces black-box query counts by modeling a latent decision boundary rather than querying the discrete classifier directly. It fits a Gaussian Process to approximate the smooth function underlying the black-box classifier, then uses Expected Improvement to prioritize informative regions near the boundary. This approach assumes the decision boundary is sufficiently smooth to be approximated by the chosen kernel.

### Mechanism 2: Constraint Relaxation via Penalty Methods
ACE handles hard constraints by relaxing them into a differentiable cost function. Instead of strictly enforcing validity and feature immutability, it uses a Lagrangian formulation where constraint violations are gradually penalized through an increasing sequence of λ values. This allows the continuous optimizer to find feasible solutions while maintaining proximity to the original instance.

### Mechanism 3: Mixed-Space Optimization via Branch and Bound
ACE enables efficient counterfactual generation on heterogeneous data by using Branch-and-Bound for categorical variables and L-BFGS-B for continuous variables. This avoids the curse of dimensionality from one-hot encoding while maintaining principled exploration of the mixed search space, assuming categorical features have limited cardinality.

## Foundational Learning

**Gaussian Process Classification (GPC)**: ACE relies on GPC to estimate uncertainty of the black-box boundary. Understanding the latent mean μₐ and variance σ²ₐ is crucial for the acquisition function to balance exploration vs. exploitation. Quick check: Can you explain why standard GP regression cannot be applied directly to binary labels (0/1) without a link function?

**Bayesian Optimization (BO) Loops**: The entire ACE algorithm is a BO loop (Surrogate → Acquisition → Query → Update). Understanding how EI guides the search is essential to distinguish this from random search. Quick check: If the surrogate model is perfectly accurate, what happens to the Expected Improvement?

**Laplace Approximation**: GPC is non-Gaussian, so ACE uses the Laplace approximation to calculate the posterior distribution of the latent function efficiently. Quick check: How does the delta method help in computing the covariance required for Monte Carlo sampling of the acquisition function?

## Architecture Onboarding

**Component map**: Surrogate (GPC with Matérn 5/2 kernel) → Acquisition (Monte Carlo EI) → Optimizer (L-BFGS-B + B&B) → Filter (LOF-based outlier detection)

**Critical path**: The acquisition optimization is where the discrete/continuous hybrid logic lives. If L-BFGS-B + B&B fails to find a point with positive EI, the algorithm stalls.

**Design tradeoffs**: Matérn 5/2 kernel chosen over RBF for robustness to non-infinite smoothness; correlated MC sampling used instead of analytic EI for better handling of cost function coupling despite higher computational cost; Sobol sampling used for final CFE selection to ensure dense coverage.

**Failure signatures**: 
- Stuck in Invalid Region: Algorithm runs until λₘₐₓ but returns invalid counterfactual. Likely cause: Plausibility constraints conflict with valid counterfactuals.
- Sparse Optimization Loop: xₖ stops changing prematurely. Likely cause: λ growth rate too low or tolerance too tight relative to step sizes.

**First 3 experiments**:
1. Synthetic Validation: Run make_moons (2D) and plot surrogate's mean boundary against ground truth to verify GP captures shape with n₀=4.
2. Categorical Stress Test: Run on "Nursery" dataset and verify B&B is actually pruning branches rather than iterating all combinations.
3. Efficiency Curve: Plot queries vs. dimensionality by subsampling features in MNIST and validate claimed scalability.

## Open Questions the Paper Calls Out
- Can ACE be extended to generate diverse sets of counterfactuals via multi-objective optimization schemes?
- What are the theoretical convergence guarantees and regret bounds for ACE?
- How does ACE's computational efficiency scale with high-cardinality categorical variables?

## Limitations
- Scalability to high-cardinality categorical features remains untested
- GP surrogate may struggle with highly non-smooth classifier boundaries
- Performance depends heavily on initialization quality without sensitivity analysis

## Confidence
- High confidence: Sample efficiency gains (query counts)
- Medium confidence: Constraint satisfaction (plausibility, actionability)
- Medium confidence: Mixed-space optimization approach

## Next Checks
1. Stress test categorical complexity by running ACE on datasets with high-cardinality categorical features and measuring B&B tree depth and query efficiency degradation.
2. Evaluate ACE on a black-box classifier known for jagged decision boundaries and compare query efficiency to smooth baselines.
3. Run ACE with varying initialization strategies on the same dataset and measure variance in query counts and CFE quality.