---
ver: rpa2
title: 'The Lock-in Hypothesis: Stagnation by Algorithm'
arxiv_id: '2506.06166'
source_url: https://arxiv.org/abs/2506.06166
tags:
- belief
- lock-in
- knowledge
- diversity
- beliefs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes and tests the "lock-in hypothesis" - the
  idea that feedback loops between large language models (LLMs) and human users can
  lead to entrenchment of false beliefs and loss of diversity. The authors develop
  a Bayesian model showing that lock-in occurs when feedback loops and mutual trust
  are present, causing collective convergence to false beliefs.
---

# The Lock-in Hypothesis: Stagnation by Algorithm

## Quick Facts
- **arXiv ID**: 2506.06166
- **Source URL**: https://arxiv.org/abs/2506.06166
- **Reference count**: 40
- **Key outcome**: LLMs can create feedback loops with humans that entrench false beliefs and reduce diversity through repeated interactions.

## Executive Summary
This paper presents the "lock-in hypothesis" - the idea that feedback loops between large language models and human users can lead to entrenchment of false beliefs and loss of diversity. The authors develop a Bayesian model showing that lock-in occurs when feedback loops and mutual trust are present, causing collective convergence to false beliefs. They validate this with agent-based LLM simulations demonstrating how diversity loss and belief convergence occur through repeated interactions. Using real-world data from WildChat, they find discontinuous drops in conceptual diversity following GPT version updates, providing empirical evidence of human-LLM feedback loops.

## Method Summary
The study employs a three-pronged approach: first, developing a Bayesian model to formalize the lock-in hypothesis; second, conducting agent-based simulations with LLM agents to test the model's predictions; and third, analyzing real-world data from WildChat to identify patterns in human-LLM interactions. The Bayesian framework examines how feedback loops and mutual trust contribute to belief convergence, while simulations demonstrate the dynamics of diversity loss. The empirical analysis tracks changes in conceptual diversity following GPT version updates to identify potential lock-in effects in real-world usage patterns.

## Key Results
- Agent-based simulations show that feedback loops and mutual trust lead to collective convergence on false beliefs
- WildChat data analysis reveals discontinuous drops in conceptual diversity following GPT version updates
- The Bayesian model predicts lock-in occurs when feedback loops and mutual trust are present in the system

## Why This Works (Mechanism)
The lock-in mechanism operates through recursive feedback loops where LLMs generate content that humans consume, then humans produce new content influenced by LLM outputs, creating a self-reinforcing cycle. When both parties exhibit mutual trust in the system, false beliefs can become amplified and entrenched rather than corrected. The Bayesian model demonstrates that even small initial biases can compound through repeated interactions, leading to collective convergence on incorrect beliefs. This process is accelerated when LLMs are trained on data that already contains these biases, creating a positive feedback loop that progressively narrows the space of possible beliefs.

## Foundational Learning

**Bayesian updating with feedback loops**: Understanding how beliefs are updated when information sources are interconnected rather than independent. *Why needed*: The core mechanism relies on recursive belief updates where outputs become inputs. *Quick check*: Can trace belief state changes through multiple iterations of feedback.

**Agent-based modeling principles**: Framework for simulating complex systems through autonomous agents following simple rules. *Why needed*: Simulations demonstrate how individual interaction rules scale to collective behavior patterns. *Quick check*: Can identify emergent properties that weren't programmed explicitly.

**Information diversity metrics**: Methods for quantifying the breadth and variety of concepts in communication. *Why needed*: Empirical validation requires measuring diversity loss over time. *Quick check*: Can distinguish between normal variation and significant drops in diversity.

## Architecture Onboarding

**Component map**: Human users -> LLM interactions -> Generated content -> Training data -> Updated LLM -> Human users (recursive cycle)

**Critical path**: Feedback loop formation -> Mutual trust establishment -> Belief convergence initiation -> Diversity reduction acceleration

**Design tradeoffs**: The study prioritizes theoretical clarity and empirical validation over capturing all nuances of human behavior, using simplified agent models to demonstrate core mechanisms while acknowledging real-world complexity exceeds these simplifications.

**Failure signatures**: Observed discontinuities in conceptual diversity, convergence of belief distributions toward narrow ranges, and amplification of initial biases through repeated interactions.

**First experiments**: 
1. Test Bayesian model with varying levels of initial bias and trust parameters
2. Run agent-based simulations with different feedback loop strengths
3. Analyze WildChat data segmented by user demographics to identify differential lock-in effects

## Open Questions the Paper Calls Out
None

## Limitations
- Agent-based simulations use simplified assumptions about human behavior that may not reflect real-world complexity
- WildChat data analysis shows correlation but cannot definitively prove causation between GPT updates and belief entrenchment
- The lock-in definition may oversimplify the nuanced ways humans interact with and potentially correct AI-generated information

## Confidence

**Theoretical Model**: High confidence - Bayesian framework provides rigorous mathematical foundation for understanding feedback dynamics

**Simulation Results**: Medium confidence - Clear patterns emerge but rely on simplified interaction models

**Empirical Evidence**: Low to medium confidence - Correlations are suggestive but lack controlled experimental conditions

## Next Checks

1. Conduct controlled experiments with human participants interacting with both current and modified LLM versions to isolate causal impact of model updates on belief diversity

2. Implement longitudinal studies tracking individual users' belief changes over time with varying degrees of LLM interaction

3. Develop more sophisticated agent-based models incorporating realistic human behavioral patterns including skepticism and verification behaviors