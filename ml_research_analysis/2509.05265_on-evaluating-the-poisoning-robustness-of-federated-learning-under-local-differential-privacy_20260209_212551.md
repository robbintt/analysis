---
ver: rpa2
title: On Evaluating the Poisoning Robustness of Federated Learning under Local Differential
  Privacy
arxiv_id: '2509.05265'
source_url: https://arxiv.org/abs/2509.05265
tags:
- clients
- learning
- federated
- attacks
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of federated learning (FL)
  protocols that incorporate local differential privacy (LDP) to model poisoning attacks
  (MPA). The authors propose a novel framework to craft adversarial updates that maximize
  global training loss while adhering to LDP constraints.
---

# On Evaluating the Poisoning Robustness of Federated Learning under Local Differential Privacy

## Quick Facts
- **arXiv ID:** 2509.05265
- **Source URL:** https://arxiv.org/abs/2509.05265
- **Reference count:** 40
- **Primary result:** LDPFL protocols are highly vulnerable to model poisoning attacks, with error rates reaching 90%+ even with robust aggregation defenses

## Executive Summary
This paper addresses the critical vulnerability of federated learning (FL) protocols incorporating local differential privacy (LDP) to model poisoning attacks (MPA). The authors propose a novel framework that crafts adversarial updates to maximize global training loss while adhering to LDP constraints. Through extensive experiments across three LDPFL protocols, three datasets, and two neural network architectures, the study demonstrates that attacks can degrade global model accuracy to below 10%, even when robust aggregation defenses like Multi-Krum and trimmed mean are employed. The research highlights the need for stronger defenses against MPA in privacy-preserving FL systems.

## Method Summary
The authors propose a comprehensive framework for poisoning attacks in LDPFL that maximizes global training loss while maintaining LDP compliance. The approach includes Local Loss Reversal Attack (LLRA) that inverts the loss function during local training, Targeted Model Manipulation Attack (TMMA) that strategically corrupts specific model parameters, and Adaptive Poisoning Attack (AdaPA) that embeds aggregation constraints to evade robust defenses. The attacks are evaluated across three LDPFL protocols (LDPSGD, PrivateFL, LDP-FL), three datasets (MNIST, Fashion-MNIST, CIFAR-10), and two neural network architectures, with compromised clients ranging from 5-15% of the total.

## Key Results
- LDPFL protocols are highly vulnerable to MPA, with error rates reaching 90%+ across all tested protocols
- Even small fractions of compromised clients (5-15%) can effectively disrupt model convergence
- Adaptive attacks (AdaPA) successfully evade robust aggregation defenses like Multi-Krum and trimmed mean
- LDP-FL shows critical stability threshold at ε ≈ 0.674, beyond which model collapse occurs even without attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverting the loss function during local training produces gradients that maximize global model loss rather than minimize it
- Mechanism: Compromised clients compute $\nabla -L(\theta_i, x_t, y_t)$ instead of $\nabla L(\theta_i, x_t, y_t)$ during gradient descent
- Core assumption: Compromised clients can control their local training objective while remaining compliant with LDP reporting formats
- Evidence anchors: [abstract] "approach is driven by the objective of maximizing the global training loss while adhering to local privacy constraints"; [section III-B] "loss function is inverted during gradient computation to intentionally maximize the global loss"

### Mechanism 2
- Claim: Embedding aggregation constraints into adversarial optimization allows malicious updates to pass robust aggregation filters
- Mechanism: AdaPA projects adversarial parameters onto feasible sets defined by robust aggregation rules
- Core assumption: Compromised clients can eavesdrop on benign clients' uploaded parameters to compute geometric medians and trimming bounds
- Evidence anchors: [abstract] "develop adaptive attacks that embed carefully crafted constraints into a reverse training process, enabling evasion of these defenses"; [section III-C] "For Multi-Krum... clip the distance between the adversarial parameters and the geometric median"

### Mechanism 3
- Claim: Discrete-valued LDP protocols constrain malicious updates to binary output values, limiting but not eliminating attack effectiveness
- Mechanism: Compromised clients use Clip2Val mapping to ensure outputs conform to two allowed values per dimension
- Core assumption: Attackers cannot bypass the discretization constraint without detection
- Evidence anchors: [section III-B] "A critical constraint in this protocol is that compromised clients must strictly conform to the same two output values of the DataPerturbation algorithm"

## Foundational Learning

- Concept: **Federated Averaging (FedAvg)**
  - Why needed here: Understanding how local model parameters aggregate into the global model is prerequisite to understanding how poisoned updates propagate
  - Quick check question: If 2 of 20 clients submit malicious updates, what maximum influence can they have on the aggregated model under FedAvg?

- Concept: **Local Differential Privacy (ε-LDP)**
  - Why needed here: LDP mechanisms obscure true updates; attackers must account for this perturbation to craft effective poisoned updates
  - Quick check question: Does a smaller ε (stronger privacy) make poisoning attacks easier or harder to execute effectively?

- Concept: **Byzantine-Robust Aggregation**
  - Why needed here: Multi-Krum and trimmed mean are standard defenses; understanding their filtering logic is essential for designing evasion strategies
  - Quick check question: Under Multi-Krum with f=8 and k=10, how many malicious clients can be tolerated before the attack threshold is crossed?

## Architecture Onboarding

- **Component map:**
  - Client-side: Local training loop → gradient clipping → LDP perturbation (Gaussian/discrete) → parameter upload
  - Server-side: Parameter collection → robust aggregation (FedAvg/Multi-Krum/trimmed mean) → global model update → broadcast
  - Attack layer: Compromised clients intercept benign uploads → compute adversarial parameters via AdaPA → submit constrained malicious updates

- **Critical path:**
  1. Identify LDPFL protocol type (gradient-noise vs. parameter-discrete)
  2. Determine robust aggregation method (if any)
  3. Select attack strategy (LLRA-I/O, TMMA-I/O, or AdaPA)
  4. Configure hyperparameters (privacy budget ε, malicious fraction, attack epochs ATE)

- **Design tradeoffs:**
  - LLRA-I vs. LLRA-O: Input poisoning preserves LDP compliance but adds noise that dilutes attack; output poisoning bypasses noise for stronger impact but risks detection
  - Multi-Krum vs. Trimmed Mean defense: Multi-Krum offers stronger protection for simpler datasets/models; trimmed mean is more robust under highly non-IID distributions
  - Privacy budget selection: Weaker budgets (ε < 1) improve privacy but cause model instability; LDP-FL shows critical threshold at ε ≈ 0.674

- **Failure signatures:**
  - Attack ineffective (error rate < 30%): Check if privacy budget is too restrictive (ε < 0.75), malicious fraction below threshold (< 10%), or robust aggregation filtering too aggressive
  - Model collapse without attack: Privacy budget too low (LDP-FL at ε = 0.673) or data heterogeneity extreme (α = 1 with trimmed mean)
  - Adaptive attack detected: Constraint fitting (Algorithm 4) may be insufficient; verify geometric median calculation uses correct benign subset

- **First 3 experiments:**
  1. Baseline attack comparison: Replicate Table II results on MNIST with VGG-Mini, comparing LLRA-I, LLRA-O, TMMA-I, TMMA-O, and RPA across all three LDPFL protocols
  2. Robust aggregation threshold discovery: Vary compromised client fraction (0-25%) against Multi-Krum on CIFAR-10 with ResNet to identify critical threshold where error rate spikes
  3. Privacy budget sensitivity: Test AdaPA against LDPSGD with varying ε (0.75, 2.5, 8.0, 13.0, 33.0) at fixed 10% malicious fraction to quantify privacy-robustness tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can novel aggregation strategies be designed to detect and filter adaptive poisoning attacks (AdaPA) in LDPFL without violating differential privacy guarantees?
- Basis in paper: [explicit] The Abstract and Conclusion explicitly state that current robust aggregators (Multi-Krum, trimmed mean) fail against AdaPA and call for "more robust LDPFL defense strategies"
- Why unresolved: The authors demonstrate the vulnerability of existing defenses but do not propose a specific defensive mechanism that is both LDP-compliant and robust to the proposed attacks
- What evidence would resolve it: A new aggregation rule or client verification method that maintains model accuracy under AdaPA attack without access to raw gradients or breaking privacy guarantees

### Open Question 2
- Question: Can Adaptive Poisoning Attacks (AdaPA) remain effective against robust aggregation if compromised clients are restricted to a local-knowledge scenario (no eavesdropping on benign updates)?
- Basis in paper: [inferred] Section III.C states that AdaPA assumes compromised clients have full knowledge of parameters uploaded by benign clients via eavesdropping
- Why unresolved: The paper does not evaluate the efficacy of the adaptive attack (Algorithm 2) when the initialization and fitting steps must rely on estimation rather than direct interception
- What evidence would resolve it: Experimental results showing attack success rates (Error Rate) when AdaPA is modified to estimate benign updates rather than intercept them

### Open Question 3
- Question: Does the application of strong Local Differential Privacy (low ε) inherently destabilize Federated Learning convergence, thereby making the system more vulnerable to model collapse independent of attack intensity?
- Basis in paper: [inferred] Section IV-E notes that for LDP-FL, reducing ε from 0.674 to 0.673 causes the error rate to spike to 88.7% even without varying the attack fraction
- Why unresolved: The interaction between high noise levels (for privacy) and model convergence stability is not theoretically characterized
- What evidence would resolve it: A theoretical analysis of convergence bounds for LDPFL protocols under perturbation, separating the noise-induced error from attack-induced error

## Limitations
- Focuses exclusively on computer vision tasks with small-scale models, limiting applicability to more complex domains
- Performance generalization to larger models (BERT, GPT), non-image data, and real-world federated networks with heterogeneous hardware remains unclear
- Privacy-utility tradeoff curves beyond the tested privacy budgets remain unexplored, particularly for ε < 0.75 where attacks may become ineffective

## Confidence
- **High**: LLRA/TMMA effectiveness in degrading model accuracy under LDP constraints
- **Medium**: AdaPA's ability to evade robust aggregation defenses across different LDPFL protocols
- **Low**: Performance generalization to larger models, non-image data, and real-world federated networks

## Next Checks
1. **Geometric Median Implementation**: Verify AdaPA geometric median calculations match the referenced library's implementation to ensure constraint fitting accuracy
2. **Privacy Budget Sensitivity**: Test attack effectiveness at privacy budgets below ε = 0.75 to establish practical attack thresholds
3. **Protocol Interoperability**: Evaluate attacks against hybrid LDPFL protocols combining gradient noise and parameter discretization to assess attack adaptability