---
ver: rpa2
title: Chest Disease Detection In X-Ray Images Using Deep Learning Classification
  Method
arxiv_id: '2505.22609'
source_url: https://arxiv.org/abs/2505.22609
tags:
- images
- x-ray
- learning
- dataset
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The project investigates the performance of various classification
  models for detecting four chest diseases (COVID-19, pneumonia, tuberculosis, and
  normal cases) in X-ray images. Transfer learning with state-of-the-art pre-trained
  Convolutional Neural Networks (CNNs) is used to fine-tune models on a large labeled
  dataset of 57,111 X-ray images.
---

# Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method

## Quick Facts
- arXiv ID: 2505.22609
- Source URL: https://arxiv.org/abs/2505.22609
- Reference count: 3
- Primary result: ResNet50 achieved 97.99% accuracy in classifying four chest diseases from X-ray images

## Executive Summary
This study investigates deep learning models for detecting four chest diseases (COVID-19, pneumonia, tuberculosis, and normal cases) in X-ray images. Using transfer learning with pre-trained CNNs on a dataset of 57,111 images, the researchers fine-tuned models and achieved high classification accuracy. The work also employed Grad-CAM for interpretability, revealing that models sometimes learned from image artifacts rather than pathological features, highlighting an important limitation in medical AI applications.

## Method Summary
The study leveraged transfer learning with pre-trained CNNs (ResNet50, VGG16, Xception, EfficientNetV2B0) fine-tuned on 57,111 chest X-ray images. Models were trained for 10 epochs with early stopping, with the last 10 layers made trainable. Hyperparameter tuning optimized dense layer configurations. Grad-CAM was applied for interpretability, and performance was evaluated using accuracy, AUC, F1-score, recall, and precision across four disease classes.

## Key Results
- ResNet50 achieved the highest overall accuracy at 97.99% with strong performance across all metrics
- EfficientNetV2B0 showed the highest raw accuracy at 98.24% with fewer parameters (6.2M total)
- Grad-CAM analysis revealed models sometimes learned from image artifacts (PII boxes, edges) rather than lung features, indicating spurious correlation learning

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning Acceleration
Transfer learning from ImageNet to chest X-rays accelerates convergence and improves accuracy by repurposing low-level visual features (edges, textures) for detecting pathological patterns. Pre-trained CNN layers act as generic feature extractors that are fine-tuned on medical imaging data.

### Mechanism 2: Grad-CAM Interpretability
Grad-CAM provides post-hoc interpretability by visualizing spatial regions driving classification decisions. It computes gradients of predicted class scores with respect to final convolutional feature maps to produce heatmaps highlighting influential image regions.

### Mechanism 3: Residual Architecture Optimization
Fine-tuning deep residual architectures like ResNet50 enables stable optimization and high accuracy across multiple disease classes. Skip connections mitigate vanishing gradients, allowing deeper networks to learn complex hierarchical features needed to distinguish subtle disease presentations.

## Foundational Learning

- **Concept: Transfer Learning & Fine-Tuning**
  - Why needed here: The study adapts existing models rather than training from scratch; understanding frozen vs. trainable layers is critical for replication
  - Quick check question: In the ResNet50 experiment, were all 24.7 million parameters updated during training? (Answer: No, only ~4.5 million; the rest were frozen)

- **Concept: Class Imbalance & Data Augmentation**
  - Why needed here: The dataset is imbalanced (TB at 24% vs. Pneumonia at 32.7%), affecting metric interpretation
  - Quick check question: If the model predicted "Normal" for every image, would high accuracy be reliable given the class distribution?

- **Concept: Spurious Correlations in Medical AI**
  - Why needed here: The paper identifies models learning from text/markers rather than anatomy
  - Quick check question: Why does the paper suggest Grad-CAM revealed a generalization limitation despite high test accuracy? (Answer: Because Grad-CAM showed the model was sometimes "looking" at PII masks or image edges, not lungs)

## Architecture Onboarding

- **Component map:** Input images (224x224 or 299x299) -> Pre-trained CNN backbone (ResNet50, VGG16, Xception, EfficientNetV2B0) -> Flatten layer -> Dense layers (hyperparameter tuned) -> Softmax output (4 classes) -> Grad-CAM module

- **Critical path:**
  1. Data prep: Normalize/resize images
  2. Base model: Load pre-trained weights, freeze base layers, unfreeze top ~10 layers
  3. Training: Run for 10 epochs with early stopping
  4. Validation: Check Grad-CAM overlays during evaluation to ensure lung focus

- **Design tradeoffs:**
  - Hybrid vs. Single Model: Abandoned hybrid (ResNet+VGG) due to 6-hour per epoch training time; favored single architectures for speed
  - Depth vs. Data: EfficientNetV2B0 achieved highest accuracy (98.24%) with fewer parameters than deeper models, suggesting efficiency beats raw depth

- **Failure signatures:**
  - Artifact Learning: Grad-CAM highlighting corners/PII masks instead of lungs
  - Instability: Xception showed validation instability early in training
  - Misclassification: High confusion between "Pneumonia" and "Normal" in EfficientNet/Xception

- **First 3 experiments:**
  1. Baseline Replication: Train frozen ResNet50 with custom dense head to verify 97% accuracy claim
  2. Explainability Audit: Run Grad-CAM on test set and bucket results into "Lung-focused" vs. "Artifact-focused"
  3. Crop-and-Retrain: Implement lung-cropping preprocessor and measure if accuracy drops (indicating model was "cheating" using boxes)

## Open Questions the Paper Calls Out

- Can integrating image classification predictions with patient symptom data in a Large Language Model (LLM) improve diagnostic accuracy compared to image-only models? (Basis: Section 8 suggests exploring LLM integration with patient data)

- Can programmatic evaluation of Grad-CAM heatmaps effectively identify and mitigate dataset biases such as models learning from PII masking or non-lung tissue? (Basis: Section 8 recommends automating Grad-CAM evaluation to identify biases)

- Does standardizing the age distribution across disease classes improve the model's ability to learn pathological features rather than demographic artifacts? (Basis: Section 7 notes normal class contained many children causing model to use features outside lungs; Section 8 recommends age standardization)

## Limitations

- Models learned from image artifacts (PII boxes, edges) rather than anatomical features, revealed through Grad-CAM analysis
- Lack of detailed hyperparameter specifications (batch sizes, optimizer settings, exact train/val/test splits) makes exact replication difficult
- No reported data deduplication methodology across the three combined datasets, potentially affecting generalizability

## Confidence

- **High Confidence:** Transfer learning efficacy (97.99% ResNet50 accuracy supported by strong AUC/metrics); Grad-CAM interpretability mechanism (standard, well-documented technique)
- **Medium Confidence:** ResNet50 superiority (consistent across multiple metrics, but performance gap vs. EfficientNetV2B0 is narrow: 97.99% vs 98.24%)
- **Low Confidence:** Artifact learning generalization (mechanism identified but solution effectiveness unquantified)

## Next Checks

1. Replicate ResNet50 baseline and measure artifact dependence via Grad-CAM before/after cropping PII boxes
2. Train EfficientNetV2B0 under identical conditions to verify the 98.24% accuracy claim and assess per-class performance stability
3. Implement class-weighted loss to test sensitivity of metrics to the observed Normal/Pneumonia confusion pattern