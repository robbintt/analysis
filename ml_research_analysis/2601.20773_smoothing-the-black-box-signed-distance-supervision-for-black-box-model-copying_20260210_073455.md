---
ver: rpa2
title: 'Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying'
arxiv_id: '2601.20773'
source_url: https://arxiv.org/abs/2601.20773
tags:
- copy
- black-box
- copying
- rmse
- algo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of black-box model copying where
  only hard-label outputs are available, which creates a discontinuous surface reconstruction
  challenge. The core method introduces signed-distance supervision, replacing hard
  labels with signed distances to the decision boundary to convert copying into a
  smooth regression problem.
---

# Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying

## Quick Facts
- arXiv ID: 2601.20773
- Source URL: https://arxiv.org/abs/2601.20773
- Authors: Rubén Jiménez; Oriol Pujol
- Reference count: 40
- Primary result: Signed-distance supervision improves black-box model copying fidelity by 10-20% through smooth regression, with provable Hölder/Lipschitz continuity guarantees.

## Executive Summary
This paper addresses the fundamental challenge of black-box model copying where only hard-label outputs are available. The core innovation converts the discontinuous hard-label supervision problem into a smooth regression task by estimating signed distances to the decision boundary. The authors introduce an α-governed regularization scheme that provides provable regularity guarantees while enabling explicit control over the fidelity-smoothness tradeoff. Two model-agnostic algorithms are proposed to estimate signed distances from hard-label queries, with extensive experiments demonstrating consistent improvements in copying fidelity and generalization accuracy across synthetic and UCI benchmark datasets.

## Method Summary
The method transforms black-box copying from a classification problem (hard labels) into a regression problem (signed distances). It uses α-governed smoothing where distance targets are raised to power α, providing α-Hölder continuity for α≤1 and Lipschitz continuity for α≥1. Two algorithms estimate signed distances: Algorithm 1 iteratively refines individual point estimates through binary search, while Algorithm 2 clusters points to share boundary queries for efficiency. The copy model is trained to predict these smoothed distance targets, with the sign indicating class and magnitude indicating uncertainty.

## Key Results
- Converting hard-label copying to signed-distance regression improves sample efficiency and boundary recovery
- α-governed target smoothing provides provable regularity with explicit fidelity-smoothness tradeoff control
- Algorithm 2 (clustered sampling) achieves better practical performance than Algorithm 1 under query budgets despite lower per-point accuracy
- Consistent improvements in fidelity and generalization accuracy over hard-label baselines across multiple datasets
- Distance outputs serve as uncertainty signals for black-box replicas, with reduced generalization error (10-20%) for neural network copies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting hard-label copying to signed-distance regression improves sample efficiency and boundary recovery.
- Mechanism: Hard labels create discontinuous supervision requiring dense sampling near boundaries. Signed distances provide smooth gradients guiding learning from sparse queries, encoding both class (via sign) and boundary proximity (via magnitude) in a single scalar target.
- Core assumption: The teacher's decision boundary is locally regular enough that estimated distances provide meaningful geometric signal.
- Evidence anchors:
  - [abstract] "converting copying into a smooth regression problem that exploits local geometry"
  - [Section 1] "hard-label outputs, providing a discontinuous supervisory signal that is especially problematic for non-smooth boundaries"
  - [corpus] Related work on hard-label attacks (arxiv 2505.16313, 2507.17577) confirms difficulty of boundary recovery from labels alone.
- Break condition: Highly irregular or fragmented decision boundaries where distance estimates become unreliable.

### Mechanism 2
- Claim: The α-governed target smoothing provides provable regularity while allowing explicit control over the fidelity-smoothness tradeoff.
- Mechanism: The transformation ℓ_α(s) = f_O(s)·ξ(s)^α modifies distance targets. For α≤1, targets are α-Hölder continuous (smoothness increases with α). For α≥1, targets become Lipschitz with regularity saturating but robustness to noise increasing.
- Core assumption: Smoothness in the target function translates to improved generalization in the learned copy.
- Evidence anchors:
  - [Section 3.1] Theorem 3.1 provides explicit bounds: "|l_α(x) - l_α(y)| ≤ 2d(x,y)^α" for α≤1
  - [Section 5.2] Figure 8 shows accuracy improving with α when teacher is overfitted
  - [corpus] Limited direct corpus support for α-regularization specifically; claim rests primarily on paper's theoretical contribution.
- Break condition: When exact boundary fidelity is required (e.g., regulatory compliance), high α may over-smooth critical features.

### Mechanism 3
- Claim: Algorithm 2 (clustered sampling) achieves better practical performance than Algorithm 1 under query budgets despite lower per-point accuracy.
- Mechanism: Algorithm 1 iteratively refines individual distance estimates (O(it_max × n × m) calls). Algorithm 2 samples cluster centers with inner/outer point clouds, sharing boundary queries across neighboring points. This yields n_c(n_in + n_out) calls with effective sample count reduced by factor of it_max × n_in.
- Core assumption: Query budget constraints dominate over per-sample precision requirements.
- Evidence anchors:
  - [Section 3.2] Algorithm 2 "represents a significant reduction in cost"
  - [Section 5.1] "Algorithm 2 outperforms Algorithm 1 in this operational setting"
  - [corpus] No direct corpus comparison; efficiency-accuracy tradeoff is paper-specific contribution.
- Break condition: Low-dimensional problems with unconstrained budgets where maximum per-point accuracy is needed.

## Foundational Learning

- Concept: Decision boundary geometry
  - Why needed here: The entire method relies on estimating and regressing distances to the teacher's boundary; without understanding how boundaries partition feature space, the supervision signal is meaningless.
  - Quick check question: Given a binary classifier in 2D, sketch what "signed distance to the decision boundary" means for points on either side.

- Concept: Hölder and Lipschitz continuity
  - Why needed here: Theorem 3.1's guarantees are expressed in these terms; understanding α's effect requires knowing that Hölder continuity with exponent α < 1 allows slower variation than Lipschitz (α = 1).
  - Quick check question: If a function is 0.5-Hölder continuous, how does function change relate to input change compared to a Lipschitz function?

- Concept: Black-box model extraction / copying
  - Why needed here: This is the problem setting—understanding that you only have query access (input → output) without gradients, logits, or training data.
  - Quick check question: List three pieces of information you cannot access in a hard-label black-box setting.

## Architecture Onboarding

- Component map:
  - Query Generator -> Distance Estimator (Algorithm 1 or 2) -> Copy Model Trainer -> α Controller

- Critical path:
  1. Sample query points in operational region R
  2. Query black-box for hard labels
  3. Estimate signed distances using Algorithm 2 (recommended for efficiency)
  4. Apply α-exponent to distances: ℓ_α = sign × distance^α
  5. Train copy model to regress ℓ_α
  6. At inference: predicted sign gives class, magnitude gives uncertainty proxy

- Design tradeoffs:
  - α=0: Maximum fidelity, no smoothing benefit (equivalent to hard-label copying)
  - α=1: Balanced smoothing with Lipschitz targets
  - α>1: Robustness to boundary noise but potential over-smoothing
  - Algorithm 1 vs 2: Accuracy vs. query efficiency
  - Network capacity: Larger networks capture more boundary detail but may overfit to teacher irregularities

- Failure signatures:
  - High-dimensional spaces (d > 30): Distance estimation quality degrades (Table 4 shows MAE increasing from ~0.02 to ~0.3+)
  - Gradient boosting students: Table 2 shows GB copies have +59.8% fidelity error at α=1 (limited smooth regression capability)
  - Over-regularization: If α too high and teacher has legitimate fine-grained structure, accuracy may degrade

- First 3 experiments:
  1. **Baseline comparison**: Replicate Figure 5 on a 2D synthetic dataset (spiral or blobs) comparing hard-copy vs. distance-copy with α∈{0.5, 1.0, 1.5} at fixed query budget (e.g., 10K samples). Verify distance-copy achieves lower fidelity error in low-data regime.
  2. **α-sweep**: On UCI Breast Cancer (30D), train copies with α∈{0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5} using 100K samples. Plot fidelity and accuracy curves similar to Figure 8. Identify optimal α for the fidelity-accuracy tradeoff.
  3. **Distance quality validation**: For a trained copy, compare predicted distances against ground-truth (computed via Algorithm 1 on held-out points). Report MAE/RMSE as in Table 4. Verify that distance magnitude correlates with prediction uncertainty (points near boundary should have lower confidence).

## Open Questions the Paper Calls Out
None

## Limitations
- Core accuracy depends heavily on reliable distance estimation, which degrades significantly in high-dimensional spaces (d > 30)
- Limited validation across diverse model types and boundary complexities
- No analysis of class imbalance effects on signed-distance estimation quality
- Uncertainty proxy claims need systematic calibration validation

## Confidence

- **High confidence**: Theoretical framework for α-governed smoothing (Theorem 3.1) is mathematically sound with proofs. Experimental demonstrations of improved fidelity and sample efficiency are convincing.
- **Medium confidence**: Practical implementation works well on tested datasets but lacks extensive validation across diverse architectures. Efficiency claims for Algorithm 2 supported but not rigorously benchmarked.
- **Low confidence**: Generalization to extremely high-dimensional spaces (>100D) and complex teacher architectures not explored. Impact of class imbalance not discussed.

## Next Checks

1. **High-dimensional stress test**: Evaluate copying fidelity and distance estimation accuracy on datasets with d=50, 100, 200 dimensions. Measure MAE degradation rate and identify dimensionality threshold where approach becomes impractical.

2. **Model-agnostic robustness**: Test Algorithm 2's performance when copying from gradient boosting machines, random forests, and small neural networks with irregular decision boundaries. Compare against Algorithm 1's accuracy to quantify true efficiency-accuracy tradeoff.

3. **Uncertainty validation**: Systematically measure correlation between predicted distance magnitude and actual prediction confidence across multiple datasets. Use calibration metrics to verify distance outputs serve as reliable uncertainty proxies for uncertainty-aware decision-making.