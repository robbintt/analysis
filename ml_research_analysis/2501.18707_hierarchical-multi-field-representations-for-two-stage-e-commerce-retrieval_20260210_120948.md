---
ver: rpa2
title: Hierarchical Multi-field Representations for Two-Stage E-commerce Retrieval
arxiv_id: '2501.18707'
source_url: https://arxiv.org/abs/2501.18707
tags:
- product
- retrieval
- fields
- representations
- field
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CHARM, a novel framework that encodes structured
  product data into hierarchical field-level representations using a block-triangular
  attention mechanism. This mechanism allows tokens of one product field to attend
  to all tokens of this field and all previous fields, creating a cascade of increasingly
  detailed representations.
---

# Hierarchical Multi-field Representations for Two-Stage E-commerce Retrieval

## Quick Facts
- arXiv ID: 2501.18707
- Source URL: https://arxiv.org/abs/2501.18707
- Authors: Niklas Freymuth; Dong Liu; Thomas Ricatte; Saab Mansour
- Reference count: 40
- Key outcome: CHARM matches or outperforms state-of-the-art baselines on large-scale e-commerce datasets while providing inherent explainability through field-specific matching

## Executive Summary
This paper introduces CHARM, a novel framework for e-commerce retrieval that encodes structured product data into hierarchical field-level representations using a block-triangular attention mechanism. By constraining attention to follow a field hierarchy, CHARM creates representations that accumulate detail without suffering from noise dilution in early layers. The framework combines these field-level representations with an aggregated representation for a two-stage retrieval pipeline, enabling efficient initial candidate selection followed by precise fine-tuning. Experiments demonstrate that CHARM aligns more complex queries with more detailed product fields, enhancing retrieval accuracy and diversity while providing inherent explainability through its field-specific matching.

## Method Summary
CHARM employs a two-stage retrieval system that first uses an aggregated vector representation for efficient candidate selection, then applies precise field-level similarity scoring on a shortlist. The core innovation is a block-triangular attention mechanism that imposes a strict hierarchy on field interactions, allowing tokens in later fields to attend to all tokens in earlier fields but not vice versa. This creates a cascade of increasingly detailed representations. The model is trained with a contrastive InfoNCE loss that optimizes both the aggregated vector for coarse matching and individual field vectors for fine-grained retrieval.

## Key Results
- CHARM matches or outperforms state-of-the-art baselines on large-scale e-commerce datasets
- The framework provides inherent explainability through field-specific matching
- Analysis shows CHARM aligns more complex queries with more detailed product fields, enhancing retrieval accuracy and diversity
- The two-stage approach achieves efficiency comparable to single-vector methods while maintaining precision close to full field-dependent retrieval

## Why This Works (Mechanism)

### Mechanism 1: Information Cascade via Block-Triangular Attention
The block-triangular attention mask enforces a directional flow of information where tokens in specific fields (e.g., Description) can attend to their own field and all preceding fields (e.g., Brand, Title), but tokens in preceding fields cannot attend to subsequent ones. This creates representations that accumulate detail while preserving early-layer purity as high-level summaries.

### Mechanism 2: Two-Stage Decoupling of Efficiency and Precision
Separating retrieval into coarse first-stage candidate selection using an aggregated vector and fine-grained second-stage field-level scoring preserves rank quality while reducing computational complexity from O(NM|F|) to O(N(M + k|F|)).

### Mechanism 3: Implicit Query-Complexity Alignment
The model inherently routes simple queries to high-level fields and complex queries to detailed fields without explicit query classification. Field disentanglement allows similarity search to naturally select the field representation that best matches the query's information density.

## Foundational Learning

- **Concept: Dense Bi-Encoders & InfoNCE Loss**
  - Why needed here: CHARM is a bi-encoder architecture mapping queries and products to shared latent space using contrastive learning
  - Quick check question: How does in-batch negative sampling differ from hard negative sampling in the context of Equation 2?

- **Concept: Transformer Attention Masks**
  - Why needed here: The core innovation modifies the standard attention mask to implement block-triangular constraints
  - Quick check question: In a standard causal mask (GPT), token t attends to t-1. In CHARM's block-triangular mask, does a token in field 3 attend to field 1?

- **Concept: Approximate Nearest Neighbor (ANN) Search**
  - Why needed here: The paper claims efficiency via two-stage process requiring effective first-stage indexing
  - Quick check question: Why is Recall@100 a critical metric for validating the first retrieval stage before proceeding to the second?

## Architecture Onboarding

- **Component map:** Input fields (ordered by hierarchy) -> BERT encoder with block-triangular mask -> Field vectors ([CLS_1]...[CLS_n]) + Aggregated vector -> Two-stage retrieval

- **Critical path:** Strict adherence to defined field hierarchy for tokenization and ordering; correct implementation of block-triangular mask logic; priority queue logic for two-stage search

- **Design tradeoffs:** Field order affects performance (length-based vs. importance-based); shortlist size k balances latency vs. accuracy

- **Failure signatures:** Representation collapse if field vectors become identical; shortlist bottleneck if aggregation weights poorly learned

- **First 3 experiments:** 1) Mask validation: verify attention weights for "Title" tokens are zero for "Description" tokens; 2) Ablation on field order: train models with randomized vs. length-based ordering; 3) Shortlist sensitivity: plot Recall@10 vs. Shortlist Size (k)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending the block-triangular attention mechanism to general attention graphs (allowing arbitrary subsets of fields to attend to each other) improve retrieval effectiveness?
- Basis in paper: The authors state in the Conclusion: "In future work, we will investigate extending the block-triangular attention matrix to more general attention graphs, allowing subsets of product fields to attend to arbitrary subsets..."
- Why unresolved: The current mechanism enforces a strict, sequential hierarchy where fields only attend to previous fields, potentially missing complex inter-field dependencies
- What evidence would resolve it: Experiments comparing block-triangular structure against architectures with learned or sparse attention masks permitting non-hierarchical inter-field communication

### Open Question 2
- Question: Does adapting the embedding dimensions of field-level representations to match their information content improve retrieval efficiency and accuracy?
- Basis in paper: The Conclusion notes: "Further, we want to use different retrieval dimensions for the different product fields, matching the amount of retrieval dimensions to the information content of the field..."
- Why unresolved: Currently, all field representations are projected into the same latent dimension, which may be inefficient for low-information fields or restrictive for high-information fields
- What evidence would resolve it: A study evaluating variable-dimension projection layers and their impact on index size, search latency, and recall

### Open Question 3
- Question: Is a static field hierarchy based on text length optimal, or would a dynamic, query-dependent field ordering yield better matching performance?
- Basis in paper: The methodology relies on fixed field order based on the assumption that length approximates information hierarchy
- Why unresolved: A fixed order forces all products into the same information cascade, potentially prioritizing generic fields over specific fields even when complex queries require the reverse
- What evidence would resolve it: An ablation study using a model that dynamically permutes the attention mask or weighting based on the input query, compared against the static length-based baseline

## Limitations

- Field Hierarchy Assumption: The block-triangular attention mechanism relies on a predefined field order that may not hold across all product categories or domains
- Shortlist Size Dependency: The two-stage retrieval's effectiveness critically depends on the initial shortlist size, with insufficient k potentially filtering out relevant items
- Query Length as Complexity Proxy: The model's implicit alignment assumes query length correlates with information need, which may fail for short but semantically complex queries

## Confidence

- **High Confidence**: Architectural innovations (block-triangular attention, two-stage retrieval) are technically sound with strong experimental support
- **Medium Confidence**: Emergent query-complexity alignment is demonstrated within tested datasets but lacks external validation across diverse domains
- **Low Confidence**: General applicability of field hierarchy assumption beyond tested product categories (Home & Kitchen, Electronics, Clothing) remains uncertain

## Next Checks

1. **Field Order Ablation**: Systematically test CHARM performance with different field orderings (randomized, domain-expert defined, and length-based) across multiple product categories to validate hierarchy assumption robustness

2. **Shortlist Size Sensitivity Analysis**: For each dataset, plot Recall@10 and Recall@50 against shortlist size (k) to identify optimal efficiency/accuracy tradeoff point and assess sensitivity to this critical hyperparameter

3. **Query Complexity Validation**: Conduct user studies or analyze click-through data to verify whether the model's implicit routing of complex queries to detailed fields actually improves user satisfaction and conversion rates in production settings