---
ver: rpa2
title: Distilling Multi-modal Large Language Models for Autonomous Driving
arxiv_id: '2501.09757'
source_url: https://arxiv.org/abs/2501.09757
tags:
- dima
- latexit
- planning
- scene
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DiMA, a framework that distills knowledge
  from multi-modal large language models (MLLMs) into vision-based planners for autonomous
  driving. DiMA addresses the challenge of leveraging MLLM's world knowledge while
  maintaining the efficiency of vision-based planners.
---

# Distilling Multi-modal Large Language Models for Autonomous Driving

## Quick Facts
- **arXiv ID:** 2501.09757
- **Source URL:** https://arxiv.org/abs/2501.09757
- **Reference count:** 40
- **Primary result:** DiMA achieves 37% reduction in L2 trajectory error and 80% reduction in collision rate for vision-based planners in autonomous driving.

## Executive Summary
DiMA is a framework that distills knowledge from multi-modal large language models (MLLMs) into vision-based planners for autonomous driving. The approach addresses the challenge of leveraging MLLM's world knowledge while maintaining the efficiency of vision-based planners. DiMA employs a joint training strategy between a vision-based planner and an MLLM, using the vision-based planner as a tokenizer to provide structured inputs to the MLLM. The MLLM is trained on visual question-answering, trajectory estimation, and surrogate tasks designed to align the vision-based planner and MLLM objectives. At inference, the MLLM is optional, enabling efficient planning without compromising robustness.

## Method Summary
DiMA uses a two-stage training process: first, a vision-based planner (such as VAD or UniAD) is pre-trained for 60 epochs on standard planning constraints to generate structured "BEAM" tokens (Bird's-eye-view, Ego, Agent, Map). Then, the MLLM (LLaVA-v1.5-7B) is initialized with Q-former adapters and jointly trained with the vision planner for 30 epochs. The joint training includes losses for planning, LLM tasks, surrogate tasks (masked reconstruction, future prediction, scene editing), and knowledge distillation via KL divergence between penultimate layers. The MLLM is optional at inference, with the vision planner capable of standalone operation.

## Key Results
- 37% reduction in L2 trajectory error for vision-based planners
- 80% reduction in collision rate compared to baselines
- 44% trajectory error reduction in long-tail scenarios on nuScenes planning benchmark

## Why This Works (Mechanism)

### Mechanism 1: Structured Tokenization with Joint Training
The vision-based planner acts as a trainable tokenizer, providing structured BEAM token embeddings to the MLLM. This joint training aligns representations to both spatio-temporal reasoning and semantic understanding, enabling richer, language-grounded scene representations.

### Mechanism 2: Surrogate Tasks for Representation Enrichment and Temporal Grounding
Auxiliary tasks (masked reconstruction, future prediction, scene editing) require the model to infer missing context, anticipate future states, and reason about counterfactuals. This forces the scene encoder to produce representations that support not just immediate planning but also richer inference.

### Mechanism 3: Asymmetric Distillation with Optional Inference
Knowledge is distilled from the MLLM to the vision-based planner during training via KL-divergence loss between penultimate layers, then the MLLM is discarded at inference. This allows efficient deployment while preserving robustness gains.

## Foundational Learning

- **Knowledge Distillation:** Understanding how a smaller model can mimic a larger one is central to DiMA's approach. Quick check: Can you explain the difference between hard and soft label distillation, and why matching penultimate-layer distributions might preserve more semantic information than matching final outputs?

- **Multi-Modal Representation Alignment:** DiMA aligns vision-based scene tokens with language tokens via Q-formers and joint training. Quick check: What challenges arise when aligning embeddings from modalities with different dimensionalities and structural properties, and how do adapter modules like Q-formers help?

- **End-to-End Autonomous Driving Paradigms:** DiMA builds on frameworks like VAD and UniAD; understanding their design (perception, prediction, planning as unified optimization) is prerequisite. Quick check: How do vectorized scene representations differ from rasterized BEV maps, and what are the tradeoffs in terms of computational efficiency and structural reasoning?

## Architecture Onboarding

- **Component map:** Vision-based planner (scene encoder + planning transformer) -> MLLM branch (Q-former adapters, LLM backbone, task heads) -> Surrogate task module (masked reconstruction, future prediction, scene editing heads) -> Distillation loss bridge between penultimate layers -> Optional dual inference path (max-pooling of vision and MLLM features)

- **Critical path:** 1) Pre-train vision-only planner for ~60 epochs under planning constraints (perception, prediction, planning). 2) Initialize MLLM with Q-former adapters; freeze LLM backbone, use LoRA for fine-tuning. 3) Joint training for ~30 epochs with all losses: L_planning + L_LLM + L_recon + L_future + L_distill. 4) Evaluate vision-only branch for efficiency; optionally use MLLM or dual branch for robustness.

- **Design tradeoffs:** Structured vs dense tokens: Structured BEAM tokens improve interpretability and alignment, but require careful design of tokenization and Q-former adapters. Optional MLLM inference: Discarding MLLM saves cost but may sacrifice some robustness; dual inference offers a middle ground. Surrogate task selection: Each task adds computational and annotation overhead; ablation shows all three help, but marginal gains diminish.

- **Failure signatures:** If collision rate remains high despite low L2 error, check surrogate tasks and distillation loss weighting. If VQA performance is poor, verify Q-former adapter training and language token alignment. If vision-only performance degrades after joint training, examine distillation loss weight and potential over-regularization.

- **First 3 experiments:** 1) Reproduce baseline vision planner (e.g., VAD-Tiny) and DiMA variant using provided configs; compare L2 and collision metrics on nuScenes validation and targeted splits. 2) Ablate surrogate tasks one by one to isolate contribution; start with masked reconstruction, then future prediction, then scene editing. 3) Compare inference latency and accuracy of vision-only, MLLM-only, and dual-branch modes to quantify efficiency-robustness tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations

- Exact weighting coefficients for multi-task loss are not specified, introducing variability in balancing objectives.
- The incremental contribution of each surrogate task is not isolated; it's unclear if all three are necessary.
- Generalization to other datasets or real-world deployment is not evaluated; reliance on structured BEAM tokens may limit adaptability.
- The "Scene Editing" surrogate task requires adding/removing agents and generating synthetic trajectories and QA pairs, but the collision-checking algorithm and trajectory plausibility constraints are not detailed.
- The conditions under which dual inference mode outperforms single-branch inference are not characterized; the tradeoff between inference cost and robustness gains is not quantified.

## Confidence

- **High Confidence:** The core claim that structured BEAM tokens, jointly trained with surrogate tasks and distillation, improve planning performance is well-supported by ablation studies and benchmark results.
- **Medium Confidence:** The claim that surrogate tasks enrich temporal and causal reasoning is plausible given the ablation trends, but isolated contributions and failure modes are not fully explored.
- **Medium Confidence:** The efficiency claim (optional MLLM at inference) is demonstrated empirically, but the conditions for when dual-branch inference is beneficial are not characterized.

## Next Checks

1. **Isolate Surrogate Task Contributions:** Perform ablation studies where each surrogate task (masked reconstruction, future prediction, scene editing) is removed individually, and the marginal impact on L2 error, collision rate, and VQA accuracy is measured. This will clarify which tasks are essential and guide resource allocation.

2. **Evaluate Dataset Generalization:** Train and evaluate DiMA on a second autonomous driving dataset (e.g., Waymo Open Dataset or Argoverse) to assess whether the structured BEAM tokenization and joint training approach generalize beyond nuScenes. Compare performance drop and adaptation cost relative to baseline planners.

3. **Characterize Dual-Inference Conditions:** Systematically vary the difficulty and distribution of test scenarios (e.g., high agent density, occlusion, long-tail events) and measure when dual-branch inference significantly outperforms vision-only or MLLM-only modes. Quantify the accuracy-latency tradeoff curve to guide deployment decisions.