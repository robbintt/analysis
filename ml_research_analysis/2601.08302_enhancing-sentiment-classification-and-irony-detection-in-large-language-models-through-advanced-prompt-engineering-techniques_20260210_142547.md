---
ver: rpa2
title: Enhancing Sentiment Classification and Irony Detection in Large Language Models
  through Advanced Prompt Engineering Techniques
arxiv_id: '2601.08302'
source_url: https://arxiv.org/abs/2601.08302
tags:
- sentiment
- prompt
- prompting
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Advanced prompting strategies\u2014few-shot learning, chain-of-thought\
  \ reasoning, and self-consistency\u2014significantly improve sentiment analysis\
  \ accuracy and F1-scores in large language models. Few-shot prompting proved most\
  \ reliable, especially for multi-class tasks and mitigating polarity bias."
---

# Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques

## Quick Facts
- **arXiv ID:** 2601.08302
- **Source URL:** https://arxiv.org/abs/2601.08302
- **Reference count:** 27
- **Key outcome:** Advanced prompting strategies—few-shot learning, chain-of-thought reasoning, and self-consistency—significantly improve sentiment analysis accuracy and F1-scores in large language models.

## Executive Summary
This study evaluates advanced prompt engineering techniques—few-shot learning, chain-of-thought (CoT) reasoning, and self-consistency—on sentiment classification and irony detection tasks using GPT-4o-mini and Gemini-1.5-flash. Few-shot prompting proved most reliable, particularly for multi-class tasks and mitigating polarity bias by encouraging neutral classifications. Chain-of-thought reasoning enhanced irony detection in Gemini-1.5-flash by up to 46% but caused performance declines in GPT-4o-mini, demonstrating model-dependent effects. Self-consistency generally underperformed, occasionally producing confident yet incorrect predictions. Results underscore that prompting efficacy depends on task complexity and model architecture, emphasizing the need for task-specific and model-aware prompt design in sentiment analysis.

## Method Summary
The study evaluates five prompting strategies—zero-shot, one-shot, few-shot (2 examples per class, 3 for neutral), CoT (1 exemplar with reasoning chain), and self-consistency (n=3 samples with majority voting)—across four benchmark datasets (SST-2, SB10k, SemEval-2014 ABSA, SemEval-2018 Irony) sampled to 1,000 instances each. Experiments were conducted using GPT-4o-mini and Gemini-1.5-flash via API with temperature set to 0.2. Performance was measured using accuracy, precision, recall, and weighted F1-score, with statistical significance assessed via bootstrap resampling (1,000 samples, 95% CI). Code is available at https://github.com/Marvin2108/ESCID-LLM-APET.

## Key Results
- Few-shot prompting most reliably improved multi-class sentiment classification and mitigated polarity bias.
- Chain-of-thought reasoning enhanced irony detection in Gemini-1.5-flash by up to 46% but caused declines in GPT-4o-mini.
- Self-consistency generally underperformed, occasionally producing confident yet incorrect predictions.
- Model-specific effectiveness of CoT highlights the need for architecture-aware prompt design.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot prompting mitigates polarity bias and improves classification accuracy by acting as implicit task conditioning.
- **Mechanism:** Providing labeled examples in the prompt allows the model to perform in-context learning, specifically reducing the tendency of LLMs to polarize ambiguous inputs (e.g., forcing a "neutral" classification rather than defaulting to positive/negative).
- **Core assumption:** The model possesses sufficient latent semantic knowledge to generalize from the provided exemplars to the target input.
- **Evidence anchors:**
  - [abstract] Few-shot prompting proved most reliable, especially for multi-class tasks and mitigating polarity bias.
  - [section 5.1] Notes that the inclusion of a single neutral exemplar encouraged models to classify instances as neutral more frequently, increasing recall from 0.37 to 0.51 in one dataset.
  - [corpus] Supporting evidence from "Retrieval-Augmented Few-Shot Prompting" confirms few-shot is a practical alternative to fine-tuning but depends heavily on example quality.
- **Break condition:** Performance degrades if the provided examples are low-quality, noisy, or unrepresentative of the target distribution (overfitting to bad examples).

### Mechanism 2
- **Claim:** Chain-of-thought (CoT) prompting improves irony detection in specific architectures by eliciting explicit reasoning steps that resolve semantic ambiguity.
- **Mechanism:** CoT forces the model to decompose complex linguistic phenomena (like irony) into intermediate reasoning steps. However, this mechanism is highly model-dependent; it works if the model's internal heuristics are calibrated, but fails if the model hallucinates a plausible-but-incorrect reasoning path.
- **Core assumption:** The model's internal representation of "irony" aligns with the dataset labels, and the model is capable of "System 2" (slow) reasoning.
- **Evidence anchors:**
  - [abstract] Chain-of-thought reasoning enhanced irony detection in one model by up to 46%, but caused declines in others.
  - [section 5.2] Highlights a misclassification example where CoT produced a "plausible line of reasoning" but an incorrect conclusion.
  - [corpus] "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting" supports CoT effectiveness for nuanced sentiment, though this paper finds it model-specific.
- **Break condition:** The mechanism fails when "reasoning transparency" does not correlate with "reasoning correctness," leading to confident errors (as seen in the GPT-4o-mini model).

### Mechanism 3
- **Claim:** Self-consistency aggregates multiple reasoning paths but risks amplifying confident, systematic errors.
- **Mechanism:** By sampling diverse reasoning paths and taking a majority vote, self-consistency aims to filter out noise. However, if the model has a strong inherent bias or flawed logic for a specific task, the majority vote simply reinforces the error.
- **Core assumption:** The correct answer is a likely outcome of at least one of the sampled reasoning paths.
- **Evidence anchors:**
  - [abstract] Self-consistency generally underperformed, occasionally producing confident yet incorrect predictions.
  - [section 5.3] Observes that for GPT-4o-mini on SST2, self-consistency resulted in a high number of false negatives, appearing "confident" in misclassifications.
  - [corpus] Weak direct evidence in neighbors regarding failure modes; this is a specific negative finding of the paper.
- **Break condition:** The approach fails when the model's sampling space is biased away from the correct answer, or if temperature settings are too low to generate meaningful path diversity.

## Foundational Learning

- **Concept:** **In-Context Learning (Few-Shot)**
  - **Why needed here:** The study relies on the model's ability to learn patterns from prompt examples without weight updates. Understanding this is crucial to grasping why the *selection* of examples (e.g., neutral class examples) directly shifts the model's decision boundary.
  - **Quick check question:** If you provide 3 positive examples and 0 negative examples in a few-shot prompt, how might the model's probability distribution shift for an ambiguous input?

- **Concept:** **Polarity Bias in LLMs**
  - **Why needed here:** The paper explicitly identifies that LLMs struggle with "neutral" classes, tending to force inputs into positive or negative buckets. This explains the design choice to oversample neutral examples in the prompt.
  - **Quick check question:** Why would a model default to "positive" for a generic statement like "The movie ended at 8 PM," and how does few-shot prompting counteract this?

- **Concept:** **Reasoning vs. Post-Hoc Justification**
  - **Why needed here:** A key finding is that CoT provides *transparency* but not necessarily *correctness*. Distinguishing between a model that reasons its way to an answer versus a model that justifies a pre-determined answer is vital for interpreting the irony detection results.
  - **Quick check question:** In the irony detection failure case, did the CoT prompt fail to generate reasoning, or did the reasoning logic itself lead to the wrong conclusion?

## Architecture Onboarding

- **Component map:**
  - LLM API (GPT-4o-mini or Gemini-1.5-flash) -> Prompt Schema (System Instruction + User Input) -> Decoding (Temperature=0.2) -> Self-Consistency Module (n=3 iterations + Majority Voting)

- **Critical path:**
  1. **Baseline:** Establish zero-shot performance to calibrate model bias.
  2. **Class Balancing:** Identify the "minority/hard" class (e.g., Neutral/Irony) and construct few-shot prompts with oversampling of this class.
  3. **Model-Specific CoT:** Apply CoT *only* if the specific model architecture supports it effectively (e.g., positive signal for Gemini on irony, negative for GPT-4o-mini).

- **Design tradeoffs:**
  - **Few-shot vs. Zero-shot:** Few-shot increases token usage and latency but significantly improves neutral class recall.
  - **CoT:** Increases interpretability and may boost irony detection (Gemini), but risks "plausible hallucinations" and degrades performance on simpler tasks (GPT-4o-mini).
  - **Self-Consistency:** Multiplies inference cost by `n` (e.g., 3x) without guaranteed accuracy gains; often entrenches errors rather than fixing them.

- **Failure signatures:**
  - **The "Confident Mistake":** Self-consistency returning a high-consensus answer that is demonstrably wrong.
  - **The "CoT Spiral":** A Chain-of-thought explanation that starts logically but diverges into semantic over-analysis, resulting in an incorrect label (e.g., seeing irony where there is none).
  - **Polarity Collapse:** The model ignoring the "Neutral" class entirely; requires explicit prompting with neutral exemplars to fix.

- **First 3 experiments:**
  1. **Neutral Class Calibration:** Run a 0-shot vs. 1-shot comparison on a subset of SB10k where the 1-shot example is explicitly "Neutral" to measure the delta in F1-score for that specific class.
  2. **CoT Toxicity Test:** Apply CoT to the Irony dataset on *both* GPT and Gemini to reproduce the "model-dependent" divergence (expect Gemini +46%, GPT decline).
  3. **Temperature Sweep for Self-Consistency:** Re-run self-consistency with a higher temperature (e.g., 0.7) on the SST2 dataset to see if increased reasoning diversity helps majority voting overcome the "confident error" failure mode observed at low temperatures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific prompting strategies exhibit differential efficacy across sentiment tasks (e.g., does CoT excel at irony detection while few-shot excels at multi-class classification)?
- Basis in paper: [explicit] The authors explicitly call for future research to "systematically evaluate whether specific prompting strategies exhibit differential efficacy across various sentiment analysis tasks" in Section 7.3.
- Why unresolved: While the study found few-shot reliable generally and CoT effective for Gemini's irony detection, it did not conclusively map specific strategies to specific task types.
- What evidence would resolve it: A controlled study isolating specific sentiment sub-tasks and measuring the comparative performance of single prompt strategies (CoT vs. Few-shot) on each.

### Open Question 2
- Question: To what extent do optimal prompting strategies transfer across different LLM architectures?
- Basis in paper: [explicit] Section 7.3 asks researchers to "examine how the effectiveness of prompting techniques varies across different LLM architectures," noting that distinct pre-training data or structures may require distinct prompts.
- Why unresolved: The study observed that CoT boosted irony detection in Gemini-1.5-flash but degraded performance in GPT-4o-mini, highlighting a lack of generalizability.
- What evidence would resolve it: A comparative analysis applying the identical set of optimized prompts across a wider range of model families (e.g., LLaMA, Claude) and sizes.

### Open Question 3
- Question: Can retrieval-augmented prompting reduce hallucinations and improve robustness in nuanced sentiment tasks?
- Basis in paper: [explicit] The authors propose "exploration of retrieval-augmented prompting" in Section 7.3 as a means to provide contextual evidence and potentially reduce errors in tasks requiring subtle reasoning.
- Why unresolved: Current approaches relied solely on the models' internal weights, which led to plausible but incorrect reasoning chains in irony detection.
- What evidence would resolve it: Experiments benchmarking standard zero-shot or CoT prompts against retrieval-augmented setups on irony or aspect-based datasets.

### Open Question 4
- Question: How does the interaction between decoding parameters (temperature) and self-consistency prompting affect the reliability of sentiment predictions?
- Basis in paper: [inferred] The authors note in Section 5.3 and 7.2 that self-consistency produced confidently incorrect predictions at a low temperature (0.2), suggesting that varying temperature might improve the diversity of reasoning paths, but this interaction was not tested.
- Why unresolved: It is unclear if the poor performance of self-consistency was inherent to the technique or a result of the fixed, low-temperature parameter preventing diverse reasoning trajectories.
- What evidence would resolve it: An ablation study measuring the accuracy of self-consistency prompts across a range of temperature settings.

## Limitations

- **Model-specific effects:** Chain-of-thought reasoning effectiveness varies significantly between architectures (Gemini vs. GPT-4o-mini), limiting generalizability.
- **Self-consistency risks:** The approach can amplify systematic errors, producing confident yet incorrect predictions.
- **Dataset size constraints:** Findings are based on 1,000-instance samples per dataset, which may not reflect performance on larger-scale data.

## Confidence

**High Confidence:**
- Few-shot prompting reliably improves multi-class sentiment classification accuracy and mitigates polarity bias across both tested models
- Temperature reduction to 0.2 improves classification stability compared to default settings
- The model-dependent effectiveness of chain-of-thought reasoning is consistently observed across both datasets and models

**Medium Confidence:**
- Chain-of-thought reasoning provides up to 46% improvement for irony detection specifically in Gemini-1.5-flash
- Self-consistency generally underperforms despite increased computational cost
- The trade-off between interpretability (CoT) and performance varies significantly by model architecture

**Low Confidence:**
- Generalization of findings to models outside the GPT and Gemini families
- Long-term stability of prompt-engineered improvements without periodic prompt updates
- Performance extrapolation beyond the 1,000-instance sample size used for each dataset

## Next Checks

1. **Model Architecture Transfer Test:** Replicate the full experimental pipeline (all five prompting strategies) on at least one additional LLM family (e.g., Claude or Llama) to validate whether the observed model-dependent effects of chain-of-thought reasoning generalize beyond GPT and Gemini architectures.

2. **Temperature Sensitivity Analysis:** Conduct a systematic temperature sweep (0.1, 0.5, 0.7, 1.0) for self-consistency on the SST-2 dataset to determine if higher temperature settings can mitigate the "confident error" failure mode by increasing reasoning path diversity.

3. **Dataset Size Scaling Experiment:** Run the same prompting strategies on progressively larger subsets of each dataset (100, 500, 2,000, 5,000 instances) to establish whether the observed performance improvements scale proportionally or if diminishing returns occur with increased data volume.