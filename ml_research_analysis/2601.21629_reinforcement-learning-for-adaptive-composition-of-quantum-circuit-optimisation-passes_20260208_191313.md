---
ver: rpa2
title: Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation
  Passes
arxiv_id: '2601.21629'
source_url: https://arxiv.org/abs/2601.21629
tags:
- circuit
- circuits
- passes
- optimisation
- quantum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of optimising quantum circuits
  by selecting the most effective sequence of optimisation passes from a suite of
  available options. While individual optimisation passes are well-tested, the order
  in which they are applied significantly impacts circuit quality, and designing optimal
  sequences for each circuit requires expert knowledge.
---

# Reinforcement Learning for Adaptive Composition of Quantum Circuit Optimisation Passes

## Quick Facts
- arXiv ID: 2601.21629
- Source URL: https://arxiv.org/abs/2601.21629
- Reference count: 40
- The paper proposes training a reinforcement learning agent to automatically select and apply sequences of optimisation passes to reduce the number of two-qubit gates in quantum circuits, achieving better results than default optimisation pass sequences.

## Executive Summary
This paper addresses the challenge of optimising quantum circuits by selecting the most effective sequence of optimisation passes from a suite of available options. While individual optimisation passes are well-tested, the order in which they are applied significantly impacts circuit quality, and designing optimal sequences for each circuit requires expert knowledge. The authors propose training a reinforcement learning agent to automatically select and apply sequences of optimisation passes to reduce the number of two-qubit gates in quantum circuits, which is critical for improving circuit performance on current noisy quantum devices.

## Method Summary
The core method involves encoding quantum circuits as graphs, where nodes represent gates and edges represent qubit connections. A reinforcement learning agent, trained using Proximal Policy Optimisation (PPO) with graph neural networks, learns to select the next optimisation pass to apply based on the current circuit state. The agent's action space consists of PyTKET optimisation passes designed to reduce two-qubit gate counts. The training reward is based on the fraction of two-qubit gates removed by the applied sequence. The approach was trained on 402,558 synthetic circuits and evaluated on a test set of 40,000 circuits.

## Key Results
- The RL agent outperforms default PyTKET optimisation pass sequences, achieving (mean, median) fraction of two-qubit gates removed of (57.7%, 56.7%) compared to (41.8%, 50.0%) for the best default pass sequence
- The agent generalises well to larger circuits than those seen during training, demonstrating scalability
- The approach is more scalable than search-based methods for generating optimisation pass sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoding quantum circuits as graphs with gate-level features allows a neural network to process variable-sized inputs and capture the structural relationships necessary for optimization.
- **Mechanism:** The system maps nodes to gates and edges to qubit connections, using a modified GINConv operator. Mean pooling aggregates node features into a graph-level representation vector, ensuring the architecture is invariant to circuit size (Section 3.3).
- **Core assumption:** The topological structure of the circuit graph contains sufficient information to predict the utility of an optimization pass without requiring physical simulation of the quantum state.
- **Evidence anchors:**
  - [Section 3.1]: "Our graph encoding of the circuit uses nodes to represent gates... Qubits are represented by edges."
  - [Section 3.3]: "We use mean pooling to construct a graph-level representation vector, ensuring that the trained network can ingest circuits of different sizes."
  - [corpus]: Weak direct evidence in the provided corpus neighbors, though related work (Optimizing Quantum Circuits via ZX Diagrams...) also relies on graph representations.
- **Break condition:** If critical optimization dependencies rely on non-local quantum correlations (e.g., global phase interactions) that are not explicitly encoded in the local node features or graph edges.

### Mechanism 2
- **Claim:** Framing compiler optimization as a sequential decision process allows an agent to accumulate small improvements that result in superior global optimization compared to single-pass or fixed-sequence heuristics.
- **Mechanism:** The agent uses Proximal Policy Optimization (PPO) to learn a stochastic policy. It selects a pass, applies it, and receives a reward proportional to the fraction of two-qubit gates removed (Eq. 4). The agent learns to delay immediate rewards (e.g., via `ThreeQubitSquash`) to enable higher rewards later (e.g., via `KAKDecomposition`).
- **Core assumption:** The "Phase Ordering Problem" can be effectively navigated via heuristics learned from reward feedback rather than formal symbolic analysis.
- **Evidence anchors:**
  - [Section 3.3]: "The reward for the $t$-th action is $r_t = \frac{n_{t-1} - n_t}{n_0}$."
  - [Section 4]: "In the case of Ordered-Clifford-SU4 circuits... removing the Clifford identities... allows the remaining two-qubit subcircuits to be optimised... The model is almost always able to identify this as the correct sequence."
  - [corpus]: Assumption reinforced by "OrQstrator" (corpus neighbor) utilizing Deep RL for similar orchestration.
- **Break condition:** If the action space lacks passes capable of interacting constructively (i.e., if all passes were mutually independent or destructive).

### Mechanism 3
- **Claim:** Training on small, synthetic circuits enables the model to generalize optimization strategies to larger, deeper circuits that are computationally expensive to train on directly.
- **Mechanism:** The GNN learns to recognize local sub-circuit patterns (e.g., a Clifford subcircuit flanked by SU(4) gates) and the associated effective pass sequences. Because large circuits are composed of similar local structures, the policy transfers.
- **Core assumption:** The optimal pass sequence is determined primarily by local structural motifs rather than the global circuit size or total gate count.
- **Evidence anchors:**
  - [Section 4]: "Qubit count is in the range [8,12]... performance... is comparable to that seen in [in-distribution testing]."
  - [Section 4]: "This demonstrates that we can deploy the model on large circuits while conducting very manageable training on smaller circuits."
  - [corpus]: No direct evidence provided in neighbors; this is a specific claim of the source paper.
- **Break condition:** If larger circuits exhibit emergent global structures or connectivity constraints not present in the random synthetic training data.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs)**
  - **Why needed here:** Standard neural networks require fixed-size inputs (like vectors or images). Quantum circuits vary in size (qubits/gates). GNNs process the circuit as a graph, treating gates as nodes, making the architecture size-agnostic.
  - **Quick check question:** How does the model handle a circuit with 50 gates vs. 500 gates? (Answer: The node features are processed identically; the aggregation step (mean pooling) condenses the graph into a fixed-size vector regardless of node count.)

- **Concept: Reinforcement Learning (PPO)**
  - **Why needed here:** There is no single "correct" output sequence to supervise (label). We only know the outcome (fewer gates is better). RL allows the model to learn by trial and error to maximize a reward signal (gate reduction).
  - **Quick check question:** Why is "reward shaping" critical here? (Answer: Raw gate count might be sparse; the paper normalizes by initial gate count and adds small action penalties to guide the agent toward efficient sequences.)

- **Concept: Quantum Transpilation Passes**
  - **Why needed here:** To understand the action space. The agent is not inventing new physics; it is orchestrating existing tools (like `KAKDecomposition` or `CliffordSimp`) that perform specific graph rewrites on the circuit.
  - **Quick check question:** Why is the "Phase Ordering Problem" hard? (Answer: Pass A might enable Pass B, but Pass B might destroy the structure Pass A needs to work effectively. The order matters.)

## Architecture Onboarding

- **Component map:** Circuit graph -> GNN encoder -> 128-dim vector -> Policy head (MLP + Softmax) and Value head (MLP) -> Action selection -> PyTKET environment applies pass -> Reward calculation -> PPO update

- **Critical path:**
  1.  **Data Generation:** Generate random circuits (Random-SU4, QAOA, etc.) and convert to graphs.
  2.  **Rollout:** Agent observes graph, samples pass, env applies pass.
  3.  **Update:** Collect trajectory (state, action, reward), calculate advantage (GAE), update GNN weights via PPO.
  4.  **Inference:** Load circuit -> Graph -> Greedy action selection (argmax) until `DoNothing` is chosen.

- **Design tradeoffs:**
  - **Action Space:** Limited to PyTKET passes for fair comparison. **Tradeoff:** Excluding passes from other libraries (e.g., Qiskit) limits maximum potential optimization.
  - **Reward:** Normalized fraction of gates removed. **Tradeoff:** Equalizes importance of small and large circuits, but might obscure optimization difficulty differences.
  - **Termination:** Agent selects `DoNothing` or hits a step limit. **Tradeoff:** Prevents infinite loops but might cut short long-horizon optimizations.

- **Failure signatures:**
  - **Pass Cycling:** Agent repeatedly applies `CliffordSimp` then `CliffordResynthesis` with no net gain. (Mitigated in paper by including "last pass index" as a node feature).
  - **Over-optimization:** Agent reduces gate count but increases depth or specific error rates not captured in the reward function.
  - **Catastrophic Forgetting:** Agent learns to optimize QAOA circuits perfectly but forgets how to optimize Random-SU4.

- **First 3 experiments:**
  1.  **Sanity Check (Overfit):** Train the agent on a single circuit instance. Verify it can learn to reduce the gate count to the theoretical minimum for that specific graph.
  2.  **Baseline Comparison:** Run the trained agent on the 40k test set vs. `FullPeepholeOptimise`. Verify the (57.7% vs 41.8%) gap exists locally.
  3.  **Scalability Stress Test:** Evaluate inference time and memory usage on 100-qubit circuits (Fig 6 replication) to confirm the linear scaling of the GNN encoder.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an RL agent improve optimization by selecting specific local regions of a circuit to rewrite, rather than applying global passes to the entire structure?
- **Basis in paper:** [explicit] The authors state, "A natural application of RL extending the ideas in this paper would be to develop an agent that can select regions of the circuit to rewrite, in addition to selecting the pass."
- **Why unresolved:** The current agent is restricted to global passes, which may improve one circuit region while degrading another, a limitation of the Phase Ordering Problem.
- **What evidence would resolve it:** Demonstration of an RL agent that outputs (region, pass) pairs and outperforms the global-only approach.

### Open Question 2
- **Question:** Does training the agent on real-world user circuits yield significantly better generalization than training on synthetic data?
- **Basis in paper:** [explicit] The Conclusion notes that using "a training set consisting wholly or partly of real user circuits would be beneficial," though no such dataset was available for this study.
- **Why unresolved:** The model was trained exclusively on synthetic circuits (e.g., Random-SU4, QAOA) due to the lack of large-scale real-world datasets.
- **What evidence would resolve it:** Benchmarking the agent's performance when fine-tuned or trained on a large corpus of user-generated quantum programs.

### Open Question 3
- **Question:** Can the proposed RL methodology be effectively adapted to minimize T-gate counts for fault-tolerant quantum computing?
- **Basis in paper:** [explicit] The paper encourages "investigations of that kind" for T-gate reduction, noting that T-gates are the dominant error source in fault-tolerant regimes.
- **Why unresolved:** The current work focused exclusively on two-qubit gate reduction, which is the primary metric for current noisy intermediate-scale quantum (NISQ) devices.
- **What evidence would resolve it:** Results from an agent trained with a T-gate count reward function matching or exceeding specialized T-gate optimization baselines.

## Limitations
- The specific circuit generation procedures remain underspecified, particularly depth and gate distribution parameters for each synthetic class
- The claim that optimal pass sequences depend primarily on local structural motifs is compelling but not rigorously tested
- The reward function normalizes by initial gate count, which may obscure class-specific optimization difficulty
- All training data is synthetic, with no results shown on real-world quantum circuits from applications like chemistry or optimization

## Confidence
- **High confidence** in the core empirical result: the RL agent outperforms default PyTKET sequences with clear numerical margins (57.7% vs 41.8% mean two-qubit gate reduction)
- **Medium confidence** in the scalability claim: while inference on larger circuits is demonstrated, the paper does not provide wall-clock timing or memory benchmarks that would validate the claimed computational advantages over search-based methods
- **Low confidence** in the generalizability to non-synthetic circuits: all training data is synthetic, and while the agent generalizes within synthetic classes, no results are shown on real-world quantum circuits

## Next Checks
1. **Architectural replication:** Reproduce the GNN encoder and PPO training pipeline using the exact hyperparameters and PyTKET action space to verify the 15.9 percentage point improvement gap
2. **Scalability verification:** Measure inference time and memory usage on circuits with 50+ qubits and compare against both default PyTKET and a simple beam search baseline to validate the claimed computational advantages
3. **Transfer robustness test:** Train on a subset of synthetic classes and evaluate on held-out classes (e.g., train on Random-SU4, test on QAOA) to quantify the agent's ability to generalize optimization strategies across circuit types