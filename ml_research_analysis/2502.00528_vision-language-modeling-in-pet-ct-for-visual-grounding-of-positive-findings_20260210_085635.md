---
ver: rpa2
title: Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings
arxiv_id: '2502.00528'
source_url: https://arxiv.org/abs/2502.00528
tags:
- suvmax
- contextual
- visual
- grounding
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of visual grounding in whole-body
  PET/CT imaging, where connecting text descriptions of lesions to their specific
  image locations could enhance radiology reporting and clinical workflows. The authors
  developed an automated weak-labeling pipeline that extracts lesion descriptions
  containing SUVmax values and axial slice numbers from radiology reports, then generates
  corresponding 3D segmentation labels using an iterative thresholding algorithm.
---

# Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings

## Quick Facts
- arXiv ID: 2502.00528
- Source URL: https://arxiv.org/abs/2502.00528
- Reference count: 31
- Primary result: Vision-language model achieved F1=0.80 on PET/CT lesion detection, outperforming LLMSeg (F1=0.22) but underperforming physicians (F1=0.94)

## Executive Summary
This study introduces ConTEXTual Net 3D, a vision-language model designed to automatically detect and segment lesions in whole-body PET/CT scans using text descriptions from radiology reports. The approach addresses the challenge of visual grounding in PET/CT by connecting textual findings (SUVmax values, slice numbers) to specific image locations through a 3D nnU-Net with cross-attention to text embeddings. Trained on 11,356 labeled image-text pairs from 25,578 PET/CT exams, the model demonstrates promising performance (F1=0.80) but still falls short of expert human readers. The work represents a significant step toward automated lesion detection that could enhance radiology reporting and clinical workflows.

## Method Summary
The authors developed an automated weak-labeling pipeline that extracts lesion descriptions containing SUVmax values and axial slice numbers from radiology reports, then generates corresponding 3D segmentation labels using an iterative thresholding algorithm. From 25,578 PET/CT exams, they created 11,356 labeled image-text pairs spanning multiple radiotracer types. The ConTEXTual Net 3D architecture integrates text embeddings with a 3D nnU-Net via cross-attention, allowing the model to use both visual and textual information for lesion localization. The model was evaluated against LLMSeg, a 2.5D approach, and two nuclear medicine physicians across different radiotracer types including FDG, DCFPyL, DOTATE, and Fluciclovine.

## Key Results
- ConTEXTual Net 3D achieved F1 score of 0.80, significantly outperforming LLMSeg (F1=0.22) and 2.5D approach (F1=0.53)
- Model performance varied by radiotracer type: FDG (F1=0.78), DCFPyL (F1=0.75), Fluciclovine (F1=0.66), DOTATE (F1=0.58)
- Underperformed two nuclear medicine physicians (F1=0.94 and 0.91) but demonstrated feasibility of vision-language modeling in PET/CT

## Why This Works (Mechanism)
The model leverages the complementary information from text descriptions (SUVmax values, anatomical location) and visual features from PET/CT images. By using cross-attention between text embeddings and 3D visual features, the model can focus on relevant regions while incorporating quantitative and qualitative information from radiology reports. The weak-labeling approach enables large-scale training without manual annotation, and the 3D architecture captures volumetric context crucial for lesion detection in whole-body imaging.

## Foundational Learning
- PET/CT imaging fundamentals: Why needed - Understanding how PET and CT modalities complement each other in lesion detection; Quick check - Can distinguish between metabolic (PET) and anatomical (CT) information
- SUVmax extraction and significance: Why needed - SUVmax is a key quantitative metric used in radiology reports; Quick check - Can explain how SUVmax relates to lesion intensity and why it's used for thresholding
- Cross-attention mechanisms: Why needed - Core to integrating text and visual information; Quick check - Can describe how cross-attention differs from self-attention in vision-language models
- Weak labeling in medical imaging: Why needed - Enables large-scale training without manual annotation; Quick check - Can explain advantages and limitations of weak labeling versus manual annotation
- nnU-Net architecture: Why needed - Foundation for 3D segmentation in medical imaging; Quick check - Can identify key components of nnU-Net and their roles

## Architecture Onboarding

**Component Map:**
Text Embedding -> Cross-Attention -> 3D nnU-Net -> Segmentation Output

**Critical Path:**
The critical path flows from text preprocessing and embedding through cross-attention layers to the 3D nnU-Net backbone, ultimately producing segmentation masks. The cross-attention mechanism is essential as it allows the model to dynamically weight visual features based on textual information.

**Design Tradeoffs:**
The authors chose weak labeling over manual annotation to enable large-scale training, accepting the trade-off of label uncertainty. They opted for a 3D architecture over 2.5D to capture volumetric context, though this increases computational complexity. The cross-attention design prioritizes interpretability and integration of textual information over pure visual performance.

**Failure Signatures:**
Performance degradation on certain radiotracers (DOTATE, Fluciclovine) suggests the model struggles with varying tracer kinetics and imaging protocols. Weak labeling errors, particularly when SUVmax values don't precisely match image locations or when multiple lesions exist, lead to false positives or missed detections. The model may also fail when text descriptions are ambiguous or incomplete.

**First Experiments:**
1. Ablation study removing text embeddings to quantify their contribution to performance
2. Evaluation of threshold sensitivity by varying the SUVmax-based segmentation parameters
3. Analysis of false positive/negative cases to identify systematic failure modes

## Open Questions the Paper Calls Out
None

## Limitations
- Weak labeling introduces inherent uncertainty, particularly when SUVmax values and slice numbers don't precisely match ground truth
- Performance degradation on certain radiotracers (DOTATE F1=0.58, Fluciclovine F1=0.66) suggests limited generalizability across different imaging protocols
- Evaluation compared primarily against two nuclear medicine physicians rather than larger cohort, limiting statistical power

## Confidence
- Model architecture and implementation: **High** - Methodology clearly described with reproducible components
- Performance metrics on curated dataset: **Medium** - Results internally consistent but limited by weak labeling quality
- Generalizability to clinical practice: **Low** - Limited evaluation scope and no false positive analysis

## Next Checks
1. Conduct multi-reader evaluation with at least 10 nuclear medicine physicians to establish performance variance and confidence intervals
2. Implement prospective study comparing model outputs against radiologist-confirmed ground truth on previously unseen cases
3. Perform ablation studies to quantify impact of text embedding quality and threshold selection on segmentation accuracy