---
ver: rpa2
title: Time-Varying Audio Effect Modeling by End-to-End Adversarial Training
arxiv_id: '2512.15313'
source_url: https://arxiv.org/abs/2512.15313
tags:
- modulation
- loss
- training
- audio
- effects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of modeling time-varying audio
  effects using only input-output recordings, without requiring extracted modulation
  signals. The proposed approach uses a Generative Adversarial Network (GAN) framework
  with a convolutional-recurrent architecture, trained via a two-stage strategy: an
  initial adversarial phase to learn the modulation behavior distribution, followed
  by a supervised fine-tuning phase with a State Prediction Network (SPN) to synchronize
  the model''s internal states.'
---

# Time-Varying Audio Effect Modeling by End-to-End Adversarial Training

## Quick Facts
- **arXiv ID:** 2512.15313
- **Source URL:** https://arxiv.org/abs/2512.15313
- **Reference count:** 29
- **Primary result:** Models time-varying audio effects (phaser) using only input-output recordings via GAN framework with State Prediction Network for synchronization

## Executive Summary
This paper addresses the challenge of modeling time-varying audio effects using only input-output recordings, without requiring extracted modulation signals. The proposed approach uses a Generative Adversarial Network (GAN) framework with a convolutional-recurrent architecture, trained via a two-stage strategy: an initial adversarial phase to learn the modulation behavior distribution, followed by a supervised fine-tuning phase with a State Prediction Network (SPN) to synchronize the model's internal states. A new objective metric based on chirp-train signals is introduced to quantify modulation accuracy. Experiments on a vintage hardware phaser demonstrate the method's ability to capture time-varying dynamics in a fully black-box context.

## Method Summary
The method trains a convolutional-recurrent GAN to model time-varying audio effects without access to modulation signals. The generator contains a modulation path (LSTM-based ModBlocks) that produces time-varying parameters and an audio path (FXBlocks with FiLM modulation) that processes the input. Training proceeds in two phases: first, adversarial training learns the modulation behavior distribution with stochastic state initialization; second, a State Prediction Network fine-tunes synchronization by predicting initial LSTM states from input-output windows. A mode-seeking regularization loss encourages sensitivity to initial states during the adversarial phase.

## Key Results
- The two-phase adversarial training strategy successfully learns time-varying modulation behavior without phase-aligned supervision
- SPN-based synchronization improves fine-tuning convergence, particularly for faster modulation rates
- The chirp-train based modulation metric effectively quantifies LFO phase alignment in validation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial training enables learning modulation behavior distribution without requiring phase-aligned supervision.
- **Mechanism:** The discriminator learns to distinguish real from generated audio, forcing the generator to produce outputs with plausible time-varying characteristics. By initializing LSTM states stochastically (h₀ sampled from distributions or angle encodings), the model explores the full modulation phase space rather than collapsing to a single phase-averaged behavior.
- **Core assumption:** The discriminator can learn to recognize authentic modulation patterns without explicit phase alignment signals.
- **Evidence anchors:**
  - [abstract] "an initial adversarial phase allows the model to learn the distribution of the modulation behavior without strict phase constraints"
  - [Section 3.4.2] "Phase I: Adversarial Training... forces the model to learn the time-varying effect's general behavior without relying on a specific LFO phase"
  - [corpus] Weak direct evidence; related work on adversarial audio modeling exists (Wright et al., Chen et al.) but phase-aligned time-varying systems remain understudied.
- **Break condition:** If discriminator overpowers generator (loss collapses to 0), or if mode collapse occurs where all outputs converge to identical modulation phases regardless of h₀ initialization.

### Mechanism 2
- **Claim:** State Prediction Network (SPN) enables fine-tuning synchronization by predicting initial LSTM states from input-output windows.
- **Mechanism:** The SPN reuses the discriminator's learned feature representations to predict h₀ values that align the generator's internal modulation phase with target recordings. Transfer learning from frozen discriminator features provides stable semantic representations; the SPN learns a mapping from (x, y, φ) tuples to appropriate initial states.
- **Core assumption:** The discriminator's feature space captures modulation-relevant information that can be inverted to predict meaningful initial states.
- **Evidence anchors:**
  - [abstract] "a State Prediction Network (SPN) estimates the initial internal states required to synchronize the model with the target"
  - [Section 3.3] "We leverage the discriminator's architecture for feature extraction... this acts as a form of transfer learning, facilitating the SPN's training"
  - [corpus] Prior work (Bourdin et al. 2024, 2025) applied SPN to time-dependent effects; extension to time-varying systems is novel here.
- **Break condition:** If SPN overfits to training windows and fails to generalize on unseen data, or if discriminator features lack modulation phase information (SPN loss plateaus without synchronization improvement).

### Mechanism 3
- **Claim:** Mode seeking regularization prevents generator from ignoring stochastic state inputs during conditional generation.
- **Mechanism:** By minimizing the inverse MR-STFT distance between outputs generated from different h₀ values (same input), the loss explicitly rewards sensitivity to initial states. This counters the conditional GAN tendency to rely solely on high-dimensional audio input while ignoring lower-dimensional stochastic inputs.
- **Core assumption:** Modulation phase variation produces measurable spectral differences that the MR-STFT distance can capture.
- **Evidence anchors:**
  - [Section 3.4.3] "In the specific context of our work, this phenomenon manifests when the generator's internal states fail to influence the time-varying behavior of the model"
  - [Section 5.1] "with mode seeking disabled, convergence was more difficult" (Slow-LFO dataset)
  - [corpus] Mode seeking (Mao et al., 2019) originally proposed for image synthesis; adaptation to audio time-series via MR-STFT is domain-specific.
- **Break condition:** If ε or λ_ms are too aggressive, spectral loss degrades (Figure 9 shows mode seeking increased final MR-STFT loss despite improving modulation metrics).

## Foundational Learning

- **Concept: Generative Adversarial Networks (hinge loss formulation)**
  - **Why needed here:** The adversarial phase relies on generator/discriminator equilibrium; understanding hinge loss (Equations 4-5) and gradient balancing (Equation 7) is essential for debugging training instability.
  - **Quick check question:** Can you explain why discriminator loss terms use +1 and -1 margins differently for real vs. fake samples?

- **Concept: Stateful recurrent processing with LSTM**
  - **Why needed here:** Modulation dynamics are encoded in LSTM hidden states; initial state (h₀) determines LFO phase. Understanding state propagation is critical for interpreting SPN outputs and diagnosing phase drift.
  - **Quick check question:** If you initialize two LSTM layers with different h₀ values but identical input sequences, what determines whether outputs diverge significantly?

- **Concept: Feature-wise Linear Modulation (FiLM)**
  - **Why needed here:** FXBlocks receive modulation tensors μ_j via FiLM conditioning; understanding gain/bias modulation of activations clarifies how the modulation path controls audio processing.
  - **Quick check question:** How would you verify whether FiLM layers are actually modulating FXBlock outputs versus being bypassed (as in snapshot modeling)?

## Architecture Onboarding

- **Component map:**
  - **Generator (SPTVMod):** Modulation path (3× ModBlocks with LSTM, pooling, 1×1 projections) → Audio path (3× FXBlocks with dilated depthwise convolutions, FiLM modulation)
  - **Discriminator:** 6× FeatBlocks (conv → PReLU → pooling) → 1×1 conv + global pooling → FC head
  - **State Prediction Network:** Shares discriminator's FeatBlocks (frozen) → 1×1 conv + global pooling → FC → h₀ outputs for each LSTM layer

- **Critical path:**
  1. Input audio x → ModBlock LSTM receives h₀ (stochastic or SPN-predicted)
  2. LSTM outputs → modulation tensors μ_j
  3. μ_j → FiLM modulates FXBlock convolutions
  4. FXBlocks process audio with time-varying parameters → output ŷ
  5. Discriminator evaluates (x, ŷ, φ) tuples

- **Design tradeoffs:**
  - **Window size (32768 samples = 743ms):** Must balance multiple LFO periods for stable modulation learning vs. computational cost. Slow-LFO (1.3s period) fits <1 period per window → instability risk; Fast-LFO (0.3s) fits ~2.5 periods → more stable.
  - **SPN pre-training:** Adds 50k iterations but prevents modulation collapse when SPN initializes poorly. Paper recommends enabling for Fast-LFO; impact on Slow-LFO is mixed.
  - **Mode seeking:** Improves modulation loss in adversarial phase but degrades final spectral accuracy in fine-tuning. Assumption: unnecessary with two-phase training.

- **Failure signatures:**
  - **No modulation in output:** Modulation loss >0.9; spectrogram shows static filtering (Figure 7h, 7p)
  - **Phase mismatch:** MR-STFT loss high despite perceptually correct modulation (Figure 8h vs 8j)
  - **Modulation collapse after window:** Streamed inference loses modulation beyond training window duration (common for Slow-LFO where T_LFO > window size)
  - **SPN failure to generalize:** Training MR-STFT decreases while validation plateaus or increases

- **First 3 experiments:**
  1. **Validate adversarial phase learning:** Train generator-only with adversarial loss on Fast-LFO dataset; verify modulation loss decreases below 0.4 and spectrograms show periodic patterns. If modulation loss stagnates above 0.9, check discriminator capacity or increase spectral loss weight (default 0.005).
  2. **Test SPN pre-training ablation:** Compare fine-tuning with vs. without 50k-iteration SPN pre-training phase. Measure validation MR-STFT after 400k fine-tuning iterations. Expect larger improvement on Fast-LFO; if neither improves, verify discriminator FeatBlocks are frozen during SPN training.
  3. **Measure window-size sensitivity:** Retrain on Slow-LFO with doubled window size (65536 samples); observe whether modulation loss stabilizes and streamed inference maintains modulation longer. Tradeoff: increased GPU memory and reduced batch diversity.

## Open Questions the Paper Calls Out

- **Can employing differentiable oscillators instead of LSTM layers enforce LFO periodicity and prevent modulation collapse during streamed processing?**
  - The authors propose replacing LSTMs with differentiable oscillators to address the issue of modulation collapse observed in streamed processing.
  - LSTMs struggle to maintain strict periodicity over long sequences during inference, causing the modulation to drift or collapse after the training window duration.
  - Comparative experiments show sustained, stable modulation in output spectrograms during streamed inference for oscillator-based models versus LSTMs.

- **Can the underlying structure of learned internal states be analyzed to allow for manual initialization, removing the dependency on the State Prediction Network (SPN)?**
  - The conclusion suggests that investigating the underlying structure of these learned states could allow for manual initialization, thereby removing the dependency on the SPN.
  - The SPN currently acts as a black-box estimator that is prone to overfitting, and the latent space geometry of the recurrent states remains uninterpreted.
  - Identification of a disentangled latent direction corresponding to phase, allowing inference via algorithmic initialization rather than network prediction.

- **Can this framework effectively model non-periodic time-varying effects, such as magnetic tape recorders with stochastic variations?**
  - The paper states the framework should be evaluated on the modeling of non-periodic time-varying effects.
  - The current modulation metric and adversarial training strategy are optimized for periodic LFO behavior; stochastic fluctuations lack the consistent patterns required for these mechanisms.
  - Successful application of the method to a tape echo simulation, measuring the accuracy of random pitch-wow variations without a periodic driving signal.

## Limitations
- The method's effectiveness for modulation frequencies below the Nyquist limit of the window size remains limited, with potential for modulation collapse beyond training windows during streamed inference.
- The SPN's reliance on frozen discriminator features assumes these capture sufficient modulation-phase information, but this transfer learning assumption lacks formal validation.
- Mode seeking regularization improves adversarial phase modulation learning but degrades final spectral accuracy, suggesting potential architectural inefficiencies in the conditioning mechanism.

## Confidence
- **High:** The two-phase training strategy successfully prevents phase-averaged collapse and enables black-box modeling without modulation signal extraction
- **Medium:** SPN-based synchronization improves fine-tuning convergence, particularly for faster modulation rates
- **Low:** Generalization to extreme parameter ranges (very slow/fast LFOs) and real-time streaming applications remains unproven

## Next Checks
1. Test streamed inference beyond training window duration to quantify modulation collapse onset for Slow-LFO settings
2. Compare SPN-frozen vs. trainable FeatBlocks to validate transfer learning assumption
3. Ablate mode seeking across multiple spectral loss weights to identify optimal tradeoff between modulation fidelity and spectral accuracy