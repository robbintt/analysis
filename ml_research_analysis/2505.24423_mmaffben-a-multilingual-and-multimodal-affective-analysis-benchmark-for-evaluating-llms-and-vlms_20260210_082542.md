---
ver: rpa2
title: 'MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating
  LLMs and VLMs'
arxiv_id: '2505.24423'
source_url: https://arxiv.org/abs/2505.24423
tags:
- emotion
- text
- video
- image
- affective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMAFFBen introduces the first comprehensive multilingual and multimodal
  affective analysis benchmark for evaluating large language and vision-language models
  across 35 languages and three modalities (text, image, video). The benchmark includes
  four key affective analysis tasks: sentiment polarity, sentiment intensity, emotion
  classification, and emotion intensity, covering 14 diverse datasets.'
---

# MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs

## Quick Facts
- **arXiv ID:** 2505.24423
- **Source URL:** https://arxiv.org/abs/2505.24423
- **Reference count:** 40
- **Primary result:** First comprehensive multilingual, multimodal affective analysis benchmark (35 languages, 3 modalities, 4 tasks) showing GPT-4o-mini leads zero-shot, while MMAFFLM-7B excels among fine-tuned models.

## Executive Summary
MMAFFBen introduces a pioneering benchmark designed to evaluate the affective understanding capabilities of large language and vision-language models (LLMs/VLMs) across 35 languages and three modalities (text, image, video). The benchmark includes four key affective analysis tasks—sentiment polarity, sentiment intensity, emotion classification, and emotion intensity—covering 14 diverse datasets. To support model development, the authors also construct MMAFFIn, an instruction-tuning dataset, and develop two specialized models (MMAFFLM-3B and MMAFFLM-7B) by fine-tuning Qwen2.5-VL on this data. Systematic evaluation of 20 representative models demonstrates that GPT-4o-mini achieves the best overall performance without task-specific fine-tuning, while MMAFFLM-7B excels among fine-tuned models, underscoring the effectiveness of instruction-tuning for multilingual, multimodal affective analysis. The benchmark and models are publicly available to foster reproducible progress in this domain.

## Method Summary
MMAFFBen evaluates LLM/VLM affective understanding using four tasks: sentiment polarity, sentiment intensity, emotion classification, and emotion intensity, across text, image, and video modalities. The benchmark uses MMAFFBen for evaluation and MMAFFIn for instruction-tuning, constructed from 14 existing datasets spanning 35 languages and reformatted into instruction-response pairs. Evaluation metrics include Pearson correlation coefficient for intensity tasks and macro F1 for classification tasks. The authors fine-tune Qwen2.5-VL-3B and -7B models on MMAFFIn using LLaMA-Factory with 1 epoch, batch size 256, and learning rate 5e-6, and assess both zero-shot and fine-tuned performance.

## Key Results
- GPT-4o-mini achieves the best overall performance in zero-shot evaluation, outperforming all fine-tuned open-source models.
- Among fine-tuned models, MMAFFLM-7B demonstrates the highest effectiveness, highlighting the benefits of instruction-tuning for affective analysis.
- The integration of video data during fine-tuning enhances performance on static image-based tasks, suggesting cross-modal benefits.
- MMAFFLM models exhibit weaker performance on sentiment intensity (regression) tasks compared to classification tasks.

## Why This Works (Mechanism)
MMAFFBen works by standardizing affective analysis across multiple languages and modalities, enabling systematic comparison of LLM/VLM capabilities. The benchmark's comprehensive scope (35 languages, 3 modalities, 4 tasks) ensures broad coverage of real-world affective understanding challenges. Instruction-tuning on MMAFFIn equips models with task-specific patterns, improving their zero-shot generalization. The inclusion of both classification and regression tasks (sentiment/emotion intensity) exposes nuanced model weaknesses, such as difficulties in continuous value prediction. Cross-modal training (image + video) appears to enrich representations, benefiting even unimodal tasks.

## Foundational Learning
- **Multilingual affective analysis:** Understanding sentiment and emotion across diverse languages is crucial for global NLP applications. *Quick check:* Verify dataset language coverage and translation consistency.
- **Multimodal fusion:** Combining text, image, and video modalities improves affective understanding by leveraging complementary cues. *Quick check:* Compare model performance with and without multimodal inputs.
- **Zero-shot vs. fine-tuned evaluation:** Zero-shot tests model generalization without task-specific adaptation; fine-tuning tailors models to affective tasks. *Quick check:* Evaluate models in both settings to quantify adaptation benefits.
- **Regression vs. classification in affective tasks:** Intensity prediction (regression) is more challenging than discrete class assignment (classification) due to continuous value requirements. *Quick check:* Analyze error distributions for intensity vs. classification tasks.
- **Instruction-tuning for affective tasks:** Reformulating datasets as instruction-response pairs helps models learn task formats, improving performance. *Quick check:* Compare performance before and after instruction-tuning.
- **Cross-modal learning benefits:** Training on multiple modalities can enhance performance even on single-modality tasks. *Quick check:* Ablate video data from training and re-evaluate image-only tasks.

## Architecture Onboarding

**Component Map:** Data Sources (14 datasets) -> MMAFFIn (instruction-tuning) -> MMAFFLM Models (3B, 7B) -> MMAFFBen (evaluation) -> Performance Metrics (pcc, ma-F1)

**Critical Path:** MMAFFIn construction (data reformatting) → MMAFFLM fine-tuning (LLaMA-Factory) → MMAFFBen evaluation (zero-shot and fine-tuned) → metric calculation and comparison

**Design Tradeoffs:** Zero-shot evaluation emphasizes generalization but may underutilize task-specific patterns; fine-tuning improves task performance but risks overfitting. Instruction-tuning balances both by teaching task formats.

**Failure Signatures:** Parsing errors in model outputs (especially for continuous intensity scores), inconsistent video preprocessing, and sampling variance in dataset splits can all undermine reproducibility.

**First Experiments:**
1. Parse model outputs on a small validation set to ensure consistent metric calculation.
2. Pre-extract and standardize video frames/transcriptions for all video datasets.
3. Run multiple evaluation seeds and report variance to assess stability.

## Open Questions the Paper Calls Out
- Does scaling open-source models beyond 13 billion parameters yield diminishing or accelerating returns specifically for affective analysis tasks compared to general tasks?
- Why does the integration of video data during training enhance performance on static image-based affective analysis tasks?
- What factors cause the MMAFFLM models to underperform specifically on sentiment intensity (SI) tasks compared to classification tasks?
- To what extent does demographic or cultural bias in the base model (e.g., QwenVL's "Eastern image data" bias) influence performance discrepancies across different cultural datasets?

## Limitations
- The benchmark relies on dataset-specific train/test splits and random sampling, which may introduce variance in reported metrics.
- Video modality evaluation is sensitive to preprocessing choices (frame sampling, audio extraction), which are not fully standardized.
- The instruction-tuning data (MMAFFIn) is assembled from 14 diverse datasets, but the exact training mixture and sampling strategy are underspecified.
- Performance gains from instruction-tuning and cross-modal training are not fully explained, with potential confounding factors.
- The authors only tested open-source models up to 13 billion parameters due to computational constraints, limiting scaling insights.

## Confidence
- **High confidence:** The benchmark's scope (four affective tasks, 35 languages, three modalities) and general experimental protocol (zero-shot and fine-tuned evaluation using standard metrics) are clearly described and can be reproduced.
- **Medium confidence:** The performance comparisons (e.g., GPT-4o-mini best zero-shot, MMAFFLM-7B best fine-tuned) are credible given the described methodology, but absolute scores may vary due to dataset sampling and parsing ambiguities.
- **Low confidence:** The reported improvements over baselines for video tasks and the exact impact of the instruction-tuning dataset are uncertain due to underspecified preprocessing and training details.

## Next Checks
1. **Parse and validate output formatting:** Test prompt templates on a small subset of each dataset, ensuring model outputs are consistently parsed into required formats (e.g., "Intensity Score: 0.5" vs. free-form text). Implement and verify robust regex or keyword-based parsers for all metric types.
2. **Standardize video preprocessing:** Pre-extract frames and/or transcriptions for all video datasets into a fixed, documented format. Use the same preprocessing pipeline for both model training and evaluation to ensure reproducible results.
3. **Verify dataset splits and sampling:** Confirm the exact train/test splits for each dataset (especially those not provided by the original authors). Run multiple evaluation seeds and report variance to assess stability and reproducibility of the reported benchmark results.