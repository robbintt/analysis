---
ver: rpa2
title: 'Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning'
arxiv_id: '2505.24726'
source_url: https://arxiv.org/abs/2505.24726
tags:
- self-reflection
- instruct
- training
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for improving large language
  models through self-reflection and reinforcement learning. The method incentivizes
  models to generate better self-reflections when they answer incorrectly, enabling
  enhancement of complex task performance even with only binary feedback.
---

# Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.24726
- Source URL: https://arxiv.org/abs/2505.24726
- Authors: Shelly Bensal; Umar Jamil; Christopher Bryant; Melisa Russak; Kiran Kamble; Dmytro Mozolevskyi; Muayad Ali; Waseem AlShikh
- Reference count: 11
- Key outcome: Novel self-improvement approach where LLMs generate better self-reflections when answering incorrectly, enabling enhancement of complex task performance even with only binary feedback

## Executive Summary
This paper introduces a novel approach for improving large language models through self-reflection and reinforcement learning. The method incentivizes models to generate better self-reflections when they answer incorrectly, enabling enhancement of complex task performance even with only binary feedback. The framework operates in two stages: upon failing a task, the model generates a self-reflective commentary, then retries the task with the self-reflection in context. If the retry succeeds, tokens from the self-reflection phase are rewarded using reinforcement learning.

Experimental results demonstrate substantial performance gains across model architectures, with improvements up to 34.7% in math equation writing and 18.1% in function calling. Notably, smaller fine-tuned models (1.5-7 billion parameters) outperformed models in the same family that were 10 times larger. The method shows minimal catastrophic forgetting on diverse benchmarks and represents an exciting pathway to more reliable language models that can self-improve on challenging tasks with limited external feedback.

## Method Summary
The approach trains models to generate better self-reflections through a two-stage process: (1) if an initial response fails a task, generate a self-reflection explaining what went wrong, and (2) retry the task with the self-reflection in context. If the retry succeeds, only the tokens generated during the self-reflection phase are rewarded using Group Relative Policy Optimization (GRPO). This selective credit assignment ensures the model learns general self-reflection skills rather than task-specific responses. The method uses a binary verifier to provide success/failure feedback and trains only on initially failed examples to improve sample efficiency. The implementation extends TRL's GRPOTrainer to handle multi-step reasoning with custom masking for token-level reward assignment.

## Key Results
- Up to 34.7% improvement in math equation writing and 18.1% improvement in function calling accuracy
- 1.5-7B parameter models outperformed models in the same family that were 10x larger
- Minimal catastrophic forgetting on diverse benchmarks (MMLU-Pro, GSM8K, HellaSwag, MATH)
- Llama-3.1-8B required only 100 training steps and utilized less than 2,000 unique queries

## Why This Works (Mechanism)

### Mechanism 1: Selective Credit Assignment via Token Masking
Rewarding only self-reflection tokens (not answer tokens) induces task-agnostic reasoning improvements. The method uses GRPO with a token mask that sets advantage terms to zero for all tokens except those in the self-reflection phase, creating a sparse reward signal targeting the meta-cognitive process rather than task-specific outputs.

### Mechanism 2: Binary Verifier as Sparse Reward Converter
A simple success/failure verifier suffices to train self-improvement when combined with retry dynamics. External validators produce binary signals, and the retry structure densifies this: a failed attempt → reflection → successful retry yields positive reward for reflection tokens.

### Mechanism 3: Dataset of Failures for Sample Efficiency
Training only on initially failed examples accelerates convergence without performance loss. Pre-sampling multiple responses per query and retaining only failure cases concentrates learning signal on examples where self-reflection could plausibly help.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO estimates advantages from grouped samples without a critic network, suited for sparse outcome rewards
  - Quick check question: Can you explain why GRPO dispenses with a value network compared to PPO?

- **Self-Reflection / Introspection in LLMs**
  - Why needed here: Understanding that reflection prompts can surface reasoning errors, but effectiveness varies by task difficulty and verification availability
  - Quick check question: Under what conditions does self-reflection typically fail (per Section 2.1)?

- **Catastrophic Forgetting**
  - Why needed here: Fine-tuning risks degrading general capabilities; the paper claims minimal forgetting due to task-agnostic training
  - Quick check question: What benchmark suite did the authors use to assess forgetting (per Section 5.2)?

## Architecture Onboarding

- **Component map:** Task prompt + first attempt → model generates initial response → binary validator → self-reflection prompt (if failed) → model generates critique → retry prompt (with reflection in context) → model makes second attempt → GRPO update (if retry succeeds) → reward only reflection tokens via masked advantage → dataset of failures → pre-filtered training samples

- **Critical path:** Validator correctness → reflection quality → retry success → reward assignment. If any step fails, no learning signal propagates.

- **Design tradeoffs:**
  - Verifier complexity vs. generality: Task-specific validators enable training but limit transfer; ground-truth labels increase data requirements
  - Sample efficiency vs. coverage: Training only on failures speeds convergence but may miss edge cases
  - Model scale vs. capacity: Paper notes sub-1.5B models struggled with both task accuracy and reflection capacity

- **Failure signatures:**
  - Models generate verbose, redundant reflections
  - No improvement after training: check base competency on task
  - High forgetting: verify KL divergence coefficient (0.001 used) and learning rate (5e-7)

- **First 3 experiments:**
  1. Validate verifier on held-out samples: Confirm binary signals align with ground truth before training
  2. Pilot on small failure dataset (500-1000 samples): Check reflection quality and retry success rate manually
  3. Ablate token masking: Compare rewarding all tokens vs. only reflection tokens to verify selective credit matters

## Open Questions the Paper Calls Out

- When is it more beneficial for models to generate concise versus verbose self-reflections?
- Does self-reflection training generalize across different tasks?
- What is the minimum baseline capability required for self-reflection training to succeed?
- Why does self-reflection training improve first-attempt performance even without explicit reflection?

## Limitations

- Effectiveness critically depends on task verifiability, limiting applicability to subjective or open-ended domains
- Reflection quality improvement may be constrained by model capacity - sub-1.5B models struggle with both task performance and generating useful self-reflections
- Training assumes initial task failure is recoverable through reflection, which may not hold for tasks requiring domain knowledge the model fundamentally lacks

## Confidence

- **High confidence:** The core mechanism of selective token masking (rewarding only reflection tokens) is well-supported by the described GRPO implementation and ablation studies
- **Medium confidence:** Claims about catastrophic forgetting are supported by reported results but limited by evaluation on only four benchmarks
- **Low confidence:** The generalizability of self-reflection improvements across arbitrary tasks remains speculative

## Next Checks

1. **Transfer validation:** Test the trained self-reflection capability on completely unseen task domains (e.g., code debugging, creative writing) to assess genuine reasoning improvement versus task-specific memorization

2. **Verifier robustness test:** Systematically introduce ambiguous or borderline cases where validator decisions are uncertain, measuring how often incorrect reward signals propagate through the training pipeline

3. **Long-term retention study:** Evaluate model performance and reflection quality after 1-2 months of storage, checking for gradual degradation that might indicate the improvements are less stable than reported