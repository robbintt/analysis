---
ver: rpa2
title: 'Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets:
  LiveRAG Challenge'
arxiv_id: '2506.22644'
source_url: https://arxiv.org/abs/2506.22644
tags:
- retrieval
- question
- hybrid
- questions
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LiveRAG Challenge 2025 evaluated retrieval-augmented generation
  (RAG) systems on dynamic test sets using the FineWeb-10BT corpus. This paper presents
  a hybrid approach combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct
  for answer generation.
---

# Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge

## Quick Facts
- arXiv ID: 2506.22644
- Source URL: https://arxiv.org/abs/2506.22644
- Reference count: 17
- Primary result: 4th place in faithfulness and 11th place in correctness among 25 teams in LiveRAG Challenge 2025

## Executive Summary
This paper presents a hybrid RAG system evaluated in the LiveRAG Challenge 2025, combining sparse (BM25) and dense (E5) retrieval with Falcon3-10B-Instruct for answer generation. The system achieved strong performance rankings while revealing important tradeoffs between development optimization and real-world robustness. Key findings include substantial MAP improvements from neural re-ranking (52% relative gain) at prohibitive computational costs, DSPy-optimized prompting increasing semantic similarity but eliminating refusal behaviors, and vocabulary alignment emerging as the strongest predictor of RAG performance.

## Method Summary
The system uses hybrid retrieval combining BM25 (OpenSearch) and dense (E5-base-v2 in Pinecone) methods, retrieving 30 documents each with normalized score fusion to select top 10. Documents are chunked into 512-token segments using a sentence-aware splitter. Answer generation employs Falcon3-10B-Instruct with temperature 0.6, top_p 0.9, and 200-token limit. The architecture optionally includes RankLLaMA-7B pointwise re-ranking (84s/query) and DSPy-optimized prompts with few-shot/CoT examples. The system was evaluated on FineWeb-10BT corpus using 200 synthetic development questions and 500 unseen test questions, measuring both retrieval metrics (MAP, nDCG, Recall) and generation quality (ROUGE, BLEU, cosine similarity, refusal rate).

## Key Results
- Achieved 4th place in faithfulness and 11th place in correctness among 25 teams
- Neural re-ranking improved MAP from 0.523 to 0.797 (52% relative improvement) but introduced 48x latency increase (84s vs 1.74s per question)
- DSPy-optimized prompting increased semantic similarity from 0.668 to 0.771 but showed 0% refusal rates, raising over-confidence concerns
- Vocabulary alignment was strongest performance predictor, with document-similar phrasing improving cosine similarity from 0.562 to 0.762

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Retrieval Fusion
Combining sparse (BM25) and dense (E5) retrieval creates a more robust foundation for downstream generation than either method alone. Sparse retrieval handles exact term matching for specific factual queries while dense retrieval captures semantic similarity. Normalized score fusion selects top documents from both streams, providing complementary signals for re-ranking or generation.

### Mechanism 2: Neural Re-ranking Quality Improvement
Generative re-ranking provides substantial retrieval quality gains but at computational costs that may be prohibitive for production time constraints. RankLLaMA performs pointwise scoring where each passage is independently evaluated against the query using a generative model, providing more nuanced relevance assessment than lexical or embedding similarity alone.

### Mechanism 3: Vocabulary Alignment Effect
Query-document vocabulary alignment is the strongest predictor of RAG performance in this system. When query terminology matches document terminology, both sparse (exact term matching) and dense (embedding similarity) retrieval benefit simultaneously, with compounding effects on downstream generation quality.

## Foundational Learning

- **Sparse vs Dense Retrieval Characteristics**: Understanding when BM25 excels (exact term matching) versus when E5 excels (semantic similarity) is necessary to diagnose retrieval failures. Quick check: Given a paraphrased query with no overlapping keywords, which retrieval method would you expect to perform better and why?

- **Refusal Rate as Calibration Signal**: DSPy-optimized prompts achieved 0% refusal rates, which was interpreted as over-confidence rather than perfect calibration—a critical distinction for trustworthy RAG. Quick check: If two systems achieve identical semantic similarity scores but one has 0% refusal and the other has 15% refusal, which would you deploy and why?

- **Pointwise vs Listwise Re-ranking**: The paper uses pointwise re-ranking (each passage scored independently); understanding this distinction is necessary for selecting appropriate re-ranking strategies. Quick check: Why might pointwise re-ranking be more robust than listwise re-ranking, and what efficiency tradeoff does it introduce?

## Architecture Onboarding

- **Component map**: Query → Parallel Sparse/Dense Retrieval → Score Fusion → (Optional: Re-ranking) → Context Assembly → Answer Generation
- **Critical path**: Query enters parallel BM25 and E5 retrieval streams, results are fused and optionally re-ranked, then passed to Falcon3-10B-Instruct for answer generation
- **Design tradeoffs**: Re-ranking provides +52% MAP but 48x latency increase; DSPy prompting improves semantic similarity by 15% but eliminates refusal behavior; hybrid vs sparse-only shows minimal MAP difference but potentially better downstream robustness
- **Failure signatures**: RankLLaMA too slow (~84s/question); DSPy prompts give 0% refusal (over-confidence); hybrid shows minimal gain over sparse alone; doc2query enhancement underperformed baseline
- **First 3 experiments**:
  1. Establish retrieval baselines: Compare sparse-only, dense-only, and hybrid on held-out set (n=50+) measuring MAP, Recall@10, and downstream generation metrics
  2. Latency budget analysis for re-ranking: Measure RankLLaMA per-question latency at varying batch sizes
  3. Vocabulary sensitivity test: Create matched question pairs differing only in document-similar vs document-distant phrasing

## Open Questions the Paper Calls Out

### Open Question 1
How can the efficiency of generative re-rankers like RankLLaMA be improved to make them computationally feasible for large-scale, time-constrained RAG applications? The paper explicitly encourages future work on strategies to increase efficiency of generative re-rankers, as the 84s/query cost was prohibitive for the competition.

### Open Question 2
Can adaptive retrieval strategies successfully adjust computational overhead based on real-time question complexity analysis? The authors suggest future work on adaptive retrieval strategies that adjust computational overhead based on question complexity, as performance varied significantly across different question types.

### Open Question 3
How can automated prompt optimization frameworks like DSPy be regularized to prevent over-confidence and maintain necessary refusal behaviors? The paper notes that DSPy-optimized prompts achieved 0% refusal rates, raising concerns about over-confidence and generalizability, resulting in the authors rejecting the optimized prompts for submission.

## Limitations
- Findings limited to FineWeb-10BT corpus and specific question distribution, may not generalize to other domains
- 52% MAP improvement from re-ranking comes at prohibitive computational cost (84s vs 1.74s per question)
- DSPy optimization achieving 0% refusal rates while maintaining high semantic similarity suggests potential over-confidence rather than perfect calibration

## Confidence

- **High Confidence**: Hybrid retrieval combining BM25 and E5 provides complementary coverage; computational cost-benefit analysis of re-ranking is empirically validated; vocabulary alignment correlates strongly with performance metrics
- **Medium Confidence**: The mechanism by which DSPy-optimized prompts eliminate refusals without degrading semantic similarity; the extent to which corpus-specific findings generalize to other domains
- **Low Confidence**: That vocabulary alignment represents a causal mechanism rather than correlation artifact; that DSPy's 0% refusal rates indicate over-confidence rather than optimal calibration

## Next Checks

1. **Domain Transfer Test**: Replicate the hybrid retrieval architecture on a different corpus (e.g., biomedical or legal) to validate whether the 52% MAP improvement from re-ranking holds across domains

2. **Calibration Assessment**: Design experiments to distinguish between optimal calibration (correct 0% refusal) and over-confidence (inappropriate 0% refusal) by measuring error rates on questions with known incorrect answers

3. **Vocabulary Alignment Causality**: Conduct controlled experiments varying query phrasing while holding document content constant to determine whether vocabulary alignment causally improves retrieval performance or merely correlates with other factors