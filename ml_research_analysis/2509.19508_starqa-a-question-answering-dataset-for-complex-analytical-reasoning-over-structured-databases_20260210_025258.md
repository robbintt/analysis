---
ver: rpa2
title: 'STARQA: A Question Answering Dataset for Complex Analytical Reasoning over
  Structured Databases'
arxiv_id: '2509.19508'
source_url: https://arxiv.org/abs/2509.19508
tags:
- player
- integer
- none
- text
- home
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STARQA is a new dataset of complex analytical reasoning questions
  and answers over structured databases, focusing on tasks that require calculations,
  time-series analysis, and scenario understanding. To tackle such questions, the
  authors propose TEXT2SQLCODE, a method that decomposes the task into SQL for data
  fetching and Python for complex reasoning.
---

# STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases

## Quick Facts
- arXiv ID: 2509.19508
- Source URL: https://arxiv.org/abs/2509.19508
- Reference count: 40
- Primary result: TEXT2SQLCODE improves performance over SQL alone by 4–15%, especially when selectively invoked for difficult questions

## Executive Summary
STARQA is a new dataset of complex analytical reasoning questions and answers over structured databases, focusing on tasks that require calculations, time-series analysis, and scenario understanding. To tackle such questions, the authors propose TEXT2SQLCODE, a method that decomposes the task into SQL for data fetching and Python for complex reasoning. Experiments show STARQA is challenging for state-of-the-art LLMs, with best models achieving only about 48% accuracy. However, TEXT2SQLCODE improves performance over SQL alone by 4–15%, especially when selectively invoked for difficult questions. The results highlight the value of combining SQL and Python and demonstrate the unique value of structured data over parametric knowledge or web search for this type of reasoning.

## Method Summary
STARQA presents a dataset of 362 complex analytical reasoning questions over three structured databases (IMDb, EuroSoccer, Olist). The proposed TEXT2SQLCODE method decomposes questions into SQL for data retrieval and Python for procedural logic, with a hybrid approach using self-consistency voting to selectively trigger Python decomposition for difficult questions. The evaluation uses execution accuracy, comparing the output tuples from generated code against gold answers. Experiments compare Text2SQL baselines with various TEXT2SQLCODE configurations (Single, Multi, Hybrid) across multiple LLM models.

## Key Results
- Best models achieve only ~48% accuracy on STARQA, demonstrating the dataset's challenge level
- TEXT2SQLCODE improves accuracy over SQL alone by 4-15%, particularly for difficult questions
- Hybrid routing using self-consistency as a proxy for difficulty shows selective invocation benefits
- Performance varies by reasoning category, with some categories showing larger gains from decomposition

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decomposition via Tool Specialization
Decomposing complex analytical questions into SQL for data retrieval and Python for procedural logic appears to improve accuracy over monolithic SQL generation, particularly for operations where SQL expressiveness is limited. SQL is optimized for relational algebra while Python is optimized for procedural logic, avoiding brittle, overly complex SQL queries prone to runtime errors.

### Mechanism 2: Uncertainty-Based Hybrid Routing
Using self-consistency on initial SQL outputs as a proxy for difficulty allows the system to selectively trigger the more expensive Python pipeline, mitigating the risk of "over-decomposing" simple questions. The system generates 3 SQL answers and escalates to Python only when outputs disagree.

### Mechanism 3: Isolation of Reasoning vs. Retrieval
Normalizing entity mentions and providing rich schema context in the prompt isolates "analytical reasoning" capabilities from "entity linking" challenges, allowing for a cleaner measurement of logic. This removes the noise of "searching" for the right column, focusing the model's capacity purely on query construction and logic application.

## Foundational Learning

- **Relational vs. Procedural Semantics**: Understanding why SQL operates on sets while Python operates on sequences is crucial for grasping the architectural split. Quick check: Can you explain why calculating a "Spearman correlation" is natively supported by Python libraries but requires complex statistical extensions in standard SQL?

- **Self-Consistency as Confidence Estimation**: The Hybrid approach depends on majority voting to gauge the model's certainty. Quick check: If an LLM outputs the same wrong answer 3 times (High Consistency, Low Accuracy), will the Hybrid approach trigger the Python fallback? (Answer: No).

- **Context Window Management for Schema**: The prompts include full schemas, data types, sample rows, and categorical value lists. Quick check: What specific metadata components does the prompt require beyond just the "CREATE TABLE" statements? (Answer: Column descriptions, sample rows, categorical value lists).

## Architecture Onboarding

- **Component map**: Input (Question + DB Schema/Metadata) -> Router (Hybrid) -> Decomposer -> SQL Executor -> Python Executor -> Evaluator

- **Critical path**: 1. Prompting: Assemble rich context into prompt 2. Routing: Run 3-shot SQL generation; if consistent -> End, if not -> Decomposition 3. Execution: Generate SQL -> Fetch DF -> Generate Python -> Run Python -> Output Tuples

- **Design tradeoffs**: Single vs. Multi-Call (lower latency vs. better modularity), Normalized vs. Raw Data (controlled environment vs. real-world complexity)

- **Failure signatures**: SQL syntax/runtime errors, context loss in multi-step decomposition, format violations in Python output

- **First 3 experiments**: 1. Baseline Run: Standard Text2SQL on IMDb subset 2. Ablation on Routing: Force Python vs. consistency-based routing 3. Error Analysis by Category: Manual inspection of "Nested Queries" failures

## Open Questions the Paper Calls Out

None

## Limitations

- Reliance on self-consistency as a proxy for reasoning difficulty may fail when models consistently hallucinate the same incorrect SQL query
- Controlled dataset environment with normalized entities and rich schema context may not represent real-world "noisy" schemas
- Performance degradation is likely when orthogonal challenges like entity linking must be handled by the model

## Confidence

- **High Confidence**: Combining SQL and Python improves accuracy over SQL alone is well-supported by evidence
- **Medium Confidence**: Effectiveness of uncertainty-based hybrid routing depends on assumption about self-consistency correlating with reasoning difficulty
- **Medium Confidence**: Dataset isolates reasoning capabilities from entity linking challenges, but real-world degradation is not empirically tested

## Next Checks

1. **Self-Consistency Validation**: Run ablation comparing consistency-based routing versus always using Text2SQLCode to determine if routing actually improves performance

2. **Schema Noise Introduction**: Systematically remove schema descriptions or normalize entities inconsistently to measure accuracy degradation compared to clean dataset

3. **Consistency vs. Accuracy Analysis**: Calculate correlation coefficient between SQL consistency and SQL accuracy across all questions to empirically validate routing assumption