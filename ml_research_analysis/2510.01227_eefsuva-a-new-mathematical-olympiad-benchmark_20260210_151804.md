---
ver: rpa2
title: 'EEFSUVA: A New Mathematical Olympiad Benchmark'
arxiv_id: '2510.01227'
source_url: https://arxiv.org/abs/2510.01227
tags:
- problems
- problem
- mathematical
- these
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce EEFSUVA, a novel benchmark of 39 mathematics
  Olympiad problems drawn from under-circulated Eastern European and former Soviet
  Union competitions and six problems from V. I.
---

# EEFSUVA: A New Mathematical Olympiad Benchmark

## Quick Facts
- arXiv ID: 2510.01227
- Source URL: https://arxiv.org/abs/2510.01227
- Reference count: 34
- Key outcome: GPT-5 Thinking scores 35.89% on EEFSUVA vs. 0% for Gemini 2.5 Pro, suggesting current models struggle with less familiar, reasoning-heavy problem types

## Executive Summary
The authors introduce EEFSUVA, a novel benchmark of 39 mathematics Olympiad problems drawn from under-circulated Eastern European and former Soviet Union competitions and six problems from V. I. Arnold's works. The problems emphasize nonstandard reasoning and have numerical answers to enable unambiguous evaluation. EEFSUVA was designed to address potential data contamination and narrow focus in existing benchmarks, which often rely on well-known IMO-style problems. Experiments with state-of-the-art LLMs show GPT-5 Thinking achieves 35.89% accuracy overall on EEFSUVA, while Gemini 2.5 Pro scores 0%. This performance drop relative to existing benchmarks suggests current models struggle with less familiar, reasoning-heavy problem types, highlighting the need for broader evaluation datasets to more fully assess mathematical reasoning and guide model development.

## Method Summary
The benchmark consists of 39 problems sourced from Eastern European/former Soviet Union Olympiads and Arnold's texts, filtered to include only those with numerical answers. Each problem was evaluated twice per model in fresh chat sessions, with correct answers counted if either attempt yielded the ground truth. The evaluation protocol used a simple binary "yes/no" scoring based on numerical match. The authors prioritized combinatorics and number theory problems with nonstandard constraints to probe flexible reasoning rather than algorithmic execution.

## Key Results
- GPT-5 Thinking achieves 35.89% accuracy on EEFSUVA, while Gemini 2.5 Pro scores 0%
- Models frequently fail by recognizing superficial similarities to known problems and forcing known solution structures onto new problems
- The benchmark reveals a reasoning gap in current LLMs when confronted with less familiar, structurally novel problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sourcing problems from under-circulated regional competitions may reduce the efficacy of retrieval-based shortcuts.
- Mechanism: By selecting problems from Eastern European and former Soviet Union Olympiads rather than widely-used datasets like the IMO or USAMO, the benchmark minimizes the probability that models rely on memorized solutions or "cached" patterns from pre-training data. This forces the model to engage in de novo reasoning.
- Core assumption: The selected problems are sufficiently absent from the training corpora of major LLMs, despite being 5+ years old.
- Evidence anchors: [abstract] "problems are far less prevalent in online corpora." [section 1] "problems, often less widely circulated than their Western counterparts, proved exceptionally effective... their relative obscurity minimizes the risk of LLM pre-training exposure." [corpus] Related work (RIMO, AMO-Bench) similarly seeks contamination-resistant datasets, suggesting a consensus on the necessity of this mechanism.
- Break condition: If analysis of training logs reveals these specific regional problems were scraped into common datasets (e.g., Common Crawl) extensively.

### Mechanism 2
- Claim: Prioritizing combinatorics and number theory with "nonstandard" constraints probes for flexible reasoning rather than algorithmic execution.
- Mechanism: The benchmark emphasizes problems that lack standard algorithmic solutions (e.g., "frustrate standard approaches"). The paper posits that LLMs excel at pattern matching familiar theorems but struggle when a problem requires "inventing new structures" or multistep logical dependencies without clear initial equations.
- Core assumption: LLMs have not yet generalized "creative" mathematical construction (inventing structures) from their training on proof-based texts.
- Evidence anchors: [abstract] "demanding nonstandard problem-solving techniques." [section 3.3] "LLMs frequently generate hallucinated intermediate steps... in Olympiad combinatorics, the structure must often be created on the spot, not retrieved." [corpus] Weak corpus evidence for this specific "invention" mechanism; related papers focus on verification rather than the specific cognitive load of "structure invention."
- Break condition: If models succeed by applying a general heuristic not identified by the authors, rather than true "invention."

### Mechanism 3
- Claim: Numerical-answer formatting provides a high-signal evaluation of reasoning chains without subjective grading.
- Mechanism: By filtering for problems with discrete numerical solutions, the benchmark removes the ambiguity and computational cost of verifying proof-based answers (which often require human judgment or complex theorem provers). This creates a binary success signal (correct number vs. incorrect number) for complex internal reasoning.
- Core assumption: A correct numerical answer implies a valid reasoning path (i.e., the model didn't guess or use a flawed method that coincidentally yielded the result).
- Evidence anchors: [section 3.1] "We focused exclusively on problems with numerical solutions... avoiding proof-based problems [which] require additional steps for verification." [section 4] "Evaluation... was a simple 'yes' or 'no' that could be quickly confirmed."
- Break condition: If models frequently guess the correct integer (low probability but possible in small ranges) or derive it via invalid logic.

## Foundational Learning

- Concept: **Data Contamination / Memorization**
  - Why needed here: The paper's central thesis is that high scores on standard benchmarks (like IMO) are suspect because the test data leaked into training.
  - Quick check question: Can you distinguish between a model solving a problem via reasoning vs. retrieving a solution it saw during pre-training?

- Concept: **Pattern Matching vs. Reasoning**
  - Why needed here: The paper argues LLMs rely on "surface-level pattern recognition" (anchoring) rather than logical derivation, leading to failure when problem structures change slightly.
  - Quick check question: How does a model react when presented with a problem that *looks* like a famous theorem but has a modified condition?

- Concept: **Automated Verification**
  - Why needed here: The benchmark relies on numerical answers to automate evaluation; understanding the trade-off between problem richness (proofs) and eval speed (numbers) is key.
  - Quick check question: What types of mathematical depth are lost when restricting a benchmark to numerical answers only?

## Architecture Onboarding

- Component map: Source Curation: Eastern European/Former Soviet Olympiads + Arnold Texts -> Filter: Must have numerical answer -> Verification: Human-solved & cross-checked against source solutions -> Evaluation: LLM Query (2 runs per problem) -> Scoring: Numerical match

- Critical path: The **Problem Curation** phase. The authors note that finding high-quality numerical problems in these specific regional archives was the bottleneck, often yielding only 1-2 viable problems per competition.

- Design tradeoffs:
  - **Breadth vs. Verifiability:** The authors excluded Geometry and proof-based problems entirely to ensure unambiguous grading. This sacrifices coverage of "visual" or "formal logic" reasoning for the sake of scalable, objective evaluation.
  - **Recency vs. Obscurity:** The authors use older problems (5+ years) which *could* be in training data, betting that their obscurity protects them better than recentness.

- Failure signatures:
  - **Anchoring/Hallucination:** The paper documents models noticing a similarity to a known problem (e.g., IMO Shortlist) and forcing the known solution structure onto the new problem, resulting in confident but incorrect answers (see Section 4 analysis of Gemini reasoning).
  - **Zero-shot reasoning gap:** A complete failure to initiate a solution strategy (Gemini's 0% on specific categories).

- First 3 experiments:
  1.  **Establish Baseline:** Run GPT-5 and Gemini 2.5 Pro on the 45-problem EEFSUVA set using the exact prompting strategy (fresh session per problem) to replicate the 35.89% and 0% scores.
  2.  **Contamination Check:** Search for the specific EEFSUVA problem strings in the training data logs of the target models (if accessible) or via web search frequency analysis to validate the "under-circulated" hypothesis.
  3.  **Ablation on Anchoring:** Modify the "anchoring" problems (identified in Section 4) to remove superficial similarities to IMO problems and measure if model accuracy improves (testing if pattern recognition was indeed the failure cause).

## Open Questions the Paper Calls Out

- Question: When models fail on problems with available online solutions, is the failure due to lack of exposure, or failure to retrieve/access existing solutions during inference?
  - Basis in paper: [explicit] "Is the model unable to solve the problem generally due to a lack of exposure, or is the model not noticing that the question and some proposed solution exist on the open web?"
  - Why unresolved: The paper observes models failing on problems with available solutions but cannot distinguish between these two failure modes.
  - What evidence would resolve it: Systematic comparison of model performance on problems with and without verifiable web access during inference, controlling for training data presence.

- Question: Does excluding proof-based problems from EEFSUVA miss important aspects of mathematical reasoning that could reveal different model capabilities?
  - Basis in paper: [inferred] The paper explicitly restricted to numerical problems to avoid subjectivity and enable automatic evaluation, acknowledging proof-based problems require different verification approaches.
  - Why unresolved: Proof-based problems may test reasoning in ways numerical answers cannot capture, but were excluded for practical evaluation reasons.
  - What evidence would resolve it: Developing automated theorem verification for EEFSUVA-style problems and comparing model performance on numerical vs. proof variants.

- Question: Can models be trained or prompted to recognize when superficially similar problems require substantially different solution approaches?
  - Basis in paper: [explicit] The paper documents models anchoring on familiar problem variants and failing to adapt reasoning when details diverge: "the model recognizes a problem type but, biased by training on the familiar variant, cannot adapt its predetermined reasoning."
  - Why unresolved: This brittleness suggests fundamental limitations in how models generalize from training examples.
  - What evidence would resolve it: Ablation studies measuring model accuracy on systematically varied problem perturbations that preserve structure but alter key constraints.

## Limitations

- Dataset Transparency: The specific problem statements are not publicly available, making independent verification challenging.
- Model Version Ambiguity: References to "GPT-5 Thinking" may not be publicly available or clearly defined, limiting reproducibility.
- Generalizability: The sample size (39 problems) is relatively small and heavily weighted toward combinatorics and number theory.

## Confidence

**High Confidence:**
- Using under-circulated problems to reduce memorization is logically sound
- Numerical-answer format provides unambiguous evaluation metric

**Medium Confidence:**
- Claim that LLMs score significantly lower on EEFSUVA than standard benchmarks
- Analysis of model failures (anchoring/hallucination) is plausible

**Low Confidence:**
- Central thesis about contamination-free evaluation relies on unverifiable assumptions
- Specific mechanisms of failure are inferred rather than directly tested

## Next Checks

1. **Dataset Availability and Verification:** Obtain the complete EEFSUVA problem set from authors or reconstruct it from cited sources, then independently verify under-circulation by searching web corpora and training datasets.

2. **Replication with Public Models:** Replicate benchmark evaluation using only publicly available LLM versions (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) to test consistency across model families.

3. **Contamination Analysis:** For a subset of EEFSUVA problems, perform detailed search to determine presence in Common Crawl, ArXiv, and other major web datasets to empirically test the "under-circulated" hypothesis.