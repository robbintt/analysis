---
ver: rpa2
title: 'Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate
  via Preferential Rationale Tuning'
arxiv_id: '2506.02519'
source_url: https://arxiv.org/abs/2506.02519
tags:
- rationale
- collate
- rationales
- providers
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving small-scale language
  models' reasoning capabilities without relying on large external models due to legal
  and cost constraints. The proposed method, COLLATE, trains small LLMs to generate
  diverse rationales and selectively choose the most useful ones for downstream tasks
  via preference optimization.
---

# Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning

## Quick Facts
- arXiv ID: 2506.02519
- Source URL: https://arxiv.org/abs/2506.02519
- Reference count: 35
- Small-scale LLMs (1B-8B) improve reasoning via diverse rationale generation and selection without external large models

## Executive Summary
This paper addresses the challenge of improving small-scale language models' reasoning capabilities without relying on large external models due to legal and cost constraints. The proposed method, COLLATE, trains small LLMs to generate diverse rationales and selectively choose the most useful ones for downstream tasks via preference optimization. It achieves this by creating multiple instances of the same model, generating varied rationales, and tuning them using likelihood-based selection. COLLATE outperforms several baselines on 5 datasets across 3 domains, achieving up to 7% gains, and works effectively across model scales from 1B to 8B parameters. Human studies confirm the rationales are reliable and diverse.

## Method Summary
The COLLATE framework trains small language models to collaborate by generating and selecting high-quality rationales. It begins by fine-tuning a base model on multi-mode instruction following (answer generation, rationale generation, and answer generation conditioned on rationales). Multiple instances of this model are then created and fine-tuned on separate data splits via direct preference optimization, where ground-truth rationales serve as preferred outputs. For downstream tasks, each rationale provider generates candidate rationales, which are ranked by their likelihood of producing correct answers. The top and bottom rationales form preference pairs for further DPO training, with filtration to retain only samples where rationales improve answer likelihood over instructions alone.

## Key Results
- Outperforms baselines (SPIN, ICL, Ape, GLaM, SOTA) on 5 datasets across 3 domains
- Achieves up to 7% accuracy gains over baselines
- Works effectively across model scales from 1B to 8B parameters
- Human studies show rationales are reliable (84% alignment with likelihood-based selection) and diverse (FMR=0.51)

## Why This Works (Mechanism)

### Mechanism 1: Diversity via Distinct Rationale Providers
- Claim: Training multiple instances of the same LLM on different data splits produces sufficiently diverse rationales to enable meaningful preference learning.
- Mechanism: Clone M_IFT S times → train each RP_s on split D_s via DPO (GT rationale as preferred over generated) → RPs develop distinct reasoning patterns → diverse candidate pool for downstream selection.
- Core assumption: Small perturbations in training data distribution produce meaningfully different reasoning behaviors rather than just stylistic noise.
- Evidence anchors:
  - [abstract]: "enforces multiple instances of the same LLM to exhibit distinct behavior and employs them to generate rationales to obtain diverse outputs"
  - [section 4.4, Table 4]: Removing distinct RPs drops GSM8K from 53.48 → 38.22
  - [section K]: "78.67% cases, one rationale is better than the other" confirms meaningful diversity
- Break condition: If RPs converge to near-identical outputs, winner/loser pairs become indistinguishable → DPO gradient signal collapses.

### Mechanism 2: Ground-Truth Likelihood as Rationale Quality Signal
- Claim: The likelihood of generating the correct answer conditioned on a rationale correlates with rationale usefulness and substitutes for external judges or human annotations.
- Mechanism: For each rationale r_s, compute l_s = π_IFT(A_GT | instruction + r_s) → rank by l_s → winner = argmax, eliminated = argmin → construct DPO pair.
- Core assumption: Higher GT likelihood indicates the rationale contains task-relevant reasoning, not merely surface features that happen to correlate with correct answers.
- Evidence anchors:
  - [section 3.3, Eq. 7]: Explicit ranking formulation using likelihood scores
  - [section 4.4, Table 4]: Replacing likelihood with LLM-as-a-judge drops GSM8K from 53.48 → 47.16
  - [section K, Table 13]: "84% cases, better rationale judged by humans aligns with likelihood-based selection"
- Break condition: If rationales can boost GT likelihood via spurious shortcuts (e.g., leaking answer patterns), selection rewards exploitation over genuine reasoning.

### Mechanism 3: Comparative Likelihood Gain Filtration
- Claim: Filtering to samples where the winning rationale improves GT likelihood over the instruction alone prevents degenerate preference pairs.
- Mechanism: Retain sample only if π_IFT(A_GT | instruction + r_winner) > π_IFT(A_GT | instruction alone); discard otherwise.
- Core assumption: Rationales that fail to improve answer likelihood provide negative or neutral training signal and should be excluded.
- Evidence anchors:
  - [section 3.3, Eq. 9]: Explicit filtration criterion with notation
  - [section 4.4, Table 4]: Removing filtration drops GSM8K from 53.48 → 42.11
- Break condition: Over-aggressive filtration reduces training data below critical mass; overly permissive filtration introduces noisy gradients that destabilize optimization.

## Foundational Learning

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed: COLLATE uses DPO to align rationale generation with task performance without training a separate reward model or using RL.
  - Quick check question: Can you write the DPO loss function and explain what π_ref represents?

- Concept: **Chain-of-Thought (CoT) Rationales**
  - Why needed: The framework presupposes that intermediate reasoning steps exist and can be generated, selected, and refined to improve downstream answers.
  - Quick check question: Why does conditioning answer generation on a rationale typically outperform direct answer generation for complex tasks?

- Concept: **Multi-task Instruction Fine-Tuning**
  - Why needed: M_IFT must switch between three modes (I→A, I→R, [I;R]→A) using prompt formatting, requiring flexible instruction-following capability.
  - Quick check question: How would you format a training sample to teach a model the [I;R]→A mode?

## Architecture Onboarding

- Component map: Base LLM (M) → Multi-mode IFT on CoT-Collection (140k) + Dolly (40k) → M_IFT → Clone S times → RP_s fine-tuned on separate splits of D_rationale (195k) via DPO → Task dataset D_T → Each RP generates rationale → M_IFT scores each via GT likelihood → Rank → Filter by Eq. 9 → DPO update RPs iteratively → Final inference: Select single best RP via validation likelihood, generate one rationale per query

- Critical path:
  1. Multi-mode IFT (enables all downstream operations; failure here cascades)
  2. RP initialization with distinct splits (establishes diversity foundation)
  3. Iterative task-guided DPO (2 iterations × 10 epochs each)

- Design tradeoffs:
  - S (number of RPs): Higher S increases diversity and compute; Table 10 shows S=3 > S=2 on GSM8K (+1.66%) and PIQA (+2.16%)
  - Iterations: Paper uses 2; Table 9 shows Iter 2 > Iter 1 > Iter 0, but marginal gains suggest diminishing returns
  - β in DPO loss: Set to 0.1 to limit deviation from reference model

- Failure signatures:
  - Moderate inter-annotator agreement (κ=0.58): Rationale quality judgment is inherently subjective
  - Qualitative errors: "Almost correct" rationales with single arithmetic operation errors (e.g., + instead of −)
  - Baseline competitiveness: SPIN achieves 98.14% on HellaSwag vs. COLLATE's 99.21%; gains are not uniformly large

- First 3 experiments:
  1. Reproduce Table 4 ablation on GSM8K alone to confirm ~15-point drop without distinct RPs; validates setup correctness.
  2. Run with S=1 and temperature-based sampling to empirically verify that data-split diversity outperforms decoding diversity.
  3. Plot likelihood score vs. human rating on 50 held-out samples to validate the 84% alignment claim in your specific setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-random, domain-specific data allocation to rationale providers improve performance over random splitting?
- Basis: [explicit] The Limitations section suggests exploring if samples could be allocated so each rationale provider "gains expertise in certain task domain(s)" to create specialized experts.
- Why unresolved: Current random splits prevent over-specialization, but it is unknown if specialized "expert" providers would yield higher quality diverse rationales for the final preference optimization.
- What evidence would resolve it: Comparing performance when training providers on clustered domain data versus random splits.

### Open Question 2
- Question: Can the optimal number of rationale providers be determined automatically for a given task?
- Basis: [explicit] The authors explicitly state that "it could be explored if the optimal number of rationale providers could be determined automatically for a given task."
- Why unresolved: The paper manually tests 2 and 3 providers, treating the count as a static hyperparameter, whereas the ideal number likely varies by task complexity.
- What evidence would resolve it: A proposed method that dynamically adjusts the number of providers based on dataset metrics or validation performance.

### Open Question 3
- Question: Can iterative rationale refinement be integrated to fix minor errors in otherwise high-quality generated rationales?
- Basis: [inferred] The Qualitative Error Analysis notes that providers generate "almost correct" rationales with minor errors, suggesting "rationale refinement can be explored in future work."
- Why unresolved: COLLATE selects or eliminates rationales but lacks a mechanism to repair a valid reasoning chain that contains a single token or logic error.
- What evidence would resolve it: A post-generation correction step that increases the accuracy of selected rationales before final answer generation.

## Limitations
- Dependence on strong multi-mode base model (M_IFT) that can flexibly handle all three prompting modes
- Diversity mechanism empirically validated rather than theoretically grounded
- Filtration step (Eq. 9) introduces potential failure point where overly aggressive filtering could starve training
- Pairwise diversity metric (FMR) at 0.51 indicates substantial overlap between rationales

## Confidence
- **High Confidence** (8/10): Core claim that training multiple distinct RPs improves downstream accuracy is strongly supported by controlled ablations
- **Medium Confidence** (6/10): Mechanism by which data-split diversity produces meaningfully different reasoning behaviors is empirically demonstrated but lacks theoretical explanation
- **Low Confidence** (4/10): Scalability claims to 1B parameter models remain untested in main results

## Next Checks
1. **Generalization Test**: Apply COLLATE to a completely different domain (e.g., medical reasoning or legal analysis) with a 1B parameter base model to verify the framework's domain adaptability and performance at smaller scales.

2. **Diversity Mechanism Analysis**: Conduct a systematic study comparing data-split diversity against alternative diversity mechanisms (temperature-based sampling, nucleus sampling with different p values) across multiple model sizes to quantify the unique contribution of the data-split approach.

3. **Filtration Sensitivity**: Perform an ablation study varying the filtration threshold and measuring its impact on training stability, final accuracy, and sample retention rate across tasks of varying difficulty to establish robust operating parameters.