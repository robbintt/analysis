---
ver: rpa2
title: Evaluating Open-Source Vision Language Models for Facial Emotion Recognition
  against Traditional Deep Learning Models
arxiv_id: '2508.13524'
source_url: https://arxiv.org/abs/2508.13524
tags:
- learning
- vision
- deep
- facial
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares traditional deep learning models (VGG19, ResNet-50,
  EfficientNet-B0) with Vision-Language Models (VLMs) including Phi-3.5 Vision and
  CLIP on the FER-2013 dataset for facial emotion recognition. A novel pipeline integrating
  GFPGAN-based image restoration was introduced to address low-resolution and noisy
  image challenges.
---

# Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models

## Quick Facts
- **arXiv ID**: 2508.13524
- **Source URL**: https://arxiv.org/abs/2508.13524
- **Reference count**: 40
- **Primary result**: Traditional deep learning models (EfficientNet-B0: 86.44%, ResNet-50: 85.72%) significantly outperformed VLMs (CLIP: 64.07%, Phi-3.5 Vision: 51.66%) on FER-2013 facial emotion recognition

## Executive Summary
This study systematically compares traditional deep learning models against Vision-Language Models (VLMs) for facial emotion recognition using the FER-2013 dataset. The researchers introduced a novel pipeline incorporating GFPGAN-based image restoration to address challenges posed by low-resolution and noisy images. Their comprehensive evaluation reveals that specialized CNN architectures like EfficientNet-B0 and ResNet-50 significantly outperform general-purpose VLMs including CLIP and Phi-3.5 Vision, with accuracy gaps exceeding 20 percentage points. The study provides valuable insights into the computational costs and practical deployment considerations for each approach.

## Method Summary
The researchers employed a multi-stage methodology involving GFPGAN preprocessing to restore 48×48 grayscale images to RGB format, followed by training traditional CNNs (VGG19, ResNet-50, EfficientNet-B0) with ImageNet pretraining and evaluating VLMs in zero-shot mode. Traditional models were trained with Adam optimizer, categorical cross-entropy loss, He-uniform initialization, batch normalization, dropout, and softmax output layers for 30-60 epochs with batch size 64. VLMs were evaluated using Hugging Face Transformers with prompt-based emotion classification, comparing performance against the CNN baselines on the 7-class FER-2013 dataset.

## Key Results
- EfficientNet-B0 achieved 86.44% accuracy, outperforming all other models on FER-2013
- ResNet-50 reached 85.72% accuracy, demonstrating strong performance in emotion recognition
- CLIP achieved 64.07% accuracy, showing significant limitations in low-quality visual tasks
- Phi-3.5 Vision performed worst at 51.66% accuracy, barely above random chance for 7 classes

## Why This Works (Mechanism)
None

## Foundational Learning
- **GFPGAN image restoration**: Why needed - to enhance low-resolution, noisy 48×48 grayscale images to improve model performance; Quick check - visually compare pre/post restoration samples to verify face preservation
- **Zero-shot VLM evaluation**: Why needed - to assess general-purpose VLMs without task-specific fine-tuning; Quick check - test multiple prompt templates to establish performance baselines
- **CNN transfer learning**: Why needed - to leverage ImageNet pretraining for improved feature extraction on small datasets; Quick check - monitor training/validation accuracy curves for overfitting indicators

## Architecture Onboarding

**Component map**: GFPGAN preprocessing -> Traditional CNNs (VGG19, ResNet-50, EfficientNet-B0) OR VLMs (CLIP, Phi-3.5 Vision) -> Emotion classification

**Critical path**: Image restoration → model training/evaluation → performance comparison on test set

**Design tradeoffs**: Traditional models offer superior accuracy but require task-specific training; VLMs provide flexibility but struggle with low-quality inputs without fine-tuning

**Failure signatures**: VLMs may produce near-random predictions if prompts don't align with pretraining; CNNs may overfit on small datasets without proper regularization

**First experiments**:
1. Test GFPGAN restoration on sample FER-2013 images to verify face preservation quality
2. Evaluate multiple prompt templates for VLMs to establish baseline performance ranges
3. Compare training curves of traditional models with and without image restoration

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can supervised fine-tuning of VLMs on low-resolution, noisy datasets like FER-2013 close the performance gap with specialized CNNs?
- **Basis in paper**: The "Future Direction" section states, "Fine-tuning VLMs for low-resolution, noisy datasets like FER-2013 could improve robustness."
- **Why unresolved**: The current study evaluated VLMs primarily in zero-shot and few-shot modes without task-specific weight updates (Section 5.3.2).
- **What evidence would resolve it**: A comparative experiment showing VLM accuracy after full supervised fine-tuning versus the current EfficientNet-B0 baseline of 86.44%.

### Open Question 2
- **Question**: Does a hybrid architecture integrating CNN feature extractors with VLMs improve feature extraction and contextual understanding compared to standalone models?
- **Basis in paper**: The authors explicitly propose future research "integrating deep learning models... with VLMs... to enhance both feature extraction and contextual understanding."
- **Why unresolved**: The paper evaluated traditional DL models and VLMs as separate, competing architectures rather than combined systems.
- **What evidence would resolve it**: Performance metrics (accuracy, F1-score) of a fused CNN-VLM pipeline on the FER-2013 dataset.

### Open Question 3
- **Question**: Do the relative performance advantages of traditional models over VLMs persist when applied to higher-resolution images or real-time video data?
- **Basis in paper**: The authors suggest "Expanding experiments to larger, more diverse datasets, including higher-resolution images or real-time video, can enhance generalization."
- **Why unresolved**: The study was restricted to the FER-2013 dataset, which consists solely of 48x48 pixel grayscale images.
- **What evidence would resolve it**: A benchmark comparison of VLMs versus EfficientNet-B0 on high-resolution video datasets (e.g., AffectNet in the wild).

## Limitations
- The study's reliance on zero-shot evaluation for VLMs without fine-tuning limits generalizability to real-world deployment scenarios where labeled data is available
- Significant discrepancy between abstract (86.44%) and Table 1 (94.72%) accuracy values for EfficientNet-B0 undermines reproducibility
- The evaluation only covers 48×48 pixel grayscale images, not reflecting real-world high-resolution or video-based applications

## Confidence
- **Traditional model results**: High confidence - extensive experimental validation with clear methodology
- **VLM performance metrics**: Medium confidence - results depend heavily on unspecified prompt templates
- **Computational cost analysis**: Medium confidence - limited details on inference time measurements

## Next Checks
1. Re-run the VLM experiments with multiple prompt template variations to establish baseline performance ranges for Phi-3.5 Vision and CLIP on FER-2013
2. Verify the actual train/validation/test split ratios and confirm whether the 94.72% accuracy in Table 1 represents a different split or calculation error compared to the abstract's 86.44%
3. Conduct a small-scale fine-tuning experiment for VLMs using a subset of labeled FER-2013 data to determine if supervised adaptation can close the performance gap with traditional models