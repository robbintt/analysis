---
ver: rpa2
title: 'Advances in Continual Graph Learning for Anti-Money Laundering Systems: A
  Comprehensive Review'
arxiv_id: '2503.24259'
source_url: https://arxiv.org/abs/2503.24259
tags:
- learning
- data
- continual
- graph
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review critically evaluates continual graph learning methods
  for anti-money laundering (AML) applications. It addresses the challenge of catastrophic
  forgetting in dynamic financial networks where money laundering tactics continuously
  evolve.
---

# Advances in Continual Graph Learning for Anti-Money Laundering Systems: A Comprehensive Review

## Quick Facts
- arXiv ID: 2503.24259
- Source URL: https://arxiv.org/abs/2503.24259
- Reference count: 40
- Key outcome: Replay-based methods (especially GEM) outperform regularization and architecture-based strategies in retaining knowledge while adapting to new AML patterns.

## Executive Summary
This review evaluates continual graph learning methods for anti-money laundering (AML) applications, addressing catastrophic forgetting in dynamic financial networks where money laundering tactics evolve. The study systematically categorizes methods into replay-based, regularization-based, and architecture-based strategies within the graph neural network (GNN) framework. Through extensive experiments on synthetic and real-world AML datasets, the research demonstrates that continual learning improves model adaptability and robustness, particularly in handling extreme class imbalances and evolving fraud patterns. The results highlight replay-based methods, especially Gradient Episodic Memory (GEM), as superior for retaining knowledge while adapting to new tasks, making them highly suitable for practical AML systems under regulatory and computational constraints.

## Method Summary
The study employs continual graph learning (CGL) for AML using Graph Convolutional Networks (GCN) on node and edge classification tasks. It uses the Elliptic dataset (203,769 nodes, 166 features, 49 time steps) and IBM HI-Small dataset (515,080 nodes, 5M edges, with "Not classified" edges removed). The backbone is a 2-layer GCN with 128 hidden dimensions, trained with Adam optimizer (LR=0.001). Continual learning strategies include GEM, EWC, MAS, LwF, and TWP, evaluated using Average Performance (AP) and Average Forgetting (AF) metrics based on Micro-F1 score. The experimental setup tests 1, 2, 5, or 10 epochs per task to assess knowledge retention.

## Key Results
- Replay-based methods, especially Gradient Episodic Memory (GEM), show the best performance in retaining knowledge while adapting to new AML tasks.
- Continual learning significantly improves model adaptability and robustness in handling extreme class imbalances in financial networks.
- Average Forgetting increases non-linearly with training epochs, highlighting the importance of optimizing epoch schedules for knowledge retention.

## Why This Works (Mechanism)
The paper's approach addresses catastrophic forgetting by implementing replay-based methods that store and replay past experiences during training. GEM specifically constrains gradient updates to prevent forgetting while allowing new learning, making it particularly effective for AML applications where patterns evolve but historical knowledge remains valuable. The method's success stems from its ability to balance stability (retaining old knowledge) and plasticity (adapting to new patterns) in dynamic graph environments.

## Foundational Learning
- **Catastrophic Forgetting**: Occurs when neural networks lose previously learned information while adapting to new tasks; critical for AML where patterns evolve but historical knowledge is essential.
- **Graph Neural Networks (GNNs)**: Deep learning models designed for graph-structured data; necessary for AML as financial transactions form complex network relationships.
- **Continual Learning**: Learning paradigm where models adapt to new tasks without forgetting previous ones; vital for AML systems facing evolving fraud tactics.
- **Replay-based Methods**: Techniques that store and replay past experiences during training; effective for AML as they preserve historical fraud patterns.
- **Micro-F1 Score**: Evaluation metric that balances precision and recall; important for AML due to extreme class imbalance between legitimate and fraudulent transactions.

## Architecture Onboarding
**Component Map**: Data Preprocessing -> GCN Backbone -> Continual Learning Strategy -> Evaluation Metrics
**Critical Path**: The GCN backbone with replay-based continual learning (GEM) represents the critical path for optimal performance, as it directly addresses catastrophic forgetting while maintaining adaptability.
**Design Tradeoffs**: Replay-based methods offer superior knowledge retention but require memory storage for past experiences, while regularization methods are computationally lighter but less effective at preventing forgetting.
**Failure Signatures**: Joint model collapse occurs when extreme imbalance causes the model to predict only majority class; sudden forgetting manifests as sharp performance drops with increased training epochs.
**First Experiments**: 1) Run GCN+GEM configuration on Elliptic dataset with 5 epochs/task to verify lower Average Forgetting than baseline. 2) Test different memory budgets (B) for GEM to establish performance bounds. 3) Evaluate per-class recall to identify potential joint model collapse.

## Open Questions the Paper Calls Out
- How does the degree of class imbalance quantitatively impact the performance and forgetting rates of different continual learning strategies in graph-based AML?
- How do continual learning models perform in dynamic AML environments where fraud patterns rotate or when there are extended periods with zero positive labels?
- To what extent do the performance benefits of replay-based methods like GEM persist when applied to attention-based GNN architectures such as GAT?

## Limitations
- Lacks specific hyperparameter settings for memory budget (B) in replay methods and regularization coefficients, which significantly impact continual learning performance.
- Treatment of "Not classified" edges in IBM dataset is vaguely described, creating ambiguity about graph topology modifications.
- Evaluation metrics focus solely on micro-F1 without addressing per-class performance, problematic given extreme class imbalance in AML detection.

## Confidence
- **High Confidence**: Replay-based methods (particularly GEM) outperform fine-tuning and regularization approaches for knowledge retention in AML tasks.
- **Medium Confidence**: Specific performance gaps between methods are less certain due to unspecified hyperparameters and evaluation protocols.
- **Low Confidence**: Claims about replay-based methods being "highly suitable for practical AML systems under regulatory and computational constraints" lack supporting evidence for computational efficiency or regulatory compliance.

## Next Checks
1. **Hyperparameter Sensitivity**: Conduct ablation studies varying memory budget (B) for GEM and regularization coefficients (Î») for EWC/MAS to establish performance bounds.
2. **Per-Class Evaluation**: Extend analysis beyond micro-F1 to include per-class recall and F1 scores, particularly for minority fraud classes.
3. **Computational Efficiency**: Measure and report training/inference time and memory usage for each continual learning method to assess practical deployment feasibility.