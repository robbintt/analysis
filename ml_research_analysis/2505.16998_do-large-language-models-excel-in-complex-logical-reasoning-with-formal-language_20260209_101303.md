---
ver: rpa2
title: Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?
arxiv_id: '2505.16998'
source_url: https://arxiv.org/abs/2505.16998
tags:
- reasoning
- bbeh
- formal
- language
- featured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates large language models'' performance on complex
  logical reasoning tasks using formal languages. The authors develop a systematic
  evaluation framework across three dimensions: spectrum of LLMs, taxonomy of reasoning
  tasks, and trajectory formats.'
---

# Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?
## Quick Facts
- arXiv ID: 2505.16998
- Source URL: https://arxiv.org/abs/2505.16998
- Reference count: 40
- Key outcome: Thinking models significantly outperform instruct models on formal reasoning tasks, with PoT format showing best cross-language generalization

## Executive Summary
This paper systematically evaluates large language models on complex logical reasoning tasks using formal languages. The authors develop a comprehensive framework across three dimensions: model spectrum, reasoning taxonomy, and trajectory formats. They collect 66 datasets covering deductive, inductive, abductive, and mixed-form reasoning, and evaluate four trajectory formats (Text, PoT, Z3, CSP). The study finds that thinking models significantly outperform instruct models when using formal languages, all models struggle with inductive reasoning, and PoT format achieves the best generalization across languages. They also propose a rejected fine-tuning approach that enhances small models' performance by more than 10% on formal reasoning tasks.

## Method Summary
The authors evaluate LLMs across 66 datasets in four reasoning categories (deductive, inductive, abductive, mixed-form) using four trajectory formats (Text, PoT, Z3, CSP). They employ zero-shot evaluation with 3-step self-refinement for formal languages, measuring accuracy and execution rate. The rejected fine-tuning (RFT) method involves sampling multiple responses from GPT-4o, filtering for executability and correctness, then fine-tuning small models (Qwen2.5-7B) using SFT with specific hyperparameters. The evaluation uses vLLM with greedy decoding, maximum sequence length of 16K tokens, and Qwen-2.5-72B-Instruct as evaluator.

## Key Results
- Thinking models significantly outperform instruct models on formal reasoning tasks, especially when formal language is employed
- PoT (Python) format achieves the best generalization performance across other formal languages
- Rejected fine-tuning with filtered formal data improves small model performance by over 10% on formal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thinking models outperform instruct models in formal language reasoning tasks.
- Mechanism: Thinking models undergo training that injects extended reasoning patterns, enabling more reliable trajectory generation for formal languages; instruct models lack this explicit reasoning-focused training signal.
- Core assumption: The performance gap stems from training methodology rather than model scale alone.
- Evidence anchors:
  - [abstract] "Thinking models significantly outperform Instruct models, especially when formal language is employed"
  - [section 3.1] "The disparities between them reflect that the Thinking mode can better elicit the LLM to provide reliable trajectories for formal reasoning"
  - [corpus] PEIRCE paper discusses integrating material and formal inference, supporting reasoning-focused training approaches
- Break condition: If instruct models with similar scale match thinking model performance, mechanism weakens; if gap disappears on simple tasks, training signal may not be the differentiator.

### Mechanism 2
- Claim: PoT (Python) trajectory format generalizes best across other formal languages.
- Mechanism: Pre-training on code data provides a structural foundation that transfers to related formal representations (Z3, CSP), while CSP has distinct structural characteristics that hinder bidirectional transfer.
- Core assumption: Code pre-training creates transferable symbolic reasoning capabilities.
- Evidence anchors:
  - [abstract] "Data with PoT format achieves the best generalization performance across other languages"
  - [section 4.2] "PoT migrates well in Text, Z3, and CSP... CSP, on the other hand, has an effect only on Text and CSP, with significant negative effects on PoT (-15.8) and Z3 (-8.9)"
  - [corpus] Intermediate Languages Matter paper notes LLMs as translators from natural to formal languages, but corpus evidence on generalization directionality is weak
- Break condition: If PoT fails to transfer to new formal language types (e.g., theorem provers like Lean), mechanism may be specific to Python-adjacent languages.

### Mechanism 3
- Claim: Rejected Fine-Tuning (RFT) with filtered formal data improves small model performance by over 10%.
- Mechanism: Sampling multiple responses from a teacher model, filtering for executability and correctness, and training on this curated data improves the student model's ability to generate valid formal trajectories.
- Core assumption: The teacher model can generate sufficient high-quality executable samples; filtering effectively removes incorrect reasoning patterns.
- Evidence anchors:
  - [abstract] "a simple rejected fine-tuning method can better enable LLMs to generalize across formal languages and achieve the best overall performance"
  - [section 5.1] "We used GPT-4o to sample the questions several times and then filtered out those samples whose code was executable and whose final answers are verified to be correct"
  - [section 5.2, Table 1] Shows CSP accuracy improving from 20.0% to 37.0%, execution rate from 52.2% to 68.1%
  - [corpus] No direct corpus evidence for RFT specifically; related work on neuro-symbolic refinement exists but doesn't address this mechanism
- Break condition: If improvements don't generalize to held-out tasks or formal languages not in training data, mechanism may be memorization rather than capability acquisition.

## Foundational Learning

- Concept: **Thinking vs Instruct Model Training Paradigms**
  - Why needed here: The paper's primary finding depends on understanding that thinking models (QwQ, DeepSeek-R1-Distill) receive training optimized for extended reasoning chains, while instruct models (GPT-4o, Qwen2.5) are optimized for direct response generation.
  - Quick check question: If a model produces shorter, more direct responses without intermediate reasoning steps, is it likely a thinking or instruct model?

- Concept: **Reasoning Taxonomy (Deductive/Inductive/Abductive/Mixed)**
  - Why needed here: The paper organizes 66 datasets into these categories and finds all models struggle specifically with inductive reasoning; understanding this taxonomy is essential for interpreting the generalization experiments.
  - Quick check question: A task asks you to identify which rule best explains a set of (premise, conclusion) pairs—which reasoning type is this?

- Concept: **Trajectory Execution and Self-Refinement**
  - Why needed here: Formal languages (PoT, Z3, CSP) require external execution engines to verify correctness; the paper uses a three-step self-refinement process during code execution.
  - Quick check question: If a model generates Z3 code that the solver cannot execute, what metric would this failure be captured in?

## Architecture Onboarding

- Component map:
  - **Evaluation Framework**: Three dimensions (Model × Task Type × Trajectory Format)
  - **Trajectory Generators**: LLMs output Text, PoT, Z3, or CSP formatted reasoning chains
  - **Execution Engines**: Python 3.12, Z3 theorem prover, python-constraint solver
  - **RFT Pipeline**: Teacher model sampling → Executability filter → Correctness verification → Student model fine-tuning

- Critical path:
  1. Dataset collection and categorization (66 datasets → 4 reasoning types)
  2. Zero-shot trajectory generation across all models and formats
  3. Execution and accuracy evaluation (with 3-step self-refinement)
  4. Generalization analysis (train on one format/reasoning type, evaluate on others)
  5. RFT data construction and small model fine-tuning

- Design tradeoffs:
  - Zero-shot vs few-shot: Paper chooses zero-shot for better generalization assessment (see Figure 10 case study)
  - Single vs mixed formal language training: Generalization experiments use single-language training data for controlled comparison
  - Filtering threshold: RFT requires both executability AND correctness, which reduces training data size but improves quality

- Failure signatures:
  - **Low execution rate**: Indicates syntactic errors in generated formal code (e.g., Qwen2.5-7B achieves only 52.1% CSP execution rate)
  - **Poor CSP transfer**: Negative transfer from CSP to PoT/Z3 suggests structural incompatibility
  - **Small model collapse on formal languages**: Performance degrades more rapidly than text (Section 3.1)
  - **Inductive reasoning failures**: Consistent across all models and trajectory formats (Section 3.1)

- First 3 experiments:
  1. **Baseline Cross-Model Comparison**: Run evaluation on a subset (e.g., deductive tasks only) across 2-3 thinking and instruct models using Text and PoT formats to verify the thinking model advantage before scaling to full evaluation.
  2. **Trajectory Format Preference Analysis**: Pick one strong model (GPT-4o or QwQ-32B) and evaluate on 5-10 representative tasks across all four trajectory formats to understand task-format alignment before full dataset evaluation.
  3. **RFT Data Quality Validation**: Sample 100 instances, generate multiple teacher responses, filter for executability/correctness, and measure the retention rate; if <30% pass filtering, adjust teacher parameters or sampling strategy before full RFT pipeline.

## Open Questions the Paper Calls Out

- Can incorporating reasoning results from different trajectory formats as individual voters in a majority voting scheme significantly improve overall accuracy? (The paper identifies that different tasks prefer different formats but doesn't experiment with aggregation methods.)

- Does constructing formal language reasoning datasets in a "thinking" style yield better performance in small models than the Instruct-style data used in this study? (The current RFT method utilized Instruct outputs; it remains unknown if data generated by Thinking models would further enhance small model performance.)

- Do the generalization trends observed between Python, Z3, and CSP hold for other symbolic systems like Lean, Prolog, or Coq? (The study focuses on PoT, Z3, and CSP, excluding other symbolic systems such as theorem provers or logic programming languages.)

## Limitations
- The RFT approach's effectiveness depends heavily on the quality of the teacher model and filtering criteria, with exact sampling parameters unspecified
- The 66 datasets, while diverse, may not fully represent the space of complex logical reasoning tasks, particularly for abductive and mixed-form reasoning
- The three-step self-refinement process for formal language execution lacks detailed implementation specifications, making exact reproduction challenging

## Confidence

**High Confidence**: The comparative performance between thinking and instruct models using formal languages is well-supported by multiple evaluation runs across diverse tasks. The RFT method's effectiveness (10%+ improvements) is demonstrated with clear before/after metrics on held-out data.

**Medium Confidence**: The generalization patterns across trajectory formats show consistent directional effects (PoT > Text > Z3 > CSP for transfer), though the magnitude varies across specific task types. The inductive reasoning limitation appears robust but may be dataset-dependent.

**Low Confidence**: The specific mechanisms underlying CSP's poor bidirectional transfer require further investigation. The exact conditions under which the RFT method fails (e.g., with different teacher models or task distributions) remain unexplored.

## Next Checks

1. **Ablation on RFT Filtering Criteria**: Run the RFT pipeline with only executability filtering versus both executability and correctness filtering to quantify the contribution of each criterion to performance gains.

2. **Cross-Teacher Model Validation**: Repeat the RFT process using QwQ-32B-Preview as the teacher instead of GPT-4o to determine if improvements are teacher-model-dependent or reflect genuine capability transfer.

3. **Inductive Reasoning Task Expansion**: Construct a targeted set of inductive reasoning tasks not present in the original 66 datasets and evaluate all models to determine if the inductive reasoning limitation persists across novel task structures.