---
ver: rpa2
title: 'IMPersona: Evaluating Individual Level LM Impersonation'
arxiv_id: '2504.04332'
source_url: https://arxiv.org/abs/2504.04332
tags:
- human
- memory
- preprint
- arxiv
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IMPersona, a framework for evaluating language
  models' ability to impersonate specific individuals in conversation. The framework
  combines supervised fine-tuning with a hierarchical memory retrieval system to replicate
  both stylistic communication patterns and personal contextual knowledge.
---

# IMPersona: Evaluating Individual Level LM Impersonation

## Quick Facts
- arXiv ID: 2504.04332
- Source URL: https://arxiv.org/abs/2504.04332
- Reference count: 40
- Key outcome: Fine-tuned models achieve 44.44% pass rate vs 25.00% for prompting in blind impersonation tests

## Executive Summary
IMPersona introduces a framework for evaluating language models' ability to impersonate specific individuals in conversation. The approach combines supervised fine-tuning with hierarchical memory retrieval to replicate both stylistic patterns and personal contextual knowledge. In blind conversation experiments with 114 participants, fine-tuned models significantly outperformed prompting-based approaches, with Llama-3.1-8B-Instruct fine-tuned on 500 examples plus memory augmentation achieving the best results. The study demonstrates that effective impersonation requires surprisingly little training data and raises important concerns about privacy, security, and potential social engineering attacks using personalized language models.

## Method Summary
The IMPersona framework employs supervised fine-tuning of language models on conversation examples specific to target individuals, combined with a hierarchical memory retrieval system that accesses both long-term contextual knowledge and short-term conversation history. The evaluation uses blind conversation experiments where participants engage with either human or AI impersonators without knowing which they're interacting with. Participants then attempt to identify whether their conversation partner was human or AI. The framework tests different model configurations including various base models (Llama-3.1, Qwen2.5, Phi-3) and training data sizes, with the best results achieved using Llama-3.1-8B-Instruct fine-tuned on 500 examples with memory augmentation.

## Key Results
- Fine-tuned models achieved 44.44% pass rate (correctly identified as human) versus 25.00% for prompting-based approaches
- Llama-3.1-8B-Instruct with 500 training examples and memory augmentation showed best performance
- Minimal training data (500 examples) proved sufficient for effective impersonation
- Hierarchical memory retrieval significantly improved impersonation accuracy by providing contextual knowledge

## Why This Works (Mechanism)
The framework works by combining two complementary approaches: supervised fine-tuning captures individual-specific stylistic patterns, communication preferences, and response tendencies, while the hierarchical memory retrieval system provides access to personal contextual knowledge that would be difficult to encode through fine-tuning alone. This dual approach allows the model to not only mimic how someone communicates but also what they know about their personal experiences, relationships, and domain-specific knowledge. The memory system uses a two-tier architecture where long-term memories provide background context and short-term memories track ongoing conversation details, enabling coherent and contextually appropriate responses that align with the target individual's knowledge base.

## Foundational Learning
- Supervised fine-tuning: Why needed - to adapt general language models to specific individual communication styles; Quick check - compare model outputs before and after fine-tuning on individual-specific data
- Hierarchical memory retrieval: Why needed - to provide contextual knowledge beyond what can be captured in model weights; Quick check - measure retrieval accuracy and relevance of retrieved memories
- Blind conversation evaluation: Why needed - to test real-world impersonation effectiveness without participants knowing they're interacting with AI; Quick check - calculate human accuracy in distinguishing AI from human responses
- Zero-shot prompting limitations: Why needed - to establish baseline performance and demonstrate value of fine-tuning; Quick check - compare pass rates between zero-shot and fine-tuned models

## Architecture Onboarding
- Component map: Input conversation -> Memory retrieval system (long-term + short-term) -> Language model (fine-tuned or prompted) -> Generated response
- Critical path: Conversation input → Memory retrieval → Context augmentation → Model inference → Response generation
- Design tradeoffs: Fine-tuning vs prompting (accuracy vs flexibility), memory complexity vs response latency, training data quantity vs performance gains
- Failure signatures: Generic responses when memory retrieval fails, inconsistent personality across conversations, factual errors about personal details, overly formal or informal tone mismatch
- First experiments: 1) Compare pass rates between different base models with identical training, 2) Test memory retrieval impact by disabling short-term vs long-term memories separately, 3) Evaluate performance degradation with decreasing training data quantities

## Open Questions the Paper Calls Out
None

## Limitations
- Convenience sample of 114 Prolific participants may introduce selection bias
- Simplified conversation protocol may not capture real-world social engineering complexity
- Short-term deception focus without examining long-term consistency or detection potential
- No exploration of ethical implications of training on scraped personal data

## Confidence
- High: Core technical findings regarding fine-tuning + memory retrieval effectiveness
- Medium: Claim that 500 examples suffice for effective impersonation
- Low: Security implications and practical feasibility of large-scale deployment

## Next Checks
1. Conduct longitudinal studies examining impersonation success rates over extended conversation periods with varying numbers of exchanges
2. Test the framework across diverse demographic groups and communication styles to assess population susceptibility differences
3. Implement adversarial evaluation scenarios where participants are explicitly primed to detect AI-generated responses to measure framework robustness against defensive awareness