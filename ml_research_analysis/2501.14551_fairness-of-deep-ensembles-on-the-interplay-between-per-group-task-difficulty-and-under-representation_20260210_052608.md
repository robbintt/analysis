---
ver: rpa2
title: 'Fairness of Deep Ensembles: On the interplay between per-group task difficulty
  and under-representation'
arxiv_id: '2501.14551'
source_url: https://arxiv.org/abs/2501.14551
tags:
- difficulty
- fairness
- performance
- ensembles
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether homogeneous deep ensembles can
  improve fairness by reducing performance disparities across demographic subgroups.
  The authors explore the interplay between per-group task difficulty and under-representation
  using synthetic and real datasets.
---

# Fairness of Deep Ensembles: On the interplay between per-group task difficulty and under-representation

## Quick Facts
- arXiv ID: 2501.14551
- Source URL: https://arxiv.org/abs/2501.14551
- Reference count: 37
- This paper demonstrates that homogeneous deep ensembles can achieve positive-sum fairness by reducing performance gaps between demographic subgroups without degrading overall accuracy.

## Executive Summary
This paper investigates whether homogeneous deep ensembles can improve fairness by reducing performance disparities across demographic subgroups. The authors explore the interplay between per-group task difficulty and under-representation using synthetic and real datasets. They construct synthetic scenarios with binary classification tasks, manipulating subgroup representation and task difficulty via label noise or rotated decision boundaries. Real datasets include CelebA (binary hair-color prediction) and CheXpert (lung opacity classification), where subgroups are defined by gender. The study shows that homogeneous ensembles consistently improve both individual subgroup performance and overall accuracy, achieving positive-sum fairness without the typical trade-off of reducing performance for one group to benefit another. Critically, the results reveal that perfectly balanced datasets may be suboptimal when task difficulty varies between subgroups; over-representing the more difficult subgroup can minimize performance gaps and maximize overall accuracy. For example, in the label noise scenario, the ideal balance ratio shifted from 50-50 to as high as 80-20 as task difficulty increased. These findings highlight the importance of considering intersectional sources of bias and suggest that traditional rebalancing techniques may be harmful if task difficulty differences are not accounted for.

## Method Summary
The paper uses synthetic Gaussian datasets with two demographic subgroups (M/F) and two target classes, along with real datasets CelebA and CheXpert. For synthetic data, they manipulate task difficulty through label noise (0-50% flipped) or rotated decision boundaries (0-45Â°). They train ensembles of 20 ResNet-50 models (or fully connected networks for synthetic data) with different random initializations, constructing ensembles by averaging predictions from k randomly selected models. The key experiment varies the M-F representation ratio from 0%-100% to 100%-0% in 10% steps while measuring per-group accuracy, overall accuracy, and performance gaps. Task difficulty is proxied by performance gaps in balanced datasets.

## Key Results
- Homogeneous deep ensembles consistently improve both individual subgroup performance and overall accuracy, achieving positive-sum fairness
- Perfectly balanced datasets are suboptimal when task difficulty varies between subgroups; over-representing the more difficult subgroup minimizes performance gaps
- In the label noise scenario, the optimal balance ratio shifted from 50-50 to as high as 80-20 as task difficulty increased
- The benefits are observed across both synthetic scenarios and real datasets (CelebA hair color, CheXpert lung opacity)

## Why This Works (Mechanism)
The mechanism relies on model diversity created through random initialization. When multiple models with different random seeds are trained on the same data, they develop slightly different decision boundaries due to stochastic gradient descent. Averaging these diverse predictions reduces variance in the ensemble's output, which particularly benefits the underperforming group by smoothing out systematic errors. This creates positive-sum fairness where both groups see performance improvements while the gap narrows.

## Foundational Learning

- **Concept: Group Fairness (Demographic Parity/Equalized Odds)**
  - **Why needed here:** The paper's central goal is to reduce *performance gaps* between groups defined by protected attributes (e.g., gender), which is the core concern of group fairness. Understanding this is prerequisite to framing the problem.
  - **Quick check question:** Can you articulate the difference between ensuring a model has similar *accuracy* for Group A and Group B versus ensuring it has similar *prediction rates*?

- **Concept: Deep Ensembles**
  - **Why needed here:** The proposed intervention is a "homogeneous deep ensemble." You must understand that this means training multiple independent neural networks with the same architecture and data but different random initializations, then averaging their predictions.
  - **Quick check question:** How does a "homogeneous" ensemble differ from a "heterogeneous" one, and why might the former be simpler to implement?

- **Concept: Positive-Sum Fairness**
  - **Why needed here:** This is a key outcome. It contrasts with the common "leveling-down" trade-off where fairness is improved by *degrading* the performance of the better-performing group. This paper claims to improve fairness by *lifting* the worse-performing group without harming others.
  - **Quick check question:** In a two-group scenario, if the accuracy for Group A increases from 85% to 88% and for Group B from 75% to 80%, is this a "positive-sum" or "leveling-down" fairness intervention?

## Architecture Onboarding

- **Component map:**
  - Data Resampling Module -> Training Loop (N times) -> Ensemble Aggregator

- **Critical path:** The effectiveness of the entire system hinges on the **Training Loop** producing models with *uncorrelated errors* despite having identical architecture and data. This diversity is driven purely by stochasticity (random initialization).

- **Design tradeoffs:**
  - **Computational Cost vs. Fairness/Performance:** Training `N` models is `N` times more expensive. This is the primary cost of the method.
  - **Representation Ratio vs. Ideal Fairness:** The optimal ratio is not fixed (e.g., it's not always 50-50). It depends on the hidden variable of "per-group task difficulty," which must be empirically discovered.
  - **Assumption:** Using performance on a balanced dataset as a proxy for task difficulty. This is a simplification and may not hold if the balanced dataset has other biases.

- **Failure signatures:**
  - **Ensemble members are too similar:** If models converge to very similar solutions, the ensemble will provide little benefit over a single model. This could happen if the task is trivially easy or if the model is severely underparameterized.
  - **Over-representation causes overfitting:** Increasing the ratio of the harder group too much (e.g., to 99-1) could cause the model to overfit that group and fail to generalize to the minority group at all.
  - **Misidentifying "difficulty":** If the performance gap on the balanced data is due to a spurious correlation (e.g., all "blond" examples in CelebA are female) rather than inherent task difficulty, simply over-representing the under-performing group may not have the intended effect.

- **First 3 experiments:**
  1.  **Establish a baseline for task difficulty:** Train a single model on a perfectly balanced (50-50) version of your dataset. Measure per-group accuracy to identify which group has higher "difficulty" (lower accuracy).
  2.  **Test the ensemble effect:** Train an ensemble (e.g., N=5, N=10, N=20) on the balanced dataset. Plot per-group accuracy vs. ensemble size. Confirm that the gap narrows and the overall performance improves (positive-sum fairness).
  3.  **Find the optimal representation ratio:** For the identified "harder" group, create training datasets with varying degrees of over-representation (e.g., 60-40, 70-30, 80-20). Train ensembles on each and plot both overall accuracy and the per-group gap. Find the ratio that jointly optimizes for high overall accuracy and a low fairness gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the positive-sum fairness benefits of homogeneous deep ensembles generalize to non-vision domains or tasks beyond binary classification?
- Basis in paper: [explicit] The conclusion states, "Substantial work remains to be done to understand if and how these findings generalize to other ML domains and tasks, opening up a valuable avenue of research."
- Why unresolved: The study strictly evaluated image classification tasks (CelebA, CheXpert) and synthetic binary scenarios.
- What evidence would resolve it: Empirical results from experiments applying homogeneous ensembles to natural language processing, tabular data, or multi-class classification tasks.

### Open Question 2
- Question: Do the fairness improvements observed in ResNet-50 models persist when using modern architectures such as Vision Transformers?
- Basis in paper: [explicit] The limitations section notes that "more complex architectures could be considered (like Visual Transformers for image classification, for example) to ensure that our conclusions also hold for larger vision models."
- Why unresolved: The real-world experiments were limited to ResNet-50 CNNs and simple fully connected networks.
- What evidence would resolve it: Comparative experiments evaluating performance gaps and optimal representation ratios using Vision Transformers on the same datasets.

### Open Question 3
- Question: Can a more robust metric for intrinsic task difficulty be developed that accounts for expert uncertainty or intra-group variability?
- Basis in paper: [explicit] The authors acknowledge their proxy for difficulty is limited and suggest future work should seek a measure that "directly captures how often experts would fail."
- Why unresolved: The paper relied on performance gaps in balanced datasets as a proxy, which may fail if subpopulations have differing sample variability.
- What evidence would resolve it: A new methodology for quantifying difficulty using datasets with multiple expert annotators to capture true label uncertainty.

## Limitations
- The findings rely on controlled synthetic scenarios and specific real datasets, limiting generalizability
- The optimal balance ratio's dependence on task difficulty may not transfer across domains with different data distributions or model architectures
- The ensemble method's effectiveness assumes sufficient model diversity, which could diminish with very simple tasks or extreme class imbalances
- The study focuses on binary classification and two demographic subgroups, leaving open questions about multi-class problems or intersectional fairness across multiple attributes

## Confidence
- **High**: Ensemble consistently improves both group performance and reduces gaps in tested scenarios
- **Medium**: Over-representation of harder groups is optimal, as this depends on accurate difficulty estimation and may not hold in all real-world contexts
- **Medium**: The mechanism by which ensembles achieve positive-sum fairness (reducing variance in predictions)

## Next Checks
1. Test the over-representation strategy on multi-class classification tasks with more than two demographic subgroups
2. Evaluate whether the optimal balance ratio persists across different model architectures (e.g., Vision Transformers vs. ResNet)
3. Assess robustness when the assumed difficulty proxy (performance on balanced data) is systematically biased