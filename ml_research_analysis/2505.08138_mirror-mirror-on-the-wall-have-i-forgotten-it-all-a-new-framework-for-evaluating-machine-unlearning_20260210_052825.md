---
ver: rpa2
title: Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating
  Machine Unlearning
arxiv_id: '2505.08138'
source_url: https://arxiv.org/abs/2505.08138
tags:
- unlearning
- computational
- learning
- adversary
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal definition of computational unlearning,
  which requires that an adversary cannot distinguish between a model produced by
  an unlearning method and a model trained from scratch without the data to be forgotten.
  The authors propose two distinguisher algorithms based on membership inference attack
  scores and Kullback-Leibler divergence, and demonstrate that several representative
  unlearning methods from the literature fail to achieve computational unlearning
  on ResNet-18 models trained on CIFAR-10.
---

# Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning

## Quick Facts
- arXiv ID: 2505.08138
- Source URL: https://arxiv.org/abs/2505.08138
- Reference count: 40
- Key outcome: Existing unlearning methods fail computational unlearning with 60-100% adversary success rates

## Executive Summary
This paper introduces a formal cryptographic framework for evaluating machine unlearning that requires an adversary cannot distinguish between a model produced by an unlearning method and a model trained from scratch without the data to be forgotten. The authors demonstrate that several representative unlearning methods from the literature fail to achieve computational unlearning on ResNet-18 models trained on CIFAR-10, with success rates ranging from 60% to 100% across different methods and forget set sizes. The paper also establishes theoretical impossibility results showing that deterministic unlearning methods cannot achieve computational unlearning, and that using differential privacy to achieve black-box computational unlearning leads to utility collapse.

## Method Summary
The authors propose a cryptographic security game where an adversary must distinguish between an unlearned model and a control model retrained from scratch without the forget set. They introduce two distinguisher algorithms based on membership inference attack scores and Kullback-Leibler divergence, demonstrating these can reliably identify unlearned models. The framework is evaluated on several unlearning methods including SISA, Amnesiac, Bad Teacher, and classwise deletion unlearning, showing all fail to achieve computational unlearning with success rates exceeding 60%.

## Key Results
- Several unlearning methods achieve only 60-100% distinguishability from control models (baseline is 50%)
- Deterministic unlearning methods cannot achieve computational unlearning due to exact computation of expected outputs
- DP-based approaches that achieve computational unlearning suffer complete utility collapse to random initialization
- Classwise unlearning (forgetting entire classes) shows 100% distinguishability using KLDScore

## Why This Works (Mechanism)

### Mechanism 1: Computational Unlearning Security Game
- Claim: An unlearning method achieves computational unlearning if and only if an adversary cannot distinguish the unlearned model from a retrained control model with probability better than random guessing.
- Mechanism: A cryptographic security game where a challenger produces both an unlearned model (M_u) and a control model (M_c) retrained without the forget set. The adversary must determine which model is which. Computational unlearning requires P(b'=b) < 1/2 + negl(λ).
- Core assumption: The adversary has access to the original model, learning algorithm description, unlearning algorithm, training set D, and forget set D_f (a strong threat model per Kerckhoffs's principle).
- Evidence anchors:
  - [abstract]: "Computational unlearning is defined as the inability for an adversary to distinguish between a mirror model and a model produced by an unlearning method."
  - [section 3.2, Definitions 7-8]: Formal game definitions for white-box and black-box settings
  - [corpus]: Limited corpus support for this specific cryptographic framing; related benchmarks like UnSLU-BENCH use different evaluation paradigms
- Break condition: If adversary success rate significantly exceeds 50% (paper shows >60% across methods), unlearning fails.

### Mechanism 2: Distinguisher Score Functions (MIAScore and KLDScore)
- Claim: Membership inference attack scores and KL divergence from the original model can reliably distinguish unlearned models from controls.
- Mechanism:
  - MIAScore: Standard membership inference; unlearned models exhibit out-of-distribution scores (often over-minimized compared to controls)
  - KLDScore: KL divergence between original model M_o and candidate on forget set with noise injection; unlearned models have lower divergence than controls
- Core assumption: The adversary retains access to original model M_o for comparison.
- Evidence anchors:
  - [abstract]: "We build distinguishing algorithms based on evaluation scores in the literature (i.e. membership inference scores) and Kullback-Leibler divergence."
  - [section 4.1, Equation 1]: Formal KLDScore definition
  - [corpus]: "Unlearning's Blind Spots" corroborates over-unlearning phenomena as distinguishing artifact
- Break condition: Unlearned model scores fall outside control distribution (paper: >60% success at all forget set sizes; 100% for classwise unlearning).

### Mechanism 3: Impossibility via Entropy and DP Utility Collapse
- Claim: Deterministic unlearning cannot achieve computational unlearning for entropic learning schemes; DP-based approaches that do achieve it suffer utility collapse to random initialization.
- Mechanism:
  - Entropic learning (e.g., SGD): Deterministic unlearn allows adversary to compute exact expected output and compare; success probability ≥ 1 - p_max (non-negligible)
  - DP-based: Must satisfy δ ≤ negl(λ) and ε ≤ ln(1 + negl(λ)); this makes M_u indistinguishable from M_o, which implies utility ≈ baseline (random init)
- Core assumption: Learning algorithms like SGD have non-trivial Shannon entropy (>1 bit) across different random seeds.
- Evidence anchors:
  - [abstract]: "Theoretical analysis reveals that deterministic unlearning methods cannot achieve computational unlearning, and using differential privacy for black-box computational unlearning leads to utility collapse."
  - [section 5.1, Theorems 18-19]: Impossibility proofs for deterministic unlearning
  - [section 5.2, Theorem 26, Corollary 27]: DP utility collapse proof
  - [corpus]: Corpus papers don't directly address these impossibility results; focus remains on heuristic improvements
- Break condition: Any deterministic unlearning method fails; any DP method with meaningful utility fails.

## Foundational Learning

- Concept: **Differential Privacy ((ε, δ)-DP)**
  - Why needed here: Understanding why conventional DP parameters are too loose for computational unlearning and why tightening them destroys utility
  - Quick check question: If ε = 0.1 and an adversary makes 1000 queries, can privacy loss accumulate to distinguishability? (Yes—computational unlearning requires ε ≤ ln(1 + negl(λ)))

- Concept: **Membership Inference Attacks (MIA)**
  - Why needed here: MIA scores are used both to evaluate and to attack unlearning; understanding why minimizing MIA ≠ achieving unlearning
  - Quick check question: If an unlearned model has MIA score 0.01 on the forget set, has it achieved unlearning? (Not necessarily—it may be anomalously low compared to control distribution)

- Concept: **Kullback-Leibler Divergence**
  - Why needed here: KLDScore measures behavioral distance from original model; unlearned models cluster near M_o while controls do not
  - Quick check question: Why does low KL(M_u || M_o) enable distinguishing? (Controls have high KL from M_o; unlearned models don't)

## Architecture Onboarding

- Component map:
  ```
  Computational Unlearning Framework
  ├── Security Game (Definitions 7-8)
  │   ├── Challenger: produces [M_c, M_u] shuffled
  │   └── Adversary: guesses ordering b'
  ├── Learning Scheme (Definition 2)
  │   ├── init(1^λ) → h
  │   ├── learn(h, D) → h
  │   ├── infer(h, d) → R^n
  │   └── unlearn(h, D_f) → h
  └── Distinguishers (Section 4.1)
      ├── MIAScore: MIA on forget set
      └── KLDScore: Σ D_KL(M(x+noise) || M_o(x+noise))
  ```

- Critical path:
  1. Train M_o = learn(init(1^λ), D)
  2. Adversary selects D_f ⊂ D
  3. Compute M_u = unlearn(M_o, D_f) and M_c = learn(init(1^λ), D \ D_f)
  4. Challenger shuffles and provides [M_c, M_u] or [M_u, M_c]
  5. Adversary computes score and decides ordering
  6. Evaluate: success rate vs. 50% baseline

- Design tradeoffs:
  - **White-box vs Black-box**: White-box (parameter access) is strictly stronger; black-box only oracle access
  - **Deterministic vs Randomized unlearning**: Deterministic simpler but provably impossible; randomized necessary but design is open problem
  - **Certified removal bounds vs Indistinguishability**: Bounds (ε, δ) don't guarantee uniform distribution within bound—directional information leaks

- Failure signatures:
  - MIAScore on forget set << control distribution (over-unlearning)
  - KLDScore(M_u, M_o) << KLDScore(M_c, M_o)
  - Adversary success rate > 60% (paper shows 60-100% depending on forget set size)
  - Classwise unlearning: 100% distinguishability (D_f = 5000 for CIFAR-10 class)

- First 3 experiments:
  1. **Baseline distinguishability**: Implement MIAScore and KLDScore on ResNet-18/CIFAR-10; verify >60% success on SSD, Amnesiac, Bad Teacher, CDU
  2. **Forget set size sweep**: Vary |D_f| ∈ {10, 100, 1000}; plot success rate curve (expect increasing with size)
  3. **CDU σ-parameter sensitivity**: Vary σ ∈ {10^-5, ..., 10^-1}; confirm 100% KLDScore distinguishability except near intersection (σ ∈ (0.001, 0.01))

## Open Questions the Paper Calls Out
None

## Limitations
- The impossibility results assume specific properties of entropic learning schemes (like SGD) that may not hold for all ML algorithms
- The cryptographic security game assumes access to the original model, which may be impractical in real-world scenarios
- The paper focuses on image classification models; generalization to other domains (NLP, structured data) remains unclear

## Confidence
- **High confidence**: The experimental demonstration that existing unlearning methods fail to achieve computational unlearning (60-100% success rates)
- **Medium confidence**: The impossibility proofs for deterministic unlearning methods, as they rely on specific assumptions about learning algorithm entropy
- **Medium confidence**: The DP utility collapse result, which follows logically from the proof but may not account for all practical considerations

## Next Checks
1. Test the distinguishers on models from other domains (e.g., NLP, tabular data) to assess generalizability beyond image classification
2. Evaluate whether approximate unlearning methods that sacrifice some utility might achieve computational unlearning with more practical privacy parameters
3. Investigate whether the over-unlearning phenomenon observed in MIAScore can be mitigated through calibration techniques while maintaining forget performance