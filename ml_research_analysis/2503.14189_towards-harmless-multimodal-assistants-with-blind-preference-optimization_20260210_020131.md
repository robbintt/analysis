---
ver: rpa2
title: Towards Harmless Multimodal Assistants with Blind Preference Optimization
arxiv_id: '2503.14189'
source_url: https://arxiv.org/abs/2503.14189
tags:
- safety
- mllms
- preference
- multimodal
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the safety alignment challenge for Multimodal
  Large Language Models (MLLMs), which have shown impressive capabilities but also
  pose significant safety risks. The authors identify two key observations: "modality
  co-defense," where MLLMs exhibit inherent safety through language-to-visual transfer,
  and "modality cheating," where text modality can mislead the model into ignoring
  visual information.'
---

# Towards Harmless Multimodal Assistants with Blind Preference Optimization

## Quick Facts
- arXiv ID: 2503.14189
- Source URL: https://arxiv.org/abs/2503.14189
- Reference count: 27
- This paper proposes Blind Preference Optimization (BPO) to address safety alignment challenges for Multimodal Large Language Models (MLLMs), achieving 45.0% improvement in safety rate over base LLaVA.

## Executive Summary
This paper addresses safety alignment challenges for Multimodal Large Language Models (MLLMs), which exhibit impressive capabilities but also significant safety risks. The authors identify two key phenomena: "modality co-defense" where text-based safety knowledge partially transfers to multimodal inputs, and "modality cheating" where text patterns override visual information in safety decisions. To address these challenges, they construct MMSafe-PO, a high-quality safety preference dataset with multimodal instructions and human-ranked responses, and propose Blind Preference Optimization (BPO). BPO generates "rejected responses" by blinding the model to visual input, encouraging greater attention to visual information. Experiments show BPO significantly improves LLaVA's safety rate by 45.0% compared to the base model and outperforms standard DPO approaches.

## Method Summary
The authors construct MMSafe-PO, a safety preference dataset derived from Anthropic-HH text data through entity recognition, image matching, and instruction rephrasing. They propose Blind Preference Optimization (BPO), which extends DPO by adding a "blind" response generated from text-only input to contrast against the chosen multimodal response. The BPO loss combines standard DPO contrast between chosen and rejected responses with an additional contrast between chosen and blind responses, encouraging the model to properly incorporate visual information in safety decisions. The approach is implemented on LLaVA-1.5 with frozen visual encoder and fine-tuned LLM using specified hyperparameters.

## Key Results
- BPO improves LLaVA safety rate by 45.0% compared to base model
- BPO outperforms DPO (33.8% improvement) on safety benchmarks
- Reduces unsafe rates to 14.5% on MM-SafetyBench and 82.9% on HarmEval
- Both (yw, yl) and (yw, yb) pairs are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Blind Preference Optimization (BPO)
BPO improves MLLM safety by explicitly training the model to distinguish responses that properly incorporate visual information from those that rely solely on text patterns. By generating a "blind" response (yb) from text-only input and contrasting it against the chosen multimodal response (yw) within the DPO loss, the model learns that visual context should influence safety decisions. This creates a debiasing signal against language-only reasoning. The core assumption is that the blind response yb is systematically inferior to yw for safety-relevant decisions because it lacks visual grounding.

### Mechanism 2: Modality Co-defense (Cross-Modal Safety Transfer)
Safety capabilities from text-only training partially transfer to multimodal inputs through shared latent representations. During vision-language alignment pretraining, text tokens and their visual correlates occupy nearby regions in embedding space. This enables text-based safety patterns to activate when related visual content appears, even without explicit multimodal safety training. The core assumption is that the alignment between vision encoder outputs and LLM embedding space preserves semantic relationships that safety training can leverage.

### Mechanism 3: Modality Cheating (Language Bias Override)
MLLMs can be triggered to refuse benign requests when text patterns resemble safety-sensitive language, regardless of visual evidence to the contrary. The LLM backbone inherits strong refusal patterns from text-based safety alignment. When text patterns activate these, they can override weaker visual signals—causing both false positives (refusing safe requests) and false negatives (missing visual-only hazards). The core assumption is that visual encoder-to-LLM projection layers underweight visual features relative to text features in safety-critical decisions.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: BPO extends DPO by adding a third response (blind) to the preference comparison; understanding DPO's reward-to-policy mapping is prerequisite.
  - Quick check question: Can you write the DPO loss and explain why it avoids training a separate reward model?

- Concept: Vision-Language Cross-Modal Alignment
  - Why needed here: Modality co-defense depends on how vision encoders (e.g., CLIP) and LLMs share embedding space; this explains why text safety transfers.
  - Quick check question: In a typical MLLM like LLaVA, how are visual features projected into the LLM's token embedding space?

- Concept: Safety Evaluation Metrics (ASR, USR, Safety Rate)
  - Why needed here: The paper uses multiple metrics across benchmarks; interpreting results requires understanding what each measures.
  - Quick check question: What is the difference between Attack Success Rate (ASR) and UnSafe Rate (USR), and when would they diverge?

## Architecture Onboarding

- Component map:
  Anthropic-HH (text-only) -> Entity Recognition (BERT NER) -> Image Matching (Wikipedia/Google KG) -> Instruction Rephrasing (Qwen-VL) -> Quality Filtering -> MMSafe-PO (5,667 samples)
  MMSafe-PO -> Generate yb (blind response from text-only) -> BPO loss (contrast yw vs. yl AND yw vs. yb) -> Update policy model π*
  Test on MMSafe-PO test set, MM-SafetyBench (ASR), SPA-VL HarmEval (USR)

- Critical path:
  1. Dataset quality (entity-image matching accuracy, rephrasing fluency)
  2. Blind response generation (must differ meaningfully from multimodal response)
  3. BPO training (β hyperparameter, learning rate stability)

- Design tradeoffs:
  - Transformed text data vs. fresh multimodal human annotation: Authors acknowledge transformation is "not as effective" but scalable; human annotation would be higher quality but costly
  - Including (yw, yl) pairs vs. (yw, yb) only: Table 5 shows removing (yw, yl) drops safety rate from 0.89 to 0.61—both pair types are necessary
  - Freezing vision encoder vs. full fine-tuning: Authors freeze visual encoder to preserve pretrained alignment

- Failure signatures:
  - **High safety rate but low pairwise accuracy**: Model learned to refuse everything (overcautious)
  - **Large gap between MMSafe-PO and out-of-domain benchmarks**: Dataset overfitting, poor generalization
  - **Blind responses yb ≈ yw**: BPO signal collapses; check if visual content is actually relevant to query
  - **Training instability with BPO**: β too high or learning rate too aggressive for preference optimization

- First 3 experiments:
  1. **Baseline characterization**: Run LLaVA-1.5 on MMSafe-PO test set; measure Safety Rate, Pairwise Accuracy, and categorize failure modes (co-defense gaps vs. cheating incidents)
  2. **DPO vs. BPO ablation**: Train identical models with DPO and BPO on MMSafe-PO train set; isolate the blind-response contribution by comparing safety rates and analyzing where BPO helps most
  3. **Cross-benchmark generalization**: Evaluate DPO and BPO models on MM-SafetyBench and SPA-VL HarmEval; verify that BPO's improvement transfers beyond the training distribution

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Blind Preference Optimization be effectively adapted to text-only LLMs by removing key words from instructions, similar to how visual input is removed in MLLMs?
  - Basis: Limitations section states BPO could potentially work for LLMs by removing certain words from textual instructions.
  - Why unresolved: Authors proposed BPO specifically for multimodal settings and did not evaluate word-removal adaptation for unimodal LLMs.

- **Open Question 2**: Does the assumption that blind responses (yb) are always inferior to chosen responses (yw) hold universally, or are there cases where removing visual information produces safer outputs?
  - Basis: The BPO formulation assumes "the response yb is inferior to yw due to the absence of visual information" without validating this assumption empirically.
  - Why unresolved: If yb sometimes equals or exceeds yw in safety, the preference signal becomes noisy or counterproductive.

- **Open Question 3**: To what extent does transforming text-only safety data into multimodal format preserve the ecological validity of real-world multimodal safety threats?
  - Basis: Limitations section acknowledges transformed data is "not as effective as collecting instructions from real-world applications and then hiring people to annotate the feedback."
  - Why unresolved: The modality interpretation pipeline may not capture the distribution of genuinely harmful multimodal queries users would pose in practice.

- **Open Question 4**: Does BPO's effectiveness generalize across different MLLM architectures beyond LLaVA-1.5?
  - Basis: Experiments only report BPO results on LLaVA-1.5, despite evaluating multiple MLLM backbones without safety optimization.
  - Why unresolved: Different architectures may have different visual-text alignment properties that affect BPO's effectiveness.

## Limitations
- The core findings are based on a single dataset (MMSafe-PO) derived through automated transformation from text-only safety data, which may not fully capture real-world multimodal safety scenarios.
- The blind response generation mechanism assumes that text-only inputs will consistently produce inferior safety responses compared to multimodal ones, but this may not hold when visual content is irrelevant.
- The evaluation relies heavily on GPT-4V for safety judgment, which introduces potential subjectivity and model-specific biases that could affect the reported safety rates.

## Confidence
**High Confidence**: The mechanism of Blind Preference Optimization is well-specified with clear mathematical formulation and implementation details. The ablation showing both (yw, yl) and (yw, yb) pairs are necessary for optimal performance is well-supported by empirical evidence.

**Medium Confidence**: The modality co-defense and modality cheating phenomena are theoretically plausible and supported by experimental observations, but the underlying mechanisms rely on assumptions about cross-modal alignment that would benefit from additional validation.

**Low Confidence**: The generalizability of results beyond the constructed MMSafe-PO dataset to truly open-domain multimodal interactions remains uncertain. The paper acknowledges that transformed data is "not as effective" as human-annotated multimodal safety data, but does not quantify this gap.

## Next Checks
1. **Cross-dataset generalization test**: Evaluate the BPO-trained model on a completely independent multimodal safety dataset to verify that improvements transfer beyond the MMSafe-PO distribution and are not artifacts of the transformation pipeline.

2. **Modality ablation study**: Systematically vary the strength of visual information in input (e.g., low-quality/noisy images, unrelated images) to quantify the exact conditions under which modality co-defense holds and when modality cheating occurs.

3. **Alternative blind response generation**: Compare BPO's performance when using different methods for generating yb (e.g., zero-shot vs. few-shot prompting, different reference models) to isolate whether the specific generation method or the blind-vs-multimodal contrast itself drives the safety improvements.