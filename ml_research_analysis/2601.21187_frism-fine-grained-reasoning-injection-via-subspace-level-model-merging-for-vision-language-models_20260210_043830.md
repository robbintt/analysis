---
ver: rpa2
title: 'FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for
  Vision-Language Models'
arxiv_id: '2601.21187'
source_url: https://arxiv.org/abs/2601.21187
tags:
- reasoning
- merging
- frism
- visual
- capabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently enhancing reasoning
  capabilities in Vision-Language Models (VLMs) by merging them with Large Reasoning
  Models (LRMs), while preserving their original visual perception abilities. Existing
  methods operate at a coarse layer level, leading to a trade-off between reasoning
  injection and visual capability preservation.
---

# FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models

## Quick Facts
- arXiv ID: 2601.21187
- Source URL: https://arxiv.org/abs/2601.21187
- Reference count: 40
- Achieves 2.4% improvement in reasoning score while maintaining visual perception on Qwen2.5-VL-32B-Instruct

## Executive Summary
This paper introduces FRISM, a fine-grained reasoning injection framework that merges Large Reasoning Models (LRMs) with Vision-Language Models (VLMs) at the subspace level to enhance reasoning capabilities while preserving visual perception. Traditional model merging approaches operate at the coarse layer level, leading to suboptimal trade-offs between reasoning injection and visual capability preservation. FRISM addresses this limitation by decomposing LRM task vectors via Singular Value Decomposition (SVD) and adaptively tuning scaling coefficients of each subspace. The framework employs a label-free self-distillation learning strategy with dual-objective optimization to balance reasoning maximization and visual preservation using common vision-language perception datasets. Extensive experiments demonstrate that FRISM consistently achieves state-of-the-art performance across diverse visual reasoning benchmarks.

## Method Summary
FRISM proposes a novel approach to enhance VLMs with reasoning capabilities by merging them with LRMs at the subspace level rather than the traditional layer level. The method begins by decomposing LRM task vectors using Singular Value Decomposition (SVD), which breaks down the reasoning capabilities into orthogonal subspaces. Each subspace is then assigned an adaptive scaling coefficient that controls the degree of reasoning injection from that specific component. This fine-grained approach allows for precise control over which reasoning capabilities are transferred while minimizing interference with the VLM's visual perception abilities. To train the merged model, FRISM employs a label-free self-distillation strategy that simultaneously optimizes for both enhanced reasoning performance and preserved visual capabilities. The framework uses common vision-language perception datasets as training data, avoiding the need for expensive reasoning-labeled datasets. Dual-objective optimization balances the trade-off between maximizing reasoning capabilities and maintaining visual perception quality.

## Key Results
- Achieves 2.4% improvement in reasoning score while maintaining visual perception on Qwen2.5-VL-32B-Instruct
- Consistently outperforms existing merging approaches across diverse visual reasoning benchmarks
- Successfully preserves original VLM visual capabilities while enhancing reasoning performance
- Demonstrates effectiveness across multiple VLM architectures and reasoning task types

## Why This Works (Mechanism)
The framework works by leveraging the mathematical properties of SVD decomposition to isolate distinct reasoning capabilities into orthogonal subspaces. This decomposition allows for independent control over each reasoning component through adaptive scaling coefficients, enabling precise tuning of the reasoning injection process. The label-free self-distillation approach is effective because it uses the inherent structure of vision-language perception datasets to guide the optimization process, eliminating the need for explicit reasoning labels while still achieving balanced performance. The dual-objective optimization ensures that neither reasoning enhancement nor visual preservation dominates the training process, leading to a more stable and effective merging outcome.

## Foundational Learning

**Singular Value Decomposition (SVD)**: A matrix factorization technique that decomposes a matrix into orthogonal components. Why needed: Enables decomposition of reasoning capabilities into independent subspaces for fine-grained control. Quick check: Verify SVD decomposition produces orthogonal components by checking dot products.

**Model Merging**: The process of combining two pre-trained models to inherit capabilities from both. Why needed: Allows VLMs to acquire reasoning capabilities from LRMs without full retraining. Quick check: Confirm merged model retains both visual and reasoning capabilities through benchmark testing.

**Self-Distillation**: A training approach where a model learns from its own predictions or outputs. Why needed: Eliminates need for expensive labeled reasoning datasets while maintaining performance. Quick check: Monitor training stability and convergence compared to supervised approaches.

**Dual-Objective Optimization**: Simultaneously optimizing for multiple, potentially competing objectives. Why needed: Balances reasoning enhancement with visual capability preservation. Quick check: Verify both objectives show improvement without significant trade-offs.

**Adaptive Scaling Coefficients**: Dynamic parameters that control the contribution of each subspace component. Why needed: Enables fine-grained control over reasoning injection intensity per capability. Quick check: Test sensitivity of final performance to scaling coefficient values.

## Architecture Onboarding

Component map: VLM -> FRISM Layer -> LRM, where FRISM Layer performs SVD decomposition and adaptive scaling

Critical path: Input image/text -> VLM processing -> FRISM merging layer (SVD + scaling) -> Reasoning enhancement -> Output

Design tradeoffs: Fine-grained subspace merging vs computational overhead, label-free training vs potential suboptimality, dual-objective balance vs complexity

Failure signatures: Visual capability degradation indicates excessive reasoning injection, reasoning underperformance suggests insufficient subspace contribution, training instability may result from poor SVD decomposition quality

First experiments:
1. Test basic SVD decomposition quality on LRM task vectors and verify orthogonality
2. Validate adaptive scaling coefficient tuning impact on reasoning injection effectiveness
3. Verify dual-objective optimization maintains visual capabilities while enhancing reasoning

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations

- SVD decomposition stability and generalizability across different LRM architectures and tasks remains unclear
- Label-free self-distillation may lead to suboptimal optimization without direct reasoning supervision
- Effectiveness may be constrained by quality and diversity of common vision-language perception datasets
- Computational overhead of fine-grained merging process compared to coarse-grained methods not thoroughly explored

## Confidence

- Effectiveness of subspace-level merging in preserving visual capabilities while enhancing reasoning: Medium confidence
- Proposed self-distillation learning strategy achieving optimal balance: Medium confidence
- Generalization of FRISM across diverse VLMs and reasoning tasks: Low confidence

## Next Checks

1. Conduct ablation studies to isolate the impact of SVD decomposition quality on final performance
2. Test FRISM on a broader range of LRM architectures and reasoning task types
3. Perform computational efficiency analysis comparing FRISM with existing merging approaches