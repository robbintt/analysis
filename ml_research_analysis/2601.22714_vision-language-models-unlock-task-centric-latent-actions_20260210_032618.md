---
ver: rpa2
title: Vision-Language Models Unlock Task-Centric Latent Actions
arxiv_id: '2601.22714'
source_url: https://arxiv.org/abs/2601.22714
tags:
- task
- action
- latent
- robot
- distractors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that vision-language models (VLMs) can provide
  task-centric promptable representations that enable latent action models (LAMs)
  to recover ground-truth actions even in the presence of action-correlated distractors.
  By using VLM-generated representations as targets during LAM training, the approach
  achieves up to a six-fold increase in downstream success rates on the Distracting
  MetaWorld benchmark, effectively restoring performance levels of LAMs trained without
  distractors.
---

# Vision-Language Models Unlock Task-Centric Latent Actions

## Quick Facts
- arXiv ID: 2601.22714
- Source URL: https://arxiv.org/abs/2601.22714
- Authors: Alexander Nikulin; Ilya Zisman; Albina Klepach; Denis Tarasov; Alexander Derevyagin; Andrei Polubarov; Lyubaykin Nikita; Vladislav Kurenkov
- Reference count: 40
- Primary result: Vision-Language Models (VLMs) provide promptable representations that enable Latent Action Models (LAMs) to recover ground-truth actions in the presence of action-correlated distractors.

## Executive Summary
Latent Action Models (LAMs) excel at learning robot actions from observation-only data, but fail catastrophically when observations contain action-correlated distractors like moving humans in the background. This work demonstrates that the failure stems from noisy targets in the Forward Dynamics Model (FDM) rather than architectural limitations. By using VLM-generated task-centric representations as clean targets for the FDM, the approach achieves up to six-fold improvements in downstream success rates on the Distracting MetaWorld benchmark, effectively restoring performance to levels without distractors.

The key insight is that VLMs can provide "promptable representations" that separate controllable changes from noise through language conditioning. When instructed to focus on the robot arm and ignore background features, VLMs generate embeddings that emphasize task-relevant dynamics. A comprehensive benchmark of 29+ VLMs reveals substantial variation in representation quality, with Molmo consistently outperforming others. The study also shows that simply instructing VLMs to ignore distractors can significantly improve latent action quality, and that VLM-based representations substantially outperform self-supervised alternatives like DINOv2 and CLIP.

## Method Summary
The method involves training a three-stage pipeline where VLM-generated representations replace raw pixel observations as targets for the FDM in LAPO (Latent Action Planning with Observations). First, expert trajectories are collected in MetaWorld MT10 with DAVIS video distractors in the background. A pre-trained VLM (Molmo-7B) encodes each observation with a task-specific prompt ("Do not describe background features. Focus on the robot arm and the [task-obj]."), extracting embeddings from the next-to-last layer using mean pooling. The LAPO model (IDM + FDM) is trained to predict these VLM embeddings rather than pixel observations. After pre-training, a behavior cloning agent is trained on the latent actions, followed by training a small action decoder on less than 1% of labeled trajectories. The final policy is evaluated using success rate metrics with stratified bootstrapping.

## Key Results
- Achieved up to six-fold increase in downstream success rates on Distracting MetaWorld benchmark
- VLM-based representations restored performance to levels of LAMs trained without distractors
- Molmo consistently outperformed other VLMs, with language-conditioned models substantially better than self-supervised alternatives like DINOv2 and CLIP
- Simple prompt engineering ("ignore background features") significantly improved latent action quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAM failures with action-correlated distractors arise from noisy pixel-space targets in the FDM, not architectural flaws.
- Mechanism: Standard LAMs use raw pixels as FDM targets, forcing the model to encode uncontrollable distractor dynamics into the latent action to minimize reconstruction loss. Replacing raw pixels with cleaner VLM embeddings that semantically preserve task-relevant changes filters out distractor noise.
- Core assumption: VLM embedding spaces maintain linearity and consistency for task-relevant visual changes while discarding irrelevant details.
- Evidence anchors: [abstract] "fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions"; [section 5] "limitation of LAMs... arises entirely from the poor FDM target... rather than from flaws in the overall idea or architecture."

### Mechanism 2
- Claim: VLMs provide task-centric "promptable representations" that separate controllable changes from noise through language conditioning.
- Mechanism: A VLM processes an observation with a text prompt specifying the task-relevant entity, generating an embedding biased toward that entity's state. Using this as the FDM target forces the LAM to learn latent actions explaining only the prompted dynamics.
- Core assumption: VLMs possess sufficient common-sense reasoning to attend to the requested object and isolate its features even with complex backgrounds.
- Evidence anchors: [abstract] "...utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise..."; [section 6] "Modern VLMs easily identify the robotic arm location... and describe it in detail, even in the presence of background noise."

### Mechanism 3
- Claim: Language-based conditioning is essential for filtering action-irrelevant features, outperforming self-supervised visual-only methods.
- Mechanism: Self-supervised models capture all salient visual features including moving distractors, while language-conditioned VLMs can dynamically re-weight attention to focus only on features specified in natural language.
- Core assumption: The ability to condition representations on linguistic task descriptions is the key differentiator for effective filtering.
- Evidence anchors: [section 6] "DINOv2 and CLIP results highlight the vital importance of language conditioning... without language conditioning there is no guarantee that these objects are the ones that are controllable"; [section 6] "OTTER... uses the same CLIP model but applies simple training-free filtering using text CLIP embeddings. This small modification significantly improves latent action quality..."

## Foundational Learning

- **Latent Action Models (LAMs)**
  - Why needed here: Core system being improved - must understand LAMs as models that compress observation transitions into compact latent actions using IDM and FDM.
  - Quick check question: What specific loss function is optimized to train the joint IDM and FDM, and what role does the information bottleneck play? (Hint: See Page 2, Section 2).

- **Promptable Representations**
  - Why needed here: Key intervention - must understand how to extract task-centric embeddings from VLMs using text prompts.
  - Quick check question: Based on the paper's experiments, which specific prompt, embedding source, and aggregation method yielded the best results? (Hint: See Page 6, Section 6).

- **Action-Correlated Distractors**
  - Why needed here: Problem being solved - must grasp why this specific noise type is so detrimental to LAMs.
  - Quick check question: Why does a moving human in the background cause standard LAPO model's action probe to degrade so severely? (Hint: See Figure 5 and Page 5 discussion).

## Architecture Onboarding

- **Component map:** Observation + Prompt → VLM Encoder → VLM Embedding → FDM → Next VLM Embedding; Observation Pair → IDM → Latent Action z; Latent Action z + Current VLM Embedding → FDM → Next VLM Embedding; Latent Action z → Action Decoder → Ground-truth Actions

- **Critical path:**
  1. **Prompt Engineering & VLM Selection:** Most critical step - selecting wrong VLM or poor prompt will fail to filter distractors.
  2. **Embedding Extraction:** Correctly extracting embedding from VLM's internal layers without gradients.
  3. **LAM Training:** Training IDM/FDM pair to predict VLM embedding, not raw pixel image.

- **Design tradeoffs:**
  - **VLM Choice:** Larger/newer models may perform worse than older ones; pre-training data may be more important than model size.
  - **Prompting vs. Segmentation:** Segmentation insufficient for non-spatial noise like lighting changes; semantic VLM embedding preferred.
  - **Latent Dimension:** Smaller dimension (e.g., 16) acts as stronger information bottleneck and can improve success rates with clean VLM targets.

- **Failure signatures:**
  - High Action Probe MSE: Latent action fails to linearly predict ground-truth actions.
  - Near-Zero Downstream Success Rate: Policy trained on these latent actions cannot solve the task.
  - VLM "Blindness": VLM fails to attend to robot arm or task object in presence of distractors.

- **First 3 experiments:**
  1. **Reproduce Ideal Target Motivation:** Create twin dataset (with/without distractors), train standard LAPO using clean observation as FDM target, verify performance restoration.
  2. **VLM Prompting Ablation:** Test multiple VLMs with fixed prompt on small data subset, evaluate latent action quality using linear action probe.
  3. **Benchmark Best VLM Configuration:** Train full LAPO+VLM pipeline with best-performing VLM and hyperparameters, measure final success rate after fine-tuning action decoder.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the utility of promptable representations for latent action learning transfer to large-scale, real-world robotic datasets like Open X-Embodiment?
- Basis in paper: [explicit] Section A states the authors rely on MetaWorld and "do not extend our analysis to large VLAs and datasets, such as Open-X."
- Why unresolved: Encoding large datasets with large VLMs is computationally prohibitive, limiting current experiments to simulated benchmarks.
- What evidence would resolve it: Evaluating the LAPO+VLM pipeline on the Open X-Embodiment dataset and measuring downstream policy success rates.

### Open Question 2
- Question: What specific pre-training data properties or architectural features cause certain VLMs (like Molmo) to generate higher-quality promptable representations than newer models?
- Basis in paper: [explicit] Section 6 asks "Why does Molmo perform so well?" and notes that "more recent VLMs may perform worse than older ones."
- Why unresolved: The authors hypothesize data is the key driver but rely on indirect comparisons rather than controlled isolation of variables.
- What evidence would resolve it: Controlled ablation studies re-training VLMs on varied data mixtures to isolate factors driving representation robustness.

### Open Question 3
- Question: Why do specialized embedding VLMs fail to provide effective targets for latent action models compared to general-purpose VLMs?
- Basis in paper: [explicit] Section 6 notes that despite their design objective, embedding VLMs "do not actually encode only prompt-specific visual information" and fail to deliver benefits.
- Why unresolved: The paper observes the performance gap but lacks analysis of why embeddings lack minimality.
- What evidence would resolve it: Probing embedding spaces of models like VLM2Vec to quantify retention of distractor information versus task-relevant features.

## Limitations
- The approach depends heavily on VLM's ability to correctly identify and focus on task-relevant objects despite significant visual noise, which may not generalize to scenarios where robot and distractor have similar visual features.
- Computational cost of running large VLMs (like Molmo-7B) at inference time for every observation is a practical limitation not addressed.
- Improvements are measured on specific benchmark with scripted expert demonstrations, leaving open questions about robustness to different data distributions or online learning scenarios.

## Confidence

- **High Confidence:** The core mechanism of using VLM-generated representations as cleaner targets for the FDM is well-demonstrated and quantitative improvements on Distracting MetaWorld benchmark are significant and reproducible.
- **Medium Confidence:** The claim that any VLM can provide task-centric representations if properly prompted. While Molmo and OTTER show strong results, performance of other VLMs varies substantially.
- **Low Confidence:** The assertion that language conditioning is the only reason VLM representations outperform self-supervised models like DINOv2. Paper doesn't rule out other factors.

## Next Checks

1. **Robustness to VLM Failures:** Conduct ablation study where VLM is intentionally "blinded" (e.g., using corrupted prompt or VLM known to fail) to quantify performance degradation and validate improvement is due to VLM's reasoning.

2. **Cross-Domain Generalization:** Test best VLM configuration (Molmo + optimal prompt) on different robotics dataset or non-robotic sequential decision-making task with distractors to evaluate generalization beyond MetaWorld+DAVIS setup.

3. **Scaling to Real-World Data:** Evaluate the LAPO+VLM pipeline on Open X-Embodiment dataset or other large-scale real-world robotic dataset to test transfer to practical applications and assess computational feasibility at scale.