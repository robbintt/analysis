---
ver: rpa2
title: Billion-scale Similarity Search Using a Hybrid Indexing Approach with Advanced
  Filtering
arxiv_id: '2501.13442'
source_url: https://arxiv.org/abs/2501.13442
tags:
- search
- filtering
- vector
- vectors
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient similarity search
  with complex filtering on billion-scale datasets, optimized for CPU inference. The
  proposed method extends the classical IVF-Flat index structure to integrate multi-dimensional
  filters, combining dense embeddings with discrete filtering attributes.
---

# Billion-scale Similarity Search Using a Hybrid Indexing Approach with Advanced Filtering

## Quick Facts
- arXiv ID: 2501.13442
- Source URL: https://arxiv.org/abs/2501.13442
- Reference count: 0
- Billion-scale similarity search with complex filtering, optimized for CPU inference

## Executive Summary
This paper addresses the challenge of efficient similarity search with complex filtering on billion-scale datasets, optimized for CPU inference. The proposed method extends the classical IVF-Flat index structure to integrate multi-dimensional filters, combining dense embeddings with discrete filtering attributes. Through a case study on the LAION-5B dataset, the method demonstrated practical applicability and efficiency, achieving search times of approximately 1.428 seconds for 1 billion vectors using parallel processing. The proposed solution offers a cost-effective, scalable approach for large-scale similarity search tasks without requiring GPU resources during inference.

## Method Summary
The method constructs hybrid vectors by concatenating core embeddings with encoded attribute vectors, enabling integrated filtering within the IVF-Flat index structure. Clustering is performed only on the core embeddings, while the inverted lists store the combined hybrid vectors. The system employs disk-resident index storage with selective RAM loading, where only the inverted lists corresponding to the nearest centroids are loaded during queries. This approach enables fast retrieval in high-dimensional spaces while supporting a wide range of filtering conditions through efficient in-memory predicate evaluation.

## Key Results
- Achieved search times of approximately 1.428 seconds for 1 billion vectors using 12 CPU threads
- Successfully integrated complex filtering conditions directly into the vector index structure
- Demonstrated practical applicability on the LAION-5B dataset with 768-dimensional CLIP embeddings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating discrete filtering attributes directly into the vector index structure enables complex multi-dimensional filtering without maintaining separate auxiliary indices.
- **Mechanism:** The system constructs a "hybrid vector" by concatenating the dense core embedding ($x_i$) with an encoded attribute vector ($a_i$). While clustering is performed only on the core embedding, the resulting inverted lists store the combined hybrid vectors. This colocation allows the search runtime to apply filter predicates directly on the list elements in memory before or during distance computation.
- **Core assumption:** The filtering attributes can be encoded into a fixed-size integer format that allows for efficient bitwise or range comparisons without significantly bloating the memory footprint or disk I/O.
- **Evidence anchors:**
  - [Section 4.1]: "construct the hybrid vectors as $h_i = [x_i || a_i]$... combining both the embeddings and the filtering attributes."
  - [Section 4.4]: "Apply the filtering conditions F... using an in-memory structure for the filters, discarding any vectors that do not satisfy the specified constraints."
- **Break condition:** If attribute cardinality is high or data types are complex (e.g., free text), the encoding size explodes, degrading the "flat" storage efficiency and increasing disk bandwidth requirements.

### Mechanism 2
- **Claim:** A disk-resident index with selective RAM loading allows billion-scale search on resource-constrained CPU hardware.
- **Mechanism:** The index separates the small set of centroids (kept in RAM) from the large inverted lists (kept on disk). During a query, only the inverted lists corresponding to the nearest $T$ centroids are loaded into memory. This "dynamic memory management" prevents the need to hold the entire 1B-vector dataset in RAM.
- **Core assumption:** The dataset size exceeds available RAM, but the subset of data relevant to any single query (the $T$ nearest lists) fits within available RAM.
- **Evidence anchors:**
  - [Abstract]: "disk-based dynamic memory management strategy... selectively loads only relevant index parts into RAM."
  - [Section 4.4]: "dynamic, memory-efficient loading strategy... only the vectors from the T most relevant inverted lists... are loaded into memory."
- **Break condition:** If the user sets $T$ (number of centroids to probe) too high in an attempt to increase recall, the volume of data loaded from disk will exceed RAM capacity or I/O bandwidth, causing thrashing or crashes.

### Mechanism 3
- **Claim:** The search bottleneck shifts from vector distance computation to filtering overhead, making CPU parallelization the primary performance lever.
- **Mechanism:** By using an IVF-Flat approach, the algorithm reduces the search space, but the remaining candidate vectors must undergo rigorous filtering. The implementation utilizes multi-threaded BLAS routines to parallelize the filtering and distance computation steps across CPU cores.
- **Core assumption:** The CPU has multiple cores and sufficient memory bandwidth to support parallel processing of the loaded inverted lists.
- **Evidence anchors:**
  - [Section 5.3]: "By utilizing this parallel processing setup, we were able to reduce the search time to approximately 1.428 s."
  - [Section 5.3, Table 2]: Shows "Filtering" taking 1.090s out of 1.428s total time, identifying it as the dominant cost.
- **Break condition:** On single-threaded or low-core-count machines, the filtering latency will scale linearly with dataset size, rendering the "billion-scale" claim impractical for real-time use cases.

## Foundational Learning

- **Concept: Inverted File Index (IVF)**
  - **Why needed here:** This is the core partitioning strategy. You must understand how Voronoi cells divide vector space to understand how the search prunes 99.9% of the data before looking at it.
  - **Quick check question:** How does increasing the number of centroids ($K$) affect the granularity of search and the size of the inverted lists?

- **Concept: Hybrid Vector Construction**
  - **Why needed here:** This is the paper's specific contribution. You need to understand how metadata is serialized into the vector to debug why a filter isn't working or why index size has increased.
  - **Quick check question:** If you have a categorical attribute with 10,000 unique values, how would you encode it into the fixed-size attribute vector without breaking the distance metric?

- **Concept: Disk-based Memory Management**
  - **Why needed here:** The system relies on the disparity between disk capacity and RAM speed. Understanding this trade-off is required to configure the $T$ parameter correctly.
  - **Quick check question:** What happens to search latency if the combined size of the top $T$ inverted lists exceeds the system's available RAM?

## Architecture Onboarding

- **Component map:** Raw vectors + Metadata -> Hybridizer (Concatenation) -> Index Builder (K-Means on Core) -> Centroids (RAM) + Inverted Lists (Disk) -> Query -> Centroid Selector (RAM) -> List Loader (Disk -> RAM) -> Filter Engine (RAM) -> Distance Computer (CPU/BLAS) -> Top-k Results

- **Critical path:** The query latency is dominated by the **List Loader** (I/O) and **Filter Engine** (CPU). The paper notes filtering alone takes ~76% of the total query time (1.09s of 1.4s).

- **Design tradeoffs:**
  - **Recall vs. Latency:** Raising $T$ (probes) increases recall but linearly increases disk I/O and filtering time.
  - **Index Time vs. Search Quality:** Using MiniBatchKMeans is faster but produces lower-quality clusters than standard K-Means, potentially reducing search accuracy.
  - **RAM vs. Scale:** The system trades RAM for Disk I/O. It requires minimal RAM (only centroids + active lists) but demands fast disk (NVMe) to minimize the "List Loader" delay.

- **Failure signatures:**
  - **High Latency (>5s):** Likely caused by setting $T$ too high, forcing excessive disk reads or CPU saturation during filtering.
  - **Low Recall:** Likely $K$ is too low (clusters too large) or $T$ is too low (missing relevant clusters).
  - **OOM Crash:** The "Filtering attributes" in-memory structure is too large for the available RAM.

- **First 3 experiments:**
  1. **Baseline Latency Test:** Run search with $T=1$ vs $T=7$ vs $T=20$ to measure the latency/recall curve on your specific hardware.
  2. **Filter Selectivity Impact:** Benchmark queries with no filter vs. high-selectivity filter (few matches) vs. low-selectivity (many matches) to isolate the filtering overhead claimed in Section 5.3.
  3. **Index Build Benchmark:** Compare MiniBatchKMeans vs. standard K-Means on a 1M vector subset to quantify the construction time vs. cluster quality tradeoff.

## Open Questions the Paper Calls Out

- **Adaptive Parameter Selection:** Can adaptive methods for selecting centroids (K) and search breadth (T) optimize the trade-off between speed and accuracy better than static heuristics?
  - **Basis in paper:** [explicit] Section 4.3 states, "Future work could explore adaptive methods for determining K and T based on dataset characteristics, query patterns, and filter selectivity."
  - **Why unresolved:** The current implementation relies on fixed heuristics (e.g., $K=\sqrt{N}$) and static values for $T$, which may be suboptimal for varying query loads.
  - **What evidence would resolve it:** Benchmarks comparing static heuristics against adaptive algorithms that adjust parameters based on real-time filter selectivity.

- **Concurrent Search Optimization:** How can intelligent caching strategies mitigate I/O bottlenecks when handling concurrent searches in the disk-based index?
  - **Basis in paper:** [explicit] Section 5.4 notes that "Concurrent searches could also become a bottleneck" and suggests exploring "intelligent caching" and "parallel access improvements."
  - **Why unresolved:** The dynamic loading strategy targets sequential access; loading different disk-resident index parts simultaneously for concurrent users creates contention.
  - **What evidence would resolve it:** Throughput and latency metrics under high-concurrency workloads comparing the current on-demand loading against predictive caching mechanisms.

- **Attribute Compression:** Can attribute compression techniques be applied to the hybrid vectors to reduce storage overhead without significantly degrading filter evaluation speed?
  - **Basis in paper:** [explicit] Section 6 identifies "attribute compression methods" as a specific avenue for future research.
  - **Why unresolved:** The case study utilized fixed-size integers (stored as float16), which prioritizes computational speed but may be storage-inefficient for billions of vectors.
  - **What evidence would resolve it:** Measurements of index size reduction and query latency when applying compression schemes (e.g., bit-packing) to the discrete filtering attributes.

## Limitations
- Heavy dependence on disk I/O and CPU filtering overhead creates scalability bottleneck for real-time applications
- Performance highly sensitive to T parameter, with improper tuning potentially leading to excessive latency or insufficient recall
- Assumes filter attributes can be efficiently encoded as fixed-size integers, which may not hold for complex data types

## Confidence
- **High Confidence:** The hybrid vector construction mechanism and disk-based memory management strategy are well-supported by experimental evidence
- **Medium Confidence:** The claimed search time of 1.428 seconds is supported by specific hardware configuration but may not generalize to different setups
- **Low Confidence:** The long-term scalability of the filtering approach for datasets with complex or high-cardinality attributes remains uncertain

## Next Checks
1. Benchmark the filtering overhead on datasets with varying attribute cardinalities (10, 100, 1000 unique values) to determine the encoding size limit
2. Test search latency on different hardware configurations (4-core, 8-core, 16-core CPUs) to validate the claimed parallelization benefits
3. Measure recall degradation when applying multi-dimensional filters with varying selectivity rates to quantify the impact on search quality