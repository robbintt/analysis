---
ver: rpa2
title: Reasoning-Aware Proxy Reward Model using Process Mining
arxiv_id: '2510.25065'
source_url: https://arxiv.org/abs/2510.25065
tags:
- uni00000013
- reasoning
- process
- reward
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TACReward, a reasoning-aware proxy reward
  model that uses process mining techniques to evaluate the structural alignment between
  policy and teacher reasoning traces in sparse-reward reinforcement learning for
  language models. By formalizing reasoning steps into structured event logs and measuring
  conformance via fitness and precision, TACReward provides a scalar reward in [0,1]
  that acts as a proxy for step-level reasoning quality without requiring human annotations
  or architectural changes.
---

# Reasoning-Aware Proxy Reward Model using Process Mining

## Quick Facts
- arXiv ID: 2510.25065
- Source URL: https://arxiv.org/abs/2510.25065
- Reference count: 40
- Key outcome: TACReward achieves 89.2% average relative accuracy improvement in mathematical reasoning tasks by aligning policy and teacher reasoning traces via process mining

## Executive Summary
TACReward introduces a reasoning-aware proxy reward model that leverages process mining techniques to evaluate structural alignment between policy and teacher reasoning traces in sparse-reward reinforcement learning for language models. The method formalizes reasoning steps into structured event logs and measures conformance via fitness and precision metrics, providing a scalar reward that acts as a proxy for step-level reasoning quality. Experiments on multiple mathematical reasoning benchmarks demonstrate consistent performance gains when integrated with GSPO, outperforming several state-of-the-art RL baselines while encouraging improved logical maturity in reasoning processes.

## Method Summary
TACReward integrates with sparse-reward policy gradient methods by generating G candidate responses from the policy model, extracting reasoning traces using a 20-activity taxonomy via DeepSeek-V3.2, and generating reference traces from DeepSeek-R1 teacher. Each policy trace is converted into a process model using Inductive Miner, and alignment costs are computed against the reference trace to determine fitness (completeness) and precision (selectivity). The harmonic mean of these metrics forms the TACReward, which is added to outcome and format rewards. The method is implemented within the TRL framework using AdamW optimization, DeepSpeed ZeRO-3, and batch processing across 4 H200 GPUs.

## Key Results
- GSPO + TAC achieves 89.2% average relative accuracy improvement across mathematical reasoning benchmarks
- Performance gains observed on MATH-500, MINERVA, OlympiadBench, LiveMathBench, KICE, and AIME 2024/2025
- TACReward effectively encourages improved logical maturity in reasoning processes compared to standard RL approaches
- Integration with GSPO shows superior performance compared to PPO and other sparse-reward methods

## Why This Works (Mechanism)

### Mechanism 1: Structural Alignment via Process Mining
- **Claim:** Formalizing reasoning as structured event logs enables meaningful comparison of reasoning quality between policy and teacher models.
- **Mechanism:** Raw reasoning responses are converted into traces using a 20-activity taxonomy (e.g., "Formulate Strategy," "Apply Known Formula"). Each trace becomes a process model via Inductive Miner. Alignment cost quantifies stepwise deviations (log-only moves = missing steps; model-only moves = spurious steps).
- **Core assumption:** The taxonomy captures meaningful reasoning structure, and surface-level token differences are less important than activity-level transitions.
- **Evidence anchors:**
  - [abstract] "TACReward aggregates stepwise structural deviations between teacher and policy reasoning using process mining techniques"
  - [Section 3.2] Alignment definition with move costs δ_m
  - [Section D.2] Direct token edit distance "cannot reliably capture structural properties"
- **Break condition:** If taxonomy labels don't map meaningfully to actual reasoning behavior, alignment costs become noise.

### Mechanism 2: Fitness-Precision Tradeoff as Proxy Reward
- **Claim:** Combining fitness (completeness) and precision (selectivity) as F1-score provides a balanced proxy for reasoning quality.
- **Mechanism:** Fitness penalizes missing essential steps (log-only moves); precision penalizes excessive or unjustified behavior (model-only moves). Their harmonic mean (Equation 14) prevents either trivial solution.
- **Core assumption:** Teacher model traces represent "logically more mature" reasoning that policy should align toward, even if imperfect.
- **Evidence anchors:**
  - [Section 3.3] Equations 12-14 defining s_fit, s_prec, and r_TAC
  - [Section 4.2, Figure 4] Fitness-only leads to longer chains; precision-only leads to conservative short responses; both yields balance
  - [Section D.3] Reference model need not be correct, only "more mature"
- **Break condition:** If teacher traces are structurally incoherent, conformance rewards optimize toward a flawed target.

### Mechanism 3: Sequence-Level Granularity Match with GSPO
- **Claim:** TACReward integrates most effectively with GSPO because both operate at sequence-level granularity.
- **Mechanism:** GSPO defines importance ratios at the sequence level (one response = one optimization unit). TACReward aggregates step-level deviations into a single conformance score per response. This alignment enables coherent credit assignment.
- **Core assumption:** Sequence-level reward signals are more stable than token-level when combined with sequence-level policy objectives.
- **Evidence anchors:**
  - [Section 4.2] "GSPO + TAC increases Avg.* from 17.2 to 32.5 (+89.2%)"
  - [Section 4.2] "Granularity match between optimization objective and reward signal"
  - [Section 4.2] PPO showed limited gains; variance from proxy rewards may destabilize token-level objectives
- **Break condition:** If sampled responses G=1 (no group comparison), group-relative advantage computation degrades.

## Foundational Learning

- **Concept:** Process Mining (Event Logs, Traces, Conformance Checking)
  - **Why needed here:** Core representational framework. Must understand how activities map to events, traces to process models, and alignment costs to deviations.
  - **Quick check question:** Given a trace ⟨A, B, C⟩ and a model allowing ⟨A, B⟩ or ⟨A, C⟩, what is the alignment cost if w_L=w_M=1?

- **Concept:** Sparse Reward Policy Gradient Methods (GRPO, GSPO, RLOO)
  - **Why needed here:** TACReward is designed as a drop-in addition to these frameworks. Understanding group-relative advantages and baseline computation is essential for integration.
  - **Quick check question:** Why does GSPO use sequence-level importance ratios while GRPO uses token-level?

- **Concept:** Inductive Miner Algorithm
  - **Why needed here:** Used to convert single traces into process models (Petri nets). Understanding what structures it can/cannot represent informs limits of the approach.
  - **Quick check question:** What process structures can Inductive Miner discover that Alpha Miner cannot?

## Architecture Onboarding

- **Component map:** Query → Policy responses → Trace extraction → Process models → Alignment with reference trace → r_TAC → Add to outcome reward → Policy update
- **Critical path:** Query → Policy responses → Trace extraction → Process models → Alignment with reference trace → r_TAC → Add to outcome reward → Policy update
- **Design tradeoffs:**
  - **Overhead:** Per-step training time increases 5-7x (Table 6: GSPO 10.9s → 67.2s)
  - **Taxonomy dependence:** Fixed 20-activity set; domain transfer requires new taxonomy (Section D.4)
  - **API dependency:** Trace extraction uses external model; future work aims to reduce this
- **Failure signatures:**
  - **Stagnant rewards:** Reference traces may be structurally similar to policy traces → low gradient signal
  - **Over-alignment:** Policy mimics teacher structure without improving correctness → high r_TAC, low r_acc
  - **Taxonomy mismatch:** Extracted activities don't match actual reasoning → noisy alignments
- **First 3 experiments:**
  1. **Validate trace extraction quality:** Manually inspect 20-50 policy traces vs. extracted event logs; verify activity labels match reasoning intent
  2. **Ablate fitness vs. precision:** Run with only fitness, only precision, and both (Table 4); confirm fitness-only increases length, precision-only decreases it
  3. **Test integration with different G values:** Compare G=1 vs. G=4 vs. G=8; verify group-relative advantage stabilizes as G increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational overhead of TACReward be substantially reduced while maintaining performance gains?
- Basis in paper: [explicit] From Conclusion: "Future studies must focus on reducing API dependence and improving the efficiency." Table 6 shows per-step training time increases from ~8-11s to ~38-67s with TACReward across algorithms.
- Why unresolved: The current implementation requires API calls (DeepSeek-V3.2) for trace formalization and process discovery for each response, creating overhead that may limit practical adoption at scale.
- What evidence would resolve it: Demonstrating equivalent performance with cached/optimized process discovery, fewer API calls, or learned neural approximators for conformance checking that achieve similar reward signals.

### Open Question 2
- Question: Can reasoning activity taxonomies be automatically discovered or adapted for non-mathematical domains?
- Basis in paper: [explicit] From Section D.4: "Applying TACReward to other domains would require defining an appropriate set of reasoning activities to formalize the traces. Automating the discovery or adaptation of such taxonomic activities is a crucial future research direction."
- Why unresolved: The current 20-activity Mathematical Reasoning Taxonomy was manually designed for mathematical problem-solving; domains like code generation, legal reasoning, or scientific analysis require different structural vocabularies.
- What evidence would resolve it: Successful application of TACReward to diverse reasoning domains with automatically discovered taxonomies or minimal manual adaptation, showing comparable performance improvements.

## Limitations
- Computational overhead is substantial (6-7× slower training) due to trace extraction and process discovery requirements
- Effectiveness depends critically on the quality and generalizability of the 20-activity reasoning taxonomy, limiting easy transfer to other domains
- The method optimizes toward structural similarity with teacher traces that may contain errors, potentially reinforcing flawed reasoning patterns

## Confidence

- **High Confidence:** The mathematical demonstration of alignment cost computation and the integration with GSPO framework are well-specified and reproducible.
- **Medium Confidence:** The empirical performance improvements (+89.2% average accuracy) are compelling but rely on specific experimental conditions (Qwen2.5-7B-Instruct, DeepSeek-R1 as teacher, particular benchmark suites).
- **Medium Confidence:** The claim that sequence-level reward alignment with GSPO explains superior performance is reasonable but could benefit from more ablation studies varying G and comparing with token-level methods.

## Next Checks

1. **Taxonomy Transferability Test:** Apply TACReward to a non-mathematical domain (e.g., code generation or scientific reasoning) using a newly constructed activity taxonomy; measure whether the same conformance-based rewards correlate with quality improvements.

2. **Teacher Error Robustness:** Deliberately corrupt 10-20% of teacher traces with structural errors; evaluate whether TACReward continues to improve policy performance or begins reinforcing incorrect reasoning patterns.

3. **Computational Efficiency Optimization:** Implement caching of teacher traces and batch process discovery; measure whether performance gains are maintained while reducing the 6-7× training overhead to below 3×.