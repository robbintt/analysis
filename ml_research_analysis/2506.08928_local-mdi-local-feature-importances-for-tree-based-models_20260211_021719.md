---
ver: rpa2
title: 'Local MDI+: Local Feature Importances for Tree-Based Models'
arxiv_id: '2506.08928'
source_url: https://arxiv.org/abs/2506.08928
tags:
- lmdi
- features
- lime
- feature
- treeshap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Local MDI+ (LMDI+), a novel extension of
  the MDI+ framework to the sample-specific setting for tree-based models. The key
  innovation lies in adapting the global MDI+ approach, which addresses biases in
  traditional feature importance methods through regularized GLMs and out-of-bag samples,
  to provide local feature importance scores for individual predictions.
---

# Local MDI+: Local Feature Importances for Tree-Based Models

## Quick Facts
- arXiv ID: 2506.08928
- Source URL: https://arxiv.org/abs/2506.08928
- Reference count: 40
- Introduces Local MDI+ (LMDI+), a sample-specific extension of MDI+ for tree-based models

## Executive Summary
This paper presents LMDI+, a novel method for computing local feature importances in tree-based models that extends the global MDI+ framework to the sample-specific setting. LMDI+ overcomes limitations of existing local methods by exploiting the mathematical equivalence between decision trees and linear models on a transformed node basis, using regularized GLMs with out-of-bag samples to correct for known biases. The method demonstrates superior performance across twelve real-world datasets, achieving an average 10% improvement in downstream task performance compared to baselines like LIME and TreeSHAP.

## Method Summary
LMDI+ computes local feature importance by representing each tree as a linear model on a transformed feature space created by concatenating stump functions for all splits with the original features. For each observation and feature, an ElasticNet GLM is fit using both in-bag and out-of-bag samples, producing sample-specific importance scores. The final importance for each feature is the average across all trees in the ensemble. This approach inherits MDI+'s bias-correction mechanisms while providing sample-specific attributions that are more stable and faithful than perturbation-based methods.

## Key Results
- Achieves 10% average improvement in downstream task performance across twelve benchmark datasets
- Outperforms baselines in identifying signal features even under strong correlation (ρ = 0.99)
- Produces more stable feature importance rankings across different model initializations
- Enables practical applications including counterfactual explanations and subgroup discovery

## Why This Works (Mechanism)

### Mechanism 1
LMDI+ produces more faithful local feature importances by exploiting the equivalence between decision tree predictions and OLS regression on a transformed node basis. Each split is represented as a "stump function" that takes three values based on whether an observation falls below/above the threshold or outside the node. Concatenating all splits creates a feature map where tree predictions are mathematically equivalent to OLS regression, transforming the problem into linear regression where feature attributions become coefficient-weighted contributions.

### Mechanism 2
Regularized GLMs with out-of-bag samples correct for MDI's documented biases toward high-entropy, low-correlation, and non-additive features. Three components work together: ElasticNet penalties constrain degrees of freedom from excess splits, OOB samples ensure importance isn't measured only on training data, and appending raw features adds continuous signal that trees inefficiently estimate via piecewise-constant splits.

### Mechanism 3
Averaging local importance scores across ensemble members produces stable, sample-specific feature rankings that identify true signal features even under strong correlation. Regularized GLMs produce consistent coefficient estimates across bootstrap samples, unlike perturbation-based methods (LIME) or permutation-based methods (SHAP) that introduce additional randomness.

## Foundational Learning

- **Concept: Decision Tree Impurity and MDI** - Understanding how MDI computes feature importance via impurity decrease at each split is essential. Quick check: If a feature is split on 10 times in a tree with total impurity decrease of 0.5, what is its raw MDI score? (Answer: Sum of n⁻¹N(v)Δ̂(s, D) across those 10 splits.)

- **Concept: Bootstrap Aggregation and Out-of-Bag Samples** - LMDI+ explicitly uses OOB samples—not just in-bag samples—to compute importance. Quick check: In a random forest with 100 trees trained on n samples, approximately what fraction of samples are OOB for each tree on average? (Answer: ~37%, since each sample has probability (1-1/n)^n ≈ e⁻¹ of not being selected.)

- **Concept: ElasticNet Regularization (ℓ₁ + ℓ₂)** - LMDI+ uses ElasticNet as its default GLM. Quick check: When two features are perfectly correlated and the true model uses only one, which penalty (ℓ₁ alone vs. ElasticNet) is more likely to select both? (Answer: ℓ₁ alone may arbitrarily pick one; ElasticNet tends to select/drop them together due to ℓ₂ grouping effect.)

## Architecture Onboarding

- **Component map:** Trained RF Ensemble -> For each tree: Extract split structure S -> Build stump function representation Ψ(X; S) -> For each feature k: Create augmented representation Ψ̃^(k)(X) = [Ψ(X; S^(k)), x_k] -> Fit regularized GLM: y ~ Ψ̃(X) -> Obtain β̂_λ coefficients -> Local attribution: LMDI+_k(x) = Ψ̃^(k)(x)⊤β̂^(k)_λ -> Ensemble aggregation: Average LMDI+ scores across all trees

- **Critical path:** The GLM fitting step is the one-time computational cost. Ensure your ElasticNet implementation efficiently handles the potentially high-dimensional Ψ̃ representation.

- **Design tradeoffs:** Depth vs. computational cost (deeper trees create more splits), regularization strength (stronger regularization increases stability but may underweight important features), raw feature inclusion (adds p features but helps additive models).

- **Failure signatures:** All-zero LMDI+ scores for a feature (feature was never split on), extremely high variance across trees (OOB sample size too small), counterintuitive rankings (strong correlation structure not handled).

- **First 3 experiments:** (1) Baseline validation on simple linear DGP comparing AUROC against LIME/TreeSHAP, (2) Correlation stress test replicating Section 4.2 with ρ = 0.9, (3) Ablation check running full ablation from Table 2 to verify progressive improvement.

## Open Questions the Paper Calls Out

- **Question:** How does LMDI+ perform when applied to gradient-boosted tree methods such as XGBoost? Basis: While currently demonstrated with random forests, it can be applied to any tree-based algorithm, including popular gradient-based methods like XGBoost.

- **Question:** How do feature interactions affect the accuracy and interpretability of LMDI+ importance scores? Basis: Future work could also include characterizing the impact of interactions.

- **Question:** Can deep learning models replace regularized GLMs in the LMDI+ framework to improve feature attribution quality? Basis: Exploring the use of deep learning models instead of GLMs.

- **Question:** What approximation strategies can mitigate the exponential computational complexity of LMDI+ with respect to tree depth? Basis: The computational complexity of LMDI+ grows exponentially with the tree's depth.

## Limitations

- High computational complexity for deep trees (exponential in number of splits) limits scalability to very large datasets.
- The assumption that tree structure captures meaningful decision boundaries may break when trees overfit to noise.
- While the method shows strong performance across diverse datasets, the exact mechanism by which GLM regularization improves local attributions versus other approaches remains partially unexplored.

## Confidence

- **High confidence**: MDI+ bias correction mechanism (proven mathematically), synthetic experiment results (AUROC metrics), stability analysis (feature count consistency across RF fits)
- **Medium confidence**: Real-world dataset performance (depends on remove-and-retrain protocol), counterfactual explanation improvements (ℓ1 distance metric), subgroup discovery applications (case study nature)
- **Low confidence**: Exact implementation details for stump function handling of samples outside split nodes, ensemble aggregation behavior when features are never split, optimal regularization parameters across all dataset types

## Next Checks

1. **Cross-validation of remove-and-retrain protocol**: On two additional OpenML datasets not in the original study, validate that LMDI+ achieves higher downstream R²/AUROC than baselines when using fewer features.

2. **Robustness to correlation structure**: Systematically vary correlation coefficients (ρ = 0.5, 0.75, 0.99) between signal and non-signal features to map the boundary where LMDI+ performance degrades relative to baselines.

3. **Computational scaling analysis**: Measure runtime and memory usage on synthetic datasets with controlled tree depth (5, 10, 15, 20) to empirically verify the exponential complexity claim and identify practical depth limits.