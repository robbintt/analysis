---
ver: rpa2
title: 'Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location
  Selection for Prostate Cancer Localisation'
arxiv_id: '2508.03953'
source_url: https://arxiv.org/abs/2508.03953
tags:
- segmentation
- policy
- image
- cancer
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a reinforcement learning framework to optimise
  prostate cancer localisation on multiparametric MRI. The approach trains a policy
  network to iteratively select the most informative imaging modality (T2-weighted,
  diffusion-weighted, or both) and local anatomical regions for segmentation.
---

# Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation

## Quick Facts
- **arXiv ID**: 2508.03953
- **Source URL**: https://arxiv.org/abs/2508.03953
- **Reference count**: 19
- **Primary result**: Reinforcement learning framework optimizes prostate cancer segmentation by iteratively selecting imaging modality and anatomical regions, achieving up to 0.44 Dice score improvement

## Executive Summary
This work introduces a reinforcement learning framework to optimize prostate cancer localization on multiparametric MRI. The approach trains a policy network to iteratively select the most informative imaging modality (T2-weighted, diffusion-weighted, or both) and local anatomical regions for segmentation. A pre-trained SwinUNETR-v2 segmentation model simulates radiologist behavior, providing feedback to guide the policy network's decisions. Evaluated on 1325 clinical cases, the method achieves improved Dice scores (up to 0.44) compared to standard segmentation baselines, particularly for challenging cases. Notably, the learned policy sometimes deviates from established PI-RADS guidelines, suggesting novel strategies. The approach enhances segmentation accuracy and efficiency, demonstrating potential for integration into radiologist workflows and interactive clinical applications.

## Method Summary
The method trains a reinforcement learning policy to optimize prostate cancer segmentation by iteratively selecting image portions and modalities. A SwinUNETR-v2 segmentation network is first pre-trained on 925 cases with channel masking to handle variable modality inputs. The policy network then learns to select actions (portion × modality combinations) based on rewards computed as segmentation improvement. During inference, the agent makes 10 sequential decisions per patient. The approach is evaluated on 1325 clinical cases, comparing Dice scores and efficiency against standard segmentation baselines.

## Key Results
- RL-guided segmentation achieves Dice scores up to 0.44, outperforming standard segmentation baselines
- The method shows particular improvement for challenging cases with low visibility lesions
- The learned policy sometimes deviates from PI-RADS guidelines, suggesting novel optimization strategies
- Improved segmentation efficiency through targeted, iterative refinement rather than full-volume processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative partial segmentation refinement improves accuracy over single-pass models by allowing adaptive focus on challenging regions.
- Mechanism: The policy network selects image portions and modalities sequentially. Each action updates only the selected region in the cumulative segmentation mask, enabling the agent to revisit and refine difficult areas multiple times.
- Core assumption: Prostate cancer visibility varies across modalities and anatomical regions; some lesions require multiple targeted inspections.
- Evidence anchors:
  - [abstract] "Taking the locally segmented regions as an input for the next step, this dynamic decision making process iterates until all cancers are best localised."
  - [section 2.2] "The updated prediction y_{t+1} is obtained by replacing the region p in the previous prediction y_t with the newly computed y_{p,t+1}, while keeping the rest of y_t unchanged."
  - [corpus] Weak direct evidence; Patch2Loc (arXiv:2506.22504) uses patch-based localization for lesion detection but without iterative refinement.
- Break condition: If lesions are uniformly visible across all modalities and regions, iterative refinement provides no advantage over single-pass inference.

### Mechanism 2
- Claim: A separately trained segmentation network can serve as a surrogate reward signal, enabling RL policy optimization without human labels during training.
- Mechanism: The pre-trained SwinUNETR-v2 provides segmentation predictions for any modality-portion combination. The reward R_t = L(y_t, ŷ) - L(y_{t+1}, ŷ) quantifies improvement, guiding the policy to select actions that reduce segmentation loss.
- Core assumption: The simulated radiologist's segmentation quality correlates with ground-truth accuracy sufficiently to provide meaningful learning signal.
- Evidence anchors:
  - [abstract] "During training, a pre-trained segmentation network mimics radiologist inspection... Taking the locally segmented regions as an input for the next step..."
  - [section 2.1] "This segmentation network f(·; θ) is trained using a loss function L(·): Y × Y → R≥0, which measures similarity of the neural network segmentation f(x̃; θ) compared to a human label ŷ ∈ Y."
  - [corpus] No directly comparable surrogate reward mechanisms in neighbor papers.
- Break condition: If the segmentation network has systematic biases or blind spots, the policy will learn to exploit these rather than improve genuine localization.

### Mechanism 3
- Claim: RL exploration can discover diagnostic strategies that deviate from clinical guidelines while achieving higher accuracy.
- Mechanism: Through trial-and-error, the agent explores modality-portion combinations beyond PI-RADS recommendations. The policy is optimized purely on segmentation accuracy, not guideline adherence, allowing discovery of patient-specific strategies.
- Core assumption: Deviations from guidelines may reflect optimal data-driven strategies rather than errors; guidelines are heuristics, not global optima.
- Evidence anchors:
  - [abstract] "Perhaps more interestingly, our trained agent independently developed its own optimal strategy, which may or may not be consistent with current radiologist guidelines such as PI-RADS."
  - [section 4] "Patient 1 presents with a tumor located in the peripheral zone (PZ), yet the agent selected T2 for assessment. This choice contrasts with the PI-RADS v2.1 guidelines... which recommend high b-value diffusion-weighted imaging (DW) as the primary modality for PZ evaluation."
  - [corpus] Clinical Inspired MRI Lesion Segmentation (arXiv:2502.16032) addresses multi-sequence MRI but follows clinical workflows rather than discovering alternatives.
- Break condition: If discovered strategies are dataset-specific artifacts rather than generalizable patterns, they will not transfer to new populations.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation**
  - Why needed here: The entire framework models segmentation as sequential decision-making with states, actions, transitions, and rewards. Understanding MDPs is essential to grasp why RL applies to this problem.
  - Quick check question: Can you explain why the reward is defined as the difference in loss before and after an action, rather than the absolute loss?

- **Concept: Policy gradient methods (REINFORCE)**
  - Why needed here: The policy network π_ϕ(a|s) is optimized using REINFORCE and GRPO, which require understanding of gradient estimation through sampled trajectories.
  - Quick check question: Why does REINFORCE require sampling actions from the policy rather than using the highest-probability action during training?

- **Concept: Partial volume segmentation and spatial indexing