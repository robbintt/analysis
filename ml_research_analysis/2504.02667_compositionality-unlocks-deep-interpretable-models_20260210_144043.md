---
ver: rpa2
title: Compositionality Unlocks Deep Interpretable Models
arxiv_id: '2504.02667'
source_url: https://arxiv.org/abs/2504.02667
tags:
- figure
- tensor
- networks
- input
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes \u03C7-nets, an intrinsically interpretable\
  \ architecture combining the compositional multilinear structure of tensor networks\
  \ with the expressivity of deep neural networks. The authors introduce the ODT (Orthogonalisation,\
  \ Diagonalisation, and Truncation) algorithm to efficiently compress trained \u03C7\
  -nets and reveal interpretable low-rank linear structures in multilayer models."
---

# Compositionality Unlocks Deep Interpretable Models

## Quick Facts
- arXiv ID: 2504.02667
- Source URL: https://arxiv.org/abs/2504.02667
- Reference count: 8
- Primary result: χ-nets achieve ~85% accuracy on SVHN while enabling 70% dimension truncation without accuracy loss

## Executive Summary
This paper introduces χ-nets, a deep neural architecture that combines the compositional multilinear structure of tensor networks with the expressivity of deep learning. The key innovation is the ODT (Orthogonalisation, Diagonalisation, and Truncation) algorithm, which compresses trained χ-nets and reveals interpretable low-rank linear structures. Experiments on SVHN show χ-nets retain competitive accuracy while achieving significant compression, enabling extraction of interpretable patterns like edge detectors and proto-digits.

## Method Summary
χ-nets use a non-linear cloning operator to duplicate inputs before bilinear cores, creating a tree tensor network structure that can be analyzed as a linear map over tensor product spaces. The ODT algorithm orthogonalizes cores via RQ decomposition, diagonalizes through Gram matrix spectral decomposition, and truncates based on singular values. This reveals low-rank structure while maintaining competitive accuracy through careful composition of multilinear operations.

## Key Results
- χ-nets achieve ~85% accuracy on SVHN, competitive with baseline networks
- ODT enables 70% dimension truncation without accuracy loss, revealing interpretable low-rank structures
- Diagonalised structure shows atoms resembling edge detectors and proto-digits, with interaction matrices displaying strong linear structure
- Early layers rely on linear summation of features rather than complex multiplicative interactions

## Why This Works (Mechanism)

### Mechanism 1: Multilinear Expansion via Non-linear Cloning
The architecture converts high-degree polynomial learning into multilinear maps via a non-linear cloning operator that duplicates inputs before bilinear cores. This creates tree tensor networks with specific weight-tying patterns, allowing standard training but compositional analysis. The core assumption is that cloning doesn't introduce gradient instabilities. Break condition: if required interaction degree (2^L) exceeds network depth, the model may underfit.

### Mechanism 2: Disentanglement via Orthogonalisation (ODT)
ODT enforces isometry on network cores through RQ decomposition, ensuring the network's Gram matrix can be diagonalized to find singular vectors corresponding to independent, composable features. The core assumption is that the network's function can be well-approximated by low-rank tensor decomposition. Break condition: if data lacks compositional structure, aggressive truncation will degrade accuracy linearly rather than allowing for observed "cliff" retention.

### Mechanism 3: Implicit Linearization of Early Layers
In shallow χ-nets on vision tasks, the network naturally learns to rely on linear summation of features in early layers, with diagonal entries and constant interactions dominating. This implies the network effectively acts as a sparse linear combiner of "atoms" (edge detectors) in early stages. The core assumption is that hierarchical vision features are sufficiently independent to be composed additively. Break condition: if applied to tasks requiring strong multiplicative feature interactions in first layer, observed sparsity would vanish.

## Foundational Learning

- **Concept: Tensor Contractions & String Diagrams**
  - Why needed: Architecture defined using tensor network notation rather than standard code blocks
  - Quick check: Given a box with one input wire and one output wire, and another box with two input wires, how do you compose them in a diagram?

- **Concept: RQ and Spectral Decomposition**
  - Why needed: ODT algorithm relies entirely on RQ decomposition and Eigendecomposition
  - Quick check: Why is RQ decomposition (vs QR) preferred when trying to isolate non-orthogonal part of transformation into specific layer?

- **Concept: Khatri-Rao Product**
  - Why needed: Core tensors parametrized using transposed Khatri-Rao product of matrices
  - Quick check: How does Khatri-Rao product differ from Kronecker product, and what constraint does it impose on rank of resulting tensor?

## Architecture Onboarding

- **Component map**: Input (I) → Embedding (e) → Core (f_i) → Unembedding (u) → Output
- **Critical path**: 
  1. Forward Pass: Implement cloning mechanism efficiently without materializing full 2^L expansion
  2. Orthogonalisation: Implement bottom-up RQ decomposition
  3. Diagonalisation: Compute Gram matrices top-down to find projectors
- **Design tradeoffs**: Higher bond dimensions allow more expressivity but reduce sparsity of interaction matrices; deeper networks capture complex logic but linear lens becomes less effective
- **Failure signatures**: Dimension explosion from naive cloning implementation; rank collapse if ODT truncates >90% dimensions and accuracy drops to random chance
- **First 3 experiments**:
  1. Train 3-layer χ-net on SVHN and verify 70% truncation claim using ODT algorithm
  2. Plot atoms and interaction matrices to confirm emergence of edge detectors and sparse diagonal structures
  3. Replace Bilinear + Cloning with Linear + ReLU layer of equivalent parameter count to measure interpretability tax

## Open Questions the Paper Calls Out

### Open Question 1
Can a theoretical bound on the loss function be established for the ODT truncation process? The paper currently derives Frobenius norm bounds but not task-specific loss bounds. Evidence needed: formal proof establishing monotonic relationship between truncation error and classification loss increase.

### Open Question 2
Can the χ-net framework retain interpretability when scaled to modern, deeper architectures? Experiments are restricted to shallow 3-layer model on SVHN. Evidence needed: successful application of ODT to deep transformers or ResNets where atoms remain semantically meaningful.

### Open Question 3
What causes the observed dip in loss when approximately 90% of dimensions are truncated? The authors note this "dip" but admit uncertainty about its cause. Evidence needed: analysis showing truncated dimensions correspond to noise features or artifacts harming generalization.

### Open Question 4
Does involving dataset statistics in the decomposition improve feature quality? Current ODT operates solely on trained weight matrices. Evidence needed: modified ODT using data covariance matrices resulting in atoms aligning more closely with semantic class features.

## Limitations
- Exact implementation details of RmsBatchNorm and its contraction mechanism are not fully specified
- Results demonstrated only on SVHN, limiting generalizability to other tasks
- Interpretability benefits rely on discovering low-rank structures that may not exist in non-compositional datasets

## Confidence

- **High Confidence**: Mathematical framework (tensor network decomposition, ODT algorithm) is rigorously defined and internally consistent
- **Medium Confidence**: 70% compression claim and accuracy retention are demonstrated on single dataset with specific hyperparameters
- **Low Confidence**: Claim that χ-nets are "competitive" with standard networks lacks direct comparison on same architecture depth

## Next Checks

1. **Generalization Test**: Apply ODT to 3-layer χ-net trained on CIFAR-10 and measure whether same 70% truncation threshold and accuracy retention holds
2. **Architecture Ablation**: Replace bilinear + cloning layers with standard linear + ReLU layers of equivalent parameter count and compare both accuracy and interpretability metrics
3. **Rank-Accuracy Scaling**: Systematically vary bond dimension and measure trade-off between accuracy, interpretability (sparsity of interaction matrices), and compression ratio to identify optimal operating point