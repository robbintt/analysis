---
ver: rpa2
title: How Does Overparameterization Affect Machine Unlearning of Deep Neural Networks?
arxiv_id: '2503.08633'
source_url: https://arxiv.org/abs/2503.08633
tags:
- unlearning
- examples
- unlearned
- privacy
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the level of model overparameterization
  affects machine unlearning performance in deep neural networks. The authors examine
  multiple unlearning methods (SCRUB, Negative Gradient, and L1-sparsity unlearning)
  across various DNN architectures and parameterization levels.
---

# How Does Overparameterization Affect Machine Unlearning of Deep Neural Networks?

## Quick Facts
- arXiv ID: 2503.08633
- Source URL: https://arxiv.org/abs/2503.08633
- Reference count: 39
- This paper investigates how the level of model overparameterization affects machine unlearning performance in deep neural networks.

## Executive Summary
This study examines how model overparameterization affects machine unlearning performance in deep neural networks. The authors evaluate three unlearning methods (SCRUB, Negative Gradient, and L1-sparsity unlearning) across various architectures and parameterization levels, finding that overparameterized models achieve superior balance between generalization and unlearning goals. The research introduces a validation-based hyperparameter tuning approach that optimizes unlearning performance for privacy preservation and bias removal. Notably, successful bias removal in overparameterized models requires unlearning methods that explicitly use the forget set, while L1-sparsity unlearning fails for this goal despite working well for privacy.

## Method Summary
The study employs width-scalable ResNet-18, ResNet-50, and AllCNN architectures trained on CIFAR-10, Lacuna-10, and Tiny ImageNet. For each model width scale, three unlearning methods are applied: SCRUB (teacher-student approach), Negative Gradient (gradient negation), and L1 Sparsity (retain-only with regularization). A validation-based hyperparameter tuning approach optimizes the tradeoff between unlearning goals and generalization using a λ parameter. Privacy unlearning aims to minimize the difference between forget error and forget-test error, while bias removal maximizes forget error. Decision region analysis measures similarity far from unlearned data and change near unlearned examples.

## Key Results
- Overparameterized models achieve better balance between generalization and unlearning goals than underparameterized models
- Overparameterization enables delicate local modifications to decision regions around unlearned examples while preserving functionality elsewhere
- Successful bias removal in overparameterized models requires unlearning methods that explicitly use the forget set
- L1-sparsity unlearning fails for bias removal in overparameterized models despite working well for privacy

## Why This Works (Mechanism)

### Mechanism 1: Delicate Local Modification via Overparameterization
Overparameterized models can modify decision regions locally around unlearned examples while preserving functionality elsewhere. High-dimensional parameter space provides redundant degrees of freedom, allowing optimization to isolate changes to small input-space regions near forget examples. This achieves higher similarity far from unlearned data and higher change near unlearned data simultaneously.

### Mechanism 2: Explicit Forget Set Usage Required for Bias Removal
For bias removal in overparameterized models, unlearning methods must explicitly optimize on the forget set; retain-only methods (L1 Sparsity) fail. Bias removal requires increasing forget error by actively pushing predictions away from original labels. Methods like SCRUB and NegGrad directly optimize against forget-set predictions, while L1 Sparsity only uses retain set, leaving memorized bias patterns intact.

### Mechanism 3: Inherent Privacy Transfer in Underparameterized Models
Underparameterized models may achieve good post-unlearning privacy if their original models already had low forget/forget-test error gap. Underparameterized models cannot perfectly fit training data, yielding inherent regularization against memorization. If original forget error ≈ forget test error, unlearning need not actively increase privacy—just maintain existing levels.

## Foundational Learning

- **Concept**: Machine Unlearning Goals (Privacy vs. Bias Removal)
  - **Why needed here**: The paper evaluates unlearning against two distinct objectives with opposite forget-error targets
  - **Quick check question**: Should forget error increase or decrease for privacy preservation? (Answer: Neither—should approach forget-test error.)

- **Concept**: Parameterization Level (Width Scaling)
  - **Why needed here**: The independent variable. Width scale determines whether models are overparameterized (training error = 0) or underparameterized
  - **Quick check question**: At what width scale does ResNet-18 on CIFAR-10 become overparameterized? (Answer: 0.2—see Appendix A.1.)

- **Concept**: Validation-Based Hyperparameter Tuning with λ Tradeoff
  - **Why needed here**: Unlearning performance depends critically on balancing unlearning goals vs. generalization
  - **Quick check question**: What does λ = 0.8 prioritize in bias removal? (Answer: Maximizing forget error over validation error.)

## Architecture Onboarding

- **Component map**: Original model -> Forget set (200 examples) -> Retain set -> Validation set -> Unlearning methods (SCRUB, NegGrad, L1 Sparsity) -> Evaluation (test error, forget error, MIA accuracy, decision-region scores)

- **Critical path**: 1) Train original model on full dataset, 2) Define forget set and unlearning goal, 3) Run validation-based hyperparameter search, 4) Select optimal hyperparameters minimizing objective function, 5) Apply unlearning method, 6) Evaluate on test, forget, and forget-test sets plus decision regions

- **Design tradeoffs**: Higher λ strengthens unlearning but risks generalization; SCRUB is most flexible but computationally heavier; NegGrad is simpler but less controlled; L1 Sparsity is fastest but fails bias removal in overparameterized models

- **Failure signatures**: L1 Sparsity + overparameterized + bias removal yields low forget error despite low test error; underparameterized + bias removal shows limited forget-error increase regardless of method; high λ + small forget set causes catastrophic forgetting on retain classes

- **First 3 experiments**: 1) Baseline sanity check replicating Figure 1/2 pattern for ResNet-18 on CIFAR-10 with privacy goal at λ = {0.2, 0.4, 0.6}, 2) Method ablation comparing SCRUB vs. L1 Sparsity for bias removal on overparameterized model at matched test-error levels, 3) Decision region validation computing S_far and C_prox at δ=10 for overparameterized vs. underparameterized models after SCRUB privacy unlearning

## Open Questions the Paper Calls Out

### Open Question 1
Can unlearning methods be designed to consider parameterization level and unlearning goals within their core optimization definition rather than relying on external validation-based tuning? The paper suggests this could inspire new unlearning methods that adapt to model width without grid search.

### Open Question 2
Does the superior unlearning performance of overparameterized models observed in CNNs generalize to Transformer-based architectures? The mechanism relies on modifying decision regions in "wide" networks, but it's unclear if Transformers exhibit the same local adaptability during unlearning.

### Open Question 3
How does varying model depth, as opposed to width, affect the ability to delicately modify decision regions during unlearning? The paper defines parameterization specifically as DNN width, leaving depth as an unexamined variable.

## Limitations
- Validation-based hyperparameter tuning critically depends on having representative validation data, but exact size and construction method is not specified
- Pretraining is used for CIFAR-10 and Lacuna-10, but specific pretrained checkpoints and training procedures are not detailed
- The study is limited to CNN architectures and image classification tasks, limiting generalizability to other model types

## Confidence

- **High confidence**: Overparameterized models achieve better balance between generalization and unlearning goals
- **Medium confidence**: Overparameterization enables localized decision region modifications
- **Medium confidence**: Explicit forget-set usage is required for bias removal in overparameterized models

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary λ values around selected points to verify stability of reported trade-offs between unlearning goals and generalization

2. **Validation set ablation**: Compare results using different validation set sizes (10%, 20%, 30% of training data) to assess robustness of validation-based tuning approach

3. **Memory capacity quantification**: Measure actual training/test error curves across width scales to precisely identify overparameterization thresholds where training error reaches zero