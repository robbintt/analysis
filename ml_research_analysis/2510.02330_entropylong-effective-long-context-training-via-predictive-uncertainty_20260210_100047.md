---
ver: rpa2
title: 'EntropyLong: Effective Long-Context Training via Predictive Uncertainty'
arxiv_id: '2510.02330'
source_url: https://arxiv.org/abs/2510.02330
tags:
- context
- data
- uni00000013
- entropylong
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EntropyLong addresses the challenge of training long-context language
  models by constructing data with verified long-range dependencies. Traditional approaches
  concatenate short documents without ensuring meaningful dependencies, limiting models'
  ability to utilize extended contexts.
---

# EntropyLong: Effective Long-Context Training via Predictive Uncertainty
## Quick Facts
- **arXiv ID**: 2510.02330
- **Source URL**: https://arxiv.org/abs/2510.02330
- **Reference count**: 30
- **Primary result**: 87.37 average score on RULER benchmarks; 31.50 on Long tasks in LongBench-v2 after instruction fine-tuning

## Executive Summary
EntropyLong addresses the challenge of training long-context language models by constructing data with verified long-range dependencies. Traditional approaches concatenate short documents without ensuring meaningful dependencies, limiting models' ability to utilize extended contexts. The core method identifies high-entropy positions where models show uncertainty, retrieves relevant contexts from large corpora, and empirically verifies that these contexts reduce predictive entropy. This model-in-the-loop verification ensures each dependency represents measurable information gain. Using FineWeb-Edu and Cosmopedia, EntropyLong generates 128K-length sequences with verified dependencies. Models trained on this data achieve state-of-the-art performance on RULER benchmarks and LongBench-v2, demonstrating that entropy-based verification is essential for effective long-context training data construction.

## Method Summary
EntropyLong constructs long-context training data through a four-stage pipeline: (1) compute per-token entropy across the document to identify uncertainty hotspots, (2) select high-entropy positions using an adaptive threshold (μ_H + ασ_H with α=2.0), (3) for each selected position, extract a query window, retrieve top-K=32 contexts via dense retrieval, and verify entropy reduction (∆I_t > ε with ε=0.4) by prepending contexts to the root document, and (4) shuffle verified contexts and concatenate with root document to create 128K sequences. The method uses jina-embeddings-v3 for retrieval and trains Llama-3-8B with 128K context window using standard long-context training hyperparameters. The key innovation is the model-in-the-loop verification that ensures each dependency provides measurable information gain rather than random context.

## Key Results
- Achieves 87.37 average score on RULER benchmarks, outperforming Quest (80.53) and NExtLong (85.22)
- Maintains 81.26 average score at 128K context length, significantly exceeding baseline performance
- After instruction fine-tuning, achieves 31.50 on Long tasks in LongBench-v2, significantly outperforming Quest (21.30) and NExtLong (23.10)

## Why This Works (Mechanism)
The method works by targeting the fundamental problem in long-context training: models need explicit training signals showing how to use extended context effectively. Traditional concatenation approaches create sequences without ensuring meaningful relationships between distant tokens. EntropyLong solves this by identifying positions where the model is genuinely uncertain (high entropy), then retrieving and verifying contexts that actually reduce this uncertainty. This creates training data where each dependency is guaranteed to be useful rather than random. The adaptive thresholding ensures selection is relative to each document's entropy distribution, while the verification step ensures only contexts that measurably improve prediction are included. This model-in-the-loop approach is computationally expensive but ensures data quality that translates to superior downstream performance.

## Foundational Learning
**Predictive Entropy Calculation**: Why needed - identifies positions where model is uncertain and needs external context; Quick check - verify entropy computation matches implementation (log base consistency)
**Adaptive Thresholding**: Why needed - ensures position selection is relative to document-specific uncertainty patterns; Quick check - validate τ_H = μ_H + ασ_H with α=2.0 produces reasonable selection rates
**Dense Retrieval with Embedding Models**: Why needed - finds candidate contexts that might resolve uncertainty; Quick check - verify Faiss index construction and retrieval pipeline functionality
**Entropy Reduction Verification**: Why needed - ensures only contexts that measurably help are included; Quick check - confirm ∆I_t > 0.4 threshold effectively filters noise
**Context Shuffling**: Why needed - prevents positional bias and ensures model learns true dependencies; Quick check - verify shuffling doesn't break sequence integrity

## Architecture Onboarding
**Component Map**: Data Indexer (Faiss + embeddings) -> Entropy Calculator -> Position Selector -> Context Retriever -> Entropy Verifier -> Sequence Constructor -> Model Trainer
**Critical Path**: Root Document → Entropy Computation → High-Entropy Position Selection → Context Retrieval → Entropy Verification → Sequence Construction → Training
**Design Tradeoffs**: Model-in-the-loop verification ensures quality but increases computational cost versus heuristic approaches; Adaptive thresholding provides document-specific selection versus fixed thresholds; Shuffling prevents positional bias versus preserving original order
**Failure Signatures**: Low verification rate (<30 dependencies/doc) indicates ε threshold too strict or retrieval quality poor; OOM during entropy computation suggests need for chunking or gradient checkpointing; Degraded short-context performance suggests data mix needs adjustment
**First Experiments**: (1) Verify retrieval pipeline with small corpus subset; (2) Test entropy computation on 128K sequences with checkpointing; (3) Validate verification rate with reduced ε on sample data

## Open Questions the Paper Calls Out
**Question 1**: Does EntropyLong scale effectively to larger parameter models (e.g., 70B+), or does reduced baseline uncertainty diminish the signal-to-noise ratio for identifying high-entropy positions?
**Question 2**: What is the computational overhead of the model-in-the-loop verification step compared to actual training time, and does this scale linearly with corpus size?
**Question 3**: How robust are the optimal thresholds (α=2.0, ε=0.4) across different training stages or domains, and should they be dynamically adjusted?
**Question 4**: How does EntropyLong's construction of "natural" dependencies compare to "synthetic" task-driven methods (e.g., NIAH-training) in teaching specific retrieval and reasoning skills?

## Limitations
- Model-in-the-loop verification is computationally expensive, requiring full forward passes for entropy computation
- Fixed thresholds (α=2.0, ε=0.4) may not be optimal across all domains or training stages
- Requires large retrieval corpora (1B+ tokens) and substantial GPU memory for 128K sequence processing
- Does not address potential degradation in short-context performance during long-context training

## Confidence
High: Methodology is well-specified with clear hyperparameters and evaluation protocols; experimental results show significant improvements over strong baselines; reproducibility notes provide detailed implementation guidance
Medium: Computational overhead analysis is incomplete; scalability to larger models is unverified; long-term training dynamics are not explored
Low: None identified

## Next Checks
- Verify retrieval pipeline functionality with sample documents and confirm Faiss indexing works correctly
- Test entropy computation on short sequences (8K-32K) to validate implementation before scaling to 128K
- Measure verification rate on sample data to ensure ε threshold produces sufficient dependencies per document (target: 30-50)