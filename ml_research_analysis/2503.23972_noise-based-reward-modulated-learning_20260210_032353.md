---
ver: rpa2
title: Noise-based reward-modulated learning
arxiv_id: '2503.23972'
source_url: https://arxiv.org/abs/2503.23972
tags:
- learning
- reward
- neural
- noise
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Noise-based Reward-Modulated Learning (NRL),
  a synaptic plasticity rule that leverages stochastic neural activity to approximate
  gradients for learning in neuromorphic systems. NRL transforms the inherent noise
  of biological and neuromorphic substrates into a computational resource by using
  directional derivatives to approximate gradients, enabling local synaptic updates
  modulated by reward prediction errors.
---

# Noise-based reward-modulated learning
## Quick Facts
- arXiv ID: 2503.23972
- Source URL: https://arxiv.org/abs/2503.23972
- Reference count: 40
- Noise-based synaptic plasticity rule that transforms neural noise into a computational resource for learning in neuromorphic systems

## Executive Summary
This paper introduces Noise-based Reward-Modulated Learning (NRL), a synaptic plasticity rule that leverages stochastic neural activity to approximate gradients for learning in neuromorphic systems. NRL transforms the inherent noise of biological and neuromorphic substrates into a computational resource by using directional derivatives to approximate gradients, enabling local synaptic updates modulated by reward prediction errors. The method incorporates eligibility traces for credit assignment over behavioral timescales.

The approach is validated on reinforcement learning benchmarks (Reaching, Acrobot, Cartpole) with immediate and delayed rewards, demonstrating competitive performance compared to backpropagation baselines and significantly superior performance to reward-modulated Hebbian learning (RMHL), especially in deeper networks. NRL scales to multi-layer networks while maintaining locality constraints, though convergence is slower than gradient-based methods. The method is particularly suited for low-power neuromorphic AI systems where local updates and noise exploitation are critical.

## Method Summary
NRL is a synaptic plasticity rule that exploits the stochastic nature of neural activity to approximate gradients for learning. The method uses directional derivatives to estimate gradient information from noise in the system, combined with reward prediction errors to modulate learning. Eligibility traces are incorporated to enable credit assignment over extended behavioral timescales. The learning rule is designed to be fully local, requiring only information available at the synapse level, making it suitable for neuromorphic hardware implementations. NRL approximates the gradient through perturbation-based directional derivative estimation, where noise in the neural activity serves as the perturbation source.

## Key Results
- NRL achieves competitive performance on standard RL benchmarks compared to backpropagation while maintaining full locality constraints
- NRL significantly outperforms RMHL on all tested tasks, with improvements becoming more pronounced in deeper networks
- The method successfully handles both immediate and delayed reward scenarios through eligibility trace mechanisms
- NRL demonstrates the ability to scale to multi-layer networks while preserving local update rules

## Why This Works (Mechanism)
NRL works by converting the stochastic noise inherent in neuromorphic systems into a computational resource. Instead of treating noise as a nuisance to be minimized, NRL uses it as a source of perturbation to estimate directional derivatives, which approximate the gradient needed for learning. The reward prediction error modulates these gradient estimates, ensuring that synaptic updates are only made when they contribute to improved performance. Eligibility traces accumulate evidence of which synapses were responsible for recent actions, enabling proper credit assignment even with delayed rewards. This three-factor learning rule (pre-synaptic activity, post-synaptic activity, and reward prediction error) operates locally at each synapse while still enabling effective learning across multiple layers.

## Foundational Learning
- **Reinforcement Learning**: Why needed - The method learns from reward signals rather than supervised labels. Quick check - Understanding of reward prediction error and policy optimization.
- **Synaptic Plasticity**: Why needed - Core mechanism for weight updates in neural networks. Quick check - Familiarity with Hebbian learning and its limitations.
- **Eligibility Traces**: Why needed - Enable credit assignment over extended timescales. Quick check - Understanding of temporal credit assignment problems in RL.
- **Directional Derivatives**: Why needed - Method for approximating gradients from noisy perturbations. Quick check - Knowledge of finite difference methods and gradient estimation.
- **Neuromorphic Computing**: Why needed - Target hardware platform where noise is inherent. Quick check - Understanding of spiking neural networks and hardware constraints.
- **Three-Factor Learning Rules**: Why needed - Framework for biologically plausible learning with global signals. Quick check - Familiarity with the interaction between local and global learning signals.

## Architecture Onboarding
**Component Map**: Input layer -> Hidden layers (2-3) -> Output layer -> Reward signal feedback -> Synaptic weight updates

**Critical Path**: Neural activity generation -> Noise perturbation -> Directional derivative estimation -> Reward prediction error calculation -> Eligibility trace update -> Synaptic weight modification

**Design Tradeoffs**: NRL sacrifices some convergence speed compared to backpropagation to maintain full locality and biological plausibility. The method trades computational efficiency for hardware compatibility, as the noise-based gradient approximation requires multiple sampling steps. The use of eligibility traces adds memory overhead but enables effective learning with delayed rewards.

**Failure Signatures**: Poor performance on tasks with very sparse rewards, instability when noise levels are too high or too low, slow convergence compared to gradient-based methods, degraded performance when eligibility trace parameters are misconfigured.

**First Experiments**:
1. Single-layer network on simple RL task (Cartpole) to verify basic functionality
2. Two-layer network on delayed reward task (Acrobot) to test eligibility trace mechanism
3. Three-layer network comparison against RMHL on reaching task to demonstrate scaling benefits

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond noting the need for further testing on more complex tasks and different neuromorphic hardware platforms.

## Limitations
- Scalability to very deep networks and high-dimensional state spaces remains untested
- Performance consistency across different neuromorphic hardware platforms with varying noise characteristics is unknown
- Limited comparison against other local learning rules beyond RMHL
- No empirical power consumption measurements on actual neuromorphic hardware
- Biological plausibility of the specific noise exploitation mechanism is uncertain

## Confidence
- Core NRL mechanism: High - Well-established mathematical framework with clear empirical improvements
- Computational efficiency claims: Medium - Method is designed for local updates but actual power measurements are not provided
- Biological plausibility: Medium - Incorporates reward-modulated plasticity but specific noise exploitation may lack biological correlates
- Scalability claims: Medium - Demonstrated on 2-3 layer networks but not tested on deeper architectures
- Hardware robustness: Low - Not tested across multiple neuromorphic platforms

## Next Checks
1. Test NRL on deeper networks (5+ layers) and more complex tasks like Atari games or continuous control with high-dimensional observations to assess scalability limits.
2. Implement NRL on multiple neuromorphic hardware platforms (e.g., Loihi, BrainScaleS) to verify robustness across different noise characteristics and evaluate actual power efficiency gains.
3. Compare NRL against a broader set of local learning rules including target propagation, synthetic gradients, and other three-factor learning methods to establish its relative performance in various learning scenarios.