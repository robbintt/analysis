---
ver: rpa2
title: 'The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective'
arxiv_id: '2510.08840'
source_url: https://arxiv.org/abs/2510.08840
tags:
- fairness
- prediction
- across
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FairTTE, the first comprehensive framework
  for assessing fairness in time-to-event (TTE) prediction in medical imaging. The
  authors develop a causal framework that decomposes data likelihood into distinct
  components to analyze bias sources, including disparities in image features, censoring
  rates, and TTE labels.
---

# The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective

## Quick Facts
- **arXiv ID:** 2510.08840
- **Source URL:** https://arxiv.org/abs/2510.08840
- **Reference count:** 40
- **Key outcome:** Introduces FairTTE framework, reveals pervasive bias across 20,000+ models on 3 medical imaging datasets, shows fairness degrades under distribution shifts

## Executive Summary
This study presents FairTTE, the first comprehensive framework for assessing fairness in time-to-event (TTE) prediction in medical imaging. The authors develop a causal decomposition approach to analyze bias sources across image features, censoring rates, and TTE labels, then evaluate over 20,000 models across three large-scale medical imaging datasets with various sensitive attributes. The results reveal pervasive bias across different imaging modalities and sensitive attributes, with performance disparities consistently observed between demographic groups. While existing fairness methods can improve fairness in some settings, they often do so at the cost of predictive accuracy. The study also finds that pre-training improves accuracy but has minimal impact on fairness, and that fairness becomes increasingly difficult to maintain under distribution shifts, highlighting the need for more robust and equitable prognostic models.

## Method Summary
The FairTTE framework evaluates time-to-event prediction fairness in medical imaging through causal decomposition. The authors analyze three datasets (MIMIC-CXR for chest X-ray mortality, ADNI for Alzheimer's onset, AREDS for AMD progression) with three TTE models (DeepHit, Nnet-survival, PMF) and five fairness algorithms (subgroup rebalancing, domain independence, fair representation learning, distributionally robust optimization, controlling for sensitive attributes). They discretize time into bins (14-128 depending on dataset), resize images to 224×224 or 128×128×96, and normalize pixels. The framework computes performance metrics (time-dependent C-index, AUC, IBS) and fairness metrics (max performance gap between demographic groups). Hyperparameter search covers learning rates 10^x (x∈[-4,-3]), decay 10^x (x∈[-6,-4]), η∈[10^-3,10^-1], λ∈[10^-5,10^2] with 10 seeds per setting.

## Key Results
- Pervasive bias exists across all three medical imaging modalities and sensitive attributes, with consistent performance disparities between demographic groups
- Fairness methods can improve fairness in some settings but typically reduce predictive accuracy, with no single approach consistently effective across all bias sources
- Pre-training significantly improves predictive accuracy but has minimal impact on fairness metrics
- Fairness performance degrades substantially under distribution shifts, becoming increasingly difficult to maintain in out-of-distribution scenarios

## Why This Works (Mechanism)
The causal framework decomposes data likelihood into distinct components to analyze bias sources, including disparities in image features, censoring rates, and TTE labels. By quantifying these five sources of bias, FairTTE provides a systematic way to understand where fairness issues arise and which aspects of the data or model contribute to them.

## Foundational Learning
- **Time-to-event prediction**: Models survival analysis where the goal is predicting when an event (death, disease onset) will occur
  - *Why needed*: Medical prognosis requires understanding not just if an event will happen, but when
  - *Quick check*: Verify models output probability distributions over time rather than single point estimates

- **Causal decomposition**: Breaking down data likelihood into components that represent different causal pathways
  - *Why needed*: Identifies specific sources of bias rather than treating fairness as a monolithic problem
  - *Quick check*: Ensure each bias source (image features, censoring, labels) can be quantified independently

- **Distribution shifts**: Changes in data distribution between training and deployment environments
  - *Why needed*: Real-world medical imaging data varies across hospitals, populations, and time
  - *Quick check*: Test model performance on held-out demographic groups or different acquisition sites

## Architecture Onboarding

**Component Map:** Data preprocessing -> TTE model training -> Fairness algorithm application -> Performance/fairness evaluation

**Critical Path:** Image preprocessing (resize/normalize) → TTE model training (time discretization, loss computation) → Fairness method integration (algorithm-specific modifications) → Evaluation (performance and fairness metrics)

**Design Tradeoffs:** The framework trades off between predictive accuracy and fairness, with no single approach consistently achieving both. Pre-training improves accuracy but doesn't help fairness, while fairness methods often hurt accuracy.

**Failure Signatures:** High variance in fairness metrics with small datasets (ADNI), sensitivity to sensitive attribute binarization thresholds, brittleness under distribution shifts, and inconsistent performance across different imaging modalities.

**First Experiments:** 1) Train baseline TTE models without fairness methods to establish performance baselines, 2) Apply subgroup rebalancing to identify basic fairness improvements, 3) Test domain independence approach to see if separate group modeling helps.

## Open Questions the Paper Calls Out
- **How can fair TTE prediction algorithms be designed to distinguish between and preserve clinically valid causal pathways while mitigating spurious ones?** Current methods treat correlations with sensitive attributes uniformly as bias, risking loss of legitimate biological signals necessary for accurate prognosis.
- **How does fairness performance degrade or shift when moving from standard single-risk TTE settings to real-world scenarios with competing risks and informative censoring?** The current framework assumes non-informative right censoring and single events, which doesn't capture the complexity of real-world medical outcomes.
- **Can mitigation strategies be developed that simultaneously address multiple sources of bias without causing significant trade-offs in predictive accuracy?** Existing algorithms are often brittle, improving one aspect of fairness while failing to correct disparities in censoring or label distributions.

## Limitations
- Exact final hyperparameters for reported best models are not provided, only search ranges, affecting reproducibility
- Sensitive attribute binarization thresholds differ across datasets without clear justification, potentially introducing inconsistencies
- Relatively small sample sizes in ADNI (2.2K) compared to other datasets may lead to higher variance in fairness estimates

## Confidence
- **High Confidence**: General finding of pervasive bias across medical imaging modalities and sensitive attributes is well-supported
- **Medium Confidence**: Specific performance disparities and effectiveness of fairness algorithms are moderately confident but depend on hyperparameters
- **Low Confidence**: Claims about relative effectiveness of specific fairness algorithms across all settings should be interpreted cautiously

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Reproduce key results using systematic grid search across reported hyperparameter ranges to assess stability of fairness-accuracy trade-offs
2. **Alternative Sensitive Attribute Definitions**: Test sensitivity of findings to different binarization thresholds for age and other sensitive attributes to evaluate robustness to preprocessing choices
3. **Cross-Modality Fairness Transfer**: Evaluate whether fairness improvements achieved in one imaging modality transfer to another when applying the same fairness algorithms, to test generalizability of proposed methods