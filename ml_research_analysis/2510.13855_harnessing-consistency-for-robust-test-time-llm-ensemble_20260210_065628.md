---
ver: rpa2
title: Harnessing Consistency for Robust Test-Time LLM Ensemble
arxiv_id: '2510.13855'
source_url: https://arxiv.org/abs/2510.13855
tags:
- token
- arxiv
- ensemble
- consistency
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robustness of LLM ensemble methods against
  erroneous signals arising from heterogeneous tokenization schemes and varying model
  expertise. It proposes CORE, a plug-and-play technique that harnesses model consistency
  at both token and model levels.
---

# Harnessing Consistency for Robust Test-Time LLM Ensemble

## Quick Facts
- arXiv ID: 2510.13855
- Source URL: https://arxiv.org/abs/2510.13855
- Reference count: 37
- Key outcome: CORE improves test-time LLM ensemble robustness, achieving average performance gains of 1.3% (Top-2) and 2.8% (Top-3) across diverse benchmarks

## Executive Summary
This paper addresses the robustness challenges in test-time LLM ensemble methods, specifically targeting erroneous signals from heterogeneous tokenization schemes and varying model expertise. The proposed CORE technique introduces a plug-and-play approach that leverages model consistency at both token and model levels. By acting as a low-pass filter for uncertain tokens and promoting outputs with high self-confidence, CORE effectively mitigates the negative impact of token misalignment and model divergence. Extensive experiments demonstrate that CORE consistently improves ensemble performance across multiple benchmarks, model combinations, and ensemble strategies, achieving significant performance gains while maintaining robustness.

## Method Summary
CORE addresses test-time LLM ensemble robustness by harnessing model consistency at two levels. Token-level consistency computes a reference probability distribution from all aligned models, calculates disparity for each token, and applies an RBF kernel as a low-pass filter to downweight uncertain tokens. Model-level consistency aggregates token consistency scores, normalizes by entropy, and uses these weights to promote outputs with high self-confidence and minimal divergence. The final ensemble combines these mechanisms through a weighted sum where token consistency multiplies aligned distributions and model consistency determines weights, with the main model weight clipped to at least 0.5. CORE is evaluated across six benchmarks using various model combinations and baseline ensemble methods.

## Key Results
- CORE achieves average performance gains of 1.3% and 2.8% on Top-2 and Top-3 model ensembles respectively
- CORE consistently improves ensemble performance across all tested baseline methods (MINED, GAC, UNITE, EVA)
- CORE demonstrates robustness by reducing the frequency of negative ensemble cases where performance drops below the best single model

## Why This Works (Mechanism)
CORE works by filtering out unreliable signals from ensemble models through consistency measures. Token-level consistency identifies and downweights tokens with high disparity across models, effectively acting as a low-pass filter that preserves reliable signals while suppressing noise from tokenization mismatches. Model-level consistency then weights each model's contribution based on its internal consistency and confidence, ensuring that models with more reliable outputs have greater influence. This dual-layer filtering mechanism addresses the core challenge of heterogeneous tokenization by focusing on consistent patterns across models rather than raw probability scores, which can be misleading when tokenizations differ.

## Foundational Learning
- **Token alignment matrix construction**: Mapping assistant model token distributions to main model vocabulary using matrix multiplication with A_i; needed to handle different tokenization schemes across models
- **RBF kernel as low-pass filter**: Using exp(-δ/σ) to downweight tokens with high disparity; needed to suppress unreliable token predictions while preserving consistent ones
- **Entropy-based confidence weighting**: Using H(p̃) to normalize model-level consistency scores; needed to ensure models with higher certainty have greater influence in the ensemble
- **Token-level consistency computation**: Calculating disparity δ_i = |p̃_assist_i - p*| where p* is the mean of aligned distributions; needed to identify tokens with high inter-model disagreement
- **Model-level consistency aggregation**: Computing s^m = Σ s^t(v) / H(p̃) to combine token-level consistency into model-level scores; needed to evaluate overall model reliability
- **Weight clipping for main model**: Enforcing w_main ≥ 0.5 to prevent over-reliance on assistant models; needed to maintain stability when assistant models are less reliable

## Architecture Onboarding
**Component map**: Input token logits → Token alignment matrix A_i → Aligned distributions → Token consistency filter → Model consistency weights → Weighted ensemble → Output token sampling

**Critical path**: Token logits from all models → Alignment and consistency computation → Weighted ensemble distribution → Token sampling → Next step decoding

**Design tradeoffs**: CORE trades computational overhead for robustness - the consistency calculations add processing time but significantly reduce negative ensemble cases. The RBF kernel parameter σ=0.5 represents a balance between being too permissive (letting unreliable tokens through) and too restrictive (discarding useful information).

**Failure signatures**: 
- Negative ensemble (performance worse than best single model) indicates consistency measures failing to filter unreliable signals
- Low token consistency scores across all tokens suggest fundamental tokenization incompatibility between models
- High disparity in aligned distributions indicates poor model alignment or contradictory model behaviors

**First experiments**:
1. Implement baseline ensemble (UNITE) and verify it reproduces stated performance before adding CORE
2. Test token alignment matrix construction on a controlled example with known token mappings
3. Implement token consistency filter independently and measure its impact on ensemble reliability

## Open Questions the Paper Calls Out
None

## Limitations
- CORE's performance depends on the quality of token alignment matrices, which are not fully specified for baseline methods
- The method requires careful tuning of the RBF kernel parameter σ to balance filtering effectiveness and information preservation
- CORE adds computational overhead to the ensemble process, potentially limiting its use in latency-sensitive applications

## Confidence
- **Core technical approach**: High - The consistency-based filtering mechanism is well-defined and theoretically sound
- **Implementation details**: Medium - Token alignment matrices for baseline methods are described at high level but not explicitly specified
- **Prompt construction**: Low - Few-shot prompt templates and CoT formatting are not provided despite shot counts being specified

## Next Checks
1. Verify token alignment matrix construction by testing on a controlled example where token mapping between models is known, ensuring the alignment correctly handles vocabulary differences
2. Implement and compare the four baseline ensemble methods (MINED, GAC, UNITE, EVA) to ensure they reproduce the stated performance before applying CORE, as CORE builds upon these baselines
3. Conduct ablation studies to isolate the contribution of token-level consistency versus model-level consistency, verifying that both components contribute to the observed performance improvements as claimed