---
ver: rpa2
title: Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework
arxiv_id: '2509.21241'
source_url: https://arxiv.org/abs/2509.21241
tags:
- graph
- semantic
- structural
- llms
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a counterfactual-based interpretability framework
  for analyzing fine-tuned LLMs using knowledge graphs. The authors construct BioToolKG,
  a domain-specific heterogeneous knowledge graph in bioinformatics, and propose CFFTLLMExplainer,
  which learns soft masks over graph nodes and edges to generate minimal structural
  perturbations that induce maximum semantic divergence.
---

# Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework

## Quick Facts
- arXiv ID: 2509.21241
- Source URL: https://arxiv.org/abs/2509.21241
- Authors: Yucheng Wang; Ziyang Chen; Md Faisal Kabir
- Reference count: 40
- One-line primary result: Counterfactual masking achieves significantly higher semantic drift (Jaccard 0.10, cosine 0.54) compared to random perturbations, revealing structural dependencies in fine-tuned LLMs

## Executive Summary
This paper introduces CFFTLLMExplainer, a counterfactual-based interpretability framework that learns soft masks over knowledge graph nodes and edges to analyze fine-tuned LLMs. The framework jointly optimizes structural sparsity and semantic divergence while enforcing interpretability constraints like entropy regularization and edge smoothness. Applied to a fine-tuned LLaMA-based LLM using BioToolKG, the method reveals that counterfactual masking exposes structural dependencies and aligns with LoRA-induced parameter shifts. The approach demonstrates superior interpretability and biological plausibility in toolchain extraction tasks compared to attention-based or random perturbations.

## Method Summary
The framework learns soft masks over knowledge graph nodes and edges using Gumbel-Sigmoid reparameterization to enable gradient-based optimization. It converts KG subgraphs to instruction-format prompts and optimizes a multi-objective loss balancing semantic divergence (via TF-IDF similarity), structural sparsity, entropy regularization, and edge smoothness. Applied to a LoRA-fine-tuned DeepSeek-R1-Distill-Llama-8B model, the method generates counterfactual subgraphs that induce maximum semantic drift while preserving graph coherence. The approach also probes LoRA adapter parameters to identify token-level embedding shifts that correlate with structural importance.

## Key Results
- Counterfactual explanations achieve Jaccard similarity of 0.10 and cosine similarity of 0.54 versus original outputs
- Learned masks induce significantly higher semantic drift compared to random perturbations
- LoRA adapter shift magnitude correlates with node importance in counterfactual masking
- Attention weights show poor correlation with structural importance (nodes with <0.002 attention weights were critical for reasoning)

## Why This Works (Mechanism)

### Mechanism 1
Soft masking over KG nodes and edges produces semantically meaningful counterfactuals that reveal structural dependencies. Learnable mask values (m_v ∈ [0,1] for nodes, m_e ∈ [0,1] for edges) are optimized via Gumbel-Sigmoid reparameterization to enable gradient-based discrete approximation. The multi-objective loss balances structural sparsity against semantic divergence, with entropy regularization pushing masks toward binary values and smoothness constraints preserving graph coherence. Core assumption: TF-IDF similarity serves as a valid surrogate signal for semantic similarity when comparing graph-derived textual representations.

### Mechanism 2
LoRA adapter parameters encode structure-sensitive patterns that can be probed through token-level embedding shifts. For each token embedding e, the LoRA-induced directional shift Δ = B · A · e is computed, where A and B are low-rank adapter matrices. The L2 norm ||Δ|| serves as a proxy for token sensitivity under task-specific adaptation. Tokens with larger adapter shifts indicate greater importance in the fine-tuned model's adapted behavior. Core assumption: Adapter shift magnitude correlates with structural importance in the knowledge graph reasoning process.

### Mechanism 3
Attention weights do not reliably indicate structural importance in fine-tuned LLMs processing knowledge graphs. The paper compares attention scores with learned counterfactual masks and adapter shifts. Nodes critical for semantic reasoning exhibited extremely low average attention weights (<0.002) despite high adapter shifts and significant impact when masked. Core assumption: Counterfactual mask importance scores better capture semantic contribution than raw attention weights.

## Foundational Learning

- **Knowledge Graphs as Heterogeneous Information Networks**
  - Why needed here: BioToolKG is formalized with multiple entity types (8) and relation types (14), requiring understanding of typed graphs for proper injection and pathfinding tasks.
  - Quick check question: Can you define the mapping functions φ:V→A and ψ:E→R for entity and relation typing?

- **Gumbel-Sigmoid Reparameterization**
  - Why needed here: Enables differentiable optimization over discrete mask sampling, allowing backpropagation through binary-like decisions.
  - Quick check question: Explain how temperature τ controls the tradeoff between discrete sampling and gradient stability.

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The framework specifically targets LoRA-fine-tuned models, probing how adapter matrices (A, B) encode structural biases.
  - Quick check question: Given W = W₀ + α·AB with rank r≪min(d,k), what proportion of parameters are trainable in the 8B model with r=8?

## Architecture Onboarding

- Component map: BioToolKG construction -> Graph-to-text conversion T(G) -> CFFTLLMExplainer (soft mask learner) -> Semantic similarity module (TF-IDF + cosine) -> LoRA adapter probe (B·A·e shift) -> Baseline perturbation generators (random, attention-based)

- Critical path: 1. Define domain KG schema with typed entities/relations 2. Convert KG subgraphs to instruction-format prompts 3. Initialize learnable masks with Gumbel-Sigmoid 4. Optimize L_total with coefficients: α=400.0, β=0.05, γ=10.0, δ=10.0, ε=5.0, τ=0.15 5. Extract counterfactual subgraph G_c via thresholding 6. Compare f_ft(G) vs f_ft(G_c) for semantic drift analysis

- Design tradeoffs:
  - TF-IDF vs pretrained embeddings: Paper chose TF-IDF for higher sensitivity to pipeline structure nuance
  - Sparsity vs semantic divergence: High α (400) prioritizes semantic drift over minimal perturbation
  - Graph scale: Small pipeline graphs (few nodes) require entropy regularization to prevent degenerate solutions

- Failure signatures:
  - Degenerate masking (all nodes removed): Increase L_preserve and L_entropy coefficients
  - Insufficient semantic drift: Increase α or verify prompt template alignment
  - Disconnected counterfactual graphs: Increase L_smooth weight and verify edge-type coefficients
  - Random baseline outperforming learned masks: Check prompt relevance weighting (w_prompt)

- First 3 experiments:
  1. Replicate transcript assembly case study with published hyperparameters, verifying Jaccard similarity ~0.10 and cosine similarity ~0.54.
  2. Run ablation removing each loss component (L_entropy, L_smooth, L_preserve) to measure individual contribution to semantic drift and structural coherence.
  3. Apply CFFTLLMExplainer to a different LoRA-fine-tuned domain (legal or scientific workflows) to test generalization beyond bioinformatics.

## Open Questions the Paper Calls Out

- **Can the graph-based counterfactual analysis be effectively extended to a prompt-driven setting to evaluate semantic drift under structural perturbations?**
  - Basis in paper: The authors state in the Future Works section that "A promising future direction is to extend the current graph-based analysis to a prompt-driven setting by constructing instruction-style prompts."
  - Why unresolved: The current framework operates on explicit graph structures (BioToolKG) and converts them to text, but has not yet been tested on direct prompt-level perturbations.
  - What evidence would resolve it: Experiments applying CFFTLLMExplainer to instruction-style prompts derived from subgraphs, demonstrating semantic drift metrics consistent with graph-based results.

- **How can human-feedback constraints, such as domain-specific path dependencies, be integrated into the CFFTLLMExplainer's loss function?**
  - Basis in paper: The authors explicitly mention plans to "integrate human-feedback constraints... such as domain-specific path constraints or tool dependencies" in future work.
  - Why unresolved: The current unsupervised optimization relies on automated metrics (semantic divergence, sparsity) and structural smoothness, lacking explicit domain expert validation or hard biological constraints.
  - What evidence would resolve it: A modified framework incorporating biological validity rules that produces counterfactuals with higher domain expert acceptance ratings.

- **Does the framework maintain biological plausibility when applied to noisy, LLM-generated pipelines rather than manually defined, clean graphs?**
  - Basis in paper: The authors utilized manually defined graphs to "ensure structural-level interpretability," noting that "pipelines generated by LLMs may have invalid or meaningless connections," implying the method's robustness to noise is untested.
  - Why unresolved: It is unclear if the entropy and smoothness constraints can successfully filter out hallucinated connections in generated graphs or if they would disrupt the optimization.
  - What evidence would resolve it: Application of CFFTLLMExplainer to LLM-generated toolchains showing that it identifies and masks invalid connections while preserving valid reasoning paths.

## Limitations
- TF-IDF similarity as semantic metric lacks validation against human judgment or more sophisticated embedding approaches
- BioToolKG construction details remain unspecified, making independent replication challenging
- Experimental scope limited to single case study (transcript assembly), raising questions about generalizability

## Confidence

**High Confidence**: The core mechanism of learning soft masks over knowledge graph nodes and edges to generate counterfactuals is well-specified and theoretically sound. The multi-objective loss formulation and Gumbel-Sigmoid reparameterization are standard techniques with clear implementation details.

**Medium Confidence**: The claim that LoRA adapter shifts reveal structural importance in knowledge graph reasoning is supported by experiments but requires more rigorous validation across different graph structures and fine-tuning tasks.

**Low Confidence**: The assertion that attention weights are unreliable indicators of structural importance in KG processing is based on a single case study comparison and needs broader empirical validation across multiple KG tasks and model architectures.

## Next Checks

1. **Human Validation of Semantic Drift**: Conduct a human evaluation study where domain experts rate the semantic similarity between original and counterfactual graph outputs. Compare these judgments against the TF-IDF-based metrics to validate whether the optimization objective aligns with human perception of semantic divergence.

2. **Cross-Domain Generalization Test**: Apply CFFTLLMExplainer to a non-bioinformatics knowledge graph domain (e.g., legal document workflows or scientific methodology chains) with comparable structure and complexity. Measure whether the method consistently identifies high-impact structural elements and induces similar levels of semantic drift.

3. **Attention vs. Counterfactual Mask Ablation**: Systematically compare attention-based perturbation strategies against learned counterfactual masks across multiple KG reasoning tasks. Vary attention importance weighting schemes and measure whether attention can match or exceed the semantic drift achieved by the proposed method, controlling for perturbation budget.