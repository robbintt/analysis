---
ver: rpa2
title: Language Modelling for Speaker Diarization in Telephonic Interviews
arxiv_id: '2501.17893'
source_url: https://arxiv.org/abs/2501.17893
tags:
- speaker
- word
- system
- acoustic
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach to speaker diarization that combines
  acoustic and linguistic information. The method uses a LSTM network that is fed
  with word embeddings from a CharCNN and an acoustic score from a GMM.
---

# Language Modelling for Speaker Diarization in Telephonic Interviews

## Quick Facts
- **arXiv ID**: 2501.17893
- **Source URL**: https://arxiv.org/abs/2501.17893
- **Reference count**: 40
- **Primary result**: 84.29% improvement in WDER over HMM/VB baseline on telephonic interviews

## Executive Summary
This paper presents a speaker diarization approach that leverages both acoustic and linguistic information for telephonic interview scenarios. The method uses character-level word embeddings processed through a CharCNN and Highway network, combined iteratively with acoustic scores from GMMs trained on MFCC features. The LSTM-based system achieves superior performance on word-level diarization compared to traditional HMM/VB baselines, particularly excelling at classifying the interviewer's longer turns while showing reduced effectiveness on very short customer responses.

## Method Summary
The system processes telephonic interviews by first converting audio to word-level transcripts via ASR, then extracting character-level embeddings using a CharCNN with max-over-time pooling and Highway network. An initial LSTM classifier (NN1) produces speaker labels from linguistic features alone. These labels train GMMs for each speaker using MFCCs, generating acoustic scores P(Customer|word). A second LSTM (NN2) fuses embeddings with acoustic scores, and the process iterates 2-3 times for refinement. The approach specifically targets binary classification (interviewer vs. customer) in structured interview settings.

## Key Results
- 84.29% improvement in word-level diarization error rate (WDER) over HMM/VB baseline
- WDER of 1.98% on ASR input after iterative refinement
- Oracle (manual transcript) input achieves 0.77% WDER
- Better performance on longer speaker turns (>6 words) compared to very short segments

## Why This Works (Mechanism)

### Mechanism 1: Character-level Word Embeddings Capture Speaker-Specific Linguistic Patterns
Character-level word embeddings derived from CharCNN encode discriminative speaker information in structured dialogue scenarios where speakers have predictable linguistic roles. Words are converted to character matrices and processed through convolutional filters of varying widths (1-6 characters), capturing character n-gram patterns. Max-over-time pooling extracts the most representative features per filter. A Highway network then models interactions between these character n-grams, producing word embeddings that reflect linguistic regularities. In interview settings, interviewers use consistent question vocabulary while customers provide short, predictable responses, making these patterns discriminative for speaker identification.

### Mechanism 2: Iterative Acoustic-Linguistic Fusion Refines Speaker Labels
Alternating between linguistic classification and acoustic modeling through an iterative loop improves speaker classification accuracy beyond either modality alone. The system operates in iterations: (1) Neural Network 1 produces initial speaker labels using only word embeddings; (2) These labels define word clusters per speaker to train speaker-specific GMMs using MFCC features; (3) Acoustic posterior probabilities P(Customer|w) are computed per word and concatenated with word embeddings; (4) Neural Network 2 refines labels using fused features; (5) Process repeats with updated labels. Convergence typically occurs in 2-3 iterations.

### Mechanism 3: LSTM Sequential Modeling with Delayed Output Captures Turn-Level Dependencies
LSTM networks with output delay and scheduled sampling effectively model sequential speaker turn patterns by leveraging both past and limited future context. The two-layer LSTM (150 nodes each) processes word sequences with output delay k=2, meaning the label for word t depends on embeddings through t+2. Scheduled sampling feeds previous ground-truth labels during training (concatenated with current input), accelerating convergence. At inference, beam search (Viterbi variant) finds the globally most likely label sequence rather than greedy per-word decisions. This captures turn-level patterns: interviewers have longer turns (>6 words), customers have short responses (1-5 words).

## Foundational Learning

- **Concept**: Character-level Convolutional Neural Networks (CharCNN) and Highway Networks
  - Why needed here: The system creates word embeddings from characters rather than using pre-trained word vectors. Understanding convolution over character sequences, max-over-time pooling, and Highway network gating is essential for debugging embedding quality.
  - Quick check question: Given a word "survey" with 6 characters and a filter of width 3, what is the dimensionality of the feature map before max pooling?

- **Concept**: Gaussian Mixture Models and Likelihood Computation
  - Why needed here: GMMs provide acoustic speaker scores. Understanding how GMM likelihoods are computed and converted to posterior probabilities is critical for interpreting and tuning the fusion mechanism.
  - Quick check question: For a word w with M frames, how does equation (7) aggregate frame-level likelihoods, and how does equation (6) convert this to a speaker posterior?

- **Concept**: Scheduled Sampling and Beam Search for Sequence Decoding
  - Why needed here: These techniques stabilize LSTM training and improve inference quality. Without understanding the training-inference mismatch they address, you cannot diagnose convergence failures.
  - Quick check question: During training, scheduled sampling sometimes feeds the ground-truth previous label and sometimes the predicted label. Why does this help, and what happens at inference time?

## Architecture Onboarding

- **Component map**:
```
Audio Input
    ├─→ ASR (Kaldi, 6% WER) → Words + Timestamps
    │                               ↓
    │                          CharCNN (500 filters, widths 1-6)
    │                               ↓
    │                          Highway Network
    │                               ↓
    │                          Word Embeddings (N-dim)
    │                               ↓
    └─→ MFCC Extraction        ┌────────────────┐
         (20 coefficients) ──→ │  ITERATIVE     │
                               │  LOOP          │
          Previous Labels ──→ │                │
                ↓             │  NN1: LSTM     │
          GMM Training ──→    │  (linguistic   │
          (CCR=7 sec/Gaussian)│   only)        │
                ↓             │        ↓       │
          P(Customer|w) ─────→│  NN2: LSTM     │
          (acoustic scores)   │  (fused input) │
                               │        ↓       │
                               │  Sigmoid Out   │
                               └────────────────┘
                                      ↓
                                Speaker Labels
                                (Beam Search, k=2 delay)
```

- **Critical path**:
  1. ASR quality (WER: 6%) directly determines linguistic input quality; degradation here propagates non-linearly
  2. NN1 initialization quality (5.34% WDER) determines iteration convergence speed
  3. CCR parameter (7 seconds/Gaussian) controls GMM complexity and acoustic score reliability on short segments
  4. Iteration count: convergence in 2-3 iterations; more iterations risk overfitting to training distribution

- **Design tradeoffs**:
  - CharCNN filter count (500): More filters capture finer patterns but increase computation and risk overfitting to interview-specific vocabulary
  - Output delay (k=2): More delay provides better future context but increases latency; beyond k=4 yields diminishing returns
  - GMM complexity (CCR): Lower CCR = more Gaussians per speaker, better for long segments but unreliable for Customer's short turns
  - Simple concatenation fusion: No learned weighting between modalities; acoustic scores could dominate or be ignored depending on scale

- **Failure signatures**:
  - Customer WDER > 10% while Interviewer WDER < 5%: Insufficient acoustic model capacity for short segments; reduce CCR
  - No improvement across iterations: NN1 labels too noisy; check training convergence or increase training data
  - WDER degrades with iteration: GMMs capturing mixed-speaker clusters; increase CCR or add regularization
  - Large gap between Oracle and ASR conditions (>2%): System sensitive to ASR errors; consider robust embeddings or error correction
  - Convergence requires >5 iterations: Learning rate or scheduled sampling probability may need adjustment

- **First 3 experiments**:
  1. Ablation study comparing NN1 (linguistic only) vs. NN2 first iteration vs. NN2 iterative, segmented by turn duration (1-2 words, 3-5 words, 6-10 words, >10 words) to quantify where fusion provides maximum benefit
  2. Hyperparameter sweep on output delay (k ∈ {1, 2, 3, 4}) and LSTM hidden size (100, 150, 200) with fixed training epochs to identify optimal sequential modeling capacity without overfitting
  3. Error analysis on misclassified words: compute precision/recall per speaker, analyze correlation with ASR confidence scores, and check if errors cluster around specific question types or response patterns to identify systematic linguistic ambiguities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed fusion approach maintain high performance in conversational scenarios where linguistic content has low correlation with speaker identity?
- Basis in paper: [explicit] The conclusion states it would be interesting to extend the approach to scenarios with less correlation between linguistic content and speaker identities.
- Why unresolved: The current study focuses on telephonic interviews where the interviewer's questions are often known a priori, artificially boosting linguistic discrimination which may not exist in general conversations.
- What evidence would resolve it: Evaluation on general meeting datasets (e.g., AMI, ICSI) where speaker roles are not defined by specific question-answer patterns.

### Open Question 2
- Question: Can advanced acoustic embedding techniques (e.g., x-vectors or transformer embeddings) replace the GMM-based scores to improve performance on very short speech segments?
- Basis in paper: [explicit] The authors note it would be interesting to explore different acoustic approaches that perform efficiently with very short utterances.
- Why unresolved: The paper demonstrates that acoustic data refines longer turns but is less effective than linguistic content for short segments; the current GMM acoustic model may be the limiting factor.
- What evidence would resolve it: Replacing the GMM block with a modern speaker embedding extractor and measuring WDER specifically for segments under 2 seconds.

### Open Question 3
- Question: How does the diarization performance degrade with higher Word Error Rates (WER) from the upstream ASR?
- Basis in paper: [inferred] The paper notes a performance gap between Oracle (manual text) and ASR inputs, but relies on a specific ASR setup (6% WER).
- Why unresolved: The linguistic modeling relies entirely on character-level word embeddings; if the ASR produces high substitution rates, the semantic information required for the LSTM classifier might be lost.
- What evidence would resolve it: Evaluating the system using ASR outputs with artificially inflated WERs to plot the degradation curve of WDER against transcription quality.

## Limitations
- Private evaluation dataset limits reproducibility and generalizability to other domains
- Performance highly sensitive to ASR quality, with significant degradation beyond 10% WER
- Simple concatenation fusion may not optimally combine acoustic and linguistic signals

## Confidence
- **High Confidence**: Character-level embeddings capture speaker-specific linguistic patterns in structured dialogue; iterative fusion of acoustic and linguistic features improves over single-modality approaches; LSTM sequential modeling with output delay effectively captures turn-level dependencies
- **Medium Confidence**: The 84.29% WDER improvement over HMM/VB baseline; convergence in 2-3 iterations under all conditions; performance generalizes beyond the Spanish call-center domain
- **Low Confidence**: System performance with WER > 10%; robustness to overlapping speech; transferability to non-interview dialogue scenarios

## Next Checks
1. **Ablation study with varying ASR quality**: Evaluate the system on the same corpus with artificially degraded ASR output (WER 6%, 10%, 15%, 20%) to quantify the ASR sensitivity threshold where iterative fusion fails to compensate for transcription errors.

2. **Cross-domain transfer validation**: Apply the trained model to a public telephonic dataset like Switchboard or CALLHOME, comparing performance to the original domain. This validates whether linguistic patterns learned from structured interviews transfer to spontaneous conversations.

3. **Oracle vs. iterative baseline comparison**: Compare the iterative fusion approach against a simpler oracle baseline where acoustic and linguistic labels are combined using weighted majority voting. This establishes whether the complex LSTM-based fusion provides measurable benefits over simpler combination strategies.