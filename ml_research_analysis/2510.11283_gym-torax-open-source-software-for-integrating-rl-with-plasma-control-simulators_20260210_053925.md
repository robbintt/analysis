---
ver: rpa2
title: 'Gym-TORAX: Open-source software for integrating RL with plasma control simulators'
arxiv_id: '2510.11283'
source_url: https://arxiv.org/abs/2510.11283
tags:
- plasma
- control
- torax
- gym-torax
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gym-TORAX is an open-source Python package that creates Gymnasium-compatible
  reinforcement learning environments for plasma control in tokamaks by wrapping the
  TORAX simulator. It enables RL researchers to define control actions, observations,
  and reward functions while handling the underlying plasma physics complexity.
---

# Gym-TORAX: Open-source software for integrating RL with plasma control simulators

## Quick Facts
- arXiv ID: 2510.11283
- Source URL: https://arxiv.org/abs/2510.11283
- Reference count: 29
- Open-source Python package wrapping TORAX simulator for RL-based plasma control

## Executive Summary
Gym-TORAX is an open-source Python package that bridges reinforcement learning and plasma physics by providing Gymnasium-compatible environments for tokamak control. The software wraps the TORAX simulator, allowing RL researchers to design control strategies without requiring deep fusion physics expertise. Users can define controllable actions, observations, and reward functions while the package handles the underlying plasma physics complexity through a standard RL interface.

## Method Summary
Gym-TORAX implements the Gymnasium API to abstract the TORAX plasma simulator into RL environments. The package supports two discretization schemes (auto and fixed) and includes error handling for simulation failures. A pre-built environment based on the ITER hybrid ramp-up scenario demonstrates the framework's capabilities. Users extend an abstract BaseEnv class by implementing methods to define action spaces, observation spaces, and reward functions, while the package manages the temporal discretization between RL steps and TORAX timesteps.

## Key Results
- PI controller optimized for expected return achieved J=3.79 versus open-loop J=3.40 (11.5% improvement)
- Gymnasium API abstraction enables RL researchers to develop plasma control strategies without deep fusion physics expertise
- Two-level temporal discretization separates RL interaction timescale from physics simulation timescale for tractable learning

## Why This Works (Mechanism)

### Mechanism 1: Gymnasium API Abstraction Layer
The Gymnasium-compatible interface enables RL practitioners to develop plasma control strategies without requiring deep domain expertise in fusion physics. Gym-TORAX wraps the TORAX simulator behind the standard Gymnasium API (`step()`, `reset()`, observation/action spaces). Users specify only which TORAX variables are controllable actions, which state variables constitute observations, and a reward function—abstracting the underlying PDE solving into a black-box transition function.

### Mechanism 2: Two-Level Temporal Discretization
Separating the RL interaction timescale from the physics simulation timescale enables tractable RL learning while maintaining numerical stability of the underlying PDE solver. Each RL step triggers K internal TORAX timesteps. The agent observes state s_t, selects action a_t, and TORAX advances the plasma state through K physics timesteps before returning s_{t+1}.

### Mechanism 3: Return-Maximizing Control Parameter Optimization
Optimizing control parameters directly against expected return (rather than tracking error) may yield policies that better achieve ultimate performance objectives. The PI controller gains (k_p, k_i) were optimized via grid search to maximize expected return J(π), not to minimize tracking error. This allows the controller to deviate from nominal trajectories when doing so improves the reward-weighted combination of fusion gain Q, safety factors, and confinement quality.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - **Why needed here:** Gym-TORAX formulates plasma control as a finite-time deterministic MDP (S, A, f, r, s_0, γ, T). Understanding state/action spaces, transition functions, and reward design is essential for creating new environments.
  - **Quick check question:** Can you explain why the transition function f is described as deterministic rather than stochastic in this formulation?

- **Concept:** Tokamak plasma state variables
  - **Why needed here:** The observation space includes temperatures (T_i, T_e), densities (n_i, n_e, n_imp), poloidal flux ψ, and derived quantities (safety factor q, beta β, fusion gain Q). Interpreting observations and designing rewards requires basic familiarity with these metrics.
  - **Quick check question:** What does the safety factor q represent, and why might q_min and q_95 be included in the reward function?

- **Concept:** Gymnasium environment API
  - **Why needed here:** Gym-TORAX implements the standard Gymnasium interface. Understanding `step()`, `reset()`, action/observation space definitions, and termination conditions is required to use or extend the package.
  - **Quick check question:** What should the `step()` function return, and how does Gym-TORAX handle simulation errors or unfeasible states?

## Architecture Onboarding

- **Component map:** BaseEnv (abstract class) -> _get_torax_config()/_define_action_space()/_define_observation_space()/_compute_reward() -> TORAX Simulator (wrapped) -> PDE solver (transport equations, current diffusion) / Geometry/physics models / Time series handling (control inputs, sources) -> Action/Observation Classes (IpAction, NbiAction, EcrhAction / AllObservation)

- **Critical path:**
  1. Install dependencies: Python 3.10, TORAX 1.0.3, Gymnasium 1.2
  2. Clone repository and run the pre-built `IterHybridEnv` to verify installation
  3. Extend `BaseEnv` class, implementing the four abstract methods
  4. Define actions via `Action` class (controllable variables + constraints)
  5. Define observations via `Observation` class
  6. Implement reward function using helper methods in `reward` module
  7. Test with simple policies (random, open-loop) before attempting RL training

- **Design tradeoffs:**
  - **Auto vs fixed discretization:** Auto adapts K dynamically (may cause variable RL step durations); fixed ensures consistent timesteps but may over/under-sample dynamics
  - **Full vs partial observability:** `AllObservation` simplifies learning but may include irrelevant variables; custom observations reduce dimensionality but require domain knowledge
  - **Reward complexity:** Simple rewards (like the ITER example) provide clear gradients but may miss important objectives; complex rewards capture more but may create optimization difficulties

- **Failure signatures:**
  - **Simulation error:** TORAX returns error → episode terminates with r_t = -1000 (check termination flag in info dict)
  - **Unfeasible state:** Plasma state becomes physically invalid → episode terminates
  - **Action clipping:** Action outside bounds or ramp-rate limits → action clipped, flag raised in `info['action_clipped']`
  - **LH transition timing:** Not currently handled as special event (noted as future work)

- **First 3 experiments:**
  1. **Baseline characterization:** Run the `IterHybridEnv` with open-loop, random, and PI policies. Verify you can reproduce the expected returns (3.40, -10.79, 3.79) and visualize action trajectories to understand the environment dynamics.
  2. **Reward ablation:** Modify the reward function weights (α_Q, α_qmin, α_q95, α_H98) and observe how optimal PI gains shift. This tests whether the optimization landscape changes predictably with reward specification.
  3. **Observation reduction:** Create a partially-observable variant by selecting only temperature and current-related observations (excluding densities or derived quantities). Compare PI policy performance to full-observability baseline to assess information requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which reinforcement learning algorithms can outperform the PI controller baseline on the ITER hybrid ramp-up scenario?
- Basis in paper: [explicit] The authors state the PI controller result "can serve as a baseline for more sophisticated policies" and encourage users to "test or improve this environment."
- Why unresolved: No RL algorithms beyond simple baselines were evaluated; the 11.5% improvement over open-loop used a classical controller, not learned policies.
- What evidence would resolve it: Benchmark results comparing standard RL algorithms (PPO, SAC, TD3) against the reported PI controller expected return of 3.79.

### Open Question 2
- Question: How can plasma and tokamak geometry be parameterized as part of the MDP state to enable generalization across configurations?
- Basis in paper: [explicit] "Future developments... will focus on... tools to parameterize the plasma and tokamak geometry directly at environment creation, introducing a new dimension to the reinforcement learning problem."
- Why unresolved: Current environments use fixed geometry; the technical approach for geometry parameterization is unspecified.
- What evidence would resolve it: Implementation of geometry parameters in the observation space demonstrating policy transfer across different tokamak configurations.

### Open Question 3
- Question: How should LH transition dynamics be incorporated into the reward structure and observation space?
- Basis in paper: [explicit] "Dedicated utilities will be added to handle specific physics-related events, such as the timing of the so-called LH transition... which plays a critical role in plasma dynamics."
- Why unresolved: The current environment does not explicitly model or signal the L-mode to H-mode transition, despite its importance.
- What evidence would resolve it: Evaluation showing whether explicit transition timing information improves policy performance compared to the current formulation.

## Limitations
- The abstraction layer may obscure critical physics needed for effective reward design and observation interpretation
- Fixed 100-second episode length and 1-second RL step duration may not capture important fast dynamics in tokamak plasmas
- Return-maximizing approach lacks validation of safety constraints and transfer to physical systems

## Confidence

- **High confidence:** The Gymnasium API abstraction and discretization mechanisms are technically sound and well-documented. The experimental comparison between open-loop, random, and PI policies is reproducible and demonstrates clear performance differences.
- **Medium confidence:** The claim that RL researchers can develop control strategies "without requiring deep expertise in fusion physics" is partially supported but may underestimate the physics knowledge needed for effective reward design and observation interpretation.
- **Low confidence:** The assertion that return-maximizing optimization yields better performance than traditional tracking-error approaches is only validated in simulation without safety validation or physical-world testing.

## Next Checks

1. **Reward design validation:** Create a controlled experiment where users with varying physics expertise (expert vs. novice) design reward functions for the same plasma scenario. Measure whether physics expertise correlates with higher expected returns or better constraint satisfaction.

2. **Temporal resolution stress test:** Systematically vary the RL step duration (from 0.1s to 10s) and K discretization factor to identify the minimum temporal resolution required for stable plasma control. Document the point at which performance degrades due to insufficient temporal resolution.

3. **Safety constraint validation:** Implement explicit safety constraints (current limits, pressure limits, disruption avoidance) and test whether the return-maximizing PI controller violates these constraints compared to a constraint-aware controller. This validates whether return optimization compromises safety.