---
ver: rpa2
title: Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language
  Models to Grid-Structured Geospatial Data
arxiv_id: '2505.17116'
source_url: https://arxiv.org/abs/2505.17116
tags:
- data
- https
- structured
- fine-tuning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how large language models (LLMs) can interpret
  grid-structured geospatial climate data, comparing prompt-based approaches to fine-tuning
  on a structured dataset of user-assistant interactions. The base model showed basic
  retrieval capabilities but struggled with unit handling, scenario selection, and
  precision.
---

# Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data

## Quick Facts
- arXiv ID: 2505.17116
- Source URL: https://arxiv.org/abs/2505.17116
- Reference count: 12
- Primary result: Fine-tuning improved semantic similarity from 0.83 to 0.90 and accuracy from 0.29 to 1.0

## Executive Summary
This paper evaluates how large language models (LLMs) can interpret grid-structured geospatial climate data, comparing prompt-based approaches to fine-tuning on a structured dataset of user-assistant interactions. The base model showed basic retrieval capabilities but struggled with unit handling, scenario selection, and precision. Fine-tuning using LoRA improved semantic similarity from 0.83 to 0.90 and accuracy from 0.29 to 1.0, demonstrating robust performance on geospatial reasoning tasks. The study highlights fine-tuning as essential for high-precision, domain-aligned LLM outputs in scientific contexts. Future work will expand the dataset and integrate real-time API-based data extraction for interactive climate applications.

## Method Summary
The method fine-tunes LLaMA 3.1 8B using LoRA (rank 8, alpha 16) on ~100 synthesized query-answer pairs formatted as JSON-style structured data from the ClimRR API. Training used Unsloth-AI with 8-bit quantization, bfloat16 precision, and a 2048-token context. The dataset contains atmospheric variables across three time periods and RCP scenarios (4.5 and 8.5). Evaluation combined regex-based accuracy scoring with cosine similarity via all-MiniLM-L6-v2 embeddings, testing on a 10% held-out set.

## Key Results
- Base model accuracy: 0.29; fine-tuned accuracy: 1.0
- Semantic similarity improved from 0.83 to 0.90 after fine-tuning
- LoRA fine-tuning enabled domain-specific alignment without full model retraining
- Structured JSON formatting mitigated tokenization failures on numerical grid data

## Why This Works (Mechanism)

### Mechanism 1
LoRA-based fine-tuning enables domain-specific numerical and semantic alignment without full model retraining. Low-rank adaptation (rank 8, scaling factor 16) updates a small subset of weights, allowing the model to learn structured geospatial patterns (units, scenarios, grid indices) while preserving base language capabilities. This creates task-specific pathways for numerical precision that standard prompting cannot achieve.

### Mechanism 2
Structured JSON-style input formatting mitigates tokenization failures on numerical grid data. By presenting atmospheric data as key-value pairs with explicit schema (grid cell ID, variable names, time periods, RCP scenarios), the model can leverage its training on structured data formats rather than treating numbers as uninterpretable token sequences.

### Mechanism 3
Fine-tuning on synthesized query-answer pairs corrects systematic domain reasoning failures (units, scenario selection). The training set (~100 examples) encodes explicit patterns for handling unspecified scenarios (default to both RCPs), unit preservation, and regional context. Gradient updates reinforce these behaviors, overriding base model tendencies toward hallucination or arbitrary defaults.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Why needed here: The paper relies on LoRA for efficient fine-tuning; understanding rank and scaling factor helps diagnose over/under-fitting. Quick check question: Can you explain why rank-8 adaptation might be insufficient for a task requiring 100+ distinct numerical patterns?
- **Representative Concentration Pathways (RCPs)**: Why needed here: Scenario comparison (RCP 4.5 vs 8.5) is a core evaluation task; misunderstanding these leads to nonsensical outputs. Quick check question: What is the physical difference between RCP 4.5 and RCP 8.5, and why might a model conflate them?
- **Cosine similarity for semantic evaluation**: Why needed here: The paper uses embedding-based similarity (all-MiniLM-L6-v2) alongside exact accuracy; understanding this distinction prevents misinterpreting results. Quick check question: Why might a response have high semantic similarity (0.89) but low accuracy (0.29)?

## Architecture Onboarding

- **Component map**: ClimRR API -> Grid cell extraction -> JSON formatting -> GPT-4 rephrasing -> Training examples -> LLaMA 3.1 8B base -> LoRA adapters (rank 8, Î±=16) -> Fine-tuned checkpoint -> Inference on held-out 10% test set -> Dual-metric evaluation (regex accuracy + SentenceTransformer embeddings)
- **Critical path**: Dataset curation (120 examples) -> LoRA fine-tuning (~100 examples used) -> Inference on held-out 10% test set -> Dual-metric evaluation
- **Design tradeoffs**: Small dataset (100-120 examples) enables fast iteration but may limit generalization to novel query structures; 8B parameter model balances inference speed vs. reasoning capacity; regex-based accuracy evaluation is brittle to output format variation
- **Failure signatures**: Base model: arbitrary scenario selection when unspecified, unit omission, rounding errors, ambiguous regional comparisons; Potential fine-tuned failures: overfitting to training templates, failure on grid cells not represented in training data
- **First 3 experiments**: (1) Ablation on training set size: Train with 25, 50, 75, 100 examples to identify minimum viable dataset size; track accuracy curve; (2) Out-of-distribution grid test: Evaluate on grid cells from regions (e.g., coastal vs. inland states) absent from training to assess spatial generalization; (3) Format robustness test: Convert JSON inputs to alternative structured formats (YAML, markdown tables) to verify whether gains are format-specific or content-driven

## Open Questions the Paper Calls Out

- How does the fine-tuned model perform when the dataset is expanded to include more complex and nuanced user queries? The current study is limited to a small dataset (~120 examples) generated from semi-automated templates, which may not capture the full complexity of naturalistic scientific inquiry. What evidence would resolve it: Evaluation benchmarks using a larger, more diverse dataset with non-templated, multi-turn reasoning queries.

- Can the model maintain high accuracy and semantic alignment within a real-time, API-driven agentic workflow? The current results rely on static, pre-processed JSON inputs; performance may degrade when data extraction and context injection are performed dynamically in real-time. What evidence would resolve it: System-level metrics showing latency, error rates, and answer accuracy in a live deployment interacting directly with the ClimRR API.

- Does the reported 1.0 accuracy reflect robust domain reasoning or overfitting to the specific grid-cell JSON schema? The extremely small sample size makes it difficult to distinguish between genuine capability acquisition and statistical overfitting to the prompt templates. What evidence would resolve it: Out-of-distribution testing using geospatial data formats or variables not present in the initial training set.

## Limitations

- Small training dataset (~100 examples) may create overfitting risks despite 10% test holdout
- Evaluation relies on brittle regex-based accuracy scoring and may not capture nuanced domain-specific errors
- Real-time API integration for interactive use cases is planned but not yet implemented

## Confidence

- High Confidence: Claims about LoRA fine-tuning improving accuracy from 0.29 to 1.0 and semantic similarity from 0.83 to 0.90
- Medium Confidence: Claims about structured JSON formatting mitigating tokenization failures and correcting systematic domain reasoning failures
- Low Confidence: Claims about LoRA enabling domain-specific alignment without full model retraining and generalization to out-of-distribution grid cells

## Next Checks

1. **Generalization Assessment**: Evaluate the fine-tuned model on grid cells from regions and states not represented in the training data to quantify spatial generalization limits and identify potential overfitting patterns.

2. **Dataset Size Sensitivity**: Conduct ablation studies training with 25, 50, 75, and 100 examples to determine the minimum viable dataset size and identify the point of diminishing returns on accuracy gains.

3. **Format Robustness Testing**: Convert training and test JSON inputs to alternative structured formats (YAML, markdown tables, CSV) to verify whether performance improvements are format-specific or driven by content and schema rather than JSON syntax.