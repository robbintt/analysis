---
ver: rpa2
title: A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel
arxiv_id: '2512.15749'
source_url: https://arxiv.org/abs/2512.15749
tags:
- derivative
- training
- origin
- arxiv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quadratic extrapolation behavior of neural networks
  under the neural tangent kernel (NTK) regime, specifically examining extrapolation
  at the origin. While previous work showed ReLU networks extrapolate linearly far
  from the origin, this paper identifies that at the origin, the network exhibits
  quadratic extrapolation.
---

# A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel

## Quick Facts
- arXiv ID: 2512.15749
- Source URL: https://arxiv.org/abs/2512.15749
- Authors: Abiel Kim
- Reference count: 40
- Primary result: NTK-induced quadratic extrapolation at the origin, distinct from linear far-field extrapolation

## Executive Summary
This paper identifies a special case of extrapolation behavior in neural networks trained under the neural tangent kernel (NTK) regime. While previous work showed ReLU networks exhibit linear extrapolation far from the origin, this paper proves that at the origin specifically, the network converges to a quadratic extrapolator. The key insight is that NTK feature maps are not translation invariant, making extrapolation at the origin fundamentally different from extrapolation far from the origin. The paper constructs a training set where inputs are pushed infinitely far from the origin while evaluation occurs at the origin, inducing this special quadratic behavior.

## Method Summary
The paper analyzes an over-parameterized two-layer ReLU MLP trained in the NTK regime (infinite width, infinitesimal learning rate) on data translated infinitely far from the origin. The method uses kernel regression with the NTK, where the predictor is the min-norm solution in the reproducing kernel Hilbert space. The proof relies on expressing directional derivatives of the predictor in terms of derivatives of the ReLU indicator function, and showing that certain components of the NTK representation coefficients are constant with respect to the bias component of feature directions. The main theoretical result proves that the network converges to a quadratic extrapolator when evaluated near the origin, with the second derivative depending on the orientation between feature directions and the evaluation direction.

## Key Results
- An over-parameterized two-layer ReLU MLP trained on data infinitely far from the origin converges to a quadratic extrapolator when evaluated near the origin
- The quadratic behavior depends on the orientation between feature directions and the evaluation direction, with the second derivative being zero when these are orthogonal
- NTK-induced feature maps are not translation invariant, making extrapolation at the origin a distinct special case from extrapolation far from the origin

## Why This Works (Mechanism)
The quadratic extrapolation behavior emerges from the specific mathematical structure of the NTK regime when training data is located infinitely far from the evaluation point. In this setup, the NTK Gram matrix simplifies in a way that makes certain beta coefficient components constant with respect to bias terms. This leads to the directional derivatives of the predictor having a specific structure where the second derivative is non-zero while higher-order derivatives vanish. The non-translation-invariant nature of the NTK feature map means that behavior at the origin cannot be inferred from behavior far from the origin.

## Foundational Learning
- Neural Tangent Kernel (NTK): A kernel that describes the behavior of infinitely-wide neural networks during gradient descent training. Why needed: The NTK regime provides the theoretical framework for analyzing the infinite-width limit. Quick check: Verify NTK kernel entries for a simple two-layer ReLU network using Monte Carlo integration.
- Reproducing Kernel Hilbert Space (RKHS): The function space where the NTK predictor lives. Why needed: The min-norm solution is expressed as an element of this space. Quick check: Confirm that the predictor can be written as a linear combination of kernel evaluations at training points.
- Directional derivatives: Derivatives of the predictor along specific directions. Why needed: The proof analyzes these to characterize the extrapolation behavior. Quick check: Compute first and second directional derivatives for a simple quadratic function.

## Architecture Onboarding
- Component map: Training data (far from origin) -> NTK kernel computation -> Beta coefficient calculation -> Predictor evaluation (at origin) -> Directional derivative analysis
- Critical path: The proof relies on the infinite-width limit of the NTK, the min-norm solution in RKHS, and the specific structure of ReLU derivatives. The key steps are computing the NTK Gram matrix, inverting it in the appropriate limit, and analyzing the resulting predictor's derivatives.
- Design tradeoffs: The infinite-width assumption provides analytical tractability but limits practical applicability. The choice of ReLU activation is crucial for the specific derivative structure used in the proof.
- Failure signatures: If the network width is insufficient or the translation distance t is not large enough, higher-order derivatives may not vanish as predicted. Ill-conditioning of the NTK Gram matrix can also cause numerical issues.
- First experiments:
  1. Implement analytical NTK for infinite-width two-layer ReLU MLP and verify kernel entries via Monte Carlo integration
  2. Construct synthetic training sets with large t along a fixed direction and compute regularized Gram matrix
  3. Evaluate directional derivatives at x₀ = 0 along test directions and check quadratic behavior

## Open Questions the Paper Calls Out
- Can the training set φ be analyzed as a point process to derive neural scaling laws? The author suggests this could be useful for future work but hasn't explored it yet.
- What distinct properties emerge from the beta components if the target function g is linear? The appendix notes this could lead to "somewhat interesting findings" but was skipped as irrelevant to the main goal.
- Does the quadratic extrapolation behavior persist if training data is far from the origin but distributed in diverse directions? The current proof relies on all training inputs being located infinitely far along the same direction v_φ.

## Limitations
- The infinite-width assumption means results cannot be directly verified in practice - only approximated with large finite networks
- The proof relies on specific properties of ReLU activation that may not generalize to other activations
- The theoretical framework doesn't address how finite-sample effects or optimization dynamics might alter the predicted behavior

## Confidence
- **High confidence**: The mathematical derivation of Theorem 1 and its supporting lemmas, given the NTK assumptions
- **Medium confidence**: The claim that this quadratic behavior represents a "special case" with practical implications
- **Low confidence**: The practical significance of origin-centered extrapolation versus extrapolation far from the origin in real-world applications

## Next Checks
1. Implement numerical verification using finite-width networks with varying widths and translation distances to empirically observe convergence toward quadratic behavior at the origin
2. Extend the analysis to other activation functions (e.g., GeLU, ELU) to determine whether quadratic origin extrapolation is specific to ReLU or more general
3. Design synthetic experiments testing the practical impact of origin-centered quadratic extrapolation versus linear far-field extrapolation on downstream tasks