---
ver: rpa2
title: Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles
arxiv_id: '2506.04849'
source_url: https://arxiv.org/abs/2506.04849
tags:
- agents
- agent
- properties
- actions
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Dec-POMDP-based simulation model for coordinated
  cyber-attacker and cyber-defender agent interactions in networked systems. The model
  abstracts network nodes into property sets and defines agent actions as property
  transitions, enabling realistic scenario implementation and evaluation.
---

# Towards a Multi-Agent Simulation of Cyber-attackers and Cyber-defenders Battles

## Quick Facts
- arXiv ID: 2506.04849
- Source URL: https://arxiv.org/abs/2506.04849
- Authors: Julien Soulé; Jean-Paul Jamont; Michel Occello; Paul Théron; Louis-Marie Traonouez
- Reference count: 22
- Primary result: MARL agents achieve attack goals comparable to decision trees, while defensive MARL significantly reduces attack success

## Executive Summary
This paper presents a Dec-POMDP-based simulation framework for modeling coordinated cyber-attacker and cyber-defender interactions in networked systems. The authors abstract network nodes into property sets and define agent actions as property transitions, enabling realistic scenario implementation using MITRE ATT&CK techniques. Using the Multi Cyber Agent Simulator (MCAS), they evaluate three behavioral approaches - random, decision tree-guided, and multi-agent reinforcement learning - in a scenario based on the GALLIUM APT group attacking a small company network. The results demonstrate that MARL agents can learn effective attack strategies, and when defensive agents are introduced, they significantly reduce attack success rates, providing a flexible framework for assessing collective defense strategies.

## Method Summary
The research develops a Dec-POMDP modeling framework for multi-agent cyber operations simulation, where network nodes are represented as sets of properties and agent actions cause transitions between these property states. The MCAS simulator, built on the PettingZoo framework, implements three agent behavior types: random action selection, decision tree-guided actions based on expert-defined attack paths, and multi-agent reinforcement learning using Q-Learning with curriculum learning (attackers train first, then defenders are added). The evaluation scenario is based on the MITRE ATT&CK GALLIUM APT group attacking a 5-subnet network with approximately 15 nodes, using 30 defined actions. Performance is measured across 1000 episodes, comparing attack success rates and goal achievement across the three behavioral approaches.

## Key Results
- MARL agents achieved attack goal success rates comparable to decision tree-guided agents
- Introduction of defensive MARL agents significantly reduced attack success rates in the simulation
- The Dec-POMDP framework successfully modeled complex attacker-defender interactions with sequential agent execution

## Why This Works (Mechanism)
The simulation works by abstracting network states into discrete property sets that agents can observe and modify through actions. Each agent's action preconditions and postconditions are defined in terms of these properties, creating a formal transition system that captures the sequential nature of cyber operations. The Dec-POMDP formulation naturally handles the partial observability and coordination challenges inherent in multi-agent cyber scenarios, while the property-based abstraction enables tractable state space representation even for complex network topologies.

## Foundational Learning
- **Dec-POMDP (Decentralized Partially Observable Markov Decision Process)**: A mathematical framework for multi-agent sequential decision-making under uncertainty where agents have partial observability and no centralized control. Needed to model realistic attacker-defender interactions where agents cannot see each other's complete state. Quick check: Can you define the components (states, actions, observations, rewards) of a Dec-POMDP?
- **Property-based state abstraction**: Representing network nodes as sets of discrete properties rather than detailed configurations, enabling tractable modeling of complex systems. Needed to avoid state space explosion while maintaining sufficient detail for meaningful cyber operations. Quick check: Can you map a network configuration to a set of properties and identify which actions would modify them?
- **Curriculum learning in MARL**: Training agents sequentially by first mastering individual objectives (attackers reaching goals) before introducing competing agents (defenders). Needed to prevent the non-stationarity issues that arise when training competing agents simultaneously. Quick check: Can you explain why training attackers first, then adding defenders, might lead to more stable learning than simultaneous training?
- **MITRE ATT&CK framework integration**: Using standardized cyber adversary behavior taxonomies to define realistic attack techniques and defender mitigations. Needed to ensure scenarios reflect real-world threat patterns and defensive capabilities. Quick check: Can you identify how specific MITRE ATT&CK techniques map to property transitions in the simulation?
- **Multi-agent reinforcement learning with Q-Learning**: Agents learn optimal policies through trial and error by updating Q-values based on observed rewards, enabling adaptation to complex, dynamic environments. Needed to discover effective attack and defense strategies without requiring complete knowledge of the game tree. Quick check: Can you write the Q-learning update equation and explain each component?
- **PettingZoo framework**: A standardized library for implementing multi-agent reinforcement learning environments, providing consistent APIs for agent-environment interactions. Needed to leverage existing multi-agent RL infrastructure and ensure reproducibility. Quick check: Can you describe the Agent Environment Cycle in PettingZoo and how it differs from single-agent RL?

## Architecture Onboarding

**Component Map**: Network Topology -> Property Definitions -> Action Definitions -> Agent Behaviors (Random/Decision Tree/MARL) -> Reward Functions -> Simulation Episodes

**Critical Path**: The simulation executes in sequential Agent Environment Cycles where each agent observes the current state properties, selects an action based on its behavior policy, executes the action to transition properties, receives rewards, and the cycle repeats until termination conditions are met (goal reached or maximum steps).

**Design Tradeoffs**: The property-based abstraction trades detailed network modeling fidelity for computational tractability, enabling larger scenarios than would be possible with full network state representation. Sequential agent execution simplifies coordination but may not capture all real-world parallel attack/defense dynamics.

**Failure Signatures**: MARL non-convergence manifests as oscillating or plateauing rewards, random agents get stuck when no actions have satisfied preconditions, and decision trees fail when the environment state deviates from expected paths.

**First Experiments**:
1. Verify basic agent-environment interaction by running a single agent with random behavior on the simplest network configuration
2. Test property transition logic by manually triggering actions and verifying expected state changes
3. Compare learning curves of MARL agents with different hyperparameters on a simplified scenario to identify convergence patterns

## Open Questions the Paper Calls Out
- How can attack/defense scenarios be automatically generated from MITRE ATT&CK data rather than requiring manual AD tree construction?
- Can agent behaviors trained in the MCAS simulator transfer effectively to emulated or real network environments?
- What coordination mechanisms would enable multi-agent defenders to handle scenarios requiring explicit cooperation?
- How do action costs, execution durations, and resource constraints affect the equilibrium strategies of attackers and defenders?

## Limitations
- Scenario generation requires manual translation of APT tactics into AD trees with handcrafted pre-conditions and post-conditions
- Abstraction level may not capture all real-world network dynamics and uncertainties needed for real-world deployment
- Current Dec-POMDP implementation lacks inter-agent communication channels for coordinated defense
- All actions are treated as equally costly and instantaneous, not reflecting real-world resource trade-offs

## Confidence
- **High confidence** in the general Dec-POMDP framework and agent abstraction approach
- **Medium confidence** in the three behavioral approaches (Random, Decision Tree, MARL) as conceptual design
- **Low confidence** in achieving identical quantitative results due to unspecified parameters and stochastic training

## Next Checks
1. Implement the basic MCAS framework with the specified network topology and verify that agents can execute actions and transition between states as expected
2. Train MARL attackers with a simple reward structure and verify convergence behavior on a simplified scenario
3. Add defensive agents and measure the impact on attack success rates, comparing against the decision tree baseline to validate relative performance claims