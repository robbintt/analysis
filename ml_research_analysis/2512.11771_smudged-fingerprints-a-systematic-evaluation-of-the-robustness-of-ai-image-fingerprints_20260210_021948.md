---
ver: rpa2
title: 'Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image
  Fingerprints'
arxiv_id: '2512.11771'
source_url: https://arxiv.org/abs/2512.11771
tags:
- attacks
- removal
- forgery
- attribution
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first systematic security evaluation of
  model fingerprint detection (MFD) techniques for AI image attribution. The authors
  formalize three threat models capturing different attacker knowledge levels and
  implement five attack strategies to evaluate 14 representative MFD methods across
  12 state-of-the-art generative models.
---

# Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints

## Quick Facts
- arXiv ID: 2512.11771
- Source URL: https://arxiv.org/abs/2512.11771
- Authors: Kai Yao; Marc Juarez
- Reference count: 40
- Key outcome: Removal attacks achieve >80% success in white-box settings and >50% under black-box access against MFD methods; no method achieves both high accuracy and robustness across all threat models

## Executive Summary
This paper provides the first systematic security evaluation of model fingerprint detection (MFD) techniques for AI image attribution. The authors formalize three threat models capturing different attacker knowledge levels and implement five attack strategies to evaluate 14 representative MFD methods across 12 state-of-the-art generative models. The evaluation reveals that removal attacks are highly effective, achieving success rates above 80% in white-box settings and over 50% under black-box access, while forgery attacks are more challenging but still show significant success. The study identifies a fundamental utility-robustness trade-off: MFD methods with higher attribution accuracy are generally more vulnerable to attacks.

## Method Summary
The paper systematically evaluates 14 MFD methods across 12 generative models using three threat models (white-box, black-box, black-box-without-surrogate). Five attack strategies are implemented: gradient-based attacks (W1-W3) with increasing levels of approximation for non-differentiable extractors, and blind attacks (B1-B2) using surrogate classifiers or generic perturbations. The evaluation uses 1,000 images per model for training attribution classifiers (MLP [512,256,128]) and 100 images for attacks. Attacks employ PGD optimization with ε=0.025 perturbation budget, 50 iterations, and success measured by Attack Success Rate (ASR) while maintaining image quality constraints (LPIPS<0.05, PSNR>35dB).

## Key Results
- Removal attacks achieve >80% success rates in white-box settings across all evaluated MFD methods
- Black-box removal attacks succeed in >50% of cases against most methods, with residual/manifold approaches showing relative robustness
- No evaluated MFD method achieves both high attribution accuracy (>0.7) and robustness across all threat models
- The utility-robustness trade-off is empirically validated: high-accuracy methods are more vulnerable to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Perturbation of Fingerprint Features
Removal and forgery attacks succeed by exploiting gradient pathways through differentiable fingerprint extractors and classifiers. PGD optimization iteratively perturbs pixel values to minimize attribution confidence (removal) or maximize target-class confidence (forgery). When the fingerprint extractor φ is differentiable (W1), gradients flow directly; when non-differentiable, surrogate approximations (W2/W3) enable indirect optimization. Attackers have at least black-box knowledge of candidate generators and can train surrogate classifiers that transfer adversarial examples to the true MFD system.

### Mechanism 2: Utility–Robustness Trade-off via Feature Sensitivity
Methods with higher clean attribution accuracy tend to be more vulnerable because they rely on fragile, high-dimensional features that are easily perturbed. High-accuracy neural network classifiers learn subtle decision boundaries that generalize well on clean data but create narrow margins susceptible to small perturbations. Lower-accuracy methods using coarser features have wider margins but sacrifice discrimination. The attribution task's complexity creates decision landscapes where high-utility classifiers overfit to fragile patterns.

### Mechanism 3: Representation Domain Determines Attack Surface
Noise residual– and manifold-based representations resist black-box attacks better than frequency or learned-feature approaches because their fingerprints are less transferable across surrogate models. Marra19a (noise residuals via BM3D denoising) and Song24-RGB (manifold deviation) produce fingerprints that require model-specific knowledge to perturb effectively. Black-box surrogate classifiers trained on raw images cannot replicate these extraction pipelines, limiting transferability.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) for adversarial attacks
  - Why needed here: All gradient-based attack strategies (W1–W3, B1) use PGD as the core optimizer; understanding its iterated projection step is essential to interpret attack success rates
  - Quick check question: Given an ℓ∞ perturbation budget ε=0.025, how does PGD constrain each iteration's update to remain within the feasible set?

- Concept: Adversarial transferability
  - Why needed here: Black-box attacks (B1) rely on transferability—adversarial examples crafted against surrogate classifiers must fool the target MFD system without direct gradient access
  - Quick check question: Why might a surrogate classifier trained on raw images transfer poorly to a residual-based fingerprint detector?

- Concept: Fingerprint extraction domains (RGB, frequency, learned)
  - Why needed here: The 14 MFD methods span three domains with different differentiability properties, determining which attack strategies (W1/W2/W3) are applicable
  - Quick check question: A frequency-domain extractor using FFT is non-differentiable in its standard implementation—what attack strategy would apply under white-box access?

## Architecture Onboarding

- Component map:
  - Fingerprint extractor φ: Transforms image x → feature vector (size varies: 1×4 for McCloskey18 to 3×256×256 for Nataraj19)
  - Attribution classifier h: MLP [512, 256, 128] mapping features → K-class logits (K=12 generators)
  - Attack pipeline: PGD optimizer → perturbed image x′ → evaluate F(x′) against removal/forgery objectives

- Critical path:
  1. Load pre-trained generator checkpoints (12 models: GANs, VAEs, diffusion)
  2. Extract fingerprints via φ for each MFD method
  3. Train attribution classifier h on fingerprint–label pairs
  4. For each attack setting (W1/W2/W3/B1/B2), run PGD with ε=0.025, 50 iterations
  5. Compute ASR over correctly classified images only

- Design tradeoffs:
  - Perturbation budget ε: Higher values increase ASR but degrade image quality (LPIPS ↑, PSNR ↓); paper uses ε=0.025 for LPIPS<0.05, PSNR>35dB
  - Surrogate vs. direct gradients: W3/B1 surrogate training adds computational cost but enables attacks when φ is non-differentiable; approximation errors can reduce effectiveness vs. W1
  - Multi-class vs. binary: Forgery is harder than removal in 12-class attribution (targeted vs. untargeted); in binary deepfake detection, they become equivalent

- Failure signatures:
  - High ASR variance (e.g., W3 on McCloskey18: ±28.81 pp): Suggests sample-specific vulnerability; certain images have inherently weaker fingerprints
  - B2 attacks outperforming gradient attacks: Indicates severe approximation errors in surrogate extractors/classifiers
  - Perfect removal ASR under white-box (e.g., Giudice21 W1 = 100%): Fingerprint is fully differentiable with no inherent robustness

- First 3 experiments:
  1. Reproduce W1 attack on Giudice21 (differentiable DCT-based method): Verify 100% removal ASR with ε=0.025, confirming white-box vulnerability baseline
  2. Compare B1 vs. B2 attacks on Marra19a: Validate that surrogate classifier transferability is low for residual-based methods (expected B1 ASR < 10%, B2 ASR also low)
  3. Ablate perturbation budget ε ∈ {0.001, 0.01, 0.025, 0.05} on Wang20: Confirm utility–robustness trade-off by plotting ASR vs. clean accuracy as ε varies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does successful fingerprint removal for model attribution guarantee evasion of binary deepfake detection under the same adversarial constraints?
- Basis in paper: [explicit] The authors state future work could study the gap between "detection-robustness" and "attribution-robustness" since "removal in attribution might still fail to evade deepfake detection."
- Why unresolved: The paper evaluates multi-class attribution but does not measure if the resulting adversarial images remain identifiable as AI-generated
- What evidence would resolve it: A joint evaluation where images successfully de-attributed by the attacks are tested against a binary deepfake detector

### Open Question 2
- Question: Do fingerprint removal or forgery attacks leave forensic artifacts that a defender could detect as a warning signal of manipulation?
- Basis in paper: [explicit] The authors suggest future work should "study whether fingerprint suppression or forgery leaves detectable traces that could be used as a warning signal."
- Why unresolved: The analysis focuses on attack success and visual quality (LPIPS), not the detectability of the attack process itself
- What evidence would resolve it: Training a specialized detector to distinguish between clean generated images and those modified by the evaluated attack strategies

### Open Question 3
- Question: How do the success rates of removal and forgery attacks scale as the pool of candidate generative models grows significantly larger than the 12 evaluated?
- Basis in paper: [explicit] The authors highlight the need to study "how attacks scale to realistic candidate pools" typical of open-world scenarios
- Why unresolved: The geometry of the decision boundaries changes with more classes; results from a 12-model pool may not extrapolate to hundreds of models
- What evidence would resolve it: Benchmarking the attack strategies on a dataset containing hundreds of distinct generative models

## Limitations

- The white-box threat model assumes perfect knowledge of fingerprint extractors, an unrealistic scenario that may overestimate vulnerability for certain methods
- Implementation details of 14 diverse fingerprint extraction methods, particularly non-differentiable ones requiring surrogate approximations, may vary across implementations
- The study focuses on attribution robustness but does not evaluate whether adversarial images remain identifiable as AI-generated (detection-robustness gap)

## Confidence

- High: Core findings on attack effectiveness (ASR >80% white-box, >50% black-box) and the utility-robustness trade-off are well-supported by extensive empirical results across 14 methods and 12 models
- Medium: The mechanism explaining why residual/manifold methods resist black-box attacks is primarily paper-internal evidence with limited corpus support
- Medium: Claims about fundamental limitations of MFD approaches requiring hybrid provenance systems are logically derived but not directly tested

## Next Checks

1. Reproduce W1 attack on Giudice21 (differentiable DCT-based method): Verify 100% removal ASR with ε=0.025, confirming white-box vulnerability baseline
2. Compare B1 vs. B2 attacks on Marra19a: Validate that surrogate classifier transferability is low for residual-based methods (expected B1 ASR < 10%, B2 ASR also low)
3. Ablate perturbation budget ε ∈ {0.001, 0.01, 0.025, 0.05} on Wang20: Confirm utility–robustness trade-off by plotting ASR vs. clean accuracy as ε varies