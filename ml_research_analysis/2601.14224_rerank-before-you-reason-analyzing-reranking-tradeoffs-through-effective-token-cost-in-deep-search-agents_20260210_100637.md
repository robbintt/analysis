---
ver: rpa2
title: 'Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective
  Token Cost in Deep Search Agents'
arxiv_id: '2601.14224'
source_url: https://arxiv.org/abs/2601.14224
tags:
- medium
- oss-120b
- oss-20b
- reranking
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes how to allocate reasoning budget in deep search\
  \ agents, focusing on the role of listwise reranking. The authors introduce a novel\
  \ effective token cost (ETC) metric to quantify efficiency\u2013effectiveness tradeoffs\
  \ and conduct experiments using the BrowseComp-Plus benchmark with OpenAI's gpt-oss-20b\
  \ and gpt-oss-120b models under various reasoning settings."
---

# Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents

## Quick Facts
- arXiv ID: 2601.14224
- Source URL: https://arxiv.org/abs/2601.14224
- Reference count: 40
- Reranking improves deep search agent accuracy at lower effective cost than increasing search-time reasoning.

## Executive Summary
This paper analyzes how to allocate reasoning budget in deep search agents, focusing on the role of listwise reranking. The authors introduce a novel effective token cost (ETC) metric to quantify efficiency–effectiveness tradeoffs and conduct experiments using the BrowseComp-Plus benchmark with OpenAI's gpt-oss-20b and gpt-oss-120b models under various reasoning settings. Their results show that reranking consistently improves retrieval quality and end-to-end accuracy, with moderate reranking (d=10-20) often yielding larger gains than increasing search-time reasoning. The study demonstrates that moderate reranking achieves comparable accuracy at substantially lower cost compared to high reasoning effort, with the most favorable efficiency–effectiveness tradeoff occurring under low reasoning budgets for reranking.

## Method Summary
The study evaluates listwise reranking in deep search agents using BrowseComp-Plus benchmark with 830 queries and a fixed human-verified document corpus. Retriever uses qwen3-embedding-8b to return top-100 candidates per query. Reranking is performed by gpt-oss-20b/120b models using zero-shot listwise reranking with sliding windows (size=20, stride=10 for d=50) at depths d∈{10,20,50}. Search agents use the same models with reasoning budgets: low (2k), medium (8k), high (16k) max output tokens. ETC metric combines input tokens (cached and non-cached) and output tokens with parameters α∈{0.1,0.3,0.5} and β∈{3,5,7}. Evaluation includes retrieval metrics (Recall@k, NDCG@k), end-to-end accuracy (LLM-as-judge with oss-120b, 5 runs averaged), calibration error, and search calls.

## Key Results
- Reranking consistently improves retrieval quality and end-to-end accuracy
- Moderate reranking (d=10-20) often yields larger gains than increasing search-time reasoning
- Low reasoning budgets offer the most favorable efficiency–effectiveness tradeoff for reranking in most settings

## Why This Works (Mechanism)

### Mechanism 1
Reranking improves retrieval quality at lower effective cost than increasing search-time reasoning. A listwise reranker reorders top-d candidates before passing top-5 to the search agent. This filters irrelevant context early, reducing the reasoning burden on the search agent. Because reranking operates on fixed-size candidate sets with lower reasoning budgets, its token cost grows more slowly than multi-turn search agent reasoning. Core assumption: The retriever provides sufficient coverage (relevant documents exist in top-100); the reranker can identify relevance more efficiently than the search agent discovering it through iterative search.

### Mechanism 2
Increasing reranking depth d dominates reasoning budget for retrieval gains. Reranking more candidates (d=10→20→50) exposes the reranker to more potentially relevant documents. The paper shows moving from d=10 to d=20 improves NDCG@5 by 5-6 points, with an additional 6-8 point gain at d=50. This is larger than the ~1.7-4.8 point gains from increasing reasoning budget at fixed d. Core assumption: The reranker maintains discriminative ability across larger candidate sets without degradation from noise.

### Mechanism 3
Low reasoning budgets are most cost-effective for reranking in iterative search pipelines. Iterative queries from search agents are typically simpler than original queries. Allocating higher reasoning to reranking yields marginal NDCG gains (~1.7 for oss-20b, ~4.8 for oss-120b at d=50) but at substantially higher token cost due to autoregressive generation. Core assumption: Agent-generated sub-queries are less complex than the original query, requiring less reasoning for relevance assessment.

## Foundational Learning

- **Listwise Reranking**
  - Why needed here: Core technique studied; requires understanding how LLMs reorder full candidate lists vs. pointwise/pairwise approaches.
  - Quick check question: Given 20 passages and a query, can you explain why listwise ranking might capture relative relevance better than scoring each independently?

- **Deep Research Agents (Agentic Search)**
  - Why needed here: The target system class; iterative retrieval-reasoning loops differ from single-shot RAG.
  - Quick check question: How does a multi-turn search agent differ from a single RAG retrieval call in terms of token accumulation?

- **Token Cost Economics (Cached vs Non-Cached, Input vs Output)**
  - Why needed here: ETC metric hinges on α (caching discount) and β (output premium); understanding throughput vs. API cost regimes is essential.
  - Quick check question: In vLLM, why does output generation have lower TPS than prefill, and how does this relate to β in the ETC formula?

## Architecture Onboarding

- Component map:
  Query → Retriever (qwen3-embedding-8b, top-100) → Reranker (oss-20b/120b, listwise, d∈{10,20,50}, top-5 output) → Search Agent (oss-20b/120b, iterative reasoning, 128k context) → Answer

- Critical path:
  1. Retriever returns top-100 candidates per query
  2. Reranker processes top-d with sliding window (size=20, stride=10 for d=50)
  3. Top-5 reranked docs passed to search agent (truncated to 512 tokens each)
  4. Agent iteratively generates queries and reasons until answer or limit

- Design tradeoffs:
  - **d (reranking depth)**: Higher d improves recall but increases reranking tokens. Paper recommends d≤20 for best efficiency-effectiveness.
  - **Reasoning budget (search agent)**: High reasoning improves accuracy but dominates ETC. Medium reasoning + deeper reranking often matches high reasoning + no reranking at lower cost.
  - **Model scale**: oss-120b outperforms oss-20b at same settings, but smaller models benefit more from increased reasoning.

- Failure signatures:
  - Low recall at retrieval stage (relevant docs not in top-100) → reranking cannot help
  - Context overflow (exceeding 128k tokens) → automatic truncation may lose critical history
  - High calibration error despite high accuracy → overconfident wrong answers (reranking reduces this)

- First 3 experiments:
  1. **Baseline without reranking**: Run oss-20b/120b with low/medium/high reasoning, d=0. Measure accuracy, recall, ETC.
  2. **Ablation on reranking depth**: Fix model and reasoning, vary d∈{10,20,50}. Plot accuracy vs. ETC to find Pareto frontier.
  3. **Cross-model reranking**: Test smaller reranker (oss-20b low) with larger search agent (oss-120b high) to validate if mixed configurations improve tradeoffs (noted as limitation in paper).

## Open Questions the Paper Calls Out

### Open Question 1
Does pairing smaller rerankers with larger search agents (or vice versa) yield more efficient cost-effectiveness tradeoffs than using the same model family for both components? The authors state in Limitations: "exploring mixed configurations—such as pairing smaller rerankers with larger search agents or vice versa—may yield more efficient tradeoffs." This remains unresolved as the paper uses the same model family for both components.

### Open Question 2
Do the observed reranking benefits generalize to deep research benchmarks that rely on live web search APIs rather than a fixed, human-verified corpus? The authors note: "our experiments are conducted on BrowseComp-Plus, which uses a fixed, human-verified document corpus rather than live web search... the results may not fully generalize."

### Open Question 3
Can a learned relevance assessor that dynamically selects a variable-sized subset of retrieved documents outperform fixed-size top-d reranking? The authors suggest: "An alternative design is to use a learned relevance assessor that dynamically selects a variable-sized subset of retrieved documents, potentially filtering redundant or irrelevant context more effectively."

## Limitations
- Model Access Dependency: Study relies on gpt-oss-20b and gpt-oss-120b models, which appear to be OpenAI-internal variants not publicly available.
- Static Retrieval Baseline: Results assume fixed retriever quality; performance gains may not generalize if initial recall drops significantly.
- Single Benchmark Scope: All experiments use BrowseComp-Plus with 830 queries, which may not transfer to other domains or query distributions.

## Confidence
- **High Confidence**: The ETC metric design, the core observation that moderate reranking (d=10-20) achieves better efficiency-effectiveness tradeoffs than high reasoning budgets, and the consistency of reranking improving both retrieval metrics and end-to-end accuracy.
- **Medium Confidence**: The specific superiority of low reasoning budgets for reranking in iterative search, as this depends on the assumption that agent-generated queries are simpler than original queries.
- **Low Confidence**: The precise scaling relationship between reranking depth and reasoning budget without mixed-model configurations, which was noted as a limitation.

## Next Checks
1. **Reproduce ETC Calculation**: Instrument vLLM to separately track cached vs non-cached tokens and output tokens to validate the paper's ETC values for at least one configuration.
2. **Mixed-Model Configuration Test**: Run a small experiment using a smaller reranker (oss-20b low reasoning) with a larger search agent (oss-120b high reasoning) to test if the efficiency-effectiveness tradeoff improves.
3. **Retrieval Quality Sensitivity**: Measure retrieval recall@100 before reranking to confirm that relevant documents exist in the top-100 for a representative sample of queries.