---
ver: rpa2
title: 'Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism
  Detection'
arxiv_id: '2506.06238'
source_url: https://arxiv.org/abs/2506.06238
tags:
- data
- task
- language
- augmentation
- women
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses sexism detection in online content, focusing
  on two main challenges: data sparsity and the nuanced interpretation required for
  fine-grained classification. The authors propose two novel data augmentation techniques:
  Definition-based Data Augmentation (DDA), which leverages category-specific definitions
  to generate semantically-aligned synthetic examples, and Contextual Semantic Expansion
  (CSE), which targets systematic model errors by enriching examples with task-specific
  semantic features.'
---

# Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection

## Quick Facts
- **arXiv ID**: 2506.06238
- **Source URL**: https://arxiv.org/abs/2506.06238
- **Reference count**: 40
- **Primary result**: Definition-based Data Augmentation and Contextual Semantic Expansion improve macro F1 scores by up to 4.1 points for fine-grained sexism detection.

## Executive Summary
This paper addresses sexism detection in online content by proposing two novel data augmentation techniques: Definition-based Data Augmentation (DDA), which leverages category-specific definitions to generate semantically-aligned synthetic examples, and Contextual Semantic Expansion (CSE), which enriches misclassified examples with task-specific semantic features to correct systematic model errors. Additionally, an ensemble strategy using Mistral-7B as a fallback model resolves prediction ties in multi-class classification. Experimental results on the EDOS dataset demonstrate state-of-the-art performance, with macro F1 improvements of 1.5 points for binary classification (Task A) and 4.1 points for fine-grained classification (Task C). The methods effectively address class imbalance and annotation ambiguities, improving both model robustness and reliability.

## Method Summary
The method employs a multi-stage pipeline: first, DeBERTa-v3-Large and RoBERTa-Large are pre-trained on 2M unlabeled EDOS samples using Masked Language Modeling (MLM) to adapt to the domain. Then, data augmentation is performed via DDA for fine-grained tasks or CSE for binary classification, using GPT-4o to generate synthetic examples. DDA incorporates category definitions into prompts to generate 3 variations per example for 5 underrepresented classes, while CSE targets high-confidence misclassifications by generating structured semantic expansions. Finally, models are fine-tuned on augmented data and aggregated using the Mistral-7B Fallback Ensemble (M7-FE), which uses majority voting with Mistral-7B as a tie-breaker.

## Key Results
- DDA improves Task C (fine-grained) macro F1 by 4.1 points over baseline.
- CSE improves Task A (binary) macro F1 by 1.5 points.
- M7-FE ensemble outperforms other fallback configurations for Task C (0.6018 vs. 0.5895 for DTFN).
- DDA reduces cross-category confusion between classes 3.1 and 3.2 by approximately 27%.
- Augmenting only 5 underrepresented classes is more effective than augmenting all 11 classes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating explicit category definitions into data augmentation prompts improves fine-grained classification by clarifying semantic boundaries between overlapping classes.
- Mechanism: Definition-based Data Augmentation (DDA) uses LLM prompts that include category-specific definitions (e.g., the precise definition of "2.1 Descriptive Attacks" vs "2.2 Aggressive and emotive attacks") to generate synthetic examples that are semantically aligned with the target class. This provides clearer training signals for subtle distinctions that annotators often disagree on.
- Core assumption: Synthetic examples generated with explicit definitions capture meaningful semantic distinctions that improve model generalization.
- Evidence anchors:
  - [abstract] "Definition-based Data Augmentation (DDA), which leverages category-specific definitions to generate semantically-aligned synthetic examples."
  - [section] Table 1 shows high annotator disagreement (e.g., 54.1% partial disagreement for "2.1 descriptive attacks"), indicating ambiguous boundaries. Figure 4 shows DDA increased correct predictions for class 2.3 by 42 and class 3.4 by 8, while reducing cross-category confusion between classes 3.1 and 3.2 by approximately 27%.
  - [corpus] Related work on expert-LLM interaction strategies for zero-shot sexism detection supports the value of explicit definitions for clarifying semantic boundaries (arxiv:2504.15392).
- Break condition: If synthetic examples introduce artifacts or biases from the generating LLM, performance may degrade, particularly for low-resource categories where the definition may not capture all nuances.

### Mechanism 2
- Claim: Enriching misclassified examples with explicit semantic analysis corrects systematic model errors by expanding contextual understanding.
- Mechanism: Contextual Semantic Expansion (CSE) targets examples the baseline model misclassifies with high confidence (p > 0.9), generating structured semantic expansions that analyze language patterns, sentiment, stereotypes, and intent. The expanded text is concatenated with the original, providing richer signals during retraining.
- Core assumption: High-confidence misclassifications indicate systematic biases rather than boundary ambiguities, which can be corrected through explicit semantic analysis.
- Evidence anchors:
  - [abstract] "Contextual Semantic Expansion (CSE), which targets systematic model errors by enriching examples with task-specific semantic features."
  - [section] The paper notes that misclassified examples showed high confidence scores (p > 0.9), suggesting systematic biases. CSE improved Task A (binary) macro F1 by 1.5 points, the largest gain for that task.
  - [corpus] Corpus evidence on semantic depth for explaining network errors is weak or not directly applicable (arxiv:2504.09956 focuses on vision networks).
- Break condition: If errors stem from fundamental ambiguities (not systematic biases), or if expansions introduce noise, CSE may not generalize.

### Mechanism 3
- Claim: A fallback ensemble using Mistral-7B resolves prediction ties in multi-class classification, improving reliability.
- Mechanism: The Mistral-7B Fallback Ensemble (M7-FE) uses majority voting among DeBERTa-v3-Large, Mistral-7B, and DTFN. When a tie occurs (two-way or complete disagreement), Mistral-7B's prediction is used as the final output, leveraging its complementary perspective.
- Core assumption: Different models provide diverse, complementary perspectives on ambiguous cases, and a large LLM fallback is a robust tie-breaker.
- Evidence anchors:
  - [abstract] "an ensemble strategy that resolves prediction ties by aggregating complementary perspectives from multiple language models."
  - [section] Section 3.6 describes the tie-handling mechanism. Table A4 shows Mistral-7B Fallback Ensemble outperforms other fallback configurations for Task C (0.6018 vs. 0.5895 for DTFN).
  - [corpus] Corpus evidence specifically supporting this fallback mechanism is weak.
- Break condition: If models share correlated errors or biases, the ensemble may amplify them rather than mitigate them.

## Foundational Learning

- Concept: Masked Language Modeling (MLM) Pre-training
  - Why needed here: The pipeline pre-trains DeBERTa and RoBERTa on 2M unlabeled EDOS samples via MLM (15% token masking) to adapt the models to the specific domain (Gab/Reddit language) before fine-tuning.
  - Quick check question: Can you explain why MLM pre-training on in-domain data helps, even when the models were already pre-trained on general corpora?

- Concept: Class Imbalance and Macro F1 Score
  - Why needed here: The EDOS dataset has severe class imbalance (e.g., some categories have fewer than 100 examples). Macro F1 is the primary metric, as it weights all classes equally, making it sensitive to performance on low-resource categories.
  - Quick check question: Why is macro F1 preferred over accuracy for evaluating performance on imbalanced datasets like EDOS?

- Concept: Hard Voting vs. Soft Voting in Ensembles
  - Why needed here: M7-FE uses hard voting (counting predicted class labels), not soft voting (averaging probabilities). Understanding this distinction is critical for implementing and debugging the ensemble.
  - Quick check question: What is the difference between hard voting and soft voting, and why might hard voting be chosen for this task?

## Architecture Onboarding

- Component map:
  1. **Pre-training**: DeBERTa-v3-Large and RoBERTa-Large are pre-trained on 2M unlabeled EDOS samples (Section 3.1).
  2. **Data Augmentation**: DDA (for fine-grained tasks) or CSE (for binary task) generates synthetic data using GPT-4o (Sections 3.3, 3.4).
  3. **Fine-tuning**: Pre-trained models are fine-tuned on augmented labeled data using categorical cross-entropy loss (Section 3.5).
  4. **Ensemble (M7-FE)**: Aggregates predictions from DeBERTa-v3-Large, Mistral-7B, and DTFN via majority voting, with Mistral-7B as fallback for ties (Section 3.6).

- Critical path:
  1. Select 5 key underrepresented classes for DDA (Section C, Table A2).
  2. Generate 3 variations per example using DDA prompts with definitions (Figure 1).
  3. For CSE, identify misclassified examples from baseline model and generate semantic expansions (Figure 3).
  4. Fine-tune models on augmented datasets with optimized hyperparameters (Table A1).
  5. Run M7-FE on test set, invoking fallback for ties.

- Design tradeoffs:
  - **DDA targets 5 vs. all 11 classes**: The paper found augmenting only the 5 most underrepresented classes was more effective and efficient than augmenting all 11 (Table A3).
  - **Choice of fallback model**: Mistral-7B was selected based on preliminary experiments showing superior performance in tie-breaking (Table A4).
  - **CSE vs. DDA**: CSE is better for binary classification (Task A), while DDA is more effective for fine-grained tasks (Tasks B, C).

- Failure signatures:
  - High-confidence misclassifications persisting after CSE may indicate fundamental ambiguity rather than correctable systematic bias.
  - If DDA-generated examples drift from category semantics, cross-category confusion may increase instead of decrease (negative values in off-diagonal of Figure 4).
  - Ensemble failing to resolve ties or defaulting frequently to fallback may indicate correlated model errors.

- First 3 experiments:
  1. **Baseline Reproduction**: Fine-tune DeBERTa-v3-Large on unaugmented EDOS data to establish baseline macro F1 for Tasks A, B, and C.
  2. **DDA Ablation**: Compare DDA (with definitions) vs. baseline prompt (without definitions) for generating synthetic data for the 5 key classes, measuring impact on Task C macro F1.
  3. **Fallback Model Comparison**: Implement the ensemble with different fallback models (Mistral-7B, DTFN, DeBERTa-v3-Large) and compare tie-resolution performance on Task C.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Definition-based Data Augmentation (DDA) and Contextual Semantic Expansion (CSE) generalize effectively to multilingual and low-resource datasets beyond English?
- Basis in paper: [explicit] The authors state: "The generalizability of our methods to multilingual and low-resource datasets remains an open question that we aim to address in future work."
- Why unresolved: The evaluation focused exclusively on the EDOS dataset, which is English-only and relatively well-curated. The techniques rely on prompt engineering with LLMs whose multilingual capabilities vary, and category definitions may not transfer directly across cultural contexts.
- What evidence would resolve it: Evaluation of DDA and CSE on multilingual sexism detection benchmarks (e.g., Arabic, Spanish) showing comparable macro F1 improvements, or analysis of performance degradation in genuinely low-resource settings with limited annotated data.

### Open Question 2
- Question: How can synthetic examples generated via LLM-based augmentation be validated to ensure they are free from unintended biases or artifacts stemming from pretraining data?
- Basis in paper: [explicit] The limitations section states: "Ensuring that these synthetic examples are free of unintended biases or artifacts is an ongoing challenge that requires further investigation."
- Why unresolved: The DDA and CSE methods use GPT-4o for generation, which may encode societal biases. The paper does not implement or propose a systematic validation mechanism for detecting such biases in augmented data.
- What evidence would resolve it: Development of a bias auditing framework for synthetic sexism examples, or empirical comparison showing that models trained on DDA/CSE-augmented data do not exhibit increased bias on fairness benchmarks compared to baselines.

### Open Question 3
- Question: Would alternative ensemble strategies such as weighted voting, confidence-based aggregation, or advanced meta-ensemble techniques further improve reliability in fine-grained sexism classification?
- Basis in paper: [explicit] The authors acknowledge: "Exploring alternative ensemble strategies, such as weighted voting, confidence-based aggregation, or advanced meta-ensemble techniques, may further enhance performance and robustness in multi-class settings."
- Why unresolved: The M7-FE ensemble uses simple majority hard voting with a fallback mechanism. While effective, the paper does not compare against more sophisticated aggregation methods that might better handle class imbalance or model uncertainty.
- What evidence would resolve it: Systematic comparison of M7-FE against weighted voting ensembles, confidence-thresholded aggregation, or stacking-based meta-learners on Tasks B and C, with analysis of performance on ambiguous/disagreed instances.

### Open Question 4
- Question: Does the ensemble aggregation approach inadvertently suppress minority annotator perspectives or amplify biases present in individual models?
- Basis in paper: [inferred] The related work section cites prior findings that "aggregation methods can inadvertently suppress minority annotator perspectives and amplify biases present within individual models." While M7-FE aims to leverage diverse perspectives, no empirical analysis of this risk is provided.
- Why unresolved: The paper notes that high annotator disagreement reflects subjective interpretations and diverse viewpoints, yet the ensemble still produces a single aggregated prediction. Whether this aggregation marginalizes certain perspectives remains unexamined.
- What evidence would resolve it: Analysis correlating ensemble prediction confidence with annotator disagreement patterns, or evaluation of whether examples with minority-annotator labels are systematically misclassified by the ensemble compared to individual models.

## Limitations
- DDA and CSE prompt templates are only partially shown, leaving critical implementation details unspecified.
- DTFN architecture is referenced but not described, creating ambiguity about the complete system.
- CSE example selection criteria and semantic expansion format are not fully specified.
- The methods may introduce LLM-generated artifacts or biases that are not systematically validated.

## Confidence
- **High Confidence**: Experimental results showing macro F1 improvements are well-documented and reproducible.
- **Medium Confidence**: The mechanisms underlying DDA and CSE are logically sound but lack complete implementation details.
- **Low Confidence**: Complete effectiveness cannot be verified without full prompt templates and DTFN architecture details.

## Next Checks
1. **Prompt Template Verification**: Implement DDA using the exact prompt examples shown (Figure 1) and generate a small sample of synthetic examples. Manually verify that the generated examples align with category definitions and capture the intended semantic distinctions between overlapping classes.

2. **Ensemble Behavior Analysis**: During M7-FE inference on the test set, log every instance where ties occur and which model's prediction is selected as the final output. Calculate the frequency of tie occurrences and validate that Mistral-7B is correctly invoked as the fallback mechanism.

3. **Ablation on Class Selection**: Run DDA experiments augmenting all 11 Task C classes versus only the 5 underrepresented classes identified in the paper. Compare macro F1 scores to verify that the targeted approach (5 classes) outperforms the exhaustive approach (11 classes) as claimed in Table A3.