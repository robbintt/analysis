---
ver: rpa2
title: 'Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions
  with Relative Salience Annotations'
arxiv_id: '2601.13225'
source_url: https://arxiv.org/abs/2601.13225
tags:
- emotion
- blended
- emotions
- salience
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BLEMORE is the first multimodal dataset of blended emotion expressions
  with relative salience annotations, containing over 3,000 video clips from 58 actors
  portraying six basic emotions and their pairwise combinations in three salience
  configurations (50/50, 70/30, 30/70). We evaluate state-of-the-art video and audio
  encoders for two tasks: emotion presence recognition and relative salience prediction.'
---

# Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations

## Quick Facts
- arXiv ID: 2601.13225
- Source URL: https://arxiv.org/abs/2601.13225
- Reference count: 40
- BLEMORE dataset: 3,050 video clips from 58 actors portraying 6 basic emotions and 10 blends in 3 salience configs

## Executive Summary
This paper introduces BLEMORE, the first multimodal dataset of blended emotion expressions with relative salience annotations. The dataset contains over 3,000 video clips from 58 actors portraying six basic emotions and their pairwise combinations in three salience configurations (50/50, 70/30, 30/70). The authors evaluate state-of-the-art video and audio encoders for two tasks: emotion presence recognition and relative salience prediction. Results show that multimodal approaches significantly outperform unimodal variants, with the best models achieving 33% presence accuracy and 18% salience accuracy on the held-out test set.

## Method Summary
The study uses the BLEMORE dataset containing 3,050 video clips of 58 actors portraying six basic emotions and their pairwise combinations in three salience configurations. Features are extracted using video encoders (OpenFace 2.0, CLIP, ImageBind, VideoMAEv2, Video Swin) and audio encoders (HuBERT Large, WavLM Large). Frame-level features are aggregated via statistical descriptors (mean, std, percentiles) or subsampled. An MLP-512 classifier is trained using KL divergence loss on soft labels representing salience proportions. Post-processing applies thresholds α (presence) and β (salience) tuned on validation data.

## Key Results
- Multimodal combinations consistently outperform unimodal variants, with ImageBind + WavLM reaching 35% presence accuracy
- Statistical aggregation of frame-level features produces more robust representations than subsampling
- HiCMAE achieves the best salience accuracy (18%) while VideoMAEv2 + HuBERT achieves best presence accuracy (33%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion improves blended emotion recognition over unimodal approaches
- Mechanism: Facial expressions and vocalizations encode complementary information about emotional blends; concatenating aggregated features from both modalities allows the classifier to leverage cues that may be ambiguous or absent in a single modality
- Core assumption: Video and audio channels provide non-redundant emotional information that can be integrated through simple early fusion
- Evidence anchors:
  - [abstract] "multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy"
  - [Section V-A] "Multimodal combinations consistently outperformed unimodal variants, with ImageBind + WavLM achieving the best composite score"
  - [corpus] CHEER-Ekman and other datasets focus on unimodal emotion classification; multimodal complementarity is not directly addressed in corpus papers
- Break condition: If one modality is corrupted or missing, or if both encode identical information, fusion gains diminish

### Mechanism 2
- Claim: Statistical aggregation of frame-level features produces more robust representations than subsampling
- Mechanism: Computing seven statistical descriptors (mean, std, 10th/25th/50th/75th/90th percentiles) across frames summarizes temporal dynamics into fixed-size vectors that generalize better across variable-length clips
- Core assumption: Global summary statistics preserve the information relevant to emotion classification better than segment-level embeddings
- Evidence anchors:
  - [Section IV-A] "Aggregation-based features generally outperformed subsampled features, see Table III"
  - [Table III] VideoMAEv2 aggregation: 27.3% presence accuracy vs subsampling: 26.0%
  - [corpus] No corpus papers directly compare aggregation strategies for emotion recognition
- Break condition: If fine-grained temporal patterns (e.g., rapid expression changes) are critical, aggregation may lose discriminative information

### Mechanism 3
- Claim: Soft label encoding with salience proportions enables models to learn relative emotion prominence
- Mechanism: Blended emotions are represented as six-dimensional soft probability vectors (e.g., 70% happiness + 30% sadness → [0, 0, 0, 0.7, 0.3, 0]); KL divergence loss trains the model to match these distributions, and thresholding converts soft outputs to discrete presence/salience predictions
- Core assumption: Salience can be meaningfully discretized into three levels (50/50, 70/30, 30/70) and threshold-based post-processing captures the underlying continuous structure
- Evidence anchors:
  - [Section III-A] "each blend has 3 different salience configurations (50/50, 70/30, and 30/70)"
  - [Section IV-B] "Blended emotions were represented as soft probability distributions over two emotions, where the label values correspond to the salience proportions"
  - [Section VI] "the thresholding procedure used to convert soft predictions into discrete presence indicators and salience levels introduces variability"
  - [corpus] Salience Adjustment paper mentions salience in emotion recognition but does not use soft label encoding for blends
- Break condition: If optimal thresholds vary significantly across data splits or if salience is truly continuous, fixed thresholds cause accuracy drops

## Foundational Learning

- Concept: **Multimodal Early Fusion**
  - Why needed here: The paper concatenates video and audio features before classification; understanding fusion strategies is essential for extending or modifying the architecture
  - Quick check question: Given two modalities with feature dimensions d₁ and d₂, what is the dimension of an early-fused representation?

- Concept: **KL Divergence Loss for Soft Labels**
  - Why needed here: Models are trained to match soft probability distributions rather than hard one-hot labels; this is central to learning salience proportions
  - Quick check question: How does KL divergence differ from cross-entropy when target distributions are soft rather than one-hot?

- Concept: **Threshold-Based Multi-Label Classification**
  - Why needed here: Converting continuous model outputs to discrete presence/salience predictions requires tuning α (presence) and β (salience) thresholds; misconfiguration directly impacts evaluation metrics
  - Quick check question: If you raise the presence threshold α, what happens to precision and recall for emotion detection?

## Architecture Onboarding

- Component map:
  Input layer -> Video Encoder (VideoMAEv2/ImageBind) + Audio Encoder (HuBERT/WavLM) -> Feature Aggregation (statistical descriptors) -> MLP-512 Classifier -> Post-processing (thresholding) -> Evaluation (ACC_presence/ACC_salience)

- Critical path:
  1. Standardize features using training set statistics (zero mean, unit variance)
  2. Train with KL divergence loss on soft labels
  3. Tune thresholds α and β on validation folds before test evaluation
  4. Use fixed thresholds from best validation fold for held-out test set

- Design tradeoffs:
  - **Aggregation vs Subsampling**: Aggregation is simpler and performed better; subsampling preserves more temporal detail but underperformed
  - **Early fusion vs Dedicated multimodal (HiCMAE)**: Early fusion achieved best ACC_presence (33%); HiCMAE achieved best ACC_salience (18%)—task-specific optimization may require different architectures
  - **Classifier depth**: MLP-512 generally outperformed Linear and MLP-256, suggesting non-linear interactions matter

- Failure signatures:
  - **Validation-test gap**: ACC_salience drops from 17% to 5% for VideoMAEv2 (aggregation) on test set—likely due to threshold mismatch
  - **High presence, low salience**: Model correctly identifies emotions but fails to rank salience, indicating threshold β is poorly tuned
  - **Overfitting to actors**: Paper uses actor-disjoint splits; if validation accuracy is high but test is low, the model may be learning identity cues

- First 3 experiments:
  1. **Baseline replication**: Train MLP-512 with VideoMAEv2 + HuBERT using aggregation and KL divergence; verify ~33% ACC_presence on validation folds
  2. **Threshold sensitivity analysis**: Sweep α ∈ [0.1, 0.5] and β ∈ [0.1, 0.5] on validation data; plot ACC_presence and ACC_salience to identify stable operating points
  3. **Modality ablation**: Compare VideoMAEv2-only, HuBERT-only, and VideoMAEv2 + HuBERT to quantify fusion benefit; check if improvement is consistent across emotion pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do regression-based formulations offer more stable predictions than the current discrete threshold-based classification for relative salience?
- Basis in paper: The Discussion section suggests that regression or ranking-based formulations might offer more stable solutions by avoiding the sensitivity issues introduced by the fixed thresholds used in the current discrete classification setup.
- Why unresolved: The paper only evaluated discrete classification tasks (ACC presence/salience), which suffered from performance drops due to post-processing sensitivity.
- What evidence would resolve it: Training models with continuous regression heads on the dataset and comparing their prediction stability and error rates against the baseline threshold-based classifiers.

### Open Question 2
- Question: Can models trained on BLEMORE's acted portrayals generalize effectively to spontaneous, "in-the-wild" blended emotion expressions?
- Basis in paper: The Limitations section notes that the dataset consists of acted expressions recorded in controlled laboratory conditions, which may limit ecological validity compared to spontaneous interactions.
- Why unresolved: The study focused on intra-dataset evaluation and did not test generalization capabilities to spontaneous or uncontrolled data.
- What evidence would resolve it: Fine-tuning BLEMORE-pretrained models on in-the-wild datasets (e.g., MAFW) and measuring the performance retention or domain gap.

### Open Question 3
- Question: Do sophisticated fusion mechanisms outperform the simple early fusion strategies established in this study?
- Basis in paper: The Limitations section states that future studies could employ tailored models based on more sophisticated fusion mechanisms to increase classification accuracy beyond the standard techniques used.
- Why unresolved: The current baselines relied on existing encoders combined via simple feature concatenation (early fusion).
- What evidence would resolve it: Implementing advanced fusion techniques (e.g., attention-based or tensor fusion) on BLEMORE to determine if they exceed the current best multimodal results.

## Limitations

- The salience discretization into three levels (50/50, 70/30, 30/70) represents a coarse approximation of continuous emotional blends
- Substantial validation-test performance gaps suggest threshold tuning issues or dataset distribution shifts
- The dataset consists of acted expressions in controlled laboratory conditions, limiting ecological validity

## Confidence

**High Confidence**: The observation that multimodal approaches consistently outperform unimodal variants is well-supported by direct experimental comparisons across multiple encoder combinations and both tasks. The finding that statistical aggregation outperforms subsampling is also robustly demonstrated with clear numerical evidence.

**Medium Confidence**: The claim that HiCMAE achieves the best salience accuracy (18%) while VideoMAEv2 + HuBERT achieves best presence accuracy (33%) requires careful interpretation given the large validation-test gaps. The mechanism that soft label encoding enables learning of relative prominence is plausible but not fully validated, as the paper doesn't explore alternative salience representations or continuous prediction frameworks.

**Low Confidence**: The assertion that the BLEMORE dataset provides a "valuable resource for advancing blended emotion recognition research" is aspirational rather than empirically demonstrated. While the dataset is novel and well-annotated, the actual research impact depends on future work and community adoption.

## Next Checks

1. **Threshold Stability Analysis**: Systematically sweep α and β thresholds on the validation set and plot ACC_presence and ACC_salience curves to identify stable operating points. Then apply the same thresholds to the test set to verify if performance gaps are primarily due to threshold mismatch rather than fundamental model limitations.

2. **Actor-Identity Ablation Study**: Train models using actor-inclusive splits (where training and test sets share actors) to quantify how much performance depends on learning identity cues versus genuine emotion recognition. Compare this to the current actor-disjoint results to assess generalization.

3. **Continuous Salience Prediction**: Modify the model to output continuous salience values rather than discrete classes, and evaluate using regression metrics (e.g., RMSE) on the held-out test set. This would test whether the discrete three-level framework is artificially constraining model performance.