---
ver: rpa2
title: Long Is More Important Than Difficult for Training Reasoning Models
arxiv_id: '2503.18069'
source_url: https://arxiv.org/abs/2503.18069
tags:
- reasoning
- problems
- length
- problem
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether problem difficulty or reasoning
  length is more important for training reasoning models. It hypothesizes that reasoning
  length, rather than problem difficulty, primarily drives model performance.
---

# Long Is More Important Than Difficult for Training Reasoning Models

## Quick Facts
- arXiv ID: 2503.18069
- Source URL: https://arxiv.org/abs/2503.18069
- Reference count: 40
- Primary result: Models trained on longer reasoning sequences from easier problems match or exceed performance of models trained on shorter difficult problems

## Executive Summary
This paper investigates whether reasoning length or problem difficulty is more important for training reasoning models. The authors hypothesize that reasoning length, rather than problem difficulty, primarily drives model performance. Through controlled experiments comparing models trained on long but easy composite problems versus difficult single problems (with matched total reasoning lengths), they demonstrate that length-focused training achieves comparable or superior results. A log-linear scaling law is identified, showing performance increases nearly linearly as reasoning data length grows exponentially. Using a simple concatenation method to generate arbitrarily long reasoning sequences, the authors create the Long1K dataset and fine-tune Qwen2.5-32B-Instruct to achieve 95.6% accuracy on MATH500 and 71.1% on GPQA Diamond, outperforming existing models with only 1,000 training samples.

## Method Summary
The method involves creating synthetic long reasoning sequences by concatenating pairs of problems and their corresponding reasoning chains, then fine-tuning a base model (Qwen2.5-32B-Instruct) using LoRA parameter-efficient training. The training uses 1,000 samples: 800 concatenated problem pairs from Openthoughts-114k with approximately 32k token reasoning chains, plus 200 single problems from s1k. The LoRA configuration uses rank 16, alpha 32, with a learning rate of 2e-4, cosine scheduler, 3 epochs, and batch size 2 per GPU across 8 A800-80GB GPUs.

## Key Results
- Models trained on long but easy composite problems achieved 71.1% on GPQA Diamond and 95.6% on MATH500
- Outperformed DeepSeek-R1-Distill-Qwen-32B using only 1,000 training samples
- Log-linear scaling relationship observed: accuracy rises nearly linearly as reasoning length increases exponentially (1.5k to 12k tokens)
- Performance on AIME2024 and AIME2025 shows consistent gains from extended reasoning length

## Why This Works (Mechanism)

### Mechanism 1: Length-Decoupled Reasoning Acquisition
Model performance is driven primarily by the length of reasoning chains rather than intrinsic problem difficulty. By extending reasoning token sequences (even via concatenated easy problems), models learn to maintain logical consistency and structure over extended contexts, decoupling capability from problem complexity. Evidence shows models trained on "Long Problems(Composite)" matched or outperformed those trained on "Difficult Problems(Composite)" on AIME2024.

### Mechanism 2: Internalization of Reflective Correction
Training on long sequences stabilizes the model's use of reflective transition tokens (e.g., "but", "wait") during error correction. Extended exposure forces frequent handling of failure modes and pivots. Analysis shows models trained on longer data maintain stable transition word usage (~2%) during failed cases, while short-data models spike usage (~9%), suggesting structured recovery.

### Mechanism 3: Log-Linear Scaling via Synthetic Concatenation
Performance scales linearly with exponential growth of training data length. Concatenating independent problems simulates continuous reasoning states, enabling compute-efficient inference by conditioning on longer contexts. The log-linear law is demonstrated through accuracy rising linearly as token length increases exponentially from 1.5k to 12k.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) vs. Long-CoT
  - Why needed: The paper distinguishes between standard CoT and "Long" reasoning (up to 32k tokens), requiring understanding of extended thinking duration
  - Quick check: How does the paper define "Long Problems" differently from just a difficult problem? (Answer: Composite/Easier vs. Single/Hard)

- **Concept:** Supervised Fine-Tuning (SFT) with LoRA
  - Why needed: The "Long1K-32B" model is produced via LoRA fine-tuning, not Reinforcement Learning (RL), requiring understanding of parameter-efficient tuning
  - Quick check: What specific parameters (rank/alpha) were used for the LoRA configuration in the experiment? (Answer: rank 16, alpha 32)

- **Concept:** Tokenization and Context Windows
  - Why needed: The core contribution relies on 32k token sequences, requiring knowledge of memory constraints for 32k context training
  - Quick check: Why is the "composite" method of data generation necessary for utilizing 32k contexts? (Answer: Difficult problems are scarce; composite problems allow arbitrary length extension)

## Architecture Onboarding

- **Component map:** Data Generator -> Tokenizer -> Trainer (LoRA) -> Base Model (Qwen2.5-32B-Instruct)
- **Critical path:** The Data Synthesis pipeline is the bottleneck. The paper relies on a specific prompt template to splice problems. Weak connectors may prevent generalization of "long" reasoning behavior.
- **Design tradeoffs:**
  - Scarcity vs. Synthetic: Difficult problems are hard (low data volume) but natural; synthetic long problems are easy (infinite data) but require ensuring concatenated flow isn't jarring
  - Compute vs. Accuracy: Training on 32k tokens is memory intensive (batch size=2 on 8 GPUs); significant compute for only 1,000 samples
- **Failure signatures:**
  - Overfitting to Splicing: Model might learn to predict "Now I will turn to the second problem" rigidly; mitigated by including 200 random single problems
  - Context Collapse: If attention mechanism cannot bridge full 32k tokens, the "scaling law" fails
- **First 3 experiments:**
  1. Validation of Scaling: Train identical models on 1.5k, 3k, 6k, and 12k token datasets to reproduce the log-linear curve on MATH500
  2. Ablation on Difficulty: Train one model on "Difficult Problems(Single)" and another on "Long Problems(Double)" with equal token counts; verify Long/Double model matches performance
  3. Connector Sensitivity: Test different "connective" prompts in synthetic data generation to see if diverse phrasing improves robustness over static templates

## Open Questions the Paper Calls Out
- How to balance the relationship between inference length and task difficulty in different task types are issues that deserve in-depth investigation
- The framework exhibits potential for extension to RLHF-based models
- Whether the length-focused training framework can effectively extend to models trained with Reinforcement Learning from Human Feedback (RLHF)

## Limitations
- The central claim rests on synthetic data generation via problem concatenation, which may not generalize to naturally occurring long reasoning chains
- The 32k token context requirement creates significant computational barriers for reproduction
- The observed log-linear scaling relationship was demonstrated only within a limited range (1.5k to 12k tokens)

## Confidence
- **High Confidence:** Experimental finding that models trained on longer reasoning sequences achieve comparable performance to those trained on shorter difficult problems on AIME2024 and MATH500
- **Medium Confidence:** The log-linear scaling law relating reasoning length to performance, though extrapolation beyond 12k tokens remains unproven
- **Low Confidence:** The generalizability of the concatenation method to produce "arbitrarily long" reasoning sequences without introducing artifacts

## Next Checks
1. **Scaling Law Boundary Test:** Train models on reasoning lengths extending beyond 12k tokens (e.g., 24k, 32k) to empirically verify whether the log-linear relationship holds or shows signs of saturation
2. **Natural vs. Synthetic Long Reasoning Comparison:** Construct a dataset of naturally occurring long reasoning chains and compare performance against the synthetic concatenated approach on the same model architecture
3. **Cross-Domain Transfer Validation:** Evaluate Long1K-32B performance on reasoning tasks outside mathematics (e.g., code generation, logical inference) to determine whether the length-based training approach generalizes beyond symbolic math problems