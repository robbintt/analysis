---
ver: rpa2
title: 'A2TTS: TTS for Low Resource Indian Languages'
arxiv_id: '2507.15272'
source_url: https://arxiv.org/abs/2507.15272
tags:
- speaker
- speech
- dataset
- languages
- indicsuperb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating high-quality, speaker-consistent
  speech for unseen speakers in low-resource Indian languages using a diffusion-based
  TTS system. The core method integrates a speaker encoder with a cross-attention-based
  duration predictor to condition both prosody and timing on reference audio, enabling
  zero-shot speaker adaptation.
---

# A2TTS: TTS for Low Resource Indian Languages

## Quick Facts
- **arXiv ID**: 2507.15272
- **Source URL**: https://arxiv.org/abs/2507.15272
- **Reference count**: 3
- **Primary result**: Diffusion-based TTS with zero-shot speaker adaptation achieves average speaker similarity score (SIM-O) of 0.7548 across seven Indian languages.

## Executive Summary
This work introduces A2TTS, a diffusion-based text-to-speech system designed for low-resource Indian languages that enables high-quality, speaker-consistent synthesis for unseen speakers using only short reference audio. The system integrates a speaker encoder with a cross-attention-based duration predictor to condition both prosody and timing on reference audio, allowing zero-shot speaker adaptation without fine-tuning. Evaluated on the IndicSUPERB dataset across seven Indian languages, A2TTS significantly outperforms the Grad-TTS baseline in speaker similarity (SIM-O: 0.73-0.77) while maintaining competitive character error rates, demonstrating strong potential for personalized TTS in diverse linguistic settings.

## Method Summary
A2TTS adapts Grad-TTS for multispeaker synthesis by incorporating a pre-trained speaker encoder (UnitSpeech, trained on VoxCeleb2) to extract speaker embeddings from reference audio, which condition both the duration predictor and DDPM decoder. A cross-attention mechanism uses text embeddings as queries and a fixed 2-second reference mel spectrogram (unrelated to input text) as keys/values to predict phoneme durations, transferring speaker-specific timing patterns. Classifier-free guidance at inference amplifies conditioning effects for improved pronunciation and speaker consistency. The system is trained on IndicSUPERB across seven Indian languages using a 2000-epoch schedule with doubled channel dimensions compared to the baseline, and employs HiFi-GAN for waveform synthesis.

## Key Results
- Achieves average SIM-O speaker similarity score of 0.7548 across seven Indian languages
- Outperforms Grad-TTS baseline in speaker similarity (0.73-0.77 range vs. baseline)
- Maintains character error rates between 16.9%-47.9% across languages, demonstrating robustness to diverse phonological structures

## Why This Works (Mechanism)

### Mechanism 1: Speaker Embedding Conditioning for Zero-Shot Adaptation
Extracting speaker embeddings from short reference audio enables voice cloning for unseen speakers without fine-tuning. A pre-trained speaker encoder converts reference audio into a compact embedding vector that conditions both duration prediction and DDPM decoding, steering synthesis toward target speaker characteristics. During inference, classifier-free guidance amplifies this conditioning by contrasting speaker-conditioned scores against an unconditional baseline. This works because speaker embeddings capture identity-relevant features that generalize across utterances, though performance depends on reference audio quality and similarity to training data distribution.

### Mechanism 2: Cross-Attention Duration Prediction from Reference Prosody
Using reference mel spectrogram as attention keys/values transfers speaker-specific timing patterns to duration prediction. Text embeddings query the reference mel to predict phoneme durations, learning prosodic characteristics rather than content-specific alignments. The reference must contain different linguistic content than the target to prevent memorization. This mechanism assumes prosodic timing is speaker-specific and transferable, which is supported by evidence that duration modeling significantly impacts prosody quality in Indian TTS systems.

### Mechanism 3: Classifier-Free Guidance for Enhanced Speaker Control
Inference-time CFG amplification improves pronunciation and speaker consistency without modifying training. The score function combines conditioned and unconditional estimates, where the unconditional term uses the dataset-wide mean mel spectrogram. The guidance scale γ controls the tradeoff between diversity and conditioning adherence. This mechanism enhances speaker control by strengthening the influence of speaker and text conditions during inference, though excessive guidance can cause over-articulation artifacts.

## Foundational Learning

- **Diffusion models (DDPM) for speech synthesis**: Why needed - core decoder uses iterative denoising to generate mel spectrograms; understanding forward/reverse processes is essential for debugging. Quick check - Can you explain why diffusion models iteratively refine noise rather than directly predicting the output?

- **Speaker embeddings (x-vectors, d-vectors)**: Why needed - system relies on pre-trained encoders to compress voice identity into fixed vectors; understanding embedding spaces informs troubleshooting speaker similarity failures. Quick check - What properties should a good speaker embedding have (e.g., speaker-discriminative, channel-invariant)?

- **Cross-attention mechanisms**: Why needed - duration prediction uses cross-attention between text and reference audio; understanding query/key/value roles helps diagnose alignment failures. Quick check - In cross-attention, what happens if keys and values come from a different modality than queries?

## Architecture Onboarding

- **Component map**: Text Encoder -> Speaker Encoder -> Cross-Attention Duration Predictor -> DDPM Decoder -> CFG Module -> HiFi-GAN Vocoder
- **Critical path**: Input text → phonemes → Text Encoder → Et; Reference audio → Speaker Encoder → es; Reference audio → Mel spectrogram → Cross-Attention(Et, M, M) → Duration Predictor → D; Et + D + es → DDPM Decoder → Mel spectrogram; Mel spectrogram + CFG → Refined mel → HiFi-GAN → Audio
- **Design tradeoffs**: Channel doubling improves multi-speaker modeling but increases memory/compute; separate reference for embedding vs. duration attention prevents overfitting but requires 2 reference segments; language-specific models optimize per-language quality at maintenance cost.
- **Failure signatures**: Low speaker similarity (SIM-O < 0.70) suggests speaker encoder input quality issues; unnatural prosody indicates reference mel content overlap or cross-attention weight problems; robotic speech suggests excessive CFG guidance; high CER points to phoneme coverage or ASR alignment issues.
- **First 3 experiments**: 1) Baseline reproduction: Train Hindi subset with Grad-TTS vs. A2TTS; compare SIM-O and CER. 2) Ablation on cross-attention duration: Compare with/without reference mel attention; measure duration accuracy and MOS. 3) CFG sweep: Test γ ∈ {1.0, 1.5, 2.0, 2.5, 3.0}; plot SIM-O vs. naturalness tradeoff.

## Open Questions the Paper Calls Out
- Can the cross-attention duration predictor generalize to languages outside the seven Indic languages tested, particularly those with significantly different phonological structures?
- Would incorporating a speaker encoder trained specifically on Indian language speech improve speaker similarity compared to the VoxCeleb2-trained encoder?
- What is the optimal length and selection strategy for the reference mel spectrogram to maximize prosody transfer without overfitting?

## Limitations
- Channel dimensions after "doubling" are not specified, creating architectural ambiguity
- CFG guidance scale γ value is provided as a formula without recommended settings
- Training requires 2000 epochs with 8×H100 GPUs, making reproduction computationally expensive

## Confidence
- **High Confidence**: Core diffusion architecture with speaker embedding conditioning, cross-attention duration prediction, and overall evaluation framework
- **Medium Confidence**: Specific architectural choices like channel doubling and UnitSpeech encoder integration
- **Low Confidence**: Exact hyperparameter settings across languages, cross-attention implementation details, and diffusion training schedule specifics

## Next Checks
1. **Ablation on Duration Prediction**: Compare cross-attention duration predictor against baseline duration model without reference mel input, measuring duration prediction accuracy and MOS for naturalness.
2. **CFG Scale Sweep**: Systematically test CFG guidance scales γ ∈ {1.0, 1.5, 2.0, 2.5, 3.0} on held-out speakers for each language, plotting SIM-O against MOS to identify optimal tradeoff.
3. **Speaker Encoder Generalization Test**: Evaluate UnitSpeech speaker encoder performance on held-out Indian language speakers not in VoxCeleb2, measuring SIM-O degradation to quantify cross-lingual generalization.