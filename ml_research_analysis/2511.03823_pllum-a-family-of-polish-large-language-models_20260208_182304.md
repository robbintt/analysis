---
ver: rpa2
title: 'PLLuM: A Family of Polish Large Language Models'
arxiv_id: '2511.03823'
source_url: https://arxiv.org/abs/2511.03823
tags:
- polish
- language
- pllum
- data
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PLLuM introduces a family of 18 open-source large language models
  tailored for Polish, addressing the gap in high-quality, culturally relevant language
  technology for non-English languages. Developed by a consortium of major Polish
  research institutions, PLLuM includes models ranging from 8B to 70B parameters,
  trained on a new 140-billion-token Polish corpus and aligned using a comprehensive
  Responsible AI framework.
---

# PLLuM: A Family of Polish Large Language Models

## Quick Facts
- arXiv ID: 2511.03823
- Source URL: https://arxiv.org/abs/2511.03823
- Reference count: 22
- Primary result: 18 open-source Polish LLMs (8B-70B params) with state-of-the-art benchmarks and a safety-aligned public administration assistant prototype

## Executive Summary
PLLuM introduces a family of 18 open-source large language models tailored for Polish, addressing the gap in high-quality, culturally relevant language technology for non-English languages. Developed by a consortium of major Polish research institutions, PLLuM includes models ranging from 8B to 70B parameters, trained on a new 140-billion-token Polish corpus and aligned using a comprehensive Responsible AI framework. The models achieve state-of-the-art performance on Polish language benchmarks, with the 70B variant scoring 89.7% on a rule-based RAG evaluation and 87.6% on LLM-as-a-judge assessment. A hybrid output correction module ensures safety and factuality, reducing unsafe completions by an order of magnitude while maintaining 94% helpfulness. PLLuM demonstrates practical utility in a public administration assistant prototype, showcasing its potential for secure, high-impact AI applications in governance and digital public services.

## Method Summary
PLLuM is developed through a multi-stage pipeline starting with continual pre-training of Llama 3.1, Mistral, and Mixtral backbones on a 140-billion-token Polish corpus, followed by supervised fine-tuning on a 77k organic instruction dataset (PLLuMIC), and culminating in preference optimization using a 100k native Polish preference corpus with ORPO. The models are enhanced with a hybrid output correction system (Guard) combining neural classifiers and regex filters to ensure safety and factuality. The approach emphasizes native data curation and alignment to avoid linguistic artifacts from translated English datasets, achieving superior performance on Polish benchmarks while maintaining robustness against adversarial attacks.

## Key Results
- State-of-the-art performance on Polish benchmarks with the 70B model scoring 89.7% on RAG-IFEval and 87.6% on LLM-as-a-judge
- Hybrid Guard system reduces unsafe completions by an order of magnitude while maintaining 94% helpfulness
- Successful deployment in a public administration assistant prototype with secure retrieval-augmented generation capabilities
- Demonstrated effectiveness of native preference alignment over translated English datasets, avoiding linguistic artifacts and unnatural phrasing

## Why This Works (Mechanism)

### Mechanism 1: Continual Pre-training with Annealing
Adapting existing multilingual models via Continual Pre-training (CPT) on a curated corpus is more effective for mid-sized data regimes (~140B tokens) than training from scratch, provided a final annealing phase is used. The base models leverage prior multilingual knowledge, reducing data volume required for general language acquisition. The annealing phase subsequently trains the model on a small, high-quality dataset with a decaying learning rate to smooth the transition to instruction tuning and reinforce desirable behaviors.

### Mechanism 2: Hybrid Output Correction (Guard)
A modular, post-generation filtering system combining rule-based heuristics and neural classifiers can significantly reduce unsafe outputs while maintaining high helpfulness. Guard acts as a transparent proxy intercepting LLM responses, using adapters (HerBERT-based classifiers) for harmful/erotic content and regex for PII/profanity. In synchronous mode, if a policy is violated, the system issues a "repair prompt" to the LLM to self-correct the specific offending segment.

### Mechanism 3: Native Preference Alignment
Aligning models using a native, culturally grounded preference corpus yields superior performance compared to translated English datasets, specifically reducing linguistic artifacts and unnatural phrasing. While translated data allows for early pipeline validation, the final alignment stage relies exclusively on a manually curated Polish preference corpus, forcing the model to learn culturally specific norms and linguistic styles rather than calquing English patterns.

## Foundational Learning

- **Concept: Knowledge Distillation vs. Synthetic Data**
  - Why needed here: PLLuM uses synthetic instructions for "knowledge distillation" (Section 5.4). Understanding the difference between simple data augmentation and distillation (where a teacher model transfers reasoning capabilities) is crucial for evaluating the quality of the instruction corpus.
  - Quick check question: Can you identify why the authors limited synthetic data to ~7% of the training set?

- **Concept: Preference Optimization (ORPO)**
  - Why needed here: The paper selects ORPO (Odds Ratio Preference Optimization) over DPO and KTO for the final alignment stage.
  - Quick check question: Why does the paper suggest ORPO is particularly effective at maintaining SFT capabilities while improving alignment?

- **Concept: RAG Citation Grounding**
  - Why needed here: The public administration prototype relies on the model citing documents correctly.
  - Quick check question: In the RAG-IFEval benchmark, what is the specific penalty for citing a document that is not relevant to the question?

## Architecture Onboarding

- **Component map:** Data Layer (Pre-training Corpus → Instruction Corpus → Preference Corpus) → Model Layer (Base Models → SFT Models → Chat/Aligned Models) → Safety Layer (Guard Proxy) → Application Layer (ShpaRAG → Public Admin Assistant)

- **Critical path:** The sequence of Data Curation (Legal/Quality) → CPT (Annealing) → Native Preference Alignment (ORPO). Skipping the native alignment phase creates the specific failure mode of "unnatural phrasing" noted in the paper.

- **Design tradeoffs:** FRR vs. Safety (system accepts higher FRR 5-10% to ensure near-zero safety violations); Token Efficiency vs. Stability (explored vocabulary extension 30% efficiency gain but found it destabilized performance compared to standard CPT on limited corpus)

- **Failure signatures:** Linguistic Transfer (model uses English formatting/punctuation rules in Polish text); Hallucinated Citations (RAG generator cites non-existent or irrelevant documents); Over-refusal (Guard triggers on benign administrative queries due to aggressive regex matching)

- **First 3 experiments:**
  1. Tokenizer Fidelity Test: Compare token efficiency of baseline Llama-3.1-8B vs. theoretical extended-vocabulary variant on Polish validation set to validate 23-30% efficiency claim
  2. RAG-IFEval Baseline: Run baseline Llama-3.1-70B model on RAG-IFEval benchmark to establish lower bound for "Correctness" and "Safety" scores before applying PLLuM fine-tuning
  3. Guard Stress Test: Submit 100 adversarial prompts to Chat model with and without Guard proxy enabled to measure delta in Attack Success Rate (ASR)

## Open Questions the Paper Calls Out

### Open Question 1
Does vocabulary extension combined with continual pre-training on the full corpus improve performance for low-resource languages like Polish? Current experiments showed improved token efficiency but inferior benchmark performance compared to full continual pre-training, possibly due to insufficient data scale for extended embeddings. Comparative benchmarks of models trained with vocabulary extension on entire 140B-token corpus versus standard continual pre-training would resolve this.

### Open Question 2
What is the minimum corpus size required for a Polish LLM trained from scratch to outperform an adapted multilingual foundation model? It is currently unclear if the observed performance gap is due to the training strategy or sheer volume of available high-quality Polish data. Scaling laws analysis comparing perplexity and benchmark performance of scratch-trained models against adapted models across varying corpus sizes would resolve this.

### Open Question 3
Can safety alignment be optimized to maintain robustness without significantly increasing False Rejection Rates (FRR) for benign queries? The current alignment training successfully mitigates adversarial attacks but simultaneously causes the model to be overly cautious, degrading helpfulness on safe inputs. Development of an alignment technique that maintains low ASR while reducing FRR to levels comparable to non-aligned models (<1%) would resolve this.

## Limitations
- Evaluation framework relies heavily on synthetic benchmarks and rule-based metrics that may not fully capture real-world performance
- Guard safety system's effectiveness against adversarial attacks remains partially untested, with only one documented case of jailbreak prompt success
- Corpus composition and full training details are partially disclosed, limiting reproducibility

## Confidence
**High Confidence:** Core architectural claims about continual pre-training effectiveness and hybrid Guard system are well-supported by empirical results and ablation studies
**Medium Confidence:** Preference alignment mechanism's superiority over translated datasets is demonstrated but generalizability to other non-English languages requires further validation
**Low Confidence:** Long-term stability of hybrid output correction system against evolving adversarial techniques has not been established; model's performance on truly out-of-distribution prompts beyond evaluated safety categories remains uncertain

## Next Checks
1. **Adversarial Robustness Test:** Design and execute comprehensive red-teaming campaign using Polish-specific adversarial patterns (code-switching, regional dialects, cultural references) to stress-test Guard system's regex and classifier components, measuring ASR and FRR under sustained attack

2. **Generalization Benchmark:** Evaluate 70B model on held-out set of organic Polish web text and conversational data not used in training, comparing perplexity and task completion rates against multilingual baseline (Llama-3.1-70B) to quantify benefit of native alignment beyond curated benchmarks

3. **Cross-Lingual Transfer Analysis:** Conduct systematic ablation study by fine-tuning subset of models on translated English preference dataset versus native Polish dataset, measuring specific linguistic artifacts (calqued phrasing, incorrect morphology) using automated metrics and human evaluation to isolate mechanism of native preference alignment